Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年4月9日 13:10
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Fri  5 Apr 24 18:00:00 GMT  to  Mon  8 Apr 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2404.04267
Date: Tue, 19 Mar 2024 16:06:27 GMT   (430kb)

Title: What AIs are not Learning (and Why)
Authors: Mark Stefik
Categories: cs.AI cs.HC
Comments: 10 pages
\\
  What applications is AI ready for? Advances in deep learning and generative
approaches have produced AI that learn from massive online data and outperform
manually built AIs. Some AIs outperform people. It is easy (but misleading) to
conclude that today's AI technologies can learn to do everything. Conversely,
it is striking that big data, deep learning, and generative AI have had so
little impact on robotics. For example, today's autonomous robots do not learn
to provide home care or to be nursing assistants. Instead, current projects
rely on mathematical models, planning frameworks, and reinforcement learning.
These methods have not lead to the leaps in performance and generality seen
with deep learning. Today's AIs do not learn to do such applications because
they do not collect, use, and effectively generalize the necessary experiential
data by interacting with the world including people. Aspirationally, robotic
AIs would learn experientially, learn from people, serve people broadly, and
collaborate with them. Getting to such a future requires understanding the
opportunity and creating a path to get there. A path forward would combine
multimodal sensing and motor control technology from robotics with deep
learning technology adapted for embodied systems. Analogous to foundation
classes in deep learning, it would create experiential foundation classes.
Success would greatly increase the broad utility of AI robots and grow the
market for them. This would lead to lower costs and democratize AI.
\\ ( https://arxiv.org/abs/2404.04267 ,  430kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04289
Date: Thu, 4 Apr 2024 03:01:57 GMT   (36kb)

Title: Designing for Human-Agent Alignment: Understanding what humans want from
  their agents
Authors: Nitesh Goyal, Minsuk Chang, Michael Terry
Categories: cs.AI cs.HC cs.LG
Comments: Human-AI Alignment, Human-Agent Alignment, Agents, Generative AI,
  Large Language Models
ACM-class: I.2.0
DOI: 10.1145/3613905.3650948
\\
  Our ability to build autonomous agents that leverage Generative AI continues
to increase by the day. As builders and users of such agents it is unclear what
parameters we need to align on before the agents start performing tasks on our
behalf. To discover these parameters, we ran a qualitative empirical research
study about designing agents that can negotiate during a fictional yet
relatable task of selling a camera online. We found that for an agent to
perform the task successfully, humans/users and agents need to align over 6
dimensions: 1) Knowledge Schema Alignment 2) Autonomy and Agency Alignment 3)
Operational Alignment and Training 4) Reputational Heuristics Alignment 5)
Ethics Alignment and 6) Human Engagement Alignment. These empirical findings
expand previous work related to process and specification alignment and the
need for values and safety in Human-AI interactions. Subsequently we discuss
three design directions for designers who are imagining a world filled with
Human-Agent collaborations.
\\ ( https://arxiv.org/abs/2404.04289 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04298
Date: Thu, 4 Apr 2024 20:27:37 GMT   (1478kb,D)

Title: SELF-[IN]CORRECT: LLMs Struggle with Refining Self-Generated Responses
Authors: Dongwei Jiang, Jingyu Zhang, Orion Weller, Nathaniel Weir, Benjamin
  Van Durme, Daniel Khashabi
Categories: cs.AI cs.CL cs.LG
\\
  Can LLMs continually improve their previous outputs for better results? An
affirmative answer would require LLMs to be better at discriminating among
previously-generated alternatives, than generating initial responses. We
explore the validity of this hypothesis in practice. We first introduce a
unified framework that allows us to compare the generative and discriminative
capability of any model on any task. Then, in our resulting experimental
analysis of several LLMs, we do not observe the performance of those models on
discrimination to be reliably better than generation. We hope these findings
inform the growing literature on self-improvement AI systems.
\\ ( https://arxiv.org/abs/2404.04298 ,  1478kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04308
Date: Fri, 5 Apr 2024 07:31:24 GMT   (13243kb,D)

Title: Visual Knowledge in the Big Model Era: Retrospect and Prospect
Authors: Wenguan Wang, Yi Yang, Yunhe Pan
Categories: cs.AI cs.CV cs.LG
\\
  Visual knowledge is a new form of knowledge representation that can
encapsulate visual concepts and their relations in a succinct, comprehensive,
and interpretable manner, with a deep root in cognitive psychology. As the
knowledge about the visual world has been identified as an indispensable
component of human cognition and intelligence, visual knowledge is poised to
have a pivotal role in establishing machine intelligence. With the recent
advance of Artificial Intelligence (AI) techniques, large AI models (or
foundation models) have emerged as a potent tool capable of extracting
versatile patterns from broad data as implicit knowledge, and abstracting them
into an outrageous amount of numeric parameters. To pave the way for creating
visual knowledge empowered AI machines in this coming wave, we present a timely
review that investigates the origins and development of visual knowledge in the
pre-big model era, and accentuates the opportunities and unique role of visual
knowledge in the big model era.
\\ ( https://arxiv.org/abs/2404.04308 ,  13243kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04326
Date: Fri, 5 Apr 2024 18:00:07 GMT   (6074kb,D)

Title: Hypothesis Generation with Large Language Models
Authors: Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, and
  Chenhao Tan
Categories: cs.AI cs.CL cs.CY cs.LG
Comments: 26 pages, 6 figures, code link:
  https://github.com/ChicagoHAI/hypothesis_generation
\\
  Effective generation of novel hypotheses is instrumental to scientific
progress. So far, researchers have been the main powerhouse behind hypothesis
generation by painstaking data analysis and thinking (also known as the Eureka
moment). In this paper, we examine the potential of large language models
(LLMs) to generate hypotheses. We focus on hypothesis generation based on data
(i.e., labeled examples). To enable LLMs to handle arbitrarily long contexts,
we generate initial hypotheses from a small number of examples and then update
them iteratively to improve the quality of hypotheses. Inspired by multi-armed
bandits, we design a reward function to inform the exploitation-exploration
tradeoff in the update process. Our algorithm is able to generate hypotheses
that enable much better predictive performance than few-shot prompting in
classification tasks, improving accuracy by 31.7% on a synthetic dataset and by
13.9%, 3.3% and, 24.9% on three real-world datasets. We also outperform
supervised learning by 12.8% and 11.2% on two challenging real-world datasets.
Furthermore, we find that the generated hypotheses not only corroborate
human-verified theories but also uncover new insights for the tasks.
\\ ( https://arxiv.org/abs/2404.04326 ,  6074kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04344
Date: Fri, 5 Apr 2024 18:27:04 GMT   (63kb)

Title: A Repository for Formal Contexts
Authors: Tom Hanika and Robert J\"aschke
Categories: cs.AI cs.DL
Comments: 16 pages
\\
  Data is always at the center of the theoretical development and investigation
of the applicability of formal concept analysis. It is therefore not surprising
that a large number of data sets are repeatedly used in scholarly articles and
software tools, acting as de facto standard data sets. However, the
distribution of the data sets poses a problem for the sustainable development
of the research field. There is a lack of a central location that provides and
describes FCA data sets and links them to already known analysis results. This
article analyses the current state of the dissemination of FCA data sets,
presents the requirements for a central FCA repository, and highlights the
challenges for this.
\\ ( https://arxiv.org/abs/2404.04344 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04436
Date: Fri, 5 Apr 2024 22:30:47 GMT   (1217kb,D)

Title: AI Knowledge and Reasoning: Emulating Expert Creativity in Scientific
  Research
Authors: Anirban Mukherjee, Hannah Hanwen Chang
Categories: cs.AI
\\
  We investigate whether modern AI can emulate expert creativity in complex
scientific endeavors. We introduce novel methodology that utilizes original
research articles published after the AI's training cutoff, ensuring no prior
exposure, mitigating concerns of rote memorization and prior training. The AI
are tasked with redacting findings, predicting outcomes from redacted research,
and assessing prediction accuracy against reported results. Analysis on 589
published studies in four leading psychology journals over a 28-month period,
showcase the AI's proficiency in understanding specialized research, deductive
reasoning, and evaluating evidentiary alignment--cognitive hallmarks of human
subject matter expertise and creativity. These findings suggest the potential
of general-purpose AI to transform academia, with roles requiring
knowledge-based creativity become increasingly susceptible to technological
substitution.
\\ ( https://arxiv.org/abs/2404.04436 ,  1217kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04442
Date: Fri, 5 Apr 2024 22:59:02 GMT   (1020kb,D)

Title: Exploring Autonomous Agents through the Lens of Large Language Models: A
  Review
Authors: Saikat Barua
Categories: cs.AI
Comments: 47 pages, 5 figures
\\
  Large Language Models (LLMs) are transforming artificial intelligence,
enabling autonomous agents to perform diverse tasks across various domains.
These agents, proficient in human-like text comprehension and generation, have
the potential to revolutionize sectors from customer service to healthcare.
However, they face challenges such as multimodality, human value alignment,
hallucinations, and evaluation. Techniques like prompting, reasoning, tool
utilization, and in-context learning are being explored to enhance their
capabilities. Evaluation platforms like AgentBench, WebArena, and ToolLLM
provide robust methods for assessing these agents in complex scenarios. These
advancements are leading to the development of more resilient and capable
autonomous agents, anticipated to become integral in our digital lives,
assisting in tasks from email responses to disease diagnosis. The future of AI,
with LLMs at the forefront, is promising.
\\ ( https://arxiv.org/abs/2404.04442 ,  1020kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04538
Date: Sat, 6 Apr 2024 07:39:44 GMT   (2115kb,D)

Title: Soft-Prompting with Graph-of-Thought for Multi-modal Representation
  Learning
Authors: Juncheng Yang, Zuchao Li, Shuai Xie, Wei Yu, Shijun Li, Bo Du
Categories: cs.AI
Comments: This paper is accepted to LREC-COLING 2024
\\
  The chain-of-thought technique has been received well in multi-modal tasks.
It is a step-by-step linear reasoning process that adjusts the length of the
chain to improve the performance of generated prompts. However, human thought
processes are predominantly non-linear, as they encompass multiple aspects
simultaneously and employ dynamic adjustment and updating mechanisms.
Therefore, we propose a novel Aggregation-Graph-of-Thought (AGoT) mechanism for
soft-prompt tuning in multi-modal representation learning. The proposed AGoT
models the human thought process not only as a chain but also models each step
as a reasoning aggregation graph to cope with the overlooked multiple aspects
of thinking in single-step reasoning. This turns the entire reasoning process
into prompt aggregation and prompt flow operations. Experiments show that our
multi-modal model enhanced with AGoT soft-prompting achieves good results in
several tasks such as text-image retrieval, visual question answering, and
image recognition. In addition, we demonstrate that it has good domain
generalization performance due to better reasoning.
\\ ( https://arxiv.org/abs/2404.04538 ,  2115kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04540
Date: Sat, 6 Apr 2024 07:44:40 GMT   (917kb,D)

Title: The Case for Developing a Foundation Model for Planning-like Tasks from
  Scratch
Authors: Biplav Srivastava, Vishal Pallagani
Categories: cs.AI
\\
  Foundation Models (FMs) have revolutionized many areas of computing,
including Automated Planning and Scheduling (APS). For example, a recent study
found them useful for planning problems: plan generation, language translation,
model construction, multi-agent planning, interactive planning, heuristics
optimization, tool integration, and brain-inspired planning. Besides APS, there
are many seemingly related tasks involving the generation of a series of
actions with varying guarantees of their executability to achieve intended
goals, which we collectively call planning-like (PL) tasks like business
processes, programs, workflows, and guidelines, where researchers have
considered using FMs. However, previous works have primarily focused on
pre-trained, off-the-shelf FMs and optionally fine-tuned them. This paper
discusses the need for a comprehensive FM for PL tasks from scratch and
explores its design considerations. We argue that such an FM will open new and
efficient avenues for PL problem-solving, just like LLMs are creating for APS.
\\ ( https://arxiv.org/abs/2404.04540 ,  917kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04619
Date: Sat, 6 Apr 2024 12:51:00 GMT   (3425kb,D)

Title: Do We Really Need a Complex Agent System? Distill Embodied Agent into a
  Single Model
Authors: Zhonghan Zhao, Ke Ma, Wenhao Chai, Xuan Wang, Kewei Chen, Dongxu Guo,
  Yanting Zhang, Hongwei Wang and Gaoang Wang
Categories: cs.AI cs.CV
Comments: arXiv admin note: text overlap with arXiv:2403.08282
\\
  With the power of large language models (LLMs), open-ended embodied agents
can flexibly understand human instructions, generate interpretable guidance
strategies, and output executable actions. Nowadays, Multi-modal Language
Models~(MLMs) integrate multi-modal signals into LLMs, further bringing richer
perception to entity agents and allowing embodied agents to perceive
world-understanding tasks more delicately. However, existing works: 1) operate
independently by agents, each containing multiple LLMs, from perception to
action, resulting in gaps between complex tasks and execution; 2) train MLMs on
static data, struggling with dynamics in open-ended scenarios; 3) input prior
knowledge directly as prompts, suppressing application flexibility. We propose
STEVE-2, a hierarchical knowledge distillation framework for open-ended
embodied tasks, characterized by 1) a hierarchical system for multi-granular
task division, 2) a mirrored distillation method for parallel simulation data,
and 3) an extra expert model for bringing additional knowledge into parallel
simulation. After distillation, embodied agents can complete complex,
open-ended tasks without additional expert guidance, utilizing the performance
and knowledge of a versatile MLM. Extensive evaluations on navigation and
creation tasks highlight the superior performance of STEVE-2 in open-ended
tasks, with $1.4 \times$ - $7.3 \times$ in performance.
\\ ( https://arxiv.org/abs/2404.04619 ,  3425kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04667
Date: Sat, 6 Apr 2024 15:50:19 GMT   (20317kb,D)

Title: Autonomous Artificial Intelligence Agents for Clinical Decision Making
  in Oncology
Authors: Dyke Ferber, Omar S. M. El Nahhas, Georg W\"olflein, Isabella C.
  Wiest, Jan Clusmann, Marie-Elisabeth Le{\ss}man, Sebastian Foersch,
  Jacqueline Lammert, Maximilian Tschochohei, Dirk J\"ager, Manuel
  Salto-Tellez, Nikolaus Schultz, Daniel Truhn, Jakob Nikolas Kather
Categories: cs.AI q-bio.TO
Comments: 91 pages, 2 Figures
\\
  Multimodal artificial intelligence (AI) systems have the potential to enhance
clinical decision-making by interpreting various types of medical data.
However, the effectiveness of these models across all medical fields is
uncertain. Each discipline presents unique challenges that need to be addressed
for optimal performance. This complexity is further increased when attempting
to integrate different fields into a single model. Here, we introduce an
alternative approach to multimodal medical AI that utilizes the generalist
capabilities of a large language model (LLM) as a central reasoning engine.
This engine autonomously coordinates and deploys a set of specialized medical
AI tools. These tools include text, radiology and histopathology image
interpretation, genomic data processing, web searches, and document retrieval
from medical guidelines. We validate our system across a series of clinical
oncology scenarios that closely resemble typical patient care workflows. We
show that the system has a high capability in employing appropriate tools
(97%), drawing correct conclusions (93.6%), and providing complete (94%), and
helpful (89.2%) recommendations for individual patient cases while consistently
referencing relevant literature (82.5%) upon instruction. This work provides
evidence that LLMs can effectively plan and execute domain-specific models to
retrieve or synthesize new information when used as autonomous agents. This
enables them to function as specialist, patient-tailored clinical assistants.
It also simplifies regulatory compliance by allowing each component tool to be
individually validated and approved. We believe, that our work can serve as a
proof-of-concept for more advanced LLM-agents in the medical domain.
\\ ( https://arxiv.org/abs/2404.04667 ,  20317kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04735
Date: Sat, 6 Apr 2024 21:39:01 GMT   (1279kb,D)

Title: MACM: Utilizing a Multi-Agent System for Condition Mining in Solving
  Complex Mathematical Problems
Authors: Bin Lei
Categories: cs.AI
\\
  Recent advancements in large language models, such as GPT-4, have
demonstrated remarkable capabilities in processing standard queries. Despite
these advancements, their performance substantially declines in
\textbf{advanced mathematical problems requiring complex, multi-step logical
reasoning}. To enhance their inferential capabilities, current research has
delved into \textit{prompting engineering}, exemplified by methodologies such
as the Tree of Thought and Graph of Thought. Nonetheless, these existing
approaches encounter two significant limitations. Firstly, their effectiveness
in tackling complex mathematical problems is somewhat constrained. Secondly,
the necessity to design distinct prompts for individual problems hampers their
generalizability. In response to these limitations, this paper introduces the
\textit{Multi-Agent System for conditional Mining} (\textbf{MACM}) prompting
method. It not only resolves intricate mathematical problems but also
demonstrates strong generalization capabilities across various mathematical
contexts. With the assistance of MACM, the accuracy of GPT-4 Turbo on the most
challenging level five mathematical problems in the MATH dataset increase from
$\mathbf{54.68\%} \text{ to } \mathbf{76.73\%}$. The code is available in
\url{https://github.com/bin123apple/MACM}.
\\ ( https://arxiv.org/abs/2404.04735 ,  1279kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04752
Date: Sat, 6 Apr 2024 22:34:07 GMT   (2663kb,D)

Title: Challenges Faced by Large Language Models in Solving Multi-Agent
  Flocking
Authors: Peihan Li, Vishnu Menon, Bhavanaraj Gudiguntla, Daniel Ting and Lifeng
  Zhou
Categories: cs.AI
\\
  Flocking is a behavior where multiple agents in a system attempt to stay
close to each other while avoiding collision and maintaining a desired
formation. This is observed in the natural world and has applications in
robotics, including natural disaster search and rescue, wild animal tracking,
and perimeter surveillance and patrol. Recently, large language models (LLMs)
have displayed an impressive ability to solve various collaboration tasks as
individual decision-makers. Solving multi-agent flocking with LLMs would
demonstrate their usefulness in situations requiring spatial and decentralized
decision-making. Yet, when LLM-powered agents are tasked with implementing
multi-agent flocking, they fall short of the desired behavior. After extensive
testing, we find that agents with LLMs as individual decision-makers typically
opt to converge on the average of their initial positions or diverge from each
other. After breaking the problem down, we discover that LLMs cannot understand
maintaining a shape or keeping a distance in a meaningful way. Solving
multi-agent flocking with LLMs would enhance their ability to understand
collaborative spatial reasoning and lay a foundation for addressing more
complex multi-agent tasks. This paper discusses the challenges LLMs face in
multi-agent flocking and suggests areas for future improvement and research.
\\ ( https://arxiv.org/abs/2404.04752 ,  2663kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04818
Date: Sun, 7 Apr 2024 05:56:42 GMT   (6714kb,D)

Title: DWE+: Dual-Way Matching Enhanced Framework for Multimodal Entity Linking
Authors: Shezheng Song, Shasha Li, Shan Zhao, Xiaopeng Li, Chengyu Wang, Jie
  Yu, Jun Ma, Tianwei Yan, Bin Ji, Xiaoguang Mao
Categories: cs.AI cs.CV
Comments: under review on TOIS
\\
  Multimodal entity linking (MEL) aims to utilize multimodal information
(usually textual and visual information) to link ambiguous mentions to
unambiguous entities in knowledge base. Current methods facing main issues:
(1)treating the entire image as input may contain redundant information. (2)the
insufficient utilization of entity-related information, such as attributes in
images. (3)semantic inconsistency between the entity in knowledge base and its
representation. To this end, we propose DWE+ for multimodal entity linking.
DWE+ could capture finer semantics and dynamically maintain semantic
consistency with entities. This is achieved by three aspects: (a)we introduce a
method for extracting fine-grained image features by partitioning the image
into multiple local objects. Then, hierarchical contrastive learning is used to
further align semantics between coarse-grained information(text and image) and
fine-grained (mention and visual objects). (b)we explore ways to extract visual
attributes from images to enhance fusion feature such as facial features and
identity. (c)we leverage Wikipedia and ChatGPT to capture the entity
representation, achieving semantic enrichment from both static and dynamic
perspectives, which better reflects the real-world entity semantics.
Experiments on Wikimel, Richpedia, and Wikidiverse datasets demonstrate the
effectiveness of DWE+ in improving MEL performance. Specifically, we optimize
these datasets and achieve state-of-the-art performance on the enhanced
datasets. The code and enhanced datasets are released on
https://github.com/season1blue/DWET
\\ ( https://arxiv.org/abs/2404.04818 ,  6714kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04902
Date: Sun, 7 Apr 2024 10:02:09 GMT   (6664kb,D)

Title: AI2Apps: A Visual IDE for Building LLM-based AI Agent Applications
Authors: Xin Pang, Zhucong Li, Jiaxiang Chen, Yuan Cheng, Yinghui Xu, Yuan Qi
Categories: cs.AI
\\
  We introduce AI2Apps, a Visual Integrated Development Environment (Visual
IDE) with full-cycle capabilities that accelerates developers to build
deployable LLM-based AI agent Applications. This Visual IDE prioritizes both
the Integrity of its development tools and the Visuality of its components,
ensuring a smooth and efficient building experience.On one hand, AI2Apps
integrates a comprehensive development toolkit ranging from a prototyping
canvas and AI-assisted code editor to agent debugger, management system, and
deployment tools all within a web-based graphical user interface. On the other
hand, AI2Apps visualizes reusable front-end and back-end code as intuitive
drag-and-drop components. Furthermore, a plugin system named AI2Apps Extension
(AAE) is designed for Extensibility, showcasing how a new plugin with 20
components enables web agent to mimic human-like browsing behavior. Our case
study demonstrates substantial efficiency improvements, with AI2Apps reducing
token consumption and API calls when debugging a specific sophisticated
multimodal agent by approximately 90% and 80%, respectively. The AI2Apps,
including an online demo, open-source code, and a screencast video, is now
publicly accessible.
\\ ( https://arxiv.org/abs/2404.04902 ,  6664kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05012
Date: Sun, 7 Apr 2024 16:35:53 GMT   (10498kb,D)

Title: Towards Reliable and Empathetic Depression-Diagnosis-Oriented Chats
Authors: Kunyao Lan, Cong Ming, Binwei Yao, Lu Chen, Mengyue Wu
Categories: cs.AI
\\
  Chatbots can serve as a viable tool for preliminary depression diagnosis via
interactive conversations with potential patients. Nevertheless, the blend of
task-oriented and chit-chat in diagnosis-related dialogues necessitates
professional expertise and empathy. Such unique requirements challenge
traditional dialogue frameworks geared towards single optimization goals. To
address this, we propose an innovative ontology definition and generation
framework tailored explicitly for depression diagnosis dialogues, combining the
reliability of task-oriented conversations with the appeal of empathy-related
chit-chat. We further apply the framework to D$^4$, the only existing public
dialogue dataset on depression diagnosis-oriented chats. Exhaustive
experimental results indicate significant improvements in task completion and
emotional support generation in depression diagnosis, fostering a more
comprehensive approach to task-oriented chat dialogue system development and
its applications in digital mental health.
\\ ( https://arxiv.org/abs/2404.05012 ,  10498kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05074
Date: Sun, 7 Apr 2024 21:06:52 GMT   (76kb)

Title: On the Uniqueness of Solution for the Bellman Equation of LTL Objectives
Authors: Zetong Xuan, Alper Kamil Bozkurt, Miroslav Pajic, Yu Wang
Categories: cs.AI cs.RO
Comments: Accepted for the 2024 Learning for Dynamics and Control Conference
  (L4DC)
\\
  Surrogate rewards for linear temporal logic (LTL) objectives are commonly
utilized in planning problems for LTL objectives. In a widely-adopted surrogate
reward approach, two discount factors are used to ensure that the expected
return approximates the satisfaction probability of the LTL objective. The
expected return then can be estimated by methods using the Bellman updates such
as reinforcement learning. However, the uniqueness of the solution to the
Bellman equation with two discount factors has not been explicitly discussed.
We demonstrate with an example that when one of the discount factors is set to
one, as allowed in many previous works, the Bellman equation may have multiple
solutions, leading to inaccurate evaluation of the expected return. We then
propose a condition for the Bellman equation to have the expected return as the
unique solution, requiring the solutions for states inside a rejecting bottom
strongly connected component (BSCC) to be 0. We prove this condition is
sufficient by showing that the solutions for the states with discounting can be
separated from those for the states without discounting under this condition
\\ ( https://arxiv.org/abs/2404.05074 ,  76kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05223
Date: Mon, 8 Apr 2024 06:36:42 GMT   (698kb,D)

Title: ITA-ECBS: A Bounded-Suboptimal Algorithm for Combined Target-Assignment
  and Path-Finding Problem
Authors: Yimin Tang, Sven Koenig, Jiaoyang Li
Categories: cs.AI
\\
  Multi-Agent Path Finding (MAPF), i.e., finding collision-free paths for
multiple robots, plays a critical role in many applications. Sometimes,
assigning a specific target to each agent also presents a challenge. The
Combined Target-Assignment and Path-Finding (TAPF) problem, a variant of MAPF,
requires simultaneously assigning targets to agents and planning collision-free
paths. Several algorithms, including CBM, CBS-TA, and ITA-CBS, can optimally
solve the TAPF problem, with ITA-CBS being the leading method of flowtime.
However, the only existing suboptimal method ECBS-TA, is derived from CBS-TA
rather than ITA-CBS, and adapting the optimal ITA-CBS method to its
bounded-suboptimal variant is a challenge due to the variability of target
assignment solutions in different search nodes. We introduce ITA-ECBS as the
first bounded-suboptimal variant of ITA-CBS. ITA-ECBS employs focal search to
enhance efficiency and determines target assignments based on a new lower bound
matrix. We show that ITA-ECBS outperforms the baseline method ECBS-TA in 87.42%
of 54,033 test cases.
\\ ( https://arxiv.org/abs/2404.05223 ,  698kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05224
Date: Mon, 8 Apr 2024 06:40:03 GMT   (402kb,D)

Title: Iof-maint -- Modular maintenance ontology
Authors: Melinda Hodkiewicz, Caitlin Woods, Matt Selway, Markus Stumptner
Categories: cs.AI cs.LO
DOI: 10.26182/chzp-vs60
\\
  In this paper we present a publicly-available maintenance ontology
(Iof-maint). Iof-maint is a modular ontology aligned with the Industrial
Ontology Foundry Core (IOF Core) and contains 20 classes and 2 relations. It
provides a set of maintenance-specific terms used in a wide variety of
practical data-driven use cases. Iof-maint supports OWL DL reasoning, is
documented, and is actively maintained on GitHub. In this paper, we describe
the evolution of the Iof-maint reference ontology based on the extraction of
common concepts identified in a number of application ontologies working with
industry maintenance work order, procedure and failure mode data.
\\ ( https://arxiv.org/abs/2404.05224 ,  402kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05235
Date: Mon, 8 Apr 2024 07:01:35 GMT   (4387kb,D)

Title: Novelty Heuristics, Multi-Queue Search, and Portfolios for Numeric
  Planning
Authors: Dillon Z. Chen, Sylvie Thi\'ebaux
Categories: cs.AI
Comments: Extended version of SoCS 2024 paper
\\
  Heuristic search is a powerful approach for solving planning problems and
numeric planning is no exception. In this paper, we boost the performance of
heuristic search for numeric planning with various powerful techniques
orthogonal to improving heuristic informedness: numeric novelty heuristics, the
Manhattan distance heuristic, and exploring the use of multi-queue search and
portfolios for combining heuristics.
\\ ( https://arxiv.org/abs/2404.05235 ,  4387kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05259
Date: Mon, 8 Apr 2024 07:49:52 GMT   (1297kb,D)

Title: Cellular automata, many-valued logic, and deep neural networks
Authors: Yani Zhang and Helmut B\"olcskei
Categories: cs.AI
\\
  We develop a theory characterizing the fundamental capability of deep neural
networks to learn, from evolution traces, the logical rules governing the
behavior of cellular automata (CA). This is accomplished by first establishing
a novel connection between CA and Lukasiewicz propositional logic. While binary
CA have been known for decades to essentially perform operations in Boolean
logic, no such relationship exists for general CA. We demonstrate that
many-valued (MV) logic, specifically Lukasiewicz propositional logic,
constitutes a suitable language for characterizing general CA as logical
machines. This is done by interpolating CA transition functions to continuous
piecewise linear functions, which, by virtue of the McNaughton theorem, yield
formulae in MV logic characterizing the CA. Recognizing that deep rectified
linear unit (ReLU) networks realize continuous piecewise linear functions, it
follows that these formulae are naturally extracted from CA evolution traces by
deep ReLU networks. A corresponding algorithm together with a software
implementation is provided. Finally, we show that the dynamical behavior of CA
can be realized by recurrent neural networks.
\\ ( https://arxiv.org/abs/2404.05259 ,  1297kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05272
Date: Mon, 8 Apr 2024 08:02:18 GMT   (972kb,D)

Title: Constructing Data Transaction Chains Based on Opportunity Cost
  Exploration
Authors: Jie Liu, Tao Feng, Yan Jiang, Peizheng Wang, Chao Wu
Categories: cs.AI
\\
  Data trading is increasingly gaining attention. However, the inherent
replicability and privacy concerns of data make it challenging to directly
apply traditional trading theories to data markets. This paper compares data
trading markets with traditional ones, focusing particularly on how the
replicability and privacy of data impact data markets. We discuss how data's
replicability fundamentally alters the concept of opportunity cost in
traditional microeconomics within the context of data markets. Additionally, we
explore how to leverage this change to maximize benefits without compromising
data privacy. This paper outlines the constraints for data circulation within
the privacy domain chain and presents a model that maximizes data's value under
these constraints. Specific application scenarios are provided, and experiments
demonstrate the solvability of this model.
\\ ( https://arxiv.org/abs/2404.05272 ,  972kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05424
Date: Mon, 8 Apr 2024 11:47:46 GMT   (244kb,D)

Title: What Are the Odds? Improving the foundations of Statistical Model
  Checking
Authors: Tobias Meggendorfer, Maximilian Weininger, Patrick Wienh\"oft
Categories: cs.AI cs.LG cs.SY eess.SY
\\
  Markov decision processes (MDPs) are a fundamental model for decision making
under uncertainty. They exhibit non-deterministic choice as well as
probabilistic uncertainty. Traditionally, verification algorithms assume exact
knowledge of the probabilities that govern the behaviour of an MDP. As this
assumption is often unrealistic in practice, statistical model checking (SMC)
was developed in the past two decades. It allows to analyse MDPs with unknown
transition probabilities and provide probably approximately correct (PAC)
guarantees on the result. Model-based SMC algorithms sample the MDP and build a
model of it by estimating all transition probabilities, essentially for every
transition answering the question: ``What are the odds?'' However, so far the
statistical methods employed by the state of the art SMC algorithms are quite
naive. Our contribution are several fundamental improvements to those methods:
On the one hand, we survey statistics literature for better concentration
inequalities; on the other hand, we propose specialised approaches that exploit
our knowledge of the MDP. Our improvements are generally applicable to many
kinds of problem statements because they are largely independent of the
setting. Moreover, our experimental evaluation shows that they lead to
significant gains, reducing the number of samples that the SMC algorithm has to
collect by up to two orders of magnitude.
\\ ( https://arxiv.org/abs/2404.05424 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05440
Date: Mon, 8 Apr 2024 12:19:04 GMT   (7307kb,D)

Title: Tree Search-Based Policy Optimization under Stochastic Execution Delay
Authors: David Valensi, Esther Derman, Shie Mannor, Gal Dalal
Categories: cs.AI
Comments: Published in ICLR 2024
\\
  The standard formulation of Markov decision processes (MDPs) assumes that the
agent's decisions are executed immediately. However, in numerous realistic
applications such as robotics or healthcare, actions are performed with a delay
whose value can even be stochastic. In this work, we introduce stochastic
delayed execution MDPs, a new formalism addressing random delays without
resorting to state augmentation. We show that given observed delay values, it
is sufficient to perform a policy search in the class of Markov policies in
order to reach optimal performance, thus extending the deterministic fixed
delay case. Armed with this insight, we devise DEZ, a model-based algorithm
that optimizes over the class of Markov policies. DEZ leverages Monte-Carlo
tree search similar to its non-delayed variant EfficientZero to accurately
infer future states from the action queue. Thus, it handles delayed execution
while preserving the sample efficiency of EfficientZero. Through a series of
experiments on the Atari suite, we demonstrate that although the previous
baseline outperforms the naive method in scenarios with constant delay, it
underperforms in the face of stochastic delays. In contrast, our approach
significantly outperforms the baselines, for both constant and stochastic
delays. The code is available at http://github.com/davidva1/Delayed-EZ .
\\ ( https://arxiv.org/abs/2404.05440 ,  7307kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05569
Date: Mon, 8 Apr 2024 14:43:13 GMT   (671kb,D)

Title: 360{\deg}REA: Towards A Reusable Experience Accumulation with 360{\deg}
  Assessment for Multi-Agent System
Authors: Shen Gao, Hao Li, Zhengliang Shi, Chengrui Huang, Quan Tu, Zhiliang
  Tian, Minlie Huang, Shuo Shang
Categories: cs.AI cs.CL
\\
  Large language model agents have demonstrated remarkable advancements across
various complex tasks. Recent works focus on optimizing the agent team or
employing self-reflection to iteratively solve complex tasks. Since these
agents are all based on the same LLM, only conducting self-evaluation or
removing underperforming agents does not substantively enhance the capability
of the agents. We argue that a comprehensive evaluation and accumulating
experience from evaluation feedback is an effective approach to improving
system performance. In this paper, we propose Reusable Experience Accumulation
with 360{\deg} Assessment (360{\deg}REA), a hierarchical multi-agent framework
inspired by corporate organizational practices. The framework employs a novel
360{\deg} performance assessment method for multi-perspective performance
evaluation with fine-grained assessment. To enhance the capability of agents in
addressing complex tasks, we introduce dual-level experience pool for agents to
accumulate experience through fine-grained assessment. Extensive experiments on
complex task datasets demonstrate the effectiveness of 360{\deg}REA.
\\ ( https://arxiv.org/abs/2404.05569 ,  671kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04261
Date: Fri, 16 Feb 2024 14:44:30 GMT   (1370kb,D)

Title: A Novel BERT-based Classifier to Detect Political Leaning of YouTube
  Videos based on their Titles
Authors: Nouar AlDahoul, Talal Rahwan, Yasir Zaki
Categories: cs.CL cs.AI
Comments: 14 pages, 4 figures
\\
  A quarter of US adults regularly get their news from YouTube. Yet, despite
the massive political content available on the platform, to date no classifier
has been proposed to identify the political leaning of YouTube videos. To fill
this gap, we propose a novel classifier based on Bert -- a language model from
Google -- to classify YouTube videos merely based on their titles into six
categories, namely: Far Left, Left, Center, Anti-Woke, Right, and Far Right. We
used a public dataset of 10 million YouTube video titles (under various
categories) to train and validate the proposed classifier. We compare the
classifier against several alternatives that we trained on the same dataset,
revealing that our classifier achieves the highest accuracy (75%) and the
highest F1 score (77%). To further validate the classification performance, we
collect videos from YouTube channels of numerous prominent news agencies, such
as Fox News and New York Times, which have widely known political leanings, and
apply our classifier to their video titles. For the vast majority of cases, the
predicted political leaning matches that of the news agency.
\\ ( https://arxiv.org/abs/2404.04261 ,  1370kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04279
Date: Tue, 2 Apr 2024 12:46:00 GMT   (2177kb)

Title: When Abel Kills Cain: What Machine Translation Cannot Capture
Authors: Aur\'elien B\'enel (Tech-CICO), Joris Falip (Tech-CICO), Philippe
  Lacour (UnB)
Categories: cs.CL cs.AI
Comments: in French language
Journal-ref: Ce qui \'echappe \'a l'Intelligence Artificielle, Hermann,
  pp.111-129, 2024, 9791037038449
DOI: 10.3166/lcn.10.4.103-132
\\
  The article aims at identifying what, from a structural point of view, AI
based automatic translators cannot fully capture. It focuses on the machine's
mistakes, in order to try to explain its causes. The biblical story of Ca\"in
and Abel has been chosen because of its rich interpretive and critical
tradition, but also because of its semantic difficulty. The investigation
begins with the observation, for the translation of this text, of the language
pairs and interfaces offered by the best known machine translation services
(Google Translate, DeepL). A typology of the most frequent translation errors
is then established. Finally, contemporary translations are compared, in order
to underline the unique contribution of each. In conclusion, the article
suggests a revision of translation theory and, corArtificial Intelligence,
Translation, Limitations, Interpretation, Comparison, Unicityelatively, a
reformulation of its technology concerning cultural texts.
\\ ( https://arxiv.org/abs/2404.04279 ,  2177kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04281
Date: Wed, 3 Apr 2024 03:17:28 GMT   (1061kb)

Title: Similar Data Points Identification with LLM: A Human-in-the-loop
  Strategy Using Summarization and Hidden State Insights
Authors: Xianlong Zeng, Fanghao Song, Ang Liu
Categories: cs.CL cs.AI
\\
  This study introduces a simple yet effective method for identifying similar
data points across non-free text domains, such as tabular and image data, using
Large Language Models (LLMs). Our two-step approach involves data point
summarization and hidden state extraction. Initially, data is condensed via
summarization using an LLM, reducing complexity and highlighting essential
information in sentences. Subsequently, the summarization sentences are fed
through another LLM to extract hidden states, serving as compact, feature-rich
representations. This approach leverages the advanced comprehension and
generative capabilities of LLMs, offering a scalable and efficient strategy for
similarity identification across diverse datasets. We demonstrate the
effectiveness of our method in identifying similar data points on multiple
datasets. Additionally, our approach enables non-technical domain experts, such
as fraud investigators or marketing operators, to quickly identify similar data
points tailored to specific scenarios, demonstrating its utility in practical
applications. In general, our results open new avenues for leveraging LLMs in
data analysis across various domains.
\\ ( https://arxiv.org/abs/2404.04281 ,  1061kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04284
Date: Wed, 3 Apr 2024 19:45:40 GMT   (684kb,D)

Title: Assessing ML Classification Algorithms and NLP Techniques for Depression
  Detection: An Experimental Case Study
Authors: Giuliano Lorenzoni, Cristina Tavares, Nathalia Nascimento, Paulo
  Alencar, Donald Cowan
Categories: cs.CL
\\
  Depression has affected millions of people worldwide and has become one of
the most common mental disorders. Early mental disorder detection can reduce
costs for public health agencies and prevent other major comorbidities.
Additionally, the shortage of specialized personnel is very concerning since
Depression diagnosis is highly dependent on expert professionals and is
time-consuming. Recent research has evidenced that machine learning (ML) and
Natural Language Processing (NLP) tools and techniques have significantly bene
ted the diagnosis of depression. However, there are still several challenges in
the assessment of depression detection approaches in which other conditions
such as post-traumatic stress disorder (PTSD) are present. These challenges
include assessing alternatives in terms of data cleaning and pre-processing
techniques, feature selection, and appropriate ML classification algorithms.
This paper tackels such an assessment based on a case study that compares
different ML classifiers, specifically in terms of data cleaning and
pre-processing, feature selection, parameter setting, and model choices. The
case study is based on the Distress Analysis Interview Corpus - Wizard-of-Oz
(DAIC-WOZ) dataset, which is designed to support the diagnosis of mental
disorders such as depression, anxiety, and PTSD. Besides the assessment of
alternative techniques, we were able to build models with accuracy levels
around 84% with Random Forest and XGBoost models, which is significantly higher
than the results from the comparable literature which presented the level of
accuracy of 72% from the SVM model.
\\ ( https://arxiv.org/abs/2404.04284 ,  684kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04285
Date: Wed, 3 Apr 2024 23:42:38 GMT   (8742kb,D)

Title: MIMIR: A Streamlined Platform for Personalized Agent Tuning in Domain
  Expertise
Authors: Chunyuan Deng, Xiangru Tang, Yilun Zhao, Hanming Wang, Haoran Wang,
  Wangchunshu Zhou, Arman Cohan, Mark Gerstein
Categories: cs.CL cs.AI
\\
  Recently, large language models (LLMs) have evolved into interactive agents,
proficient in planning, tool use, and task execution across a wide variety of
tasks. However, without specific agent tuning, open-source models like LLaMA
currently struggle to match the efficiency of GPT- 4, particularly given the
scarcity of agent-tuning datasets for fine-tuning. In response, we introduce
\textsc{Mimir}: a streamlined platform offering a customizable pipeline that
enables users to leverage both private knowledge and publicly available,
legally compliant datasets at scale for \textbf{personalized agent tuning}.
Additionally, \textsc{Mimir} supports the generation of general
instruction-tuning datasets from the same input. This dual capability ensures
that language agents developed through the platform possess both specific agent
abilities and general competencies. \textsc{Mimir} integrates these features
into a cohesive end-to-end platform, facilitating everything from the uploading
of personalized files to one-click agent fine-tuning.
\\ ( https://arxiv.org/abs/2404.04285 ,  8742kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04286
Date: Thu, 4 Apr 2024 02:01:25 GMT   (10407kb,D)

Title: Language Model Evolution: An Iterated Learning Perspective
Authors: Yi Ren, Shangmin Guo, Linlu Qiu, Bailin Wang, Danica J. Sutherland
Categories: cs.CL cs.AI cs.LG
\\
  With the widespread adoption of Large Language Models (LLMs), the prevalence
of iterative interactions among these models is anticipated to increase.
Notably, recent advancements in multi-round self-improving methods allow LLMs
to generate new examples for training subsequent models. At the same time,
multi-agent LLM systems, involving automated interactions among agents, are
also increasing in prominence. Thus, in both short and long terms, LLMs may
actively engage in an evolutionary process. We draw parallels between the
behavior of LLMs and the evolution of human culture, as the latter has been
extensively studied by cognitive scientists for decades. Our approach involves
leveraging Iterated Learning (IL), a Bayesian framework that elucidates how
subtle biases are magnified during human cultural evolution, to explain some
behaviors of LLMs. This paper outlines key characteristics of agents' behavior
in the Bayesian-IL framework, including predictions that are supported by
experimental verification with various LLMs. This theoretical framework could
help to more effectively predict and guide the evolution of LLMs in desired
directions.
\\ ( https://arxiv.org/abs/2404.04286 ,  10407kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04287
Date: Thu, 4 Apr 2024 02:58:21 GMT   (320kb,D)

Title: CONFLARE: CONFormal LArge language model REtrieval
Authors: Pouria Rouzrokh, Shahriar Faghani, Cooper U. Gamble, Moein Shariatnia,
  Bradley J. Erickson
Categories: cs.CL cs.AI
Comments: Github code:
  https://github.com/Mayo-Radiology-Informatics-Lab/conflare
\\
  Retrieval-augmented generation (RAG) frameworks enable large language models
(LLMs) to retrieve relevant information from a knowledge base and incorporate
it into the context for generating responses. This mitigates hallucinations and
allows for the updating of knowledge without retraining the LLM. However, RAG
does not guarantee valid responses if retrieval fails to identify the necessary
information as the context for response generation. Also, if there is
contradictory content, the RAG response will likely reflect only one of the two
possible responses. Therefore, quantifying uncertainty in the retrieval process
is crucial for ensuring RAG trustworthiness. In this report, we introduce a
four-step framework for applying conformal prediction to quantify retrieval
uncertainty in RAG frameworks. First, a calibration set of questions answerable
from the knowledge base is constructed. Each question's embedding is compared
against document embeddings to identify the most relevant document chunks
containing the answer and record their similarity scores. Given a
user-specified error rate ({\alpha}), these similarity scores are then analyzed
to determine a similarity score cutoff threshold. During inference, all chunks
with similarity exceeding this threshold are retrieved to provide context to
the LLM, ensuring the true answer is captured in the context with a
(1-{\alpha}) confidence level. We provide a Python package that enables users
to implement the entire workflow proposed in our work, only using LLMs and
without human intervention.
\\ ( https://arxiv.org/abs/2404.04287 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04292
Date: Thu, 4 Apr 2024 06:16:35 GMT   (551kb,D)

Title: Conversational Disease Diagnosis via External Planner-Controlled Large
  Language Models
Authors: Zhoujian Sun, Cheng Luo, Zhengxing Huang
Categories: cs.CL cs.AI
Comments: Work in Progress
\\
  The advancement of medical artificial intelligence (AI) has set the stage for
the realization of conversational diagnosis, where AI systems mimic human
doctors by engaging in dialogue with patients to deduce diagnoses. This study
introduces an innovative approach using external planners augmented with large
language models (LLMs) to develop a medical task-oriented dialogue system. This
system comprises a policy module for information gathering, a LLM based module
for natural language understanding and generation, addressing the limitations
of previous AI systems in these areas. By emulating the two-phase
decision-making process of doctors disease screening and differential
diagnosis. we designed two distinct planners. The first focuses on collecting
patient symptoms to identify potential diseases, while the second delves into
specific inquiries to confirm or exclude these diseases. Utilizing
reinforcement learning and active learning with LLMs, we trained these planners
to navigate medical dialogues effectively. Our evaluation on the MIMIC-IV
dataset demonstrated the system's capability to outperform existing models,
indicating a significant step towards achieving automated conversational
disease diagnostics and enhancing the precision and accessibility of medical
diagnoses.
\\ ( https://arxiv.org/abs/2404.04292 ,  551kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04293
Date: Thu, 4 Apr 2024 08:38:03 GMT   (1119kb,D)

Title: Reason from Fallacy: Enhancing Large Language Models' Logical Reasoning
  through Logical Fallacy Understanding
Authors: Yanda Li, Dixuan Wang, Jiaqing Liang, Guochao Jiang, Qianyu He,
  Yanghua Xiao, Deqing Yang
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) have demonstrated good performance in many
reasoning tasks, but they still struggle with some complicated reasoning tasks
including logical reasoning. One non-negligible reason for LLMs' suboptimal
performance on logical reasoning is their overlooking of understanding logical
fallacies correctly. To evaluate LLMs' capability of logical fallacy
understanding (LFU), we propose five concrete tasks from three cognitive
dimensions of WHAT, WHY, and HOW in this paper. Towards these LFU tasks, we
have successfully constructed a new dataset LFUD based on GPT-4 accompanied by
a little human effort. Our extensive experiments justify that our LFUD can be
used not only to evaluate LLMs' LFU capability, but also to fine-tune LLMs to
obtain significantly enhanced performance on logical reasoning.
\\ ( https://arxiv.org/abs/2404.04293 ,  1119kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04295
Date: Thu, 4 Apr 2024 17:03:47 GMT   (572kb,D)

Title: Transducers with Pronunciation-aware Embeddings for Automatic Speech
  Recognition
Authors: Hainan Xu, Zhehuai Chen, Fei Jia, Boris Ginsburg
Categories: cs.CL cs.LG
Comments: accepted at the ICASSP 2024 conference
\\
  This paper proposes Transducers with Pronunciation-aware Embeddings (PET).
Unlike conventional Transducers where the decoder embeddings for different
tokens are trained independently, the PET model's decoder embedding
incorporates shared components for text tokens with the same or similar
pronunciations. With experiments conducted in multiple datasets in Mandarin
Chinese and Korean, we show that PET models consistently improve speech
recognition accuracy compared to conventional Transducers. Our investigation
also uncovers a phenomenon that we call error chain reactions. Instead of
recognition errors being evenly spread throughout an utterance, they tend to
group together, with subsequent errors often following earlier ones. Our
analysis shows that PET models effectively mitigate this issue by substantially
reducing the likelihood of the model generating additional errors following a
prior one. Our implementation will be open-sourced with the NeMo toolkit.
\\ ( https://arxiv.org/abs/2404.04295 ,  572kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04302
Date: Thu, 4 Apr 2024 21:47:43 GMT   (1226kb,D)

Title: CBR-RAG: Case-Based Reasoning for Retrieval Augmented Generation in LLMs
  for Legal Question Answering
Authors: Nirmalie Wiratunga, Ramitha Abeyratne, Lasal Jayawardena, Kyle Martin,
  Stewart Massie, Ikechukwu Nkisi-Orji, Ruvan Weerasinghe, Anne Liret, Bruno
  Fleisch
Categories: cs.CL cs.AI
Comments: Submitted to ICCBR'24
\\
  Retrieval-Augmented Generation (RAG) enhances Large Language Model (LLM)
output by providing prior knowledge as context to input. This is beneficial for
knowledge-intensive and expert reliant tasks, including legal
question-answering, which require evidence to validate generated text outputs.
We highlight that Case-Based Reasoning (CBR) presents key opportunities to
structure retrieval as part of the RAG process in an LLM. We introduce CBR-RAG,
where CBR cycle's initial retrieval stage, its indexing vocabulary, and
similarity knowledge containers are used to enhance LLM queries with
contextually relevant cases. This integration augments the original LLM query,
providing a richer prompt. We present an evaluation of CBR-RAG, and examine
different representations (i.e. general and domain-specific embeddings) and
methods of comparison (i.e. inter, intra and hybrid similarity) on the task of
legal question-answering. Our results indicate that the context provided by
CBR's case reuse enforces similarity between relevant components of the
questions and the evidence base leading to significant improvements in the
quality of generated answers.
\\ ( https://arxiv.org/abs/2404.04302 ,  1226kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04332
Date: Fri, 5 Apr 2024 18:01:02 GMT   (287kb,D)

Title: Scope Ambiguities in Large Language Models
Authors: Gaurav Kamath, Sebastian Schuster, Sowmya Vajjala and Siva Reddy
Categories: cs.CL cs.AI
Comments: To be published in Transactions of the Association for Computational
  Linguistics
\\
  Sentences containing multiple semantic operators with overlapping scope often
create ambiguities in interpretation, known as scope ambiguities. These
ambiguities offer rich insights into the interaction between semantic structure
and world knowledge in language processing. Despite this, there has been little
research into how modern large language models treat them. In this paper, we
investigate how different versions of certain autoregressive language models --
GPT-2, GPT-3/3.5, Llama 2 and GPT-4 -- treat scope ambiguous sentences, and
compare this with human judgments. We introduce novel datasets that contain a
joint total of almost 1,000 unique scope-ambiguous sentences, containing
interactions between a range of semantic operators, and annotated for human
judgments. Using these datasets, we find evidence that several models (i) are
sensitive to the meaning ambiguity in these sentences, in a way that patterns
well with human judgments, and (ii) can successfully identify human-preferred
readings at a high level of accuracy (over 90% in some cases).
\\ ( https://arxiv.org/abs/2404.04332 ,  287kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04351
Date: Fri, 5 Apr 2024 18:44:54 GMT   (536kb,D)

Title: Assisting humans in complex comparisons: automated information
  comparison at scale
Authors: Truman Yuen, Graham A. Watt, Yuri Lawryshyn
Categories: cs.CL cs.AI cs.LG
Comments: 11 pages, 7 figures, 5 tables
ACM-class: I.2.7; I.2.8
\\
  Generative Large Language Models enable efficient analytics across knowledge
domains, rivalling human experts in information comparisons. However, the
applications of LLMs for information comparisons face scalability challenges
due to the difficulties in maintaining information across large contexts and
overcoming model token limitations. To address these challenges, we developed
the novel Abstractive Summarization \& Criteria-driven Comparison Endpoint
(ASC$^2$End) system to automate information comparison at scale. Our system
employs Semantic Text Similarity comparisons for generating evidence-supported
analyses. We utilize proven data-handling strategies such as abstractive
summarization and retrieval augmented generation to overcome token limitations
and retain relevant information during model inference. Prompts were designed
using zero-shot strategies to contextualize information for improved model
reasoning. We evaluated abstractive summarization using ROUGE scoring and
assessed the generated comparison quality using survey responses. Models
evaluated on the ASC$^2$End system show desirable results providing insights on
the expected performance of the system. ASC$^2$End is a novel system and tool
that enables accurate, automated information comparison at scale across
knowledge domains, overcoming limitations in context length and retrieval.
\\ ( https://arxiv.org/abs/2404.04351 ,  536kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04361
Date: Fri, 5 Apr 2024 19:14:38 GMT   (1149kb,D)

Title: Deciphering Political Entity Sentiment in News with Large Language
  Models: Zero-Shot and Few-Shot Strategies
Authors: Alapan Kuila, Sudeshna Sarkar
Categories: cs.CL
Comments: Accepted in PoliticalNLP workshop co-located with LREC-COLING 2024
\\
  Sentiment analysis plays a pivotal role in understanding public opinion,
particularly in the political domain where the portrayal of entities in news
articles influences public perception. In this paper, we investigate the
effectiveness of Large Language Models (LLMs) in predicting entity-specific
sentiment from political news articles. Leveraging zero-shot and few-shot
strategies, we explore the capability of LLMs to discern sentiment towards
political entities in news content. Employing a chain-of-thought (COT) approach
augmented with rationale in few-shot in-context learning, we assess whether
this method enhances sentiment prediction accuracy. Our evaluation on
sentiment-labeled datasets demonstrates that LLMs, outperform fine-tuned BERT
models in capturing entity-specific sentiment. We find that learning in-context
significantly improves model performance, while the self-consistency mechanism
enhances consistency in sentiment prediction. Despite the promising results, we
observe inconsistencies in the effectiveness of the COT prompting method.
Overall, our findings underscore the potential of LLMs in entity-centric
sentiment analysis within the political news domain and highlight the
importance of suitable prompting strategies and model architectures.
\\ ( https://arxiv.org/abs/2404.04361 ,  1149kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04445
Date: Fri, 5 Apr 2024 23:12:46 GMT   (874kb)

Title: Towards Realistic Few-Shot Relation Extraction: A New Meta Dataset and
  Evaluation
Authors: Fahmida Alam, Md Asiful Islam, Robert Vacareanu, Mihai Surdeanu
Categories: cs.CL cs.IR
\\
  We introduce a meta dataset for few-shot relation extraction, which includes
two datasets derived from existing supervised relation extraction datasets
NYT29 (Takanobu et al., 2019; Nayak and Ng, 2020) and WIKIDATA (Sorokin and
Gurevych, 2017) as well as a few-shot form of the TACRED dataset (Sabo et al.,
2021). Importantly, all these few-shot datasets were generated under realistic
assumptions such as: the test relations are different from any relations a
model might have seen before, limited training data, and a preponderance of
candidate relation mentions that do not correspond to any of the relations of
interest. Using this large resource, we conduct a comprehensive evaluation of
six recent few-shot relation extraction methods, and observe that no method
comes out as a clear winner. Further, the overall performance on this task is
low, indicating substantial need for future research. We release all versions
of the data, i.e., both supervised and few-shot, for future research.
\\ ( https://arxiv.org/abs/2404.04445 ,  874kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04487
Date: Sat, 6 Apr 2024 03:40:36 GMT   (351kb,D)

Title: KazQAD: Kazakh Open-Domain Question Answering Dataset
Authors: Rustem Yeshpanov, Pavel Efimov, Leonid Boytsov, Ardak Shalkarbayuli,
  Pavel Braslavski
Categories: cs.CL
Comments: To appear in Proceedings of the 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation (LREC-COLING
  2024)
\\
  We introduce KazQAD -- a Kazakh open-domain question answering (ODQA) dataset
-- that can be used in both reading comprehension and full ODQA settings, as
well as for information retrieval experiments. KazQAD contains just under 6,000
unique questions with extracted short answers and nearly 12,000 passage-level
relevance judgements. We use a combination of machine translation, Wikipedia
search, and in-house manual annotation to ensure annotation efficiency and data
quality. The questions come from two sources: translated items from the Natural
Questions (NQ) dataset (only for training) and the original Kazakh Unified
National Testing (UNT) exam (for development and testing). The accompanying
text corpus contains more than 800,000 passages from the Kazakh Wikipedia. As a
supplementary dataset, we release around 61,000 question-passage-answer triples
from the NQ dataset that have been machine-translated into Kazakh. We develop
baseline retrievers and readers that achieve reasonable scores in retrieval
(NDCG@10 = 0.389 MRR = 0.382), reading comprehension (EM = 38.5 F1 = 54.2), and
full ODQA (EM = 17.8 F1 = 28.7) settings. Nevertheless, these results are
substantially lower than state-of-the-art results for English QA collections,
and we think that there should still be ample room for improvement. We also
show that the current OpenAI's ChatGPTv3.5 is not able to answer KazQAD test
questions in the closed-book setting with acceptable quality. The dataset is
freely available under the Creative Commons licence (CC BY-SA) at
https://github.com/IS2AI/KazQAD.
\\ ( https://arxiv.org/abs/2404.04487 ,  351kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04510
Date: Sat, 6 Apr 2024 05:44:53 GMT   (373kb,D)

Title: IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe
  Biomedical Natural Language Inference for Clinical Trials
Authors: Shreyasi Mandal and Ashutosh Modi
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at SemEval 2024, NAACL 2024; 8 Pages
\\
  Large Language models (LLMs) have demonstrated state-of-the-art performance
in various natural language processing (NLP) tasks across multiple domains, yet
they are prone to shortcut learning and factual inconsistencies. This research
investigates LLMs' robustness, consistency, and faithful reasoning when
performing Natural Language Inference (NLI) on breast cancer Clinical Trial
Reports (CTRs) in the context of SemEval 2024 Task 2: Safe Biomedical Natural
Language Inference for Clinical Trials. We examine the reasoning capabilities
of LLMs and their adeptness at logical problem-solving. A comparative analysis
is conducted on pre-trained language models (PLMs), GPT-3.5, and Gemini Pro
under zero-shot settings using Retrieval-Augmented Generation (RAG) framework,
integrating various reasoning chains. The evaluation yields an F1 score of
0.69, consistency of 0.71, and a faithfulness score of 0.90 on the test
dataset.
\\ ( https://arxiv.org/abs/2404.04510 ,  373kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04513
Date: Sat, 6 Apr 2024 05:58:42 GMT   (645kb,D)

Title: IITK at SemEval-2024 Task 1: Contrastive Learning and Autoencoders for
  Semantic Textual Relatedness in Multilingual Texts
Authors: Udvas Basak and Rajarshi Dutta and Shivam Pandey and Ashutosh Modi
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at SemEval 2024, NAACL 2024; 6 pages
\\
  This paper describes our system developed for the SemEval-2024 Task 1:
Semantic Textual Relatedness. The challenge is focused on automatically
detecting the degree of relatedness between pairs of sentences for 14 languages
including both high and low-resource Asian and African languages. Our team
participated in two subtasks consisting of Track A: supervised and Track B:
unsupervised. This paper focuses on a BERT-based contrastive learning and
similarity metric based approach primarily for the supervised track while
exploring autoencoders for the unsupervised track. It also aims on the creation
of a bigram relatedness corpus using negative sampling strategy, thereby
producing refined word embeddings.
\\ ( https://arxiv.org/abs/2404.04513 ,  645kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04514
Date: Sat, 6 Apr 2024 05:59:02 GMT   (17009kb,D)

Title: Joint Visual and Text Prompting for Improved Object-Centric Perception
  with Multimodal Large Language Models
Authors: Songtao Jiang, Yan Zhang, Chenyi Zhou, Yeying Jin, Yang Feng, Jian Wu,
  Zuozhu Liu
Categories: cs.CL
\\
  Multimodal Large Language Models (MLLMs) such as GPT-4V and Gemini Pro face
challenges in achieving human-level perception in Visual Question Answering
(VQA), particularly in object-oriented perception tasks which demand
fine-grained understanding of object identities, locations or attributes, as
indicated by empirical findings. This is mainly due to their limited capability
to effectively integrate complex visual cues with textual information and
potential object hallucinations. In this paper, we present a novel approach,
Joint Visual and Text Prompting (VTPrompt), that employs fine-grained visual
information to enhance the capability of MLLMs in VQA, especially for
object-oriented perception. VTPrompt merges visual and text prompts to extract
key concepts from textual questions and employs a detection model to highlight
relevant objects as visual prompts in images. The processed images alongside
text prompts are subsequently fed into MLLMs to produce more accurate answers.
Our experiments with GPT-4V and Gemini Pro, on three benchmarks, i.e., MME ,
MMB and POPE, demonstrate significant improvements. Particularly, our method
led to a score improvement of up to 183.5 for GPT-4V on MME and enhanced MMB
performance by 8.17\% for GPT-4V and 15.69\% for Gemini Pro.
\\ ( https://arxiv.org/abs/2404.04514 ,  17009kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04520
Date: Sat, 6 Apr 2024 06:28:02 GMT   (2393kb,D)

Title: IITK at SemEval-2024 Task 4: Hierarchical Embeddings for Detection of
  Persuasion Techniques in Memes
Authors: Shreenaga Chikoti and Shrey Mehta and Ashutosh Modi
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at SemEval 2024, NAACL 2024; 9 pages
\\
  Memes are one of the most popular types of content used in an online
disinformation campaign. They are primarily effective on social media platforms
since they can easily reach many users. Memes in a disinformation campaign
achieve their goal of influencing the users through several rhetorical and
psychological techniques, such as causal oversimplification, name-calling, and
smear. The SemEval 2024 Task 4 \textit{Multilingual Detection of Persuasion
Technique in Memes} on identifying such techniques in the memes is divided
across three sub-tasks: ($\mathbf{1}$) Hierarchical multi-label classification
using only textual content of the meme, ($\mathbf{2}$) Hierarchical multi-label
classification using both, textual and visual content of the meme and
($\mathbf{3}$) Binary classification of whether the meme contains a persuasion
technique or not using it's textual and visual content. This paper proposes an
ensemble of Class Definition Prediction (CDP) and hyperbolic embeddings-based
approaches for this task. We enhance meme classification accuracy and
comprehensiveness by integrating HypEmo's hierarchical label embeddings (Chen
et al., 2023) and a multi-task learning framework for emotion prediction. We
achieve a hierarchical F1-score of 0.60, 0.67, and 0.48 on the respective
sub-tasks.
\\ ( https://arxiv.org/abs/2404.04520 ,  2393kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04522
Date: Sat, 6 Apr 2024 06:44:41 GMT   (205kb,D)

Title: Q-PEFT: Query-dependent Parameter Efficient Fine-tuning for Text
  Reranking with Large Language Models
Authors: Zhiyuan Peng, Xuyang Wu, Qifan Wang, Sravanthi Rajanala, Yi Fang
Categories: cs.CL cs.AI cs.IR cs.LG
\\
  Parameter Efficient Fine-Tuning (PEFT) methods have been extensively utilized
in Large Language Models (LLMs) to improve the down-streaming tasks without the
cost of fine-tuing the whole LLMs. Recent studies have shown how to effectively
use PEFT for fine-tuning LLMs in ranking tasks with convincing performance;
there are some limitations, including the learned prompt being fixed for
different documents, overfitting to specific tasks, and low adaptation ability.
In this paper, we introduce a query-dependent parameter efficient fine-tuning
(Q-PEFT) approach for text reranking to leak the information of the true
queries to LLMs and then make the generation of true queries from input
documents much easier. Specifically, we utilize the query to extract the
top-$k$ tokens from concatenated documents, serving as contextual clues. We
further augment Q-PEFT by substituting the retrieval mechanism with a
multi-head attention layer to achieve end-to-end training and cover all the
tokens in the documents, guiding the LLMs to generate more document-specific
synthetic queries, thereby further improving the reranking performance.
Extensive experiments are conducted on four public datasets, demonstrating the
effectiveness of our proposed approach.
\\ ( https://arxiv.org/abs/2404.04522 ,  205kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04525
Date: Sat, 6 Apr 2024 06:47:44 GMT   (477kb,D)

Title: IITK at SemEval-2024 Task 10: Who is the speaker? Improving Emotion
  Recognition and Flip Reasoning in Conversations via Speaker Embeddings
Authors: Shubham Patel and Divyaksh Shukla and Ashutosh Modi
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at SemEval 2024, NAACL 2024; 10 Pages
\\
  This paper presents our approach for the SemEval-2024 Task 10: Emotion
Discovery and Reasoning its Flip in Conversations. For the Emotion Recognition
in Conversations (ERC) task, we utilize a masked-memory network along with
speaker participation. We propose a transformer-based speaker-centric model for
the Emotion Flip Reasoning (EFR) task. We also introduce Probable Trigger Zone,
a region of the conversation that is more likely to contain the utterances
causing the emotion to flip. For sub-task 3, the proposed approach achieves a
5.9 (F1 score) improvement over the task baseline. The ablation study results
highlight the significance of various design choices in the proposed method.
\\ ( https://arxiv.org/abs/2404.04525 ,  477kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04530
Date: Sat, 6 Apr 2024 07:10:47 GMT   (1417kb,D)

Title: A Morphology-Based Investigation of Positional Encodings
Authors: Poulami Ghosh, Shikhar Vashishth, Raj Dabre, Pushpak Bhattacharyya
Categories: cs.CL
Comments: Work in Progress
\\
  How does the importance of positional encoding in pre-trained language models
(PLMs) vary across languages with different morphological complexity? In this
paper, we offer the first study addressing this question, encompassing 23
morphologically diverse languages and 5 different downstream tasks. We choose
two categories of tasks: syntactic tasks (part-of-speech tagging, named entity
recognition, dependency parsing) and semantic tasks (natural language
inference, paraphrasing). We consider language-specific BERT models trained on
monolingual corpus for our investigation. The main experiment consists of
nullifying the effect of positional encoding during fine-tuning and
investigating its impact across various tasks and languages. Our findings
demonstrate that the significance of positional encoding diminishes as the
morphological complexity of a language increases. Across all experiments, we
observe clustering of languages according to their morphological typology -
with analytic languages at one end and synthetic languages at the opposite end.
\\ ( https://arxiv.org/abs/2404.04530 ,  1417kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04626
Date: Sat, 6 Apr 2024 13:24:37 GMT   (1382kb)

Title: Towards Analyzing and Understanding the Limitations of DPO: A
  Theoretical Perspective
Authors: Duanyu Feng, Bowen Qin, Chen Huang, Zheng Zhang, Wenqiang Lei
Categories: cs.CL cs.AI
Comments: Draft version
\\
  Direct Preference Optimization (DPO), which derives reward signals directly
from pairwise preference data, has shown its effectiveness on aligning Large
Language Models (LLMs) with human preferences. Despite its widespread use
across various tasks, DPO has been criticized for its sensitivity to the SFT's
effectiveness and its hindrance to the learning capacity towards
human-preferred responses, leading to less satisfactory performance. To
overcome those limitations, the theoretical understanding of DPO are
indispensable but still lacking. To this end, we take a step towards
theoretically analyzing and understanding the limitations of DPO. Specifically,
we provide an analytical framework using the field theory to analyze the
optimization process of DPO. By analyzing the gradient vector field of the DPO
loss function, we find that the DPO loss function decreases the probability of
producing human dispreferred data at a faster rate than it increases the
probability of producing preferred data. This provides theoretical insights for
understanding the limitations of DPO discovered in the related research
experiments, thereby setting the foundation for its improvement.
\\ ( https://arxiv.org/abs/2404.04626 ,  1382kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04631
Date: Sat, 6 Apr 2024 13:38:15 GMT   (2363kb,D)

Title: On the Limitations of Large Language Models (LLMs): False Attribution
Authors: Tosin Adewumi, Nudrat Habib, Lama Alkhaled and Elisa Barney
Categories: cs.CL
Comments: 8 pages, 5 figures
\\
  In this work, we provide insight into one important limitation of large
language models (LLMs), i.e. false attribution, and introduce a new
hallucination metric - Simple Hallucination Index (SHI). The task of automatic
author attribution for relatively small chunks of text is an important NLP task
but can be challenging. We empirically evaluate the power of 3 open SotA LLMs
in zero-shot setting (LLaMA-2-13B, Mixtral 8x7B, and Gemma-7B), especially as
human annotation can be costly. We collected the top 10 most popular books,
according to Project Gutenberg, divided each one into equal chunks of 400
words, and asked each LLM to predict the author. We then randomly sampled 162
chunks for human evaluation from each of the annotated books, based on the
error margin of 7% and a confidence level of 95% for the book with the most
chunks (Great Expectations by Charles Dickens, having 922 chunks). The average
results show that Mixtral 8x7B has the highest prediction accuracy, the lowest
SHI, and a Pearson's correlation (r) of 0.737, 0.249, and -0.9996,
respectively, followed by LLaMA-2-13B and Gemma-7B. However, Mixtral 8x7B
suffers from high hallucinations for 3 books, rising as high as an SHI of 0.87
(in the range 0-1, where 1 is the worst). The strong negative correlation of
accuracy and SHI, given by r, demonstrates the fidelity of the new
hallucination metric, which is generalizable to other tasks. We publicly
release the annotated chunks of data and our codes to aid the reproducibility
and evaluation of other models.
\\ ( https://arxiv.org/abs/2404.04631 ,  2363kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04633
Date: Sat, 6 Apr 2024 13:46:53 GMT   (4746kb,D)

Title: Context versus Prior Knowledge in Language Models
Authors: Kevin Du, V\'esteinn Sn{\ae}bjarnarson, Niklas Stoehr, Jennifer C.
  White, Aaron Schein, Ryan Cotterell
Categories: cs.CL
\\
  To answer a question, language models often need to integrate prior knowledge
learned during pretraining and new information presented in context. We
hypothesize that models perform this integration in a predictable way across
different questions and contexts: models will rely more on prior knowledge for
questions about entities (e.g., persons, places, etc.) that they are more
familiar with due to higher exposure in the training corpus, and be more easily
persuaded by some contexts than others. To formalize this problem, we propose
two mutual information-based metrics to measure a model's dependency on a
context and on its prior about an entity: first, the persuasion score of a
given context represents how much a model depends on the context in its
decision, and second, the susceptibility score of a given entity represents how
much the model can be swayed away from its original answer distribution about
an entity. Following well-established measurement modeling methods, we
empirically test for the validity and reliability of these metrics. Finally, we
explore and find a relationship between the scores and the model's expected
familiarity with an entity, and provide two use cases to illustrate their
benefits.
\\ ( https://arxiv.org/abs/2404.04633 ,  4746kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04645
Date: Sat, 6 Apr 2024 14:34:46 GMT   (597kb,D)

Title: HyperTTS: Parameter Efficient Adaptation in Text to Speech using
  Hypernetworks
Authors: Yingting Li, Rishabh Bhardwaj, Ambuj Mehrish, Bo Cheng, Soujanya Poria
Categories: cs.CL cs.LG cs.SD
\\
  Neural speech synthesis, or text-to-speech (TTS), aims to transform a signal
from the text domain to the speech domain. While developing TTS architectures
that train and test on the same set of speakers has seen significant
improvements, out-of-domain speaker performance still faces enormous
limitations. Domain adaptation on a new set of speakers can be achieved by
fine-tuning the whole model for each new domain, thus making it
parameter-inefficient. This problem can be solved by Adapters that provide a
parameter-efficient alternative to domain adaptation. Although famous in NLP,
speech synthesis has not seen much improvement from Adapters. In this work, we
present HyperTTS, which comprises a small learnable network, "hypernetwork",
that generates parameters of the Adapter blocks, allowing us to condition
Adapters on speaker representations and making them dynamic. Extensive
evaluations of two domain adaptation settings demonstrate its effectiveness in
achieving state-of-the-art performance in the parameter-efficient regime. We
also compare different variants of HyperTTS, comparing them with baselines in
different studies. Promising results on the dynamic adaptation of adapter
parameters using hypernetworks open up new avenues for domain-generic
multi-speaker TTS systems. The audio samples and code are available at
https://github.com/declare-lab/HyperTTS.
\\ ( https://arxiv.org/abs/2404.04645 ,  597kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04659
Date: Sat, 6 Apr 2024 15:25:06 GMT   (6089kb,D)

Title: Multilingual Pretraining and Instruction Tuning Improve Cross-Lingual
  Knowledge Alignment, But Only Shallowly
Authors: Changjiang Gao, Hongda Hu, Peng Hu, Jiajun Chen, Jixing Li, Shujian
  Huang
Categories: cs.CL
\\
  Despite their strong ability to retrieve knowledge in English, current large
language models show imbalance abilities in different languages. Two approaches
are proposed to address this, i.e., multilingual pretraining and multilingual
instruction tuning. However, whether and how do such methods contribute to the
cross-lingual knowledge alignment inside the models is unknown. In this paper,
we propose CLiKA, a systematic framework to assess the cross-lingual knowledge
alignment of LLMs in the Performance, Consistency and Conductivity levels, and
explored the effect of multilingual pretraining and instruction tuning on the
degree of alignment. Results show that: while both multilingual pretraining and
instruction tuning are beneficial for cross-lingual knowledge alignment, the
training strategy needs to be carefully designed. Namely, continued pretraining
improves the alignment of the target language at the cost of other languages,
while mixed pretraining affect other languages less. Also, the overall
cross-lingual knowledge alignment, especially in the conductivity level, is
unsatisfactory for all tested LLMs, and neither multilingual pretraining nor
instruction tuning can substantially improve the cross-lingual knowledge
conductivity.
\\ ( https://arxiv.org/abs/2404.04659 ,  6089kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04671
Date: Sat, 6 Apr 2024 16:16:30 GMT   (4036kb,D)

Title: Inferring the Phylogeny of Large Language Models and Predicting their
  Performances in Benchmarks
Authors: Nicolas Yax, Pierre-Yves Oudeyer, Stefano Palminteri
Categories: cs.CL cs.LG q-bio.PE
\\
  This paper introduces PhyloLM, a method applying phylogenetic algorithms to
Large Language Models to explore their finetuning relationships, and predict
their performance characteristics. By leveraging the phylogenetic distance
metric, we construct dendrograms, which satisfactorily capture distinct LLM
families (across a set of 77 open-source and 22 closed models). Furthermore,
phylogenetic distance predicts performances in benchmarks (we test MMLU and
ARC), thus enabling a time and cost-effective estimation of LLM capabilities.
The approach translates genetic concepts to machine learning, offering tools to
infer LLM development, relationships, and capabilities, even in the absence of
transparent training information.
\\ ( https://arxiv.org/abs/2404.04671 ,  4036kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04676
Date: Sat, 6 Apr 2024 16:39:07 GMT   (7938kb,D)

Title: Order-Based Pre-training Strategies for Procedural Text Understanding
Authors: Abhilash Nandy, Yash Kulkarni, Pawan Goyal, Niloy Ganguly
Categories: cs.CL
Comments: 8 pages (Accepted for publication at NAACL 2024 (Main Conference))
\\
  In this paper, we propose sequence-based pretraining methods to enhance
procedural understanding in natural language processing. Procedural text,
containing sequential instructions to accomplish a task, is difficult to
understand due to the changing attributes of entities in the context. We focus
on recipes, which are commonly represented as ordered instructions, and use
this order as a supervision signal. Our work is one of the first to compare
several 'order as-supervision' transformer pre-training methods, including
Permutation Classification, Embedding Regression, and Skip-Clip, and shows that
these methods give improved results compared to the baselines and SoTA LLMs on
two downstream Entity-Tracking datasets: NPN-Cooking dataset in recipe domain
and ProPara dataset in open domain. Our proposed methods address the
non-trivial Entity Tracking Task that requires prediction of entity states
across procedure steps, which requires understanding the order of steps. These
methods show an improvement over the best baseline by 1.6% and 7-9% on
NPN-Cooking and ProPara Datasets respectively across metrics.
\\ ( https://arxiv.org/abs/2404.04676 ,  7938kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04722
Date: Sat, 6 Apr 2024 20:02:20 GMT   (512kb,D)

Title: PoLLMgraph: Unraveling Hallucinations in Large Language Models via State
  Transition Dynamics
Authors: Derui Zhu, Dingfan Chen, Qing Li, Zongxiong Chen, Lei Ma, Jens
  Grossklags, Mario Fritz
Categories: cs.CL cs.CR cs.SE
Comments: 15 pages
\\
  Despite tremendous advancements in large language models (LLMs) over recent
years, a notably urgent challenge for their practical deployment is the
phenomenon of hallucination, where the model fabricates facts and produces
non-factual statements. In response, we propose PoLLMgraph, a Polygraph for
LLMs, as an effective model-based white-box detection and forecasting approach.
PoLLMgraph distinctly differs from the large body of existing research that
concentrates on addressing such challenges through black-box evaluations. In
particular, we demonstrate that hallucination can be effectively detected by
analyzing the LLM's internal state transition dynamics during generation via
tractable probabilistic models. Experimental results on various open-source
LLMs confirm the efficacy of PoLLMgraph, outperforming state-of-the-art methods
by a considerable margin, evidenced by over 20% improvement in AUC-ROC on
common benchmarking datasets like TruthfulQA. Our work paves a new way for
model-based white-box analysis of LLMs, motivating the research community to
further explore, understand, and refine the intricate dynamics of LLM
behaviors.
\\ ( https://arxiv.org/abs/2404.04722 ,  512kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04728
Date: Sat, 6 Apr 2024 20:42:46 GMT   (469kb,D)

Title: Navigating the Landscape of Hint Generation Research: From the Past to
  the Future
Authors: Anubhav Jangra, Jamshid Mozafari, Adam Jatowt, Smaranda Muresan
Categories: cs.CL cs.HC
Comments: Submitted to TACL'24
\\
  Digital education has gained popularity in the last decade, especially after
the COVID-19 pandemic. With the improving capabilities of large language models
to reason and communicate with users, envisioning intelligent tutoring systems
(ITSs) that can facilitate self-learning is not very far-fetched. One integral
component to fulfill this vision is the ability to give accurate and effective
feedback via hints to scaffold the learning process. In this survey article, we
present a comprehensive review of prior research on hint generation, aiming to
bridge the gap between research in education and cognitive science, and
research in AI and Natural Language Processing. Informed by our findings, we
propose a formal definition of the hint generation task, and discuss the
roadmap of building an effective hint generation system aligned with the formal
definition, including open challenges, future directions and ethical
considerations.
\\ ( https://arxiv.org/abs/2404.04728 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04748
Date: Sat, 6 Apr 2024 22:16:32 GMT   (2438kb,D)

Title: Multilingual Brain Surgeon: Large Language Models Can be Compressed
  Leaving No Language Behind
Authors: Hongchuan Zeng, Hongshen Xu, Lu Chen, Kai Yu
Categories: cs.CL
Comments: 22 pages, 8 figures, 13 tables. Accepted by LREC-COLING 2024
\\
  Large Language Models (LLMs) have ushered in a new era in Natural Language
Processing, but their massive size demands effective compression techniques for
practicality. Although numerous model compression techniques have been
investigated, they typically rely on a calibration set that overlooks the
multilingual context and results in significant accuracy degradation for
low-resource languages. This paper introduces Multilingual Brain Surgeon (MBS),
a novel calibration data sampling method for multilingual LLMs compression. MBS
overcomes the English-centric limitations of existing methods by sampling
calibration data from various languages proportionally to the language
distribution of the model training datasets. Our experiments, conducted on the
BLOOM multilingual LLM, demonstrate that MBS improves the performance of
existing English-centric compression methods, especially for low-resource
languages. We also uncover the dynamics of language interaction during
compression, revealing that the larger the proportion of a language in the
training set and the more similar the language is to the calibration language,
the better performance the language retains after compression. In conclusion,
MBS presents an innovative approach to compressing multilingual LLMs,
addressing the performance disparities and improving the language inclusivity
of existing compression techniques.
\\ ( https://arxiv.org/abs/2404.04748 ,  2438kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04759
Date: Sat, 6 Apr 2024 23:52:53 GMT   (857kb,D)

Title: What Happens When Small Is Made Smaller? Exploring the Impact of
  Compression on Small Data Pretrained Language Models
Authors: Busayo Awobade, Mardiyyah Oduwole, Steven Kolawole
Categories: cs.CL cs.LG
Comments: AfricaNLP workshop at ICLR 2024
\\
  Compression techniques have been crucial in advancing machine learning by
enabling efficient training and deployment of large-scale language models.
However, these techniques have received limited attention in the context of
low-resource language models, which are trained on even smaller amounts of data
and under computational constraints, a scenario known as the "low-resource
double-bind." This paper investigates the effectiveness of pruning, knowledge
distillation, and quantization on an exclusively low-resourced, small-data
language model, AfriBERTa. Through a battery of experiments, we assess the
effects of compression on performance across several metrics beyond accuracy.
Our study provides evidence that compression techniques significantly improve
the efficiency and effectiveness of small-data language models, confirming that
the prevailing beliefs regarding the effects of compression on large, heavily
parameterized models hold true for less-parameterized, small-data models.
\\ ( https://arxiv.org/abs/2404.04759 ,  857kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04770
Date: Sun, 7 Apr 2024 00:50:21 GMT   (7907kb,D)

Title: Generating Uncontextualized and Contextualized Questions for
  Document-Level Event Argument Extraction
Authors: Md Nayem Uddin, Enfa Rose George, Eduardo Blanco, Steven Corman
Categories: cs.CL
Comments: Accepted at NAACL 2024
\\
  This paper presents multiple question generation strategies for
document-level event argument extraction. These strategies do not require human
involvement and result in uncontextualized questions as well as contextualized
questions grounded on the event and document of interest. Experimental results
show that combining uncontextualized and contextualized questions is
beneficial, especially when event triggers and arguments appear in different
sentences. Our approach does not have corpus-specific components, in
particular, the question generation strategies transfer across corpora. We also
present a qualitative analysis of the most common errors made by our best
model.
\\ ( https://arxiv.org/abs/2404.04770 ,  7907kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04809
Date: Sun, 7 Apr 2024 05:04:38 GMT   (931kb,D)

Title: Low-Resource Machine Translation through Retrieval-Augmented LLM
  Prompting: A Study on the Mambai Language
Authors: Rapha\"el Merx, Aso Mahmudi, Katrina Langford, Leo Alberto de Araujo,
  Ekaterina Vylomova
Categories: cs.CL
\\
  This study explores the use of large language models (LLMs) for translating
English into Mambai, a low-resource Austronesian language spoken in
Timor-Leste, with approximately 200,000 native speakers. Leveraging a novel
corpus derived from a Mambai language manual and additional sentences
translated by a native speaker, we examine the efficacy of few-shot LLM
prompting for machine translation (MT) in this low-resource context. Our
methodology involves the strategic selection of parallel sentences and
dictionary entries for prompting, aiming to enhance translation accuracy, using
open-source and proprietary LLMs (LlaMa 2 70b, Mixtral 8x7B, GPT-4). We find
that including dictionary entries in prompts and a mix of sentences retrieved
through TF-IDF and semantic embeddings significantly improves translation
quality. However, our findings reveal stark disparities in translation
performance across test sets, with BLEU scores reaching as high as 21.2 on
materials from the language manual, in contrast to a maximum of 4.4 on a test
set provided by a native speaker. These results underscore the importance of
diverse and representative corpora in assessing MT for low-resource languages.
Our research provides insights into few-shot LLM prompting for low-resource MT,
and makes available an initial corpus for the Mambai language.
\\ ( https://arxiv.org/abs/2404.04809 ,  931kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04817
Date: Sun, 7 Apr 2024 05:54:28 GMT   (112kb,D)

Title: FRACTAL: Fine-Grained Scoring from Aggregate Text Labels
Authors: Yukti Makhija and Priyanka Agrawal and Rishi Saket and Aravindan
  Raghuveer
Categories: cs.CL
Comments: 22 pages, 1 figure
\\
  Large language models (LLMs) are being increasingly tuned to power complex
generation tasks such as writing, fact-seeking, querying and reasoning.
Traditionally, human or model feedback for evaluating and further tuning LLM
performance has been provided at the response level, enabling faster and more
cost-effective assessments. However, recent works (Amplayo et al. [2022], Wu et
al. [2023]) indicate that sentence-level labels may provide more accurate and
interpretable feedback for LLM optimization. In this work, we introduce methods
to disaggregate response-level labels into sentence-level (pseudo-)labels. Our
approach leverages multiple instance learning (MIL) and learning from label
proportions (LLP) techniques in conjunction with prior information (e.g.,
document-sentence cosine similarity) to train a specialized model for
sentence-level scoring. We also employ techniques which use model predictions
to pseudo-label the train-set at the sentence-level for model training to
further improve performance.
  We conduct extensive evaluations of our methods across six datasets and four
tasks: retrieval, question answering, summarization, and math reasoning. Our
results demonstrate improved performance compared to multiple baselines across
most of these tasks. Our work is the first to develop response-level feedback
to sentence-level scoring techniques, leveraging sentence-level prior
information, along with comprehensive evaluations on multiple tasks as well as
end-to-end finetuning evaluation showing performance comparable to a model
trained on fine-grained human annotated labels.
\\ ( https://arxiv.org/abs/2404.04817 ,  112kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04838
Date: Sun, 7 Apr 2024 07:24:45 GMT   (1662kb,D)

Title: Data Bias According to Bipol: Men are Naturally Right and It is the Role
  of Women to Follow Their Lead
Authors: Irene Pagliai, Goya van Boven, Tosin Adewumi, Lama Alkhaled, Namrata
  Gurung, Isabella S\"odergren and Elisa Barney
Categories: cs.CL
Comments: 11 pages, 6 figures
\\
  We introduce new large labeled datasets on bias in 3 languages and show in
experiments that bias exists in all 10 datasets of 5 languages evaluated,
including benchmark datasets on the English GLUE/SuperGLUE leaderboards. The 3
new languages give a total of almost 6 million labeled samples and we benchmark
on these datasets using SotA multilingual pretrained models: mT5 and mBERT. The
challenge of social bias, based on prejudice, is ubiquitous, as recent events
with AI and large language models (LLMs) have shown. Motivated by this
challenge, we set out to estimate bias in multiple datasets. We compare some
recent bias metrics and use bipol, which has explainability in the metric. We
also confirm the unverified assumption that bias exists in toxic comments by
randomly sampling 200 samples from a toxic dataset population using the
confidence level of 95% and error margin of 7%. Thirty gold samples were
randomly distributed in the 200 samples to secure the quality of the
annotation. Our findings confirm that many of the datasets have male bias
(prejudice against women), besides other types of bias. We publicly release our
new datasets, lexica, models, and codes.
\\ ( https://arxiv.org/abs/2404.04838 ,  1662kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04845
Date: Sun, 7 Apr 2024 07:34:49 GMT   (7950kb,D)

Title: SLPL SHROOM at SemEval\-2024 Task 06: A comprehensive study on models
  ability to detect hallucination
Authors: Pouya Fallah, Soroush Gooran, Mohammad Jafarinasab, Pouya Sadeghi,
  Reza Farnia, Amirreza Tarabkhah, Zainab Sadat Taghavi, Hossein Sameti
Categories: cs.CL cs.AI
\\
  Language models, particularly generative models, are susceptible to
hallucinations, generating outputs that contradict factual knowledge or the
source text. This study explores methods for detecting hallucinations in three
SemEval-2024 Task 6 tasks: Machine Translation, Definition Modeling, and
Paraphrase Generation. We evaluate two methods: semantic similarity between the
generated text and factual references, and an ensemble of language models that
judge each other's outputs. Our results show that semantic similarity achieves
moderate accuracy and correlation scores in trial data, while the ensemble
method offers insights into the complexities of hallucination detection but
falls short of expectations. This work highlights the challenges of
hallucination detection and underscores the need for further research in this
critical area.
\\ ( https://arxiv.org/abs/2404.04845 ,  7950kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04846
Date: Sun, 7 Apr 2024 07:39:45 GMT   (7895kb,D)

Title: F-MALLOC: Feed-forward Memory Allocation for Continual Learning in
  Neural Machine Translation
Authors: Junhong Wu, Yuchen Liu, Chengqing Zong
Categories: cs.CL
Comments: Accepted to the main conference of NAACL 2024
\\
  In the evolving landscape of Neural Machine Translation (NMT), the
pretrain-then-finetune paradigm has yielded impressive results. However, the
persistent challenge of Catastrophic Forgetting (CF) remains a hurdle. While
previous work has introduced Continual Learning (CL) methods to address CF,
these approaches grapple with the delicate balance between avoiding forgetting
and maintaining system extensibility. To address this, we propose a CL method,
named $\textbf{F-MALLOC}$ ($\textbf{F}$eed-forward $\textbf{M}$emory
$\textbf{ALLOC}ation)$. F-MALLOC is inspired by recent insights highlighting
that feed-forward layers emulate neural memories and encapsulate crucial
translation knowledge. It decomposes feed-forward layers into discrete memory
cells and allocates these memories to different tasks. By learning to allocate
and safeguard these memories, our method effectively alleviates CF while
ensuring robust extendability. Besides, we propose a comprehensive assessment
protocol for multi-stage CL of NMT systems. Experiments conducted following
this new protocol showcase the superior performance of F-MALLOC, evidenced by
higher BLEU scores and almost zero forgetting.
\\ ( https://arxiv.org/abs/2404.04846 ,  7895kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04850
Date: Sun, 7 Apr 2024 07:44:33 GMT   (452kb,D)

Title: Lucky 52: How Many Languages Are Needed to Instruction Fine-Tune Large
  Language Models?
Authors: Shaoxiong Ji and Pinzhen Chen
Categories: cs.CL
\\
  Fine-tuning large language models for multilingual downstream tasks requires
a diverse set of languages to capture the nuances and structures of different
linguistic contexts effectively. While the specific number varies depending on
the desired scope and target languages, we argue that the number of languages,
language exposure, and similarity that incorporate the selection of languages
for fine-tuning are some important aspects to examine. By fine-tuning large
multilingual models on 1 to 52 languages, this paper answers one question: How
many languages are needed in instruction fine-tuning for multilingual tasks? We
investigate how multilingual instruction fine-tuned models behave on
multilingual benchmarks with an increasing number of languages and discuss our
findings from the perspective of language exposure and similarity.
\\ ( https://arxiv.org/abs/2404.04850 ,  452kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04889
Date: Sun, 7 Apr 2024 09:10:47 GMT   (196kb,D)

Title: Ethos and Pathos in Online Group Discussions: Corpora for Polarisation
  Issues in Social Media
Authors: Ewelina Gajewska, Katarzyna Budzynska, Barbara Konat, Marcin Koszowy,
  Konrad Kiljan, Maciej Uberna, He Zhang
Categories: cs.CL
\\
  Growing polarisation in society caught the attention of the scientific
community as well as news media, which devote special issues to this
phenomenon. At the same time, digitalisation of social interactions requires to
revise concepts from social science regarding establishment of trust, which is
a key feature of all human interactions, and group polarisation, as well as new
computational tools to process large quantities of available data. Existing
methods seem insufficient to tackle the problem fully, thus, we propose to
approach the problem by investigating rhetorical strategies employed by
individuals in polarising discussions online. To this end, we develop
multi-topic and multi-platform corpora with manual annotation of appeals to
ethos and pathos, two modes of persuasion in Aristotelian rhetoric. It can be
employed for training language models to advance the study of communication
strategies online on a large scale. With the use of computational methods, our
corpora allows an investigation of recurring patterns in polarising exchanges
across topics of discussion and media platforms, and conduct both quantitative
and qualitative analyses of language structures leading to and engaged in
polarisation.
\\ ( https://arxiv.org/abs/2404.04889 ,  196kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04900
Date: Sun, 7 Apr 2024 09:52:31 GMT   (3415kb,D)

Title: Radial Networks: Dynamic Layer Routing for High-Performance Large
  Language Models
Authors: Jordan Dotzel, Yash Akhauri, Ahmed S. AbouElhamayed, Carly Jiang,
  Mohamed Abdelfattah, Zhiru Zhang
Categories: cs.CL
Comments: First two authors have equal contribution
\\
  Large language models (LLMs) often struggle with strict memory, latency, and
power demands. To meet these demands, various forms of dynamic sparsity have
been proposed that reduce compute on an input-by-input basis. These methods
improve over static methods by exploiting the variance across individual
inputs, which has steadily grown with the exponential increase in training
data. Yet, the increasing depth within modern models, currently with hundreds
of layers, has opened opportunities for dynamic layer sparsity, which skips the
computation for entire layers. In this work, we explore the practicality of
layer sparsity by profiling residual connections and establish the relationship
between model depth and layer sparsity. For example, the residual blocks in the
OPT-66B model have a median contribution of 5% to its output. We then take
advantage of this dynamic sparsity and propose Radial Networks, which perform
token-level routing between layers guided by a trained router module. These
networks can be used in a post-training distillation from sequential networks
or trained from scratch to co-learn the router and layer weights. They enable
scaling to larger model sizes by decoupling the number of layers from the
dynamic depth of the network, and their design allows for layer reuse. By
varying the compute token by token, they reduce the overall resources needed
for generating entire sequences. Overall, this leads to larger capacity
networks with significantly lower compute and serving costs for large language
models.
\\ ( https://arxiv.org/abs/2404.04900 ,  3415kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04925
Date: Sun, 7 Apr 2024 11:52:44 GMT   (2521kb,D)

Title: Multilingual Large Language Model: A Survey of Resources, Taxonomy and
  Frontiers
Authors: Libo Qin, Qiguang Chen, Yuhang Zhou, Zhi Chen, Yinghui Li, Lizi Liao,
  Min Li, Wanxiang Che, Philip S. Yu
Categories: cs.CL
\\
  Multilingual Large Language Models are capable of using powerful Large
Language Models to handle and respond to queries in multiple languages, which
achieves remarkable success in multilingual natural language processing tasks.
Despite these breakthroughs, there still remains a lack of a comprehensive
survey to summarize existing approaches and recent developments in this field.
To this end, in this paper, we present a thorough review and provide a unified
perspective to summarize the recent progress as well as emerging trends in
multilingual large language models (MLLMs) literature. The contributions of
this paper can be summarized: (1) First survey: to our knowledge, we take the
first step and present a thorough review in MLLMs research field according to
multi-lingual alignment; (2) New taxonomy: we offer a new and unified
perspective to summarize the current progress of MLLMs; (3) New frontiers: we
highlight several emerging frontiers and discuss the corresponding challenges;
(4) Abundant resources: we collect abundant open-source resources, including
relevant papers, data corpora, and leaderboards. We hope our work can provide
the community with quick access and spur breakthrough research in MLLMs.
\\ ( https://arxiv.org/abs/2404.04925 ,  2521kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04932
Date: Sun, 7 Apr 2024 12:10:04 GMT   (167kb,D)

Title: Towards Understanding the Influence of Reward Margin on Preference Model
  Performance
Authors: Bowen Qin, Duanyu Feng, Xi Yang
Categories: cs.CL cs.AI
\\
  Reinforcement Learning from Human Feedback (RLHF) is a widely used framework
for the training of language models. However, the process of using RLHF to
develop a language model that is well-aligned presents challenges, especially
when it comes to optimizing the reward model. Our research has found that
existing reward models, when trained using the traditional ranking objective
based on human preference data, often struggle to effectively distinguish
between responses that are more or less favorable in real-world scenarios. To
bridge this gap, our study introduces a novel method to estimate the preference
differences without the need for detailed, exhaustive labels from human
annotators. Our experimental results provide empirical evidence that
incorporating margin values into the training process significantly improves
the effectiveness of reward models. This comparative analysis not only
demonstrates the superiority of our approach in terms of reward prediction
accuracy but also highlights its effectiveness in practical applications.
\\ ( https://arxiv.org/abs/2404.04932 ,  167kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04941
Date: Sun, 7 Apr 2024 12:25:35 GMT   (9332kb,D)

Title: Prompting Large Language Models for Zero-shot Essay Scoring via
  Multi-trait Specialization
Authors: Sanwoo Lee, Yida Cai, Desong Meng, Ziyang Wang, Yunfang Wu
Categories: cs.CL
\\
  Advances in automated essay scoring (AES) have traditionally relied on
labeled essays, requiring tremendous cost and expertise for their acquisition.
Recently, large language models (LLMs) have achieved great success in various
tasks, but their potential is less explored in AES. In this paper, we propose
Multi Trait Specialization (MTS), a zero-shot prompting framework to elicit
essay scoring capabilities in LLMs. Specifically, we leverage ChatGPT to
decompose writing proficiency into distinct traits and generate scoring
criteria for each trait. Then, an LLM is prompted to extract trait scores from
several conversational rounds, each round scoring one of the traits based on
the scoring criteria. Finally, we derive the overall score via trait averaging
and min-max scaling. Experimental results on two benchmark datasets demonstrate
that MTS consistently outperforms straightforward prompting (Vanilla) in
average QWK across all LLMs and datasets, with maximum gains of 0.437 on
TOEFL11 and 0.355 on ASAP. Additionally, with the help of MTS, the small-sized
Llama2-13b-chat substantially outperforms ChatGPT, facilitating an effective
deployment in real applications.
\\ ( https://arxiv.org/abs/2404.04941 ,  9332kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04949
Date: Sun, 7 Apr 2024 13:02:21 GMT   (8780kb,D)

Title: SilverSight: A Multi-Task Chinese Financial Large Language Model Based
  on Adaptive Semantic Space Learning
Authors: Yuhang Zhou and Zeping Li and Siyu Tian and Yuchen Ni and Sen Liu and
  Guangnan Ye and Hongfeng Chai
Categories: cs.CL cs.CE
Comments: 17 pages, 17 figures
\\
  Large language models (LLMs) are increasingly being applied across various
specialized fields, leveraging their extensive knowledge to empower a multitude
of scenarios within these domains. However, each field encompasses a variety of
specific tasks that require learning, and the diverse, heterogeneous data
across these domains can lead to conflicts during model task transfer. In
response to this challenge, our study introduces an Adaptive Semantic Space
Learning (ASSL) framework, which utilizes the adaptive reorganization of data
distributions within the semantic space to enhance the performance and
selection efficacy of multi-expert models. Utilizing this framework, we trained
a financial multi-task LLM named "SilverSight". Our research findings
demonstrate that our framework can achieve results close to those obtained with
full data training using only 10% of the data, while also exhibiting strong
generalization capabilities.
\\ ( https://arxiv.org/abs/2404.04949 ,  8780kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04959
Date: Sun, 7 Apr 2024 13:37:30 GMT   (900kb,D)

Title: A Two Dimensional Feature Engineering Method for Relation Extraction
Authors: Hao Wang, Yanping Chen, Weizhe Yang, Yongbin Qin, Ruizhang Huang
Categories: cs.CL
\\
  Transforming a sentence into a two-dimensional (2D) representation (e.g., the
table filling) has the ability to unfold a semantic plane, where an element of
the plane is a word-pair representation of a sentence which may denote a
possible relation representation composed of two named entities. The 2D
representation is effective in resolving overlapped relation instances.
However, in related works, the representation is directly transformed from a
raw input. It is weak to utilize prior knowledge, which is important to support
the relation extraction task. In this paper, we propose a two-dimensional
feature engineering method in the 2D sentence representation for relation
extraction. Our proposed method is evaluated on three public datasets (ACE05
Chinese, ACE05 English, and SanWen) and achieves the state-of-the-art
performance. The results indicate that two-dimensional feature engineering can
take advantage of a two-dimensional sentence representation and make full use
of prior knowledge in traditional feature engineering. Our code is publicly
available at
https://github.com/Wang-ck123/A-Two-Dimensional-Feature-Engineering-Method-for-Entity-Relation-Extraction
\\ ( https://arxiv.org/abs/2404.04959 ,  900kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04963
Date: Sun, 7 Apr 2024 13:58:41 GMT   (8288kb,D)

Title: SemEval-2024 Task 2: Safe Biomedical Natural Language Inference for
  Clinical Trials
Authors: Mael Jullien, Marco Valentino, Andr\'e Freitas
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) are at the forefront of NLP achievements but
fall short in dealing with shortcut learning, factual inconsistency, and
vulnerability to adversarial inputs.These shortcomings are especially critical
in medical contexts, where they can misrepresent actual model capabilities.
Addressing this, we present SemEval-2024 Task 2: Safe Biomedical Natural
Language Inference for ClinicalTrials. Our contributions include the refined
NLI4CT-P dataset (i.e., Natural Language Inference for Clinical Trials -
Perturbed), designed to challenge LLMs with interventional and causal reasoning
tasks, along with a comprehensive evaluation of methods and results for
participant submissions. A total of 106 participants registered for the task
contributing to over 1200 individual submissions and 25 system overview papers.
This initiative aims to advance the robustness and applicability of NLI models
in healthcare, ensuring safer and more dependable AI assistance in clinical
decision-making. We anticipate that the dataset, models, and outcomes of this
task can support future research in the field of biomedical NLI. The dataset,
competition leaderboard, and website are publicly available.
\\ ( https://arxiv.org/abs/2404.04963 ,  8288kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04990
Date: Sun, 7 Apr 2024 15:23:28 GMT   (487kb,D)

Title: MLaKE: Multilingual Knowledge Editing Benchmark for Large Language
  Models
Authors: Zihao Wei, Jingcheng Deng, Liang Pang, Hanxing Ding, Huawei Shen,
  Xueqi Cheng
Categories: cs.CL
\\
  The extensive utilization of large language models (LLMs) underscores the
crucial necessity for precise and contemporary knowledge embedded within their
intrinsic parameters. Existing research on knowledge editing primarily
concentrates on monolingual scenarios, neglecting the complexities presented by
multilingual contexts and multi-hop reasoning. To address these challenges, our
study introduces MLaKE (Multilingual Language Knowledge Editing), a novel
benchmark comprising 4072 multi-hop and 5360 single-hop questions designed to
evaluate the adaptability of knowledge editing methods across five languages:
English, Chinese, Japanese, French, and German. MLaKE aggregates fact chains
from Wikipedia across languages and utilizes LLMs to generate questions in both
free-form and multiple-choice. We evaluate the multilingual knowledge editing
generalization capabilities of existing methods on MLaKE. Existing knowledge
editing methods demonstrate higher success rates in English samples compared to
other languages. However, their generalization capabilities are limited in
multi-language experiments. Notably, existing knowledge editing methods often
show relatively high generalization for languages within the same language
family compared to languages from different language families. These results
underscore the imperative need for advancements in multilingual knowledge
editing and we hope MLaKE can serve as a valuable resource for benchmarking and
solution development.
\\ ( https://arxiv.org/abs/2404.04990 ,  487kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05088
Date: Sun, 7 Apr 2024 22:06:19 GMT   (1127kb,D)

Title: How much reliable is ChatGPT's prediction on Information Extraction
  under Input Perturbations?
Authors: Ishani Mondal and Abhilasha Sancheti
Categories: cs.CL
Comments: 3 Figures, 7 Tables
\\
  In this paper, we assess the robustness (reliability) of ChatGPT under input
perturbations for one of the most fundamental tasks of Information Extraction
(IE) i.e. Named Entity Recognition (NER). Despite the hype, the majority of the
researchers have vouched for its language understanding and generation
capabilities; a little attention has been paid to understand its robustness:
How the input-perturbations affect 1) the predictions, 2) the confidence of
predictions and 3) the quality of rationale behind its prediction. We perform a
systematic analysis of ChatGPT's robustness (under both zero-shot and few-shot
setup) on two NER datasets using both automatic and human evaluation. Based on
automatic evaluation metrics, we find that 1) ChatGPT is more brittle on Drug
or Disease replacements (rare entities) compared to the perturbations on widely
known Person or Location entities, 2) the quality of explanations for the same
entity considerably differ under different types of "Entity-Specific" and
"Context-Specific" perturbations and the quality can be significantly improved
using in-context learning, and 3) it is overconfident for majority of the
incorrect predictions, and hence it could lead to misguidance of the end-users.
\\ ( https://arxiv.org/abs/2404.05088 ,  1127kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05089
Date: Sun, 7 Apr 2024 22:13:43 GMT   (1029kb,D)

Title: SEER-MoE: Sparse Expert Efficiency through Regularization for
  Mixture-of-Experts
Authors: Alexandre Muzio, Alex Sun, Churan He
Categories: cs.CL cs.LG
Comments: 8+3 pages
\\
  The advancement of deep learning has led to the emergence of
Mixture-of-Experts (MoEs) models, known for their dynamic allocation of
computational resources based on input. Despite their promise, MoEs face
challenges, particularly in terms of memory requirements. To address this, our
work introduces SEER-MoE, a novel two-stage framework for reducing both the
memory footprint and compute requirements of pre-trained MoE models. The first
stage involves pruning the total number of experts using a heavy-hitters
counting guidance, while the second stage employs a regularization-based
fine-tuning strategy to recover accuracy loss and reduce the number of
activated experts during inference. Our empirical studies demonstrate the
effectiveness of our method, resulting in a sparse MoEs model optimized for
inference efficiency with minimal accuracy trade-offs.
\\ ( https://arxiv.org/abs/2404.05089 ,  1029kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05091
Date: Sun, 7 Apr 2024 22:16:50 GMT   (109kb,D)

Title: Advancing Geometric Problem Solving: A Comprehensive Benchmark for
  Multimodal Model Evaluation
Authors: Kai Sun and Yushi Bai and Nianyi Lin
Categories: cs.CL
\\
  In this work, we present the MM-MATH dataset, a novel benchmark developed to
rigorously evaluate the performance of advanced large language and multimodal
models - including but not limited to GPT-4, GPT-4V, and Claude - within the
domain of geometric computation. This dataset comprises 5,929 meticulously
crafted geometric problems, each paired with a corresponding image, aimed at
mirroring the complexity and requirements typical of ninth-grade mathematics.
The motivation behind MM-MATH stems from the burgeoning interest and
significant strides in multimodal technology, which necessitates a paradigm
shift in assessment methodologies from mere outcome analysis to a more holistic
evaluation encompassing reasoning and procedural correctness. Despite
impressive gains in various benchmark performances, our analysis uncovers a
persistent and notable deficiency in these models' ability to parse and
interpret geometric information accurately from images, accounting for over 60%
of observed errors. By deploying a dual-focused evaluation approach, examining
both the end results and the underlying problem-solving processes, we unearthed
a marked discrepancy between the capabilities of current multimodal models and
human-level proficiency. The introduction of MM-MATH represents a tripartite
contribution to the field: it not only serves as a comprehensive and
challenging benchmark for assessing geometric problem-solving prowess but also
illuminates critical gaps in textual and visual comprehension that current
models exhibit. Through this endeavor, we aspire to catalyze further research
and development aimed at bridging these gaps, thereby advancing the state of
multimodal model capabilities to new heights.
\\ ( https://arxiv.org/abs/2404.05091 ,  109kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05133
Date: Mon, 8 Apr 2024 01:21:11 GMT   (2381kb,D)

Title: EcoVerse: An Annotated Twitter Dataset for Eco-Relevance Classification,
  Environmental Impact Analysis, and Stance Detection
Authors: Francesca Grasso, Stefano Locci, Giovanni Siragusa, and Luigi Di Caro
Categories: cs.CL
\\
  Anthropogenic ecological crisis constitutes a significant challenge that all
within the academy must urgently face, including the Natural Language
Processing (NLP) community. While recent years have seen increasing work
revolving around climate-centric discourse, crucial environmental and
ecological topics outside of climate change remain largely unaddressed, despite
their prominent importance. Mainstream NLP tasks, such as sentiment analysis,
dominate the scene, but there remains an untouched space in the literature
involving the analysis of environmental impacts of certain events and
practices. To address this gap, this paper presents EcoVerse, an annotated
English Twitter dataset of 3,023 tweets spanning a wide spectrum of
environmental topics. We propose a three-level annotation scheme designed for
Eco-Relevance Classification, Stance Detection, and introducing an original
approach for Environmental Impact Analysis. We detail the data collection,
filtering, and labeling process that led to the creation of the dataset.
Remarkable Inter-Annotator Agreement indicates that the annotation scheme
produces consistent annotations of high quality. Subsequent classification
experiments using BERT-based models, including ClimateBERT, are presented.
These yield encouraging results, while also indicating room for a model
specifically tailored for environmental texts. The dataset is made freely
available to stimulate further research.
\\ ( https://arxiv.org/abs/2404.05133 ,  2381kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05143
Date: Mon, 8 Apr 2024 01:54:28 GMT   (2446kb,D)

Title: Plug and Play with Prompts: A Prompt Tuning Approach for Controlling
  Text Generation
Authors: Rohan Deepak Ajwani, Zining Zhu, Jonathan Rose, Frank Rudzicz
Categories: cs.CL cs.AI cs.LG
Comments: 9 pages, 3 figures, Presented at Deployable AI Workshop at AAAI-2024
Journal-ref: Presented at Deployable AI Workshop at AAAI-2024
\\
  Transformer-based Large Language Models (LLMs) have shown exceptional
language generation capabilities in response to text-based prompts. However,
controlling the direction of generation via textual prompts has been
challenging, especially with smaller models. In this work, we explore the use
of Prompt Tuning to achieve controlled language generation. Generated text is
steered using prompt embeddings, which are trained using a small language
model, used as a discriminator. Moreover, we demonstrate that these prompt
embeddings can be trained with a very small dataset, with as low as a few
hundred training examples. Our method thus offers a data and parameter
efficient solution towards controlling language model outputs. We carry out
extensive evaluation on four datasets: SST-5 and Yelp (sentiment analysis),
GYAFC (formality) and JIGSAW (toxic language). Finally, we demonstrate the
efficacy of our method towards mitigating harmful, toxic, and biased text
generated by language models.
\\ ( https://arxiv.org/abs/2404.05143 ,  2446kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05144
Date: Mon, 8 Apr 2024 01:55:28 GMT   (1072kb,D)

Title: Enhancing Clinical Efficiency through LLM: Discharge Note Generation for
  Cardiac Patients
Authors: HyoJe Jung, Yunha Kim, Heejung Choi, Hyeram Seo, Minkyoung Kim, JiYe
  Han, Gaeun Kee, Seohyun Park, Soyoung Ko, Byeolhee Kim, Suyeon Kim, Tae Joon
  Jun, Young-Hak Kim
Categories: cs.CL cs.CV cs.LG
Comments: 10 pages, 1 figure, 3 tables, conference
\\
  Medical documentation, including discharge notes, is crucial for ensuring
patient care quality, continuity, and effective medical communication. However,
the manual creation of these documents is not only time-consuming but also
prone to inconsistencies and potential errors. The automation of this
documentation process using artificial intelligence (AI) represents a promising
area of innovation in healthcare. This study directly addresses the
inefficiencies and inaccuracies in creating discharge notes manually,
particularly for cardiac patients, by employing AI techniques, specifically
large language model (LLM). Utilizing a substantial dataset from a cardiology
center, encompassing wide-ranging medical records and physician assessments,
our research evaluates the capability of LLM to enhance the documentation
process. Among the various models assessed, Mistral-7B distinguished itself by
accurately generating discharge notes that significantly improve both
documentation efficiency and the continuity of care for patients. These notes
underwent rigorous qualitative evaluation by medical expert, receiving high
marks for their clinical relevance, completeness, readability, and contribution
to informed decision-making and care planning. Coupled with quantitative
analyses, these results confirm Mistral-7B's efficacy in distilling complex
medical information into concise, coherent summaries. Overall, our findings
illuminate the considerable promise of specialized LLM, such as Mistral-7B, in
refining healthcare documentation workflows and advancing patient care. This
study lays the groundwork for further integrating advanced AI technologies in
healthcare, demonstrating their potential to revolutionize patient
documentation and support better care outcomes.
\\ ( https://arxiv.org/abs/2404.05144 ,  1072kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05159
Date: Mon, 8 Apr 2024 02:55:01 GMT   (520kb)

Title: Semantic Stealth: Adversarial Text Attacks on NLP Using Several Methods
Authors: Roopkatha Dey, Aivy Debnath, Sayak Kumar Dutta, Kaustav Ghosh, Arijit
  Mitra, Arghya Roy Chowdhury, Jaydip Sen
Categories: cs.CL cs.CR cs.LG
Comments: This report pertains to the Capstone Project done by Group 2 of the
  Fall batch of 2023 students at Praxis Tech School, Kolkata, India. The
  reports consists of 28 pages and it includes 10 tables. This is the preprint
  which will be submitted to IEEE CONIT 2024 for review
\\
  In various real-world applications such as machine translation, sentiment
analysis, and question answering, a pivotal role is played by NLP models,
facilitating efficient communication and decision-making processes in domains
ranging from healthcare to finance. However, a significant challenge is posed
to the robustness of these natural language processing models by text
adversarial attacks. These attacks involve the deliberate manipulation of input
text to mislead the predictions of the model while maintaining human
interpretability. Despite the remarkable performance achieved by
state-of-the-art models like BERT in various natural language processing tasks,
they are found to remain vulnerable to adversarial perturbations in the input
text. In addressing the vulnerability of text classifiers to adversarial
attacks, three distinct attack mechanisms are explored in this paper using the
victim model BERT: BERT-on-BERT attack, PWWS attack, and Fraud Bargain's Attack
(FBA). Leveraging the IMDB, AG News, and SST2 datasets, a thorough comparative
analysis is conducted to assess the effectiveness of these attacks on the BERT
classifier model. It is revealed by the analysis that PWWS emerges as the most
potent adversary, consistently outperforming other methods across multiple
evaluation scenarios, thereby emphasizing its efficacy in generating
adversarial examples for text classification. Through comprehensive
experimentation, the performance of these attacks is assessed and the findings
indicate that the PWWS attack outperforms others, demonstrating lower runtime,
higher accuracy, and favorable semantic similarity scores. The key insight of
this paper lies in the assessment of the relative performances of three
prevalent state-of-the-art attack mechanisms.
\\ ( https://arxiv.org/abs/2404.05159 ,  520kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05160
Date: Mon, 8 Apr 2024 03:00:10 GMT   (306kb)

Title: Linguistic Changes in Spontaneous Speech for Detecting Parkinsons
  Disease Using Large Language Models
Authors: Jonathan Crawford
Categories: cs.CL eess.AS
Comments: 12 pages, 3 figures
\\
  Parkinsons disease is the second most prevalent neurodegenerative disorder
with over ten million active cases worldwide and one million new diagnoses per
year. Detecting and subsequently diagnosing the disease is challenging because
of symptom heterogeneity with respect to complexity, as well as the type and
timing of phenotypic manifestations. Typically, language impairment can present
in the prodromal phase and precede motor symptoms suggesting that a
linguistic-based approach could serve as a diagnostic method for incipient
Parkinsons disease. Additionally, improved linguistic models may enhance other
approaches through ensemble techniques. The field of large language models is
advancing rapidly, presenting the opportunity to explore the use of these new
models for detecting Parkinsons disease and to improve on current linguistic
approaches with high-dimensional representations of linguistics. We evaluate
the application of state-of-the-art large language models to detect Parkinsons
disease automatically from spontaneous speech with up to 73% accuracy.
\\ ( https://arxiv.org/abs/2404.05160 ,  306kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05221
Date: Mon, 8 Apr 2024 06:35:09 GMT   (3732kb,D)

Title: LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step
  Reasoning with Large Language Models
Authors: Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang,
  Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, Zhen Wang, Zhiting Hu
Categories: cs.CL cs.AI
Comments: Project website: https://www.llm-reasoners.net/
\\
  Generating accurate step-by-step reasoning is essential for Large Language
Models (LLMs) to address complex problems and enhance robustness and
interpretability. Despite the flux of research on developing advanced reasoning
approaches, systematically analyzing the diverse LLMs and reasoning strategies
in generating reasoning chains remains a significant challenge. The
difficulties stem from the lack of two key elements: (1) an automatic method
for evaluating the generated reasoning chains on different tasks, and (2) a
unified formalism and implementation of the diverse reasoning approaches for
systematic comparison. This paper aims to close the gap: (1) We introduce
AutoRace for fully automated reasoning chain evaluation. Existing metrics rely
on expensive human annotations or pre-defined LLM prompts not adaptable to
different tasks. In contrast, AutoRace automatically creates detailed
evaluation criteria tailored for each task, and uses GPT-4 for accurate
evaluation following the criteria. (2) We develop LLM Reasoners, a library for
standardized modular implementation of existing and new reasoning algorithms,
under a unified formulation of the search, reward, and world model components.
With the new evaluation and library, (3) we conduct extensive study of
different reasoning approaches (e.g., CoT, ToT, RAP). The analysis reveals
interesting findings about different factors contributing to reasoning,
including the reward-guidance, breadth-vs-depth in search, world model, and
prompt formats, etc.
\\ ( https://arxiv.org/abs/2404.05221 ,  3732kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05243
Date: Mon, 8 Apr 2024 07:15:06 GMT   (8156kb,D)

Title: Product Description and QA Assisted Self-Supervised Opinion
  Summarization
Authors: Tejpalsingh Siledar, Rupasai Rangaraju, Sankara Sri Raghava Ravindra
  Muddu, Suman Banerjee, Amey Patil, Sudhanshu Shekhar Singh, Muthusamy
  Chelliah, Nikesh Garera, Swaprava Nath, Pushpak Bhattacharyya
Categories: cs.CL cs.AI
\\
  In e-commerce, opinion summarization is the process of summarizing the
consensus opinions found in product reviews. However, the potential of
additional sources such as product description and question-answers (QA) has
been considered less often. Moreover, the absence of any supervised training
data makes this task challenging. To address this, we propose a novel synthetic
dataset creation (SDC) strategy that leverages information from reviews as well
as additional sources for selecting one of the reviews as a pseudo-summary to
enable supervised training. Our Multi-Encoder Decoder framework for Opinion
Summarization (MEDOS) employs a separate encoder for each source, enabling
effective selection of information while generating the summary. For
evaluation, due to the unavailability of test sets with additional sources, we
extend the Amazon, Oposum+, and Flipkart test sets and leverage ChatGPT to
annotate summaries. Experiments across nine test sets demonstrate that the
combination of our SDC approach and MEDOS model achieves on average a 14.5%
improvement in ROUGE-1 F1 over the SOTA. Moreover, comparative analysis
underlines the significance of incorporating additional sources for generating
more informative summaries. Human evaluations further indicate that MEDOS
scores relatively higher in coherence and fluency with 0.41 and 0.5 (-1 to 1)
respectively, compared to existing models. To the best of our knowledge, we are
the first to generate opinion summaries leveraging additional sources in a
self-supervised setting.
\\ ( https://arxiv.org/abs/2404.05243 ,  8156kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05245
Date: Mon, 8 Apr 2024 07:21:46 GMT   (1817kb,D)

Title: Supervised Gradual Machine Learning for Aspect Category Detection
Authors: Murtadha Ahmed, Qun Chen
Categories: cs.CL
\\
  Aspect Category Detection (ACD) aims to identify implicit and explicit
aspects in a given review sentence. The state-of-the-art approaches for ACD use
Deep Neural Networks (DNNs) to address the problem as a multi-label
classification task. However, learning category-specific representations
heavily rely on the amount of labeled examples, which may not readily available
in real-world scenarios. In this paper, we propose a novel approach to tackle
the ACD task by combining DNNs with Gradual Machine Learning (GML) in a
supervised setting. we aim to leverage the strength of DNN in semantic relation
modeling, which can facilitate effective knowledge transfer between labeled and
unlabeled instances during the gradual inference of GML. To achieve this, we
first analyze the learned latent space of the DNN to model the relations, i.e.,
similar or opposite, between instances. We then represent these relations as
binary features in a factor graph to efficiently convey knowledge. Finally, we
conduct a comparative study of our proposed solution on real benchmark datasets
and demonstrate that the GML approach, in collaboration with DNNs for feature
extraction, consistently outperforms pure DNN solutions.
\\ ( https://arxiv.org/abs/2404.05245 ,  1817kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05250
Date: Mon, 8 Apr 2024 07:26:27 GMT   (8244kb,D)

Title: Interpreting Themes from Educational Stories
Authors: Yigeng Zhang, Fabio A. Gonz\'alez, Thamar Solorio
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024 (long paper)
\\
  Reading comprehension continues to be a crucial research focus in the NLP
community. Recent advances in Machine Reading Comprehension (MRC) have mostly
centered on literal comprehension, referring to the surface-level understanding
of content. In this work, we focus on the next level - interpretive
comprehension, with a particular emphasis on inferring the themes of a
narrative text. We introduce the first dataset specifically designed for
interpretive comprehension of educational narratives, providing corresponding
well-edited theme texts. The dataset spans a variety of genres and cultural
origins and includes human-annotated theme keywords with varying levels of
granularity. We further formulate NLP tasks under different abstractions of
interpretive comprehension toward the main idea of a story. After conducting
extensive experiments with state-of-the-art methods, we found the task to be
both challenging and significant for NLP research. The dataset and source code
have been made publicly available to the research community at
https://github.com/RiTUAL-UH/EduStory.
\\ ( https://arxiv.org/abs/2404.05250 ,  8244kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05281
Date: Mon, 8 Apr 2024 08:13:40 GMT   (241kb)

Title: Multi-Task Learning for Features Extraction in Financial Annual Reports
Authors: Syrielle Montariol and Matej Martinc and Andra\v{z} Pelicon and Senja
  Pollak and Boshko Koloski and Igor Lon\v{c}arski and Aljo\v{s}a
  Valentin\v{c}i\v{c}
Categories: cs.CL
Comments: Accepted at MIDAS Workshop at ECML-PKDD 2022
\\
  For assessing various performance indicators of companies, the focus is
shifting from strictly financial (quantitative) publicly disclosed information
to qualitative (textual) information. This textual data can provide valuable
weak signals, for example through stylistic features, which can complement the
quantitative data on financial performance or on Environmental, Social and
Governance (ESG) criteria. In this work, we use various multi-task learning
methods for financial text classification with the focus on financial
sentiment, objectivity, forward-looking sentence prediction and ESG-content
detection. We propose different methods to combine the information extracted
from training jointly on different tasks; our best-performing method highlights
the positive effect of explicitly adding auxiliary task predictions as features
for the final target task during the multi-task training. Next, we use these
classifiers to extract textual features from annual reports of FTSE350
companies and investigate the link between ESG quantitative scores and these
features.
\\ ( https://arxiv.org/abs/2404.05281 ,  241kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05333
Date: Mon, 8 Apr 2024 09:22:41 GMT   (42kb)

Title: PORTULAN ExtraGLUE Datasets and Models: Kick-starting a Benchmark for
  the Neural Processing of Portuguese
Authors: Tom\'as Os\'orio, Bernardo Leite, Henrique Lopes Cardoso, Lu\'is
  Gomes, Jo\~ao Rodrigues, Rodrigo Santos, Ant\'onio Branco
Categories: cs.CL
Comments: Preprint - Paper accepted for BUCC 2024
\\
  Leveraging research on the neural modelling of Portuguese, we contribute a
collection of datasets for an array of language processing tasks and a
corresponding collection of fine-tuned neural language models on these
downstream tasks. To align with mainstream benchmarks in the literature,
originally developed in English, and to kick start their Portuguese
counterparts, the datasets were machine-translated from English with a
state-of-the-art translation engine. The resulting PORTULAN ExtraGLUE benchmark
is a basis for research on Portuguese whose improvement can be pursued in
future work. Similarly, the respective fine-tuned neural language models,
developed with a low-rank adaptation approach, are made available as baselines
that can stimulate future work on the neural processing of Portuguese. All
datasets and models have been developed and are made available for two variants
of Portuguese: European and Brazilian.
\\ ( https://arxiv.org/abs/2404.05333 ,  42kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05337
Date: Mon, 8 Apr 2024 09:25:32 GMT   (14187kb,D)

Title: Towards Objectively Benchmarking Social Intelligence for Language Agents
  at Action Level
Authors: Chenxu Wang, Bin Dai, Huaping Liu, and Baoyuan Wang
Categories: cs.CL cs.AI
\\
  Prominent large language models have exhibited human-level performance in
many domains, even enabling the derived agents to simulate human and social
interactions. While practical works have substantiated the practicability of
grounding language agents in sandbox simulation or embodied simulators, current
social intelligence benchmarks either stay at the language level or use
subjective metrics. In pursuit of a more realistic and objective evaluation, we
introduce the Social Tasks in Sandbox Simulation (STSS) benchmark, which
assesses language agents \textbf{objectively} at the \textbf{action level} by
scrutinizing the goal achievements within the multi-agent simulation.
Additionally, we sample conversation scenarios to build a language-level
benchmark to provide an economically prudent preliminary evaluation and align
with prevailing benchmarks. To gauge the significance of agent architecture, we
implement a target-driven planning (TDP) module as an adjunct to the existing
agent. Our evaluative findings highlight that the STSS benchmark is challenging
for state-of-the-art language agents. Furthermore, it effectively discriminates
between distinct language agents, suggesting its usefulness as a benchmark for
evaluating both language models and agent architectures.
\\ ( https://arxiv.org/abs/2404.05337 ,  14187kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05365
Date: Mon, 8 Apr 2024 10:04:55 GMT   (1176kb,D)

Title: NLP Progress in Indigenous Latin American Languages
Authors: Atnafu Lambebo Tonja, Fazlourrahman Balouchzahi, Sabur Butt, Olga
  Kolesnikova, Hector Ceballos, Alexander Gelbukh, Thamar Solorio
Categories: cs.CL
Comments: Accepted at NAACL 2024
\\
  The paper focuses on the marginalization of indigenous language communities
in the face of rapid technological advancements. We highlight the cultural
richness of these languages and the risk they face of being overlooked in the
realm of Natural Language Processing (NLP). We aim to bridge the gap between
these communities and researchers, emphasizing the need for inclusive
technological advancements that respect indigenous community perspectives. We
show the NLP progress of indigenous Latin American languages and the survey
that covers the status of indigenous languages in Latin America, their
representation in NLP, and the challenges and innovations required for their
preservation and development. The paper contributes to the current literature
in understanding the need and progress of NLP for indigenous communities of
Latin America, specifically low-resource and indigenous communities in general.
\\ ( https://arxiv.org/abs/2404.05365 ,  1176kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05399
Date: Mon, 8 Apr 2024 10:57:25 GMT   (527kb,D)

Title: SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and
  Improving Large Language Model Safety
Authors: Paul R\"ottger, Fabio Pernisi, Bertie Vidgen, Dirk Hovy
Categories: cs.CL cs.AI
\\
  The last two years have seen a rapid growth in concerns around the safety of
large language models (LLMs). Researchers and practitioners have met these
concerns by introducing an abundance of new datasets for evaluating and
improving LLM safety. However, much of this work has happened in parallel, and
with very different goals in mind, ranging from the mitigation of near-term
risks around bias and toxic content generation to the assessment of longer-term
catastrophic risk potential. This makes it difficult for researchers and
practitioners to find the most relevant datasets for a given use case, and to
identify gaps in dataset coverage that future work may fill. To remedy these
issues, we conduct a first systematic review of open datasets for evaluating
and improving LLM safety. We review 102 datasets, which we identified through
an iterative and community-driven process over the course of several months. We
highlight patterns and trends, such as a a trend towards fully synthetic
datasets, as well as gaps in dataset coverage, such as a clear lack of
non-English datasets. We also examine how LLM safety datasets are used in
practice -- in LLM release publications and popular LLM benchmarks -- finding
that current evaluation practices are highly idiosyncratic and make use of only
a small fraction of available datasets. Our contributions are based on
SafetyPrompts.com, a living catalogue of open datasets for LLM safety, which we
commit to updating continuously as the field of LLM safety develops.
\\ ( https://arxiv.org/abs/2404.05399 ,  527kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05405
Date: Mon, 8 Apr 2024 11:11:31 GMT   (1280kb,D)

Title: Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws
Authors: Zeyuan Allen-Zhu and Yuanzhi Li
Categories: cs.CL cs.AI cs.LG
\\
  Scaling laws describe the relationship between the size of language models
and their capabilities. Unlike prior studies that evaluate a model's capability
via loss or benchmarks, we estimate the number of knowledge bits a model
stores. We focus on factual knowledge represented as tuples, such as (USA,
capital, Washington D.C.) from a Wikipedia page. Through multiple controlled
datasets, we establish that language models can and only can store 2 bits of
knowledge per parameter, even when quantized to int8, and such knowledge can be
flexibly extracted for downstream applications. Consequently, a 7B model can
store 14B bits of knowledge, surpassing the English Wikipedia and textbooks
combined based on our estimation.
  More broadly, we present 12 results on how (1) training duration, (2) model
architecture, (3) quantization, (4) sparsity constraints such as MoE, and (5)
data signal-to-noise ratio affect a model's knowledge storage capacity. Notable
insights include:
  * The GPT-2 architecture, with rotary embedding, matches or even surpasses
LLaMA/Mistral architectures in knowledge storage, particularly over shorter
training durations. This arises because LLaMA/Mistral uses GatedMLP, which is
less stable and harder to train.
  * Prepending training data with domain names (e.g., wikipedia.org)
significantly increases a model's knowledge capacity. Language models can
autonomously identify and prioritize domains rich in knowledge, optimizing
their storage capacity.
\\ ( https://arxiv.org/abs/2404.05405 ,  1280kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05406
Date: Mon, 8 Apr 2024 11:14:58 GMT   (216kb)

Title: PerkwE_COQA: enhance Persian Conversational Question Answering by
  combining contextual keyword extraction with Large Language Models
Authors: Pardis Moradbeiki, Nasser Ghadiri
Categories: cs.CL cs.AI
MSC-class: 68T50, 68T07
ACM-class: J.3; I.2.7; I.2.1
\\
  Smart cities need the involvement of their residents to enhance quality of
life. Conversational query-answering is an emerging approach for user
engagement. There is an increasing demand of an advanced conversational
question-answering that goes beyond classic systems. Existing approaches have
shown that LLMs offer promising capabilities for CQA, but may struggle to
capture the nuances of conversational contexts. The new approach involves
understanding the content and engaging in a multi-step conversation with the
user to fulfill their needs. This paper presents a novel method to elevate the
performance of Persian Conversational question-answering (CQA) systems. It
combines the strengths of Large Language Models (LLMs) with contextual keyword
extraction. Our method extracts keywords specific to the conversational flow,
providing the LLM with additional context to understand the user's intent and
generate more relevant and coherent responses. We evaluated the effectiveness
of this combined approach through various metrics, demonstrating significant
improvements in CQA performance compared to an LLM-only baseline. The proposed
method effectively handles implicit questions, delivers contextually relevant
answers, and tackles complex questions that rely heavily on conversational
context. The findings indicate that our method outperformed the evaluation
benchmarks up to 8% higher than existing methods and the LLM-only baseline.
\\ ( https://arxiv.org/abs/2404.05406 ,  216kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05411
Date: Mon, 8 Apr 2024 11:25:30 GMT   (2574kb,D)

Title: Know When To Stop: A Study of Semantic Drift in Text Generation
Authors: Ava Spataru, Eric Hambro, Elena Voita and Nicola Cancedda
Categories: cs.CL
\\
  In this work, we explicitly show that modern LLMs tend to generate correct
facts first, then "drift away" and generate incorrect facts later: this was
occasionally observed but never properly measured. We develop a semantic drift
score that measures the degree of separation between correct and incorrect
facts in generated texts and confirm our hypothesis when generating
Wikipedia-style biographies. This correct-then-incorrect generation pattern
suggests that factual accuracy can be improved by knowing when to stop
generation. Therefore, we explore the trade-off between information quantity
and factual accuracy for several early stopping methods and manage to improve
factuality by a large margin. We further show that reranking with semantic
similarity can further improve these results, both compared to the baseline and
when combined with early stopping. Finally, we try calling external API to
bring the model back to the right generation path, but do not get positive
results. Overall, our methods generalize and can be applied to any long-form
text generation to produce more reliable information, by balancing trade-offs
between factual accuracy, information quantity and computational cost.
\\ ( https://arxiv.org/abs/2404.05411 ,  2574kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05415
Date: Mon, 8 Apr 2024 11:33:00 GMT   (766kb)

Title: Relation Extraction Using Large Language Models: A Case Study on
  Acupuncture Point Locations
Authors: Yiming Li, Xueqing Peng, Jianfu Li, Xu Zuo, Suyuan Peng, Donghong Pei,
  Cui Tao, Hua Xu, Na Hong
Categories: cs.CL cs.AI
\\
  In acupuncture therapy, the accurate location of acupoints is essential for
its effectiveness. The advanced language understanding capabilities of large
language models (LLMs) like Generative Pre-trained Transformers (GPT) present a
significant opportunity for extracting relations related to acupoint locations
from textual knowledge sources. This study aims to compare the performance of
GPT with traditional deep learning models (Long Short-Term Memory (LSTM) and
Bidirectional Encoder Representations from Transformers for Biomedical Text
Mining (BioBERT)) in extracting acupoint-related location relations and assess
the impact of pretraining and fine-tuning on GPT's performance. We utilized the
World Health Organization Standard Acupuncture Point Locations in the Western
Pacific Region (WHO Standard) as our corpus, which consists of descriptions of
361 acupoints. Five types of relations ('direction_of,' 'distance_of,'
'part_of,' 'near_acupoint,' and 'located_near') (n= 3,174) between acupoints
were annotated. Five models were compared: BioBERT, LSTM, pre-trained GPT-3.5,
and fine-tuned GPT-3.5, as well as pre-trained GPT-4. Performance metrics
included micro-average exact match precision, recall, and F1 scores. Our
results demonstrate that fine-tuned GPT-3.5 consistently outperformed other
models in F1 scores across all relation types. Overall, it achieved the highest
micro-average F1 score of 0.92. This study underscores the effectiveness of
LLMs like GPT in extracting relations related to acupoint locations, with
implications for accurately modeling acupuncture knowledge and promoting
standard implementation in acupuncture training and practice. The findings also
contribute to advancing informatics applications in traditional and
complementary medicine, showcasing the potential of LLMs in natural language
processing.
\\ ( https://arxiv.org/abs/2404.05415 ,  766kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05428
Date: Mon, 8 Apr 2024 11:55:44 GMT   (398kb,D)

Title: Language Models on a Diet: Cost-Efficient Development of Encoders for
  Closely-Related Languages via Additional Pretraining
Authors: Nikola Ljube\v{s}i\'c, V\'it Suchomel, Peter Rupnik, Taja Kuzman, Rik
  van Noord
Categories: cs.CL
\\
  The world of language models is going through turbulent times, better and
ever larger models are coming out at an unprecedented speed. However, we argue
that, especially for the scientific community, encoder models of up to 1
billion parameters are still very much needed, their primary usage being in
enriching large collections of data with metadata necessary for downstream
research. We investigate the best way to ensure the existence of such encoder
models on the set of very closely related languages - Croatian, Serbian,
Bosnian and Montenegrin, by setting up a diverse benchmark for these languages,
and comparing the trained-from-scratch models with the new models constructed
via additional pretraining of existing multilingual models. We show that
comparable performance to dedicated from-scratch models can be obtained by
additionally pretraining available multilingual models even with a limited
amount of computation. We also show that neighboring languages, in our case
Slovenian, can be included in the additional pretraining with little to no loss
in the performance of the final model.
\\ ( https://arxiv.org/abs/2404.05428 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05446
Date: Mon, 8 Apr 2024 12:29:07 GMT   (7452kb,D)

Title: XL$^2$Bench: A Benchmark for Extremely Long Context Understanding with
  Long-range Dependencies
Authors: Xuanfan Ni, Hengyi Cai, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Piji
  Li
Categories: cs.CL
Comments: Work in progress
\\
  Large Language Models (LLMs) have demonstrated remarkable performance across
diverse tasks but are constrained by their small context window sizes. Various
efforts have been proposed to expand the context window to accommodate even up
to 200K input tokens. Meanwhile, building high-quality benchmarks with much
longer text lengths and more demanding tasks to provide comprehensive
evaluations is of immense practical interest to facilitate long context
understanding research of LLMs. However, prior benchmarks create datasets that
ostensibly cater to long-text comprehension by expanding the input of
traditional tasks, which falls short to exhibit the unique characteristics of
long-text understanding, including long dependency tasks and longer text length
compatible with modern LLMs' context window size. In this paper, we introduce a
benchmark for extremely long context understanding with long-range
dependencies, XL$^2$Bench, which includes three scenarios: Fiction Reading,
Paper Reading, and Law Reading, and four tasks of increasing complexity: Memory
Retrieval, Detailed Understanding, Overall Understanding, and Open-ended
Generation, covering 27 subtasks in English and Chinese. It has an average
length of 100K+ words (English) and 200K+ characters (Chinese). Evaluating six
leading LLMs on XL$^2$Bench, we find that their performance significantly lags
behind human levels. Moreover, the observed decline in performance across both
the original and enhanced datasets underscores the efficacy of our approach to
mitigating data contamination.
\\ ( https://arxiv.org/abs/2404.05446 ,  7452kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05449
Date: Mon, 8 Apr 2024 12:31:23 GMT   (7867kb,D)

Title: RoT: Enhancing Large Language Models with Reflection on Search Trees
Authors: Wenyang Hui, Yan Wang, Kewei Tu, Chengyue Jiang
Categories: cs.CL
Comments: 9 pages main
\\
  Large language models (LLMs) have demonstrated impressive capability in
reasoning and planning when integrated with tree-search-based prompting
methods. However, since these methods ignore the previous search experiences,
they often make the same mistakes in the search process. To address this issue,
we introduce Reflection on search Trees (RoT), an LLM reflection framework
designed to improve the performance of tree-search-based prompting methods. It
uses a strong LLM to summarize guidelines from previous tree search experiences
to enhance the ability of a weak LLM. The guidelines are instructions about
solving this task through tree search which can prevent the weak LLMs from
making similar mistakes in the past search process. In addition, we proposed a
novel state selection method, which identifies the critical information from
historical search processes to help RoT generate more specific and meaningful
guidelines. In our extensive experiments, we find that RoT significantly
improves the performance of LLMs in reasoning or planning tasks with various
tree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based
prompting methods such as Chain-of-Thought (CoT) can also benefit from RoT
guidelines since RoT can provide task-specific knowledge collected from the
search experience.
\\ ( https://arxiv.org/abs/2404.05449 ,  7867kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05483
Date: Mon, 8 Apr 2024 13:05:02 GMT   (397kb,D)

Title: PetKaz at SemEval-2024 Task 8: Can Linguistics Capture the Specifics of
  LLM-generated Text?
Authors: Kseniia Petukhova, Roman Kazakov, Ekaterina Kochmar
Categories: cs.CL cs.AI
Comments: 8 pages, 3 figures, 5 tables, to be published in the Proceedings of
  the 18th International Workshop on Semantic Evaluation (SemEval-2024), for
  associated code, see https://github.com/sachertort/petkaz-semeval-m4
ACM-class: I.2.7
\\
  In this paper, we present our submission to the SemEval-2024 Task 8
"Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text
Detection", focusing on the detection of machine-generated texts (MGTs) in
English. Specifically, our approach relies on combining embeddings from the
RoBERTa-base with diversity features and uses a resampled training set. We
score 12th from 124 in the ranking for Subtask A (monolingual track), and our
results show that our approach is generalizable across unseen models and
domains, achieving an accuracy of 0.91.
\\ ( https://arxiv.org/abs/2404.05483 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05502
Date: Mon, 8 Apr 2024 13:25:03 GMT   (1151kb,D)

Title: PetKaz at SemEval-2024 Task 3: Advancing Emotion Classification with an
  LLM for Emotion-Cause Pair Extraction in Conversations
Authors: Roman Kazakov, Kseniia Petukhova, Ekaterina Kochmar
Categories: cs.CL cs.AI
Comments: 8 pages, 7 figures, 2 tables, to be published in the Proceedings of
  the 18th International Workshop on Semantic Evaluation (SemEval-2024), for
  associated code, see https://github.com/sachertort/petkaz-semeval-ecac
ACM-class: I.2.7
\\
  In this paper, we present our submission to the SemEval-2023 Task~3 "The
Competition of Multimodal Emotion Cause Analysis in Conversations", focusing on
extracting emotion-cause pairs from dialogs. Specifically, our approach relies
on combining fine-tuned GPT-3.5 for emotion classification and a BiLSTM-based
neural network to detect causes. We score 2nd in the ranking for Subtask 1,
demonstrating the effectiveness of our approach through one of the highest
weighted-average proportional F1 scores recorded at 0.264.
\\ ( https://arxiv.org/abs/2404.05502 ,  1151kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05530
Date: Mon, 8 Apr 2024 13:59:02 GMT   (3511kb,D)

Title: Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data
Authors: Tim Baumg\"artner, Yang Gao, Dana Alon, Donald Metzler
Categories: cs.CL cs.AI cs.CR cs.LG
\\
  Reinforcement Learning from Human Feedback (RLHF) is a popular method for
aligning Language Models (LM) with human values and preferences. RLHF requires
a large number of preference pairs as training data, which are often used in
both the Supervised Fine-Tuning and Reward Model training, and therefore
publicly available datasets are commonly used. In this work, we study to what
extent a malicious actor can manipulate the LMs generations by poisoning the
preferences, i.e., injecting poisonous preference pairs into these datasets and
the RLHF training process. We propose strategies to build poisonous preference
pairs and test their performance by poisoning two widely used preference
datasets. Our results show that preference poisoning is highly effective: by
injecting a small amount of poisonous data (1-5% of the original dataset), we
can effectively manipulate the LM to generate a target entity in a target
sentiment (positive or negative). The findings from our experiments also shed
light on strategies to defend against the preference poisoning attack.
\\ ( https://arxiv.org/abs/2404.05530 ,  3511kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05540
Date: Mon, 8 Apr 2024 14:08:56 GMT   (277kb,D)

Title: OPSD: an Offensive Persian Social media Dataset and its baseline
  evaluations
Authors: Mehran Safayani, Amir Sartipi, Amir Hossein Ahmadi, Parniyan Jalali,
  Amir Hossein Mansouri, Mohammad Bisheh-Niasar and Zahra Pourbahman
Categories: cs.CL cs.AI cs.LG
Comments: 16 pages, 5 figures, 8 tables
\\
  The proliferation of hate speech and offensive comments on social media has
become increasingly prevalent due to user activities. Such comments can have
detrimental effects on individuals' psychological well-being and social
behavior. While numerous datasets in the English language exist in this domain,
few equivalent resources are available for Persian language. To address this
gap, this paper introduces two offensive datasets. The first dataset comprises
annotations provided by domain experts, while the second consists of a large
collection of unlabeled data obtained through web crawling for unsupervised
learning purposes. To ensure the quality of the former dataset, a meticulous
three-stage labeling process was conducted, and kappa measures were computed to
assess inter-annotator agreement. Furthermore, experiments were performed on
the dataset using state-of-the-art language models, both with and without
employing masked language modeling techniques, as well as machine learning
algorithms, in order to establish the baselines for the dataset using
contemporary cutting-edge approaches. The obtained F1-scores for the
three-class and two-class versions of the dataset were 76.9% and 89.9% for
XLM-RoBERTa, respectively.
\\ ( https://arxiv.org/abs/2404.05540 ,  277kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05560
Date: Mon, 8 Apr 2024 14:32:52 GMT   (215kb,D)

Title: Chinese Sequence Labeling with Semi-Supervised Boundary-Aware Language
  Model Pre-training
Authors: Longhui Zhang, Dingkun Long, Meishan Zhang, Yanzhao Zhang, Pengjun Xie
  and Min Zhang
Categories: cs.CL
Comments: Accepted to COLING 2024
\\
  Chinese sequence labeling tasks are heavily reliant on accurate word boundary
demarcation. Although current pre-trained language models (PLMs) have achieved
substantial gains on these tasks, they rarely explicitly incorporate boundary
information into the modeling process. An exception to this is BABERT, which
incorporates unsupervised statistical boundary information into Chinese BERT's
pre-training objectives. Building upon this approach, we input supervised
high-quality boundary information to enhance BABERT's learning, developing a
semi-supervised boundary-aware PLM. To assess PLMs' ability to encode
boundaries, we introduce a novel ``Boundary Information Metric'' that is both
simple and effective. This metric allows comparison of different PLMs without
task-specific fine-tuning. Experimental results on Chinese sequence labeling
datasets demonstrate that the improved BABERT variant outperforms the vanilla
version, not only on these tasks but also more broadly across a range of
Chinese natural language understanding tasks. Additionally, our proposed metric
offers a convenient and accurate means of evaluating PLMs' boundary awareness.
\\ ( https://arxiv.org/abs/2404.05560 ,  215kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05587
Date: Mon, 8 Apr 2024 15:00:36 GMT   (118kb)

Title: Enhancing Software Related Information Extraction with Generative
  Language Models through Single-Choice Question Answering
Authors: Wolfgang Otto, Sharmila Upadhyaya, Stefan Dietze
Categories: cs.CL
Comments: Accepted at: 1st Workshop on Natural Scientific Language Processing
  and Research Knowledge Graphs (NSLP 2024) Co-located with Extended Semantic
  Web Conference (ESWC 2024)
ACM-class: I.2.7
\\
  This paper describes our participation in the Shared Task on Software
Mentions Disambiguation (SOMD), with a focus on improving relation extraction
in scholarly texts through Generative Language Models (GLMs) using
single-choice question-answering. The methodology prioritises the use of
in-context learning capabilities of GLMs to extract software-related entities
and their descriptive attributes, such as distributive information. Our
approach uses Retrieval-Augmented Generation (RAG) techniques and GLMs for
Named Entity Recognition (NER) and Attributive NER to identify relationships
between extracted software entities, providing a structured solution for
analysing software citations in academic literature. The paper provides a
detailed description of our approach, demonstrating how using GLMs in a
single-choice QA paradigm can greatly enhance IE methodologies. Our
participation in the SOMD shared task highlights the importance of precise
software citation practices and showcases our system's ability to overcome the
challenges of disambiguating and extracting relationships between software
mentions. This sets the groundwork for future research and development in this
field.
\\ ( https://arxiv.org/abs/2404.05587 ,  118kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05590
Date: Mon, 8 Apr 2024 15:03:57 GMT   (6743kb,D)

Title: MedExpQA: Multilingual Benchmarking of Large Language Models for Medical
  Question Answering
Authors: I\~nigo Alonso, Maite Oronoz, Rodrigo Agerri
Categories: cs.CL
\\
  Large Language Models (LLMs) have the potential of facilitating the
development of Artificial Intelligence technology to assist medical experts for
interactive decision support, which has been demonstrated by their competitive
performances in Medical QA. However, while impressive, the required quality bar
for medical applications remains far from being achieved. Currently, LLMs
remain challenged by outdated knowledge and by their tendency to generate
hallucinated content. Furthermore, most benchmarks to assess medical knowledge
lack reference gold explanations which means that it is not possible to
evaluate the reasoning of LLMs predictions. Finally, the situation is
particularly grim if we consider benchmarking LLMs for languages other than
English which remains, as far as we know, a totally neglected topic. In order
to address these shortcomings, in this paper we present MedExpQA, the first
multilingual benchmark based on medical exams to evaluate LLMs in Medical
Question Answering. To the best of our knowledge, MedExpQA includes for the
first time reference gold explanations written by medical doctors which can be
leveraged to establish various gold-based upper-bounds for comparison with LLMs
performance. Comprehensive multilingual experimentation using both the gold
reference explanations and Retrieval Augmented Generation (RAG) approaches show
that performance of LLMs still has large room for improvement, especially for
languages other than English. Furthermore, and despite using state-of-the-art
RAG methods, our results also demonstrate the difficulty of obtaining and
integrating readily available medical knowledge that may positively impact
results on downstream evaluations for Medical Question Answering. So far the
benchmark is available in four languages, but we hope that this work may
encourage further development to other languages.
\\ ( https://arxiv.org/abs/2404.05590 ,  6743kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05600
Date: Mon, 8 Apr 2024 15:21:17 GMT   (3292kb,D)

Title: SpeechAlign: Aligning Speech Generation to Human Preferences
Authors: Dong Zhang, Zhaowei Li, Shimin Li, Xin Zhang, Pengyu Wang, Yaqian
  Zhou, Xipeng Qiu
Categories: cs.CL cs.SD
Comments: Work in progress
\\
  Speech language models have significantly advanced in generating realistic
speech, with neural codec language models standing out. However, the
integration of human feedback to align speech outputs to human preferences is
often neglected. This paper addresses this gap by first analyzing the
distribution gap in codec language models, highlighting how it leads to
discrepancies between the training and inference phases, which negatively
affects performance. Then we explore leveraging learning from human feedback to
bridge the distribution gap. We introduce SpeechAlign, an iterative
self-improvement strategy that aligns speech language models to human
preferences. SpeechAlign involves constructing a preference codec dataset
contrasting golden codec tokens against synthetic tokens, followed by
preference optimization to improve the codec language model. This cycle of
improvement is carried out iteratively to steadily convert weak models to
strong ones. Through both subjective and objective evaluations, we show that
SpeechAlign can bridge the distribution gap and facilitating continuous
self-improvement of the speech language model. Moreover, SpeechAlign exhibits
robust generalization capabilities and works for smaller models. Code and
models will be available at https://github.com/0nutation/SpeechGPT.
\\ ( https://arxiv.org/abs/2404.05600 ,  3292kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05622
Date: Mon, 8 Apr 2024 15:53:29 GMT   (1049kb,D)

Title: How to Evaluate Entity Resolution Systems: An Entity-Centric Framework
  with Application to Inventor Name Disambiguation
Authors: Olivier Binette, Youngsoo Baek, Siddharth Engineer, Christina Jones,
  Abel Dasylva and Jerome P. Reiter
Categories: cs.CL cs.LG stat.ME
Comments: 33 pages, 11 figures
\\
  Entity resolution (record linkage, microclustering) systems are notoriously
difficult to evaluate. Looking for a needle in a haystack, traditional
evaluation methods use sophisticated, application-specific sampling schemes to
find matching pairs of records among an immense number of non-matches. We
propose an alternative that facilitates the creation of representative,
reusable benchmark data sets without necessitating complex sampling schemes.
These benchmark data sets can then be used for model training and a variety of
evaluation tasks. Specifically, we propose an entity-centric data labeling
methodology that integrates with a unified framework for monitoring summary
statistics, estimating key performance metrics such as cluster and pairwise
precision and recall, and analyzing root causes for errors. We validate the
framework in an application to inventor name disambiguation and through
simulation studies. Software: https://github.com/OlivierBinette/er-evaluation/
\\ ( https://arxiv.org/abs/2404.05622 ,  1049kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05624
Date: Mon, 8 Apr 2024 15:54:02 GMT   (486kb)

Title: LTNER: Large Language Model Tagging for Named Entity Recognition with
  Contextualized Entity Marking
Authors: Faren Yan and Peng Yu and Xin Chen
Categories: cs.CL cs.AI
Comments: 13 pages
\\
  The use of LLMs for natural language processing has become a popular trend in
the past two years, driven by their formidable capacity for context
comprehension and learning, which has inspired a wave of research from
academics and industry professionals. However, for certain NLP tasks, such as
NER, the performance of LLMs still falls short when compared to supervised
learning methods. In our research, we developed a NER processing framework
called LTNER that incorporates a revolutionary Contextualized Entity Marking
Gen Method. By leveraging the cost-effective GPT-3.5 coupled with context
learning that does not require additional training, we significantly improved
the accuracy of LLMs in handling NER tasks. The F1 score on the CoNLL03 dataset
increased from the initial 85.9% to 91.9%, approaching the performance of
supervised fine-tuning. This outcome has led to a deeper understanding of the
potential of LLMs.
\\ ( https://arxiv.org/abs/2404.05624 ,  486kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05632
Date: Mon, 8 Apr 2024 16:04:26 GMT   (7744kb,D)

Title: Fighting crime with Transformers: Empirical analysis of address parsing
  methods in payment data
Authors: Haitham Hammami, Louis Baligand, Bojan Petrovski
Categories: cs.CL
\\
  In the financial industry, identifying the location of parties involved in
payments is a major challenge in the context of various regulatory
requirements. For this purpose address parsing entails extracting fields such
as street, postal code, or country from free text message attributes. While
payment processing platforms are updating their standards with more structured
formats such as SWIFT with ISO 20022, address parsing remains essential for a
considerable volume of messages. With the emergence of Transformers and
Generative Large Language Models (LLM), we explore the performance of
state-of-the-art solutions given the constraint of processing a vast amount of
daily data. This paper also aims to show the need for training robust models
capable of dealing with real-world noisy transactional data. Our results
suggest that a well fine-tuned Transformer model using early-stopping
significantly outperforms other approaches. Nevertheless, generative LLMs
demonstrate strong zero-shot performance and warrant further investigations.
\\ ( https://arxiv.org/abs/2404.05632 ,  7744kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05656
Date: Mon, 8 Apr 2024 16:39:34 GMT   (330kb)

Title: Causality Extraction from Nuclear Licensee Event Reports Using a Hybrid
  Framework
Authors: Sohag Rahman, Sai Zhang, Min Xian, Shoukun Sun, Fei Xu, Zhegang Ma
Categories: cs.CL cs.AI cs.LG
\\
  Industry-wide nuclear power plant operating experience is a critical source
of raw data for performing parameter estimations in reliability and risk
models. Much operating experience information pertains to failure events and is
stored as reports containing unstructured data, such as narratives. Event
reports are essential for understanding how failures are initiated and
propagated, including the numerous causal relations involved. Causal relation
extraction using deep learning represents a significant frontier in the field
of natural language processing (NLP), and is crucial since it enables the
interpretation of intricate narratives and connections contained within vast
amounts of written information. This paper proposed a hybrid framework for
causality detection and extraction from nuclear licensee event reports. The
main contributions include: (1) we compiled an LER corpus with 20,129 text
samples for causality analysis, (2) developed an interactive tool for labeling
cause effect pairs, (3) built a deep-learning-based approach for causal
relation detection, and (4) developed a knowledge based cause-effect extraction
approach.
\\ ( https://arxiv.org/abs/2404.05656 ,  330kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05659
Date: Mon, 8 Apr 2024 16:43:52 GMT   (353kb)

Title: VietMed: A Dataset and Benchmark for Automatic Speech Recognition of
  Vietnamese in the Medical Domain
Authors: Khai Le-Duc
Categories: cs.CL cs.AI eess.AS
Comments: LREC-COLING 2024
\\
  Due to privacy restrictions, there's a shortage of publicly available speech
recognition datasets in the medical domain. In this work, we present VietMed -
a Vietnamese speech recognition dataset in the medical domain comprising 16h of
labeled medical speech, 1000h of unlabeled medical speech and 1200h of
unlabeled general-domain speech. To our best knowledge, VietMed is by far the
world's largest public medical speech recognition dataset in 7 aspects: total
duration, number of speakers, diseases, recording conditions, speaker roles,
unique medical terms and accents. VietMed is also by far the largest public
Vietnamese speech dataset in terms of total duration. Additionally, we are the
first to present a medical ASR dataset covering all ICD-10 disease groups and
all accents within a country. Moreover, we release the first public large-scale
pre-trained models for Vietnamese ASR, w2v2-Viet and XLSR-53-Viet, along with
the first public large-scale fine-tuned models for medical ASR. Even without
any medical data in unsupervised pre-training, our best pre-trained model
XLSR-53-Viet generalizes very well to the medical domain by outperforming
state-of-the-art XLSR-53, from 51.8% to 29.6% WER on test set (a relative
reduction of more than 40%). All code, data and models are made publicly
available here: https://github.com/leduckhai/MultiMed.
\\ ( https://arxiv.org/abs/2404.05659 ,  353kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05692
Date: Mon, 8 Apr 2024 17:18:04 GMT   (325kb,D)

Title: Evaluating Mathematical Reasoning Beyond Accuracy
Authors: Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, Pengfei Liu
Categories: cs.CL
\\
  The leaderboard of Large Language Models (LLMs) in mathematical tasks has
been continuously updated. However, the majority of evaluations focus solely on
the final results, neglecting the quality of the intermediate steps. This
oversight can mask underlying problems, such as logical errors or unnecessary
steps in the reasoning process. To measure reasoning beyond final-answer
accuracy, we introduce ReasonEval, a new methodology for evaluating the quality
of reasoning steps. ReasonEval employs $\textit{validity}$ and
$\textit{redundancy}$ to characterize the reasoning quality, as well as
accompanying LLMs to assess them automatically. Instantiated by base models
that possess strong mathematical knowledge and trained with high-quality
labeled data, ReasonEval achieves state-of-the-art performance on human-labeled
datasets and can accurately detect different types of errors generated by
perturbation. When applied to evaluate LLMs specialized in math, we find that
an increase in final-answer accuracy does not necessarily guarantee an
improvement in the overall quality of the reasoning steps for challenging
mathematical problems. Additionally, we observe that ReasonEval can play a
significant role in data selection. We release the best-performing model,
meta-evaluation script, and all evaluation results at
https://github.com/GAIR-NLP/ReasonEval.
\\ ( https://arxiv.org/abs/2404.05692 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05694
Date: Mon, 8 Apr 2024 17:24:04 GMT   (173kb,D)

Title: Comprehensive Study on German Language Models for Clinical and
  Biomedical Text Understanding
Authors: Ahmad Idrissi-Yaghir, Amin Dada, Henning Sch\"afer, Kamyar Arzideh,
  Giulia Baldini, Jan Trienes, Max Hasin, Jeanette Bewersdorff, Cynthia S.
  Schmidt, Marie Bauer, Kaleb E. Smith, Jiang Bian, Yonghui Wu, J\"org
  Schl\"otterer, Torsten Zesch, Peter A. Horn, Christin Seifert, Felix Nensa,
  Jens Kleesiek, Christoph M. Friedrich
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at LREC-COLING 2024
\\
  Recent advances in natural language processing (NLP) can be largely
attributed to the advent of pre-trained language models such as BERT and
RoBERTa. While these models demonstrate remarkable performance on general
datasets, they can struggle in specialized domains such as medicine, where
unique domain-specific terminologies, domain-specific abbreviations, and
varying document structures are common. This paper explores strategies for
adapting these models to domain-specific requirements, primarily through
continuous pre-training on domain-specific data. We pre-trained several German
medical language models on 2.4B tokens derived from translated public English
medical data and 3B tokens of German clinical data. The resulting models were
evaluated on various German downstream tasks, including named entity
recognition (NER), multi-label classification, and extractive question
answering. Our results suggest that models augmented by clinical and
translation-based pre-training typically outperform general domain models in
medical contexts. We conclude that continuous pre-training has demonstrated the
ability to match or even exceed the performance of clinical models trained from
scratch. Furthermore, pre-training on clinical data or leveraging translated
texts have proven to be reliable methods for domain adaptation in medical NLP
tasks.
\\ ( https://arxiv.org/abs/2404.05694 ,  173kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05720
Date: Mon, 8 Apr 2024 17:56:43 GMT   (8094kb,D)

Title: Language-Independent Representations Improve Zero-Shot Summarization
Authors: Vladimir Solovyev, Danni Liu, Jan Niehues
Categories: cs.CL cs.AI
Comments: NAACL 2024
\\
  Finetuning pretrained models on downstream generation tasks often leads to
catastrophic forgetting in zero-shot conditions. In this work, we focus on
summarization and tackle the problem through the lens of language-independent
representations. After training on monolingual summarization, we perform
zero-shot transfer to new languages or language pairs. We first show naively
finetuned models are highly language-specific in both output behavior and
internal representations, resulting in poor zero-shot performance. Next, we
propose query-key (QK) finetuning to decouple task-specific knowledge from the
pretrained language generation abilities. Then, after showing downsides of the
standard adversarial language classifier, we propose a balanced variant that
more directly enforces language-agnostic representations. Moreover, our
qualitative analyses show removing source language identity correlates to
zero-shot summarization performance. Our code is openly available.
\\ ( https://arxiv.org/abs/2404.05720 ,  8094kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04291
Date: Thu, 4 Apr 2024 05:38:44 GMT   (1215kb,D)

Title: Investigating Regularization of Self-Play Language Models
Authors: Reda Alami, Abdalgader Abubaker, Mastane Achab, Mohamed El Amine
  Seddik, Salem Lahlou
Categories: cs.LG
\\
  This paper explores the effects of various forms of regularization in the
context of language model alignment via self-play. While both reinforcement
learning from human feedback (RLHF) and direct preference optimization (DPO)
require to collect costly human-annotated pairwise preferences, the self-play
fine-tuning (SPIN) approach replaces the rejected answers by data generated
from the previous iterate. However, the SPIN method presents a performance
instability issue in the learning phase, which can be mitigated by playing
against a mixture of the two previous iterates. In the same vein, we propose in
this work to address this issue from two perspectives: first, by incorporating
an additional Kullback-Leibler (KL) regularization to stay at the proximity of
the reference policy; second, by using the idea of fictitious play which
smoothens the opponent policy across all previous iterations. In particular, we
show that the KL-based regularizer boils down to replacing the previous policy
by its geometric mixture with the base policy inside of the SPIN loss function.
We finally discuss empirical results on MT-Bench as well as on the Hugging Face
Open LLM Leaderboard.
\\ ( https://arxiv.org/abs/2404.04291 ,  1215kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04311
Date: Fri, 5 Apr 2024 11:03:36 GMT   (808kb)

Title: A Real-time Anomaly Detection Using Convolutional Autoencoder with
  Dynamic Threshold
Authors: Sarit Maitra, Sukanya Kundu, Aishwarya Shankar
Categories: cs.LG cs.AI
\\
  The majority of modern consumer-level energy is generated by real-time smart
metering systems. These frequently contain anomalies, which prevent reliable
estimates of the series' evolution. This work introduces a hybrid modeling
approach combining statistics and a Convolutional Autoencoder with a dynamic
threshold. The threshold is determined based on Mahalanobis distance and moving
averages. It has been tested using real-life energy consumption data collected
from smart metering systems. The solution includes a real-time, meter-level
anomaly detection system that connects to an advanced monitoring system. This
makes a substantial contribution by detecting unusual data movements and
delivering an early warning. Early detection and subsequent troubleshooting can
financially benefit organizations and consumers and prevent disasters from
occurring.
\\ ( https://arxiv.org/abs/2404.04311 ,  808kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04312
Date: Fri, 5 Apr 2024 12:03:19 GMT   (10325kb,D)

Title: Half-Space Feature Learning in Neural Networks
Authors: Mahesh Lorik Yadav, Harish Guruprasad Ramaswamy, Chandrashekar
  Lakshminarayanan
Categories: cs.LG cs.AI cs.NE
\\
  There currently exist two extreme viewpoints for neural network feature
learning -- (i) Neural networks simply implement a kernel method (a la NTK) and
hence no features are learned (ii) Neural networks can represent (and hence
learn) intricate hierarchical features suitable for the data. We argue in this
paper neither interpretation is likely to be correct based on a novel
viewpoint. Neural networks can be viewed as a mixture of experts, where each
expert corresponds to a (number of layers length) path through a sequence of
hidden units. We use this alternate interpretation to motivate a model, called
the Deep Linearly Gated Network (DLGN), which sits midway between deep linear
networks and ReLU networks. Unlike deep linear networks, the DLGN is capable of
learning non-linear features (which are then linearly combined), and unlike
ReLU networks these features are ultimately simple -- each feature is
effectively an indicator function for a region compactly described as an
intersection of (number of layers) half-spaces in the input space. This
viewpoint allows for a comprehensive global visualization of features, unlike
the local visualizations for neurons based on saliency/activation/gradient
maps. Feature learning in DLGNs is shown to happen and the mechanism with which
this happens is through learning half-spaces in the input space that contain
smooth regions of the target function. Due to the structure of DLGNs, the
neurons in later layers are fundamentally the same as those in earlier layers
-- they all represent a half-space -- however, the dynamics of gradient descent
impart a distinct clustering to the later layer neurons. We hypothesize that
ReLU networks also have similar feature learning behaviour.
\\ ( https://arxiv.org/abs/2404.04312 ,  10325kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04314
Date: Fri, 5 Apr 2024 13:18:10 GMT   (32023kb,D)

Title: Faraday: Synthetic Smart Meter Generator for the smart grid
Authors: Sheng Chai and Gus Chadney
Categories: cs.LG
Comments: Published as a workshop paper at Tackling Climate Change with Machine
  Learning, ICLR 2024
\\
  Access to smart meter data is essential to rapid and successful transitions
to electrified grids, underpinned by flexibility delivered by low carbon
technologies, such as electric vehicles (EV) and heat pumps, and powered by
renewable energy. Yet little of this data is available for research and
modelling purposes due consumer privacy protections. Whilst many are calling
for raw datasets to be unlocked through regulatory changes, we believe this
approach will take too long. Synthetic data addresses these challenges directly
by overcoming privacy issues. In this paper, we present Faraday, a Variational
Auto-encoder (VAE)-based model trained over 300 million smart meter data
readings from an energy supplier in the UK, with information such as property
type and low carbon technologies (LCTs) ownership. The model produces
household-level synthetic load profiles conditioned on these labels, and we
compare its outputs against actual substation readings to show how the model
can be used for real-world applications by grid modellers interested in
modelling energy grids of the future.
\\ ( https://arxiv.org/abs/2404.04314 ,  32023kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04316
Date: Fri, 5 Apr 2024 15:28:44 GMT   (993kb,D)

Title: Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation
Authors: Xinyu Ma, Xu Chu, Zhibang Yang, Yang Lin, Xin Gao, Junfeng Zhao
Categories: cs.LG cs.AI cs.CL
\\
  With the increasingly powerful performances and enormous scales of Pretrained
Language Models (PLMs), promoting parameter efficiency in fine-tuning has
become a crucial need for effective and efficient adaptation to various
downstream tasks. One representative line of fine-tuning methods is Orthogonal
Fine-tuning (OFT), which rigorously preserves the angular distances within the
parameter space to preserve the pretrained knowledge. Despite the empirical
effectiveness, OFT still suffers low parameter efficiency at $\mathcal{O}(d^2)$
and limited capability of downstream adaptation. Inspired by Givens rotation,
in this paper, we proposed quasi-Givens Orthogonal Fine-Tuning (qGOFT) to
address the problems. We first use $\mathcal{O}(d)$ Givens rotations to
accomplish arbitrary orthogonal transformation in $SO(d)$ with provable
equivalence, reducing parameter complexity from $\mathcal{O}(d^2)$ to
$\mathcal{O}(d)$. Then we introduce flexible norm and relative angular
adjustments under soft orthogonality regularization to enhance the adaptation
capability of downstream semantic deviations. Extensive experiments on various
tasks and PLMs validate the effectiveness of our methods.
\\ ( https://arxiv.org/abs/2404.04316 ,  993kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04356
Date: Fri, 5 Apr 2024 18:56:00 GMT   (3307kb,D)

Title: Pixel-wise RL on Diffusion Models: Reinforcement Learning from Rich
  Feedback
Authors: Mo Kordzanganeh, Danial Keshvary, Nariman Arian
Categories: cs.LG cs.CV
Comments: 6 pages, 7 figures
\\
  Latent diffusion models are the state-of-the-art for synthetic image
generation. To align these models with human preferences, training the models
using reinforcement learning on human feedback is crucial. Black et. al 2024
introduced denoising diffusion policy optimisation (DDPO), which accounts for
the iterative denoising nature of the generation by modelling it as a Markov
chain with a final reward. As the reward is a single value that determines the
model's performance on the entire image, the model has to navigate a very
sparse reward landscape and so requires a large sample count. In this work, we
extend the DDPO by presenting the Pixel-wise Policy Optimisation (PXPO)
algorithm, which can take feedback for each pixel, providing a more nuanced
reward to the model.
\\ ( https://arxiv.org/abs/2404.04356 ,  3307kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04360
Date: Fri, 5 Apr 2024 19:14:14 GMT   (287kb,D)

Title: Prompt Public Large Language Models to Synthesize Data for Private
  On-device Applications
Authors: Shanshan Wu, Zheng Xu, Yanxiang Zhang, Yuanbo Zhang, Daniel Ramage
Categories: cs.LG cs.CL cs.CR
\\
  Pre-training on public data is an effective method to improve the performance
for federated learning (FL) with differential privacy (DP). This paper
investigates how large language models (LLMs) trained on public data can
improve the quality of pre-training data for the on-device language models
trained with DP and FL. We carefully design LLM prompts to filter and transform
existing public data, and generate new data to resemble the real user data
distribution. The model pre-trained on our synthetic dataset achieves relative
improvement of 19.0% and 22.8% in next word prediction accuracy compared to the
baseline model pre-trained on a standard public dataset, when evaluated over
the real user data in Gboard (Google Keyboard, a production mobile keyboard
application). Furthermore, our method achieves evaluation accuracy better than
or comparable to the baseline during the DP FL fine-tuning over millions of
mobile devices, and our final model outperforms the baseline in production A/B
testing. Our experiments demonstrate the strengths of LLMs in synthesizing data
close to the private distribution even without accessing the private data, and
also suggest future research directions to further reduce the distribution gap.
\\ ( https://arxiv.org/abs/2404.04360 ,  287kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04375
Date: Fri, 5 Apr 2024 19:36:26 GMT   (143kb)

Title: Compositional Estimation of Lipschitz Constants for Deep Neural Networks
Authors: Yuezhu Xu, S. Sivaranjani
Categories: cs.LG cs.SY eess.SY
\\
  The Lipschitz constant plays a crucial role in certifying the robustness of
neural networks to input perturbations and adversarial attacks, as well as the
stability and safety of systems with neural network controllers. Therefore,
estimation of tight bounds on the Lipschitz constant of neural networks is a
well-studied topic. However, typical approaches involve solving a large matrix
verification problem, the computational cost of which grows significantly for
deeper networks. In this letter, we provide a compositional approach to
estimate Lipschitz constants for deep feedforward neural networks by obtaining
an exact decomposition of the large matrix verification problem into smaller
sub-problems. We further obtain a closed-form solution that applies to most
common neural network activation functions, which will enable rapid robustness
and stability certificates for neural networks deployed in online control
settings. Finally, we demonstrate through numerical experiments that our
approach provides a steep reduction in computation time while yielding
Lipschitz bounds that are very close to those achieved by state-of-the-art
approaches.
\\ ( https://arxiv.org/abs/2404.04375 ,  143kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04397
Date: Fri, 5 Apr 2024 20:50:06 GMT   (2251kb,D)

Title: Generating Synthetic Ground Truth Distributions for Multi-step
  Trajectory Prediction using Probabilistic Composite B\'ezier Curves
Authors: Ronny Hug, Stefan Becker, Wolfgang H\"ubner, Michael Arens
Categories: cs.LG
\\
  An appropriate data basis grants one of the most important aspects for
training and evaluating probabilistic trajectory prediction models based on
neural networks. In this regard, a common shortcoming of current benchmark
datasets is their limitation to sets of sample trajectories and a lack of
actual ground truth distributions, which prevents the use of more expressive
error metrics, such as the Wasserstein distance for model evaluation. Towards
this end, this paper proposes a novel approach to synthetic dataset generation
based on composite probabilistic B\'ezier curves, which is capable of
generating ground truth data in terms of probability distributions over full
trajectories. This allows the calculation of arbitrary posterior distributions.
The paper showcases an exemplary trajectory prediction model evaluation using
generated ground truth distribution data.
\\ ( https://arxiv.org/abs/2404.04397 ,  2251kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04405
Date: Fri, 5 Apr 2024 21:03:11 GMT   (8259kb,D)

Title: Dynamic Switch Layers For Unsupervised Learning
Authors: Haiguang Li, Usama Pervaiz, Micha{\l} Matuszak, Robert Kamara, Gilles
  Roux, Trausti Thormundsson, Joseph Antognini
Categories: cs.LG
Comments: Initial Submission
\\
  On-device machine learning (ODML) enables intelligent applications on
resource-constrained devices. However, power consumption poses a major
challenge, forcing a trade-off between model accuracy and power efficiency that
often limits model complexity. The previously established Gated Compression
(GC) layers offer a solution, enabling power efficiency without sacrificing
model performance by selectively gating samples that lack signals of interest.
However, their reliance on ground truth labels limits GC layers to supervised
tasks. This work introduces the Dynamic Switch Layer (DSL), extending the
benefits of GC layers to unsupervised learning scenarios, and maintaining power
efficiency without the need for labeled data. The DSL builds upon the GC
architecture, leveraging a dynamic pathway selection, and adapting model
complexity in response to the innate structure of the data. We integrate the
DSL into the SoundStream architecture and demonstrate that by routing up to 80%
of samples through a lightweight pass we achieve a 12.3x reduction in the
amount of computation performed and a 20.9x reduction in model size. This
reduces the on-device inference latency by up to 26.5% and improves power
efficiency by up to 21.4% without impacting model performance.
\\ ( https://arxiv.org/abs/2404.04405 ,  8259kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04454
Date: Fri, 5 Apr 2024 23:56:50 GMT   (589kb,D)

Title: Implicit Bias of AdamW: $\ell_\infty$ Norm Constrained Optimization
Authors: Shuo Xie, Zhiyuan Li
Categories: cs.LG math.OC stat.ML
\\
  Adam with decoupled weight decay, also known as AdamW, is widely acclaimed
for its superior performance in language modeling tasks, surpassing Adam with
$\ell_2$ regularization in terms of generalization and optimization. However,
this advantage is not theoretically well-understood. One challenge here is that
though intuitively Adam with $\ell_2$ regularization optimizes the $\ell_2$
regularized loss, it is not clear if AdamW optimizes a specific objective. In
this work, we make progress toward understanding the benefit of AdamW by
showing that it implicitly performs constrained optimization. More concretely,
we show in the full-batch setting, if AdamW converges with any non-increasing
learning rate schedule whose partial sum diverges, it must converge to a KKT
point of the original loss under the constraint that the $\ell_\infty$ norm of
the parameter is bounded by the inverse of the weight decay factor. This result
is built on the observation that Adam can be viewed as a smoothed version of
SignGD, which is the normalized steepest descent with respect to $\ell_\infty$
norm, and a surprising connection between normalized steepest descent with
weight decay and Frank-Wolfe.
\\ ( https://arxiv.org/abs/2404.04454 ,  589kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04475
Date: Sat, 6 Apr 2024 02:29:02 GMT   (3264kb,D)

Title: Length-Controlled AlpacaEval: A Simple Way to Debias Automatic
  Evaluators
Authors: Yann Dubois and Bal\'azs Galambosi and Percy Liang and Tatsunori B.
  Hashimoto
Categories: cs.LG cs.AI cs.CL stat.ML
\\
  LLM-based auto-annotators have become a key component of the LLM development
process due to their cost-effectiveness and scalability compared to human-based
evaluation. However, these auto-annotators can introduce complex biases that
are hard to remove. Even simple, known confounders such as preference for
longer outputs remain in existing automated evaluation metrics. We propose a
simple regression analysis approach for controlling biases in auto-evaluations.
As a real case study, we focus on reducing the length bias of AlpacaEval, a
fast and affordable benchmark for chat LLMs that uses LLMs to estimate response
quality. Despite being highly correlated with human preferences, AlpacaEval is
known to favor models that generate longer outputs. We introduce a
length-controlled AlpacaEval that aims to answer the counterfactual question:
"What would the preference be if the model's and baseline's output had the same
length?". To achieve this, we first fit a generalized linear model to predict
the biased output of interest (auto-annotator preferences) based on the
mediators we want to control for (length difference) and other relevant
features. We then obtain length-controlled preferences by predicting
preferences while conditioning the GLM with a zero difference in lengths.
Length-controlling not only improves the robustness of the metric to
manipulations in model verbosity, we also find that it increases the Spearman
correlation with LMSYS' Chatbot Arena from 0.94 to 0.98. We release the code
and leaderboard at https://tatsu-lab.github.io/alpaca_eval/ .
\\ ( https://arxiv.org/abs/2404.04475 ,  3264kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04476
Date: Sat, 6 Apr 2024 02:33:04 GMT   (2521kb,D)

Title: DELTA: Decoupling Long-Tailed Online Continual Learning
Authors: Siddeshwar Raghavan, Jiangpeng He, Fengqing Zhu
Categories: cs.LG cs.CV
Comments: CVPR Workshop acceptance archival track
\\
  A significant challenge in achieving ubiquitous Artificial Intelligence is
the limited ability of models to rapidly learn new information in real-world
scenarios where data follows long-tailed distributions, all while avoiding
forgetting previously acquired knowledge. In this work, we study the
under-explored problem of Long-Tailed Online Continual Learning (LTOCL), which
aims to learn new tasks from sequentially arriving class-imbalanced data
streams. Each data is observed only once for training without knowing the task
data distribution. We present DELTA, a decoupled learning approach designed to
enhance learning representations and address the substantial imbalance in
LTOCL. We enhance the learning process by adapting supervised contrastive
learning to attract similar samples and repel dissimilar (out-of-class)
samples. Further, by balancing gradients during training using an equalization
loss, DELTA significantly enhances learning outcomes and successfully mitigates
catastrophic forgetting. Through extensive evaluation, we demonstrate that
DELTA improves the capacity for incremental learning, surpassing existing OCL
methods. Our results suggest considerable promise for applying OCL in
real-world applications.
\\ ( https://arxiv.org/abs/2404.04476 ,  2521kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04490
Date: Sat, 6 Apr 2024 03:46:42 GMT   (1111kb,D)

Title: Hyperparameter Optimization for SecureBoost via Constrained
  Multi-Objective Federated Learning
Authors: Yan Kang, Ziyao Ren, Lixin Fan, Linghua Yang, Yongxin Tong, and Qiang
  Yang
Categories: cs.LG
\\
  SecureBoost is a tree-boosting algorithm that leverages homomorphic
encryption (HE) to protect data privacy in vertical federated learning.
SecureBoost and its variants have been widely adopted in fields such as finance
and healthcare. However, the hyperparameters of SecureBoost are typically
configured heuristically for optimizing model performance (i.e., utility)
solely, assuming that privacy is secured. Our study found that SecureBoost and
some of its variants are still vulnerable to label leakage. This vulnerability
may lead the current heuristic hyperparameter configuration of SecureBoost to a
suboptimal trade-off between utility, privacy, and efficiency, which are
pivotal elements toward a trustworthy federated learning system. To address
this issue, we propose the Constrained Multi-Objective SecureBoost (CMOSB)
algorithm, which aims to approximate Pareto optimal solutions that each
solution is a set of hyperparameters achieving an optimal trade-off between
utility loss, training cost, and privacy leakage. We design measurements of the
three objectives, including a novel label inference attack named instance
clustering attack (ICA) to measure the privacy leakage of SecureBoost.
Additionally, we provide two countermeasures against ICA. The experimental
results demonstrate that the CMOSB yields superior hyperparameters over those
optimized by grid search and Bayesian optimization regarding the trade-off
between utility loss, training cost, and privacy leakage.
\\ ( https://arxiv.org/abs/2404.04490 ,  1111kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04509
Date: Sat, 6 Apr 2024 05:34:12 GMT   (2374kb,D)

Title: Distributed No-Regret Learning for Multi-Stage Systems with End-to-End
  Bandit Feedback
Authors: I-Hong Hou
Categories: cs.LG cs.NI
\\
  This paper studies multi-stage systems with end-to-end bandit feedback. In
such systems, each job needs to go through multiple stages, each managed by a
different agent, before generating an outcome. Each agent can only control its
own action and learn the final outcome of the job. It has neither knowledge nor
control on actions taken by agents in the next stage. The goal of this paper is
to develop distributed online learning algorithms that achieve sublinear regret
in adversarial environments.
  The setting of this paper significantly expands the traditional multi-armed
bandit problem, which considers only one agent and one stage. In addition to
the exploration-exploitation dilemma in the traditional multi-armed bandit
problem, we show that the consideration of multiple stages introduces a third
component, education, where an agent needs to choose its actions to facilitate
the learning of agents in the next stage. To solve this newly introduced
exploration-exploitation-education trilemma, we propose a simple distributed
online learning algorithm, $\epsilon-$EXP3. We theoretically prove that the
$\epsilon-$EXP3 algorithm is a no-regret policy that achieves sublinear regret.
Simulation results show that the $\epsilon-$EXP3 algorithm significantly
outperforms existing no-regret online learning algorithms for the traditional
multi-armed bandit problem.
\\ ( https://arxiv.org/abs/2404.04509 ,  2374kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04534
Date: Sat, 6 Apr 2024 07:21:41 GMT   (28kb)

Title: Impact of Fairness Regulations on Institutions' Policies and Population
  Qualifications
Authors: Hamidreza Montaseri, Amin Gohari
Categories: cs.LG cs.AI cs.CY
\\
  The proliferation of algorithmic systems has fueled discussions surrounding
the regulation and control of their social impact. Herein, we consider a system
whose primary objective is to maximize utility by selecting the most qualified
individuals. To promote demographic parity in the selection algorithm, we
consider penalizing discrimination across social groups. We examine conditions
under which a discrimination penalty can effectively reduce disparity in the
selection. Additionally, we explore the implications of such a penalty when
individual qualifications may evolve over time in response to the imposed
penalizing policy. We identify scenarios where the penalty could hinder the
natural attainment of equity within the population. Moreover, we propose
certain conditions that can counteract this undesirable outcome, thus ensuring
fairness.
\\ ( https://arxiv.org/abs/2404.04534 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04559
Date: Sat, 6 Apr 2024 08:53:26 GMT   (90kb)

Title: Spectral GNN via Two-dimensional (2-D) Graph Convolution
Authors: Guoming Li, Jian Yang, Shangsong Liang, Dongsheng Luo
Categories: cs.LG cs.NA eess.SP math.NA
Comments: Preprint
\\
  Spectral Graph Neural Networks (GNNs) have achieved tremendous success in
graph learning. As an essential part of spectral GNNs, spectral graph
convolution extracts crucial frequency information in graph data, leading to
superior performance of spectral GNNs in downstream tasks. However, in this
paper, we show that existing spectral GNNs remain critical drawbacks in
performing the spectral graph convolution. Specifically, considering the
spectral graph convolution as a construction operation towards target output,
we prove that existing popular convolution paradigms cannot construct the
target output with mild conditions on input graph signals, causing spectral
GNNs to fall into suboptimal solutions. To address the issues, we rethink the
spectral graph convolution from a more general two-dimensional (2-D) signal
convolution perspective and propose a new convolution paradigm, named 2-D graph
convolution. We prove that 2-D graph convolution unifies existing graph
convolution paradigms, and is capable to construct arbitrary target output.
Based on the proposed 2-D graph convolution, we further propose ChebNet2D, an
efficient and effective GNN implementation of 2-D graph convolution through
applying Chebyshev interpolation. Extensive experiments on benchmark datasets
demonstrate both effectiveness and efficiency of the ChebNet2D.
\\ ( https://arxiv.org/abs/2404.04559 ,  90kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04575
Date: Sat, 6 Apr 2024 09:55:03 GMT   (1158kb,D)

Title: To Cool or not to Cool? Temperature Network Meets Large Foundation
  Models via DRO
Authors: Zi-Hao Qiu, Siqi Guo, Mao Xu, Tuo Zhao, Lijun Zhang, Tianbao Yang
Categories: cs.LG cs.AI math.OC
Comments: 41 pages, 10 figures
\\
  The temperature parameter plays a profound role during training and/or
inference with large foundation models (LFMs) such as large language models
(LLMs) and CLIP models. Particularly, it adjusts the logits in the softmax
function in LLMs, which is crucial for next token generation, and it scales the
similarities in the contrastive loss for training CLIP models. A significant
question remains: Is it viable to learn a neural network to predict a
personalized temperature of any input data for enhancing LFMs"? In this paper,
we present a principled framework for learning a small yet generalizable
temperature prediction network (TempNet) to improve LFMs. Our solution is
composed of a novel learning framework with a robust loss underpinned by
constrained distributionally robust optimization (DRO), and a properly designed
TempNet with theoretical inspiration. TempNet can be trained together with a
large foundation model from scratch or learned separately given a pretrained
foundation model. It is not only useful for predicting personalized temperature
to promote the training of LFMs but also generalizable and transferable to new
tasks. Our experiments on LLMs and CLIP models demonstrate that TempNet greatly
improves the performance of existing solutions or models, e.g. Table 1. The
code to reproduce the experimental results in this paper can be found at
https://github.com/zhqiu/TempNet.
\\ ( https://arxiv.org/abs/2404.04575 ,  1158kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04612
Date: Sat, 6 Apr 2024 12:40:21 GMT   (2023kb,D)

Title: Spectral Graph Pruning Against Over-Squashing and Over-Smoothing
Authors: Adarsh Jamadandi, Celia Rubio-Madrigal, Rebekka Burkholz
Categories: cs.LG stat.ML
\\
  Message Passing Graph Neural Networks are known to suffer from two problems
that are sometimes believed to be diametrically opposed: over-squashing and
over-smoothing. The former results from topological bottlenecks that hamper the
information flow from distant nodes and are mitigated by spectral gap
maximization, primarily, by means of edge additions. However, such additions
often promote over-smoothing that renders nodes of different classes less
distinguishable. Inspired by the Braess phenomenon, we argue that deleting
edges can address over-squashing and over-smoothing simultaneously. This
insight explains how edge deletions can improve generalization, thus connecting
spectral gap optimization to a seemingly disconnected objective of reducing
computational resources by pruning graphs for lottery tickets. To this end, we
propose a more effective spectral gap optimization framework to add or delete
edges and demonstrate its effectiveness on large heterophilic datasets.
\\ ( https://arxiv.org/abs/2404.04612 ,  2023kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04616
Date: Sat, 6 Apr 2024 12:49:20 GMT   (887kb,D)

Title: Vanishing Variance Problem in Fully Decentralized Neural-Network Systems
Authors: Yongding Tian, Zaid Al-Ars, Maksim Kitsak, Peter Hofstee
Categories: cs.LG
Comments: 7 pages
\\
  Federated learning and gossip learning are emerging methodologies designed to
mitigate data privacy concerns by retaining training data on client devices and
exclusively sharing locally-trained machine learning (ML) models with others.
The primary distinction between the two lies in their approach to model
aggregation: federated learning employs a centralized parameter server, whereas
gossip learning adopts a fully decentralized mechanism, enabling direct model
exchanges among nodes. This decentralized nature often positions gossip
learning as less efficient compared to federated learning. Both methodologies
involve a critical step: computing a representation of received ML models and
integrating this representation into the existing model. Conventionally, this
representation is derived by averaging the received models, exemplified by the
FedAVG algorithm. Our findings suggest that this averaging approach inherently
introduces a potential delay in model convergence. We identify the underlying
cause and refer to it as the "vanishing variance" problem, where averaging
across uncorrelated ML models undermines the optimal variance established by
the Xavier weight initialization. Unlike federated learning where the central
server ensures model correlation, and unlike traditional gossip learning which
circumvents this problem through model partitioning and sampling, our research
introduces a variance-corrected model averaging algorithm. This novel algorithm
preserves the optimal variance needed during model averaging, irrespective of
network topology or non-IID data distributions. Our extensive simulation
results demonstrate that our approach enables gossip learning to achieve
convergence efficiency comparable to that of federated learning.
\\ ( https://arxiv.org/abs/2404.04616 ,  887kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04623
Date: Sat, 6 Apr 2024 13:13:45 GMT   (3223kb,D)

Title: An Automated Machine Learning Approach to Inkjet Printed Component
  Analysis: A Step Toward Smart Additive Manufacturing
Authors: Abhishek Sahu, Peter H. Aaen and Praveen Damacharla
Categories: cs.LG cs.ET
Comments: 2024 IEEE Texas Symposium on Wireless & Micrwowave Circuits and
  Systems
\\
  In this paper, we present a machine learning based architecture for microwave
characterization of inkjet printed components on flexible substrates. Our
proposed architecture uses several machine learning algorithms and
automatically selects the best algorithm to extract the material parameters
(ink conductivity and dielectric properties) from on-wafer measurements.
Initially, the mutual dependence between material parameters of the inkjet
printed coplanar waveguides (CPWs) and EM-simulated propagation constants is
utilized to train the machine learning models. Next, these machine learning
models along with measured propagation constants are used to extract the ink
conductivity and dielectric properties of the test prototypes. To demonstrate
the applicability of our proposed approach, we compare and contrast four
heuristic based machine learning models. It is shown that eXtreme Gradient
Boosted Trees Regressor (XGB) and Light Gradient Boosting (LGB) algorithms
perform best for the characterization problem under study.
\\ ( https://arxiv.org/abs/2404.04623 ,  3223kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04656
Date: Sat, 6 Apr 2024 15:20:59 GMT   (1466kb,D)

Title: Binary Classifier Optimization for Large Language Model Alignment
Authors: Seungjae Jung, Gunsoo Han, Daniel Wontae Nam and Kyoung-Woon On
Categories: cs.LG cs.AI cs.CL
Comments: 18 pages, 9 figures
\\
  Aligning Large Language Models (LLMs) to human preferences through preference
optimization has been crucial but labor-intensive, necessitating for each
prompt a comparison of both a chosen and a rejected text completion by
evaluators. Recently, Kahneman-Tversky Optimization (KTO) has demonstrated that
LLMs can be aligned using merely binary "thumbs-up" or "thumbs-down" signals on
each prompt-completion pair. In this paper, we present theoretical foundations
to explain the successful alignment achieved through these binary signals. Our
analysis uncovers a new perspective: optimizing a binary classifier, whose
logit is a reward, implicitly induces minimizing the Direct Preference
Optimization (DPO) loss. In the process of this discovery, we identified two
techniques for effective alignment: reward shift and underlying distribution
matching. Consequently, we propose a new algorithm, \textit{Binary Classifier
Optimization}, that integrates the techniques. We validate our methodology in
two settings: first, on a paired preference dataset, where our method performs
on par with DPO and KTO; and second, on binary signal datasets simulating
real-world conditions with divergent underlying distributions between thumbs-up
and thumbs-down data. Our model consistently demonstrates effective and robust
alignment across two base LLMs and three different binary signal datasets,
showcasing the strength of our approach to learning from binary feedback.
\\ ( https://arxiv.org/abs/2404.04656 ,  1466kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04661
Date: Sat, 6 Apr 2024 15:31:17 GMT   (446kb,D)

Title: Transform then Explore: a Simple and Effective Technique for Exploratory
  Combinatorial Optimization with Reinforcement Learning
Authors: Tianle Pu, Changjun Fan, Mutian Shen, Yizhou Lu, Li Zeng, Zohar
  Nussinov, Chao Chen, Zhong Liu
Categories: cs.LG cs.AI
\\
  Many complex problems encountered in both production and daily life can be
conceptualized as combinatorial optimization problems (COPs) over graphs.
Recent years, reinforcement learning (RL) based models have emerged as a
promising direction, which treat the COPs solving as a heuristic learning
problem. However, current finite-horizon-MDP based RL models have inherent
limitations. They are not allowed to explore adquately for improving solutions
at test time, which may be necessary given the complexity of NP-hard
optimization tasks. Some recent attempts solve this issue by focusing on reward
design and state feature engineering, which are tedious and ad-hoc. In this
work, we instead propose a much simpler but more effective technique, named
gauge transformation (GT). The technique is originated from physics, but is
very effective in enabling RL agents to explore to continuously improve the
solutions during test. Morever, GT is very simple, which can be implemented
with less than 10 lines of Python codes, and can be applied to a vast majority
of RL models. Experimentally, we show that traditional RL models with GT
technique produce the state-of-the-art performances on the MaxCut problem.
Furthermore, since GT is independent of any RL models, it can be seamlessly
integrated into various RL frameworks, paving the way of these models for more
effective explorations in the solving of general COPs.
\\ ( https://arxiv.org/abs/2404.04661 ,  446kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04662
Date: Sat, 6 Apr 2024 15:31:20 GMT   (7665kb,D)

Title: Learning Minimal NAP Specifications for Neural Network Verification
Authors: Chuqin Geng, Zhaoyue Wang, Haolin Ye, Saifei Liao, Xujie Si
Categories: cs.LG cs.PL
Comments: 29 pages,8 figures
\\
  Specifications play a crucial role in neural network verification. They
define the precise input regions we aim to verify, typically represented as
L-infinity norm balls. While recent research suggests using neural activation
patterns (NAPs) as specifications for verifying unseen test set data, it
focuses on computing the most refined NAPs, often limited to very small regions
in the input space. In this paper, we study the following problem: Given a
neural network, find a minimal (coarsest) NAP that is sufficient for formal
verification of the network's robustness. Finding the minimal NAP specification
not only expands verifiable bounds but also provides insights into which
neurons contribute to the model's robustness. To address this problem, we
propose several exact and approximate approaches. Our exact approaches leverage
the verification tool to find minimal NAP specifications in either a
deterministic or statistical manner. Whereas the approximate methods
efficiently estimate minimal NAPs using adversarial examples and local
gradients, without making calls to the verification tool. This allows us to
inspect potential causal links between neurons and the robustness of
state-of-the-art neural networks, a task for which existing verification
frameworks fail to scale. Our experimental results suggest that minimal NAP
specifications require much smaller fractions of neurons compared to the most
refined NAP specifications, yet they can significantly expand the verifiable
boundaries to several orders of magnitude larger.
\\ ( https://arxiv.org/abs/2404.04662 ,  7665kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04669
Date: Sat, 6 Apr 2024 16:05:48 GMT   (1981kb,D)

Title: Domain Generalisation via Imprecise Learning
Authors: Anurag Singh, Siu Lun Chau, Shahine Bouabid and Krikamol Muandet
Categories: cs.LG
\\
  Out-of-distribution (OOD) generalisation is challenging because it involves
not only learning from empirical data, but also deciding among various notions
of generalisation, e.g., optimising the average-case risk, worst-case risk, or
interpolations thereof. While this choice should in principle be made by the
model operator like medical doctors, this information might not always be
available at training time. The institutional separation between machine
learners and model operators leads to arbitrary commitments to specific
generalisation strategies by machine learners due to these deployment
uncertainties. We introduce the Imprecise Domain Generalisation framework to
mitigate this, featuring an imprecise risk optimisation that allows learners to
stay imprecise by optimising against a continuous spectrum of generalisation
strategies during training, and a model framework that allows operators to
specify their generalisation preference at deployment. Supported by both
theoretical and empirical evidence, our work showcases the benefits of
integrating imprecision into domain generalisation.
\\ ( https://arxiv.org/abs/2404.04669 ,  1981kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04678
Date: Sat, 6 Apr 2024 16:48:12 GMT   (2511kb,D)

Title: Automatic Gradient Estimation for Calibrating Crowd Models with Discrete
  Decision Making
Authors: Philipp Andelfinger, Justin N. Kreikemeyer
Categories: cs.LG cs.MA
Comments: Accepted at International Conference on Computational Science
  (ICCS'24)
\\
  Recently proposed gradient estimators enable gradient descent over stochastic
programs with discrete jumps in the response surface, which are not covered by
automatic differentiation (AD) alone. Although these estimators' capability to
guide a swift local search has been shown for certain problems, their
applicability to models relevant to real-world applications remains largely
unexplored. As the gradients governing the choice in candidate solutions are
calculated from sampled simulation trajectories, the optimization procedure
bears similarities to metaheuristics such as particle swarm optimization, which
puts the focus on the different methods' calibration progress per function
evaluation. Here, we consider the calibration of force-based crowd evacuation
models based on the popular Social Force model augmented by discrete decision
making. After studying the ability of an AD-based estimator for branching
programs to capture the simulation's rugged response surface, calibration
problems are tackled using gradient descent and two metaheuristics. As our main
insights, we find 1) that the estimation's fidelity benefits from disregarding
jumps of large magnitude inherent to the Social Force model, and 2) that the
common problem of calibration by adjusting a simulation input distribution
obviates the need for AD across the Social Force calculations, allowing
gradient descent to excel.
\\ ( https://arxiv.org/abs/2404.04678 ,  2511kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04682
Date: Sat, 6 Apr 2024 17:02:18 GMT   (3966kb,D)

Title: Compositional Conservatism: A Transductive Approach in Offline
  Reinforcement Learning
Authors: Yeda Song, Dongwook Lee, Gunhee Kim
Categories: cs.LG cs.AI cs.RO
Comments: ICLR 2024
\\
  Offline reinforcement learning (RL) is a compelling framework for learning
optimal policies from past experiences without additional interaction with the
environment. Nevertheless, offline RL inevitably faces the problem of
distributional shifts, where the states and actions encountered during policy
execution may not be in the training dataset distribution. A common solution
involves incorporating conservatism into the policy or the value function to
safeguard against uncertainties and unknowns. In this work, we focus on
achieving the same objectives of conservatism but from a different perspective.
We propose COmpositional COnservatism with Anchor-seeking (COCOA) for offline
RL, an approach that pursues conservatism in a compositional manner on top of
the transductive reparameterization (Netanyahu et al., 2023), which decomposes
the input variable (the state in our case) into an anchor and its difference
from the original input. Our COCOA seeks both in-distribution anchors and
differences by utilizing the learned reverse dynamics model, encouraging
conservatism in the compositional input space for the policy or value function.
Such compositional conservatism is independent of and agnostic to the prevalent
behavioral conservatism in offline RL. We apply COCOA to four state-of-the-art
offline RL algorithms and evaluate them on the D4RL benchmark, where COCOA
generally improves the performance of each algorithm. The code is available at
https://github.com/runamu/compositional-conservatism.
\\ ( https://arxiv.org/abs/2404.04682 ,  3966kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04686
Date: Sat, 6 Apr 2024 17:23:21 GMT   (1004kb)

Title: Predictive Modeling for Breast Cancer Classification in the Context of
  Bangladeshi Patients: A Supervised Machine Learning Approach with Explainable
  AI
Authors: Taminul Islam, Md. Alif Sheakh, Mst. Sazia Tahosin, Most. Hasna Hena,
  Shopnil Akash, Yousef A. Bin Jardan, Gezahign Fentahun Wondmie, Hiba-Allah
  Nafidi, Mohammed Bourhia
Categories: cs.LG cs.AI cs.CV
Comments: Accepted for the Scientific Reports (Nature) journal. 32 pages, 12
  figures
\\
  Breast cancer has rapidly increased in prevalence in recent years, making it
one of the leading causes of mortality worldwide. Among all cancers, it is by
far the most common. Diagnosing this illness manually requires significant time
and expertise. Since detecting breast cancer is a time-consuming process,
preventing its further spread can be aided by creating machine-based forecasts.
Machine learning and Explainable AI are crucial in classification as they not
only provide accurate predictions but also offer insights into how the model
arrives at its decisions, aiding in the understanding and trustworthiness of
the classification results. In this study, we evaluate and compare the
classification accuracy, precision, recall, and F-1 scores of five different
machine learning methods using a primary dataset (500 patients from Dhaka
Medical College Hospital). Five different supervised machine learning
techniques, including decision tree, random forest, logistic regression, naive
bayes, and XGBoost, have been used to achieve optimal results on our dataset.
Additionally, this study applied SHAP analysis to the XGBoost model to
interpret the model's predictions and understand the impact of each feature on
the model's output. We compared the accuracy with which several algorithms
classified the data, as well as contrasted with other literature in this field.
After final evaluation, this study found that XGBoost achieved the best model
accuracy, which is 97%.
\\ ( https://arxiv.org/abs/2404.04686 ,  1004kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04690
Date: Sat, 6 Apr 2024 17:37:45 GMT   (1016kb)

Title: The Identification and Categorization of Anemia Through Artificial
  Neural Networks: A Comparative Analysis of Three Models
Authors: Mohammed A. A. Elmaleeh
Categories: cs.LG cs.SY eess.SY
\\
  This paper presents different neural network-based classifier algorithms for
diagnosing and classifying Anemia. The study compares these classifiers with
established models such as Feed Forward Neural Network (FFNN), Elman network,
and Non-linear Auto-Regressive Exogenous model (NARX). Experimental evaluations
were conducted using data from clinical laboratory test results for 230
patients. The proposed neural network features nine inputs (age, gender, RBC,
HGB, HCT, MCV, MCH, MCHC, WBCs) and one output. The simulation outcomes for
diverse patients demonstrate that the suggested artificial neural network
rapidly and accurately detects the presence of the disease. Consequently, the
network could be seamlessly integrated into clinical laboratories for automatic
generation of Anemia patients' reports Additionally, the suggested method is
affordable and can be deployed on hardware at low costs.
\\ ( https://arxiv.org/abs/2404.04690 ,  1016kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04714
Date: Sat, 6 Apr 2024 19:27:57 GMT   (11547kb,D)

Title: Data Poisoning Attacks on Off-Policy Policy Evaluation Methods
Authors: Elita Lobo, Harvineet Singh, Marek Petrik, Cynthia Rudin, Himabindu
  Lakkaraju
Categories: cs.LG cs.AI cs.CR
Comments: Accepted at UAI 2022
\\
  Off-policy Evaluation (OPE) methods are a crucial tool for evaluating
policies in high-stakes domains such as healthcare, where exploration is often
infeasible, unethical, or expensive. However, the extent to which such methods
can be trusted under adversarial threats to data quality is largely unexplored.
In this work, we make the first attempt at investigating the sensitivity of OPE
methods to marginal adversarial perturbations to the data. We design a generic
data poisoning attack framework leveraging influence functions from robust
statistics to carefully construct perturbations that maximize error in the
policy value estimates. We carry out extensive experimentation with multiple
healthcare and control datasets. Our results demonstrate that many existing OPE
methods are highly prone to generating value estimates with large errors when
subject to data poisoning attacks, even for small adversarial perturbations.
These findings question the reliability of policy values derived using OPE
methods and motivate the need for developing OPE methods that are statistically
robust to train-time data poisoning attacks.
\\ ( https://arxiv.org/abs/2404.04714 ,  11547kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04793
Date: Sun, 7 Apr 2024 03:08:14 GMT   (676kb,D)

Title: SqueezeAttention: 2D Management of KV-Cache in LLM Inference via
  Layer-wise Optimal Budget
Authors: Zihao Wang, Shaoduo Gan
Categories: cs.LG
\\
  Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has
been considered critical to saving the cost of inference. Most of the existing
KV-cache compression algorithms attempted to sparsify the sequence of tokens by
taking advantage of the different importance of tokens. In this work, we found
that by identifying the importance of attention layers, we could optimize the
KV-cache jointly from two dimensions. Based on our observations regarding
layer-wise importance in inference, we propose SqueezeAttention to precisely
optimize the allocation of KV-cache budget among layers on-the-fly and then
incorporate three representative token sparsification algorithms to compress
the KV-cache for each layer with its very own budget. By optimizing the
KV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves
around 30% to 70% of the memory reductions and up to 2.2 times of throughput
improvements in a wide range of LLMs and benchmarks. The code is available at
https://github.com/hetailang/SqueezeAttention.
\\ ( https://arxiv.org/abs/2404.04793 ,  676kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04800
Date: Sun, 7 Apr 2024 03:41:45 GMT   (4709kb,D)

Title: Coordinated Sparse Recovery of Label Noise
Authors: Yukun Yang, Naihao Wang, Haixin Yang, Ruirui Li
Categories: cs.LG cs.CV stat.ML
Comments: Pre-print prior to submission to journal
\\
  Label noise is a common issue in real-world datasets that inevitably impacts
the generalization of models. This study focuses on robust classification tasks
where the label noise is instance-dependent. Estimating the transition matrix
accurately in this task is challenging, and methods based on sample selection
often exhibit confirmation bias to varying degrees. Sparse over-parameterized
training (SOP) has been theoretically effective in estimating and recovering
label noise, offering a novel solution for noise-label learning. However, this
study empirically observes and verifies a technical flaw of SOP: the lack of
coordination between model predictions and noise recovery leads to increased
generalization error. To address this, we propose a method called Coordinated
Sparse Recovery (CSR). CSR introduces a collaboration matrix and confidence
weights to coordinate model predictions and noise recovery, reducing error
leakage. Based on CSR, this study designs a joint sample selection strategy and
constructs a comprehensive and powerful learning framework called CSR+. CSR+
significantly reduces confirmation bias, especially for datasets with more
classes and a high proportion of instance-specific noise. Experimental results
on simulated and real-world noisy datasets demonstrate that both CSR and CSR+
achieve outstanding performance compared to methods at the same level.
\\ ( https://arxiv.org/abs/2404.04800 ,  4709kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04814
Date: Sun, 7 Apr 2024 05:47:41 GMT   (3015kb)

Title: Inference-Time Rule Eraser: Distilling and Removing Bias Rules to
  Mitigate Bias in Deployed Models
Authors: Yi Zhang, Jitao Sang
Categories: cs.LG cs.AI
\\
  Fairness is critical for artificial intelligence systems, especially for
those deployed in high-stakes applications such as hiring and justice. Existing
efforts toward fairness in machine learning fairness require retraining or
fine-tuning the neural network weights to meet the fairness criteria. However,
this is often not feasible in practice for regular model users due to the
inability to access and modify model weights. In this paper, we propose a more
flexible fairness paradigm, Inference-Time Rule Eraser, or simply Eraser, which
considers the case where model weights can not be accessed and tackles fairness
issues from the perspective of biased rules removal at inference-time. We first
verified the feasibility of modifying the model output to wipe the biased rule
through Bayesian analysis, and deduced Inference-Time Rule Eraser via
subtracting the logarithmic value associated with unfair rules (i.e., the
model's response to biased features) from the model's logits output as a means
of removing biased rules. Moreover, we present a specific implementation of
Rule Eraser that involves two stages: (1) limited queries are performed on the
model with inaccessible weights to distill its biased rules into an additional
patched model, and (2) during inference time, the biased rules already
distilled into the patched model are excluded from the output of the original
model, guided by the removal strategy outlined in Rule Eraser. Exhaustive
experimental evaluation demonstrates the effectiveness and superior performance
of the proposed Rule Eraser in addressing fairness concerns.
\\ ( https://arxiv.org/abs/2404.04814 ,  3015kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04824
Date: Sun, 7 Apr 2024 06:23:18 GMT   (14490kb,D)

Title: Mixup Domain Adaptations for Dynamic Remaining Useful Life Predictions
Authors: Muhammad Tanzil Furqon, Mahardhika Pratama, Lin Liu, Habibullah,
  Kutluyil Dogancay
Categories: cs.LG cs.AI stat.ML
Comments: accepted for publication in Knowledge-based Systems
Journal-ref: Knowledge-based Systems, 2024
\\
  Remaining Useful Life (RUL) predictions play vital role for asset planning
and maintenance leading to many benefits to industries such as reduced
downtime, low maintenance costs, etc. Although various efforts have been
devoted to study this topic, most existing works are restricted for i.i.d
conditions assuming the same condition of the training phase and the deployment
phase. This paper proposes a solution to this problem where a mix-up domain
adaptation (MDAN) is put forward. MDAN encompasses a three-staged mechanism
where the mix-up strategy is not only performed to regularize the source and
target domains but also applied to establish an intermediate mix-up domain
where the source and target domains are aligned. The self-supervised learning
strategy is implemented to prevent the supervision collapse problem. Rigorous
evaluations have been performed where MDAN is compared to recently published
works for dynamic RUL predictions. MDAN outperforms its counterparts with
substantial margins in 12 out of 12 cases. In addition, MDAN is evaluated with
the bearing machine dataset where it beats prior art with significant gaps in 8
of 12 cases. Source codes of MDAN are made publicly available in
\url{https://github.com/furqon3009/MDAN}.
\\ ( https://arxiv.org/abs/2404.04824 ,  14490kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04825
Date: Sun, 7 Apr 2024 06:24:47 GMT   (24128kb,D)

Title: Gradient-based Design of Computational Granular Crystals
Authors: Atoosa Parsa, Corey S. O'Hern, Rebecca Kramer-Bottiglio, Josh Bongard
Categories: cs.LG cs.AI cs.NE
\\
  There is growing interest in engineering unconventional computing devices
that leverage the intrinsic dynamics of physical substrates to perform fast and
energy-efficient computations. Granular metamaterials are one such substrate
that has emerged as a promising platform for building wave-based information
processing devices with the potential to integrate sensing, actuation, and
computation. Their high-dimensional and nonlinear dynamics result in nontrivial
and sometimes counter-intuitive wave responses that can be shaped by the
material properties, geometry, and configuration of individual grains. Such
highly tunable rich dynamics can be utilized for mechanical computing in
special-purpose applications. However, there are currently no general
frameworks for the inverse design of large-scale granular materials. Here, we
build upon the similarity between the spatiotemporal dynamics of wave
propagation in material and the computational dynamics of Recurrent Neural
Networks to develop a gradient-based optimization framework for harmonically
driven granular crystals. We showcase how our framework can be utilized to
design basic logic gates where mechanical vibrations carry the information at
predetermined frequencies. We compare our design methodology with classic
gradient-free methods and find that our approach discovers higher-performing
configurations with less computational effort. Our findings show that a
gradient-based optimization method can greatly expand the design space of
metamaterials and provide the opportunity to systematically traverse the
parameter space to find materials with the desired functionalities.
\\ ( https://arxiv.org/abs/2404.04825 ,  24128kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04854
Date: Sun, 7 Apr 2024 07:56:14 GMT   (504kb,D)

Title: Contextual Chart Generation for Cyber Deception
Authors: David D. Nguyen, David Liebowitz, Surya Nepal, Salil S. Kanhere,
  Sharif Abuadbba
Categories: cs.LG cs.AI cs.CR
Comments: 13 pages including references
\\
  Honeyfiles are security assets designed to attract and detect intruders on
compromised systems. Honeyfiles are a type of honeypot that mimic real,
sensitive documents, creating the illusion of the presence of valuable data.
Interaction with a honeyfile reveals the presence of an intruder, and can
provide insights into their goals and intentions. Their practical use, however,
is limited by the time, cost and effort associated with manually creating
realistic content. The introduction of large language models has made
high-quality text generation accessible, but honeyfiles contain a variety of
content including charts, tables and images. This content needs to be plausible
and realistic, as well as semantically consistent both within honeyfiles and
with the real documents they mimic, to successfully deceive an intruder.
  In this paper, we focus on an important component of the honeyfile content
generation problem: document charts. Charts are ubiquitous in corporate
documents and are commonly used to communicate quantitative and scientific
data. Existing image generation models, such as DALL-E, are rather prone to
generating charts with incomprehensible text and unconvincing data. We take a
multi-modal approach to this problem by combining two purpose-built generative
models: a multitask Transformer and a specialized multi-head autoencoder. The
Transformer generates realistic captions and plot text, while the autoencoder
generates the underlying tabular data for the plot.
  To advance the field of automated honeyplot generation, we also release a new
document-chart dataset and propose a novel metric Keyword Semantic Matching
(KSM). This metric measures the semantic consistency between keywords of a
corpus and a smaller bag of words. Extensive experiments demonstrate excellent
performance against multiple large language models, including ChatGPT and GPT4.
\\ ( https://arxiv.org/abs/2404.04854 ,  504kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04859
Date: Sun, 7 Apr 2024 08:07:02 GMT   (278kb,D)

Title: Demystifying Lazy Training of Neural Networks from a Macroscopic
  Viewpoint
Authors: Yuqing Li, Tao Luo, Qixuan Zhou
Categories: cs.LG stat.ML
\\
  In this paper, we advance the understanding of neural network training
dynamics by examining the intricate interplay of various factors introduced by
weight parameters in the initialization process. Motivated by the foundational
work of Luo et al. (J. Mach. Learn. Res., Vol. 22, Iss. 1, No. 71, pp
3327-3373), we explore the gradient descent dynamics of neural networks through
the lens of macroscopic limits, where we analyze its behavior as width $m$
tends to infinity. Our study presents a unified approach with refined
techniques designed for multi-layer fully connected neural networks, which can
be readily extended to other neural network architectures. Our investigation
reveals that gradient descent can rapidly drive deep neural networks to zero
training loss, irrespective of the specific initialization schemes employed by
weight parameters, provided that the initial scale of the output function
$\kappa$ surpasses a certain threshold. This regime, characterized as the
theta-lazy area, accentuates the predominant influence of the initial scale
$\kappa$ over other factors on the training behavior of neural networks.
Furthermore, our approach draws inspiration from the Neural Tangent Kernel
(NTK) paradigm, and we expand its applicability. While NTK typically assumes
that $\lim_{m\to\infty}\frac{\log \kappa}{\log m}=\frac{1}{2}$, and imposes
each weight parameters to scale by the factor $\frac{1}{\sqrt{m}}$, in our
theta-lazy regime, we discard the factor and relax the conditions to
$\lim_{m\to\infty}\frac{\log \kappa}{\log m}>0$. Similar to NTK, the behavior
of overparameterized neural networks within the theta-lazy regime trained by
gradient descent can be effectively described by a specific kernel. Through
rigorous analysis, our investigation illuminates the pivotal role of $\kappa$
in governing the training dynamics of neural networks.
\\ ( https://arxiv.org/abs/2404.04859 ,  278kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04865
Date: Sun, 7 Apr 2024 08:17:48 GMT   (137kb,D)

Title: On the Learnability of Out-of-distribution Detection
Authors: Zhen Fang, Yixuan Li, Feng Liu, Bo Han, Jie Lu
Categories: cs.LG cs.CV stat.ML
Comments: Accepted by JMLR in 7th of April, 2024. This is a journal extension
  of the previous NeurIPS 2022 Outstanding Paper "Is Out-of-distribution
  Detection Learnable?" [arXiv:2210.14707]
\\
  Supervised learning aims to train a classifier under the assumption that
training and test data are from the same distribution. To ease the above
assumption, researchers have studied a more realistic setting:
out-of-distribution (OOD) detection, where test data may come from classes that
are unknown during training (i.e., OOD data). Due to the unavailability and
diversity of OOD data, good generalization ability is crucial for effective OOD
detection algorithms, and corresponding learning theory is still an open
problem. To study the generalization of OOD detection, this paper investigates
the probably approximately correct (PAC) learning theory of OOD detection that
fits the commonly used evaluation metrics in the literature. First, we find a
necessary condition for the learnability of OOD detection. Then, using this
condition, we prove several impossibility theorems for the learnability of OOD
detection under some scenarios. Although the impossibility theorems are
frustrating, we find that some conditions of these impossibility theorems may
not hold in some practical scenarios. Based on this observation, we next give
several necessary and sufficient conditions to characterize the learnability of
OOD detection in some practical scenarios. Lastly, we offer theoretical support
for representative OOD detection works based on our OOD theory.
\\ ( https://arxiv.org/abs/2404.04865 ,  137kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04870
Date: Sun, 7 Apr 2024 08:31:35 GMT   (4947kb,D)

Title: Signal-noise separation using unsupervised reservoir computing
Authors: Jaesung Choi and Pilwon Kim
Categories: cs.LG nlin.CD
\\
  Removing noise from a signal without knowing the characteristics of the noise
is a challenging task. This paper introduces a signal-noise separation method
based on time series prediction. We use Reservoir Computing (RC) to extract the
maximum portion of "predictable information" from a given signal. Reproducing
the deterministic component of the signal using RC, we estimate the noise
distribution from the difference between the original signal and reconstructed
one. The method is based on a machine learning approach and requires no prior
knowledge of either the deterministic signal or the noise distribution. It
provides a way to identify additivity/multiplicativity of noise and to estimate
the signal-to-noise ratio (SNR) indirectly. The method works successfully for
combinations of various signal and noise, including chaotic signal and highly
oscillating sinusoidal signal which are corrupted by non-Gaussian additive/
multiplicative noise. The separation performances are robust and notably
outstanding for signals with strong noise, even for those with negative SNR.
\\ ( https://arxiv.org/abs/2404.04870 ,  4947kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04871
Date: Sun, 7 Apr 2024 08:32:16 GMT   (135kb,D)

Title: Data Stream Sampling with Fuzzy Task Boundaries and Noisy Labels
Authors: Yu-Hsi Chen
Categories: cs.LG cs.AI cs.CV
\\
  In the realm of continual learning, the presence of noisy labels within data
streams represents a notable obstacle to model reliability and fairness. We
focus on the data stream scenario outlined in pertinent literature,
characterized by fuzzy task boundaries and noisy labels. To address this
challenge, we introduce a novel and intuitive sampling method called Noisy Test
Debiasing (NTD) to mitigate noisy labels in evolving data streams and establish
a fair and robust continual learning algorithm. NTD is straightforward to
implement, making it feasible across various scenarios. Our experiments
benchmark four datasets, including two synthetic noise datasets (CIFAR10 and
CIFAR100) and real-world noise datasets (mini-WebVision and Food-101N). The
results validate the efficacy of NTD for online continual learning in scenarios
with noisy labels in data streams. Compared to the previous leading approach,
NTD achieves a training speedup enhancement over two times while maintaining or
surpassing accuracy levels. Moreover, NTD utilizes less than one-fifth of the
GPU memory resources compared to previous leading methods.
\\ ( https://arxiv.org/abs/2404.04871 ,  135kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04874
Date: Sun, 7 Apr 2024 08:38:35 GMT   (343kb,D)

Title: Graph Neural Networks for Binary Programming
Authors: Moshe Eliasof, Eldad Haber
Categories: cs.LG cs.AI cs.ET
\\
  This paper investigates a link between Graph Neural Networks (GNNs) and
Binary Programming (BP) problems, laying the groundwork for GNNs to approximate
solutions for these computationally challenging problems. By analyzing the
sensitivity of BP problems, we are able to frame the solution of BP problems as
a heterophilic node classification task. We then propose Binary-Programming GNN
(BPGNN), an architecture that integrates graph representation learning
techniques with BP-aware features to approximate BP solutions efficiently.
Additionally, we introduce a self-supervised data generation mechanism, to
enable efficient and tractable training data acquisition even for large-scale
BP problems. Experimental evaluations of BPGNN across diverse BP problem sizes
showcase its superior performance compared to exhaustive search and heuristic
approaches. Finally, we discuss open challenges in the under-explored field of
BP problems with GNNs.
\\ ( https://arxiv.org/abs/2404.04874 ,  343kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04885
Date: Sun, 7 Apr 2024 09:05:09 GMT   (1152kb)

Title: TimeGPT in Load Forecasting: A Large Time Series Model Perspective
Authors: Wenlong Liao, Fernando Porte-Agel, Jiannong Fang, Christian Rehtanz,
  Shouxiang Wang, Dechang Yang, and Zhe Yang
Categories: cs.LG
Comments: 10 pages
\\
  Machine learning models have made significant progress in load forecasting,
but their forecast accuracy is limited in cases where historical load data is
scarce. Inspired by the outstanding performance of large language models (LLMs)
in computer vision and natural language processing, this paper aims to discuss
the potential of large time series models in load forecasting with scarce
historical data. Specifically, the large time series model is constructed as a
time series generative pre-trained transformer (TimeGPT), which is trained on
massive and diverse time series datasets consisting of 100 billion data points
(e.g., finance, transportation, banking, web traffic, weather, energy,
healthcare, etc.). Then, the scarce historical load data is used to fine-tune
the TimeGPT, which helps it to adapt to the data distribution and
characteristics associated with load forecasting. Simulation results show that
TimeGPT outperforms the benchmarks (e.g., popular machine learning models and
statistical models) for load forecasting on several real datasets with scarce
training samples, particularly for short look-ahead times. However, it cannot
be guaranteed that TimeGPT is always superior to benchmarks for load
forecasting with scarce data, since the performance of TimeGPT may be affected
by the distribution differences between the load data and the training data. In
practical applications, we can divide the historical data into a training set
and a validation set, and then use the validation set loss to decide whether
TimeGPT is the best choice for a specific dataset.
\\ ( https://arxiv.org/abs/2404.04885 ,  1152kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04903
Date: Sun, 7 Apr 2024 10:07:56 GMT   (1962kb,D)

Title: Online Learning under Haphazard Input Conditions: A Comprehensive Review
  and Analysis
Authors: Rohit Agarwal, Arijit Das, Alexander Horsch, Krishna Agarwal, and
  Dilip K. Prasad
Categories: cs.LG cs.AI
\\
  The domain of online learning has experienced multifaceted expansion owing to
its prevalence in real-life applications. Nonetheless, this progression
operates under the assumption that the input feature space of the streaming
data remains constant. In this survey paper, we address the topic of online
learning in the context of haphazard inputs, explicitly foregoing such an
assumption. We discuss, classify, evaluate, and compare the methodologies that
are adept at modeling haphazard inputs, additionally providing the
corresponding code implementations and their carbon footprint. Moreover, we
classify the datasets related to the field of haphazard inputs and introduce
evaluation metrics specifically designed for datasets exhibiting imbalance. The
code of each methodology can be found at
https://github.com/Rohit102497/HaphazardInputsReview
\\ ( https://arxiv.org/abs/2404.04903 ,  1962kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04920
Date: Sun, 7 Apr 2024 11:20:32 GMT   (3079kb,D)

Title: Regularized Conditional Diffusion Model for Multi-Task Preference
  Alignment
Authors: Xudong Yu, Chenjia Bai, Haoran He, Changhong Wang, Xuelong Li
Categories: cs.LG
\\
  Sequential decision-making is desired to align with human intents and exhibit
versatility across various tasks. Previous methods formulate it as a
conditional generation process, utilizing return-conditioned diffusion models
to directly model trajectory distributions. Nevertheless, the
return-conditioned paradigm relies on pre-defined reward functions, facing
challenges when applied in multi-task settings characterized by varying reward
functions (versatility) and showing limited controllability concerning human
preferences (alignment). In this work, we adopt multi-task preferences as a
unified condition for both single- and multi-task decision-making, and propose
preference representations aligned with preference labels. The learned
representations are used to guide the conditional generation process of
diffusion models, and we introduce an auxiliary objective to maximize the
mutual information between representations and corresponding generated
trajectories, improving alignment between trajectories and preferences.
Extensive experiments in D4RL and Meta-World demonstrate that our method
presents favorable performance in single- and multi-task scenarios, and
exhibits superior alignment with preferences.
\\ ( https://arxiv.org/abs/2404.04920 ,  3079kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04931
Date: Sun, 7 Apr 2024 12:07:33 GMT   (300kb,D)

Title: The Sample Complexity of Gradient Descent in Stochastic Convex
  Optimization
Authors: Roi Livni
Categories: cs.LG
\\
  We analyze the sample complexity of full-batch Gradient Descent (GD) in the
setup of non-smooth Stochastic Convex Optimization. We show that the
generalization error of GD, with (minmax) optimal choice of hyper-parameters,
can be $\tilde \Theta(d/m + 1/\sqrt{m})$, where $d$ is the dimension and $m$ is
the sample size. This matches the sample complexity of \emph{worst-case}
empirical risk minimizers. That means that, in contrast with other algorithms,
GD has no advantage over naive ERMs. Our bound follows from a new
generalization bound that depends on both the dimension as well as the learning
rate and number of iterations. Our bound also shows that, for general
hyper-parameters, when the dimension is strictly larger than number of samples,
$T=\Omega(1/\epsilon^4)$ iterations are necessary to avoid overfitting. This
resolves an open problem by \citet*{schliserman2024dimension, amir2021sgd}, and
improves over previous lower bounds that demonstrated that the sample size must
be at least square root of the dimension.
\\ ( https://arxiv.org/abs/2404.04931 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04940
Date: Sun, 7 Apr 2024 12:25:03 GMT   (237kb,D)

Title: Fuzzy K-Means Clustering without Cluster Centroids
Authors: Han Lu, Fangfang Li, Quanxue Gao, Cheng Deng, Chris Ding, Qianqian
  Wang
Categories: cs.LG
\\
  Fuzzy K-Means clustering is a critical technique in unsupervised data
analysis. However, the performance of popular Fuzzy K-Means algorithms is
sensitive to the selection of initial cluster centroids and is also affected by
noise when updating mean cluster centroids. To address these challenges, this
paper proposes a novel Fuzzy K-Means clustering algorithm that entirely
eliminates the reliance on cluster centroids, obtaining membership matrices
solely through distance matrix computation. This innovation enhances
flexibility in distance measurement between sample points, thus improving the
algorithm's performance and robustness. The paper also establishes theoretical
connections between the proposed model and popular Fuzzy K-Means clustering
techniques. Experimental results on several real datasets demonstrate the
effectiveness of the algorithm.
\\ ( https://arxiv.org/abs/2404.04940 ,  237kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04943
Date: Sun, 7 Apr 2024 12:40:37 GMT   (10399kb)

Title: Chiplet Placement Order Exploration Based on Learning to Rank with Graph
  Representation
Authors: Zhihui Deng, Yuanyuan Duan, Leilai Shao, Xiaolei Zhu
Categories: cs.LG cs.AI cs.AR
Comments: 6 pages, 8 figures and 6 tables, accepted by the Conference ISEDA
\\
  Chiplet-based systems, integrating various silicon dies manufactured at
different integrated circuit technology nodes on a carrier interposer, have
garnered significant attention in recent years due to their cost-effectiveness
and competitive performance. The widespread adoption of reinforcement learning
as a sequential placement method has introduced a new challenge in determining
the optimal placement order for each chiplet. The order in which chiplets are
placed on the interposer influences the spatial resources available for earlier
and later placed chiplets, making the placement results highly sensitive to the
sequence of chiplet placement. To address these challenges, we propose a
learning to rank approach with graph representation, building upon the
reinforcement learning framework RLPlanner. This method aims to select the
optimal chiplet placement order for each chiplet-based system. Experimental
results demonstrate that compared to placement order obtained solely based on
the descending order of the chiplet area and the number of interconnect wires
between the chiplets, utilizing the placement order obtained from the learning
to rank network leads to further improvements in system temperature and
inter-chiplet wirelength. Specifically, applying the top-ranked placement order
obtained from the learning to rank network results in a 10.05% reduction in
total inter-chiplet wirelength and a 1.01% improvement in peak system
temperature during the chiplet placement process.
\\ ( https://arxiv.org/abs/2404.04943 ,  10399kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04969
Date: Sun, 7 Apr 2024 14:19:22 GMT   (1152kb,D)

Title: Temporal Generalization Estimation in Evolving Graphs
Authors: Bin Lu, Tingyan Ma, Xiaoying Gan, Xinbing Wang, Yunqiang Zhu, Chenghu
  Zhou, Shiyu Liang
Categories: cs.LG cs.AI
Comments: Published as a conference paper at ICLR 2024
\\
  Graph Neural Networks (GNNs) are widely deployed in vast fields, but they
often struggle to maintain accurate representations as graphs evolve. We
theoretically establish a lower bound, proving that under mild conditions,
representation distortion inevitably occurs over time. To estimate the temporal
distortion without human annotation after deployment, one naive approach is to
pre-train a recurrent model (e.g., RNN) before deployment and use this model
afterwards, but the estimation is far from satisfactory. In this paper, we
analyze the representation distortion from an information theory perspective,
and attribute it primarily to inaccurate feature extraction during evolution.
Consequently, we introduce Smart, a straightforward and effective baseline
enhanced by an adaptive feature extractor through self-supervised graph
reconstruction. In synthetic random graphs, we further refine the former lower
bound to show the inevitable distortion over time and empirically observe that
Smart achieves good estimation performance. Moreover, we observe that Smart
consistently shows outstanding generalization estimation on four real-world
evolving graphs. The ablation studies underscore the necessity of graph
reconstruction. For example, on OGB-arXiv dataset, the estimation metric MAPE
deteriorates from 2.19% to 8.00% without reconstruction.
\\ ( https://arxiv.org/abs/2404.04969 ,  1152kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04970
Date: Sun, 7 Apr 2024 14:20:51 GMT   (31786kb,D)

Title: How to characterize imprecision in multi-view clustering?
Authors: Jinyi Xu, Zuowei Zhang, Ze Lin, Yixiang Chen, Zhe Liu, Weiping Ding
Categories: cs.LG
Comments: 19 pages with 8 pages of supplementary
\\
  It is still challenging to cluster multi-view data since existing methods can
only assign an object to a specific (singleton) cluster when combining
different view information. As a result, it fails to characterize imprecision
of objects in overlapping regions of different clusters, thus leading to a high
risk of errors. In this paper, we thereby want to answer the question: how to
characterize imprecision in multi-view clustering? Correspondingly, we propose
a multi-view low-rank evidential c-means based on entropy constraint (MvLRECM).
The proposed MvLRECM can be considered as a multi-view version of evidential
c-means based on the theory of belief functions. In MvLRECM, each object is
allowed to belong to different clusters with various degrees of support (masses
of belief) to characterize uncertainty when decision-making. Moreover, if an
object is in the overlapping region of several singleton clusters, it can be
assigned to a meta-cluster, defined as the union of these singleton clusters,
to characterize the local imprecision in the result. In addition,
entropy-weighting and low-rank constraints are employed to reduce imprecision
and improve accuracy. Compared to state-of-the-art methods, the effectiveness
of MvLRECM is demonstrated based on several toy and UCI real datasets.
\\ ( https://arxiv.org/abs/2404.04970 ,  31786kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04997
Date: Sun, 7 Apr 2024 15:44:20 GMT   (309kb,D)

Title: Adapting LLMs for Efficient Context Processing through Soft Prompt
  Compression
Authors: Cangqing Wang, Yutian Yang, Ruisi Li, Dan Sun, Ruicong Cai, Yuzhu
  Zhang, Chengqian Fu and Lillian Floyd
Categories: cs.LG cs.AI cs.CL
Comments: This paper has been accepted by the 2024 International Conference on
  Image Processing and Computer Applications (IPCA 2024)
\\
  The rapid advancement of Large Language Models (LLMs) has inaugurated a
transformative epoch in natural language processing, fostering unprecedented
proficiency in text generation, comprehension, and contextual scrutiny.
Nevertheless, effectively handling extensive contexts, crucial for myriad
applications, poses a formidable obstacle owing to the intrinsic constraints of
the models' context window sizes and the computational burdens entailed by
their operations. This investigation presents an innovative framework that
strategically tailors LLMs for streamlined context processing by harnessing the
synergies among natural language summarization, soft prompt compression, and
augmented utility preservation mechanisms. Our methodology, dubbed
SoftPromptComp, amalgamates natural language prompts extracted from
summarization methodologies with dynamically generated soft prompts to forge a
concise yet semantically robust depiction of protracted contexts. This
depiction undergoes further refinement via a weighting mechanism optimizing
information retention and utility for subsequent tasks. We substantiate that
our framework markedly diminishes computational overhead and enhances LLMs'
efficacy across various benchmarks, while upholding or even augmenting the
caliber of the produced content. By amalgamating soft prompt compression with
sophisticated summarization, SoftPromptComp confronts the dual challenges of
managing lengthy contexts and ensuring model scalability. Our findings point
towards a propitious trajectory for augmenting LLMs' applicability and
efficiency, rendering them more versatile and pragmatic for real-world
applications. This research enriches the ongoing discourse on optimizing
language models, providing insights into the potency of soft prompts and
summarization techniques as pivotal instruments for the forthcoming generation
of NLP solutions.
\\ ( https://arxiv.org/abs/2404.04997 ,  309kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05019
Date: Sun, 7 Apr 2024 17:17:23 GMT   (2074kb,D)

Title: Shortcut-connected Expert Parallelism for Accelerating
  Mixture-of-Experts
Authors: Weilin Cai, Juyong Jiang, Le Qin, Junwei Cui, Sunghun Kim, Jiayi Huang
Categories: cs.LG cs.CL cs.DC
\\
  Expert parallelism has been introduced as a strategy to distribute the
computational workload of sparsely-gated mixture-of-experts (MoE) models across
multiple computing devices, facilitating the execution of these increasingly
large-scale models. However, the All-to-All communication intrinsic to expert
parallelism constitutes a significant overhead, diminishing the MoE models'
efficiency. Current optimization approaches offer some relief, yet they are
constrained by the sequential interdependence of communication and computation
operations. To address this limitation, we present a novel shortcut-connected
MoE architecture with overlapping parallel strategy, designated as ScMoE, which
effectively decouples communication from its conventional sequence, allowing
for a substantial overlap of 70% to 100% with computation. When compared with
the prevalent top-2 MoE architecture, ScMoE demonstrates training speed
improvements of 30% and 11%, and inference improvements of 40% and 15%, in our
PCIe and NVLink hardware environments, respectively, where communication
constitutes 60% and 15% of the total MoE time consumption. On the other hand,
extensive experiments and theoretical analyses indicate that ScMoE not only
achieves comparable but in some instances surpasses the model quality of
existing approaches in vision and language tasks.
\\ ( https://arxiv.org/abs/2404.05019 ,  2074kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05043
Date: Sun, 7 Apr 2024 18:55:33 GMT   (3452kb,D)

Title: Optimizing Privacy and Utility Tradeoffs for Group Interests Through
  Harmonization
Authors: Bishwas Mandal, George Amariucai, Shuangqing Wei
Categories: cs.LG cs.CR
Comments: 10 pages, Paper accepted at 2024 IEEE International Joint Conference
  on Neural Networks (IJCNN)
\\
  We propose a novel problem formulation to address the privacy-utility
tradeoff, specifically when dealing with two distinct user groups characterized
by unique sets of private and utility attributes. Unlike previous studies that
primarily focus on scenarios where all users share identical private and
utility attributes and often rely on auxiliary datasets or manual annotations,
we introduce a collaborative data-sharing mechanism between two user groups
through a trusted third party. This third party uses adversarial privacy
techniques with our proposed data-sharing mechanism to internally sanitize data
for both groups and eliminates the need for manual annotation or auxiliary
datasets. Our methodology ensures that private attributes cannot be accurately
inferred while enabling highly accurate predictions of utility features.
Importantly, even if analysts or adversaries possess auxiliary datasets
containing raw data, they are unable to accurately deduce private features.
Additionally, our data-sharing mechanism is compatible with various existing
adversarially trained privacy techniques. We empirically demonstrate the
effectiveness of our approach using synthetic and real-world datasets,
showcasing its ability to balance the conflicting goals of privacy and utility.
\\ ( https://arxiv.org/abs/2404.05043 ,  3452kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05047
Date: Sun, 7 Apr 2024 19:02:50 GMT   (565kb,D)

Title: Initial Exploration of Zero-Shot Privacy Utility Tradeoffs in Tabular
  Data Using GPT-4
Authors: Bishwas Mandal, George Amariucai, Shuangqing Wei
Categories: cs.LG cs.CR
Comments: 8 pages, Paper accepted at 2024 IEEE International Joint Conference
  on Neural Networks (IJCNN)
\\
  We investigate the application of large language models (LLMs), specifically
GPT-4, to scenarios involving the tradeoff between privacy and utility in
tabular data. Our approach entails prompting GPT-4 by transforming tabular data
points into textual format, followed by the inclusion of precise sanitization
instructions in a zero-shot manner. The primary objective is to sanitize the
tabular data in such a way that it hinders existing machine learning models
from accurately inferring private features while allowing models to accurately
infer utility-related attributes. We explore various sanitization instructions.
Notably, we discover that this relatively simple approach yields performance
comparable to more complex adversarial optimization methods used for managing
privacy-utility tradeoffs. Furthermore, while the prompts successfully obscure
private features from the detection capabilities of existing machine learning
models, we observe that this obscuration alone does not necessarily meet a
range of fairness metrics. Nevertheless, our research indicates the potential
effectiveness of LLMs in adhering to these fairness metrics, with some of our
experimental results aligning with those achieved by well-established
adversarial optimization techniques.
\\ ( https://arxiv.org/abs/2404.05047 ,  565kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05051
Date: Sun, 7 Apr 2024 19:22:51 GMT   (26724kb,D)

Title: Skill Transfer and Discovery for Sim-to-Real Learning: A
  Representation-Based Viewpoint
Authors: Haitong Ma, Zhaolin Ren, Bo Dai, Na Li
Categories: cs.LG
Comments: 9 pages, 6 figures. Project page:
  https://congharvard.github.io/steady-sim-to-real/
\\
  We study sim-to-real skill transfer and discovery in the context of robotics
control using representation learning. We draw inspiration from spectral
decomposition of Markov decision processes. The spectral decomposition brings
about representation that can linearly represent the state-action value
function induced by any policies, thus can be regarded as skills. The skill
representations are transferable across arbitrary tasks with the same
transition dynamics. Moreover, to handle the sim-to-real gap in the dynamics,
we propose a skill discovery algorithm that learns new skills caused by the
sim-to-real gap from real-world data. We promote the discovery of new skills by
enforcing orthogonal constraints between the skills to learn and the skills
from simulators, and then synthesize the policy using the enlarged skill sets.
We demonstrate our methodology by transferring quadrotor controllers from
simulators to Crazyflie 2.1 quadrotors. We show that we can learn the skill
representations from a single simulator task and transfer these to multiple
different real-world tasks including hovering, taking off, landing and
trajectory tracking. Our skill discovery approach helps narrow the sim-to-real
gap and improve the real-world controller performance by up to 30.2%.
\\ ( https://arxiv.org/abs/2404.05051 ,  26724kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05055
Date: Sun, 7 Apr 2024 19:29:09 GMT   (3874kb,D)

Title: Percentile Criterion Optimization in Offline Reinforcement Learning
Authors: Elita A. Lobo, Cyrus Cousins, Yair Zick, Marek Petrik
Categories: cs.LG cs.AI
Comments: Accepted at Neurips 2023
\\
  In reinforcement learning, robust policies for high-stakes decision-making
problems with limited data are usually computed by optimizing the
\emph{percentile criterion}. The percentile criterion is approximately solved
by constructing an \emph{ambiguity set} that contains the true model with high
probability and optimizing the policy for the worst model in the set. Since the
percentile criterion is non-convex, constructing ambiguity sets is often
challenging. Existing work uses \emph{Bayesian credible regions} as ambiguity
sets, but they are often unnecessarily large and result in learning overly
conservative policies. To overcome these shortcomings, we propose a novel
Value-at-Risk based dynamic programming algorithm to optimize the percentile
criterion without explicitly constructing any ambiguity sets. Our theoretical
and empirical results show that our algorithm implicitly constructs much
smaller ambiguity sets and learns less conservative robust policies.
\\ ( https://arxiv.org/abs/2404.05055 ,  3874kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05057
Date: Sun, 7 Apr 2024 19:39:14 GMT   (2981kb,D)

Title: TimeCSL: Unsupervised Contrastive Learning of General Shapelets for
  Explorable Time Series Analysis
Authors: Zhiyu Liang, Chen Liang, Zheng Liang, Hongzhi Wang, Bo Zheng
Categories: cs.LG cs.DB
\\
  Unsupervised (a.k.a. Self-supervised) representation learning (URL) has
emerged as a new paradigm for time series analysis, because it has the ability
to learn generalizable time series representation beneficial for many
downstream tasks without using labels that are usually difficult to obtain.
Considering that existing approaches have limitations in the design of the
representation encoder and the learning objective, we have proposed Contrastive
Shapelet Learning (CSL), the first URL method that learns the general-purpose
shapelet-based representation through unsupervised contrastive learning, and
shown its superior performance in several analysis tasks, such as time series
classification, clustering, and anomaly detection. In this paper, we develop
TimeCSL, an end-to-end system that makes full use of the general and
interpretable shapelets learned by CSL to achieve explorable time series
analysis in a unified pipeline. We introduce the system components and
demonstrate how users interact with TimeCSL to solve different analysis tasks
in the unified pipeline, and gain insight into their time series by exploring
the learned shapelets and representation.
\\ ( https://arxiv.org/abs/2404.05057 ,  2981kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05058
Date: Sun, 7 Apr 2024 20:05:49 GMT   (95kb,D)

Title: A robust assessment for invariant representations
Authors: Wenlu Tang, Zicheng Liu
Categories: cs.LG stat.ML
\\
  The performance of machine learning models can be impacted by changes in data
over time. A promising approach to address this challenge is invariant
learning, with a particular focus on a method known as invariant risk
minimization (IRM). This technique aims to identify a stable data
representation that remains effective with out-of-distribution (OOD) data.
While numerous studies have developed IRM-based methods adaptive to data
augmentation scenarios, there has been limited attention on directly assessing
how well these representations preserve their invariant performance under
varying conditions. In our paper, we propose a novel method to evaluate
invariant performance, specifically tailored for IRM-based methods. We
establish a bridge between the conditional expectation of an invariant
predictor across different environments through the likelihood ratio. Our
proposed criterion offers a robust basis for evaluating invariant performance.
We validate our approach with theoretical support and demonstrate its
effectiveness through extensive numerical studies.These experiments illustrate
how our method can assess the invariant performance of various representation
techniques.
\\ ( https://arxiv.org/abs/2404.05058 ,  95kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05064
Date: Sun, 7 Apr 2024 20:24:44 GMT   (4599kb,D)

Title: A Structure-Guided Gauss-Newton Method for Shallow ReLU Neural Network
Authors: Zhiqiang Cai, Tong Ding, Min Liu, Xinyu Liu, Jianlin Xia
Categories: cs.LG
MSC-class: 65D15, 65K10
\\
  In this paper, we propose a structure-guided Gauss-Newton (SgGN) method for
solving least squares problems using a shallow ReLU neural network. The method
effectively takes advantage of both the least squares structure and the neural
network structure of the objective function. By categorizing the weights and
biases of the hidden and output layers of the network as nonlinear and linear
parameters, respectively, the method iterates back and forth between the
nonlinear and linear parameters. The nonlinear parameters are updated by a
damped Gauss-Newton method and the linear ones are updated by a linear solver.
Moreover, at the Gauss-Newton step, a special form of the Gauss-Newton matrix
is derived for the shallow ReLU neural network and is used for efficient
iterations. It is shown that the corresponding mass and Gauss-Newton matrices
in the respective linear and nonlinear steps are symmetric and positive
definite under reasonable assumptions. Thus, the SgGN method naturally produces
an effective search direction without the need of additional techniques like
shifting in the Levenberg-Marquardt method to achieve invertibility of the
Gauss-Newton matrix. The convergence and accuracy of the method are
demonstrated numerically for several challenging function approximation
problems, especially those with discontinuities or sharp transition layers that
pose significant challenges for commonly used training algorithms in machine
learning.
\\ ( https://arxiv.org/abs/2404.05064 ,  4599kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05071
Date: Sun, 7 Apr 2024 20:50:13 GMT   (188kb,D)

Title: Test-Time Training for Depression Detection
Authors: Sri Harsha Dumpala, Chandramouli Shama Sastry, Rudolf Uher, Sageev
  Oore
Categories: cs.LG cs.SD
\\
  Previous works on depression detection use datasets collected in similar
environments to train and test the models. In practice, however, the train and
test distributions cannot be guaranteed to be identical. Distribution shifts
can be introduced due to variations such as recording environment (e.g.,
background noise) and demographics (e.g., gender, age, etc). Such
distributional shifts can surprisingly lead to severe performance degradation
of the depression detection models. In this paper, we analyze the application
of test-time training (TTT) to improve robustness of models trained for
depression detection. When compared to regular testing of the models, we find
TTT can significantly improve the robustness of the model under a variety of
distributional shifts introduced due to: (a) background-noise, (b) gender-bias,
and (c) data collection and curation procedure (i.e., train and test samples
are from separate datasets).
\\ ( https://arxiv.org/abs/2404.05071 ,  188kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05086
Date: Sun, 7 Apr 2024 22:00:50 GMT   (12kb)

Title: A Note on LoRA
Authors: Vlad Fomenko, Han Yu, Jongho Lee, Stanley Hsieh, Weizhu Chen
Categories: cs.LG cs.AI cs.CL
\\
  LoRA (Low-Rank Adaptation) has emerged as a preferred method for efficiently
adapting Large Language Models (LLMs) with remarkable simplicity and efficacy.
This note extends the original LoRA paper by offering new perspectives that
were not initially discussed and presents a series of insights for deploying
LoRA at scale. Without introducing new experiments, we aim to improve the
understanding and application of LoRA.
\\ ( https://arxiv.org/abs/2404.05086 ,  12kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05090
Date: Sun, 7 Apr 2024 22:15:13 GMT   (5952kb,D)

Title: How Bad is Training on Synthetic Data? A Statistical Analysis of
  Language Model Collapse
Authors: Mohamed El Amine Seddik, Suei-Wen Chen, Soufiane Hayou, Pierre
  Youssef, Merouane Debbah
Categories: cs.LG cs.AI cs.CL
\\
  The phenomenon of model collapse, introduced in (Shumailov et al., 2023),
refers to the deterioration in performance that occurs when new models are
trained on synthetic data generated from previously trained models. This
recursive training loop makes the tails of the original distribution disappear,
thereby making future-generation models forget about the initial (real)
distribution. With the aim of rigorously understanding model collapse in
language models, we consider in this paper a statistical model that allows us
to characterize the impact of various recursive training scenarios.
Specifically, we demonstrate that model collapse cannot be avoided when
training solely on synthetic data. However, when mixing both real and synthetic
data, we provide an estimate of a maximal amount of synthetic data below which
model collapse can eventually be avoided. Our theoretical conclusions are
further supported by empirical validations.
\\ ( https://arxiv.org/abs/2404.05090 ,  5952kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05094
Date: Sun, 7 Apr 2024 22:31:34 GMT   (4981kb,D)

Title: Active Test-Time Adaptation: Theoretical Analyses and An Algorithm
Authors: Shurui Gui, Xiner Li, Shuiwang Ji
Categories: cs.LG cs.AI
\\
  Test-time adaptation (TTA) addresses distribution shifts for streaming test
data in unsupervised settings. Currently, most TTA methods can only deal with
minor shifts and rely heavily on heuristic and empirical studies.
  To advance TTA under domain shifts, we propose the novel problem setting of
active test-time adaptation (ATTA) that integrates active learning within the
fully TTA setting.
  We provide a learning theory analysis, demonstrating that incorporating
limited labeled test instances enhances overall performances across test
domains with a theoretical guarantee. We also present a sample entropy
balancing for implementing ATTA while avoiding catastrophic forgetting (CF). We
introduce a simple yet effective ATTA algorithm, known as SimATTA, using
real-time sample selection techniques. Extensive experimental results confirm
consistency with our theoretical analyses and show that the proposed ATTA
method yields substantial performance improvements over TTA methods while
maintaining efficiency and shares similar effectiveness to the more demanding
active domain adaptation (ADA) methods. Our code is available at
https://github.com/divelab/ATTA
\\ ( https://arxiv.org/abs/2404.05094 ,  4981kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05155
Date: Mon, 8 Apr 2024 02:41:32 GMT   (32kb,D)

Title: On the price of exact truthfulness in incentive-compatible online
  learning with bandit feedback: A regret lower bound for WSU-UX
Authors: Ali Mortazavi, Junhao Lin, Nishant A. Mehta
Categories: cs.LG cs.GT stat.ML
Comments: Accepted to AISTATS 2024
\\
  In one view of the classical game of prediction with expert advice with
binary outcomes, in each round, each expert maintains an adversarially chosen
belief and honestly reports this belief. We consider a recently introduced,
strategic variant of this problem with selfish (reputation-seeking) experts,
where each expert strategically reports in order to maximize their expected
future reputation based on their belief. In this work, our goal is to design an
algorithm for the selfish experts problem that is incentive-compatible (IC, or
\emph{truthful}), meaning each expert's best strategy is to report truthfully,
while also ensuring the algorithm enjoys sublinear regret with respect to the
expert with the best belief. Freeman et al. (2020) recently studied this
problem in the full information and bandit settings and obtained truthful,
no-regret algorithms by leveraging prior work on wagering mechanisms. While
their results under full information match the minimax rate for the classical
("honest experts") problem, the best-known regret for their bandit algorithm
WSU-UX is $O(T^{2/3})$, which does not match the minimax rate for the classical
("honest bandits") setting. It was unclear whether the higher regret was an
artifact of their analysis or a limitation of WSU-UX. We show, via explicit
construction of loss sequences, that the algorithm suffers a worst-case
$\Omega(T^{2/3})$ lower bound. Left open is the possibility that a different IC
algorithm obtains $O(\sqrt{T})$ regret. Yet, WSU-UX was a natural choice for
such an algorithm owing to the limited design room for IC algorithms in this
setting.
\\ ( https://arxiv.org/abs/2404.05155 ,  32kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05168
Date: Mon, 8 Apr 2024 03:29:58 GMT   (20003kb,D)

Title: Adapting to Covariate Shift in Real-time by Encoding Trees with Motion
  Equations
Authors: Tham Yik Foong, Heng Zhang, Mao Po Yuan, Danilo Vasconcellos Vargas
Categories: cs.LG
Comments: 7 figures, 2 tables
\\
  Input distribution shift presents a significant problem in many real-world
systems. Here we present Xenovert, an adaptive algorithm that can dynamically
adapt to changes in input distribution. It is a perfect binary tree that
adaptively divides a continuous input space into several intervals of uniform
density while receiving a continuous stream of input. This process indirectly
maps the source distribution to the shifted target distribution, preserving the
data's relationship with the downstream decoder/operation, even after the shift
occurs. In this paper, we demonstrated how a neural network integrated with
Xenovert achieved better results in 4 out of 5 shifted datasets, saving the
hurdle of retraining a machine learning model. We anticipate that Xenovert can
be applied to many more applications that require adaptation to unforeseen
input distribution shifts, even when the distribution shift is drastic.
\\ ( https://arxiv.org/abs/2404.05168 ,  20003kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05182
Date: Mon, 8 Apr 2024 04:14:02 GMT   (3321kb,D)

Title: DLoRA: Distributed Parameter-Efficient Fine-Tuning Solution for Large
  Language Model
Authors: Chao Gao and Sai Qian Zhang
Categories: cs.LG cs.AI cs.CL
\\
  To enhance the performance of large language models (LLM) on downstream
tasks, one solution is to fine-tune certain LLM parameters and make it better
align with the characteristics of the training dataset. This process is
commonly known as parameter-efficient fine-tuning (PEFT). Due to the scale of
LLM, PEFT operations are usually executed in the public environment (e.g.,
cloud server). This necessitates the sharing of sensitive user data across
public environments, thereby raising potential privacy concerns.
  To tackle these challenges, we propose a distributed PEFT framework called
DLoRA. DLoRA enables scalable PEFT operations to be performed collaboratively
between the cloud and user devices. Coupled with the proposed Kill and Revive
algorithm, the evaluation results demonstrate that DLoRA can significantly
reduce the computation and communication workload over the user devices while
achieving superior accuracy and privacy protection.
\\ ( https://arxiv.org/abs/2404.05182 ,  3321kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05192
Date: Mon, 8 Apr 2024 04:41:39 GMT   (3806kb,D)

Title: ATFNet: Adaptive Time-Frequency Ensembled Network for Long-term Time
  Series Forecasting
Authors: Hengyu Ye, Jiadong Chen, Shijin Gong, Fuxin Jiang, Tieying Zhang,
  Jianjun Chen, Xiaofeng Gao
Categories: cs.LG
\\
  The intricate nature of time series data analysis benefits greatly from the
distinct advantages offered by time and frequency domain representations. While
the time domain is superior in representing local dependencies, particularly in
non-periodic series, the frequency domain excels in capturing global
dependencies, making it ideal for series with evident periodic patterns. To
capitalize on both of these strengths, we propose ATFNet, an innovative
framework that combines a time domain module and a frequency domain module to
concurrently capture local and global dependencies in time series data.
Specifically, we introduce Dominant Harmonic Series Energy Weighting, a novel
mechanism for dynamically adjusting the weights between the two modules based
on the periodicity of the input time series. In the frequency domain module, we
enhance the traditional Discrete Fourier Transform (DFT) with our Extended DFT,
designed to address the challenge of discrete frequency misalignment.
Additionally, our Complex-valued Spectrum Attention mechanism offers a novel
approach to discern the intricate relationships between different frequency
combinations. Extensive experiments across multiple real-world datasets
demonstrate that our ATFNet framework outperforms current state-of-the-art
methods in long-term time series forecasting.
\\ ( https://arxiv.org/abs/2404.05192 ,  3806kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05219
Date: Mon, 8 Apr 2024 06:27:38 GMT   (22779kb,D)

Title: Out-of-Distribution Data: An Acquaintance of Adversarial Examples -- A
  Survey
Authors: Naveen Karunanayake, Ravin Gunawardena, Suranga Seneviratne, Sanjay
  Chawla
Categories: cs.LG
\\
  Deep neural networks (DNNs) deployed in real-world applications can encounter
out-of-distribution (OOD) data and adversarial examples. These represent
distinct forms of distributional shifts that can significantly impact DNNs'
reliability and robustness. Traditionally, research has addressed OOD detection
and adversarial robustness as separate challenges. This survey focuses on the
intersection of these two areas, examining how the research community has
investigated them together. Consequently, we identify two key research
directions: robust OOD detection and unified robustness. Robust OOD detection
aims to differentiate between in-distribution (ID) data and OOD data, even when
they are adversarially manipulated to deceive the OOD detector. Unified
robustness seeks a single approach to make DNNs robust against both adversarial
attacks and OOD inputs. Accordingly, first, we establish a taxonomy based on
the concept of distributional shifts. This framework clarifies how robust OOD
detection and unified robustness relate to other research areas addressing
distributional shifts, such as OOD detection, open set recognition, and anomaly
detection. Subsequently, we review existing work on robust OOD detection and
unified robustness. Finally, we highlight the limitations of the existing work
and propose promising research directions that explore adversarial and OOD
inputs within a unified framework.
\\ ( https://arxiv.org/abs/2404.05219 ,  22779kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05229
Date: Mon, 8 Apr 2024 06:49:59 GMT   (3921kb,D)

Title: Empirical Upscaling of Point-scale Soil Moisture Measurements for
  Spatial Evaluation of Model Simulations and Satellite Retrievals
Authors: Yi Yu, Brendan P. Malone and Luigi J. Renzullo
Categories: cs.LG
Comments: Accepted and selected as the Student Paper Competition finalists at
  the 2024 IEEE International Geoscience and Remote Sensing Symposium (IGARSS
  2024)
\\
  The evaluation of modelled or satellite-derived soil moisture (SM) estimates
is usually dependent on comparisons against in-situ SM measurements. However,
the inherent mismatch in spatial support (i.e., scale) necessitates a cautious
interpretation of point-to-pixel comparisons. The upscaling of the in-situ
measurements to a commensurate resolution to that of the modelled or retrieved
SM will lead to a fairer comparison and statistically more defensible
evaluation. In this study, we presented an upscaling approach that combines
spatiotemporal fusion with machine learning to extrapolate point-scale SM
measurements from 28 in-situ sites to a 100 m resolution for an agricultural
area of 100 km by 100 km. We conducted a four-fold cross-validation, which
consistently demonstrated comparable correlation performance across folds,
ranging from 0.6 to 0.9. The proposed approach was further validated based on a
cross-cluster strategy by using two spatial subsets within the study area,
denoted as cluster A and B, each of which equally comprised of 12 in-situ
sites. The cross-cluster validation underscored the capability of the upscaling
approach to map the spatial variability of SM within areas that were not
covered by in-situ sites, with correlation performance ranging between 0.6 and
0.8. In general, our proposed upscaling approach offers an avenue to
extrapolate point measurements of SM to a spatial scale more akin to climatic
model grids or remotely sensed observations. Future investigations should delve
into a further evaluation of the upscaling approach using independent data,
such as model simulations, satellite retrievals or field campaign data.
\\ ( https://arxiv.org/abs/2404.05229 ,  3921kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05241
Date: Mon, 8 Apr 2024 07:11:33 GMT   (315kb,D)

Title: Lightweight Inference for Forward-Forward Training Algorithm
Authors: Amin Aminifar, Baichuan Huang, Azra Abtahi, Amir Aminifar
Categories: cs.LG
\\
  The human brain performs tasks with an outstanding energy-efficiency, i.e.,
with approximately 20 Watts. The state-of-the-art Artificial/Deep Neural
Networks (ANN/DNN), on the other hand, have recently been shown to consume
massive amounts of energy. The training of these ANNs/DNNs is done almost
exclusively based on the back-propagation algorithm, which is known to be
biologically implausible. This has led to a new generation of forward-only
techniques, including the Forward-Forward algorithm. In this paper, we propose
a lightweight inference scheme specifically designed for DNNs trained using the
Forward-Forward algorithm. We have evaluated our proposed lightweight inference
scheme in the case of the MNIST and CIFAR datasets, as well as two real-world
applications, namely, epileptic seizure detection and cardiac arrhythmia
classification using wearable technologies, where complexity overheads/energy
consumption is a major constraint, and demonstrate its relevance.
\\ ( https://arxiv.org/abs/2404.05241 ,  315kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05311
Date: Mon, 8 Apr 2024 08:59:26 GMT   (31424kb,D)

Title: BruSLeAttack: A Query-Efficient Score-Based Black-Box Sparse Adversarial
  Attack
Authors: Viet Quoc Vo, Ehsan Abbasnejad, Damith C. Ranasinghe
Categories: cs.LG
Comments: Published as a conference paper at the International Conference on
  Learning Representations (ICLR 2024). Code is available at
  https://brusliattack.github.io/
\\
  We study the unique, less-well understood problem of generating sparse
adversarial samples simply by observing the score-based replies to model
queries. Sparse attacks aim to discover a minimum number-the l0
bounded-perturbations to model inputs to craft adversarial examples and
misguide model decisions. But, in contrast to query-based dense attack
counterparts against black-box models, constructing sparse adversarial
perturbations, even when models serve confidence score information to queries
in a score-based setting, is non-trivial. Because, such an attack leads to i)
an NP-hard problem; and ii) a non-differentiable search space. We develop the
BruSLeAttack-a new, faster (more query-efficient) Bayesian algorithm for the
problem. We conduct extensive attack evaluations including an attack
demonstration against a Machine Learning as a Service (MLaaS) offering
exemplified by Google Cloud Vision and robustness testing of adversarial
training regimes and a recent defense against black-box attacks. The proposed
attack scales to achieve state-of-the-art attack success rates and query
efficiency on standard computer vision tasks such as ImageNet across different
model architectures. Our artefacts and DIY attack samples are available on
GitHub. Importantly, our work facilitates faster evaluation of model
vulnerabilities and raises our vigilance on the safety, security and
reliability of deployed systems.
\\ ( https://arxiv.org/abs/2404.05311 ,  31424kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05316
Date: Mon, 8 Apr 2024 09:06:16 GMT   (525kb,D)

Title: HOEG: A New Approach for Object-Centric Predictive Process Monitoring
Authors: Tim K. Smit, Hajo A. Reijers, Xixi Lu
Categories: cs.LG
Comments: accepted to 36th International Conference on Advanced Information
  Systems Engineering (CAISE), 2024
\\
  Predictive Process Monitoring focuses on predicting future states of ongoing
process executions, such as forecasting the remaining time. Recent developments
in Object-Centric Process Mining have enriched event data with objects and
their explicit relations between events. To leverage this enriched data, we
propose the Heterogeneous Object Event Graph encoding (HOEG), which integrates
events and objects into a graph structure with diverse node types. It does so
without aggregating object features, thus creating a more nuanced and
informative representation. We then adopt a heterogeneous Graph Neural Network
architecture, which incorporates these diverse object features in prediction
tasks. We evaluate the performance and scalability of HOEG in predicting
remaining time, benchmarking it against two established graph-based encodings
and two baseline models. Our evaluation uses three Object-Centric Event Logs
(OCELs), including one from a real-life process at a major Dutch financial
institution. The results indicate that HOEG competes well with existing models
and surpasses them when OCELs contain informative object attributes and
event-object interactions.
\\ ( https://arxiv.org/abs/2404.05316 ,  525kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05318
Date: Mon, 8 Apr 2024 09:08:59 GMT   (2410kb,D)

Title: Stochastic Online Optimization for Cyber-Physical and Robotic Systems
Authors: Hao Ma, Melanie Zeilinger, Michael Muehlebach
Categories: cs.LG cs.RO
Comments: 46 pages, 16 figures
\\
  We propose a novel gradient-based online optimization framework for solving
stochastic programming problems that frequently arise in the context of
cyber-physical and robotic systems. Our problem formulation accommodates
constraints that model the evolution of a cyber-physical system, which has, in
general, a continuous state and action space, is nonlinear, and where the state
is only partially observed. We also incorporate an approximate model of the
dynamics as prior knowledge into the learning process and show that even rough
estimates of the dynamics can significantly improve the convergence of our
algorithms. Our online optimization framework encompasses both gradient descent
and quasi-Newton methods, and we provide a unified convergence analysis of our
algorithms in a non-convex setting. We also characterize the impact of modeling
errors in the system dynamics on the convergence rate of the algorithms.
Finally, we evaluate our algorithms in simulations of a flexible beam, a
four-legged walking robot, and in real-world experiments with a ping-pong
playing robot.
\\ ( https://arxiv.org/abs/2404.05318 ,  2410kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05324
Date: Mon, 8 Apr 2024 09:13:16 GMT   (3242kb,D)

Title: Back to the Future: GNN-based NO2 Forecasting via Future Covariates
Authors: Antonio Giganti, Sara Mandelli, Paolo Bestagini, Umberto Giuriato,
  Alessandro D'Ausilio, Marco Marcon, Stefano Tubaro
Categories: cs.LG cs.AI eess.SP
Comments: 5 pages, 4 figures, 1 table, accepted at IEEE-IGARSS 2024
\\
  Due to the latest environmental concerns in keeping at bay contaminants
emissions in urban areas, air pollution forecasting has been rising the
forefront of all researchers around the world. When predicting pollutant
concentrations, it is common to include the effects of environmental factors
that influence these concentrations within an extended period, like traffic,
meteorological conditions and geographical information. Most of the existing
approaches exploit this information as past covariates, i.e., past exogenous
variables that affected the pollutant but were not affected by it. In this
paper, we present a novel forecasting methodology to predict NO2 concentration
via both past and future covariates. Future covariates are represented by
weather forecasts and future calendar events, which are already known at
prediction time. In particular, we deal with air quality observations in a
city-wide network of ground monitoring stations, modeling the data structure
and estimating the predictions with a Spatiotemporal Graph Neural Network
(STGNN). We propose a conditioning block that embeds past and future covariates
into the current observations. After extracting meaningful spatiotemporal
representations, these are fused together and projected into the forecasting
horizon to generate the final prediction. To the best of our knowledge, it is
the first time that future covariates are included in time series predictions
in a structured way. Remarkably, we find that conditioning on future weather
information has a greater impact than considering past traffic conditions. We
release our code implementation at https://github.com/polimi-ispl/MAGCRN.
\\ ( https://arxiv.org/abs/2404.05324 ,  3242kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05350
Date: Mon, 8 Apr 2024 09:38:22 GMT   (1859kb,D)

Title: Certified PEFTSmoothing: Parameter-Efficient Fine-Tuning with Randomized
  Smoothing
Authors: Chengyan Fu and Wenjie Wang
Categories: cs.LG cs.CR
\\
  Randomized smoothing is the primary certified robustness method for accessing
the robustness of deep learning models to adversarial perturbations in the
l2-norm, by adding isotropic Gaussian noise to the input image and returning
the majority votes over the base classifier. Theoretically, it provides a
certified norm bound, ensuring predictions of adversarial examples are stable
within this bound. A notable constraint limiting widespread adoption is the
necessity to retrain base models entirely from scratch to attain a robust
version. This is because the base model fails to learn the noise-augmented data
distribution to give an accurate vote. One intuitive way to overcome this
challenge is to involve a custom-trained denoiser to eliminate the noise.
However, this approach is inefficient and sub-optimal. Inspired by recent large
model training procedures, we explore an alternative way named PEFTSmoothing to
adapt the base model to learn the Gaussian noise-augmented data with
Parameter-Efficient Fine-Tuning (PEFT) methods in both white-box and black-box
settings. Extensive results demonstrate the effectiveness and efficiency of
PEFTSmoothing, which allow us to certify over 98% accuracy for ViT on CIFAR-10,
20% higher than SoTA denoised smoothing, and over 61% accuracy on ImageNet
which is 30% higher than CNN-based denoiser and comparable to the
Diffusion-based denoiser.
\\ ( https://arxiv.org/abs/2404.05350 ,  1859kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05363
Date: Mon, 8 Apr 2024 09:57:02 GMT   (2452kb,D)

Title: A parameter-free clustering algorithm for missing datasets
Authors: Qi Li, Xianjun Zeng, Shuliang Wang, Wenhao Zhu, Shijie Ruan, Zhimeng
  Yuan
Categories: cs.LG
\\
  Missing datasets, in which some objects have missing values in certain
dimensions, are prevalent in the Real-world. Existing clustering algorithms for
missing datasets first impute the missing values and then perform clustering.
However, both the imputation and clustering processes require input parameters.
Too many input parameters inevitably increase the difficulty of obtaining
accurate clustering results. Although some studies have shown that decision
graphs can replace the input parameters of clustering algorithms, current
decision graphs require equivalent dimensions among objects and are therefore
not suitable for missing datasets. To this end, we propose a Single-Dimensional
Clustering algorithm, i.e., SDC. SDC, which removes the imputation process and
adapts the decision graph to the missing datasets by splitting dimension and
partition intersection fusion, can obtain valid clustering results on the
missing datasets without input parameters. Experiments demonstrate that, across
three evaluation metrics, SDC outperforms baseline algorithms by at least
13.7%(NMI), 23.8%(ARI), and 8.1%(Purity).
\\ ( https://arxiv.org/abs/2404.05363 ,  2452kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05482
Date: Mon, 8 Apr 2024 13:01:25 GMT   (1007kb,D)

Title: WaveCatBoost for Probabilistic Forecasting of Regional Air Quality Data
Authors: Jintu Borah, Tanujit Chakraborty, Md. Shahrul Md. Nadzir, Mylene G.
  Cayetano, Shubhankar Majumdar
Categories: cs.LG
\\
  Accurate and reliable air quality forecasting is essential for protecting
public health, sustainable development, pollution control, and enhanced urban
planning. This letter presents a novel WaveCatBoost architecture designed to
forecast the real-time concentrations of air pollutants by combining the
maximal overlapping discrete wavelet transform (MODWT) with the CatBoost model.
This hybrid approach efficiently transforms time series into high-frequency and
low-frequency components, thereby extracting signal from noise and improving
prediction accuracy and robustness. Evaluation of two distinct regional
datasets, from the Central Air Pollution Control Board (CPCB) sensor network
and a low-cost air quality sensor system (LAQS), underscores the superior
performance of our proposed methodology in real-time forecasting compared to
the state-of-the-art statistical and deep learning architectures. Moreover, we
employ a conformal prediction strategy to provide probabilistic bands with our
forecasts.
\\ ( https://arxiv.org/abs/2404.05482 ,  1007kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05484
Date: Mon, 8 Apr 2024 13:06:23 GMT   (851kb,D)

Title: Tangling-Untangling Cycle for Efficient Learning
Authors: Xin Li
Categories: cs.LG
\\
  The conventional wisdom of manifold learning is based on nonlinear
dimensionality reduction techniques such as IsoMAP and locally linear embedding
(LLE). We challenge this paradigm by exploiting the blessing of dimensionality.
Our intuition is simple: it is easier to untangle a low-dimensional manifold in
a higher-dimensional space due to its vastness, as guaranteed by Whitney
embedding theorem. A new insight brought by this work is to introduce class
labels as the context variables in the lifted higher-dimensional space (so
supervised learning becomes unsupervised learning). We rigorously show that
manifold untangling leads to linearly separable classifiers in the lifted
space. To correct the inevitable overfitting, we consider the dual process of
manifold untangling -- tangling or aliasing -- which is important for
generalization. Using context as the bonding element, we construct a pair of
manifold untangling and tangling operators, known as tangling-untangling cycle
(TUC). Untangling operator maps context-independent representations (CIR) in
low-dimensional space to context-dependent representations (CDR) in
high-dimensional space by inducing context as hidden variables. The tangling
operator maps CDR back to CIR by a simple integral transformation for
invariance and generalization. We also present the hierarchical extensions of
TUC based on the Cartesian product and the fractal geometry. Despite the
conceptual simplicity, TUC admits a biologically plausible and energy-efficient
implementation based on the time-locking behavior of polychronization neural
groups (PNG) and sleep-wake cycle (SWC). The TUC-based theory applies to the
computational modeling of various cognitive functions by
hippocampal-neocortical systems.
\\ ( https://arxiv.org/abs/2404.05484 ,  851kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05545
Date: Mon, 8 Apr 2024 14:15:56 GMT   (220kb,D)

Title: Evaluating Interventional Reasoning Capabilities of Large Language
  Models
Authors: Tejas Kasetty, Divyat Mahajan, Gintare Karolina Dziugaite, Alexandre
  Drouin, Dhanya Sridhar
Categories: cs.LG cs.AI cs.CL stat.ME
Comments: 17 pages
\\
  Numerous decision-making tasks require estimating causal effects under
interventions on different parts of a system. As practitioners consider using
large language models (LLMs) to automate decisions, studying their causal
reasoning capabilities becomes crucial. A recent line of work evaluates LLMs
ability to retrieve commonsense causal facts, but these evaluations do not
sufficiently assess how LLMs reason about interventions. Motivated by the role
that interventions play in causal inference, in this paper, we conduct
empirical analyses to evaluate whether LLMs can accurately update their
knowledge of a data-generating process in response to an intervention. We
create benchmarks that span diverse causal graphs (e.g., confounding,
mediation) and variable types, and enable a study of intervention-based
reasoning. These benchmarks allow us to isolate the ability of LLMs to
accurately predict changes resulting from their ability to memorize facts or
find other shortcuts. Our analysis on four LLMs highlights that while GPT- 4
models show promising accuracy at predicting the intervention effects, they
remain sensitive to distracting factors in the prompts.
\\ ( https://arxiv.org/abs/2404.05545 ,  220kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05555
Date: Mon, 8 Apr 2024 14:28:27 GMT   (1791kb,D)

Title: On the Convergence of Continual Learning with Adaptive Methods
Authors: Seungyub Han, Yeongmo Kim, Taehyun Cho, Jungwoo Lee
Categories: cs.LG cs.AI stat.ML
Comments: Seungyub Han, Yeongmo Kim, Taehyun Cho, Jungwoo Lee Proceedings of
  the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence 2023
  https://proceedings.mlr.press/v216/han23a.html
Journal-ref: PMLR 216:809-818, 2023
\\
  One of the objectives of continual learning is to prevent catastrophic
forgetting in learning multiple tasks sequentially, and the existing solutions
have been driven by the conceptualization of the plasticity-stability dilemma.
However, the convergence of continual learning for each sequential task is less
studied so far. In this paper, we provide a convergence analysis of
memory-based continual learning with stochastic gradient descent and empirical
evidence that training current tasks causes the cumulative degradation of
previous tasks. We propose an adaptive method for nonconvex continual learning
(NCCL), which adjusts step sizes of both previous and current tasks with the
gradients. The proposed method can achieve the same convergence rate as the SGD
method when the catastrophic forgetting term which we define in the paper is
suppressed at each iteration. Further, we demonstrate that the proposed
algorithm improves the performance of continual learning over existing methods
for several image classification tasks.
\\ ( https://arxiv.org/abs/2404.05555 ,  1791kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05567
Date: Mon, 8 Apr 2024 14:39:49 GMT   (404kb,D)

Title: Dense Training, Sparse Inference: Rethinking Training of
  Mixture-of-Experts Language Models
Authors: Bowen Pan, Yikang Shen, Haokun Liu, Mayank Mishra, Gaoyuan Zhang, Aude
  Oliva, Colin Raffel, Rameswar Panda
Categories: cs.LG cs.AI cs.CL
\\
  Mixture-of-Experts (MoE) language models can reduce computational costs by
2-4$\times$ compared to dense models without sacrificing performance, making
them more efficient in computation-bounded scenarios. However, MoE models
generally require 2-4$\times$ times more parameters to achieve comparable
performance to a dense model, which incurs larger GPU memory requirements and
makes MoE models less efficient in I/O-bounded scenarios like autoregressive
generation. In this work, we propose a hybrid dense training and sparse
inference framework for MoE models (DS-MoE) which achieves strong computation
and parameter efficiency by employing dense computation across all experts
during training and sparse computation during inference. Our experiments on
training LLMs demonstrate that our DS-MoE models are more parameter-efficient
than standard sparse MoEs and are on par with dense models in terms of total
parameter size and performance while being computationally cheaper (activating
30-40% of the model's parameters). Performance tests using vLLM show that our
DS-MoE-6B model runs up to $1.86\times$ faster than similar dense models like
Mistral-7B, and between $1.50\times$ and $1.71\times$ faster than comparable
MoEs, such as DeepSeekMoE-16B and Qwen1.5-MoE-A2.7B.
\\ ( https://arxiv.org/abs/2404.05567 ,  404kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05576
Date: Mon, 8 Apr 2024 14:52:48 GMT   (1828kb,D)

Title: Dynamic Backtracking in GFlowNet: Enhancing Decision Steps with
  Reward-Dependent Adjustment Mechanisms
Authors: Shuai Guo, Jielei Chu, Lei Zhu, Tianrui Li
Categories: cs.LG
\\
  Generative Flow Networks (GFlowNets) are probabilistic models predicated on
Markov flows, employing specific amortization algorithms to learn stochastic
policies that generate compositional substances including biomolecules,
chemical materials, and more. Demonstrating formidable prowess in generating
high-performance biochemical molecules, GFlowNets accelerate the discovery of
scientific substances, effectively circumventing the time-consuming,
labor-intensive, and costly shortcomings intrinsic to conventional material
discovery. However, previous work often struggles to accumulate exploratory
experience and is prone to becoming disoriented within expansive sampling
spaces. Attempts to address this issue, such as LS-GFN, are limited to local
greedy searches and lack broader global adjustments. This paper introduces a
novel GFlowNet variant, the Dynamic Backtracking GFN (DB-GFN), which enhances
the adaptability of decision-making steps through a reward-based dynamic
backtracking mechanism. DB-GFN permits backtracking during the network
construction process according to the current state's reward value, thus
correcting disadvantageous decisions and exploring alternative pathways during
the exploration process. Applied to generative tasks of biochemical molecules
and genetic material sequences, DB-GFN surpasses existing GFlowNet models and
traditional reinforcement learning methods in terms of sample quality,
exploration sample quantity, and training convergence speed. Furthermore, the
orthogonal nature of DB-GFN suggests its potential as a powerful tool for
future improvements in GFN networks, with the promise of integrating with other
strategies to achieve more efficient search performance.
\\ ( https://arxiv.org/abs/2404.05576 ,  1828kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05579
Date: Mon, 8 Apr 2024 14:55:35 GMT   (33561kb,D)

Title: Robust Data Pruning: Uncovering and Overcoming Implicit Bias
Authors: Artem Vysogorets, Kartik Ahuja, Julia Kempe
Categories: cs.LG cs.CV
\\
  In the era of exceptionally data-hungry models, careful selection of the
training data is essential to mitigate the extensive costs of deep learning.
Data pruning offers a solution by removing redundant or uninformative samples
from the dataset, which yields faster convergence and improved neural scaling
laws. However, little is known about its impact on classification bias of the
trained models. We conduct the first systematic study of this effect and reveal
that existing data pruning algorithms can produce highly biased classifiers. At
the same time, we argue that random data pruning with appropriate class ratios
has potential to improve the worst-class performance. We propose a
"fairness-aware" approach to pruning and empirically demonstrate its
performance on standard computer vision benchmarks. In sharp contrast to
existing algorithms, our proposed method continues improving robustness at a
tolerable drop of average performance as we prune more from the datasets. We
present theoretical analysis of the classification risk in a mixture of
Gaussians to further motivate our algorithm and support our findings.
\\ ( https://arxiv.org/abs/2404.05579 ,  33561kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05604
Date: Mon, 8 Apr 2024 15:24:20 GMT   (4772kb,D)

Title: Technical Report: The Graph Spectral Token -- Enhancing Graph
  Transformers with Spectral Information
Authors: Zihan Pengmei, Zimu Li
Categories: cs.LG
Comments: Technical Report. The code is available at
  https://github.com/zpengmei/SubFormer-Spec
\\
  Graph Transformers have emerged as a powerful alternative to Message-Passing
Graph Neural Networks (MP-GNNs) to address limitations such as over-squashing
of information exchange. However, incorporating graph inductive bias into
transformer architectures remains a significant challenge. In this report, we
propose the Graph Spectral Token, a novel approach to directly encode graph
spectral information, which captures the global structure of the graph, into
the transformer architecture. By parameterizing the auxiliary [CLS] token and
leaving other tokens representing graph nodes, our method seamlessly integrates
spectral information into the learning process. We benchmark the effectiveness
of our approach by enhancing two existing graph transformers, GraphTrans and
SubFormer. The improved GraphTrans, dubbed GraphTrans-Spec, achieves over 10%
improvements on large graph benchmark datasets while maintaining efficiency
comparable to MP-GNNs. SubFormer-Spec demonstrates strong performance across
various datasets.
\\ ( https://arxiv.org/abs/2404.05604 ,  4772kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05605
Date: Mon, 8 Apr 2024 15:25:25 GMT   (343kb,D)

Title: Graph Neural Networks Automated Design and Deployment on Device-Edge
  Co-Inference Systems
Authors: Ao Zhou, Jianlei Yang, Tong Qiao, Yingjie Qi, Zhi Yang, Weisheng Zhao,
  Chunming Hu
Categories: cs.LG cs.AI
Comments: Accepted by DAC'24
\\
  The key to device-edge co-inference paradigm is to partition models into
computation-friendly and computation-intensive parts across the device and the
edge, respectively. However, for Graph Neural Networks (GNNs), we find that
simply partitioning without altering their structures can hardly achieve the
full potential of the co-inference paradigm due to various
computational-communication overheads of GNN operations over heterogeneous
devices. We present GCoDE, the first automatic framework for GNN that
innovatively Co-designs the architecture search and the mapping of each
operation on Device-Edge hierarchies. GCoDE abstracts the device communication
process into an explicit operation and fuses the search of architecture and the
operations mapping in a unified space for joint-optimization. Also, the
performance-awareness approach, utilized in the constraint-based search process
of GCoDE, enables effective evaluation of architecture efficiency in diverse
heterogeneous systems. We implement the co-inference engine and runtime
dispatcher in GCoDE to enhance the deployment efficiency. Experimental results
show that GCoDE can achieve up to $44.9\times$ speedup and $98.2\%$ energy
reduction compared to existing approaches across various applications and
system configurations.
\\ ( https://arxiv.org/abs/2404.05605 ,  343kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05613
Date: Mon, 8 Apr 2024 15:40:22 GMT   (640kb,D)

Title: Deep Representation Learning for Multi-functional Degradation Modeling
  of Community-dwelling Aging Population
Authors: Suiyao Chen, Xinyi Liu, Yulei Li, Jing Wu, Handong Yao
Categories: cs.LG cs.AI
\\
  As the aging population grows, particularly for the baby boomer generation,
the United States is witnessing a significant increase in the elderly
population experiencing multifunctional disabilities. These disabilities,
stemming from a variety of chronic diseases, injuries, and impairments, present
a complex challenge due to their multidimensional nature, encompassing both
physical and cognitive aspects. Traditional methods often use univariate
regression-based methods to model and predict single degradation conditions and
assume population homogeneity, which is inadequate to address the complexity
and diversity of aging-related degradation. This study introduces a novel
framework for multi-functional degradation modeling that captures the
multidimensional (e.g., physical and cognitive) and heterogeneous nature of
elderly disabilities. Utilizing deep learning, our approach predicts health
degradation scores and uncovers latent heterogeneity from elderly health
histories, offering both efficient estimation and explainable insights into the
diverse effects and causes of aging-related degradation. A real-case study
demonstrates the effectiveness and marks a pivotal contribution to accurately
modeling the intricate dynamics of elderly degradation, and addresses the
healthcare challenges in the aging population.
\\ ( https://arxiv.org/abs/2404.05613 ,  640kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05623
Date: Mon, 8 Apr 2024 15:53:46 GMT   (1831kb,D)

Title: AnchorAL: Computationally Efficient Active Learning for Large and
  Imbalanced Datasets
Authors: Pietro Lesci and Andreas Vlachos
Categories: cs.LG cs.CL
Comments: Published at the NAACL 2024 Conference (main)
\\
  Active learning for imbalanced classification tasks is challenging as the
minority classes naturally occur rarely. Gathering a large pool of unlabelled
data is thus essential to capture minority instances. Standard pool-based
active learning is computationally expensive on large pools and often reaches
low accuracy by overfitting the initial decision boundary, thus failing to
explore the input space and find minority instances. To address these issues we
propose AnchorAL. At each iteration, AnchorAL chooses class-specific instances
from the labelled set, or anchors, and retrieves the most similar unlabelled
instances from the pool. This resulting subpool is then used for active
learning. Using a small, fixed-sized subpool AnchorAL allows scaling any active
learning strategy to large pools. By dynamically selecting different anchors at
each iteration it promotes class balance and prevents overfitting the initial
decision boundary, thus promoting the discovery of new clusters of minority
instances. Experiments across different classification tasks, active learning
strategies, and model architectures AnchorAL is (i) faster, often reducing
runtime from hours to minutes, (ii) trains more performant models, (iii) and
returns more balanced datasets than competing methods.
\\ ( https://arxiv.org/abs/2404.05623 ,  1831kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05639
Date: Mon, 8 Apr 2024 16:20:15 GMT   (483kb,D)

Title: Investigating the Impact of Quantization on Adversarial Robustness
Authors: Qun Li, Yuan Meng, Chen Tang, Jiacheng Jiang, Zhi Wang
Categories: cs.LG cs.AI
Comments: Accepted to ICLR 2024 Workshop PML4LRS
\\
  Quantization is a promising technique for reducing the bit-width of deep
models to improve their runtime performance and storage efficiency, and thus
becomes a fundamental step for deployment. In real-world scenarios, quantized
models are often faced with adversarial attacks which cause the model to make
incorrect inferences by introducing slight perturbations. However, recent
studies have paid less attention to the impact of quantization on the model
robustness. More surprisingly, existing studies on this topic even present
inconsistent conclusions, which prompted our in-depth investigation. In this
paper, we conduct a first-time analysis of the impact of the quantization
pipeline components that can incorporate robust optimization under the settings
of Post-Training Quantization and Quantization-Aware Training. Through our
detailed analysis, we discovered that this inconsistency arises from the use of
different pipelines in different studies, specifically regarding whether robust
optimization is performed and at which quantization stage it occurs. Our
research findings contribute insights into deploying more secure and robust
quantized networks, assisting practitioners in reference for scenarios with
high-security requirements and limited resources.
\\ ( https://arxiv.org/abs/2404.05639 ,  483kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05688
Date: Mon, 8 Apr 2024 17:14:32 GMT   (242kb,D)

Title: David and Goliath: An Empirical Evaluation of Attacks and Defenses for
  QNNs at the Deep Edge
Authors: Miguel Costa and Sandro Pinto
Categories: cs.LG cs.AI
ACM-class: I.2.0
\\
  ML is shifting from the cloud to the edge. Edge computing reduces the surface
exposing private data and enables reliable throughput guarantees in real-time
applications. Of the panoply of devices deployed at the edge,
resource-constrained MCUs, e.g., Arm Cortex-M, are more prevalent, orders of
magnitude cheaper, and less power-hungry than application processors or GPUs.
Thus, enabling intelligence at the deep edge is the zeitgeist, with researchers
focusing on unveiling novel approaches to deploy ANNs on these constrained
devices. Quantization is a well-established technique that has proved effective
in enabling the deployment of neural networks on MCUs; however, it is still an
open question to understand the robustness of QNNs in the face of adversarial
examples.
  To fill this gap, we empirically evaluate the effectiveness of attacks and
defenses from (full-precision) ANNs on (constrained) QNNs. Our evaluation
includes three QNNs targeting TinyML applications, ten attacks, and six
defenses. With this study, we draw a set of interesting findings. First,
quantization increases the point distance to the decision boundary and leads
the gradient estimated by some attacks to explode or vanish. Second,
quantization can act as a noise attenuator or amplifier, depending on the noise
magnitude, and causes gradient misalignment. Regarding adversarial defenses, we
conclude that input pre-processing defenses show impressive results on small
perturbations; however, they fall short as the perturbation increases. At the
same time, train-based defenses increase the average point distance to the
decision boundary, which holds after quantization. However, we argue that
train-based defenses still need to smooth the quantization-shift and gradient
misalignment phenomenons to counteract adversarial example transferability to
QNNs. All artifacts are open-sourced to enable independent validation of
results.
\\ ( https://arxiv.org/abs/2404.05688 ,  242kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05689
Date: Mon, 8 Apr 2024 17:15:37 GMT   (3099kb,D)

Title: Automated discovery of symbolic laws governing skill acquisition from
  naturally occurring data
Authors: Sannyuya Liu, Qing Li, Xiaoxuan Shen, Jianwen Sun, Zongkai Yang
Categories: cs.LG cs.AI
\\
  Skill acquisition is a key area of research in cognitive psychology as it
encompasses multiple psychological processes. The laws discovered under
experimental paradigms are controversial and lack generalizability. This paper
aims to unearth the laws of skill learning from large-scale training log data.
A two-stage algorithm was developed to tackle the issues of unobservable
cognitive states and algorithmic explosion in searching. Initially a deep
learning model is employed to determine the learner's cognitive state and
assess the feature importance. Subsequently, symbolic regression algorithms are
utilized to parse the neural network model into algebraic equations. The
experimental results of simulated data demonstrate that the proposed algorithm
can accurately restore various preset laws within a certain range of noise, in
continues feedback setting. Application of proposed method to Lumosity training
data demonstrates superior performance compared to traditional and latest
models in terms of fitness. The results indicate the discovery of two new forms
of skill acquisition laws, while some previous findings have been reaffirmed.
\\ ( https://arxiv.org/abs/2404.05689 ,  3099kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05723
Date: Mon, 8 Apr 2024 17:58:22 GMT   (9652kb)

Title: Predicting Overtakes in Trucks Using CAN Data
Authors: Talha Hanif Butt and Prayag Tiwari and Fernando Alonso-Fernandez
Categories: cs.LG
\\
  Safe overtakes in trucks are crucial to prevent accidents, reduce congestion,
and ensure efficient traffic flow, making early prediction essential for timely
and informed driving decisions. Accordingly, we investigate the detection of
truck overtakes from CAN data. Three classifiers, Artificial Neural Networks
(ANN), Random Forest, and Support Vector Machines (SVM), are employed for the
task. Our analysis covers up to 10 seconds before the overtaking event, using
an overlapping sliding window of 1 second to extract CAN features. We observe
that the prediction scores of the overtake class tend to increase as we
approach the overtake trigger, while the no-overtake class remain stable or
oscillates depending on the classifier. Thus, the best accuracy is achieved
when approaching the trigger, making early overtaking prediction challenging.
The classifiers show good accuracy in classifying overtakes (Recall/TPR > 93%),
but accuracy is suboptimal in classifying no-overtakes (TNR typically 80-90%
and below 60% for one SVM variant). We further combine two classifiers (Random
Forest and linear SVM) by averaging their output scores. The fusion is observed
to improve no-overtake classification (TNR > 92%) at the expense of reducing
overtake accuracy (TPR). However, the latter is kept above 91% near the
overtake trigger. Therefore, the fusion balances TPR and TNR, providing more
consistent performance than individual classifiers.
\\ ( https://arxiv.org/abs/2404.05723 ,  9652kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05728
Date: Mon, 8 Apr 2024 17:59:44 GMT   (70kb)

Title: A Large-Scale Exploration of $\mu$-Transfer
Authors: Lucas Lingle
Categories: cs.LG
Comments: 9 pages
\\
  Large neural network models have become a mainstay of natural language
processing and computer vision, yet their initialization and learning rates are
set in a largely heuristic fashion, potentially varying from paper to paper and
one model size to the next. The $\mu$-Parameterization ($\mu$P) offers a
potential solution to these challenges, yielding scaling rules for model
initialization and learning rates, and reportedly enabling zero-shot
hyperparameter transfer from small to large models in a variety of cases.
  Despite the evident promise, the $\mu$P scaling rules are not yet widely
adopted, perhaps due to higher implementation complexity, many variations, or
complex theoretical background. This work investigates $\mu$P empirically,
focusing on the ubiquitous transformer architecture, and aims to answer a
simple question: does $\mu$-Transfer yield optimal learning rates in practice?
From models with 2M to 10B parameters, we show that $\mu$-Transfer works as
intended for the majority of important cases, but also identify some surprising
cases where it may not.
\\ ( https://arxiv.org/abs/2404.05728 ,  70kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2404.04264 (*cross-listing*)
Date: Sun, 17 Mar 2024 17:01:45 GMT   (2599kb,D)

Title: Logic Query of Thoughts: Guiding Large Language Models to Answer Complex
  Logic Queries with Knowledge Graphs
Authors: Lihui Liu, Zihao Wang, Ruizhong Qiu, Yikun Ban, Hanghang Tong
Categories: cs.IR cs.AI
\\
  Despite the superb performance in many tasks, large language models (LLMs)
bear the risk of generating hallucination or even wrong answers when confronted
with tasks that demand the accuracy of knowledge. The issue becomes even more
noticeable when addressing logic queries that require multiple logic reasoning
steps. On the other hand, knowledge graph (KG) based question answering methods
are capable of accurately identifying the correct answers with the help of
knowledge graph, yet its accuracy could quickly deteriorate when the knowledge
graph itself is sparse and incomplete. It remains a critical challenge on how
to integrate knowledge graph reasoning with LLMs in a mutually beneficial way
so as to mitigate both the hallucination problem of LLMs as well as the
incompleteness issue of knowledge graphs. In this paper, we propose
'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs
with knowledge graph based logic query reasoning. LGOT seamlessly combines
knowledge graph reasoning and LLMs, effectively breaking down complex logic
queries into easy to answer subquestions. Through the utilization of both
knowledge graph reasoning and LLMs, it successfully derives answers for each
subquestion. By aggregating these results and selecting the highest quality
candidate answers for each step, LGOT achieves accurate results to complex
questions. Our experimental findings demonstrate substantial performance
enhancements, with up to 20% improvement over ChatGPT.
\\ ( https://arxiv.org/abs/2404.04264 ,  2599kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04268 (*cross-listing*)
Date: Tue, 19 Mar 2024 18:17:46 GMT   (1284kb)

Title: The Use of Generative Search Engines for Knowledge Work and Complex
  Tasks
Authors: Siddharth Suri, Scott Counts, Leijie Wang, Chacha Chen, Mengting Wan,
  Tara Safavi, Jennifer Neville, Chirag Shah, Ryen W. White, Reid Andersen,
  Georg Buscher, Sathish Manivannan, Nagu Rangan, Longqi Yang
Categories: cs.IR cs.AI cs.CY cs.SI
Comments: 32 pages, 3 figures, 4 tables
ACM-class: J.4
\\
  Until recently, search engines were the predominant method for people to
access online information. The recent emergence of large language models (LLMs)
has given machines new capabilities such as the ability to generate new digital
artifacts like text, images, code etc., resulting in a new tool, a generative
search engine, which combines the capabilities of LLMs with a traditional
search engine. Through the empirical analysis of Bing Copilot (Bing Chat), one
of the first publicly available generative search engines, we analyze the types
and complexity of tasks that people use Bing Copilot for compared to Bing
Search. Findings indicate that people use the generative search engine for more
knowledge work tasks that are higher in cognitive complexity than were commonly
done with a traditional search engine.
\\ ( https://arxiv.org/abs/2404.04268 ,  1284kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04271 (*cross-listing*)
Date: Fri, 22 Mar 2024 04:22:36 GMT   (12870kb,D)

Title: Towards Effective Next POI Prediction: Spatial and Semantic Augmentation
  with Remote Sensing Data
Authors: Nan Jiang, Haitao Yuan, Jianing Si, Minxiao Chen, Shangguang Wang
Categories: cs.IR cs.AI cs.DB
Comments: 12 pages, 11 figures, Accepted by ICDE 2024
\\
  The next point-of-interest (POI) prediction is a significant task in
location-based services, yet its complexity arises from the consolidation of
spatial and semantic intent. This fusion is subject to the influences of
historical preferences, prevailing location, and environmental factors, thereby
posing significant challenges. In addition, the uneven POI distribution further
complicates the next POI prediction procedure. To address these challenges, we
enrich input features and propose an effective deep-learning method within a
two-step prediction framework. Our method first incorporates remote sensing
data, capturing pivotal environmental context to enhance input features
regarding both location and semantics. Subsequently, we employ a region
quad-tree structure to integrate urban remote sensing, road network, and POI
distribution spaces, aiming to devise a more coherent graph representation
method for urban spatial. Leveraging this method, we construct the QR-P graph
for the user's historical trajectories to encapsulate historical travel
knowledge, thereby augmenting input features with comprehensive spatial and
semantic insights. We devise distinct embedding modules to encode these
features and employ an attention mechanism to fuse diverse encodings. In the
two-step prediction procedure, we initially identify potential spatial zones by
predicting user-preferred tiles, followed by pinpointing specific POIs of a
designated type within the projected tiles. Empirical findings from four
real-world location-based social network datasets underscore the remarkable
superiority of our proposed approach over competitive baseline methods.
\\ ( https://arxiv.org/abs/2404.04271 ,  12870kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04299 (*cross-listing*)
Date: Thu, 4 Apr 2024 20:53:30 GMT   (4362kb,D)

Title: GENEVIC: GENetic data Exploration and Visualization via Intelligent
  interactive Console
Authors: Anindita Nath (1), Savannah Mwesigwa (1), Yulin Dai (1), Xiaoqian
  Jiang (2) and Zhongming Zhao (1 and 3) ((1) Center for Precision Health,
  McWilliams School of Biomedical Informatics, UT Health Houston, TX (2)
  Department of Health Data Science and Artificial Intelligence, McWilliams
  School of Biomedical Informatics, UT Health Houston, TX, (3) MD Anderson
  Cancer Center, UTHealth Graduate School of Biomedical Sciences, Houston, TX)
Categories: q-bio.QM cs.AI
\\
  Summary: The vast generation of genetic data poses a significant challenge in
efficiently uncovering valuable knowledge. Introducing GENEVIC, an AI-driven
chat framework that tackles this challenge by bridging the gap between genetic
data generation and biomedical knowledge discovery. Leveraging generative AI,
notably ChatGPT, it serves as a biologist's 'copilot'. It automates the
analysis, retrieval, and visualization of customized domain-specific genetic
information, and integrates functionalities to generate protein interaction
networks, enrich gene sets, and search scientific literature from PubMed,
Google Scholar, and arXiv, making it a comprehensive tool for biomedical
research. In its pilot phase, GENEVIC is assessed using a curated database that
ranks genetic variants associated with Alzheimer's disease, schizophrenia, and
cognition, based on their effect weights from the Polygenic Score Catalog, thus
enabling researchers to prioritize genetic variants in complex diseases.
GENEVIC's operation is user-friendly, accessible without any specialized
training, secured by Azure OpenAI's HIPAA-compliant infrastructure, and
evaluated for its efficacy through real-time query testing. As a prototype,
GENEVIC is set to advance genetic research, enabling informed biomedical
decisions.
  Availability and implementation: GENEVIC is publicly accessible at
https://genevic-anath2024.streamlit.app. The underlying code is open-source and
available via GitHub at https://github.com/anath2110/GENEVIC.git.
\\ ( https://arxiv.org/abs/2404.04299 ,  4362kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04306 (*cross-listing*)
Date: Fri, 5 Apr 2024 07:19:13 GMT   (319kb,D)

Title: AuditGPT: Auditing Smart Contracts with ChatGPT
Authors: Shihao Xia, Shuai Shao, Mengting He, Tingting Yu, Linhai Song, Yiying
  Zhang
Categories: cs.CR cs.AI cs.CL
\\
  To govern smart contracts running on Ethereum, multiple Ethereum Request for
Comment (ERC) standards have been developed, each containing a set of rules to
guide the behaviors of smart contracts. Violating the ERC rules could cause
serious security issues and financial loss, signifying the importance of
verifying smart contracts follow ERCs. Today's practices of such verification
are to either manually audit each single contract or use expert-developed,
limited-scope program-analysis tools, both of which are far from being
effective in identifying ERC rule violations. This paper presents a tool named
AuditGPT that leverages large language models (LLMs) to automatically and
comprehensively verify ERC rules against smart contracts. To build AuditGPT, we
first conduct an empirical study on 222 ERC rules specified in four popular
ERCs to understand their content, their security impacts, their specification
in natural language, and their implementation in Solidity. Guided by the study,
we construct AuditGPT by separating the large, complex auditing process into
small, manageable tasks and design prompts specialized for each ERC rule type
to enhance LLMs' auditing performance. In the evaluation, AuditGPT successfully
pinpoints 418 ERC rule violations and only reports 18 false positives,
showcasing its effectiveness and accuracy. Moreover, AuditGPT beats an auditing
service provided by security experts in effectiveness, accuracy, and cost,
demonstrating its advancement over state-of-the-art smart-contract auditing
practices.
\\ ( https://arxiv.org/abs/2404.04306 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04310 (*cross-listing*)
Date: Fri, 5 Apr 2024 10:29:18 GMT   (1805kb,D)

Title: Suppressing Modulation Instability with Reinforcement Learning
Authors: Nikolay Kalmykov, Rishat Zagidullin, Oleg Rogov, Sergey Rykovanov,
  Dmitry V. Dylov
Categories: nlin.PS cs.AI cs.LG cs.SY eess.SY physics.app-ph
\\
  Modulation instability is a phenomenon of spontaneous pattern formation in
nonlinear media, oftentimes leading to an unpredictable behaviour and a
degradation of a signal of interest. We propose an approach based on
reinforcement learning to suppress the unstable modes by optimizing the
parameters for the time modulation of the potential in the nonlinear system. We
test our approach in 1D and 2D cases and propose a new class of
physically-meaningful reward functions to guarantee tamed instability.
\\ ( https://arxiv.org/abs/2404.04310 ,  1805kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04313 (*cross-listing*)
Date: Fri, 5 Apr 2024 12:25:00 GMT   (17261kb,D)

Title: JobFormer: Skill-Aware Job Recommendation with Semantic-Enhanced
  Transformer
Authors: Zhihao Guan, Jia-Qi Yang, Yang Yang, Hengshu Zhu, Wenjie Li, Hui Xiong
Categories: cs.IR cs.AI
\\
  Job recommendation aims to provide potential talents with suitable job
descriptions (JDs) consistent with their career trajectory, which plays an
essential role in proactive talent recruitment. In real-world management
scenarios, the available JD-user records always consist of JDs, user profiles,
and click data, in which the user profiles are typically summarized as the
user's skill distribution for privacy reasons. Although existing sophisticated
recommendation methods can be directly employed, effective recommendation still
has challenges considering the information deficit of JD itself and the natural
heterogeneous gap between JD and user profile. To address these challenges, we
proposed a novel skill-aware recommendation model based on the designed
semantic-enhanced transformer to parse JDs and complete personalized job
recommendation. Specifically, we first model the relative items of each JD and
then adopt an encoder with the local-global attention mechanism to better mine
the intra-job and inter-job dependencies from JD tuples. Moreover, we adopt a
two-stage learning strategy for skill-aware recommendation, in which we utilize
the skill distribution to guide JD representation learning in the recall stage,
and then combine the user profiles for final prediction in the ranking stage.
Consequently, we can embed rich contextual semantic representations for
learning JDs, while skill-aware recommendation provides effective JD-user joint
representation for click-through rate (CTR) prediction. To validate the
superior performance of our method for job recommendation, we present a
thorough empirical analysis of large-scale real-world and public datasets to
demonstrate its effectiveness and interpretability.
\\ ( https://arxiv.org/abs/2404.04313 ,  17261kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04318 (*cross-listing*)
Date: Fri, 5 Apr 2024 17:55:33 GMT   (9854kb,D)

Title: Robust Depth Enhancement via Polarization Prompt Fusion Tuning
Authors: Kei Ikemura, Yiming Huang, Felix Heide, Zhaoxiang Zhang, Qifeng Chen,
  Chenyang Lei
Categories: cs.CV cs.AI
Comments: CVPR 2024. Project page: https://lastbasket.github.io/PPFT/. The
  first two authors contribute equally
\\
  Existing depth sensors are imperfect and may provide inaccurate depth values
in challenging scenarios, such as in the presence of transparent or reflective
objects. In this work, we present a general framework that leverages
polarization imaging to improve inaccurate depth measurements from various
depth sensors. Previous polarization-based depth enhancement methods focus on
utilizing pure physics-based formulas for a single sensor. In contrast, our
method first adopts a learning-based strategy where a neural network is trained
to estimate a dense and complete depth map from polarization data and a sensor
depth map from different sensors. To further improve the performance, we
propose a Polarization Prompt Fusion Tuning (PPFT) strategy to effectively
utilize RGB-based models pre-trained on large-scale datasets, as the size of
the polarization dataset is limited to train a strong model from scratch. We
conducted extensive experiments on a public dataset, and the results
demonstrate that the proposed method performs favorably compared to existing
depth enhancement baselines. Code and demos are available at
https://lastbasket.github.io/PPFT/.
\\ ( https://arxiv.org/abs/2404.04318 ,  9854kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04376 (*cross-listing*)
Date: Fri, 5 Apr 2024 19:38:18 GMT   (7605kb,D)

Title: ClickDiffusion: Harnessing LLMs for Interactive Precise Image Editing
Authors: Alec Helbling, Seongmin Lee, Polo Chau
Categories: cs.CV cs.AI
Comments: arXiv admin note: substantial text overlap with arXiv:2402.07925
\\
  Recently, researchers have proposed powerful systems for generating and
manipulating images using natural language instructions. However, it is
difficult to precisely specify many common classes of image transformations
with text alone. For example, a user may wish to change the location and breed
of a particular dog in an image with several similar dogs. This task is quite
difficult with natural language alone, and would require a user to write a
laboriously complex prompt that both disambiguates the target dog and describes
the destination. We propose ClickDiffusion, a system for precise image
manipulation and generation that combines natural language instructions with
visual feedback provided by the user through a direct manipulation interface.
We demonstrate that by serializing both an image and a multi-modal instruction
into a textual representation it is possible to leverage LLMs to perform
precise transformations of the layout and appearance of an image. Code
available at https://github.com/poloclub/ClickDiffusion.
\\ ( https://arxiv.org/abs/2404.04376 ,  7605kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04392 (*cross-listing*)
Date: Fri, 5 Apr 2024 20:31:45 GMT   (1440kb,D)

Title: Increased LLM Vulnerabilities from Fine-tuning and Quantization
Authors: Divyanshu Kumar, Anurakt Kumar, Sahil Agarwal and Prashanth Harshangi
Categories: cs.CR cs.AI
\\
  Large Language Models (LLMs) have become very popular and have found use
cases in many domains, such as chatbots, auto-task completion agents, and much
more. However, LLMs are vulnerable to different types of attacks, such as
jailbreaking, prompt injection attacks, and privacy leakage attacks.
Foundational LLMs undergo adversarial and alignment training to learn not to
generate malicious and toxic content. For specialized use cases, these
foundational LLMs are subjected to fine-tuning or quantization for better
performance and efficiency. We examine the impact of downstream tasks such as
fine-tuning and quantization on LLM vulnerability. We test foundation models
like Mistral, Llama, MosaicML, and their fine-tuned versions. Our research
shows that fine-tuning and quantization reduces jailbreak resistance
significantly, leading to increased LLM vulnerabilities. Finally, we
demonstrate the utility of external guardrails in reducing LLM vulnerabilities.
\\ ( https://arxiv.org/abs/2404.04392 ,  1440kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04399 (*cross-listing*)
Date: Fri, 5 Apr 2024 20:56:15 GMT   (821kb,D)

Title: Longitudinal Targeted Minimum Loss-based Estimation with
  Temporal-Difference Heterogeneous Transformer
Authors: Toru Shirakawa, Yi Li, Yulun Wu, Sky Qiu, Yuxuan Li, Mingduo Zhao,
  Hiroyasu Iso, Mark van der Laan
Categories: stat.ML cs.AI cs.LG stat.AP stat.ME
\\
  We propose Deep Longitudinal Targeted Minimum Loss-based Estimation (Deep
LTMLE), a novel approach to estimate the counterfactual mean of outcome under
dynamic treatment policies in longitudinal problem settings. Our approach
utilizes a transformer architecture with heterogeneous type embedding trained
using temporal-difference learning. After obtaining an initial estimate using
the transformer, following the targeted minimum loss-based likelihood
estimation (TMLE) framework, we statistically corrected for the bias commonly
associated with machine learning algorithms. Furthermore, our method also
facilitates statistical inference by enabling the provision of 95% confidence
intervals grounded in asymptotic statistical theory. Simulation results
demonstrate our method's superior performance over existing approaches,
particularly in complex, long time-horizon scenarios. It remains effective in
small-sample, short-duration contexts, matching the performance of
asymptotically efficient estimators. To demonstrate our method in practice, we
applied our method to estimate counterfactual mean outcomes for standard versus
intensive blood pressure management strategies in a real-world cardiovascular
epidemiology cohort study.
\\ ( https://arxiv.org/abs/2404.04399 ,  821kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04403 (*cross-listing*)
Date: Fri, 5 Apr 2024 21:00:43 GMT   (4973kb,D)

Title: Low-Rank Robust Subspace Tensor Clustering for Metro Passenger Flow
  Modeling
Authors: Jiuyun Hu, Ziyue Li, Chen Zhang, Fugee Tsung, Hao Yan
Categories: stat.ME cs.AI
Comments: Conditionally Accepted in INFORMS Journal of Data Science
\\
  Tensor clustering has become an important topic, specifically in
spatio-temporal modeling, due to its ability to cluster spatial modes (e.g.,
stations or road segments) and temporal modes (e.g., time of the day or day of
the week). Our motivating example is from subway passenger flow modeling, where
similarities between stations are commonly found. However, the challenges lie
in the innate high-dimensionality of tensors and also the potential existence
of anomalies. This is because the three tasks, i.e., dimension reduction,
clustering, and anomaly decomposition, are inter-correlated to each other, and
treating them in a separate manner will render a suboptimal performance. Thus,
in this work, we design a tensor-based subspace clustering and anomaly
decomposition technique for simultaneously outlier-robust dimension reduction
and clustering for high-dimensional tensors. To achieve this, a novel low-rank
robust subspace clustering decomposition model is proposed by combining Tucker
decomposition, sparse anomaly decomposition, and subspace clustering. An
effective algorithm based on Block Coordinate Descent is proposed to update the
parameters. Prudent experiments prove the effectiveness of the proposed
framework via the simulation study, with a gain of +25% clustering accuracy
than benchmark methods in a hard case. The interrelations of the three tasks
are also analyzed via ablation studies, validating the interrelation
assumption. Moreover, a case study in the station clustering based on real
passenger flow data is conducted, with quite valuable insights discovered.
\\ ( https://arxiv.org/abs/2404.04403 ,  4973kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04420 (*cross-listing*)
Date: Fri, 5 Apr 2024 21:41:20 GMT   (4060kb,D)

Title: The NES Video-Music Database: A Dataset of Symbolic Video Game Music
  Paired with Gameplay Videos
Authors: Igor Cardoso and Rubens O. Moraes and Lucas N. Ferreira
Categories: cs.SD cs.AI cs.LG
Comments: Accepted for publication at the 19th International Conference on the
  Foundations of Digital Games
DOI: 10.1145/3649921.3650011
\\
  Neural models are one of the most popular approaches for music generation,
yet there aren't standard large datasets tailored for learning music directly
from game data. To address this research gap, we introduce a novel dataset
named NES-VMDB, containing 98,940 gameplay videos from 389 NES games, each
paired with its original soundtrack in symbolic format (MIDI). NES-VMDB is
built upon the Nintendo Entertainment System Music Database (NES-MDB),
encompassing 5,278 music pieces from 397 NES games. Our approach involves
collecting long-play videos for 389 games of the original dataset, slicing them
into 15-second-long clips, and extracting the audio from each clip.
Subsequently, we apply an audio fingerprinting algorithm (similar to Shazam) to
automatically identify the corresponding piece in the NES-MDB dataset.
Additionally, we introduce a baseline method based on the Controllable Music
Transformer to generate NES music conditioned on gameplay clips. We evaluated
this approach with objective metrics, and the results showed that the
conditional CMT improves musical structural quality when compared to its
unconditional counterpart. Moreover, we used a neural classifier to predict the
game genre of the generated pieces. Results showed that the CMT generator can
learn correlations between gameplay videos and game genres, but further
research has to be conducted to achieve human-level performance.
\\ ( https://arxiv.org/abs/2404.04420 ,  4060kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04446 (*cross-listing*)
Date: Fri, 5 Apr 2024 23:17:25 GMT   (940kb,D)

Title: Bounding Causal Effects with Leaky Instruments
Authors: David S. Watson, Jordan Penn, Lee M. Gunderson, Gecia
  Bravo-Hermsdorff, Afsaneh Mastouri, Ricardo Silva
Categories: stat.ME cs.AI
Comments: 22 pages, 9 figures
\\
  Instrumental variables (IVs) are a popular and powerful tool for estimating
causal effects in the presence of unobserved confounding. However, classical
approaches rely on strong assumptions such as the $\textit{exclusion
criterion}$, which states that instrumental effects must be entirely mediated
by treatments. This assumption often fails in practice. When IV methods are
improperly applied to data that do not meet the exclusion criterion, estimated
causal effects may be badly biased. In this work, we propose a novel solution
that provides $\textit{partial}$ identification in linear models given a set of
$\textit{leaky instruments}$, which are allowed to violate the exclusion
criterion to some limited degree. We derive a convex optimization objective
that provides provably sharp bounds on the average treatment effect under some
common forms of information leakage, and implement inference procedures to
quantify the uncertainty of resulting estimates. We demonstrate our method in a
set of experiments with simulated data, where it performs favorably against the
state of the art.
\\ ( https://arxiv.org/abs/2404.04446 ,  940kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04452 (*cross-listing*)
Date: Fri, 5 Apr 2024 23:38:57 GMT   (4964kb,D)

Title: Vision Transformers in Domain Adaptation and Generalization: A Study of
  Robustness
Authors: Shadi Alijani, Jamil Fayyad and Homayoun Najjaran
Categories: cs.CV cs.AI
Comments: 28 pages, 5 figures, Preprint submitted to Elsevier
ACM-class: I.2.10; I.4.9; I.4.7; I.5.1
\\
  Deep learning models are often evaluated in scenarios where the data
distribution is different from those used in the training and validation
phases. The discrepancy presents a challenge for accurately predicting the
performance of models once deployed on the target distribution. Domain
adaptation and generalization are widely recognized as effective strategies for
addressing such shifts, thereby ensuring reliable performance. The recent
promising results in applying vision transformers in computer vision tasks,
coupled with advancements in self-attention mechanisms, have demonstrated their
significant potential for robustness and generalization in handling
distribution shifts. Motivated by the increased interest from the research
community, our paper investigates the deployment of vision transformers in
domain adaptation and domain generalization scenarios. For domain adaptation
methods, we categorize research into feature-level, instance-level, model-level
adaptations, and hybrid approaches, along with other categorizations with
respect to diverse strategies for enhancing domain adaptation. Similarly, for
domain generalization, we categorize research into multi-domain learning,
meta-learning, regularization techniques, and data augmentation strategies. We
further classify diverse strategies in research, underscoring the various
approaches researchers have taken to address distribution shifts by integrating
vision transformers. The inclusion of comprehensive tables summarizing these
categories is a distinct feature of our work, offering valuable insights for
researchers. These findings highlight the versatility of vision transformers in
managing distribution shifts, crucial for real-world applications, especially
in critical safety and decision-making scenarios.
\\ ( https://arxiv.org/abs/2404.04452 ,  4964kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04456 (*cross-listing*)
Date: Sat, 6 Apr 2024 00:04:19 GMT   (2081kb)

Title: Beyond the Known: Adversarial Autoencoders in Novelty Detection
Authors: Muhammad Asad, Ihsan Ullah, Ganesh Sistu and Michael G. Madden
Categories: cs.CV cs.AI cs.LG
Comments: Accepted at the VISAAP 2024
\\
  In novelty detection, the goal is to decide if a new data point should be
categorized as an inlier or an outlier, given a training dataset that primarily
captures the inlier distribution. Recent approaches typically use deep encoder
and decoder network frameworks to derive a reconstruction error, and employ
this error either to determine a novelty score, or as the basis for a one-class
classifier. In this research, we use a similar framework but with a lightweight
deep network, and we adopt a probabilistic score with reconstruction error. Our
methodology calculates the probability of whether the sample comes from the
inlier distribution or not. This work makes two key contributions. The first is
that we compute the novelty probability by linearizing the manifold that holds
the structure of the inlier distribution. This allows us to interpret how the
probability is distributed and can be determined in relation to the local
coordinates of the manifold tangent space. The second contribution is that we
improve the training protocol for the network. Our results indicate that our
approach is effective at learning the target class, and it outperforms recent
state-of-the-art methods on several benchmark datasets.
\\ ( https://arxiv.org/abs/2404.04456 ,  2081kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04481 (*cross-listing*)
Date: Sat, 6 Apr 2024 03:11:31 GMT   (5744kb,D)

Title: Joint Identifiability of Cross-Domain Recommendation via Hierarchical
  Subspace Disentanglement
Authors: Jing Du, Zesheng Ye, Bin Guo, Zhiwen Yu, Lina Yao
Categories: cs.IR cs.AI cs.LG
Comments: accepted to SIGIR 2024 as a Full Research Paper
\\
  Cross-Domain Recommendation (CDR) seeks to enable effective knowledge
transfer across domains. Existing works rely on either representation alignment
or transformation bridges, but they struggle on identifying domain-shared from
domain-specific latent factors. Specifically, while CDR describes user
representations as a joint distribution over two domains, these methods fail to
account for its joint identifiability as they primarily fixate on the marginal
distribution within a particular domain. Such a failure may overlook the
conditionality between two domains and how it contributes to latent factor
disentanglement, leading to negative transfer when domains are weakly
correlated. In this study, we explore what should and should not be transferred
in cross-domain user representations from a causality perspective. We propose a
Hierarchical subspace disentanglement approach to explore the Joint
IDentifiability of cross-domain joint distribution, termed HJID, to preserve
domain-specific behaviors from domain-shared factors. HJID organizes user
representations into layers: generic shallow subspaces and domain-oriented deep
subspaces. We first encode the generic pattern in the shallow subspace by
minimizing the Maximum Mean Discrepancy of initial layer activation. Then, to
dissect how domain-oriented latent factors are encoded in deeper layers
activation, we construct a cross-domain causality-based data generation graph,
which identifies cross-domain consistent and domain-specific components,
adhering to the Minimal Change principle. This allows HJID to maintain
stability whilst discovering unique factors for different domains, all within a
generative framework of invertible transformations that guarantee the joint
identifiability. With experiments on real-world datasets, we show that HJID
outperforms SOTA methods on a range of strongly and weakly correlated CDR
tasks.
\\ ( https://arxiv.org/abs/2404.04481 ,  5744kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04492 (*cross-listing*)
Date: Sat, 6 Apr 2024 03:48:29 GMT   (296kb)

Title: Automated Lane Change Behavior Prediction and Environmental Perception
  Based on SLAM Technology
Authors: Han Lei, Baoming Wang, Zuwei Shui, Peiyuan Yang, Penghao Liang
Categories: cs.RO cs.AI cs.CV
\\
  In addition to environmental perception sensors such as cameras, radars, etc.
in the automatic driving system, the external environment of the vehicle is
perceived, in fact, there is also a perception sensor that has been silently
dedicated in the system, that is, the positioning module. This paper explores
the application of SLAM (Simultaneous Localization and Mapping) technology in
the context of automatic lane change behavior prediction and environment
perception for autonomous vehicles. It discusses the limitations of traditional
positioning methods, introduces SLAM technology, and compares LIDAR SLAM with
visual SLAM. Real-world examples from companies like Tesla, Waymo, and Mobileye
showcase the integration of AI-driven technologies, sensor fusion, and SLAM in
autonomous driving systems. The paper then delves into the specifics of SLAM
algorithms, sensor technologies, and the importance of automatic lane changes
in driving safety and efficiency. It highlights Tesla's recent update to its
Autopilot system, which incorporates automatic lane change functionality using
SLAM technology. The paper concludes by emphasizing the crucial role of SLAM in
enabling accurate environment perception, positioning, and decision-making for
autonomous vehicles, ultimately enhancing safety and driving experience.
\\ ( https://arxiv.org/abs/2404.04492 ,  296kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04500 (*cross-listing*)
Date: Sat, 6 Apr 2024 04:43:06 GMT   (115kb,D)

Title: Trustless Audits without Revealing Data or Models
Authors: Suppakit Waiwitlikhit, Ion Stoica, Yi Sun, Tatsunori Hashimoto, Daniel
  Kang
Categories: cs.CR cs.AI cs.LG
\\
  There is an increasing conflict between business incentives to hide models
and data as trade secrets, and the societal need for algorithmic transparency.
For example, a rightsholder wishing to know whether their copyrighted works
have been used during training must convince the model provider to allow a
third party to audit the model and data. Finding a mutually agreeable third
party is difficult, and the associated costs often make this approach
impractical.
  In this work, we show that it is possible to simultaneously allow model
providers to keep their model weights (but not architecture) and data secret
while allowing other parties to trustlessly audit model and data properties. We
do this by designing a protocol called ZkAudit in which model providers publish
cryptographic commitments of datasets and model weights, alongside a
zero-knowledge proof (ZKP) certifying that published commitments are derived
from training the model. Model providers can then respond to audit requests by
privately computing any function F of the dataset (or model) and releasing the
output of F alongside another ZKP certifying the correct execution of F. To
enable ZkAudit, we develop new methods of computing ZKPs for SGD on modern
neural nets for simple recommender systems and image classification models
capable of high accuracies on ImageNet. Empirically, we show it is possible to
provide trustless audits of DNNs, including copyright, censorship, and
counterfactual audits with little to no loss in accuracy.
\\ ( https://arxiv.org/abs/2404.04500 ,  115kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04511 (*cross-listing*)
Date: Sat, 6 Apr 2024 05:55:14 GMT   (2909kb,D)

Title: Cluster-based Video Summarization with Temporal Context Awareness
Authors: Hai-Dang Huynh-Lam, Ngoc-Phuong Ho-Thi, Minh-Triet Tran, Trung-Nghia
  Le
Categories: cs.CV cs.AI
Comments: 14 pages, 6 figures, accepted in PSIVT 2023
\\
  In this paper, we present TAC-SUM, a novel and efficient training-free
approach for video summarization that addresses the limitations of existing
cluster-based models by incorporating temporal context. Our method partitions
the input video into temporally consecutive segments with clustering
information, enabling the injection of temporal awareness into the clustering
process, setting it apart from prior cluster-based summarization methods. The
resulting temporal-aware clusters are then utilized to compute the final
summary, using simple rules for keyframe selection and frame importance
scoring. Experimental results on the SumMe dataset demonstrate the
effectiveness of our proposed approach, outperforming existing unsupervised
methods and achieving comparable performance to state-of-the-art supervised
summarization techniques. Our source code is available for reference at
\url{https://github.com/hcmus-thesis-gulu/TAC-SUM}.
\\ ( https://arxiv.org/abs/2404.04511 ,  2909kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04517 (*cross-listing*)
Date: Sat, 6 Apr 2024 06:15:07 GMT   (478kb,D)

Title: Latent-based Diffusion Model for Long-tailed Recognition
Authors: Pengxiao Han, Changkun Ye, Jieming Zhou, Jing Zhang, Jie Hong, Xuesong
  Li
Categories: cs.CV cs.AI
Comments: 8 pages, 3 figures, accepted by L3DIVU-CVPR2024
\\
  Long-tailed imbalance distribution is a common issue in practical computer
vision applications. Previous works proposed methods to address this problem,
which can be categorized into several classes: re-sampling, re-weighting,
transfer learning, and feature augmentation. In recent years, diffusion models
have shown an impressive generation ability in many sub-problems of deep
computer vision. However, its powerful generation has not been explored in
long-tailed problems. We propose a new approach, the Latent-based Diffusion
Model for Long-tailed Recognition (LDMLR), as a feature augmentation method to
tackle the issue. First, we encode the imbalanced dataset into features using
the baseline model. Then, we train a Denoising Diffusion Implicit Model (DDIM)
using these encoded features to generate pseudo-features. Finally, we train the
classifier using the encoded and pseudo-features from the previous two steps.
The model's accuracy shows an improvement on the CIFAR-LT and ImageNet-LT
datasets by using the proposed method.
\\ ( https://arxiv.org/abs/2404.04517 ,  478kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04527 (*cross-listing*)
Date: Sat, 6 Apr 2024 06:49:55 GMT   (4531kb,D)

Title: VTR: An Optimized Vision Transformer for SAR ATR Acceleration on FPGA
Authors: Sachini Wickramasinghe, Dhruv Parikh, Bingyi Zhang, Rajgopal Kannan,
  Viktor Prasanna, Carl Busart
Categories: cs.CV cs.AI cs.AR cs.DC
Comments: SPIE DCS 2024
\\
  Synthetic Aperture Radar (SAR) Automatic Target Recognition (ATR) is a key
technique used in military applications like remote-sensing image recognition.
Vision Transformers (ViTs) are the current state-of-the-art in various computer
vision applications, outperforming their CNN counterparts. However, using ViTs
for SAR ATR applications is challenging due to (1) standard ViTs require
extensive training data to generalize well due to their low locality; the
standard SAR datasets, however, have a limited number of labeled training data
which reduces the learning capability of ViTs; (2) ViTs have a high parameter
count and are computation intensive which makes their deployment on
resource-constrained SAR platforms difficult. In this work, we develop a
lightweight ViT model that can be trained directly on small datasets without
any pre-training by utilizing the Shifted Patch Tokenization (SPT) and Locality
Self-Attention (LSA) modules. We directly train this model on SAR datasets
which have limited training samples to evaluate its effectiveness for SAR ATR
applications. We evaluate our proposed model, that we call VTR (ViT for SAR
ATR), on three widely used SAR datasets: MSTAR, SynthWakeSAR, and GBSAR.
Further, we propose a novel FPGA accelerator for VTR, in order to enable
deployment for real-time SAR ATR applications.
\\ ( https://arxiv.org/abs/2404.04527 ,  4531kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04544 (*cross-listing*)
Date: Sat, 6 Apr 2024 07:53:49 GMT   (40011kb,D)

Title: BeyondScene: Higher-Resolution Human-Centric Scene Generation With
  Pretrained Diffusion
Authors: Gwanghyun Kim, Hayeon Kim, Hoigi Seo, Dong Un Kang, Se Young Chun
Categories: cs.CV cs.AI
Comments: Project page: https://janeyeon.github.io/beyond-scene
\\
  Generating higher-resolution human-centric scenes with details and controls
remains a challenge for existing text-to-image diffusion models. This challenge
stems from limited training image size, text encoder capacity (limited tokens),
and the inherent difficulty of generating complex scenes involving multiple
humans. While current methods attempted to address training size limit only,
they often yielded human-centric scenes with severe artifacts. We propose
BeyondScene, a novel framework that overcomes prior limitations, generating
exquisite higher-resolution (over 8K) human-centric scenes with exceptional
text-image correspondence and naturalness using existing pretrained diffusion
models. BeyondScene employs a staged and hierarchical approach to initially
generate a detailed base image focusing on crucial elements in instance
creation for multiple humans and detailed descriptions beyond token limit of
diffusion model, and then to seamlessly convert the base image to a
higher-resolution output, exceeding training image size and incorporating
details aware of text and instances via our novel instance-aware hierarchical
enlargement process that consists of our proposed high-frequency injected
forward diffusion and adaptive joint diffusion. BeyondScene surpasses existing
methods in terms of correspondence with detailed text descriptions and
naturalness, paving the way for advanced applications in higher-resolution
human-centric scene creation beyond the capacity of pretrained diffusion models
without costly retraining. Project page:
https://janeyeon.github.io/beyond-scene.
\\ ( https://arxiv.org/abs/2404.04544 ,  40011kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04547 (*cross-listing*)
Date: Sat, 6 Apr 2024 08:07:48 GMT   (3730kb,D)

Title: Exhaustive Exploitation of Nature-inspired Computation for Cancer
  Screening in an Ensemble Manner
Authors: Xubin Wang and Yunhe Wang and Zhiqing Ma and Ka-Chun Wong and Xiangtao
  Li
Categories: cs.NE cs.AI cs.LG
DOI: 10.1109/TCBB.2024.3385402
\\
  Accurate screening of cancer types is crucial for effective cancer detection
and precise treatment selection. However, the association between gene
expression profiles and tumors is often limited to a small number of biomarker
genes. While computational methods using nature-inspired algorithms have shown
promise in selecting predictive genes, existing techniques are limited by
inefficient search and poor generalization across diverse datasets. This study
presents a framework termed Evolutionary Optimized Diverse Ensemble Learning
(EODE) to improve ensemble learning for cancer classification from gene
expression data. The EODE methodology combines an intelligent grey wolf
optimization algorithm for selective feature space reduction, guided random
injection modeling for ensemble diversity enhancement, and subset model
optimization for synergistic classifier combinations. Extensive experiments
were conducted across 35 gene expression benchmark datasets encompassing varied
cancer types. Results demonstrated that EODE obtained significantly improved
screening accuracy over individual and conventionally aggregated models. The
integrated optimization of advanced feature selection, directed specialized
modeling, and cooperative classifier ensembles helps address key challenges in
current nature-inspired approaches. This provides an effective framework for
robust and generalized ensemble learning with gene expression biomarkers.
Specifically, we have opened EODE source code on Github at
https://github.com/wangxb96/EODE.
\\ ( https://arxiv.org/abs/2404.04547 ,  3730kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04564 (*cross-listing*)
Date: Sat, 6 Apr 2024 09:08:34 GMT   (11605kb,D)

Title: Enhancing Video Summarization with Context Awareness
Authors: Hai-Dang Huynh-Lam, Ngoc-Phuong Ho-Thi, Minh-Triet Tran, Trung-Nghia
  Le
Categories: cs.CV cs.AI
Comments: 115 pages, 1 supplementary paper, undergraduate thesis report at
  US-VNUHCM
\\
  Video summarization is a crucial research area that aims to efficiently
browse and retrieve relevant information from the vast amount of video content
available today. With the exponential growth of multimedia data, the ability to
extract meaningful representations from videos has become essential. Video
summarization techniques automatically generate concise summaries by selecting
keyframes, shots, or segments that capture the video's essence. This process
improves the efficiency and accuracy of various applications, including video
surveillance, education, entertainment, and social media. Despite the
importance of video summarization, there is a lack of diverse and
representative datasets, hindering comprehensive evaluation and benchmarking of
algorithms. Existing evaluation metrics also fail to fully capture the
complexities of video summarization, limiting accurate algorithm assessment and
hindering the field's progress. To overcome data scarcity challenges and
improve evaluation, we propose an unsupervised approach that leverages video
data structure and information for generating informative summaries. By moving
away from fixed annotations, our framework can produce representative summaries
effectively. Moreover, we introduce an innovative evaluation pipeline tailored
specifically for video summarization. Human participants are involved in the
evaluation, comparing our generated summaries to ground truth summaries and
assessing their informativeness. This human-centric approach provides valuable
insights into the effectiveness of our proposed techniques. Experimental
results demonstrate that our training-free framework outperforms existing
unsupervised approaches and achieves competitive results compared to
state-of-the-art supervised methods.
\\ ( https://arxiv.org/abs/2404.04564 ,  11605kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04587 (*cross-listing*)
Date: Sat, 6 Apr 2024 10:54:35 GMT   (3189kb,D)

Title: Neuroevolving Electronic Dynamical Networks
Authors: Derek Whitley
Categories: cs.NE cs.AI cs.AR
Comments: 8 pages, 3 figures
\\
  Neuroevolution is a powerful method of applying an evolutionary algorithm to
refine the performance of artificial neural networks through natural selection;
however, the fitness evaluation of these networks can be time-consuming and
computationally expensive, particularly for continuous time recurrent neural
networks (CTRNNs) that necessitate the simulation of differential equations. To
overcome this challenge, field programmable gate arrays (FPGAs) have emerged as
an increasingly popular solution, due to their high performance and low power
consumption. Further, their ability to undergo dynamic and partial
reconfiguration enables the extremely rapid evaluation of the fitness of
CTRNNs, effectively addressing the bottleneck associated with conventional
methods. By incorporating fitness evaluation directly upon the programmable
logic of the FPGA, hyper-parallel evaluation becomes feasible, dramatically
reducing the time required for assessment. This inherent parallelism of FPGAs
accelerates the entire neuroevolutionary process by several orders of
magnitude, facilitating faster convergence to an optimal solution. The work
presented in this study demonstrates the potential of utilizing dynamic and
partial reconfiguration on capable FPGAs as a powerful platform for
neuroevolving dynamic neural networks.
\\ ( https://arxiv.org/abs/2404.04587 ,  3189kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04608 (*cross-listing*)
Date: Sat, 6 Apr 2024 12:27:21 GMT   (1434kb,D)

Title: Panoptic Perception: A Novel Task and Fine-grained Dataset for Universal
  Remote Sensing Image Interpretation
Authors: Danpei Zhao, Bo Yuan, Ziqiang Chen, Tian Li, Zhuoran Liu, Wentao Li,
  Yue Gao
Categories: cs.CV cs.AI
Comments: Undergoing Review
\\
  Current remote-sensing interpretation models often focus on a single task
such as detection, segmentation, or caption. However, the task-specific
designed models are unattainable to achieve the comprehensive multi-level
interpretation of images. The field also lacks support for multi-task joint
interpretation datasets. In this paper, we propose Panoptic Perception, a novel
task and a new fine-grained dataset (FineGrip) to achieve a more thorough and
universal interpretation for RSIs. The new task, 1) integrates pixel-level,
instance-level, and image-level information for universal image perception, 2)
captures image information from coarse to fine granularity, achieving deeper
scene understanding and description, and 3) enables various independent tasks
to complement and enhance each other through multi-task learning. By
emphasizing multi-task interactions and the consistency of perception results,
this task enables the simultaneous processing of fine-grained foreground
instance segmentation, background semantic segmentation, and global
fine-grained image captioning. Concretely, the FineGrip dataset includes 2,649
remote sensing images, 12,054 fine-grained instance segmentation masks
belonging to 20 foreground things categories, 7,599 background semantic masks
for 5 stuff classes and 13,245 captioning sentences. Furthermore, we propose a
joint optimization-based panoptic perception model. Experimental results on
FineGrip demonstrate the feasibility of the panoptic perception task and the
beneficial effect of multi-task joint optimization on individual tasks. The
dataset will be publicly available.
\\ ( https://arxiv.org/abs/2404.04608 ,  1434kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04615 (*cross-listing*)
Date: Sat, 6 Apr 2024 12:49:09 GMT   (37929kb,D)

Title: PointSAGE: Mesh-independent superresolution approach to fluid flow
  predictions
Authors: Rajat Sarkar, Krishna Sai Sudhir Aripirala, Vishal Sudam Jadhav, Sagar
  Srinivas Sakhinana, Venkataramana Runkana
Categories: physics.flu-dyn cs.AI cs.LG
Comments: Accepted at Data-Centric Machine Learning Workshop (DMLR) at ICLR,
  2024
\\
  Computational Fluid Dynamics (CFD) serves as a powerful tool for simulating
fluid flow across diverse industries. High-resolution CFD simulations offer
valuable insights into fluid behavior and flow patterns, aiding in optimizing
design features or enhancing system performance. However, as resolution
increases, computational data requirements and time increase proportionately.
This presents a persistent challenge in CFD. Recently, efforts have been
directed towards accurately predicting fine-mesh simulations using coarse-mesh
simulations, with geometry and boundary conditions as input. Drawing
inspiration from models designed for super-resolution, deep learning techniques
like UNets have been applied to address this challenge. However, these existing
methods are limited to structured data and fail if the mesh is unstructured due
to its inability to convolute. Additionally, incorporating geometry/mesh
information in the training process introduces drawbacks such as increased data
requirements, challenges in generalizing to unseen geometries for the same
physical phenomena, and issues with robustness to mesh distortions. To address
these concerns, we propose a novel framework, PointSAGE a mesh-independent
network that leverages the unordered, mesh-less nature of Pointcloud to learn
the complex fluid flow and directly predict fine simulations, completely
neglecting mesh information. Utilizing an adaptable framework, the model
accurately predicts the fine data across diverse point cloud sizes, regardless
of the training dataset's dimension. We have evaluated the effectiveness of
PointSAGE on diverse datasets in different scenarios, demonstrating notable
results and a significant acceleration in computational time in generating fine
simulations compared to standard CFD techniques.
\\ ( https://arxiv.org/abs/2404.04615 ,  37929kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04642 (*cross-listing*)
Date: Sat, 6 Apr 2024 14:27:22 GMT   (443kb)

Title: Power-Efficient Image Storage: Leveraging Super Resolution Generative
  Adversarial Network for Sustainable Compression and Reduced Carbon Footprint
Authors: Ashok Mondal (1), Satyam Singh (1) ((1) Vellore Institute of
  Technology, Chennai)
Categories: eess.IV cs.AI cs.LG
Comments: 5 pages, 5 figures
MSC-class: 68T07
ACM-class: I.2.m; H.3.2
\\
  In recent years, large-scale adoption of cloud storage solutions has
revolutionized the way we think about digital data storage. However, the
exponential increase in data volume, especially images, has raised
environmental concerns regarding power and resource consumption, as well as the
rising digital carbon footprint emissions. The aim of this research is to
propose a methodology for cloud-based image storage by integrating image
compression technology with SuperResolution Generative Adversarial Networks
(SRGAN). Rather than storing images in their original format directly on the
cloud, our approach involves initially reducing the image size through
compression and downsizing techniques before storage. Upon request, these
compressed images will be retrieved and processed by SRGAN to generate images.
The efficacy of the proposed method is evaluated in terms of PSNR and SSIM
metrics. Additionally, a mathematical analysis is given to calculate power
consumption and carbon footprint assesment. The proposed data compression
technique provides a significant solution to achieve a reasonable trade off
between environmental sustainability and industrial efficiency.
\\ ( https://arxiv.org/abs/2404.04642 ,  443kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04663 (*cross-listing*)
Date: Sat, 6 Apr 2024 15:31:57 GMT   (5959kb,D)

Title: Focused Active Learning for Histopathological Image Classification
Authors: Arne Schmidt, Pablo Morales-\'Alvarez, Lee A.D. Cooper, Lee A.
  Newberg, Andinet Enquobahrie, Aggelos K. Katsaggelos, Rafael Molina
Categories: cs.CV cs.AI
\\
  Active Learning (AL) has the potential to solve a major problem of digital
pathology: the efficient acquisition of labeled data for machine learning
algorithms. However, existing AL methods often struggle in realistic settings
with artifacts, ambiguities, and class imbalances, as commonly seen in the
medical field. The lack of precise uncertainty estimations leads to the
acquisition of images with a low informative value. To address these
challenges, we propose Focused Active Learning (FocAL), which combines a
Bayesian Neural Network with Out-of-Distribution detection to estimate
different uncertainties for the acquisition function. Specifically, the
weighted epistemic uncertainty accounts for the class imbalance, aleatoric
uncertainty for ambiguous images, and an OoD score for artifacts. We perform
extensive experiments to validate our method on MNIST and the real-world Panda
dataset for the classification of prostate cancer. The results confirm that
other AL methods are 'distracted' by ambiguities and artifacts which harm the
performance. FocAL effectively focuses on the most informative images, avoiding
ambiguities and artifacts during acquisition. For both experiments, FocAL
outperforms existing AL approaches, reaching a Cohen's kappa of 0.764 with only
0.69% of the labeled Panda data.
\\ ( https://arxiv.org/abs/2404.04663 ,  5959kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04665 (*cross-listing*)
Date: Sat, 6 Apr 2024 15:48:14 GMT   (1842kb,D)

Title: Adaptive Intra-Class Variation Contrastive Learning for Unsupervised
  Person Re-Identification
Authors: Lingzhi Liu, Haiyang Zhang, Chengwei Tang, Tiantian Zhang
Categories: cs.CV cs.AI
\\
  The memory dictionary-based contrastive learning method has achieved
remarkable results in the field of unsupervised person Re-ID. However, The
method of updating memory based on all samples does not fully utilize the
hardest sample to improve the generalization ability of the model, and the
method based on hardest sample mining will inevitably introduce false-positive
samples that are incorrectly clustered in the early stages of the model.
Clustering-based methods usually discard a significant number of outliers,
leading to the loss of valuable information. In order to address the issues
mentioned before, we propose an adaptive intra-class variation contrastive
learning algorithm for unsupervised Re-ID, called AdaInCV. And the algorithm
quantitatively evaluates the learning ability of the model for each class by
considering the intra-class variations after clustering, which helps in
selecting appropriate samples during the training process of the model. To be
more specific, two new strategies are proposed: Adaptive Sample Mining (AdaSaM)
and Adaptive Outlier Filter (AdaOF). The first one gradually creates more
reliable clusters to dynamically refine the memory, while the second can
identify and filter out valuable outliers as negative samples.
\\ ( https://arxiv.org/abs/2404.04665 ,  1842kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04692 (*cross-listing*)
Date: Sat, 6 Apr 2024 17:41:00 GMT   (10839kb,D)

Title: Securing the Skies: An IRS-Assisted AoI-Aware Secure Multi-UAV System
  with Efficient Task Offloading
Authors: Poorvi Joshi (1), Alakesh Kalita (2), Mohan Gurusamy (1) ((1) National
  University of Singapore, (2) Singapore University of Technology and Design)
Categories: eess.SY cs.AI cs.CR cs.LG cs.SY
Comments: 7 pages, 5 figures, to be published in IEEE 99th Vehicular Technology
  Conference (VTC2024-Spring)
\\
  Unmanned Aerial Vehicles (UAVs) are integral in various sectors like
agriculture, surveillance, and logistics, driven by advancements in 5G.
However, existing research lacks a comprehensive approach addressing both data
freshness and security concerns. In this paper, we address the intricate
challenges of data freshness, and security, especially in the context of
eavesdropping and jamming in modern UAV networks. Our framework incorporates
exponential AoI metrics and emphasizes secrecy rate to tackle eavesdropping and
jamming threats. We introduce a transformer-enhanced Deep Reinforcement
Learning (DRL) approach to optimize task offloading processes. Comparative
analysis with existing algorithms showcases the superiority of our scheme,
indicating its promising advancements in UAV network management.
\\ ( https://arxiv.org/abs/2404.04692 ,  10839kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04718 (*cross-listing*)
Date: Sat, 6 Apr 2024 19:42:25 GMT   (5889kb,D)

Title: Interpretable Multimodal Learning for Cardiovascular Hemodynamics
  Assessment
Authors: Prasun C Tripathi, Sina Tabakhi, Mohammod N I Suvon, Lawrence Sch\"ob,
  Samer Alabed, Andrew J Swift, Shuo Zhou, and Haiping Lu
Categories: cs.CV cs.AI
\\
  Pulmonary Arterial Wedge Pressure (PAWP) is an essential cardiovascular
hemodynamics marker to detect heart failure. In clinical practice, Right Heart
Catheterization is considered a gold standard for assessing cardiac
hemodynamics while non-invasive methods are often needed to screen high-risk
patients from a large population. In this paper, we propose a multimodal
learning pipeline to predict PAWP marker. We utilize complementary information
from Cardiac Magnetic Resonance Imaging (CMR) scans (short-axis and
four-chamber) and Electronic Health Records (EHRs). We extract spatio-temporal
features from CMR scans using tensor-based learning. We propose a graph
attention network to select important EHR features for prediction, where we
model subjects as graph nodes and feature relationships as graph edges using
the attention mechanism. We design four feature fusion strategies: early,
intermediate, late, and hybrid fusion. With a linear classifier and linear
fusion strategies, our pipeline is interpretable. We validate our pipeline on a
large dataset of $2,641$ subjects from our ASPIRE registry. The comparative
study against state-of-the-art methods confirms the superiority of our
pipeline. The decision curve analysis further validates that our pipeline can
be applied to screen a large population. The code is available at
https://github.com/prasunc/hemodynamics.
\\ ( https://arxiv.org/abs/2404.04718 ,  5889kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04736 (*cross-listing*)
Date: Sat, 6 Apr 2024 21:39:49 GMT   (96kb,D)

Title: ProtoAL: Interpretable Deep Active Learning with prototypes for medical
  imaging
Authors: Iury B. de A. Santos, Andr\'e C.P.L.F. de Carvalho
Categories: cs.CV cs.AI cs.LG
\\
  The adoption of Deep Learning algorithms in the medical imaging field is a
prominent area of research, with high potential for advancing AI-based
Computer-aided diagnosis (AI-CAD) solutions. However, current solutions face
challenges due to a lack of interpretability features and high data demands,
prompting recent efforts to address these issues. In this study, we propose the
ProtoAL method, where we integrate an interpretable DL model into the Deep
Active Learning (DAL) framework. This approach aims to address both challenges
by focusing on the medical imaging context and utilizing an inherently
interpretable model based on prototypes. We evaluated ProtoAL on the Messidor
dataset, achieving an area under the precision-recall curve of 0.79 while
utilizing only 76.54\% of the available labeled data. These capabilities can
enhances the practical usability of a DL model in the medical field, providing
a means of trust calibration in domain experts and a suitable solution for
learning in the data scarcity context often found.
\\ ( https://arxiv.org/abs/2404.04736 ,  96kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04744 (*cross-listing*)
Date: Sat, 6 Apr 2024 22:08:09 GMT   (1676kb,D)

Title: Adapting Multi-objectivized Software Configuration Tuning
Authors: Tao Chen and Miqing Li
Categories: cs.SE cs.AI cs.DC
Comments: This paper has been accepted at ACM FSE'24
DOI: 10.1145/3643751
\\
  When tuning software configuration for better performance (e.g., latency or
throughput), an important issue that many optimizers face is the presence of
local optimum traps, compounded by a highly rugged configuration landscape and
expensive measurements. To mitigate these issues, a recent effort has shifted
to focus on the level of optimization model (called meta multi-objectivization
or MMO) instead of designing better optimizers as in traditional methods. This
is done by using an auxiliary performance objective, together with the target
performance objective, to help the search jump out of local optima. While
effective, MMO needs a fixed weight to balance the two objectives-a parameter
that has been found to be crucial as there is a large deviation of the
performance between the best and the other settings. However, given the variety
of configurable software systems, the "sweet spot" of the weight can vary
dramatically in different cases and it is not possible to find the right
setting without time-consuming trial and error. In this paper, we seek to
overcome this significant shortcoming of MMO by proposing a weight adaptation
method, dubbed AdMMO. Our key idea is to adaptively adjust the weight at the
right time during tuning, such that a good proportion of the nondominated
configurations can be maintained. Moreover, we design a partial duplicate
retention mechanism to handle the issue of too many duplicate configurations
without losing the rich information provided by the "good" duplicates.
  Experiments on several real-world systems, objectives, and budgets show that,
for 71% of the cases, AdMMO is considerably superior to MMO and a wide range of
state-of-the-art optimizers while achieving generally better efficiency with
the best speedup between 2.2x and 20x.
\\ ( https://arxiv.org/abs/2404.04744 ,  1676kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04763 (*cross-listing*)
Date: Sun, 7 Apr 2024 00:28:13 GMT   (2965kb,D)

Title: GenEARL: A Training-Free Generative Framework for Multimodal Event
  Argument Role Labeling
Authors: Hritik Bansal, Po-Nien Kung, P. Jeffrey Brantingham, Kai-Wei Chang,
  Nanyun Peng
Categories: cs.CV cs.AI
Comments: 20 pages, 15 Figures, 13 figures
\\
  Multimodal event argument role labeling (EARL), a task that assigns a role
for each event participant (object) in an image is a complex challenge. It
requires reasoning over the entire image, the depicted event, and the
interactions between various objects participating in the event. Existing
models heavily rely on high-quality event-annotated training data to understand
the event semantics and structures, and they fail to generalize to new event
types and domains. In this paper, we propose GenEARL, a training-free
generative framework that harness the power of the modern generative models to
understand event task descriptions given image contexts to perform the EARL
task. Specifically, GenEARL comprises two stages of generative prompting with a
frozen vision-language model (VLM) and a frozen large language model (LLM).
First, a generative VLM learns the semantics of the event argument roles and
generates event-centric object descriptions based on the image. Subsequently, a
LLM is prompted with the generated object descriptions with a predefined
template for EARL (i.e., assign an object with an event argument role). We show
that GenEARL outperforms the contrastive pretraining (CLIP) baseline by 9.4%
and 14.2% accuracy for zero-shot EARL on the M2E2 and SwiG datasets,
respectively. In addition, we outperform CLIP-Event by 22% precision on M2E2
dataset. The framework also allows flexible adaptation and generalization to
unseen domains.
\\ ( https://arxiv.org/abs/2404.04763 ,  2965kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04821 (*cross-listing*)
Date: Sun, 7 Apr 2024 06:05:25 GMT   (1389kb,D)

Title: A Data-to-Product Multimodal Conceptual Framework to Achieve Automated
  Software Evolution for Context-rich Intelligent Applications
Authors: Songhui Yue
Categories: cs.SE cs.AI
\\
  While AI is extensively transforming Software Engineering (SE) fields, SE is
still in need of a framework to overall consider all phases to facilitate
Automated Software Evolution (ASEv), particularly for intelligent applications
that are context-rich, instead of conquering each division independently. Its
complexity comes from the intricacy of the intelligent applications, the
heterogeneity of the data sources, and the constant changes in the context.
This study proposes a conceptual framework for achieving automated software
evolution, emphasizing the importance of multimodality learning. A Selective
Sequential Scope Model (3S) model is developed based on the conceptual
framework, and it can be used to categorize existing and future research when
it covers different SE phases and multimodal learning tasks. This research is a
preliminary step toward the blueprint of a higher-level ASEv. The proposed
conceptual framework can act as a practical guideline for practitioners to
prepare themselves for diving into this area. Although the study is about
intelligent applications, the framework and analysis methods may be adapted for
other types of software as AI brings more intelligence into their life cycles.
\\ ( https://arxiv.org/abs/2404.04821 ,  1389kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04839 (*cross-listing*)
Date: Sun, 7 Apr 2024 07:24:58 GMT   (618kb)

Title: AI for DevSecOps: A Landscape and Future Opportunities
Authors: Michael Fu, Jirat Pasuksmit, Chakkrit Tantithamthavorn
Categories: cs.SE cs.AI
\\
  DevOps has emerged as one of the most rapidly evolving software development
paradigms. With the growing concerns surrounding security in software systems,
the DevSecOps paradigm has gained prominence, urging practitioners to
incorporate security practices seamlessly into the DevOps workflow. However,
integrating security into the DevOps workflow can impact agility and impede
delivery speed. Recently, the advancement of artificial intelligence (AI) has
revolutionized automation in various software domains, including software
security. AI-driven security approaches, particularly those leveraging machine
learning or deep learning, hold promise in automating security workflows. They
reduce manual efforts, which can be integrated into DevOps to ensure
uninterrupted delivery speed and align with the DevSecOps paradigm
simultaneously. This paper seeks to contribute to the critical intersection of
AI and DevSecOps by presenting a comprehensive landscape of AI-driven security
techniques applicable to DevOps and identifying avenues for enhancing security,
trust, and efficiency in software development processes. We analyzed 99
research papers spanning from 2017 to 2023. Specifically, we address two key
research questions (RQs). In RQ1, we identified 12 security tasks associated
with the DevOps process and reviewed existing AI-driven security approaches. In
RQ2, we discovered 15 challenges encountered by existing AI-driven security
approaches and derived future research opportunities. Drawing insights from our
findings, we discussed the state-of-the-art AI-driven security approaches,
highlighted challenges in existing research, and proposed avenues for future
opportunities.
\\ ( https://arxiv.org/abs/2404.04839 ,  618kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04848 (*cross-listing*)
Date: Sun, 7 Apr 2024 07:42:04 GMT   (1693kb,D)

Title: Task-Aware Encoder Control for Deep Video Compression
Authors: Xingtong Ge, Jixiang Luo, Xinjie Zhang, Tongda Xu, Guo Lu, Dailan He,
  Jing Geng, Yan Wang, Jun Zhang, Hongwei Qin
Categories: eess.IV cs.AI cs.CV
Comments: Accepted by CVPR 2024
\\
  Prior research on deep video compression (DVC) for machine tasks typically
necessitates training a unique codec for each specific task, mandating a
dedicated decoder per task. In contrast, traditional video codecs employ a
flexible encoder controller, enabling the adaptation of a single codec to
different tasks through mechanisms like mode prediction. Drawing inspiration
from this, we introduce an innovative encoder controller for deep video
compression for machines. This controller features a mode prediction and a
Group of Pictures (GoP) selection module. Our approach centralizes control at
the encoding stage, allowing for adaptable encoder adjustments across different
tasks, such as detection and tracking, while maintaining compatibility with a
standard pre-trained DVC decoder. Empirical evidence demonstrates that our
method is applicable across multiple tasks with various existing pre-trained
DVCs. Moreover, extensive experiments demonstrate that our method outperforms
previous DVC by about 25% bitrate for different tasks, with only one
pre-trained decoder.
\\ ( https://arxiv.org/abs/2404.04848 ,  1693kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04849 (*cross-listing*)
Date: Sun, 7 Apr 2024 07:42:12 GMT   (237kb,D)

Title: Hidden You Malicious Goal Into Benigh Narratives: Jailbreak Large
  Language Models through Logic Chain Injection
Authors: Zhilong Wang, Yebo Cao, and Peng Liu
Categories: cs.CR cs.AI
\\
  Jailbreak attacks on Language Model Models (LLMs) entail crafting prompts
aimed at exploiting the models to generate malicious content. Existing
jailbreak attacks can successfully deceive the LLMs, however they cannot
deceive the human. This paper proposes a new type of jailbreak attacks which
can deceive both the LLMs and human (i.e., security analyst). The key insight
of our idea is borrowed from the social psychology - that is human are easily
deceived if the lie is hidden in truth. Based on this insight, we proposed the
logic-chain injection attacks to inject malicious intention into benign truth.
Logic-chain injection attack firstly dissembles its malicious target into a
chain of benign narrations, and then distribute narrations into a related
benign article, with undoubted facts. In this way, newly generate prompt cannot
only deceive the LLMs, but also deceive human.
\\ ( https://arxiv.org/abs/2404.04849 ,  237kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04856 (*cross-listing*)
Date: Sun, 7 Apr 2024 08:03:42 GMT   (10508kb,D)

Title: Msmsfnet: a multi-stream and multi-scale fusion net for edge detection
Authors: Chenguang Liu, Chisheng Wang, Feifei Dong, Xin Su, Chuanhua Zhu, Dejin
  Zhang, Qingquan Li
Categories: cs.CV cs.AI
\\
  Edge detection is a long standing problem in computer vision. Recent deep
learning based algorithms achieve state of-the-art performance in publicly
available datasets. Despite the efficiency of these algorithms, their
performance, however, relies heavily on the pretrained weights of the backbone
network on the ImageNet dataset. This limits heavily the design space of deep
learning based edge detectors. Whenever we want to devise a new model, we have
to train this new model on the ImageNet dataset first, and then fine tune the
model using the edge detection datasets. The comparison would be unfair
otherwise. However, it is usually not feasible for many researchers to train a
model on the ImageNet dataset due to the limited computation resources. In this
work, we study the performance that can be achieved by state-of-the-art deep
learning based edge detectors in publicly available datasets when they are
trained from scratch, and devise a new network architecture, the multi-stream
and multi scale fusion net (msmsfnet), for edge detection. We show in our
experiments that by training all models from scratch to ensure the fairness of
comparison, out model outperforms state-of-the art deep learning based edge
detectors in three publicly available datasets.
\\ ( https://arxiv.org/abs/2404.04856 ,  10508kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04869 (*cross-listing*)
Date: Sun, 7 Apr 2024 08:31:12 GMT   (8139kb,D)

Title: Prompting Multi-Modal Tokens to Enhance End-to-End Autonomous Driving
  Imitation Learning with LLMs
Authors: Yiqun Duan, Qiang Zhang, Renjing Xu
Categories: cs.RO cs.AI
\\
  The utilization of Large Language Models (LLMs) within the realm of
reinforcement learning, particularly as planners, has garnered a significant
degree of attention in recent scholarly literature. However, a substantial
proportion of existing research predominantly focuses on planning models for
robotics that transmute the outputs derived from perception models into
linguistic forms, thus adopting a `pure-language' strategy. In this research,
we propose a hybrid End-to-End learning framework for autonomous driving by
combining basic driving imitation learning with LLMs based on multi-modality
prompt tokens. Instead of simply converting perception results from the
separated train model into pure language input, our novelty lies in two
aspects. 1) The end-to-end integration of visual and LiDAR sensory input into
learnable multi-modality tokens, thereby intrinsically alleviating description
bias by separated pre-trained perception models. 2) Instead of directly letting
LLMs drive, this paper explores a hybrid setting of letting LLMs help the
driving model correct mistakes and complicated scenarios. The results of our
experiments suggest that the proposed methodology can attain driving scores of
49.21%, coupled with an impressive route completion rate of 91.34% in the
offline evaluation conducted via CARLA. These performance metrics are
comparable to the most advanced driving models.
\\ ( https://arxiv.org/abs/2404.04869 ,  8139kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04886 (*cross-listing*)
Date: Sun, 7 Apr 2024 09:06:14 GMT   (884kb,D)

Title: PagPassGPT: Pattern Guided Password Guessing via Generative Pretrained
  Transformer
Authors: Xingyu Su, Xiaojie Zhu, Yang Li, Yong Li, Chi Chen, Paulo
  Esteves-Ver\'issimo
Categories: cs.CR cs.AI
\\
  Amidst the surge in deep learning-based password guessing models, challenges
of generating high-quality passwords and reducing duplicate passwords persist.
To address these challenges, we present PagPassGPT, a password guessing model
constructed on Generative Pretrained Transformer (GPT). It can perform pattern
guided guessing by incorporating pattern structure information as background
knowledge, resulting in a significant increase in the hit rate. Furthermore, we
propose D&C-GEN to reduce the repeat rate of generated passwords, which adopts
the concept of a divide-and-conquer approach. The primary task of guessing
passwords is recursively divided into non-overlapping subtasks. Each subtask
inherits the knowledge from the parent task and predicts succeeding tokens. In
comparison to the state-of-the-art model, our proposed scheme exhibits the
capability to correctly guess 12% more passwords while producing 25% fewer
duplicates.
\\ ( https://arxiv.org/abs/2404.04886 ,  884kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04904 (*cross-listing*)
Date: Sun, 7 Apr 2024 10:10:15 GMT   (1755kb,D)

Title: Cross-Domain Audio Deepfake Detection: Dataset and Analysis
Authors: Yuang Li, Min Zhang, Mengxin Ren, Miaomiao Ma, Daimeng Wei, Hao Yang
Categories: cs.SD cs.AI
\\
  Audio deepfake detection (ADD) is essential for preventing the misuse of
synthetic voices that may infringe on personal rights and privacy. Recent
zero-shot text-to-speech (TTS) models pose higher risks as they can clone
voices with a single utterance. However, the existing ADD datasets are
outdated, leading to suboptimal generalization of detection models. In this
paper, we construct a new cross-domain ADD dataset comprising over 300 hours of
speech data that is generated by five advanced zero-shot TTS models. To
simulate real-world scenarios, we employ diverse attack methods and audio
prompts from different datasets. Experiments show that, through novel
attack-augmented training, the Wav2Vec2-large and Whisper-medium models achieve
equal error rates of 4.1\% and 6.5\% respectively. Additionally, we demonstrate
our models' outstanding few-shot ADD ability by fine-tuning with just one
minute of target-domain data. Nonetheless, neural codec compressors greatly
affect the detection accuracy, necessitating further research.
\\ ( https://arxiv.org/abs/2404.04904 ,  1755kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04905 (*cross-listing*)
Date: Sun, 7 Apr 2024 10:11:22 GMT   (1080kb,D)

Title: Review for Handling Missing Data with special missing mechanism
Authors: Youran Zhou, Sunil Aryal, Mohamed Reda Bouadjenek
Categories: stat.ME cs.AI cs.LG
\\
  Missing data poses a significant challenge in data science, affecting
decision-making processes and outcomes. Understanding what missing data is, how
it occurs, and why it is crucial to handle it appropriately is paramount when
working with real-world data, especially in tabular data, one of the most
commonly used data types in the real world. Three missing mechanisms are
defined in the literature: Missing Completely At Random (MCAR), Missing At
Random (MAR), and Missing Not At Random (MNAR), each presenting unique
challenges in imputation. Most existing work are focused on MCAR that is
relatively easy to handle. The special missing mechanisms of MNAR and MAR are
less explored and understood. This article reviews existing literature on
handling missing values. It compares and contrasts existing methods in terms of
their ability to handle different missing mechanisms and data types. It
identifies research gap in the existing literature and lays out potential
directions for future research in the field. The information in this review
will help data analysts and researchers to adopt and promote good practices for
handling missing data in real-world problems.
\\ ( https://arxiv.org/abs/2404.04905 ,  1080kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04922 (*cross-listing*)
Date: Sun, 7 Apr 2024 11:25:04 GMT   (15404kb,D)

Title: Efficient Learnable Collaborative Attention for Single Image
  Super-Resolution
Authors: Yigang Zhao Chaowei Zheng, Jiannan Su, GuangyongChen and MinGan
Categories: cs.CV cs.AI
\\
  Non-Local Attention (NLA) is a powerful technique for capturing long-range
feature correlations in deep single image super-resolution (SR). However, NLA
suffers from high computational complexity and memory consumption, as it
requires aggregating all non-local feature information for each query response
and recalculating the similarity weight distribution for different abstraction
levels of features. To address these challenges, we propose a novel Learnable
Collaborative Attention (LCoA) that introduces inductive bias into non-local
modeling. Our LCoA consists of two components: Learnable Sparse Pattern (LSP)
and Collaborative Attention (CoA). LSP uses the k-means clustering algorithm to
dynamically adjust the sparse attention pattern of deep features, which reduces
the number of non-local modeling rounds compared with existing sparse
solutions. CoA leverages the sparse attention pattern and weights learned by
LSP, and co-optimizes the similarity matrix across different abstraction
levels, which avoids redundant similarity matrix calculations. The experimental
results show that our LCoA can reduce the non-local modeling time by about 83%
in the inference stage. In addition, we integrate our LCoA into a deep
Learnable Collaborative Attention Network (LCoAN), which achieves competitive
performance in terms of inference time, memory consumption, and reconstruction
quality compared with other state-of-the-art SR methods.
\\ ( https://arxiv.org/abs/2404.04922 ,  15404kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04924 (*cross-listing*)
Date: Sun, 7 Apr 2024 11:48:07 GMT   (261kb,D)

Title: GvT: A Graph-based Vision Transformer with Talking-Heads Utilizing
  Sparsity, Trained from Scratch on Small Datasets
Authors: Dongjing Shan, guiqiang chen
Categories: cs.CV cs.AI
\\
  Vision Transformers (ViTs) have achieved impressive results in large-scale
image classification. However, when training from scratch on small datasets,
there is still a significant performance gap between ViTs and Convolutional
Neural Networks (CNNs), which is attributed to the lack of inductive bias. To
address this issue, we propose a Graph-based Vision Transformer (GvT) that
utilizes graph convolutional projection and graph-pooling. In each block,
queries and keys are calculated through graph convolutional projection based on
the spatial adjacency matrix, while dot-product attention is used in another
graph convolution to generate values. When using more attention heads, the
queries and keys become lower-dimensional, making their dot product an
uninformative matching function. To overcome this low-rank bottleneck in
attention heads, we employ talking-heads technology based on bilinear pooled
features and sparse selection of attention tensors. This allows interaction
among filtered attention scores and enables each attention mechanism to depend
on all queries and keys. Additionally, we apply graph-pooling between two
intermediate blocks to reduce the number of tokens and aggregate semantic
information more effectively. Our experimental results show that GvT produces
comparable or superior outcomes to deep convolutional networks and surpasses
vision transformers without pre-training on large datasets. The code for our
proposed model is publicly available on the website.
\\ ( https://arxiv.org/abs/2404.04924 ,  261kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04947 (*cross-listing*)
Date: Sun, 7 Apr 2024 12:57:46 GMT   (1741kb,D)

Title: Gull: A Generative Multifunctional Audio Codec
Authors: Yi Luo, Jianwei Yu, Hangting Chen, Rongzhi Gu, Chao Weng
Categories: eess.AS cs.AI cs.LG cs.SD eess.SP
Comments: Demo page: https://yluo42.github.io/Gull/
\\
  We introduce Gull, a generative multifunctional audio codec. Gull is a
general purpose neural audio compression and decompression model which can be
applied to a wide range of tasks and applications such as real-time
communication, audio super-resolution, and codec language models. The key
components of Gull include (1) universal-sample-rate modeling via subband
modeling schemes motivated by recent progress in audio source separation, (2)
gain-shape representations motivated by traditional audio codecs, (3) improved
residual vector quantization modules for simpler training, (4) elastic decoder
network that enables user-defined model size and complexity during inference
time, (5) built-in ability for audio super-resolution without the increase of
bitrate. We compare Gull with existing traditional and neural audio codecs and
show that Gull is able to achieve on par or better performance across various
sample rates, bitrates and model complexities in both subjective and objective
evaluation metrics.
\\ ( https://arxiv.org/abs/2404.04947 ,  1741kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04974 (*cross-listing*)
Date: Sun, 7 Apr 2024 14:33:06 GMT   (120kb,D)

Title: Neural Network Modeling for Forecasting Tourism Demand in Stopi\'{c}a
  Cave: A Serbian Cave Tourism Study
Authors: Buda Baji\'c, Sr{\dj}an Mili\'cevi\'c, Aleksandar Anti\'c, Slobodan
  Markovi\'c, Nemanja Tomi\'c
Categories: econ.EM cs.AI
\\
  For modeling the number of visits in Stopi\'{c}a cave (Serbia) we consider
the classical Auto-regressive Integrated Moving Average (ARIMA) model, Machine
Learning (ML) method Support Vector Regression (SVR), and hybrid NeuralPropeth
method which combines classical and ML concepts. The most accurate predictions
were obtained with NeuralPropeth which includes the seasonal component and
growing trend of time-series. In addition, non-linearity is modeled by shallow
Neural Network (NN), and Google Trend is incorporated as an exogenous variable.
Modeling tourist demand represents great importance for management structures
and decision-makers due to its applicability in establishing sustainable
tourism utilization strategies in environmentally vulnerable destinations such
as caves. The data provided insights into the tourist demand in Stopi\'{c}a
cave and preliminary data for addressing the issues of carrying capacity within
the most visited cave in Serbia.
\\ ( https://arxiv.org/abs/2404.04974 ,  120kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04983 (*cross-listing*)
Date: Sun, 7 Apr 2024 15:03:46 GMT   (2105kb)

Title: Primary liver cancer classification from routine tumour biopsy using
  weakly supervised deep learning
Authors: Aur\'elie Beaufr\`ere, Nora Ouzir, Paul Emile Zafar, Astrid
  Laurent-Bellue, Miguel Albuquerque, Gwladys Lubuela, Jules Gr\'egory,
  Catherine Guettier, K\'evin Mondet, Jean-Christophe Pesquet, Val\'erie
  Paradis
Categories: q-bio.TO cs.AI cs.CV
Comments: https://www.sciencedirect.com/science/article/pii/S2589555924000090
ACM-class: I.5; I.4; I.2
Journal-ref: JHEP Reports, Volume 6, Issue 3, 2024
DOI: 10.1016/j.jhepr.2024.101008
\\
  The diagnosis of primary liver cancers (PLCs) can be challenging, especially
on biopsies and for combined hepatocellular-cholangiocarcinoma (cHCC-CCA). We
automatically classified PLCs on routine-stained biopsies using a weakly
supervised learning method. Weak tumour/non-tumour annotations served as labels
for training a Resnet18 neural network, and the network's last convolutional
layer was used to extract new tumour tile features. Without knowledge of the
precise labels of the malignancies, we then applied an unsupervised clustering
algorithm. Our model identified specific features of hepatocellular carcinoma
(HCC) and intrahepatic cholangiocarcinoma (iCCA). Despite no specific features
of cHCC-CCA being recognized, the identification of HCC and iCCA tiles within a
slide could facilitate the diagnosis of primary liver cancers, particularly
cHCC-CCA.
  Method and results: 166 PLC biopsies were divided into training, internal and
external validation sets: 90, 29 and 47 samples. Two liver pathologists
reviewed each whole-slide hematein eosin saffron (HES)-stained image (WSI).
After annotating the tumour/non-tumour areas, 256x256 pixel tiles were
extracted from the WSIs and used to train a ResNet18. The network was used to
extract new tile features. An unsupervised clustering algorithm was then
applied to the new tile features. In a two-cluster model, Clusters 0 and 1
contained mainly HCC and iCCA histological features. The diagnostic agreement
between the pathological diagnosis and the model predictions in the internal
and external validation sets was 100% (11/11) and 96% (25/26) for HCC and 78%
(7/9) and 87% (13/15) for iCCA, respectively. For cHCC-CCA, we observed a
highly variable proportion of tiles from each cluster (Cluster 0: 5-97%;
Cluster 1: 2-94%).
\\ ( https://arxiv.org/abs/2404.04983 ,  2105kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04986 (*cross-listing*)
Date: Sun, 7 Apr 2024 15:06:48 GMT   (25092kb,D)

Title: Dynamic Distinction Learning: Adaptive Pseudo Anomalies for Video
  Anomaly Detection
Authors: Demetris Lappas, Vasileios Argyriou, Dimitrios Makris
Categories: cs.CV cs.AI
Comments: To be published in the CVPR2024 Workshop
\\
  We introduce Dynamic Distinction Learning (DDL) for Video Anomaly Detection,
a novel video anomaly detection methodology that combines pseudo-anomalies,
dynamic anomaly weighting, and a distinction loss function to improve detection
accuracy. By training on pseudo-anomalies, our approach adapts to the
variability of normal and anomalous behaviors without fixed anomaly thresholds.
Our model showcases superior performance on the Ped2, Avenue and ShanghaiTech
datasets, where individual models are tailored for each scene. These
achievements highlight DDL's effectiveness in advancing anomaly detection,
offering a scalable and adaptable solution for video surveillance challenges.
\\ ( https://arxiv.org/abs/2404.04986 ,  25092kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04998 (*cross-listing*)
Date: Sun, 7 Apr 2024 15:48:33 GMT   (2843kb,D)

Title: Weakly Supervised Deep Hyperspherical Quantization for Image Retrieval
Authors: Jinpeng Wang, Bin Chen, Qiang Zhang, Zaiqiao Meng, Shangsong Liang,
  Shu-Tao Xia
Categories: cs.CV cs.AI cs.IR
Comments: In proceedings of AAAI 2021. Code and data are available
DOI: 10.1609/aaai.v35i4.16380
\\
  Deep quantization methods have shown high efficiency on large-scale image
retrieval. However, current models heavily rely on ground-truth information,
hindering the application of quantization in label-hungry scenarios. A more
realistic demand is to learn from inexhaustible uploaded images that are
associated with informal tags provided by amateur users. Though such sketchy
tags do not obviously reveal the labels, they actually contain useful semantic
information for supervising deep quantization. To this end, we propose
Weakly-Supervised Deep Hyperspherical Quantization (WSDHQ), which is the first
work to learn deep quantization from weakly tagged images. Specifically, 1) we
use word embeddings to represent the tags and enhance their semantic
information based on a tag correlation graph. 2) To better preserve semantic
information in quantization codes and reduce quantization error, we jointly
learn semantics-preserving embeddings and supervised quantizer on hypersphere
by employing a well-designed fusion layer and tailor-made loss functions.
Extensive experiments show that WSDHQ can achieve state-of-art performance on
weakly-supervised compact coding. Code is available at
https://github.com/gimpong/AAAI21-WSDHQ.
\\ ( https://arxiv.org/abs/2404.04998 ,  2843kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05003 (*cross-listing*)
Date: Sun, 7 Apr 2024 15:58:25 GMT   (4330kb,D)

Title: Camera-Based Remote Physiology Sensing for Hundreds of Subjects Across
  Skin Tones
Authors: Jiankai Tang, Xinyi Li, Jiacheng Liu, Xiyuxing Zhang, Zeyu Wang,
  Yuntao Wang
Categories: cs.CV cs.AI
Comments: 11 pages, 5 figures, CHI24 Workshop PhysioCHI
\\
  Remote photoplethysmography (rPPG) emerges as a promising method for
non-invasive, convenient measurement of vital signs, utilizing the widespread
presence of cameras. Despite advancements, existing datasets fall short in
terms of size and diversity, limiting comprehensive evaluation under diverse
conditions. This paper presents an in-depth analysis of the VitalVideo dataset,
the largest real-world rPPG dataset to date, encompassing 893 subjects and 6
Fitzpatrick skin tones. Our experimentation with six unsupervised methods and
three supervised models demonstrates that datasets comprising a few hundred
subjects(i.e., 300 for UBFC-rPPG, 500 for PURE, and 700 for MMPD-Simple) are
sufficient for effective rPPG model training. Our findings highlight the
importance of diversity and consistency in skin tones for precise performance
evaluation across different datasets.
\\ ( https://arxiv.org/abs/2404.05003 ,  4330kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05061 (*cross-listing*)
Date: Sun, 7 Apr 2024 20:15:40 GMT   (904kb,D)

Title: Automated Prediction of Breast Cancer Response to Neoadjuvant
  Chemotherapy from DWI Data
Authors: Shir Nitzan, Maya Gilad and Moti Freiman
Categories: cs.CV cs.AI
Comments: Accepted for presentation at the IEEE International Symposium on
  Biomedical Imaging (ISBI)
\\
  Effective surgical planning for breast cancer hinges on accurately predicting
pathological complete response (pCR) to neoadjuvant chemotherapy (NAC).
Diffusion-weighted MRI (DWI) and machine learning offer a non-invasive approach
for early pCR assessment. However, most machine-learning models require manual
tumor segmentation, a cumbersome and error-prone task. We propose a deep
learning model employing "Size-Adaptive Lesion Weighting" for automatic DWI
tumor segmentation to enhance pCR prediction accuracy. Despite
histopathological changes during NAC complicating DWI image segmentation, our
model demonstrates robust performance. Utilizing the BMMR2 challenge dataset,
it matches human experts in pCR prediction pre-NAC with an area under the curve
(AUC) of 0.76 vs. 0.796, and surpasses standard automated methods mid-NAC, with
an AUC of 0.729 vs. 0.654 and 0.576. Our approach represents a significant
advancement in automating breast cancer treatment planning, enabling more
reliable pCR predictions without manual segmentation.
\\ ( https://arxiv.org/abs/2404.05061 ,  904kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05101 (*cross-listing*)
Date: Sun, 7 Apr 2024 22:53:43 GMT   (978kb,D)

Title: StockGPT: A GenAI Model for Stock Prediction and Trading
Authors: Dat Mai
Categories: q-fin.CP cs.AI q-fin.PM q-fin.PR q-fin.ST
Comments: 19 pages, 3 figures, 6 tables
\\
  This paper introduces StockGPT, an autoregressive "number" model pretrained
directly on the history of daily U.S. stock returns. Treating each return
series as a sequence of tokens, the model excels at understanding and
predicting the highly intricate stock return dynamics. Instead of relying on
handcrafted trading patterns using historical stock prices, StockGPT
automatically learns the hidden representations predictive of future returns
via its attention mechanism. On a held-out test sample from 2001 to 2023, a
daily rebalanced long-short portfolio formed from StockGPT predictions earns an
annual return of 119% with a Sharpe ratio of 6.5. The StockGPT-based portfolio
completely explains away momentum and long-/short-term reversals, eliminating
the need for manually crafted price-based strategies and also encompasses most
leading stock market factors. This highlights the immense promise of generative
AI in surpassing human in making complex financial investment decisions and
illustrates the efficacy of the attention mechanism of large language models
when applied to a completely different domain.
\\ ( https://arxiv.org/abs/2404.05101 ,  978kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05136 (*cross-listing*)
Date: Mon, 8 Apr 2024 01:29:10 GMT   (936kb,D)

Title: Self-Supervised Multi-Object Tracking with Path Consistency
Authors: Zijia Lu, Bing Shuai, Yanbei Chen, Zhenlin Xu, Davide Modolo
Categories: cs.CV cs.AI
Comments: Accepted at CVPR 2024
\\
  In this paper, we propose a novel concept of path consistency to learn robust
object matching without using manual object identity supervision. Our key idea
is that, to track a object through frames, we can obtain multiple different
association results from a model by varying the frames it can observe, i.e.,
skipping frames in observation. As the differences in observations do not alter
the identities of objects, the obtained association results should be
consistent. Based on this rationale, we generate multiple observation paths,
each specifying a different set of frames to be skipped, and formulate the Path
Consistency Loss that enforces the association results are consistent across
different observation paths. We use the proposed loss to train our object
matching model with only self-supervision. By extensive experiments on three
tracking datasets (MOT17, PersonPath22, KITTI), we demonstrate that our method
outperforms existing unsupervised methods with consistent margins on various
evaluation metrics, and even achieves performance close to supervised methods.
\\ ( https://arxiv.org/abs/2404.05136 ,  936kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05180 (*cross-listing*)
Date: Mon, 8 Apr 2024 04:10:50 GMT   (3764kb,D)

Title: GloSoFarID: Global multispectral dataset for Solar Farm IDentification
  in satellite imagery
Authors: Zhiyuan Yang and Ryan Rad
Categories: cs.CV cs.AI
\\
  Solar Photovoltaic (PV) technology is increasingly recognized as a pivotal
solution in the global pursuit of clean and renewable energy. This technology
addresses the urgent need for sustainable energy alternatives by converting
solar power into electricity without greenhouse gas emissions. It not only
curtails global carbon emissions but also reduces reliance on finite,
non-renewable energy sources. In this context, monitoring solar panel farms
becomes essential for understanding and facilitating the worldwide shift toward
clean energy. This study contributes to this effort by developing the first
comprehensive global dataset of multispectral satellite imagery of solar panel
farms. This dataset is intended to form the basis for training robust machine
learning models, which can accurately map and analyze the expansion and
distribution of solar panel farms globally. The insights gained from this
endeavor will be instrumental in guiding informed decision-making for a
sustainable energy future. https://github.com/yzyly1992/GloSoFarID
\\ ( https://arxiv.org/abs/2404.05180 ,  3764kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05188 (*cross-listing*)
Date: Mon, 8 Apr 2024 04:30:33 GMT   (851kb,D)

Title: Have You Merged My Model? On The Robustness of Large Language Model IP
  Protection Methods Against Model Merging
Authors: Tianshuo Cong and Delong Ran and Zesen Liu and Xinlei He and Jinyuan
  Liu and Yichen Gong and Qi Li and Anyu Wang and Xiaoyun Wang
Categories: cs.CR cs.AI cs.CL
Comments: Technical Report
\\
  Model merging is a promising lightweight model empowerment technique that
does not rely on expensive computing devices (e.g., GPUs) or require the
collection of specific training data. Instead, it involves editing different
upstream model parameters to absorb their downstream task capabilities.
However, uncertified model merging can infringe upon the Intellectual Property
(IP) rights of the original upstream models. In this paper, we conduct the
first study on the robustness of IP protection methods in model merging
scenarios. We investigate two state-of-the-art IP protection techniques:
Quantization Watermarking and Instructional Fingerprint, along with various
advanced model merging technologies, such as Task Arithmetic, TIES-MERGING, and
so on. Experimental results indicate that current Large Language Model (LLM)
watermarking techniques cannot survive in the merged models, whereas model
fingerprinting techniques can. Our research aims to highlight that model
merging should be an indispensable consideration in the robustness assessment
of model IP protection techniques, thereby promoting the healthy development of
the open-source LLM community.
\\ ( https://arxiv.org/abs/2404.05188 ,  851kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05206 (*cross-listing*)
Date: Mon, 8 Apr 2024 05:19:28 GMT   (7118kb,D)

Title: SoundingActions: Learning How Actions Sound from Narrated Egocentric
  Videos
Authors: Changan Chen, Kumar Ashutosh, Rohit Girdhar, David Harwath, Kristen
  Grauman
Categories: cs.CV cs.AI cs.SD
Comments: Accepted at CVPR 2024. Project page:
  https://vision.cs.utexas.edu/projects/soundingactions
\\
  We propose a novel self-supervised embedding to learn how actions sound from
narrated in-the-wild egocentric videos. Whereas existing methods rely on
curated data with known audio-visual correspondence, our multimodal
contrastive-consensus coding (MC3) embedding reinforces the associations
between audio, language, and vision when all modality pairs agree, while
diminishing those associations when any one pair does not. We show our approach
can successfully discover how the long tail of human actions sound from
egocentric video, outperforming an array of recent multimodal embedding
techniques on two datasets (Ego4D and EPIC-Sounds) and multiple cross-modal
tasks.
\\ ( https://arxiv.org/abs/2404.05206 ,  7118kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05213 (*cross-listing*)
Date: Mon, 8 Apr 2024 06:00:14 GMT   (120kb,D)

Title: Evaluation of an LLM in Identifying Logical Fallacies: A Call for Rigor
  When Adopting LLMs in HCI Research
Authors: Gionnieve Lim, Simon T. Perrault
Categories: cs.HC cs.AI
\\
  There is increasing interest in the adoption of LLMs in HCI research.
However, LLMs may often be regarded as a panacea because of their powerful
capabilities with an accompanying oversight on whether they are suitable for
their intended tasks. We contend that LLMs should be adopted in a critical
manner following rigorous evaluation. Accordingly, we present the evaluation of
an LLM in identifying logical fallacies that will form part of a digital
misinformation intervention. By comparing to a labeled dataset, we found that
GPT-4 achieves an accuracy of 0.79, and for our intended use case that excludes
invalid or unidentified instances, an accuracy of 0.90. This gives us the
confidence to proceed with the application of the LLM while keeping in mind the
areas where it still falls short. The paper describes our evaluation approach,
results and reflections on the use of the LLM for our intended task.
\\ ( https://arxiv.org/abs/2404.05213 ,  120kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05218 (*cross-listing*)
Date: Mon, 8 Apr 2024 06:15:13 GMT   (32765kb,D)

Title: Multi-agent Long-term 3D Human Pose Forecasting via Interaction-aware
  Trajectory Conditioning
Authors: Jaewoo Jeong, Daehee Park, Kuk-Jin Yoon
Categories: cs.CV cs.AI
Comments: 2024 CVPR Highlight
\\
  Human pose forecasting garners attention for its diverse applications.
However, challenges in modeling the multi-modal nature of human motion and
intricate interactions among agents persist, particularly with longer
timescales and more agents. In this paper, we propose an interaction-aware
trajectory-conditioned long-term multi-agent human pose forecasting model,
utilizing a coarse-to-fine prediction approach: multi-modal global trajectories
are initially forecasted, followed by respective local pose forecasts
conditioned on each mode. In doing so, our Trajectory2Pose model introduces a
graph-based agent-wise interaction module for a reciprocal forecast of local
motion-conditioned global trajectory and trajectory-conditioned local pose. Our
model effectively handles the multi-modality of human motion and the complexity
of long-term multi-agent interactions, improving performance in complex
environments. Furthermore, we address the lack of long-term (6s+) multi-agent
(5+) datasets by constructing a new dataset from real-world images and 2D
annotations, enabling a comprehensive evaluation of our proposed model.
State-of-the-art prediction performance on both complex and simpler datasets
confirms the generalized effectiveness of our method. The code is available at
https://github.com/Jaewoo97/T2P.
\\ ( https://arxiv.org/abs/2404.05218 ,  32765kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05256 (*cross-listing*)
Date: Mon, 8 Apr 2024 07:43:23 GMT   (22345kb,D)

Title: Text-to-Image Synthesis for Any Artistic Styles: Advancements in
  Personalized Artistic Image Generation via Subdivision and Dual Binding
Authors: Junseo Park, Beomseok Ko, Hyeryung Jang
Categories: cs.CV cs.AI
Comments: 20 pages, 12 figuers
\\
  Recent advancements in text-to-image models, such as Stable Diffusion, have
demonstrated their ability to synthesize visual images through natural language
prompts. One approach of personalizing text-to-image models, exemplified by
DreamBooth, fine-tunes the pre-trained model by binding unique text identifiers
with a few images of a specific subject. Although existing fine-tuning methods
have demonstrated competence in rendering images according to the styles of
famous painters, it is still challenging to learn to produce images
encapsulating distinct art styles due to abstract and broad visual perceptions
of stylistic attributes such as lines, shapes, textures, and colors. In this
paper, we introduce a new method, Single-StyleForge, for personalization. It
fine-tunes pre-trained text-to-image diffusion models to generate diverse
images in specified styles from text prompts. By using around 15-20 images of
the target style, the approach establishes a foundational binding of a unique
token identifier with a broad range of the target style. It also utilizes
auxiliary images to strengthen this binding, resulting in offering specific
guidance on representing elements such as persons in a target style-consistent
manner. In addition, we present ways to improve the quality of style and
text-image alignment through a method called Multi-StyleForge, which inherits
the strategy used in StyleForge and learns tokens in multiple. Experimental
evaluation conducted on six distinct artistic styles demonstrates substantial
improvements in both the quality of generated images and the perceptual
fidelity metrics, such as FID, KID, and CLIP scores.
\\ ( https://arxiv.org/abs/2404.05256 ,  22345kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05290 (*cross-listing*)
Date: Mon, 8 Apr 2024 08:28:19 GMT   (6753kb,D)

Title: MindSet: Vision. A toolbox for testing DNNs on key psychological
  experiments
Authors: Valerio Biscione, Dong Yin, Gaurav Malhotra, Marin Dujmovic, Milton L.
  Montero, Guillermo Puebla, Federico Adolfi, Rachel F. Heaton, John E. Hummel,
  Benjamin D. Evans, Karim Habashy, Jeffrey S. Bowers
Categories: cs.CV cs.AI
\\
  Multiple benchmarks have been developed to assess the alignment between deep
neural networks (DNNs) and human vision. In almost all cases these benchmarks
are observational in the sense they are composed of behavioural and brain
responses to naturalistic images that have not been manipulated to test
hypotheses regarding how DNNs or humans perceive and identify objects. Here we
introduce the toolbox MindSet: Vision, consisting of a collection of image
datasets and related scripts designed to test DNNs on 30 psychological
findings. In all experimental conditions, the stimuli are systematically
manipulated to test specific hypotheses regarding human visual perception and
object recognition. In addition to providing pre-generated datasets of images,
we provide code to regenerate these datasets, offering many configurable
parameters which greatly extend the dataset versatility for different research
contexts, and code to facilitate the testing of DNNs on these image datasets
using three different methods (similarity judgments, out-of-distribution
classification, and decoder method), accessible at
https://github.com/MindSetVision/mindset-vision. We test ResNet-152 on each of
these methods as an example of how the toolbox can be used.
\\ ( https://arxiv.org/abs/2404.05290 ,  6753kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05348 (*cross-listing*)
Date: Mon, 8 Apr 2024 09:33:40 GMT   (3499kb,D)

Title: Iterative Refinement Strategy for Automated Data Labeling: Facial
  Landmark Diagnosis in Medical Imaging
Authors: Yu-Hsi Chen
Categories: cs.CV cs.AI cs.LG
\\
  Automated data labeling techniques are crucial for accelerating the
development of deep learning models, particularly in complex medical imaging
applications. However, ensuring accuracy and efficiency remains challenging.
This paper presents iterative refinement strategies for automated data labeling
in facial landmark diagnosis to enhance accuracy and efficiency for deep
learning models in medical applications, including dermatology, plastic
surgery, and ophthalmology. Leveraging feedback mechanisms and advanced
algorithms, our approach iteratively refines initial labels, reducing reliance
on manual intervention while improving label quality. Through empirical
evaluation and case studies, we demonstrate the effectiveness of our proposed
strategies in deep learning tasks across medical imaging domains. Our results
highlight the importance of iterative refinement in automated data labeling to
enhance the capabilities of deep learning systems in medical imaging
applications.
\\ ( https://arxiv.org/abs/2404.05348 ,  3499kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05384 (*cross-listing*)
Date: Mon, 8 Apr 2024 10:45:29 GMT   (20062kb,D)

Title: Rethinking the Spatial Inconsistency in Classifier-Free Diffusion
  Guidance
Authors: Dazhong Shen, Guanglu Song, Zeyue Xue, Fu-Yun Wang, Yu Liu
Categories: cs.CV cs.AI
Comments: accepted by CVPR-2024
\\
  Classifier-Free Guidance (CFG) has been widely used in text-to-image
diffusion models, where the CFG scale is introduced to control the strength of
text guidance on the whole image space. However, we argue that a global CFG
scale results in spatial inconsistency on varying semantic strengths and
suboptimal image quality. To address this problem, we present a novel approach,
Semantic-aware Classifier-Free Guidance (S-CFG), to customize the guidance
degrees for different semantic units in text-to-image diffusion models.
Specifically, we first design a training-free semantic segmentation method to
partition the latent image into relatively independent semantic regions at each
denoising step. In particular, the cross-attention map in the denoising U-net
backbone is renormalized for assigning each patch to the corresponding token,
while the self-attention map is used to complete the semantic regions. Then, to
balance the amplification of diverse semantic units, we adaptively adjust the
CFG scales across different semantic regions to rescale the text guidance
degrees into a uniform level. Finally, extensive experiments demonstrate the
superiority of S-CFG over the original CFG strategy on various text-to-image
diffusion models, without requiring any extra training cost. our codes are
available at https://github.com/SmilesDZgk/S-CFG.
\\ ( https://arxiv.org/abs/2404.05384 ,  20062kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05393 (*cross-listing*)
Date: Mon, 8 Apr 2024 10:52:29 GMT   (31814kb,D)

Title: PAT: Pixel-wise Adaptive Training for Long-tailed Segmentation
Authors: Khoi Do, Duong Nguyen, Nguyen H. Tran, Viet Dung Nguyen
Categories: cs.CV cs.AI
\\
  Beyond class frequency, we recognize the impact of class-wise relationships
among various class-specific predictions and the imbalance in label masks on
long-tailed segmentation learning. To address these challenges, we propose an
innovative Pixel-wise Adaptive Training (PAT) technique tailored for
long-tailed segmentation. PAT has two key features: 1) class-wise gradient
magnitude homogenization, and 2) pixel-wise class-specific loss adaptation
(PCLA). First, the class-wise gradient magnitude homogenization helps alleviate
the imbalance among label masks by ensuring equal consideration of the
class-wise impact on model updates. Second, PCLA tackles the detrimental impact
of both rare classes within the long-tailed distribution and inaccurate
predictions from previous training stages by encouraging learning classes with
low prediction confidence and guarding against forgetting classes with high
confidence. This combined approach fosters robust learning while preventing the
model from forgetting previously learned knowledge. PAT exhibits significant
performance improvements, surpassing the current state-of-the-art by 2.2% in
the NyU dataset. Moreover, it enhances overall pixel-wise accuracy by 2.85% and
intersection over union value by 2.07%, with a particularly notable declination
of 0.39% in detecting rare classes compared to Balance Logits Variation, as
demonstrated on the three popular datasets, i.e., OxfordPetIII, CityScape, and
NYU.
\\ ( https://arxiv.org/abs/2404.05393 ,  31814kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05403 (*cross-listing*)
Date: Mon, 8 Apr 2024 11:05:45 GMT   (2881kb,D)

Title: SoK: Gradient Leakage in Federated Learning
Authors: Jiacheng Du, Jiahui Hu, Zhibo Wang, Peng Sun, Neil Zhenqiang Gong, Kui
  Ren
Categories: cs.CR cs.AI
\\
  Federated learning (FL) enables collaborative model training among multiple
clients without raw data exposure. However, recent studies have shown that
clients' private training data can be reconstructed from the gradients they
share in FL, known as gradient inversion attacks (GIAs). While GIAs have
demonstrated effectiveness under \emph{ideal settings and auxiliary
assumptions}, their actual efficacy against \emph{practical FL systems} remains
under-explored. To address this gap, we conduct a comprehensive study on GIAs
in this work. We start with a survey of GIAs that establishes a milestone to
trace their evolution and develops a systematization to uncover their inherent
threats. Specifically, we categorize the auxiliary assumptions used by existing
GIAs based on their practical accessibility to potential adversaries. To
facilitate deeper analysis, we highlight the challenges that GIAs face in
practical FL systems from three perspectives: \textit{local training},
\textit{model}, and \textit{post-processing}. We then perform extensive
theoretical and empirical evaluations of state-of-the-art GIAs across diverse
settings, utilizing eight datasets and thirteen models. Our findings indicate
that GIAs have inherent limitations when reconstructing data under practical
local training settings. Furthermore, their efficacy is sensitive to the
trained model, and even simple post-processing measures applied to gradients
can be effective defenses. Overall, our work provides crucial insights into the
limited effectiveness of GIAs in practical FL systems. By rectifying prior
misconceptions, we hope to inspire more accurate and realistic investigations
on this topic.
\\ ( https://arxiv.org/abs/2404.05403 ,  2881kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05417 (*cross-listing*)
Date: Mon, 8 Apr 2024 11:33:58 GMT   (3205kb,D)

Title: Indexing Analytics to Instances: How Integrating a Dashboard can Support
  Design Education
Authors: Ajit Jain, Andruid Kerne, Nic Lupfer, Gabriel Britain, Aaron Perrine,
  Yoonsuck Choe, John Keyser, Ruihong Huang, Jinsil Seo, Annie Sungkajun,
  Robert Lightfoot, Timothy McGuire
Categories: cs.HC cs.AI cs.CY
Comments: 22 pages, 4 figures, Submitted to ACM DIS
ACM-class: H.5.2
\\
  We investigate how to use AI-based analytics to support design education. The
analytics at hand measure multiscale design, that is, students' use of space
and scale to visually and conceptually organize their design work. With the
goal of making the analytics intelligible to instructors, we developed a
research artifact integrating a design analytics dashboard with design
instances, and the design environment that students use to create them. We
theorize about how Suchman's notion of mutual intelligibility requires
contextualized investigation of AI in order to develop findings about how
analytics work for people. We studied the research artifact in 5 situated
course contexts, in 3 departments. A total of 236 students used the multiscale
design environment. The 9 instructors who taught those students experienced the
analytics via the new research artifact.
  We derive findings from a qualitative analysis of interviews with instructors
regarding their experiences. Instructors reflected on how the analytics and
their presentation in the dashboard have the potential to affect design
education. We develop research implications addressing: (1) how indexing design
analytics in the dashboard to actual design work instances helps design
instructors reflect on what they mean and, more broadly, is a technique for how
AI-based design analytics can support instructors' assessment and feedback
experiences in situated course contexts; and (2) how multiscale design
analytics, in particular, have the potential to support design education. By
indexing, we mean linking which provides context, here connecting the numbers
of the analytics with visually annotated design work instances.
\\ ( https://arxiv.org/abs/2404.05417 ,  3205kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05423 (*cross-listing*)
Date: Mon, 8 Apr 2024 11:43:40 GMT   (448kb,D)

Title: Residual Chain Prediction for Autonomous Driving Path Planning
Authors: Liguo Zhou, Yirui Zhou, Huaming Liu, Alois Knoll
Categories: cs.RO cs.AI
Comments: 6 pages, 2 figures
\\
  In the rapidly evolving field of autonomous driving systems, the refinement
of path planning algorithms is paramount for navigating vehicles through
dynamic environments, particularly in complex urban scenarios. Traditional path
planning algorithms, which are heavily reliant on static rules and manually
defined parameters, often fall short in such contexts, highlighting the need
for more adaptive, learning-based approaches. Among these, behavior cloning
emerges as a noteworthy strategy for its simplicity and efficiency, especially
within the realm of end-to-end path planning. However, behavior cloning faces
challenges, such as covariate shift when employing traditional Manhattan
distance as the metric. Addressing this, our study introduces the novel concept
of Residual Chain Loss. Residual Chain Loss dynamically adjusts the loss
calculation process to enhance the temporal dependency and accuracy of
predicted path points, significantly improving the model's performance without
additional computational overhead. Through testing on the nuScenes dataset, we
underscore the method's substantial advancements in addressing covariate shift,
facilitating dynamic loss adjustments, and ensuring seamless integration with
end-to-end path planning frameworks. Our findings highlight the potential of
Residual Chain Loss to revolutionize planning component of autonomous driving
systems, marking a significant step forward in the quest for level 5 autonomous
driving system.
\\ ( https://arxiv.org/abs/2404.05423 ,  448kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05427 (*cross-listing*)
Date: Mon, 8 Apr 2024 11:55:09 GMT   (1152kb,D)

Title: AutoCodeRover: Autonomous Program Improvement
Authors: Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, Abhik Roychoudhury
Categories: cs.SE cs.AI
\\
  Researchers have made significant progress in automating the software
development process in the past decades. Automated techniques for issue
summarization, bug reproduction, fault localization, and program repair have
been built to ease the workload of developers. Recent progress in Large
Language Models (LLMs) has significantly impacted the development process,
where developers can use LLM-based programming assistants to achieve automated
coding. Nevertheless software engineering involves the process of program
improvement apart from coding, specifically to enable software maintenance
(e.g. program repair to fix bugs) and software evolution (e.g. feature
additions). In this paper, we propose an automated approach for solving Github
issues to autonomously achieve program improvement. In our approach called
AutoCodeRover, LLMs are combined with sophisticated code search capabilities,
ultimately leading to a program modification or patch. In contrast to recent
LLM agent approaches from AI researchers and practitioners, our outlook is more
software engineering oriented. We work on a program representation (abstract
syntax tree) as opposed to viewing a software project as a mere collection of
files. Our code search exploits the program structure in the form of
classes/methods to enhance LLM's understanding of the issue's root cause, and
effectively retrieve a context via iterative search. The use of spectrum based
fault localization using tests, further sharpens the context. Experiments on
the recently proposed SWE-bench-lite which consists of 300 real-life Github
issues involving bug fixing and feature additions show increased efficacy
(resolving more than 20% on SWE-bench-lite), as compared to recent efforts from
the AI community. We posit that our workflow enables autonomous software
engineering, where, in future, auto-generated code from LLMs can be
autonomously improved.
\\ ( https://arxiv.org/abs/2404.05427 ,  1152kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05458 (*cross-listing*)
Date: Mon, 8 Apr 2024 12:40:27 GMT   (3659kb,D)

Title: Teaching Higher-Order Logic Using Isabelle
Authors: Simon Tobias Lund (Technical University of Denmark), J{\o}rgen
  Villadsen (Technical University of Denmark)
Categories: cs.LO cs.AI
Comments: In Proceedings ThEdu'23, arXiv:2404.03709
ACM-class: F.4; I.2.3; K.3.1
Journal-ref: EPTCS 400, 2024, pp. 59-78
DOI: 10.4204/EPTCS.400.5
\\
  We present a formalization of higher-order logic in the Isabelle proof
assistant, building directly on the foundational framework Isabelle/Pure and
developed to be as small and readable as possible. It should therefore serve as
a good introduction for someone looking into learning about higher-order logic
and proof assistants, without having to study the much more complex
Isabelle/HOL with heavier automation. To showcase our development and approach
we explain a sample proof, describe the axioms and rules of our higher-order
logic, and discuss our experience with teaching the subject in a classroom
setting.
\\ ( https://arxiv.org/abs/2404.05458 ,  3659kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05499 (*cross-listing*)
Date: Mon, 8 Apr 2024 13:22:24 GMT   (1828kb)

Title: Constraining Large Language Model for Generating Computer-Parsable
  Content
Authors: Jiaye Wang
Categories: cs.SE cs.AI
Comments: 44 pages, 39 figures, 8 tables
\\
  We propose a method to guide Large Language Models (LLMs) in generating
structured content adhering to specific conventions without fine-tuning. By
utilizing coroutine-based content generation constraints through a pre-agreed
context-free grammar (CFG), LLMs are directed during decoding to produce formal
language compliant outputs. This enhances stability and consistency in
generating target data structures, types, or instructions, reducing application
development complexities. Experimentally, error rates of GPT-2 and Gemma exceed
95% for DSLs longer than 36 and 282 tokens, respectively. We introduce
YieldLang, a coroutine-based DSL generation framework, and evaluate it with
LLMs on various tasks including JSON and Mermaid flowchart generation. Compared
to benchmarks, our approach improves accuracy by 1.09 to 11.6 times, with LLMs
requiring only about 16.5% of the samples to generate JSON effectively. This
enhances usability of LLM-generated content for computer programs.
\\ ( https://arxiv.org/abs/2404.05499 ,  1828kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05501 (*cross-listing*)
Date: Mon, 8 Apr 2024 13:25:02 GMT   (18192kb)

Title: Data Science In Olfaction
Authors: Vivek Agarwal, Joshua Harvey, Dmitry Rinberg and Vasant Dhar
Categories: q-bio.NC cs.AI cs.LG
Comments: 20 pages, 10 Figures, 2 Appendix, 1 Table
\\
  Advances in neural sensing technology are making it possible to observe the
olfactory process in great detail. In this paper, we conceptualize smell from a
Data Science and AI perspective, that relates the properties of odorants to how
they are sensed and analyzed in the olfactory system from the nose to the
brain. Drawing distinctions to color vision, we argue that smell presents
unique measurement challenges, including the complexity of stimuli, the high
dimensionality of the sensory apparatus, as well as what constitutes ground
truth. In the face of these challenges, we argue for the centrality of
odorant-receptor interactions in developing a theory of olfaction. Such a
theory is likely to find widespread industrial applications, and enhance our
understanding of smell, and in the longer-term, how it relates to other senses
and language. As an initial use case of the data, we present results using
machine learning-based classification of neural responses to odors as they are
recorded in the mouse olfactory bulb with calcium imaging.
\\ ( https://arxiv.org/abs/2404.05501 ,  18192kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05508 (*cross-listing*)
Date: Mon, 8 Apr 2024 13:28:11 GMT   (5789kb,D)

Title: Synergy of Large Language Model and Model Driven Engineering for
  Automated Development of Centralized Vehicular Systems
Authors: Nenad Petrovic, Fengjunjie Pan, Krzysztof Lebioda, Vahid Zolfaghari,
  Sven Kirchner, Nils Purschke, Muhammad Aqib Khan, Viktor Vorobev, Alois Knoll
Categories: cs.SE cs.AI cs.CL
Report-no: TUM-I24109
ACM-class: D.2.1; D.2.2; D.2.4; I.2.7; I.2.2; I.7.0
\\
  We present a prototype of a tool leveraging the synergy of model driven
engineering (MDE) and Large Language Models (LLM) for the purpose of software
development process automation in the automotive industry. In this approach,
the user-provided input is free form textual requirements, which are first
translated to Ecore model instance representation using an LLM, which is
afterwards checked for consistency using Object Constraint Language (OCL)
rules. After successful consistency check, the model instance is fed as input
to another LLM for the purpose of code generation. The generated code is
evaluated in a simulated environment using CARLA simulator connected to an
example centralized vehicle architecture, in an emergency brake scenario.
\\ ( https://arxiv.org/abs/2404.05508 ,  5789kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05534 (*cross-listing*)
Date: Mon, 8 Apr 2024 14:00:50 GMT   (367kb)

Title: Ordre public exceptions for algorithmic surveillance patents
Authors: Alina Wernick
Categories: cs.CY cs.AI
Comments: 14 pages
DOI: 10.1007/978-3-662-68599-0_33
\\
  This chapter explores the role of patent protection in algorithmic
surveillance and whether ordre public exceptions from patentability should
apply to such patents, due to their potential to enable human rights
violations. It concludes that in most cases, it is undesirable to exclude
algorithmic surveillance patents from patentability, as the patent system is
ill-equipped to evaluate the impacts of the exploitation of such technologies.
Furthermore, the disclosure of such patents has positive externalities from the
societal perspective by opening the black box of surveillance for public
scrutiny.
\\ ( https://arxiv.org/abs/2404.05534 ,  367kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05553 (*cross-listing*)
Date: Mon, 8 Apr 2024 14:21:34 GMT   (8438kb,D)

Title: Alljoined -- A dataset for EEG-to-Image decoding
Authors: Jonathan Xu, Bruno Aristimunha, Max Emanuel Feucht, Emma Qian, Charles
  Liu, Tazik Shahjahan, Martyna Spyra, Steven Zifan Zhang, Nicholas Short, Jioh
  Kim, Paula Perdomo, Ricky Renfeng Mao, Yashvir Sabharwal, Michael Ahedor Moaz
  Shoura, Adrian Nestor
Categories: q-bio.NC cs.AI
Comments: 8 Pages, 6 Figures
ACM-class: I.5.1; I.6.3; I.2.6; K.3.2
\\
  We present Alljoined, a dataset built specifically for EEG-to-Image decoding.
Recognizing that an extensive and unbiased sampling of neural responses to
visual stimuli is crucial for image reconstruction efforts, we collected data
from 8 participants looking at 10,000 natural images each. We have currently
gathered 46,080 epochs of brain responses recorded with a 64-channel EEG
headset. The dataset combines response-based stimulus timing, repetition
between blocks and sessions, and diverse image classes with the goal of
improving signal quality. For transparency, we also provide data quality
scores. We publicly release the dataset and all code at
https://linktr.ee/alljoined1.
\\ ( https://arxiv.org/abs/2404.05553 ,  8438kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05603 (*cross-listing*)
Date: Mon, 8 Apr 2024 15:22:38 GMT   (9075kb,D)

Title: Self-Explainable Affordance Learning with Embodied Caption
Authors: Zhipeng Zhang, Zhimin Wei, Guolei Sun, Peng Wang, Luc Van Gool
Categories: cs.CV cs.AI
\\
  In the field of visual affordance learning, previous methods mainly used
abundant images or videos that delineate human behavior patterns to identify
action possibility regions for object manipulation, with a variety of
applications in robotic tasks. However, they encounter a main challenge of
action ambiguity, illustrated by the vagueness like whether to beat or carry a
drum, and the complexities involved in processing intricate scenes. Moreover,
it is important for human intervention to rectify robot errors in time. To
address these issues, we introduce Self-Explainable Affordance learning (SEA)
with embodied caption. This innovation enables robots to articulate their
intentions and bridge the gap between explainable vision-language caption and
visual affordance learning. Due to a lack of appropriate dataset, we unveil a
pioneering dataset and metrics tailored for this task, which integrates images,
heatmaps, and embodied captions. Furthermore, we propose a novel model to
effectively combine affordance grounding with self-explanation in a simple but
efficient manner. Extensive quantitative and qualitative experiments
demonstrate our method's effectiveness.
\\ ( https://arxiv.org/abs/2404.05603 ,  9075kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05648 (*cross-listing*)
Date: Mon, 8 Apr 2024 16:34:35 GMT   (6401kb,D)

Title: Resistive Memory-based Neural Differential Equation Solver for
  Score-based Diffusion Model
Authors: Jichang Yang, Hegan Chen, Jia Chen, Songqi Wang, Shaocong Wang, Yifei
  Yu, Xi Chen, Bo Wang, Xinyuan Zhang, Binbin Cui, Yi Li, Ning Lin, Meng Xu, Yi
  Li, Xiaoxin Xu, Xiaojuan Qi, Zhongrui Wang, Xumeng Zhang, Dashan Shang, Han
  Wang, Qi Liu, Kwang-Ting Cheng, Ming Liu
Categories: cs.AR cs.AI cs.ET cs.NE
\\
  Human brains image complicated scenes when reading a novel. Replicating this
imagination is one of the ultimate goals of AI-Generated Content (AIGC).
However, current AIGC methods, such as score-based diffusion, are still
deficient in terms of rapidity and efficiency. This deficiency is rooted in the
difference between the brain and digital computers. Digital computers have
physically separated storage and processing units, resulting in frequent data
transfers during iterative calculations, incurring large time and energy
overheads. This issue is further intensified by the conversion of inherently
continuous and analog generation dynamics, which can be formulated by neural
differential equations, into discrete and digital operations. Inspired by the
brain, we propose a time-continuous and analog in-memory neural differential
equation solver for score-based diffusion, employing emerging resistive memory.
The integration of storage and computation within resistive memory synapses
surmount the von Neumann bottleneck, benefiting the generative speed and energy
efficiency. The closed-loop feedback integrator is time-continuous, analog, and
compact, physically implementing an infinite-depth neural network. Moreover,
the software-hardware co-design is intrinsically robust to analog noise. We
experimentally validate our solution with 180 nm resistive memory in-memory
computing macros. Demonstrating equivalent generative quality to the software
baseline, our system achieved remarkable enhancements in generative speed for
both unconditional and conditional generation tasks, by factors of 64.8 and
156.5, respectively. Moreover, it accomplished reductions in energy consumption
by factors of 5.2 and 4.1. Our approach heralds a new horizon for hardware
solutions in edge computing for generative AI applications.
\\ ( https://arxiv.org/abs/2404.05648 ,  6401kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05695 (*cross-listing*)
Date: Mon, 8 Apr 2024 17:26:28 GMT   (6491kb,D)

Title: Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot
  Sim2Real Transfer
Authors: Xinyang Gu, Yen-Jen Wang, Jianyu Chen
Categories: cs.RO cs.AI cs.LG cs.SY eess.SY
\\
  Humanoid-Gym is an easy-to-use reinforcement learning (RL) framework based on
Nvidia Isaac Gym, designed to train locomotion skills for humanoid robots,
emphasizing zero-shot transfer from simulation to the real-world environment.
Humanoid-Gym also integrates a sim-to-sim framework from Isaac Gym to Mujoco
that allows users to verify the trained policies in different physical
simulations to ensure the robustness and generalization of the policies. This
framework is verified by RobotEra's XBot-S (1.2-meter tall humanoid robot) and
XBot-L (1.65-meter tall humanoid robot) in a real-world environment with
zero-shot sim-to-real transfer. The project website and source code can be
found at: https://sites.google.com/view/humanoid-gym/.
\\ ( https://arxiv.org/abs/2404.05695 ,  6491kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05717 (*cross-listing*)
Date: Mon, 8 Apr 2024 17:52:29 GMT   (19745kb,D)

Title: SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual
  Editing
Authors: Jing Gu, Yilin Wang, Nanxuan Zhao, Wei Xiong, Qing Liu, Zhifei Zhang,
  He Zhang, Jianming Zhang, HyunJoon Jung, Xin Eric Wang
Categories: cs.CV cs.AI
Comments: 18 pages, 16 figures, 3 tables
\\
  Effective editing of personal content holds a pivotal role in enabling
individuals to express their creativity, weaving captivating narratives within
their visual stories, and elevate the overall quality and impact of their
visual content. Therefore, in this work, we introduce SwapAnything, a novel
framework that can swap any objects in an image with personalized concepts
given by the reference, while keeping the context unchanged. Compared with
existing methods for personalized subject swapping, SwapAnything has three
unique advantages: (1) precise control of arbitrary objects and parts rather
than the main subject, (2) more faithful preservation of context pixels, (3)
better adaptation of the personalized concept to the image. First, we propose
targeted variable swapping to apply region control over latent feature maps and
swap masked variables for faithful context preservation and initial semantic
concept swapping. Then, we introduce appearance adaptation, to seamlessly adapt
the semantic concept into the original image in terms of target location,
shape, style, and content during the image generation process. Extensive
results on both human and automatic evaluation demonstrate significant
improvements of our approach over baseline methods on personalized swapping.
Furthermore, SwapAnything shows its precise and faithful swapping abilities
across single object, multiple objects, partial object, and cross-domain
swapping tasks. SwapAnything also achieves great performance on text-based
swapping and tasks beyond swapping such as object insertion.
\\ ( https://arxiv.org/abs/2404.05717 ,  19745kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04272 (*cross-listing*)
Date: Fri, 22 Mar 2024 08:10:32 GMT   (9601kb,D)

Title: Selecting Query-bag as Pseudo Relevance Feedback for Information-seeking
  Conversations
Authors: Xiaoqing Zhang, Xiuying Chen, Shen Gao, Shuqi Li, Xin Gao, Ji-Rong Wen
  and Rui Yan
Categories: cs.IR cs.CL
\\
  Information-seeking dialogue systems are widely used in e-commerce systems,
with answers that must be tailored to fit the specific settings of the online
system. Given the user query, the information-seeking dialogue systems first
retrieve a subset of response candidates, then further select the best response
from the candidate set through re-ranking. Current methods mainly retrieve
response candidates based solely on the current query, however, incorporating
similar questions could introduce more diverse content, potentially refining
the representation and improving the matching process. Hence, in this paper, we
proposed a Query-bag based Pseudo Relevance Feedback framework (QB-PRF), which
constructs a query-bag with related queries to serve as pseudo signals to guide
information-seeking conversations. Concretely, we first propose a Query-bag
Selection module (QBS), which utilizes contrastive learning to train the
selection of synonymous queries in an unsupervised manner by leveraging the
representations learned from pre-trained VAE. Secondly, we come up with a
Query-bag Fusion module (QBF) that fuses synonymous queries to enhance the
semantic representation of the original query through multidimensional
attention computation. We verify the effectiveness of the QB-PRF framework on
two competitive pretrained backbone models, including BERT and GPT-2.
Experimental results on two benchmark datasets show that our framework achieves
superior performance over strong baselines.
\\ ( https://arxiv.org/abs/2404.04272 ,  9601kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04393 (*cross-listing*)
Date: Fri, 5 Apr 2024 20:36:30 GMT   (48kb,D)

Title: Counting Like Transformers: Compiling Temporal Counting Logic Into
  Softmax Transformers
Authors: Andy Yang, David Chiang
Categories: cs.LO cs.CL cs.FL cs.LG
\\
  Deriving formal bounds on the expressivity of transformers, as well as
studying transformers that are constructed to implement known algorithms, are
both effective methods for better understanding the computational power of
transformers. Towards both ends, we introduce the temporal counting logic
$\textbf{K}_\text{t}$[#] alongside the RASP variant $\textbf{C-RASP}$. We show
they are equivalent to each other, and that together they are the best-known
lower bound on the formal expressivity of future-masked soft attention
transformers with unbounded input size. We prove this by showing all
$\textbf{K}_\text{t}$[#] formulas can be compiled into these transformers. As a
case study, we demonstrate on paper how to use $\textbf{C-RASP}$ to construct
simple transformer language models that, using greedy decoding, can only
generate sentences that have given properties formally specified in
$\textbf{K}_\text{t}$[#].
\\ ( https://arxiv.org/abs/2404.04393 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04516 (*cross-listing*)
Date: Sat, 6 Apr 2024 06:12:13 GMT   (367kb,D)

Title: Language Models as Critical Thinking Tools: A Case Study of Philosophers
Authors: Andre Ye, Jared Moore, Rose Novick, Amy X. Zhang
Categories: cs.HC cs.CL cs.CY
\\
  Current work in language models (LMs) helps us speed up or even skip thinking
by accelerating and automating cognitive work. But can LMs help us with
critical thinking -- thinking in deeper, more reflective ways which challenge
assumptions, clarify ideas, and engineer new concepts? We treat philosophy as a
case study in critical thinking, and interview 21 professional philosophers
about how they engage in critical thinking and on their experiences with LMs.
We find that philosophers do not find LMs to be useful because they lack a
sense of selfhood (memory, beliefs, consistency) and initiative (curiosity,
proactivity). We propose the selfhood-initiative model for critical thinking
tools to characterize this gap. Using the model, we formulate three roles LMs
could play as critical thinking tools: the Interlocutor, the Monitor, and the
Respondent. We hope that our work inspires LM researchers to further develop
LMs as critical thinking tools and philosophers and other 'critical thinkers'
to imagine intellectually substantive uses of LMs.
\\ ( https://arxiv.org/abs/2404.04516 ,  367kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04545 (*cross-listing*)
Date: Sat, 6 Apr 2024 07:56:09 GMT   (896kb,D)

Title: TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment
  Analysis
Authors: Ming Zhou, Weize Quan, Ziqi Zhou, Kai Wang, Tong Wang and Dong-Ming
  Yan
Categories: cs.MM cs.CL
\\
  Multimodal Sentiment Analysis (MSA) endeavors to understand human sentiment
by leveraging language, visual, and acoustic modalities. Despite the remarkable
performance exhibited by previous MSA approaches, the presence of inherent
multimodal heterogeneities poses a challenge, with the contribution of
different modalities varying considerably. Past research predominantly focused
on improving representation learning techniques and feature fusion strategies.
However, many of these efforts overlooked the variation in semantic richness
among different modalities, treating each modality uniformly. This approach may
lead to underestimating the significance of strong modalities while
overemphasizing the importance of weak ones. Motivated by these insights, we
introduce a Text-oriented Cross-Attention Network (TCAN), emphasizing the
predominant role of the text modality in MSA. Specifically, for each multimodal
sample, by taking unaligned sequences of the three modalities as inputs, we
initially allocate the extracted unimodal features into a visual-text and an
acoustic-text pair. Subsequently, we implement self-attention on the text
modality and apply text-queried cross-attention to the visual and acoustic
modalities. To mitigate the influence of noise signals and redundant features,
we incorporate a gated control mechanism into the framework. Additionally, we
introduce unimodal joint learning to gain a deeper understanding of homogeneous
emotional tendencies across diverse modalities through backpropagation.
Experimental results demonstrate that TCAN consistently outperforms
state-of-the-art MSA methods on two datasets (CMU-MOSI and CMU-MOSEI).
\\ ( https://arxiv.org/abs/2404.04545 ,  896kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04892 (*cross-listing*)
Date: Sun, 7 Apr 2024 09:21:09 GMT   (18241kb,D)

Title: Elementary fractal geometry. 5. Weak separation is strong separation
Authors: Christoph Bandt and Michael F. Barnsley
Categories: math.DS cs.CL
Comments: 27 pages, 12 figures
MSC-class: 28A80, 11A63, 37B10, 54B15, 68Q45
\\
  For self-similar sets, there are two important separation properties: the
open set condition and the weak separation condition introduced by Zerner,
which may be replaced by the formally stronger finite type property of Ngai and
Wang. We show that any finite type self-similar set can be represented as a
graph-directed construction obeying the open set condition. The proof is based
on a combinatorial algorithm which performed well in computer experiments.
\\ ( https://arxiv.org/abs/2404.04892 ,  18241kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05046 (*cross-listing*)
Date: Sun, 7 Apr 2024 19:00:45 GMT   (7312kb,D)

Title: FGAIF: Aligning Large Vision-Language Models with Fine-grained AI
  Feedback
Authors: Liqiang Jing and Xinya Du
Categories: cs.CV cs.CL
\\
  Large Vision-Language Models (LVLMs) have demonstrated proficiency in
tackling a variety of visual-language tasks. However, current LVLMs suffer from
misalignment between text and image modalities which causes three kinds of
hallucination problems, i.e., object existence, object attribute, and object
relationship. To tackle this issue, existing methods mainly utilize
Reinforcement Learning (RL) to align modalities in LVLMs. However, they still
suffer from three main limitations: (1) General feedback can not indicate the
hallucination type contained in the response; (2) Sparse rewards only give the
sequence-level reward for the whole response; and (3)Annotation cost is
time-consuming and labor-intensive. To handle these limitations, we propose an
innovative method to align modalities in LVLMs through Fine-Grained Artificial
Intelligence Feedback (FGAIF), which mainly consists of three steps: AI-based
Feedback Collection, Fine-grained Reward Model Training, and Reinforcement
Learning with Fine-grained Reward. Specifically, We first utilize AI tools to
predict the types of hallucination for each segment in the response and obtain
a collection of fine-grained feedback. Then, based on the collected reward
data, three specialized reward models are trained to produce dense rewards.
Finally, a novel fine-grained feedback module is integrated into the Proximal
Policy Optimization (PPO) algorithm. Extensive experiments are conducted on
hallucination and general benchmarks, demonstrating the superior performance of
our proposed method. Notably, compared with previous models trained with the
RL-based aligning method, our proposed method is effective even with fewer
parameters.
\\ ( https://arxiv.org/abs/2404.05046 ,  7312kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05073 (*cross-listing*)
Date: Sun, 7 Apr 2024 21:02:55 GMT   (321kb,D)

Title: QRscript: Embedding a Programming Language in QR codes to support
  Decision and Management
Authors: Stefano Scanzio, Gianluca Cena, Adriano Valenzano
Categories: cs.NI cs.CL
Comments: preprint, 8 pages
Journal-ref: 27th IEEE International Conference on Emerging Technologies and
  Factory Automation (ETFA 2022)
DOI: 10.1109/ETFA52439.2022.9921530
\\
  Embedding a programming language in a QR code is a new and extremely
promising opportunity, as it makes devices and objects smarter without
necessarily requiring an Internet connection. In this paper, all the steps
needed to translate a program written in a high-level programming language to
its binary representation encoded in a QR code, and the opposite process that,
starting from the QR code, executes it by means of a virtual machine, have been
carefully detailed. The proposed programming language was named QRscript, and
can be easily extended so as to integrate new features. One of the main design
goals was to produce a very compact target binary code. In particular, in this
work we propose a specific sub-language (a dialect) that is aimed at encoding
decision trees. Besides industrial scenarios, this is useful in many other
application fields. The reported example, related to the configuration of an
industrial networked device, highlights the potential of the proposed
technology, and permits to better understand all the translation steps.
\\ ( https://arxiv.org/abs/2404.05073 ,  321kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05083 (*cross-listing*)
Date: Sun, 7 Apr 2024 21:46:47 GMT   (10883kb,D)

Title: HaVTR: Improving Video-Text Retrieval Through Augmentation Using Large
  Foundation Models
Authors: Yimu Wang, Shuai Yuan, Xiangru Jian, Wei Pang, Mushi Wang, Ning Yu
Categories: cs.CV cs.CL cs.IR cs.LG
\\
  While recent progress in video-text retrieval has been driven by the
exploration of powerful model architectures and training strategies, the
representation learning ability of video-text retrieval models is still limited
due to low-quality and scarce training data annotations. To address this issue,
we present a novel video-text learning paradigm, HaVTR, which augments video
and text data to learn more generalized features. Specifically, we first adopt
a simple augmentation method, which generates self-similar data by randomly
duplicating or dropping subwords and frames. In addition, inspired by the
recent advancement in visual and language generative models, we propose a more
powerful augmentation method through textual paraphrasing and video stylization
using large language models (LLMs) and visual generative models (VGMs).
Further, to bring richer information into video and text, we propose a
hallucination-based augmentation method, where we use LLMs and VGMs to generate
and add new relevant information to the original data. Benefiting from the
enriched data, extensive experiments on several video-text retrieval benchmarks
demonstrate the superiority of HaVTR over existing methods.
\\ ( https://arxiv.org/abs/2404.05083 ,  10883kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05225 (*cross-listing*)
Date: Mon, 8 Apr 2024 06:40:28 GMT   (4370kb,D)

Title: LayoutLLM: Layout Instruction Tuning with Large Language Models for
  Document Understanding
Authors: Chuwei Luo, Yufan Shen, Zhaoqing Zhu, Qi Zheng, Zhi Yu, Cong Yao
Categories: cs.CV cs.CL
Comments: CVPR 2024
\\
  Recently, leveraging large language models (LLMs) or multimodal large
language models (MLLMs) for document understanding has been proven very
promising. However, previous works that employ LLMs/MLLMs for document
understanding have not fully explored and utilized the document layout
information, which is vital for precise document understanding. In this paper,
we propose LayoutLLM, an LLM/MLLM based method for document understanding. The
core of LayoutLLM is a layout instruction tuning strategy, which is specially
designed to enhance the comprehension and utilization of document layouts. The
proposed layout instruction tuning strategy consists of two components:
Layout-aware Pre-training and Layout-aware Supervised Fine-tuning. To capture
the characteristics of document layout in Layout-aware Pre-training, three
groups of pre-training tasks, corresponding to document-level, region-level and
segment-level information, are introduced. Furthermore, a novel module called
layout chain-of-thought (LayoutCoT) is devised to enable LayoutLLM to focus on
regions relevant to the question and generate accurate answers. LayoutCoT is
effective for boosting the performance of document understanding. Meanwhile, it
brings a certain degree of interpretability, which could facilitate manual
inspection and correction. Experiments on standard benchmarks show that the
proposed LayoutLLM significantly outperforms existing methods that adopt
open-source 7B LLMs/MLLMs for document understanding. The training data of the
LayoutLLM is publicly available at
https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/LayoutLLM
\\ ( https://arxiv.org/abs/2404.05225 ,  4370kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05719 (*cross-listing*)
Date: Mon, 8 Apr 2024 17:55:44 GMT   (23745kb,D)

Title: Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs
Authors: Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin,
  Jeffrey Nichols, Yinfei Yang, Zhe Gan
Categories: cs.CV cs.CL cs.HC
\\
  Recent advancements in multimodal large language models (MLLMs) have been
noteworthy, yet, these general-domain MLLMs often fall short in their ability
to comprehend and interact effectively with user interface (UI) screens. In
this paper, we present Ferret-UI, a new MLLM tailored for enhanced
understanding of mobile UI screens, equipped with referring, grounding, and
reasoning capabilities. Given that UI screens typically exhibit a more
elongated aspect ratio and contain smaller objects of interest (e.g., icons,
texts) than natural images, we incorporate "any resolution" on top of Ferret to
magnify details and leverage enhanced visual features. Specifically, each
screen is divided into 2 sub-images based on the original aspect ratio (i.e.,
horizontal division for portrait screens and vertical division for landscape
screens). Both sub-images are encoded separately before being sent to LLMs. We
meticulously gather training samples from an extensive range of elementary UI
tasks, such as icon recognition, find text, and widget listing. These samples
are formatted for instruction-following with region annotations to facilitate
precise referring and grounding. To augment the model's reasoning ability, we
further compile a dataset for advanced tasks, including detailed description,
perception/interaction conversations, and function inference. After training on
the curated datasets, Ferret-UI exhibits outstanding comprehension of UI
screens and the capability to execute open-ended instructions. For model
evaluation, we establish a comprehensive benchmark encompassing all the
aforementioned tasks. Ferret-UI excels not only beyond most open-source UI
MLLMs, but also surpasses GPT-4V on all the elementary UI tasks.
\\ ( https://arxiv.org/abs/2404.05719 ,  23745kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04265 (*cross-listing*)
Date: Mon, 18 Mar 2024 16:27:33 GMT   (2766kb,D)

Title: Accelerating Matrix Factorization by Dynamic Pruning for Fast
  Recommendation
Authors: Yining Wu, Shengyu Duan, Gaole Sai, Chenhong Cao and Guobing Zou
Categories: cs.IR cs.LG
\\
  Matrix factorization (MF) is a widely used collaborative filtering (CF)
algorithm for recommendation systems (RSs), due to its high prediction
accuracy, great flexibility and high efficiency in big data processing.
However, with the dramatically increased number of users/items in current RSs,
the computational complexity for training a MF model largely increases. Many
existing works have accelerated MF, by either putting in additional
computational resources or utilizing parallel systems, introducing a large
cost. In this paper, we propose algorithmic methods to accelerate MF, without
inducing any additional computational resources. In specific, we observe
fine-grained structured sparsity in the decomposed feature matrices when
considering a certain threshold. The fine-grained structured sparsity causes a
large amount of unnecessary operations during both matrix multiplication and
latent factor update, increasing the computational time of the MF training
process. Based on the observation, we firstly propose to rearrange the feature
matrices based on joint sparsity, which potentially makes a latent vector with
a smaller index more dense than that with a larger index. The feature matrix
rearrangement is given to limit the error caused by the later performed pruning
process. We then propose to prune the insignificant latent factors by an early
stopping process during both matrix multiplication and latent factor update.
The pruning process is dynamically performed according to the sparsity of the
latent factors for different users/items, to accelerate the process. The
experiments show that our method can achieve 1.2-1.65 speedups, with up to
20.08% error increase, compared with the conventional MF training process. We
also prove the proposed methods are applicable considering different
hyperparameters including optimizer, optimization strategy and initialization
method.
\\ ( https://arxiv.org/abs/2404.04265 ,  2766kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04269 (*cross-listing*)
Date: Tue, 19 Mar 2024 23:27:15 GMT   (4598kb,D)

Title: Algorithmic Collective Action in Recommender Systems: Promoting Songs by
  Reordering Playlists
Authors: Joachim Baumann and Celestine Mendler-D\"unner
Categories: cs.IR cs.LG cs.SI
\\
  We investigate algorithmic collective action in transformer-based recommender
systems. Our use case is a collective of fans aiming to promote the visibility
of an artist by strategically placing one of their songs in the existing
playlists they control. The success of the collective is measured by the
increase in test-time recommendations of the targeted song. We introduce two
easily implementable strategies towards this goal and test their efficacy on a
publicly available recommender system model released by a major music streaming
platform. Our findings reveal that even small collectives (controlling less
than 0.01% of the training data) can achieve up 25x amplification of
recommendations by strategically choosing the position at which to insert the
song. We then focus on investigating the externalities of the strategy. We find
that the performance loss for the platform is negligible, and the
recommendations of other songs are largely preserved, minimally impairing the
user experience of participants. Moreover, the costs are evenly distributed
among other artists. Taken together, our findings demonstrate how collective
action strategies can be effective while not necessarily being adversarial,
raising new questions around incentives, social dynamics, and equilibria in
recommender systems.
\\ ( https://arxiv.org/abs/2404.04269 ,  4598kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04270 (*cross-listing*)
Date: Fri, 22 Mar 2024 00:29:06 GMT   (2646kb,D)

Title: Accelerating Recommender Model Training by Dynamically Skipping Stale
  Embeddings
Authors: Yassaman Ebrahimzadeh Maboud, Muhammad Adnan, Divya Mahajan, Prashant
  J. Nair
Categories: cs.IR cs.LG
\\
  Training recommendation models pose significant challenges regarding resource
utilization and performance. Prior research has proposed an approach that
categorizes embeddings into popular and non-popular classes to reduce the
training time for recommendation models. We observe that, even among the
popular embeddings, certain embeddings undergo rapid training and exhibit
minimal subsequent variation, resulting in saturation. Consequently, updates to
these embeddings lack any contribution to model quality. This paper presents
Slipstream, a software framework that identifies stale embeddings on the fly
and skips their updates to enhance performance. This capability enables
Slipstream to achieve substantial speedup, optimize CPU-GPU bandwidth usage,
and eliminate unnecessary memory access. SlipStream showcases training time
reductions of 2x, 2.4x, 1.2x, and 1.175x across real-world datasets and
configurations, compared to Baseline XDL, Intel-optimized DRLM, FAE, and
Hotline, respectively.
\\ ( https://arxiv.org/abs/2404.04270 ,  2646kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04282 (*cross-listing*)
Date: Wed, 3 Apr 2024 07:27:59 GMT   (530kb)

Title: Analyzing Economic Convergence Across the Americas: A Survival Analysis
  Approach to GDP per Capita Trajectories
Authors: Diego Vallarino
Categories: econ.GN cs.LG q-fin.EC
\\
  By integrating survival analysis, machine learning algorithms, and economic
interpretation, this research examines the temporal dynamics associated with
attaining a 5 percent rise in purchasing power parity-adjusted GDP per capita
over a period of 120 months (2013-2022). A comparative investigation reveals
that DeepSurv is proficient at capturing non-linear interactions, although
standard models exhibit comparable performance under certain circumstances.
  The weight matrix evaluates the economic ramifications of vulnerabilities,
risks, and capacities. In order to meet the GDPpc objective, the findings
emphasize the need of a balanced approach to risk-taking, strategic
vulnerability reduction, and investment in governmental capacities and social
cohesiveness. Policy guidelines promote individualized approaches that take
into account the complex dynamics at play while making decisions.
\\ ( https://arxiv.org/abs/2404.04282 ,  530kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04283 (*cross-listing*)
Date: Wed, 3 Apr 2024 17:48:31 GMT   (19871kb,D)

Title: Translation-based Video-to-Video Synthesis
Authors: Pratim Saha and Chengcui Zhang
Categories: cs.CV cs.LG eess.IV
Comments: 25 pages, 9 figures
\\
  Translation-based Video Synthesis (TVS) has emerged as a vital research area
in computer vision, aiming to facilitate the transformation of videos between
distinct domains while preserving both temporal continuity and underlying
content features. This technique has found wide-ranging applications,
encompassing video super-resolution, colorization, segmentation, and more, by
extending the capabilities of traditional image-to-image translation to the
temporal domain. One of the principal challenges faced in TVS is the inherent
risk of introducing flickering artifacts and inconsistencies between frames
during the synthesis process. This is particularly challenging due to the
necessity of ensuring smooth and coherent transitions between video frames.
Efforts to tackle this challenge have induced the creation of diverse
strategies and algorithms aimed at mitigating these unwanted consequences. This
comprehensive review extensively examines the latest progress in the realm of
TVS. It thoroughly investigates emerging methodologies, shedding light on the
fundamental concepts and mechanisms utilized for proficient video synthesis.
This survey also illuminates their inherent strengths, limitations, appropriate
applications, and potential avenues for future development.
\\ ( https://arxiv.org/abs/2404.04283 ,  19871kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04317 (*cross-listing*)
Date: Fri, 5 Apr 2024 17:47:50 GMT   (2461kb,D)

Title: DeepLINK-T: deep learning inference for time series data using knockoffs
  and LSTM
Authors: Wenxuan Zuo, Zifan Zhu, Yuxuan Du, Yi-Chun Yeh, Jed A. Fuhrman, Jinchi
  Lv, Yingying Fan, Fengzhu Sun
Categories: stat.ML cs.LG q-bio.QM
\\
  High-dimensional longitudinal time series data is prevalent across various
real-world applications. Many such applications can be modeled as regression
problems with high-dimensional time series covariates. Deep learning has been a
popular and powerful tool for fitting these regression models. Yet, the
development of interpretable and reproducible deep-learning models is
challenging and remains underexplored. This study introduces a novel method,
Deep Learning Inference using Knockoffs for Time series data (DeepLINK-T),
focusing on the selection of significant time series variables in regression
while controlling the false discovery rate (FDR) at a predetermined level.
DeepLINK-T combines deep learning with knockoff inference to control FDR in
feature selection for time series models, accommodating a wide variety of
feature distributions. It addresses dependencies across time and features by
leveraging a time-varying latent factor structure in time series covariates.
Three key ingredients for DeepLINK-T are 1) a Long Short-Term Memory (LSTM)
autoencoder for generating time series knockoff variables, 2) an LSTM
prediction network using both original and knockoff variables, and 3) the
application of the knockoffs framework for variable selection with FDR control.
Extensive simulation studies have been conducted to evaluate DeepLINK-T's
performance, showing its capability to control FDR effectively while
demonstrating superior feature selection power for high-dimensional
longitudinal time series data compared to its non-time series counterpart.
DeepLINK-T is further applied to three metagenomic data sets, validating its
practical utility and effectiveness, and underscoring its potential in
real-world applications.
\\ ( https://arxiv.org/abs/2404.04317 ,  2461kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04388 (*cross-listing*)
Date: Fri, 5 Apr 2024 20:12:02 GMT   (1460kb,D)

Title: Mining Potentially Explanatory Patterns via Partial Solutions
Authors: GianCarlo Catalano, Alexander E.I. Brownlee, David Cairns, John
  McCall, Russell Ainslie
Categories: cs.NE cs.LG
Comments: 9 pages, 4 figures. For source code, visit
  https://github.com/Giancarlo-Catalano/PS_Minimal_Showcase
ACM-class: I.2.8
\\
  Genetic Algorithms have established their capability for solving many complex
optimization problems. Even as good solutions are produced, the user's
understanding of a problem is not necessarily improved, which can lead to a
lack of confidence in the results. To mitigate this issue, explainability aims
to give insight to the user by presenting them with the knowledge obtained by
the algorithm. In this paper we introduce Partial Solutions in order to improve
the explainability of solutions to combinatorial optimization problems. Partial
Solutions represent beneficial traits found by analyzing a population, and are
presented to the user for explainability, but also provide an explicit model
from which new solutions can be generated. We present an algorithm that
assembles a collection of Partial Solutions chosen to strike a balance between
high fitness, simplicity and atomicity. Experiments with standard benchmarks
show that the proposed algorithm is able to find Partial Solutions which
improve explainability at reasonable computational cost without affecting
search performance.
\\ ( https://arxiv.org/abs/2404.04388 ,  1460kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04425 (*cross-listing*)
Date: Fri, 5 Apr 2024 21:47:32 GMT   (723kb,D)

Title: Bayesian Additive Regression Networks
Authors: Danielle Van Boxel
Categories: stat.ML cs.LG
\\
  We apply Bayesian Additive Regression Tree (BART) principles to training an
ensemble of small neural networks for regression tasks. Using Markov Chain
Monte Carlo, we sample from the posterior distribution of neural networks that
have a single hidden layer. To create an ensemble of these, we apply Gibbs
sampling to update each network against the residual target value (i.e.
subtracting the effect of the other networks). We demonstrate the effectiveness
of this technique on several benchmark regression problems, comparing it to
equivalent shallow neural networks, BART, and ordinary least squares. Our
Bayesian Additive Regression Networks (BARN) provide more consistent and often
more accurate results. On test data benchmarks, BARN averaged between 5 to 20
percent lower root mean square error. This error performance does come at the
cost, however, of greater computation time. BARN sometimes takes on the order
of a minute where competing methods take a second or less. But, BARN without
cross-validated hyperparameter tuning takes about the same amount of
computation time as tuned other methods. Yet BARN is still typically more
accurate.
\\ ( https://arxiv.org/abs/2404.04425 ,  723kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04434 (*cross-listing*)
Date: Fri, 5 Apr 2024 22:21:49 GMT   (15583kb,D)

Title: Robust Few-Shot Ensemble Learning with Focal Diversity-Based Pruning
Authors: Selim Furkan Tekin, Fatih Ilhan, Tiansheng Huang, Sihao Hu, Ka-Ho
  Chow, Margaret L. Loper, Ling Liu
Categories: cs.CV cs.LG
\\
  This paper presents FusionShot, a focal diversity optimized few-shot ensemble
learning approach for boosting the robustness and generalization performance of
pre-trained few-shot models. The paper makes three original contributions.
First, we explore the unique characteristics of few-shot learning to ensemble
multiple few-shot (FS) models by creating three alternative fusion channels.
Second, we introduce the concept of focal error diversity to learn the most
efficient ensemble teaming strategy, rather than assuming that an ensemble of a
larger number of base models will outperform those sub-ensembles of smaller
size. We develop a focal-diversity ensemble pruning method to effectively prune
out the candidate ensembles with low ensemble error diversity and recommend
top-$K$ FS ensembles with the highest focal error diversity. Finally, we
capture the complex non-linear patterns of ensemble few-shot predictions by
designing the learn-to-combine algorithm, which can learn the diverse weight
assignments for robust ensemble fusion over different member models. Extensive
experiments on representative few-shot benchmarks show that the top-K ensembles
recommended by FusionShot can outperform the representative SOTA few-shot
models on novel tasks (different distributions and unknown at training), and
can prevail over existing few-shot learners in both cross-domain settings and
adversarial settings. For reproducibility purposes, FusionShot trained models,
results, and code are made available at https://github.com/sftekin/fusionshot
\\ ( https://arxiv.org/abs/2404.04434 ,  15583kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04439 (*cross-listing*)
Date: Fri, 5 Apr 2024 22:48:57 GMT   (1284kb,D)

Title: Rethinking Non-Negative Matrix Factorization with Implicit Neural
  Representations
Authors: Krishna Subramani, Paris Smaragdis, Takuya Higuchi, Mehrez Souden
Categories: eess.AS cs.LG cs.SD
Comments: Submitted to IEEE SPL, Code:
  https://github.com/SubramaniKrishna/in-nmf
\\
  Non-negative Matrix Factorization (NMF) is a powerful technique for analyzing
regularly-sampled data, i.e., data that can be stored in a matrix. For audio,
this has led to numerous applications using time-frequency (TF) representations
like the Short-Time Fourier Transform. However extending these applications to
irregularly-spaced TF representations, like the Constant-Q transform, wavelets,
or sinusoidal analysis models, has not been possible since these
representations cannot be directly stored in matrix form. In this paper, we
formulate NMF in terms of continuous functions (instead of fixed vectors) and
show that NMF can be extended to a wider variety of signal classes that need
not be regularly sampled.
\\ ( https://arxiv.org/abs/2404.04439 ,  1284kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04467 (*cross-listing*)
Date: Sat, 6 Apr 2024 01:39:51 GMT   (2247kb,D)

Title: Demand Balancing in Primal-Dual Optimization for Blind Network Revenue
  Management
Authors: Sentao Miao, Yining Wang
Categories: stat.ML cs.LG
\\
  This paper proposes a practically efficient algorithm with optimal
theoretical regret which solves the classical network revenue management (NRM)
problem with unknown, nonparametric demand. Over a time horizon of length $T$,
in each time period the retailer needs to decide prices of $N$ types of
products which are produced based on $M$ types of resources with
unreplenishable initial inventory. When demand is nonparametric with some mild
assumptions, Miao and Wang (2021) is the first paper which proposes an
algorithm with $O(\text{poly}(N,M,\ln(T))\sqrt{T})$ type of regret (in
particular, $\tilde O(N^{3.5}\sqrt{T})$ plus additional high-order terms that
are $o(\sqrt{T})$ with sufficiently large $T\gg N$). In this paper, we improve
the previous result by proposing a primal-dual optimization algorithm which is
not only more practical, but also with an improved regret of $\tilde
O(N^{3.25}\sqrt{T})$ free from additional high-order terms. A key technical
contribution of the proposed algorithm is the so-called demand balancing, which
pairs the primal solution (i.e., the price) in each time period with another
price to offset the violation of complementary slackness on resource inventory
constraints. Numerical experiments compared with several benchmark algorithms
further illustrate the effectiveness of our algorithm.
\\ ( https://arxiv.org/abs/2404.04467 ,  2247kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04491 (*cross-listing*)
Date: Sat, 6 Apr 2024 03:48:11 GMT   (11825kb,D)

Title: Galaxy 3D Shape Recovery using Mixture Density Network
Authors: Suk Yee Yong, K. E. Harborne, Caroline Foster, Robert Bassett, Gregory
  B. Poole, Mitchell Cavanagh
Categories: astro-ph.IM astro-ph.GA cs.LG
Comments: Accepted for publication in PASA. 18 pages, 12 figures, 2 tables
\\
  Since the turn of the century, astronomers have been exploiting the rich
information afforded by combining stellar kinematic maps and imaging in an
attempt to recover the intrinsic, three-dimensional (3D) shape of a galaxy. A
common intrinsic shape recovery method relies on an expected monotonic
relationship between the intrinsic misalignment of the kinematic and
morphological axes and the triaxiality parameter. Recent studies have, however,
cast doubt about underlying assumptions relating shape and intrinsic kinematic
misalignment. In this work, we aim to recover the 3D shape of individual
galaxies using their projected stellar kinematic and flux distributions using a
supervised machine learning approach with mixture density network (MDN). Using
a mock dataset of the EAGLE hydrodynamical cosmological simulation, we train
the MDN model for a carefully selected set of common kinematic and photometric
parameters. Compared to previous methods, we demonstrate potential improvements
achieved with the MDN model to retrieve the 3D galaxy shape along with the
uncertainties, especially for prolate and triaxial systems. We make specific
recommendations for recovering galaxy intrinsic shapes relevant for current and
future integral field spectroscopic galaxy surveys.
\\ ( https://arxiv.org/abs/2404.04491 ,  11825kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04498 (*cross-listing*)
Date: Sat, 6 Apr 2024 04:22:48 GMT   (637kb,D)

Title: Bayesian Inference for Consistent Predictions in Overparameterized
  Nonlinear Regression
Authors: Tomoya Wakayama
Categories: stat.ML cs.LG stat.ME
\\
  The remarkable generalization performance of overparameterized models has
challenged the conventional wisdom of statistical learning theory. While recent
theoretical studies have shed light on this behavior in linear models or
nonlinear classifiers, a comprehensive understanding of overparameterization in
nonlinear regression remains lacking. This paper explores the predictive
properties of overparameterized nonlinear regression within the Bayesian
framework, extending the methodology of adaptive prior based on the intrinsic
spectral structure of the data. We establish posterior contraction for
single-neuron models with Lipschitz continuous activation functions and for
generalized linear models, demonstrating that our approach achieves consistent
predictions in the overparameterized regime. Moreover, our Bayesian framework
allows for uncertainty estimation of the predictions. The proposed method is
validated through numerical simulations and a real data application, showcasing
its ability to achieve accurate predictions and reliable uncertainty estimates.
Our work advances the theoretical understanding of the blessing of
overparameterization and offers a principled Bayesian approach for prediction
in large nonlinear models.
\\ ( https://arxiv.org/abs/2404.04498 ,  637kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04549 (*cross-listing*)
Date: Sat, 6 Apr 2024 08:17:07 GMT   (275kb,D)

Title: Efficient Learning Using Spiking Neural Networks Equipped With Affine
  Encoders and Decoders
Authors: A. Martina Neuman, Philipp Christian Petersen
Categories: cs.NE cs.LG math.FA stat.ML
\\
  We study the learning problem associated with spiking neural networks.
Specifically, we consider hypothesis sets of spiking neural networks with
affine temporal encoders and decoders and simple spiking neurons having only
positive synaptic weights. We demonstrate that the positivity of the weights
continues to enable a wide range of expressivity results, including
rate-optimal approximation of smooth functions or approximation without the
curse of dimensionality. Moreover, positive-weight spiking neural networks are
shown to depend continuously on their parameters which facilitates classical
covering number-based generalization statements. Finally, we observe that from
a generalization perspective, contrary to feedforward neural networks or
previous results for general spiking neural networks, the depth has little to
no adverse effect on the generalization capabilities.
\\ ( https://arxiv.org/abs/2404.04549 ,  275kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04567 (*cross-listing*)
Date: Sat, 6 Apr 2024 09:30:38 GMT   (236kb,D)

Title: Optimization of Lightweight Malware Detection Models For AIoT Devices
Authors: Felicia Lo, Shin-Ming Cheng, Rafael Kaliski
Categories: cs.CR cs.LG
Comments: Accepted by WF-IOT 2023, 7 pages, 4 figures
\\
  Malware intrusion is problematic for Internet of Things (IoT) and Artificial
Intelligence of Things (AIoT) devices as they often reside in an ecosystem of
connected devices, such as a smart home. If any devices are infected, the whole
ecosystem can be compromised. Although various Machine Learning (ML) models are
deployed to detect malware and network intrusion, generally speaking, robust
high-accuracy models tend to require resources not found in all IoT devices,
compared to less robust models defined by weak learners. In order to combat
this issue, Fadhilla proposed a meta-learner ensemble model comprised of less
robust prediction results inherent with weak learner ML models to produce a
highly robust meta-learning ensemble model. The main problem with the prior
research is that it cannot be deployed in low-end AIoT devices due to the
limited resources comprising processing power, storage, and memory (the
required libraries quickly exhaust low-end AIoT devices' resources.) Hence,
this research aims to optimize the proposed super learner meta-learning
ensemble model to make it viable for low-end AIoT devices. We show the library
and ML model memory requirements associated with each optimization stage and
emphasize that optimization of current ML models is necessitated for low-end
AIoT devices. Our results demonstrate that we can obtain similar accuracy and
False Positive Rate (FPR) metrics from high-end AIoT devices running the
derived ML model, with a lower inference duration and smaller memory footprint.
\\ ( https://arxiv.org/abs/2404.04567 ,  236kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04687 (*cross-listing*)
Date: Sat, 6 Apr 2024 17:23:43 GMT   (14430kb,D)

Title: Z-Splat: Z-Axis Gaussian Splatting for Camera-Sonar Fusion
Authors: Ziyuan Qu, Omkar Vengurlekar, Mohamad Qadri, Kevin Zhang, Michael
  Kaess, Christopher Metzler, Suren Jayasuriya, Adithya Pediredla
Categories: cs.CV cs.GR cs.LG
\\
  Differentiable 3D-Gaussian splatting (GS) is emerging as a prominent
technique in computer vision and graphics for reconstructing 3D scenes. GS
represents a scene as a set of 3D Gaussians with varying opacities and employs
a computationally efficient splatting operation along with analytical
derivatives to compute the 3D Gaussian parameters given scene images captured
from various viewpoints. Unfortunately, capturing surround view ($360^{\circ}$
viewpoint) images is impossible or impractical in many real-world imaging
scenarios, including underwater imaging, rooms inside a building, and
autonomous navigation. In these restricted baseline imaging scenarios, the GS
algorithm suffers from a well-known 'missing cone' problem, which results in
poor reconstruction along the depth axis. In this manuscript, we demonstrate
that using transient data (from sonars) allows us to address the missing cone
problem by sampling high-frequency data along the depth axis. We extend the
Gaussian splatting algorithms for two commonly used sonars and propose fusion
algorithms that simultaneously utilize RGB camera data and sonar data. Through
simulations, emulations, and hardware experiments across various imaging
scenarios, we show that the proposed fusion algorithms lead to significantly
better novel view synthesis (5 dB improvement in PSNR) and 3D geometry
reconstruction (60% lower Chamfer distance).
\\ ( https://arxiv.org/abs/2404.04687 ,  14430kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04689 (*cross-listing*)
Date: Sat, 6 Apr 2024 17:33:37 GMT   (376kb,D)

Title: Multicalibration for Confidence Scoring in LLMs
Authors: Gianluca Detommaso, Martin Bertran, Riccardo Fogliato, Aaron Roth
Categories: stat.ML cs.LG
\\
  This paper proposes the use of "multicalibration" to yield interpretable and
reliable confidence scores for outputs generated by large language models
(LLMs). Multicalibration asks for calibration not just marginally, but
simultaneously across various intersecting groupings of the data. We show how
to form groupings for prompt/completion pairs that are correlated with the
probability of correctness via two techniques: clustering within an embedding
space, and "self-annotation" - querying the LLM by asking it various yes-or-no
questions about the prompt. We also develop novel variants of multicalibration
algorithms that offer performance improvements by reducing their tendency to
overfit. Through systematic benchmarking across various question answering
datasets and LLMs, we show how our techniques can yield confidence scores that
provide substantial improvements in fine-grained measures of both calibration
and accuracy compared to existing methods.
\\ ( https://arxiv.org/abs/2404.04689 ,  376kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04710 (*cross-listing*)
Date: Sat, 6 Apr 2024 19:07:12 GMT   (1708kb)

Title: Explaining Indian Stock Market through Geometry of Scale free Networks
Authors: Pawanesh Yadav, Charu Sharma, Niteesh Sahni
Categories: physics.soc-ph cs.LG
Comments: 44 pages, 11 figures
\\
  This paper presents an analysis of the Indian stock market using a method
based on embedding the network in a hyperbolic space using Machine learning
techniques. We claim novelty on four counts. First, it is demonstrated that the
hyperbolic clusters resemble the topological network communities more closely
than the Euclidean clusters. Second, we are able to clearly distinguish between
periods of market stability and volatility through a statistical analysis of
hyperbolic distance and hyperbolic shortest path distance corresponding to the
embedded network. Third, we demonstrate that using the modularity of the
embedded network significant market changes can be spotted early. Lastly, the
coalescent embedding is able to segregate the certain market sectors thereby
underscoring its natural clustering ability.
\\ ( https://arxiv.org/abs/2404.04710 ,  1708kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04810 (*cross-listing*)
Date: Sun, 7 Apr 2024 05:17:43 GMT   (1771kb,D)

Title: AlphaCrystal-II: Distance matrix based crystal structure prediction
  using deep learning
Authors: Yuqi Song, Rongzhi Dong, Lai Wei, Qin Li, Jianjun Hu
Categories: cond-mat.mtrl-sci cs.LG
Comments: 16 pages
\\
  Computational prediction of stable crystal structures has a profound impact
on the large-scale discovery of novel functional materials. However, predicting
the crystal structure solely from a material's composition or formula is a
promising yet challenging task, as traditional ab initio crystal structure
prediction (CSP) methods rely on time-consuming global searches and
first-principles free energy calculations. Inspired by the recent success of
deep learning approaches in protein structure prediction, which utilize
pairwise amino acid interactions to describe 3D structures, we present
AlphaCrystal-II, a novel knowledge-based solution that exploits the abundant
inter-atomic interaction patterns found in existing known crystal structures.
AlphaCrystal-II predicts the atomic distance matrix of a target crystal
material and employs this matrix to reconstruct its 3D crystal structure. By
leveraging the wealth of inter-atomic relationships of known crystal
structures, our approach demonstrates remarkable effectiveness and reliability
in structure prediction through comprehensive experiments. This work highlights
the potential of data-driven methods in accelerating the discovery and design
of new materials with tailored properties.
\\ ( https://arxiv.org/abs/2404.04810 ,  1771kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04815 (*cross-listing*)
Date: Sun, 7 Apr 2024 05:47:54 GMT   (753kb,D)

Title: Allo: A Programming Model for Composable Accelerator Design
Authors: Hongzheng Chen, Niansong Zhang, Shaojie Xiang, Zhichen Zeng, Mengjia
  Dai, Zhiru Zhang
Categories: cs.PL cs.AR cs.LG
Comments: Accepted to PLDI'24
DOI: 10.1145/3656401
\\
  Special-purpose hardware accelerators are increasingly pivotal for sustaining
performance improvements in emerging applications, especially as the benefits
of technology scaling continue to diminish. However, designers currently lack
effective tools and methodologies to construct complex, high-performance
accelerator architectures in a productive manner. Existing high-level synthesis
(HLS) tools often require intrusive source-level changes to attain satisfactory
quality of results. Despite the introduction of several new accelerator design
languages (ADLs) aiming to enhance or replace HLS, their advantages are more
evident in relatively simple applications with a single kernel. Existing ADLs
prove less effective for realistic hierarchical designs with multiple kernels,
even if the design hierarchy is flattened.
  In this paper, we introduce Allo, a composable programming model for
efficient spatial accelerator design. Allo decouples hardware customizations,
including compute, memory, communication, and data type from algorithm
specification, and encapsulates them as a set of customization primitives. Allo
preserves the hierarchical structure of an input program by combining
customizations from different functions in a bottom-up, type-safe manner. This
approach facilitates holistic optimizations that span across function
boundaries. We conduct comprehensive experiments on commonly-used HLS
benchmarks and several realistic deep learning models. Our evaluation shows
that Allo can outperform state-of-the-art HLS tools and ADLs on all test cases
in the PolyBench. For the GPT2 model, the inference latency of the Allo
generated accelerator is 1.7x faster than the NVIDIA A100 GPU with 5.4x higher
energy efficiency, demonstrating the capability of Allo to handle large-scale
designs.
\\ ( https://arxiv.org/abs/2404.04815 ,  753kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04891 (*cross-listing*)
Date: Sun, 7 Apr 2024 09:17:00 GMT   (756kb)

Title: DL-EWF: Deep Learning Empowering Women's Fashion with
  Grounded-Segment-Anything Segmentation for Body Shape Classification
Authors: Fatemeh Asghari, Mohammad Reza Soheili, Faezeh Gholamrezaie
Categories: cs.CV cs.LG
\\
  The global fashion industry plays a pivotal role in the global economy, and
addressing fundamental issues within the industry is crucial for developing
innovative solutions. One of the most pressing challenges in the fashion
industry is the mismatch between body shapes and the garments of individuals
they purchase. This issue is particularly prevalent among individuals with
non-ideal body shapes, exacerbating the challenges faced. Considering
inter-individual variability in body shapes is essential for designing and
producing garments that are widely accepted by consumers. Traditional methods
for determining human body shape are limited due to their low accuracy, high
costs, and time-consuming nature. New approaches, utilizing digital imaging and
deep neural networks (DNN), have been introduced to identify human body shape.
In this study, the Style4BodyShape dataset is used for classifying body shapes
into five categories: Rectangle, Triangle, Inverted Triangle, Hourglass, and
Apple. In this paper, the body shape segmentation of a person is extracted from
the image, disregarding the surroundings and background. Then, Various
pre-trained models, such as ResNet18, ResNet34, ResNet50, VGG16, VGG19, and
Inception v3, are used to classify the segmentation results. Among these
pre-trained models, the Inception V3 model demonstrates superior performance
regarding f1-score evaluation metric and accuracy compared to the other models.
\\ ( https://arxiv.org/abs/2404.04891 ,  756kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04916 (*cross-listing*)
Date: Sun, 7 Apr 2024 10:57:54 GMT   (2724kb,D)

Title: Correcting Diffusion-Based Perceptual Image Compression with Privileged
  End-to-End Decoder
Authors: Yiyang Ma, Wenhan Yang, Jiaying Liu
Categories: eess.IV cs.CV cs.LG
\\
  The images produced by diffusion models can attain excellent perceptual
quality. However, it is challenging for diffusion models to guarantee
distortion, hence the integration of diffusion models and image compression
models still needs more comprehensive explorations. This paper presents a
diffusion-based image compression method that employs a privileged end-to-end
decoder model as correction, which achieves better perceptual quality while
guaranteeing the distortion to an extent. We build a diffusion model and design
a novel paradigm that combines the diffusion model and an end-to-end decoder,
and the latter is responsible for transmitting the privileged information
extracted at the encoder side. Specifically, we theoretically analyze the
reconstruction process of the diffusion models at the encoder side with the
original images being visible. Based on the analysis, we introduce an
end-to-end convolutional decoder to provide a better approximation of the score
function $\nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t)$ at the encoder side and
effectively transmit the combination. Experiments demonstrate the superiority
of our method in both distortion and perception compared with previous
perceptual compression methods.
\\ ( https://arxiv.org/abs/2404.04916 ,  2724kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04979 (*cross-listing*)
Date: Sun, 7 Apr 2024 14:47:07 GMT   (2037kb,D)

Title: CAVIAR: Categorical-Variable Embeddings for Accurate and Robust
  Inference
Authors: Anirban Mukherjee, Hannah Hanwen Chang
Categories: econ.EM cs.LG
\\
  Social science research often hinges on the relationship between categorical
variables and outcomes. We introduce CAVIAR, a novel method for embedding
categorical variables that assume values in a high-dimensional ambient space
but are sampled from an underlying manifold. Our theoretical and numerical
analyses outline challenges posed by such categorical variables in causal
inference. Specifically, dynamically varying and sparse levels can lead to
violations of the Donsker conditions and a failure of the estimation
functionals to converge to a tight Gaussian process. Traditional approaches,
including the exclusion of rare categorical levels and principled variable
selection models like LASSO, fall short. CAVIAR embeds the data into a
lower-dimensional global coordinate system. The mapping can be derived from
both structured and unstructured data, and ensures stable and robust estimates
through dimensionality reduction. In a dataset of direct-to-consumer apparel
sales, we illustrate how high-dimensional categorical variables, such as zip
codes, can be succinctly represented, facilitating inference and analysis.
\\ ( https://arxiv.org/abs/2404.04979 ,  2037kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05022 (*cross-listing*)
Date: Sun, 7 Apr 2024 17:25:52 GMT   (11785kb,D)

Title: DinoBloom: A Foundation Model for Generalizable Cell Embeddings in
  Hematology
Authors: Valentin Koch, Sophia J. Wagner, Salome Kazeminia, Ece Sancar,
  Matthias Hehr, Julia Schnabel, Tingying Peng, Carsten Marr
Categories: cs.CV cs.LG
\\
  In hematology, computational models offer significant potential to improve
diagnostic accuracy, streamline workflows, and reduce the tedious work of
analyzing single cells in peripheral blood or bone marrow smears. However,
clinical adoption of computational models has been hampered by the lack of
generalization due to large batch effects, small dataset sizes, and poor
performance in transfer learning from natural images. To address these
challenges, we introduce DinoBloom, the first foundation model for single cell
images in hematology, utilizing a tailored DINOv2 pipeline. Our model is built
upon an extensive collection of 13 diverse, publicly available datasets of
peripheral blood and bone marrow smears, the most substantial open-source
cohort in hematology so far, comprising over 380,000 white blood cell images.
To assess its generalization capability, we evaluate it on an external dataset
with a challenging domain shift. We show that our model outperforms existing
medical and non-medical vision models in (i) linear probing and k-nearest
neighbor evaluations for cell-type classification on blood and bone marrow
smears and (ii) weakly supervised multiple instance learning for acute myeloid
leukemia subtyping by a large margin. A family of four DinoBloom models (small,
base, large, and giant) can be adapted for a wide range of downstream
applications, be a strong baseline for classification problems, and facilitate
the assessment of batch effects in new datasets. All models are available at
github.com/marrlab/DinoBloom.
\\ ( https://arxiv.org/abs/2404.05022 ,  11785kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05062 (*cross-listing*)
Date: Sun, 7 Apr 2024 20:16:37 GMT   (9427kb,D)

Title: New methods for computing the generalized chi-square distribution
Authors: Abhranil Das
Categories: stat.CO cs.LG stat.ME stat.ML
\\
  We present several exact and approximate mathematical methods and open-source
software to compute the cdf, pdf and inverse cdf of the generalized chi-square
distribution, which appears in Bayesian classification problems. Some methods
are geared for speed, while others are designed to be accurate far into the
tails, using which we can also measure large values of the discriminability
index $d'$ between multinormals. We compare the accuracy and speed of these
methods against the best existing methods.
\\ ( https://arxiv.org/abs/2404.05062 ,  9427kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05102 (*cross-listing*)
Date: Sun, 7 Apr 2024 22:58:18 GMT   (16231kb,D)

Title: LHU-Net: A Light Hybrid U-Net for Cost-Efficient, High-Performance
  Volumetric Medical Image Segmentation
Authors: Yousef Sadegheih, Afshin Bozorgpour, Pratibha Kumari, Reza Azad, Dorit
  Merhof
Categories: eess.IV cs.CV cs.LG
\\
  As a result of the rise of Transformer architectures in medical image
analysis, specifically in the domain of medical image segmentation, a multitude
of hybrid models have been created that merge the advantages of Convolutional
Neural Networks (CNNs) and Transformers. These hybrid models have achieved
notable success by significantly improving segmentation accuracy. Yet, this
progress often comes at the cost of increased model complexity, both in terms
of parameters and computational demand. Moreover, many of these models fail to
consider the crucial interplay between spatial and channel features, which
could further refine and improve segmentation outcomes. To address this, we
introduce LHU-Net, a Light Hybrid U-Net architecture optimized for volumetric
medical image segmentation. LHU-Net is meticulously designed to prioritize
spatial feature analysis in its initial layers before shifting focus to
channel-based features in its deeper layers, ensuring a comprehensive feature
extraction process. Rigorous evaluation across five benchmark datasets -
Synapse, LA, Pancreas, ACDC, and BRaTS 2018 - underscores LHU-Net's superior
performance, showcasing its dual capacity for efficiency and accuracy. Notably,
LHU-Net sets new performance benchmarks, such as attaining a Dice score of
92.66 on the ACDC dataset, while simultaneously reducing parameters by 85% and
quartering the computational load compared to existing state-of-the-art models.
Achieved without any reliance on pre-training, additional data, or model
ensemble, LHU-Net's effectiveness is further evidenced by its state-of-the-art
performance across all evaluated datasets, utilizing fewer than 11 million
parameters. This achievement highlights that balancing computational efficiency
with high accuracy in medical image segmentation is feasible. Our
implementation of LHU-Net is freely accessible to the research community on
GitHub.
\\ ( https://arxiv.org/abs/2404.05102 ,  16231kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05108 (*cross-listing*)
Date: Sun, 7 Apr 2024 23:34:51 GMT   (88kb,D)

Title: Efficient Gradient Estimation of Variational Quantum Circuits with Lie
  Algebraic Symmetries
Authors: Mohsen Heidari, Masih Mozakka, Wojciech Szpankowski
Categories: quant-ph cs.IT cs.LG math.IT
\\
  Hybrid quantum-classical optimization and learning strategies are among the
most promising approaches to harnessing quantum information or gaining a
quantum advantage over classical methods. However, efficient estimation of the
gradient of the objective function in such models remains a challenge due to
several factors including the exponential dimensionality of the Hilbert spaces,
and information loss of quantum measurements. In this work, we study generic
parameterized circuits in the context of variational methods. We develop a
framework for gradient estimation that exploits the algebraic symmetries of
Hamiltonian characterized through Lie algebra or group theory. Particularly, we
prove that when the dimension of the dynamical Lie algebra is polynomial in the
number of qubits, one can estimate the gradient with polynomial classical and
quantum resources. This is done by a series of Hadamard tests applied to the
output of the ansatz with no change to its circuit. We show that this approach
can be equipped with classical shadow tomography to further reduce the
measurement shot complexity to scale logarithmically with the number of
parameters.
\\ ( https://arxiv.org/abs/2404.05108 ,  88kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05128 (*cross-listing*)
Date: Mon, 8 Apr 2024 01:08:41 GMT   (28579kb,D)

Title: Improving Deep Learning Predictions with Simulated Images, and Vice
  Versa
Authors: Nazifa Azam Khan, Mikolaj Cieslak, Ian McQuillan
Categories: cs.CV cs.LG
\\
  Artificial neural networks are often used to identify features of crop
plants. However, training their models requires many annotated images, which
can be expensive and time-consuming to acquire. Procedural models of plants,
such as those developed with Lindenmayer-systems (L-systems) can be created to
produce visually realistic simulations, and hence images of plant simulations,
where annotations are implicitly known. These synthetic images can either
augment or completely replace real images in training neural networks for
phenotyping tasks. In this paper, we systematically vary amounts of real and
synthetic images used for training in both maize and canola to better
understand situations where synthetic images generated from L-systems can help
prediction on real images. This work also explores the degree to which realism
in the synthetic images improves prediction. Furthermore, we see how neural
network predictions can be used to help calibrate L-systems themselves,
creating a feedback loop.
\\ ( https://arxiv.org/abs/2404.05128 ,  28579kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05183 (*cross-listing*)
Date: Mon, 8 Apr 2024 04:17:27 GMT   (4095kb,D)

Title: Progressive Alignment with VLM-LLM Feature to Augment Defect
  Classification for the ASE Dataset
Authors: Chih-Chung Hsu, Chia-Ming Lee, Chun-Hung Sun, and Kuang-Ming Wu
Categories: cs.CV cs.LG
Comments: MULA 2024
\\
  Traditional defect classification approaches are facing with two barriers.
(1) Insufficient training data and unstable data quality. Collecting sufficient
defective sample is expensive and time-costing, consequently leading to dataset
variance. It introduces the difficulty on recognition and learning. (2)
Over-dependence on visual modality. When the image pattern and texture is
monotonic for all defect classes in a given dataset, the performance of
conventional AOI system cannot be guaranteed. In scenarios where image quality
is compromised due to mechanical failures or when defect information is
inherently difficult to discern, the performance of deep models cannot be
guaranteed. A main question is, "how to solve those two problems when they
occur at the same time?" The feasible strategy is to explore another feature
within dataset and combine an eminent vision-language model (VLM) and
Large-Language model (LLM) with their astonishing zero-shot capability. In this
work, we propose the special ASE dataset, including rich data description
recorded on image, for defect classification, but the defect feature is uneasy
to learn directly. Secondly, We present the prompting for VLM-LLM against
defect classification with the proposed ASE dataset to activate extra-modality
feature from images to enhance performance. Then, We design the novel
progressive feature alignment (PFA) block to refine image-text feature to
alleviate the difficulty of alignment under few-shot scenario. Finally, the
proposed Cross-modality attention fusion (CMAF) module can effectively fuse
different modality feature. Experiment results have demonstrated our method's
effectiveness over several defect classification methods for the ASE dataset.
\\ ( https://arxiv.org/abs/2404.05183 ,  4095kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05184 (*cross-listing*)
Date: Mon, 8 Apr 2024 04:18:54 GMT   (7111kb,D)

Title: Predicting the Geothermal Gradient in Colombia: a Machine Learning
  Approach
Authors: Juan C. Mej {\i}a-Fragoso, Manuel A. Florez and Roc{\i}o Bernal-Olaya
Categories: physics.geo-ph cs.LG
\\
  Accurate determination of the geothermal gradient is critical for assessing
the geothermal energy potential of a given region. Of particular interest is
the case of Colombia, a country with abundant geothermal resources. A history
of active oil and gas exploration and production has left drilled boreholes in
different geological settings, providing direct measurements of the geothermal
gradient. Unfortunately, large regions of the country where geothermal
resources might exist lack such measurements. Indirect geophysical measurements
are costly and difficult to perform at regional scales. Computational thermal
models could be constructed, but they require very detailed knowledge of the
underlying geology and uniform sampling of subsurface temperatures to be
well-constrained. We present an alternative approach that leverages recent
advances in supervised machine learning and available direct measurements to
predict the geothermal gradient in regions where only global-scale geophysical
datasets and course geological knowledge are available. We find that a Gradient
Boosted Regression Tree algorithm yields optimal predictions and extensively
validate the trained model. We show that predictions of our model are within
12\% accuracy and that independent measurements performed by other authors
agree well with our model. Finnally, we present a geothermal gradient map for
Colombia that highlights regions where futher exploration and data collection
should be performed.
\\ ( https://arxiv.org/abs/2404.05184 ,  7111kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05185 (*cross-listing*)
Date: Mon, 8 Apr 2024 04:22:55 GMT   (117kb,D)

Title: Convergence analysis of controlled particle systems arising in deep
  learning: from finite to infinite sample size
Authors: Huafu Liao, Alp\'ar R. M\'esz\'aros, Chenchen Mou, Chao Zhou
Categories: math.OC cs.LG math.PR stat.ML
Comments: 45 pages, 2 figures
MSC-class: 49N80, 65C35, 49L12, 62M45
\\
  This paper deals with a class of neural SDEs and studies the limiting
behavior of the associated sampled optimal control problems as the sample size
grows to infinity. The neural SDEs with N samples can be linked to the
N-particle systems with centralized control. We analyze the
Hamilton--Jacobi--Bellman equation corresponding to the N-particle system and
establish regularity results which are uniform in N. The uniform regularity
estimates are obtained by the stochastic maximum principle and the analysis of
a backward stochastic Riccati equation. Using these uniform regularity results,
we show the convergence of the minima of objective functionals and optimal
parameters of the neural SDEs as the sample size N tends to infinity. The
limiting objects can be identified with suitable functions defined on the
Wasserstein space of Borel probability measures. Furthermore, quantitative
algebraic convergence rates are also obtained.
\\ ( https://arxiv.org/abs/2404.05185 ,  117kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05210 (*cross-listing*)
Date: Mon, 8 Apr 2024 05:45:03 GMT   (759kb,D)

Title: Bidirectional Long-Range Parser for Sequential Data Understanding
Authors: George Leotescu, Daniel Voinea, Alin-Ionut Popa
Categories: cs.CV cs.LG
\\
  The transformer is a powerful data modelling framework responsible for
remarkable performance on a wide range of tasks. However, they are limited in
terms of scalability as it is suboptimal and inefficient to process
long-sequence data. To this purpose we introduce BLRP (Bidirectional Long-Range
Parser), a novel and versatile attention mechanism designed to increase
performance and efficiency on long-sequence tasks. It leverages short and long
range heuristics in the form of a local sliding window approach combined with a
global bidirectional latent space synthesis technique. We show the benefits and
versatility of our approach on vision and language domains by demonstrating
competitive results against state-of-the-art methods on the Long-Range-Arena
and CIFAR benchmarks together with ablations demonstrating the computational
efficiency.
\\ ( https://arxiv.org/abs/2404.05210 ,  759kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05249 (*cross-listing*)
Date: Mon, 8 Apr 2024 07:25:25 GMT   (13850kb,D)

Title: SAFE-GIL: SAFEty Guided Imitation Learning
Authors: Yusuf Umut Ciftci, Zeyuan Feng and Somil Bansal
Categories: cs.RO cs.LG cs.SY eess.SY
\\
  Behavior Cloning is a popular approach to Imitation Learning, in which a
robot observes an expert supervisor and learns a control policy. However,
behavior cloning suffers from the "compounding error" problem - the policy
errors compound as it deviates from the expert demonstrations and might lead to
catastrophic system failures, limiting its use in safety-critical applications.
On-policy data aggregation methods are able to address this issue at the cost
of rolling out and repeated training of the imitation policy, which can be
tedious and computationally prohibitive. We propose SAFE-GIL, an off-policy
behavior cloning method that guides the expert via adversarial disturbance
during data collection. The algorithm abstracts the imitation error as an
adversarial disturbance in the system dynamics, injects it during data
collection to expose the expert to safety critical states, and collects
corrective actions. Our method biases training to more closely replicate expert
behavior in safety-critical states and allows more variance in less critical
states. We compare our method with several behavior cloning techniques and
DAgger on autonomous navigation and autonomous taxiing tasks and show higher
task success and safety, especially in low data regimes where the likelihood of
error is higher, at a slight drop in the performance.
\\ ( https://arxiv.org/abs/2404.05249 ,  13850kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05270 (*cross-listing*)
Date: Mon, 8 Apr 2024 08:00:05 GMT   (1155kb,D)

Title: Exploiting Preference Elicitation in Interactive and User-centered
  Algorithmic Recourse: An Initial Exploration
Authors: Seyedehdelaram Esfahani, Giovanni De Toni, Bruno Lepri, Andrea
  Passerini, Katya Tentori, Massimo Zancanaro
Categories: cs.HC cs.CY cs.LG
\\
  Algorithmic Recourse aims to provide actionable explanations, or recourse
plans, to overturn potentially unfavourable decisions taken by automated
machine learning models. In this paper, we propose an interaction paradigm
based on a guided interaction pattern aimed at both eliciting the users'
preferences and heading them toward effective recourse interventions. In a
fictional task of money lending, we compare this approach with an exploratory
interaction pattern based on a combination of alternative plans and the
possibility of freely changing the configurations by the users themselves. Our
results suggest that users may recognize that the guided interaction paradigm
improves efficiency. However, they also feel less freedom to experiment with
"what-if" scenarios. Nevertheless, the time spent on the purely exploratory
interface tends to be perceived as a lack of efficiency, which reduces
attractiveness, perspicuity, and dependability. Conversely, for the guided
interface, more time on the interface seems to increase its attractiveness,
perspicuity, and dependability while not impacting the perceived efficiency.
That might suggest that this type of interfaces should combine these two
approaches by trying to support exploratory behavior while gently pushing
toward a guided effective solution.
\\ ( https://arxiv.org/abs/2404.05270 ,  1155kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05298 (*cross-listing*)
Date: Mon, 8 Apr 2024 08:35:50 GMT   (2900kb,D)

Title: In-Flight Estimation of Instrument Spectral Response Functions Using
  Sparse Representations
Authors: Jihanne El Haouari and Jean-Michel Gaucel and Christelle Pittet and
  Jean-Yves Tourneret and Herwig Wendt
Categories: math.NA cs.LG cs.NA
\\
  Accurate estimates of Instrument Spectral Response Functions (ISRFs) are
crucial in order to have a good characterization of high resolution
spectrometers. Spectrometers are composed of different optical elements that
can induce errors in the measurements and therefore need to be modeled as
accurately as possible. Parametric models are currently used to estimate these
response functions. However, these models cannot always take into account the
diversity of ISRF shapes that are encountered in practical applications. This
paper studies a new ISRF estimation method based on a sparse representation of
atoms belonging to a dictionary. This method is applied to different
high-resolution spectrometers in order to assess its reproducibility for
multiple remote sensing missions. The proposed method is shown to be very
competitive when compared to the more commonly used parametric models, and
yields normalized ISRF estimation errors less than 1%.
\\ ( https://arxiv.org/abs/2404.05298 ,  2900kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05304 (*cross-listing*)
Date: Mon, 8 Apr 2024 08:47:46 GMT   (634kb,D)

Title: Liquid Neural Network-based Adaptive Learning vs. Incremental Learning
  for Link Load Prediction amid Concept Drift due to Network Failures
Authors: Omran Ayoub, Davide Andreoletti, Aleksandra Knapi\'nska, R\'o\.za
  Go\'scie\'n, Piotr Lechowicz, Tiziano Leidi, Silvia Giordano, Cristina
  Rottondi, Krzysztof Walkowiak
Categories: cs.NI cs.LG
\\
  Adapting to concept drift is a challenging task in machine learning, which is
usually tackled using incremental learning techniques that periodically re-fit
a learning model leveraging newly available data. A primary limitation of these
techniques is their reliance on substantial amounts of data for retraining. The
necessity of acquiring fresh data introduces temporal delays prior to
retraining, potentially rendering the models inaccurate if a sudden concept
drift occurs in-between two consecutive retrainings. In communication networks,
such issue emerges when performing traffic forecasting following a~failure
event: post-failure re-routing may induce a drastic shift in distribution and
pattern of traffic data, thus requiring a timely model adaptation. In this
work, we address this challenge for the problem of traffic forecasting and
propose an approach that exploits adaptive learning algorithms, namely, liquid
neural networks, which are capable of self-adaptation to abrupt changes in data
patterns without requiring any retraining. Through extensive simulations of
failure scenarios, we compare the predictive performance of our proposed
approach to that of a reference method based on incremental learning.
Experimental results show that our proposed approach outperforms incremental
learning-based methods in situations where the shifts in traffic patterns are
drastic.
\\ ( https://arxiv.org/abs/2404.05304 ,  634kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05359 (*cross-listing*)
Date: Mon, 8 Apr 2024 09:52:19 GMT   (1479kb,D)

Title: Improving Algorithm-Selection and Performance-Prediction via Learning
  Discriminating Training Samples
Authors: Quentin Renau and Emma Hart
Categories: cs.NE cs.LG
Comments: To appear in the proceedings of The Genetic and Evolutionary
  Computation Conference 2024
\\
  The choice of input-data used to train algorithm-selection models is
recognised as being a critical part of the model success. Recently,
feature-free methods for algorithm-selection that use short trajectories
obtained from running a solver as input have shown promise. However, it is
unclear to what extent these trajectories reliably discriminate between
solvers. We propose a meta approach to generating discriminatory trajectories
with respect to a portfolio of solvers. The algorithm-configuration tool irace
is used to tune the parameters of a simple Simulated Annealing algorithm (SA)
to produce trajectories that maximise the performance metrics of ML models
trained on this data. We show that when the trajectories obtained from the
tuned SA algorithm are used in ML models for algorithm-selection and
performance prediction, we obtain significantly improved performance metrics
compared to models trained both on raw trajectory data and on exploratory
landscape features.
\\ ( https://arxiv.org/abs/2404.05359 ,  1479kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05368 (*cross-listing*)
Date: Mon, 8 Apr 2024 10:10:30 GMT   (319kb,D)

Title: Exploring Quantization and Mapping Synergy in Hardware-Aware Deep Neural
  Network Accelerators
Authors: Jan Klhufek, Miroslav Safar, Vojtech Mrazek, Zdenek Vasicek, Lukas
  Sekanina
Categories: cs.AR cs.LG
Comments: To appear at the 2024 27th International Symposium on Design &
  Diagnostics of Electronic Circuits & Systems (DDECS)
\\
  Energy efficiency and memory footprint of a convolutional neural network
(CNN) implemented on a CNN inference accelerator depend on many factors,
including a weight quantization strategy (i.e., data types and bit-widths) and
mapping (i.e., placement and scheduling of DNN elementary operations on
hardware units of the accelerator). We show that enabling rich mixed
quantization schemes during the implementation can open a previously hidden
space of mappings that utilize the hardware resources more effectively. CNNs
utilizing quantized weights and activations and suitable mappings can
significantly improve trade-offs among the accuracy, energy, and memory
requirements compared to less carefully optimized CNN implementations. To find,
analyze, and exploit these mappings, we: (i) extend a general-purpose
state-of-the-art mapping tool (Timeloop) to support mixed quantization, which
is not currently available; (ii) propose an efficient multi-objective
optimization algorithm to find the most suitable bit-widths and mapping for
each DNN layer executed on the accelerator; and (iii) conduct a detailed
experimental evaluation to validate the proposed method. On two CNNs
(MobileNetV1 and MobileNetV2) and two accelerators (Eyeriss and Simba) we show
that for a given quality metric (such as the accuracy on ImageNet), energy
savings are up to 37% without any accuracy drop.
\\ ( https://arxiv.org/abs/2404.05368 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05444 (*cross-listing*)
Date: Mon, 8 Apr 2024 12:26:06 GMT   (843kb)

Title: The Open Autonomy Safety Case Framework
Authors: Michael Wagner and Carmen Carlan
Categories: cs.SE cs.LG cs.RO cs.SY eess.SY
Journal-ref: Safety-Critical Systems eJournal, Vol. 3 No. 1 (2024)
\\
  A system safety case is a compelling, comprehensible, and valid argument
about the satisfaction of the safety goals of a given system operating in a
given environment supported by convincing evidence. Since the publication of UL
4600 in 2020, safety cases have become a best practice for measuring, managing,
and communicating the safety of autonomous vehicles (AVs). Although UL 4600
provides guidance on how to build the safety case for an AV, the complexity of
AVs and their operating environments, the novelty of the used technology, the
need for complying with various regulations and technical standards, and for
addressing cybersecurity concerns and ethical considerations make the
development of safety cases for AVs challenging. To this end, safety case
frameworks have been proposed that bring strategies, argument templates, and
other guidance together to support the development of a safety case. This paper
introduces the Open Autonomy Safety Case Framework, developed over years of
work with the autonomous vehicle industry, as a roadmap for how AVs can be
deployed safely and responsibly.
\\ ( https://arxiv.org/abs/2404.05444 ,  843kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05445 (*cross-listing*)
Date: Mon, 8 Apr 2024 12:27:00 GMT   (22749kb,D)

Title: Unsupervised Training of Convex Regularizers using Maximum Likelihood
  Estimation
Authors: Hong Ye Tan, Ziruo Cai, Marcelo Pereyra, Subhadip Mukherjee, Junqi
  Tang, Carola-Bibiane Sch\"onlieb
Categories: stat.ME cs.LG stat.CO
MSC-class: 62C12, 62F15, 65C40, 65J22
\\
  Unsupervised learning is a training approach in the situation where ground
truth data is unavailable, such as inverse imaging problems. We present an
unsupervised Bayesian training approach to learning convex neural network
regularizers using a fixed noisy dataset, based on a dual Markov chain
estimation method. Compared to classical supervised adversarial regularization
methods, where there is access to both clean images as well as unlimited to
noisy copies, we demonstrate close performance on natural image Gaussian
deconvolution and Poisson denoising tasks.
\\ ( https://arxiv.org/abs/2404.05445 ,  22749kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05465 (*cross-listing*)
Date: Mon, 8 Apr 2024 12:43:32 GMT   (3956kb,D)

Title: HAMMR: HierArchical MultiModal React agents for generic VQA
Authors: Lluis Castrejon, Thomas Mensink, Howard Zhou, Vittorio Ferrari, Andre
  Araujo, Jasper Uijlings
Categories: cs.CV cs.LG
\\
  Combining Large Language Models (LLMs) with external specialized tools
(LLMs+tools) is a recent paradigm to solve multimodal tasks such as Visual
Question Answering (VQA). While this approach was demonstrated to work well
when optimized and evaluated for each individual benchmark, in practice it is
crucial for the next generation of real-world AI systems to handle a broad
range of multimodal problems. Therefore we pose the VQA problem from a unified
perspective and evaluate a single system on a varied suite of VQA tasks
including counting, spatial reasoning, OCR-based reasoning, visual pointing,
external knowledge, and more. In this setting, we demonstrate that naively
applying the LLM+tools approach using the combined set of all tools leads to
poor results. This motivates us to introduce HAMMR: HierArchical MultiModal
React. We start from a multimodal ReAct-based system and make it hierarchical
by enabling our HAMMR agents to call upon other specialized agents. This
enhances the compositionality of the LLM+tools approach, which we show to be
critical for obtaining high accuracy on generic VQA. Concretely, on our generic
VQA suite, HAMMR outperforms the naive LLM+tools approach by 19.5%.
Additionally, HAMMR achieves state-of-the-art results on this task,
outperforming the generic standalone PaLI-X VQA model by 5.0%.
\\ ( https://arxiv.org/abs/2404.05465 ,  3956kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05468 (*cross-listing*)
Date: Mon, 8 Apr 2024 12:46:39 GMT   (3620kb,D)

Title: Mind-to-Image: Projecting Visual Mental Imagination of the Brain from
  fMRI
Authors: Hugo Caselles-Dupr\'e, Charles Mellerio, Paul H\'erent, Aliz\'ee
  Lopez-Persem, Benoit B\'eranger, Mathieu Soularue, Pierre Fautrel, Gauthier
  Vernier, Matthieu Cord
Categories: cs.CV cs.LG
Comments: Pre-print to be updated
\\
  The reconstruction of images observed by subjects from fMRI data collected
during visual stimuli has made significant strides in the past decade, thanks
to the availability of extensive fMRI datasets and advancements in generative
models for image generation. However, the application of visual reconstruction
has remained limited. Reconstructing visual imagination presents a greater
challenge, with potentially revolutionary applications ranging from aiding
individuals with disabilities to verifying witness accounts in court. The
primary hurdles in this field are the absence of data collection protocols for
visual imagery and the lack of datasets on the subject. Traditionally,
fMRI-to-image relies on data collected from subjects exposed to visual stimuli,
which poses issues for generating visual imagery based on the difference of
brain activity between visual stimulation and visual imagery. For the first
time, we have compiled a substantial dataset (around 6h of scans) on visual
imagery along with a proposed data collection protocol. We then train a
modified version of an fMRI-to-image model and demonstrate the feasibility of
reconstructing images from two modes of imagination: from memory and from pure
imagination. This marks an important step towards creating a technology that
allow direct reconstruction of visual imagery.
\\ ( https://arxiv.org/abs/2404.05468 ,  3620kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05505 (*cross-listing*)
Date: Mon, 8 Apr 2024 13:27:07 GMT   (2112kb,D)

Title: Taming Transformers for Realistic Lidar Point Cloud Generation
Authors: Hamed Haghighi, Amir Samadi, Mehrdad Dianati, Valentina Donzella, Kurt
  Debattista
Categories: cs.CV cs.LG cs.RO
\\
  Diffusion Models (DMs) have achieved State-Of-The-Art (SOTA) results in the
Lidar point cloud generation task, benefiting from their stable training and
iterative refinement during sampling. However, DMs often fail to realistically
model Lidar raydrop noise due to their inherent denoising process. To retain
the strength of iterative sampling while enhancing the generation of raydrop
noise, we introduce LidarGRIT, a generative model that uses auto-regressive
transformers to iteratively sample the range images in the latent space rather
than image space. Furthermore, LidarGRIT utilises VQ-VAE to separately decode
range images and raydrop masks. Our results show that LidarGRIT achieves
superior performance compared to SOTA models on KITTI-360 and KITTI odometry
datasets. Code available at:https://github.com/hamedhaghighi/LidarGRIT.
\\ ( https://arxiv.org/abs/2404.05505 ,  2112kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05519 (*cross-listing*)
Date: Mon, 8 Apr 2024 13:40:01 GMT   (3408kb,D)

Title: Investigating the Effectiveness of Cross-Attention to Unlock Zero-Shot
  Editing of Text-to-Video Diffusion Models
Authors: Saman Motamed and Wouter Van Gansbeke and Luc Van Gool
Categories: cs.CV cs.LG
Comments: Generative Models for Computer Vision Generative Models for Computer
  Vision CVPR 2024 Workshop
\\
  With recent advances in image and video diffusion models for content
creation, a plethora of techniques have been proposed for customizing their
generated content. In particular, manipulating the cross-attention layers of
Text-to-Image (T2I) diffusion models has shown great promise in controlling the
shape and location of objects in the scene. Transferring image-editing
techniques to the video domain, however, is extremely challenging as object
motion and temporal consistency are difficult to capture accurately. In this
work, we take a first look at the role of cross-attention in Text-to-Video
(T2V) diffusion models for zero-shot video editing. While one-shot models have
shown potential in controlling motion and camera movement, we demonstrate
zero-shot control over object shape, position and movement in T2V models. We
show that despite the limitations of current T2V models, cross-attention
guidance can be a promising approach for editing videos.
\\ ( https://arxiv.org/abs/2404.05519 ,  3408kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05538 (*cross-listing*)
Date: Mon, 8 Apr 2024 14:06:52 GMT   (975kb,D)

Title: Cell-Free Multi-User MIMO Equalization via In-Context Learning
Authors: Matteo Zecchin, Kai Zu and Osvaldo Simeone
Categories: cs.IT cs.LG eess.SP math.IT
\\
  Large pre-trained sequence models, such as transformers, excel as few-shot
learners capable of in-context learning (ICL). In ICL, a model is trained to
adapt its operation to a new task based on limited contextual information,
typically in the form of a few training examples for the given task. Previous
work has explored the use of ICL for channel equalization in single-user
multi-input and multiple-output (MIMO) systems. In this work, we demonstrate
that ICL can be also used to tackle the problem of multi-user equalization in
cell-free MIMO systems with limited fronthaul capacity. In this scenario, a
task is defined by channel statistics, signal-to-noise ratio, and modulation
schemes. The context encompasses the users' pilot sequences, the corresponding
quantized received signals, and the current received data signal. Different
prompt design strategies are proposed and evaluated that encompass also
large-scale fading and modulation information. Experiments demonstrate that
ICL-based equalization provides estimates with lower mean squared error as
compared to the linear minimum mean squared error equalizer, especially in the
presence of limited fronthaul capacity and pilot contamination.
\\ ( https://arxiv.org/abs/2404.05538 ,  975kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05678 (*cross-listing*)
Date: Mon, 8 Apr 2024 16:57:44 GMT   (556kb,D)

Title: Flexible Fairness Learning via Inverse Conditional Permutation
Authors: Yuheng Lai, Leying Guan
Categories: stat.ML cs.LG
\\
  Equalized odds, as a popular notion of algorithmic fairness, aims to ensure
that sensitive variables, such as race and gender, do not unfairly influence
the algorithm prediction when conditioning on the true outcome. Despite rapid
advancements, most of the current research focuses on the violation of
equalized odds caused by one sensitive attribute, leaving the challenge of
simultaneously accounting for multiple attributes under-addressed. We address
this gap by introducing a fairness learning approach that integrates
adversarial learning with a novel inverse conditional permutation. This
approach effectively and flexibly handles multiple sensitive attributes,
potentially of mixed data types. The efficacy and flexibility of our method are
demonstrated through both simulation studies and empirical analysis of
real-world datasets.
\\ ( https://arxiv.org/abs/2404.05678 ,  556kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05693 (*cross-listing*)
Date: Mon, 8 Apr 2024 17:18:30 GMT   (5578kb,D)

Title: Evaluating the Efficacy of Cut-and-Paste Data Augmentation in Semantic
  Segmentation for Satellite Imagery
Authors: Ionut M. Motoi, Leonardo Saraceni, Daniele Nardi, Thomas A. Ciarfuglia
Categories: cs.CV cs.LG eess.IV
Comments: Accepted for publication in IEEE 2024 International Geoscience &
  Remote Sensing Symposium (IGARSS 2024)
\\
  Satellite imagery is crucial for tasks like environmental monitoring and
urban planning. Typically, it relies on semantic segmentation or Land Use Land
Cover (LULC) classification to categorize each pixel. Despite the advancements
brought about by Deep Neural Networks (DNNs), their performance in segmentation
tasks is hindered by challenges such as limited availability of labeled data,
class imbalance and the inherent variability and complexity of satellite
images. In order to mitigate those issues, our study explores the effectiveness
of a Cut-and-Paste augmentation technique for semantic segmentation in
satellite images. We adapt this augmentation, which usually requires labeled
instances, to the case of semantic segmentation. By leveraging the connected
components in the semantic segmentation labels, we extract instances that are
then randomly pasted during training. Using the DynamicEarthNet dataset and a
U-Net model for evaluation, we found that this augmentation significantly
enhances the mIoU score on the test set from 37.9 to 44.1. This finding
highlights the potential of the Cut-and-Paste augmentation to improve the
generalization capabilities of semantic segmentation models in satellite
imagery.
\\ ( https://arxiv.org/abs/2404.05693 ,  5578kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2307.08262
replaced with revised version Sun, 7 Apr 2024 15:47:41 GMT   (2651kb,D)

Title: MuLMINet: Multi-Layer Multi-Input Transformer Network with Weighted Loss
Authors: Minwoo Seong, Jeongseok Oh, SeungJun Kim
Categories: cs.AI
Comments: 4 pages, 3 figures
\\ ( https://arxiv.org/abs/2307.08262 ,  2651kb)
------------------------------------------------------------------------------
\\
arXiv:2308.03929
replaced with revised version Mon, 8 Apr 2024 14:47:37 GMT   (387kb,D)

Title: Fact-Checking Generative AI: Ontology-Driven Biological Graphs for
  Disease-Gene Link Verification
Authors: Ahmed Abdeen Hamed and Byung Suk Lee and Alessandro Crimi and
  Magdalena M. Misiak
Categories: cs.AI cs.CL
Comments: Accepted in the 24th International Conference on Computational
  Science (ICCS'24), in April 1st 2024. Will appear in the Springer LNCS
  proceeding as a short paper
ACM-class: I.2
\\ ( https://arxiv.org/abs/2308.03929 ,  387kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12411
replaced with revised version Fri, 5 Apr 2024 21:36:17 GMT   (970kb,D)

Title: A Theory of Intelligences
Authors: Michael E. Hochberg
Categories: cs.AI
Comments: 37 pages, 1 Table, 6 figures
\\ ( https://arxiv.org/abs/2308.12411 ,  970kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00280
replaced with revised version Sun, 7 Apr 2024 16:20:59 GMT   (816kb,D)

Title: Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model
  Collaboration
Authors: Qiushi Sun, Zhangyue Yin, Xiang Li, Zhiyong Wu, Xipeng Qiu, Lingpeng
  Kong
Categories: cs.AI cs.CL
Comments: work in progress, presented in ICLR 2024 Workshop on LLM Agents
\\ ( https://arxiv.org/abs/2310.00280 ,  816kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06174
replaced with revised version Sat, 6 Apr 2024 22:59:54 GMT   (13848kb,D)

Title: Cost-Efficient Prompt Engineering for Unsupervised Entity Resolution
Authors: Navapat Nananukul, Khanin Sisaengsuwanchai, Mayank Kejriwal
Categories: cs.AI cs.SE
\\ ( https://arxiv.org/abs/2310.06174 ,  13848kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11542
replaced with revised version Sun, 7 Apr 2024 12:22:45 GMT   (950kb)

Title: Data-driven project planning: An integrated network learning and
  constraint relaxation approach in favor of scheduling
Authors: Izack Cohen
Categories: cs.AI
DOI: 10.1109/TEM.2024.3382727
\\ ( https://arxiv.org/abs/2311.11542 ,  950kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12229
replaced with revised version Sat, 6 Apr 2024 00:17:01 GMT   (25239kb,D)

Title: NeuroPrompts: An Adaptive Framework to Optimize Prompts for
  Text-to-Image Generation
Authors: Shachar Rosenman, Vasudev Lal, and Phillip Howard
Categories: cs.AI
Comments: Accepted to EACL 2024 System Demonstration Track
\\ ( https://arxiv.org/abs/2311.12229 ,  25239kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13905
replaced with revised version Mon, 8 Apr 2024 13:31:25 GMT   (473kb,D)

Title: A DRL solution to help reduce the cost in waiting time of securing a
  traffic light for cyclists
Authors: Lucas Magnana (AGORA), Herv\'e Rivano (AGORA), Nicolas Chiabaut
Categories: cs.AI
\\ ( https://arxiv.org/abs/2311.13905 ,  473kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09680
replaced with revised version Mon, 8 Apr 2024 12:31:58 GMT   (3005kb)

Title: Tiny Multi-Agent DRL for Twins Migration in UAV Metaverses: A
  Multi-Leader Multi-Follower Stackelberg Game Approach
Authors: Jiawen Kang, Yue Zhong, Minrui Xu, Jiangtian Nie, Jinbo Wen, Hongyang
  Du, Dongdong Ye, Xumin Huang, Dusit Niyato, Shengli Xie
Categories: cs.AI cs.GT
\\ ( https://arxiv.org/abs/2401.09680 ,  3005kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16119
replaced with revised version Mon, 8 Apr 2024 08:19:19 GMT   (6158kb,D)

Title: Triple Disentangled Representation Learning for Multimodal Affective
  Analysis
Authors: Ying Zhou, Xuefeng Liang, Han Chen, Yin Zhao, Xin Chen, Lida Yu
Categories: cs.AI
Comments: 14 pages, 6 figures
\\ ( https://arxiv.org/abs/2401.16119 ,  6158kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05359
replaced with revised version Fri, 5 Apr 2024 23:22:07 GMT   (586kb,D)

Title: Prompting with Divide-and-Conquer Program Makes Large Language Models
  Discerning to Hallucination and Deception
Authors: Yizhou Zhang, Lun Du, Defu Cao, Qiang Fu, Yan Liu
Categories: cs.AI cs.CL cs.LG
Comments: Preprint
\\ ( https://arxiv.org/abs/2402.05359 ,  586kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00284
replaced with revised version Sat, 6 Apr 2024 07:02:46 GMT   (13133kb,D)

Title: A Survey of Route Recommendations: Methods, Applications, and
  Opportunities
Authors: Shiming Zhang, Zhipeng Luo, Li Yang, Fei Teng, Tianrui Li
Categories: cs.AI cs.LG
Comments: 24 pages, 13 figures
\\ ( https://arxiv.org/abs/2403.00284 ,  13133kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03894
replaced with revised version Mon, 8 Apr 2024 14:02:41 GMT   (2441kb,D)

Title: IRCoder: Intermediate Representations Make Language Models Robust
  Multilingual Code Generators
Authors: Indraneil Paul, Goran Glava\v{s}, Iryna Gurevych
Categories: cs.AI cs.CL cs.PL
\\ ( https://arxiv.org/abs/2403.03894 ,  2441kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15933
replaced with revised version Mon, 8 Apr 2024 10:13:37 GMT   (80kb)

Title: Understanding Domain-Size Generalization in Markov Logic Networks
Authors: Florian Chen, Felix Weitk\"amper and Sagar Malhotra
Categories: cs.AI cs.LG
Comments: Under Review. Minor clarifications added in Lemma 1
\\ ( https://arxiv.org/abs/2403.15933 ,  80kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17601
replaced with revised version Mon, 8 Apr 2024 11:32:03 GMT   (0kb,I)

Title: LASIL: Learner-Aware Supervised Imitation Learning For Long-term
  Microscopic Traffic Simulation
Authors: Ke Guo, Zhenwei Miao, Wei Jing, Weiwei Liu, Weizi Li, Dayang Hao, Jia
  Pan
Categories: cs.AI cs.LG
Comments: My company has this rule: if the data for external use is not
  published on official website, it needs to be disclosed through external data
  disclosure. I have not done external data disclosure before submitting it
\\ ( https://arxiv.org/abs/2403.17601 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19826
replaced with revised version Mon, 8 Apr 2024 14:55:53 GMT   (0kb,I)

Title: Segmentation Re-thinking Uncertainty Estimation Metrics for Semantic
  Segmentation
Authors: Qitian Ma and Shyam Nanda Rai and Carlo Masone and Tatiana Tommasi
Categories: cs.AI
Comments: Premature Submission: accidentally submitted before it was ready
\\ ( https://arxiv.org/abs/2403.19826 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02454
replaced with revised version Sun, 7 Apr 2024 19:47:33 GMT   (28kb)

Title: Techniques for Measuring the Inferential Strength of Forgetting Policies
Authors: Patrick Doherty and Andrzej Szalas
Categories: cs.AI cs.LO
\\ ( https://arxiv.org/abs/2404.02454 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2003.11517
replaced with revised version Sat, 6 Apr 2024 05:14:33 GMT   (0kb,I)

Title: From Algebraic Word Problem to Program: A Formalized Approach
Authors: Adam Wiemerslage and Shafiuddin Rehan Ahmed
Categories: cs.CL cs.PL
Comments: No longer useful work
\\ ( https://arxiv.org/abs/2003.11517 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2102.09600
replaced with revised version Sat, 6 Apr 2024 05:14:07 GMT   (0kb,I)

Title: Within-Document Event Coreference with BERT-Based Contextualized
  Representations
Authors: Shafiuddin Rehan Ahmed and James H. Martin
Categories: cs.CL cs.AI cs.IR
Comments: No longer useful work
\\ ( https://arxiv.org/abs/2102.09600 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2205.15231
replaced with revised version Mon, 8 Apr 2024 11:09:35 GMT   (6391kb,D)

Title: A Survey in Mathematical Language Processing
Authors: Jordan Meadows and Andre Freitas
Categories: cs.CL
Comments: TACL 2023 (Introduction to Mathematical Language Processing...)
\\ ( https://arxiv.org/abs/2205.15231 ,  6391kb)
------------------------------------------------------------------------------
\\
arXiv:2211.00255
replaced with revised version Mon, 8 Apr 2024 02:31:20 GMT   (472kb,D)

Title: CARE: Causality Reasoning for Empathetic Responses by Conditional Graph
  Generation
Authors: Jiashuo Wang, Yi Cheng, Wenjie Li
Categories: cs.CL
Comments: EMNLP 2022 findings
\\ ( https://arxiv.org/abs/2211.00255 ,  472kb)
------------------------------------------------------------------------------
\\
arXiv:2301.10527
replaced with revised version Fri, 5 Apr 2024 18:52:55 GMT   (291kb,D)

Title: Cross-lingual Argument Mining in the Medical Domain
Authors: Anar Yeginbergen and Rodrigo Agerri
Categories: cs.CL
\\ ( https://arxiv.org/abs/2301.10527 ,  291kb)
------------------------------------------------------------------------------
\\
arXiv:2304.06875
replaced with revised version Sat, 6 Apr 2024 05:50:39 GMT   (928kb,D)

Title: nanoLM: an Affordable LLM Pre-training Benchmark via Accurate Loss
  Prediction across Scales
Authors: Yiqun Yao, Siqi fan, Xiusheng Huang, Xuezhi Fang, Xiang Li, Ziyi Ni,
  Xin Jiang, Xuying Meng, Peng Han, Shuo Shang, Kang Liu, Aixin Sun, Yequan
  Wang
Categories: cs.CL cs.LG
Comments: This is a modified and extended version of our previous Mu-scaling
  work released in April 2023 (see v1)
\\ ( https://arxiv.org/abs/2304.06875 ,  928kb)
------------------------------------------------------------------------------
\\
arXiv:2304.07633
replaced with revised version Fri, 5 Apr 2024 23:39:52 GMT   (2295kb,D)

Title: Interpretable Detection of Out-of-Context Misinformation with
  Neural-Symbolic-Enhanced Large Multimodal Model
Authors: Yizhou Zhang, Loc Trinh, Defu Cao, Zijun Cui, Yan Liu
Categories: cs.CL cs.LG
Comments: 9 Pages, 3 Figures
\\ ( https://arxiv.org/abs/2304.07633 ,  2295kb)
------------------------------------------------------------------------------
\\
arXiv:2304.13567
replaced with revised version Sun, 7 Apr 2024 01:22:16 GMT   (302kb,D)

Title: Technical Report: Impact of Position Bias on Language Models in Token
  Classification
Authors: Mehdi Ben Amor, Michael Granitzer, Jelena Mitrovi\'c
Categories: cs.CL cs.AI
Comments: Updated content of the preprint
DOI: 10.1145/3605098.3636126
\\ ( https://arxiv.org/abs/2304.13567 ,  302kb)
------------------------------------------------------------------------------
\\
arXiv:2305.02869
replaced with revised version Sat, 6 Apr 2024 06:18:26 GMT   (555kb,D)

Title: Masked Structural Growth for 2x Faster Language Model Pre-training
Authors: Yiqun Yao, Zheng Zhang, Jing Li, and Yequan Wang
Categories: cs.CL
Comments: ICLR 2024 camera ready
\\ ( https://arxiv.org/abs/2305.02869 ,  555kb)
------------------------------------------------------------------------------
\\
arXiv:2305.04978
replaced with revised version Sat, 6 Apr 2024 00:15:25 GMT   (5387kb,D)

Title: NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge
Authors: Phillip Howard, Junlin Wang, Vasudev Lal, Gadi Singer, Yejin Choi,
  Swabha Swayamdipta
Categories: cs.CL
Comments: Accepted to NAACL 2024 Findings
\\ ( https://arxiv.org/abs/2305.04978 ,  5387kb)
------------------------------------------------------------------------------
\\
arXiv:2305.09620
replaced with revised version Sun, 7 Apr 2024 02:10:04 GMT   (6266kb,D)

Title: AI-Augmented Surveys: Leveraging Large Language Models and Surveys for
  Opinion Prediction
Authors: Junsol Kim, Byungkyu Lee
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2305.09620 ,  6266kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12563
replaced with revised version Mon, 8 Apr 2024 14:29:06 GMT   (817kb,D)

Title: A Symbolic Framework for Evaluating Mathematical Reasoning and
  Generalisation with Transformers
Authors: Jordan Meadows, Marco Valentino, Damien Teney, Andre Freitas
Categories: cs.CL cs.LG
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2305.12563 ,  817kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14970
replaced with revised version Mon, 8 Apr 2024 14:59:53 GMT   (593kb,D)

Title: Getting Sick After Seeing a Doctor? Diagnosing and Mitigating Knowledge
  Conflicts in Event Temporal Reasoning
Authors: Tianqing Fang, Zhaowei Wang, Wenxuan Zhou, Hongming Zhang, Yangqiu
  Song, Muhao Chen
Categories: cs.CL cs.AI
Comments: Findings of NAACL 2024
\\ ( https://arxiv.org/abs/2305.14970 ,  593kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15047
replaced with revised version Fri, 5 Apr 2024 23:57:35 GMT   (688kb,D)

Title: Ghostbuster: Detecting Text Ghostwritten by Large Language Models
Authors: Vivek Verma, Eve Fleisig, Nicholas Tomlin, Dan Klein
Categories: cs.CL cs.AI
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2305.15047 ,  688kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16635
replaced with revised version Fri, 5 Apr 2024 20:10:51 GMT   (8437kb,D)

Title: Impossible Distillation: from Low-Quality Model to High-Quality Dataset
  & Model for Summarization and Paraphrasing
Authors: Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu,
  Jillian Fisher, Taylor Sorensen, Yejin Choi
Categories: cs.CL cs.AI cs.LG
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2305.16635 ,  8437kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12886
replaced with revised version Sun, 7 Apr 2024 16:01:11 GMT   (1751kb,D)

Title: Unveiling Global Narratives: A Multilingual Twitter Dataset of News
  Media on the Russo-Ukrainian Conflict
Authors: Sherzod Hakimov and Gullal S. Cheema
Categories: cs.CL cs.DL
Comments: ICMR 2024
Journal-ref: ICMR 2024 - ACM International Conference on Multimedia Retrieval
  2024
\\ ( https://arxiv.org/abs/2306.12886 ,  1751kb)
------------------------------------------------------------------------------
\\
arXiv:2307.04642
replaced with revised version Fri, 5 Apr 2024 20:08:57 GMT   (8945kb,D)

Title: TRAQ: Trustworthy Retrieval Augmented Question Answering via Conformal
  Prediction
Authors: Shuo Li, Sangdon Park, Insup Lee, Osbert Bastani
Categories: cs.CL cs.AI
Comments: 23 pages, 17 figures, 2024 Annual Conference of the North American
  Chapter of the Association for Computational Linguistics
\\ ( https://arxiv.org/abs/2307.04642 ,  8945kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12564
replaced with revised version Mon, 8 Apr 2024 04:42:01 GMT   (4112kb,D)

Title: Towards Generalising Neural Topical Representations
Authors: Xiaohao Yang, He Zhao, Dinh Phung and Lan Du
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2307.12564 ,  4112kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05361
replaced with revised version Sun, 7 Apr 2024 03:26:38 GMT   (22129kb,D)

Title: WeaverBird: Empowering Financial Decision-Making with Large Language
  Model, Knowledge Base, and Search Engine
Authors: Siqiao Xue, Fan Zhou, Yi Xu, Ming Jin, Qingsong Wen, Hongyan Hao,
  Qingyang Dai, Caigao Jiang, Hongyu Zhao, Shuo Xie, Jianshan He, James Zhang,
  Hongyuan Mei
Categories: cs.CL
Comments: revise abstract
\\ ( https://arxiv.org/abs/2308.05361 ,  22129kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07282
replaced with revised version Mon, 8 Apr 2024 13:01:17 GMT   (49kb)

Title: Comparison between parameter-efficient techniques and full fine-tuning:
  A case study on multilingual news article classification
Authors: Olesya Razuvayevskaya, Ben Wu, Joao A. Leite, Freddy Heppell, Ivan
  Srba, Carolina Scarton, Kalina Bontcheva, Xingyi Song
Categories: cs.CL
Journal-ref: PLOS ONE 2024
DOI: 10.1371/journal.pone.0301738
\\ ( https://arxiv.org/abs/2308.07282 ,  49kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12032
replaced with revised version Sat, 6 Apr 2024 03:52:04 GMT   (11610kb,D)

Title: From Quantity to Quality: Boosting LLM Performance with Self-Guided Data
  Selection for Instruction Tuning
Authors: Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng,
  Jianzong Wang, Tianyi Zhou, Jing Xiao
Categories: cs.CL
Comments: NAACL, Camera-ready
\\ ( https://arxiv.org/abs/2308.12032 ,  11610kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10182
replaced with revised version Mon, 8 Apr 2024 07:31:28 GMT   (1295kb,D)

Title: Positive and Risky Message Assessment for Music Products
Authors: Yigeng Zhang, Mahsa Shafaei, Fabio A. Gonz\'alez, Thamar Solorio
Categories: cs.CL cs.AI
Comments: Accepted at LREC-COLING 2024 (long paper)
\\ ( https://arxiv.org/abs/2309.10182 ,  1295kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12960
replaced with revised version Sun, 7 Apr 2024 14:52:55 GMT   (1166kb,D)

Title: Nested Event Extraction upon Pivot Element Recogniton
Authors: Weicheng Ren, Zixuan Li, Xiaolong Jin, Long Bai, Miao Su, Yantao Liu,
  Saiping Guan, Jiafeng Guo, Xueqi Cheng
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2309.12960 ,  1166kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17453
replaced with revised version Sun, 7 Apr 2024 00:56:53 GMT   (13231kb,D)

Title: Efficient Streaming Language Models with Attention Sinks
Authors: Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis
Categories: cs.CL cs.AI
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2309.17453 ,  13231kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00322
replaced with revised version Sat, 6 Apr 2024 16:48:20 GMT   (2519kb,D)

Title: Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language
  Models
Authors: Chengdong Ma, Ziran Yang, Minquan Gao, Hai Ci, Jun Gao, Xuehai Pan,
  Yaodong Yang
Categories: cs.CL cs.GT
\\ ( https://arxiv.org/abs/2310.00322 ,  2519kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02168
replaced with revised version Sat, 6 Apr 2024 14:32:40 GMT   (9688kb,D)

Title: Editing Personality for Large Language Models
Authors: Shengyu Mao, Xiaohan Wang, Mengru Wang, Yong Jiang, Pengjun Xie, Fei
  Huang, Ningyu Zhang
Categories: cs.CL cs.AI cs.CY cs.LG cs.MA
Comments: Work in progress
\\ ( https://arxiv.org/abs/2310.02168 ,  9688kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05046
replaced with revised version Sat, 6 Apr 2024 08:10:43 GMT   (1254kb,D)

Title: FakeGPT: Fake News Generation, Explanation and Detection of Large
  Language Models
Authors: Yue Huang and Lichao Sun
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.05046 ,  1254kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06770
replaced with revised version Fri, 5 Apr 2024 18:16:29 GMT   (2258kb,D)

Title: SWE-bench: Can Language Models Resolve Real-World GitHub Issues?
Authors: Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei,
  Ofir Press, Karthik Narasimhan
Categories: cs.CL cs.AI cs.SE
Comments: Data, code, and leaderboard are available at https://www.swebench.com
  ICLR 2024, https://openreview.net/forum?id=VTF8yNQM66
\\ ( https://arxiv.org/abs/2310.06770 ,  2258kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08041
replaced with revised version Sat, 6 Apr 2024 10:22:57 GMT   (1659kb,D)

Title: QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large
  Language Models
Authors: Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, Bohan
  Zhuang
Categories: cs.CL cs.AI cs.LG
Comments: ICLR 2024 camera ready; Code is available at
  https://github.com/ziplab/QLLM and https://github.com/ModelTC/QLLM
\\ ( https://arxiv.org/abs/2310.08041 ,  1659kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10118
replaced with revised version Mon, 8 Apr 2024 13:53:48 GMT   (600kb,D)

Title: Learning to Rank Context for Named Entity Recognition Using a Synthetic
  Dataset
Authors: Arthur Amalvy (LIA), Vincent Labatut (LIA), Richard Dufour (LS2N -
  \'equipe TALN)
Categories: cs.CL
Journal-ref: Conference on Empirical Methods in Natural Language Processing
  (EMNLP), ACL, Dec 2023, Singapore, Singapore. pp.10372-10382
\\ ( https://arxiv.org/abs/2310.10118 ,  600kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10648
replaced with revised version Sat, 6 Apr 2024 16:15:27 GMT   (1797kb,D)

Title: Bridging the Novice-Expert Gap via Models of Decision-Making: A Case
  Study on Remediating Math Mistakes
Authors: Rose E. Wang, Qingyang Zhang, Carly Robinson, Susanna Loeb, Dorottya
  Demszky
Categories: cs.CL cs.AI
Comments: NAACL 2024. Code: https://github.com/rosewang2008/bridge
\\ ( https://arxiv.org/abs/2310.10648 ,  1797kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12942
replaced with revised version Mon, 8 Apr 2024 13:30:05 GMT   (7903kb,D)

Title: On the Representational Capacity of Recurrent Neural Language Models
Authors: Franz Nowak, Anej Svete, Li Du, Ryan Cotterell
Categories: cs.CL cs.LG
Comments: Added requirement for non-negative probabilities to definitions 2.3
  and 3.1
Journal-ref: Proceedings of the 2023 Conference on Empirical Methods in Natural
  Language Processing, pages 7011-7034
DOI: 10.18653/v1/2023.emnlp-main.434
\\ ( https://arxiv.org/abs/2310.12942 ,  7903kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15147
replaced with revised version Sat, 6 Apr 2024 15:20:18 GMT   (1592kb,D)

Title: S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large
  Language Models
Authors: Fangyu Lei, Qian Liu, Yiming Huang, Shizhu He, Jun Zhao, Kang Liu
Categories: cs.CL
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2310.15147 ,  1592kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01605
replaced with revised version Mon, 8 Apr 2024 15:35:22 GMT   (49kb,D)

Title: Faithful and Robust Local Interpretability for Textual Predictions
Authors: Gianluigi Lopardo, Frederic Precioso, Damien Garreau
Categories: cs.CL cs.LG stat.ML
\\ ( https://arxiv.org/abs/2311.01605 ,  49kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03839
replaced with revised version Mon, 8 Apr 2024 13:47:49 GMT   (537kb,D)

Title: Aspects of human memory and Large Language Models
Authors: Romuald A. Janik
Categories: cs.CL cs.AI cs.LG q-bio.NC
Comments: 13+3 pages; v2: abstract expanded and future research directions
  added; v3: minor clarifications added
\\ ( https://arxiv.org/abs/2311.03839 ,  537kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05553
replaced with revised version Fri, 5 Apr 2024 23:30:56 GMT   (374kb,D)

Title: Removing RLHF Protections in GPT-4 via Fine-Tuning
Authors: Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori
  Hashimoto, Daniel Kang
Categories: cs.CL cs.AI
Comments: Accepted to NAACL 2024. (7 pages)
\\ ( https://arxiv.org/abs/2311.05553 ,  374kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06694
replaced with revised version Sat, 6 Apr 2024 22:14:25 GMT   (1350kb,D)

Title: Which One? Leveraging Context Between Objects and Multiple Views for
  Language Grounding
Authors: Chancharik Mitra, Abrar Anwar, Rodolfo Corona, Dan Klein, Trevor
  Darrell, Jesse Thomason
Categories: cs.CL cs.AI cs.CV cs.RO
\\ ( https://arxiv.org/abs/2311.06694 ,  1350kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07092
replaced with revised version Mon, 8 Apr 2024 05:54:40 GMT   (8227kb,D)

Title: To Tell The Truth: Language of Deception and Language Models
Authors: Sanchaita Hazra, Bodhisattwa Prasad Majumder
Categories: cs.CL cs.AI
Comments: Accepted as a full paper in NAACL 2024 (Main)
\\ ( https://arxiv.org/abs/2311.07092 ,  8227kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07853
replaced with revised version Mon, 8 Apr 2024 15:23:39 GMT   (1964kb,D)

Title: Learning Mutually Informed Representations for Characters and Subwords
Authors: Yilin Wang, Xinyi Hu, Matthew R. Gormley
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2311.07853 ,  1964kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08268
replaced with revised version Sun, 7 Apr 2024 03:04:10 GMT   (9519kb,D)

Title: A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can
  Fool Large Language Models Easily
Authors: Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen,
  Shujian Huang
Categories: cs.CL
Comments: Acccepted by NAACL 2024, 18 pages, 7 figures, 13 tables
\\ ( https://arxiv.org/abs/2311.08268 ,  9519kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09214
replaced with revised version Sun, 7 Apr 2024 19:17:53 GMT   (8364kb,D)

Title: Mind's Mirror: Distilling Self-Evaluation Capability and Comprehensive
  Thinking from Large Language Models
Authors: Weize Liu, Guocong Li, Kai Zhang, Bang Du, Qiyuan Chen, Xuming Hu,
  Hongxia Xu, Jintai Chen, Jian Wu
Categories: cs.CL
Comments: Accepted to NAACL 2024 Main Conference
\\ ( https://arxiv.org/abs/2311.09214 ,  8364kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09702
replaced with revised version Fri, 5 Apr 2024 18:08:51 GMT   (7999kb,D)

Title: Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go
  without Hallucination?
Authors: Bangzheng Li, Ben Zhou, Fei Wang, Xingyu Fu, Dan Roth, Muhao Chen
Categories: cs.CL cs.AI
Comments: Work accepted by NAACL 2024
\\ ( https://arxiv.org/abs/2311.09702 ,  7999kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10771
replaced with revised version Sun, 7 Apr 2024 00:48:10 GMT   (902kb,D)

Title: Automatic Restoration of Diacritics for Speech Data Sets
Authors: Sara Shatnawi, Sawsan Alqahtani, Hanan Aldarmaki
Categories: cs.CL cs.SD
\\ ( https://arxiv.org/abs/2311.10771 ,  902kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03699
replaced with revised version Mon, 8 Apr 2024 13:32:31 GMT   (388kb,D)

Title: PROMISE: A Framework for Developing Complex Conversational Interactions
  (Technical Report)
Authors: Wenyuan Wu, Jasmin Heierli, Max Meisterhans, Adrian Moser, Andri
  F\"arber, Mateusz Dolata, Elena Gavagnin, Alexandre de Spindler, and Gerhard
  Schwabe
Categories: cs.CL
Comments: Title and abstract changed
\\ ( https://arxiv.org/abs/2312.03699 ,  388kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03719
replaced with revised version Sat, 6 Apr 2024 18:37:05 GMT   (449kb)

Title: Comparative Analysis of ChatGPT, GPT-4, and Microsoft Bing Chatbots for
  GRE Test
Authors: Mohammad Abu-Haifa, Bara'a Etawi, Huthaifa Alkhatatbeh, and Ayman
  Ababneh
Categories: cs.CL cs.AI
Comments: 17 Pages, 6 figures, and 6 tables
\\ ( https://arxiv.org/abs/2312.03719 ,  449kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09203
replaced with revised version Mon, 8 Apr 2024 00:33:54 GMT   (3952kb,D)

Title: Measurement in the Age of LLMs: An Application to Ideological Scaling
Authors: Sean O'Hagan and Aaron Schein
Categories: cs.CL
Comments: Under review a Harvard Data Science Review. Previously presented at
  the 4th International Conference of Social Computing in Beijing, China,
  September 2023, the New Directions in Analyzing Text as Data (TADA) meeting
  in Amherst, MA, USA, November 2023, and the NeurIPS workshop titled "I Can't
  Believe It's Not Better!'' Failure Modes in the Age of Foundation Models in
  New Orleans, LA, December 2023
\\ ( https://arxiv.org/abs/2312.09203 ,  3952kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11779
replaced with revised version Sat, 6 Apr 2024 09:32:53 GMT   (7517kb,D)

Title: Tokenization Matters: Navigating Data-Scarce Tokenization for Gender
  Inclusive Language Technologies
Authors: Anaelia Ovalle, Ninareh Mehrabi, Palash Goyal, Jwala Dhamala, Kai-Wei
  Chang, Richard Zemel, Aram Galstyan, Yuval Pinter, Rahul Gupta
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to NAACL 2024 findings
\\ ( https://arxiv.org/abs/2312.11779 ,  7517kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11882
replaced with revised version Sun, 7 Apr 2024 17:16:42 GMT   (1278kb,D)

Title: ConsistentEE: A Consistent and Hardness-Guided Early Exiting Method for
  Accelerating Language Models Inference
Authors: Ziqian Zeng, Yihuai Hong, Hongliang Dai, Huiping Zhuang, Cen Chen
Categories: cs.CL cs.AI cs.LG
Comments: Accepted in AAAI24
\\ ( https://arxiv.org/abs/2312.11882 ,  1278kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13871
replaced with revised version Mon, 8 Apr 2024 07:36:48 GMT   (413kb,D)

Title: Evaluating Task-oriented Dialogue Systems: A Systematic Review of
  Measures, Constructs and their Operationalisations
Authors: Anouck Braggaar, Christine Liebrecht, Emiel van Miltenburg, Emiel
  Krahmer
Categories: cs.CL cs.HC
Comments: Added section 3.3 and updated other parts to refer to this section.
  Also updated Prisma figure to clarify counts
\\ ( https://arxiv.org/abs/2312.13871 ,  413kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15656
replaced with revised version Mon, 8 Apr 2024 03:50:39 GMT   (748kb,D)

Title: LLsM: Generative Linguistic Steganography with Large Language Model
Authors: Yihao Wang and Ruiqi Song and Ru Zhang and Jianyi Liu and Lingxiao Li
Categories: cs.CL
Comments: 13 pages
\\ ( https://arxiv.org/abs/2401.15656 ,  748kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16184
replaced with revised version Mon, 8 Apr 2024 07:08:48 GMT   (4344kb,D)

Title: On the Semantics of LM Latent Space: A Vocabulary-defined Approach
Authors: Jian Gu, Aldeida Aleti, Chunyang Chen, Hongyu Zhang
Categories: cs.CL cs.LG
Comments: under peer-review
\\ ( https://arxiv.org/abs/2401.16184 ,  4344kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17390
replaced with revised version Mon, 8 Apr 2024 05:22:51 GMT   (650kb,D)

Title: Customizing Language Model Responses with Contrastive In-Context
  Learning
Authors: Xiang Gao, Kamalika Das
Categories: cs.CL cs.AI
Comments: Accepted to appear at AAAI 2024
\\ ( https://arxiv.org/abs/2401.17390 ,  650kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02655
replaced with revised version Sat, 6 Apr 2024 04:29:58 GMT   (9397kb,D)

Title: VlogQA: Task, Dataset, and Baseline Models for Vietnamese Spoken-Based
  Machine Reading Comprehension
Authors: Thinh Phuoc Ngo, Khoa Tran Anh Dang, Son T. Luu, Kiet Van Nguyen, Ngan
  Luu-Thuy Nguyen
Categories: cs.CL
Comments: To appear as the main conference paper at EACL 2024
\\ ( https://arxiv.org/abs/2402.02655 ,  9397kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05125
replaced with revised version Mon, 8 Apr 2024 01:43:43 GMT   (1531kb,D)

Title: Zero-Shot Clinical Trial Patient Matching with LLMs
Authors: Michael Wornow, Alejandro Lozano, Dev Dash, Jenelle Jindal, Kenneth W.
  Mahaffey, Nigam H. Shah
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.05125 ,  1531kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07689
replaced with revised version Sat, 6 Apr 2024 21:41:10 GMT   (815kb,D)

Title: OrderBkd: Textual backdoor attack through repositioning
Authors: Irina Alekseevskaia and Konstantin Arkhipenko
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.07689 ,  815kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14207
replaced with revised version Mon, 8 Apr 2024 05:38:50 GMT   (8683kb,D)

Title: Assisting in Writing Wikipedia-like Articles From Scratch with Large
  Language Models
Authors: Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab,
  Monica S. Lam
Categories: cs.CL cs.AI
Comments: 27 pages, NAACL 2024 Main Conference
\\ ( https://arxiv.org/abs/2402.14207 ,  8683kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14710
replaced with revised version Mon, 8 Apr 2024 06:01:42 GMT   (1475kb,D)

Title: IEPile: Unearthing Large-Scale Schema-Based Information Extraction
  Corpus
Authors: Honghao Gui, Lin Yuan, Hongbin Ye, Ningyu Zhang, Mengshu Sun, Lei
  Liang, Huajun Chen
Categories: cs.CL cs.AI cs.DB cs.IR cs.LG
Comments: Ongoing work; 19 pages; Github: https://github.com/zjunlp/IEPile
\\ ( https://arxiv.org/abs/2402.14710 ,  1475kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15755
replaced with revised version Sat, 6 Apr 2024 08:18:57 GMT   (420kb)

Title: Dental Severity Assessment through Few-shot Learning and SBERT
  Fine-tuning
Authors: Mohammad Dehghani
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.15755 ,  420kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16311
replaced with revised version Mon, 8 Apr 2024 03:22:41 GMT   (3567kb,D)

Title: Cross-domain Chinese Sentence Pattern Parsing
Authors: Jingsi Yu, Cunliang Kong, Liner Yang, Meishan Zhang, Lin Zhu, Yujie
  Wang, Haozhe Lin, Maosong Sun, Erhong Yang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.16311 ,  3567kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03689
replaced with revised version Sat, 6 Apr 2024 04:07:49 GMT   (175kb,D)

Title: General2Specialized LLMs Translation for E-commerce
Authors: Kaidi Chen, Ben Chen, Dehong Gao, Huangyu Dai, Wen Jiang, Wei Ning,
  Shanqing Yu, Libin Yang, Xiaoyan Cai
Categories: cs.CL cs.AI
Comments: 4 pages, 1 figure, WWW2024 accepted
\\ ( https://arxiv.org/abs/2403.03689 ,  175kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03888
replaced with revised version Mon, 8 Apr 2024 14:49:52 GMT   (609kb,D)

Title: FaaF: Facts as a Function for the evaluation of generated text
Authors: Vasileios Katranidis and Gabor Barany
Categories: cs.CL
Comments: 15 pages, 3 figures
\\ ( https://arxiv.org/abs/2403.03888 ,  609kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04787
replaced with revised version Sun, 7 Apr 2024 04:31:30 GMT   (4195kb,D)

Title: Ever-Evolving Memory by Blending and Refining the Past
Authors: Seo Hyun Kim, Keummin Ka, Yohan Jo, Seung-won Hwang, Dongha Lee,
  Jinyoung Yeo
Categories: cs.CL cs.AI
Comments: 17 pages, 4 figures, 7 tables
\\ ( https://arxiv.org/abs/2403.04787 ,  4195kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07865
replaced with revised version Sun, 7 Apr 2024 15:39:24 GMT   (8288kb,D)

Title: Exploring Safety Generalization Challenges of Large Language Models via
  Code
Authors: Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Yu Qiao, Wai
  Lam, Lizhuang Ma
Categories: cs.CL cs.AI cs.CR cs.LG cs.SE
\\ ( https://arxiv.org/abs/2403.07865 ,  8288kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09516
replaced with revised version Fri, 5 Apr 2024 18:35:37 GMT   (9890kb,D)

Title: Leveraging Prototypical Representations for Mitigating Social Bias
  without Demographic Information
Authors: Shadi Iskander, Kira Radinsky, Yonatan Belinkov
Categories: cs.CL cs.CY cs.LG
\\ ( https://arxiv.org/abs/2403.09516 ,  9890kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09722
replaced with revised version Sat, 6 Apr 2024 10:39:02 GMT   (640kb)

Title: Enhancing Readmission Prediction with Deep Learning: Extracting
  Biomedical Concepts from Clinical Texts
Authors: Rasoul Samani, Mohammad Dehghani, Fahime Shahrokh
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.09722 ,  640kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11169
replaced with revised version Sat, 6 Apr 2024 08:49:31 GMT   (32292kb,D)

Title: Correcting misinformation on social media with a large language model
Authors: Xinyi Zhou, Ashish Sharma, Amy X. Zhang, Tim Althoff
Categories: cs.CL cs.AI
Comments: 52 pages
\\ ( https://arxiv.org/abs/2403.11169 ,  32292kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14171
replaced with revised version Mon, 8 Apr 2024 08:30:06 GMT   (2312kb,D)

Title: MMIDR: Teaching Large Language Model to Interpret Multimodal
  Misinformation via Knowledge Distillation
Authors: Longzheng Wang, Xiaohan Xu, Lei Zhang, Jiarui Lu, Yongxiu Xu, Hongbo
  Xu, Minghao Tang, Chuang Zhang
Categories: cs.CL
Comments: 10 pages, 3 figures
\\ ( https://arxiv.org/abs/2403.14171 ,  2312kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14990
replaced with revised version Sat, 6 Apr 2024 00:42:36 GMT   (8501kb,D)

Title: MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic
  Textual Relatedness
Authors: Dhiman Goswami, Sadiya Sayara Chowdhury Puspo, Md Nishat Raihan, Al
  Nahian Bin Emran, Amrita Ganguly, Marcos Zampieri
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.14990 ,  8501kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18349
replaced with revised version Sun, 7 Apr 2024 07:41:28 GMT   (3696kb,D)

Title: Rejection Improves Reliability: Training LLMs to Refuse Unknown
  Questions Using RL from Knowledge Feedback
Authors: Hongshen Xu, Zichen Zhu, Situo Zhang, Da Ma, Shuai Fan, Lu Chen, Kai
  Yu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.18349 ,  3696kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20196
replaced with revised version Sat, 6 Apr 2024 13:54:43 GMT   (8385kb,D)

Title: Automatic Alignment of Discourse Relations of Different Discourse
  Annotation Frameworks
Authors: Yingxue Fu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.20196 ,  8385kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00482
replaced with revised version Sun, 7 Apr 2024 16:56:35 GMT   (1100kb,D)

Title: Cross-lingual Named Entity Corpus for Slavic Languages
Authors: Jakub Piskorski, Micha{\l} Marci\'nczuk, Roman Yangarber
Categories: cs.CL cs.AI cs.LG
Comments: Published in LREC-COLING 2024 - The 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
\\ ( https://arxiv.org/abs/2404.00482 ,  1100kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01129
replaced with revised version Sat, 6 Apr 2024 16:44:37 GMT   (1112kb,D)

Title: Structured Information Matters: Incorporating Abstract Meaning
  Representation into LLMs for Improved Open-Domain Dialogue Evaluation
Authors: Bohao Yang, Kun Zhao, Chen Tang, Liang Zhan, Chenghua Lin
Categories: cs.CL
\\ ( https://arxiv.org/abs/2404.01129 ,  1112kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02491
replaced with revised version Sun, 7 Apr 2024 05:02:39 GMT   (9299kb,D)

Title: Measuring Social Norms of Large Language Models
Authors: Ye Yuan, Kexin Tang, Jianhao Shen, Ming Zhang, Chenguang Wang
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2404.02491 ,  9299kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03304
replaced with revised version Sat, 6 Apr 2024 07:04:35 GMT   (12014kb,D)

Title: Concept -- An Evaluation Protocol on Conversation Recommender Systems
  with System-centric and User-centric Factors
Authors: Chen Huang, Peixin Qin, Yang Deng, Wenqiang Lei, Jiancheng Lv,
  Tat-Seng Chua
Categories: cs.CL cs.AI
Comments: 27 pages, 18 tables, and 10 figures
\\ ( https://arxiv.org/abs/2404.03304 ,  12014kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03471
replaced with revised version Sun, 7 Apr 2024 21:55:38 GMT   (884kb,D)

Title: The Impact of Unstated Norms in Bias Analysis of Language Models
Authors: Farnaz Kohankhaki, Jacob-Junqi Tian, David Emerson, Laleh
  Seyyed-Kalantari, Faiza Khan Khattak
Categories: cs.CL cs.CY cs.LG
\\ ( https://arxiv.org/abs/2404.03471 ,  884kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03592
replaced with revised version Mon, 8 Apr 2024 03:06:10 GMT   (1457kb,D)

Title: ReFT: Representation Finetuning for Language Models
Authors: Zhengxuan Wu and Aryaman Arora and Zheng Wang and Atticus Geiger and
  Dan Jurafsky and Christopher D. Manning and Christopher Potts
Categories: cs.CL cs.AI cs.LG
Comments: 40 pages, preprint
\\ ( https://arxiv.org/abs/2404.03592 ,  1457kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03887
replaced with revised version Mon, 8 Apr 2024 03:05:08 GMT   (736kb,D)

Title: SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical
  Reasoning in Large Language Models
Authors: Hyeonwoo Kim, Gyoungjin Gim, Yungi Kim, Jihoo Kim, Byungju Kim,
  Wonseok Lee, Chanjun Park
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2404.03887 ,  736kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04167
replaced with revised version Mon, 8 Apr 2024 16:33:57 GMT   (2385kb,D)

Title: Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model
Authors: Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang
  Ma, Ruibin Yuan, Xingwei Qu, Jiaheng Liu, Tianyu Zheng, Xinchen Luo, Guorui
  Zhou, Binhang Yuan, Wenhu Chen, Jie Fu, and Ge Zhang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2404.04167 ,  2385kb)
------------------------------------------------------------------------------
\\
arXiv:2112.14368
replaced with revised version Sun, 7 Apr 2024 06:50:01 GMT   (2250kb,D)

Title: Adaptivity and Non-stationarity: Problem-dependent Dynamic Regret for
  Online Convex Optimization
Authors: Peng Zhao, Yu-Jie Zhang, Lijun Zhang, Zhi-Hua Zhou
Categories: cs.LG
Comments: V3 changes: improve the presentation (highlighting the techniques);
  V2 changes: Updated to improve the presentation (emphasizing the importance
  of gradient-variation bounds and the contribution of Sword++ algorithm)
Journal-ref: Journal of Machine Learning Research, 2024
\\ ( https://arxiv.org/abs/2112.14368 ,  2250kb)
------------------------------------------------------------------------------
\\
arXiv:2206.03234
replaced with revised version Fri, 5 Apr 2024 18:00:01 GMT   (670kb,D)

Title: Fairness and Unfairness in Binary and Multiclass Classification:
  Quantifying, Calculating, and Bounding
Authors: Sivan Sabato, Eran Treister, Elad Yom-Tov
Categories: cs.LG cs.CY stat.ML
\\ ( https://arxiv.org/abs/2206.03234 ,  670kb)
------------------------------------------------------------------------------
\\
arXiv:2207.10226
replaced with revised version Sun, 7 Apr 2024 01:52:18 GMT   (1591kb,D)

Title: Improving Privacy-Preserving Vertical Federated Learning by Efficient
  Communication with ADMM
Authors: Chulin Xie, Pin-Yu Chen, Qinbin Li, Arash Nourian, Ce Zhang, Bo Li
Categories: cs.LG cs.CR
Comments: IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)
  2024
\\ ( https://arxiv.org/abs/2207.10226 ,  1591kb)
------------------------------------------------------------------------------
\\
arXiv:2210.00226
replaced with revised version Sun, 7 Apr 2024 17:57:58 GMT   (863kb,D)

Title: Towards Understanding and Mitigating Dimensional Collapse in
  Heterogeneous Federated Learning
Authors: Yujun Shi, Jian Liang, Wenqing Zhang, Vincent Y. F. Tan, Song Bai
Categories: cs.LG
Comments: camera ready version of ICLR 2023
\\ ( https://arxiv.org/abs/2210.00226 ,  863kb)
------------------------------------------------------------------------------
\\
arXiv:2212.07591
replaced with revised version Fri, 5 Apr 2024 18:43:10 GMT   (1251kb,D)

Title: Dissecting Distribution Inference
Authors: Anshuman Suri, Yifu Lu, Yanjin Chen, David Evans
Categories: cs.LG cs.AI cs.CR
Comments: Accepted at SaTML 2023 (updated Yifu's email address)
\\ ( https://arxiv.org/abs/2212.07591 ,  1251kb)
------------------------------------------------------------------------------
\\
arXiv:2301.11826
replaced with revised version Mon, 8 Apr 2024 14:57:42 GMT   (1179kb,D)

Title: Deep Clustering Survival Machines with Interpretable Expert
  Distributions
Authors: Bojian Hou, Hongming Li, Zhicheng Jiao, Zhen Zhou, Hao Zheng, Yong Fan
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2301.11826 ,  1179kb)
------------------------------------------------------------------------------
\\
arXiv:2302.06637
replaced with revised version Sun, 7 Apr 2024 03:00:05 GMT   (320kb,D)

Title: PerAda: Parameter-Efficient Federated Learning Personalization with
  Generalization Guarantees
Authors: Chulin Xie, De-An Huang, Wenda Chu, Daguang Xu, Chaowei Xiao, Bo Li,
  Anima Anandkumar
Categories: cs.LG cs.AI
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2302.06637 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2302.09440
replaced with revised version Mon, 8 Apr 2024 04:32:25 GMT   (21968kb,D)

Title: Online Continuous Hyperparameter Optimization for Generalized Linear
  Contextual Bandits
Authors: Yue Kang, Cho-Jui Hsieh, Thomas C. M. Lee
Categories: cs.LG stat.ML
Comments: Published in Transactions on Machine Learning Research (TMLR)
\\ ( https://arxiv.org/abs/2302.09440 ,  21968kb)
------------------------------------------------------------------------------
\\
arXiv:2302.11628
replaced with revised version Sat, 6 Apr 2024 22:35:20 GMT   (355kb,D)

Title: Provable Robustness Against a Union of $\ell_0$ Adversarial Attacks
Authors: Zayd Hammoudeh and Daniel Lowd
Categories: cs.LG
Comments: Accepted at AAAI 2024 -- Extended version including the supplementary
  material
DOI: 10.1609/aaai.v38i19.30106
\\ ( https://arxiv.org/abs/2302.11628 ,  355kb)
------------------------------------------------------------------------------
\\
arXiv:2303.07540
replaced with revised version Sat, 6 Apr 2024 19:56:42 GMT   (2139kb,D)

Title: Tensor-based Multimodal Learning for Prediction of Pulmonary Arterial
  Wedge Pressure from Cardiac MRI
Authors: Prasun C. Tripathi, Mohammod N. I. Suvon, Lawrence Schobs, Shuo Zhou,
  Samer Alabed, Andrew J. Swift, Haiping Lu
Categories: cs.LG cs.CV q-bio.QM
DOI: 10.1007/978-3-031-43990-2_20
\\ ( https://arxiv.org/abs/2303.07540 ,  2139kb)
------------------------------------------------------------------------------
\\
arXiv:2305.03935
replaced with revised version Sat, 6 Apr 2024 07:01:53 GMT   (4196kb,D)

Title: Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs
Authors: Kaiwen Zheng, Cheng Lu, Jianfei Chen, Jun Zhu
Categories: cs.LG
Comments: Accepted in ICML2023
\\ ( https://arxiv.org/abs/2305.03935 ,  4196kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12511
replaced with revised version Sat, 6 Apr 2024 18:44:44 GMT   (5835kb,D)

Title: PCF-GAN: generating sequential data via the characteristic function of
  measures on the path space
Authors: Hang Lou, Siran Li, Hao Ni
Categories: cs.LG
Journal-ref: Advances in Neural Information Processing Systems 36 (2024)
\\ ( https://arxiv.org/abs/2305.12511 ,  5835kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17544
replaced with revised version Sun, 7 Apr 2024 18:45:20 GMT   (49kb)

Title: Faster Margin Maximization Rates for Generic and Adversarially Robust
  Optimization Methods
Authors: Guanghui Wang, Zihao Hu, Claudio Gentile, Vidya Muthukumar, Jacob
  Abernethy
Categories: cs.LG
Comments: Undated version: New results for implicit bias in adversarial
  training
\\ ( https://arxiv.org/abs/2305.17544 ,  49kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18240
replaced with revised version Sun, 7 Apr 2024 16:07:53 GMT   (660kb,D)

Title: XGrad: Boosting Gradient-Based Optimizers With Weight Prediction
Authors: Lei Guan, Dongsheng Li, Yanqi Shi, Jian Meng
Categories: cs.LG cs.AI
Comments: arXiv admin note: text overlap with arXiv:2302.00195
\\ ( https://arxiv.org/abs/2305.18240 ,  660kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09750
replaced with revised version Mon, 8 Apr 2024 11:38:11 GMT   (1505kb,D)

Title: Fedstellar: A Platform for Decentralized Federated Learning
Authors: Enrique Tom\'as Mart\'inez Beltr\'an, \'Angel Luis Perales G\'omez,
  Chao Feng, Pedro Miguel S\'anchez S\'anchez, Sergio L\'opez Bernal,
  G\'er\^ome Bovet, Manuel Gil P\'erez, Gregorio Mart\'inez P\'erez and Alberto
  Huertas Celdr\'an
Categories: cs.LG cs.AI cs.DC cs.NI
DOI: 10.1016/j.eswa.2023.122861
\\ ( https://arxiv.org/abs/2306.09750 ,  1505kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10125
replaced with revised version Mon, 8 Apr 2024 15:38:59 GMT   (14120kb,D)

Title: Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress,
  and Prospects
Authors: Kexin Zhang, Qingsong Wen, Chaoli Zhang, Rongyao Cai, Ming Jin, Yong
  Liu, James Zhang, Yuxuan Liang, Guansong Pang, Dongjin Song, Shirui Pan
Categories: cs.LG cs.AI eess.SP stat.AP
Comments: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI); 26 pages, 200+ references; the first work to
  comprehensively and systematically summarize self-supervised learning for
  time series analysis (SSL4TS). The GitHub repository is
  https://github.com/qingsongedu/Awesome-SSL4TS
\\ ( https://arxiv.org/abs/2306.10125 ,  14120kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12212
replaced with revised version Mon, 8 Apr 2024 08:00:42 GMT   (1452kb,D)

Title: MimiC: Combating Client Dropouts in Federated Learning by Mimicking
  Central Updates
Authors: Yuchang Sun and Yuyi Mao and Jun Zhang
Categories: cs.LG cs.DC
DOI: 10.1109/TMC.2023.3338021
\\ ( https://arxiv.org/abs/2306.12212 ,  1452kb)
------------------------------------------------------------------------------
\\
arXiv:2306.13263
replaced with revised version Mon, 8 Apr 2024 09:31:33 GMT   (1311kb,D)

Title: Synthetic data shuffling accelerates the convergence of federated
  learning under data heterogeneity
Authors: Bo Li, Yasin Esfandiari, Mikkel N. Schmidt, Tommy S. Alstr{\o}m,
  Sebastian U. Stich
Categories: cs.LG cs.CV cs.DC
Comments: Accepted at TMLR
\\ ( https://arxiv.org/abs/2306.13263 ,  1311kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08873
replaced with revised version Sat, 6 Apr 2024 12:30:26 GMT   (1670kb)

Title: Enhancing Convergence Speed with Feature-Enforcing Physics-Informed
  Neural Networks: Utilizing Boundary Conditions as Prior Knowledge for Faster
  Convergence
Authors: Mahyar Jahaninasab, Mohamad Ali Bijarchi
Categories: cs.LG
Comments: 26 pages, 10 figures, 6 tables
\\ ( https://arxiv.org/abs/2308.08873 ,  1670kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09296
replaced with revised version Sun, 7 Apr 2024 11:30:40 GMT   (1097kb,D)

Title: CARLA: Self-supervised Contrastive Representation Learning for Time
  Series Anomaly Detection
Authors: Zahra Zamanzadeh Darban, Geoffrey I. Webb, Shirui Pan, Charu C.
  Aggarwal, Mahsa Salehi
Categories: cs.LG cs.NE
Comments: 35 pages, 9 figures, 10 tables
\\ ( https://arxiv.org/abs/2308.09296 ,  1097kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02214
replaced with revised version Mon, 8 Apr 2024 07:55:43 GMT   (924kb,D)

Title: Improving equilibrium propagation without weight symmetry through
  Jacobian homeostasis
Authors: Axel Laborieux and Friedemann Zenke
Categories: cs.LG cs.AI cs.NE
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2309.02214 ,  924kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08571
replaced with revised version Sat, 6 Apr 2024 21:05:36 GMT   (759kb,D)

Title: A Bayesian Approach to Robust Inverse Reinforcement Learning
Authors: Ran Wei, Siliang Zeng, Chenliang Li, Alfredo Garcia, Anthony McDonald,
  Mingyi Hong
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.08571 ,  759kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12815
replaced with revised version Sun, 7 Apr 2024 11:18:41 GMT   (2294kb,D)

Title: Improving Generalization in Game Agents with Data Augmentation in
  Imitation Learning
Authors: Derek Yadgaroff, Alessandro Sestini, Konrad Tollmar, Ayca Ozcelikkale,
  Linus Gissl\'en
Categories: cs.LG
Comments: 8 pages, 5 figures
\\ ( https://arxiv.org/abs/2309.12815 ,  2294kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13508
replaced with revised version Sat, 6 Apr 2024 17:07:13 GMT   (4747kb,D)

Title: Guided Cooperation in Hierarchical Reinforcement Learning via
  Model-based Rollout
Authors: Haoran Wang, Zeshen Tang, Leya Yang, Yaoru Sun, Fang Wang, Siyu Zhang,
  Yeming Chen
Categories: cs.LG cs.AI
Comments: Resubmit a revised version, in which we provided more illustrative
  examples, corrected the writing errors, and added references
\\ ( https://arxiv.org/abs/2309.13508 ,  4747kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16032
replaced with revised version Fri, 5 Apr 2024 19:44:37 GMT   (8527kb)

Title: Learning Dissipative Neural Dynamical Systems
Authors: Yuezhu Xu and S. Sivaranjani
Categories: cs.LG cs.SY eess.SY math.DS math.OC
Comments: 6 pages
Journal-ref: IEEE Control Systems Letters 2023
DOI: 10.1109/LCSYS.2023.3337851
\\ ( https://arxiv.org/abs/2309.16032 ,  8527kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03223
replaced with revised version Sun, 7 Apr 2024 17:16:22 GMT   (7717kb,D)

Title: TacoGFN: Target-conditioned GFlowNet for Structure-based Drug Design
Authors: Tony Shen, Seonghwan Seo, Grayson Lee, Mohit Pandey, Jason R Smith,
  Artem Cherkasov, Woo Youn Kim, Martin Ester
Categories: cs.LG
Comments: Accepted at NeurIPS 2023 AID3 and at NeurIPS 2023 GenBio as Spotlight
Journal-ref: NeurIPS 2023 Generative AI and Biology (GenBio) Workshop
\\ ( https://arxiv.org/abs/2310.03223 ,  7717kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06253
replaced with revised version Sat, 6 Apr 2024 20:56:20 GMT   (366kb,D)

Title: A Unified View on Solving Objective Mismatch in Model-Based
  Reinforcement Learning
Authors: Ran Wei, Nathan Lambert, Anthony McDonald, Alfredo Garcia, Roberto
  Calandra
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.06253 ,  366kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07518
replaced with revised version Mon, 8 Apr 2024 10:52:32 GMT   (895kb,D)

Title: Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement
  Learning
Authors: Mirco Mutti, Riccardo De Santi, Marcello Restelli, Alexander Marx,
  Giorgia Ramponi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.07518 ,  895kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17385
replaced with revised version Mon, 8 Apr 2024 09:42:16 GMT   (13566kb,D)

Title: Multitask Online Learning: Listen to the Neighborhood Buzz
Authors: Juliette Achddou, Nicol\`o Cesa-Bianchi, Pierre Laforgue
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.17385 ,  13566kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02328
replaced with revised version Sun, 7 Apr 2024 01:36:29 GMT   (8081kb,D)

Title: An Operator Learning Framework for Spatiotemporal Super-resolution of
  Scientific Simulations
Authors: Valentin Duruisseaux and Amit Chakraborty
Categories: cs.LG
Comments: 31 pages
\\ ( https://arxiv.org/abs/2311.02328 ,  8081kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04818
replaced with revised version Fri, 5 Apr 2024 20:58:34 GMT   (3500kb,D)

Title: Cross-Silo Federated Learning Across Divergent Domains with Iterative
  Parameter Alignment
Authors: Matt Gorbett, Hossein Shirazi, Indrakshi Ray
Categories: cs.LG cs.CV cs.DC
Comments: Published at IEEE Big Data 2023
\\ ( https://arxiv.org/abs/2311.04818 ,  3500kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12624
replaced with revised version Mon, 8 Apr 2024 11:55:36 GMT   (17kb)

Title: Bridging Algorithmic Information Theory and Machine Learning: A New
  Approach to Kernel Learning
Authors: Boumediene Hamzi, Marcus Hutter, Houman Owhadi
Categories: cs.LG cs.IT math.IT stat.ML
Comments: An earlier version of this paper appeared at
  https://www.researchgate.net/publication/371875631_A_note_on_learning_kernels_from_data_from_an_Algorithmic_Information_Theoretic_point_of_view.
  arXiv admin note: text overlap with arXiv:2111.13037, arXiv:2007.05074
DOI: 10.13140/RG.2.2.36344.01285
\\ ( https://arxiv.org/abs/2311.12624 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13163
replaced with revised version Mon, 8 Apr 2024 07:05:14 GMT   (1117kb,D)

Title: Have Your Cake and Eat It Too: Toward Efficient and Accurate Split
  Federated Learning
Authors: Dengke Yan and Ming Hu and Zeke Xia and Yanxin Yang and Jun Xia and
  Xiaofei Xie and Mingsong Chen
Categories: cs.LG cs.DC
\\ ( https://arxiv.org/abs/2311.13163 ,  1117kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01585
replaced with revised version Sun, 7 Apr 2024 15:40:22 GMT   (5710kb,D)

Title: OCGEC: One-class Graph Embedding Classification for DNN Backdoor
  Detection
Authors: Haoyu Jiang, Haiyang Yu, Nan Li, Ping Yi
Categories: cs.LG cs.AI cs.CR
Comments: v2
\\ ( https://arxiv.org/abs/2312.01585 ,  5710kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03259
replaced with revised version Sun, 7 Apr 2024 09:51:04 GMT   (3315kb,D)

Title: f-FERM: A Scalable Framework for Robust Fair Empirical Risk Minimization
Authors: Sina Baharlouei, Shivam Patel, Meisam Razaviyayn
Categories: cs.LG
Comments: 24 Pages,5 figures
Journal-ref: ICLR 2024
\\ ( https://arxiv.org/abs/2312.03259 ,  3315kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04038
replaced with revised version Mon, 8 Apr 2024 06:42:44 GMT   (7946kb,D)

Title: Reconstruction of dynamical systems from data without time labels
Authors: Zhijun Zeng, Pipi Hu, Chenglong Bao, Yi Zhu, Zuoqiang Shi
Categories: cs.LG cs.NA math.DS math.NA
\\ ( https://arxiv.org/abs/2312.04038 ,  7946kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07425
replaced with revised version Mon, 8 Apr 2024 16:56:17 GMT   (5224kb,D)

Title: Deep Internal Learning: Deep Learning from a Single Input
Authors: Tom Tirer, Raja Giryes, Se Young Chun, Yonina C. Eldar
Categories: cs.LG cs.CV eess.IV eess.SP
Comments: Accepted to IEEE Signal Processing Magazine
\\ ( https://arxiv.org/abs/2312.07425 ,  5224kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10401
replaced with revised version Mon, 8 Apr 2024 15:31:31 GMT   (3265kb,D)

Title: Rethinking Dimensional Rationale in Graph Contrastive Learning from
  Causal Perspective
Authors: Qirui Ji, Jiangmeng Li, Jie Hu, Rui Wang, Changwen Zheng, Fanjiang Xu
Categories: cs.LG cs.AI
Comments: Accepted by AAAI2024
\\ ( https://arxiv.org/abs/2312.10401 ,  3265kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15159
replaced with revised version Sun, 7 Apr 2024 06:03:02 GMT   (2498kb,D)

Title: Understanding the Potential of FPGA-Based Spatial Acceleration for Large
  Language Model Inference
Authors: Hongzheng Chen, Jiahao Zhang, Yixiao Du, Shaojie Xiang, Zichao Yue,
  Niansong Zhang, Yaohui Cai, Zhiru Zhang
Categories: cs.LG cs.AI cs.AR cs.CL
Comments: Accepted for publication in the FCCM'24 Journal Track and will appear
  in ACM Transactions on Reconfigurable Technology and Systems (TRETS)
DOI: 10.1145/3656177
\\ ( https://arxiv.org/abs/2312.15159 ,  2498kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17667
replaced with revised version Mon, 8 Apr 2024 12:30:47 GMT   (294kb)

Title: AIJack: Let's Hijack AI! Security and Privacy Risk Simulator for Machine
  Learning
Authors: Hideaki Takahashi
Categories: cs.LG cs.CR
\\ ( https://arxiv.org/abs/2312.17667 ,  294kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03955
replaced with revised version Sat, 6 Apr 2024 17:16:18 GMT   (2250kb,D)

Title: Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced
  Zero/Few-Shot Forecasting of Multivariate Time Series
Authors: Vijay Ekambaram, Arindam Jati, Nam H. Nguyen, Pankaj Dayama, Chandra
  Reddy, Wesley M. Gifford, Jayant Kalagnanam
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.03955 ,  2250kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08478
replaced with revised version Sun, 7 Apr 2024 11:29:37 GMT   (25267kb,D)

Title: Solving Continual Offline Reinforcement Learning with Decision
  Transformer
Authors: Kaixin Huang, Li Shen, Chen Zhao, Chun Yuan, Dacheng Tao
Categories: cs.LG cs.AI
Comments: 11 pages, 6 figures
\\ ( https://arxiv.org/abs/2401.08478 ,  25267kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08961
replaced with revised version Sun, 7 Apr 2024 05:29:24 GMT   (1347kb,D)

Title: Cascading Reinforcement Learning
Authors: Yihan Du, R. Srikant, Wei Chen
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.08961 ,  1347kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11667
replaced with revised version Mon, 8 Apr 2024 03:12:52 GMT   (3461kb,D)

Title: INCPrompt: Task-Aware incremental Prompting for Rehearsal-Free
  Class-incremental Learning
Authors: Zhiyuan Wang, Xiaoyang Qu, Jing Xiao, Bokui Chen, Jianzong Wang
Categories: cs.LG
Comments: Accepted by the 49th IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP 2024)
\\ ( https://arxiv.org/abs/2401.11667 ,  3461kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13034
replaced with revised version Mon, 8 Apr 2024 06:05:04 GMT   (2526kb,D)

Title: Locality Sensitive Sparse Encoding for Learning World Models Online
Authors: Zichen Liu, Chao Du, Wee Sun Lee, Min Lin
Categories: cs.LG cs.AI
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2401.13034 ,  2526kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16025
replaced with revised version Mon, 8 Apr 2024 04:44:43 GMT   (18002kb,D)

Title: Simple Policy Optimization
Authors: Zhengpeng Xie
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.16025 ,  18002kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17548
replaced with revised version Sun, 7 Apr 2024 02:44:18 GMT   (3732kb,D)

Title: Rethinking Channel Dependence for Multivariate Time Series Forecasting:
  Learning from Leading Indicators
Authors: Lifan Zhao, Yanyan Shen
Categories: cs.LG cs.AI
Comments: Accepted to ICLR 2024. Code is at https://github.com/SJTU-Quant/LIFT
Journal-ref: The Twelfth International Conference on Learning Representations,
  2024
\\ ( https://arxiv.org/abs/2401.17548 ,  3732kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01987
replaced with revised version Sun, 7 Apr 2024 22:10:09 GMT   (791kb,D)

Title: Online Transfer Learning for RSV Case Detection
Authors: Yiming Sun, Yuhe Gao, Runxue Bao, Gregory F. Cooper, Jessi Espino,
  Harry Hochheiser, Marian G. Michaels, John M. Aronis, Chenxi Song, Ye Ye
Categories: cs.LG cs.AI
Comments: 10 pages, 2 figures
\\ ( https://arxiv.org/abs/2402.01987 ,  791kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02277
replaced with revised version Sat, 6 Apr 2024 04:34:58 GMT   (1972kb,D)

Title: Causal Bayesian Optimization via Exogenous Distribution Learning
Authors: Shaogang Ren, Xiaoning Qian
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.02277 ,  1972kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03992
replaced with revised version Mon, 8 Apr 2024 04:44:23 GMT   (1190kb,D)

Title: Space Group Constrained Crystal Generation
Authors: Rui Jiao, Wenbing Huang, Yu Liu, Deli Zhao, Yang Liu
Categories: cs.LG cond-mat.mtrl-sci
Comments: ICLR 2024 poster
\\ ( https://arxiv.org/abs/2402.03992 ,  1190kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05713
replaced with revised version Sun, 7 Apr 2024 16:59:41 GMT   (15276kb,D)

Title: Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on
  Vulnerable Patient Populations
Authors: Pranav Kulkarni, Andrew Chan, Nithya Navarathna, Skylar Chan, Paul H.
  Yi, Vishwa S. Parekh
Categories: cs.LG cs.AI cs.CV
Comments: 29 pages, 4 figures
\\ ( https://arxiv.org/abs/2402.05713 ,  15276kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05964
replaced with revised version Sun, 7 Apr 2024 13:03:58 GMT   (459kb,D)

Title: A Survey on Transformer Compression
Authors: Yehui Tang, Yunhe Wang, Jianyuan Guo, Zhijun Tu, Kai Han, Hailin Hu,
  and Dacheng Tao
Categories: cs.LG cs.CL cs.CV
Comments: Model Compression, Transformer, Large Language Model, Large Vision
  Model, LLM
\\ ( https://arxiv.org/abs/2402.05964 ,  459kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06716
replaced with revised version Sat, 6 Apr 2024 12:38:45 GMT   (3343kb,D)

Title: Dynamic Graph Information Bottleneck
Authors: Haonan Yuan, Qingyun Sun, Xingcheng Fu, Cheng Ji, Jianxin Li
Categories: cs.LG cs.AI
Comments: Accepted by the research tracks of The Web Conference 2024 (WWW 2024)
\\ ( https://arxiv.org/abs/2402.06716 ,  3343kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07710
replaced with revised version Sat, 6 Apr 2024 12:49:43 GMT   (1058kb,D)

Title: Optimizing Sparse Convolution on GPUs with CUDA for 3D Point Cloud
  Processing in Embedded Systems
Authors: Chester Luo, Kevin Lai
Categories: cs.LG cs.CV
Comments: 9 pages
\\ ( https://arxiv.org/abs/2402.07710 ,  1058kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09288
replaced with revised version Sun, 7 Apr 2024 16:40:59 GMT   (7679kb,D)

Title: EcoVal: An Efficient Data Valuation Framework for Machine Learning
Authors: Ayush K Tarun, Vikram S Chundawat, Murari Mandal, Hong Ming Tan, Bowei
  Chen, Mohan Kankanhalli
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.09288 ,  7679kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10184
replaced with revised version Mon, 8 Apr 2024 07:50:17 GMT   (747kb,D)

Title: Rethinking Information Structures in RLHF: Reward Generalization from a
  Graph Theory Perspective
Authors: Tianyi Qiu, Fanzhi Zeng, Jiaming Ji, Dong Yan, Kaile Wang, Jiayi Zhou,
  Yang Han, Josef Dai, Xuehai Pan, Yaodong Yang
Categories: cs.LG cs.AI cs.CL cs.DM
\\ ( https://arxiv.org/abs/2402.10184 ,  747kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15102
replaced with revised version Mon, 8 Apr 2024 09:33:10 GMT   (1267kb,D)

Title: Trajectory-wise Iterative Reinforcement Learning Framework for
  Auto-bidding
Authors: Haoming Li, Yusen Huo, Shuai Dou, Zhenzhe Zheng, Zhilin Zhang, Chuan
  Yu, Jian Xu, Fan Wu
Categories: cs.LG cs.AI cs.GT cs.IR
Comments: Accepted by The Web Conference 2024 (WWW'24) as an oral paper
\\ ( https://arxiv.org/abs/2402.15102 ,  1267kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17453
replaced with revised version Sat, 6 Apr 2024 12:28:57 GMT   (370kb,D)

Title: DS-Agent: Automated Data Science by Empowering Large Language Models
  with Case-Based Reasoning
Authors: Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.17453 ,  370kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01232
replaced with revised version Sat, 6 Apr 2024 23:26:26 GMT   (469kb,D)

Title: Polynormer: Polynomial-Expressive Graph Transformer in Linear Time
Authors: Chenhui Deng, Zichao Yue, Zhiru Zhang
Categories: cs.LG cs.AI
Comments: Published as a conference paper at International Conference on
  Learning Representations (ICLR) 2024
\\ ( https://arxiv.org/abs/2403.01232 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01317
replaced with revised version Sat, 6 Apr 2024 23:23:56 GMT   (5420kb,D)

Title: Less is More: Hop-Wise Graph Attention for Scalable and Generalizable
  Learning on Circuits
Authors: Chenhui Deng, Zichao Yue, Cunxi Yu, Gokce Sarar, Ryan Carey, Rajeev
  Jain, Zhiru Zhang
Categories: cs.LG cs.AR
Comments: Published as a conference paper at Design Automation Conference (DAC)
  2024
\\ ( https://arxiv.org/abs/2403.01317 ,  5420kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04810
replaced with revised version Mon, 8 Apr 2024 11:51:31 GMT   (50kb,D)

Title: Restricted Bayesian Neural Network
Authors: Sourav Ganguly and Saprativa Bhattacharjee
Categories: cs.LG cs.AI cs.NE
\\ ( https://arxiv.org/abs/2403.04810 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09054
replaced with revised version Sat, 6 Apr 2024 00:22:37 GMT   (1516kb,D)

Title: Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient
  Generative Inference
Authors: Muhammad Adnan and Akhil Arunkumar and Gaurav Jain and Prashant J.
  Nair and Ilya Soloveychik and Purushotham Kamath
Categories: cs.LG cs.AI cs.AR cs.CL
MSC-class: 68U35
ACM-class: I.2.7; C.0
Journal-ref: Proceedings of the 7th Annual Conference on Machine Learning and
  Systems (MLSys), 2024
\\ ( https://arxiv.org/abs/2403.09054 ,  1516kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14917
replaced with revised version Sun, 7 Apr 2024 09:08:29 GMT   (109kb,D)

Title: Mean-field Analysis on Two-layer Neural Networks from a Kernel
  Perspective
Authors: Shokichi Takakura, Taiji Suzuki
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2403.14917 ,  109kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17886
replaced with revised version Mon, 8 Apr 2024 15:20:27 GMT   (4072kb,D)

Title: Neural Embedding Compression For Efficient Multi-Task Earth Observation
  Modelling
Authors: Carlos Gomes and Thomas Brunschwiler
Categories: cs.LG
Comments: Published at IGARSS 2024
\\ ( https://arxiv.org/abs/2403.17886 ,  4072kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18742
replaced with revised version Mon, 8 Apr 2024 15:51:17 GMT   (2293kb,D)

Title: Understanding the Learning Dynamics of Alignment with Human Feedback
Authors: Shawn Im, Yixuan Li
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.18742 ,  2293kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20150
replaced with revised version Mon, 8 Apr 2024 06:52:34 GMT   (13361kb,D)

Title: TFB: Towards Comprehensive and Fair Benchmarking of Time Series
  Forecasting Methods
Authors: Xiangfei Qiu, Jilin Hu, Lekui Zhou, Xingjian Wu, Junyang Du, Buang
  Zhang, Chenjuan Guo, Aoying Zhou, Christian S. Jensen, Zhenli Sheng and Bin
  Yang
Categories: cs.LG cs.AI cs.CY
Comments: Accepted by PVLDB 2024
\\ ( https://arxiv.org/abs/2403.20150 ,  13361kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20208
replaced with revised version Sun, 7 Apr 2024 02:21:25 GMT   (712kb,D)

Title: Unleashing the Potential of Large Language Models for Predictive Tabular
  Tasks in Data Science
Authors: Yazheng Yang, Yuqi Wang, Sankalok Sen, Lei Li, Qi Liu
Categories: cs.LG cs.AI
Comments: 10 pages
\\ ( https://arxiv.org/abs/2403.20208 ,  712kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00521
replaced with revised version Sun, 7 Apr 2024 15:04:47 GMT   (6509kb,D)

Title: CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz
  continuity constrAIned Normalization
Authors: Yao Ni, Piotr Koniusz
Categories: cs.LG cs.CV
Comments: Accepted by CVPR2024. 26 pages full version. Code:
  https://github.com/MaxwellYaoNi/CHAIN
\\ ( https://arxiv.org/abs/2404.00521 ,  6509kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01981
replaced with revised version Fri, 5 Apr 2024 20:53:20 GMT   (79kb)

Title: Zero-Shot Multi-Lingual Speaker Verification in Clinical Trials
Authors: Ali Akram, Marija Stanojevic, Malikeh Ehghaghi, Jekaterina Novikova
Categories: cs.LG cs.SD eess.AS
\\ ( https://arxiv.org/abs/2404.01981 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02866
replaced with revised version Sat, 6 Apr 2024 21:18:01 GMT   (258kb,D)

Title: Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds
Authors: Kamalika Chaudhuri, Chuan Guo, Laurens van der Maaten, Saeed
  Mahloujifar, and Mark Tygert
Categories: cs.LG cs.CR cs.CY stat.ML
Comments: 18 pages, 6 figures
\\ ( https://arxiv.org/abs/2404.02866 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02892
replaced with revised version Sun, 7 Apr 2024 01:02:08 GMT   (380kb,D)

Title: MODNO: Multi Operator Learning With Distributed Neural Operators
Authors: Zecheng Zhang
Categories: cs.LG cs.NA math.NA
\\ ( https://arxiv.org/abs/2404.02892 ,  380kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02937
replaced with revised version Mon, 8 Apr 2024 15:52:00 GMT   (5136kb,D)

Title: Explainable Traffic Flow Prediction with Large Language Models
Authors: Xusen Guo, Qiming Zhang, Mingxing Peng, Meixin Zhu, and Hao (Frank)
  Yang
Categories: cs.LG cs.AI
Comments: 25pages, 8 figures
\\ ( https://arxiv.org/abs/2404.02937 ,  5136kb)
------------------------------------------------------------------------------
\\
arXiv:2005.07842 (*cross-listing*)
replaced with revised version Sun, 7 Apr 2024 02:55:23 GMT   (2159kb)

Title: DEFM: Delay E mbedding based Forecast Machine for Time Series
  Forecasting by Spatiotemporal Information Transformation
Authors: Hao Peng, Wei Wang, Pei Chen, Rui Liu
Categories: eess.SP cs.AI
Comments: 28 pages, 5 figures
Journal-ref: Chaos 1 April 2024; 34 (4): 043112
DOI: 10.1063/5.0181791
\\ ( https://arxiv.org/abs/2005.07842 ,  2159kb)
------------------------------------------------------------------------------
\\
arXiv:2301.05089
replaced with revised version Sat, 6 Apr 2024 00:50:16 GMT   (2539kb,D)

Title: Approximate Information States for Worst-Case Control and Learning in
  Uncertain Systems
Authors: Aditya Dave, Nishanth Venkatesh, Andreas A. Malikopoulos
Categories: eess.SY cs.AI cs.SY math.OC
Comments: Preliminary results related to this article were reported in
  arXiv:2203.15271
\\ ( https://arxiv.org/abs/2301.05089 ,  2539kb)
------------------------------------------------------------------------------
\\
arXiv:2303.04989
replaced with revised version Sun, 7 Apr 2024 05:50:18 GMT   (5550kb,D)

Title: ARS-DETR: Aspect Ratio-Sensitive Detection Transformer for Aerial
  Oriented Object Detection
Authors: Ying Zeng, Yushi Chen, Xue Yang, Qingyun Li, Junchi Yan
Categories: cs.CV cs.AI
Comments: 15 pages, 13 figures, 13 tables, the source code is available at
  https://github.com/httle/ARS-DETR
Journal-ref: IEEE Transactions on Geoscience and Remote Sensing, vol. 62, pp.
  1-15, 2024
DOI: 10.1109/TGRS.2024.3364713
\\ ( https://arxiv.org/abs/2303.04989 ,  5550kb)
------------------------------------------------------------------------------
\\
arXiv:2304.04067
replaced with revised version Mon, 8 Apr 2024 03:30:56 GMT   (4170kb,D)

Title: Efficiently Tackling Million-Dimensional Multiobjective Problems: A
  Direction Sampling and Fine-Tuning Approach
Authors: Haokai Hong, Min Jiang, Qiuzhen Lin and Kay Chen Tan
Categories: cs.NE cs.AI
Comments: 12 pages, 6 figures
\\ ( https://arxiv.org/abs/2304.04067 ,  4170kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17328
replaced with revised version Sun, 7 Apr 2024 20:20:09 GMT   (13522kb,D)

Title: Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention
  Graph in Pre-Trained Transformers
Authors: Hongjie Wang, Bhishma Dedhia, Niraj K. Jha
Categories: cs.CV cs.AI cs.LG eess.IV
Comments: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
  2024
\\ ( https://arxiv.org/abs/2305.17328 ,  13522kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09862 (*cross-listing*)
replaced with revised version Sun, 7 Apr 2024 02:37:50 GMT   (1303kb,D)

Title: DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock
  Trend Forecasting
Authors: Lifan Zhao, Shuming Kong, Yanyan Shen
Categories: q-fin.ST cs.AI cs.CE cs.LG q-fin.CP
Comments: Accepted by KDD 2023. Code is at https://github.com/SJTU-Quant/qlib
\\ ( https://arxiv.org/abs/2306.09862 ,  1303kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09913
replaced with revised version Mon, 8 Apr 2024 07:19:32 GMT   (657kb,D)

Title: Exploring Non-Regular Extensions of Propositional Dynamic Logic with
  Description-Logics Features
Authors: Bartosz Bednarczyk
Categories: cs.LO cs.AI
Comments: Full version of our JELIA 2023 paper, accepted for publication to
  Logical Methods in Computer Science
\\ ( https://arxiv.org/abs/2307.09913 ,  657kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12574
replaced with revised version Mon, 8 Apr 2024 03:37:00 GMT   (1223kb,D)

Title: Modeling Uncertainty and Using Post-fusion as Fallback Improves
  Retrieval Augmented Generation with LLMs
Authors: Ye Liu, Semih Yavuz, Rui Meng, Meghana Moorthy, Shafiq Joty, Caiming
  Xiong, Yingbo Zhou
Categories: cs.IR cs.AI
\\ ( https://arxiv.org/abs/2308.12574 ,  1223kb)
------------------------------------------------------------------------------
\\
arXiv:2309.03467
replaced with revised version Mon, 8 Apr 2024 07:49:47 GMT   (12714kb,D)

Title: Autoregressive Omni-Aware Outpainting for Open-Vocabulary 360-Degree
  Image Generation
Authors: Zhuqiang Lu, Kun Hu, Chaoyue Wang, Lei Bai, Zhiyong Wang
Categories: cs.CV cs.AI
Comments: Accepted by AAAI 24
ACM-class: I.4.0
\\ ( https://arxiv.org/abs/2309.03467 ,  12714kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05310
replaced with revised version Mon, 8 Apr 2024 15:44:31 GMT   (23959kb,D)

Title: ImitationNet: Unsupervised Human-to-Robot Motion Retargeting via Shared
  Latent Space
Authors: Yashuai Yan and Esteve Valls Mascaro and Dongheui Lee
Categories: cs.RO cs.AI
Comments: Accepted to Humanoids 2023. Website: https://evm7.github.io/UnsH2R/
\\ ( https://arxiv.org/abs/2309.05310 ,  23959kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13933
replaced with revised version Mon, 8 Apr 2024 13:47:39 GMT   (1287kb,D)

Title: Fairness and Bias in Algorithmic Hiring: a Multidisciplinary Survey
Authors: Alessandro Fabris, Nina Baranowska, Matthew J. Dennis, David Graus,
  Philipp Hacker, Jorge Saldivar, Frederik Zuiderveen Borgesius, Asia J. Biega
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2309.13933 ,  1287kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09560
replaced with revised version Sat, 6 Apr 2024 03:17:33 GMT   (30785kb,D)

Title: You Only Train Once: A Unified Framework for Both Full-Reference and
  No-Reference Image Quality Assessment
Authors: Yi Ke Yun, Weisi Lin
Categories: cs.CV cs.AI eess.IV
\\ ( https://arxiv.org/abs/2310.09560 ,  30785kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08077
replaced with revised version Mon, 8 Apr 2024 13:23:47 GMT   (1455kb,D)

Title: Zero-Shot Segmentation of Eye Features Using the Segment Anything Model
  (SAM)
Authors: Virmarie Maquiling, Sean Anthony Byrne, Diederick C. Niehorster,
  Marcus Nystr\"om, Enkelejda Kasneci
Categories: cs.CV cs.AI cs.HC
Comments: 14 pages, 8 figures, 1 table, Accepted to ETRA 2024: ACM Symposium on
  Eye Tracking Research & Applications
DOI: 10.1145/3654704
\\ ( https://arxiv.org/abs/2311.08077 ,  1455kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08393
replaced with revised version Mon, 8 Apr 2024 02:57:55 GMT   (12901kb,D)

Title: MVSA-Net: Multi-View State-Action Recognition for Robust and Deployable
  Trajectory Generation
Authors: Ehsan Asali, Prashant Doshi, Jin Sun
Categories: cs.CV cs.AI cs.LG cs.RO
Comments: Presented at Deployable AI Workshop at AAAI-2024 and 'Towards
  Reliable and Deployable Learning-Based Robotic Systems' Workshop at CoRL2023
\\ ( https://arxiv.org/abs/2311.08393 ,  12901kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16514
replaced with revised version Sun, 7 Apr 2024 13:33:56 GMT   (9817kb,D)

Title: Video Anomaly Detection via Spatio-Temporal Pseudo-Anomaly Generation :
  A Unified Approach
Authors: Ayush K. Rai, Tarun Krishna, Feiyan Hu, Alexandru Drimbarean, Kevin
  McGuinness, Alan F. Smeaton, Noel E. O'Connor
Categories: cs.CV cs.AI cs.LG
Comments: Accepted in CVPRW 2024 - VAND Workshop
\\ ( https://arxiv.org/abs/2311.16514 ,  9817kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17518
replaced with revised version Fri, 5 Apr 2024 19:18:30 GMT   (37900kb,D)

Title: The devil is in the fine-grained details: Evaluating open-vocabulary
  object detectors for fine-grained understanding
Authors: Lorenzo Bianchi, Fabio Carrara, Nicola Messina, Claudio Gennaro and
  Fabrizio Falchi
Categories: cs.CV cs.AI cs.LG
Comments: Accepted as Highlight at CVPR2024
\\ ( https://arxiv.org/abs/2311.17518 ,  37900kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06462
replaced with revised version Sun, 7 Apr 2024 14:05:53 GMT   (7336kb,D)

Title: Cooperation Does Matter: Exploring Multi-Order Bilateral Relations for
  Audio-Visual Segmentation
Authors: Qi Yang and Xing Nie and Tong Li and Pengfei Gao and Ying Guo and
  Cheng Zhen and Pengfei Yan and Shiming Xiang
Categories: cs.CV cs.AI cs.SD eess.AS
Comments: CVPR 2024 Highlight. 13 pages, 10 figures
\\ ( https://arxiv.org/abs/2312.06462 ,  7336kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12462 (*cross-listing*)
replaced with revised version Mon, 8 Apr 2024 15:00:53 GMT   (12149kb,D)

Title: Towards an end-to-end artificial intelligence driven global weather
  forecasting system
Authors: Kun Chen, Lei Bai, Fenghua Ling, Peng Ye, Tao Chen, Jing-Jia Luo, Hao
  Chen, Yi Xiao, Kang Chen, Tao Han, Wanli Ouyang
Categories: physics.ao-ph cs.AI cs.LG
\\ ( https://arxiv.org/abs/2312.12462 ,  12149kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03424
replaced with revised version Mon, 8 Apr 2024 12:50:54 GMT   (244kb,D)

Title: MLCA-AVSR: Multi-Layer Cross Attention Fusion based Audio-Visual Speech
  Recognition
Authors: He Wang, Pengcheng Guo, Pan Zhou, Lei Xie
Categories: cs.SD cs.AI eess.AS
Comments: 5 pages, 3 figures Accepted at ICASSP 2024
DOI: 10.1109/ICASSP48485.2024.10446769
\\ ( https://arxiv.org/abs/2401.03424 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04747
replaced with revised version Sat, 6 Apr 2024 14:53:51 GMT   (6020kb,D)

Title: DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven
  Holistic 3D Expression and Gesture Generation
Authors: Junming Chen, Yunfei Liu, Jianan Wang, Ailing Zeng, Yu Li, Qifeng Chen
Categories: cs.SD cs.AI cs.CV cs.GR eess.AS
Comments: Accepted by CVPR 2024. Project page:
  https://jeremycjm.github.io/proj/DiffSHEG
\\ ( https://arxiv.org/abs/2401.04747 ,  6020kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01134
replaced with revised version Sun, 7 Apr 2024 06:30:39 GMT   (17958kb,D)

Title: DeepAAT: Deep Automated Aerial Triangulation for Fast UAV-based Mapping
Authors: Zequan Chen, Jianping Li, Qusheng Li, Bisheng Yang, Zhen Dong
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2402.01134 ,  17958kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10100
replaced with revised version Fri, 5 Apr 2024 21:40:33 GMT   (500kb,D)

Title: Tuning In: Analysis of Audio Classifier Performance in Clinical Settings
  with Limited Data
Authors: Hamza Mahdi, Eptehal Nashnoush, Rami Saab, Arjun Balachandar, Rishit
  Dagli, Lucas X. Perri, and Houman Khosravani
Categories: cs.SD cs.AI cs.LG eess.AS
Comments: CHIL 2024
\\ ( https://arxiv.org/abs/2402.10100 ,  500kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05297
replaced with revised version Mon, 8 Apr 2024 12:17:24 GMT   (25588kb,D)

Title: PEEB: Part-based Image Classifiers with an Explainable and Editable
  Language Bottleneck
Authors: Thang M. Pham, Peijie Chen, Tin Nguyen, Seunghyun Yoon, Trung Bui, Anh
  Nguyen
Categories: cs.CV cs.AI cs.CL
Comments: Findings of NAACL 2024 (long paper)
\\ ( https://arxiv.org/abs/2403.05297 ,  25588kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09680
replaced with revised version Mon, 8 Apr 2024 17:51:31 GMT   (3619kb,D)

Title: Pre-Sorted Tsetlin Machine (The Genetic K-Medoid Method)
Authors: Jordan Morris
Categories: cs.NE cs.AI cs.LG
Comments: 6 pages, 12 figures, 3 tables
ACM-class: B.6.0; B.7.0; C.1.0; I.2.6
\\ ( https://arxiv.org/abs/2403.09680 ,  3619kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12488
replaced with revised version Sun, 7 Apr 2024 11:38:48 GMT   (2207kb,D)

Title: DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of
  MLLM
Authors: Yixuan Wu, Yizhou Wang, Shixiang Tang, Wenhao Wu, Tong He, Wanli
  Ouyang, Jian Wu, Philip Torr
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.12488 ,  2207kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13041
replaced with revised version Mon, 8 Apr 2024 13:20:54 GMT   (569kb,D)

Title: Provable Privacy with Non-Private Pre-Processing
Authors: Yaxi Hu, Amartya Sanyal, Bernhard Sch\"olkopf
Categories: cs.CR cs.AI cs.LG stat.ML
\\ ( https://arxiv.org/abs/2403.13041 ,  569kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13245
replaced with revised version Sun, 7 Apr 2024 19:25:47 GMT   (6629kb,D)

Title: Federated reinforcement learning for robot motion planning with
  zero-shot generalization
Authors: Zhenyuan Yuan, Siyuan Xu, Minghui Zhu
Categories: eess.SY cs.AI cs.DC cs.LG cs.RO cs.SY
\\ ( https://arxiv.org/abs/2403.13245 ,  6629kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13421
replaced with revised version Fri, 5 Apr 2024 18:23:38 GMT   (277kb,D)

Title: Caching-Augmented Lifelong Multi-Agent Path Finding
Authors: Yimin Tang, Zhenghong Yu, Yi Zheng, T. K. Satish Kumar, Jiaoyang Li,
  Sven Koenig
Categories: cs.RO cs.AI cs.MA
\\ ( https://arxiv.org/abs/2403.13421 ,  277kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13801
replaced with revised version Sat, 6 Apr 2024 04:12:47 GMT   (644kb,D)

Title: Natural Language as Policies: Reasoning for Coordinate-Level Embodied
  Control with LLMs
Authors: Yusuke Mikami, Andrew Melnik, Jun Miura, Ville Hautam\"aki
Categories: cs.RO cs.AI cs.CL
Comments: 8 pages, 2 figures
ACM-class: I.2.9; I.2.7
\\ ( https://arxiv.org/abs/2403.13801 ,  644kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13808
replaced with revised version Fri, 5 Apr 2024 18:22:02 GMT   (2179kb,D)

Title: On Pretraining Data Diversity for Self-Supervised Learning
Authors: Hasan Abed Al Kader Hammoud, Tuhin Das, Fabio Pizzati, Philip Torr,
  Adel Bibi, Bernard Ghanem
Categories: cs.CV cs.AI cs.LG
Comments: Under review
\\ ( https://arxiv.org/abs/2403.13808 ,  2179kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15457
replaced with revised version Sat, 6 Apr 2024 10:45:35 GMT   (23765kb,D)

Title: The Journey to Trustworthy AI- Part 1: Pursuit of Pragmatic Frameworks
Authors: Mohamad M Nasr-Azadani and Jean-Luc Chatelain
Categories: cs.CY cs.AI cs.HC
Comments: Updates: Fixed typos. Fixed NIST checkmarks in table 1. Added new
  subsections: copyright (4.6) and risks on webcrawled datasets (5.2.1).
  Updated figure 3 to show EU-AI Act passing
\\ ( https://arxiv.org/abs/2403.15457 ,  23765kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16687
replaced with revised version Mon, 8 Apr 2024 09:23:43 GMT   (455kb)

Title: Investigation of the effectiveness of applying ChatGPT in Dialogic
  Teaching Using Electroencephalography
Authors: Jiayue Zhang, Yiheng Liu, Wenqi Cai, Yali Peng, Jingjing Yu, Senqing
  Qi, Taotao Long, Bao Ge
Categories: cs.CY cs.AI physics.ed-ph
\\ ( https://arxiv.org/abs/2403.16687 ,  455kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20199
replaced with revised version Sun, 7 Apr 2024 08:40:45 GMT   (725kb)

Title: NeuraLunaDTNet: Feedforward Neural Network-Based Routing Protocol for
  Delay-Tolerant Lunar Communication Networks
Authors: Parth Patel, Milena Radenkovic
Categories: cs.NI cs.AI
\\ ( https://arxiv.org/abs/2403.20199 ,  725kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20261 (*cross-listing*)
replaced with revised version Sun, 7 Apr 2024 07:35:01 GMT   (1465kb,D)

Title: FABind+: Enhancing Molecular Docking through Improved Pocket Prediction
  and Pose Generation
Authors: Kaiyuan Gao, Qizhi Pei, Jinhua Zhu, Kun He, Lijun Wu
Categories: q-bio.BM cs.AI cs.LG
Comments: 17 pages, 14 figures, 5 tables
\\ ( https://arxiv.org/abs/2403.20261 ,  1465kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00076
replaced with revised version Sun, 7 Apr 2024 04:38:37 GMT   (6823kb,D)

Title: A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping
  Attacks
Authors: Orson Mengara
Categories: cs.CR cs.AI cs.CL cs.LG eess.SP
Comments: Accept by "IEEE Access" let's take a look at our global approach to
  the DNN(s) model(s) deployment chain in production: Danger NLP-Speech
  (Trigger universal approach)
DOI: 10.1109/ACCESS.2024.3382839
\\ ( https://arxiv.org/abs/2404.00076 ,  6823kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00722
replaced with revised version Mon, 8 Apr 2024 15:15:56 GMT   (9389kb,D)

Title: DRCT: Saving Image Super-resolution away from Information Bottleneck
Authors: Chih-Chung Hsu, Chia-Ming Lee, and Yi-Shiuan Chou
Categories: cs.CV cs.AI
Comments: NTIRE 2024 Image Super-resolution (x4)
\\ ( https://arxiv.org/abs/2404.00722 ,  9389kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00989
replaced with revised version Mon, 8 Apr 2024 02:37:25 GMT   (33248kb,D)

Title: 360+x: A Panoptic Multi-modal Scene Understanding Dataset
Authors: Hao Chen, Yuqi Hou, Chenyuan Qu, Irene Testini, Xiaohan Hong, Jianbo
  Jiao
Categories: cs.CV cs.AI cs.MM cs.SD eess.AS
Comments: CVPR 2024 (Oral Presentation), Project page:
  https://x360dataset.github.io/
Journal-ref: The IEEE/CVF Computer Vision and Pattern Recognition Conference
  (CVPR) 2024
\\ ( https://arxiv.org/abs/2404.00989 ,  33248kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01622
replaced with revised version Sat, 6 Apr 2024 02:12:13 GMT   (1659kb)

Title: Gen4DS: Workshop on Data Storytelling in an Era of Generative AI
Authors: Xingyu Lan, Leni Yang, Zezhong Wang, Yun Wang, Danqing Shi, Sheelagh
  Carpendale
Categories: cs.HC cs.AI cs.GR
\\ ( https://arxiv.org/abs/2404.01622 ,  1659kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02090
replaced with revised version Mon, 8 Apr 2024 01:07:43 GMT   (24kb)

Title: Already Moderate Population Sizes Provably Yield Strong Robustness to
  Noise
Authors: Denis Antipov, Benjamin Doerr, Alexandra Ivanova
Categories: cs.NE cs.AI
Comments: Full version of the same-titled paper accepted at GECCO 2024
\\ ( https://arxiv.org/abs/2404.02090 ,  24kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02448 (*cross-listing*)
replaced with revised version Mon, 8 Apr 2024 02:46:38 GMT   (5396kb,D)

Title: Electric Vehicle Routing Problem for Emergency Power Supply: Towards
  Telecom Base Station Relief
Authors: Daisuke Kikuta and Hiroki Ikeuchi and Kengo Tajiri and Yuta Toyama and
  Masaki Nakamura and Yuusuke Nakano
Categories: math.OC cs.AI cs.LG cs.MA
Comments: Accepted at AAMAS 2024 (extended abstract). 10 pages, 5 figures. Work
  in progress
\\ ( https://arxiv.org/abs/2404.02448 ,  5396kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02807 (*cross-listing*)
replaced with revised version Sat, 6 Apr 2024 03:24:27 GMT   (7001kb,D)

Title: An Optimization Framework to Personalize Passive Cardiac Mechanics
Authors: Lei Shi, Ian Chen, Hiroo Takayama, Vijay Vedula
Categories: physics.med-ph cs.AI
\\ ( https://arxiv.org/abs/2404.02807 ,  7001kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03204 (*cross-listing*)
replaced with revised version Sat, 6 Apr 2024 04:35:50 GMT   (233kb,D)

Title: RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting
  for Text-to-Speech Synthesis
Authors: Detai Xin, Xu Tan, Kai Shen, Zeqian Ju, Dongchao Yang, Yuancheng Wang,
  Shinnosuke Takamichi, Hiroshi Saruwatari, Shujie Liu, Jinyu Li, Sheng Zhao
Categories: eess.AS cs.AI cs.CL cs.LG cs.SD
\\ ( https://arxiv.org/abs/2404.03204 ,  233kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03354
replaced with revised version Sun, 7 Apr 2024 05:57:45 GMT   (702kb,D)

Title: A Comprehensive Survey on Self-Supervised Learning for Recommendation
Authors: Xubin Ren, Wei Wei, Lianghao Xia and Chao Huang
Categories: cs.IR cs.AI
\\ ( https://arxiv.org/abs/2404.03354 ,  702kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03543
replaced with revised version Sat, 6 Apr 2024 04:29:25 GMT   (2064kb,D)

Title: CodeEditorBench: Evaluating Code Editing Capability of Large Language
  Models
Authors: Jiawei Guo, Ziming Li, Xueling Liu, Kaijing Ma, Tianyu Zheng,
  Zhouliang Yu, Ding Pan, Yizhi LI, Ruibo Liu, Yue Wang, Shuyue Guo, Xingwei
  Qu, Xiang Yue, Ge Zhang, Wenhu Chen, Jie Fu
Categories: cs.SE cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2404.03543 ,  2064kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11380
replaced with revised version Sun, 7 Apr 2024 13:27:08 GMT   (1620kb,D)

Title: Convoifilter: A case study of doing cocktail party speech recognition
Authors: Thai-Binh Nguyen and Alexander Waibel
Categories: cs.SD cs.CL eess.AS
Comments: Accepted at HSCMA 2024
\\ ( https://arxiv.org/abs/2308.11380 ,  1620kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07765
replaced with revised version Mon, 8 Apr 2024 03:30:34 GMT   (4125kb,D)

Title: Echotune: A Modular Extractor Leveraging the Variable-Length Nature of
  Speech in ASR Tasks
Authors: Sizhou Chen, Songyang Gao, Sen Fang
Categories: cs.SD cs.CL eess.AS
\\ ( https://arxiv.org/abs/2309.07765 ,  4125kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16671
replaced with revised version Sun, 7 Apr 2024 17:22:46 GMT   (277kb,D)

Title: Demystifying CLIP Data
Authors: Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes,
  Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, Christoph
  Feichtenhofer
Categories: cs.CV cs.CL
Comments: 17 pages. arXiv admin note: text overlap with arXiv:2103.00020 by
  other authors
\\ ( https://arxiv.org/abs/2309.16671 ,  277kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12362
replaced with revised version Mon, 8 Apr 2024 00:16:46 GMT   (904kb,D)

Title: REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative
  Large Language Models
Authors: Ruisi Zhang, Shehzeen Samarah Hussain, Paarth Neekhara, Farinaz
  Koushanfar
Categories: cs.CR cs.CL
Comments: accept to usenix security 2024
\\ ( https://arxiv.org/abs/2310.12362 ,  904kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13289
replaced with revised version Mon, 8 Apr 2024 06:12:52 GMT   (1402kb,D)

Title: SALMONN: Towards Generic Hearing Abilities for Large Language Models
Authors: Changli Tang and Wenyi Yu and Guangzhi Sun and Xianzhao Chen and Tian
  Tan and Wei Li and Lu Lu and Zejun Ma and Chao Zhang
Categories: cs.SD cs.CL eess.AS
\\ ( https://arxiv.org/abs/2310.13289 ,  1402kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04854
replaced with revised version Sun, 7 Apr 2024 14:30:23 GMT   (8364kb,D)

Title: Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey
Authors: Jinghong Li, Huy Phan, Wen Gu, Koichi Ota, Shinobu Hasegawa
Categories: cs.DL cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.04854 ,  8364kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18276
replaced with revised version Sun, 7 Apr 2024 06:44:28 GMT   (349kb,D)

Title: RankMamba: Benchmarking Mamba's Document Ranking Performance in the Era
  of Transformers
Authors: Zhichao Xu
Categories: cs.IR cs.CL
\\ ( https://arxiv.org/abs/2403.18276 ,  349kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00226
replaced with revised version Mon, 8 Apr 2024 13:05:11 GMT   (1613kb,D)

Title: Design as Desired: Utilizing Visual Question Answering for Multimodal
  Pre-training
Authors: Tongkun Su and Jun Li and Xi Zhang and Haibo Jin and Hao Chen and
  Qiong Wang and Faqin Lv and Baoliang Zhao and Yin Hu
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2404.00226 ,  1613kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00938
replaced with revised version Fri, 5 Apr 2024 21:55:47 GMT   (16kb)

Title: How Can Large Language Models Enable Better Socially Assistive
  Human-Robot Interaction: A Brief Survey
Authors: Zhonghao Shi, Ellen Landrum, Amy O' Connell, Mina Kian, Leticia
  Pinto-Alva, Kaleen Shrestha, Xiaoyuan Zhu, Maja J Matari\'c
Categories: cs.HC cs.CL cs.CV cs.RO
Comments: 2 pages, accepted to the Proceedings of the AAAI Symposium Series,
  2024
\\ ( https://arxiv.org/abs/2404.00938 ,  16kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01800
replaced with revised version Sat, 6 Apr 2024 19:08:08 GMT   (200kb,D)

Title: Sentiment Analysis of Citations in Scientific Articles Using ChatGPT:
  Identifying Potential Biases and Conflicts of Interest
Authors: Walid Hariri
Categories: cs.DL cs.CL
\\ ( https://arxiv.org/abs/2404.01800 ,  200kb)
------------------------------------------------------------------------------
\\
arXiv:2001.03048 (*cross-listing*)
replaced with revised version Sun, 7 Apr 2024 21:04:52 GMT   (1387kb,D)

Title: Resource-Efficient Neural Networks for Embedded Systems
Authors: Wolfgang Roth, G\"unther Schindler, Bernhard Klein, Robert Peharz,
  Sebastian Tschiatschek, Holger Fr\"oning, Franz Pernkopf, Zoubin Ghahramani
Categories: stat.ML cs.LG
Comments: arXiv admin note: text overlap with arXiv:1812.02240; accepted at
  JMLR
\\ ( https://arxiv.org/abs/2001.03048 ,  1387kb)
------------------------------------------------------------------------------
\\
arXiv:2005.14064 (*cross-listing*)
replaced with revised version Mon, 8 Apr 2024 07:15:19 GMT   (6493kb)

Title: Codebook-Based Beam Tracking for Conformal ArrayEnabled UAV MmWave
  Networks
Authors: Jinglin Zhang, Wenjun Xu, Hui Gao, Miao Pan, Zhu Han, and Ping Zhang
Categories: eess.SP cs.LG
\\ ( https://arxiv.org/abs/2005.14064 ,  6493kb)
------------------------------------------------------------------------------
\\
arXiv:2102.13273 (*cross-listing*)
replaced with revised version Mon, 8 Apr 2024 05:37:01 GMT   (783kb,D)

Title: Application-Driven Learning: A Closed-Loop Prediction and Optimization
  Approach Applied to Dynamic Reserves and Demand Forecasting
Authors: Joaquim Dias Garcia, Alexandre Street, Tito Homem-de-Mello and
  Francisco D. Mu\~noz
Categories: math.OC cs.LG cs.SY eess.SY stat.ME stat.ML
\\ ( https://arxiv.org/abs/2102.13273 ,  783kb)
------------------------------------------------------------------------------
\\
arXiv:2112.04731
replaced with revised version Sun, 7 Apr 2024 17:09:58 GMT   (811kb,D)

Title: Mimicking the Oracle: An Initial Phase Decorrelation Approach for Class
  Incremental Learning
Authors: Yujun Shi, Kuangqi Zhou, Jian Liang, Zihang Jiang, Jiashi Feng, Philip
  Torr, Song Bai, Vincent Y. F. Tan
Categories: cs.CV cs.LG
Comments: CVPR 2022 Camera-Ready Version
\\ ( https://arxiv.org/abs/2112.04731 ,  811kb)
------------------------------------------------------------------------------
\\
arXiv:2112.14758 (*cross-listing*)
replaced with revised version Fri, 5 Apr 2024 18:27:12 GMT   (3090kb,D)

Title: Multivariate Trend Filtering for Lattice Data
Authors: Veeranjaneyulu Sadhanala, Yu-Xiang Wang, Addison J. Hu, Ryan J.
  Tibshirani
Categories: stat.ML cs.LG math.ST stat.TH
\\ ( https://arxiv.org/abs/2112.14758 ,  3090kb)
------------------------------------------------------------------------------
\\
arXiv:2207.08993 (*cross-listing*)
replaced with revised version Sun, 7 Apr 2024 17:59:07 GMT   (2973kb,D)

Title: Machine Learning in Orbit Estimation: a Survey
Authors: Francisco Caldas and Cl\'audia Soares
Categories: astro-ph.EP cs.LG
Comments: Accepted for Publication to Acta Astronautica
\\ ( https://arxiv.org/abs/2207.08993 ,  2973kb)
------------------------------------------------------------------------------
\\
arXiv:2210.02184 (*cross-listing*)
replaced with revised version Mon, 8 Apr 2024 06:32:02 GMT   (1686kb,D)

Title: Rediscovery of Numerical L\"uscher's Formula from the Neural Network
Authors: Yu Lu, Yi-Jia Wang, Ying Chen, Jia-Jun Wu
Categories: hep-lat cs.LG hep-ph hep-th
Comments: 7 figures, accepted by Chinese Physics C
DOI: 10.1088/1674-1137/ad3b9c
\\ ( https://arxiv.org/abs/2210.02184 ,  1686kb)
------------------------------------------------------------------------------
\\
arXiv:2211.06617 (*cross-listing*)
replaced with revised version Mon, 8 Apr 2024 07:44:38 GMT   (314kb,D)

Title: Empirical Risk Minimization with Relative Entropy Regularization
Authors: Samir M. Perlaza, Gaetan Bisson, I\~naki Esnaola, Alain Jean-Marie,
  Stefano Rini
Categories: math.ST cs.IT cs.LG math.IT stat.TH
Comments: Appears in IEEE Transactions on Information Theory: Submitted June
  2023. Revised in October 2023. Accepted January 2024. CameraReady February
  2024. Also available as: Research Report, INRIA, No. RR-9454, Centre Inria
  d'Universit\'e C\^ote d'Azur, Sophia Antipolis, France, Feb., 2022. Last
  version: Version 7
Report-no: RR-9454
DOI: 10.1109/TIT.2024.3365728
\\ ( https://arxiv.org/abs/2211.06617 ,  314kb)
------------------------------------------------------------------------------
\\
arXiv:2302.07517
replaced with revised version Sun, 7 Apr 2024 09:34:03 GMT   (8305kb,D)

Title: Versatile User Identification in Extended Reality using Pretrained
  Similarity-Learning
Authors: Christian Rack, Konstantin Kobs, Tamara Fernando, Andreas Hotho, Marc
  Erich Latoschik
Categories: cs.HC cs.LG
\\ ( https://arxiv.org/abs/2302.07517 ,  8305kb)
------------------------------------------------------------------------------
\\
arXiv:2304.14870 (*cross-listing*)
replaced with revised version Mon, 8 Apr 2024 08:38:44 GMT   (1699kb,D)

Title: Using a Deep Learning Model to Simulate Human Stock Trader's Methods of
  Chart Analysis
Authors: Sungwoo Kang, Jong-Kook Kim
Categories: q-fin.ST cs.LG
Comments: 35 pages, 14 figures
ACM-class: I.2
\\ ( https://arxiv.org/abs/2304.14870 ,  1699kb)
------------------------------------------------------------------------------
\\
arXiv:2305.00510
replaced with revised version Sun, 7 Apr 2024 05:59:05 GMT   (25937kb,D)

Title: Towards AI-Architecture Liberty: A Comprehensive Survey on Designing and
  Collaborating Virtual Architecture by Deep Learning in the Metaverse
Authors: Anqi Wang, Jiahua Dong, Lik-Hang Lee, Jiachuan Shen, Pan Hui
Categories: cs.HC cs.CV cs.LG
Comments: 37 pages, 9 figures, and 5 tables
ACM-class: I.2.1; J.5; J.6; I.3.7
\\ ( https://arxiv.org/abs/2305.00510 ,  25937kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18436 (*cross-listing*)
replaced with revised version Fri, 5 Apr 2024 19:29:13 GMT   (874kb,D)

Title: Statistically Optimal K-means Clustering via Nonnegative Low-rank
  Semidefinite Programming
Authors: Yubo Zhuang, Xiaohui Chen, Yun Yang, Richard Y. Zhang
Categories: stat.ML cs.LG math.OC
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2305.18436 ,  874kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09610
replaced with revised version Fri, 5 Apr 2024 20:26:02 GMT   (440kb,D)

Title: CHORUS: Foundation Models for Unified Data Discovery and Exploration
Authors: Moe Kayali, Anton Lykov, Ilias Fountalis, Nikolaos Vasiloglou, Dan
  Olteanu, Dan Suciu
Categories: cs.DB cs.LG
Comments: To appear in VLDB 2024
\\ ( https://arxiv.org/abs/2306.09610 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2307.05845
replaced with revised version Sun, 7 Apr 2024 23:27:06 GMT   (13780kb,D)

Title: PIGEON: Predicting Image Geolocations
Authors: Lukas Haas, Michal Skreta, Silas Alberti, Chelsea Finn
Categories: cs.CV cs.LG
Comments: Preprint
\\ ( https://arxiv.org/abs/2307.05845 ,  13780kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02464 (*cross-listing*)
replaced with revised version Sun, 7 Apr 2024 13:58:46 GMT   (1189kb)

Title: Universal Approximation of Linear Time-Invariant (LTI) Systems through
  RNNs: Power of Randomness in Reservoir Computing
Authors: Shashank Jere, Lizhong Zheng, Karim Said and Lingjia Liu
Categories: eess.SP cs.LG cs.SY eess.SY
Comments: This work has been accepted to IEEE Journal of Selected Topics in
  Signal Processing (JSTSP). Copyright may be transferred without notice, after
  which this version may no longer be accessible
\\ ( https://arxiv.org/abs/2308.02464 ,  1189kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02958 (*cross-listing*)
replaced with revised version Fri, 5 Apr 2024 21:39:23 GMT   (16762kb,D)

Title: K-band: Self-supervised MRI Reconstruction via Stochastic Gradient
  Descent over K-space Subsets
Authors: Frederic Wang, Han Qi, Alfredo De Goyeneche, Reinhard Heckel, Michael
  Lustig, and Efrat Shimron
Categories: eess.IV cs.CV cs.LG eess.SP physics.med-ph
\\ ( https://arxiv.org/abs/2308.02958 ,  16762kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08736
replaced with revised version Mon, 8 Apr 2024 09:06:03 GMT   (392kb,D)

Title: On the Effectiveness of Log Representation for Log-based Anomaly
  Detection
Authors: Xingfang Wu, Heng Li, Foutse Khomh
Categories: cs.SE cs.LG
Comments: Accepted by Journal of Empirical Software Engineering (EMSE)
Journal-ref: Empirical Software Engineering (2023) 28:137
DOI: 10.1007/s10664-023-10364-1
\\ ( https://arxiv.org/abs/2308.08736 ,  392kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13888
replaced with revised version Mon, 8 Apr 2024 10:04:29 GMT   (13251kb,D)

Title: Neural Implicit Morphing of Face Images
Authors: Guilherme Schardong, Tiago Novello, Hallison Paz, Iurii Medvedev,
  Vin\'icius da Silva, Luiz Velho, Nuno Gon\c{c}alves
Categories: cs.CV cs.LG
Comments: 14 pages, 20 figures, accepted for CVPR 2024
ACM-class: I.4.8; I.4.10
\\ ( https://arxiv.org/abs/2308.13888 ,  13251kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04001
replaced with revised version Sun, 7 Apr 2024 22:46:13 GMT   (6216kb,D)

Title: MMSFormer: Multimodal Transformer for Material and Semantic Segmentation
Authors: Md Kaykobad Reza, Ashley Prater-Bennette, M. Salman Asif
Categories: cs.CV cs.LG
Comments: Accepted by IEEE Open Journal of Signal Processing. 15 pages, 3
  figures, 9 tables
\\ ( https://arxiv.org/abs/2309.04001 ,  6216kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05575
replaced with revised version Mon, 8 Apr 2024 15:26:25 GMT   (187kb,D)

Title: Anisotropic Diffusion Stencils: From Simple Derivations over Stability
  Estimates to ResNet Implementations
Authors: Karl Schrader, Joachim Weickert, Michael Krause
Categories: math.NA cs.LG cs.NA eess.IV
Comments: To appear
\\ ( https://arxiv.org/abs/2309.05575 ,  187kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07169 (*cross-listing*)
replaced with revised version Mon, 8 Apr 2024 16:37:58 GMT   (827kb,D)

Title: Spectral Convergence of Simplicial Complex Signals
Authors: Purui Zhang, Xingchao Jian, Feng Ji, Wee Peng Tay, Bihan Wen
Categories: eess.SP cs.LG
Comments: 9 pages, 2 figures
\\ ( https://arxiv.org/abs/2309.07169 ,  827kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11104 (*cross-listing*)
replaced with revised version Sun, 7 Apr 2024 20:05:43 GMT   (205kb)

Title: Local Lipschitz Constant Computation of ReLU-FNNs: Upper Bound
  Computation with Exactness Verification
Authors: Yoshio Ebihara and Xin Dai and Victor Magron and Dimitri Peaucelle and
  Sophie Tarbouriech
Categories: math.OC cs.LG
Comments: 8 pages, 3 figures, Fixed authors' display in the bibliography
\\ ( https://arxiv.org/abs/2310.11104 ,  205kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00537 (*cross-listing*)
replaced with revised version Fri, 5 Apr 2024 22:50:25 GMT   (7689kb,AD)

Title: Machine Learning Without a Processor: Emergent Learning in a Nonlinear
  Electronic Metamaterial
Authors: Sam Dillavou, Benjamin D Beyer, Menachem Stern, Andrea J Liu, Marc Z
  Miskin, Douglas J Durian
Categories: cond-mat.soft cs.ET cs.LG
Comments: 11 pages 8 figures
\\ ( https://arxiv.org/abs/2311.00537 ,  7689kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12850
replaced with revised version Sun, 7 Apr 2024 02:18:23 GMT   (48431kb,D)

Title: PrivImage: Differentially Private Synthetic Image Generation using
  Diffusion Models with Semantic-Aware Pretraining
Authors: Kecen Li, Chen Gong, Zhixiang Li, Yuzhong Zhao, Xinwen Hou, Tianhao
  Wang
Categories: cs.CV cs.CR cs.LG
Comments: Accepted at USENIX Security 2024
\\ ( https://arxiv.org/abs/2311.12850 ,  48431kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16909 (*cross-listing*)
replaced with revised version Sat, 6 Apr 2024 11:38:31 GMT   (2638kb)

Title: Multinomial belief networks for healthcare data
Authors: H. C. Donker, D. Neijzen, J. de Jong, G. A. Lunter
Categories: stat.ML cs.LG stat.AP
Comments: 18 pages, 4 figs; supplement: 22 pages
\\ ( https://arxiv.org/abs/2311.16909 ,  2638kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15541
replaced with revised version Mon, 8 Apr 2024 14:10:12 GMT   (21634kb,D)

Title: Stitching Satellites to the Edge: Pervasive and Efficient Federated LEO
  Satellite Learning
Authors: Mohamed Elmahallawy and Tie Luo
Categories: cs.DC cs.LG
\\ ( https://arxiv.org/abs/2401.15541 ,  21634kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09439 (*cross-listing*)
replaced with revised version Mon, 8 Apr 2024 02:41:54 GMT   (4961kb)

Title: Deep-Learning-Based Channel Estimation for IRS-Assisted ISAC System
Authors: Yu Liu, Ibrahim Al-Nahhal, Octavia A. Dobre, and Fanggang Wang
Categories: eess.SP cs.LG
Journal-ref: Published in IEEE Global Communications Conference, Rio de
  Janeiro, Brazil, Dec. 2022, pp. 4220-4225
DOI: 10.1109/GLOBECOM48099.2022.10001672
\\ ( https://arxiv.org/abs/2402.09439 ,  4961kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09440 (*cross-listing*)
replaced with revised version Mon, 8 Apr 2024 02:32:01 GMT   (7626kb)

Title: Extreme Learning Machine-based Channel Estimation in IRS-Assisted
  Multi-User ISAC System
Authors: Yu Liu, Ibrahim Al-Nahhal, Octavia A. Dobre, Fanggang Wang, and
  Hyundong Shin
Categories: eess.SP cs.LG
Journal-ref: Published in IEEE Transactions on Communications, vol. 71, no. 12,
  pp. 6993-7007, Dec. 2023
DOI: 10.1109/TCOMM.2023.3308150
\\ ( https://arxiv.org/abs/2402.09440 ,  7626kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09441 (*cross-listing*)
replaced with revised version Mon, 8 Apr 2024 02:03:43 GMT   (7484kb)

Title: Deep-Learning Channel Estimation for IRS-Assisted Integrated Sensing and
  Communication System
Authors: Yu Liu, Ibrahim Al-Nahhal, Octavia A. Dobre, and Fanggang Wang
Categories: eess.SP cs.LG
Journal-ref: Published in IEEE Transactions on Vehicular Technology, vol. 72,
  no. 5, pp. 6181-6193, May 2023
DOI: 10.1109/TVT.2022.3231727
\\ ( https://arxiv.org/abs/2402.09441 ,  7484kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15781
replaced with revised version Mon, 8 Apr 2024 17:45:28 GMT   (70kb)

Title: Analysis of Off-Policy Multi-Step TD-Learning with Linear Function
  Approximation
Authors: Donghwan Lee
Categories: eess.SY cs.LG cs.SY
\\ ( https://arxiv.org/abs/2402.15781 ,  70kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03954
replaced with revised version Mon, 8 Apr 2024 02:46:38 GMT   (27493kb,D)

Title: 3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple
  3D Representations
Authors: Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, Huazhe
  Xu
Categories: cs.RO cs.CV cs.LG
Comments: Videos, code, and data: https://3d-diffusion-policy.github.io
\\ ( https://arxiv.org/abs/2403.03954 ,  27493kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15417 (*cross-listing*)
replaced with revised version Fri, 5 Apr 2024 18:17:08 GMT   (1266kb,D)

Title: Enhancing Automatic Modulation Recognition for IoT Applications Using
  Transformers
Authors: Narges Rashvand, Kenneth Witham, Gabriel Maldonado, Vinit Katariya,
  Nishanth Marer Prabhu, Gunar Schirner, Hamed Tabkhi
Categories: eess.SP cs.LG
\\ ( https://arxiv.org/abs/2403.15417 ,  1266kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19964
replaced with revised version Fri, 5 Apr 2024 20:33:14 GMT   (10603kb,D)

Title: FairRAG: Fair Human Generation via Fair Retrieval Augmentation
Authors: Robik Shrestha, Yang Zou, Qiuyu Chen, Zhiheng Li, Yusheng Xie, Siqi
  Deng
Categories: cs.CV cs.CY cs.LG
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2403.19964 ,  10603kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00797
replaced with revised version Sun, 7 Apr 2024 02:05:49 GMT   (2162kb,D)

Title: Metarobotics for Industry and Society: Vision, Technologies, and
  Opportunities
Authors: Eric Guiffo Kaigom
Categories: cs.RO cs.CY cs.LG cs.SY eess.SY
Comments: Published on IEEE Transactions on Industrial Informatics, Volume 20,
  Issue 4, April 2024
Journal-ref: IEEE Transactions on Industrial Informatics, Volume 20, Issue 4,
  April 2024
DOI: 10.1109/TII.2023.3337380
\\ ( https://arxiv.org/abs/2404.00797 ,  2162kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01518
replaced with revised version Mon, 8 Apr 2024 05:09:19 GMT   (1197kb,D)

Title: Temporally Consistent Unbalanced Optimal Transport for Unsupervised
  Action Segmentation
Authors: Ming Xu, Stephen Gould
Categories: cs.CV cs.LG eess.IV
Comments: Accepted to CVPR 2024 (Oral)
\\ ( https://arxiv.org/abs/2404.01518 ,  1197kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01959
replaced with revised version Sun, 7 Apr 2024 05:26:08 GMT   (19414kb,D)

Title: Bi-LORA: A Vision-Language Approach for Synthetic Image Detection
Authors: Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdenour
  Hadid, Abdelmalik Taleb-Ahmed
Categories: cs.CV cs.CR cs.LG
\\ ( https://arxiv.org/abs/2404.01959 ,  19414kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03019
replaced with revised version Mon, 8 Apr 2024 01:06:38 GMT   (384kb,D)

Title: GeoT: Tensor Centric Library for Graph Neural Network via Efficient
  Segment Reduction on GPU
Authors: Zhongming Yu, Genghan Zhang, Hanxian Huang, Xin Chen, Jishen Zhao
Categories: cs.DC cs.LG
\\ ( https://arxiv.org/abs/2404.03019 ,  384kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
