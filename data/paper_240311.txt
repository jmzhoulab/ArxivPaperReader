paper_240311.txt

Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 100021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月11日 15:18
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Thu  7 Mar 24 19:00:00 GMT  to  Fri  8 Mar 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.04772
Date: Mon, 26 Feb 2024 11:00:45 GMT   (17kb)

Title: Representing Pedagogic Content Knowledge Through Rough Sets
Authors: A Mani
Categories: cs.AI cs.LO
\\
  A teacher's knowledge base consists of knowledge of mathematics content,
knowledge of student epistemology, and pedagogical knowledge. It has severe
implications on the understanding of student's knowledge of content, and the
learning context in general. The necessity to formalize the different content
knowledge in approximate senses is recognized in the education research
literature. A related problem is that of coherent formalizability. Responsive
or smart AI-based software systems do not concern themselves with meaning, and
trained ones are replete with their own issues. In the present research, many
issues in modeling teachers' understanding of content are identified, and a
two-tier rough set-based model is proposed by the present author. The main
advantage of the proposed approach is in its ability to coherently handle
vagueness, granularity and multi-modality. An extended example to equational
reasoning is used to demonstrate these.
\\ ( https://arxiv.org/abs/2403.04772 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04859
Date: Thu, 7 Mar 2024 19:16:17 GMT   (220kb)

Title: Self-Supervision in Time for Satellite Images(S3-TSS): A novel method of
  SSL technique in Satellite images
Authors: Akansh Maurya, Hewan Shrestha, Mohammad Munem Shahriar
Categories: cs.AI
\\
  With the limited availability of labeled data with various atmospheric
conditions in remote sensing images, it seems useful to work with
self-supervised algorithms. Few pretext-based algorithms, including from
rotation, spatial context and jigsaw puzzles are not appropriate for satellite
images. Often, satellite images have a higher temporal frequency. So, the
temporal dimension of remote sensing data provides natural augmentation without
requiring us to create artificial augmentation of images. Here, we propose
S3-TSS, a novel method of self-supervised learning technique that leverages
natural augmentation occurring in temporal dimension. We compare our results
with current state-of-the-art methods and also perform various experiments. We
observed that our method was able to perform better than baseline SeCo in four
downstream datasets. Code for our work can be found here:
https://github.com/hewanshrestha/Why-Self-Supervision-in-Time
\\ ( https://arxiv.org/abs/2403.04859 ,  220kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04866
Date: Thu, 7 Mar 2024 19:29:36 GMT   (614kb,D)

Title: A Modular End-to-End Multimodal Learning Method for Structured and
  Unstructured Data
Authors: Marco D Alessandro, Enrique Calabr\'es, Mikel Elkano
Categories: cs.AI
Comments: 8 pages, 1 figure
\\
  Multimodal learning is a rapidly growing research field that has
revolutionized multitasking and generative modeling in AI. While much of the
research has focused on dealing with unstructured data (e.g., language, images,
audio, or video), structured data (e.g., tabular data, time series, or signals)
has received less attention. However, many industry-relevant use cases involve
or can be benefited from both types of data. In this work, we propose a
modular, end-to-end multimodal learning method called MAGNUM, which can
natively handle both structured and unstructured data. MAGNUM is flexible
enough to employ any specialized unimodal module to extract, compress, and fuse
information from all available modalities.
\\ ( https://arxiv.org/abs/2403.04866 ,  614kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04893
Date: Thu, 7 Mar 2024 20:55:08 GMT   (341kb,D)

Title: A Safe Harbor for AI Evaluation and Red Teaming
Authors: Shayne Longpre, Sayash Kapoor, Kevin Klyman, Ashwin Ramaswami, Rishi
  Bommasani, Borhane Blili-Hamelin, Yangsibo Huang, Aviya Skowron, Zheng-Xin
  Yong, Suhas Kotha, Yi Zeng, Weiyan Shi, Xianjun Yang, Reid Southen, Alexander
  Robey, Patrick Chao, Diyi Yang, Ruoxi Jia, Daniel Kang, Sandy Pentland,
  Arvind Narayanan, Percy Liang, Peter Henderson
Categories: cs.AI
\\
  Independent evaluation and red teaming are critical for identifying the risks
posed by generative AI systems. However, the terms of service and enforcement
strategies used by prominent AI companies to deter model misuse have
disincentives on good faith safety evaluations. This causes some researchers to
fear that conducting such research or releasing their findings will result in
account suspensions or legal reprisal. Although some companies offer researcher
access programs, they are an inadequate substitute for independent research
access, as they have limited community representation, receive inadequate
funding, and lack independence from corporate incentives. We propose that major
AI developers commit to providing a legal and technical safe harbor,
indemnifying public interest safety research and protecting it from the threat
of account suspensions or legal reprisal. These proposals emerged from our
collective experience conducting safety, privacy, and trustworthiness research
on generative AI systems, where norms and incentives could be better aligned
with public interests, without exacerbating model misuse. We believe these
commitments are a necessary step towards more inclusive and unimpeded community
efforts to tackle the risks of generative AI.
\\ ( https://arxiv.org/abs/2403.04893 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04919
Date: Thu, 7 Mar 2024 22:04:35 GMT   (429kb)

Title: Identifying Causal Effects Under Functional Dependencies
Authors: Yizuo Chen and Adnan Darwiche
Categories: cs.AI cs.LG cs.SC stat.ME
\\
  We study the identification of causal effects, motivated by two improvements
to identifiability which can be attained if one knows that some variables in a
causal graph are functionally determined by their parents (without needing to
know the specific functions). First, an unidentifiable causal effect may become
identifiable when certain variables are functional. Second, certain functional
variables can be excluded from being observed without affecting the
identifiability of a causal effect, which may significantly reduce the number
of needed variables in observational data. Our results are largely based on an
elimination procedure which removes functional variables from a causal graph
while preserving key properties in the resulting causal graph, including the
identifiability of causal effects.
\\ ( https://arxiv.org/abs/2403.04919 ,  429kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04931
Date: Thu, 7 Mar 2024 22:37:49 GMT   (2709kb,D)

Title: A Survey on Human-AI Teaming with Large Pre-Trained Models
Authors: Vanshika Vats, Marzia Binta Nizam, Minghao Liu, Ziyuan Wang, Richard
  Ho, Mohnish Sai Prasad, Vincent Titterton, Sai Venkat Malreddy, Riya
  Aggarwal, Yanwen Xu, Lei Ding, Jay Mehta, Nathan Grinnell, Li Liu, Sijia
  Zhong, Devanathan Nallur Gandamani, Xinyi Tang, Rohan Ghosalkar, Celeste
  Shen, Rachel Shen, Nafisa Hussain, Kesav Ravichandran, James Davis
Categories: cs.AI cs.CL cs.HC
\\
  In the rapidly evolving landscape of artificial intelligence (AI), the
collaboration between human intelligence and AI systems, known as Human-AI
(HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and
decision-making processes. The advent of Large Pre-trained Models (LPtM) has
significantly transformed this landscape, offering unprecedented capabilities
by leveraging vast amounts of data to understand and predict complex patterns.
This paper surveys the pivotal integration of LPtMs with HAI, emphasizing how
these models enhance collaborative intelligence beyond traditional approaches.
It examines the synergistic potential of LPtMs in augmenting human
capabilities, discussing this collaboration for AI model improvements,
effective teaming, ethical considerations, and their broad applied implications
in various sectors. Through this exploration, the study sheds light on the
transformative impact of LPtM-enhanced HAI Teaming, providing insights for
future research, policy development, and strategic implementations aimed at
harnessing the full potential of this collaboration for research and societal
benefit.
\\ ( https://arxiv.org/abs/2403.04931 ,  2709kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04957
Date: Thu, 7 Mar 2024 23:46:20 GMT   (889kb,D)

Title: Automatic and Universal Prompt Injection Attacks against Large Language
  Models
Authors: Xiaogeng Liu, Zhiyuan Yu, Yizhe Zhang, Ning Zhang, Chaowei Xiao
Categories: cs.AI
Comments: Pre-print, code is available at
  https://github.com/SheltonLiu-N/Universal-Prompt-Injection
\\
  Large Language Models (LLMs) excel in processing and generating human
language, powered by their ability to interpret and follow instructions.
However, their capabilities can be exploited through prompt injection attacks.
These attacks manipulate LLM-integrated applications into producing responses
aligned with the attacker's injected content, deviating from the user's actual
requests. The substantial risks posed by these attacks underscore the need for
a thorough understanding of the threats. Yet, research in this area faces
challenges due to the lack of a unified goal for such attacks and their
reliance on manually crafted prompts, complicating comprehensive assessments of
prompt injection robustness. We introduce a unified framework for understanding
the objectives of prompt injection attacks and present an automated
gradient-based method for generating highly effective and universal prompt
injection data, even in the face of defensive measures. With only five training
samples (0.3% relative to the test data), our attack can achieve superior
performance compared with baselines. Our findings emphasize the importance of
gradient-based testing, which can avoid overestimation of robustness,
especially for defense mechanisms.
\\ ( https://arxiv.org/abs/2403.04957 ,  889kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04964
Date: Fri, 8 Mar 2024 00:27:57 GMT   (290kb)

Title: Tell me the truth: A system to measure the trustworthiness of Large
  Language Models
Authors: Carlo Lipizzi
Categories: cs.AI cs.CL cs.CY
\\
  Large Language Models (LLM) have taken the front seat in most of the news
since November 2023, when ChatGPT was introduced. After more than one year, one
of the major reasons companies are resistant to adopting them is the limited
confidence they have in the trustworthiness of those systems. In a study by
(Baymard, 2023), ChatGPT-4 showed an 80.1% false-positive error rate in
identifying usability issues on websites. A Jan. '24 study by JAMA Pediatrics
found that ChatGPT has an accuracy rate of 17% percent when diagnosing
pediatric medical cases (Barile et al., 2024). But then, what is "trust"? Trust
is a relative, subject condition that can change based on culture, domain,
individuals. And then, given a domain, how can the trustworthiness of a system
be measured? In this paper, I present a systematic approach to measure
trustworthiness based on a predefined ground truth, represented as a knowledge
graph of the domain. The approach is a process with humans in the loop to
validate the representation of the domain and to fine-tune the system.
  Measuring the trustworthiness would be essential for all the entities
operating in critical environments, such as healthcare, defense, finance, but
it would be very relevant for all the users of LLMs.
\\ ( https://arxiv.org/abs/2403.04964 ,  290kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05000
Date: Fri, 8 Mar 2024 02:42:34 GMT   (1401kb,D)

Title: Medical Speech Symptoms Classification via Disentangled Representation
Authors: Jianzong Wang, Pengcheng Li, Xulong Zhang, Ning Cheng, Jing Xiao
Categories: cs.AI
Comments: Accepted by the 27th International Conference on Computer Supported
  Cooperative Work in Design (CSCWD 2024)
\\
  Intent is defined for understanding spoken language in existing works. Both
textual features and acoustic features involved in medical speech contain
intent, which is important for symptomatic diagnosis. In this paper, we propose
a medical speech classification model named DRSC that automatically learns to
disentangle intent and content representations from textual-acoustic data for
classification. The intent representations of the text domain and the
Mel-spectrogram domain are extracted via intent encoders, and then the
reconstructed text feature and the Mel-spectrogram feature are obtained through
two exchanges. After combining the intent from two domains into a joint
representation, the integrated intent representation is fed into a decision
layer for classification. Experimental results show that our model obtains an
average accuracy rate of 95% in detecting 25 different medical symptoms.
\\ ( https://arxiv.org/abs/2403.05000 ,  1401kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05025
Date: Fri, 8 Mar 2024 04:03:54 GMT   (594kb,D)

Title: Towards Multimodal Human Intention Understanding Debiasing via
  Subject-Deconfounding
Authors: Dingkang Yang, Dongling Xiao, Ke Li, Yuzheng Wang, Zhaoyu Chen, Jinjie
  Wei, Lihua Zhang
Categories: cs.AI
Comments: 14 pages
\\
  Multimodal intention understanding (MIU) is an indispensable component of
human expression analysis (e.g., sentiment or humor) from heterogeneous
modalities, including visual postures, linguistic contents, and acoustic
behaviors. Existing works invariably focus on designing sophisticated
structures or fusion strategies to achieve impressive improvements.
Unfortunately, they all suffer from the subject variation problem due to data
distribution discrepancies among subjects. Concretely, MIU models are easily
misled by distinct subjects with different expression customs and
characteristics in the training data to learn subject-specific spurious
correlations, significantly limiting performance and generalizability across
uninitiated subjects.Motivated by this observation, we introduce a
recapitulative causal graph to formulate the MIU procedure and analyze the
confounding effect of subjects. Then, we propose SuCI, a simple yet effective
causal intervention module to disentangle the impact of subjects acting as
unobserved confounders and achieve model training via true causal effects. As a
plug-and-play component, SuCI can be widely applied to most methods that seek
unbiased predictions. Comprehensive experiments on several MIU benchmarks
clearly demonstrate the effectiveness of the proposed module.
\\ ( https://arxiv.org/abs/2403.05025 ,  594kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05029
Date: Fri, 8 Mar 2024 04:19:56 GMT   (16358kb,D)

Title: BjTT: A Large-scale Multimodal Dataset for Traffic Prediction
Authors: Chengyang Zhang, Yong Zhang, Qitan Shao, Bo Li, Yisheng Lv, Xinglin
  Piao, Baocai Yin
Categories: cs.AI
\\
  Traffic prediction is one of the most significant foundations in Intelligent
Transportation Systems (ITS). Traditional traffic prediction methods rely only
on historical traffic data to predict traffic trends and face two main
challenges. 1) insensitivity to unusual events. 2) limited performance in
long-term prediction. In this work, we explore how generative models combined
with text describing the traffic system can be applied for traffic generation,
and name the task Text-to-Traffic Generation (TTG). The key challenge of the
TTG task is how to associate text with the spatial structure of the road
network and traffic data for generating traffic situations. To this end, we
propose ChatTraffic, the first diffusion model for text-to-traffic generation.
To guarantee the consistency between synthetic and real data, we augment a
diffusion model with the Graph Convolutional Network (GCN) to extract spatial
correlations of traffic data. In addition, we construct a large dataset
containing text-traffic pairs for the TTG task. We benchmarked our model
qualitatively and quantitatively on the released dataset. The experimental
results indicate that ChatTraffic can generate realistic traffic situations
from the text. Our code and dataset are available at
https://github.com/ChyaZhang/ChatTraffic.
\\ ( https://arxiv.org/abs/2403.05029 ,  16358kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05112
Date: Fri, 8 Mar 2024 07:19:43 GMT   (580kb,D)

Title: RLPeri: Accelerating Visual Perimetry Test with Reinforcement Learning
  and Convolutional Feature Extraction
Authors: Tanvi Verma, Linh Le Dinh, Nicholas Tan, Xinxing Xu, Chingyu Cheng,
  Yong Liu
Categories: cs.AI
Comments: Published at AAAI-24
Journal-ref: The 38th Annual AAAI Conference on Artificial Intelligence, 2024
\\
  Visual perimetry is an important eye examination that helps detect vision
problems caused by ocular or neurological conditions. During the test, a
patient's gaze is fixed at a specific location while light stimuli of varying
intensities are presented in central and peripheral vision. Based on the
patient's responses to the stimuli, the visual field mapping and sensitivity
are determined. However, maintaining high levels of concentration throughout
the test can be challenging for patients, leading to increased examination
times and decreased accuracy.
  In this work, we present RLPeri, a reinforcement learning-based approach to
optimize visual perimetry testing. By determining the optimal sequence of
locations and initial stimulus values, we aim to reduce the examination time
without compromising accuracy. Additionally, we incorporate reward shaping
techniques to further improve the testing performance. To monitor the patient's
responses over time during testing, we represent the test's state as a pair of
3D matrices. We apply two different convolutional kernels to extract spatial
features across locations as well as features across different stimulus values
for each location. Through experiments, we demonstrate that our approach
results in a 10-20% reduction in examination time while maintaining the
accuracy as compared to state-of-the-art methods. With the presented approach,
we aim to make visual perimetry testing more efficient and patient-friendly,
while still providing accurate results.
\\ ( https://arxiv.org/abs/2403.05112 ,  580kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05130
Date: Fri, 8 Mar 2024 07:55:42 GMT   (7947kb,D)

Title: From Chain to Tree: Refining Chain-like Rules into Tree-like Rules on
  Knowledge Graphs
Authors: Wangtao Sun, Shizhu He, Jun Zhao, Kang Liu
Categories: cs.AI
\\
  With good explanatory power and controllability, rule-based methods play an
important role in many tasks such as knowledge reasoning and decision support.
However, existing studies primarily focused on learning chain-like rules, which
limit their semantic expressions and accurate prediction abilities. As a
result, chain-like rules usually fire on the incorrect grounding values,
producing inaccurate or even erroneous reasoning results. In this paper, we
propose the concept of tree-like rules on knowledge graphs to expand the
application scope and improve the reasoning ability of rule-based methods.
Meanwhile, we propose an effective framework for refining chain-like rules into
tree-like rules. Experimental comparisons on four public datasets show that the
proposed framework can easily adapt to other chain-like rule induction methods
and the refined tree-like rules consistently achieve better performances than
chain-like rules on link prediction. The data and code of this paper can be
available at https://anonymous.4open.science/r/tree-rule-E3CD/.
\\ ( https://arxiv.org/abs/2403.05130 ,  7947kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05131
Date: Fri, 8 Mar 2024 07:58:13 GMT   (1904kb,D)

Title: Sora as an AGI World Model? A Complete Survey on Text-to-Video
  Generation
Authors: Joseph Cho, Fachrina Dewi Puspitasari, Sheng Zheng, Jingyao Zheng,
  Lik-Hang Lee, Tae-Ho Kim, Choong Seon Hong, Chaoning Zhang
Categories: cs.AI cs.CV
Comments: First complete survey on Text-to-Video Generation, 36 pages, 16
  figures
\\
  Text-to-video generation marks a significant frontier in the rapidly evolving
domain of generative AI, integrating advancements in text-to-image synthesis,
video captioning, and text-guided editing. This survey critically examines the
progression of text-to-video technologies, focusing on the shift from
traditional generative models to the cutting-edge Sora model, highlighting
developments in scalability and generalizability. Distinguishing our analysis
from prior works, we offer an in-depth exploration of the technological
frameworks and evolutionary pathways of these models. Additionally, we delve
into practical applications and address ethical and technological challenges
such as the inability to perform multiple entity handling, comprehend
causal-effect learning, understand physical interaction, perceive object
scaling and proportioning, and combat object hallucination which is also a
long-standing problem in generative models. Our comprehensive discussion covers
the topic of enablement of text-to-video generation models as human-assistive
tools and world models, as well as eliciting model's shortcomings and
summarizing future improvement direction that mainly centers around training
datasets and evaluation metrics (both automatic and human-centered). Aimed at
both newcomers and seasoned researchers, this survey seeks to catalyze further
innovation and discussion in the growing field of text-to-video generation,
paving the way for more reliable and practical generative artificial
intelligence technologies.
\\ ( https://arxiv.org/abs/2403.05131 ,  1904kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05229
Date: Fri, 8 Mar 2024 11:32:00 GMT   (1208kb)

Title: Developing Federated Time-to-Event Scores Using Heterogeneous Real-World
  Survival Data
Authors: Siqi Li, Yuqing Shang, Ziwen Wang, Qiming Wu, Chuan Hong, Yilin Ning,
  Di Miao, Marcus Eng Hock Ong, Bibhas Chakraborty, Nan Liu
Categories: cs.AI
\\
  Survival analysis serves as a fundamental component in numerous healthcare
applications, where the determination of the time to specific events (such as
the onset of a certain disease or death) for patients is crucial for clinical
decision-making. Scoring systems are widely used for swift and efficient risk
prediction. However, existing methods for constructing survival scores presume
that data originates from a single source, posing privacy challenges in
collaborations with multiple data owners. We propose a novel framework for
building federated scoring systems for multi-site survival outcomes, ensuring
both privacy and communication efficiency. We applied our approach to sites
with heterogeneous survival data originating from emergency departments in
Singapore and the United States. Additionally, we independently developed local
scores at each site. In testing datasets from each participant site, our
proposed federated scoring system consistently outperformed all local models,
evidenced by higher integrated area under the receiver operating characteristic
curve (iAUC) values, with a maximum improvement of 11.6%. Additionally, the
federated score's time-dependent AUC(t) values showed advantages over local
scores, exhibiting narrower confidence intervals (CIs) across most time points.
The model developed through our proposed method exhibits effective performance
on each local site, signifying noteworthy implications for healthcare research.
Sites participating in our proposed federated scoring model training gained
benefits by acquiring survival models with enhanced prediction accuracy and
efficiency. This study demonstrates the effectiveness of our privacy-preserving
federated survival score generation framework and its applicability to
real-world heterogeneous survival data.
\\ ( https://arxiv.org/abs/2403.05229 ,  1208kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05260
Date: Fri, 8 Mar 2024 12:31:03 GMT   (7116kb,D)

Title: Predicting Single-cell Drug Sensitivity by Adaptive Weighted Feature for
  Adversarial Multi-source Domain Adaptation
Authors: Wei Duan, Hui Liu
Categories: cs.AI
\\
  The development of single-cell sequencing technology had promoted the
generation of a large amount of single-cell transcriptional profiles, providing
valuable opportunities to explore drug-resistant cell subpopulations in a
tumor. However, the drug sensitivity data in single-cell level is still scarce
to date, pressing an urgent and highly challenging task for computational
prediction of the drug sensitivity to individual cells. This paper proposed
scAdaDrug, a multi-source adaptive weighting model to predict single-cell drug
sensitivity. We used an autoencoder to extract domain-invariant features
related to drug sensitivity from multiple source domains by exploiting
adversarial domain adaptation. Especially, we introduced an adaptive weight
generator to produce importance-aware and mutual independent weights, which
could adaptively modulate the embedding of each sample in dimension-level for
both source and target domains. Extensive experimental results showed that our
model achieved state-of-the-art performance in predicting drug sensitivity on
sinle-cell datasets, as well as on cell line and patient datasets.
\\ ( https://arxiv.org/abs/2403.05260 ,  7116kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05265
Date: Fri, 8 Mar 2024 12:42:04 GMT   (3515kb,D)

Title: MMoE: Robust Spoiler Detection with Multi-modal Information and
  Domain-aware Mixture-of-Experts
Authors: Zinan Zeng, Sen Ye, Zijian Cai, Heng Wang, Yuhan Liu, Qinghua Zheng,
  Minnan Luo
Categories: cs.AI
\\
  Online movie review websites are valuable for information and discussion
about movies. However, the massive spoiler reviews detract from the
movie-watching experience, making spoiler detection an important task. Previous
methods simply focus on reviews' text content, ignoring the heterogeneity of
information in the platform. For instance, the metadata and the corresponding
user's information of a review could be helpful. Besides, the spoiler language
of movie reviews tends to be genre-specific, thus posing a domain
generalization challenge for existing methods. To this end, we propose MMoE, a
multi-modal network that utilizes information from multiple modalities to
facilitate robust spoiler detection and adopts Mixture-of-Experts to enhance
domain generalization. MMoE first extracts graph, text, and meta feature from
the user-movie network, the review's textual content, and the review's metadata
respectively. To handle genre-specific spoilers, we then adopt
Mixture-of-Experts architecture to process information in three modalities to
promote robustness. Finally, we use an expert fusion layer to integrate the
features from different perspectives and make predictions based on the fused
embedding. Experiments demonstrate that MMoE achieves state-of-the-art
performance on two widely-used spoiler detection datasets, surpassing previous
SOTA methods by 2.56\% and 8.41\% in terms of accuracy and F1-score. Further
experiments also demonstrate MMoE's superiority in robustness and
generalization.
\\ ( https://arxiv.org/abs/2403.05265 ,  3515kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05307
Date: Fri, 8 Mar 2024 13:34:20 GMT   (3700kb,D)

Title: Tapilot-Crossing: Benchmarking and Evolving LLMs Towards Interactive
  Data Analysis Agents
Authors: Jinyang Li, Nan Huo, Yan Gao, Jiayi Shi, Yingxiu Zhao, Ge Qu, Yurong
  Wu, Chenhao Ma, Jian-Guang Lou, Reynold Cheng
Categories: cs.AI
Comments: 30 pages, 7 figures
\\
  Interactive Data Analysis, the collaboration between humans and LLM agents,
enables real-time data exploration for informed decision-making. The challenges
and costs of collecting realistic interactive logs for data analysis hinder the
quantitative evaluation of Large Language Model (LLM) agents in this task. To
mitigate this issue, we introduce Tapilot-Crossing, a new benchmark to evaluate
LLM agents on interactive data analysis. Tapilot-Crossing contains 1024
interactions, covering 4 practical scenarios: Normal, Action, Private, and
Private Action. Notably, Tapilot-Crossing is constructed by an economical
multi-agent environment, Decision Company, with few human efforts. We evaluate
popular and advanced LLM agents in Tapilot-Crossing, which underscores the
challenges of interactive data analysis. Furthermore, we propose Adaptive
Interaction Reflection (AIR), a self-generated reflection strategy that guides
LLM agents to learn from successful history. Experiments demonstrate that Air
can evolve LLMs into effective interactive data analysis agents, achieving a
relative performance improvement of up to 44.5%.
\\ ( https://arxiv.org/abs/2403.05307 ,  3700kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05318
Date: Fri, 8 Mar 2024 13:49:21 GMT   (5022kb,D)

Title: Looking Ahead to Avoid Being Late: Solving Hard-Constrained Traveling
  Salesman Problem
Authors: Jingxiao Chen, Ziqin Gong, Minghuan Liu, Jun Wang, Yong Yu, Weinan
  Zhang
Categories: cs.AI cs.LG
\\
  Many real-world problems can be formulated as a constrained Traveling
Salesman Problem (TSP). However, the constraints are always complex and
numerous, making the TSPs challenging to solve. When the number of complicated
constraints grows, it is time-consuming for traditional heuristic algorithms to
avoid illegitimate outcomes. Learning-based methods provide an alternative to
solve TSPs in a soft manner, which also supports GPU acceleration to generate
solutions quickly. Nevertheless, the soft manner inevitably results in
difficulty solving hard-constrained problems with learning algorithms, and the
conflicts between legality and optimality may substantially affect the
optimality of the solution. To overcome this problem and to have an effective
solution against hard constraints, we proposed a novel learning-based method
that uses looking-ahead information as the feature to improve the legality of
TSP with Time Windows (TSPTW) solutions. Besides, we constructed TSPTW datasets
with hard constraints in order to accurately evaluate and benchmark the
statistical performance of various approaches, which can serve the community
for future research. With comprehensive experiments on diverse datasets, MUSLA
outperforms existing baselines and shows generalizability potential.
\\ ( https://arxiv.org/abs/2403.05318 ,  5022kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05407
Date: Fri, 8 Mar 2024 16:05:47 GMT   (1118kb,D)

Title: Algorithmic Identification of Essential Exogenous Nodes for Causal
  Sufficiency in Brain Networks
Authors: Abdolmahdi Bagheri, Mahdi Dehshiri, Babak Nadjar Araabi, Alireza
  Akhondi Asl
Categories: cs.AI
\\
  In the investigation of any causal mechanisms, such as the brain's causal
networks, the assumption of causal sufficiency plays a critical role. Notably,
neglecting this assumption can result in significant errors, a fact that is
often disregarded in the causal analysis of brain networks. In this study, we
propose an algorithmic identification approach for determining essential
exogenous nodes that satisfy the critical need for causal sufficiency to adhere
to it in such inquiries. Our approach consists of three main steps: First, by
capturing the essence of the Peter-Clark (PC) algorithm, we conduct
independence tests for pairs of regions within a network, as well as for the
same pairs conditioned on nodes from other networks. Next, we distinguish
candidate confounders by analyzing the differences between the conditional and
unconditional results, using the Kolmogorov-Smirnov test. Subsequently, we
utilize Non-Factorized identifiable Variational Autoencoders (NF-iVAE) along
with the Correlation Coefficient index (CCI) metric to identify the confounding
variables within these candidate nodes. Applying our method to the Human
Connectome Projects (HCP) movie-watching task data, we demonstrate that while
interactions exist between dorsal and ventral regions, only dorsal regions
serve as confounders for the visual networks, and vice versa. These findings
align consistently with those resulting from the neuroscientific perspective.
Finally, we show the reliability of our results by testing 30 independent runs
for NF-iVAE initialization.
\\ ( https://arxiv.org/abs/2403.05407 ,  1118kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05525
Date: Fri, 8 Mar 2024 18:46:00 GMT   (7220kb,D)

Title: DeepSeek-VL: Towards Real-World Vision-Language Understanding
Authors: Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu,
  Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, Chengqi Deng, Hanwei
  Xu, Zhenda Xie, Chong Ruan
Categories: cs.AI
Comments: None
\\
  We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed
for real-world vision and language understanding applications. Our approach is
structured around three key dimensions:
  We strive to ensure our data is diverse, scalable, and extensively covers
real-world scenarios including web screenshots, PDFs, OCR, charts, and
knowledge-based content, aiming for a comprehensive representation of practical
contexts. Further, we create a use case taxonomy from real user scenarios and
construct an instruction tuning dataset accordingly. The fine-tuning with this
dataset substantially improves the model's user experience in practical
applications. Considering efficiency and the demands of most real-world
scenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficiently
processes high-resolution images (1024 x 1024), while maintaining a relatively
low computational overhead. This design choice ensures the model's ability to
capture critical semantic and detailed information across various visual tasks.
We posit that a proficient Vision-Language Model should, foremost, possess
strong language abilities. To ensure the preservation of LLM capabilities
during pretraining, we investigate an effective VL pretraining strategy by
integrating LLM training from the beginning and carefully managing the
competitive dynamics observed between vision and language modalities.
  The DeepSeek-VL family (both 1.3B and 7B models) showcases superior user
experiences as a vision-language chatbot in real-world applications, achieving
state-of-the-art or competitive performance across a wide range of
visual-language benchmarks at the same model size while maintaining robust
performance on language-centric benchmarks. We have made both 1.3B and 7B
models publicly accessible to foster innovations based on this foundation
model.
\\ ( https://arxiv.org/abs/2403.05525 ,  7220kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04770
Date: Mon, 26 Feb 2024 01:55:45 GMT   (3109kb,D)

Title: Social Orientation: A New Feature for Dialogue Analysis
Authors: Todd Morrill, Zhaoyuan Deng, Yanda Chen, Amith Ananthram, Colin Wayne
  Leach, Kathleen McKeown
Categories: cs.CL cs.LG
Comments: Accepted to LREC-COLING 2024
\\
  There are many settings where it is useful to predict and explain the success
or failure of a dialogue. Circumplex theory from psychology models the social
orientations (e.g., Warm-Agreeable, Arrogant-Calculating) of conversation
participants and can be used to predict and explain the outcome of social
interactions. Our work is novel in its systematic application of social
orientation tags to modeling conversation outcomes. In this paper, we introduce
a new data set of dialogue utterances machine-labeled with social orientation
tags. We show that social orientation tags improve task performance, especially
in low-resource settings, on both English and Chinese language benchmarks. We
also demonstrate how social orientation tags help explain the outcomes of
social interactions when used in neural models. Based on these results showing
the utility of social orientation tags for dialogue outcome prediction tasks,
we release our data sets, code, and models that are fine-tuned to predict
social orientation tags on dialogue utterances.
\\ ( https://arxiv.org/abs/2403.04770 ,  3109kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04771
Date: Mon, 26 Feb 2024 05:34:16 GMT   (9175kb,D)

Title: QASE Enhanced PLMs: Improved Control in Text Generation for MRC
Authors: Lin Ai, Zheng Hui, Zizhou Liu, Julia Hirschberg
Categories: cs.CL
\\
  To address the challenges of out-of-control generation in generative models
for machine reading comprehension (MRC), we introduce the Question-Attended
Span Extraction (QASE) module. Integrated during the fine-tuning of pre-trained
generative language models (PLMs), QASE enables these PLMs to match SOTA
extractive methods and outperform leading LLMs like GPT-4 in MRC tasks, without
significant increases in computational costs.
\\ ( https://arxiv.org/abs/2403.04771 ,  9175kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04780
Date: Sat, 2 Mar 2024 09:27:32 GMT   (11453kb,D)

Title: MuseGraph: Graph-oriented Instruction Tuning of Large Language Models
  for Generic Graph Mining
Authors: Yanchao Tan, Hang Lv, Xinyi Huang, Jiawei Zhang, Shiping Wang, Carl
  Yang
Categories: cs.CL cs.AI
\\
  Graphs with abundant attributes are essential in modeling interconnected
entities and improving predictions in various real-world applications.
Traditional Graph Neural Networks (GNNs), which are commonly used for modeling
attributed graphs, need to be re-trained every time when applied to different
graph tasks and datasets. Although the emergence of Large Language Models
(LLMs) has introduced a new paradigm in natural language processing, the
generative potential of LLMs in graph mining remains largely under-explored. To
this end, we propose a novel framework MuseGraph, which seamlessly integrates
the strengths of GNNs and LLMs and facilitates a more effective and generic
approach for graph mining across different tasks and datasets. Specifically, we
first introduce a compact graph description via the proposed adaptive input
generation to encapsulate key information from the graph under the constraints
of language token limitations. Then, we propose a diverse instruction
generation mechanism, which distills the reasoning capabilities from LLMs
(e.g., GPT-4) to create task-specific Chain-of-Thought-based instruction
packages for different graph tasks. Finally, we propose a graph-aware
instruction tuning with a dynamic instruction package allocation strategy
across tasks and datasets, ensuring the effectiveness and generalization of the
training process. Our experimental results demonstrate significant improvements
in different graph tasks, showcasing the potential of our MuseGraph in
enhancing the accuracy of graph-oriented downstream tasks while keeping the
generation powers of LLMs.
\\ ( https://arxiv.org/abs/2403.04780 ,  11453kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04782
Date: Sat, 2 Mar 2024 16:21:45 GMT   (245kb,D)

Title: A Survey on Temporal Knowledge Graph: Representation Learning and
  Applications
Authors: Li Cai, Xin Mao, Yuhao Zhou, Zhaoguang Long, Changxu Wu, Man Lan
Categories: cs.CL cs.AI
\\
  Knowledge graphs have garnered significant research attention and are widely
used to enhance downstream applications. However, most current studies mainly
focus on static knowledge graphs, whose facts do not change with time, and
disregard their dynamic evolution over time. As a result, temporal knowledge
graphs have attracted more attention because a large amount of structured
knowledge exists only within a specific period. Knowledge graph representation
learning aims to learn low-dimensional vector embeddings for entities and
relations in a knowledge graph. The representation learning of temporal
knowledge graphs incorporates time information into the standard knowledge
graph framework and can model the dynamics of entities and relations over time.
In this paper, we conduct a comprehensive survey of temporal knowledge graph
representation learning and its applications. We begin with an introduction to
the definitions, datasets, and evaluation metrics for temporal knowledge graph
representation learning. Next, we propose a taxonomy based on the core
technologies of temporal knowledge graph representation learning methods, and
provide an in-depth analysis of different methods in each category. Finally, we
present various downstream applications related to the temporal knowledge
graphs. In the end, we conclude the paper and have an outlook on the future
research directions in this area.
\\ ( https://arxiv.org/abs/2403.04782 ,  245kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04785
Date: Sat, 2 Mar 2024 22:33:17 GMT   (3668kb,D)

Title: Large Language Multimodal Models for 5-Year Chronic Disease Cohort
  Prediction Using EHR Data
Authors: Jun-En Ding, Phan Nguyen Minh Thao, Wen-Chih Peng, Jian-Zhe Wang,
  Chun-Cheng Chug, Min-Chen Hsieh, Yun-Chien Tseng, Ling Chen, Dongsheng Luo,
  Chi-Te Wang, Pei-fu Chen, Feng Liu, and Fang-Ming Hung
Categories: cs.CL cs.AI
\\
  Chronic diseases such as diabetes are the leading causes of morbidity and
mortality worldwide. Numerous research studies have been attempted with various
deep learning models in diagnosis. However, most previous studies had certain
limitations, including using publicly available datasets (e.g. MIMIC), and
imbalanced data. In this study, we collected five-year electronic health
records (EHRs) from the Taiwan hospital database, including 1,420,596 clinical
notes, 387,392 laboratory test results, and more than 1,505 laboratory test
items, focusing on research pre-training large language models. We proposed a
novel Large Language Multimodal Models (LLMMs) framework incorporating
multimodal data from clinical notes and laboratory test results for the
prediction of chronic disease risk. Our method combined a text embedding
encoder and multi-head attention layer to learn laboratory test values,
utilizing a deep neural network (DNN) module to merge blood features with
chronic disease semantics into a latent space. In our experiments, we observe
that clinicalBERT and PubMed-BERT, when combined with attention fusion, can
achieve an accuracy of 73% in multiclass chronic diseases and diabetes
prediction. By transforming laboratory test values into textual descriptions
and employing the Flan T-5 model, we achieved a 76% Area Under the ROC Curve
(AUROC), demonstrating the effectiveness of leveraging numerical text data for
training and inference in language models. This approach significantly improves
the accuracy of early-stage diabetes prediction.
\\ ( https://arxiv.org/abs/2403.04785 ,  3668kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04787
Date: Sun, 3 Mar 2024 08:12:59 GMT   (11153kb,D)

Title: Ever-Evolving Memory by Blending and Refining the Past
Authors: Seo Hyun Kim, Keummin Ka, Yohan Jo, Seung-won Hwang, Dongha Lee,
  Jinyoung Yeo
Categories: cs.CL cs.AI
Comments: 9 pages, 2 figures
\\
  For a human-like chatbot, constructing a long-term memory is crucial. A naive
approach for making a memory could be simply listing the summarized dialogue.
However, this can lead to problems when the speaker's status change over time
and contradictory information gets accumulated. It is important that the memory
stays organized to lower the confusion for the response generator. In this
paper, we propose a novel memory scheme for long-term conversation, CREEM.
Unlike existing approaches that construct memory based solely on current
sessions, our proposed model blending past memories during memory formation.
Additionally, we introduce refining process to handle redundant or outdated
information. This innovative approach seeks for overall improvement and
coherence of chatbot responses by ensuring a more informed and dynamically
evolving long-term memory.
\\ ( https://arxiv.org/abs/2403.04787 ,  11153kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04788
Date: Mon, 4 Mar 2024 01:41:07 GMT   (647kb)

Title: Topic Modeling Analysis of Aviation Accident Reports: A Comparative
  Study between LDA and NMF Models
Authors: Aziida Nanyonga, Hassan Wasswa and Graham Wild
Categories: cs.CL
Comments: 6 pages, 12 figures, 2 tables
MSC-class: Topic Modeling, Aviation Safety, Aviation Accident Reports, Machine
  Learning, LDA, NMF
DOI: 10.1109/SMARTGENCON60755.2023.10442471
\\
  Aviation safety is paramount in the modern world, with a continuous
commitment to reducing accidents and improving safety standards. Central to
this endeavor is the analysis of aviation accident reports, rich textual
resources that hold insights into the causes and contributing factors behind
aviation mishaps. This paper compares two prominent topic modeling techniques,
Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF),
in the context of aviation accident report analysis. The study leverages the
National Transportation Safety Board (NTSB) Dataset with the primary objective
of automating and streamlining the process of identifying latent themes and
patterns within accident reports. The Coherence Value (C_v) metric was used to
evaluate the quality of generated topics. LDA demonstrates higher topic
coherence, indicating stronger semantic relevance among words within topics. At
the same time, NMF excelled in producing distinct and granular topics, enabling
a more focused analysis of specific aspects of aviation accidents.
\\ ( https://arxiv.org/abs/2403.04788 ,  647kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04789
Date: Mon, 4 Mar 2024 08:38:53 GMT   (2340kb,D)

Title: TopicDiff: A Topic-enriched Diffusion Approach for Multimodal
  Conversational Emotion Detection
Authors: Jiamin Luo, Jingjing Wang, Guodong Zhou
Categories: cs.CL cs.AI cs.LG
\\
  Multimodal Conversational Emotion (MCE) detection, generally spanning across
the acoustic, vision and language modalities, has attracted increasing interest
in the multimedia community. Previous studies predominantly focus on learning
contextual information in conversations with only a few considering the topic
information in single language modality, while always neglecting the acoustic
and vision topic information. On this basis, we propose a model-agnostic
Topic-enriched Diffusion (TopicDiff) approach for capturing multimodal topic
information in MCE tasks. Particularly, we integrate the diffusion model into
neural topic model to alleviate the diversity deficiency problem of neural
topic model in capturing topic information. Detailed evaluations demonstrate
the significant improvements of TopicDiff over the state-of-the-art MCE
baselines, justifying the importance of multimodal topic information to MCE and
the effectiveness of TopicDiff in capturing such information. Furthermore, we
observe an interesting finding that the topic information in acoustic and
vision is more discriminative and robust compared to the language.
\\ ( https://arxiv.org/abs/2403.04789 ,  2340kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04790
Date: Mon, 4 Mar 2024 10:00:55 GMT   (416kb,D)

Title: Online Training of Large Language Models: Learn while chatting
Authors: Juhao Liang, Ziwei Wang, Zhuoheng Ma, Jianquan Li, Zhiyi Zhang,
  Xiangbo Wu and Benyou Wang
Categories: cs.CL cs.AI
\\
  Large Language Models(LLMs) have dramatically revolutionized the field of
Natural Language Processing(NLP), offering remarkable capabilities that have
garnered widespread usage. However, existing interaction paradigms between LLMs
and users are constrained by either inflexibility, limitations in
customization, or a lack of persistent learning. This inflexibility is
particularly evident as users, especially those without programming skills,
have restricted avenues to enhance or personalize the model. Existing
frameworks further complicate the model training and deployment process due to
their computational inefficiencies and lack of user-friendly interfaces. To
overcome these challenges, this paper introduces a novel interaction
paradigm-'Online Training using External Interactions'-that merges the benefits
of persistent, real-time model updates with the flexibility for individual
customization through external interactions such as AI agents or online/offline
knowledge bases.
\\ ( https://arxiv.org/abs/2403.04790 ,  416kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04791
Date: Mon, 4 Mar 2024 10:13:30 GMT   (1302kb)

Title: LLM vs. Lawyers: Identifying a Subset of Summary Judgments in a Large UK
  Case Law Dataset
Authors: Ahmed Izzidien and Holli Sargeant and Felix Steffek
Categories: cs.CL cs.IR cs.LG
Comments: 36 pages, 13 figures. This work was funded by the Nuffield Foundation
  grant Access to Justice Through Artificial Intelligence. The views expressed
  are those of the authors and not necessarily of the Foundation. Visit
  www.nuffieldfoundation.org. We are grateful to Nicola Mathew for excellent
  research assistance
\\
  To undertake computational research of the law, efficiently identifying
datasets of court decisions that relate to a specific legal issue is a crucial
yet challenging endeavour. This study addresses the gap in the literature
working with large legal corpora about how to isolate cases, in our case
summary judgments, from a large corpus of UK court decisions. We introduce a
comparative analysis of two computational methods: (1) a traditional natural
language processing-based approach leveraging expert-generated keywords and
logical operators and (2) an innovative application of the Claude 2 large
language model to classify cases based on content-specific prompts. We use the
Cambridge Law Corpus of 356,011 UK court decisions and determine that the large
language model achieves a weighted F1 score of 0.94 versus 0.78 for keywords.
Despite iterative refinement, the search logic based on keywords fails to
capture nuances in legal language. We identify and extract 3,102 summary
judgment cases, enabling us to map their distribution across various UK courts
over a temporal span. The paper marks a pioneering step in employing advanced
natural language processing to tackle core legal research tasks, demonstrating
how these technologies can bridge systemic gaps and enhance the accessibility
of legal information. We share the extracted dataset metrics to support further
research on summary judgments.
\\ ( https://arxiv.org/abs/2403.04791 ,  1302kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04792
Date: Mon, 4 Mar 2024 14:01:11 GMT   (1096kb)

Title: Breaking the Language Barrier: Can Direct Inference Outperform
  Pre-Translation in Multilingual LLM Applications?
Authors: Yotam Intrator, Matan Halfon, Roman Goldenberg, Reut Tsarfaty, Matan
  Eyal, Ehud Rivlin, Yossi Matias, Natalia Aizenberg
Categories: cs.CL cs.LG
\\
  Large language models hold significant promise in multilingual applications.
However, inherent biases stemming from predominantly English-centric
pre-training have led to the widespread practice of pre-translation, i.e.,
translating non-English inputs to English before inference, leading to
complexity and information loss. This study re-evaluates the need for
pre-translation in the context of PaLM2 models (Anil et al., 2023), which have
been established as highly performant in multilingual tasks. We offer a
comprehensive investigation across 108 languages and 6 diverse benchmarks,
including open-end generative tasks, which were excluded from previous similar
studies. Our findings challenge the pre-translation paradigm established in
prior research, highlighting the advantages of direct inference in PaLM2.
Specifically, PaLM2-L consistently outperforms pre-translation in 94 out of 108
languages. These findings pave the way for more efficient and effective
multilingual applications, alleviating the limitations associated with
pre-translation and unlocking linguistic authenticity.
\\ ( https://arxiv.org/abs/2403.04792 ,  1096kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04795
Date: Mon, 4 Mar 2024 16:18:36 GMT   (534kb)

Title: Large Language Models in Fire Engineering: An Examination of Technical
  Questions Against Domain Knowledge
Authors: Haley Hostetter, M.Z. Naser, Xinyan Huang, John Gales
Categories: cs.CL cs.AI cs.LG
\\
  This communication presents preliminary findings from comparing two recent
chatbots, OpenAI's ChatGPT and Google's Bard, in the context of fire
engineering by evaluating their responses in handling fire safety related
queries. A diverse range of fire engineering questions and scenarios were
created and examined, including structural fire design, fire prevention
strategies, evacuation, building code compliance, and fire suppression systems
(some of which resemble those commonly present in the Fire Protection exam
(FPE)). The results reveal some key differences in the performance of the
chatbots, with ChatGPT demonstrating a relatively superior performance. Then,
this communication highlights the potential for chatbot technology to
revolutionize fire engineering practices by providing instant access to
critical information while outlining areas for further improvement and
research. Evidently, and when it matures, this technology will likely be
elemental to our engineers' practice and education.
\\ ( https://arxiv.org/abs/2403.04795 ,  534kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04797
Date: Tue, 5 Mar 2024 04:58:37 GMT   (1822kb,D)

Title: Found in the Middle: How Language Models Use Long Contexts Better via
  Plug-and-Play Positional Encoding
Authors: Zhenyu Zhang, Runjin Chen, Shiwei Liu, Zhewei Yao, Olatunji Ruwase,
  Beidi Chen, Xiaoxia Wu, Zhangyang Wang
Categories: cs.CL cs.LG
\\
  This paper aims to overcome the "lost-in-the-middle" challenge of large
language models (LLMs). While recent advancements have successfully enabled
LLMs to perform stable language modeling with up to 4 million tokens, the
persistent difficulty faced by most LLMs in identifying relevant information
situated in the middle of the context has not been adequately tackled. To
address this problem, this paper introduces Multi-scale Positional Encoding
(Ms-PoE) which is a simple yet effective plug-and-play approach to enhance the
capacity of LLMs to handle the relevant information located in the middle of
the context, without fine-tuning or introducing any additional overhead. Ms-PoE
leverages the position indice rescaling to relieve the long-term decay effect
introduced by RoPE, while meticulously assigning distinct scaling ratios to
different attention heads to preserve essential knowledge learned during the
pre-training step, forming a multi-scale context fusion from short to long
distance. Extensive experiments with a wide range of LLMs demonstrate the
efficacy of our approach. Notably, Ms-PoE achieves an average accuracy gain of
up to 3.8 on the Zero-SCROLLS benchmark over the original LLMs. Code are
available at https://github.com/VITA-Group/Ms-PoE.
\\ ( https://arxiv.org/abs/2403.04797 ,  1822kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04798
Date: Tue, 5 Mar 2024 12:07:18 GMT   (4062kb,D)

Title: JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using
  in-context learning with GPT and instruction-tuned Llama models
Authors: Arefa, Mohammed Abbas Ansari, Chandni Saxena, Tanvir Ahmad
Categories: cs.CL cs.LG
Comments: Paper submitted to SemEval 2024
\\
  This paper presents our system development for SemEval-2024 Task 3: "The
Competition of Multimodal Emotion Cause Analysis in Conversations". Effectively
capturing emotions in human conversations requires integrating multiple
modalities such as text, audio, and video. However, the complexities of these
diverse modalities pose challenges for developing an efficient multimodal
emotion cause analysis (ECA) system. Our proposed approach addresses these
challenges by a two-step framework. We adopt two different approaches in our
implementation. In Approach 1, we employ instruction-tuning with two separate
Llama 2 models for emotion and cause prediction. In Approach 2, we use GPT-4V
for conversation-level video description and employ in-context learning with
annotated conversation using GPT 3.5. Our system wins rank 4, and system
ablation experiments demonstrate that our proposed solutions achieve
significant performance gains. All the experimental codes are available on
Github.
\\ ( https://arxiv.org/abs/2403.04798 ,  4062kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04799
Date: Tue, 5 Mar 2024 12:27:28 GMT   (275kb)

Title: AI Literacy in Low-Resource Languages:Insights from creating AI in
  Yoruba videos
Authors: Wuraola Oyewusi
Categories: cs.CL cs.AI cs.CY
Comments: Accepted at the Global AI Cultures Workshop, ICLR 2024
\\
  To effectively navigate the AI revolution, AI literacy is crucial. However,
content predominantly exists in dominant languages, creating a gap for
low-resource languages like Yoruba (41 million native speakers). This case
study explores bridging this gap by creating and distributing AI videos in
Yoruba.The project developed 26 videos covering foundational, intermediate, and
advanced AI concepts, leveraging storytelling and accessible explanations.
These videos were created using a cost-effective methodology and distributed
across YouTube, LinkedIn, and Twitter, reaching an estimated global audience of
22 countries. Analysis of YouTube reveals insights into viewing patterns, with
the 25-44 age group contributing the most views. Notably, over half of the
traffic originated from external sources, highlighting the potential of
cross-platform promotion.This study demonstrates the feasibility and impact of
creating AI literacy content in low-resource languages. It emphasizes that
accurate interpretation requires both technical expertise in AI and fluency in
the target language. This work contributes a replicable methodology, a 22-word
Yoruba AI vocabulary, and data-driven insights into audience demographics and
acquisition channel
\\ ( https://arxiv.org/abs/2403.04799 ,  275kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04801
Date: Tue, 5 Mar 2024 19:32:01 GMT   (1898kb,D)

Title: Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs
Authors: Aly M. Kassem, Omar Mahmoud, Niloofar Mireshghallah, Hyunwoo Kim,
  Yulia Tsvetkov, Yejin Choi, Sherif Saad, Santu Rana
Categories: cs.CL
\\
  In this paper, we introduce a black-box prompt optimization method that uses
an attacker LLM agent to uncover higher levels of memorization in a victim
agent, compared to what is revealed by prompting the target model with the
training data directly, which is the dominant approach of quantifying
memorization in LLMs. We use an iterative rejection-sampling optimization
process to find instruction-based prompts with two main characteristics: (1)
minimal overlap with the training data to avoid presenting the solution
directly to the model, and (2) maximal overlap between the victim model's
output and the training data, aiming to induce the victim to spit out training
data. We observe that our instruction-based prompts generate outputs with 23.7%
higher overlap with training data compared to the baseline prefix-suffix
measurements. Our findings show that (1) instruction-tuned models can expose
pre-training data as much as their base-models, if not more so, (2) contexts
other than the original training data can lead to leakage, and (3) using
instructions proposed by other LLMs can open a new avenue of automated attacks
that we should further study and explore. The code can be found at
https://github.com/Alymostafa/Instruction_based_attack .
\\ ( https://arxiv.org/abs/2403.04801 ,  1898kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04814
Date: Thu, 7 Mar 2024 05:05:56 GMT   (245kb,D)

Title: Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks
Authors: Linyuan Gong, Sida Wang, Mostafa Elhoushi, Alvin Cheung
Categories: cs.CL cs.AI cs.LG cs.SE
\\
  We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for
evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM)
task. This benchmark focuses on syntax-aware completions of program structures
such as code blocks and conditional expressions, and includes 17,720 examples
from multiple programming languages, sourced from recent code submissions after
April 2022 to minimize data contamination. SAFIM provides a robust framework
with various prompt designs and novel syntax-aware post-processing techniques,
facilitating accurate and fair comparisons across LLMs. Our comprehensive
evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM
proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our
findings challenge conventional beliefs and suggest that pretraining methods
and data quality have more impact than model size. SAFIM thus serves as a
foundational platform for future research in effective pretraining strategies
for code LLMs. The evaluation toolkit and dataset are available at
https://github.com/gonglinyuan/safim, and the leaderboard is available at
https://safimbenchmark.com.
\\ ( https://arxiv.org/abs/2403.04814 ,  245kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04819
Date: Thu, 7 Mar 2024 13:53:03 GMT   (901kb,D)

Title: Automating the Information Extraction from Semi-Structured Interview
  Transcripts
Authors: Angelina Parfenova
Categories: cs.CL cs.CY cs.IR cs.SI
Comments: Accepted by WebConf (WWW'2024)
DOI: 10.1145/3589335.3651230
\\
  This paper explores the development and application of an automated system
designed to extract information from semi-structured interview transcripts.
Given the labor-intensive nature of traditional qualitative analysis methods,
such as coding, there exists a significant demand for tools that can facilitate
the analysis process. Our research investigates various topic modeling
techniques and concludes that the best model for analyzing interview texts is a
combination of BERT embeddings and HDBSCAN clustering. We present a
user-friendly software prototype that enables researchers, including those
without programming skills, to efficiently process and visualize the thematic
structure of interview data. This tool not only facilitates the initial stages
of qualitative analysis but also offers insights into the interconnectedness of
topics revealed, thereby enhancing the depth of qualitative analysis.
\\ ( https://arxiv.org/abs/2403.04819 ,  901kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04858
Date: Thu, 7 Mar 2024 19:15:40 GMT   (8996kb,D)

Title: Evaluating Biases in Context-Dependent Health Questions
Authors: Sharon Levy, Tahilin Sanchez Karver, William D. Adler, Michelle R.
  Kaufman, Mark Dredze
Categories: cs.CL
\\
  Chat-based large language models have the opportunity to empower individuals
lacking high-quality healthcare access to receive personalized information
across a variety of topics. However, users may ask underspecified questions
that require additional context for a model to correctly answer. We study how
large language model biases are exhibited through these contextual questions in
the healthcare domain. To accomplish this, we curate a dataset of sexual and
reproductive healthcare questions that are dependent on age, sex, and location
attributes. We compare models' outputs with and without demographic context to
determine group alignment among our contextual questions. Our experiments
reveal biases in each of these attributes, where young adult female users are
favored.
\\ ( https://arxiv.org/abs/2403.04858 ,  8996kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04872
Date: Thu, 7 Mar 2024 19:46:03 GMT   (479kb,D)

Title: Code-Mixed Probes Show How Pre-Trained Models Generalise On
  Code-Switched Text
Authors: Frances A. Laureano De Leon, Harish Tayyar Madabushi, Mark Lee
Categories: cs.CL
Comments: Accepted for publication at Joint International Conference on
  Computational Linguistics, Language Resources and Evaluation (LREC-COLING
  2024). Data and code available at
  https://github.com/francesita/code-mixed-probes
\\
  Code-switching is a prevalent linguistic phenomenon in which multilingual
individuals seamlessly alternate between languages. Despite its widespread use
online and recent research trends in this area, research in code-switching
presents unique challenges, primarily stemming from the scarcity of labelled
data and available resources. In this study we investigate how pre-trained
Language Models handle code-switched text in three dimensions: a) the ability
of PLMs to detect code-switched text, b) variations in the structural
information that PLMs utilise to capture code-switched text, and c) the
consistency of semantic information representation in code-switched text. To
conduct a systematic and controlled evaluation of the language models in
question, we create a novel dataset of well-formed naturalistic code-switched
text along with parallel translations into the source languages. Our findings
reveal that pre-trained language models are effective in generalising to
code-switched text, shedding light on the abilities of these models to
generalise representations to CS corpora. We release all our code and data
including the novel corpus at https://github.com/francesita/code-mixed-probes.
\\ ( https://arxiv.org/abs/2403.04872 ,  479kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04890
Date: Thu, 7 Mar 2024 20:48:40 GMT   (9608kb,AD)

Title: Few shot chain-of-thought driven reasoning to prompt LLMs for open ended
  medical question answering
Authors: Ojas Gramopadhye, Saeel Sandeep Nachane, Prateek Chanda, Ganesh
  Ramakrishnan, Kshitij Sharad Jadhav, Yatin Nandwani, Dinesh Raghu, Sachindra
  Joshi
Categories: cs.CL
\\
  Large Language models (LLMs) have demonstrated significant potential in
transforming healthcare by automating tasks such as clinical documentation,
information retrieval, and decision support. In this aspect, carefully
engineered prompts have emerged as a powerful tool for using LLMs for medical
scenarios, e.g., patient clinical scenarios. In this paper, we propose a
modified version of the MedQA-USMLE dataset, which is subjective, to mimic
real-life clinical scenarios. We explore the Chain of Thought (CoT) reasoning
based on subjective response generation for the modified MedQA-USMLE dataset
with appropriate LM-driven forward reasoning for correct responses to the
medical questions. Keeping in mind the importance of response verification in
the medical setting, we utilize a reward training mechanism whereby the
language model also provides an appropriate verified response for a particular
response to a clinical question. In this regard, we also include
human-in-the-loop for different evaluation aspects. We develop better
in-contrast learning strategies by modifying the 5-shot-codex-CoT-prompt from
arXiv:2207.08143 for the subjective MedQA dataset and developing our
incremental-reasoning prompt. Our evaluations show that the incremental
reasoning prompt performs better than the modified codex prompt in certain
scenarios. We also show that greedy decoding with the incremental reasoning
method performs better than other strategies, such as prompt chaining and
eliminative reasoning.
\\ ( https://arxiv.org/abs/2403.04890 ,  9608kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04894
Date: Thu, 7 Mar 2024 20:58:04 GMT   (8360kb,D)

Title: ConstitutionalExperts: Training a Mixture of Principle-based Prompts
Authors: Savvas Petridis, Ben Wedin, Ann Yuan, James Wexler, Nithum Thain
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) are highly capable at a variety of tasks given
the right prompt, but writing one is still a difficult and tedious process. In
this work, we introduce ConstitutionalExperts, a method for learning a prompt
consisting of constitutional principles (i.e. rules), given a training dataset.
Unlike prior methods that optimize the prompt as a single entity, our method
incrementally improves the prompt by surgically editing individual principles.
We also show that we can improve overall performance by learning unique prompts
for different semantic regions of the training data and using a
mixture-of-experts (MoE) architecture to route inputs at inference time. We
compare our method to other state of the art prompt-optimization techniques
across six benchmark datasets. We also investigate whether MoE improves these
other techniques. Our results suggest that ConstitutionalExperts outperforms
other prompt optimization techniques by 10.9% (F1) and that mixture-of-experts
improves all techniques, suggesting its broad applicability.
\\ ( https://arxiv.org/abs/2403.04894 ,  8360kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04945
Date: Thu, 7 Mar 2024 23:20:56 GMT   (4140kb,D)

Title: Electrocardiogram Instruction Tuning for Report Generation
Authors: Zhongwei Wan, Che Liu, Xin Wang, Chaofan Tao, Hui Shen, Zhenwu Peng,
  Jie Fu, Rossella Arcucci, Huaxiu Yao, Mi Zhang
Categories: cs.CL cs.LG eess.SP
Comments: Under review
\\
  Electrocardiogram (ECG) serves as the primary non-invasive diagnostic tool
for cardiac conditions monitoring, are crucial in assisting clinicians. Recent
studies have concentrated on classifying cardiac conditions using ECG data but
have overlooked ECG report generation, which is not only time-consuming but
also requires clinical expertise. To automate ECG report generation and ensure
its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT)
framework, the \textit{first} attempt to tackle ECG report generation with LLMs
and multimodal instructions. To facilitate future research, we establish a
benchmark to evaluate MEIT with various LLMs backbones across two large-scale
ECG datasets. Our approach uniquely aligns the representations of the ECG
signal and the report, and we conduct extensive experiments to benchmark MEIT
with nine open source LLMs, using more than 800,000 ECG reports. MEIT's results
underscore the superior performance of instruction-tuned LLMs, showcasing their
proficiency in quality report generation, zero-shot capabilities, and
resilience to signal perturbation. These findings emphasize the efficacy of our
MEIT framework and its potential for real-world clinical application.
\\ ( https://arxiv.org/abs/2403.04945 ,  4140kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04963
Date: Fri, 8 Mar 2024 00:19:24 GMT   (2430kb,D)

Title: An In-depth Evaluation of GPT-4 in Sentence Simplification with
  Error-based Human Assessment
Authors: Xuanxin Wu and Yuki Arase
Categories: cs.CL cs.AI
\\
  Sentence simplification, which rewrites a sentence to be easier to read and
understand, is a promising technique to help people with various reading
difficulties. With the rise of advanced large language models (LLMs),
evaluating their performance in sentence simplification has become imperative.
Recent studies have used both automatic metrics and human evaluations to assess
the simplification abilities of LLMs. However, the suitability of existing
evaluation methodologies for LLMs remains in question. First, the suitability
of current automatic metrics on LLMs' simplification evaluation is still
uncertain. Second, current human evaluation approaches in sentence
simplification often fall into two extremes: they are either too superficial,
failing to offer a clear understanding of the models' performance, or overly
detailed, making the annotation process complex and prone to inconsistency,
which in turn affects the evaluation's reliability. To address these problems,
this study provides in-depth insights into LLMs' performance while ensuring the
reliability of the evaluation. We design an error-based human annotation
framework to assess the GPT-4's simplification capabilities. Results show that
GPT-4 generally generates fewer erroneous simplification outputs compared to
the current state-of-the-art. However, LLMs have their limitations, as seen in
GPT-4's struggles with lexical paraphrasing. Furthermore, we conduct
meta-evaluations on widely used automatic metrics using our human annotations.
We find that while these metrics are effective for significant quality
differences, they lack sufficient sensitivity to assess the overall
high-quality simplification by GPT-4.
\\ ( https://arxiv.org/abs/2403.04963 ,  2430kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04997
Date: Fri, 8 Mar 2024 02:24:27 GMT   (5729kb,D)

Title: DiffChat: Learning to Chat with Text-to-Image Synthesis Models for
  Interactive Image Creation
Authors: Jiapeng Wang, Chengyu Wang, Tingfeng Cao, Jun Huang, Lianwen Jin
Categories: cs.CL cs.CV
\\
  We present DiffChat, a novel method to align Large Language Models (LLMs) to
"chat" with prompt-as-input Text-to-Image Synthesis (TIS) models (e.g., Stable
Diffusion) for interactive image creation. Given a raw prompt/image and a
user-specified instruction, DiffChat can effectively make appropriate
modifications and generate the target prompt, which can be leveraged to create
the target image of high quality. To achieve this, we first collect an
instruction-following prompt engineering dataset named InstructPE for the
supervised training of DiffChat. Next, we propose a reinforcement learning
framework with the feedback of three core criteria for image creation, i.e.,
aesthetics, user preference, and content integrity. It involves an action-space
dynamic modification technique to obtain more relevant positive samples and
harder negative samples during the off-policy sampling. Content integrity is
also introduced into the value estimation function for further improvement of
produced images. Our method can exhibit superior performance than baseline
models and strong competitors based on both automatic and human evaluations,
which fully demonstrates its effectiveness.
\\ ( https://arxiv.org/abs/2403.04997 ,  5729kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05004
Date: Fri, 8 Mar 2024 03:03:20 GMT   (38kb,D)

Title: Can't Remember Details in Long Documents? You Need Some R&R
Authors: Devanshu Agrawal, Shang Gao, Martin Gajek
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: 13 pages, 1 figure, 9 tables. For associated code repository see
  https://github.com/casetext/r-and-r
\\
  Long-context large language models (LLMs) hold promise for tasks such as
question-answering (QA) over long documents, but they tend to miss important
information in the middle of context documents (arXiv:2307.03172v3). Here, we
introduce $\textit{R&R}$ -- a combination of two novel prompt-based methods
called $\textit{reprompting}$ and $\textit{in-context retrieval}$ (ICR) -- to
alleviate this effect in document-based QA. In reprompting, we repeat the
prompt instructions periodically throughout the context document to remind the
LLM of its original task. In ICR, rather than instructing the LLM to answer the
question directly, we instruct it to retrieve the top $k$ passage numbers most
relevant to the given question, which are then used as an abbreviated context
in a second QA prompt. We test R&R with GPT-4 Turbo and Claude-2.1 on documents
up to 80k tokens in length and observe a 16-point boost in QA accuracy on
average. Our further analysis suggests that R&R improves performance on long
document-based QA because it reduces the distance between relevant context and
the instructions. Finally, we show that compared to short-context chunkwise
methods, R&R enables the use of larger chunks that cost fewer LLM calls and
output tokens, while minimizing the drop in accuracy.
\\ ( https://arxiv.org/abs/2403.05004 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05020
Date: Fri, 8 Mar 2024 03:49:17 GMT   (1312kb,D)

Title: Is this the real life? Is this just fantasy? The Misleading Success of
  Simulating Social Interactions With LLMs
Authors: Xuhui Zhou, Zhe Su, Tiwalayo Eisape, Hyunwoo Kim, Maarten Sap
Categories: cs.CL cs.AI
\\
  Recent advances in large language models (LLM) have enabled richer social
simulations, allowing for the study of various social phenomena with LLM-based
agents. However, most work has used an omniscient perspective on these
simulations (e.g., single LLM to generate all interlocutors), which is
fundamentally at odds with the non-omniscient, information asymmetric
interactions that humans have. To examine these differences, we develop an
evaluation framework to simulate social interactions with LLMs in various
settings (omniscient, non-omniscient). Our experiments show that interlocutors
simulated omnisciently are much more successful at accomplishing social goals
compared to non-omniscient agents, despite the latter being the more realistic
setting. Furthermore, we demonstrate that learning from omniscient simulations
improves the apparent naturalness of interactions but scarcely enhances goal
achievement in cooperative scenarios. Our findings indicate that addressing
information asymmetry remains a fundamental challenge for LLM-based agents.
\\ ( https://arxiv.org/abs/2403.05020 ,  1312kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05023
Date: Fri, 8 Mar 2024 03:55:27 GMT   (1345kb,D)

Title: Towards Multimodal Sentiment Analysis Debiasing via Bias Purification
Authors: Dingkang Yang, Mingcheng Li, Dongling Xiao, Yang Liu, Kun Yang, Zhaoyu
  Chen, Yuzheng Wang, Peng Zhai, Ke Li, Lihua Zhang
Categories: cs.CL cs.CV
Comments: 14 pages
\\
  Multimodal Sentiment Analysis (MSA) aims to understand human intentions by
integrating emotion-related clues from diverse modalities, such as visual,
language, and audio. Unfortunately, the current MSA task invariably suffers
from unplanned dataset biases, particularly multimodal utterance-level label
bias and word-level context bias. These harmful biases potentially mislead
models to focus on statistical shortcuts and spurious correlations, causing
severe performance bottlenecks. To alleviate these issues, we present a
Multimodal Counterfactual Inference Sentiment (MCIS) analysis framework based
on causality rather than conventional likelihood. Concretely, we first
formulate a causal graph to discover harmful biases from already-trained
vanilla models. In the inference phase, given a factual multimodal input, MCIS
imagines two counterfactual scenarios to purify and mitigate these biases.
Then, MCIS can make unbiased decisions from biased observations by comparing
factual and counterfactual outcomes. We conduct extensive experiments on
several standard MSA benchmarks. Qualitative and quantitative results show the
effectiveness of the proposed framework.
\\ ( https://arxiv.org/abs/2403.05023 ,  1345kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05045
Date: Fri, 8 Mar 2024 04:44:25 GMT   (6333kb,D)

Title: Are Human Conversations Special? A Large Language Model Perspective
Authors: Toshish Jawale and Chaitanya Animesh and Sekhar Vallath and Kartik
  Talamadupula and Larry Heck
Categories: cs.CL cs.AI cs.LG
\\
  This study analyzes changes in the attention mechanisms of large language
models (LLMs) when used to understand natural conversations between humans
(human-human). We analyze three use cases of LLMs: interactions over web
content, code, and mathematical texts. By analyzing attention distance,
dispersion, and interdependency across these domains, we highlight the unique
challenges posed by conversational data. Notably, conversations require nuanced
handling of long-term contextual relationships and exhibit higher complexity
through their attention patterns. Our findings reveal that while language
models exhibit domain-specific attention behaviors, there is a significant gap
in their ability to specialize in human conversations. Through detailed
attention entropy analysis and t-SNE visualizations, we demonstrate the need
for models trained with a diverse array of high-quality conversational data to
enhance understanding and generation of human-like dialogue. This research
highlights the importance of domain specialization in language models and
suggests pathways for future advancement in modeling human conversational
nuances.
\\ ( https://arxiv.org/abs/2403.05045 ,  6333kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05065
Date: Fri, 8 Mar 2024 05:34:29 GMT   (9068kb,D)

Title: Can we obtain significant success in RST discourse parsing by using
  Large Language Models?
Authors: Aru Maekawa, Tsutomu Hirao, Hidetaka Kamigaito, Manabu Okumura
Categories: cs.CL
Comments: Accepted in the main conference of EACL 2024
\\
  Recently, decoder-only pre-trained large language models (LLMs), with several
tens of billion parameters, have significantly impacted a wide range of natural
language processing (NLP) tasks. While encoder-only or encoder-decoder
pre-trained language models have already proved to be effective in discourse
parsing, the extent to which LLMs can perform this task remains an open
research question. Therefore, this paper explores how beneficial such LLMs are
for Rhetorical Structure Theory (RST) discourse parsing. Here, the parsing
process for both fundamental top-down and bottom-up strategies is converted
into prompts, which LLMs can work with. We employ Llama 2 and fine-tune it with
QLoRA, which has fewer parameters that can be tuned. Experimental results on
three benchmark datasets, RST-DT, Instr-DT, and the GUM corpus, demonstrate
that Llama 2 with 70 billion parameters in the bottom-up strategy obtained
state-of-the-art (SOTA) results with significant differences. Furthermore, our
parsers demonstrated generalizability when evaluated on RST-DT, showing that,
in spite of being trained with the GUM corpus, it obtained similar performances
to those of existing parsers trained with RST-DT.
\\ ( https://arxiv.org/abs/2403.05065 ,  9068kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05101
Date: Fri, 8 Mar 2024 07:06:43 GMT   (471kb,D)

Title: Rule-driven News Captioning
Authors: Ning Xu, Tingting Zhang, Hongshuo Tian, Yongdong Zhang, An-An Liu
Categories: cs.CL cs.AI
\\
  News captioning task aims to generate sentences by describing named entities
or concrete events for an image with its news article. Existing methods have
achieved remarkable results by relying on the large-scale pre-trained models,
which primarily focus on the correlations between the input news content and
the output predictions. However, the news captioning requires adhering to some
fundamental rules of news reporting, such as accurately describing the
individuals and actions associated with the event. In this paper, we propose
the rule-driven news captioning method, which can generate image descriptions
following designated rule signal. Specifically, we first design the news-aware
semantic rule for the descriptions. This rule incorporates the primary action
depicted in the image (e.g., "performing") and the roles played by named
entities involved in the action (e.g., "Agent" and "Place"). Second, we inject
this semantic rule into the large-scale pre-trained model, BART, with the
prefix-tuning strategy, where multiple encoder layers are embedded with
news-aware semantic rule. Finally, we can effectively guide BART to generate
news sentences that comply with the designated rule. Extensive experiments on
two widely used datasets (i.e., GoodNews and NYTimes800k) demonstrate the
effectiveness of our method.
\\ ( https://arxiv.org/abs/2403.05101 ,  471kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05132
Date: Fri, 8 Mar 2024 07:59:19 GMT   (1379kb,D)

Title: ChatUIE: Exploring Chat-based Unified Information Extraction using Large
  Language Models
Authors: Jun Xu, Mengshu Sun, Zhiqiang Zhang and Jun Zhou
Categories: cs.CL cs.AI
Comments: Accepted by LREC-COLING 2024
\\
  Recent advancements in large language models have shown impressive
performance in general chat. However, their domain-specific capabilities,
particularly in information extraction, have certain limitations. Extracting
structured information from natural language that deviates from known schemas
or instructions has proven challenging for previous prompt-based methods. This
motivated us to explore domain-specific modeling in chat-based language models
as a solution for extracting structured information from natural language. In
this paper, we present ChatUIE, an innovative unified information extraction
framework built upon ChatGLM. Simultaneously, reinforcement learning is
employed to improve and align various tasks that involve confusing and limited
samples. Furthermore, we integrate generation constraints to address the issue
of generating elements that are not present in the input. Our experimental
results demonstrate that ChatUIE can significantly improve the performance of
information extraction with a slight decrease in chatting ability.
\\ ( https://arxiv.org/abs/2403.05132 ,  1379kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05152
Date: Fri, 8 Mar 2024 08:41:14 GMT   (483kb)

Title: Towards a Psychology of Machines: Large Language Models Predict Human
  Memory
Authors: Markus Huff and Elanur Ulak\c{c}{\i}
Categories: cs.CL cs.AI
Comments: 32 pages, 3 figures, 2 tables
\\
  Large language models (LLMs) are demonstrating remarkable capabilities across
various tasks despite lacking a foundation in human cognition. This raises the
question: can these models, beyond simply mimicking human language patterns,
offer insights into the mechanisms underlying human cognition? This study
explores the ability of ChatGPT to predict human performance in a
language-based memory task. Building upon theories of text comprehension, we
hypothesize that recognizing ambiguous sentences (e.g., "Because Bill drinks
wine is never kept in the house") is facilitated by preceding them with
contextually relevant information. Participants, both human and ChatGPT, were
presented with pairs of sentences. The second sentence was always a garden-path
sentence designed to be inherently ambiguous, while the first sentence either
provided a fitting (e.g., "Bill has chronic alcoholism") or an unfitting
context (e.g., "Bill likes to play golf"). We measured both human's and
ChatGPT's ratings of sentence relatedness, ChatGPT's memorability ratings for
the garden-path sentences, and humans' spontaneous memory for the garden-path
sentences. The results revealed a striking alignment between ChatGPT's
assessments and human performance. Sentences deemed more related and assessed
as being more memorable by ChatGPT were indeed better remembered by humans,
even though ChatGPT's internal mechanisms likely differ significantly from
human cognition. This finding, which was confirmed with a robustness check
employing synonyms, underscores the potential of generative AI models to
predict human performance accurately. We discuss the broader implications of
these findings for leveraging LLMs in the development of psychological theories
and for gaining a deeper understanding of human cognition.
\\ ( https://arxiv.org/abs/2403.05152 ,  483kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05186
Date: Fri, 8 Mar 2024 09:54:56 GMT   (8016kb,D)

Title: ROUGE-K: Do Your Summaries Have Keywords?
Authors: Sotaro Takeshita, Simone Paolo Ponzetto, Kai Eckert
Categories: cs.CL
\\
  Keywords, that is, content-relevant words in summaries play an important role
in efficient information conveyance, making it critical to assess if
system-generated summaries contain such informative words during evaluation.
However, existing evaluation metrics for extreme summarization models do not
pay explicit attention to keywords in summaries, leaving developers ignorant of
their presence. To address this issue, we present a keyword-oriented evaluation
metric, dubbed ROUGE-K, which provides a quantitative answer to the question of
-- \textit{How well do summaries include keywords?} Through the lens of this
keyword-aware metric, we surprisingly find that a current strong baseline model
often misses essential information in their summaries. Our analysis reveals
that human annotators indeed find the summaries with more keywords to be more
relevant to the source documents. This is an important yet previously
overlooked aspect in evaluating summarization systems. Finally, to enhance
keyword inclusion, we propose four approaches for incorporating word importance
into a transformer-based model and experimentally show that it enables guiding
models to include more keywords while keeping the overall quality. Our code is
released at https://github.com/sobamchan/rougek.
\\ ( https://arxiv.org/abs/2403.05186 ,  8016kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05188
Date: Fri, 8 Mar 2024 09:56:45 GMT   (280kb,D)

Title: CommitBench: A Benchmark for Commit Message Generation
Authors: Maximilian Schall, Tamara Czinczoll, Gerard de Melo
Categories: cs.CL cs.SE
Comments: Submitted and accepted at SANER 2024
\\
  Writing commit messages is a tedious daily task for many software developers,
and often remains neglected. Automating this task has the potential to save
time while ensuring that messages are informative. A high-quality dataset and
an objective benchmark are vital preconditions for solid research and
evaluation towards this goal. We show that existing datasets exhibit various
problems, such as the quality of the commit selection, small sample sizes,
duplicates, privacy issues, and missing licenses for redistribution. This can
lead to unusable models and skewed evaluations, where inferior models achieve
higher evaluation scores due to biases in the data. We compile a new
large-scale dataset, CommitBench, adopting best practices for dataset creation.
We sample commits from diverse projects with licenses that permit
redistribution and apply our filtering and dataset enhancements to improve the
quality of generated commit messages. We use CommitBench to compare existing
models and show that other approaches are outperformed by a Transformer model
pretrained on source code. We hope to accelerate future research by publishing
the source code( https://github.com/Maxscha/commitbench ).
\\ ( https://arxiv.org/abs/2403.05188 ,  280kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05189
Date: Fri, 8 Mar 2024 10:09:57 GMT   (244kb,D)

Title: Tracing the Roots of Facts in Multilingual Language Models: Independent,
  Shared, and Transferred Knowledge
Authors: Xin Zhao, Naoki Yoshinaga, Daisuke Oba
Categories: cs.CL cs.AI
Comments: EACL 2024 main conference
\\
  Acquiring factual knowledge for language models (LMs) in low-resource
languages poses a serious challenge, thus resorting to cross-lingual transfer
in multilingual LMs (ML-LMs). In this study, we ask how ML-LMs acquire and
represent factual knowledge. Using the multilingual factual knowledge probing
dataset, mLAMA, we first conducted a neuron investigation of ML-LMs
(specifically, multilingual BERT). We then traced the roots of facts back to
the knowledge source (Wikipedia) to identify the ways in which ML-LMs acquire
specific facts. We finally identified three patterns of acquiring and
representing facts in ML-LMs: language-independent, cross-lingual shared and
transferred, and devised methods for differentiating them. Our findings
highlight the challenge of maintaining consistent factual knowledge across
languages, underscoring the need for better fact representation learning in
ML-LMs.
\\ ( https://arxiv.org/abs/2403.05189 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05216
Date: Fri, 8 Mar 2024 11:00:09 GMT   (206kb,D)

Title: SocialPET: Socially Informed Pattern Exploiting Training for Few-Shot
  Stance Detection in Social Media
Authors: Parisa Jamadi Khiabani, Arkaitz Zubiaga
Categories: cs.CL cs.SI
\\
  Stance detection, as the task of determining the viewpoint of a social media
post towards a target as 'favor' or 'against', has been understudied in the
challenging yet realistic scenario where there is limited labeled data for a
certain target. Our work advances research in few-shot stance detection by
introducing SocialPET, a socially informed approach to leveraging language
models for the task. Our proposed approach builds on the Pattern Exploiting
Training (PET) technique, which addresses classification tasks as cloze
questions through the use of language models. To enhance the approach with
social awareness, we exploit the social network structure surrounding social
media posts. We prove the effectiveness of SocialPET on two stance datasets,
Multi-target and P-Stance, outperforming competitive stance detection models as
well as the base model, PET, where the labeled instances for the target under
study is as few as 100. When we delve into the results, we observe that
SocialPET is comparatively strong in identifying instances of the `against'
class, where baseline models underperform.
\\ ( https://arxiv.org/abs/2403.05216 ,  206kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05217
Date: Fri, 8 Mar 2024 11:09:13 GMT   (816kb,D)

Title: Harnessing Multi-Role Capabilities of Large Language Models for
  Open-Domain Question Answering
Authors: Hongda Sun, Yuxuan Liu, Chengwei Wu, Haiyu Yan, Cheng Tai, Xin Gao,
  Shuo Shang, Rui Yan
Categories: cs.CL cs.AI cs.IR
Comments: TheWebConf 2024 (WWW 2024) oral, code repo:
  https://github.com/EthanLeo-LYX/LLMQA
\\
  Open-domain question answering (ODQA) has emerged as a pivotal research
spotlight in information systems. Existing methods follow two main paradigms to
collect evidence: (1) The \textit{retrieve-then-read} paradigm retrieves
pertinent documents from an external corpus; and (2) the
\textit{generate-then-read} paradigm employs large language models (LLMs) to
generate relevant documents. However, neither can fully address multifaceted
requirements for evidence. To this end, we propose LLMQA, a generalized
framework that formulates the ODQA process into three basic steps: query
expansion, document selection, and answer generation, combining the superiority
of both retrieval-based and generation-based evidence. Since LLMs exhibit their
excellent capabilities to accomplish various tasks, we instruct LLMs to play
multiple roles as generators, rerankers, and evaluators within our framework,
integrating them to collaborate in the ODQA process. Furthermore, we introduce
a novel prompt optimization algorithm to refine role-playing prompts and steer
LLMs to produce higher-quality evidence and answers. Extensive experimental
results on widely used benchmarks (NQ, WebQ, and TriviaQA) demonstrate that
LLMQA achieves the best performance in terms of both answer accuracy and
evidence quality, showcasing its potential for advancing ODQA research and
applications.
\\ ( https://arxiv.org/abs/2403.05217 ,  816kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05257
Date: Fri, 8 Mar 2024 12:28:15 GMT   (790kb,D)

Title: Cross-lingual Transfer or Machine Translation? On Data Augmentation for
  Monolingual Semantic Textual Similarity
Authors: Sho Hoshino, Akihiko Kato, Soichiro Murakami, Peinan Zhang
Categories: cs.CL
Comments: LREC-COLING 2024
\\
  Learning better sentence embeddings leads to improved performance for natural
language understanding tasks including semantic textual similarity (STS) and
natural language inference (NLI). As prior studies leverage large-scale labeled
NLI datasets for fine-tuning masked language models to yield sentence
embeddings, task performance for languages other than English is often left
behind. In this study, we directly compared two data augmentation techniques as
potential solutions for monolingual STS: (a) cross-lingual transfer that
exploits English resources alone as training data to yield non-English sentence
embeddings as zero-shot inference, and (b) machine translation that coverts
English data into pseudo non-English training data in advance. In our
experiments on monolingual STS in Japanese and Korean, we find that the two
data techniques yield performance on par. Rather, we find a superiority of the
Wikipedia domain over the NLI domain for these languages, in contrast to prior
studies that focused on NLI as training data. Combining our findings, we
demonstrate that the cross-lingual transfer of Wikipedia data exhibits improved
performance, and that native Wikipedia data can further improve performance for
monolingual STS.
\\ ( https://arxiv.org/abs/2403.05257 ,  790kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05266
Date: Fri, 8 Mar 2024 12:42:36 GMT   (1075kb,D)

Title: ERBench: An Entity-Relationship based Automatically Verifiable
  Hallucination Benchmark for Large Language Models
Authors: Jio Oh, Soyeon Kim, Junseok Seo, Jindong Wang, Ruochen Xu, Xing Xie,
  Steven Euijong Whang
Categories: cs.CL cs.AI cs.LG
\\
  Large language models (LLMs) have achieved unprecedented performance in
various applications, yet their evaluation remains a critical issue. Existing
hallucination benchmarks are either static or lack adjustable complexity for
thorough analysis. We contend that utilizing existing relational databases is a
promising approach for constructing benchmarks due to their accurate knowledge
description via functional dependencies. We propose ERBench to automatically
convert any relational database into a benchmark based on the
entity-relationship (ER) model. Our key idea is to construct questions using
the database schema, records, and functional dependencies such that they can be
automatically verified. In addition, we use foreign key constraints to join
relations and construct multihop questions, which can be arbitrarily complex
and used to debug the intermediate answers of LLMs. Finally, ERBench supports
continuous evaluation, multimodal questions, and various prompt engineering
techniques. In our experiments, we construct an LLM benchmark using databases
of multiple domains and make an extensive comparison of contemporary LLMs. We
observe that better LLMs like GPT-4 can handle a larger variety of question
types, but are by no means perfect. Also, correct answers do not necessarily
imply correct rationales, which is an important evaluation that ERBench does
better than other benchmarks for various question types. Code is available at
https: //github.com/DILAB-KAIST/ERBench.
\\ ( https://arxiv.org/abs/2403.05266 ,  1075kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05268
Date: Fri, 8 Mar 2024 12:45:53 GMT   (339kb)

Title: Deep Prompt Multi-task Network for Abuse Language Detection
Authors: Jian Zhu, Yuping Ruan, Jingfei Chang, and Cheng Luo
Categories: cs.CL cs.LG
Comments: Submitted to the International Conference on Pattern Recognition
  (ICPR) 2024
\\
  The detection of abusive language remains a long-standing challenge with the
extensive use of social networks. The detection task of abusive language
suffers from limited accuracy. We argue that the existing detection methods
utilize the fine-tuning technique of the pre-trained language models (PLMs) to
handle downstream tasks. Hence, these methods fail to stimulate the general
knowledge of the PLMs. To address the problem, we propose a novel Deep Prompt
Multi-task Network (DPMN) for abuse language detection. Specifically, DPMN
first attempts to design two forms of deep prompt tuning and light prompt
tuning for the PLMs. The effects of different prompt lengths, tuning
strategies, and prompt initialization methods on detecting abusive language are
studied. In addition, we propose a Task Head based on Bi-LSTM and FFN, which
can be used as a short text classifier. Eventually, DPMN utilizes multi-task
learning to improve detection metrics further. The multi-task network has the
function of transferring effective knowledge. The proposed DPMN is evaluated
against eight typical methods on three public datasets: OLID, SOLID, and
AbuseAnalyzer. The experimental results show that our DPMN outperforms the
state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.05268 ,  339kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05303
Date: Fri, 8 Mar 2024 13:32:01 GMT   (518kb,D)

Title: ACLSum: A New Dataset for Aspect-based Summarization of Scientific
  Publications
Authors: Sotaro Takeshita, Tommaso Green, Ines Reinig, Kai Eckert, Simone Paolo
  Ponzetto
Categories: cs.CL
\\
  Extensive efforts in the past have been directed toward the development of
summarization datasets. However, a predominant number of these resources have
been (semi)-automatically generated, typically through web data crawling,
resulting in subpar resources for training and evaluating summarization
systems, a quality compromise that is arguably due to the substantial costs
associated with generating ground-truth summaries, particularly for diverse
languages and specialized domains. To address this issue, we present ACLSum, a
novel summarization dataset carefully crafted and evaluated by domain experts.
In contrast to previous datasets, ACLSum facilitates multi-aspect summarization
of scientific papers, covering challenges, approaches, and outcomes in depth.
Through extensive experiments, we evaluate the quality of our resource and the
performance of models based on pretrained language models and state-of-the-art
large language models (LLMs). Additionally, we explore the effectiveness of
extractive versus abstractive summarization within the scholarly domain on the
basis of automatically discovered aspects. Our results corroborate previous
findings in the general domain and indicate the general superiority of
end-to-end aspect-based summarization. Our data is released at
https://github.com/sobamchan/aclsum.
\\ ( https://arxiv.org/abs/2403.05303 ,  518kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05313
Date: Fri, 8 Mar 2024 13:42:19 GMT   (9482kb,D)

Title: RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in
  Long-Horizon Generation
Authors: Zihao Wang and Anji Liu and Haowei Lin and Jiaqi Li and Xiaojian Ma
  and Yitao Liang
Categories: cs.CL cs.AI
\\
  We explore how iterative revising a chain of thoughts with the help of
information retrieval significantly improves large language models' reasoning
and generation ability in long-horizon generation tasks, while hugely
mitigating hallucination. In particular, the proposed method --
*retrieval-augmented thoughts* (RAT) -- revises each thought step one by one
with retrieved information relevant to the task query, the current and the past
thought steps, after the initial zero-shot CoT is generated. Applying RAT to
GPT-3.5, GPT-4, and CodeLLaMA-7b substantially improves their performances on
various long-horizon generation tasks; on average of relatively increasing
rating scores by 13.63% on code generation, 16.96% on mathematical reasoning,
19.2% on creative writing, and 42.78% on embodied task planning. The demo page
can be found at https://craftjarvis.github.io/RAT
\\ ( https://arxiv.org/abs/2403.05313 ,  9482kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05326
Date: Fri, 8 Mar 2024 14:05:36 GMT   (1927kb,D)

Title: ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in
  Dialogues
Authors: Yiding Liu and Jingjing Wang and Jiaming Luo and Tao Zeng and Guodong
  Zhou
Categories: cs.CL cs.AI
\\
  Aspect Sentiment Understanding (ASU) in interactive scenarios (e.g.,
Question-Answering and Dialogue) has attracted ever-more interest in recent
years and achieved important progresses. However, existing studies on
interactive ASU largely ignore the coreference issue for opinion targets (i.e.,
aspects), while this phenomenon is ubiquitous in interactive scenarios
especially dialogues, limiting the ASU performance. Recently, large language
models (LLMs) shows the powerful ability to integrate various NLP tasks with
the chat paradigm. In this way, this paper proposes a new Chat-based Aspect
Sentiment Understanding (ChatASU) task, aiming to explore LLMs' ability in
understanding aspect sentiments in dialogue scenarios. Particularly, this
ChatASU task introduces a sub-task, i.e., Aspect Chain Reasoning (ACR) task, to
address the aspect coreference issue. On this basis, we propose a Trusted
Self-reflexion Approach (TSA) with ChatGLM as backbone to ChatASU.
Specifically, this TSA treats the ACR task as an auxiliary task to boost the
performance of the primary ASU task, and further integrates trusted learning
into reflexion mechanisms to alleviate the LLMs-intrinsic factual hallucination
problem in TSA. Furthermore, a high-quality ChatASU dataset is annotated to
evaluate TSA, and extensive experiments show that our proposed TSA can
significantly outperform several state-of-the-art baselines, justifying the
effectiveness of TSA to ChatASU and the importance of considering the
coreference and hallucination issues in ChatASU.
\\ ( https://arxiv.org/abs/2403.05326 ,  1927kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05330
Date: Fri, 8 Mar 2024 14:07:44 GMT   (305kb,D)

Title: Consecutive Model Editing with Batch alongside HooK Layers
Authors: Shuaiyi Li, Yang Deng, Deng Cai, Hongyuan Lu, Liang Chen, Wai Lam
Categories: cs.CL
Comments: Under review
\\
  As the typical retraining paradigm is unacceptably time- and
resource-consuming, researchers are turning to model editing in order to seek
an effective, consecutive, and batch-supportive way to edit the model behavior
directly. Despite all these practical expectations, existing model editing
methods fail to realize all of them. Furthermore, the memory demands for such
succession-supportive model editing approaches tend to be prohibitive,
frequently necessitating an external memory that grows incrementally over time.
To cope with these challenges, we propose COMEBA-HK, a model editing method
that is both consecutive and batch-supportive. COMEBA-HK is memory-friendly as
it only needs a small amount of it to store several hook layers with updated
weights. Experimental results demonstrate the superiority of our method over
other batch-supportive model editing methods under both single-round and
consecutive batch editing scenarios. Extensive analyses of COMEBA-HK have been
conducted to verify the stability of our method over 1) the number of
consecutive steps and 2) the number of editing instance.
\\ ( https://arxiv.org/abs/2403.05330 ,  305kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05338
Date: Fri, 8 Mar 2024 14:14:37 GMT   (1133kb,D)

Title: Explaining Pre-Trained Language Models with Attribution Scores: An
  Analysis in Low-Resource Settings
Authors: Wei Zhou, Heike Adel, Hendrik Schuff, Ngoc Thang Vu
Categories: cs.CL
\\
  Attribution scores indicate the importance of different input parts and can,
thus, explain model behaviour. Currently, prompt-based models are gaining
popularity, i.a., due to their easier adaptability in low-resource settings.
However, the quality of attribution scores extracted from prompt-based models
has not been investigated yet. In this work, we address this topic by analyzing
attribution scores extracted from prompt-based models w.r.t. plausibility and
faithfulness and comparing them with attribution scores extracted from
fine-tuned models and large language models. In contrast to previous work, we
introduce training size as another dimension into the analysis. We find that
using the prompting paradigm (with either encoder-based or decoder-based
models) yields more plausible explanations than fine-tuning the models in
low-resource settings and Shapley Value Sampling consistently outperforms
attention and Integrated Gradients in terms of leading to more plausible and
faithful explanations.
\\ ( https://arxiv.org/abs/2403.05338 ,  1133kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05365
Date: Fri, 8 Mar 2024 14:55:05 GMT   (837kb)

Title: The Impact of Quantization on the Robustness of Transformer-based Text
  Classifiers
Authors: Seyed Parsa Neshaei, Yasaman Boreshban, Gholamreza Ghassem-Sani, Seyed
  Abolghasem Mirroshandel
Categories: cs.CL cs.LG
\\
  Transformer-based models have made remarkable advancements in various NLP
areas. Nevertheless, these models often exhibit vulnerabilities when confronted
with adversarial attacks. In this paper, we explore the effect of quantization
on the robustness of Transformer-based models. Quantization usually involves
mapping a high-precision real number to a lower-precision value, aiming at
reducing the size of the model at hand. To the best of our knowledge, this work
is the first application of quantization on the robustness of NLP models. In
our experiments, we evaluate the impact of quantization on BERT and DistilBERT
models in text classification using SST-2, Emotion, and MR datasets. We also
evaluate the performance of these models against TextFooler, PWWS, and PSO
adversarial attacks. Our findings show that quantization significantly improves
(by an average of 18.68%) the adversarial accuracy of the models. Furthermore,
we compare the effect of quantization versus that of the adversarial training
approach on robustness. Our experiments indicate that quantization increases
the robustness of the model by 18.80% on average compared to adversarial
training without imposing any extra computational overhead during training.
Therefore, our results highlight the effectiveness of quantization in improving
the robustness of NLP models.
\\ ( https://arxiv.org/abs/2403.05365 ,  837kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05434
Date: Fri, 8 Mar 2024 16:37:36 GMT   (871kb)

Title: Cost-Performance Optimization for Processing Low-Resource Language Tasks
  Using Commercial LLMs
Authors: Arijit Nag, Animesh Mukherjee, Niloy Ganguly, Soumen Chakrabarti
Categories: cs.CL
\\
  Large Language Models (LLMs) exhibit impressive zero/few-shot inference and
generation quality for high-resource languages(HRLs). A few of them have been
trained in low-resource languages (LRLs) and give decent performance. Owing to
the prohibitive costs of training LLMs, they are usually used as a network
service, with the client charged by the count of input and output tokens. The
number of tokens strongly depends on the script and language, as well as the
LLM's sub-word vocabulary. We show that LRLs are at a pricing disadvantage,
because the well-known LLMs produce more tokens for LRLs than HRLs. This is
because most currently popular LLMs are optimized for HRL vocabularies. Our
objective is to level the playing field: reduce the cost of processing LRLs in
contemporary LLMs while ensuring that predictive and generative qualities are
not compromised. As means to reduce the number of tokens processed by the LLM,
we consider code-mixing, translation, and transliteration of LRLs to HRLs. We
perform an extensive study using the IndicXTREME dataset, covering 15 Indian
languages, while using GPT-4 (one of the costliest LLM services released so
far) as a commercial LLM. We observe and analyze interesting patterns involving
token count, cost,and quality across a multitude of languages and tasks. We
show that choosing the best policy to interact with the LLM can reduce cost by
90% while giving better or comparable performance, compared to communicating
with the LLM in the original LRL.
\\ ( https://arxiv.org/abs/2403.05434 ,  871kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05468
Date: Fri, 8 Mar 2024 17:30:41 GMT   (1688kb,D)

Title: Will GPT-4 Run DOOM?
Authors: Adrian de Wynter
Categories: cs.CL cs.AI cs.CV
\\
  We show that GPT-4's reasoning and planning capabilities extend to the 1993
first-person shooter Doom. This large language model (LLM) is able to run and
play the game with only a few instructions, plus a textual
description--generated by the model itself from screenshots--about the state of
the game being observed. We find that GPT-4 can play the game to a passable
degree: it is able to manipulate doors, combat enemies, and perform pathing.
More complex prompting strategies involving multiple model calls provide better
results. While further work is required to enable the LLM to play the game as
well as its classical, reinforcement learning-based counterparts, we note that
GPT-4 required no training, leaning instead on its own reasoning and
observational capabilities. We hope our work pushes the boundaries on
intelligent, LLM-based agents in video games. We conclude by discussing the
ethical implications of our work.
\\ ( https://arxiv.org/abs/2403.05468 ,  1688kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05488
Date: Fri, 8 Mar 2024 17:53:58 GMT   (832kb)

Title: FFSTC: Fongbe to French Speech Translation Corpus
Authors: D. Fortune Kponou, Frejus A. A. Laleye, Eugene C. Ezin
Categories: cs.CL
\\
  In this paper, we introduce the Fongbe to French Speech Translation Corpus
(FFSTC) for the first time. This corpus encompasses approximately 31 hours of
collected Fongbe language content, featuring both French transcriptions and
corresponding Fongbe voice recordings. FFSTC represents a comprehensive dataset
compiled through various collection methods and the efforts of dedicated
individuals. Furthermore, we conduct baseline experiments using Fairseq's
transformer_s and conformer models to evaluate data quality and validity. Our
results indicate a score of 8.96 for the transformer_s model and 8.14 for the
conformer model, establishing a baseline for the FFSTC corpus.
\\ ( https://arxiv.org/abs/2403.05488 ,  832kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05493
Date: Fri, 8 Mar 2024 18:04:03 GMT   (41kb,D)

Title: To Err Is Human, but Llamas Can Learn It Too
Authors: Agnes Luhtaru, Taido Purason, Martin Vainikko, Maksym Del, Mark Fishel
Categories: cs.CL
\\
  This study explores enhancing grammatical error correction (GEC) through
artificial error generation (AEG) using language models (LMs). Specifically, we
fine-tune Llama 2-based LMs for error generation and find that this approach
yields synthetic errors akin to human errors. Next, we train GEC Llama models
with the help of these artificial errors and outperform previous
state-of-the-art error correction models, with gains ranging between 0.8 and 6
F0.5 points across all tested languages (German, Ukrainian, and Estonian).
Moreover, we demonstrate that generating errors by fine-tuning smaller
sequence-to-sequence models and prompting large commercial LMs (GPT-3.5 and
GPT-4) also results in synthetic errors beneficially affecting error generation
models.
\\ ( https://arxiv.org/abs/2403.05493 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05518
Date: Fri, 8 Mar 2024 18:41:42 GMT   (244kb,D)

Title: Bias-Augmented Consistency Training Reduces Biased Reasoning in
  Chain-of-Thought
Authors: James Chua, Edward Rees, Hunar Batra, Samuel R. Bowman, Julian
  Michael, Ethan Perez, Miles Turpin
Categories: cs.CL cs.AI
\\
  While chain-of-thought prompting (CoT) has the potential to improve the
explainability of language model reasoning, it can systematically misrepresent
the factors influencing models' behavior--for example, rationalizing answers in
line with a user's opinion without mentioning this bias. To mitigate this
biased reasoning problem, we introduce bias-augmented consistency training
(BCT), an unsupervised fine-tuning scheme that trains models to give consistent
reasoning across prompts with and without biasing features. We construct a
suite testing nine forms of biased reasoning on seven question-answering tasks,
and find that applying BCT to GPT-3.5-Turbo with one bias reduces the rate of
biased reasoning by 86% on held-out tasks. Moreover, this model generalizes to
other forms of bias, reducing biased reasoning on held-out biases by an average
of 37%. As BCT generalizes to held-out biases and does not require gold labels,
this method may hold promise for reducing biased reasoning from as-of-yet
unknown biases and on tasks where supervision for ground truth reasoning is
unavailable.
\\ ( https://arxiv.org/abs/2403.05518 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05519
Date: Fri, 8 Mar 2024 18:42:59 GMT   (2268kb,D)

Title: Authorship Attribution in Bangla Literature (AABL) via Transfer Learning
  using ULMFiT
Authors: Aisha Khatun, Anisur Rahman, Md Saiful Islam, Hemayet Ahmed Chowdhury,
  Ayesha Tasnim
Categories: cs.CL
Comments: Accepted in ACM TALLIP August 2022
DOI: 10.1145/3530691
\\
  Authorship Attribution is the task of creating an appropriate
characterization of text that captures the authors' writing style to identify
the original author of a given piece of text. With increased anonymity on the
internet, this task has become increasingly crucial in various security and
plagiarism detection fields. Despite significant advancements in other
languages such as English, Spanish, and Chinese, Bangla lacks comprehensive
research in this field due to its complex linguistic feature and sentence
structure. Moreover, existing systems are not scalable when the number of
author increases, and the performance drops for small number of samples per
author. In this paper, we propose the use of Average-Stochastic Gradient
Descent Weight-Dropped Long Short-Term Memory (AWD-LSTM) architecture and an
effective transfer learning approach that addresses the problem of complex
linguistic features extraction and scalability for authorship attribution in
Bangla Literature (AABL). We analyze the effect of different tokenization, such
as word, sub-word, and character level tokenization, and demonstrate the
effectiveness of these tokenizations in the proposed model. Moreover, we
introduce the publicly available Bangla Authorship Attribution Dataset of 16
authors (BAAD16) containing 17,966 sample texts and 13.4+ million words to
solve the standard dataset scarcity problem and release six variations of
pre-trained language models for use in any Bangla NLP downstream task. For
evaluation, we used our developed BAAD16 dataset as well as other publicly
available datasets. Empirically, our proposed model outperformed
state-of-the-art models and achieved 99.8% accuracy in the BAAD16 dataset.
Furthermore, we showed that the proposed system scales much better even with an
increasing number of authors, and performance remains steady despite few
training samples.
\\ ( https://arxiv.org/abs/2403.05519 ,  2268kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05530
Date: Fri, 8 Mar 2024 18:54:20 GMT   (7059kb)

Title: Gemini 1.5: Unlocking multimodal understanding across millions of tokens
  of context
Authors: Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
  Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou,
  Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian
  Borgeaud, Andrew Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault
  Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James
  Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy,
  Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica
  Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen
  Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand,
  Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav
  Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, et al.
  (619 additional authors not shown)
Categories: cs.CL cs.AI
\\
  In this report, we present the latest model of the Gemini family, Gemini 1.5
Pro, a highly compute-efficient multimodal mixture-of-experts model capable of
recalling and reasoning over fine-grained information from millions of tokens
of context, including multiple long documents and hours of video and audio.
Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks
across modalities, improves the state-of-the-art in long-document QA,
long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's
state-of-the-art performance across a broad set of benchmarks. Studying the
limits of Gemini 1.5 Pro's long-context ability, we find continued improvement
in next-token prediction and near-perfect retrieval (>99%) up to at least 10M
tokens, a generational leap over existing models such as Claude 2.1 (200k) and
GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large
language models at the frontier; when given a grammar manual for Kalamang, a
language with fewer than 200 speakers worldwide, the model learns to translate
English to Kalamang at a similar level to a person who learned from the same
content.
\\ ( https://arxiv.org/abs/2403.05530 ,  7059kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05534
Date: Fri, 8 Mar 2024 18:57:52 GMT   (4595kb,D)

Title: Bayesian Preference Elicitation with Language Models
Authors: Kunal Handa, Yarin Gal, Ellie Pavlick, Noah Goodman, Jacob Andreas,
  Alex Tamkin, Belinda Z. Li
Categories: cs.CL
\\
  Aligning AI systems to users' interests requires understanding and
incorporating humans' complex values and preferences. Recently, language models
(LMs) have been used to gather information about the preferences of human
users. This preference data can be used to fine-tune or guide other LMs and/or
AI systems. However, LMs have been shown to struggle with crucial aspects of
preference learning: quantifying uncertainty, modeling human mental states, and
asking informative questions. These challenges have been addressed in other
areas of machine learning, such as Bayesian Optimal Experimental Design (BOED),
which focus on designing informative queries within a well-defined feature
space. But these methods, in turn, are difficult to scale and apply to
real-world problems where simply identifying the relevant features can be
difficult. We introduce OPEN (Optimal Preference Elicitation with Natural
language) a framework that uses BOED to guide the choice of informative
questions and an LM to extract features and translate abstract BOED queries
into natural language questions. By combining the flexibility of LMs with the
rigor of BOED, OPEN can optimize the informativity of queries while remaining
adaptable to real-world domains. In user studies, we find that OPEN outperforms
existing LM- and BOED-based methods for preference elicitation.
\\ ( https://arxiv.org/abs/2403.05534 ,  4595kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04778
Date: Sat, 2 Mar 2024 01:05:25 GMT   (1175kb,D)

Title: An Efficient Difference-of-Convex Solver for Privacy Funnel
Authors: Teng-Hui Huang and Hesham El Gamal
Categories: cs.LG cs.CR cs.IT math.IT
\\
  We propose an efficient solver for the privacy funnel (PF) method, leveraging
its difference-of-convex (DC) structure. The proposed DC separation results in
a closed-form update equation, which allows straightforward application to both
known and unknown distribution settings. For known distribution case, we prove
the convergence (local stationary points) of the proposed non-greedy solver,
and empirically show that it outperforms the state-of-the-art approaches in
characterizing the privacy-utility trade-off. The insights of our DC approach
apply to unknown distribution settings where labeled empirical samples are
available instead. Leveraging the insights, our alternating minimization solver
satisfies the fundamental Markov relation of PF in contrast to previous
variational inference-based solvers. Empirically, we evaluate the proposed
solver with MNIST and Fashion-MNIST datasets. Our results show that under a
comparable reconstruction quality, an adversary suffers from higher prediction
error from clustering our compressed codes than that with the compared methods.
Most importantly, our solver is independent to private information in inference
phase contrary to the baselines.
\\ ( https://arxiv.org/abs/2403.04778 ,  1175kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04783
Date: Sat, 2 Mar 2024 16:52:22 GMT   (533kb,D)

Title: AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks
Authors: Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, Qingyun Wu
Categories: cs.LG cs.CL cs.CR
\\
  Despite extensive pre-training and fine-tuning in moral alignment to prevent
generating harmful information at user request, large language models (LLMs)
remain vulnerable to jailbreak attacks. In this paper, we propose AutoDefense,
a response-filtering based multi-agent defense framework that filters harmful
responses from LLMs. This framework assigns different roles to LLM agents and
employs them to complete the defense task collaboratively. The division in
tasks enhances the overall instruction-following of LLMs and enables the
integration of other defense components as tools. AutoDefense can adapt to
various sizes and kinds of open-source LLMs that serve as agents. Through
conducting extensive experiments on a large scale of harmful and safe prompts,
we validate the effectiveness of the proposed AutoDefense in improving the
robustness against jailbreak attacks, while maintaining the performance at
normal user request. Our code and data are publicly available at
https://github.com/XHMY/AutoDefense.
\\ ( https://arxiv.org/abs/2403.04783 ,  533kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04793
Date: Mon, 4 Mar 2024 14:20:41 GMT   (1852kb)

Title: A Data-Driven Two-Phase Multi-Split Causal Ensemble Model for Time
  Series
Authors: Zhipeng Ma, Marco Kemmerling, Daniel Buschmann, Chrismarie Enslin,
  Daniel L\"utticke, Robert H. Schmitt
Categories: cs.LG cs.AI stat.ME
Journal-ref: Symmetry 2023, 15(5), 982
DOI: 10.3390/sym15050982
\\
  Causal inference is a fundamental research topic for discovering the
cause-effect relationships in many disciplines. However, not all algorithms are
equally well-suited for a given dataset. For instance, some approaches may only
be able to identify linear relationships, while others are applicable for
non-linearities. Algorithms further vary in their sensitivity to noise and
their ability to infer causal information from coupled vs. non-coupled time
series. Therefore, different algorithms often generate different causal
relationships for the same input. To achieve a more robust causal inference
result, this publication proposes a novel data-driven two-phase multi-split
causal ensemble model to combine the strengths of different causality base
algorithms. In comparison to existing approaches, the proposed ensemble method
reduces the influence of noise through a data partitioning scheme in the first
phase. To achieve this, the data are initially divided into several partitions
and the base algorithms are applied to each partition. Subsequently, Gaussian
mixture models are used to identify the causal relationships derived from the
different partitions that are likely to be valid. In the second phase, the
identified relationships from each base algorithm are then merged based on
three combination rules. The proposed ensemble approach is evaluated using
multiple metrics, among them a newly developed evaluation index for causal
ensemble approaches. We perform experiments using three synthetic datasets with
different volumes and complexity, which are specifically designed to test
causality detection methods under different circumstances while knowing the
ground truth causal relationships. In these experiments, our causality ensemble
outperforms each of its base algorithms. In practical applications, the use of
the proposed method could hence lead to more robust and reliable causality
results.
\\ ( https://arxiv.org/abs/2403.04793 ,  1852kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04805
Date: Tue, 5 Mar 2024 23:02:55 GMT   (6536kb,D)

Title: Not all tickets are equal and we know it: Guiding pruning with
  domain-specific knowledge
Authors: Intekhab Hossain, Jonas Fischer, Rebekka Burkholz, John Quackenbush
Categories: cs.LG q-bio.QM stat.AP stat.ML
\\
  Neural structure learning is of paramount importance for scientific discovery
and interpretability. Yet, contemporary pruning algorithms that focus on
computational resource efficiency face algorithmic barriers to select a
meaningful model that aligns with domain expertise. To mitigate this challenge,
we propose DASH, which guides pruning by available domain-specific structural
information. In the context of learning dynamic gene regulatory network models,
we show that DASH combined with existing general knowledge on interaction
partners provides data-specific insights aligned with biology. For this task,
we show on synthetic data with ground truth information and two real world
applications the effectiveness of DASH, which outperforms competing methods by
a large margin and provides more meaningful biological insights. Our work shows
that domain specific structural information bears the potential to improve
model-derived scientific insights.
\\ ( https://arxiv.org/abs/2403.04805 ,  6536kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04807
Date: Wed, 6 Mar 2024 08:45:29 GMT   (1627kb,D)

Title: Mathematics of Neural Networks (Lecture Notes Graduate Course)
Authors: Bart M.N. Smets
Categories: cs.LG cs.AI
Comments: Lecture notes of the graduate course 2MMA80 Mathematics of Neural
  Networks as thought at the Eindhoven University of Technology from 2021 to
  2023
\\
  These are the lecture notes that accompanied the course of the same name that
I taught at the Eindhoven University of Technology from 2021 to 2023. The
course is intended as an introduction to neural networks for mathematics
students at the graduate level and aims to make mathematics students interested
in further researching neural networks. It consists of two parts: first a
general introduction to deep learning that focuses on introducing the field in
a formal mathematical way. The second part provides an introduction to the
theory of Lie groups and homogeneous spaces and how it can be applied to design
neural networks with desirable geometric equivariances. The lecture notes were
made to be as self-contained as possible so as to accessible for any student
with a moderate mathematics background. The course also included coding
tutorials and assignments in the form of a set of Jupyter notebooks that are
publicly available at
https://gitlab.com/bsmetsjr/mathematics_of_neural_networks.
\\ ( https://arxiv.org/abs/2403.04807 ,  1627kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04810
Date: Wed, 6 Mar 2024 19:09:11 GMT   (50kb,D)

Title: Restricted Bayesian Neural Network
Authors: Sourav Ganguly
Categories: cs.LG cs.AI cs.NE
\\
  Modern deep learning tools are remarkably effective in addressing intricate
problems. However, their operation as black-box models introduces increased
uncertainty in predictions. Additionally, they contend with various challenges,
including the need for substantial storage space in large networks, issues of
overfitting, underfitting, vanishing gradients, and more. This study explores
the concept of Bayesian Neural Networks, presenting a novel architecture
designed to significantly alleviate the storage space complexity of a network.
Furthermore, we introduce an algorithm adept at efficiently handling
uncertainties, ensuring robust convergence values without becoming trapped in
local optima, particularly when the objective function lacks perfect convexity.
\\ ( https://arxiv.org/abs/2403.04810 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04812
Date: Thu, 7 Mar 2024 01:00:55 GMT   (28345kb,D)

Title: TrafPS: A Shapley-based Visual Analytics Approach to Interpret Traffic
Authors: Zezheng Feng, Yifan Jiang, Hongjun Wang, Zipei Fan, Yuxin Ma,
  Shuang-Hua Yang, Huamin Qu, Xuan Song
Categories: cs.LG cs.HC
\\
  Recent achievements in deep learning (DL) have shown its potential for
predicting traffic flows. Such predictions are beneficial for understanding the
situation and making decisions in traffic control. However, most
state-of-the-art DL models are considered "black boxes" with little to no
transparency for end users with respect to the underlying mechanisms. Some
previous work tried to "open the black boxes" and increase the interpretability
of how predictions are generated. However, it still remains challenging to
handle complex models on large-scale spatio-temporal data and discover salient
spatial and temporal patterns that significantly influence traffic flows. To
overcome the challenges, we present TrafPS, a visual analytics approach for
interpreting traffic prediction outcomes to support decision-making in traffic
management and urban planning. The measurements, region SHAP and trajectory
SHAP, are proposed to quantify the impact of flow patterns on urban traffic at
different levels. Based on the task requirement from the domain experts, we
employ an interactive visual interface for multi-aspect exploration and
analysis of significant flow patterns. Two real-world case studies demonstrate
the effectiveness of TrafPS in identifying key routes and decision-making
support for urban planning.
\\ ( https://arxiv.org/abs/2403.04812 ,  28345kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04818
Date: Thu, 7 Mar 2024 13:19:38 GMT   (10920kb,D)

Title: Storm Surge Modeling in the AI ERA: Using LSTM-based Machine Learning
  for Enhancing Forecasting Accuracy
Authors: Stefanos Giaremis, Noujoud Nader, Clint Dawson, Hartmut Kaiser, Carola
  Kaiser, Efstratios Nikidis
Categories: cs.LG physics.ao-ph
\\
  Physics simulation results of natural processes usually do not fully capture
the real world. This is caused for instance by limits in what physical
processes are simulated and to what accuracy. In this work we propose and
analyze the use of an LSTM-based deep learning network machine learning (ML)
architecture for capturing and predicting the behavior of the systemic error
for storm surge forecast models with respect to real-world water height
observations from gauge stations during hurricane events. The overall goal of
this work is to predict the systemic error of the physics model and use it to
improve the accuracy of the simulation results post factum. We trained our
proposed ML model on a dataset of 61 historical storms in the coastal regions
of the U.S. and we tested its performance in bias correcting modeled water
level data predictions from hurricane Ian (2022). We show that our model can
consistently improve the forecasting accuracy for hurricane Ian -- unknown to
the ML model -- at all gauge station coordinates used for the initial data.
Moreover, by examining the impact of using different subsets of the initial
training dataset, containing a number of relatively similar or different
hurricanes in terms of hurricane track, we found that we can obtain similar
quality of bias correction by only using a subset of six hurricanes. This is an
important result that implies the possibility to apply a pre-trained ML model
to real-time hurricane forecasting results with the goal of bias correcting and
improving the produced simulation accuracy. The presented work is an important
first step in creating a bias correction system for real-time storm surge
forecasting applicable to the full simulation area. It also presents a highly
transferable and operationally applicable methodology for improving the
accuracy in a wide range of physics simulation scenarios beyond storm surge
forecasting.
\\ ( https://arxiv.org/abs/2403.04818 ,  10920kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04847
Date: Thu, 7 Mar 2024 19:02:13 GMT   (9369kb,D)

Title: Solving Inverse Problems with Model Mismatch using Untrained Neural
  Networks within Model-based Architectures
Authors: Peimeng Guan, Naveed Iqbal, Mark A. Davenport, Mudassir Masood
Categories: cs.LG eess.SP
\\
  Model-based deep learning methods such as \emph{loop unrolling} (LU) and
\emph{deep equilibrium model} (DEQ) extensions offer outstanding performance in
solving inverse problems (IP). These methods unroll the optimization iterations
into a sequence of neural networks that in effect learn a regularization
function from data. While these architectures are currently state-of-the-art in
numerous applications, their success heavily relies on the accuracy of the
forward model. This assumption can be limiting in many physical applications
due to model simplifications or uncertainties in the apparatus. To address
forward model mismatch, we introduce an untrained forward model residual block
within the model-based architecture to match the data consistency in the
measurement domain for each instance. We propose two variants in well-known
model-based architectures (LU and DEQ) and prove convergence under mild
conditions. The experiments show significant quality improvement in removing
artifacts and preserving details across three distinct applications,
encompassing both linear and nonlinear inverse problems. Moreover, we highlight
reconstruction effectiveness in intermediate steps and showcase robustness to
random initialization of the residual block and a higher number of iterations
during evaluation.
\\ ( https://arxiv.org/abs/2403.04847 ,  9369kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04861
Date: Thu, 7 Mar 2024 19:27:01 GMT   (225kb,D)

Title: A Survey of Lottery Ticket Hypothesis
Authors: Bohan Liu, Zijie Zhang, Peixiong He, Zhensen Wang, Yang Xiao, Ruimeng
  Ye, Yang Zhou, Wei-Shinn Ku, Bo Hui
Categories: cs.LG cs.NE
\\
  The Lottery Ticket Hypothesis (LTH) states that a dense neural network model
contains a highly sparse subnetwork (i.e., winning tickets) that can achieve
even better performance than the original model when trained in isolation.
While LTH has been proved both empirically and theoretically in many works,
there still are some open issues, such as efficiency and scalability, to be
addressed. Also, the lack of open-source frameworks and consensual experimental
setting poses a challenge to future research on LTH. We, for the first time,
examine previous research and studies on LTH from different perspectives. We
also discuss issues in existing works and list potential directions for further
exploration. This survey aims to provide an in-depth look at the state of LTH
and develop a duly maintained platform to conduct experiments and compare with
the most updated baselines.
\\ ( https://arxiv.org/abs/2403.04861 ,  225kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04882
Date: Thu, 7 Mar 2024 20:14:20 GMT   (205kb,D)

Title: Efficient High-Resolution Time Series Classification via Attention
  Kronecker Decomposition
Authors: Aosong Feng, Jialin Chen, Juan Garza, Brooklyn Berry, Francisco
  Salazar, Yifeng Gao, Rex Ying, Leandros Tassiulas
Categories: cs.LG
\\
  The high-resolution time series classification problem is essential due to
the increasing availability of detailed temporal data in various domains. To
tackle this challenge effectively, it is imperative that the state-of-the-art
attention model is scalable to accommodate the growing sequence lengths
typically encountered in high-resolution time series data, while also
demonstrating robustness in handling the inherent noise prevalent in such
datasets. To address this, we propose to hierarchically encode the long time
series into multiple levels based on the interaction ranges. By capturing
relationships at different levels, we can build more robust, expressive, and
efficient models that are capable of capturing both short-term fluctuations and
long-term trends in the data. We then propose a new time series transformer
backbone (KronTime) by introducing Kronecker-decomposed attention to process
such multi-level time series, which sequentially calculates attention from the
lower level to the upper level. Experiments on four long time series datasets
demonstrate superior classification results with improved efficiency compared
to baseline methods.
\\ ( https://arxiv.org/abs/2403.04882 ,  205kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04923
Date: Thu, 7 Mar 2024 22:14:04 GMT   (678kb,D)

Title: Control-based Graph Embeddings with Data Augmentation for Contrastive
  Learning
Authors: Obaid Ullah Ahmad, Anwar Said, Mudassir Shabbir, Waseem Abbas, and
  Xenofon Koutsoukos
Categories: cs.LG cs.MA cs.SY eess.SY
Comments: Accepted in 2024 American Control Conference (ACC), July 8-12, 2024
  in Toronto, ON, Canada
\\
  In this paper, we study the problem of unsupervised graph representation
learning by harnessing the control properties of dynamical networks defined on
graphs. Our approach introduces a novel framework for contrastive learning, a
widely prevalent technique for unsupervised representation learning. A crucial
step in contrastive learning is the creation of 'augmented' graphs from the
input graphs. Though different from the original graphs, these augmented graphs
retain the original graph's structural characteristics. Here, we propose a
unique method for generating these augmented graphs by leveraging the control
properties of networks. The core concept revolves around perturbing the
original graph to create a new one while preserving the controllability
properties specific to networks and graphs. Compared to the existing methods,
we demonstrate that this innovative approach enhances the effectiveness of
contrastive learning frameworks, leading to superior results regarding the
accuracy of the classification tasks. The key innovation lies in our ability to
decode the network structure using these control properties, opening new
avenues for unsupervised graph representation learning.
\\ ( https://arxiv.org/abs/2403.04923 ,  678kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04929
Date: Thu, 7 Mar 2024 22:35:22 GMT   (4852kb,D)

Title: On the Markov Property of Neural Algorithmic Reasoning: Analyses and
  Methods
Authors: Montgomery Bohde, Meng Liu, Alexandra Saxton, Shuiwang Ji
Categories: cs.LG cs.AI cs.NE
Comments: To appear at ICLR 2024 (Spotlight paper). 17 pages, 10 figures
\\
  Neural algorithmic reasoning is an emerging research direction that endows
neural networks with the ability to mimic algorithmic executions step-by-step.
A common paradigm in existing designs involves the use of historical embeddings
in predicting the results of future execution steps. Our observation in this
work is that such historical dependence intrinsically contradicts the Markov
nature of algorithmic reasoning tasks. Based on this motivation, we present our
ForgetNet, which does not use historical embeddings and thus is consistent with
the Markov nature of the tasks. To address challenges in training ForgetNet at
early stages, we further introduce G-ForgetNet, which uses a gating mechanism
to allow for the selective integration of historical embeddings. Such an
enhanced capability provides valuable computational pathways during the model's
early training phase. Our extensive experiments, based on the CLRS-30
algorithmic reasoning benchmark, demonstrate that both ForgetNet and
G-ForgetNet achieve better generalization capability than existing methods.
Furthermore, we investigate the behavior of the gating mechanism, highlighting
its degree of alignment with our intuitions and its effectiveness for robust
performance.
\\ ( https://arxiv.org/abs/2403.04929 ,  4852kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04937
Date: Thu, 7 Mar 2024 23:00:49 GMT   (768kb,D)

Title: Gradient-free neural topology optimization
Authors: Gawel Kus, Miguel A. Bessa
Categories: cs.LG cs.NA math.NA
\\
  Gradient-free optimizers allow for tackling problems regardless of the
smoothness or differentiability of their objective function, but they require
many more iterations to converge when compared to gradient-based algorithms.
This has made them unviable for topology optimization due to the high
computational cost per iteration and high dimensionality of these problems. We
propose a pre-trained neural reparameterization strategy that leads to at least
one order of magnitude decrease in iteration count when optimizing the designs
in latent space, as opposed to the conventional approach without latent
reparameterization. We demonstrate this via extensive computational experiments
in- and out-of-distribution with the training data. Although gradient-based
topology optimization is still more efficient for differentiable problems, such
as compliance optimization of structures, we believe this work will open up a
new path for problems where gradient information is not readily available (e.g.
fracture).
\\ ( https://arxiv.org/abs/2403.04937 ,  768kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04978
Date: Fri, 8 Mar 2024 01:23:25 GMT   (724kb,D)

Title: Stacking as Accelerated Gradient Descent
Authors: Naman Agarwal and Pranjal Awasthi and Satyen Kale and Eric Zhao
Categories: cs.LG stat.ML
\\
  Stacking, a heuristic technique for training deep residual networks by
progressively increasing the number of layers and initializing new layers by
copying parameters from older layers, has proven quite successful in improving
the efficiency of training deep neural networks. In this paper, we propose a
theoretical explanation for the efficacy of stacking: viz., stacking implements
a form of Nesterov's accelerated gradient descent. The theory also covers
simpler models such as the additive ensembles constructed in boosting methods,
and provides an explanation for a similar widely-used practical heuristic for
initializing the new classifier in each round of boosting. We also prove that
for certain deep linear residual networks, stacking does provide accelerated
training, via a new potential function analysis of the Nesterov's accelerated
gradient method which allows errors in updates. We conduct proof-of-concept
experiments to validate our theory as well.
\\ ( https://arxiv.org/abs/2403.04978 ,  724kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05006
Date: Fri, 8 Mar 2024 03:05:11 GMT   (44kb)

Title: Provable Multi-Party Reinforcement Learning with Diverse Human Feedback
Authors: Huiying Zhong, Zhun Deng, Weijie J. Su, Zhiwei Steven Wu, Linjun Zhang
Categories: cs.LG cs.AI stat.ME stat.ML
\\
  Reinforcement learning with human feedback (RLHF) is an emerging paradigm to
align models with human preferences. Typically, RLHF aggregates preferences
from multiple individuals who have diverse viewpoints that may conflict with
each other. Our work \textit{initiates} the theoretical study of multi-party
RLHF that explicitly models the diverse preferences of multiple individuals. We
show how traditional RLHF approaches can fail since learning a single reward
function cannot capture and balance the preferences of multiple individuals. To
overcome such limitations, we incorporate meta-learning to learn multiple
preferences and adopt different social welfare functions to aggregate the
preferences across multiple parties. We focus on the offline learning setting
and establish sample complexity bounds, along with efficiency and fairness
guarantees, for optimizing diverse social welfare functions such as Nash,
Utilitarian, and Leximin welfare functions. Our results show a separation
between the sample complexities of multi-party RLHF and traditional
single-party RLHF. Furthermore, we consider a reward-free setting, where each
individual's preference is no longer consistent with a reward model, and give
pessimistic variants of the von Neumann Winner based on offline preference
data. Taken together, our work showcases the advantage of multi-party RLHF but
also highlights its more demanding statistical complexity.
\\ ( https://arxiv.org/abs/2403.05006 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05014
Date: Fri, 8 Mar 2024 03:27:58 GMT   (578kb,D)

Title: Simple Multigraph Convolution Networks
Authors: Danyang Wu, Xinjie Shen, Jitao Lu, Jin Xu, Feiping Nie
Categories: cs.LG cs.AI
Comments: Accepted by WWW 2024 Short
\\
  Existing multigraph convolution methods either ignore the cross-view
interaction among multiple graphs, or induce extremely high computational cost
due to standard cross-view polynomial operators. To alleviate this problem,
this paper proposes a Simple MultiGraph Convolution Networks (SMGCN) which
first extracts consistent cross-view topology from multigraphs including
edge-level and subgraph-level topology, then performs polynomial expansion
based on raw multigraphs and consistent topologies. In theory, SMGCN utilizes
the consistent topologies in polynomial expansion rather than standard
cross-view polynomial expansion, which performs credible cross-view spatial
message-passing, follows the spectral convolution paradigm, and effectively
reduces the complexity of standard polynomial expansion. In the simulations,
experimental results demonstrate that SMGCN achieves state-of-the-art
performance on ACM and DBLP multigraph benchmark datasets. Our codes are
available at https://github.com/frinkleko/SMGCN.
\\ ( https://arxiv.org/abs/2403.05014 ,  578kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05026
Date: Fri, 8 Mar 2024 04:07:23 GMT   (367kb,D)

Title: Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts
Authors: Zeyang Zhang, Xin Wang, Ziwei Zhang, Zhou Qin, Weigao Wen, Hui Xue,
  Haoyang Li, Wenwu Zhu
Categories: cs.LG cs.AI
Comments: NeurIPS'23
\\
  Dynamic graph neural networks (DyGNNs) currently struggle with handling
distribution shifts that are inherent in dynamic graphs. Existing work on
DyGNNs with out-of-distribution settings only focuses on the time domain,
failing to handle cases involving distribution shifts in the spectral domain.
In this paper, we discover that there exist cases with distribution shifts
unobservable in the time domain while observable in the spectral domain, and
propose to study distribution shifts on dynamic graphs in the spectral domain
for the first time. However, this investigation poses two key challenges: i) it
is non-trivial to capture different graph patterns that are driven by various
frequency components entangled in the spectral domain; and ii) it remains
unclear how to handle distribution shifts with the discovered spectral
patterns. To address these challenges, we propose Spectral Invariant Learning
for Dynamic Graphs under Distribution Shifts (SILD), which can handle
distribution shifts on dynamic graphs by capturing and utilizing invariant and
variant spectral patterns. Specifically, we first design a DyGNN with Fourier
transform to obtain the ego-graph trajectory spectrums, allowing the mixed
dynamic graph patterns to be transformed into separate frequency components. We
then develop a disentangled spectrum mask to filter graph dynamics from various
frequency components and discover the invariant and variant spectral patterns.
Finally, we propose invariant spectral filtering, which encourages the model to
rely on invariant patterns for generalization under distribution shifts.
Experimental results on synthetic and real-world dynamic graph datasets
demonstrate the superiority of our method for both node classification and link
prediction tasks under distribution shifts.
\\ ( https://arxiv.org/abs/2403.05026 ,  367kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05033
Date: Fri, 8 Mar 2024 04:23:50 GMT   (302kb,D)

Title: Quantifying Manifolds: Do the manifolds learned by Generative
  Adversarial Networks converge to the real data manifold
Authors: Anupam Chaudhuri, Anj Simmons, Mohamed Abdelrazek
Categories: cs.LG cs.AI
Comments: arXiv admin note: text overlap with arXiv:2311.13102
\\
  This paper presents our experiments to quantify the manifolds learned by ML
models (in our experiment, we use a GAN model) as they train. We compare the
manifolds learned at each epoch to the real manifolds representing the real
data. To quantify a manifold, we study the intrinsic dimensions and topological
features of the manifold learned by the ML model, how these metrics change as
we continue to train the model, and whether these metrics convergence over the
course of training to the metrics of the real data manifold.
\\ ( https://arxiv.org/abs/2403.05033 ,  302kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05064
Date: Fri, 8 Mar 2024 05:23:55 GMT   (247kb,D)

Title: Unsupervised Graph Neural Architecture Search with Disentangled
  Self-supervision
Authors: Zeyang Zhang, Xin Wang, Ziwei Zhang, Guangyao Shen, Shiqi Shen, Wenwu
  Zhu
Categories: cs.LG cs.AI
Comments: NeurIPS'23
\\
  The existing graph neural architecture search (GNAS) methods heavily rely on
supervised labels during the search process, failing to handle ubiquitous
scenarios where supervisions are not available. In this paper, we study the
problem of unsupervised graph neural architecture search, which remains
unexplored in the literature. The key problem is to discover the latent graph
factors that drive the formation of graph data as well as the underlying
relations between the factors and the optimal neural architectures. Handling
this problem is challenging given that the latent graph factors together with
architectures are highly entangled due to the nature of the graph and the
complexity of the neural architecture search process. To address the challenge,
we propose a novel Disentangled Self-supervised Graph Neural Architecture
Search (DSGAS) model, which is able to discover the optimal architectures
capturing various latent graph factors in a self-supervised fashion based on
unlabeled graph data. Specifically, we first design a disentangled graph
super-network capable of incorporating multiple architectures with factor-wise
disentanglement, which are optimized simultaneously. Then, we estimate the
performance of architectures under different factors by our proposed
self-supervised training with joint architecture-graph disentanglement.
Finally, we propose a contrastive search with architecture augmentations to
discover architectures with factor-specific expertise. Extensive experiments on
11 real-world datasets demonstrate that the proposed model is able to achieve
state-of-the-art performance against several baseline methods in an
unsupervised manner.
\\ ( https://arxiv.org/abs/2403.05064 ,  247kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05066
Date: Fri, 8 Mar 2024 05:37:59 GMT   (1264kb,D)

Title: Reset & Distill: A Recipe for Overcoming Negative Transfer in Continual
  Reinforcement Learning
Authors: Hongjoon Ahn, Jinu Hyeon, Youngmin Oh, Bosun Hwang, and Taesup Moon
Categories: cs.LG cs.AI
\\
  We argue that one of the main obstacles for developing effective Continual
Reinforcement Learning (CRL) algorithms is the negative transfer issue
occurring when the new task to learn arrives. Through comprehensive
experimental validation, we demonstrate that such issue frequently exists in
CRL and cannot be effectively addressed by several recent work on mitigating
plasticity loss of RL agents. To that end, we develop Reset & Distill (R&D), a
simple yet highly effective method, to overcome the negative transfer problem
in CRL. R&D combines a strategy of resetting the agent's online actor and
critic networks to learn a new task and an offline learning step for distilling
the knowledge from the online actor and previous expert's action probabilities.
We carried out extensive experiments on long sequence of Meta-World tasks and
show that our method consistently outperforms recent baselines, achieving
significantly higher success rates across a range of tasks. Our findings
highlight the importance of considering negative transfer in CRL and emphasize
the need for robust strategies like R&D to mitigate its detrimental effects.
\\ ( https://arxiv.org/abs/2403.05066 ,  1264kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05075
Date: Fri, 8 Mar 2024 05:59:56 GMT   (480kb,D)

Title: Benchmarking Large Language Models for Molecule Prediction Tasks
Authors: Zhiqiang Zhong and Kuangyu Zhou and Davide Mottin
Categories: cs.LG q-bio.BM
\\
  Large Language Models (LLMs) stand at the forefront of a number of Natural
Language Processing (NLP) tasks. Despite the widespread adoption of LLMs in
NLP, much of their potential in broader fields remains largely unexplored, and
significant limitations persist in their design and implementation. Notably,
LLMs struggle with structured data, such as graphs, and often falter when
tasked with answering domain-specific questions requiring deep expertise, such
as those in biology and chemistry. In this paper, we explore a fundamental
question: Can LLMs effectively handle molecule prediction tasks? Rather than
pursuing top-tier performance, our goal is to assess how LLMs can contribute to
diverse molecule tasks. We identify several classification and regression
prediction tasks across six standard molecule datasets. Subsequently, we
carefully design a set of prompts to query LLMs on these tasks and compare
their performance with existing Machine Learning (ML) models, which include
text-based models and those specifically designed for analysing the geometric
structure of molecules. Our investigation reveals several key insights:
Firstly, LLMs generally lag behind ML models in achieving competitive
performance on molecule tasks, particularly when compared to models adept at
capturing the geometric structure of molecules, highlighting the constrained
ability of LLMs to comprehend graph data. Secondly, LLMs show promise in
enhancing the performance of ML models when used collaboratively. Lastly, we
engage in a discourse regarding the challenges and promising avenues to harness
LLMs for molecule prediction tasks. The code and models are available at
https://github.com/zhiqiangzhongddu/LLMaMol.
\\ ( https://arxiv.org/abs/2403.05075 ,  480kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05106
Date: Fri, 8 Mar 2024 07:09:56 GMT   (373kb,D)

Title: Simulating Battery-Powered TinyML Systems Optimised using Reinforcement
  Learning in Image-Based Anomaly Detection
Authors: Jared M. Ping and Ken J. Nixon
Categories: cs.LG
Comments: Accepted as a full paper by the tinyML Research Symposium 2024
\\
  Advances in Tiny Machine Learning (TinyML) have bolstered the creation of
smart industry solutions, including smart agriculture, healthcare and smart
cities. Whilst related research contributes to enabling TinyML solutions on
constrained hardware, there is a need to amplify real-world applications by
optimising energy consumption in battery-powered systems. The work presented
extends and contributes to TinyML research by optimising battery-powered
image-based anomaly detection Internet of Things (IoT) systems. Whilst previous
work in this area has yielded the capabilities of on-device inferencing and
training, there has yet to be an investigation into optimising the management
of such capabilities using machine learning approaches, such as Reinforcement
Learning (RL), to improve the deployment battery life of such systems. Using
modelled simulations, the battery life effects of an RL algorithm are
benchmarked against static and dynamic optimisation approaches, with the
foundation laid for a hardware benchmark to follow. It is shown that using RL
within a TinyML-enabled IoT system to optimise the system operations, including
cloud anomaly processing and on-device training, yields an improved battery
life of 22.86% and 10.86% compared to static and dynamic optimisation
approaches respectively. The proposed solution can be deployed to
resource-constrained hardware, given its low memory footprint of 800 B, which
could be further reduced. This further facilitates the real-world deployment of
such systems, including key sectors such as smart agriculture.
\\ ( https://arxiv.org/abs/2403.05106 ,  373kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05123
Date: Fri, 8 Mar 2024 07:36:46 GMT   (2166kb,D)

Title: ECToNAS: Evolutionary Cross-Topology Neural Architecture Search
Authors: Elisabeth J. Schiessler and Roland C. Aydin and Christian J. Cyron
Categories: cs.LG cs.CV cs.NE
Comments: 15 pages, 7 figures, 6 tables
\\
  We present ECToNAS, a cost-efficient evolutionary cross-topology neural
architecture search algorithm that does not require any pre-trained meta
controllers. Our framework is able to select suitable network architectures for
different tasks and hyperparameter settings, independently performing
cross-topology optimisation where required. It is a hybrid approach that fuses
training and topology optimisation together into one lightweight,
resource-friendly process. We demonstrate the validity and power of this
approach with six standard data sets (CIFAR-10, CIFAR-100, EuroSAT, Fashion
MNIST, MNIST, SVHN), showcasing the algorithm's ability to not only optimise
the topology within an architectural type, but also to dynamically add and
remove convolutional cells when and where required, thus crossing boundaries
between different network types. This enables researchers without a background
in machine learning to make use of appropriate model types and topologies and
to apply machine learning methods in their domains, with a computationally
cheap, easy-to-use cross-topology neural architecture search framework that
fully encapsulates the topology optimisation within the training process.
\\ ( https://arxiv.org/abs/2403.05123 ,  2166kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05158
Date: Fri, 8 Mar 2024 08:51:37 GMT   (472kb)

Title: Adaptive Split Learning over Energy-Constrained Wireless Edge Networks
Authors: Zuguang Li, Wen Wu, Shaohua Wu, and Wei Wang
Categories: cs.LG cs.AI cs.NI
Comments: 6 pages, 5 figures, 20 conferences
\\
  Split learning (SL) is a promising approach for training artificial
intelligence (AI) models, in which devices collaborate with a server to train
an AI model in a distributed manner, based on a same fixed split point.
However, due to the device heterogeneity and variation of channel conditions,
this way is not optimal in training delay and energy consumption. In this
paper, we design an adaptive split learning (ASL) scheme which can dynamically
select split points for devices and allocate computing resource for the server
in wireless edge networks. We formulate an optimization problem to minimize the
average training latency subject to long-term energy consumption constraint.
The difficulties in solving this problem are the lack of future information and
mixed integer programming (MIP). To solve it, we propose an online algorithm
leveraging the Lyapunov theory, named OPEN, which decomposes it into a new MIP
problem only with the current information. Then, a two-layer optimization
method is proposed to solve the MIP problem. Extensive simulation results
demonstrate that the ASL scheme can reduce the average training delay and
energy consumption by 53.7% and 22.1%, respectively, as compared to the
existing SL schemes.
\\ ( https://arxiv.org/abs/2403.05158 ,  472kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05164
Date: Fri, 8 Mar 2024 09:09:15 GMT   (862kb,D)

Title: Synthetic data generation for system identification: leveraging
  knowledge transfer from similar systems
Authors: Dario Piga, Matteo Rufolo, Gabriele Maroni, Manas Mejari, Marco
  Forgione
Categories: cs.LG cs.AI cs.SY eess.SY
\\
  This paper addresses the challenge of overfitting in the learning of
dynamical systems by introducing a novel approach for the generation of
synthetic data, aimed at enhancing model generalization and robustness in
scenarios characterized by data scarcity. Central to the proposed methodology
is the concept of knowledge transfer from systems within the same class.
Specifically, synthetic data is generated through a pre-trained meta-model that
describes a broad class of systems to which the system of interest is assumed
to belong. Training data serves a dual purpose: firstly, as input to the
pre-trained meta model to discern the system's dynamics, enabling the
prediction of its behavior and thereby generating synthetic output sequences
for new input sequences; secondly, in conjunction with synthetic data, to
define the loss function used for model estimation. A validation dataset is
used to tune a scalar hyper-parameter balancing the relative importance of
training and synthetic data in the definition of the loss function. The same
validation set can be also used for other purposes, such as early stopping
during the training, fundamental to avoid overfitting in case of small-size
training datasets. The efficacy of the approach is shown through a numerical
example that highlights the advantages of integrating synthetic data into the
system identification process.
\\ ( https://arxiv.org/abs/2403.05164 ,  862kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05171
Date: Fri, 8 Mar 2024 09:20:12 GMT   (394kb,D)

Title: Overcoming Reward Overoptimization via Adversarial Policy Optimization
  with Lightweight Uncertainty Estimation
Authors: Xiaoying Zhang, Jean-Francois Ton, Wei Shen, Hongning Wang, Yang Liu
Categories: cs.LG cs.AI
\\
  We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the
pervasive issue of reward over-optimization in Reinforcement Learning from
Human Feedback (RLHF) for Large Language Models (LLMs). Over-optimization
occurs when a reward model serves as an imperfect proxy for human preference,
and RL-driven policy optimization erroneously exploits reward inaccuracies. In
this paper, we begin by introducing a lightweight way to quantify uncertainties
in rewards, relying solely on the last layer embeddings of the reward model,
without the need for computationally expensive reward ensembles. AdvPO then
addresses a distributionally robust optimization problem centred around the
confidence interval of the reward model's predictions for policy improvement.
Through comprehensive experiments on the Anthropic HH and TL;DR summarization
datasets, we illustrate the efficacy of AdvPO in mitigating the
overoptimization issue, consequently resulting in enhanced performance as
evaluated through human-assisted evaluation.
\\ ( https://arxiv.org/abs/2403.05171 ,  394kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05174
Date: Fri, 8 Mar 2024 09:28:42 GMT   (33339kb,D)

Title: VTruST: Controllable value function based subset selection for
  Data-Centric Trustworthy AI
Authors: Soumi Das, Shubhadip Nag, Shreyyash Sharma, Suparna Bhattacharya,
  Sourangshu Bhattacharya
Categories: cs.LG
Comments: Accepted in ICLR 2024 DMLR workshop
\\
  Trustworthy AI is crucial to the widespread adoption of AI in high-stakes
applications with fairness, robustness, and accuracy being some of the key
trustworthiness metrics. In this work, we propose a controllable framework for
data-centric trustworthy AI (DCTAI)- VTruST, that allows users to control the
trade-offs between the different trustworthiness metrics of the constructed
training datasets. A key challenge in implementing an efficient DCTAI framework
is to design an online value-function-based training data subset selection
algorithm. We pose the training data valuation and subset selection problem as
an online sparse approximation formulation. We propose a novel online version
of the Orthogonal Matching Pursuit (OMP) algorithm for solving this problem.
Experimental results show that VTruST outperforms the state-of-the-art
baselines on social, image, and scientific datasets. We also show that the data
values generated by VTruST can provide effective data-centric explanations for
different trustworthiness metrics.
\\ ( https://arxiv.org/abs/2403.05174 ,  33339kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05175
Date: Fri, 8 Mar 2024 09:32:43 GMT   (613kb,D)

Title: Continual Learning and Catastrophic Forgetting
Authors: Gido M. van de Ven, Nicholas Soures, Dhireesha Kudithipudi
Categories: cs.LG cs.AI cs.CV q-bio.NC stat.ML
Comments: Preprint of a book chapter; 21 pages, 4 figures
\\
  This book chapter delves into the dynamics of continual learning, which is
the process of incrementally learning from a non-stationary stream of data.
Although continual learning is a natural skill for the human brain, it is very
challenging for artificial neural networks. An important reason is that, when
learning something new, these networks tend to quickly and drastically forget
what they had learned before, a phenomenon known as catastrophic forgetting.
Especially in the last decade, continual learning has become an extensively
studied topic in deep learning. This book chapter reviews the insights that
this field has generated.
\\ ( https://arxiv.org/abs/2403.05175 ,  613kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05181
Date: Fri, 8 Mar 2024 09:43:27 GMT   (1694kb,D)

Title: Adversarial Sparse Teacher: Defense Against Distillation-Based Model
  Stealing Attacks Using Adversarial Examples
Authors: Eda Yilmaz and Hacer Yalim Keles
Categories: cs.LG cs.CR cs.CV
Comments: 12 pages, 3 figures, 6 tables
\\
  Knowledge Distillation (KD) facilitates the transfer of discriminative
capabilities from an advanced teacher model to a simpler student model,
ensuring performance enhancement without compromising accuracy. It is also
exploited for model stealing attacks, where adversaries use KD to mimic the
functionality of a teacher model. Recent developments in this domain have been
influenced by the Stingy Teacher model, which provided empirical analysis
showing that sparse outputs can significantly degrade the performance of
student models. Addressing the risk of intellectual property leakage, our work
introduces an approach to train a teacher model that inherently protects its
logits, influenced by the Nasty Teacher concept. Differing from existing
methods, we incorporate sparse outputs of adversarial examples with standard
training data to strengthen the teacher's defense against student distillation.
Our approach carefully reduces the relative entropy between the original and
adversarially perturbed outputs, allowing the model to produce adversarial
logits with minimal impact on overall performance. The source codes will be
made publicly available soon.
\\ ( https://arxiv.org/abs/2403.05181 ,  1694kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05196
Date: Fri, 8 Mar 2024 10:19:00 GMT   (10518kb,D)

Title: Denoising Autoregressive Representation Learning
Authors: Yazhe Li, Jorg Bornschein, Ting Chen
Categories: cs.LG cs.CV
\\
  In this paper, we explore a new generative approach for learning visual
representations. Our method, DARL, employs a decoder-only Transformer to
predict image patches autoregressively. We find that training with Mean Squared
Error (MSE) alone leads to strong representations. To enhance the image
generation ability, we replace the MSE loss with the diffusion objective by
using a denoising patch decoder. We show that the learned representation can be
improved by using tailored noise schedules and longer training in larger
models. Notably, the optimal schedule differs significantly from the typical
ones used in standard image diffusion models. Overall, despite its simple
architecture, DARL delivers performance remarkably close to state-of-the-art
masked prediction models under the fine-tuning protocol. This marks an
important step towards a unified model capable of both visual perception and
generation, effectively combining the strengths of autoregressive and denoising
diffusion models.
\\ ( https://arxiv.org/abs/2403.05196 ,  10518kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05209
Date: Fri, 8 Mar 2024 10:49:37 GMT   (1757kb,D)

Title: Overcoming Data Inequality across Domains with Semi-Supervised Domain
  Generalization
Authors: Jinha Park, Wonguk Cho, Taesup Kim
Categories: cs.LG cs.AI cs.CV
Comments: 20 pages, 4 figures
\\
  While there have been considerable advancements in machine learning driven by
extensive datasets, a significant disparity still persists in the availability
of data across various sources and populations. This inequality across domains
poses challenges in modeling for those with limited data, which can lead to
profound practical and ethical concerns. In this paper, we address a
representative case of data inequality problem across domains termed
Semi-Supervised Domain Generalization (SSDG), in which only one domain is
labeled while the rest are unlabeled. We propose a novel algorithm, ProUD,
which can effectively learn domain-invariant features via domain-aware
prototypes along with progressive generalization via uncertainty-adaptive
mixing of labeled and unlabeled domains. Our experiments on three different
benchmark datasets demonstrate the effectiveness of ProUD, outperforming all
baseline models including single domain generalization and semi-supervised
learning. Source code will be released upon acceptance of the paper.
\\ ( https://arxiv.org/abs/2403.05209 ,  1757kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05235
Date: Fri, 8 Mar 2024 11:51:00 GMT   (1312kb)

Title: Fairness-Aware Interpretable Modeling (FAIM) for Trustworthy Machine
  Learning in Healthcare
Authors: Mingxuan Liu, Yilin Ning, Yuhe Ke, Yuqing Shang, Bibhas Chakraborty,
  Marcus Eng Hock Ong, Roger Vaughan, Nan Liu
Categories: cs.LG cs.AI cs.CY
\\
  The escalating integration of machine learning in high-stakes fields such as
healthcare raises substantial concerns about model fairness. We propose an
interpretable framework - Fairness-Aware Interpretable Modeling (FAIM), to
improve model fairness without compromising performance, featuring an
interactive interface to identify a "fairer" model from a set of
high-performing models and promoting the integration of data-driven evidence
and clinical expertise to enhance contextualized fairness. We demonstrated
FAIM's value in reducing sex and race biases by predicting hospital admission
with two real-world databases, MIMIC-IV-ED and SGH-ED. We show that for both
datasets, FAIM models not only exhibited satisfactory discriminatory
performance but also significantly mitigated biases as measured by
well-established fairness metrics, outperforming commonly used bias-mitigation
methods. Our approach demonstrates the feasibility of improving fairness
without sacrificing performance and provides an a modeling mode that invites
domain experts to engage, fostering a multidisciplinary effort toward tailored
AI fairness.
\\ ( https://arxiv.org/abs/2403.05235 ,  1312kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05290
Date: Fri, 8 Mar 2024 13:16:17 GMT   (32kb)

Title: Foundational propositions of hesitant fuzzy soft $\beta$-covering
  approximation spaces
Authors: Shizhan Lu
Categories: cs.LG cs.LO
Comments: 23 pages
\\
  Soft set theory serves as a mathematical framework for handling uncertain
information, and hesitant fuzzy sets find extensive application in scenarios
involving uncertainty and hesitation. Hesitant fuzzy sets exhibit diverse
membership degrees, giving rise to various forms of inclusion relationships
among them. This article introduces the notions of hesitant fuzzy soft
$\beta$-coverings and hesitant fuzzy soft $\beta$-neighborhoods, which are
formulated based on distinct forms of inclusion relationships among hesitancy
fuzzy sets. Subsequently, several associated properties are investigated.
Additionally, specific variations of hesitant fuzzy soft $\beta$-coverings are
introduced by incorporating hesitant fuzzy rough sets, followed by an
exploration of properties pertaining to hesitant fuzzy soft $\beta$-covering
approximation spaces.
\\ ( https://arxiv.org/abs/2403.05290 ,  32kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05293
Date: Fri, 8 Mar 2024 13:21:07 GMT   (3283kb,D)

Title: Leveraging Continuous Time to Understand Momentum When Training Diagonal
  Linear Networks
Authors: Hristo Papazov, Scott Pesme, Nicolas Flammarion
Categories: cs.LG math.OC stat.ML
\\
  In this work, we investigate the effect of momentum on the optimisation
trajectory of gradient descent. We leverage a continuous-time approach in the
analysis of momentum gradient descent with step size $\gamma$ and momentum
parameter $\beta$ that allows us to identify an intrinsic quantity $\lambda =
\frac{ \gamma }{ (1 - \beta)^2 }$ which uniquely defines the optimisation path
and provides a simple acceleration rule. When training a $2$-layer diagonal
linear network in an overparametrised regression setting, we characterise the
recovered solution through an implicit regularisation problem. We then prove
that small values of $\lambda$ help to recover sparse solutions. Finally, we
give similar but weaker results for stochastic momentum gradient descent. We
provide numerical experiments which support our claims.
\\ ( https://arxiv.org/abs/2403.05293 ,  3283kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05300
Date: Fri, 8 Mar 2024 13:29:46 GMT   (11252kb,D)

Title: Unity by Diversity: Improved Representation Learning in Multimodal VAEs
Authors: Thomas M. Sutter, Yang Meng, Norbert Fortin, Julia E. Vogt, Stephan
  Mandt
Categories: cs.LG cs.AI
\\
  Variational Autoencoders for multimodal data hold promise for many tasks in
data analysis, such as representation learning, conditional generation, and
imputation. Current architectures either share the encoder output, decoder
input, or both across modalities to learn a shared representation. Such
architectures impose hard constraints on the model. In this work, we show that
a better latent representation can be obtained by replacing these hard
constraints with a soft constraint. We propose a new mixture-of-experts prior,
softly guiding each modality's latent representation towards a shared aggregate
posterior. This approach results in a superior latent representation and allows
each encoding to preserve information from its uncompressed original features
better. In extensive experiments on multiple benchmark datasets and a
challenging real-world neuroscience data set, we show improved learned latent
representations and imputation of missing data modalities compared to existing
methods.
\\ ( https://arxiv.org/abs/2403.05300 ,  11252kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05385
Date: Fri, 8 Mar 2024 15:30:58 GMT   (136kb,D)

Title: Switching the Loss Reduces the Cost in Batch Reinforcement Learning
Authors: Alex Ayoub, Kaiwen Wang, Vincent Liu, Samuel Robertson, James
  McInerney, Dawen Liang, Nathan Kallus, and Csaba Szepesv\'ari
Categories: cs.LG
\\
  We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch
reinforcement learning (RL). We show that the number of samples needed to learn
a near-optimal policy with FQI-LOG scales with the accumulated cost of the
optimal policy, which is zero in problems where acting optimally achieves the
goal and incurs no cost. In doing so, we provide a general framework for
proving $\textit{small-cost}$ bounds, i.e. bounds that scale with the optimal
achievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG uses
fewer samples than FQI trained with squared loss on problems where the optimal
policy reliably achieves the goal.
\\ ( https://arxiv.org/abs/2403.05385 ,  136kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05395
Date: Fri, 8 Mar 2024 15:45:13 GMT   (592kb,D)

Title: Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems
  trained with Gradient Descent
Authors: Nathan Buskulic, Jalal Fadili, Yvain Qu\'eau
Categories: cs.LG
\\
  Advanced machine learning methods, and more prominently neural networks, have
become standard to solve inverse problems over the last years. However, the
theoretical recovery guarantees of such methods are still scarce and difficult
to achieve. Only recently did unsupervised methods such as Deep Image Prior
(DIP) get equipped with convergence and recovery guarantees for generic loss
functions when trained through gradient flow with an appropriate
initialization. In this paper, we extend these results by proving that these
guarantees hold true when using gradient descent with an appropriately chosen
step-size/learning rate. We also show that the discretization only affects the
overparametrization bound for a two-layer DIP network by a constant and thus
that the different guarantees found for the gradient flow will hold for
gradient descent.
\\ ( https://arxiv.org/abs/2403.05395 ,  592kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05406
Date: Fri, 8 Mar 2024 16:04:36 GMT   (1956kb,D)

Title: Considering Nonstationary within Multivariate Time Series with
  Variational Hierarchical Transformer for Forecasting
Authors: Muyao Wang, Wenchao Chen, Bo Chen
Categories: cs.LG cs.AI
Comments: accepted by AAAI2024
\\
  The forecasting of Multivariate Time Series (MTS) has long been an important
but challenging task. Due to the non-stationary problem across long-distance
time steps, previous studies primarily adopt stationarization method to
attenuate the non-stationary problem of the original series for better
predictability. However, existing methods always adopt the stationarized
series, which ignores the inherent non-stationarity, and has difficulty in
modeling MTS with complex distributions due to the lack of stochasticity. To
tackle these problems, we first develop a powerful hierarchical probabilistic
generative module to consider the non-stationarity and stochastic
characteristics within MTS, and then combine it with transformer for a
well-defined variational generative dynamic model named Hierarchical Time
series Variational Transformer (HTV-Trans), which recovers the intrinsic
non-stationary information into temporal dependencies. Being a powerful
probabilistic model, HTV-Trans is utilized to learn expressive representations
of MTS and applied to forecasting tasks. Extensive experiments on diverse
datasets show the efficiency of HTV-Trans on MTS forecasting tasks
\\ ( https://arxiv.org/abs/2403.05406 ,  1956kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05446
Date: Fri, 8 Mar 2024 16:54:27 GMT   (28kb)

Title: An Improved Algorithm for Learning Drifting Discrete Distributions
Authors: Alessio Mazzetto
Categories: cs.LG stat.ML
Comments: To be published in AISTATS 2024
\\
  We present a new adaptive algorithm for learning discrete distributions under
distribution drift. In this setting, we observe a sequence of independent
samples from a discrete distribution that is changing over time, and the goal
is to estimate the current distribution. Since we have access to only a single
sample for each time step, a good estimation requires a careful choice of the
number of past samples to use. To use more samples, we must resort to samples
further in the past, and we incur a drift error due to the bias introduced by
the change in distribution. On the other hand, if we use a small number of past
samples, we incur a large statistical error as the estimation has a high
variance. We present a novel adaptive algorithm that can solve this trade-off
without any prior knowledge of the drift. Unlike previous adaptive results, our
algorithm characterizes the statistical error using data-dependent bounds. This
technicality enables us to overcome the limitations of the previous work that
require a fixed finite support whose size is known in advance and that cannot
change over time. Additionally, we can obtain tighter bounds depending on the
complexity of the drifting distribution, and also consider distributions with
infinite support.
\\ ( https://arxiv.org/abs/2403.05446 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05490
Date: Fri, 8 Mar 2024 17:55:41 GMT   (1007kb,D)

Title: Poly-View Contrastive Learning
Authors: Amitis Shidani, Devon Hjelm, Jason Ramapuram, Russ Webb, Eeshan Gunesh
  Dhekane, Dan Busbridge
Categories: cs.LG cs.AI cs.CV cs.IT math.IT stat.ML
Comments: Accepted to ICLR 2024. 42 pages, 7 figures, 3 tables, loss
  pseudo-code included in appendix
\\
  Contrastive learning typically matches pairs of related views among a number
of unrelated negative views. Views can be generated (e.g. by augmentations) or
be observed. We investigate matching when there are more than two related views
which we call poly-view tasks, and derive new representation learning
objectives using information maximization and sufficient statistics. We show
that with unlimited computation, one should maximize the number of related
views, and with a fixed compute budget, it is beneficial to decrease the number
of unique samples whilst increasing the number of views of those samples. In
particular, poly-view contrastive models trained for 128 epochs with batch size
256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k,
challenging the belief that contrastive models require large batch sizes and
many training epochs.
\\ ( https://arxiv.org/abs/2403.05490 ,  1007kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05527
Date: Fri, 8 Mar 2024 18:48:30 GMT   (2567kb,D)

Title: GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless
  Generative Inference of LLM
Authors: Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu,
  Tushar Krishna, Tuo Zhao
Categories: cs.LG cs.AI cs.CL
\\
  Key-value (KV) caching has become the de-facto to accelerate generation speed
for large language models (LLMs) inference. However, the growing cache demand
with increasing sequence length has transformed LLM inference to be a memory
bound problem, significantly constraining the system throughput. Existing
methods rely on dropping unimportant tokens or quantizing all entries
uniformly. Such methods, however, often incur high approximation errors to
represent the compressed matrices. The autoregressive decoding process further
compounds the error of each step, resulting in critical deviation in model
generation and deterioration of performance. To tackle this challenge, we
propose GEAR, an efficient KV cache compression framework that achieves
near-lossless high-ratio compression. GEAR first applies quantization to
majority of entries of similar magnitudes to ultra-low precision. It then
employs a low rank matrix to approximate the quantization error, and a sparse
matrix to remedy individual errors from outlier entries. By adeptly integrating
three techniques, GEAR is able to fully exploit their synergistic potentials.
Our experiments demonstrate that compared to alternatives, GEAR achieves
near-lossless 4-bit KV cache compression with up to 2.38x throughput
improvement, while reducing peak-memory size up to 2.29x. Our code is publicly
available at https://github.com/HaoKang-Timmy/GEAR.
\\ ( https://arxiv.org/abs/2403.05527 ,  2567kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05529
Date: Fri, 8 Mar 2024 18:50:19 GMT   (7269kb,D)

Title: The Computational Complexity of Learning Gaussian Single-Index Models
Authors: Alex Damian, Loucas Pillaud-Vivien, Jason D. Lee, Joan Bruna
Categories: cs.LG stat.ML
Comments: 57 pages
\\
  Single-Index Models are high-dimensional regression problems with planted
structure, whereby labels depend on an unknown one-dimensional projection of
the input via a generic, non-linear, and potentially non-deterministic
transformation. As such, they encompass a broad class of statistical inference
tasks, and provide a rich template to study statistical and computational
trade-offs in the high-dimensional regime.
  While the information-theoretic sample complexity to recover the hidden
direction is linear in the dimension $d$, we show that computationally
efficient algorithms, both within the Statistical Query (SQ) and the Low-Degree
Polynomial (LDP) framework, necessarily require $\Omega(d^{k^\star/2})$
samples, where $k^\star$ is a "generative" exponent associated with the model
that we explicitly characterize. Moreover, we show that this sample complexity
is also sufficient, by establishing matching upper bounds using a partial-trace
algorithm. Therefore, our results provide evidence of a sharp
computational-to-statistical gap (under both the SQ and LDP class) whenever
$k^\star>2$. To complete the study, we provide examples of smooth and Lipschitz
deterministic target functions with arbitrarily large generative exponents
$k^\star$.
\\ ( https://arxiv.org/abs/2403.05529 ,  7269kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05532
Date: Fri, 8 Mar 2024 18:57:00 GMT   (2223kb,D)

Title: Tune without Validation: Searching for Learning Rate and Weight Decay on
  Training Sets
Authors: Lorenzo Brigato and Stavroula Mougiakakou
Categories: cs.LG cs.CV
Comments: Pre-print
\\
  We introduce Tune without Validation (Twin), a pipeline for tuning learning
rate and weight decay without validation sets. We leverage a recent theoretical
framework concerning learning phases in hypothesis space to devise a heuristic
that predicts what hyper-parameter (HP) combinations yield better
generalization. Twin performs a grid search of trials according to an
early-/non-early-stopping scheduler and then segments the region that provides
the best results in terms of training loss. Among these trials, the weight norm
strongly correlates with predicting generalization. To assess the effectiveness
of Twin, we run extensive experiments on 20 image classification datasets and
train several families of deep networks, including convolutional, transformer,
and feed-forward models. We demonstrate proper HP selection when training from
scratch and fine-tuning, emphasizing small-sample scenarios.
\\ ( https://arxiv.org/abs/2403.05532 ,  2223kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2403.04769 (*cross-listing*)
Date: Fri, 16 Feb 2024 17:02:53 GMT   (3851kb,D)

Title: Removing GPT4's Filter
Authors: Benjamin Lemkin
Categories: cs.CR cs.AI cs.CL cs.LG
\\
  GPT4 was initially trained on large amounts of data, and then fine-tuned
using Reinforcement learning from Human Feedback (RLHF), which is when
volunteers give feedback in order to teach GPT4 not to create inappropriate
content. In this paper, we present a method to manipulate the fine-tuned
version into reverting to pre-RLHF behavior, effectively removing all safety
mechanisms that the model learned during RLHF. In particular, when GPT4 acts
without RLHF, it loses all inhibition, and can complete very inappropriate
content given only the first few words.
\\ ( https://arxiv.org/abs/2403.04769 ,  3851kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04775 (*cross-listing*)
Date: Thu, 29 Feb 2024 11:35:49 GMT   (125kb,D)

Title: Superposition with Delayed Unification
Authors: Ahmed Bhayat, Johannes Schoisswohl, Michael Rawson
Categories: cs.LO cs.AI
Comments: 16 pages, 0 figures, 1 table
Journal-ref: International Conference on Automated Deduction (CADE) 2023. LNAI
  volume 14132, 2023, pp. 23-40
DOI: 10.1007/978-3-031-38499-8_2
\\
  Classically, in saturation-based proof systems, unification has been
considered atomic. However, it is also possible to move unification to the
calculus level, turning the steps of the unification algorithm into inferences.
For calculi that rely on unification procedures returning large or even
infinite sets of unifiers, integrating unification into the calculus is an
attractive method of dovetailing unification and inference. This applies, for
example, to AC-superposition and higher-order superposition. We show that
first-order superposition remains complete when moving unification rules to the
calculus level. We discuss some of the benefits this has even for standard
first-order superposition and provide an experimental evaluation.
\\ ( https://arxiv.org/abs/2403.04775 ,  125kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04803 (*cross-listing*)
Date: Tue, 5 Mar 2024 20:54:56 GMT   (347kb,D)

Title: Enhancing Security in Federated Learning through Adaptive
  Consensus-Based Model Update Validation
Authors: Zahir Alsulaimawi
Categories: cs.CR cs.AI cs.DC cs.LG
\\
  This paper introduces an advanced approach for fortifying Federated Learning
(FL) systems against label-flipping attacks. We propose a simplified
consensus-based verification process integrated with an adaptive thresholding
mechanism. This dynamic thresholding is designed to adjust based on the
evolving landscape of model updates, offering a refined layer of anomaly
detection that aligns with the real-time needs of distributed learning
environments. Our method necessitates a majority consensus among participating
clients to validate updates, ensuring that only vetted and consensual
modifications are applied to the global model. The efficacy of our approach is
validated through experiments on two benchmark datasets in deep learning,
CIFAR-10 and MNIST. Our results indicate a significant mitigation of
label-flipping attacks, bolstering the FL system's resilience. This method
transcends conventional techniques that depend on anomaly detection or
statistical validation by incorporating a verification layer reminiscent of
blockchain's participatory validation without the associated cryptographic
overhead. The innovation of our approach rests in striking an optimal balance
between heightened security measures and the inherent limitations of FL
systems, such as computational efficiency and data privacy. Implementing a
consensus mechanism specifically tailored for FL environments paves the way for
more secure, robust, and trustworthy distributed machine learning applications,
where safeguarding data integrity and model robustness is critical.
\\ ( https://arxiv.org/abs/2403.04803 ,  347kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04899 (*cross-listing*)
Date: Thu, 7 Mar 2024 21:08:51 GMT   (1650kb,D)

Title: Towards Scene Graph Anticipation
Authors: Rohith Peddi, Saksham Singh, Saurabh, Parag Singla, Vibhav Gogate
Categories: cs.CV cs.AI
Comments: Under review
\\
  Spatio-temporal scene graphs represent interactions in a video by decomposing
scenes into individual objects and their pair-wise temporal relationships.
Long-term anticipation of the fine-grained pair-wise relationships between
objects is a challenging problem. To this end, we introduce the task of Scene
Graph Anticipation (SGA). We adapt state-of-the-art scene graph generation
methods as baselines to anticipate future pair-wise relationships between
objects and propose a novel approach SceneSayer. In SceneSayer, we leverage
object-centric representations of relationships to reason about the observed
video frames and model the evolution of relationships between objects. We take
a continuous time perspective and model the latent dynamics of the evolution of
object interactions using concepts of NeuralODE and NeuralSDE, respectively. We
infer representations of future relationships by solving an Ordinary
Differential Equation and a Stochastic Differential Equation, respectively.
Extensive experimentation on the Action Genome dataset validates the efficacy
of the proposed methods.
\\ ( https://arxiv.org/abs/2403.04899 ,  1650kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04917 (*cross-listing*)
Date: Thu, 7 Mar 2024 22:03:36 GMT   (963kb)

Title: A Mixed-Integer Conic Program for the Moving-Target Traveling Salesman
  Problem based on a Graph of Convex Sets
Authors: Allen George Philip, Zhongqiang Ren, Sivakumar Rathinam, Howie Choset
Categories: cs.RO cs.AI cs.DS
Comments: 7 pages, 4 figures
\\
  This paper introduces a new formulation that finds the optimum for the
Moving-Target Traveling Salesman Problem (MT-TSP), which seeks to find a
shortest path for an agent, that starts at a depot, visits a set of moving
targets exactly once within their assigned time-windows, and returns to the
depot. The formulation relies on the key idea that when the targets move along
lines, their trajectories become convex sets within the space-time coordinate
system. The problem then reduces to finding the shortest path within a graph of
convex sets, subject to some speed constraints. We compare our formulation with
the current state-of-the-art Mixed Integer Conic Program (MICP) solver for the
MT-TSP. The experimental results show that our formulation outperforms the MICP
for instances with up to 20 targets, with up to two orders of magnitude
reduction in runtime, and up to a 60\% tighter optimality gap. We also show
that the solution cost from the convex relaxation of our formulation provides
significantly tighter lower bounds for the MT-TSP than the ones from the MICP.
\\ ( https://arxiv.org/abs/2403.04917 ,  963kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04934 (*cross-listing*)
Date: Thu, 7 Mar 2024 22:42:24 GMT   (32134kb,D)

Title: LeTac-MPC: Learning Model Predictive Control for Tactile-reactive
  Grasping
Authors: Zhengtong Xu, Yu She
Categories: cs.RO cs.AI
\\
  Grasping is a crucial task in robotics, necessitating tactile feedback and
reactive grasping adjustments for robust grasping of objects under various
conditions and with differing physical properties. In this paper, we introduce
LeTac-MPC, a learning-based model predictive control (MPC) for tactile-reactive
grasping. Our approach enables the gripper grasp objects with different
physical properties on dynamic and force-interactive tasks. We utilize a
vision-based tactile sensor, GelSight, which is capable of perceiving
high-resolution tactile feedback that contains the information of physical
properties and states of the grasped object. LeTac-MPC incorporates a
differentiable MPC layer designed to model the embeddings extracted by a neural
network (NN) from tactile feedback. This design facilitates convergent and
robust grasping control at a frequency of 25 Hz. We propose a fully automated
data collection pipeline and collect a dataset only using standardized blocks
with different physical properties. However, our trained controller can
generalize to daily objects with different sizes, shapes, materials, and
textures. Experimental results demonstrate the effectiveness and robustness of
the proposed approach. We compare LeTac-MPC with two purely model-based
tactile-reactive controllers (MPC and PD) and open-loop grasping. Our results
show that LeTac-MPC has the best performance on dynamic and force-interactive
tasks and the best generalization ability. We release our code and dataset at
https://github.com/ZhengtongXu/LeTac-MPC.
\\ ( https://arxiv.org/abs/2403.04934 ,  32134kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04940 (*cross-listing*)
Date: Thu, 7 Mar 2024 23:07:46 GMT   (2988kb)

Title: A spatiotemporal style transfer algorithm for dynamic visual stimulus
  generation
Authors: Antonino Greco and Markus Siegel
Categories: cs.CV cs.AI cs.LG q-bio.NC
\\
  Understanding how visual information is encoded in biological and artificial
systems often requires vision scientists to generate appropriate stimuli to
test specific hypotheses. Although deep neural network models have
revolutionized the field of image generation with methods such as image style
transfer, available methods for video generation are scarce. Here, we introduce
the Spatiotemporal Style Transfer (STST) algorithm, a dynamic visual stimulus
generation framework that allows powerful manipulation and synthesis of video
stimuli for vision research. It is based on a two-stream deep neural network
model that factorizes spatial and temporal features to generate dynamic visual
stimuli whose model layer activations are matched to those of input videos. As
an example, we show that our algorithm enables the generation of model
metamers, dynamic stimuli whose layer activations within our two-stream model
are matched to those of natural videos. We show that these generated stimuli
match the low-level spatiotemporal features of their natural counterparts but
lack their high-level semantic features, making it a powerful paradigm to study
object recognition. Late layer activations in deep vision models exhibited a
lower similarity between natural and metameric stimuli compared to early
layers, confirming the lack of high-level information in the generated stimuli.
Finally, we use our generated stimuli to probe the representational
capabilities of predictive coding deep networks. These results showcase
potential applications of our algorithm as a versatile tool for dynamic
stimulus generation in vision science.
\\ ( https://arxiv.org/abs/2403.04940 ,  2988kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04954 (*cross-listing*)
Date: Thu, 7 Mar 2024 23:44:10 GMT   (7314kb)

Title: Fooling Neural Networks for Motion Forecasting via Adversarial Attacks
Authors: Edgar Medina, Leyong Loh
Categories: cs.CV cs.AI
Comments: 11 pages, 8 figures, VISSAP 2024
\\
  Human motion prediction is still an open problem, which is extremely
important for autonomous driving and safety applications. Although there are
great advances in this area, the widely studied topic of adversarial attacks
has not been applied to multi-regression models such as GCNs and MLP-based
architectures in human motion prediction. This work intends to reduce this gap
using extensive quantitative and qualitative experiments in state-of-the-art
architectures similar to the initial stages of adversarial attacks in image
classification. The results suggest that models are susceptible to attacks even
on low levels of perturbation. We also show experiments with 3D transformations
that affect the model performance, in particular, we show that most models are
sensitive to simple rotations and translations which do not alter joint
distances. We conclude that similar to earlier CNN models, motion forecasting
tasks are susceptible to small perturbations and simple 3D transformations.
\\ ( https://arxiv.org/abs/2403.04954 ,  7314kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04960 (*cross-listing*)
Date: Fri, 8 Mar 2024 00:02:30 GMT   (1600kb,D)

Title: SecGPT: An Execution Isolation Architecture for LLM-Based Systems
Authors: Yuhao Wu, Franziska Roesner, Tadayoshi Kohno, Ning Zhang, Umar Iqbal
Categories: cs.CR cs.AI cs.CL cs.CY cs.LG
\\
  Large language models (LLMs) extended as systems, such as ChatGPT, have begun
supporting third-party applications. These LLM apps leverage the de facto
natural language-based automated execution paradigm of LLMs: that is, apps and
their interactions are defined in natural language, provided access to user
data, and allowed to freely interact with each other and the system. These LLM
app ecosystems resemble the settings of earlier computing platforms, where
there was insufficient isolation between apps and the system. Because
third-party apps may not be trustworthy, and exacerbated by the imprecision of
the natural language interfaces, the current designs pose security and privacy
risks for users. In this paper, we propose SecGPT, an architecture for
LLM-based systems that aims to mitigate the security and privacy issues that
arise with the execution of third-party apps. SecGPT's key idea is to isolate
the execution of apps and more precisely mediate their interactions outside of
their isolated environments. We evaluate SecGPT against a number of case study
attacks and demonstrate that it protects against many security, privacy, and
safety issues that exist in non-isolated LLM-based systems. The performance
overhead incurred by SecGPT to improve security is under 0.3x for
three-quarters of the tested queries. To foster follow-up research, we release
SecGPT's source code at https://github.com/llm-platform-security/SecGPT.
\\ ( https://arxiv.org/abs/2403.04960 ,  1600kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04965 (*cross-listing*)
Date: Fri, 8 Mar 2024 00:30:25 GMT   (24632kb,D)

Title: StereoDiffusion: Training-Free Stereo Image Generation Using Latent
  Diffusion Models
Authors: Lezhong Wang, Jeppe Revall Frisvad, Mark Bo Jensen, Siavash Arjomand
  Bigdeli
Categories: cs.CV cs.AI
\\
  The demand for stereo images increases as manufacturers launch more XR
devices. To meet this demand, we introduce StereoDiffusion, a method that,
unlike traditional inpainting pipelines, is trainning free, remarkably
straightforward to use, and it seamlessly integrates into the original Stable
Diffusion model. Our method modifies the latent variable to provide an
end-to-end, lightweight capability for fast generation of stereo image pairs,
without the need for fine-tuning model weights or any post-processing of
images. Using the original input to generate a left image and estimate a
disparity map for it, we generate the latent vector for the right image through
Stereo Pixel Shift operations, complemented by Symmetric Pixel Shift Masking
Denoise and Self-Attention Layers Modification methods to align the right-side
image with the left-side image. Moreover, our proposed method maintains a high
standard of image quality throughout the stereo generation process, achieving
state-of-the-art scores in various quantitative evaluations.
\\ ( https://arxiv.org/abs/2403.04965 ,  24632kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04977 (*cross-listing*)
Date: Fri, 8 Mar 2024 01:23:12 GMT   (21316kb,D)

Title: Node Centrality Approximation For Large Networks Based On Inductive
  Graph Neural Networks
Authors: Yiwei Zou, Ting Li, Zong-fu Luo
Categories: cs.SI cs.AI
\\
  Closeness Centrality (CC) and Betweenness Centrality (BC) are crucial metrics
in network analysis, providing essential reference for discerning the
significance of nodes within complex networks. These measures find wide
applications in critical tasks, such as community detection and network
dismantling. However, their practical implementation on extensive networks
remains computationally demanding due to their high time complexity. To
mitigate these computational challenges, numerous approximation algorithms have
been developed to expedite the computation of CC and BC. Nevertheless, even
these approximations still necessitate substantial processing time when applied
to large-scale networks. Furthermore, their output proves sensitive to even
minor perturbations within the network structure.
  In this work, We redefine the CC and BC node ranking problem as a machine
learning problem and propose the CNCA-IGE model, which is an encoder-decoder
model based on inductive graph neural networks designed to rank nodes based on
specified CC or BC metrics. We incorporate the MLP-Mixer model as the decoder
in the BC ranking prediction task to enhance the model's robustness and
capacity. Our approach is evaluated on diverse synthetic and real-world
networks of varying scales, and the experimental results demonstrate that the
CNCA-IGE model outperforms state-of-the-art baseline models, significantly
reducing execution time while improving performance.
\\ ( https://arxiv.org/abs/2403.04977 ,  21316kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05010 (*cross-listing*)
Date: Fri, 8 Mar 2024 03:16:47 GMT   (3984kb,D)

Title: RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction
Authors: Peng Liu, Dongyang Dai
Categories: cs.SD cs.AI eess.AS
\\
  Recent advancements in generative modeling have led to significant progress
in audio waveform reconstruction from diverse representations. Although
diffusion models have been used for reconstructing audio waveforms, they tend
to exhibit latency issues because they operate at the level of individual
sample points and require a relatively large number of sampling steps. In this
study, we introduce RFWave, a novel multi-band Rectified Flow approach that
reconstructs high-fidelity audio waveforms from Mel-spectrograms. RFWave is
distinctive for generating complex spectrograms and operating at the frame
level, processing all subbands concurrently to enhance efficiency. Thanks to
Rectified Flow, which aims for a flat transport trajectory, RFWave requires
only 10 sampling steps. Empirical evaluations demonstrate that RFWave achieves
exceptional reconstruction quality and superior computational efficiency,
capable of generating audio at a speed 90 times faster than real-time.
\\ ( https://arxiv.org/abs/2403.05010 ,  3984kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05030 (*cross-listing*)
Date: Fri, 8 Mar 2024 04:22:48 GMT   (1142kb,D)

Title: Defending Against Unforeseen Failure Modes with Latent Adversarial
  Training
Authors: Stephen Casper, Lennart Schulze, Oam Patel, Dylan Hadfield-Menell
Categories: cs.CR cs.AI cs.LG
\\
  AI systems sometimes exhibit harmful unintended behaviors post-deployment.
This is often despite extensive diagnostics and debugging by developers.
Minimizing risks from models is challenging because the attack surface is so
large. It is not tractable to exhaustively search for inputs that may cause a
model to fail. Red-teaming and adversarial training (AT) are commonly used to
make AI systems more robust. However, they have not been sufficient to avoid
many real-world failure modes that differ from the ones adversarially trained
on. In this work, we utilize latent adversarial training (LAT) to defend
against vulnerabilities without generating inputs that elicit them. LAT
leverages the compressed, abstract, and structured latent representations of
concepts that the network actually uses for prediction. We use LAT to remove
trojans and defend against held-out classes of adversarial attacks. We show in
image classification, text classification, and text generation tasks that LAT
usually improves both robustness and performance on clean data relative to AT.
This suggests that LAT can be a promising tool for defending against failure
modes that are not explicitly identified by developers.
\\ ( https://arxiv.org/abs/2403.05030 ,  1142kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05050 (*cross-listing*)
Date: Fri, 8 Mar 2024 04:53:53 GMT   (10747kb,D)

Title: DyRoNet: A Low-Rank Adapter Enhanced Dynamic Routing Network for
  Streaming Perception
Authors: Xiang Huang, Zhi-Qi Cheng, Jun-Yan He, Chenyang Li, Wangmeng Xiang,
  Baigui Sun, Xiao Wu
Categories: cs.CV cs.AI cs.MM
\\
  Autonomous driving systems demand real-time, accurate perception to navigate
complex environments. Addressing this, we introduce the Dynamic Router Network
(DyRoNet), a framework that innovates with low-rank dynamic routing for
enhanced streaming perception. By integrating specialized pre-trained branch
networks, fine-tuned for various environmental conditions, DyRoNet achieves a
balance between latency and precision. Its core feature, the speed router
module, intelligently directs input data to the best-suited branch network,
optimizing performance. The extensive evaluations reveal that DyRoNet adapts
effectively to multiple branch selection strategies, setting a new benchmark in
performance across a range of scenarios. DyRoNet not only establishes a new
benchmark for streaming perception but also provides valuable engineering
insights for future work. More project information is available at
https://tastevision.github.io/DyRoNet/
\\ ( https://arxiv.org/abs/2403.05050 ,  10747kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05053 (*cross-listing*)
Date: Fri, 8 Mar 2024 04:58:49 GMT   (13213kb,D)

Title: PrimeComposer: Faster Progressively Combined Diffusion for Image
  Composition with Attention Steering
Authors: Yibin Wang and Weizhong Zhang and Jianwei Zheng and Cheng Jin
Categories: cs.CV cs.AI
\\
  Image composition involves seamlessly integrating given objects into a
specific visual context. The current training-free methods rely on composing
attention weights from several samplers to guide the generator. However, since
these weights are derived from disparate contexts, their combination leads to
coherence confusion in synthesis and loss of appearance information. These
issues worsen with their excessive focus on background generation, even when
unnecessary in this task. This not only slows down inference but also
compromises foreground generation quality. Moreover, these methods introduce
unwanted artifacts in the transition area. In this paper, we formulate image
composition as a subject-based local editing task, solely focusing on
foreground generation. At each step, the edited foreground is combined with the
noisy background to maintain scene consistency. To address the remaining
issues, we propose PrimeComposer, a faster training-free diffuser that
composites the images by well-designed attention steering across different
noise levels. This steering is predominantly achieved by our Correlation
Diffuser, utilizing its self-attention layers at each step. Within these
layers, the synthesized subject interacts with both the referenced object and
background, capturing intricate details and coherent relationships. This prior
information is encoded into the attention weights, which are then integrated
into the self-attention layers of the generator to guide the synthesis process.
Besides, we introduce a Region-constrained Cross-Attention to confine the
impact of specific subject-related words to desired regions, addressing the
unwanted artifacts shown in the prior method thereby further improving the
coherence in the transition area. Our method exhibits the fastest inference
efficiency and extensive experiments demonstrate our superiority both
qualitatively and quantitatively.
\\ ( https://arxiv.org/abs/2403.05053 ,  13213kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05063 (*cross-listing*)
Date: Fri, 8 Mar 2024 05:23:27 GMT   (936kb,D)

Title: Aligning Large Language Models for Controllable Recommendations
Authors: Wensheng Lu, Jianxun Lian, Wei Zhang, Guanghua Li, Mingyang Zhou, Hao
  Liao, Xing Xie
Categories: cs.IR cs.AI
Comments: 13 pages
MSC-class: 68T50
\\
  Inspired by the exceptional general intelligence of Large Language Models
(LLMs), researchers have begun to explore their application in pioneering the
next generation of recommender systems - systems that are conversational,
explainable, and controllable. However, existing literature primarily
concentrates on integrating domain-specific knowledge into LLMs to enhance
accuracy, often neglecting the ability to follow instructions. To address this
gap, we initially introduce a collection of supervised learning tasks,
augmented with labels derived from a conventional recommender model, aimed at
explicitly improving LLMs' proficiency in adhering to recommendation-specific
instructions. Subsequently, we develop a reinforcement learning-based alignment
procedure to further strengthen LLMs' aptitude in responding to users'
intentions and mitigating formatting errors. Through extensive experiments on
two real-world datasets, our method markedly advances the capability of LLMs to
comply with instructions within recommender systems, while sustaining a high
level of accuracy performance.
\\ ( https://arxiv.org/abs/2403.05063 ,  936kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05100 (*cross-listing*)
Date: Fri, 8 Mar 2024 07:03:18 GMT   (3868kb,D)

Title: Exploring the Adversarial Frontier: Quantifying Robustness via
  Adversarial Hypervolume
Authors: Ping Guo, Cheng Gong, Xi Lin, Zhiyuan Yang, Qingfu Zhang
Categories: cs.CR cs.AI cs.CV cs.LG
\\
  The escalating threat of adversarial attacks on deep learning models,
particularly in security-critical fields, has underscored the need for robust
deep learning systems. Conventional robustness evaluations have relied on
adversarial accuracy, which measures a model's performance under a specific
perturbation intensity. However, this singular metric does not fully
encapsulate the overall resilience of a model against varying degrees of
perturbation. To address this gap, we propose a new metric termed adversarial
hypervolume, assessing the robustness of deep learning models comprehensively
over a range of perturbation intensities from a multi-objective optimization
standpoint. This metric allows for an in-depth comparison of defense mechanisms
and recognizes the trivial improvements in robustness afforded by less potent
defensive strategies. Additionally, we adopt a novel training algorithm that
enhances adversarial robustness uniformly across various perturbation
intensities, in contrast to methods narrowly focused on optimizing adversarial
accuracy. Our extensive empirical studies validate the effectiveness of the
adversarial hypervolume metric, demonstrating its ability to reveal subtle
differences in robustness that adversarial accuracy overlooks. This research
contributes a new measure of robustness and establishes a standard for
assessing and benchmarking the resilience of current and future defensive
models against adversarial threats.
\\ ( https://arxiv.org/abs/2403.05100 ,  3868kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05104 (*cross-listing*)
Date: Fri, 8 Mar 2024 07:08:19 GMT   (1367kb,D)

Title: How Culture Shapes What People Want From AI
Authors: Xiao Ge, Chunchen Xu, Daigo Misaki, Hazel Rose Markus, Jeanne L Tsai
Categories: cs.HC cs.AI
Comments: To appear at CHI 2024
DOI: 10.1145/3613904.3642660
\\
  There is an urgent need to incorporate the perspectives of culturally diverse
groups into AI developments. We present a novel conceptual framework for
research that aims to expand, reimagine, and reground mainstream visions of AI
using independent and interdependent cultural models of the self and the
environment. Two survey studies support this framework and provide preliminary
evidence that people apply their cultural models when imagining their ideal AI.
Compared with European American respondents, Chinese respondents viewed it as
less important to control AI and more important to connect with AI, and were
more likely to prefer AI with capacities to influence. Reflecting both cultural
models, findings from African American respondents resembled both European
American and Chinese respondents. We discuss study limitations and future
directions and highlight the need to develop culturally responsive and relevant
AI to serve a broader segment of the world population.
\\ ( https://arxiv.org/abs/2403.05104 ,  1367kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05105 (*cross-listing*)
Date: Fri, 8 Mar 2024 07:09:30 GMT   (1001kb,D)

Title: Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval
Authors: Haochen Han, Qinghua Zheng, Guang Dai, Minnan Luo, Jingdong Wang
Categories: cs.CV cs.AI cs.MM
Comments: CVPR 2024
\\
  Collecting well-matched multimedia datasets is crucial for training
cross-modal retrieval models. However, in real-world scenarios, massive
multimodal data are harvested from the Internet, which inevitably contains
Partially Mismatched Pairs (PMPs). Undoubtedly, such semantical irrelevant data
will remarkably harm the cross-modal retrieval performance. Previous efforts
tend to mitigate this problem by estimating a soft correspondence to
down-weight the contribution of PMPs. In this paper, we aim to address this
challenge from a new perspective: the potential semantic similarity among
unpaired samples makes it possible to excavate useful knowledge from mismatched
pairs. To achieve this, we propose L2RM, a general framework based on Optimal
Transport (OT) that learns to rematch mismatched pairs. In detail, L2RM aims to
generate refined alignments by seeking a minimal-cost transport plan across
different modalities. To formalize the rematching idea in OT, first, we propose
a self-supervised cost function that automatically learns from explicit
similarity-cost mapping relation. Second, we present to model a partial OT
problem while restricting the transport among false positives to further boost
refined alignments. Extensive experiments on three benchmarks demonstrate our
L2RM significantly improves the robustness against PMPs for existing models.
The code is available at https://github.com/hhc1997/L2RM.
\\ ( https://arxiv.org/abs/2403.05105 ,  1001kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05108 (*cross-listing*)
Date: Fri, 8 Mar 2024 07:10:46 GMT   (5628kb)

Title: A Task-Driven Multi-UAV Coalition Formation Mechanism
Authors: Xinpeng Lu, Heng Song, Huailing Ma and Junwu Zhu
Categories: cs.GT cs.AI
\\
  With the rapid advancement of UAV technology, the problem of UAV coalition
formation has become a hotspot. Therefore, designing task-driven multi-UAV
coalition formation mechanism has become a challenging problem. However,
existing coalition formation mechanisms suffer from low relevance between UAVs
and task requirements, resulting in overall low coalition utility and unstable
coalition structures. To address these problems, this paper proposed a novel
multi-UAV coalition network collaborative task completion model, considering
both coalition work capacity and task-requirement relationships. This model
stimulated the formation of coalitions that match task requirements by using a
revenue function based on the coalition's revenue threshold. Subsequently, an
algorithm for coalition formation based on marginal utility was proposed.
Specifically, the algorithm utilized Shapley value to achieve fair utility
distribution within the coalition, evaluated coalition values based on marginal
utility preference order, and achieved stable coalition partition through a
limited number of iterations. Additionally, we theoretically proved that this
algorithm has Nash equilibrium solution. Finally, experimental results
demonstrated that the proposed algorithm, compared to currently classical
algorithms, not only forms more stable coalitions but also further enhances the
overall utility of coalitions effectively.
\\ ( https://arxiv.org/abs/2403.05108 ,  5628kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05110 (*cross-listing*)
Date: Fri, 8 Mar 2024 07:15:38 GMT   (22923kb,D)

Title: Efficient Data Collection for Robotic Manipulation via Compositional
  Generalization
Authors: Jensen Gao, Annie Xie, Ted Xiao, Chelsea Finn, Dorsa Sadigh
Categories: cs.RO cs.AI cs.LG
Comments: 17 pages
\\
  Data collection has become an increasingly important problem in robotic
manipulation, yet there still lacks much understanding of how to effectively
collect data to facilitate broad generalization. Recent works on large-scale
robotic data collection typically vary a wide range of environmental factors
during data collection, such as object types and table textures. While these
works attempt to cover a diverse variety of scenarios, they do not explicitly
account for the possible compositional abilities of policies trained on the
data. If robot policies are able to compose different environmental factors of
variation (e.g., object types, table heights) from their training data to
succeed when encountering unseen factor combinations, then we can exploit this
to avoid collecting data for situations that composition would address. To
investigate this possibility, we conduct thorough empirical studies both in
simulation and on a real robot that compare data collection strategies and
assess whether visual imitation learning policies can compose environmental
factors. We find that policies do exhibit composition, although leveraging
prior robotic datasets is critical for this on a real robot. We use these
insights to provide better practices for in-domain data collection by proposing
data collection strategies that exploit composition, which can induce better
generalization than naive approaches for the same amount of effort during data
collection. We further demonstrate that a real robot policy trained on data
from such a strategy achieves a success rate of 77.5% when transferred to
entirely new environments that encompass unseen combinations of environmental
factors, whereas policies trained using data collected without accounting for
environmental variation fail to transfer effectively, with a success rate of
only 2.5%. We provide videos at http://iliad.stanford.edu/robot-data-comp/.
\\ ( https://arxiv.org/abs/2403.05110 ,  22923kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05125 (*cross-listing*)
Date: Fri, 8 Mar 2024 07:41:47 GMT   (24359kb,D)

Title: Evaluating Text-to-Image Generative Models: An Empirical Study on Human
  Image Synthesis
Authors: Muxi Chen, Yi Liu, Jian Yi, Changran Xu, Qiuxia Lai, Hongliang Wang,
  Tsung-Yi Ho, Qiang Xu
Categories: cs.CV cs.AI
\\
  In this paper, we present an empirical study introducing a nuanced evaluation
framework for text-to-image (T2I) generative models, applied to human image
synthesis. Our framework categorizes evaluations into two distinct groups:
first, focusing on image qualities such as aesthetics and realism, and second,
examining text conditions through concept coverage and fairness. We introduce
an innovative aesthetic score prediction model that assesses the visual appeal
of generated images and unveils the first dataset marked with low-quality
regions in generated human images to facilitate automatic defect detection. Our
exploration into concept coverage probes the model's effectiveness in
interpreting and rendering text-based concepts accurately, while our analysis
of fairness reveals biases in model outputs, with an emphasis on gender, race,
and age. While our study is grounded in human imagery, this dual-faceted
approach is designed with the flexibility to be applicable to other forms of
image generation, enhancing our understanding of generative models and paving
the way to the next generation of more sophisticated, contextually aware, and
ethically attuned generative models. We will release our code, the data used
for evaluating generative models and the dataset annotated with defective areas
soon.
\\ ( https://arxiv.org/abs/2403.05125 ,  24359kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05129 (*cross-listing*)
Date: Fri, 8 Mar 2024 07:52:17 GMT   (3552kb,D)

Title: Unraveling the Molecular Magic: AI Insights on the Formation of
  Extraordinarily Stretchable Hydrogels
Authors: Shahriar Hojjati Emmami, Ali Pilehvar Meibody, Lobat Tayebi,
  Mohammadamin Tavakoli, Pierre Baldi
Categories: cond-mat.soft cs.AI
\\
  The deliberate manipulation of ammonium persulfate, methylenebisacrylamide,
dimethyleacrylamide, and polyethylene oxide concentrations resulted in the
development of a hydrogel with an exceptional stretchability, capable of
extending up to 260 times its original length. This study aims to elucidate the
molecular architecture underlying this unique phenomenon by exploring potential
reaction mechanisms, facilitated by an artificial intelligence prediction
system. Artificial intelligence predictor introduces a novel approach to
interlinking two polymers, involving the formation of networks interconnected
with linear chains following random chain scission. This novel configuration
leads to the emergence of a distinct type of hydrogel, herein referred to as a
"Span Network." Additionally, Fourier-transform infrared spectroscopy (FTIR) is
used to investigate functional groups that may be implicated in the proposed
mechanism, with ester formation confirmed among numerous hydroxyl end groups
obtained from chain scission of PEO and carboxyl groups formed on hydrogel
networks.
\\ ( https://arxiv.org/abs/2403.05129 ,  3552kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05149 (*cross-listing*)
Date: Fri, 8 Mar 2024 08:38:50 GMT   (2445kb,D)

Title: Inverse Design of Photonic Crystal Surface Emitting Lasers is a Sequence
  Modeling Problem
Authors: Ceyao Zhang, Renjie Li, Cheng Zhang, Zhaoyu Zhang, Feng Yin
Categories: physics.app-ph cs.AI
Comments: accepted by AAAI workshop
  AI2ASE(2024)https://ai-2-ase.github.io/papers/29%5cCameraReady%5cPIT__PSCEL_inverse_design_transformer.pdf
\\
  Photonic Crystal Surface Emitting Lasers (PCSEL)'s inverse design demands
expert knowledge in physics, materials science, and quantum mechanics which is
prohibitively labor-intensive. Advanced AI technologies, especially
reinforcement learning (RL), have emerged as a powerful tool to augment and
accelerate this inverse design process. By modeling the inverse design of PCSEL
as a sequential decision-making problem, RL approaches can construct a
satisfactory PCSEL structure from scratch. However, the data inefficiency
resulting from online interactions with precise and expensive simulation
environments impedes the broader applicability of RL approaches. Recently,
sequential models, especially the Transformer architecture, have exhibited
compelling performance in sequential decision-making problems due to their
simplicity and scalability to large language models. In this paper, we
introduce a novel framework named PCSEL Inverse Design Transformer (PiT) that
abstracts the inverse design of PCSEL as a sequence modeling problem. The
central part of our PiT is a Transformer-based structure that leverages the
past trajectories and current states to predict the current actions. Compared
with the traditional RL approaches, PiT can output the optimal actions and
achieve target PCSEL designs by leveraging offline data and conditioning on the
desired return. Results demonstrate that PiT achieves superior performance and
data efficiency compared to baselines.
\\ ( https://arxiv.org/abs/2403.05149 ,  2445kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05168 (*cross-listing*)
Date: Fri, 8 Mar 2024 09:16:47 GMT   (4966kb,D)

Title: Unlocking the Potential of Multimodal Unified Discrete Representation
  through Training-Free Codebook Optimization and Hierarchical Alignment
Authors: Hai Huang, Yan Xia, Shengpeng Ji, Shulei Wang, Hanting Wang, Jieming
  Zhu, Zhenhua Dong, Zhou Zhao
Categories: cs.CV cs.AI
\\
  Recent advances in representation learning have demonstrated the significance
of multimodal alignment. The Dual Cross-modal Information Disentanglement
(DCID) model, utilizing a unified codebook, shows promising results in
achieving fine-grained representation and cross-modal generalization. However,
it is still hindered by equal treatment of all channels and neglect of minor
event information, resulting in interference from irrelevant channels and
limited performance in fine-grained tasks. Thus, in this work, We propose a
Training-free Optimization of Codebook (TOC) method to enhance model
performance by selecting important channels in the unified space without
retraining. Additionally, we introduce the Hierarchical Dual Cross-modal
Information Disentanglement (H-DCID) approach to extend information separation
and alignment to two levels, capturing more cross-modal details. The experiment
results demonstrate significant improvements across various downstream tasks,
with TOC contributing to an average improvement of 1.70% for DCID on four
tasks, and H-DCID surpassing DCID by an average of 3.64%. The combination of
TOC and H-DCID further enhances performance, exceeding DCID by 4.43%. These
findings highlight the effectiveness of our methods in facilitating robust and
nuanced cross-modal learning, opening avenues for future enhancements. The
source code and pre-trained models can be accessed at
https://github.com/haihuangcode/TOC_H-DCID.
\\ ( https://arxiv.org/abs/2403.05168 ,  4966kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05220 (*cross-listing*)
Date: Fri, 8 Mar 2024 11:18:26 GMT   (7040kb,D)

Title: Synthetic Privileged Information Enhances Medical Image Representation
  Learning
Authors: Lucas Farndale, Chris Walsh, Robert Insall, Ke Yuan
Categories: cs.CV cs.AI cs.LG q-bio.TO
\\
  Multimodal self-supervised representation learning has consistently proven to
be a highly effective method in medical image analysis, offering strong task
performance and producing biologically informed insights. However, these
methods heavily rely on large, paired datasets, which is prohibitive for their
use in scenarios where paired data does not exist, or there is only a small
amount available. In contrast, image generation methods can work well on very
small datasets, and can find mappings between unpaired datasets, meaning an
effectively unlimited amount of paired synthetic data can be generated. In this
work, we demonstrate that representation learning can be significantly improved
by synthetically generating paired information, both compared to training on
either single-modality (up to 4.4x error reduction) or authentic multi-modal
paired datasets (up to 5.6x error reduction).
\\ ( https://arxiv.org/abs/2403.05220 ,  7040kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05239 (*cross-listing*)
Date: Fri, 8 Mar 2024 11:59:32 GMT   (20245kb,D)

Title: Towards Effective Usage of Human-Centric Priors in Diffusion Models for
  Text-based Human Image Generation
Authors: Junyan Wang, Zhenhong Sun, Zhiyu Tan, Xuanbai Chen, Weihua Chen, Hao
  Li, Cheng Zhang, Yang Song
Categories: cs.CV cs.AI cs.LG
Comments: Accepted to CVPR 2024
\\
  Vanilla text-to-image diffusion models struggle with generating accurate
human images, commonly resulting in imperfect anatomies such as unnatural
postures or disproportionate limbs.Existing methods address this issue mostly
by fine-tuning the model with extra images or adding additional controls --
human-centric priors such as pose or depth maps -- during the image generation
phase. This paper explores the integration of these human-centric priors
directly into the model fine-tuning stage, essentially eliminating the need for
extra conditions at the inference stage. We realize this idea by proposing a
human-centric alignment loss to strengthen human-related information from the
textual prompts within the cross-attention maps. To ensure semantic detail
richness and human structural accuracy during fine-tuning, we introduce
scale-aware and step-wise constraints within the diffusion process, according
to an in-depth analysis of the cross-attention layer. Extensive experiments
show that our method largely improves over state-of-the-art text-to-image
models to synthesize high-quality human images based on user-written prompts.
Project page: \url{https://hcplayercvpr2024.github.io}.
\\ ( https://arxiv.org/abs/2403.05239 ,  20245kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05245 (*cross-listing*)
Date: Fri, 8 Mar 2024 12:07:18 GMT   (7243kb,D)

Title: Noise Level Adaptive Diffusion Model for Robust Reconstruction of
  Accelerated MRI
Authors: Shoujin Huang, Guanxiong Luo, Xi Wang, Ziran Chen, Yuwan Wang,
  Huaishui Yang, Pheng-Ann Heng, Lingyan Zhang, Mengye Lyu
Categories: eess.IV cs.AI cs.CV
\\
  In general, diffusion model-based MRI reconstruction methods incrementally
remove artificially added noise while imposing data consistency to reconstruct
the underlying images. However, real-world MRI acquisitions already contain
inherent noise due to thermal fluctuations. This phenomenon is particularly
notable when using ultra-fast, high-resolution imaging sequences for advanced
research, or using low-field systems favored by low- and middle-income
countries. These common scenarios can lead to sub-optimal performance or
complete failure of existing diffusion model-based reconstruction techniques.
Specifically, as the artificially added noise is gradually removed, the
inherent MRI noise becomes increasingly pronounced, making the actual noise
level inconsistent with the predefined denoising schedule and consequently
inaccurate image reconstruction. To tackle this problem, we propose a posterior
sampling strategy with a novel NoIse Level Adaptive Data Consistency (Nila-DC)
operation. Extensive experiments are conducted on two public datasets and an
in-house clinical dataset with field strength ranging from 0.3T to 3T, showing
that our method surpasses the state-of-the-art MRI reconstruction methods, and
is highly robust against various noise levels. The code will be released after
review.
\\ ( https://arxiv.org/abs/2403.05245 ,  7243kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05297 (*cross-listing*)
Date: Fri, 8 Mar 2024 13:24:46 GMT   (26788kb,D)

Title: PEEB: Part-based Image Classifiers with an Explainable and Editable
  Language Bottleneck
Authors: Thang M. Pham, Peijie Chen, Tin Nguyen, Seunghyun Yoon, Trung Bui, Anh
  Nguyen
Categories: cs.CV cs.AI cs.CL
Comments: Under review
\\
  CLIP-based classifiers rely on the prompt containing a {class name} that is
known to the text encoder. That is, CLIP performs poorly on new classes or the
classes whose names rarely appear on the Internet (e.g., scientific names of
birds). For fine-grained classification, we propose PEEB - an explainable and
editable classifier to (1) express the class name into a set of pre-defined
text descriptors that describe the visual parts of that class; and (2) match
the embeddings of the detected parts to their textual descriptors in each class
to compute a logit score for classification. In a zero-shot setting where the
class names are unknown, PEEB outperforms CLIP by a large margin (~10x in
accuracy). Compared to part-based classifiers, PEEB is not only the
state-of-the-art on the supervised-learning setting (88.80% accuracy) but also
the first to enable users to edit the class definitions to form a new
classifier without retraining. Compared to concept bottleneck models, PEEB is
also the state-of-the-art in both zero-shot and supervised learning settings.
\\ ( https://arxiv.org/abs/2403.05297 ,  26788kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05334 (*cross-listing*)
Date: Fri, 8 Mar 2024 14:10:25 GMT   (585kb,D)

Title: WatChat: Explaining perplexing programs by debugging mental models
Authors: Kartik Chandra, Tzu-Mao Li, Rachit Nigam, Joshua Tenenbaum, Jonathan
  Ragan-Kelley
Categories: cs.PL cs.AI cs.HC
\\
  Often, a good explanation for a program's unexpected behavior is a bug in the
programmer's code. But sometimes, an even better explanation is a bug in the
programmer's mental model of the language they are using. Instead of merely
debugging our current code ("giving the programmer a fish"), what if our tools
could directly debug our mental models ("teaching the programmer to fish")? In
this paper, we apply ideas from computational cognitive science to do exactly
that. Given a perplexing program, we use program synthesis techniques to
automatically infer potential misconceptions that might cause the user to be
surprised by the program's behavior. By analyzing these misconceptions, we
provide succinct, useful explanations of the program's behavior. Our methods
can even be inverted to synthesize pedagogical example programs for diagnosing
and correcting misconceptions in students.
\\ ( https://arxiv.org/abs/2403.05334 ,  585kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05379 (*cross-listing*)
Date: Fri, 8 Mar 2024 15:16:15 GMT   (11097kb,D)

Title: Self-Supervised Multiple Instance Learning for Acute Myeloid Leukemia
  Classification
Authors: Salome Kazeminia, Max Joosten, Dragan Bosnacki, Carsten Marr
Categories: cs.CV cs.AI
\\
  Automated disease diagnosis using medical image analysis relies on deep
learning, often requiring large labeled datasets for supervised model training.
Diseases like Acute Myeloid Leukemia (AML) pose challenges due to scarce and
costly annotations on a single-cell level. Multiple Instance Learning (MIL)
addresses weakly labeled scenarios but necessitates powerful encoders typically
trained with labeled data. In this study, we explore Self-Supervised Learning
(SSL) as a pre-training approach for MIL-based AML subtype classification from
blood smears, removing the need for labeled data during encoder training. We
investigate the three state-of-the-art SSL methods SimCLR, SwAV, and DINO, and
compare their performance against supervised pre-training. Our findings show
that SSL-pretrained encoders achieve comparable performance, showcasing the
potential of SSL in MIL. This breakthrough offers a cost-effective and
data-efficient solution, propelling the field of AI-based disease diagnosis.
\\ ( https://arxiv.org/abs/2403.05379 ,  11097kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05396 (*cross-listing*)
Date: Fri, 8 Mar 2024 15:51:43 GMT   (6772kb,D)

Title: HistGen: Histopathology Report Generation via Local-Global Feature
  Encoding and Cross-modal Context Interaction
Authors: Zhengrui Guo, Jiabo Ma, Yingxue Xu, Yihui Wang, Liansheng Wang, and
  Hao Chen
Categories: cs.CV cs.AI cs.CL cs.LG
\\
  Histopathology serves as the gold standard in cancer diagnosis, with clinical
reports being vital in interpreting and understanding this process, guiding
cancer treatment and patient care. The automation of histopathology report
generation with deep learning stands to significantly enhance clinical
efficiency and lessen the labor-intensive, time-consuming burden on
pathologists in report writing. In pursuit of this advancement, we introduce
HistGen, a multiple instance learning-empowered framework for histopathology
report generation together with the first benchmark dataset for evaluation.
Inspired by diagnostic and report-writing workflows, HistGen features two
delicately designed modules, aiming to boost report generation by aligning
whole slide images (WSIs) and diagnostic reports from local and global
granularity. To achieve this, a local-global hierarchical encoder is developed
for efficient visual feature aggregation from a region-to-slide perspective.
Meanwhile, a cross-modal context module is proposed to explicitly facilitate
alignment and interaction between distinct modalities, effectively bridging the
gap between the extensive visual sequences of WSIs and corresponding highly
summarized reports. Experimental results on WSI report generation show the
proposed model outperforms state-of-the-art (SOTA) models by a large margin.
Moreover, the results of fine-tuning our model on cancer subtyping and survival
analysis tasks further demonstrate superior performance compared to SOTA
methods, showcasing strong transfer learning capability. Dataset, model
weights, and source code are available in
https://github.com/dddavid4real/HistGen.
\\ ( https://arxiv.org/abs/2403.05396 ,  6772kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05465 (*cross-listing*)
Date: Fri, 8 Mar 2024 17:28:49 GMT   (5649kb,D)

Title: Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit
  Encodings for Efficient DNN Inference
Authors: Akshat Ramachandran, Zishen Wan, Geonhwa Jeong, John Gustafson, Tushar
  Krishna
Categories: cs.AR cs.AI cs.LG cs.NE
Comments: 2024 61st IEEE/ACM Design Automation Conference (DAC)
\\
  Traditional Deep Neural Network (DNN) quantization methods using integer,
fixed-point, or floating-point data types struggle to capture diverse DNN
parameter distributions at low precision, and often require large silicon
overhead and intensive quantization-aware training. In this study, we introduce
Logarithmic Posits (LP), an adaptive, hardware-friendly data type inspired by
posits that dynamically adapts to DNN weight/activation distributions by
parameterizing LP bit fields. We also develop a novel genetic-algorithm based
framework, LP Quantization (LPQ), to find optimal layer-wise LP parameters
while reducing representational divergence between quantized and full-precision
models through a novel global-local contrastive objective. Additionally, we
design a unified mixed-precision LP accelerator (LPA) architecture comprising
of processing elements (PEs) incorporating LP in the computational datapath.
Our algorithm-hardware co-design demonstrates on average <1% drop in top-1
accuracy across various CNN and ViT models. It also achieves ~ 2x improvements
in performance per unit area and 2.2x gains in energy efficiency compared to
state-of-the-art quantization accelerators using different data types.
\\ ( https://arxiv.org/abs/2403.05465 ,  5649kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05535 (*cross-listing*)
Date: Fri, 8 Mar 2024 18:58:46 GMT   (6368kb,D)

Title: Tell, Don't Show!: Language Guidance Eases Transfer Across Domains in
  Images and Videos
Authors: Tarun Kalluri and Bodhisattwa Prasad Majumder and Manmohan Chandraker
Categories: cs.CV cs.AI cs.CL
Comments: Project Page and Code: https://tarun005.github.io/lagtran/
\\
  We introduce LaGTran, a novel framework that utilizes readily available or
easily acquired text descriptions to guide robust transfer of discriminative
knowledge from labeled source to unlabeled target data with domain shifts.
While unsupervised adaptation methods have been established to address this
problem, they show limitations in handling challenging domain shifts due to
their exclusive operation within the pixel-space. Motivated by our observation
that semantically richer text modality has more favorable transfer properties,
we devise a transfer mechanism to use a source-trained text-classifier to
generate predictions on the target text descriptions, and utilize these
predictions as supervision for the corresponding images. Our approach driven by
language guidance is surprisingly easy and simple, yet significantly
outperforms all prior approaches on challenging datasets like GeoNet and
DomainNet, validating its extreme effectiveness. To further extend the scope of
our study beyond images, we introduce a new benchmark to study ego-exo transfer
in videos and find that our language-aided LaGTran yields significant gains in
this highly challenging and non-trivial transfer setting. Code, models, and
proposed datasets are publicly available at
https://tarun005.github.io/lagtran/.
\\ ( https://arxiv.org/abs/2403.05535 ,  6368kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04786 (*cross-listing*)
Date: Sun, 3 Mar 2024 04:46:21 GMT   (6918kb,D)

Title: Breaking Down the Defenses: A Comparative Survey of Attacks on Large
  Language Models
Authors: Arijit Ghosh Chowdhury, Md Mofijul Islam, Vaibhav Kumar, Faysal
  Hossain Shezan, Vaibhav Kumar, Vinija Jain, Aman Chadha
Categories: cs.CR cs.CL
\\
  Large Language Models (LLMs) have become a cornerstone in the field of
Natural Language Processing (NLP), offering transformative capabilities in
understanding and generating human-like text. However, with their rising
prominence, the security and vulnerability aspects of these models have
garnered significant attention. This paper presents a comprehensive survey of
the various forms of attacks targeting LLMs, discussing the nature and
mechanisms of these attacks, their potential impacts, and current defense
strategies. We delve into topics such as adversarial attacks that aim to
manipulate model outputs, data poisoning that affects model training, and
privacy concerns related to training data exploitation. The paper also explores
the effectiveness of different attack methodologies, the resilience of LLMs
against these attacks, and the implications for model integrity and user trust.
By examining the latest research, we provide insights into the current
landscape of LLM vulnerabilities and defense mechanisms. Our objective is to
offer a nuanced understanding of LLM attacks, foster awareness within the AI
community, and inspire robust solutions to mitigate these risks in future
developments.
\\ ( https://arxiv.org/abs/2403.04786 ,  6918kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04804 (*cross-listing*)
Date: Tue, 5 Mar 2024 22:09:58 GMT   (304kb,D)

Title: AttentionStitch: How Attention Solves the Speech Editing Problem
Authors: Antonios Alexos, Pierre Baldi
Categories: eess.AS cs.CL cs.LG cs.MM
Comments: Accepted in Machine Learning for Audio workship in NeurIPS 2023
\\
  The generation of natural and high-quality speech from text is a challenging
problem in the field of natural language processing. In addition to speech
generation, speech editing is also a crucial task, which requires the seamless
and unnoticeable integration of edited speech into synthesized speech. We
propose a novel approach to speech editing by leveraging a pre-trained
text-to-speech (TTS) model, such as FastSpeech 2, and incorporating a double
attention block network on top of it to automatically merge the synthesized
mel-spectrogram with the mel-spectrogram of the edited text. We refer to this
model as AttentionStitch, as it harnesses attention to stitch audio samples
together. We evaluate the proposed AttentionStitch model against
state-of-the-art baselines on both single and multi-speaker datasets, namely
LJSpeech and VCTK. We demonstrate its superior performance through an objective
and a subjective evaluation test involving 15 human participants.
AttentionStitch is capable of producing high-quality speech, even for words not
seen during training, while operating automatically without the need for human
intervention. Moreover, AttentionStitch is fast during both training and
inference and is able to generate human-sounding edited speech.
\\ ( https://arxiv.org/abs/2403.04804 ,  304kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04808 (*cross-listing*)
Date: Wed, 6 Mar 2024 10:55:30 GMT   (5369kb,D)

Title: WaterMax: breaking the LLM watermark detectability-robustness-quality
  trade-off
Authors: Eva Giboulot and Furon Teddy
Categories: cs.CR cs.CL cs.LG
\\
  Watermarking is a technical means to dissuade malfeasant usage of Large
Language Models. This paper proposes a novel watermarking scheme, so-called
WaterMax, that enjoys high detectability while sustaining the quality of the
generated text of the original LLM. Its new design leaves the LLM untouched (no
modification of the weights, logits, temperature, or sampling technique).
WaterMax balances robustness and complexity contrary to the watermarking
techniques of the literature inherently provoking a trade-off between quality
and robustness. Its performance is both theoretically proven and experimentally
validated. It outperforms all the SotA techniques under the most complete
benchmark suite.
\\ ( https://arxiv.org/abs/2403.04808 ,  5369kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04811 (*cross-listing*)
Date: Wed, 6 Mar 2024 21:45:35 GMT   (263kb,D)

Title: Quantifying Contamination in Evaluating Code Generation Capabilities of
  Language Models
Authors: Martin Riddell, Ansong Ni, Arman Cohan
Categories: cs.SE cs.CL cs.LG
\\
  While large language models have achieved remarkable performance on various
code generation benchmarks, there have been growing concerns regarding
potential contamination of these benchmarks as they may be leaked into
pretraining and finetuning data. While recent work has investigated
contamination in natural language generation and understanding tasks, there has
been less extensive research into how data contamination impacts the evaluation
of code generation, which is critical for understanding the robustness and
reliability of LLMs in programming contexts. In this work, we perform a
comprehensive study of data contamination of popular code generation
benchmarks, and precisely quantify their overlap with pretraining corpus
through both surface-level and semantic-level matching. In our experiments, we
show that there are substantial overlap between popular code generation
benchmarks and open training corpus, and models perform significantly better on
the subset of the benchmarks where similar solutions are seen during training.
We also conduct extensive analysis on the factors that affects model
memorization and generalization, such as model size, problem difficulty, and
question length. We release all resulting files from our matching pipeline for
future research.
\\ ( https://arxiv.org/abs/2403.04811 ,  263kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05286 (*cross-listing*)
Date: Fri, 8 Mar 2024 13:10:59 GMT   (7991kb,D)

Title: LLM4Decompile: Decompiling Binary Code with Large Language Models
Authors: Hanzhuo Tan, Qi Luo, Jing Li, Yuqun Zhang
Categories: cs.PL cs.CL
Comments: on going
\\
  Decompilation aims to restore compiled code to human-readable source code,
but struggles with details like names and structure. Large language models
(LLMs) show promise for programming tasks, motivating their application to
decompilation. However, there does not exist any open-source LLM for
decompilation. Moreover, existing decompilation evaluation systems mainly
consider token-level accuracy and largely ignore code executability, which is
the most important feature of any program. Therefore, we release the first
open-access decompilation LLMs ranging from 1B to 33B pre-trained on 4 billion
tokens of C source code and the corresponding assembly code. The open-source
LLMs can serve as baselines for further development in the field. To ensure
practical program evaluation, we introduce Decompile-Eval, the first dataset
that considers re-compilability and re-executability for decompilation. The
benchmark emphasizes the importance of evaluating the decompilation model from
the perspective of program semantics. Experiments indicate that our
LLM4Decompile has demonstrated the capability to accurately decompile 21% of
the assembly code, which achieves a 50% improvement over GPT-4. Our code,
dataset, and models are released at
https://github.com/albertan017/LLM4Decompile
\\ ( https://arxiv.org/abs/2403.05286 ,  7991kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04781 (*cross-listing*)
Date: Sat, 2 Mar 2024 11:20:24 GMT   (1099kb)

Title: Selective Encryption using Segmentation Mask with Chaotic Henon Map for
  Multidimensional Medical Images
Authors: S Arut Prakash, Aditya Ganesh Kumar, Prabhu Shankar K. C., Lithicka
  Anandavel, Aditya Lakshmi Narayanan
Categories: cs.CR cs.CV cs.LG eess.IV
\\
  A user-centric design and resource optimization should be at the center of
any technology or innovation. The user-centric perspective gives the developer
the opportunity to develop with task-based optimization. The user in the
medical image field is a medical professional who analyzes the medical images
and gives their diagnosis results to the patient. This scheme, having the
medical professional user's perspective, innovates in the area of Medical Image
storage and security. The architecture is designed with three main segments,
namely: Segmentation, Storage, and Retrieval. This architecture was designed
owing to the fact that the number of retrieval operations done by medical
professionals was toweringly higher when compared to the storage operations
done for some handful number of times for a particular medical image. This
gives room for our innovation to segment out the medically indispensable part
of the medical image, encrypt it, and store it. By encrypting the vital parts
of the image using a strong encryption algorithm like the chaotic Henon map, we
are able to keep the security intact. Now retrieving the medical image demands
only the computationally less stressing decryption of the segmented region of
interest. The decryption of the segmented region of interest results in the
full recovery of the medical image which can be viewed on demand by the medical
professionals for various diagnosis purposes. In this scheme, we were able to
achieve a retrieval speed improvement of around 47% when compared to a full
image encryption of brain medical CT images.
\\ ( https://arxiv.org/abs/2403.04781 ,  1099kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04784 (*cross-listing*)
Date: Sat, 2 Mar 2024 20:25:38 GMT   (2106kb,D)

Title: Analysis of Privacy Leakage in Federated Large Language Models
Authors: Minh N. Vu, Truc Nguyen, Tre' R. Jeter, My T. Thai
Categories: cs.CR cs.LG
\\
  With the rapid adoption of Federated Learning (FL) as the training and tuning
protocol for applications utilizing Large Language Models (LLMs), recent
research highlights the need for significant modifications to FL to accommodate
the large-scale of LLMs. While substantial adjustments to the protocol have
been introduced as a response, comprehensive privacy analysis for the adapted
FL protocol is currently lacking.
  To address this gap, our work delves into an extensive examination of the
privacy analysis of FL when used for training LLMs, both from theoretical and
practical perspectives. In particular, we design two active membership
inference attacks with guaranteed theoretical success rates to assess the
privacy leakages of various adapted FL configurations. Our theoretical findings
are translated into practical attacks, revealing substantial privacy
vulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and
OpenAI's GPTs, across multiple real-world language datasets. Additionally, we
conduct thorough experiments to evaluate the privacy leakage of these models
when data is protected by state-of-the-art differential privacy (DP)
mechanisms.
\\ ( https://arxiv.org/abs/2403.04784 ,  2106kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04800 (*cross-listing*)
Date: Tue, 5 Mar 2024 18:52:50 GMT   (1636kb,D)

Title: (Un)paired signal-to-signal translation with 1D conditional GANs
Authors: Eric Easthope
Categories: eess.AS cs.CV cs.GR cs.LG
\\
  I show that a one-dimensional (1D) conditional generative adversarial network
(cGAN) with an adversarial training architecture is capable of unpaired
signal-to-signal ("sig2sig") translation. Using a simplified CycleGAN model
with 1D layers and wider convolutional kernels, mirroring WaveGAN to reframe
two-dimensional (2D) image generation as 1D audio generation, I show that
recasting the 2D image-to-image translation task to a 1D signal-to-signal
translation task with deep convolutional GANs is possible without substantial
modification to the conventional U-Net model and adversarial architecture
developed as CycleGAN. With this I show for a small tunable dataset that noisy
test signals unseen by the 1D CycleGAN model and without paired training
transform from the source domain to signals similar to paired test signals in
the translated domain, especially in terms of frequency, and I quantify these
differences in terms of correlation and error.
\\ ( https://arxiv.org/abs/2403.04800 ,  1636kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04809 (*cross-listing*)
Date: Wed, 6 Mar 2024 18:33:27 GMT   (26348kb,D)

Title: Investigation of the Impact of Synthetic Training Data in the Industrial
  Application of Terminal Strip Object Detection
Authors: Nico Baumgart, Markus Lange-Hegermann, Mike M\"ucke
Categories: cs.CV cs.LG
\\
  In industrial manufacturing, numerous tasks of visually inspecting or
detecting specific objects exist that are currently performed manually or by
classical image processing methods. Therefore, introducing recent deep learning
models to industrial environments holds the potential to increase productivity
and enable new applications. However, gathering and labeling sufficient data is
often intractable, complicating the implementation of such projects. Hence,
image synthesis methods are commonly used to generate synthetic training data
from 3D models and annotate them automatically, although it results in a
sim-to-real domain gap. In this paper, we investigate the sim-to-real
generalization performance of standard object detectors on the complex
industrial application of terminal strip object detection. Combining domain
randomization and domain knowledge, we created an image synthesis pipeline for
automatically generating the training data. Moreover, we manually annotated 300
real images of terminal strips for the evaluation. The results show the
cruciality of the objects of interest to have the same scale in either domain.
Nevertheless, under optimized scaling conditions, the sim-to-real performance
difference in mean average precision amounts to 2.69 % for RetinaNet and 0.98 %
for Faster R-CNN, qualifying this approach for industrial requirements.
\\ ( https://arxiv.org/abs/2403.04809 ,  26348kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04822 (*cross-listing*)
Date: Thu, 7 Mar 2024 15:44:50 GMT   (3381kb,D)

Title: UniTable: Towards a Unified Framework for Table Structure Recognition
  via Self-Supervised Pretraining
Authors: ShengYun Peng, Seongmin Lee, Xiaojing Wang, Rajarajeswari
  Balasubramaniyan, Duen Horng Chau
Categories: cs.CV cs.LG
\\
  Tables convey factual and quantitative data with implicit conventions created
by humans that are often challenging for machines to parse. Prior work on table
structure recognition (TSR) has mainly centered around complex task-specific
combinations of available inputs and tools. We present UniTable, a training
framework that unifies both the training paradigm and training objective of
TSR. Its training paradigm combines the simplicity of purely pixel-level inputs
with the effectiveness and scalability empowered by self-supervised pretraining
(SSP) from diverse unannotated tabular images. Our framework unifies the
training objectives of all three TSR tasks - extracting table structure, cell
content, and cell bounding box (bbox) - into a unified task-agnostic training
objective: language modeling. Extensive quantitative and qualitative analyses
highlight UniTable's state-of-the-art (SOTA) performance on four of the largest
TSR datasets. To promote reproducible research, enhance transparency, and SOTA
innovations, we open-source our code at https://github.com/poloclub/unitable
and release the first-of-its-kind Jupyter Notebook of the whole inference
pipeline, fine-tuned across multiple TSR datasets, supporting all three TSR
tasks.
\\ ( https://arxiv.org/abs/2403.04822 ,  3381kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04867 (*cross-listing*)
Date: Thu, 7 Mar 2024 19:36:05 GMT   (8819kb,D)

Title: Group Privacy Amplification and Unified Amplification by Subsampling for
  R\'enyi Differential Privacy
Authors: Jan Schuchardt, Mihail Stoian, Arthur Kosmala, Stephan G\"unnemann
Categories: cs.CR cs.LG stat.ML
\\
  Differential privacy (DP) has various desirable properties, such as
robustness to post-processing, group privacy, and amplification by subsampling,
which can be derived independently of each other. Our goal is to determine
whether stronger privacy guarantees can be obtained by considering multiple of
these properties jointly. To this end, we focus on the combination of group
privacy and amplification by subsampling. To provide guarantees that are
amenable to machine learning algorithms, we conduct our analysis in the
framework of R\'enyi-DP, which has more favorable composition properties than
$(\epsilon,\delta)$-DP. As part of this analysis, we develop a unified
framework for deriving amplification by subsampling guarantees for R\'enyi-DP,
which represents the first such framework for a privacy accounting method and
is of independent interest. We find that it not only lets us improve upon and
generalize existing amplification results for R\'enyi-DP, but also derive
provably tight group privacy amplification guarantees stronger than existing
principles. These results establish the joint study of different DP properties
as a promising research direction.
\\ ( https://arxiv.org/abs/2403.04867 ,  8819kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04875 (*cross-listing*)
Date: Thu, 7 Mar 2024 19:47:48 GMT   (732kb,D)

Title: Aligning GPTRec with Beyond-Accuracy Goals with Reinforcement Learning
Authors: Aleksandr Petrov and Craig Macdonald
Categories: cs.IR cs.LG
Comments: Accepted by the 2nd Workshop The 2nd Workshop on Recommendation with
  Generative Models, in conjunction with The Web Conference 2024
\\
  Adaptations of Transformer models, such as BERT4Rec and SASRec, achieve
state-of-the-art performance in the sequential recommendation task according to
accuracy-based metrics, such as NDCG. These models treat items as tokens and
then utilise a score-and-rank approach (Top-K strategy), where the model first
computes item scores and then ranks them according to this score. While this
approach works well for accuracy-based metrics, it is hard to use it for
optimising more complex beyond-accuracy metrics such as diversity. Recently,
the GPTRec model, which uses a different Next-K strategy, has been proposed as
an alternative to the Top-K models. In contrast with traditional Top-K
recommendations, Next-K generates recommendations item-by-item and, therefore,
can account for complex item-to-item interdependencies important for the
beyond-accuracy measures. However, the original GPTRec paper focused only on
accuracy in experiments and needed to address how to optimise the model for
complex beyond-accuracy metrics. Indeed, training GPTRec for beyond-accuracy
goals is challenging because the interaction training data available for
training recommender systems typically needs to be aligned with beyond-accuracy
recommendation goals. To solve the misalignment problem, we train GPTRec using
a 2-stage approach: in the first stage, we use a teacher-student approach to
train GPTRec, mimicking the behaviour of traditional Top-K models; in the
second stage, we use Reinforcement Learning to align the model for
beyond-accuracy goals. In particular, we experiment with increasing
recommendation diversity and reducing popularity bias. Our experiments on two
datasets show that in 3 out of 4 cases, GPTRec's Next-K generation approach
offers a better tradeoff between accuracy and secondary metrics than classic
greedy re-ranking techniques.
\\ ( https://arxiv.org/abs/2403.04875 ,  732kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04883 (*cross-listing*)
Date: Thu, 7 Mar 2024 20:16:18 GMT   (4435kb,D)

Title: Learning Traveling Solitary Waves Using Separable Gaussian Neural
  Networks
Authors: Siyuan Xing and Efstathios G. Charalampidis
Categories: nlin.PS cs.LG
Comments: 19 pages, 15 figures, 3 tables
\\
  In this paper, we apply a machine-learning approach to learn traveling
solitary waves across various families of partial differential equations
(PDEs). Our approach integrates a novel interpretable neural network (NN)
architecture, called Separable Gaussian Neural Networks (SGNN) into the
framework of Physics-Informed Neural Networks (PINNs). Unlike the traditional
PINNs that treat spatial and temporal data as independent inputs, the present
method leverages wave characteristics to transform data into the so-called
co-traveling wave frame. This adaptation effectively addresses the issue of
propagation failure in PINNs when applied to large computational domains. Here,
the SGNN architecture demonstrates robust approximation capabilities for
single-peakon, multi-peakon, and stationary solutions within the
(1+1)-dimensional, $b$-family of PDEs. In addition, we expand our
investigations, and explore not only peakon solutions in the $ab$-family but
also compacton solutions in (2+1)-dimensional, Rosenau-Hyman family of PDEs. A
comparative analysis with MLP reveals that SGNN achieves comparable accuracy
with fewer than a tenth of the neurons, underscoring its efficiency and
potential for broader application in solving complex nonlinear PDEs.
\\ ( https://arxiv.org/abs/2403.04883 ,  4435kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04884 (*cross-listing*)
Date: Thu, 7 Mar 2024 20:16:42 GMT   (8858kb,D)

Title: Optimizing Retinal Prosthetic Stimuli with Conditional Invertible Neural
  Networks
Authors: Yuli Wu, Julian Wittmann, Peter Walter, Johannes Stegmaier
Categories: cs.CV cs.LG
\\
  Implantable retinal prostheses offer a promising solution to restore partial
vision by circumventing damaged photoreceptor cells in the retina and directly
stimulating the remaining functional retinal cells. However, the information
transmission between the camera and retinal cells is often limited by the low
resolution of the electrode array and the lack of specificity for different
ganglion cell types, resulting in suboptimal stimulations. In this work, we
propose to utilize normalizing flow-based conditional invertible neural
networks to optimize retinal implant stimulation in an unsupervised manner. The
invertibility of these networks allows us to use them as a surrogate for the
computational model of the visual system, while also encoding input camera
signals into optimized electrical stimuli on the electrode array. Compared to
other methods, such as trivial downsampling, linear models, and feed-forward
convolutional neural networks, the flow-based invertible neural network and its
conditional extension yield better visual reconstruction qualities w.r.t.
various metrics using a physiologically validated simulation tool.
\\ ( https://arxiv.org/abs/2403.04884 ,  8858kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04962 (*cross-listing*)
Date: Fri, 8 Mar 2024 00:15:43 GMT   (8701kb,D)

Title: C2P-GCN: Cell-to-Patch Graph Convolutional Network for Colorectal Cancer
  Grading
Authors: Sudipta Paul, Bulent Yener, Amanda W. Lund
Categories: eess.IV cs.CV cs.LG
\\
  Graph-based learning approaches, due to their ability to encode tissue/organ
structure information, are increasingly favored for grading colorectal cancer
histology images. Recent graph-based techniques involve dividing whole slide
images (WSIs) into smaller or medium-sized patches, and then building graphs on
each patch for direct use in training. This method, however, fails to capture
the tissue structure information present in an entire WSI and relies on
training from a significantly large dataset of image patches. In this paper, we
propose a novel cell-to-patch graph convolutional network (C2P-GCN), which is a
two-stage graph formation-based approach. In the first stage, it forms a
patch-level graph based on the cell organization on each patch of a WSI. In the
second stage, it forms an image-level graph based on a similarity measure
between patches of a WSI considering each patch as a node of a graph. This
graph representation is then fed into a multi-layer GCN-based classification
network. Our approach, through its dual-phase graph construction, effectively
gathers local structural details from individual patches and establishes a
meaningful connection among all patches across a WSI. As C2P-GCN integrates the
structural data of an entire WSI into a single graph, it allows our model to
work with significantly fewer training data compared to the latest models for
colorectal cancer. Experimental validation of C2P-GCN on two distinct
colorectal cancer datasets demonstrates the effectiveness of our method.
\\ ( https://arxiv.org/abs/2403.04962 ,  8701kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04990 (*cross-listing*)
Date: Fri, 8 Mar 2024 02:02:23 GMT   (13122kb,D)

Title: Jet Discrimination with Quantum Complete Graph Neural Network
Authors: Yi-An Chen, Kai-Feng Chen
Categories: hep-ph cs.LG quant-ph
\\
  Machine learning, particularly deep neural networks, has been widely utilized
in high energy physics and has shown remarkable results in various
applications. Moreover, the concept of machine learning has been extended to
quantum computers, giving rise to a new research area known as quantum machine
learning. In this paper, we propose a novel variational quantum circuit model,
Quantum Complete Graph Neural Network (QCGNN), designed for learning complete
graphs. We argue that QCGNN has a polynomial speedup against its classical
counterpart, due to the property of quantum parallelism. In this paper, we
study the application of QCGNN through the challenging jet discrimination,
where the jets are represented with complete graphs. Subsequently, we conduct a
comparative analysis with classical graph neural networks to establish a
benchmark.
\\ ( https://arxiv.org/abs/2403.04990 ,  13122kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05024 (*cross-listing*)
Date: Fri, 8 Mar 2024 04:02:34 GMT   (4411kb,D)

Title: A Probabilistic Hadamard U-Net for MRI Bias Field Correction
Authors: Xin Zhu, Hongyi Pan, Yury Velichko, Adam B. Murphy, Ashley Ross, Baris
  Turkbey, Ahmet Enis Cetin and Ulas Bagci
Categories: eess.IV cs.CV cs.LG
\\
  Magnetic field inhomogeneity correction remains a challenging task in MRI
analysis. Most established techniques are designed for brain MRI by supposing
that image intensities in the identical tissue follow a uniform distribution.
Such an assumption cannot be easily applied to other organs, especially those
that are small in size and heterogeneous in texture (large variations in
intensity), such as the prostate. To address this problem, this paper proposes
a probabilistic Hadamard U-Net (PHU-Net) for prostate MRI bias field
correction. First, a novel Hadamard U-Net (HU-Net) is introduced to extract the
low-frequency scalar field, multiplied by the original input to obtain the
prototypical corrected image. HU-Net converts the input image from the time
domain into the frequency domain via Hadamard transform. In the frequency
domain, high-frequency components are eliminated using the trainable filter
(scaling layer), hard-thresholding layer, and sparsity penalty. Next, a
conditional variational autoencoder is used to encode possible bias
field-corrected variants into a low-dimensional latent space. Random samples
drawn from latent space are then incorporated with a prototypical corrected
image to generate multiple plausible images. Experimental results demonstrate
the effectiveness of PHU-Net in correcting bias-field in prostate MRI with a
fast inference speed. It has also been shown that prostate MRI segmentation
accuracy improves with the high-quality corrected images from PHU-Net. The code
will be available in the final version of this manuscript.
\\ ( https://arxiv.org/abs/2403.05024 ,  4411kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05034 (*cross-listing*)
Date: Fri, 8 Mar 2024 04:25:29 GMT   (4512kb,D)

Title: CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction
  Model
Authors: Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen,
  Dajiang Yu, Chongxuan Li, Hang Su, Jun Zhu
Categories: cs.CV cs.LG
Comments: Project page: https://ml.cs.tsinghua.edu.cn/~zhengyi/CRM/
\\
  Feed-forward 3D generative models like the Large Reconstruction Model (LRM)
have demonstrated exceptional generation speed. However, the transformer-based
methods do not leverage the geometric priors of the triplane component in their
architecture, often leading to sub-optimal quality given the limited size of 3D
data and slow training. In this work, we present the Convolutional
Reconstruction Model (CRM), a high-fidelity feed-forward single image-to-3D
generative model. Recognizing the limitations posed by sparse 3D data, we
highlight the necessity of integrating geometric priors into network design.
CRM builds on the key observation that the visualization of triplane exhibits
spatial correspondence of six orthographic images. First, it generates six
orthographic view images from a single input image, then feeds these images
into a convolutional U-Net, leveraging its strong pixel-level alignment
capabilities and significant bandwidth to create a high-resolution triplane.
CRM further employs Flexicubes as geometric representation, facilitating direct
end-to-end optimization on textured meshes. Overall, our model delivers a
high-fidelity textured mesh from an image in just 10 seconds, without any
test-time optimization.
\\ ( https://arxiv.org/abs/2403.05034 ,  4512kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05054 (*cross-listing*)
Date: Fri, 8 Mar 2024 05:01:43 GMT   (706kb,D)

Title: A Sinkhorn-type Algorithm for Constrained Optimal Transport
Authors: Xun Tang, Holakou Rahmanian, Michael Shavlovsky, Kiran Koshy
  Thekumparampil, Tesi Xiao, Lexing Ying
Categories: math.OC cs.LG
\\
  Entropic optimal transport (OT) and the Sinkhorn algorithm have made it
practical for machine learning practitioners to perform the fundamental task of
calculating transport distance between statistical distributions. In this work,
we focus on a general class of OT problems under a combination of equality and
inequality constraints. We derive the corresponding entropy regularization
formulation and introduce a Sinkhorn-type algorithm for such constrained OT
problems supported by theoretical guarantees. We first bound the approximation
error when solving the problem through entropic regularization, which reduces
exponentially with the increase of the regularization parameter. Furthermore,
we prove a sublinear first-order convergence rate of the proposed Sinkhorn-type
algorithm in the dual space by characterizing the optimization procedure with a
Lyapunov function. To achieve fast and higher-order convergence under weak
entropy regularization, we augment the Sinkhorn-type algorithm with dynamic
regularization scheduling and second-order acceleration. Overall, this work
systematically combines recent theoretical and numerical advances in entropic
optimal transport with the constrained case, allowing practitioners to derive
approximate transport plans in complex scenarios.
\\ ( https://arxiv.org/abs/2403.05054 ,  706kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05069 (*cross-listing*)
Date: Fri, 8 Mar 2024 05:43:00 GMT   (3770kb,D)

Title: Improving Diffusion-Based Generative Models via Approximated Optimal
  Transport
Authors: Daegyu Kim, Jooyoung Choi, Chaehun Shin, Uiwon Hwang, Sungroh Yoon
Categories: cs.CV cs.LG
\\
  We introduce the Approximated Optimal Transport (AOT) technique, a novel
training scheme for diffusion-based generative models. Our approach aims to
approximate and integrate optimal transport into the training process,
significantly enhancing the ability of diffusion models to estimate the
denoiser outputs accurately. This improvement leads to ODE trajectories of
diffusion models with lower curvature and reduced truncation errors during
sampling. We achieve superior image quality and reduced sampling steps by
employing AOT in training. Specifically, we achieve FID scores of 1.88 with
just 27 NFEs and 1.73 with 29 NFEs in unconditional and conditional
generations, respectively. Furthermore, when applying AOT to train the
discriminator for guidance, we establish new state-of-the-art FID scores of
1.68 and 1.58 for unconditional and conditional generations, respectively, each
with 29 NFEs. This outcome demonstrates the effectiveness of AOT in enhancing
the performance of diffusion models.
\\ ( https://arxiv.org/abs/2403.05069 ,  3770kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05119 (*cross-listing*)
Date: Fri, 8 Mar 2024 07:32:28 GMT   (279kb,D)

Title: Estimation of Electronic Band Gap Energy From Material Properties Using
  Machine Learning
Authors: Sagar Prakash Barad, Sajag Kumar, Subhankar Mishra
Categories: cond-mat.mtrl-sci cs.LG
Comments: 6 pages, IC-CGU 2024
\\
  Machine learning techniques are utilized to estimate the electronic band gap
energy and forecast the band gap category of materials based on experimentally
quantifiable properties. The determination of band gap energy is critical for
discerning various material properties, such as its metallic nature, and
potential applications in electronic and optoelectronic devices. While
numerical methods exist for computing band gap energy, they often entail high
computational costs and have limitations in accuracy and scalability. A machine
learning-driven model capable of swiftly predicting material band gap energy
using easily obtainable experimental properties would offer a superior
alternative to conventional density functional theory (DFT) methods. Our model
does not require any preliminary DFT-based calculation or knowledge of the
structure of the material. We present a scheme for improving the performance of
simple regression and classification models by partitioning the dataset into
multiple clusters. A new evaluation scheme for comparing the performance of
ML-based models in material sciences involving both regression and
classification tasks is introduced based on traditional evaluation metrics. It
is shown that on this new evaluation metric, our method of clustering the
dataset results in better performance.
\\ ( https://arxiv.org/abs/2403.05119 ,  279kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05122 (*cross-listing*)
Date: Fri, 8 Mar 2024 07:36:14 GMT   (2825kb,D)

Title: Multi-Tower Multi-Interest Recommendation with User Representation Repel
Authors: Tianyu Xiong, Xiaohan Yu
Categories: cs.IR cs.LG
Comments: 9 pages, 5 figures, 4 tables, Submitted to ACM/SIGIR - The 47th
  International ACM SIGIR Conference on Research and Development in Information
  Retrieval
ACM-class: H.3.3
\\
  In the era of information overload, the value of recommender systems has been
profoundly recognized in academia and industry alike. Multi-interest sequential
recommendation, in particular, is a subfield that has been receiving increasing
attention in recent years. By generating multiple-user representations,
multi-interest learning models demonstrate superior expressiveness than
single-user representation models, both theoretically and empirically. Despite
major advancements in the field, three major issues continue to plague the
performance and adoptability of multi-interest learning methods, the difference
between training and deployment objectives, the inability to access item
information, and the difficulty of industrial adoption due to its single-tower
architecture. We address these challenges by proposing a novel multi-tower
multi-interest framework with user representation repel. Experimental results
across multiple large-scale industrial datasets proved the effectiveness and
generalizability of our proposed framework.
\\ ( https://arxiv.org/abs/2403.05122 ,  2825kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05133 (*cross-listing*)
Date: Fri, 8 Mar 2024 08:05:50 GMT   (4535kb,D)

Title: RIS-empowered Topology Control for Distributed Learning in Urban Air
  Mobility
Authors: Kai Xiong, Rui Wang, Supeng Leng, Wenyang Che, Chongwen Huang, Chau
  Yuen
Categories: cs.IT cs.LG cs.NI math.IT
\\
  Urban Air Mobility (UAM) expands vehicles from the ground to the near-ground
space, envisioned as a revolution for transportation systems. Comprehensive
scene perception is the foundation for autonomous aerial driving. However, UAM
encounters the intelligent perception challenge: high perception learning
requirements conflict with the limited sensors and computing chips of flying
cars. To overcome the challenge, federated learning (FL) and other
collaborative learning have been proposed to enable resource-limited devices to
conduct onboard deep learning (DL) collaboratively. But traditional
collaborative learning like FL relies on a central integrator for DL model
aggregation, which is difficult to deploy in dynamic environments. The fully
decentralized learning schemes may be the intuitive solution while the
convergence of distributed learning cannot be guaranteed. Accordingly, this
paper explores reconfigurable intelligent surfaces (RIS) empowered distributed
learning, taking account of topological attributes to facilitate the learning
performance with convergence guarantee. We propose several FL topological
criteria for optimizing the transmission delay and convergence rate by
exploiting the Laplacian matrix eigenvalues of the communication network.
Subsequently, we innovatively leverage the RIS link modification ability to
remold the current network according to the proposed topological criteria. This
paper rethinks the functions of RIS from the perspective of the network layer.
Furthermore, a deep deterministic policy gradient-based RIS phase shift control
algorithm is developed to construct or deconstruct the network links
simultaneously to reshape the communication network. Simulation experiments are
conducted over MobileNet-based multi-view learning to verify the efficiency of
the distributed FL framework.
\\ ( https://arxiv.org/abs/2403.05133 ,  4535kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05134 (*cross-listing*)
Date: Fri, 8 Mar 2024 08:07:26 GMT   (6717kb,D)

Title: Follow-the-Perturbed-Leader with Fr\'{e}chet-type Tail Distributions:
  Optimality in Adversarial Bandits and Best-of-Both-Worlds
Authors: Jongyeong Lee and Junya Honda and Shinji Ito and Min-hwan Oh
Categories: stat.ML cs.LG
Comments: 54 pages
\\
  This paper studies the optimality of the Follow-the-Perturbed-Leader (FTPL)
policy in both adversarial and stochastic $K$-armed bandits. Despite the
widespread use of the Follow-the-Regularized-Leader (FTRL) framework with
various choices of regularization, the FTPL framework, which relies on random
perturbations, has not received much attention, despite its inherent
simplicity. In adversarial bandits, there has been conjecture that FTPL could
potentially achieve $\mathcal{O}(\sqrt{KT})$ regrets if perturbations follow a
distribution with a Fr\'{e}chet-type tail. Recent work by Honda et al. (2023)
showed that FTPL with Fr\'{e}chet distribution with shape $\alpha=2$ indeed
attains this bound and, notably logarithmic regret in stochastic bandits,
meaning the Best-of-Both-Worlds (BOBW) capability of FTPL. However, this result
only partly resolves the above conjecture because their analysis heavily relies
on the specific form of the Fr\'{e}chet distribution with this shape. In this
paper, we establish a sufficient condition for perturbations to achieve
$\mathcal{O}(\sqrt{KT})$ regrets in the adversarial setting, which covers,
e.g., Fr\'{e}chet, Pareto, and Student-$t$ distributions. We also demonstrate
the BOBW achievability of FTPL with certain Fr\'{e}chet-type tail
distributions. Our results contribute not only to resolving existing
conjectures through the lens of extreme value theory but also potentially offer
insights into the effect of the regularization functions in FTRL through the
mapping from FTPL to FTRL.
\\ ( https://arxiv.org/abs/2403.05134 ,  6717kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05138 (*cross-listing*)
Date: Fri, 8 Mar 2024 08:12:05 GMT   (9936kb)

Title: Greedy feature selection: Classifier-dependent feature selection via
  greedy methods
Authors: Fabiana Camattari, Sabrina Guastavino, Francesco Marchetti, Michele
  Piana, Emma Perracchione
Categories: stat.ML cs.LG cs.NA math.NA
MSC-class: 68Q32, 68T07, 65D12
\\
  The purpose of this study is to introduce a new approach to feature ranking
for classification tasks, called in what follows greedy feature selection. In
statistical learning, feature selection is usually realized by means of methods
that are independent of the classifier applied to perform the prediction using
that reduced number of features. Instead, greedy feature selection identifies
the most important feature at each step and according to the selected
classifier. In the paper, the benefits of such scheme are investigated
theoretically in terms of model capacity indicators, such as the
Vapnik-Chervonenkis (VC) dimension or the kernel alignment, and tested
numerically by considering its application to the problem of predicting
geo-effective manifestations of the active Sun.
\\ ( https://arxiv.org/abs/2403.05138 ,  9936kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05185 (*cross-listing*)
Date: Fri, 8 Mar 2024 09:53:07 GMT   (3180kb,D)

Title: Personalized Audiobook Recommendations at Spotify Through Graph Neural
  Networks
Authors: Marco De Nadai, Francesco Fabbri, Paul Gigioli, Alice Wang, Ang Li,
  Fabrizio Silvestri, Laura Kim, Shawn Lin, Vladan Radosavljevic, Sandeep
  Ghael, David Nyhan, Hugues Bouchard, Mounia Lalmas-Roelleke, Andreas Damianou
Categories: cs.IR cs.LG
Comments: To appear in The Web Conference 2024 proceedings
\\
  In the ever-evolving digital audio landscape, Spotify, well-known for its
music and talk content, has recently introduced audiobooks to its vast user
base. While promising, this move presents significant challenges for
personalized recommendations. Unlike music and podcasts, audiobooks, initially
available for a fee, cannot be easily skimmed before purchase, posing higher
stakes for the relevance of recommendations. Furthermore, introducing a new
content type into an existing platform confronts extreme data sparsity, as most
users are unfamiliar with this new content type. Lastly, recommending content
to millions of users requires the model to react fast and be scalable. To
address these challenges, we leverage podcast and music user preferences and
introduce 2T-HGNN, a scalable recommendation system comprising Heterogeneous
Graph Neural Networks (HGNNs) and a Two Tower (2T) model. This novel approach
uncovers nuanced item relationships while ensuring low latency and complexity.
We decouple users from the HGNN graph and propose an innovative multi-link
neighbor sampler. These choices, together with the 2T component, significantly
reduce the complexity of the HGNN model. Empirical evaluations involving
millions of users show significant improvement in the quality of personalized
recommendations, resulting in a +46% increase in new audiobooks start rate and
a +23% boost in streaming rates. Intriguingly, our model's impact extends
beyond audiobooks, benefiting established products like podcasts.
\\ ( https://arxiv.org/abs/2403.05185 ,  3180kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05249 (*cross-listing*)
Date: Fri, 8 Mar 2024 12:13:11 GMT   (1128kb,D)

Title: On Representing Electronic Wave Functions with Sign Equivariant Neural
  Networks
Authors: Nicholas Gao, Stephan G\"unnemann
Categories: quant-ph cs.LG physics.chem-ph physics.comp-ph
Comments: Published at Workshop on AI4DifferentialEquations in Science at ICLR
  2024
\\
  Recent neural networks demonstrated impressively accurate approximations of
electronic ground-state wave functions. Such neural networks typically consist
of a permutation-equivariant neural network followed by a
permutation-antisymmetric operation to enforce the electronic exchange
symmetry. While accurate, such neural networks are computationally expensive.
In this work, we explore the flipped approach, where we first compute
antisymmetric quantities based on the electronic coordinates and then apply
sign equivariant neural networks to preserve the antisymmetry. While this
approach promises acceleration thanks to the lower-dimensional representation,
we demonstrate that it reduces to a Jastrow factor, a commonly used
permutation-invariant multiplicative factor in the wave function. Our empirical
results support this further, finding little to no improvements over baselines.
We conclude with neither theoretical nor empirical advantages of sign
equivariant functions for representing electronic wave functions within the
evaluation of this work.
\\ ( https://arxiv.org/abs/2403.05249 ,  1128kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05256 (*cross-listing*)
Date: Fri, 8 Mar 2024 12:26:48 GMT   (3643kb,D)

Title: DuDoUniNeXt: Dual-domain unified hybrid model for single and
  multi-contrast undersampled MRI reconstruction
Authors: Ziqi Gao and Yue Zhang and Xinwen Liu and Kaiyan Li and S. Kevin Zhou
Categories: eess.IV cs.CV cs.LG
Comments: 11 pages, 4 figures, 2 tables
\\
  Multi-contrast (MC) Magnetic Resonance Imaging (MRI) reconstruction aims to
incorporate a reference image of auxiliary modality to guide the reconstruction
process of the target modality. Known MC reconstruction methods perform well
with a fully sampled reference image, but usually exhibit inferior performance,
compared to single-contrast (SC) methods, when the reference image is missing
or of low quality. To address this issue, we propose DuDoUniNeXt, a unified
dual-domain MRI reconstruction network that can accommodate to scenarios
involving absent, low-quality, and high-quality reference images. DuDoUniNeXt
adopts a hybrid backbone that combines CNN and ViT, enabling specific
adjustment of image domain and k-space reconstruction. Specifically, an
adaptive coarse-to-fine feature fusion module (AdaC2F) is devised to
dynamically process the information from reference images of varying qualities.
Besides, a partially shared shallow feature extractor (PaSS) is proposed, which
uses shared and distinct parameters to handle consistent and discrepancy
information among contrasts. Experimental results demonstrate that the proposed
model surpasses state-of-the-art SC and MC models significantly. Ablation
studies show the effectiveness of the proposed hybrid backbone, AdaC2F, PaSS,
and the dual-domain unified learning scheme.
\\ ( https://arxiv.org/abs/2403.05256 ,  3643kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05340 (*cross-listing*)
Date: Fri, 8 Mar 2024 14:17:07 GMT   (2191kb,D)

Title: Embedded Deployment of Semantic Segmentation in Medicine through
  Low-Resolution Inputs
Authors: Erik Ostrowski, Muhammad Shafique
Categories: eess.IV cs.CV cs.LG
\\
  When deploying neural networks in real-life situations, the size and
computational effort are often the limiting factors. This is especially true in
environments where big, expensive hardware is not affordable, like in embedded
medical devices, where budgets are often tight. State-of-the-art proposed
multiple different lightweight solutions for such use cases, mostly by changing
the base model architecture, not taking the input and output resolution into
consideration. In this paper, we propose our architecture that takes advantage
of the fact that in hardware-limited environments, we often refrain from using
the highest available input resolutions to guarantee a higher throughput.
Although using lower-resolution input leads to a significant reduction in
computing and memory requirements, it may also incur reduced prediction
quality. Our architecture addresses this problem by exploiting the fact that we
can still utilize high-resolution ground-truths in training. The proposed model
inputs lower-resolution images and high-resolution ground truths, which can
improve the prediction quality by 5.5% while adding less than 200 parameters to
the model. %reducing the frames per second only from 25 to 20. We conduct an
extensive analysis to illustrate that our architecture enhances existing
state-of-the-art frameworks for lightweight semantic segmentation of cancer in
MRI images. We also tested the deployment speed of state-of-the-art lightweight
networks and our architecture on Nvidia's Jetson Nano to emulate deployment in
resource-constrained embedded scenarios.
\\ ( https://arxiv.org/abs/2403.05340 ,  2191kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05353 (*cross-listing*)
Date: Fri, 8 Mar 2024 14:34:32 GMT   (2706kb,D)

Title: Hybridized Convolutional Neural Networks and Long Short-Term Memory for
  Improved Alzheimer's Disease Diagnosis from MRI Scans
Authors: Maleka Khatun, Md Manowarul Islam, Habibur Rahman Rifat, Md. Shamim
  Bin Shahid, Md. Alamin Talukder, Md Ashraf Uddin
Categories: eess.IV cs.CV cs.LG
Comments: Accepted In The 26th International Conference on Computer and
  Information Technology (ICCIT) On 13-15 December 2023
DOI: 10.1109/ICCIT60459.2023.10441274
\\
  Brain-related diseases are more sensitive than other diseases due to several
factors, including the complexity of surgical procedures, high costs, and other
challenges. Alzheimer's disease is a common brain disorder that causes memory
loss and the shrinking of brain cells. Early detection is critical for
providing proper treatment to patients. However, identifying Alzheimer's at an
early stage using manual scanning of CT or MRI scans is challenging. Therefore,
researchers have delved into the exploration of computer-aided systems,
employing Machine Learning and Deep Learning methodologies, which entail the
training of datasets to detect Alzheimer's disease. This study aims to present
a hybrid model that combines a CNN model's feature extraction capabilities with
an LSTM model's detection capabilities. This study has applied the transfer
learning called VGG16 in the hybrid model to extract features from MRI images.
The LSTM detects features between the convolution layer and the fully connected
layer. The output layer of the fully connected layer uses the softmax function.
The training of the hybrid model involved utilizing the ADNI dataset. The trial
findings revealed that the model achieved a level of accuracy of 98.8%, a
sensitivity rate of 100%, and a specificity rate of 76%. The proposed hybrid
model outperforms its contemporary CNN counterparts, showcasing a superior
performance.
\\ ( https://arxiv.org/abs/2403.05353 ,  2706kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05358 (*cross-listing*)
Date: Fri, 8 Mar 2024 14:45:18 GMT   (1912kb,D)

Title: Variational Inference of Parameters in Opinion Dynamics Models
Authors: Jacopo Lenti, Fabrizio Silvestri, Gianmarco De Francisci Morales
Categories: cs.CY cs.LG cs.SI stat.ML
\\
  Despite the frequent use of agent-based models (ABMs) for studying social
phenomena, parameter estimation remains a challenge, often relying on costly
simulation-based heuristics. This work uses variational inference to estimate
the parameters of an opinion dynamics ABM, by transforming the estimation
problem into an optimization task that can be solved directly.
  Our proposal relies on probabilistic generative ABMs (PGABMs): we start by
synthesizing a probabilistic generative model from the ABM rules. Then, we
transform the inference process into an optimization problem suitable for
automatic differentiation. In particular, we use the Gumbel-Softmax
reparameterization for categorical agent attributes and stochastic variational
inference for parameter estimation. Furthermore, we explore the trade-offs of
using variational distributions with different complexity: normal distributions
and normalizing flows.
  We validate our method on a bounded confidence model with agent roles
(leaders and followers). Our approach estimates both macroscopic (bounded
confidence intervals and backfire thresholds) and microscopic ($200$
categorical, agent-level roles) more accurately than simulation-based and MCMC
methods. Consequently, our technique enables experts to tune and validate their
ABMs against real-world observations, thus providing insights into human
behavior in social systems via data-driven analysis.
\\ ( https://arxiv.org/abs/2403.05358 ,  1912kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05368 (*cross-listing*)
Date: Fri, 8 Mar 2024 14:59:15 GMT   (96kb,D)

Title: Exploring the Links between the Fundamental Lemma and Kernel Regression
Authors: Oleksii Molodchyk and Timm Faulwasser
Categories: eess.SY cs.LG cs.SY math.OC
MSC-class: 93B30 (Primary) 93C10, 46E22 (Secondary)
\\
  Generalizations and variations of the fundamental lemma by Willems et al. are
an active topic of recent research. In this note, we explore and formalize the
links between kernel regression and known nonlinear extensions of the
fundamental lemma. Applying a transformation to the usual linear equation in
Hankel matrices, we arrive at an alternative implicit kernel representation of
the system trajectories while keeping the requirements on persistency of
excitation. We show that this representation is equivalent to the solution of a
specific kernel regression problem. We explore the possible structures of the
underlying kernel as well as the system classes to which they correspond.
\\ ( https://arxiv.org/abs/2403.05368 ,  96kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05440 (*cross-listing*)
Date: Fri, 8 Mar 2024 16:48:20 GMT   (5737kb,D)

Title: Is Cosine-Similarity of Embeddings Really About Similarity?
Authors: Harald Steck, Chaitanya Ekanadham, Nathan Kallus
Categories: cs.IR cs.LG
Comments: 9 pages
Journal-ref: ACM Web Conference 2024 (WWW 2024 Companion)
DOI: 10.1145/3589335.3651526
\\
  Cosine-similarity is the cosine of the angle between two vectors, or
equivalently the dot product between their normalizations. A popular
application is to quantify semantic similarity between high-dimensional objects
by applying cosine-similarity to a learned low-dimensional feature embedding.
This can work better but sometimes also worse than the unnormalized dot-product
between embedded vectors in practice. To gain insight into this empirical
observation, we study embeddings derived from regularized linear models, where
closed-form solutions facilitate analytical insights. We derive analytically
how cosine-similarity can yield arbitrary and therefore meaningless
`similarities.' For some linear models the similarities are not even unique,
while for others they are implicitly controlled by the regularization. We
discuss implications beyond linear models: a combination of different
regularizations are employed when learning deep models; these have implicit and
unintended effects when taking cosine-similarities of the resulting embeddings,
rendering results opaque and possibly arbitrary. Based on these insights, we
caution against blindly using cosine-similarity and outline alternatives.
\\ ( https://arxiv.org/abs/2403.05440 ,  5737kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05441 (*cross-listing*)
Date: Fri, 8 Mar 2024 16:51:27 GMT   (3581kb,D)

Title: Bayesian Hierarchical Probabilistic Forecasting of Intraday Electricity
  Prices
Authors: Daniel Nickelsen, Gernot M\"uller
Categories: stat.AP cs.LG
Comments: 22 pages, 13 figures, 4 tables
ACM-class: G.3; I.2.6; I.6.3; I.6.5
\\
  We present a first study of Bayesian forecasting of electricity prices traded
on the German continuous intraday market which fully incorporates parameter
uncertainty. Our target variable is the IDFull price index, forecasts are given
in terms of posterior predictive distributions. For validation we use the
exceedingly volatile electricity prices of 2022, which have hardly been the
subject of forecasting studies before. As a benchmark model, we use all
available intraday transactions at the time of forecast creation to compute a
current value for the IDFull. According to the weak-form efficiency hypothesis,
it would not be possible to significantly improve this benchmark built from
last price information. We do, however, observe statistically significant
improvement in terms of both point measures and probability scores. Finally, we
challenge the declared gold standard of using LASSO for feature selection in
electricity price forecasting by presenting strong statistical evidence that
Orthogonal Matching Pursuit (OMP) leads to better forecasting performance.
\\ ( https://arxiv.org/abs/2403.05441 ,  3581kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05452 (*cross-listing*)
Date: Fri, 8 Mar 2024 16:57:54 GMT   (17619kb,D)

Title: The R2D2 deep neural network series paradigm for fast precision imaging
  in radio astronomy
Authors: Amir Aghabiglou, Chung San Chu, Arwa Dabbech, Yves Wiaux
Categories: astro-ph.IM cs.CV cs.LG
Comments: 20 pages, 10 figures, submitted to APJ
\\
  Radio-interferometric (RI) imaging entails solving high-resolution
high-dynamic range inverse problems from large data volumes. Recent image
reconstruction techniques grounded in optimization theory have demonstrated
remarkable capability for imaging precision, well beyond CLEAN's capability.
These range from advanced proximal algorithms propelled by handcrafted
regularization operators, such as the SARA family, to hybrid plug-and-play
(PnP) algorithms propelled by learned regularization denoisers, such as AIRI.
Optimization and PnP structures are however highly iterative, which hinders
their ability to handle the extreme data sizes expected from future
instruments. To address this scalability challenge, we introduce a novel deep
learning approach, dubbed ``Residual-to-Residual DNN series for high-Dynamic
range imaging'. R2D2's reconstruction is formed as a series of residual images,
iteratively estimated as outputs of Deep Neural Networks (DNNs) taking the
previous iteration's image estimate and associated data residual as inputs. It
thus takes a hybrid structure between a PnP algorithm and a learned version of
the matching pursuit algorithm that underpins CLEAN. We present a comprehensive
study of our approach, featuring its multiple incarnations distinguished by
their DNN architectures. We provide a detailed description of its training
process, targeting a telescope-specific approach. R2D2's capability to deliver
high precision is demonstrated in simulation, across a variety of image and
observation settings using the Very Large Array (VLA). Its reconstruction speed
is also demonstrated: with only few iterations required to clean data residuals
at dynamic ranges up to 105, R2D2 opens the door to fast precision imaging.
R2D2 codes are available in the BASPLib library on GitHub.
\\ ( https://arxiv.org/abs/2403.05452 ,  17619kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2311.02026
replaced with revised version Fri, 8 Mar 2024 06:29:28 GMT   (6248kb)

Title: APRICOT-Mamba: Acuity Prediction in Intensive Care Unit (ICU):
  Development and Validation of a Stability, Transitions, and Life-Sustaining
  Therapies Prediction Model
Authors: Miguel Contreras, Brandon Silva, Benjamin Shickel, Tezcan
  Ozrazgat-Baslanti, Yuanfang Ren, Ziyuan Guan, Jeremy Balch, Jiaqing Zhang,
  Sabyasachi Bandyopadhyay, Kia Khezeli, Azra Bihorac, Parisa Rashidi
Categories: cs.AI
\\ ( https://arxiv.org/abs/2311.02026 ,  6248kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04235
replaced with revised version Fri, 8 Mar 2024 17:04:49 GMT   (851kb,D)

Title: Can LLMs Follow Simple Rules?
Authors: Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian,
  Lulwa Aljeraisy, Basel Alomair, Dan Hendrycks, David Wagner
Categories: cs.AI cs.CL cs.LG
Comments: Project website: https://eecs.berkeley.edu/~normanmu/llm_rules;
  revised content
\\ ( https://arxiv.org/abs/2311.04235 ,  851kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07213
replaced with revised version Fri, 8 Mar 2024 02:29:21 GMT   (507kb,D)

Title: Human-computer Interaction for Brain-inspired Computing Based on Machine
  Learning And Deep Learning: A Review
Authors: Bihui Yu, Sibo Zhang, Lili Zhou, Jingxuan Wei, Linzhuang Sun, Liping
  Bu
Categories: cs.AI
Comments: 25pages, 8 figures and 4 tables
\\ ( https://arxiv.org/abs/2312.07213 ,  507kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04732
replaced with revised version Fri, 8 Mar 2024 06:47:08 GMT   (372kb,D)

Title: How Far Are We from Intelligent Visual Deductive Reasoning?
Authors: Yizhe Zhang, He Bai, Ruixiang Zhang, Jiatao Gu, Shuangfei Zhai, Josh
  Susskind, Navdeep Jaitly
Categories: cs.AI cs.CL cs.CV
Comments: ICLR 2024 AGI workshop. https://github.com/apple/ml-rpm-bench
\\ ( https://arxiv.org/abs/2403.04732 ,  372kb)
------------------------------------------------------------------------------
\\
arXiv:2305.02869
replaced with revised version Fri, 8 Mar 2024 08:54:08 GMT   (555kb,D)

Title: Masked Structural Growth for 2x Faster Language Model Pre-training
Authors: Yiqun Yao, Zheng Zhang, Jing Li, and Yequan Wang
Categories: cs.CL
Comments: ICLR 2024 camera ready
\\ ( https://arxiv.org/abs/2305.02869 ,  555kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14877
replaced with revised version Fri, 8 Mar 2024 18:51:03 GMT   (210kb,D)

Title: Improving Probability-based Prompt Selection Through Unified Evaluation
  and Analysis
Authors: Sohee Yang, Jonghyeon Kim, Joel Jang, Seonghyeon Ye, Hyunji Lee,
  Minjoon Seo
Categories: cs.CL
Comments: TACL 2024 (Pre-MIT Press publication version)
\\ ( https://arxiv.org/abs/2305.14877 ,  210kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12307
replaced with revised version Fri, 8 Mar 2024 15:26:38 GMT   (842kb,D)

Title: LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models
Authors: Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song
  Han, Jiaya Jia
Categories: cs.CL cs.AI cs.LG
Comments: Code, models, dataset, and demo are available at
  https://github.com/dvlab-research/LongLoRA
\\ ( https://arxiv.org/abs/2309.12307 ,  842kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14556
replaced with revised version Fri, 8 Mar 2024 05:20:08 GMT   (7227kb,D)

Title: Art or Artifice? Large Language Models and the False Promise of
  Creativity
Authors: Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan,
  Chien-Sheng Wu
Categories: cs.CL cs.AI cs.HC
Comments: ACM CHI 2024
\\ ( https://arxiv.org/abs/2309.14556 ,  7227kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09499
replaced with revised version Fri, 8 Mar 2024 13:01:36 GMT   (242kb,D)

Title: One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language
  Models
Authors: Hang Shao, Bei Liu, Bo Xiao, Ke Zeng, Guanglu Wan, Yanmin Qian
Categories: cs.CL cs.AI
Comments: Accepted by ICASSP2024
\\ ( https://arxiv.org/abs/2310.09499 ,  242kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12557
replaced with revised version Fri, 8 Mar 2024 14:16:55 GMT   (458kb,D)

Title: DepWiGNN: A Depth-wise Graph Neural Network for Multi-hop Spatial
  Reasoning in Text
Authors: Shuaiyi Li, Yang Deng, Wai Lam
Categories: cs.CL cs.AI
Comments: EMNLP 2023 Findings
\\ ( https://arxiv.org/abs/2310.12557 ,  458kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05922
replaced with revised version Fri, 8 Mar 2024 06:51:43 GMT   (504kb,D)

Title: Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation
  Extraction
Authors: Xilai Ma, Jing Li and Min Zhang
Categories: cs.CL
Comments: Findings of EMNLP 2023
\\ ( https://arxiv.org/abs/2311.05922 ,  504kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09889
replaced with revised version Fri, 8 Mar 2024 14:23:49 GMT   (3822kb,D)

Title: Language Generation from Brain Recordings
Authors: Ziyi Ye, Qingyao Ai, Yiqun Liu, Maarten de Rijke, Min Zhang, Christina
  Lioma, Tuukka Ruotsalo
Categories: cs.CL
Comments: Preprint. Under Submission
\\ ( https://arxiv.org/abs/2311.09889 ,  3822kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16787
replaced with revised version Fri, 8 Mar 2024 12:21:12 GMT   (470kb,D)

Title: Evaluating Optimal Reference Translations
Authors: Vil\'em Zouhar, V\v{e}ra Kloudov\'a, Martin Popel, Ond\v{r}ej Bojar
Categories: cs.CL
Comments: To appear in Natural Language Engineering 2024
\\ ( https://arxiv.org/abs/2311.16787 ,  470kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00849
replaced with revised version Fri, 8 Mar 2024 06:42:37 GMT   (2806kb,D)

Title: RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from
  Fine-grained Correctional Human Feedback
Authors: Tianyu Yu and Yuan Yao and Haoye Zhang and Taiwen He and Yifeng Han
  and Ganqu Cui and Jinyi Hu and Zhiyuan Liu and Hai-Tao Zheng and Maosong Sun
  and Tat-Seng Chua
Categories: cs.CL cs.CV
Comments: Accepted by CVPR 2024
\\ ( https://arxiv.org/abs/2312.00849 ,  2806kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09979
replaced with revised version Fri, 8 Mar 2024 13:13:54 GMT   (1899kb,D)

Title: LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models
  via MoE-Style Plugin
Authors: Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen,
  Yuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, Shiliang Pu, Jiang Zhu, Rui
  Zheng, Tao Gui, Qi Zhang, Xuanjing Huang
Categories: cs.CL
Comments: 14 pages, 7 figures
\\ ( https://arxiv.org/abs/2312.09979 ,  1899kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14197
replaced with revised version Fri, 8 Mar 2024 07:58:48 GMT   (1433kb,D)

Title: Benchmarking and Defending Against Indirect Prompt Injection Attacks on
  Large Language Models
Authors: Jingwei Yi, Yueqi Xie, Bin Zhu, Emre Kiciman, Guangzhong Sun, Xing
  Xie, Fangzhao Wu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2312.14197 ,  1433kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05871
replaced with revised version Fri, 8 Mar 2024 07:56:57 GMT   (1071kb,D)

Title: Enhancing Personality Recognition in Dialogue by Data Augmentation and
  Heterogeneous Conversational Graph Networks
Authors: Yahui Fu, Haiyue Song, Tianyu Zhao, Tatsuya Kawahara
Categories: cs.CL
Comments: This paper has been accepted for presentation at International
  Workshop on Spoken Dialogue Systems Technology 2024 (IWSDS 2024)
\\ ( https://arxiv.org/abs/2401.05871 ,  1071kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14040
replaced with revised version Fri, 8 Mar 2024 15:00:31 GMT   (1181kb,D)

Title: (Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection
Authors: Francesco Periti, Haim Dubossarsky, Nina Tahmasebi
Categories: cs.CL
Comments: Accepted to the Findings of EACL 2024
\\ ( https://arxiv.org/abs/2401.14040 ,  1181kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14280
replaced with revised version Fri, 8 Mar 2024 18:04:24 GMT   (767kb,D)

Title: RomanSetu: Efficiently unlocking multilingual capabilities of Large
  Language Models models via Romanization
Authors: Jaavid Aktar Husain, Raj Dabre, Aswanth Kumar, Jay Gala, Thanmay
  Jayakumar, Ratish Puduppully, Anoop Kunchukuttan
Categories: cs.CL cs.AI
Comments: Work in progress
\\ ( https://arxiv.org/abs/2401.14280 ,  767kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04437
replaced with revised version Fri, 8 Mar 2024 04:03:27 GMT   (2545kb,D)

Title: Structured Entity Extraction Using Large Language Models
Authors: Haolun Wu, Ye Yuan, Liana Mikaelyan, Alexander Meulemans, Xue Liu,
  James Hensman, Bhaskar Mitra
Categories: cs.CL cs.LG
Comments: 18 pages
\\ ( https://arxiv.org/abs/2402.04437 ,  2545kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12011
replaced with revised version Fri, 8 Mar 2024 14:50:40 GMT   (1542kb,D)

Title: A Systematic Comparison of Contextualized Word Embeddings for Lexical
  Semantic Change
Authors: Francesco Periti, Nina Tahmasebi
Categories: cs.CL
Comments: Submitted to NAACL 2024
\\ ( https://arxiv.org/abs/2402.12011 ,  1542kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13116
replaced with revised version Fri, 8 Mar 2024 13:29:03 GMT   (1060kb,D)

Title: A Survey on Knowledge Distillation of Large Language Models
Authors: Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang
  Li, Can Xu, Dacheng Tao, Tianyi Zhou
Categories: cs.CL
Comments: 44 pages
\\ ( https://arxiv.org/abs/2402.13116 ,  1060kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13709
replaced with revised version Fri, 8 Mar 2024 14:35:30 GMT   (3368kb,D)

Title: SaGE: Evaluating Moral Consistency in Large Language Models
Authors: Vamshi Krishna Bonagiri, Sreeram Vennam, Priyanshul Govil, Ponnurangam
  Kumaraguru, Manas Gaur
Categories: cs.CL cs.AI
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2402.13709 ,  3368kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14809
replaced with revised version Fri, 8 Mar 2024 15:15:47 GMT   (1187kb,D)

Title: CriticBench: Benchmarking LLMs for Critique-Correct Reasoning
Authors: Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, Yujiu
  Yang
Categories: cs.CL cs.AI cs.LG
Comments: Corrected computation errors in Tables 1, 7-11; updated corresponding
  figs
\\ ( https://arxiv.org/abs/2402.14809 ,  1187kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14891
replaced with revised version Fri, 8 Mar 2024 03:47:32 GMT   (4104kb,D)

Title: LLMBind: A Unified Modality-Task Integration Framework
Authors: Bin Zhu, Peng Jin, Munan Ning, Bin Lin, Jinfa Huang, Qi Song, Jiaxi
  Cui, Junwu Zhang, Zhenyu Tang, Mingjun Pan, Xing Zhou, Li Yuan
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.14891 ,  4104kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00835
replaced with revised version Fri, 8 Mar 2024 00:13:31 GMT   (1239kb,D)

Title: CLLMs: Consistency Large Language Models
Authors: Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, Hao Zhang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.00835 ,  1239kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04460
replaced with revised version Fri, 8 Mar 2024 04:54:31 GMT   (3839kb,D)

Title: Pearl: A Review-driven Persona-Knowledge Grounded Conversational
  Recommendation Dataset
Authors: Minjin Kim, Minju Kim, Hana Kim, Beong-woo Kwak, Soyeon Chun, Hyunseo
  Kim, SeongKu Kang, Youngjae Yu, Jinyoung Yeo, Dongha Lee
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.04460 ,  3839kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04481
replaced with revised version Fri, 8 Mar 2024 04:47:16 GMT   (7063kb,D)

Title: Do Large Language Model Understand Multi-Intent Spoken Language ?
Authors: Shangjian Yin, Peijie Huang, Yuhong Xu, Haojing Huang, Jiatian Chen
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.04481 ,  7063kb)
------------------------------------------------------------------------------
\\
arXiv:1711.04366
replaced with revised version Thu, 7 Mar 2024 20:43:31 GMT   (1765kb,D)

Title: A unified framework for hard and soft clustering with regularized
  optimal transport
Authors: Jean-Fr\'ed\'eric Diebold and Nicolas Papadakis and Arnaud Dessein and
  Charles-Alban Deledalle
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/1711.04366 ,  1765kb)
------------------------------------------------------------------------------
\\
arXiv:2109.03396
replaced with revised version Fri, 8 Mar 2024 06:31:18 GMT   (30kb)

Title: A Bayesian Learning Algorithm for Unknown Zero-sum Stochastic Games with
  an Arbitrary Opponent
Authors: Mehdi Jafarnia-Jahromi, Rahul Jain, Ashutosh Nayyar
Categories: cs.LG cs.GT
\\ ( https://arxiv.org/abs/2109.03396 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2110.05365
replaced with revised version Fri, 8 Mar 2024 18:10:06 GMT   (10446kb,D)

Title: Intriguing Properties of Input-dependent Randomized Smoothing
Authors: Peter S\'uken\'ik, Aleksei Kuvshinov, Stephan G\"unnemann
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2110.05365 ,  10446kb)
------------------------------------------------------------------------------
\\
arXiv:2205.14545
replaced with revised version Fri, 8 Mar 2024 04:50:17 GMT   (6034kb,D)

Title: Functional Linear Regression of Cumulative Distribution Functions
Authors: Qian Zhang, Anuran Makur, and Kamyar Azizzadenesheli
Categories: cs.LG math.ST stat.TH
Comments: 56 pages, 7 figures, accepted by TMLR
\\ ( https://arxiv.org/abs/2205.14545 ,  6034kb)
------------------------------------------------------------------------------
\\
arXiv:2210.07290
replaced with revised version Fri, 8 Mar 2024 05:04:23 GMT   (21453kb,D)

Title: Joint control variate for faster black-box variational inference
Authors: Xi Wang, Tomas Geffner, Justin Domke
Categories: cs.LG stat.ML
Comments: Published in the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS 2024)
\\ ( https://arxiv.org/abs/2210.07290 ,  21453kb)
------------------------------------------------------------------------------
\\
arXiv:2211.15856
replaced with revised version Fri, 8 Mar 2024 17:45:35 GMT   (18385kb,D)

Title: Beyond Ensemble Averages: Leveraging Climate Model Ensembles for
  Subseasonal Forecasting
Authors: Elena Orlova, Haokun Liu, Raphael Rossellini, Benjamin Cash, Rebecca
  Willett
Categories: cs.LG physics.ao-ph
\\ ( https://arxiv.org/abs/2211.15856 ,  18385kb)
------------------------------------------------------------------------------
\\
arXiv:2302.03357
replaced with revised version Fri, 8 Mar 2024 12:38:27 GMT   (1366kb,D)

Title: Towards Enhancing Time Series Contrastive Learning: A Dynamic Bad Pair
  Mining Approach
Authors: Xiang Lan, Hanshu Yan, Shenda Hong, Mengling Feng
Categories: cs.LG
Comments: ICLR 2024 Camera Ready
\\ ( https://arxiv.org/abs/2302.03357 ,  1366kb)
------------------------------------------------------------------------------
\\
arXiv:2302.08434
replaced with revised version Fri, 8 Mar 2024 18:01:00 GMT   (1025kb,D)

Title: On marginal feature attributions of tree-based models
Authors: Khashayar Filom, Alexey Miroshnikov, Konstandinos Kotsiopoulos, Arjun
  Ravi Kannan
Categories: cs.LG cs.GT
Comments: Typos corrected. Section 4 is reorganized and new experiments on Owen
  values are added. Minor changes in the Introduction. 30 pages+appendix (64
  pages in total), 10 figures
\\ ( https://arxiv.org/abs/2302.08434 ,  1025kb)
------------------------------------------------------------------------------
\\
arXiv:2303.10139
replaced with revised version Fri, 8 Mar 2024 14:51:55 GMT   (1214kb,D)

Title: Distill n' Explain: explaining graph neural networks using simple
  surrogates
Authors: Tamara Pereira and Erik Nascimento and Lucas E. Resck and Diego
  Mesquita and Amauri Souza
Categories: cs.LG cs.AI
Comments: To appear in AISTATS 2023
Journal-ref: PMLR 206 (2023) 6199-6214; AISTATS 2023
\\ ( https://arxiv.org/abs/2303.10139 ,  1214kb)
------------------------------------------------------------------------------
\\
arXiv:2306.13773
replaced with revised version Thu, 7 Mar 2024 21:07:35 GMT   (394kb)

Title: Nearest Neighbour with Bandit Feedback
Authors: Stephen Pasteris, Chris Hicks, Vasilios Mavroudis
Categories: cs.LG
\\ ( https://arxiv.org/abs/2306.13773 ,  394kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16208
replaced with revised version Fri, 8 Mar 2024 02:50:45 GMT   (131kb,D)

Title: Continuous-time q-learning for mean-field control problems
Authors: Xiaoli Wei, Xiang Yu
Categories: cs.LG math.OC q-fin.CP
Comments: Keywords: Continuous-time reinforcement learning, continuous-time
  q-function, Mckean-Vlasov control, weak martingale characterization, test
  policies
\\ ( https://arxiv.org/abs/2306.16208 ,  131kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05436
replaced with revised version Fri, 8 Mar 2024 12:12:38 GMT   (1982kb,D)

Title: Quantized Fourier and Polynomial Features for more Expressive Tensor
  Network Models
Authors: Frederiek Wesel, Kim Batselier
Categories: cs.LG
Comments: 9 pages, 4 figures. Reviewed version after peer-review. To be
  published in the proceedings of the 27th International Conference on
  Artificial Intelligence and Statistics (AISTATS)
ACM-class: I.5.0
\\ ( https://arxiv.org/abs/2309.05436 ,  1982kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05365
replaced with revised version Fri, 8 Mar 2024 09:54:25 GMT   (1567kb,D)

Title: Molecular De Novo Design through Transformer-based Reinforcement
  Learning
Authors: Pengcheng Xu, Tao Feng, Tianfan Fu, Siddhartha Laghuvarapu, Jimeng Sun
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.05365 ,  1567kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05905
replaced with revised version Fri, 8 Mar 2024 06:39:39 GMT   (1227kb,D)

Title: TAIL: Task-specific Adapters for Imitation Learning with Large
  Pretrained Models
Authors: Zuxin Liu, Jesse Zhang, Kavosh Asadi, Yao Liu, Ding Zhao, Shoham
  Sabach, Rasool Fakoor
Categories: cs.LG cs.AI cs.RO
Comments: Published on ICLR 2024
\\ ( https://arxiv.org/abs/2310.05905 ,  1227kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06045
replaced with revised version Thu, 7 Mar 2024 21:25:31 GMT   (10961kb,D)

Title: Generative ensemble deep learning severe weather prediction from a
  deterministic convection-allowing model
Authors: Yingkai Sha, Ryan A. Sobash, David John Gagne II
Categories: cs.LG cs.AI physics.ao-ph
\\ ( https://arxiv.org/abs/2310.06045 ,  10961kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06549
replaced with revised version Fri, 8 Mar 2024 11:07:51 GMT   (6854kb,D)

Title: Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield
  but Also a Catalyst for Model Inversion Attacks
Authors: Lukas Struppek, Dominik Hintersdorf, Kristian Kersting
Categories: cs.LG cs.CR cs.CV
Comments: Published as a conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2310.06549 ,  6854kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17110
replaced with revised version Fri, 8 Mar 2024 03:03:10 GMT   (333kb,D)

Title: LLM4DyG: Can Large Language Models Solve Spatial-Temporal Problems on
  Dynamic Graphs?
Authors: Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, Wenwu Zhu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.17110 ,  333kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18882
replaced with revised version Fri, 8 Mar 2024 02:13:00 GMT   (805kb,D)

Title: Differentiable Learning of Generalized Structured Matrices for Efficient
  Deep Neural Networks
Authors: Changwoo Lee, Hun-Seok Kim
Categories: cs.LG cs.AI cs.CV eess.IV eess.SP
\\ ( https://arxiv.org/abs/2310.18882 ,  805kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20673
replaced with revised version Fri, 8 Mar 2024 00:22:38 GMT   (2220kb,D)

Title: Balancing Act: Constraining Disparate Impact in Sparse Models
Authors: Meraj Hashemizadeh, Juan Ramirez, Rohan Sukumaran, Golnoosh Farnadi,
  Simon Lacoste-Julien, Jose Gallego-Posada
Categories: cs.LG cs.CY
Comments: Published at ICLR 2024. Code available at
  https://github.com/merajhashemi/balancing-act
\\ ( https://arxiv.org/abs/2310.20673 ,  2220kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02455
replaced with revised version Fri, 8 Mar 2024 11:04:49 GMT   (8866kb,D)

Title: Mixed Models with Multiple Instance Learning
Authors: Jan P. Engelmann, Alessandro Palma, Jakub M. Tomczak, Fabian J. Theis,
  Francesco Paolo Casale
Categories: cs.LG q-bio.GN q-bio.QM stat.AP
Comments: AISTATS 2024 Oral, Code: https://github.com/AIH-SGML/MixMIL
\\ ( https://arxiv.org/abs/2311.02455 ,  8866kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04056
replaced with revised version Fri, 8 Mar 2024 15:43:50 GMT   (1707kb,D)

Title: Multi-View Causal Representation Learning with Partial Observability
Authors: Dingling Yao, Danru Xu, S\'ebastien Lachapelle, Sara Magliacane,
  Perouz Taslakian, Georg Martius, Julius von K\"ugelgen and Francesco
  Locatello
Categories: cs.LG cs.AI
Comments: 28 pages, 10 figures, 11 tables
\\ ( https://arxiv.org/abs/2311.04056 ,  1707kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07323
replaced with revised version Fri, 8 Mar 2024 10:09:43 GMT   (1063kb,D)

Title: A Voting Approach for Explainable Classification with Rule Learning
Authors: Albert N\"ossig, Tobias Hell, Georg Moser
Categories: cs.LG
Comments: 35 pages, 10 figures
\\ ( https://arxiv.org/abs/2311.07323 ,  1063kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14255
replaced with revised version Fri, 8 Mar 2024 06:25:50 GMT   (1491kb,D)

Title: Out-of-Distribution Generalized Dynamic Graph Neural Network with
  Disentangled Intervention and Invariance Promotion
Authors: Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Wenwu Zhu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2311.14255 ,  1491kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15545
replaced with revised version Fri, 8 Mar 2024 03:49:24 GMT   (1321kb,D)

Title: Out-of-Distribution Generalized Dynamic Graph Neural Network for Human
  Albumin Prediction
Authors: Zeyang Zhang and Xingwang Li and Fei Teng and Ning Lin and Xueling Zhu
  and Xin Wang and Wenwu Zhu
Categories: cs.LG cs.AI cs.CE
Comments: MedAI'23
\\ ( https://arxiv.org/abs/2311.15545 ,  1321kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16496
replaced with revised version Fri, 8 Mar 2024 03:39:24 GMT   (19626kb,D)

Title: DPOD: Domain-Specific Prompt Tuning for Multimodal Fake News Detection
Authors: Debarshi Brahma, Amartya Bhattacharya, Suraj Nagaje Mahadev, Anmol
  Asati, Vikas Verma, Soma Biswas
Categories: cs.LG
\\ ( https://arxiv.org/abs/2311.16496 ,  19626kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07624
replaced with revised version Fri, 8 Mar 2024 02:37:16 GMT   (8737kb,D)

Title: A dynamical clipping approach with task feedback for Proximal Policy
  Optimization
Authors: Ziqi Zhang, Jingzehua Xu, Zifeng Zhuang, Jinxin Liu, Donglin wang,
  Shuai Zhang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2312.07624 ,  8737kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08852
replaced with revised version Fri, 8 Mar 2024 12:29:44 GMT   (28974kb,D)

Title: ERASE: Error-Resilient Representation Learning on Graphs for Label Noise
  Tolerance
Authors: Ling-Hao Chen, Yuanshuo Zhang, Taohua Huang, Liangcai Su, Zeyi Lin, Xi
  Xiao, Xiaobo Xia, and Tongliang Liu
Categories: cs.LG
Comments: 24 pages, 14 figures, 15 tables and a project page at
  https://eraseai.github.io/ERASE-page
\\ ( https://arxiv.org/abs/2312.08852 ,  28974kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13454
replaced with revised version Fri, 8 Mar 2024 13:29:29 GMT   (9078kb,D)

Title: MixEHR-SurG: a joint proportional hazard and guided topic model for
  inferring mortality-associated topics from electronic health records
Authors: Yixuan Li, Ariane Marelli, Archer Y. Yang, Yue Li
Categories: cs.LG stat.ME
Comments: 45 pages total, 19 pages main text, 6 main figures, 10 supplementary
  figrues
\\ ( https://arxiv.org/abs/2312.13454 ,  9078kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16083
replaced with revised version Fri, 8 Mar 2024 02:41:37 GMT   (34245kb,D)

Title: A Variational Autoencoder for Neural Temporal Point Processes with
  Dynamic Latent Graphs
Authors: Sikun Yang, Hongyuan Zha
Categories: cs.LG stat.ML
Comments: Accepted by AAAI-2024
\\ ( https://arxiv.org/abs/2312.16083 ,  34245kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04305
replaced with revised version Fri, 8 Mar 2024 00:32:13 GMT   (28578kb,D)

Title: Advancing Deep Active Learning & Data Subset Selection: Unifying
  Principles with Information-Theory Intuitions
Authors: Andreas Kirsch
Categories: cs.LG cs.IT math.IT
Comments: PhD thesis
DOI: 10.5287/ora-koewoxvaw
\\ ( https://arxiv.org/abs/2401.04305 ,  28578kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08732
replaced with revised version Thu, 7 Mar 2024 22:57:25 GMT   (33367kb,D)

Title: Bayes Conditional Distribution Estimation for Knowledge Distillation
  Based on Conditional Mutual Information
Authors: Linfeng Ye, Shayan Mohajer Hamidi, Renhao Tan, En-Hui Yang
Categories: cs.LG cs.CV cs.IT math.IT
Comments: 32 pages, 19 figures, Published as a conference paper at ICLR 2024
MSC-class: 68T30
ACM-class: I.2.6
Journal-ref: International Conference on Learning Representations 2024 (ICLR)
\\ ( https://arxiv.org/abs/2401.08732 ,  33367kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08850
replaced with revised version Fri, 8 Mar 2024 10:06:53 GMT   (2103kb,D)

Title: REValueD: Regularised Ensemble Value-Decomposition for Factorisable
  Markov Decision Processes
Authors: David Ireland and Giovanni Montana
Categories: cs.LG cs.AI
Comments: ICLR camera ready version
\\ ( https://arxiv.org/abs/2401.08850 ,  2103kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08977
replaced with revised version Fri, 8 Mar 2024 13:37:55 GMT   (2576kb)

Title: FedLoGe: Joint Local and Generic Federated Learning under Long-tailed
  Data
Authors: Zikai Xiao, Zihan Chen, Liyinglan Liu, Yang Feng, Jian Wu, Wanlu Liu,
  Joey Tianyi Zhou, Howard Hao Yang, Zuozhu Liu
Categories: cs.LG cs.AI
Comments: Accepted by ICLR 2024, code: https://github.com/ZackZikaiXiao/FedLoGe
ACM-class: I.2.0
\\ ( https://arxiv.org/abs/2401.08977 ,  2576kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10632
replaced with revised version Fri, 8 Mar 2024 10:51:42 GMT   (5303kb,D)

Title: Interventional Fairness on Partially Known Causal Graphs: A Constrained
  Optimization Approach
Authors: Aoqi Zuo, Yiqing Li, Susan Wei, Mingming Gong
Categories: cs.LG
Comments: Accepted to ICLR24
\\ ( https://arxiv.org/abs/2401.10632 ,  5303kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01943
replaced with revised version Fri, 8 Mar 2024 13:34:12 GMT   (1847kb,D)

Title: Precedence-Constrained Winter Value for Effective Graph Data Valuation
Authors: Hongliang Chi, Wei Jin, Charu Aggarwal, Yao Ma
Categories: cs.LG
Comments: 17 pages in total
\\ ( https://arxiv.org/abs/2402.01943 ,  1847kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03921
replaced with revised version Fri, 8 Mar 2024 12:23:56 GMT   (13869kb,D)

Title: Large Language Models to Enhance Bayesian Optimization
Authors: Tennison Liu and Nicol\'as Astorga and Nabeel Seedat and Mihaela van
  der Schaar
Categories: cs.LG cs.AI
Comments: Accepted as Poster at ICLR2024
\\ ( https://arxiv.org/abs/2402.03921 ,  13869kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06295
replaced with revised version Fri, 8 Mar 2024 10:50:21 GMT   (4471kb,D)

Title: Multimodal Interpretable Data-Driven Models for Early Prediction of
  Antimicrobial Multidrug Resistance Using Multivariate Time-Series
Authors: Sergio Mart\'inez-Ag\"uero, Antonio G. Marques, Inmaculada
  Mora-Jim\'enez, Joaqu\'in Alv\'arez-Rodr\'iguez, Cristina Soguero-Ruiz
Categories: cs.LG q-bio.QM
\\ ( https://arxiv.org/abs/2402.06295 ,  4471kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11148
replaced with revised version Thu, 7 Mar 2024 22:41:33 GMT   (558kb,D)

Title: Knowledge Distillation Based on Transformed Teacher Matching
Authors: Kaixiang Zheng and En-Hui Yang
Categories: cs.LG cs.CV
Comments: Published as a conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2402.11148 ,  558kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17269
replaced with revised version Fri, 8 Mar 2024 06:00:12 GMT   (128kb,D)

Title: Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion
  Recognition
Authors: Cam-Van Thi Nguyen, Cao-Bach Nguyen, Quang-Thuy Ha, Duc-Trong Le
Categories: cs.LG
Comments: Accepted by LREC-COLING 2024
\\ ( https://arxiv.org/abs/2402.17269 ,  128kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18863
replaced with revised version Fri, 8 Mar 2024 02:03:38 GMT   (237kb,D)

Title: Probabilistic Lipschitzness and the Stable Rank for Comparing
  Explanation Models
Authors: Lachlan Simpson, Kyle Millar, Adriel Cheng, Cheng-Chew Lim, Hong Gunn
  Chew
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.18863 ,  237kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02292
replaced with revised version Thu, 7 Mar 2024 22:07:00 GMT   (4085kb,D)

Title: A Decade of Privacy-Relevant Android App Reviews: Large Scale Trends
Authors: Omer Akgul, Sai Teja Peddinti, Nina Taft, Michelle L. Mazurek, Hamza
  Harkous, Animesh Srivastava, Benoit Seguin
Categories: cs.LG cs.HC
Comments: This is the extended version of the paper accepted to USENIX Security
  2024
\\ ( https://arxiv.org/abs/2403.02292 ,  4085kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02545
replaced with revised version Fri, 8 Mar 2024 03:39:59 GMT   (971kb,D)

Title: Wukong: Towards a Scaling Law for Large-Scale Recommendation
Authors: Buyun Zhang, Liang Luo, Yuxin Chen, Jade Nie, Xi Liu, Daifeng Guo,
  Yanli Zhao, Shen Li, Yuchen Hao, Yantao Yao, Guna Lakshminarayanan, Ellie
  Dingqiao Wen, Jongsoo Park, Maxim Naumov, Wenlin Chen
Categories: cs.LG cs.AI
Comments: 12 pages
\\ ( https://arxiv.org/abs/2403.02545 ,  971kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03020
replaced with revised version Fri, 8 Mar 2024 14:51:29 GMT   (14540kb,D)

Title: SplAgger: Split Aggregation for Meta-Reinforcement Learning
Authors: Jacob Beck, Matthew Jackson, Risto Vuorio, Zheng Xiong, Shimon
  Whiteson
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.03020 ,  14540kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03542
replaced with revised version Fri, 8 Mar 2024 03:24:00 GMT   (884kb,D)

Title: DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE
  Pre-Training
Authors: Zhongkai Hao, Chang Su, Songming Liu, Julius Berner, Chengyang Ying,
  Hang Su, Anima Anandkumar, Jian Song, Jun Zhu
Categories: cs.LG cs.NA math.NA
\\ ( https://arxiv.org/abs/2403.03542 ,  884kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04493
replaced with revised version Fri, 8 Mar 2024 17:02:55 GMT   (62kb)

Title: What makes an image realistic?
Authors: Lucas Theis
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2403.04493 ,  62kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04605
replaced with revised version Fri, 8 Mar 2024 14:49:34 GMT   (367kb,D)

Title: In-n-Out: Calibrating Graph Neural Networks for Link Prediction
Authors: Erik Nascimento, Diego Mesquita, Samuel Kaski, Amauri H Souza
Categories: cs.LG
Comments: 18 pages, 4 figures, 8 tables
\\ ( https://arxiv.org/abs/2403.04605 ,  367kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04629
replaced with revised version Fri, 8 Mar 2024 07:52:32 GMT   (2583kb,D)

Title: Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI
  Collaboration
Authors: Julian Rodemann, Federico Croppi, Philipp Arens, Yusuf Sale, Julia
  Herbinger, Bernd Bischl, Eyke H\"ullermeier, Thomas Augustin, Conor J. Walsh,
  Giuseppe Casalicchio
Categories: cs.LG cs.AI cs.HC cs.RO stat.ML
Comments: Preprint. Copyright by the authors. 19 pages, 24 figures
ACM-class: I.2.6; I.2.9; F.2.2; J.6
\\ ( https://arxiv.org/abs/2403.04629 ,  2583kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04650
replaced with revised version Fri, 8 Mar 2024 14:29:41 GMT   (1567kb,D)

Title: Context-Based Multimodal Fusion
Authors: Bilal Faye, Hanane Azzag, Mustapha Lebbah, Djamel Bouchaffra
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.04650 ,  1567kb)
------------------------------------------------------------------------------
\\
arXiv:2102.02649 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 20:47:45 GMT   (1554kb)

Title: A step toward a reinforcement learning de novo genome assembler
Authors: Kleber Padovani, Roberto Xavier, Rafael Cabral Borges, Andre Carvalho,
  Anna Reali, Annie Chateau, Ronnie Alves
Categories: q-bio.GN cs.AI cs.LG
\\ ( https://arxiv.org/abs/2102.02649 ,  1554kb)
------------------------------------------------------------------------------
\\
arXiv:2303.16281
replaced with revised version Fri, 8 Mar 2024 00:15:02 GMT   (2103kb)

Title: A "Perspectival" Mirror of the Elephant: Investigating Language Bias on
  Google, ChatGPT, YouTube, and Wikipedia
Authors: Queenie Luo, Michael J. Puett, Michael D. Smith
Categories: cs.CY cs.AI cs.CL cs.LG cs.SI
\\ ( https://arxiv.org/abs/2303.16281 ,  2103kb)
------------------------------------------------------------------------------
\\
arXiv:2305.03105
replaced with revised version Fri, 8 Mar 2024 13:30:58 GMT   (5761kb,D)

Title: HAISTA-NET: Human Assisted Instance Segmentation Through Attention
Authors: Muhammed Korkmaz, T. Metin Sezgin
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2305.03105 ,  5761kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15278
replaced with revised version Fri, 8 Mar 2024 07:29:13 GMT   (9102kb,D)

Title: Out of Sight, Still in Mind: Reasoning and Planning about Unobserved
  Objects with Video Tracking Enabled Memory Models
Authors: Yixuan Huang, Jialin Yuan, Chanho Kim, Pupul Pradhan, Bryan Chen, Li
  Fuxin, Tucker Hermans
Categories: cs.RO cs.AI cs.CV cs.LG
Comments: Accepted at IEEE Conference on Robotics and Automation (ICRA) 2024.
  Website: https://sites.google.com/view/rdmemory
\\ ( https://arxiv.org/abs/2309.15278 ,  9102kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07726
replaced with revised version Fri, 8 Mar 2024 08:58:10 GMT   (13938kb,D)

Title: Warfare:Breaking the Watermark Protection of AI-Generated Content
Authors: Guanlin Li, Yifei Chen, Jie Zhang, Jiwei Li, Shangwei Guo, Tianwei
  Zhang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2310.07726 ,  13938kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10486
replaced with revised version Fri, 8 Mar 2024 14:10:02 GMT   (5230kb,D)

Title: ManyQuadrupeds: Learning a Single Locomotion Policy for Diverse
  Quadruped Robots
Authors: Milad Shafiee, Guillaume Bellegarda and Auke Ijspeert
Categories: cs.RO cs.AI cs.LG cs.SY eess.SY
Comments: Accepted for IEEE International Conference on Robotics and Automation
  (ICRA) 2024, Webpage: https://miladshafiee.github.io/ManyQuadrupeds/
\\ ( https://arxiv.org/abs/2310.10486 ,  5230kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19812 (*cross-listing*)
replaced with revised version Fri, 8 Mar 2024 13:35:13 GMT   (22496kb,D)

Title: Brain decoding: toward real-time reconstruction of visual perception
Authors: Yohann Benchetrit, Hubert Banville and Jean-R\'emi King
Categories: eess.IV cs.AI cs.LG q-bio.NC
Comments: 25 pages, 13 figures, updated version following acceptance at ICLR
  2024
\\ ( https://arxiv.org/abs/2310.19812 ,  22496kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01534
replaced with revised version Fri, 8 Mar 2024 14:36:50 GMT   (11941kb,D)

Title: Approximate Multiagent Reinforcement Learning for On-Demand Urban
  Mobility Problem on a Large Map (extended version)
Authors: Daniel Garces, Sushmita Bhattacharya, Dimitri Bertsekas, Stephanie Gil
Categories: cs.MA cs.AI cs.RO
Comments: 11 pages, 5 figures, 1 lemma, and 2 theorems
\\ ( https://arxiv.org/abs/2311.01534 ,  11941kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15993
replaced with revised version Fri, 8 Mar 2024 09:12:57 GMT   (12853kb,D)

Title: Unified Batch Normalization: Identifying and Alleviating the Feature
  Condensation in Batch Normalization and a Unified Framework
Authors: Shaobo Wang, Xiangdong Zhang, Dongrui Liu, Junchi Yan
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2311.15993 ,  12853kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03263
replaced with revised version Thu, 7 Mar 2024 22:42:20 GMT   (20187kb,D)

Title: Weathering Ongoing Uncertainty: Learning and Planning in a Time-Varying
  Partially Observable Environment
Authors: Gokul Puthumanaillam, Xiangyu Liu, Negar Mehr and Melkior Ornik
Categories: cs.RO cs.AI cs.SY eess.SY
Comments: Page 3, fixed typo
\\ ( https://arxiv.org/abs/2312.03263 ,  20187kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07214
replaced with revised version Fri, 8 Mar 2024 13:33:21 GMT   (17820kb,D)

Title: Exploring Large Language Models to Facilitate Variable Autonomy for
  Human-Robot Teaming
Authors: Younes Lakhnati, Max Pascher, Jens Gerken
Categories: cs.HC cs.AI cs.RO
Comments: Submitted to: Frontiers in Robotics and AI, Variable Autonomy for
  Human-Robot Teaming
\\ ( https://arxiv.org/abs/2312.07214 ,  17820kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15832
replaced with revised version Fri, 8 Mar 2024 11:31:58 GMT   (17442kb,D)

Title: Multiple Instance Learning for Glioma Diagnosis using Hematoxylin and
  Eosin Whole Slide Images: An Indian Cohort Study
Authors: Ekansh Chauhan, Amit Sharma, Megha S Uppin, C.V. Jawahar and P.K.
  Vinod
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2402.15832 ,  17442kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17862
replaced with revised version Fri, 8 Mar 2024 07:03:57 GMT   (957kb,D)

Title: REPrune: Channel Pruning via Kernel Representative Selection
Authors: Mincheol Park, Dongjin Kim, Cheonjun Park, Yuna Park, Gyeong Eun Gong,
  Won Woo Ro, Suhyun Kim
Categories: cs.CV cs.AI
Comments: Published at AAAI2024
\\ ( https://arxiv.org/abs/2402.17862 ,  957kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18152 (*cross-listing*)
replaced with revised version Fri, 8 Mar 2024 04:11:05 GMT   (46581kb,D)

Title: Boosting Neural Representations for Videos with a Conditional Decoder
Authors: Xinjie Zhang, Ren Yang, Dailan He, Xingtong Ge, Tongda Xu, Yan Wang,
  Hongwei Qin, Jun Zhang
Categories: eess.IV cs.AI cs.CV
Comments: Accept by CVPR 2024
\\ ( https://arxiv.org/abs/2402.18152 ,  46581kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18920
replaced with revised version Fri, 8 Mar 2024 08:51:29 GMT   (43301kb,D)

Title: Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation
Authors: Dongliang Cao, Marvin Eisenberger, Nafie El Amrani, Daniel Cremers,
  Florian Bernard
Categories: cs.CV cs.AI cs.CG
Comments: accepted by CVPR2024
\\ ( https://arxiv.org/abs/2402.18920 ,  43301kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01564
replaced with revised version Thu, 7 Mar 2024 22:42:34 GMT   (18557kb,D)

Title: ComTraQ-MPC: Meta-Trained DQN-MPC Integration for Trajectory Tracking
  with Limited Active Localization Updates
Authors: Gokul Puthumanaillam, Manav Vora and Melkior Ornik
Categories: cs.RO cs.AI cs.SY eess.SY
Comments: * Equal contribution
\\ ( https://arxiv.org/abs/2403.01564 ,  18557kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02118
replaced with revised version Fri, 8 Mar 2024 10:03:44 GMT   (20900kb,D)

Title: Towards Implicit Prompt For Text-To-Image Models
Authors: Yue Yang, Yuqi lin, Hong Liu, Wenqi Shao, Runjian Chen, Hailong Shang,
  Yu Wang, Yu Qiao, Kaipeng Zhang, Ping Luo
Categories: cs.CY cs.AI cs.CV
\\ ( https://arxiv.org/abs/2403.02118 ,  20900kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03691
replaced with revised version Fri, 8 Mar 2024 06:32:12 GMT   (4009kb,D)

Title: MolNexTR: A Generalized Deep Learning Model for Molecular Image
  Recognition
Authors: Yufan Chen, Ching Ting Leung, Yong Huang, Jianwei Sun, Hao Chen, Hanyu
  Gao
Categories: cs.CV cs.AI
Comments: Submitted to the Journal of Cheminformatics
\\ ( https://arxiv.org/abs/2403.03691 ,  4009kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04115
replaced with revised version Fri, 8 Mar 2024 09:56:47 GMT   (24916kb,D)

Title: DNAct: Diffusion Guided Multi-Task 3D Policy Learning
Authors: Ge Yan, Yueh-Hua Wu, Xiaolong Wang
Categories: cs.RO cs.AI cs.CV
\\ ( https://arxiv.org/abs/2403.04115 ,  24916kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04202
replaced with revised version Fri, 8 Mar 2024 02:25:09 GMT   (28472kb,D)

Title: Dynamics of Moral Behavior in Heterogeneous Populations of Learning
  Agents
Authors: Elizaveta Tennant, Stephen Hailes, Mirco Musolesi
Categories: cs.MA cs.AI cs.CY cs.LG
\\ ( https://arxiv.org/abs/2403.04202 ,  28472kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04634
replaced with revised version Fri, 8 Mar 2024 18:28:28 GMT   (16130kb,D)

Title: Pix2Gif: Motion-Guided Diffusion for GIF Generation
Authors: Hitesh Kandala, Jianfeng Gao, Jianwei Yang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.04634 ,  16130kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11351
replaced with revised version Fri, 8 Mar 2024 03:07:18 GMT   (326kb,D)

Title: MMAPS: End-to-End Multi-Grained Multi-Modal Attribute-Aware Product
  Summarization
Authors: Tao Chen, Ze Lin, Hui Li, Jiayi Ji, Yiyi Zhou, Guanbin Li and Rongrong
  Ji
Categories: cs.MM cs.CL
Comments: LREC-COLING 2024.11 pages, 4 figures
\\ ( https://arxiv.org/abs/2308.11351 ,  326kb)
------------------------------------------------------------------------------
\\
arXiv:2109.01051 (*cross-listing*)
replaced with revised version Fri, 8 Mar 2024 03:47:22 GMT   (1984kb,D)

Title: Can Error Mitigation Improve Trainability of Noisy Variational Quantum
  Algorithms?
Authors: Samson Wang, Piotr Czarnik, Andrew Arrasmith, M. Cerezo, Lukasz
  Cincio, Patrick J. Coles
Categories: quant-ph cs.LG
Comments: 24+29 pages, 6+4 figures
Report-no: LA-UR-21-28574
\\ ( https://arxiv.org/abs/2109.01051 ,  1984kb)
------------------------------------------------------------------------------
\\
arXiv:2201.00087 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 18:38:19 GMT   (23895kb)

Title: Persistent Homological State-Space Estimation of Functional Human Brain
  Networks at Rest
Authors: Moo K. Chung, Shih-Gu Huang, Ian C. Carroll, Vince D. Calhoun, H. Hill
  Goldsmith
Categories: math.AT cs.LG q-bio.NC
Comments: To be published in PLOS Computational Biology
\\ ( https://arxiv.org/abs/2201.00087 ,  23895kb)
------------------------------------------------------------------------------
\\
arXiv:2203.01707 (*cross-listing*)
replaced with revised version Fri, 8 Mar 2024 01:00:47 GMT   (1545kb,D)

Title: Testing Stationarity and Change Point Detection in Reinforcement
  Learning
Authors: Mengbing Li, Chengchun Shi, Zhenke Wu and Piotr Fryzlewicz
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2203.01707 ,  1545kb)
------------------------------------------------------------------------------
\\
arXiv:2204.05275 (*cross-listing*)
replaced with revised version Fri, 8 Mar 2024 18:40:01 GMT   (5767kb,D)

Title: Settling the Sample Complexity of Model-Based Offline Reinforcement
  Learning
Authors: Gen Li and Laixi Shi and Yuxin Chen and Yuejie Chi and Yuting Wei
Categories: stat.ML cs.IT cs.LG cs.SY eess.SY math.IT math.ST stat.TH
Comments: accepted to the Annals of Statistics
Journal-ref: Annals of Statistics, vol. 52, no. 1, pp. 233-260, 2024
\\ ( https://arxiv.org/abs/2204.05275 ,  5767kb)
------------------------------------------------------------------------------
\\
arXiv:2210.03859 (*cross-listing*)
replaced with revised version Fri, 8 Mar 2024 16:48:08 GMT   (459kb,D)

Title: Spectrally-Corrected and Regularized Linear Discriminant Analysis for
  Spiked Covariance Model
Authors: Hua Li, Wenya Luo, Zhidong Bai, Huanchao Zhou, Zhangni Pu
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2210.03859 ,  459kb)
------------------------------------------------------------------------------
\\
arXiv:2211.03464 (*cross-listing*)
replaced with revised version Fri, 8 Mar 2024 10:06:43 GMT   (5219kb,D)

Title: A Survey on Quantum Reinforcement Learning
Authors: Nico Meyer, Christian Ufrecht, Maniraman Periyasamy, Daniel D.
  Scherer, Axel Plinge, and Christopher Mutschler
Categories: quant-ph cs.LG
Comments: 83 pages, 18 figures
\\ ( https://arxiv.org/abs/2211.03464 ,  5219kb)
------------------------------------------------------------------------------
\\
arXiv:2211.12971
replaced with revised version Fri, 8 Mar 2024 16:40:10 GMT   (526kb,D)

Title: Cooperative data-driven modeling
Authors: Aleksandr Dekhovich, O. Taylan Turan, Jiaxiang Yi, Miguel A. Bessa
Categories: math.NA cond-mat.mtrl-sci cs.LG cs.NA
Journal-ref: Computer Methods in Applied Mechanics and Engineering, 417, 116432
  (2023)
DOI: 10.1016/j.cma.2023.116432
\\ ( https://arxiv.org/abs/2211.12971 ,  526kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18231 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 20:28:54 GMT   (41703kb,D)

Title: High-Fidelity Image Compression with Score-based Generative Models
Authors: Emiel Hoogeboom, Eirikur Agustsson, Fabian Mentzer, Luca Versari,
  George Toderici, Lucas Theis
Categories: eess.IV cs.CV cs.LG stat.ML
\\ ( https://arxiv.org/abs/2305.18231 ,  41703kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01669
replaced with revised version Fri, 8 Mar 2024 03:19:39 GMT   (1415kb,D)

Title: Enhancing CLIP with CLIP: Exploring Pseudolabeling for Limited-Label
  Prompt Tuning
Authors: Cristina Menghini, Andrew Delworth, Stephen H. Bach
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2306.01669 ,  1415kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04527 (*cross-listing*)
replaced with revised version Fri, 8 Mar 2024 17:28:47 GMT   (27877kb,D)

Title: ContriMix: Scalable stain color augmentation for domain generalization
  without domain labels in digital pathology
Authors: Tan H. Nguyen, Dinkar Juyal, Jin Li, Aaditya Prakash, Shima Nofallah,
  Chintan Shah, Sai Chowdary Gullapally, Limin Yu, Michael Griffin, Anand
  Sampat, John Abel, Justin Lee, Amaro Taylor-Weiner
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2306.04527 ,  27877kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10715
replaced with revised version Fri, 8 Mar 2024 12:07:10 GMT   (12858kb,D)

Title: Maximum Entropy Heterogeneous-Agent Reinforcement Learning
Authors: Jiarong Liu, Yifan Zhong, Siyi Hu, Haobo Fu, Qiang Fu, Xiaojun Chang,
  Yaodong Yang
Categories: cs.MA cs.LG
Comments: ICLR 2024 spotlight
\\ ( https://arxiv.org/abs/2306.10715 ,  12858kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00529
replaced with revised version Fri, 8 Mar 2024 07:42:28 GMT   (370kb,D)

Title: New intelligent defense systems to reduce the risks of Selfish Mining
  and Double-Spending attacks using Learning Automata
Authors: Seyed Ardalan Ghoreishi and Mohammad Reza Meybodi
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2307.00529 ,  370kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06782 (*cross-listing*)
replaced with revised version Fri, 8 Mar 2024 15:01:11 GMT   (1684kb,D)

Title: Improved particle-flow event reconstruction with scalable neural
  networks for current and future particle detectors
Authors: Joosep Pata, Eric Wulff, Farouk Mokhtar, David Southwick, Mengke
  Zhang, Maria Girone, Javier Duarte
Categories: physics.data-an cs.LG hep-ex physics.ins-det stat.ML
Comments: 21 pages, 10 figures
\\ ( https://arxiv.org/abs/2309.06782 ,  1684kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07138 (*cross-listing*)
replaced with revised version Fri, 8 Mar 2024 16:45:37 GMT   (1704kb)

Title: Blind Source Separation of Single-Channel Mixtures via Multi-Encoder
  Autoencoders
Authors: Matthew B. Webster and Joonnyong Lee
Categories: eess.SP cs.LG
Comments: 24 pages (with appendix), 12 figures(with appendix), resubmitted for
  review
MSC-class: I.2.6
\\ ( https://arxiv.org/abs/2309.07138 ,  1704kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05446 (*cross-listing*)
replaced with revised version Fri, 8 Mar 2024 11:31:28 GMT   (33501kb,D)

Title: RetSeg: Retention-based Colorectal Polyps Segmentation Network
Authors: Khaled ELKarazle, Valliappan Raman, Caslon Chua and Patrick Then
Categories: eess.IV cs.CV cs.LG
Comments: Updated PDF
\\ ( https://arxiv.org/abs/2310.05446 ,  33501kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07222 (*cross-listing*)
replaced with revised version Fri, 8 Mar 2024 02:14:15 GMT   (33488kb,D)

Title: Neural General Circulation Models for Weather and Climate
Authors: Dmitrii Kochkov, Janni Yuval, Ian Langmore, Peter Norgaard, Jamie
  Smith, Griffin Mooers, Milan Kl\"ower, James Lottes, Stephan Rasp, Peter
  D\"uben, Sam Hatfield, Peter Battaglia, Alvaro Sanchez-Gonzalez, Matthew
  Willson, Michael P. Brenner, Stephan Hoyer
Categories: physics.ao-ph cs.LG physics.comp-ph
Comments: 92 pages, 54 figures
\\ ( https://arxiv.org/abs/2311.07222 ,  33488kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16609
replaced with revised version Fri, 8 Mar 2024 01:20:29 GMT   (91kb,D)

Title: Eigenmatrix for unstructured sparse recovery
Authors: Lexing Ying
Categories: math.NA cs.IT cs.LG cs.NA eess.SP math.IT
\\ ( https://arxiv.org/abs/2311.16609 ,  91kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12924 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 20:59:19 GMT   (1391kb,D)

Title: Performance Analysis of Support Vector Machine (SVM) on Challenging
  Datasets for Forest Fire Detection
Authors: Ankan Kar, Nirjhar Nath, Utpalraj Kemprai, Aman
Categories: stat.ML cs.LG stat.ME
Comments: 19 pages, 8 figures
Journal-ref: Int. J. Communications, Network and System Sciences, 17, 11-29
  (2024)
DOI: 10.4236/ijcns.2024.172002
\\ ( https://arxiv.org/abs/2401.12924 ,  1391kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02245
replaced with revised version Thu, 7 Mar 2024 20:48:52 GMT   (42655kb,D)

Title: Revisiting Generative Adversarial Networks for Binary Semantic
  Segmentation on Imbalanced Datasets
Authors: Lei Xu and Moncef Gabbouj
Categories: cs.CV cs.LG eess.IV
\\ ( https://arxiv.org/abs/2402.02245 ,  42655kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13005 (*cross-listing*)
replaced with revised version Fri, 8 Mar 2024 09:21:14 GMT   (614kb,D)

Title: SzCORE: A Seizure Community Open-source Research Evaluation framework
  for the validation of EEG-based automated seizure detection algorithms
Authors: Jonathan Dan, Una Pale, Alireza Amirshahi, William Cappelletti, Thorir
  Mar Ingolfsson, Xiaying Wang, Andrea Cossettini, Adriano Bernini, Luca
  Benini, S\'andor Beniczky, David Atienza, Philippe Ryvlin
Categories: eess.SP cs.LG
\\ ( https://arxiv.org/abs/2402.13005 ,  614kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17987 (*cross-listing*)
replaced with revised version Fri, 8 Mar 2024 17:47:21 GMT   (4907kb,D)

Title: Multistatic-Radar RCS-Signature Recognition of Aerial Vehicles: A
  Bayesian Fusion Approach
Authors: Michael Potter, Murat Akcakaya, Marius Necsoiu, Gunar Schirner, Deniz
  Erdogmus, Tales Imbiriba
Categories: eess.SP cs.CV cs.LG math.PR stat.ML
Comments: To be submitted to IEEE Transactions on Aerospace and Electronic
  Systems
\\ ( https://arxiv.org/abs/2402.17987 ,  4907kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19095 (*cross-listing*)
replaced with revised version Fri, 8 Mar 2024 05:30:10 GMT   (384kb)

Title: A Protein Structure Prediction Approach Leveraging Transformer and CNN
  Integration
Authors: Yanlin Zhou, Kai Tan, Xinyu Shen, Zheng He, Haotian Zheng
Categories: q-bio.BM cs.LG
\\ ( https://arxiv.org/abs/2402.19095 ,  384kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00158 (*cross-listing*)
replaced with revised version Fri, 8 Mar 2024 16:26:03 GMT   (1076kb,D)

Title: Automated Efficient Estimation using Monte Carlo Efficient Influence
  Functions
Authors: Raj Agrawal, Sam Witty, Andy Zane, Eli Bingham
Categories: stat.CO cs.LG stat.ME
\\ ( https://arxiv.org/abs/2403.00158 ,  1076kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01192 (*cross-listing*)
replaced with revised version Fri, 8 Mar 2024 15:18:19 GMT   (315kb,D)

Title: A Composite Decomposition Method for Large-Scale Global Optimization
Authors: Maojiang Tian, Minyang Chen, Wei Du, Yang Tang, Yaochu Jin, Gary G.
  Yen
Categories: math.OC cs.LG cs.NE
\\ ( https://arxiv.org/abs/2403.01192 ,  315kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02746
replaced with revised version Fri, 8 Mar 2024 09:59:40 GMT   (20466kb,D)

Title: Learning without Exact Guidance: Updating Large-scale High-resolution
  Land Cover Maps from Low-resolution Historical Labels
Authors: Zhuohong Li, Wei He, Jiepan Li, Fangxiao Lu, Hongyan Zhang
Categories: cs.CV cs.LG
Comments: 11 pages, 9 figures, accepted by CVPR 2024
\\ ( https://arxiv.org/abs/2403.02746 ,  20466kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
