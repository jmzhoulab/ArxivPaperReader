paper_240305.txt

Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月6日 01:17
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Fri  1 Mar 24 19:00:00 GMT  to  Mon  4 Mar 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.00783
Date: Sun, 18 Feb 2024 15:53:32 GMT   (342kb,D)

Title: On the Roles of LLMs in Planning: Embedding LLMs into Planning Graphs
Authors: Hankz Hankui Zhuo and Xin Chen and Rong Pan
Categories: cs.AI
\\
  Plan synthesis aims to generate a course of actions or policies to transit
given initial states to goal states, provided domain models that could be
designed by experts or learnt from training data or interactions with the
world. Intrigued by the claims of emergent planning capabilities in large
language models (LLMs), works have been proposed to investigate the planning
effectiveness of LLMs, without considering any utilization of off-the-shelf
planning techniques in LLMs. In this paper, we aim to further study the insight
of the planning capability of LLMs by investigating the roles of LLMs in
off-the-shelf planning frameworks. To do this, we investigate the effectiveness
of embedding LLMs into one of the well-known planning frameworks, graph-based
planning, proposing a novel LLMs-based planning framework with LLMs embedded in
two levels of planning graphs, i.e., mutual constraints generation level and
constraints solving level. We empirically exhibit the effectiveness of our
proposed framework in various planning domains.
\\ ( https://arxiv.org/abs/2403.00783 ,  342kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00805
Date: Sat, 24 Feb 2024 10:06:04 GMT   (3723kb)

Title: A New Dynamic Distributed Planning Approach: Application to DPDP
  Problems
Authors: Zakaria Tolba
Categories: cs.AI cs.MA
Comments: Master's thesis, in French language
\\
  In this work, we proposed a new dynamic distributed planning approach that is
able to take into account the changes that the agent introduces on his set of
actions to be planned in order to take into account the changes that occur in
his environment. Our approach fits into the context of distributed planning for
distributed plans where each agent can produce its own plans. According to our
approach the generation of the plans is based on the satisfaction of the
constraints by the use of the genetic algorithms. Our approach is to generate,
a new plan by each agent, whenever there is a change in its set of actions to
plan. This in order to take into account the new actions introduced in its new
plan. In this new plan, the agent takes, each time, as a new action set to plan
all the old un-executed actions of the old plan and the new actions engendered
by the changes and as a new initial state; the state in which the set of
actions of the agent undergoes a change. In our work, we used a concrete case
to illustrate and demonstrate the utility of our approach.
\\ ( https://arxiv.org/abs/2403.00805 ,  3723kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00810
Date: Sun, 25 Feb 2024 01:40:30 GMT   (2702kb,D)

Title: Bootstrapping Cognitive Agents with a Large Language Model
Authors: Feiyu Zhu, Reid Simmons
Categories: cs.AI cs.CL
\\
  Large language models contain noisy general knowledge of the world, yet are
hard to train or fine-tune. On the other hand cognitive architectures have
excellent interpretability and are flexible to update but require a lot of
manual work to instantiate. In this work, we combine the best of both worlds:
bootstrapping a cognitive-based model with the noisy knowledge encoded in large
language models. Through an embodied agent doing kitchen tasks, we show that
our proposed framework yields better efficiency compared to an agent based
entirely on large language models. Our experiments indicate that large language
models are a good source of information for cognitive architectures, and the
cognitive architecture in turn can verify and update the knowledge of large
language models to a specific domain.
\\ ( https://arxiv.org/abs/2403.00810 ,  2702kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00811
Date: Sun, 25 Feb 2024 02:35:56 GMT   (7208kb,D)

Title: Cognitive Bias in High-Stakes Decision-Making with LLMs
Authors: Jessica Echterhoff, Yao Liu, Abeer Alessa, Julian McAuley, Zexue He
Categories: cs.AI cs.CL
\\
  Large language models (LLMs) offer significant potential as tools to support
an expanding range of decision-making tasks. However, given their training on
human (created) data, LLMs can inherit both societal biases against protected
groups, as well as be subject to cognitive bias. Such human-like bias can
impede fair and explainable decisions made with LLM assistance. Our work
introduces BiasBuster, a framework designed to uncover, evaluate, and mitigate
cognitive bias in LLMs, particularly in high-stakes decision-making tasks.
Inspired by prior research in psychology and cognitive sciences, we develop a
dataset containing 16,800 prompts to evaluate different cognitive biases (e.g.,
prompt-induced, sequential, inherent). We test various bias mitigation
strategies, amidst proposing a novel method using LLMs to debias their own
prompts. Our analysis provides a comprehensive picture on the presence and
effects of cognitive bias across different commercial and open-source models.
We demonstrate that our self-help debiasing effectively mitigate cognitive bias
without having to manually craft examples for each bias type.
\\ ( https://arxiv.org/abs/2403.00811 ,  7208kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00823
Date: Mon, 26 Feb 2024 23:15:07 GMT   (276kb,D)

Title: Adapting to Teammates in a Cooperative Language Game
Authors: Christopher Archibald and Spencer Brosnahan
Categories: cs.AI cs.CL
\\
  The game of Codenames has recently emerged as a domain of interest for
intelligent agent design. The game is unique due to the way that language and
coordination between teammates play important roles. Previous approaches to
designing agents for this game have utilized a single internal language model
to determine action choices. This often leads to good performance with some
teammates and inferior performance with other teammates, as the agent cannot
adapt to any specific teammate. In this paper we present the first adaptive
agent for playing Codenames. We adopt an ensemble approach with the goal of
determining, during the course of interacting with a specific teammate, which
of our internal expert agents, each potentially with its own language model, is
the best match. One difficulty faced in this approach is the lack of a single
numerical metric that accurately captures the performance of a Codenames team.
Prior Codenames research has utilized a handful of different metrics to
evaluate agent teams. We propose a novel single metric to evaluate the
performance of a Codenames team, whether playing a single team (solitaire)
game, or a competitive game against another team. We then present and analyze
an ensemble agent which selects an internal expert on each turn in order to
maximize this proposed metric. Experimental analysis shows that this ensemble
approach adapts to individual teammates and often performs nearly as well as
the best internal expert with a teammate. Crucially, this success does not
depend on any previous knowledge about the teammates, the ensemble agents, or
their compatibility. This research represents an important step to making
language-based agents for cooperative language settings like Codenames more
adaptable to individual teammates.
\\ ( https://arxiv.org/abs/2403.00823 ,  276kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00829
Date: Wed, 28 Feb 2024 03:40:46 GMT   (538kb,D)

Title: TroubleLLM: Align to Red Team Expert
Authors: Zhuoer Xu, Jianping Zhang, Shiwen Cui, Changhua Meng, Weiqiang Wang
Categories: cs.AI cs.CL
\\
  Large Language Models (LLMs) become the start-of-the-art solutions for a
variety of natural language tasks and are integrated into real-world
applications. However, LLMs can be potentially harmful in manifesting
undesirable safety issues like social biases and toxic content. It is
imperative to assess its safety issues before deployment. However, the quality
and diversity of test prompts generated by existing methods are still far from
satisfactory. Not only are these methods labor-intensive and require large
budget costs, but the controllability of test prompt generation is lacking for
the specific testing domain of LLM applications. With the idea of LLM for LLM
testing, we propose the first LLM, called TroubleLLM, to generate controllable
test prompts on LLM safety issues. Extensive experiments and human evaluation
illustrate the superiority of TroubleLLM on generation quality and generation
controllability.
\\ ( https://arxiv.org/abs/2403.00829 ,  538kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00830
Date: Wed, 28 Feb 2024 08:30:49 GMT   (8433kb,D)

Title: MedAide: Leveraging Large Language Models for On-Premise Medical
  Assistance on Edge Devices
Authors: Abdul Basit, Khizar Hussain, Muhammad Abdullah Hanif, Muhammad
  Shafique
Categories: cs.AI cs.CL
Comments: 7 pages, 11 figures, ACM conference paper, 33 references
ACM-class: I.2.7
\\
  Large language models (LLMs) are revolutionizing various domains with their
remarkable natural language processing (NLP) abilities. However, deploying LLMs
in resource-constrained edge computing and embedded systems presents
significant challenges. Another challenge lies in delivering medical assistance
in remote areas with limited healthcare facilities and infrastructure. To
address this, we introduce MedAide, an on-premise healthcare chatbot. It
leverages tiny-LLMs integrated with LangChain, providing efficient edge-based
preliminary medical diagnostics and support. MedAide employs model
optimizations for minimal memory footprint and latency on embedded edge devices
without server infrastructure. The training process is optimized using low-rank
adaptation (LoRA). Additionally, the model is trained on diverse medical
datasets, employing reinforcement learning from human feedback (RLHF) to
enhance its domain-specific capabilities. The system is implemented on various
consumer GPUs and Nvidia Jetson development board. MedAide achieves 77\%
accuracy in medical consultations and scores 56 in USMLE benchmark, enabling an
energy-efficient healthcare assistance platform that alleviates privacy
concerns due to edge-based deployment, thereby empowering the community.
\\ ( https://arxiv.org/abs/2403.00830 ,  8433kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00833
Date: Wed, 28 Feb 2024 16:09:56 GMT   (2555kb,D)

Title: Position Paper: Agent AI Towards a Holistic Intelligence
Authors: Qiuyuan Huang, Naoki Wake, Bidipta Sarkar, Zane Durante, Ran Gong,
  Rohan Taori, Yusuke Noda, Demetri Terzopoulos, Noboru Kuno, Ade Famoti,
  Ashley Llorens, John Langford, Hoi Vo, Li Fei-Fei, Katsu Ikeuchi, Jianfeng
  Gao
Categories: cs.AI
Comments: 22 pages, 4 figures. arXiv admin note: substantial text overlap with
  arXiv:2401.03568
\\
  Recent advancements in large foundation models have remarkably enhanced our
understanding of sensory information in open-world environments. In leveraging
the power of foundation models, it is crucial for AI research to pivot away
from excessive reductionism and toward an emphasis on systems that function as
cohesive wholes. Specifically, we emphasize developing Agent AI -- an embodied
system that integrates large foundation models into agent actions. The emerging
field of Agent AI spans a wide range of existing embodied and agent-based
multimodal interactions, including robotics, gaming, and healthcare systems,
etc. In this paper, we propose a novel large action model to achieve embodied
intelligent behavior, the Agent Foundation Model. On top of this idea, we
discuss how agent AI exhibits remarkable capabilities across a variety of
domains and tasks, challenging our understanding of learning and cognition.
Furthermore, we discuss the potential of Agent AI from an interdisciplinary
perspective, underscoring AI cognition and consciousness within scientific
discourse. We believe that those discussions serve as a basis for future
research directions and encourage broader societal engagement.
\\ ( https://arxiv.org/abs/2403.00833 ,  2555kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00839
Date: Thu, 29 Feb 2024 02:04:00 GMT   (1757kb,D)

Title: ToolNet: Connecting Large Language Models with Massive Tools via Tool
  Graph
Authors: Xukun Liu, Zhiyuan Peng, Xiaoyuan Yi, Xing Xie, Lirong Xiang, Yuchen
  Liu, Dongkuan Xu
Categories: cs.AI cs.CL
\\
  While achieving remarkable progress in a broad range of tasks, large language
models (LLMs) remain significantly limited in properly using massive external
tools. Existing in-context learning approaches simply format tools into a list
of plain text descriptions and input them to LLMs, from which, LLMs generate a
sequence of tool calls to solve problems step by step. Such a paradigm ignores
the intrinsic dependency between tools and offloads all reasoning loads to
LLMs, making them restricted to a limited number of specifically designed
tools. It thus remains challenging for LLMs to operate on a library of massive
tools, casting a great limitation when confronted with real-world scenarios.
This paper proposes ToolNet, a plug-and-play framework that scales up the
number of tools to thousands with a moderate increase in token consumption.
ToolNet organizes tools into a directed graph. Each node represents a tool, and
weighted edges denote tool transition. Starting from an initial tool node, an
LLM navigates in the graph by iteratively choosing the next one from its
successors until the task is resolved. Extensive experiments show that ToolNet
can achieve impressive results in challenging multi-hop tool learning datasets
and is resilient to tool failures.
\\ ( https://arxiv.org/abs/2403.00839 ,  1757kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00859
Date: Thu, 29 Feb 2024 20:15:13 GMT   (124kb,D)

Title: Team Formation amidst Conflicts
Authors: Iasonas Nikolaou, Evimaria Terzi
Categories: cs.AI cs.GT cs.SI
DOI: 10.1145/3589334.3645444
\\
  In this work, we formulate the problem of team formation amidst conflicts.
The goal is to assign individuals to tasks, with given capacities, taking into
account individuals' task preferences and the conflicts between them. Using
dependent rounding schemes as our main toolbox, we provide efficient
approximation algorithms. Our framework is extremely versatile and can model
many different real-world scenarios as they arise in educational settings and
human-resource management. We test and deploy our algorithms on real-world
datasets and we show that our algorithms find assignments that are better than
those found by natural baselines. In the educational setting we also show how
our assignments are far better than those done manually by human experts. In
the human resource management application we show how our assignments increase
the diversity of teams. Finally, using a synthetic dataset we demonstrate that
our algorithms scale very well in practice.
\\ ( https://arxiv.org/abs/2403.00859 ,  124kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00861
Date: Thu, 29 Feb 2024 21:03:46 GMT   (58kb)

Title: Pivoting Retail Supply Chain with Deep Generative Techniques: Taxonomy,
  Survey and Insights
Authors: Yuan Wang, Lokesh Kumar Sambasivan, Mingang Fu, Prakhar Mehrotra
Categories: cs.AI cs.LG
\\
  Generative AI applications, such as ChatGPT or DALL-E, have shown the world
their impressive capabilities in generating human-like text or image. Diving
deeper, the science stakeholder for those AI applications are Deep Generative
Models, a.k.a DGMs, which are designed to learn the underlying distribution of
the data and generate new data points that are statistically similar to the
original dataset. One critical question is raised: how can we leverage DGMs
into morden retail supply chain realm? To address this question, this paper
expects to provide a comprehensive review of DGMs and discuss their existing
and potential usecases in retail supply chain, by (1) providing a taxonomy and
overview of state-of-the-art DGMs and their variants, (2) reviewing existing
DGM applications in retail supply chain from a end-to-end view of point, and
(3) discussing insights and potential directions on how DGMs can be further
utilized on solving retail supply chain problems.
\\ ( https://arxiv.org/abs/2403.00861 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00898
Date: Fri, 1 Mar 2024 17:29:34 GMT   (93kb)

Title: The Algorithm Configuration Problem
Authors: Gabriele Iommazzo, Claudia D'Ambrosio, Antonio Frangioni, Leo Liberti
Categories: cs.AI cs.LG math.OC
Journal-ref: In: Pardalos, P.M., Prokopyev, O.A. (eds) Encyclopedia of
  Optimization. Springer, Cham. (2023)
DOI: 10.1007/978-3-030-54621-2_749-1
\\
  The field of algorithmic optimization has significantly advanced with the
development of methods for the automatic configuration of algorithmic
parameters. This article delves into the Algorithm Configuration Problem,
focused on optimizing parametrized algorithms for solving specific instances of
decision/optimization problems. We present a comprehensive framework that not
only formalizes the Algorithm Configuration Problem, but also outlines
different approaches for its resolution, leveraging machine learning models and
heuristic strategies. The article categorizes existing methodologies into
per-instance and per-problem approaches, distinguishing between offline and
online strategies for model construction and deployment. By synthesizing these
approaches, we aim to provide a clear pathway for both understanding and
addressing the complexities inherent in algorithm configuration.
\\ ( https://arxiv.org/abs/2403.00898 ,  93kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00980
Date: Fri, 1 Mar 2024 21:04:48 GMT   (3970kb,D)

Title: Even-Ifs From If-Onlys: Are the Best Semi-Factual Explanations Found
  Using Counterfactuals As Guides?
Authors: Saugat Aryal, Mark T. Keane
Categories: cs.AI
Comments: 16 pages, 5 figures
\\
  Recently, counterfactuals using "if-only" explanations have become very
popular in eXplainable AI (XAI), as they describe which changes to
feature-inputs of a black-box AI system result in changes to a (usually
negative) decision-outcome. Even more recently, semi-factuals using "even-if"
explanations have gained more attention. They elucidate the feature-input
changes that do \textit{not} change the decision-outcome of the AI system, with
a potential to suggest more beneficial recourses. Some semi-factual methods use
counterfactuals to the query-instance to guide semi-factual production
(so-called counterfactual-guided methods), whereas others do not (so-called
counterfactual-free methods). In this work, we perform comprehensive tests of 8
semi-factual methods on 7 datasets using 5 key metrics, to determine whether
counterfactual guidance is necessary to find the best semi-factuals. The
results of these tests suggests not, but rather that computing other aspects of
the decision space lead to better semi-factual XAI.
\\ ( https://arxiv.org/abs/2403.00980 ,  3970kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01199
Date: Sat, 2 Mar 2024 12:41:11 GMT   (1708kb)

Title: The Case for Animal-Friendly AI
Authors: Sankalpa Ghose, Yip Fai Tse, Kasra Rasaee, Jeff Sebo, Peter Singer
Categories: cs.AI
Comments: AAAI 2024 Workshop on Public Sector LLMs: Algorithmic and
  Sociotechnical Design. 12 pages, 11 figures
\\
  Artificial intelligence is seen as increasingly important, and potentially
profoundly so, but the fields of AI ethics and AI engineering have not fully
recognized that these technologies, including large language models (LLMs),
will have massive impacts on animals. We argue that this impact matters,
because animals matter morally.
  As a first experiment in evaluating animal consideration in LLMs, we
constructed a proof-of-concept Evaluation System, which assesses LLM responses
and biases from multiple perspectives. This system evaluates LLM outputs by two
criteria: their truthfulness, and the degree of consideration they give to the
interests of animals. We tested OpenAI ChatGPT 4 and Anthropic Claude 2.1 using
a set of structured queries and predefined normative perspectives. Preliminary
results suggest that the outcomes of the tested models can be benchmarked
regarding the consideration they give to animals, and that generated positions
and biases might be addressed and mitigated with more developed and validated
systems.
  Our research contributes one possible approach to integrating animal ethics
in AI, opening pathways for future studies and practical applications in
various fields, including education, public policy, and regulation, that
involve or relate to animals and society. Overall, this study serves as a step
towards more useful and responsible AI systems that better recognize and
respect the vital interests and perspectives of all sentient beings.
\\ ( https://arxiv.org/abs/2403.01199 ,  1708kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01508
Date: Sun, 3 Mar 2024 13:13:53 GMT   (1257kb,D)

Title: Soft Reasoning on Uncertain Knowledge Graphs
Authors: Weizhi Fei, Zihao Wang, Hang Yin, Yang Duan, Hanghang Tong, Yangqiu
  Song
Categories: cs.AI
Comments: 10 pages
\\
  The study of machine learning-based logical query-answering enables reasoning
with large-scale and incomplete knowledge graphs. This paper further advances
this line of research by considering the uncertainty in the knowledge. The
uncertain nature of knowledge is widely observed in the real world, but
\textit{does not} align seamlessly with the first-order logic underpinning
existing studies. To bridge this gap, we study the setting of soft queries on
uncertain knowledge, which is motivated by the establishment of soft constraint
programming. We further propose an ML-based approach with both forward
inference and backward calibration to answer soft queries on large-scale,
incomplete, and uncertain knowledge graphs. Theoretical discussions present
that our methods share the same complexity as state-of-the-art inference
algorithms for first-order queries. Empirical results justify the superior
performance of our approach against previous ML-based methods with number
embedding extensions.
\\ ( https://arxiv.org/abs/2403.01508 ,  1257kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01757
Date: Mon, 4 Mar 2024 06:24:21 GMT   (12412kb,D)

Title: How Multimodal Integration Boost the Performance of LLM for
  Optimization: Case Study on Capacitated Vehicle Routing Problems
Authors: Yuxiao Huang, Wenjie Zhang, Liang Feng, Xingyu Wu, Kay Chen Tan
Categories: cs.AI cs.CL cs.LG cs.NE math.OC
Comments: 8pages,3 figures, 2 tables
\\
  Recently, large language models (LLMs) have notably positioned them as
capable tools for addressing complex optimization challenges. Despite this
recognition, a predominant limitation of existing LLM-based optimization
methods is their struggle to capture the relationships among decision variables
when relying exclusively on numerical text prompts, especially in
high-dimensional problems. Keeping this in mind, we first propose to enhance
the optimization performance using multimodal LLM capable of processing both
textual and visual prompts for deeper insights of the processed optimization
problem. This integration allows for a more comprehensive understanding of
optimization problems, akin to human cognitive processes. We have developed a
multimodal LLM-based optimization framework that simulates human
problem-solving workflows, thereby offering a more nuanced and effective
analysis. The efficacy of this method is evaluated through extensive empirical
studies focused on a well-known combinatorial optimization problem, i.e.,
capacitated vehicle routing problem. The results are compared against those
obtained from the LLM-based optimization algorithms that rely solely on textual
prompts, demonstrating the significant advantages of our multimodal approach.
\\ ( https://arxiv.org/abs/2403.01757 ,  12412kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01784
Date: Mon, 4 Mar 2024 07:26:07 GMT   (1151kb,D)

Title: CatCode: A Comprehensive Evaluation Framework for LLMs On the Mixture of
  Code and Text
Authors: Zhenru Lin, Yiqun Yao, Yang Yuan
Categories: cs.AI cs.PL
Comments: 10 pages, 5 figures
\\
  Large language models (LLMs) such as ChatGPT are increasingly proficient in
understanding and generating a mixture of code and text. Evaluation based on
such $\textit{mixture}$ can lead to a more comprehensive understanding of the
models' abilities in solving coding problems. However, in this context, current
evaluation methods are either limited in task coverage or lack standardization.
To address this issue, we propose using category theory as a framework for
evaluation. Specifically, morphisms within a code category can represent code
debugging and transformation, functors between two categories represent code
translation, and functors between a code category and a natural language
category represent code generation, explanation, and reproduction. We present
an automatic evaluation framework called $\textbf{CatCode}$
($\textbf{Cat}$egory $\textbf{Code}$) that can comprehensively assess the
coding abilities of LLMs, including ChatGPT, Text-Davinci, and CodeGeeX.
\\ ( https://arxiv.org/abs/2403.01784 ,  1151kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01816
Date: Mon, 4 Mar 2024 08:04:41 GMT   (2050kb,D)

Title: SMAUG: A Sliding Multidimensional Task Window-Based MARL Framework for
  Adaptive Real-Time Subtask Recognition
Authors: Wenjing Zhang, Wei Zhang
Categories: cs.AI cs.MA
\\
  Instead of making behavioral decisions directly from the exponentially
expanding joint observational-action space, subtask-based multi-agent
reinforcement learning (MARL) methods enable agents to learn how to tackle
different subtasks. Most existing subtask-based MARL methods are based on
hierarchical reinforcement learning (HRL). However, these approaches often
limit the number of subtasks, perform subtask recognition periodically, and can
only identify and execute a specific subtask within the predefined fixed time
period, which makes them inflexible and not suitable for diverse and dynamic
scenarios with constantly changing subtasks. To break through above
restrictions, a \textbf{S}liding \textbf{M}ultidimensional t\textbf{A}sk window
based m\textbf{U}ti-agent reinforcement learnin\textbf{G} framework (SMAUG) is
proposed for adaptive real-time subtask recognition. It leverages a sliding
multidimensional task window to extract essential information of subtasks from
trajectory segments concatenated based on observed and predicted trajectories
in varying lengths. An inference network is designed to iteratively predict
future trajectories with the subtask-oriented policy network. Furthermore,
intrinsic motivation rewards are defined to promote subtask exploration and
behavior diversity. SMAUG can be integrated with any Q-learning-based approach.
Experiments on StarCraft II show that SMAUG not only demonstrates performance
superiority in comparison with all baselines but also presents a more prominent
and swift rise in rewards during the initial training stage.
\\ ( https://arxiv.org/abs/2403.01816 ,  2050kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01832
Date: Mon, 4 Mar 2024 08:29:15 GMT   (177kb)

Title: Model-Based Data-Centric AI: Bridging the Divide Between Academic Ideals
  and Industrial Pragmatism
Authors: Chanjun Park, Minsoo Khang, Dahyun Kim
Categories: cs.AI cs.CL
Comments: Accepted for Data-centric Machine Learning Research (DMLR) Workshop
  at ICLR 2024
\\
  This paper delves into the contrasting roles of data within academic and
industrial spheres, highlighting the divergence between Data-Centric AI and
Model-Agnostic AI approaches. We argue that while Data-Centric AI focuses on
the primacy of high-quality data for model performance, Model-Agnostic AI
prioritizes algorithmic flexibility, often at the expense of data quality
considerations. This distinction reveals that academic standards for data
quality frequently do not meet the rigorous demands of industrial applications,
leading to potential pitfalls in deploying academic models in real-world
settings. Through a comprehensive analysis, we address these disparities,
presenting both the challenges they pose and strategies for bridging the gap.
Furthermore, we propose a novel paradigm: Model-Based Data-Centric AI, which
aims to reconcile these differences by integrating model considerations into
data optimization processes. This approach underscores the necessity for
evolving data requirements that are sensitive to the nuances of both academic
research and industrial deployment. By exploring these discrepancies, we aim to
foster a more nuanced understanding of data's role in AI development and
encourage a convergence of academic and industrial standards to enhance AI's
real-world applicability.
\\ ( https://arxiv.org/abs/2403.01832 ,  177kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01888
Date: Mon, 4 Mar 2024 09:49:35 GMT   (397kb,D)

Title: Fast Benchmarking of Asynchronous Multi-Fidelity Optimization on
  Zero-Cost Benchmarks
Authors: Shuhei Watanabe and Neeratyoy Mallik and Edward Bergman and Frank
  Hutter
Categories: cs.AI cs.LG
Comments: Submitted to AutoML Conference 2024 ABCD Track
\\
  While deep learning has celebrated many successes, its results often hinge on
the meticulous selection of hyperparameters (HPs). However, the time-consuming
nature of deep learning training makes HP optimization (HPO) a costly endeavor,
slowing down the development of efficient HPO tools. While zero-cost
benchmarks, which provide performance and runtime without actual training,
offer a solution for non-parallel setups, they fall short in parallel setups as
each worker must communicate its queried runtime to return its evaluation in
the exact order. This work addresses this challenge by introducing a
user-friendly Python package that facilitates efficient parallel HPO with
zero-cost benchmarks. Our approach calculates the exact return order based on
the information stored in file system, eliminating the need for long waiting
times and enabling much faster HPO evaluations. We first verify the correctness
of our approach through extensive testing and the experiments with 6 popular
HPO libraries show its applicability to diverse libraries and its ability to
achieve over 1000x speedup compared to a traditional approach. Our package can
be installed via pip install mfhpo-simulator.
\\ ( https://arxiv.org/abs/2403.01888 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02053
Date: Mon, 4 Mar 2024 13:57:34 GMT   (684kb)

Title: A Scoping Review of Energy-Efficient Driving Behaviors and Applied
  State-of-the-Art AI Methods
Authors: Zhipeng Ma, Bo N{\o}rregaard J{\o}rgensen, Zheng Ma
Categories: cs.AI
Journal-ref: Energies 2024, 17, 500
DOI: 10.3390/en17020500
\\
  The transportation sector remains a major contributor to greenhouse gas
emissions. The understanding of energy-efficient driving behaviors and
utilization of energy-efficient driving strategies are essential to reduce
vehicles' fuel consumption. However, there is no comprehensive investigation
into energy-efficient driving behaviors and strategies. Furthermore, many
state-of-the-art AI models have been applied for the analysis of eco-friendly
driving styles, but no overview is available. To fill the gap, this paper
conducts a thorough literature review on ecological driving behaviors and
styles and analyzes the driving factors influencing energy consumption and
state-of-the-art methodologies. With a thorough scoping review process, the
methodological and related data are compared. The results show that the factors
that impact driving behaviors can be summarized into eleven features including
speed, acceleration, deceleration, pedal, and so on. This paper finds that
supervised/unsupervised learning algorithms and reinforcement learning
frameworks have been popularly used to model the vehicle's energy consumption
with multi-dimensional data. Furthermore, the literature shows that the driving
data are collected from either simulators or real-world experiments, and the
real-world data are mainly stored and transmitted by meters, controller area
networks, onboard data services, smartphones, and additional sensors installed
in the vehicle. Based on driving behavior factors, driver characteristics, and
safety rules, this paper recommends nine energy-efficient driving styles
including four guidelines for the drivers' selection and adjustment of the
vehicle parameters, three recommendations for the energy-efficient driving
styles in different driving scenarios, and two subjective suggestions for
different types of drivers and employers.
\\ ( https://arxiv.org/abs/2403.02053 ,  684kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02054
Date: Mon, 4 Mar 2024 13:57:37 GMT   (7409kb,D)

Title: Large Language Model-Based Evolutionary Optimizer: Reasoning with
  elitism
Authors: Shuvayan Brahmachary, Subodh M. Joshi, Aniruddha Panda, Kaushik
  Koneripalli, Arun Kumar Sagotra, Harshil Patel, Ankush Sharma, Ameya D.
  Jagtap, Kaushic Kalyanaraman
Categories: cs.AI
\\
  Large Language Models (LLMs) have demonstrated remarkable reasoning
abilities, prompting interest in their application as black-box optimizers.
This paper asserts that LLMs possess the capability for zero-shot optimization
across diverse scenarios, including multi-objective and high-dimensional
problems. We introduce a novel population-based method for numerical
optimization using LLMs called Language-Model-Based Evolutionary Optimizer
(LEO). Our hypothesis is supported through numerical examples, spanning
benchmark and industrial engineering problems such as supersonic nozzle shape
optimization, heat transfer, and windfarm layout optimization. We compare our
method to several gradient-based and gradient-free optimization approaches.
While LLMs yield comparable results to state-of-the-art methods, their
imaginative nature and propensity to hallucinate demand careful handling. We
provide practical guidelines for obtaining reliable answers from LLMs and
discuss method limitations and potential research directions.
\\ ( https://arxiv.org/abs/2403.02054 ,  7409kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02164
Date: Mon, 4 Mar 2024 16:11:57 GMT   (2924kb)

Title: Cognition is All You Need - The Next Layer of AI Above Large Language
  Models
Authors: Nova Spivack, Sam Douglas, Michelle Crames, Tim Connors
Categories: cs.AI cs.MA
Comments: 62 pages, 18 figures
ACM-class: I.2.0
\\
  Recent studies of the applications of conversational AI tools, such as
chatbots powered by large language models, to complex real-world knowledge work
have shown limitations related to reasoning and multi-step problem solving.
Specifically, while existing chatbots simulate shallow reasoning and
understanding they are prone to errors as problem complexity increases. The
failure of these systems to address complex knowledge work is due to the fact
that they do not perform any actual cognition. In this position paper, we
present Cognitive AI, a higher-level framework for implementing
programmatically defined neuro-symbolic cognition above and outside of large
language models. Specifically, we propose a dual-layer functional architecture
for Cognitive AI that serves as a roadmap for AI systems that can perform
complex multi-step knowledge work. We propose that Cognitive AI is a necessary
precursor for the evolution of higher forms of AI, such as AGI, and
specifically claim that AGI cannot be achieved by probabilistic approaches on
their own. We conclude with a discussion of the implications for large language
models, adoption cycles in AI, and commercial Cognitive AI development.
\\ ( https://arxiv.org/abs/2403.02164 ,  2924kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02290
Date: Mon, 4 Mar 2024 18:19:48 GMT   (3324kb,D)

Title: Koopman-Assisted Reinforcement Learning
Authors: Preston Rozwood, Edward Mehrez, Ludger Paehler, Wen Sun, Steven L.
  Brunton
Categories: cs.AI cs.LG math.DS math.OC
Comments: 35 pages, 12 figures
\\
  The Bellman equation and its continuous form, the Hamilton-Jacobi-Bellman
(HJB) equation, are ubiquitous in reinforcement learning (RL) and control
theory. However, these equations quickly become intractable for systems with
high-dimensional states and nonlinearity. This paper explores the connection
between the data-driven Koopman operator and Markov Decision Processes (MDPs),
resulting in the development of two new RL algorithms to address these
limitations. We leverage Koopman operator techniques to lift a nonlinear system
into new coordinates where the dynamics become approximately linear, and where
HJB-based methods are more tractable. In particular, the Koopman operator is
able to capture the expectation of the time evolution of the value function of
a given system via linear dynamics in the lifted coordinates. By parameterizing
the Koopman operator with the control actions, we construct a ``Koopman
tensor'' that facilitates the estimation of the optimal value function. Then, a
transformation of Bellman's framework in terms of the Koopman tensor enables us
to reformulate two max-entropy RL algorithms: soft value iteration and soft
actor-critic (SAC). This highly flexible framework can be used for
deterministic or stochastic systems as well as for discrete or continuous-time
dynamics. Finally, we show that these Koopman Assisted Reinforcement Learning
(KARL) algorithms attain state-of-the-art (SOTA) performance with respect to
traditional neural network-based SAC and linear quadratic regulator (LQR)
baselines on four controlled dynamical systems: a linear state-space system,
the Lorenz system, fluid flow past a cylinder, and a double-well potential with
non-isotropic stochastic forcing.
\\ ( https://arxiv.org/abs/2403.02290 ,  3324kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00788
Date: Tue, 20 Feb 2024 04:26:31 GMT   (1944kb)

Title: PRECISE Framework: GPT-based Text For Improved Readability, Reliability,
  and Understandability of Radiology Reports For Patient-Centered Care
Authors: Satvik Tripathi, Liam Mutter, Meghana Muppuri, Suhani Dheer, Emiliano
  Garza-Frias, Komal Awan, Aakash Jha, Michael Dezube, Azadeh Tabari,
  Christopher P. Bridge, Dania Daye
Categories: cs.CL cs.AI cs.HC cs.LG
\\
  This study introduces and evaluates the PRECISE framework, utilizing OpenAI's
GPT-4 to enhance patient engagement by providing clearer and more accessible
chest X-ray reports at a sixth-grade reading level. The framework was tested on
500 reports, demonstrating significant improvements in readability,
reliability, and understandability. Statistical analyses confirmed the
effectiveness of the PRECISE approach, highlighting its potential to foster
patient-centric care delivery in healthcare decision-making.
\\ ( https://arxiv.org/abs/2403.00788 ,  1944kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00791
Date: Thu, 22 Feb 2024 20:11:24 GMT   (833kb,D)

Title: $\textit{L+M-24}$: Building a Dataset for Language + Molecules @ ACL
  2024
Authors: Carl Edwards and Qingyun Wang and Lawrence Zhao and Heng Ji
Categories: cs.CL cs.AI q-bio.BM q-bio.QM
Comments: The dataset, finetuned baselines, and evaluation code are released
  publicly at https://github.com/language-plus-molecules/LPM-24-Dataset through
  https://huggingface.co/language-plus-molecules
\\
  Language-molecule models have emerged as an exciting direction for molecular
discovery and understanding. However, training these models is challenging due
to the scarcity of molecule-language pair datasets. At this point, datasets
have been released which are 1) small and scraped from existing databases, 2)
large but noisy and constructed by performing entity linking on the scientific
literature, and 3) built by converting property prediction datasets to natural
language using templates. In this document, we detail the $\textit{L+M-24}$
dataset, which has been created for the Language + Molecules Workshop shared
task at ACL 2024. In particular, $\textit{L+M-24}$ is designed to focus on
three key benefits of natural language in molecule design: compositionality,
functionality, and abstraction.
\\ ( https://arxiv.org/abs/2403.00791 ,  833kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00794
Date: Fri, 23 Feb 2024 02:58:12 GMT   (280kb,D)

Title: Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large
  Language Models
Authors: Zachary Horvitz, Jingru Chen, Rahul Aditya, Harshvardhan Srivastava,
  Robert West, Zhou Yu, Kathleen McKeown
Categories: cs.CL cs.AI cs.LG
\\
  Humor is a fundamental facet of human cognition and interaction. Yet, despite
recent advances in natural language processing, humor detection remains a
challenging task that is complicated by the scarcity of datasets that pair
humorous texts with similar non-humorous counterparts. In our work, we
investigate whether large language models (LLMs), can generate synthetic data
for humor detection via editing texts. We benchmark LLMs on an existing human
dataset and show that current LLMs display an impressive ability to `unfun'
jokes, as judged by humans and as measured on the downstream task of humor
detection. We extend our approach to a code-mixed English-Hindi humor dataset,
where we find that GPT-4's synthetic data is highly rated by bilingual
annotators and provides challenging adversarial examples for humor classifiers.
\\ ( https://arxiv.org/abs/2403.00794 ,  280kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00795
Date: Fri, 23 Feb 2024 05:31:36 GMT   (898kb)

Title: Executing Natural Language-Described Algorithms with Large Language
  Models: An Investigation
Authors: Xin Zheng, Qiming Zhu, Hongyu Lin, Yaojie Lu, Xianpei Han and Le Sun
Categories: cs.CL cs.AI
Comments: Accepted at LREC-COLING 2024
\\
  Executing computer programs described in natural language has long been a
pursuit of computer science. With the advent of enhanced natural language
understanding capabilities exhibited by large language models (LLMs), the path
toward this goal has been illuminated. In this paper, we seek to examine the
capacity of present-day LLMs to comprehend and execute algorithms outlined in
natural language. We established an algorithm test set sourced from
Introduction to Algorithm, a well-known textbook that contains many
representative widely-used algorithms. To systematically assess LLMs' code
execution abilities, we selected 30 algorithms, generated 300 random-sampled
instances in total, and evaluated whether popular LLMs can understand and
execute these algorithms. Our findings reveal that LLMs, notably GPT-4, can
effectively execute programs described in natural language, as long as no heavy
numeric computation is involved. We believe our findings contribute to
evaluating LLMs' code execution abilities and would encourage further
investigation and application for the computation power of LLMs.
\\ ( https://arxiv.org/abs/2403.00795 ,  898kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00799
Date: Fri, 23 Feb 2024 17:38:43 GMT   (621kb,D)

Title: An Empirical Study of Data Ability Boundary in LLMs' Math Reasoning
Authors: Zui Chen, Yezeng Chen, Jiaqi Han, Zhijie Huang, Ji Qi, Yi Zhou
Categories: cs.CL cs.AI cs.LG
Comments: 33 pages, 5 figures
\\
  Large language models (LLMs) are displaying emergent abilities for math
reasoning tasks,and there is a growing attention on enhancing the ability of
open-source LLMs through supervised fine-tuning (SFT).In this paper, we aim to
explore a general data strategy for supervised data to help optimize and expand
math reasoning ability.Firstly, we determine the ability boundary of reasoning
paths augmentation by identifying these paths' minimal optimal set.Secondly, we
validate that different abilities of the model can be cumulatively enhanced by
Mix of Minimal Optimal Sets of corresponding types of data, while our models
MMOS achieve SOTA performance on series base models under much lower
construction costs.Besides, we point out GSM-HARD is not really hard and
today's LLMs no longer lack numerical robustness.Also, we provide an Auto
Problem Generator for robustness testing and educational applications.Our code
and data are publicly available at https://github.com/cyzhh/MMOS.
\\ ( https://arxiv.org/abs/2403.00799 ,  621kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00800
Date: Fri, 23 Feb 2024 17:40:31 GMT   (201kb,D)

Title: Brain-Inspired Two-Stage Approach: Enhancing Mathematical Reasoning by
  Imitating Human Thought Processes
Authors: Yezeng Chen, Zui Chen, Yi Zhou
Categories: cs.CL cs.AI cs.LG
Comments: 12 pages, 5 figures
\\
  Although large language models demonstrate emergent abilities in solving math
word problems, there is a challenging task in complex multi-step mathematical
reasoning tasks. To improve model performance on mathematical reasoning tasks,
previous work has conducted supervised fine-tuning on open-source models by
improving the quality and quantity of data. In this paper, we propose a novel
approach, named Brain, to imitate human thought processes to enhance
mathematical reasoning abilities, using the Frontal Lobe Model to generate
plans, and then employing the Parietal Lobe Model to generate code and execute
to obtain answers. First, we achieve SOTA performance in comparison with Code
LLaMA 7B based models through this method. Secondly, we find that plans can be
explicitly extracted from natural language, code, or formal language. Our code
and data are publicly available at https://github.com/cyzhh/Brain.
\\ ( https://arxiv.org/abs/2403.00800 ,  201kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00804
Date: Sat, 24 Feb 2024 00:15:09 GMT   (1684kb,D)

Title: Uncovering Customer Issues through Topological Natural Language Analysis
Authors: Shu-Ting Pi, Sidarth Srinivasan, Yuying Zhu, Michael Yang, Qun Liu
Categories: cs.CL cs.AI cs.LG
Comments: Accepted in KDD 2023 Workshop on Decision Intelligence and Analytics
  for Online Marketplaces
\\
  E-commerce companies deal with a high volume of customer service requests
daily. While a simple annotation system is often used to summarize the topics
of customer contacts, thoroughly exploring each specific issue can be
challenging. This presents a critical concern, especially during an emerging
outbreak where companies must quickly identify and address specific issues. To
tackle this challenge, we propose a novel machine learning algorithm that
leverages natural language techniques and topological data analysis to monitor
emerging and trending customer issues. Our approach involves an end-to-end deep
learning framework that simultaneously tags the primary question sentence of
each customer's transcript and generates sentence embedding vectors. We then
whiten the embedding vectors and use them to construct an undirected graph.
From there, we define trending and emerging issues based on the topological
properties of each transcript. We have validated our results through various
methods and found that they are highly consistent with news sources.
\\ ( https://arxiv.org/abs/2403.00804 ,  1684kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00808
Date: Sat, 24 Feb 2024 14:18:11 GMT   (657kb,D)

Title: IPED: An Implicit Perspective for Relational Triple Extraction based on
  Diffusion Model
Authors: Jianli Zhao, Changhao Xu, Bin Jiang
Categories: cs.CL cs.AI
Comments: 12 pages, 4 figures, committed to NAACL 2024
\\
  Relational triple extraction is a fundamental task in the field of
information extraction, and a promising framework based on table filling has
recently gained attention as a potential baseline for entity relation
extraction. However, inherent shortcomings such as redundant information and
incomplete triple recognition remain problematic. To address these challenges,
we propose an Implicit Perspective for relational triple Extraction based on
Diffusion model (IPED), an innovative approach for extracting relational
triples. Our classifier-free solution adopts an implicit strategy using block
coverage to complete the tables, avoiding the limitations of explicit tagging
methods. Additionally, we introduce a generative model structure, the
block-denoising diffusion model, to collaborate with our implicit perspective
and effectively circumvent redundant information disruptions. Experimental
results on two popular datasets demonstrate that IPED achieves state-of-the-art
performance while gaining superior inference speed and low computational
complexity. To support future research, we have made our source code publicly
available online.
\\ ( https://arxiv.org/abs/2403.00808 ,  657kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00809
Date: Sat, 24 Feb 2024 20:00:03 GMT   (329kb,D)

Title: Abdelhak at SemEval-2024 Task 9 : Decoding Brainteasers, The Efficacy of
  Dedicated Models Versus ChatGPT
Authors: Abdelhak Kelious, Mounir Okirim
Categories: cs.CL cs.AI
\\
  This study introduces a dedicated model aimed at solving the BRAINTEASER task
9 , a novel challenge designed to assess models lateral thinking capabilities
through sentence and word puzzles. Our model demonstrates remarkable efficacy,
securing Rank 1 in sentence puzzle solving during the test phase with an
overall score of 0.98. Additionally, we explore the comparative performance of
ChatGPT, specifically analyzing how variations in temperature settings affect
its ability to engage in lateral thinking and problem-solving. Our findings
indicate a notable performance disparity between the dedicated model and
ChatGPT, underscoring the potential of specialized approaches in enhancing
creative reasoning in AI.
\\ ( https://arxiv.org/abs/2403.00809 ,  329kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00812
Date: Sun, 25 Feb 2024 07:09:10 GMT   (10044kb,D)

Title: LoRA Meets Dropout under a Unified Framework
Authors: Sheng Wang, Liheng Chen, Jiyue Jiang, Boyang Xue, Lingpeng Kong, Chuan
  Wu
Categories: cs.CL cs.AI
\\
  With the remarkable capabilities, large language models (LLMs) have emerged
as essential elements in numerous NLP applications, while parameter-efficient
finetuning, especially LoRA, has gained popularity as a lightweight approach
for model customization. Meanwhile, various dropout methods, initially designed
for full finetuning with all the parameters updated, alleviates overfitting
associated with excessive parameter redundancy. Hence, a possible contradiction
arises from negligible trainable parameters of LoRA and the effectiveness of
previous dropout methods, which has been largely overlooked. To fill this gap,
we first confirm that parameter-efficient LoRA is also overfitting-prone. We
then revisit transformer-specific dropout methods, and establish their
equivalence and distinctions mathematically and empirically. Building upon this
comparative analysis, we introduce a unified framework for a comprehensive
investigation, which instantiates these methods based on dropping position,
structural pattern and compensation measure. Through this framework, we reveal
the new preferences and performance comparisons of them when involved with
limited trainable parameters. This framework also allows us to amalgamate the
most favorable aspects into a novel dropout method named HiddenKey. Extensive
experiments verify the remarkable superiority and sufficiency of HiddenKey
across multiple models and tasks, which highlights it as the preferred approach
for high-performance and parameter-efficient finetuning of LLMs.
\\ ( https://arxiv.org/abs/2403.00812 ,  10044kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00813
Date: Sun, 25 Feb 2024 12:37:29 GMT   (606kb,D)

Title: UrbanGPT: Spatio-Temporal Large Language Models
Authors: Zhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia,
  Dawei Yin, Chao Huang
Categories: cs.CL cs.AI cs.CY
Comments: 11 pages
\\
  Spatio-temporal prediction aims to forecast and gain insights into the
ever-changing dynamics of urban environments across both time and space. Its
purpose is to anticipate future patterns, trends, and events in diverse facets
of urban life, including transportation, population movement, and crime rates.
Although numerous efforts have been dedicated to developing neural network
techniques for accurate predictions on spatio-temporal data, it is important to
note that many of these methods heavily depend on having sufficient labeled
data to generate precise spatio-temporal representations. Unfortunately, the
issue of data scarcity is pervasive in practical urban sensing scenarios.
Consequently, it becomes necessary to build a spatio-temporal model with strong
generalization capabilities across diverse spatio-temporal learning scenarios.
Taking inspiration from the remarkable achievements of large language models
(LLMs), our objective is to create a spatio-temporal LLM that can exhibit
exceptional generalization capabilities across a wide range of downstream urban
tasks. To achieve this objective, we present the UrbanGPT, which seamlessly
integrates a spatio-temporal dependency encoder with the instruction-tuning
paradigm. This integration enables LLMs to comprehend the complex
inter-dependencies across time and space, facilitating more comprehensive and
accurate predictions under data scarcity. To validate the effectiveness of our
approach, we conduct extensive experiments on various public datasets, covering
different spatio-temporal prediction tasks. The results consistently
demonstrate that our UrbanGPT, with its carefully designed architecture,
consistently outperforms state-of-the-art baselines. These findings highlight
the potential of building large language models for spatio-temporal learning,
particularly in zero-shot scenarios where labeled data is scarce.
\\ ( https://arxiv.org/abs/2403.00813 ,  606kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00815
Date: Sun, 25 Feb 2024 23:10:20 GMT   (8166kb,D)

Title: RAM-EHR: Retrieval Augmentation Meets Clinical Predictions on Electronic
  Health Records
Authors: Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Bowen Jin, May D. Wang,
  Joyce C. Ho, Carl Yang
Categories: cs.CL cs.AI cs.IR q-bio.OT
\\
  We present RAM-EHR, a Retrieval AugMentation pipeline to improve clinical
predictions on Electronic Health Records (EHRs). RAM-EHR first collects
multiple knowledge sources, converts them into text format, and uses dense
retrieval to obtain information related to medical concepts. This strategy
addresses the difficulties associated with complex names for the concepts.
RAM-EHR then augments the local EHR predictive model co-trained with
consistency regularization to capture complementary information from patient
visits and summarized knowledge. Experiments on two EHR datasets show the
efficacy of RAM-EHR over previous knowledge-enhanced baselines (3.4% gain in
AUROC and 7.2% gain in AUPR), emphasizing the effectiveness of the summarized
knowledge from RAM-EHR for clinical prediction tasks. The code will be
published at \url{https://github.com/ritaranx/RAM-EHR}.
\\ ( https://arxiv.org/abs/2403.00815 ,  8166kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00818
Date: Mon, 26 Feb 2024 09:21:59 GMT   (304kb,D)

Title: DenseMamba: State Space Models with Dense Hidden Connection for
  Efficient Large Language Models
Authors: Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo,
  Yunhe Wang
Categories: cs.CL cs.LG
\\
  Large language models (LLMs) face a daunting challenge due to the excessive
computational and memory requirements of the commonly used Transformer
architecture. While state space model (SSM) is a new type of foundational
network architecture offering lower computational complexity, their performance
has yet to fully rival that of Transformers. This paper introduces DenseSSM, a
novel approach to enhance the flow of hidden information between layers in
SSMs. By selectively integrating shallowlayer hidden states into deeper layers,
DenseSSM retains fine-grained information crucial for the final output. Dense
connections enhanced DenseSSM still maintains the training parallelizability
and inference efficiency. The proposed method can be widely applicable to
various SSM types like RetNet and Mamba. With similar model size, DenseSSM
achieves significant improvements, exemplified by DenseRetNet outperforming the
original RetNet with up to 5% accuracy improvement on public benchmarks.
\\ ( https://arxiv.org/abs/2403.00818 ,  304kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00821
Date: Mon, 26 Feb 2024 16:17:19 GMT   (415kb,D)

Title: Social Media as a Sensor: Analyzing Twitter Data for Breast Cancer
  Medication Effects Using Natural Language Processing
Authors: Seibi Kobara, Alireza Rafiei, Masoud Nateghi, Selen Bozkurt,
  Rishikesan Kamaleswaran, Abeed Sarker
Categories: cs.CL cs.LG cs.SI
\\
  Breast cancer is a significant public health concern and is the leading cause
of cancer-related deaths among women. Despite advances in breast cancer
treatments, medication non-adherence remains a major problem. As electronic
health records do not typically capture patient-reported outcomes that may
reveal information about medication-related experiences, social media presents
an attractive resource for enhancing our understanding of the patients'
treatment experiences. In this paper, we developed natural language processing
(NLP) based methodologies to study information posted by an automatically
curated breast cancer cohort from social media. We employed a transformer-based
classifier to identify breast cancer patients/survivors on X (Twitter) based on
their self-reported information, and we collected longitudinal data from their
profiles. We then designed a multi-layer rule-based model to develop a breast
cancer therapy-associated side effect lexicon and detect patterns of medication
usage and associated side effects among breast cancer patients. 1,454,637 posts
were available from 583,962 unique users, of which 62,042 were detected as
breast cancer members using our transformer-based model. 198 cohort members
mentioned breast cancer medications with tamoxifen as the most common. Our side
effect lexicon identified well-known side effects of hormone and chemotherapy.
Furthermore, it discovered a subject feeling towards cancer and medications,
which may suggest a pre-clinical phase of side effects or emotional distress.
This analysis highlighted not only the utility of NLP techniques in
unstructured social media data to identify self-reported breast cancer posts,
medication usage patterns, and treatment side effects but also the richness of
social data on such clinical questions.
\\ ( https://arxiv.org/abs/2403.00821 ,  415kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00824
Date: Tue, 27 Feb 2024 00:24:42 GMT   (30641kb,D)

Title: Information Flow Routes: Automatically Interpreting Language Models at
  Scale
Authors: Javier Ferrando and Elena Voita
Categories: cs.CL cs.AI
\\
  Information flows by routes inside the network via mechanisms implemented in
the model. These routes can be represented as graphs where nodes correspond to
token representations and edges to operations inside the network. We
automatically build these graphs in a top-down manner, for each prediction
leaving only the most important nodes and edges. In contrast to the existing
workflows relying on activation patching, we do this through attribution: this
allows us to efficiently uncover existing circuits with just a single forward
pass. Additionally, the applicability of our method is far beyond patching: we
do not need a human to carefully design prediction templates, and we can
extract information flow routes for any prediction (not just the ones among the
allowed templates). As a result, we can talk about model behavior in general,
for specific types of predictions, or different domains. We experiment with
Llama 2 and show that the role of some attention heads is overall important,
e.g. previous token heads and subword merging heads. Next, we find similarities
in Llama 2 behavior when handling tokens of the same part of speech. Finally,
we show that some model components can be specialized on domains such as coding
or multilingual texts.
\\ ( https://arxiv.org/abs/2403.00824 ,  30641kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00825
Date: Tue, 27 Feb 2024 07:26:16 GMT   (58kb)

Title: Comparing effectiveness of regularization methods on text
  classification: Simple and complex model in data shortage situation
Authors: Jongga Lee, Jaeseung Yim, Seohee Park, Changwon Lim
Categories: cs.CL
Comments: 13 pages, 2 figures
\\
  Text classification is the task of assigning a document to a predefined
class. However, it is expensive to acquire enough labeled documents or to label
them. In this paper, we study the regularization methods' effects on various
classification models when only a few labeled data are available. We compare a
simple word embedding-based model, which is simple but effective, with complex
models (CNN and BiLSTM). In supervised learning, adversarial training can
further regularize the model. When an unlabeled dataset is available, we can
regularize the model using semi-supervised learning methods such as the Pi
model and virtual adversarial training. We evaluate the regularization effects
on four text classification datasets (AG news, DBpedia, Yahoo! Answers, Yelp
Polarity), using only 0.1% to 0.5% of the original labeled training documents.
The simple model performs relatively well in fully supervised learning, but
with the help of adversarial training and semi-supervised learning, both simple
and complex models can be regularized, showing better results for complex
models. Although the simple model is robust to overfitting, a complex model
with well-designed prior beliefs can be also robust to overfitting.
\\ ( https://arxiv.org/abs/2403.00825 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00826
Date: Tue, 27 Feb 2024 10:22:45 GMT   (2114kb,D)

Title: LLMGuard: Guarding Against Unsafe LLM Behavior
Authors: Shubh Goyal, Medha Hira, Shubham Mishra, Sukriti Goyal, Arnav Goel,
  Niharika Dadu, Kirushikesh DB, Sameep Mehta, Nishtha Madaan
Categories: cs.CL cs.CR cs.LG
Comments: accepted in demonstration track of AAAI-24
\\
  Although the rise of Large Language Models (LLMs) in enterprise settings
brings new opportunities and capabilities, it also brings challenges, such as
the risk of generating inappropriate, biased, or misleading content that
violates regulations and can have legal concerns. To alleviate this, we present
"LLMGuard", a tool that monitors user interactions with an LLM application and
flags content against specific behaviours or conversation topics. To do this
robustly, LLMGuard employs an ensemble of detectors.
\\ ( https://arxiv.org/abs/2403.00826 ,  2114kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00827
Date: Tue, 27 Feb 2024 19:13:01 GMT   (471kb,D)

Title: Self-Refinement of Language Models from External Proxy Metrics Feedback
Authors: Keshav Ramji, Young-Suk Lee, Ram\'on Fernandez Astudillo, Md Arafat
  Sultan, Tahira Naseem, Asim Munawar, Radu Florian, Salim Roukos
Categories: cs.CL cs.AI cs.LG
\\
  It is often desirable for Large Language Models (LLMs) to capture multiple
objectives when providing a response. In document-grounded response generation,
for example, agent responses are expected to be relevant to a user's query
while also being grounded in a given document. In this paper, we introduce
Proxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine
its own initial response along key dimensions of quality guided by external
metrics feedback, yielding an overall better final response. ProMiSe leverages
feedback on response quality through principle-specific proxy metrics, and
iteratively refines its response one principle at a time. We apply ProMiSe to
open source language models Flan-T5-XXL and Llama-2-13B-Chat, to evaluate its
performance on document-grounded question answering datasets, MultiDoc2Dial and
QuAC, demonstrating that self-refinement improves response quality. We further
show that fine-tuning Llama-2-13B-Chat on the synthetic dialogue data generated
by ProMiSe yields significant performance improvements over the zero-shot
baseline as well as a supervised fine-tuned model on human annotated data.
\\ ( https://arxiv.org/abs/2403.00827 ,  471kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00828
Date: Tue, 27 Feb 2024 19:16:39 GMT   (373kb,D)

Title: Deep Learning Detection Method for Large Language Models-Generated
  Scientific Content
Authors: Bushra Alhijawi, Rawan Jarrar, Aseel AbuAlRub, and Arwa Bader
Categories: cs.CL cs.AI cs.LG
\\
  Large Language Models (LLMs), such as GPT-3 and BERT, reshape how textual
content is written and communicated. These models have the potential to
generate scientific content that is indistinguishable from that written by
humans. Hence, LLMs carry severe consequences for the scientific community,
which relies on the integrity and reliability of publications. This research
paper presents a novel ChatGPT-generated scientific text detection method,
AI-Catcher. AI-Catcher integrates two deep learning models, multilayer
perceptron (MLP) and convolutional neural networks (CNN). The MLP learns the
feature representations of the linguistic and statistical features. The CNN
extracts high-level representations of the sequential patterns from the textual
content. AI-Catcher is a multimodal model that fuses hidden patterns derived
from MLP and CNN. In addition, a new ChatGPT-Generated scientific text dataset
is collected to enhance AI-generated text detection tools, AIGTxt. AIGTxt
contains 3000 records collected from published academic articles across ten
domains and divided into three classes: Human-written, ChatGPT-generated, and
Mixed text. Several experiments are conducted to evaluate the performance of
AI-Catcher. The comparative results demonstrate the capability of AI-Catcher to
distinguish between human-written and ChatGPT-generated scientific text more
accurately than alternative methods. On average, AI-Catcher improved accuracy
by 37.4%.
\\ ( https://arxiv.org/abs/2403.00828 ,  373kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00835
Date: Wed, 28 Feb 2024 20:17:04 GMT   (1239kb,D)

Title: CLLMs: Consistency Large Language Models
Authors: Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, Hao Zhang
Categories: cs.CL cs.AI
\\
  Parallel decoding methods such as Jacobi decoding show promise for more
efficient LLM inference as it breaks the sequential nature of the LLM decoding
process and transforms it into parallelizable computation. However, in
practice, it achieves little speedup compared to traditional autoregressive
(AR) decoding, primarily because Jacobi decoding seldom accurately predicts
more than one token in a single fixed-point iteration step. To address this, we
develop a new approach aimed at realizing fast convergence from any state to
the fixed point on a Jacobi trajectory. This is accomplished by refining the
target LLM to consistently predict the fixed point given any state as input.
Extensive experiments demonstrate the effectiveness of our method, showing
2.4$\times$ to 3.4$\times$ improvements in generation speed while preserving
generation quality across both domain-specific and open-domain benchmarks.
\\ ( https://arxiv.org/abs/2403.00835 ,  1239kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00840
Date: Thu, 29 Feb 2024 09:35:41 GMT   (1597kb)

Title: EyeGPT: Ophthalmic Assistant with Large Language Models
Authors: Xiaolan Chen, Ziwei Zhao, Weiyi Zhang, Pusheng Xu, Le Gao, Mingpu Xu,
  Yue Wu, Yinwen Li, Danli Shi, Mingguang He
Categories: cs.CL cs.AI
Comments: 47 pages, 4 figures, 1 table, 2 supplementary figures and 9
  supplementary tables
\\
  Artificial intelligence (AI) has gained significant attention in healthcare
consultation due to its potential to improve clinical workflow and enhance
medical communication. However, owing to the complex nature of medical
information, large language models (LLM) trained with general world knowledge
might not possess the capability to tackle medical-related tasks at an expert
level. Here, we introduce EyeGPT, a specialized LLM designed specifically for
ophthalmology, using three optimization strategies including role-playing,
finetuning, and retrieval-augmented generation. In particular, we proposed a
comprehensive evaluation framework that encompasses a diverse dataset, covering
various subspecialties of ophthalmology, different users, and diverse inquiry
intents. Moreover, we considered multiple evaluation metrics, including
accuracy, understandability, trustworthiness, empathy, and the proportion of
hallucinations. By assessing the performance of different EyeGPT variants, we
identify the most effective one, which exhibits comparable levels of
understandability, trustworthiness, and empathy to human ophthalmologists (all
Ps>0.05). Overall, ur study provides valuable insights for future research,
facilitating comprehensive comparisons and evaluations of different strategies
for developing specialized LLMs in ophthalmology. The potential benefits
include enhancing the patient experience in eye care and optimizing
ophthalmologists' services.
\\ ( https://arxiv.org/abs/2403.00840 ,  1597kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00862
Date: Thu, 29 Feb 2024 21:05:14 GMT   (165kb,D)

Title: NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and
  Safety Adherence in Chinese Journalistic Editorial Applications
Authors: Miao Li and Ming-Bin Chen and Bo Tang and Shengbin Hou and Pengyu Wang
  and Haiying Deng and Zhiyu Li and Feiyu Xiong and Keming Mao and Peng Cheng
  and Yi Luo
Categories: cs.CL cs.AI
Comments: 27 pages
\\
  This study presents NewsBench, a novel benchmark framework developed to
evaluate the capability of Large Language Models (LLMs) in Chinese Journalistic
Writing Proficiency (JWP) and their Safety Adherence (SA), addressing the gap
between journalistic ethics and the risks associated with AI utilization.
Comprising 1,267 tasks across 5 editorial applications, 7 aspects (including
safety and journalistic writing with 4 detailed facets), and spanning 24 news
topics domains, NewsBench employs two GPT-4 based automatic evaluation
protocols validated by human assessment. Our comprehensive analysis of 11 LLMs
highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative
deficiency in journalistic ethic adherence during creative writing tasks. These
findings underscore the need for enhanced ethical guidance in AI-generated
journalistic content, marking a step forward in aligning AI capabilities with
journalistic standards and safety considerations.
\\ ( https://arxiv.org/abs/2403.00862 ,  165kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00868
Date: Fri, 1 Mar 2024 04:39:16 GMT   (2118kb,D)

Title: SoftTiger: A Clinical Foundation Model for Healthcare Workflows
Authors: Ye Chen, Igor Couto, Wei Cai, Cong Fu, Bruno Dorneles
Categories: cs.CL cs.AI
\\
  We release and introduce SoftTiger, a clinical large language model (CLaM)
designed as a foundation model for healthcare workflows. The narrative and
unstructured nature of clinical notes is a major obstacle for healthcare
intelligentization. We address a critical problem of structuring clinical notes
into clinical data, according to international interoperability standards. We
collect and annotate data for three critical subtasks, namely, international
patient summary, clinical impression and medical encounter. We then supervised
fine-tuned a state-of-the-art LLM using public and credentialed clinical data.
The training is orchestrated in a way that the target model can first support
basic clinical tasks such as abbreviation expansion and temporal information
extraction, and then learn to perform more complex downstream clinical tasks
such as impression and encounter summary. Moreover, we address, several
modeling challenges in the healthcare context, e.g., extra long context window.
Our blind pairwise evaluation shows that SoftTiger outperforms other popular
open-source models and GPT-3.5, comparable to Gemini-pro, and only has a mild
gap from GPT-4. We believe that LLMs may become a step-stone towards healthcare
digitalization and democratization. Therefore, we publicly release SoftTiger
models at scales of 13 billion and 70 billion parameters, as well as datasets
and code for our innovative scalable evaluation, hopefully, making a
significant contribution to the healthcare industry.
\\ ( https://arxiv.org/abs/2403.00868 ,  2118kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00876
Date: Fri, 1 Mar 2024 08:13:48 GMT   (6956kb,D)

Title: Word Order and World Knowledge
Authors: Qinghua Zhao, Vinit Ravishankar, Nicolas Garneau and Anders S{\o}gaard
Categories: cs.CL cs.AI
\\
  Word order is an important concept in natural language, and in this work, we
study how word order affects the induction of world knowledge from raw text
using language models. We use word analogies to probe for such knowledge.
Specifically, in addition to the natural word order, we first respectively
extract texts of six fixed word orders from five languages and then pretrain
the language models on these texts. Finally, we analyze the experimental
results of the fixed word orders on word analogies and show that i) certain
fixed word orders consistently outperform or underperform others, though the
specifics vary across languages, and ii) the Wov2Lex hypothesis is not hold in
pre-trained language models, and the natural word order typically yields
mediocre results. The source code will be made publicly available at
https://github.com/lshowway/probing_by_analogy.
\\ ( https://arxiv.org/abs/2403.00876 ,  6956kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00888
Date: Fri, 1 Mar 2024 11:54:14 GMT   (74kb,D)

Title: Margin Discrepancy-based Adversarial Training for Multi-Domain Text
  Classification
Authors: Yuan Wu
Categories: cs.CL cs.LG
Comments: 16 pages
\\
  Multi-domain text classification (MDTC) endeavors to harness available
resources from correlated domains to enhance the classification accuracy of the
target domain. Presently, most MDTC approaches that embrace adversarial
training and the shared-private paradigm exhibit cutting-edge performance.
Unfortunately, these methods face a non-negligible challenge: the absence of
theoretical guarantees in the design of MDTC algorithms. The dearth of
theoretical underpinning poses a substantial impediment to the advancement of
MDTC algorithms. To tackle this problem, we first provide a theoretical
analysis of MDTC by decomposing the MDTC task into multiple domain adaptation
tasks. We incorporate the margin discrepancy as the measure of domain
divergence and establish a new generalization bound based on Rademacher
complexity. Subsequently, we propose a margin discrepancy-based adversarial
training (MDAT) approach for MDTC, in accordance with our theoretical analysis.
To validate the efficacy of the proposed MDAT method, we conduct empirical
studies on two MDTC benchmarks. The experimental results demonstrate that our
MDAT approach surpasses state-of-the-art baselines on both datasets.
\\ ( https://arxiv.org/abs/2403.00888 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00896
Date: Fri, 1 Mar 2024 15:38:55 GMT   (440kb,D)

Title: DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large
  Language Models
Authors: Kedi Chen and Qin Chen and Jie Zhou and Yishen He and Liang He
Categories: cs.CL cs.AI
\\
  Since large language models (LLMs) achieve significant success in recent
years, the hallucination issue remains a challenge, numerous benchmarks are
proposed to detect the hallucination. Nevertheless, some of these benchmarks
are not naturally generated by LLMs but are intentionally induced. Also, many
merely focus on the factuality hallucination while ignoring the faithfulness
hallucination. Additionally, although dialogue pattern is more widely utilized
in the era of LLMs, current benchmarks only concentrate on sentence-level and
passage-level hallucination. In this study, we propose DiaHalu, the first
dialogue-level hallucination evaluation benchmark to our knowledge. Initially,
we integrate the collected topics into system prompts and facilitate a dialogue
between two ChatGPT3.5. Subsequently, we manually modify the contents that do
not adhere to human language conventions and then have LLMs re-generate,
simulating authentic human-machine interaction scenarios. Finally, professional
scholars annotate all the samples in the dataset. DiaHalu covers four common
multi-turn dialogue domains and five hallucination subtypes, extended from
factuality and faithfulness hallucination. Experiments through some well-known
LLMs and detection methods on the dataset show that DiaHalu is a challenging
benchmark, holding significant value for further research.
\\ ( https://arxiv.org/abs/2403.00896 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00952
Date: Fri, 1 Mar 2024 20:03:44 GMT   (443kb,D)

Title: MediSwift: Efficient Sparse Pre-trained Biomedical Language Models
Authors: Vithursan Thangarasa, Mahmoud Salem, Shreyas Saxena, Kevin Leong, Joel
  Hestness, Sean Lie
Categories: cs.CL cs.LG
\\
  Large language models (LLMs) are typically trained on general source data for
various domains, but a recent surge in domain-specific LLMs has shown their
potential to outperform general-purpose models in domain-specific tasks (e.g.,
biomedicine). Although domain-specific pre-training enhances efficiency and
leads to smaller models, the computational costs of training these LLMs remain
high, posing budgeting challenges. We introduce MediSwift, a suite of
biomedical LMs that leverage sparse pre-training on domain-specific biomedical
text data. By inducing up to 75% weight sparsity during the pre-training phase,
MediSwift achieves a 2-2.5x reduction in training FLOPs. Notably, all sparse
pre-training was performed on the Cerebras CS-2 system, which is specifically
designed to realize the acceleration benefits from unstructured weight
sparsity, thereby significantly enhancing the efficiency of the MediSwift
models. Through subsequent dense fine-tuning and strategic soft prompting,
MediSwift models outperform existing LLMs up to 7B parameters on biomedical
tasks, setting new benchmarks w.r.t efficiency-accuracy on tasks such as
PubMedQA. Our results show that sparse pre-training, along with dense
fine-tuning and soft prompting, offers an effective method for creating
high-performing, computationally efficient models in specialized domains.
\\ ( https://arxiv.org/abs/2403.00952 ,  443kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00953
Date: Fri, 1 Mar 2024 20:06:39 GMT   (910kb)

Title: AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge
  Graph Construction Based on Ontologies-enhanced Large Language Models
Authors: Lang Cao, Jimeng Sun, Adam Cross
Categories: cs.CL cs.AI
\\
  Objectives: Our objective is to create an end-to-end system called AutoRD,
which automates extracting information from clinical text about rare diseases.
We have conducted various tests to evaluate the performance of AutoRD and
highlighted its strengths and limitations in this paper.
  Materials and Methods: Our system, AutoRD, is a software pipeline involving
data preprocessing, entity extraction, relation extraction, entity calibration,
and knowledge graph construction. We implement this using large language models
and medical knowledge graphs developed from open-source medical ontologies. We
quantitatively evaluate our system on entity extraction, relation extraction,
and the performance of knowledge graph construction.
  Results: AutoRD achieves an overall F1 score of 47.3%, a 14.4% improvement
compared to the base LLM. In detail, AutoRD achieves an overall entity
extraction F1 score of 56.1% (rare_disease: 83.5%, disease: 35.8%,
symptom_and_sign: 46.1%, anaphor: 67.5%) and an overall relation extraction F1
score of 38.6% (produces: 34.7%, increases_risk_of: 12.4%, is_a: 37.4%,
is_acronym: 44.1%, is_synonym: 16.3%, anaphora: 57.5%). Our qualitative
experiment also demonstrates that the performance in constructing the knowledge
graph is commendable.
  Discussion: AutoRD demonstrates the potential of LLM applications in rare
disease detection. This improvement is attributed to several design, including
the integration of ontologies-enhanced LLMs.
  Conclusion: AutoRD is an automated end-to-end system for extracting rare
disease information from text to build knowledge graphs. It uses
ontologies-enhanced LLMs for a robust medical knowledge base. The superior
performance of AutoRD is validated by experimental evaluations, demonstrating
the potential of LLMs in healthcare.
\\ ( https://arxiv.org/abs/2403.00953 ,  910kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00964
Date: Fri, 1 Mar 2024 20:31:10 GMT   (58kb,D)

Title: MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM
  Hallucination Detection
Authors: Federico Borra, Claudio Savelli, Giacomo Rosso, Alkis Koudounas,
  Flavio Giobergia
Categories: cs.CL cs.LG
Comments: Under revision at SemEval 2024
\\
  In Natural Language Generation (NLG), contemporary Large Language Models
(LLMs) face several challenges, such as generating fluent yet inaccurate
outputs and reliance on fluency-centric metrics. This often leads to neural
networks exhibiting "hallucinations". The SHROOM challenge focuses on
automatically identifying these hallucinations in the generated text. To tackle
these issues, we introduce two key components, a data augmentation pipeline
incorporating LLM-assisted pseudo-labelling and sentence rephrasing, and a
voting ensemble from three models pre-trained on Natural Language Inference
(NLI) tasks and fine-tuned on diverse datasets.
\\ ( https://arxiv.org/abs/2403.00964 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00982
Date: Fri, 1 Mar 2024 21:10:20 GMT   (770kb,D)

Title: LocalRQA: From Generating Data to Locally Training, Testing, and
  Deploying Retrieval-Augmented QA Systems
Authors: Xiao Yu, Yunan Lu, Zhou Yu
Categories: cs.CL
\\
  Retrieval-augmented question-answering systems combine retrieval techniques
with large language models to provide answers that are more accurate and
informative. Many existing toolkits allow users to quickly build such systems
using off-the-shelf models, but they fall short in supporting researchers and
developers to customize the model training, testing, and deployment process. We
propose LocalRQA, an open-source toolkit that features a wide selection of
model training algorithms, evaluation methods, and deployment tools curated
from the latest research. As a showcase, we build QA systems using online
documentation obtained from Databricks and Faire's websites. We find 7B-models
trained and deployed using LocalRQA reach a similar performance compared to
using OpenAI's text-ada-002 and GPT-4-turbo.
\\ ( https://arxiv.org/abs/2403.00982 ,  770kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00986
Date: Fri, 1 Mar 2024 21:16:29 GMT   (1340kb,D)

Title: Merging Text Transformer Models from Different Initializations
Authors: Neha Verma, Maha Elbayad
Categories: cs.CL cs.AI cs.LG
\\
  Recent work on one-shot permutation-based model merging has shown impressive
low- or zero-barrier mode connectivity between models from completely different
initializations. However, this line of work has not yet extended to the
Transformer architecture, despite its dominant popularity in the language
domain. Therefore, in this work, we investigate the extent to which separate
Transformer minima learn similar features, and propose a model merging
technique to investigate the relationship between these minima in the loss
landscape. The specifics of the architecture, like its residual connections,
multi-headed attention, and discrete, sequential input, require specific
interventions in order to compute model permutations that remain within the
same functional equivalence class. In merging these models with our method, we
consistently find lower loss barriers between minima compared to model
averaging for several models trained on a masked-language modeling task or
fine-tuned on a language understanding benchmark. Our results show that the
minima of these models are less sharp and isolated than previously understood,
and provide a basis for future work on merging separately trained Transformer
models.
\\ ( https://arxiv.org/abs/2403.00986 ,  1340kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00990
Date: Fri, 1 Mar 2024 21:24:24 GMT   (8388kb,D)

Title: Formulation Comparison for Timeline Construction using LLMs
Authors: Kimihiro Hasegawa, Nikhil Kandukuri, Susan Holm, Yukari Yamakawa,
  Teruko Mitamura
Categories: cs.CL
\\
  Constructing a timeline requires identifying the chronological order of
events in an article. In prior timeline construction datasets, temporal orders
are typically annotated by either event-to-time anchoring or event-to-event
pairwise ordering, both of which suffer from missing temporal information. To
mitigate the issue, we develop a new evaluation dataset, TimeSET, consisting of
single-document timelines with document-level order annotation. TimeSET
features saliency-based event selection and partial ordering, which enable a
practical annotation workload. Aiming to build better automatic timeline
construction systems, we propose a novel evaluation framework to compare
multiple task formulations with TimeSET by prompting open LLMs, i.e., Llama 2
and Flan-T5. Considering that identifying temporal orders of events is a core
subtask in timeline construction, we further benchmark open LLMs on existing
event temporal ordering datasets to gain a robust understanding of their
capabilities. Our experiments show that (1) NLI formulation with Flan-T5
demonstrates a strong performance among others, while (2) timeline construction
and event temporal ordering are still challenging tasks for few-shot LLMs. Our
code and data are available at https://github.com/kimihiroh/timeset.
\\ ( https://arxiv.org/abs/2403.00990 ,  8388kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00998
Date: Fri, 1 Mar 2024 21:48:08 GMT   (833kb,D)

Title: Predictions from language models for multiple-choice tasks are not
  robust under variation of scoring methods
Authors: Polina Tsvilodub, Hening Wang, Sharon Grosch and Michael Franke
Categories: cs.CL
Comments: 8 pages, 3 figures
\\
  This paper systematically compares different methods of deriving item-level
predictions of language models for multiple-choice tasks. It compares scoring
methods for answer options based on free generation of responses, various
probability-based scores, a Likert-scale style rating method, and embedding
similarity. In a case study on pragmatic language interpretation, we find that
LLM predictions are not robust under variation of method choice, both within a
single LLM and across different LLMs. As this variability entails pronounced
researcher degrees of freedom in reporting results, knowledge of the
variability is crucial to secure robustness of results and research integrity.
\\ ( https://arxiv.org/abs/2403.00998 ,  833kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01002
Date: Fri, 1 Mar 2024 21:59:03 GMT   (442kb,D)

Title: Attribute Structuring Improves LLM-Based Evaluation of Clinical Text
  Summaries
Authors: Zelalem Gero, Chandan Singh, Yiqing Xie, Sheng Zhang, Tristan Naumann,
  Jianfeng Gao, Hoifung Poon
Categories: cs.CL cs.AI
Comments: 4 pages
\\
  Summarizing clinical text is crucial in health decision-support and clinical
research. Large language models (LLMs) have shown the potential to generate
accurate clinical text summaries, but still struggle with issues regarding
grounding and evaluation, especially in safety-critical domains such as health.
Holistically evaluating text summaries is challenging because they may contain
unsubstantiated information. Here, we explore a general mitigation framework
using Attribute Structuring (AS), which structures the summary evaluation
process. It decomposes the evaluation process into a grounded procedure that
uses an LLM for relatively simple structuring and scoring tasks, rather than
the full task of holistic summary evaluation. Experiments show that AS
consistently improves the correspondence between human annotations and
automated metrics in clinical text summarization. Additionally, AS yields
interpretations in the form of a short text span corresponding to each output,
which enables efficient human auditing, paving the way towards trustworthy
evaluation of clinical information in resource-constrained scenarios. We
release our code, prompts, and an open-source benchmark at
https://github.com/microsoft/attribute-structuring.
\\ ( https://arxiv.org/abs/2403.01002 ,  442kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01031
Date: Fri, 1 Mar 2024 23:38:02 GMT   (46486kb,D)

Title: Peacock: A Family of Arabic Multimodal Large Language Models and
  Benchmarks
Authors: Fakhraddin Alwajih, El Moatez Billah Nagoudi, Gagan Bhatia,
  Abdelrahman Mohamed, Muhammad Abdul-Mageed
Categories: cs.CL cs.AI
Comments: Under Review
\\
  Multimodal large language models (MLLMs) have proven effective in a wide
range of tasks requiring complex reasoning and linguistic comprehension.
However, due to a lack of high-quality multimodal resources in languages other
than English, success of MLLMs remains relatively limited to English-based
settings. This poses significant challenges in developing comparable models for
other languages, including even those with large speaker populations such as
Arabic. To alleviate this challenge, we introduce a comprehensive family of
Arabic MLLMs, dubbed \textit{Peacock}, with strong vision and language
capabilities. Through comprehensive qualitative and quantitative analysis, we
demonstrate the solid performance of our models on various visual reasoning
tasks and further show their emerging dialectal potential. Additionally, we
introduce ~\textit{Henna}, a new benchmark specifically designed for assessing
MLLMs on aspects related to Arabic culture, setting the first stone for
culturally-aware Arabic MLLMs.The GitHub repository for the \textit{Peacock}
project is available at \url{https://github.com/UBC-NLP/peacock}.
\\ ( https://arxiv.org/abs/2403.01031 ,  46486kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01061
Date: Sat, 2 Mar 2024 01:52:14 GMT   (2961kb,D)

Title: Reading Subtext: Evaluating Large Language Models on Short Story
  Summarization with Writers
Authors: Melanie Subbiah, Sean Zhang, Lydia B. Chilton, Kathleen McKeown
Categories: cs.CL
\\
  We evaluate recent Large language Models (LLMs) on the challenging task of
summarizing short stories, which can be lengthy, and include nuanced subtext or
scrambled timelines. Importantly, we work directly with authors to ensure that
the stories have not been shared online (and therefore are unseen by the
models), and to obtain informed evaluations of summary quality using judgments
from the authors themselves. Through quantitative and qualitative analysis
grounded in narrative theory, we compare GPT-4, Claude-2.1, and LLama-2-70B. We
find that all three models make faithfulness mistakes in over 50% of summaries
and struggle to interpret difficult subtext. However, at their best, the models
can provide thoughtful thematic analysis of stories. We additionally
demonstrate that LLM judgments of summary quality do not match the feedback
from the writers.
\\ ( https://arxiv.org/abs/2403.01061 ,  2961kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01063
Date: Sat, 2 Mar 2024 02:00:51 GMT   (2015kb,D)

Title: FaiMA: Feature-aware In-context Learning for Multi-domain Aspect-based
  Sentiment Analysis
Authors: Songhua Yang, Xinke Jiang, Hanjie Zhao, Wenxuan Zeng, Hongde Liu,
  Yuxiang Jia
Categories: cs.CL
\\
  Multi-domain aspect-based sentiment analysis (ABSA) seeks to capture
fine-grained sentiment across diverse domains. While existing research narrowly
focuses on single-domain applications constrained by methodological limitations
and data scarcity, the reality is that sentiment naturally traverses multiple
domains. Although large language models (LLMs) offer a promising solution for
ABSA, it is difficult to integrate effectively with established techniques,
including graph-based models and linguistics, because modifying their internal
architecture is not easy. To alleviate this problem, we propose a novel
framework, Feature-aware In-context Learning for Multi-domain ABSA (FaiMA). The
core insight of FaiMA is to utilize in-context learning (ICL) as a
feature-aware mechanism that facilitates adaptive learning in multi-domain ABSA
tasks. Specifically, we employ a multi-head graph attention network as a text
encoder optimized by heuristic rules for linguistic, domain, and sentiment
features. Through contrastive learning, we optimize sentence representations by
focusing on these diverse features. Additionally, we construct an efficient
indexing mechanism, allowing FaiMA to stably retrieve highly relevant examples
across multiple dimensions for any given input. To evaluate the efficacy of
FaiMA, we build the first multi-domain ABSA benchmark dataset. Extensive
experimental results demonstrate that FaiMA achieves significant performance
improvements in multiple domains compared to baselines, increasing F1 by 2.07%
on average. Source code and data sets are anonymously available at
https://github.com/SupritYoung/FaiMA.
\\ ( https://arxiv.org/abs/2403.01063 ,  2015kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01069
Date: Sat, 2 Mar 2024 02:25:55 GMT   (9801kb,D)

Title: LLMCRIT: Teaching Large Language Models to Use Criteria
Authors: Weizhe Yuan and Pengfei Liu and Matthias Gall\'e
Categories: cs.CL
Comments: 8 pages, 4 tables, 3 figures in the main text
\\
  Humans follow criteria when they execute tasks, and these criteria are
directly used to assess the quality of task completion. Therefore, having
models learn to use criteria to provide feedback can help humans or models to
perform tasks better. However, existing research in this field tends to
consider only a limited set of criteria or quality assessment aspects. To fill
this gap, we propose a general framework that enables large language models
(LLMs) to use comprehensive criteria for a task in delivering natural language
feedback on task execution. In particular, we present a model-in-the-loop
framework that semi-automatically derives criteria from collected guidelines
for different writing tasks and constructs in-context demonstrations for each
criterion. We choose three tasks from real-world scenarios to operationalize
this idea: paper introduction writing, Python code writing, and Reddit post
writing, and evaluate our feedback generation framework using different LLMs.
The results reveal the fine-grained effects of incorporating criteria and
demonstrations and provide valuable insights on how to teach LLMs to use
criteria more effectively.
\\ ( https://arxiv.org/abs/2403.01069 ,  9801kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01081
Date: Sat, 2 Mar 2024 03:48:37 GMT   (1468kb,D)

Title: LAB: Large-Scale Alignment for ChatBots
Authors: Shivchander Sudalairaj, Abhishek Bhandwaldar, Aldo Pareja, Kai Xu,
  David D. Cox, Akash Srivastava
Categories: cs.CL cs.LG
\\
  This work introduces LAB (Large-scale Alignment for chatBots), a novel
methodology designed to overcome the scalability challenges in the
instruction-tuning phase of large language model (LLM) training. Leveraging a
taxonomy-guided synthetic data generation process and a multi-phase tuning
framework, LAB significantly reduces reliance on expensive human annotations
and proprietary models like GPT-4. We demonstrate that LAB-trained models can
achieve competitive performance across several benchmarks compared to models
trained with traditional human-annotated or GPT-4 generated synthetic data.
Thus offering a scalable, cost-effective solution for enhancing LLM
capabilities and instruction-following behaviors without the drawbacks of
catastrophic forgetting, marking a step forward in the efficient training of
LLMs for a wide range of applications.
\\ ( https://arxiv.org/abs/2403.01081 ,  1468kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01106
Date: Sat, 2 Mar 2024 06:38:15 GMT   (9250kb,D)

Title: Distilling Text Style Transfer With Self-Explanation From LLMs
Authors: Chiyu Zhang, Honglong Cai, Yuezhang (Music) Li, Yuexin Wu, Le Hou,
  Muhammad Abdul-Mageed
Categories: cs.CL cs.AI
Comments: under review
\\
  Text Style Transfer (TST) seeks to alter the style of text while retaining
its core content. Given the constraints of limited parallel datasets for TST,
we propose CoTeX, a framework that leverages large language models (LLMs)
alongside chain-of-thought (CoT) prompting to facilitate TST. CoTeX distills
the complex rewriting and reasoning capabilities of LLMs into more streamlined
models capable of working with both non-parallel and parallel data. Through
experimentation across four TST datasets, CoTeX is shown to surpass traditional
supervised fine-tuning and knowledge distillation methods, particularly in
low-resource settings. We conduct a comprehensive evaluation, comparing CoTeX
against current unsupervised, supervised, in-context learning (ICL) techniques,
and instruction-tuned LLMs. Furthermore, CoTeX distinguishes itself by offering
transparent explanations for its style transfer process.
\\ ( https://arxiv.org/abs/2403.01106 ,  9250kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01116
Date: Sat, 2 Mar 2024 07:49:57 GMT   (1450kb,D)

Title: MulCogBench: A Multi-modal Cognitive Benchmark Dataset for Evaluating
  Chinese and English Computational Language Models
Authors: Yunhao Zhang, Xiaohan Zhang, Chong Li, Shaonan Wang, Chengqing Zong
Categories: cs.CL
\\
  Pre-trained computational language models have recently made remarkable
progress in harnessing the language abilities which were considered unique to
humans. Their success has raised interest in whether these models represent and
process language like humans. To answer this question, this paper proposes
MulCogBench, a multi-modal cognitive benchmark dataset collected from native
Chinese and English participants. It encompasses a variety of cognitive data,
including subjective semantic ratings, eye-tracking, functional magnetic
resonance imaging (fMRI), and magnetoencephalography (MEG). To assess the
relationship between language models and cognitive data, we conducted a
similarity-encoding analysis which decodes cognitive data based on its pattern
similarity with textual embeddings. Results show that language models share
significant similarities with human cognitive data and the similarity patterns
are modulated by the data modality and stimuli complexity. Specifically,
context-aware models outperform context-independent models as language stimulus
complexity increases. The shallow layers of context-aware models are better
aligned with the high-temporal-resolution MEG signals whereas the deeper layers
show more similarity with the high-spatial-resolution fMRI. These results
indicate that language models have a delicate relationship with brain language
representations. Moreover, the results between Chinese and English are highly
consistent, suggesting the generalizability of these findings across languages.
\\ ( https://arxiv.org/abs/2403.01116 ,  1450kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01139
Date: Sat, 2 Mar 2024 08:53:40 GMT   (4489kb,D)

Title: ParallelPARC: A Scalable Pipeline for Generating Natural-Language
  Analogies
Authors: Oren Sultan, Yonatan Bitton, Ron Yosef, Dafna Shahaf
Categories: cs.CL cs.AI
\\
  Analogy-making is central to human cognition, allowing us to adapt to novel
situations -- an ability that current AI systems still lack. Most analogy
datasets today focus on simple analogies (e.g., word analogies); datasets
including complex types of analogies are typically manually curated and very
small. We believe that this holds back progress in computational analogy. In
this work, we design a data generation pipeline, ParallelPARC (Parallel
Paragraph Creator) leveraging state-of-the-art Large Language Models (LLMs) to
create complex, paragraph-based analogies, as well as distractors, both simple
and challenging. We demonstrate our pipeline and create ProPara-Logy, a dataset
of analogies between scientific processes. We publish a gold-set, validated by
humans, and a silver-set, generated automatically. We test LLMs' and humans'
analogy recognition in binary and multiple-choice settings, and found that
humans outperform the best models (~13% gap) after a light supervision. We
demonstrate that our silver-set is useful for training models. Lastly, we show
challenging distractors confuse LLMs, but not humans. We hope our pipeline will
encourage research in this emerging field.
\\ ( https://arxiv.org/abs/2403.01139 ,  4489kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01152
Date: Sat, 2 Mar 2024 09:39:13 GMT   (342kb,D)

Title: A Survey of AI-generated Text Forensic Systems: Detection, Attribution,
  and Characterization
Authors: Tharindu Kumarage, Garima Agrawal, Paras Sheth, Raha Moraffah, Aman
  Chadha, Joshua Garland, Huan Liu
Categories: cs.CL cs.AI
\\
  We have witnessed lately a rapid proliferation of advanced Large Language
Models (LLMs) capable of generating high-quality text. While these LLMs have
revolutionized text generation across various domains, they also pose
significant risks to the information ecosystem, such as the potential for
generating convincing propaganda, misinformation, and disinformation at scale.
This paper offers a review of AI-generated text forensic systems, an emerging
field addressing the challenges of LLM misuses. We present an overview of the
existing efforts in AI-generated text forensics by introducing a detailed
taxonomy, focusing on three primary pillars: detection, attribution, and
characterization. These pillars enable a practical understanding of
AI-generated text, from identifying AI-generated content (detection),
determining the specific AI model involved (attribution), and grouping the
underlying intents of the text (characterization). Furthermore, we explore
available resources for AI-generated text forensics research and discuss the
evolving challenges and future directions of forensic systems in an AI era.
\\ ( https://arxiv.org/abs/2403.01152 ,  342kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01163
Date: Sat, 2 Mar 2024 10:34:11 GMT   (1214kb,D)

Title: BootTOD: Bootstrap Task-oriented Dialogue Representations by Aligning
  Diverse Responses
Authors: Weihao Zeng, Keqing He, Yejie Wang, Dayuan Fu, Weiran Xu
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  Pre-trained language models have been successful in many scenarios. However,
their usefulness in task-oriented dialogues is limited due to the intrinsic
linguistic differences between general text and task-oriented dialogues.
Current task-oriented dialogue pre-training methods rely on a contrastive
framework, which faces challenges such as selecting true positives and hard
negatives, as well as lacking diversity. In this paper, we propose a novel
dialogue pre-training model called BootTOD. It learns task-oriented dialogue
representations via a self-bootstrapping framework. Unlike contrastive
counterparts, BootTOD aligns context and context+response representations and
dismisses the requirements of contrastive pairs. BootTOD also uses multiple
appropriate response targets to model the intrinsic one-to-many diversity of
human conversations. Experimental results show that BootTOD outperforms strong
TOD baselines on diverse downstream dialogue tasks.
\\ ( https://arxiv.org/abs/2403.01163 ,  1214kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01165
Date: Sat, 2 Mar 2024 10:38:10 GMT   (7103kb,D)

Title: STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient
  Fine-Tuning of Large Language Models
Authors: Linhai Zhang, Jialong Wu, Deyu Zhou, Guoqiang Xu
Categories: cs.CL cs.AI
Comments: Our code and results will be available at
  https://github.com/callanwu/STAR
\\
  Though Large Language Models (LLMs) have demonstrated the powerful
capabilities of few-shot learning through prompting methods, supervised
training is still necessary for complex reasoning tasks. Because of their
extensive parameters and memory consumption, both Parameter-Efficient
Fine-Tuning (PEFT) methods and Memory-Efficient Fine-Tuning methods have been
proposed for LLMs. Nevertheless, the issue of large annotated data consumption,
the aim of Data-Efficient Fine-Tuning, remains unexplored. One obvious way is
to combine the PEFT method with active learning. However, the experimental
results show that such a combination is not trivial and yields inferior
results. Through probe experiments, such observation might be explained by two
main reasons: uncertainty gap and poor model calibration. Therefore, in this
paper, we propose a novel approach to effectively integrate uncertainty-based
active learning and LoRA. Specifically, for the uncertainty gap, we introduce a
dynamic uncertainty measurement that combines the uncertainty of the base model
and the uncertainty of the full model during the iteration of active learning.
For poor model calibration, we incorporate the regularization method during
LoRA training to keep the model from being over-confident, and the Monte-Carlo
dropout mechanism is employed to enhance the uncertainty estimation.
Experimental results show that the proposed approach outperforms existing
baseline models on three complex reasoning tasks.
\\ ( https://arxiv.org/abs/2403.01165 ,  7103kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01166
Date: Sat, 2 Mar 2024 10:38:31 GMT   (7514kb,D)

Title: DINER: Debiasing Aspect-based Sentiment Analysis with Multi-variable
  Causal Inference
Authors: Jialong Wu, Linhai Zhang, Deyu Zhou, Guoqiang Xu
Categories: cs.CL cs.AI
Comments: Our code and results will be available at
  https://github.com/callanwu/DINER
\\
  Though notable progress has been made, neural-based aspect-based sentiment
analysis (ABSA) models are prone to learn spurious correlations from annotation
biases, resulting in poor robustness on adversarial data transformations. Among
the debiasing solutions, causal inference-based methods have attracted much
research attention, which can be mainly categorized into causal intervention
methods and counterfactual reasoning methods. However, most of the present
debiasing methods focus on single-variable causal inference, which is not
suitable for ABSA with two input variables (the target aspect and the review).
In this paper, we propose a novel framework based on multi-variable causal
inference for debiasing ABSA. In this framework, different types of biases are
tackled based on different causal intervention methods. For the review branch,
the bias is modeled as indirect confounding from context, where backdoor
adjustment intervention is employed for debiasing. For the aspect branch, the
bias is described as a direct correlation with labels, where counterfactual
reasoning is adopted for debiasing. Extensive experiments demonstrate the
effectiveness of the proposed method compared to various baselines on the two
widely used real-world aspect robustness test set datasets.
\\ ( https://arxiv.org/abs/2403.01166 ,  7514kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01185
Date: Sat, 2 Mar 2024 11:54:55 GMT   (841kb,D)

Title: Balancing Exploration and Exploitation in LLM using Soft RLLF for
  Enhanced Negation Understanding
Authors: Ha-Thanh Nguyen, Ken Satoh
Categories: cs.CL cs.AI
Comments: JURISIN 2024
\\
  Finetuning approaches in NLP often focus on exploitation rather than
exploration, which may lead to suboptimal models. Given the vast search space
of natural language, this limited exploration can restrict their performance in
complex, high-stakes domains, where accurate negation understanding and logical
reasoning abilities are crucial. To address this issue, we leverage
Reinforcement Learning from Logical Feedback (RLLF) to create an effective
balance between exploration and exploitation in LLMs. Our approach employs an
appropriate benchmark dataset for training and evaluation, highlighting the
importance of exploration in enhancing negation understanding capabilities. We
compare the performance of our RLLF-enhanced LLMs with baseline models trained
without RLLF, demonstrating the value of this balanced approach. Furthermore,
we showcase the potential of our method in legal AI applications by employing
transfer learning and evaluating its impact on negation understanding. Our
experimental results exhibit the effectiveness of balancing exploration and
exploitation with RLLF in improving LLMs' negation capabilities. This has
implications for the development of more accurate, reliable, and logically
consistent language models in high-stakes domains.
\\ ( https://arxiv.org/abs/2403.01185 ,  841kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01187
Date: Sat, 2 Mar 2024 11:58:24 GMT   (18kb)

Title: A Compositional Typed Semantics for Universal Dependencies
Authors: Laurestine Bradford, Timothy John O'Donnell, Siva Reddy
Categories: cs.CL
Comments: 10 pages, 6 figures, 1 table. For related code, see
  https://github.com/McGill-NLP/ud-to-meaning
\\
  Languages may encode similar meanings using different sentence structures.
This makes it a challenge to provide a single set of formal rules that can
derive meanings from sentences in many languages at once. To overcome the
challenge, we can take advantage of language-general connections between
meaning and syntax, and build on cross-linguistically parallel syntactic
structures. We introduce UD Type Calculus, a compositional, principled, and
language-independent system of semantic types and logical forms for lexical
items which builds on a widely-used language-general dependency syntax
framework. We explain the essential features of UD Type Calculus, which all
involve giving dependency relations denotations just like those of words. These
allow UD-TC to derive correct meanings for sentences with a wide range of
syntactic structures by making use of dependency labels. Finally, we present
evaluation results on a large existing corpus of sentences and their logical
forms, showing that UD-TC can produce meanings comparable with our baseline.
\\ ( https://arxiv.org/abs/2403.01187 ,  18kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01193
Date: Sat, 2 Mar 2024 12:19:04 GMT   (520kb,D)

Title: RAGged Edges: The Double-Edged Sword of Retrieval-Augmented Chatbots
Authors: Philip Feldman. James R. Foulds, Shimei Pan
Categories: cs.CL cs.AI
Comments: 7 Pages, 1 Figure, 1 Table
ACM-class: H.3.3; I.2.7
\\
  Large language models (LLMs) like ChatGPT demonstrate the remarkable progress
of artificial intelligence. However, their tendency to hallucinate -- generate
plausible but false information -- poses a significant challenge. This issue is
critical, as seen in recent court cases where ChatGPT's use led to citations of
non-existent legal rulings. This paper explores how Retrieval-Augmented
Generation (RAG) can counter hallucinations by integrating external knowledge
with prompts. We empirically evaluate RAG against standard LLMs using prompts
designed to induce hallucinations. Our results show that RAG increases accuracy
in some cases, but can still be misled when prompts directly contradict the
model's pre-trained understanding. These findings highlight the complex nature
of hallucinations and the need for more robust solutions to ensure LLM
reliability in real-world applications. We offer practical recommendations for
RAG deployment and discuss implications for the development of more trustworthy
LLMs.
\\ ( https://arxiv.org/abs/2403.01193 ,  520kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01196
Date: Sat, 2 Mar 2024 12:29:28 GMT   (2790kb,D)

Title: Machine Translation in the Covid domain: an English-Irish case study for
  LoResMT 2021
Authors: S\'eamus Lankford, Haithem Afli and Andy Way
Categories: cs.CL cs.AI
Journal-ref: Proceedings of the 4th Workshop on Technologies for MT of Low
  Resource Languages (LoResMT2021)
\\
  Translation models for the specific domain of translating Covid data from
English to Irish were developed for the LoResMT 2021 shared task. Domain
adaptation techniques, using a Covid-adapted generic 55k corpus from the
Directorate General of Translation, were applied. Fine-tuning, mixed
fine-tuning and combined dataset approaches were compared with models trained
on an extended in-domain dataset. As part of this study, an English-Irish
dataset of Covid related data, from the Health and Education domains, was
developed. The highest-performing model used a Transformer architecture trained
with an extended in-domain Covid dataset. In the context of this study, we have
demonstrated that extending an 8k in-domain baseline dataset by just 5k lines
improved the BLEU score by 27 points.
\\ ( https://arxiv.org/abs/2403.01196 ,  2790kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01197
Date: Sat, 2 Mar 2024 12:31:22 GMT   (7463kb,D)

Title: DMoERM: Recipes of Mixture-of-Experts for Effective Reward Modeling
Authors: Shanghaoran Quan
Categories: cs.CL
Comments: 20 pages, 7 figures
\\
  The performance of the reward model (RM) is a critical factor in improving
the effectiveness of the large language model (LLM) during alignment
fine-tuning. There remain two challenges in RM training: 1) training the same
RM using various categories of data may cause its generalization performance to
suffer from multi-task disturbance, and 2) the human annotation consistency
rate is generally only $60\%$ to $75\%$, causing training data to contain a lot
of noise. To tackle these two challenges, we introduced the idea of
Mixture-of-Experts (MoE) into the field of RM for the first time. We propose
the Double-Layer MoE RM (DMoERM). The outer layer MoE is a sparse model. After
classifying an input into task categories, we route it to the corresponding
inner layer task-specific model. The inner layer MoE is a dense model. We
decompose the specific task into multiple capability dimensions and
individually fine-tune a LoRA expert on each one. Their outputs are then
synthesized by an MLP to compute the final rewards. To minimize costs, we call
a public LLM API to obtain the capability preference labels. The validation on
manually labeled datasets confirms that our model attains superior consistency
with human preference and outstrips advanced generative approaches. Meanwhile,
through BoN sampling and RL experiments, we demonstrate that our model
outperforms state-of-the-art ensemble methods of RM and mitigates the
overoptimization problem. Our code and dataset are available at:
https://github.com/quanshr/DMoERM-v1.
\\ ( https://arxiv.org/abs/2403.01197 ,  7463kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01216
Date: Sat, 2 Mar 2024 14:14:45 GMT   (283kb,D)

Title: API Is Enough: Conformal Prediction for Large Language Models Without
  Logit-Access
Authors: Jiayuan Su, Jing Luo, Hongwei Wang, Lu Cheng
Categories: cs.CL cs.AI cs.LG
\\
  This study aims to address the pervasive challenge of quantifying uncertainty
in large language models (LLMs) without logit-access. Conformal Prediction
(CP), known for its model-agnostic and distribution-free features, is a desired
approach for various LLMs and data distributions. However, existing CP methods
for LLMs typically assume access to the logits, which are unavailable for some
API-only LLMs. In addition, logits are known to be miscalibrated, potentially
leading to degraded CP performance. To tackle these challenges, we introduce a
novel CP method that (1) is tailored for API-only LLMs without logit-access;
(2) minimizes the size of prediction sets; and (3) ensures a statistical
guarantee of the user-defined coverage. The core idea of this approach is to
formulate nonconformity measures using both coarse-grained (i.e., sample
frequency) and fine-grained uncertainty notions (e.g., semantic similarity).
Experimental results on both close-ended and open-ended Question Answering
tasks show our approach can mostly outperform the logit-based CP baselines.
\\ ( https://arxiv.org/abs/2403.01216 ,  283kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01222
Date: Sat, 2 Mar 2024 14:38:03 GMT   (8258kb,D)

Title: Emotion Analysis in NLP: Trends, Gaps and Roadmap for Future Directions
Authors: Flor Miriam Plaza-del-Arco, Alba Curry, Amanda Cercas Curry, Dirk Hovy
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  Emotions are a central aspect of communication. Consequently, emotion
analysis (EA) is a rapidly growing field in natural language processing (NLP).
However, there is no consensus on scope, direction, or methods. In this paper,
we conduct a thorough review of 154 relevant NLP publications from the last
decade. Based on this review, we address four different questions: (1) How are
EA tasks defined in NLP? (2) What are the most prominent emotion frameworks and
which emotions are modeled? (3) Is the subjectivity of emotions considered in
terms of demographics and cultural factors? and (4) What are the primary NLP
applications for EA? We take stock of trends in EA and tasks, emotion
frameworks used, existing datasets, methods, and applications. We then discuss
four lacunae: (1) the absence of demographic and cultural aspects does not
account for the variation in how emotions are perceived, but instead assumes
they are universally experienced in the same manner; (2) the poor fit of
emotion categories from the two main emotion theories to the task; (3) the lack
of standardized EA terminology hinders gap identification, comparison, and
future goals; and (4) the absence of interdisciplinary research isolates EA
from insights in other fields. Our work will enable more focused research into
EA and a more holistic approach to modeling emotions in NLP.
\\ ( https://arxiv.org/abs/2403.01222 ,  8258kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01241
Date: Sat, 2 Mar 2024 16:05:26 GMT   (33839kb,D)

Title: IntactKV: Improving Large Language Model Quantization by Keeping Pivot
  Tokens Intact
Authors: Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu,
  Lu Hou, Jun Yao, Chun Yuan
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) excel in natural language processing but demand
intensive computation. To mitigate this, various quantization methods have been
explored, yet they compromise LLM performance. This paper unveils a previously
overlooked type of outlier in LLMs. Such outliers are found to allocate most of
the attention scores on initial tokens of input, termed as pivot tokens, which
is crucial to the performance of quantized LLMs. Given that, we propose
IntactKV to generate the KV cache of pivot tokens losslessly from the
full-precision model. The approach is simple and easy to combine with existing
quantization solutions. Besides, IntactKV can be calibrated as additional LLM
parameters to boost the quantized LLMs further. Mathematical analysis also
proves that IntactKV effectively reduces the upper bound of quantization error.
Empirical results show that IntactKV brings consistent improvement and achieves
lossless weight-only INT4 quantization on various downstream tasks, leading to
the new state-of-the-art for LLM quantization.
\\ ( https://arxiv.org/abs/2403.01241 ,  33839kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01244
Date: Sat, 2 Mar 2024 16:11:23 GMT   (397kb,D)

Title: Mitigating Catastrophic Forgetting in Large Language Models with
  Self-Synthesized Rehearsal
Authors: Jianheng Huang, Leyang Cui, Ante Wang, Chengyi Yang, Xinting Liao,
  Linfeng Song, Junfeng Yao, Jinsong Su
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) suffer from catastrophic forgetting during
continual learning. Conventional rehearsal-based methods rely on previous
training data to retain the model's ability, which may not be feasible in
real-world applications. When conducting continual learning based on a
publicly-released LLM checkpoint, the availability of the original training
data may be non-existent. To address this challenge, we propose a framework
called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic
instances for rehearsal. Concretely, we first employ the base LLM for
in-context learning to generate synthetic instances. Subsequently, we utilize
the latest LLM to refine the instance outputs based on the synthetic inputs,
preserving its acquired ability. Finally, we select diverse high-quality
synthetic instances for rehearsal in future stages. Experimental results
demonstrate that SSR achieves superior or comparable performance compared to
conventional rehearsal-based approaches while being more data-efficient.
Besides, SSR effectively preserves the generalization capabilities of LLMs in
general domains.
\\ ( https://arxiv.org/abs/2403.01244 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01251
Date: Sat, 2 Mar 2024 16:23:44 GMT   (3827kb,D)

Title: Accelerating Greedy Coordinate Gradient via Probe Sampling
Authors: Yiran Zhao, Wenyue Zheng, Tianle Cai, Xuan Long Do, Kenji Kawaguchi,
  Anirudh Goyal, Michael Shieh
Categories: cs.CL
\\
  Safety of Large Language Models (LLMs) has become a central issue given their
rapid progress and wide applications. Greedy Coordinate Gradient (GCG) is shown
to be effective in constructing prompts containing adversarial suffixes to
break the presumingly safe LLMs, but the optimization of GCG is time-consuming
and limits its practicality. To reduce the time cost of GCG and enable more
comprehensive studies of LLM safety, in this work, we study a new algorithm
called $\texttt{Probe sampling}$ to accelerate the GCG algorithm. At the core
of the algorithm is a mechanism that dynamically determines how similar a
smaller draft model's predictions are to the target model's predictions for
prompt candidates. When the target model is similar to the draft model, we rely
heavily on the draft model to filter out a large number of potential prompt
candidates to reduce the computation time. Probe sampling achieves up to $5.6$
times speedup using Llama2-7b and leads to equal or improved attack success
rate (ASR) on the AdvBench.
\\ ( https://arxiv.org/abs/2403.01251 ,  3827kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01270
Date: Sat, 2 Mar 2024 17:13:47 GMT   (4942kb)

Title: A comprehensive cross-language framework for harmful content detection
  with the aid of sentiment analysis
Authors: Mohammad Dehghani
Categories: cs.CL
\\
  In today's digital world, social media plays a significant role in
facilitating communication and content sharing. However, the exponential rise
in user-generated content has led to challenges in maintaining a respectful
online environment. In some cases, users have taken advantage of anonymity in
order to use harmful language, which can negatively affect the user experience
and pose serious social problems. Recognizing the limitations of manual
moderation, automatic detection systems have been developed to tackle this
problem. Nevertheless, several obstacles persist, including the absence of a
universal definition for harmful language, inadequate datasets across
languages, the need for detailed annotation guideline, and most importantly, a
comprehensive framework. This study aims to address these challenges by
introducing, for the first time, a detailed framework adaptable to any
language. This framework encompasses various aspects of harmful language
detection. A key component of the framework is the development of a general and
detailed annotation guideline. Additionally, the integration of sentiment
analysis represents a novel approach to enhancing harmful language detection.
Also, a definition of harmful language based on the review of different related
concepts is presented. To demonstrate the effectiveness of the proposed
framework, its implementation in a challenging low-resource language is
conducted. We collected a Persian dataset and applied the annotation guideline
for harmful detection and sentiment analysis. Next, we present baseline
experiments utilizing machine and deep learning methods to set benchmarks.
Results prove the framework's high performance, achieving an accuracy of 99.4%
in offensive language detection and 66.2% in sentiment analysis.
\\ ( https://arxiv.org/abs/2403.01270 ,  4942kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01289
Date: Sat, 2 Mar 2024 19:01:40 GMT   (33kb,D)

Title: Greed is All You Need: An Evaluation of Tokenizer Inference Methods
Authors: Omri Uzan, Craig W. Schmidt, Chris Tanner, Yuval Pinter
Categories: cs.CL
\\
  While subword tokenizers such as BPE and WordPiece are typically used to
build vocabularies for NLP models, the method of decoding text into a sequence
of tokens from these vocabularies is often left unspecified, or ill-suited to
the method in which they were constructed. We provide a controlled analysis of
seven tokenizer inference methods across four different algorithms and three
vocabulary sizes, performed on a novel intrinsic evaluation suite we curated
for English, combining measures rooted in morphology, cognition, and
information theory. We show that for the most commonly used tokenizers, greedy
inference performs surprisingly well; and that SaGe, a recently-introduced
contextually-informed tokenizer, outperforms all others on morphological
alignment.
\\ ( https://arxiv.org/abs/2403.01289 ,  33kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01304
Date: Sat, 2 Mar 2024 20:25:50 GMT   (39kb)

Title: Improving the Validity of Automatically Generated Feedback via
  Reinforcement Learning
Authors: Alexander Scarlatos, Digory Smith, Simon Woodhead, Andrew Lan
Categories: cs.CL
\\
  Automatically generating feedback via large language models (LLMs) in
intelligent tutoring systems and online learning platforms has the potential to
improve the learning outcomes of many students. However, both feedback
generation and evaluation are challenging: feedback content has to be valid
especially in subjects like math, which requires models to understand the
problem, the solution, and where the student's error lies. Feedback also has to
be pedagogically valid to reflect effective tutoring strategies, such as
explaining possible misconceptions and encouraging the student, among other
desirable features. In this work, we address both problems of automatically
generating and evaluating feedback while considering both correctness and
alignment. First, we propose a rubric for evaluating math feedback and show
that GPT-4 is able to effectively use it to annotate human-written and
LLM-generated feedback. Second, we propose a framework for feedback generation
that optimizes both correctness and alignment using reinforcement learning
(RL). Specifically, we use GPT-4's annotations to create preferences over
feedback pairs in an augmented dataset for training via direct preference
optimization (DPO). We show that our methods significantly increase the
correctness and alignment of generated feedback with Llama 2, an open-source
LLM, qualitatively analyze our generation and evaluation systems using case
studies, and outline several areas for future work.
\\ ( https://arxiv.org/abs/2403.01304 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01308
Date: Sat, 2 Mar 2024 20:40:11 GMT   (101kb,D)

Title: VBART: The Turkish LLM
Authors: Meliksah Turker, Mehmet Erdi Ari, Aydin Han
Categories: cs.CL cs.AI cs.LG
\\
  We present VBART, the first Turkish sequence-to-sequence Large Language
Models (LLMs) pre-trained on a large corpus from scratch. VBART are compact
LLMs based on good ideas leveraged from BART and mBART models and come in two
sizes, Large and XLarge. Fine-tuned VBART models surpass the prior
state-of-the-art results in abstractive text summarization, title generation,
text paraphrasing, question answering and question generation tasks. They allow
fine-tuning for future text generation tasks and datasets, carving a new path
for Turkish Natural Language Processing (NLP) research. Our work shows that
having a pre-trained LLM for Turkish outperforms up to 3x multilingual models,
improving existing results and providing efficient models for training and
inference. Moreover, we show that our monolingual tokenizer is 7x more
efficient than OpenAI's multilingual tokenizer. Last but not least, we
introduce a method to enlarge an existing pre-trained LLM and question the
relevancy of Chinchilla Scaling Law to sequence-to-sequence masked language
models. Our fine-tuned models, tokenizer and cleaned web corpus of 135 GB are
publicly available at huggingface.co/vngrs-ai.
\\ ( https://arxiv.org/abs/2403.01308 ,  101kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01309
Date: Sat, 2 Mar 2024 20:46:56 GMT   (166kb,D)

Title: VNLP: Turkish NLP Package
Authors: Meliksah Turker, Mehmet Erdi Ari, Aydin Han
Categories: cs.CL cs.AI cs.LG
\\
  In this work, we present VNLP: the first dedicated, complete, open-source,
well-documented, lightweight, production-ready, state-of-the-art Natural
Language Processing (NLP) package for the Turkish language. It contains a wide
variety of tools, ranging from the simplest tasks, such as sentence splitting
and text normalization, to the more advanced ones, such as text and token
classification models. Its token classification models are based on "Context
Model", a novel architecture that is both an encoder and an auto-regressive
model. NLP tasks solved by VNLP models include but are not limited to Sentiment
Analysis, Named Entity Recognition, Morphological Analysis \& Disambiguation
and Part-of-Speech Tagging. Moreover, it comes with pre-trained word embeddings
and corresponding SentencePiece Unigram tokenizers. VNLP has an open-source
GitHub repository, ReadtheDocs documentation, PyPi package for convenient
installation, Python and command-line API and a demo page to test all the
functionality. Consequently, our main contribution is a complete, compact,
easy-to-install and easy-to-use NLP package for Turkish.
\\ ( https://arxiv.org/abs/2403.01309 ,  166kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01342
Date: Sat, 2 Mar 2024 23:32:33 GMT   (1339kb)

Title: LM4OPT: Unveiling the Potential of Large Language Models in Formulating
  Mathematical Optimization Problems
Authors: Tasnim Ahmed, Salimur Choudhury
Categories: cs.CL cs.IR
\\
  In the rapidly evolving field of natural language processing, the translation
of linguistic descriptions into mathematical formulation of optimization
problems presents a formidable challenge, demanding intricate understanding and
processing capabilities from Large Language Models (LLMs). This study compares
prominent LLMs, including GPT-3.5, GPT-4, and Llama-2-7b, in zero-shot and
one-shot settings for this task. Our findings show GPT-4's superior
performance, particularly in the one-shot scenario. A central part of this
research is the introduction of `LM4OPT,' a progressive fine-tuning framework
for Llama-2-7b that utilizes noisy embeddings and specialized datasets.
However, this research highlights a notable gap in the contextual understanding
capabilities of smaller models such as Llama-2-7b compared to larger
counterparts, especially in processing lengthy and complex input contexts. Our
empirical investigation, utilizing the NL4Opt dataset, unveils that GPT-4
surpasses the baseline performance established by previous research, achieving
an F1-score of 0.63, solely based on the problem description in natural
language, and without relying on any additional named entity information.
GPT-3.5 follows closely, both outperforming the fine-tuned Llama-2-7b. These
findings not only benchmark the current capabilities of LLMs in a novel
application area but also lay the groundwork for future improvements in
mathematical formulation of optimization problems from natural language input.
\\ ( https://arxiv.org/abs/2403.01342 ,  1339kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01364
Date: Sun, 3 Mar 2024 01:47:52 GMT   (372kb,D)

Title: Improving Cross-lingual Representation for Semantic Retrieval with
  Code-switching
Authors: Mieradilijiang Maimaiti, Yuanhang Zheng, Ji Zhang, Fei Huang, Yue
  Zhang, Wenpei Luo, Kaiyu Huang
Categories: cs.CL
\\
  Semantic Retrieval (SR) has become an indispensable part of the FAQ system in
the task-oriented question-answering (QA) dialogue scenario. The demands for a
cross-lingual smart-customer-service system for an e-commerce platform or some
particular business conditions have been increasing recently. Most previous
studies exploit cross-lingual pre-trained models (PTMs) for multi-lingual
knowledge retrieval directly, while some others also leverage the continual
pre-training before fine-tuning PTMs on the downstream tasks. However, no
matter which schema is used, the previous work ignores to inform PTMs of some
features of the downstream task, i.e. train their PTMs without providing any
signals related to SR. To this end, in this work, we propose an Alternative
Cross-lingual PTM for SR via code-switching. We are the first to utilize the
code-switching approach for cross-lingual SR. Besides, we introduce the novel
code-switched continual pre-training instead of directly using the PTMs on the
SR tasks. The experimental results show that our proposed approach consistently
outperforms the previous SOTA methods on SR and semantic textual similarity
(STS) tasks with three business corpora and four open datasets in 20+
languages.
\\ ( https://arxiv.org/abs/2403.01364 ,  372kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01373
Date: Sun, 3 Mar 2024 02:31:11 GMT   (522kb,D)

Title: Evaluating and Mitigating Number Hallucinations in Large Vision-Language
  Models: A Consistency Perspective
Authors: Huixuan Zhang, Junzhe Zhang, Xiaojun Wan
Categories: cs.CL
Comments: 13 pages
\\
  Large vision language models have demonstrated remarkable efficacy in
addressing challenges related to both textual and visual content. Nevertheless,
these models are susceptible to various hallucinations. In this paper, we focus
on a new form of hallucination, specifically termed as number hallucination,
which denotes instances where models fail to accurately identify the quantity
of objects in an image. We establish a dataset and employ evaluation metrics to
assess number hallucination, revealing a pronounced prevalence of this issue
across mainstream large vision language models (LVLMs). Additionally, we delve
into a thorough analysis of number hallucination, examining inner and outer
inconsistency problem from two related perspectives. We assert that this
inconsistency is one cause of number hallucination and propose a consistency
training method as a means to alleviate such hallucination, which achieves an
average improvement of 8\% compared with direct finetuning method.
\\ ( https://arxiv.org/abs/2403.01373 ,  522kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01382
Date: Sun, 3 Mar 2024 03:06:31 GMT   (468kb,D)

Title: Automatic Question-Answer Generation for Long-Tail Knowledge
Authors: Rohan Kumar, Youngmin Kim, Sunitha Ravi, Haitian Sun, Christos
  Faloutsos, Ruslan Salakhutdinov, Minji Yoon
Categories: cs.CL
Comments: Accepted at KDD 2023 KnowledgeNLP
\\
  Pretrained Large Language Models (LLMs) have gained significant attention for
addressing open-domain Question Answering (QA). While they exhibit high
accuracy in answering questions related to common knowledge, LLMs encounter
difficulties in learning about uncommon long-tail knowledge (tail entities).
Since manually constructing QA datasets demands substantial human resources,
the types of existing QA datasets are limited, leaving us with a scarcity of
datasets to study the performance of LLMs on tail entities. In this paper, we
propose an automatic approach to generate specialized QA datasets for tail
entities and present the associated research challenges. We conduct extensive
experiments by employing pretrained LLMs on our newly generated long-tail QA
datasets, comparing their performance with and without external resources
including Wikipedia and Wikidata knowledge graphs.
\\ ( https://arxiv.org/abs/2403.01382 ,  468kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01390
Date: Sun, 3 Mar 2024 04:22:13 GMT   (968kb,D)

Title: Right for Right Reasons: Large Language Models for Verifiable
  Commonsense Knowledge Graph Question Answering
Authors: Armin Toroghi, Willis Guo, Mohammad Mahdi Abdollah Pour, Scott Sanner
Categories: cs.CL
Comments: 8 pages
ACM-class: I.2.7
\\
  Knowledge Graph Question Answering (KGQA) methods seek to answer Natural
Language questions using the relational information stored in Knowledge Graphs
(KGs). With the recent advancements of Large Language Models (LLMs) and their
remarkable reasoning abilities, there is a growing trend to leverage them for
KGQA. However, existing methodologies have only focused on answering factual
questions, e.g., "In which city was Silvio Berlusconi's first wife born?",
leaving questions involving commonsense reasoning that real-world users may
pose more often, e.g., "Do I need separate visas to see the Venus of Willendorf
and attend the Olympics this summer?" unaddressed. In this work, we first
observe that existing LLM-based methods for KGQA struggle with hallucination on
such questions, especially on queries targeting long-tail entities (e.g.,
non-mainstream and recent entities), thus hindering their applicability in
real-world applications especially since their reasoning processes are not
easily verifiable. In response, we propose Right for Right Reasons (R3), a
commonsense KGQA methodology that allows for a verifiable reasoning procedure
by axiomatically surfacing intrinsic commonsense knowledge of LLMs and
grounding every factual reasoning step on KG triples. Through experimental
evaluations across three different tasks--question answering, claim
verification, and preference matching--our findings showcase R3 as a superior
approach, outperforming existing methodologies and notably reducing instances
of hallucination and reasoning errors.
\\ ( https://arxiv.org/abs/2403.01390 ,  968kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01395
Date: Sun, 3 Mar 2024 04:47:01 GMT   (874kb,D)

Title: CR-LT-KGQA: A Knowledge Graph Question Answering Dataset Requiring
  Commonsense Reasoning and Long-Tail Knowledge
Authors: Willis Guo, Armin Toroghi, Scott Sanner
Categories: cs.CL
Comments: 7 pages
ACM-class: I.2.4, I.2.7
\\
  Knowledge graph question answering (KGQA) is a well-established field that
seeks to provide factual answers to natural language (NL) questions by
leveraging knowledge graphs (KGs). However, existing KGQA datasets suffer from
two significant limitations: (1) no existing KGQA dataset requires commonsense
reasoning to arrive at an answer and (2) existing KGQA datasets focus on
popular entities for which large language models (LLMs) can directly answer
without hallucinating and without leveraging the KG. In this work, we seek a
novel KGQA dataset that supports commonsense reasoning and focuses on long-tail
entities (e.g., non-mainstream and recent entities) where LLMs frequently
hallucinate, and thus create the need for novel methodologies that leverage the
KG for factual and attributable commonsense inference. We create a novel
Commonsense Reasoning (CR) and Long-Tail (LT) KGQA dataset with two subtasks --
question answering and claim verification -- that address both limitations (1)
and (2). We construct CR-LT-KGQA by building extensions to existing reasoning
datasets StrategyQA and CREAK over Wikidata. While existing KGQA methods are
not applicable due to their lack of commonsense inference support, baseline
evaluation of LLMs on CR-LT KGQA demonstrate a high rate of hallucination.
Thus, CR-LT KGQA poses significant challenges for hallucination-prone LLMs,
hence paving the way for future commonsense KGQA research to provide accurate
and factual answers for long-tail entities in the era of LLMs.
\\ ( https://arxiv.org/abs/2403.01395 ,  874kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01404
Date: Sun, 3 Mar 2024 05:45:27 GMT   (13535kb,D)

Title: What Is Missing in Multilingual Visual Reasoning and How to Fix It
Authors: Yueqi Song, Simran Khanuja, Graham Neubig
Categories: cs.CL
\\
  NLP models today strive for supporting multiple languages and modalities,
improving accessibility for diverse users. In this paper, we evaluate their
multilingual, multimodal capabilities by testing on a visual reasoning task. We
observe that proprietary systems like GPT-4V obtain the best performance on
this task now, but open models lag in comparison. Surprisingly, GPT-4V exhibits
similar performance between English and other languages, indicating the
potential for equitable system development across languages. Our analysis on
model failures reveals three key aspects that make this task challenging:
multilinguality, complex reasoning, and multimodality. To address these
challenges, we propose three targeted interventions including a translate-test
approach to tackle multilinguality, a visual programming approach to break down
complex reasoning, and a novel method that leverages image captioning to
address multimodality. Our interventions achieve the best open performance on
this task in a zero-shot setting, boosting open model LLaVA by 13.4%, while
also minorly improving GPT-4V's performance.
\\ ( https://arxiv.org/abs/2403.01404 ,  13535kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01411
Date: Sun, 3 Mar 2024 06:47:51 GMT   (2517kb,D)

Title: OVEL: Large Language Model as Memory Manager for Online Video Entity
  Linking
Authors: Haiquan Zhao and Xuwu Wang and Shisong Chen and Zhixu Li and Xin Zheng
  and Yanghua Xiao
Categories: cs.CL
Comments: 13 pages, 6 figures
\\
  In recent years, multi-modal entity linking (MEL) has garnered increasing
attention in the research community due to its significance in numerous
multi-modal applications. Video, as a popular means of information
transmission, has become prevalent in people's daily lives. However, most
existing MEL methods primarily focus on linking textual and visual mentions or
offline videos's mentions to entities in multi-modal knowledge bases, with
limited efforts devoted to linking mentions within online video content. In
this paper, we propose a task called Online Video Entity Linking OVEL, aiming
to establish connections between mentions in online videos and a knowledge base
with high accuracy and timeliness. To facilitate the research works of OVEL, we
specifically concentrate on live delivery scenarios and construct a live
delivery entity linking dataset called LIVE. Besides, we propose an evaluation
metric that considers timelessness, robustness, and accuracy. Furthermore, to
effectively handle OVEL task, we leverage a memory block managed by a Large
Language Model and retrieve entity candidates from the knowledge base to
augment LLM performance on memory management. The experimental results prove
the effectiveness and efficiency of our method.
\\ ( https://arxiv.org/abs/2403.01411 ,  2517kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01432
Date: Sun, 3 Mar 2024 08:07:55 GMT   (9659kb,D)

Title: Fine Tuning vs. Retrieval Augmented Generation for Less Popular
  Knowledge
Authors: Heydar Soudani, Evangelos Kanoulas, Faegheh Hasibi
Categories: cs.CL
\\
  Large language models (LLMs) memorize a vast amount of factual knowledge,
exhibiting strong performance across diverse tasks and domains. However, it has
been observed that the performance diminishes when dealing with less-popular or
low-frequency concepts and entities, for example in domain specific
applications. The two prominent approaches to enhance the performance of LLMs
on low-frequent topics are: Retrieval Augmented Generation (RAG) and
fine-tuning (FT) over synthetic data. This paper explores and evaluates the
impact of RAG and FT on customizing LLMs in handling low-frequency entities on
question answering task. Our findings indicate that FT significantly boosts the
performance across entities of varying popularity, especially in the most and
least popular groups, while RAG surpasses other methods. Additionally, the
success of both RAG and FT approaches is amplified by advancements in retrieval
and data augmentation techniques. We release our data and code at
https://github.com/HeydarSoudani/RAGvsFT.
\\ ( https://arxiv.org/abs/2403.01432 ,  9659kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01456
Date: Sun, 3 Mar 2024 09:18:05 GMT   (6489kb)

Title: Controlling Cloze-test Question Item Difficulty with PLM-based Surrogate
  Models for IRT Assessment
Authors: Jingshen Zhang and Jiajun Xie and Xinying Qiu
Categories: cs.CL cs.AI cs.CY
\\
  Item difficulty plays a crucial role in adaptive testing. However, few works
have focused on generating questions of varying difficulty levels, especially
for multiple-choice (MC) cloze tests. We propose training pre-trained language
models (PLMs) as surrogate models to enable item response theory (IRT)
assessment, avoiding the need for human test subjects. We also propose two
strategies to control the difficulty levels of both the gaps and the
distractors using ranking rules to reduce invalid distractors. Experimentation
on a benchmark dataset demonstrates that our proposed framework and methods can
effectively control and evaluate the difficulty levels of MC cloze tests.
\\ ( https://arxiv.org/abs/2403.01456 ,  6489kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01461
Date: Sun, 3 Mar 2024 09:55:35 GMT   (7247kb,D)

Title: Answerability in Retrieval-Augmented Open-Domain Question Answering
Authors: Rustam Abdumalikov, Pasquale Minervini and Yova Kementchedjhieva
Categories: cs.CL
Comments: 5 pages, 3 tables
\\
  The performance of Open-Domain Question Answering (ODQA) retrieval systems
can exhibit sub-optimal behavior, providing text excerpts with varying degrees
of irrelevance. Unfortunately, many existing ODQA datasets lack examples
specifically targeting the identification of irrelevant text excerpts. Previous
attempts to address this gap have relied on a simplistic approach of pairing
questions with random text excerpts. This paper aims to investigate the
effectiveness of models trained using this randomized strategy, uncovering an
important limitation in their ability to generalize to irrelevant text excerpts
with high semantic overlap. As a result, we observed a substantial decrease in
predictive accuracy, from 98% to 1%. To address this limitation, we discovered
an efficient approach for training models to recognize such excerpts. By
leveraging unanswerable pairs from the SQuAD 2.0 dataset, our models achieve a
nearly perfect (~100%) accuracy when confronted with these challenging text
excerpts.
\\ ( https://arxiv.org/abs/2403.01461 ,  7247kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01469
Date: Sun, 3 Mar 2024 10:31:49 GMT   (934kb,D)

Title: KorMedMCQA: Multi-Choice Question Answering Benchmark for Korean
  Healthcare Professional Licensing Examinations
Authors: Sunjun Kweon, Byungjin Choi, Minkyu Kim, Rae Woong Park, Edward Choi
Categories: cs.CL
\\
  We introduce KorMedMCQA, the first Korean multiple-choice question answering
(MCQA) benchmark derived from Korean healthcare professional licensing
examinations, covering from the year 2012 to year 2023. This dataset consists
of a selection of questions from the license examinations for doctors, nurses,
and pharmacists, featuring a diverse array of subjects. We conduct baseline
experiments on various large language models, including
proprietary/open-source, multilingual/Korean-additional pretrained, and
clinical context pretrained models, highlighting the potential for further
enhancements. We make our data publicly available on HuggingFace and provide a
evaluation script via LM-Harness, inviting further exploration and advancement
in Korean healthcare environments.
\\ ( https://arxiv.org/abs/2403.01469 ,  934kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01479
Date: Sun, 3 Mar 2024 11:13:44 GMT   (364kb,D)

Title: Align-to-Distill: Trainable Attention Alignment for Knowledge
  Distillation in Neural Machine Translation
Authors: Heegon Jin, Seonil Son, Jemin Park, Youngseok Kim, Hyungjong Noh,
  Yeonsoo Lee
Categories: cs.CL cs.AI
Comments: Accepted to LREC-COLING 2024
MSC-class: 68T50
ACM-class: I.2.7
\\
  The advent of scalable deep models and large datasets has improved the
performance of Neural Machine Translation. Knowledge Distillation (KD) enhances
efficiency by transferring knowledge from a teacher model to a more compact
student model. However, KD approaches to Transformer architecture often rely on
heuristics, particularly when deciding which teacher layers to distill from. In
this paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to
address the feature mapping problem by adaptively aligning student attention
heads with their teacher counterparts during training. The Attention Alignment
Module in A2D performs a dense head-by-head comparison between student and
teacher attention heads across layers, turning the combinatorial mapping
heuristics into a learning problem. Our experiments show the efficacy of A2D,
demonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De->Dsb
and WMT-2014 En->De, respectively, compared to Transformer baselines.
\\ ( https://arxiv.org/abs/2403.01479 ,  364kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01481
Date: Sun, 3 Mar 2024 11:19:26 GMT   (325kb,D)

Title: Infusing Knowledge into Large Language Models with Contextual Prompts
Authors: Kinshuk Vasisht, Balaji Ganesan, Vikas Kumar, Vasudha Bhatnagar
Categories: cs.CL
Comments: 5 pages, 1 figure, In Proceedings of ICON 2023
\\
  Knowledge infusion is a promising method for enhancing Large Language Models
for domain-specific NLP tasks rather than pre-training models over large data
from scratch. These augmented LLMs typically depend on additional pre-training
or knowledge prompts from an existing knowledge graph, which is impractical in
many applications. In contrast, knowledge infusion directly from relevant
documents is more generalisable and alleviates the need for structured
knowledge graphs while also being useful for entities that are usually not
found in any knowledge graph. With this motivation, we propose a simple yet
generalisable approach for knowledge infusion by generating prompts from the
context in the input text. Our experiments show the effectiveness of our
approach which we evaluate by probing the fine-tuned LLMs.
\\ ( https://arxiv.org/abs/2403.01481 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01509
Date: Sun, 3 Mar 2024 13:14:47 GMT   (200kb,D)

Title: Fantastic Semantics and Where to Find Them: Investigating Which Layers
  of Generative LLMs Reflect Lexical Semantics
Authors: Zhu Liu, Cunliang Kong, Ying Liu and Maosong Sun
Categories: cs.CL
Comments: This work was completed on February 15th, 2024, and submitted to ACL
  2024
\\
  Large language models have achieved remarkable success in general language
understanding tasks. However, as a family of generative methods with the
objective of next token prediction, the semantic evolution with the depth of
these models are not fully explored, unlike their predecessors, such as
BERT-like architectures. In this paper, we specifically investigate the
bottom-up evolution of lexical semantics for a popular LLM, namely Llama2, by
probing its hidden states at the end of each layer using a contextualized word
identification task. Our experiments show that the representations in lower
layers encode lexical semantics, while the higher layers, with weaker semantic
induction, are responsible for prediction. This is in contrast to models with
discriminative objectives, such as mask language modeling, where the higher
layers obtain better lexical semantics. The conclusion is further supported by
the monotonic increase in performance via the hidden states for the last
meaningless symbols, such as punctuation, in the prompting strategy.
\\ ( https://arxiv.org/abs/2403.01509 ,  200kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01518
Date: Sun, 3 Mar 2024 14:03:48 GMT   (1486kb,D)

Title: Revisiting Dynamic Evaluation: Online Adaptation for Large Language
  Models
Authors: Amal Rannen-Triki, Jorg Bornschein, Razvan Pascanu, Marcus Hutter,
  Andras Gy\"orgy, Alexandre Galashov, Yee Whye Teh, Michalis K. Titsias
Categories: cs.CL cs.LG
\\
  We consider the problem of online fine tuning the parameters of a language
model at test time, also known as dynamic evaluation. While it is generally
known that this approach improves the overall predictive performance,
especially when considering distributional shift between training and
evaluation data, we here emphasize the perspective that online adaptation turns
parameters into temporally changing states and provides a form of
context-length extension with memory in weights, more in line with the concept
of memory in neuroscience. We pay particular attention to the speed of
adaptation (in terms of sample efficiency),sensitivity to the overall
distributional drift, and the computational overhead for performing gradient
computations and parameter updates. Our empirical study provides insights on
when online adaptation is particularly interesting. We highlight that with
online adaptation the conceptual distinction between in-context learning and
fine tuning blurs: both are methods to condition the model on previously
observed tokens.
\\ ( https://arxiv.org/abs/2403.01518 ,  1486kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01528
Date: Sun, 3 Mar 2024 14:59:47 GMT   (1299kb,D)

Title: Leveraging Biomolecule and Natural Language through Multi-Modal
  Learning: A Survey
Authors: Qizhi Pei, Lijun Wu, Kaiyuan Gao, Jinhua Zhu, Yue Wang, Zun Wang, Tao
  Qin, and Rui Yan
Categories: cs.CL cs.AI q-bio.BM
Comments: Survey Paper. 27 pages, 9 figures, and 3 tables
\\
  The integration of biomolecular modeling with natural language (BL) has
emerged as a promising interdisciplinary area at the intersection of artificial
intelligence, chemistry and biology. This approach leverages the rich,
multifaceted descriptions of biomolecules contained within textual data sources
to enhance our fundamental understanding and enable downstream computational
tasks such as biomolecule property prediction. The fusion of the nuanced
narratives expressed through natural language with the structural and
functional specifics of biomolecules described via various molecular modeling
techniques opens new avenues for comprehensively representing and analyzing
biomolecules. By incorporating the contextual language data that surrounds
biomolecules into their modeling, BL aims to capture a holistic view
encompassing both the symbolic qualities conveyed through language as well as
quantitative structural characteristics. In this review, we provide an
extensive analysis of recent advancements achieved through cross modeling of
biomolecules and natural language. (1) We begin by outlining the technical
representations of biomolecules employed, including sequences, 2D graphs, and
3D structures. (2) We then examine in depth the rationale and key objectives
underlying effective multi-modal integration of language and molecular data
sources. (3) We subsequently survey the practical applications enabled to date
in this developing research area. (4) We also compile and summarize the
available resources and datasets to facilitate future work. (5) Looking ahead,
we identify several promising research directions worthy of further exploration
and investment to continue advancing the field. The related resources and
contents are updating in
\url{https://github.com/QizhiPei/Awesome-Biomolecule-Language-Cross-Modeling}.
\\ ( https://arxiv.org/abs/2403.01528 ,  1299kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01548
Date: Sun, 3 Mar 2024 15:53:41 GMT   (1339kb,D)

Title: In-Context Sharpness as Alerts: An Inner Representation Perspective for
  Hallucination Mitigation
Authors: Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang
  Gao, Junxian He
Categories: cs.CL cs.AI cs.LG
\\
  Large language models (LLMs) frequently hallucinate and produce factual
errors, yet our understanding of why they make these errors remains limited. In
this study, we delve into the underlying mechanisms of LLM hallucinations from
the perspective of inner representations, and discover a salient pattern
associated with hallucinations: correct generations tend to have sharper
context activations in the hidden states of the in-context tokens, compared to
the incorrect ones. Leveraging this insight, we propose an entropy-based metric
to quantify the ``sharpness'' among the in-context hidden states and
incorporate it into the decoding process to formulate a constrained decoding
approach. Experiments on various knowledge-seeking and hallucination benchmarks
demonstrate our approach's consistent effectiveness, for example, achieving up
to an 8.6 point improvement on TruthfulQA. We believe this study can improve
our understanding of hallucinations and serve as a practical solution for
hallucination mitigation.
\\ ( https://arxiv.org/abs/2403.01548 ,  1339kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01570
Date: Sun, 3 Mar 2024 17:35:52 GMT   (1381kb,D)

Title: SERVAL: Synergy Learning between Vertical Models and LLMs towards
  Oracle-Level Zero-shot Medical Prediction
Authors: Jiahuan Yan, Jintai Chen, Chaowen Hu, Bo Zheng, Yaojun Hu, Jimeng Sun,
  Jian Wu
Categories: cs.CL cs.LG
\\
  Recent development of large language models (LLMs) has exhibited impressive
zero-shot proficiency on generic and common sense questions. However, LLMs'
application on domain-specific vertical questions still lags behind, primarily
due to the humiliation problems and deficiencies in vertical knowledge.
Furthermore, the vertical data annotation process often requires
labor-intensive expert involvement, thereby presenting an additional challenge
in enhancing the model's vertical capabilities. In this paper, we propose
SERVAL, a synergy learning pipeline designed for unsupervised development of
vertical capabilities in both LLMs and small models by mutual enhancement.
Specifically, SERVAL utilizes the LLM's zero-shot outputs as annotations,
leveraging its confidence to teach a robust vertical model from scratch.
Reversely, the trained vertical model guides the LLM fine-tuning to enhance its
zero-shot capability, progressively improving both models through an iterative
process. In medical domain, known for complex vertical knowledge and costly
annotations, comprehensive experiments show that, without access to any gold
labels, SERVAL with the synergy learning of OpenAI GPT-3.5 and a simple model
attains fully-supervised competitive performance across ten widely used medical
datasets. These datasets represent vertically specialized medical diagnostic
scenarios (e.g., diabetes, heart diseases, COVID-19), highlighting the
potential of SERVAL in refining the vertical capabilities of LLMs and training
vertical models from scratch, all achieved without the need for annotations.
\\ ( https://arxiv.org/abs/2403.01570 ,  1381kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01580
Date: Sun, 3 Mar 2024 18:08:30 GMT   (8038kb,D)

Title: Enhancing Neural Machine Translation of Low-Resource Languages: Corpus
  Development, Human Evaluation and Explainable AI Architectures
Authors: S\'eamus Lankford
Categories: cs.CL cs.AI
Comments: PhD thesis
\\
  In the current machine translation (MT) landscape, the Transformer
architecture stands out as the gold standard, especially for high-resource
language pairs. This research delves into its efficacy for low-resource
language pairs including both the English$\leftrightarrow$Irish and
English$\leftrightarrow$Marathi language pairs. Notably, the study identifies
the optimal hyperparameters and subword model type to significantly improve the
translation quality of Transformer models for low-resource language pairs.
  The scarcity of parallel datasets for low-resource languages can hinder MT
development. To address this, gaHealth was developed, the first bilingual
corpus of health data for the Irish language. Focusing on the health domain,
models developed using this in-domain dataset exhibited very significant
improvements in BLEU score when compared with models from the LoResMT2021
Shared Task. A subsequent human evaluation using the multidimensional quality
metrics error taxonomy showcased the superior performance of the Transformer
system in reducing both accuracy and fluency errors compared to an RNN-based
counterpart.
  Furthermore, this thesis introduces adaptNMT and adaptMLLM, two open-source
applications streamlined for the development, fine-tuning, and deployment of
neural machine translation models. These tools considerably simplify the setup
and evaluation process, making MT more accessible to both developers and
translators. Notably, adaptNMT, grounded in the OpenNMT ecosystem, promotes
eco-friendly natural language processing research by highlighting the
environmental footprint of model development. Fine-tuning of MLLMs by adaptMLLM
demonstrated advancements in translation performance for two low-resource
language pairs: English$\leftrightarrow$Irish and
English$\leftrightarrow$Marathi, compared to baselines from the LoResMT2021
Shared Task.
\\ ( https://arxiv.org/abs/2403.01580 ,  8038kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01616
Date: Sun, 3 Mar 2024 21:24:35 GMT   (147kb,D)

Title: Towards Comprehensive Vietnamese Retrieval-Augmented Generation and
  Large Language Models
Authors: Nguyen Quang Duc, Le Hai Son, Nguyen Duc Nhan, Nguyen Dich Nhat Minh,
  Le Thanh Huong, Dinh Viet Sang
Categories: cs.CL
\\
  This paper presents our contributions towards advancing the state of
Vietnamese language understanding and generation through the development and
dissemination of open datasets and pre-trained models for Vietnamese
Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs).
\\ ( https://arxiv.org/abs/2403.01616 ,  147kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01638
Date: Sun, 3 Mar 2024 23:10:36 GMT   (931kb,D)

Title: Multi-level Product Category Prediction through Text Classification
Authors: Wesley Ferreira Maia, Angelo Carmignani, Gabriel Bortoli, Lucas
  Maretti, David Luz, Daniel Camilo Fuentes Guzman, Marcos Jardel Henriques,
  Francisco Louzada Neto
Categories: cs.CL
\\
  This article investigates applying advanced machine learning models,
specifically LSTM and BERT, for text classification to predict multiple
categories in the retail sector. The study demonstrates how applying data
augmentation techniques and the focal loss function can significantly enhance
accuracy in classifying products into multiple categories using a robust
Brazilian retail dataset. The LSTM model, enriched with Brazilian word
embedding, and BERT, known for its effectiveness in understanding complex
contexts, were adapted and optimized for this specific task. The results showed
that the BERT model, with an F1 Macro Score of up to $99\%$ for segments,
$96\%$ for categories and subcategories and $93\%$ for name products,
outperformed LSTM in more detailed categories. However, LSTM also achieved high
performance, especially after applying data augmentation and focal loss
techniques. These results underscore the effectiveness of NLP techniques in
retail and highlight the importance of the careful selection of modelling and
preprocessing strategies. This work contributes significantly to the field of
NLP in retail, providing valuable insights for future research and practical
applications.
\\ ( https://arxiv.org/abs/2403.01638 ,  931kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01698
Date: Mon, 4 Mar 2024 03:21:40 GMT   (2556kb,D)

Title: Hypertext Entity Extraction in Webpage
Authors: Yifei Yang, Tianqiao Liu, Bo Shao, Hai Zhao, Linjun Shou, Ming Gong,
  Daxin Jiang
Categories: cs.CL cs.AI
\\
  Webpage entity extraction is a fundamental natural language processing task
in both research and applications. Nowadays, the majority of webpage entity
extraction models are trained on structured datasets which strive to retain
textual content and its structure information. However, existing datasets all
overlook the rich hypertext features (e.g., font color, font size) which show
their effectiveness in previous works. To this end, we first collect a
\textbf{H}ypertext \textbf{E}ntity \textbf{E}xtraction \textbf{D}ataset
(\textit{HEED}) from the e-commerce domains, scraping both the text and the
corresponding explicit hypertext features with high-quality manual entity
annotations. Furthermore, we present the \textbf{Mo}E-based \textbf{E}ntity
\textbf{E}xtraction \textbf{F}ramework (\textit{MoEEF}), which efficiently
integrates multiple features to enhance model performance by Mixture of Experts
and outperforms strong baselines, including the state-of-the-art small-scale
models and GPT-3.5-turbo. Moreover, the effectiveness of hypertext features in
\textit{HEED} and several model components in \textit{MoEEF} are analyzed.
\\ ( https://arxiv.org/abs/2403.01698 ,  2556kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01699
Date: Mon, 4 Mar 2024 03:24:18 GMT   (1259kb,D)

Title: Brilla AI: AI Contestant for the National Science and Maths Quiz
Authors: George Boateng, Jonathan Abrefah Mensah, Kevin Takyi Yeboah, William
  Edor, Andrew Kojo Mensah-Onumah, Naafi Dasana Ibrahim, and Nana Sam Yeboah
Categories: cs.CL cs.AI cs.CY cs.SD eess.AS
Comments: 13 pages. Under review at the 25th International Conference on AI in
  Education (AIED 2024)
\\
  The African continent lacks enough qualified teachers which hampers the
provision of adequate learning support. An AI could potentially augment the
efforts of the limited number of teachers, leading to better learning outcomes.
Towards that end, this work describes and evaluates the first key output for
the NSMQ AI Grand Challenge, which proposes a robust, real-world benchmark for
such an AI: "Build an AI to compete live in Ghana's National Science and Maths
Quiz (NSMQ) competition and win - performing better than the best contestants
in all rounds and stages of the competition". The NSMQ is an annual live
science and mathematics competition for senior secondary school students in
Ghana in which 3 teams of 2 students compete by answering questions across
biology, chemistry, physics, and math in 5 rounds over 5 progressive stages
until a winning team is crowned for that year. In this work, we built Brilla
AI, an AI contestant that we deployed to unofficially compete remotely and live
in the Riddles round of the 2023 NSMQ Grand Finale, the first of its kind in
the 30-year history of the competition. Brilla AI is currently available as a
web app that livestreams the Riddles round of the contest, and runs 4 machine
learning systems: (1) speech to text (2) question extraction (3) question
answering and (4) text to speech that work together in real-time to quickly and
accurately provide an answer, and then say it with a Ghanaian accent. In its
debut, our AI answered one of the 4 riddles ahead of the 3 human contesting
teams, unofficially placing second (tied). Improvements and extensions of this
AI could potentially be deployed to offer science tutoring to students and
eventually enable millions across Africa to have one-on-one learning
interactions, democratizing science education.
\\ ( https://arxiv.org/abs/2403.01699 ,  1259kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01748
Date: Mon, 4 Mar 2024 05:55:01 GMT   (330kb,D)

Title: Decode Neural signal as Speech
Authors: Yiqian Yang, Yiqun Duan, Qiang Zhang, Renjing Xu, Hui Xiong
Categories: cs.CL cs.AI
\\
  Decoding language from brain dynamics is an important open direction in the
realm of brain-computer interface (BCI), especially considering the rapid
growth of large language models. Compared to invasive-based signals which
require electrode implantation surgery, non-invasive neural signals (e.g. EEG,
MEG) have attracted increasing attention considering their safety and
generality. However, the exploration is not adequate in three aspects: 1)
previous methods mainly focus on EEG but none of the previous works address
this problem on MEG with better signal quality; 2) prior works have
predominantly used ``teacher-forcing" during generative decoding, which is
impractical; 3) prior works are mostly ``BART-based" not fully auto-regressive,
which performs better in other sequence tasks. In this paper, we explore the
brain-to-text translation of MEG signals in a speech-decoding formation. Here
we are the first to investigate a cross-attention-based ``whisper" model for
generating text directly from MEG signals without teacher forcing. Our model
achieves impressive BLEU-1 scores of 60.30 and 52.89 without pretraining \&
teacher-forcing on two major datasets (\textit{GWilliams} and
\textit{Schoffelen}). This paper conducts a comprehensive review to understand
how speech decoding formation performs on the neural decoding tasks, including
pretraining initialization, training \& evaluation set splitting, augmentation,
and scaling law.
\\ ( https://arxiv.org/abs/2403.01748 ,  330kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01749
Date: Mon, 4 Mar 2024 05:57:50 GMT   (884kb,D)

Title: Differentially Private Synthetic Data via Foundation Model APIs 2: Text
Authors: Chulin Xie, Zinan Lin, Arturs Backurs, Sivakanth Gopi, Da Yu, Huseyin
  A Inan, Harsha Nori, Haotian Jiang, Huishuai Zhang, Yin Tat Lee, Bo Li,
  Sergey Yekhanin
Categories: cs.CL
\\
  Text data has become extremely valuable due to the emergence of machine
learning algorithms that learn from it. A lot of high-quality text data
generated in the real world is private and therefore cannot be shared or used
freely due to privacy concerns. Generating synthetic replicas of private text
data with a formal privacy guarantee, i.e., differential privacy (DP), offers a
promising and scalable solution. However, existing methods necessitate DP
finetuning of large language models (LLMs) on private data to generate DP
synthetic data. This approach is not viable for proprietary LLMs (e.g.,
GPT-3.5) and also demands considerable computational resources for open-source
LLMs. Lin et al. (2024) recently introduced the Private Evolution (PE)
algorithm to generate DP synthetic images with only API access to diffusion
models. In this work, we propose an augmented PE algorithm, named Aug-PE, that
applies to the complex setting of text. We use API access to an LLM and
generate DP synthetic text without any model training. We conduct comprehensive
experiments on three benchmark datasets. Our results demonstrate that Aug-PE
produces DP synthetic text that yields competitive utility with the SOTA DP
finetuning baselines. This underscores the feasibility of relying solely on API
access of LLMs to produce high-quality DP synthetic texts, thereby facilitating
more accessible routes to privacy-preserving LLM applications. Our code and
data are available at https://github.com/AI-secure/aug-pe.
\\ ( https://arxiv.org/abs/2403.01749 ,  884kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01754
Date: Mon, 4 Mar 2024 06:20:31 GMT   (170kb,D)

Title: Derivative-Free Optimization for Low-Rank Adaptation in Large Language
  Models
Authors: Feihu Jin, Yin Liu, Ying Tan
Categories: cs.CL
Comments: 14 pages, 4 figures, 5 tables
\\
  Parameter-efficient tuning methods such as LoRA could achieve comparable
performance to model tuning by tuning a small portion of the parameters.
However, substantial computational resources are still required, as this
process involves calculating gradients and performing back-propagation
throughout the model. Much effort has recently been devoted to utilizing the
derivative-free optimization method to eschew the computation of gradients and
showcase an augmented level of robustness in few-shot settings. In this paper,
we prepend the low-rank modules into each self-attention layer of the model and
employ two derivative-free optimization methods to optimize these low-rank
modules at each layer alternately. Extensive results on various tasks and
language models demonstrate that our proposed method achieves substantial
improvement and exhibits clear advantages in memory usage and convergence speed
compared to existing gradient-based parameter-efficient tuning and
derivative-free optimization methods in few-shot settings.
\\ ( https://arxiv.org/abs/2403.01754 ,  170kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01767
Date: Mon, 4 Mar 2024 06:52:19 GMT   (14620kb,D)

Title: KeNet:Knowledge-enhanced Doc-Label Attention Network for Multi-label
  text classification
Authors: Bo Li and Yuyan Chen and Liang Zeng
Categories: cs.CL
Comments: Accepted in ICASSP 2024
\\
  Multi-Label Text Classification (MLTC) is a fundamental task in the field of
Natural Language Processing (NLP) that involves the assignment of multiple
labels to a given text. MLTC has gained significant importance and has been
widely applied in various domains such as topic recognition, recommendation
systems, sentiment analysis, and information retrieval. However, traditional
machine learning and Deep neural network have not yet addressed certain issues,
such as the fact that some documents are brief but have a large number of
labels and how to establish relationships between the labels. It is imperative
to additionally acknowledge that the significance of knowledge is substantiated
in the realm of MLTC. To address this issue, we provide a novel approach known
as Knowledge-enhanced Doc-Label Attention Network (KeNet). Specifically, we
design an Attention Network that incorporates external knowledge, label
embedding, and a comprehensive attention mechanism. In contrast to conventional
methods, we use comprehensive representation of documents, knowledge and labels
to predict all labels for each single text. Our approach has been validated by
comprehensive research conducted on three multi-label datasets. Experimental
results demonstrate that our method outperforms state-of-the-art MLTC method.
Additionally, a case study is undertaken to illustrate the practical
implementation of KeNet.
\\ ( https://arxiv.org/abs/2403.01767 ,  14620kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01774
Date: Mon, 4 Mar 2024 07:06:41 GMT   (833kb,D)

Title: WebCiteS: Attributed Query-Focused Summarization on Chinese Web Search
  Results with Citations
Authors: Haolin Deng, Chang Wang, Xin Li, Dezhang Yuan, Junlang Zhan, Tianhua
  Zhou, Jin Ma, Jun Gao, Ruifeng Xu
Categories: cs.CL
Comments: 19 pages, 7 figures
\\
  Enhancing the attribution in large language models (LLMs) is a crucial task.
One feasible approach is to enable LLMs to cite external sources that support
their generations. However, existing datasets and evaluation methods in this
domain still exhibit notable limitations. In this work, we formulate the task
of attributed query-focused summarization (AQFS) and present WebCiteS, a
Chinese dataset featuring 7k human-annotated summaries with citations. WebCiteS
derives from real-world user queries and web search results, offering a
valuable resource for model training and evaluation. Prior works in attribution
evaluation do not differentiate between groundedness errors and citation
errors. They also fall short in automatically verifying sentences that draw
partial support from multiple sources. We tackle these issues by developing
detailed metrics and enabling the automatic evaluator to decompose the
sentences into sub-claims for fine-grained verification. Our comprehensive
evaluation of both open-source and proprietary models on WebCiteS highlights
the challenge LLMs face in correctly citing sources, underscoring the necessity
for further improvement. The dataset and code will be open-sourced to
facilitate further research in this crucial field.
\\ ( https://arxiv.org/abs/2403.01774 ,  833kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01777
Date: Mon, 4 Mar 2024 07:10:31 GMT   (7053kb,D)

Title: NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language
  Models
Authors: Lizhou Fan, Wenyue Hua, Xiang Li, Kaijie Zhu, Mingyu Jin, Lingyao Li,
  Haoyang Ling, Jinkui Chi, Jindong Wang, Xin Ma, Yongfeng Zhang
Categories: cs.CL cs.CV
Comments: 16 pages, 10 figures, 2 tables
\\
  Understanding the reasoning capabilities of Multimodal Large Language Models
(MLLMs) is an important area of research. In this study, we introduce a dynamic
benchmark, NPHardEval4V, aimed at addressing the existing gaps in evaluating
the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to
disentangle the effect of various factors such as image recognition and
instruction following, from the overall performance of the models, allowing us
to focus solely on evaluating their reasoning abilities. Our findings reveal
significant discrepancies in reasoning abilities across different models and
highlight the relatively weak performance of MLLMs compared to LLMs in terms of
reasoning. We also investigate the impact of different prompting styles,
including visual, text, and combined vision and text prompts, on the reasoning
abilities of MLLMs, demonstrating the different impacts of multimodal inputs in
model performance. Unlike traditional benchmarks, which primarily focus on
static evaluations, our benchmark will update on a monthly basis to prevent
overfitting and ensure a more accurate evaluation of the models. We believe
that this benchmark can aid understand and guide the further development of
reasoning abilities in MLLMs. The benchmark dataset and code are available at
https://github.com/lizhouf/NPHardEval4V
\\ ( https://arxiv.org/abs/2403.01777 ,  7053kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01811
Date: Mon, 4 Mar 2024 07:58:26 GMT   (246kb,D)

Title: Enhancing Multi-Domain Automatic Short Answer Grading through an
  Explainable Neuro-Symbolic Pipeline
Authors: Felix K\"unnecke, Anna Filighera, Colin Leong, Tim Steuer
Categories: cs.CL
\\
  Grading short answer questions automatically with interpretable reasoning
behind the grading decision is a challenging goal for current transformer
approaches. Justification cue detection, in combination with logical reasoners,
has shown a promising direction for neuro-symbolic architectures in ASAG. But,
one of the main challenges is the requirement of annotated justification cues
in the students' responses, which only exist for a few ASAG datasets. To
overcome this challenge, we contribute (1) a weakly supervised annotation
procedure for justification cues in ASAG datasets, and (2) a neuro-symbolic
model for explainable ASAG based on justification cues. Our approach improves
upon the RMSE by 0.24 to 0.3 compared to the state-of-the-art on the Short
Answer Feedback dataset in a bilingual, multi-domain, and multi-question
training setup. This result shows that our approach provides a promising
direction for generating high-quality grades and accompanying explanations for
future research in ASAG and educational NLP.
\\ ( https://arxiv.org/abs/2403.01811 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01817
Date: Mon, 4 Mar 2024 08:05:34 GMT   (6904kb,D)

Title: NusaBERT: Teaching IndoBERT to be Multilingual and Multicultural
Authors: Wilson Wongso, David Samuel Setiawan, Steven Limcorn, Ananto
  Joyoadikusumo
Categories: cs.CL
\\
  Indonesia's linguistic landscape is remarkably diverse, encompassing over 700
languages and dialects, making it one of the world's most linguistically rich
nations. This diversity, coupled with the widespread practice of code-switching
and the presence of low-resource regional languages, presents unique challenges
for modern pre-trained language models. In response to these challenges, we
developed NusaBERT, building upon IndoBERT by incorporating vocabulary
expansion and leveraging a diverse multilingual corpus that includes regional
languages and dialects. Through rigorous evaluation across a range of
benchmarks, NusaBERT demonstrates state-of-the-art performance in tasks
involving multiple languages of Indonesia, paving the way for future natural
language understanding research for under-represented languages.
\\ ( https://arxiv.org/abs/2403.01817 ,  6904kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01841
Date: Mon, 4 Mar 2024 08:38:56 GMT   (1471kb,D)

Title: Making Pre-trained Language Models Great on Tabular Prediction
Authors: Jiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Chen, Jimeng Sun,
  Jian Wu, Jintai Chen
Categories: cs.CL cs.LG
Comments: Accepted to ICLR 2024 as spotlight presentation (Notable Top 5%).
  Codes will be available soon
\\
  The transferability of deep neural networks (DNNs) has made significant
progress in image and language processing. However, due to the heterogeneity
among tables, such DNN bonus is still far from being well exploited on tabular
data prediction (e.g., regression or classification tasks). Condensing
knowledge from diverse domains, language models (LMs) possess the capability to
comprehend feature names from various tables, potentially serving as versatile
learners in transferring knowledge across distinct tables and diverse
prediction tasks, but their discrete text representation space is inherently
incompatible with numerical feature values in tables. In this paper, we present
TP-BERTa, a specifically pre-trained LM model for tabular data prediction.
Concretely, a novel relative magnitude tokenization converts scalar numerical
feature values to finely discrete, high-dimensional tokens, and an
intra-feature attention approach integrates feature values with the
corresponding feature names. Comprehensive experiments demonstrate that our
pre-trained TP-BERTa leads the performance among tabular DNNs and is
competitive with Gradient Boosted Decision Tree models in typical tabular data
regime.
\\ ( https://arxiv.org/abs/2403.01841 ,  1471kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01848
Date: Mon, 4 Mar 2024 08:55:34 GMT   (9469kb,D)

Title: CET2: Modelling Topic Transitions for Coherent and Engaging
  Knowledge-Grounded Conversations
Authors: Lin Xu, Qixian Zhou, Jinlan Fu, See-Kiong Ng
Categories: cs.CL
Comments: Accepted by TASLP
\\
  Knowledge-grounded dialogue systems aim to generate coherent and engaging
responses based on the dialogue contexts and selected external knowledge.
Previous knowledge selection methods tend to rely too heavily on the dialogue
contexts or over-emphasize the new information in the selected knowledge,
resulting in the selection of repetitious or incongruous knowledge and further
generating repetitive or incoherent responses, as the generation of the
response depends on the chosen knowledge. To address these shortcomings, we
introduce a Coherent and Engaging Topic Transition (CET2) framework to model
topic transitions for selecting knowledge that is coherent to the context of
the conversations while providing adequate knowledge diversity for topic
development. Our CET2 framework considers multiple factors for knowledge
selection, including valid transition logic from dialogue contexts to the
following topics and systematic comparisons between available knowledge
candidates. Extensive experiments on two public benchmarks demonstrate the
superiority and the better generalization ability of CET2 on knowledge
selection. This is due to our well-designed transition features and comparative
knowledge selection strategy, which are more transferable to conversations
about unseen topics. Analysis of fine-grained knowledge selection accuracy also
shows that CET2 can better balance topic entailment (contextual coherence) and
development (knowledge diversity) in dialogue than existing approaches.
\\ ( https://arxiv.org/abs/2403.01848 ,  9469kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01851
Date: Mon, 4 Mar 2024 09:01:10 GMT   (1421kb,D)

Title: Rethinking LLM Language Adaptation: A Case Study on Chinese Mixtral
Authors: Yiming Cui, Xin Yao
Categories: cs.CL cs.AI
Comments: 13 pages
\\
  Mixtral, a representative sparse mixture of experts (SMoE) language model,
has received significant attention due to its unique model design and superior
performance. Based on Mixtral-8x7B-v0.1, in this paper, we propose
Chinese-Mixtral and Chinese-Mixtral-Instruct with improved Chinese language
abilities by adopting further pre-training and instruction fine-tuning.
Experimental results show that our Chinese-Mixtral and Chinese-Mixtral-Instruct
successfully improve Chinese understanding and generation performance while
retaining the original English abilities. Then, we discuss several key
questions when performing language adaptation on large language models,
including the necessity of extending the language-specific vocabulary and the
choice of the initialization model (foundation model v.s. instruction model),
by providing empirical results and analysis. We also present the visualizations
of each expert to examine their importance on downstream tasks. Our resources
are publicly available through \url{https://github.com/ymcui/Chinese-Mixtral}.
\\ ( https://arxiv.org/abs/2403.01851 ,  1421kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01858
Date: Mon, 4 Mar 2024 09:13:33 GMT   (173kb,D)

Title: An Improved Traditional Chinese Evaluation Suite for Foundation Model
Authors: Zhi-Rui Tam, Ya-Ting Pai, Yen-Wei Lee, Sega Cheng, Hong-Han Shuai
Categories: cs.CL
\\
  We present TMMLU+, a comprehensive dataset designed for the Traditional
Chinese massive multitask language understanding dataset. TMMLU+ is a
multiple-choice question-answering dataset with 66 subjects from elementary to
professional level. Compared to its predecessor, TMMLU, TMMLU+ is six times
larger and boasts a more balanced subject distribution. We included benchmark
results in TMMLU+ from closed-source models and 24 open-weight Chinese large
language models of parameters ranging from 1.8B to 72B. Our findings reveal
that Traditional Chinese models still trail behind their Simplified Chinese
counterparts. Additionally, current large language models have yet to
outperform human performance in average scores. We publicly release our dataset
and the corresponding benchmark source code.
\\ ( https://arxiv.org/abs/2403.01858 ,  173kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01886
Date: Mon, 4 Mar 2024 09:48:55 GMT   (2451kb,D)

Title: FCDS: Fusing Constituency and Dependency Syntax into Document-Level
  Relation Extraction
Authors: Xudong Zhu, Zhao Kang, Bei Hui
Categories: cs.CL cs.AI
Comments: Appear in COLING 2024
\\
  Document-level Relation Extraction (DocRE) aims to identify relation labels
between entities within a single document. It requires handling several
sentences and reasoning over them. State-of-the-art DocRE methods use a graph
structure to connect entities across the document to capture dependency syntax
information. However, this is insufficient to fully exploit the rich syntax
information in the document. In this work, we propose to fuse constituency and
dependency syntax into DocRE. It uses constituency syntax to aggregate the
whole sentence information and select the instructive sentences for the pairs
of targets. It exploits the dependency syntax in a graph structure with
constituency syntax enhancement and chooses the path between entity pairs based
on the dependency graph. The experimental results on datasets from various
domains demonstrate the effectiveness of the proposed method. The code is
publicly available at this url.
\\ ( https://arxiv.org/abs/2403.01886 ,  2451kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01897
Date: Mon, 4 Mar 2024 09:56:47 GMT   (64kb)

Title: Fostering the Ecosystem of Open Neural Encoders for Portuguese with
  Albertina PT* Family
Authors: Rodrigo Santos, Jo\~ao Rodrigues, Lu\'is Gomes, Jo\~ao Silva,
  Ant\'onio Branco, Henrique Lopes Cardoso, Tom\'as Freitas Os\'orio, Bernardo
  Leite
Categories: cs.CL
\\
  To foster the neural encoding of Portuguese, this paper contributes
foundation encoder models that represent an expansion of the still very scarce
ecosystem of large language models specifically developed for this language
that are fully open, in the sense that they are open source and openly
distributed for free under an open license for any purpose, thus including
research and commercial usages. Like most languages other than English,
Portuguese is low-resourced in terms of these foundational language resources,
there being the inaugural 900 million parameter Albertina and 335 million
Bertimbau. Taking this couple of models as an inaugural set, we present the
extension of the ecosystem of state-of-the-art open encoders for Portuguese
with a larger, top performance-driven model with 1.5 billion parameters, and a
smaller, efficiency-driven model with 100 million parameters. While achieving
this primary goal, further results that are relevant for this ecosystem were
obtained as well, namely new datasets for Portuguese based on the SuperGLUE
benchmark, which we also distribute openly.
\\ ( https://arxiv.org/abs/2403.01897 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01921
Date: Mon, 4 Mar 2024 10:37:48 GMT   (864kb)

Title: Arabic Text Sentiment Analysis: Reinforcing Human-Performed Surveys with
  Wider Topic Analysis
Authors: Latifah Almurqren, Ryan Hodgson, Alexandra Cristea
Categories: cs.CL
\\
  Sentiment analysis (SA) has been, and is still, a thriving research area.
However, the task of Arabic sentiment analysis (ASA) is still underrepresented
in the body of research. This study offers the first in-depth and in-breadth
analysis of existing ASA studies of textual content and identifies their common
themes, domains of application, methods, approaches, technologies and
algorithms used. The in-depth study manually analyses 133 ASA papers published
in the English language between 2002 and 2020 from four academic databases
(SAGE, IEEE, Springer, WILEY) and from Google Scholar. The in-breadth study
uses modern, automatic machine learning techniques, such as topic modelling and
temporal analysis, on Open Access resources, to reinforce themes and trends
identified by the prior study, on 2297 ASA publications between 2010-2020. The
main findings show the different approaches used for ASA: machine learning,
lexicon-based and hybrid approaches. Other findings include ASA 'winning'
algorithms (SVM, NB, hybrid methods). Deep learning methods, such as LSTM can
provide higher accuracy, but for ASA sometimes the corpora are not large enough
to support them. Additionally, whilst there are some ASA corpora and lexicons,
more are required. Specifically, Arabic tweets corpora and datasets are
currently only moderately sized. Moreover, Arabic lexicons that have high
coverage contain only Modern Standard Arabic (MSA) words, and those with Arabic
dialects are quite small. Thus, new corpora need to be created. On the other
hand, ASA tools are stringently lacking. There is a need to develop ASA tools
that can be used in industry, as well as in academia, for Arabic text SA.
Hence, our study offers insights into the challenges associated with ASA
research and provides suggestions for ways to move the field forward such as
lack of Dialectical Arabic resource, Arabic tweets, corpora and data sets for
SA.
\\ ( https://arxiv.org/abs/2403.01921 ,  864kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01924
Date: Mon, 4 Mar 2024 10:41:52 GMT   (4464kb,D)

Title: To Generate or to Retrieve? On the Effectiveness of Artificial Contexts
  for Medical Open-Domain Question Answering
Authors: Giacomo Frisoni, Alessio Cocchieri, Alex Presepi, Gianluca Moro,
  Zaiqiao Meng
Categories: cs.CL cs.AI
\\
  Medical open-domain question answering demands substantial access to
specialized knowledge. Recent efforts have sought to decouple knowledge from
model parameters, counteracting architectural scaling and allowing for training
on common low-resource hardware. The retrieve-then-read paradigm has become
ubiquitous, with model predictions grounded on relevant knowledge pieces from
external repositories such as PubMed, textbooks, and UMLS. An alternative path,
still under-explored but made possible by the advent of domain-specific large
language models, entails constructing artificial contexts through prompting. As
a result, "to generate or to retrieve" is the modern equivalent of Hamlet's
dilemma. This paper presents MedGENIE, the first generate-then-read framework
for multiple-choice question answering in medicine. We conduct extensive
experiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical
perspective by assuming a maximum of 24GB VRAM. MedGENIE sets a new
state-of-the-art (SOTA) in the open-book setting of each testbed, even allowing
a small-scale reader to outcompete zero-shot closed-book 175B baselines while
using up to 706$\times$ fewer parameters. Overall, our findings reveal that
generated passages are more effective than retrieved counterparts in attaining
higher accuracy.
\\ ( https://arxiv.org/abs/2403.01924 ,  4464kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01926
Date: Mon, 4 Mar 2024 10:42:08 GMT   (39165kb,D)

Title: IndicVoices: Towards building an Inclusive Multilingual Speech Dataset
  for Indian Languages
Authors: Tahir Javed, Janki Atul Nawale, Eldho Ittan George, Sakshi Joshi,
  Kaushal Santosh Bhogale, Deovrat Mehendale, Ishvinder Virender Sethi, Aparna
  Ananthanarayanan, Hafsah Faquih, Pratiti Palit, Sneha Ravishankar, Saranya
  Sukumaran, Tripura Panchagnula, Sunjay Murali, Kunal Sharad Gandhi,
  Ambujavalli R, Manickam K M, C Venkata Vaijayanthi, Krishnan Srinivasa
  Raghavan Karunganni, Pratyush Kumar, Mitesh M Khapra
Categories: cs.CL
\\
  We present INDICVOICES, a dataset of natural and spontaneous speech
containing a total of 7348 hours of read (9%), extempore (74%) and
conversational (17%) audio from 16237 speakers covering 145 Indian districts
and 22 languages. Of these 7348 hours, 1639 hours have already been
transcribed, with a median of 73 hours per language. Through this paper, we
share our journey of capturing the cultural, linguistic and demographic
diversity of India to create a one-of-its-kind inclusive and representative
dataset. More specifically, we share an open-source blueprint for data
collection at scale comprising of standardised protocols, centralised tools, a
repository of engaging questions, prompts and conversation scenarios spanning
multiple domains and topics of interest, quality control mechanisms,
comprehensive transcription guidelines and transcription tools. We hope that
this open source blueprint will serve as a comprehensive starter kit for data
collection efforts in other multilingual regions of the world. Using
INDICVOICES, we build IndicASR, the first ASR model to support all the 22
languages listed in the 8th schedule of the Constitution of India. All the
data, tools, guidelines, models and other materials developed as a part of this
work will be made publicly available
\\ ( https://arxiv.org/abs/2403.01926 ,  39165kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01929
Date: Mon, 4 Mar 2024 10:48:13 GMT   (1699kb,D)

Title: Analyzing and Adapting Large Language Models for Few-Shot Multilingual
  NLU: Are We There Yet?
Authors: Evgeniia Razumovskaia, Ivan Vuli\'c, Anna Korhonen
Categories: cs.CL
\\
  Supervised fine-tuning (SFT), supervised instruction tuning (SIT) and
in-context learning (ICL) are three alternative, de facto standard approaches
to few-shot learning. ICL has gained popularity recently with the advent of
LLMs due to its simplicity and sample efficiency. Prior research has conducted
only limited investigation into how these approaches work for multilingual
few-shot learning, and the focus so far has been mostly on their performance.
In this work, we present an extensive and systematic comparison of the three
approaches, testing them on 6 high- and low-resource languages, three different
NLU tasks, and a myriad of language and domain setups. Importantly, performance
is only one aspect of the comparison, where we also analyse the approaches
through the optics of their computational, inference and financial costs. Our
observations show that supervised instruction tuning has the best trade-off
between performance and resource requirements. As another contribution, we
analyse the impact of target language adaptation of pretrained LLMs and find
that the standard adaptation approaches can (superficially) improve target
language generation capabilities, but language understanding elicited through
ICL does not improve and remains limited, with low scores especially for
low-resource languages.
\\ ( https://arxiv.org/abs/2403.01929 ,  1699kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01931
Date: Mon, 4 Mar 2024 10:57:14 GMT   (975kb,D)

Title: VariErr NLI: Separating Annotation Error from Human Label Variation
Authors: Leon Weber-Genzel, Siyao Peng, Marie-Catherine de Marneffe, Barbara
  Plank
Categories: cs.CL
Comments: 13 pages, under review
\\
  Human label variation arises when annotators assign different labels to the
same item for valid reasons, while annotation errors occur when labels are
assigned for invalid reasons. These two issues are prevalent in NLP benchmarks,
yet existing research has studied them in isolation. To the best of our
knowledge, there exists no prior work that focuses on teasing apart error from
signal, especially in cases where signal is beyond black-and-white. To fill
this gap, we introduce a systematic methodology and a new dataset, VariErr
(variation versus error), focusing on the NLI task in English. We propose a
2-round annotation scheme with annotators explaining each label and
subsequently judging the validity of label-explanation pairs. \name{} contains
7,574 validity judgments on 1,933 explanations for 500 re-annotated NLI items.
We assess the effectiveness of various automatic error detection (AED) methods
and GPTs in uncovering errors versus human label variation. We find that
state-of-the-art AED methods significantly underperform compared to GPTs and
humans. While GPT-4 is the best system, it still falls short of human
performance. Our methodology is applicable beyond NLI, offering fertile ground
for future research on error versus plausible variation, which in turn can
yield better and more trustworthy NLP systems.
\\ ( https://arxiv.org/abs/2403.01931 ,  975kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01954
Date: Mon, 4 Mar 2024 11:49:08 GMT   (987kb,D)

Title: DECIDER: A Rule-Controllable Decoding Strategy for Language Generation
  by Imitating Dual-System Cognitive Theory
Authors: Chen Xu, Tian Lan, Changlong Yu, Wei Wang, Jun Gao, Yu Ji, Qunxi Dong,
  Kun Qian, Piji Li, Wei Bi, and Bin Hu
Categories: cs.CL cs.AI cs.LO
Comments: Submitted to IEEE TKDE, 12 pages, 6 figures
\\
  Lexicon-based constrained decoding approaches aim to control the meaning or
style of the generated text through certain target concepts. Existing
approaches over-focus the targets themselves, leading to a lack of high-level
reasoning about how to achieve them. However, human usually tackles tasks by
following certain rules that not only focuses on the targets but also on
semantically relevant concepts that induce the occurrence of targets. In this
work, we present DECIDER, a rule-controllable decoding strategy for constrained
language generation inspired by dual-system cognitive theory. Specifically, in
DECIDER, a pre-trained language model (PLM) is equiped with a logic reasoner
that takes high-level rules as input. Then, the DECIDER allows rule signals to
flow into the PLM at each decoding step. Extensive experimental results
demonstrate that DECIDER can effectively follow given rules to guide generation
direction toward the targets in a more human-like manner.
\\ ( https://arxiv.org/abs/2403.01954 ,  987kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01969
Date: Mon, 4 Mar 2024 12:13:59 GMT   (3418kb,D)

Title: AS-ES Learning: Towards Efficient CoT Learning in Small Models
Authors: Nuwa Xi, Yuhan Chen, Sendong Zhao, Haochun Wang, Bing Qin and Ting Liu
Categories: cs.CL
\\
  Chain-of-Thought (CoT) serves as a critical emerging ability in LLMs,
especially when it comes to logical reasoning. Attempts have been made to
induce such ability in small models as well by distilling from the data with
CoT generated by Large Language Models (LLMs). However, existing methods often
simply generate and incorporate more data from LLMs and fail to note the
importance of efficiently utilizing existing CoT data. We here propose a new
training paradigm AS-ES (Abstractive Segments - Extractive Segments) learning,
which exploits the inherent information in CoT for iterative generation.
Experiments show that our methods surpass the direct seq2seq training on
CoT-extensive tasks like MWP and PET summarization, without data augmentation
or altering the model itself. Furthermore, we explore the reason behind the
inefficiency of small models in learning CoT and provide an explanation of why
AS-ES learning works, giving insights into the underlying mechanism of CoT.
\\ ( https://arxiv.org/abs/2403.01969 ,  3418kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01972
Date: Mon, 4 Mar 2024 12:16:15 GMT   (359kb,D)

Title: Multi-perspective Improvement of Knowledge Graph Completion with Large
  Language Models
Authors: Derong Xu, Ziheng Zhang, Zhenxi Lin, Xian Wu, Zhihong Zhu, Tong Xu,
  Xiangyu Zhao, Yefeng Zheng and Enhong Chen
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\
  Knowledge graph completion (KGC) is a widely used method to tackle
incompleteness in knowledge graphs (KGs) by making predictions for missing
links. Description-based KGC leverages pre-trained language models to learn
entity and relation representations with their names or descriptions, which
shows promising results. However, the performance of description-based KGC is
still limited by the quality of text and the incomplete structure, as it lacks
sufficient entity descriptions and relies solely on relation names, leading to
sub-optimal results. To address this issue, we propose MPIKGC, a general
framework to compensate for the deficiency of contextualized knowledge and
improve KGC by querying large language models (LLMs) from various perspectives,
which involves leveraging the reasoning, explanation, and summarization
capabilities of LLMs to expand entity descriptions, understand relations, and
extract structures, respectively. We conducted extensive evaluation of the
effectiveness and improvement of our framework based on four description-based
KGC models and four datasets, for both link prediction and triplet
classification tasks.
\\ ( https://arxiv.org/abs/2403.01972 ,  359kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01976
Date: Mon, 4 Mar 2024 12:19:28 GMT   (4202kb,D)

Title: SciAssess: Benchmarking LLM Proficiency in Scientific Literature
  Analysis
Authors: Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Changxin
  Wang, Zhifeng Gao, Yongge Li, Mujie Lin, Shuwen Yang, Jiankun Wang, Yuqi Yin,
  Yaqi Li, Linfeng Zhang, Guolin Ke
Categories: cs.CL
\\
  Recent breakthroughs in Large Language Models (LLMs) have revolutionized
natural language understanding and generation, igniting a surge of interest in
leveraging these technologies for the nuanced field of scientific literature
analysis. Existing benchmarks, however, inadequately evaluate the proficiency
of LLMs in the scientific domain, especially in scenarios involving complex
comprehension and multimodal data. In response, we introduced SciAssess, a
benchmark tailored for the in-depth analysis of scientific literature, crafted
to provide a thorough assessment of LLMs' efficacy. SciAssess focuses on
evaluating LLMs' abilities in memorization, comprehension, and analysis within
scientific contexts. It includes representative tasks from diverse scientific
fields, such as general chemistry, organic materials, and alloy materials. And
rigorous quality control measures ensure its reliability in terms of
correctness, anonymization, and copyright compliance. SciAssess evaluates
leading LLMs, including GPT-4, GPT-3.5-turbo, and Gemini, identifying their
strengths and areas for improvement and supporting the ongoing development of
LLM applications in scientific literature analysis. SciAssess and its resources
are made available at https://sci-assess.github.io, offering a valuable tool
for advancing LLM capabilities in scientific literature analysis.
\\ ( https://arxiv.org/abs/2403.01976 ,  4202kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01983
Date: Mon, 4 Mar 2024 12:27:32 GMT   (1898kb)

Title: Language and Speech Technology for Central Kurdish Varieties
Authors: Sina Ahmadi, Daban Q. Jaff, Md Mahfuz Ibn Alam, Antonios
  Anastasopoulos
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  Kurdish, an Indo-European language spoken by over 30 million speakers, is
considered a dialect continuum and known for its diversity in language
varieties. Previous studies addressing language and speech technology for
Kurdish handle it in a monolithic way as a macro-language, resulting in
disparities for dialects and varieties for which there are few resources and
tools available. In this paper, we take a step towards developing resources for
language and speech technology for varieties of Central Kurdish, creating a
corpus by transcribing movies and TV series as an alternative to fieldwork.
Additionally, we report the performance of machine translation, automatic
speech recognition, and language identification as downstream tasks evaluated
on Central Kurdish varieties. Data and models are publicly available under an
open license at https://github.com/sinaahmadi/CORDI.
\\ ( https://arxiv.org/abs/2403.01983 ,  1898kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01985
Date: Mon, 4 Mar 2024 12:29:59 GMT   (2192kb,D)

Title: Transformers for Low-Resource Languages:Is F\'eidir Linn!
Authors: S\'eamus Lankford, Haithem Afli and Andy Way
Categories: cs.CL cs.AI
Comments: 13 pages
Journal-ref: Proceedings of Machine Translation Summit XVIII: Research Track
  2021
\\
  The Transformer model is the state-of-the-art in Machine Translation.
However, in general, neural translation models often under perform on language
pairs with insufficient training data. As a consequence, relatively few
experiments have been carried out using this architecture on low-resource
language pairs. In this study, hyperparameter optimization of Transformer
models in translating the low-resource English-Irish language pair is
evaluated. We demonstrate that choosing appropriate parameters leads to
considerable performance improvements. Most importantly, the correct choice of
subword model is shown to be the biggest driver of translation performance.
SentencePiece models using both unigram and BPE approaches were appraised.
Variations on model architectures included modifying the number of layers,
testing various regularisation techniques and evaluating the optimal number of
heads for attention. A generic 55k DGT corpus and an in-domain 88k public admin
corpus were used for evaluation. A Transformer optimized model demonstrated a
BLEU score improvement of 7.8 points when compared with a baseline RNN model.
Improvements were observed across a range of metrics, including TER, indicating
a substantially reduced post editing effort for Transformer optimized models
with 16k BPE subword models. Bench-marked against Google Translate, our
translation engines demonstrated significant improvements. The question of
whether or not Transformers can be used effectively in a low-resource setting
of English-Irish translation has been addressed. Is f\'eidir linn - yes we can.
\\ ( https://arxiv.org/abs/2403.01985 ,  2192kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01988
Date: Mon, 4 Mar 2024 12:35:09 GMT   (4751kb,D)

Title: FakeNewsGPT4: Advancing Multimodal Fake News Detection through
  Knowledge-Augmented LVLMs
Authors: Xuannan Liu and Peipei Li and Huaibo Huang and Zekun Li and Xing Cui
  and Jiahao Liang and Lixiong Qin and Weihong Deng and Zhaofeng He
Categories: cs.CL
\\
  The massive generation of multimodal fake news exhibits substantial
distribution discrepancies, prompting the need for generalized detectors.
However, the insulated nature of training within specific domains restricts the
capability of classical detectors to obtain open-world facts. In this paper, we
propose FakeNewsGPT4, a novel framework that augments Large Vision-Language
Models (LVLMs) with forgery-specific knowledge for manipulation reasoning while
inheriting extensive world knowledge as complementary. Knowledge augmentation
in FakeNewsGPT4 involves acquiring two types of forgery-specific knowledge,
i.e., semantic correlation and artifact trace, and merging them into LVLMs.
Specifically, we design a multi-level cross-modal reasoning module that
establishes interactions across modalities for extracting semantic
correlations. Concurrently, a dual-branch fine-grained verification module is
presented to comprehend localized details to encode artifact traces. The
generated knowledge is translated into refined embeddings compatible with
LVLMs. We also incorporate candidate answer heuristics and soft prompts to
enhance input informativeness. Extensive experiments on the public benchmark
demonstrate that FakeNewsGPT4 achieves superior cross-domain performance
compared to previous methods. Code will be available.
\\ ( https://arxiv.org/abs/2403.01988 ,  4751kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01994
Date: Mon, 4 Mar 2024 12:40:28 GMT   (7930kb,D)

Title: Vanilla Transformers are Transfer Capability Teachers
Authors: Xin Lu, Yanyan Zhao, Bing Qin
Categories: cs.CL
\\
  Recently, Mixture of Experts (MoE) Transformers have garnered increasing
attention due to their advantages in model capacity and computational
efficiency. However, studies have indicated that MoE Transformers underperform
vanilla Transformers in many downstream tasks, significantly diminishing the
practical value of MoE models. To explain this issue, we propose that the
pre-training performance and transfer capability of a model are joint
determinants of its downstream task performance. MoE models, in comparison to
vanilla models, have poorer transfer capability, leading to their subpar
performance in downstream tasks. To address this issue, we introduce the
concept of transfer capability distillation, positing that although vanilla
models have weaker performance, they are effective teachers of transfer
capability. The MoE models guided by vanilla models can achieve both strong
pre-training performance and transfer capability, ultimately enhancing their
performance in downstream tasks. We design a specific distillation method and
conduct experiments on the BERT architecture. Experimental results show a
significant improvement in downstream performance of MoE models, and many
further evidences also strongly support the concept of transfer capability
distillation. Finally, we attempt to interpret transfer capability distillation
and provide some insights from the perspective of model feature.
\\ ( https://arxiv.org/abs/2403.01994 ,  7930kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01999
Date: Mon, 4 Mar 2024 12:50:25 GMT   (2895kb,D)

Title: LLM-Oriented Retrieval Tuner
Authors: Si Sun, Hanqing Zhang, Zhiyuan Liu, Jie Bao, Dawei Song
Categories: cs.CL
Comments: 16 pages, 8 figures, 5 tables
\\
  Dense Retrieval (DR) is now considered as a promising tool to enhance the
memorization capacity of Large Language Models (LLM) such as GPT3 and GPT-4 by
incorporating external memories. However, due to the paradigm discrepancy
between text generation of LLM and DR, it is still an open challenge to
integrate the retrieval and generation tasks in a shared LLM. In this paper, we
propose an efficient LLM-Oriented Retrieval Tuner, namely LMORT, which
decouples DR capacity from base LLM and non-invasively coordinates the
optimally aligned and uniform layers of the LLM towards a unified DR space,
achieving an efficient and effective DR without tuning the LLM itself. The
extensive experiments on six BEIR datasets show that our approach could achieve
competitive zero-shot retrieval performance compared to a range of strong DR
models while maintaining the generation ability of LLM.
\\ ( https://arxiv.org/abs/2403.01999 ,  2895kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02009
Date: Mon, 4 Mar 2024 13:10:08 GMT   (612kb,D)

Title: Topic Aware Probing: From Sentence Length Prediction to Idiom
  Identification how reliant are Neural Language Models on Topic?
Authors: Vasudevan Nedumpozhimana, John D. Kelleher
Categories: cs.CL
\\
  Transformer-based Neural Language Models achieve state-of-the-art performance
on various natural language processing tasks. However, an open question is the
extent to which these models rely on word-order/syntactic or word
co-occurrence/topic-based information when processing natural language. This
work contributes to this debate by addressing the question of whether these
models primarily use topic as a signal, by exploring the relationship between
Transformer-based models' (BERT and RoBERTa's) performance on a range of
probing tasks in English, from simple lexical tasks such as sentence length
prediction to complex semantic tasks such as idiom token identification, and
the sensitivity of these tasks to the topic information. To this end, we
propose a novel probing method which we call topic-aware probing. Our initial
results indicate that Transformer-based models encode both topic and non-topic
information in their intermediate layers, but also that the facility of these
models to distinguish idiomatic usage is primarily based on their ability to
identify and encode topic. Furthermore, our analysis of these models'
performance on other standard probing tasks suggests that tasks that are
relatively insensitive to the topic information are also tasks that are
relatively difficult for these models.
\\ ( https://arxiv.org/abs/2403.02009 ,  612kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02078
Date: Mon, 4 Mar 2024 14:24:47 GMT   (7136kb,D)

Title: Automated Generation of Multiple-Choice Cloze Questions for Assessing
  English Vocabulary Using GPT-turbo 3.5
Authors: Qiao Wang, Ralph Rose, Naho Orita, Ayaka Sugawara
Categories: cs.CL
Journal-ref: Mika H\"am\"al\"ainen, Emily \"Ohman, Flammie Pirinen, Khalid
  Alnajjar, So Miyagawa, Yuri Bizzoni, Niko Partanen, and Jack Rueter. 2023.
  Proc. of the Joint 3rd International Conference on NLP4DH and 8th IWCLUL.
  ACL, Tokyo, Japan, edition
\\
  A common way of assessing language learners' mastery of vocabulary is via
multiple-choice cloze (i.e., fill-in-the-blank) questions. But the creation of
test items can be laborious for individual teachers or in large-scale language
programs. In this paper, we evaluate a new method for automatically generating
these types of questions using large language models (LLM). The VocaTT
(vocabulary teaching and training) engine is written in Python and comprises
three basic steps: pre-processing target word lists, generating sentences and
candidate word options using GPT, and finally selecting suitable word options.
To test the efficiency of this system, 60 questions were generated targeting
academic words. The generated items were reviewed by expert reviewers who
judged the well-formedness of the sentences and word options, adding comments
to items judged not well-formed. Results showed a 75% rate of well-formedness
for sentences and 66.85% rate for suitable word options. This is a marked
improvement over the generator used earlier in our research which did not take
advantage of GPT's capabilities. Post-hoc qualitative analysis reveals several
points for improvement in future work including cross-referencing
part-of-speech tagging, better sentence validation, and improving GPT prompts.
\\ ( https://arxiv.org/abs/2403.02078 ,  7136kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02121
Date: Mon, 4 Mar 2024 15:27:49 GMT   (699kb,D)

Title: Leveraging Weakly Annotated Data for Hate Speech Detection in Code-Mixed
  Hinglish: A Feasibility-Driven Transfer Learning Approach with Large Language
  Models
Authors: Sargam Yadav (1), Abhishek Kaushik (1) and Kevin McDaid (1) ((1)
  Dundalk Institute of Technology, Dundalk)
Categories: cs.CL cs.AI
Comments: This paper is accepted in the 16th ISDSI-Global Conference 2023
  https://isdsi2023.iimranchi.ac.in
\\
  The advent of Large Language Models (LLMs) has advanced the benchmark in
various Natural Language Processing (NLP) tasks. However, large amounts of
labelled training data are required to train LLMs. Furthermore, data annotation
and training are computationally expensive and time-consuming. Zero and
few-shot learning have recently emerged as viable options for labelling data
using large pre-trained models. Hate speech detection in mix-code low-resource
languages is an active problem area where the use of LLMs has proven
beneficial. In this study, we have compiled a dataset of 100 YouTube comments,
and weakly labelled them for coarse and fine-grained misogyny classification in
mix-code Hinglish. Weak annotation was applied due to the labor-intensive
annotation process. Zero-shot learning, one-shot learning, and few-shot
learning and prompting approaches have then been applied to assign labels to
the comments and compare them to human-assigned labels. Out of all the
approaches, zero-shot classification using the Bidirectional Auto-Regressive
Transformers (BART) large model and few-shot prompting using Generative
Pre-trained Transformer- 3 (ChatGPT-3) achieve the best results
\\ ( https://arxiv.org/abs/2403.02121 ,  699kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02130
Date: Mon, 4 Mar 2024 15:39:59 GMT   (634kb,D)

Title: Using LLMs for the Extraction and Normalization of Product Attribute
  Values
Authors: Nick Baumann, Alexander Brinkmann, Christian Bizer
Categories: cs.CL
\\
  Product offers on e-commerce websites often consist of a textual product
title and a textual product description. In order to provide features such as
faceted product filtering or content-based product recommendation, the websites
need to extract attribute-value pairs from the unstructured product
descriptions. This paper explores the potential of using large language models
(LLMs), such as OpenAI's GPT-3.5 and GPT-4, to extract and normalize attribute
values from product titles and product descriptions. For our experiments, we
introduce the WDC Product Attribute-Value Extraction (WDC PAVE) dataset. WDC
PAVE consists of product offers from 87 websites that provide schema.org
annotations. The offers belong to five different categories, each featuring a
specific set of attributes. The dataset provides manually verified
attribute-value pairs in two forms: (i) directly extracted values and (ii)
normalized attribute values. The normalization of the attribute values requires
systems to perform the following types of operations: name expansion,
generalization, unit of measurement normalization, and string wrangling. Our
experiments demonstrate that GPT-4 outperforms PLM-based extraction methods by
10%, achieving an F1-Score of 91%. For the extraction and normalization of
product attribute values, GPT-4 achieves a similar performance to the
extraction scenario, while being particularly strong at string wrangling and
name expansion.
\\ ( https://arxiv.org/abs/2403.02130 ,  634kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02173
Date: Mon, 4 Mar 2024 16:20:14 GMT   (452kb,D)

Title: What has LeBenchmark Learnt about French Syntax?
Authors: Zdravko Dugonji\'c, Adrien Pupier, Benjamin Lecouteux, Maximin Coavoux
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  The paper reports on a series of experiments aiming at probing LeBenchmark, a
pretrained acoustic model trained on 7k hours of spoken French, for syntactic
information. Pretrained acoustic models are increasingly used for downstream
speech tasks such as automatic speech recognition, speech translation, spoken
language understanding or speech parsing. They are trained on very low level
information (the raw speech signal), and do not have explicit lexical
knowledge. Despite that, they obtained reasonable results on tasks that
requires higher level linguistic knowledge. As a result, an emerging question
is whether these models encode syntactic information. We probe each
representation layer of LeBenchmark for syntax, using the Orf\'eo treebank, and
observe that it has learnt some syntactic information. Our results show that
syntactic information is more easily extractable from the middle layers of the
network, after which a very sharp decrease is observed.
\\ ( https://arxiv.org/abs/2403.02173 ,  452kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02176
Date: Mon, 4 Mar 2024 16:21:13 GMT   (41kb,D)

Title: EEE-QA: Exploring Effective and Efficient Question-Answer
  Representations
Authors: Zhanghao Hu, Yijun Yang, Junjie Xu, Yifu Qiu, Pinzhen Chen
Categories: cs.CL
Comments: LREC-COLING 2024
\\
  Current approaches to question answering rely on pre-trained language models
(PLMs) like RoBERTa. This work challenges the existing question-answer encoding
convention and explores finer representations. We begin with testing various
pooling methods compared to using the begin-of-sentence token as a question
representation for better quality. Next, we explore opportunities to
simultaneously embed all answer candidates with the question. This enables
cross-reference between answer choices and improves inference throughput via
reduced memory usage. Despite their simplicity and effectiveness, these methods
have yet to be widely studied in current frameworks. We experiment with
different PLMs, and with and without the integration of knowledge graphs.
Results prove that the memory efficacy of the proposed techniques with little
sacrifice in performance. Practically, our work enhances 38-100% throughput
with 26-65% speedups on consumer-grade GPUs by allowing for considerably larger
batch sizes. Our work sends a message to the community with promising
directions in both representation quality and efficiency for the
question-answering task in natural language processing.
\\ ( https://arxiv.org/abs/2403.02176 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02177
Date: Mon, 4 Mar 2024 16:21:19 GMT   (1140kb,D)

Title: ProTrix: Building Models for Planning and Reasoning over Tables with
  Sentence Context
Authors: Zirui Wu and Yansong Feng
Categories: cs.CL
Comments: under review
\\
  Tables play a crucial role in conveying information in various domains,
serving as indispensable tools for organizing and presenting data in a
structured manner. We propose a Plan-then-Reason framework to answer different
types of user queries over tables with sentence context. The framework first
plans the reasoning paths over the context, then assigns each step to
program-based or textual reasoning to reach the final answer. We construct an
instruction tuning set TrixInstruct following the framework. Our dataset cover
queries that are program-unsolvable or need combining information from tables
and sentences to obtain planning and reasoning abilities. We present ProTrix by
finetuning Llama-2-7B on TrixInstruct. Our experiments show that ProTrix
generalizes to diverse tabular tasks and achieves comparable performance to
GPT-3.5-turbo. We further demonstrate that ProTrix can generate accurate and
faithful explanations to answer complex free-form questions. Our work
underscores the importance of the planning and reasoning abilities towards a
model over tabular tasks with generalizability and interpretability. We will
release our dataset and model at https://github.com/WilliamZR/ProTrix.
\\ ( https://arxiv.org/abs/2403.02177 ,  1140kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02178
Date: Mon, 4 Mar 2024 16:21:54 GMT   (8963kb,D)

Title: Masked Thought: Simply Masking Partial Reasoning Steps Can Improve
  Mathematical Reasoning Learning of Language Models
Authors: Changyu Chen, Xiting Wang, Ting-En Lin, Ang Lv, Yuchuan Wu, Xin Gao,
  Ji-Rong Wen, Rui Yan and Yongbin Li
Categories: cs.CL cs.AI cs.LG
\\
  In reasoning tasks, even a minor error can cascade into inaccurate results,
leading to suboptimal performance of large language models in such domains.
Earlier fine-tuning approaches sought to mitigate this by leveraging more
precise supervisory signals from human labeling, larger models, or
self-sampling, although at a high cost. Conversely, we develop a method that
avoids external resources, relying instead on introducing perturbations to the
input. Our training approach randomly masks certain tokens within the chain of
thought, a technique we found to be particularly effective for reasoning tasks.
When applied to fine-tuning with GSM8K, this method achieved a 5% improvement
in accuracy over standard supervised fine-tuning with a few codes modified and
no additional labeling effort. Furthermore, it is complementary to existing
methods. When integrated with related data augmentation methods, it leads to an
average improvement of 3% improvement in GSM8K accuracy and 1% improvement in
MATH accuracy across five datasets of various quality and size, as well as two
base models. We further investigate the mechanisms behind this improvement
through case studies and quantitative analysis, suggesting that our approach
may provide superior support for the model in capturing long-distance
dependencies, especially those related to questions. This enhancement could
deepen understanding of premises in questions and prior steps. Our code is
available at Github.
\\ ( https://arxiv.org/abs/2403.02178 ,  8963kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02181
Date: Mon, 4 Mar 2024 16:23:58 GMT   (878kb,D)

Title: Not all Layers of LLMs are Necessary during Inference
Authors: Siqi Fan, Xin Jiang, Xiang Li, Xuying Meng, Peng Han, Shuo Shang,
  Aixin Sun, Yequan Wang, Zhongyuan Wang
Categories: cs.CL cs.AI cs.LG
\\
  The inference phase of Large Language Models (LLMs) is very expensive. An
ideal inference stage of LLMs could utilize fewer computational resources while
still maintaining its capabilities (e.g., generalization and in-context
learning ability). In this paper, we try to answer the question, "During LLM
inference, can we use shallow layers for easy instances; and deep layers for
hard ones?" To answer this question, we first indicate that Not all Layers are
Necessary during Inference by statistically analyzing the activated layers
across tasks. Then, we propose a simple algorithm named AdaInfer to determine
the inference termination moment based on the input instance adaptively. More
importantly, AdaInfer does not alter LLM parameters and maintains
generalizability across tasks. Experiments on well-known LLMs (i.e., Llama2
series and OPT) show that AdaInfer saves an average of 14.8% of computational
resources, even up to 50% on sentiment tasks, while maintaining comparable
performance. Additionally, this method is orthogonal to other model
acceleration techniques, potentially boosting inference efficiency further.
\\ ( https://arxiv.org/abs/2403.02181 ,  878kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02246
Date: Mon, 4 Mar 2024 17:34:34 GMT   (1272kb)

Title: PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large
  Language Models
Authors: Fiona Anting Tan, Gerard Christopher Yeo, Fanyou Wu, Weijie Xu, Vinija
  Jain, Aman Chadha, Kokil Jaidka, Yang Liu, See-Kiong Ng
Categories: cs.CL
\\
  Recent advances in large language models (LLMs) demonstrate that their
capabilities are comparable, or even superior, to humans in many tasks in
natural language processing. Despite this progress, LLMs are still inadequate
at social-cognitive reasoning, which humans are naturally good at. Drawing
inspiration from psychological research on the links between certain
personality traits and Theory-of-Mind (ToM) reasoning, and from prompt
engineering research on the hyper-sensitivity of prompts in affecting LLMs
capabilities, this study investigates how inducing personalities in LLMs using
prompts affects their ToM reasoning capabilities. Our findings show that
certain induced personalities can significantly affect the LLMs' reasoning
capabilities in three different ToM tasks. In particular, traits from the Dark
Triad have a larger variable effect on LLMs like GPT-3.5, Llama 2, and Mistral
across the different ToM tasks. We find that LLMs that exhibit a higher
variance across personality prompts in ToM also tends to be more controllable
in personality tests: personality traits in LLMs like GPT-3.5, Llama 2 and
Mistral can be controllably adjusted through our personality prompts. In
today's landscape where role-play is a common strategy when using LLMs, our
research highlights the need for caution, as models that adopt specific
personas with personalities potentially also alter their reasoning abilities in
an unexpected manner.
\\ ( https://arxiv.org/abs/2403.02246 ,  1272kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02247
Date: Mon, 4 Mar 2024 17:34:46 GMT   (452kb)

Title: Birbal: An efficient 7B instruct-model fine-tuned with curated datasets
Authors: Ashvini Kumar Jindal, Pawan Kumar Rajpoot, Ankur Parikh
Categories: cs.CL
\\
  LLMOps incur significant costs due to hardware requirements, hindering their
widespread accessibility. Additionally, a lack of transparency in model
training methods and data contributes to the majority of models being
non-reproducible. To tackle these challenges, the LLM Efficiency Challenge was
introduced at NeurIPS Workshop, aiming to adapt foundation models on a diverse
set of tasks via fine-tuning on a single GPU (RTX 4090 or A100 with 40GB)
within a 24-hour timeframe. In this system description paper, we introduce
Birbal, our Mistral-7B based winning model, fine-tuned on a single RTX 4090 for
16 hours. Birbal's success lies in curating high-quality instructions covering
diverse tasks, resulting in a 35% performance improvement over second-best
Qwen-14B based submission.
\\ ( https://arxiv.org/abs/2403.02247 ,  452kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02268
Date: Mon, 4 Mar 2024 17:56:28 GMT   (7712kb,D)

Title: Subjective $\textit{Isms}$? On the Danger of Conflating Hate and Offence
  in Abusive Language Detection
Authors: Amanda Cercas Curry, Gavin Abercrombie, Zeerak Talat
Categories: cs.CL cs.AI cs.CY
\\
  Natural language processing research has begun to embrace the notion of
annotator subjectivity, motivated by variations in labelling. This approach
understands each annotator's view as valid, which can be highly suitable for
tasks that embed subjectivity, e.g., sentiment analysis. However, this
construction may be inappropriate for tasks such as hate speech detection, as
it affords equal validity to all positions on e.g., sexism or racism. We argue
that the conflation of hate and offence can invalidate findings on hate speech,
and call for future work to be situated in theory, disentangling hate from its
orthogonal concept, offence.
\\ ( https://arxiv.org/abs/2403.02268 ,  7712kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02270
Date: Mon, 4 Mar 2024 17:57:18 GMT   (8257kb,D)

Title: FENICE: Factuality Evaluation of summarization based on Natural language
  Inference and Claim Extraction
Authors: Alessandro Scir\`e and Karim Ghonim and Roberto Navigli
Categories: cs.CL
Comments: 9 pages, long paper
\\
  Recent advancements in text summarization, particularly with the advent of
Large Language Models (LLMs), have shown remarkable performance. However, a
notable challenge persists as a substantial number of automatically-generated
summaries exhibit factual inconsistencies, such as hallucinations. In response
to this issue, various approaches for the evaluation of consistency for
summarization have emerged. Yet, these newly-introduced metrics face several
limitations, including lack of interpretability, focus on short document
summaries (e.g., news articles), and computational impracticality, especially
for LLM-based metrics. To address these shortcomings, we propose Factuality
Evaluation of summarization based on Natural language Inference and Claim
Extraction (FENICE), a more interpretable and efficient factuality-oriented
metric. FENICE leverages an NLI-based alignment between information in the
source document and a set of atomic facts, referred to as claims, extracted
from the summary. Our metric sets a new state of the art on AGGREFACT, the
de-facto benchmark for factuality evaluation. Moreover, we extend our
evaluation to a more challenging setting by conducting a human annotation
process of long-form summarization.
\\ ( https://arxiv.org/abs/2403.02270 ,  8257kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02271
Date: Mon, 4 Mar 2024 17:58:09 GMT   (7229kb,D)

Title: RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language
  Models
Authors: Saeed Najafi and Alona Fyshe
Categories: cs.CL cs.LG
Comments: Version Submitted to ACL2024. Review Discussion here:
  https://openreview.net/forum?id=_gFGBVMRN1
\\
  Pre-trained Language Models (PLMs) can be accurately fine-tuned for
downstream text processing tasks. Recently, researchers have introduced several
parameter-efficient fine-tuning methods that optimize input prompts or adjust a
small number of model parameters (e.g LoRA). In this study, we explore the
impact of altering the input text of the original task in conjunction with
parameter-efficient fine-tuning methods. To most effectively rewrite the input
text, we train a few-shot paraphrase model with a Maximum-Marginal Likelihood
objective. Using six few-shot text classification datasets, we show that
enriching data with paraphrases at train and test time enhances the performance
beyond what can be achieved with parameter-efficient fine-tuning alone.
\\ ( https://arxiv.org/abs/2403.02271 ,  7229kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02281
Date: Mon, 4 Mar 2024 18:12:10 GMT   (7752kb,D)

Title: Emotion Granularity from Text: An Aggregate-Level Indicator of Mental
  Health
Authors: Krishnapriya Vishnubhotla, Daniela Teodorescu, Mallory J. Feldman,
  Kristen A. Lindquist, Saif M. Mohammad
Categories: cs.CL
Comments: 9 pages plus appendices
\\
  We are united in how emotions are central to shaping our experiences; and
yet, individuals differ greatly in how we each identify, categorize, and
express emotions. In psychology, variation in the ability of individuals to
differentiate between emotion concepts is called emotion granularity
(determined through self-reports of one's emotions). High emotion granularity
has been linked with better mental and physical health; whereas low emotion
granularity has been linked with maladaptive emotion regulation strategies and
poor health outcomes. In this work, we propose computational measures of
emotion granularity derived from temporally-ordered speaker utterances in
social media (in lieu of self-reports that suffer from various biases). We then
investigate the effectiveness of such text-derived measures of emotion
granularity in functioning as markers of various mental health conditions
(MHCs). We establish baseline measures of emotion granularity derived from
textual utterances, and show that, at an aggregate level, emotion granularities
are significantly lower for people self-reporting as having an MHC than for the
control population. This paves the way towards a better understanding of the
MHCs, and specifically the role emotions play in our well-being.
\\ ( https://arxiv.org/abs/2403.02281 ,  7752kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02285
Date: Mon, 4 Mar 2024 18:15:14 GMT   (282kb,D)

Title: Detection of Non-recorded Word Senses in English and Swedish
Authors: Jonathan Lautenschlager, Emma Sk\"oldberg, Simon Hengchen, Dominik
  Schlechtweg
Categories: cs.CL
Comments: 9 pages
\\
  This study addresses the task of Unknown Sense Detection in English and
Swedish. The primary objective of this task is to determine whether the meaning
of a particular word usage is documented in a dictionary or not. For this
purpose, sense entries are compared with word usages from modern and historical
corpora using a pre-trained Word-in-Context embedder that allows us to model
this task in a few-shot scenario. Additionally, we use human annotations to
adapt and evaluate our models. Compared to a random sample from a corpus, our
model is able to considerably increase the detected number of word usages with
non-recorded senses.
\\ ( https://arxiv.org/abs/2403.02285 ,  282kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02333
Date: Mon, 4 Mar 2024 18:58:30 GMT   (1224kb,D)

Title: Key-Point-Driven Data Synthesis with its Enhancement on Mathematical
  Reasoning
Authors: Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan,
  Weizhu Chen
Categories: cs.CL cs.AI
Comments: In progress
\\
  Large language models (LLMs) have shown great potential in complex reasoning
tasks, yet their performance is often hampered by the scarcity of high-quality,
reasoning-focused training datasets. Addressing this challenge, we propose
Key-Point-Driven Data Synthesis (KPDDS), a novel data synthesis framework that
synthesizes question-answer pairs by leveraging key points and exemplar pairs
from authentic data sources. KPDDS ensures the generation of novel questions
with rigorous quality control and substantial scalability. As a result, we
present KPMath, the most extensive synthetic dataset tailored for mathematical
reasoning to date, comprising over one million question-answer pairs. Utilizing
KPMath and augmenting it with additional reasoning-intensive corpora, we create
the comprehensive KPMath-Plus dataset. Fine-tuning the Mistral-7B model on
KPMath-Plus yields a zero-shot PASS@1 accuracy of 39.3% on the MATH test set, a
performance that not only outpaces other finetuned 7B models but also exceeds
that of certain 34B models. Our ablation studies further confirm the
substantial enhancement in mathematical reasoning across various subtopics,
marking a significant stride in LLMs' reasoning capabilities.
\\ ( https://arxiv.org/abs/2403.02333 ,  1224kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00773
Date: Tue, 13 Feb 2024 17:11:08 GMT   (63kb,D)

Title: Misconduct in Post-Selections and Deep Learning
Authors: Juyang Weng
Categories: cs.LG
Comments: 9 pages, 2 figures, published in peer-viewed conference proceedings,
  Proc. 2023 the 8th International Conf. on Control, Robotics and Cybernetics
  (CRC 2023), pp. 1-9, IEEE Press, ISBN: 979-8-3503-3057-1, Changsha, China,
  Dec. 22-24, 2023
ACM-class: I.2
\\
  This is a theoretical paper on "Deep Learning" misconduct in particular and
Post-Selection in general. As far as the author knows, the first peer-reviewed
papers on Deep Learning misconduct are [32], [37], [36]. Regardless of learning
modes, e.g., supervised, reinforcement, adversarial, and evolutional, almost
all machine learning methods (except for a few methods that train a sole
system) are rooted in the same misconduct -- cheating and hiding -- (1)
cheating in the absence of a test and (2) hiding bad-looking data. It was
reasoned in [32], [37], [36] that authors must report at least the average
error of all trained networks, good and bad, on the validation set (called
general cross-validation in this paper). Better, report also five percentage
positions of ranked errors. From the new analysis here, we can see that the
hidden culprit is Post-Selection. This is also true for Post-Selection on
hand-tuned or searched hyperparameters, because they are random, depending on
random observation data. Does cross-validation on data splits rescue
Post-Selections from the Misconducts (1) and (2)? The new result here says: No.
Specifically, this paper reveals that using cross-validation for data splits is
insufficient to exonerate Post-Selections in machine learning. In general,
Post-Selections of statistical learners based on their errors on the validation
set are statistically invalid.
\\ ( https://arxiv.org/abs/2403.00773 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00780
Date: Sat, 17 Feb 2024 15:00:45 GMT   (26074kb)

Title: Empirical and Experimental Insights into Data Mining Techniques for
  Crime Prediction: A Comprehensive Survey
Authors: Kamal Taha
Categories: cs.LG cs.AI
\\
  This survey paper presents a comprehensive analysis of crime prediction
methodologies, exploring the various techniques and technologies utilized in
this area. The paper covers the statistical methods, machine learning
algorithms, and deep learning techniques employed to analyze crime data, while
also examining their effectiveness and limitations. We propose a methodological
taxonomy that classifies crime prediction algorithms into specific techniques.
This taxonomy is structured into four tiers, including methodology category,
methodology sub-category, methodology techniques, and methodology
sub-techniques. Empirical and experimental evaluations are provided to rank the
different techniques. The empirical evaluation assesses the crime prediction
techniques based on four criteria, while the experimental evaluation ranks the
algorithms that employ the same sub-technique, the different sub-techniques
that employ the same technique, the different techniques that employ the same
methodology sub-category, the different methodology sub-categories within the
same category, and the different methodology categories. The combination of
methodological taxonomy, empirical evaluations, and experimental comparisons
allows for a nuanced and comprehensive understanding of crime prediction
algorithms, aiding researchers in making informed decisions. Finally, the paper
provides a glimpse into the future of crime prediction techniques, highlighting
potential advancements and opportunities for further research in this field
\\ ( https://arxiv.org/abs/2403.00780 ,  26074kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00853
Date: Thu, 29 Feb 2024 18:03:03 GMT   (10052kb,D)

Title: Distributed Momentum Methods Under Biased Gradient Estimations
Authors: Ali Beikmohammadi, Sarit Khirirat, Sindri Magn\'usson
Categories: cs.LG
\\
  Distributed stochastic gradient methods are gaining prominence in solving
large-scale machine learning problems that involve data distributed across
multiple nodes. However, obtaining unbiased stochastic gradients, which have
been the focus of most theoretical research, is challenging in many distributed
machine learning applications. The gradient estimations easily become biased,
for example, when gradients are compressed or clipped, when data is shuffled,
and in meta-learning and reinforcement learning. In this work, we establish
non-asymptotic convergence bounds on distributed momentum methods under biased
gradient estimation on both general non-convex and $\mu$-PL non-convex
problems. Our analysis covers general distributed optimization problems, and we
work out the implications for special cases where gradient estimates are
biased, i.e., in meta-learning and when the gradients are compressed or
clipped. Our numerical experiments on training deep neural networks with
Top-$K$ sparsification and clipping verify faster convergence performance of
momentum methods than traditional biased gradient descent.
\\ ( https://arxiv.org/abs/2403.00853 ,  10052kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00858
Date: Thu, 29 Feb 2024 19:55:06 GMT   (865kb,D)

Title: Direct Alignment of Draft Model for Speculative Decoding with
  Chat-Fine-Tuned LLMs
Authors: Raghavv Goel, Mukul Gagrani, Wonseok Jeon, Junyoung Park, Mingu Lee,
  Christopher Lott
Categories: cs.LG cs.AI cs.CL
Comments: 8 pages, 3 figures
\\
  Text generation with Large Language Models (LLMs) is known to be memory bound
due to the combination of their auto-regressive nature, huge parameter counts,
and limited memory bandwidths, often resulting in low token rates. Speculative
decoding has been proposed as a solution for LLM inference acceleration.
However, since draft models are often unavailable in the modern open-source LLM
families, e.g., for Llama 2 7B, training a high-quality draft model is required
to enable inference acceleration via speculative decoding. In this paper, we
propose a simple draft model training framework for direct alignment to
chat-capable target models. With the proposed framework, we train Llama 2 Chat
Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of
the original size. Our training framework only consists of pretraining,
distillation dataset generation, and finetuning with knowledge distillation,
with no additional alignment procedure. For the finetuning step, we use
instruction-response pairs generated by target model for distillation in
plausible data distribution, and propose a new Total Variation Distance++
(TVD++) loss that incorporates variance reduction techniques inspired from the
policy gradient method in reinforcement learning. Our empirical results show
that Llama 2 Chat Drafter 115M with speculative decoding achieves up to 2.3
block efficiency and 2.4$\times$ speed-up relative to autoregressive decoding
on various tasks with no further task-specific fine-tuning.
\\ ( https://arxiv.org/abs/2403.00858 ,  865kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00860
Date: Thu, 29 Feb 2024 20:48:39 GMT   (288kb,D)

Title: Parallel Algorithms for Exact Enumeration of Deep Neural Network
  Activation Regions
Authors: Sabrina Drammis, Bowen Zheng, Karthik Srinivasan, Robert C. Berwick,
  Nancy A. Lynch, Robert Ajemian
Categories: cs.LG cs.AI cs.NE
\\
  A feedforward neural network using rectified linear units constructs a
mapping from inputs to outputs by partitioning its input space into a set of
convex regions where points within a region share a single affine
transformation. In order to understand how neural networks work, when and why
they fail, and how they compare to biological intelligence, we need to
understand the organization and formation of these regions. Step one is to
design and implement algorithms for exact region enumeration in networks beyond
toy examples.
  In this work, we present parallel algorithms for exact enumeration in deep
(and shallow) neural networks. Our work has three main contributions: (1) we
present a novel algorithm framework and parallel algorithms for region
enumeration; (2) we implement one of our algorithms on a variety of network
architectures and experimentally show how the number of regions dictates
runtime; and (3) we show, using our algorithm's output, how the dimension of a
region's affine transformation impacts further partitioning of the region by
deeper layers.
  To our knowledge, we run our implemented algorithm on networks larger than
all of the networks used in the existing region enumeration literature.
Further, we experimentally demonstrate the importance of parallelism for region
enumeration of any reasonably sized network.
\\ ( https://arxiv.org/abs/2403.00860 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00869
Date: Fri, 1 Mar 2024 04:42:47 GMT   (6968kb,D)

Title: Enhancing Multivariate Time Series Forecasting with Mutual
  Information-driven Cross-Variable and Temporal Modeling
Authors: Shiyi Qi, Liangjian Wen, Yiduo Li, Yuanhang Yang, Zhe Li, Zhongwen
  Rao, Lujia Pan, Zenglin Xu
Categories: cs.LG stat.ML
\\
  Recent advancements have underscored the impact of deep learning techniques
on multivariate time series forecasting (MTSF). Generally, these techniques are
bifurcated into two categories: Channel-independence and Channel-mixing
approaches. Although Channel-independence methods typically yield better
results, Channel-mixing could theoretically offer improvements by leveraging
inter-variable correlations. Nonetheless, we argue that the integration of
uncorrelated information in channel-mixing methods could curtail the potential
enhancement in MTSF model performance. To substantiate this claim, we introduce
the Cross-variable Decorrelation Aware feature Modeling (CDAM) for
Channel-mixing approaches, aiming to refine Channel-mixing by minimizing
redundant information between channels while enhancing relevant mutual
information. Furthermore, we introduce the Temporal correlation Aware Modeling
(TAM) to exploit temporal correlations, a step beyond conventional single-step
forecasting methods. This strategy maximizes the mutual information between
adjacent sub-sequences of both the forecasted and target series. Combining CDAM
and TAM, our novel framework significantly surpasses existing models, including
those previously considered state-of-the-art, in comprehensive tests.
\\ ( https://arxiv.org/abs/2403.00869 ,  6968kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00877
Date: Fri, 1 Mar 2024 08:26:44 GMT   (575kb,D)

Title: Disaggregated Multi-Tower: Topology-aware Modeling Technique for
  Efficient Large-Scale Recommendation
Authors: Liang Luo, Buyun Zhang, Michael Tsang, Yinbin Ma, Ching-Hsiang Chu,
  Yuxin Chen, Shen Li, Yuchen Hao, Yanli Zhao, Guna Lakshminarayanan, Ellie
  Dingqiao Wen, Jongsoo Park, Dheevatsa Mudigere, Maxim Naumov
Categories: cs.LG cs.DC cs.IR
\\
  We study a mismatch between the deep learning recommendation models' flat
architecture, common distributed training paradigm and hierarchical data center
topology. To address the associated inefficiencies, we propose Disaggregated
Multi-Tower (DMT), a modeling technique that consists of (1)
Semantic-preserving Tower Transform (SPTT), a novel training paradigm that
decomposes the monolithic global embedding lookup process into disjoint towers
to exploit data center locality; (2) Tower Module (TM), a synergistic dense
component attached to each tower to reduce model complexity and communication
volume through hierarchical feature interaction; and (3) Tower Partitioner
(TP), a feature partitioner to systematically create towers with meaningful
feature interactions and load balanced assignments to preserve model quality
and training throughput via learned embeddings. We show that DMT can achieve up
to 1.9x speedup compared to the state-of-the-art baselines without losing
accuracy across multiple generations of hardware at large data center scales.
\\ ( https://arxiv.org/abs/2403.00877 ,  575kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00881
Date: Fri, 1 Mar 2024 09:14:10 GMT   (2196kb,D)

Title: FedRDMA: Communication-Efficient Cross-Silo Federated LLM via Chunked
  RDMA Transmission
Authors: Zeling Zhang, Dongqi Cai, Yiran Zhang, Mengwei Xu, Shangguang Wang, Ao
  Zhou
Categories: cs.LG cs.DC cs.NI
Comments: under review
\\
  Communication overhead is a significant bottleneck in federated learning
(FL), which has been exaggerated with the increasing size of AI models. In this
paper, we propose FedRDMA, a communication-efficient cross-silo FL system that
integrates RDMA into the FL communication protocol. To overcome the limitations
of RDMA in wide-area networks (WANs), FedRDMA divides the updated model into
chunks and designs a series of optimization techniques to improve the
efficiency and robustness of RDMA-based communication. We implement FedRDMA
atop the industrial federated learning framework and evaluate it on a
real-world cross-silo FL scenario. The experimental results show that \sys can
achieve up to 3.8$\times$ speedup in communication efficiency compared to
traditional TCP/IP-based FL systems.
\\ ( https://arxiv.org/abs/2403.00881 ,  2196kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00886
Date: Fri, 1 Mar 2024 10:19:17 GMT   (265kb,D)

Title: Evaluating and Correcting Performative Effects of Decision Support
  Systems via Causal Domain Shift
Authors: Philip Boeken, Onno Zoeter, Joris M. Mooij
Categories: cs.LG math.ST stat.TH
Comments: Accepted at CLeaR 2024
\\
  When predicting a target variable $Y$ from features $X$, the prediction
$\hat{Y}$ can be performative: an agent might act on this prediction, affecting
the value of $Y$ that we eventually observe. Performative predictions are
deliberately prevalent in algorithmic decision support, where a Decision
Support System (DSS) provides a prediction for an agent to affect the value of
the target variable. When deploying a DSS in high-stakes settings (e.g.
healthcare, law, predictive policing, or child welfare screening) it is
imperative to carefully assess the performative effects of the DSS. In the case
that the DSS serves as an alarm for a predicted negative outcome, naive
retraining of the prediction model is bound to result in a model that
underestimates the risk, due to effective workings of the previous model. In
this work, we propose to model the deployment of a DSS as causal domain shift
and provide novel cross-domain identification results for the conditional
expectation $E[Y | X]$, allowing for pre- and post-hoc assessment of the
deployment of the DSS, and for retraining of a model that assesses the risk
under a baseline policy where the DSS is not deployed. Using a running example,
we empirically show that a repeated regression procedure provides a practical
framework for estimating these quantities, even when the data is affected by
sample selection bias and selective labelling, offering for a practical,
unified solution for multiple forms of target variable bias.
\\ ( https://arxiv.org/abs/2403.00886 ,  265kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00891
Date: Fri, 1 Mar 2024 13:04:12 GMT   (1821kb,D)

Title: A Regularization-based Transfer Learning Method for Information
  Extraction via Instructed Graph Decoder
Authors: Kedi Chen and Jie Zhou and Qin Chen and Shunyu Liu and Liang He
Categories: cs.LG cs.AI cs.CL
\\
  Information extraction (IE) aims to extract complex structured information
from the text. Numerous datasets have been constructed for various IE tasks,
leading to time-consuming and labor-intensive data annotations. Nevertheless,
most prevailing methods focus on training task-specific models, while the
common knowledge among different IE tasks is not explicitly modeled. Moreover,
the same phrase may have inconsistent labels in different tasks, which poses a
big challenge for knowledge transfer using a unified model. In this study, we
propose a regularization-based transfer learning method for IE (TIE) via an
instructed graph decoder. Specifically, we first construct an instruction pool
for datasets from all well-known IE tasks, and then present an instructed graph
decoder, which decodes various complex structures into a graph uniformly based
on corresponding instructions. In this way, the common knowledge shared with
existing datasets can be learned and transferred to a new dataset with new
labels. Furthermore, to alleviate the label inconsistency problem among various
IE tasks, we introduce a task-specific regularization strategy, which does not
update the gradients of two tasks with 'opposite direction'. We conduct
extensive experiments on 12 datasets spanning four IE tasks, and the results
demonstrate the great advantages of our proposed method
\\ ( https://arxiv.org/abs/2403.00891 ,  1821kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00930
Date: Fri, 1 Mar 2024 19:21:10 GMT   (51kb)

Title: Scale-free Adversarial Reinforcement Learning
Authors: Mingyu Chen, Xuezhou Zhang
Categories: cs.LG cs.AI
\\
  This paper initiates the study of scale-free learning in Markov Decision
Processes (MDPs), where the scale of rewards/losses is unknown to the learner.
We design a generic algorithmic framework, \underline{S}cale
\underline{C}lipping \underline{B}ound (\texttt{SCB}), and instantiate this
framework in both the adversarial Multi-armed Bandit (MAB) setting and the
adversarial MDP setting. Through this framework, we achieve the first minimax
optimal expected regret bound and the first high-probability regret bound in
scale-free adversarial MABs, resolving an open problem raised in
\cite{hadiji2023adaptation}. On adversarial MDPs, our framework also give birth
to the first scale-free RL algorithm with a $\tilde{\mathcal{O}}(\sqrt{T})$
high-probability regret guarantee.
\\ ( https://arxiv.org/abs/2403.00930 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00932
Date: Fri, 1 Mar 2024 19:22:24 GMT   (306kb,D)

Title: Differentially Private Knowledge Distillation via Synthetic Text
  Generation
Authors: James Flemings and Murali Annavaram
Categories: cs.LG cs.CL cs.CR
\\
  Large Language models (LLMs) are achieving state-of-the-art performance in
many different downstream tasks. However, the increasing urgency of data
privacy requires LLMs to train with Differential Privacy (DP) on private data.
Concurrently it is also necessary to compress LLMs for real-life deployments on
resource-constrained devices or latency-sensitive applications. Differential
privacy and model compression generally must trade off utility loss to achieve
their objectives. Moreover, concurrently achieving both can result in even more
utility loss. To this end, we propose a novel differentially private knowledge
distillation algorithm that exploits synthetic data generated by a
differentially private LLM. The knowledge of a teacher model is transferred
onto the student in two ways: one way from the synthetic data itself, the hard
labels, and the other way by the output distribution of the teacher model
evaluated on the synthetic data, the soft labels. Furthermore, if the teacher
and student share a similar architectural structure, we can further distill
knowledge by exploiting hidden representations. Our results show that our
framework substantially improves the utility over existing baselines with
strong privacy parameters, {\epsilon} = 2, validating that we can successfully
compress autoregressive LLMs while preserving the privacy of training data.
\\ ( https://arxiv.org/abs/2403.00932 ,  306kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00942
Date: Fri, 1 Mar 2024 19:39:19 GMT   (7528kb,D)

Title: Resilience of Entropy Model in Distributed Neural Networks
Authors: Milin Zhang, Mohammad Abdi, Shahriar Rifat, Francesco Restuccia
Categories: cs.LG cs.AI cs.CR
\\
  Distributed deep neural networks (DNNs) have emerged as a key technique to
reduce communication overhead without sacrificing performance in edge computing
systems. Recently, entropy coding has been introduced to further reduce the
communication overhead. The key idea is to train the distributed DNN jointly
with an entropy model, which is used as side information during inference time
to adaptively encode latent representations into bit streams with variable
length. To the best of our knowledge, the resilience of entropy models is yet
to be investigated. As such, in this paper we formulate and investigate the
resilience of entropy models to intentional interference (e.g., adversarial
attacks) and unintentional interference (e.g., weather changes and motion
blur). Through an extensive experimental campaign with 3 different DNN
architectures, 2 entropy models and 4 rate-distortion trade-off factors, we
demonstrate that the entropy attacks can increase the communication overhead by
up to 95%. By separating compression features in frequency and spatial domain,
we propose a new defense mechanism that can reduce the transmission overhead of
the attacked input by about 9% compared to unperturbed data, with only about 2%
accuracy loss. Importantly, the proposed defense mechanism is a standalone
approach which can be applied in conjunction with approaches such as
adversarial training to further improve robustness. Code will be shared for
reproducibility.
\\ ( https://arxiv.org/abs/2403.00942 ,  7528kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00946
Date: Fri, 1 Mar 2024 19:50:22 GMT   (6186kb,D)

Title: Fine-tuning with Very Large Dropout
Authors: Jianyu Zhang, L\'eon Bottou
Categories: cs.LG cs.CV
Comments: 13 pages
\\
  It is impossible today to pretend that the practice of machine learning is
compatible with the idea that training and testing data follow the same
distribution. Several authors have recently used ensemble techniques to show
how scenarios involving multiple data distributions are best served by
representations that are both richer than those obtained by regularizing for
the best in-distribution performance, and richer than those obtained under the
influence of the implicit sparsity bias of common stochastic gradient
procedures.
  This contribution investigates the use of very high dropout rates instead of
ensembles to obtain such rich representations. Although training a deep network
from scratch using such dropout rates is virtually impossible, fine-tuning a
large pre-trained model under such conditions is not only possible but also
achieves out-of-distribution performances that exceed those of both ensembles
and weight averaging methods such as model soups. This result has practical
significance because the importance of the fine-tuning scenario has
considerably grown in recent years. This result also provides interesting
insights on the nature of rich representations and on the intrinsically linear
nature of fine-tuning a large network using a comparatively small dataset.
\\ ( https://arxiv.org/abs/2403.00946 ,  6186kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00963
Date: Fri, 1 Mar 2024 20:26:33 GMT   (2095kb,D)

Title: Tree-Regularized Tabular Embeddings
Authors: Xuan Li, Yun Wang, Bo Li
Categories: cs.LG
Comments: Table Representation Learning Workshop at NeurIPS 2023
\\
  Tabular neural network (NN) has attracted remarkable attentions and its
recent advances have gradually narrowed the performance gap with respect to
tree-based models on many public datasets. While the mainstreams focus on
calibrating NN to fit tabular data, we emphasize the importance of homogeneous
embeddings and alternately concentrate on regularizing tabular inputs through
supervised pretraining. Specifically, we extend a recent work (DeepTLF) and
utilize the structure of pretrained tree ensembles to transform raw variables
into a single vector (T2V), or an array of tokens (T2T). Without loss of space
efficiency, these binarized embeddings can be consumed by canonical tabular NN
with fully-connected or attention-based building blocks. Through quantitative
experiments on 88 OpenML datasets with binary classification task, we validated
that the proposed tree-regularized representation not only tapers the
difference with respect to tree-based models, but also achieves on-par and
better performance when compared with advanced NN models. Most importantly, it
possesses better robustness and can be easily scaled and generalized as
standalone encoder for tabular modality. Codes:
https://github.com/milanlx/tree-regularized-embedding.
\\ ( https://arxiv.org/abs/2403.00963 ,  2095kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00974
Date: Fri, 1 Mar 2024 20:51:10 GMT   (4288kb)

Title: Motif distribution and function of sparse deep neural networks
Authors: Olivia T. Zahn, Thomas L. Daniel, J. Nathan Kutz
Categories: cs.LG
\\
  We characterize the connectivity structure of feed-forward, deep neural
networks (DNNs) using network motif theory. To address whether a particular
motif distribution is characteristic of the training task, or function of the
DNN, we compare the connectivity structure of 350 DNNs trained to simulate a
bio-mechanical flight control system with different randomly initialized
parameters. We develop and implement algorithms for counting second- and
third-order motifs and calculate their significance using their Z-score. The
DNNs are trained to solve the inverse problem of the flight dynamics model in
Bustamante, et al. (2022) (i.e., predict the controls necessary for controlled
flight from the initial and final state-space inputs) and are sparsified
through an iterative pruning and retraining algorithm Zahn, et al. (2022). We
show that, despite random initialization of network parameters, enforced
sparsity causes DNNs to converge to similar connectivity patterns as
characterized by their motif distributions. The results suggest how neural
network function can be encoded in motif distributions, suggesting a variety of
experiments for informing function and control.
\\ ( https://arxiv.org/abs/2403.00974 ,  4288kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00975
Date: Fri, 1 Mar 2024 20:54:31 GMT   (18233kb,D)

Title: Equipment Health Assessment: Time Series Analysis for Wind Turbine
  Performance
Authors: Jana Backhus, Aniruddha Rajendra Rao, Chandrasekar Venkatraman,
  Abhishek Padmanabhan, A.Vinoth Kumar, Chetan Gupta
Categories: cs.LG cs.AI math.FA stat.AP
Comments: 19 Pages, 17 Figures, 3 Tables, Submitted at Applied Sciences (MDPI)
\\
  In this study, we leverage SCADA data from diverse wind turbines to predict
power output, employing advanced time series methods, specifically Functional
Neural Networks (FNN) and Long Short-Term Memory (LSTM) networks. A key
innovation lies in the ensemble of FNN and LSTM models, capitalizing on their
collective learning. This ensemble approach outperforms individual models,
ensuring stable and accurate power output predictions. Additionally, machine
learning techniques are applied to detect wind turbine performance
deterioration, enabling proactive maintenance strategies and health assessment.
Crucially, our analysis reveals the uniqueness of each wind turbine,
necessitating tailored models for optimal predictions. These insight
underscores the importance of providing automatized customization for different
turbines to keep human modeling effort low. Importantly, the methodologies
developed in this analysis are not limited to wind turbines; they can be
extended to predict and optimize performance in various machinery, highlighting
the versatility and applicability of our research across diverse industrial
contexts.
\\ ( https://arxiv.org/abs/2403.00975 ,  18233kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00993
Date: Fri, 1 Mar 2024 21:28:19 GMT   (492kb,D)

Title: On the Role of Information Structure in Reinforcement Learning for
  Partially-Observable Sequential Teams and Games
Authors: Awni Altabaa, Zhuoran Yang
Categories: cs.LG cs.AI stat.ML
Comments: 57 pages, 5 figures
\\
  In a sequential decision-making problem, the information structure is the
description of how events in the system occurring at different points in time
affect each other. Classical models of reinforcement learning (e.g., MDPs,
POMDPs, Dec-POMDPs, and POMGs) assume a very simple and highly regular
information structure, while more general models like predictive state
representations do not explicitly model the information structure. By contrast,
real-world sequential decision-making problems typically involve a complex and
time-varying interdependence of system variables, requiring a rich and flexible
representation of information structure.
  In this paper, we argue for the perspective that explicit representation of
information structures is an important component of analyzing and solving
reinforcement learning problems. We propose novel reinforcement learning models
with an explicit representation of information structure, capturing classical
models as special cases. We show that this leads to a richer analysis of
sequential decision-making problems and enables more tailored algorithm design.
In particular, we characterize the "complexity" of the observable dynamics of
any sequential decision-making problem through a graph-theoretic analysis of
the DAG representation of its information structure. The central quantity in
this analysis is the minimal set of variables that $d$-separates the past
observations from future observations. Furthermore, through constructing a
generalization of predictive state representations, we propose tailored
reinforcement learning algorithms and prove that the sample complexity is in
part determined by the information structure. This recovers known tractability
results and gives a novel perspective on reinforcement learning in general
sequential decision-making problems, providing a systematic way of identifying
new tractable classes of problems.
\\ ( https://arxiv.org/abs/2403.00993 ,  492kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00999
Date: Fri, 1 Mar 2024 21:49:34 GMT   (14829kb,D)

Title: Distributional Dataset Distillation with Subtask Decomposition
Authors: Tian Qin, Zhiwei Deng, David Alvarez-Melis
Categories: cs.LG
\\
  What does a neural network learn when training from a task-specific dataset?
Synthesizing this knowledge is the central idea behind Dataset Distillation,
which recent work has shown can be used to compress large datasets into a small
set of input-label pairs ($\textit{prototypes}$) that capture essential aspects
of the original dataset. In this paper, we make the key observation that
existing methods distilling into explicit prototypes are very often suboptimal,
incurring in unexpected storage cost from distilled labels. In response, we
propose $\textit{Distributional Dataset Distillation}$ (D3), which encodes the
data using minimal sufficient per-class statistics and paired with a decoder,
we distill dataset into a compact distributional representation that is more
memory-efficient compared to prototype-based methods. To scale up the process
of learning these representations, we propose $\textit{Federated
distillation}$, which decomposes the dataset into subsets, distills them in
parallel using sub-task experts and then re-aggregates them. We thoroughly
evaluate our algorithm on a three-dimensional metric and show that our method
achieves state-of-the-art results on TinyImageNet and ImageNet-1K.
Specifically, we outperform the prior art by $6.9\%$ on ImageNet-1K under the
storage budget of 2 images per class.
\\ ( https://arxiv.org/abs/2403.00999 ,  14829kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01014
Date: Fri, 1 Mar 2024 22:24:11 GMT   (32039kb,D)

Title: A Case for Validation Buffer in Pessimistic Actor-Critic
Authors: Michal Nauman, Mateusz Ostaszewski and Marek Cygan
Categories: cs.LG
Comments: Preprint
\\
  In this paper, we investigate the issue of error accumulation in critic
networks updated via pessimistic temporal difference objectives. We show that
the critic approximation error can be approximated via a recursive fixed-point
model similar to that of the Bellman value. We use such recursive definition to
retrieve the conditions under which the pessimistic critic is unbiased.
Building on these insights, we propose Validation Pessimism Learning (VPL)
algorithm. VPL uses a small validation buffer to adjust the levels of pessimism
throughout the agent training, with the pessimism set such that the
approximation error of the critic targets is minimized. We investigate the
proposed approach on a variety of locomotion and manipulation tasks and report
improvements in sample efficiency and performance.
\\ ( https://arxiv.org/abs/2403.01014 ,  32039kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01046
Date: Sat, 2 Mar 2024 00:33:45 GMT   (4106kb,D)

Title: A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex
  Lasso Models with Reflection Features
Authors: Emi Zeger, Yifei Wang, Aaron Mishkin, Tolga Ergen, Emmanuel Cand\`es,
  Mert Pilanci
Categories: cs.LG cs.AI cs.NE math.OC stat.ML
\\
  We prove that training neural networks on 1-D data is equivalent to solving a
convex Lasso problem with a fixed, explicitly defined dictionary matrix of
features. The specific dictionary depends on the activation and depth. We
consider 2-layer networks with piecewise linear activations, deep narrow ReLU
networks with up to 4 layers, and rectangular and tree networks with sign
activation and arbitrary depth. Interestingly in ReLU networks, a fourth layer
creates features that represent reflections of training data about themselves.
The Lasso representation sheds insight to globally optimal networks and the
solution landscape.
\\ ( https://arxiv.org/abs/2403.01046 ,  4106kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01053
Date: Sat, 2 Mar 2024 00:56:05 GMT   (1223kb,D)

Title: Seeing Unseen: Discover Novel Biomedical Concepts via
  GeometryConstrained Probabilistic Modeling
Authors: Jianan Fan, Dongnan Liu, Hang Chang, Heng Huang, Mei Chen, and Weidong
  Cai
Categories: cs.LG cs.AI cs.CV
Comments: CVPR 2024
\\
  Machine learning holds tremendous promise for transforming the fundamental
practice of scientific discovery by virtue of its data-driven nature. With the
ever-increasing stream of research data collection, it would be appealing to
autonomously explore patterns and insights from observational data for
discovering novel classes of phenotypes and concepts. However, in the
biomedical domain, there are several challenges inherently presented in the
cumulated data which hamper the progress of novel class discovery. The
non-i.i.d. data distribution accompanied by the severe imbalance among
different groups of classes essentially leads to ambiguous and biased semantic
representations. In this work, we present a geometry-constrained probabilistic
modeling treatment to resolve the identified issues. First, we propose to
parameterize the approximated posterior of instance embedding as a marginal von
MisesFisher distribution to account for the interference of distributional
latent bias. Then, we incorporate a suite of critical geometric properties to
impose proper constraints on the layout of constructed embedding space, which
in turn minimizes the uncontrollable risk for unknown class learning and
structuring. Furthermore, a spectral graph-theoretic method is devised to
estimate the number of potential novel classes. It inherits two intriguing
merits compared to existent approaches, namely high computational efficiency
and flexibility for taxonomy-adaptive estimation. Extensive experiments across
various biomedical scenarios substantiate the effectiveness and general
applicability of our method.
\\ ( https://arxiv.org/abs/2403.01053 ,  1223kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01059
Date: Sat, 2 Mar 2024 01:40:37 GMT   (1792kb,D)

Title: Continuous Mean-Zero Disagreement-Regularized Imitation Learning
  (CMZ-DRIL)
Authors: Noah Ford, Ryan W. Gardner, Austin Juhl, and Nathan Larson
Categories: cs.LG
\\
  Machine-learning paradigms such as imitation learning and reinforcement
learning can generate highly performant agents in a variety of complex
environments. However, commonly used methods require large quantities of data
and/or a known reward function. This paper presents a method called Continuous
Mean-Zero Disagreement-Regularized Imitation Learning (CMZ-DRIL) that employs a
novel reward structure to improve the performance of imitation-learning agents
that have access to only a handful of expert demonstrations. CMZ-DRIL uses
reinforcement learning to minimize uncertainty among an ensemble of agents
trained to model the expert demonstrations. This method does not use any
environment-specific rewards, but creates a continuous and mean-zero reward
function from the action disagreement of the agent ensemble. As demonstrated in
a waypoint-navigation environment and in two MuJoCo environments, CMZ-DRIL can
generate performant agents that behave more similarly to the expert than
primary previous approaches in several key metrics.
\\ ( https://arxiv.org/abs/2403.01059 ,  1792kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01071
Date: Sat, 2 Mar 2024 02:28:20 GMT   (5089kb,D)

Title: GraphRCG: Self-conditioned Graph Generation via Bootstrapped
  Representations
Authors: Song Wang, Zhen Tan, Xinyu Zhao, Tianlong Chen, Huan Liu, Jundong Li
Categories: cs.LG cs.AI
\\
  Graph generation generally aims to create new graphs that closely align with
a specific graph distribution. Existing works often implicitly capture this
distribution through the optimization of generators, potentially overlooking
the intricacies of the distribution itself. Furthermore, these approaches
generally neglect the insights offered by the learned distribution for graph
generation. In contrast, in this work, we propose a novel self-conditioned
graph generation framework designed to explicitly model graph distributions and
employ these distributions to guide the generation process. We first perform
self-conditioned modeling to capture the graph distributions by transforming
each graph sample into a low-dimensional representation and optimizing a
representation generator to create new representations reflective of the
learned distribution. Subsequently, we leverage these bootstrapped
representations as self-conditioned guidance for the generation process,
thereby facilitating the generation of graphs that more accurately reflect the
learned distributions. We conduct extensive experiments on generic and
molecular graph datasets across various fields. Our framework demonstrates
superior performance over existing state-of-the-art graph generation methods in
terms of graph quality and fidelity to training data.
\\ ( https://arxiv.org/abs/2403.01071 ,  5089kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01078
Date: Sat, 2 Mar 2024 03:26:09 GMT   (15801kb,D)

Title: $\Gamma$-VAE: Curvature regularized variational autoencoders for
  uncovering emergent low dimensional geometric structure in high dimensional
  data
Authors: Jason Z. Kim, Nicolas Perrin-Gilbert, Erkan Narmanli, Paul Klein,
  Christopher R. Myers, Itai Cohen, Joshua J. Waterfall, James P. Sethna
Categories: cs.LG cs.AI physics.bio-ph q-bio.GN
Comments: 8 pages, 4 figures
\\
  Natural systems with emergent behaviors often organize along low-dimensional
subsets of high-dimensional spaces. For example, despite the tens of thousands
of genes in the human genome, the principled study of genomics is fruitful
because biological processes rely on coordinated organization that results in
lower dimensional phenotypes. To uncover this organization, many nonlinear
dimensionality reduction techniques have successfully embedded high-dimensional
data into low-dimensional spaces by preserving local similarities between data
points. However, the nonlinearities in these methods allow for too much
curvature to preserve general trends across multiple non-neighboring data
clusters, thereby limiting their interpretability and generalizability to
out-of-distribution data. Here, we address both of these limitations by
regularizing the curvature of manifolds generated by variational autoencoders,
a process we coin ``$\Gamma$-VAE''. We demonstrate its utility using two
example data sets: bulk RNA-seq from the The Cancer Genome Atlas (TCGA) and the
Genotype Tissue Expression (GTEx); and single cell RNA-seq from a lineage
tracing experiment in hematopoietic stem cell differentiation. We find that the
resulting regularized manifolds identify mesoscale structure associated with
different cancer cell types, and accurately re-embed tissues from completely
unseen, out-of distribution cancers as if they were originally trained on them.
Finally, we show that preserving long-range relationships to differentiated
cells separates undifferentiated cells -- which have not yet specialized --
according to their eventual fate. Broadly, we anticipate that regularizing the
curvature of generative models will enable more consistent, predictive, and
generalizable models in any high-dimensional system with emergent
low-dimensional behavior.
\\ ( https://arxiv.org/abs/2403.01078 ,  15801kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01079
Date: Sat, 2 Mar 2024 03:29:11 GMT   (1780kb,D)

Title: Teaching MLP More Graph Information: A Three-stage Multitask Knowledge
  Distillation Framework
Authors: Junxian Li, Bin Shi, Erfei Cui, Hua Wei, Qinghua Zheng
Categories: cs.LG cs.AI
Comments: 20 pages, with Appendix
\\
  We study the challenging problem for inference tasks on large-scale graph
datasets of Graph Neural Networks: huge time and memory consumption, and try to
overcome it by reducing reliance on graph structure. Even though distilling
graph knowledge to student MLP is an excellent idea, it faces two major
problems of positional information loss and low generalization. To solve the
problems, we propose a new three-stage multitask distillation framework. In
detail, we use Positional Encoding to capture positional information. Also, we
introduce Neural Heat Kernels responsible for graph data processing in GNN and
utilize hidden layer outputs matching for better performance of student MLP's
hidden layers. To the best of our knowledge, it is the first work to include
hidden layer distillation for student MLP on graphs and to combine graph
Positional Encoding with MLP. We test its performance and robustness with
several settings and draw the conclusion that our work can outperform well with
good stability.
\\ ( https://arxiv.org/abs/2403.01079 ,  1780kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01091
Date: Sat, 2 Mar 2024 04:30:09 GMT   (841kb,D)

Title: COOL: A Conjoint Perspective on Spatio-Temporal Graph Neural Network for
  Traffic Forecasting
Authors: Wei Ju, Yusheng Zhao, Yifang Qin, Siyu Yi, Jingyang Yuan, Zhiping
  Xiao, Xiao Luo, Xiting Yan, and Ming Zhang
Categories: cs.LG cs.AI cs.IR cs.SI
Comments: Accepted by Information Fusion 2024
\\
  This paper investigates traffic forecasting, which attempts to forecast the
future state of traffic based on historical situations. This problem has
received ever-increasing attention in various scenarios and facilitated the
development of numerous downstream applications such as urban planning and
transportation management. However, the efficacy of existing methods remains
sub-optimal due to their tendency to model temporal and spatial relationships
independently, thereby inadequately accounting for complex high-order
interactions of both worlds. Moreover, the diversity of transitional patterns
in traffic forecasting makes them challenging to capture for existing
approaches, warranting a deeper exploration of their diversity. Toward this
end, this paper proposes Conjoint Spatio-Temporal graph neural network
(abbreviated as COOL), which models heterogeneous graphs from prior and
posterior information to conjointly capture high-order spatio-temporal
relationships. On the one hand, heterogeneous graphs connecting sequential
observation are constructed to extract composite spatio-temporal relationships
via prior message passing. On the other hand, we model dynamic relationships
using constructed affinity and penalty graphs, which guide posterior message
passing to incorporate complementary semantic information into node
representations. Moreover, to capture diverse transitional properties to
enhance traffic forecasting, we propose a conjoint self-attention decoder that
models diverse temporal patterns from both multi-rank and multi-scale views.
Experimental results on four popular benchmark datasets demonstrate that our
proposed COOL provides state-of-the-art performance compared with the
competitive baselines.
\\ ( https://arxiv.org/abs/2403.01091 ,  841kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01092
Date: Sat, 2 Mar 2024 04:31:28 GMT   (799kb,D)

Title: Pairwise Alignment Improves Graph Domain Adaptation
Authors: Shikun Liu, Deyu Zou, Han Zhao, Pan Li
Categories: cs.LG
Comments: Our code and data are available at:
  https://github.com/Graph-COM/Pair-Align
\\
  Graph-based methods, pivotal for label inference over interconnected objects
in many real-world applications, often encounter generalization challenges, if
the graph used for model training differs significantly from the graph used for
testing. This work delves into Graph Domain Adaptation (GDA) to address the
unique complexities of distribution shifts over graph data, where
interconnected data points experience shifts in features, labels, and in
particular, connecting patterns. We propose a novel, theoretically principled
method, Pairwise Alignment (Pair-Align) to counter graph structure shift by
mitigating conditional structure shift (CSS) and label shift (LS). Pair-Align
uses edge weights to recalibrate the influence among neighboring nodes to
handle CSS and adjusts the classification loss with label weights to handle LS.
Our method demonstrates superior performance in real-world applications,
including node classification with region shift in social networks, and the
pileup mitigation task in particle colliding experiments. For the first
application, we also curate the largest dataset by far for GDA studies. Our
method shows strong performance in synthetic and other existing benchmark
datasets.
\\ ( https://arxiv.org/abs/2403.01092 ,  799kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01101
Date: Sat, 2 Mar 2024 06:01:34 GMT   (1469kb,D)

Title: Feature Alignment: Rethinking Efficient Active Learning via Proxy in the
  Context of Pre-trained Models
Authors: Ziting Wen, Oscar Pizarro, Stefan Williams
Categories: cs.LG cs.AI
\\
  Fine-tuning the pre-trained model with active learning holds promise for
reducing annotation costs. However, this combination introduces significant
computational costs, particularly with the growing scale of pre-trained models.
Recent research has proposed proxy-based active learning, which pre-computes
features to reduce computational costs. Yet, this approach often incurs a
significant loss in active learning performance, which may even outweigh the
computational cost savings. In this paper, we argue the performance drop stems
not only from pre-computed features' inability to distinguish between
categories of labeled samples, resulting in the selection of redundant samples
but also from the tendency to compromise valuable pre-trained information when
fine-tuning with samples selected through the proxy model. To address this
issue, we propose a novel method called aligned selection via proxy to update
pre-computed features while selecting a proper training method to inherit
valuable pre-training information. Extensive experiments validate that our
method significantly improves the total cost of efficient active learning while
maintaining computational efficiency.
\\ ( https://arxiv.org/abs/2403.01101 ,  1469kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01112
Date: Sat, 2 Mar 2024 07:37:05 GMT   (31056kb,D)

Title: Efficient Episodic Memory Utilization of Cooperative Multi-Agent
  Reinforcement Learning
Authors: Hyungho Na, Yunkyeong Seo, Il-chul Moon
Categories: cs.LG cs.MA
Comments: Accepted at ICLR 2024
\\
  In cooperative multi-agent reinforcement learning (MARL), agents aim to
achieve a common goal, such as defeating enemies or scoring a goal. Existing
MARL algorithms are effective but still require significant learning time and
often get trapped in local optima by complex tasks, subsequently failing to
discover a goal-reaching policy. To address this, we introduce Efficient
episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a)
accelerating reinforcement learning by leveraging semantically coherent memory
from an episodic buffer and (b) selectively promoting desirable transitions to
prevent local convergence. To achieve (a), EMU incorporates a trainable
encoder/decoder structure alongside MARL, creating coherent memory embeddings
that facilitate exploratory memory recall. To achieve (b), EMU introduces a
novel reward structure called episodic incentive based on the desirability of
states. This reward improves the TD target in Q-learning and acts as an
additional incentive for desirable transitions. We provide theoretical support
for the proposed incentive and demonstrate the effectiveness of EMU compared to
conventional episodic control. The proposed method is evaluated in StarCraft II
and Google Research Football, and empirical results indicate further
performance improvement over state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.01112 ,  31056kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01121
Date: Sat, 2 Mar 2024 08:05:03 GMT   (266kb,D)

Title: OpenGraph: Towards Open Graph Foundation Models
Authors: Lianghao Xia, Ben Kao and Chao Huang
Categories: cs.LG cs.AI cs.SI
\\
  Graph learning has become indispensable for interpreting and harnessing
relational data in diverse fields, ranging from recommendation systems to
social network analysis. In this context, a variety of GNNs have emerged as
promising methodologies for encoding the structural information of graphs. By
effectively capturing the graph's underlying structure, these GNNs have shown
great potential in enhancing performance in graph learning tasks, such as link
prediction and node classification. However, despite their successes, a
significant challenge persists: these advanced methods often face difficulties
in generalizing to unseen graph data that significantly differs from the
training instances. In this work, our aim is to advance the graph learning
paradigm by developing a general graph foundation model. This model is designed
to understand the complex topological patterns present in diverse graph data,
enabling it to excel in zero-shot graph learning tasks across different
downstream datasets. To achieve this goal, we address several key technical
challenges in our OpenGraph model. Firstly, we propose a unified graph
tokenizer to adapt our graph model to generalize well on unseen graph data,
even when the underlying graph properties differ significantly from those
encountered during training. Secondly, we develop a scalable graph transformer
as the foundational encoder, which effectively captures node-wise dependencies
within the global topological context. Thirdly, we introduce a data
augmentation mechanism enhanced by a LLM to alleviate the limitations of data
scarcity in real-world scenarios. Extensive experiments validate the
effectiveness of our framework. By adapting our OpenGraph to new graph
characteristics and comprehending the nuances of diverse graphs, our approach
achieves remarkable zero-shot graph learning performance across various
settings and domains.
\\ ( https://arxiv.org/abs/2403.01121 ,  266kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01128
Date: Sat, 2 Mar 2024 08:18:32 GMT   (727kb,D)

Title: Sensitivity Analysis On Loss Landscape
Authors: Salman Faroz
Categories: cs.LG
\\
  Gradients can be employed for sensitivity analysis. Here, we leverage the
advantages of the Loss Landscape to comprehend which independent variables
impact the dependent variable. We seek to grasp the loss landscape by utilizing
first, second, and third derivatives through automatic differentiation. we know
that Spearman's rank correlation coefficient can detect the monotonic
relationship between two variables. However, I have found that second-order
gradients, with certain configurations and parameters, provide information that
can be visualized similarly to Spearman's results.In our approach, we
incorporate a loss function with an activation function, resulting in a
non-linear pattern. Each exploration of the loss landscape through retraining
yields new valuable information. Furthermore, the first and third derivatives
are also beneficial, as they indicate the extent to which independent variables
influence the dependent variable.
\\ ( https://arxiv.org/abs/2403.01128 ,  727kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01132
Date: Sat, 2 Mar 2024 08:27:05 GMT   (3417kb)

Title: MPIPN: A Multi Physics-Informed PointNet for solving parametric
  acoustic-structure systems
Authors: Chu Wang, Jinhong Wu, Yanzhi Wang, Zhijian Zha, Qi Zhou
Categories: cs.LG cs.SD eess.AS
Comments: The number of figures is 16. The number of tables is 5. The number of
  words is 9717
\\
  Machine learning is employed for solving physical systems governed by general
nonlinear partial differential equations (PDEs). However, complex multi-physics
systems such as acoustic-structure coupling are often described by a series of
PDEs that incorporate variable physical quantities, which are referred to as
parametric systems. There are lack of strategies for solving parametric systems
governed by PDEs that involve explicit and implicit quantities. In this paper,
a deep learning-based Multi Physics-Informed PointNet (MPIPN) is proposed for
solving parametric acoustic-structure systems. First, the MPIPN induces an
enhanced point-cloud architecture that encompasses explicit physical quantities
and geometric features of computational domains. Then, the MPIPN extracts local
and global features of the reconstructed point-cloud as parts of solving
criteria of parametric systems, respectively. Besides, implicit physical
quantities are embedded by encoding techniques as another part of solving
criteria. Finally, all solving criteria that characterize parametric systems
are amalgamated to form distinctive sequences as the input of the MPIPN, whose
outputs are solutions of systems. The proposed framework is trained by adaptive
physics-informed loss functions for corresponding computational domains. The
framework is generalized to deal with new parametric conditions of systems. The
effectiveness of the MPIPN is validated by applying it to solve steady
parametric acoustic-structure coupling systems governed by the Helmholtz
equations. An ablation experiment has been implemented to demonstrate the
efficacy of physics-informed impact with a minority of supervised data. The
proposed method yields reasonable precision across all computational domains
under constant parametric conditions and changeable combinations of parametric
conditions for acoustic-structure systems.
\\ ( https://arxiv.org/abs/2403.01132 ,  3417kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01133
Date: Sat, 2 Mar 2024 08:29:08 GMT   (6615kb,D)

Title: Evaluating Large Language Models as Virtual Annotators for Time-series
  Physical Sensing Data
Authors: Aritra Hota, Soumyajit Chatterjee, Sandip Chakraborty
Categories: cs.LG eess.SP
\\
  Traditional human-in-the-loop-based annotation for time-series data like
inertial data often requires access to alternate modalities like video or audio
from the environment. These alternate sources provide the necessary information
to the human annotator, as the raw numeric data is often too obfuscated even
for an expert. However, this traditional approach has many concerns surrounding
overall cost, efficiency, storage of additional modalities, time, scalability,
and privacy. Interestingly, recent large language models (LLMs) are also
trained with vast amounts of publicly available alphanumeric data, which allows
them to comprehend and perform well on tasks beyond natural language
processing. Naturally, this opens up a potential avenue to explore LLMs as
virtual annotators where the LLMs will be directly provided the raw sensor data
for annotation instead of relying on any alternate modality. Naturally, this
could mitigate the problems of the traditional human-in-the-loop approach.
Motivated by this observation, we perform a detailed study in this paper to
assess whether the state-of-the-art (SOTA) LLMs can be used as virtual
annotators for labeling time-series physical sensing data. To perform this in a
principled manner, we segregate the study into two major phases. In the first
phase, we investigate the challenges an LLM like GPT-4 faces in comprehending
raw sensor data. Considering the observations from phase 1, in the next phase,
we investigate the possibility of encoding the raw sensor data using SOTA SSL
approaches and utilizing the projected time-series data to get annotations from
the LLM. Detailed evaluation with four benchmark HAR datasets shows that
SSL-based encoding and metric-based guidance allow the LLM to make more
reasonable decisions and provide accurate annotations without requiring
computationally expensive fine-tuning or sophisticated prompt engineering.
\\ ( https://arxiv.org/abs/2403.01133 ,  6615kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01136
Date: Sat, 2 Mar 2024 08:40:07 GMT   (1121kb,D)

Title: LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition
  and Adaptive Quantization
Authors: Juntao Zhao, Borui Wan, Yanghua Peng, Haibin Lin, Chuan Wu
Categories: cs.LG cs.AI cs.DC
\\
  Recent breakthroughs in Large-scale language models (LLMs) have demonstrated
impressive performance on various tasks. The immense sizes of LLMs have led to
very high resource demand and cost for running the models. Though the models
are largely served using uniform high-caliber GPUs nowadays, utilizing a
heterogeneous cluster with a mix of available high- and low-capacity GPUs can
potentially substantially reduce the serving cost. There is a lack of designs
to support efficient LLM serving using a heterogeneous cluster, while the
current solutions focus on model partition and uniform compression among
homogeneous devices. This paper proposes LLM-PQ, a system that advocates
adaptive model quantization and phase-aware partition to improve LLM serving
efficiency on heterogeneous GPU clusters. We carefully decide on
mixed-precision model quantization together with phase-aware model partition
and micro-batch sizing in distributed LLM serving with an efficient algorithm,
to greatly enhance inference throughput while fulfilling user-specified model
quality targets. Extensive experiments on production inference workloads in 11
different clusters demonstrate that LLM-PQ achieves up to 2.88x (2.26x on
average) throughput improvement in inference, showing great advantages over
state-of-the-art works.
\\ ( https://arxiv.org/abs/2403.01136 ,  1121kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01147
Date: Sat, 2 Mar 2024 09:28:04 GMT   (2205kb)

Title: A Hybrid Model for Traffic Incident Detection based on Generative
  Adversarial Networks and Transformer Model
Authors: Xinying Lu, Doudou Zhang, Jianli Xiao
Categories: cs.LG cs.AI
Comments: 19 pages, 8 figures
\\
  In addition to enhancing traffic safety and facilitating prompt emergency
response, traffic incident detection plays an indispensable role in intelligent
transportation systems by providing real-time traffic status information. This
enables the realization of intelligent traffic control and management. Previous
research has identified that apart from employing advanced algorithmic models,
the effectiveness of detection is also significantly influenced by challenges
related to acquiring large datasets and addressing dataset imbalances. A hybrid
model combining transformer and generative adversarial networks (GANs) is
proposed to address these challenges. Experiments are conducted on four real
datasets to validate the superiority of the transformer in traffic incident
detection. Additionally, GANs are utilized to expand the dataset and achieve a
balanced ratio of 1:4, 2:3, and 1:1. The proposed model is evaluated against
the baseline model. The results demonstrate that the proposed model enhances
the dataset size, balances the dataset, and improves the performance of traffic
incident detection in various aspects.
\\ ( https://arxiv.org/abs/2403.01147 ,  2205kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01189
Date: Sat, 2 Mar 2024 12:06:42 GMT   (43875kb,D)

Title: Training Unbiased Diffusion Models From Biased Dataset
Authors: Yeongmin Kim, Byeonghu Na, Minsang Park, JoonHo Jang, Dongjun Kim,
  Wanmo Kang, Il-Chul Moon
Categories: cs.LG cs.CV
Comments: International Conference on Learning Representations (ICLR 2024)
\\
  With significant advancements in diffusion models, addressing the potential
risks of dataset bias becomes increasingly important. Since generated outputs
directly suffer from dataset bias, mitigating latent bias becomes a key factor
in improving sample quality and proportion. This paper proposes time-dependent
importance reweighting to mitigate the bias for the diffusion models. We
demonstrate that the time-dependent density ratio becomes more precise than
previous approaches, thereby minimizing error propagation in generative
learning. While directly applying it to score-matching is intractable, we
discover that using the time-dependent density ratio both for reweighting and
score correction can lead to a tractable form of the objective function to
regenerate the unbiased data density. Furthermore, we theoretically establish a
connection with traditional score-matching, and we demonstrate its convergence
to an unbiased distribution. The experimental evidence supports the usefulness
of the proposed method, which outperforms baselines including time-independent
importance reweighting on CIFAR-10, CIFAR-100, FFHQ, and CelebA with various
bias settings. Our code is available at https://github.com/alsdudrla10/TIW-DSM.
\\ ( https://arxiv.org/abs/2403.01189 ,  43875kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01203
Date: Sat, 2 Mar 2024 12:44:59 GMT   (2004kb,D)

Title: Pseudo-Label Calibration Semi-supervised Multi-Modal Entity Alignment
Authors: Luyao Wang and Pengnian Qi and Xigang Bao and Chunlai Zhou and Biao
  Qin
Categories: cs.LG cs.CL cs.DB
Comments: accepted by AAAI2024
\\
  Multi-modal entity alignment (MMEA) aims to identify equivalent entities
between two multi-modal knowledge graphs for integration. Unfortunately, prior
arts have attempted to improve the interaction and fusion of multi-modal
information, which have overlooked the influence of modal-specific noise and
the usage of labeled and unlabeled data in semi-supervised settings. In this
work, we introduce a Pseudo-label Calibration Multi-modal Entity Alignment
(PCMEA) in a semi-supervised way. Specifically, in order to generate holistic
entity representations, we first devise various embedding modules and attention
mechanisms to extract visual, structural, relational, and attribute features.
Different from the prior direct fusion methods, we next propose to exploit
mutual information maximization to filter the modal-specific noise and to
augment modal-invariant commonality. Then, we combine pseudo-label calibration
with momentum-based contrastive learning to make full use of the labeled and
unlabeled data, which improves the quality of pseudo-label and pulls aligned
entities closer. Finally, extensive experiments on two MMEA datasets
demonstrate the effectiveness of our PCMEA, which yields state-of-the-art
performance.
\\ ( https://arxiv.org/abs/2403.01203 ,  2004kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01204
Date: Sat, 2 Mar 2024 12:45:01 GMT   (1136kb)

Title: Stochastic gradient descent for streaming linear and rectified linear
  systems with Massart noise
Authors: Halyun Jeong, Deanna Needell, Elizaveta Rebrova
Categories: cs.LG cs.NA math.NA stat.ML
Comments: Submitted to a journal
MSC-class: 65F10, 60-XX
\\
  We propose SGD-exp, a stochastic gradient descent approach for linear and
ReLU regressions under Massart noise (adversarial semi-random corruption model)
for the fully streaming setting. We show novel nearly linear convergence
guarantees of SGD-exp to the true parameter with up to $50\%$ Massart
corruption rate, and with any corruption rate in the case of symmetric
oblivious corruptions. This is the first convergence guarantee result for
robust ReLU regression in the streaming setting, and it shows the improved
convergence rate over previous robust methods for $L_1$ linear regression due
to a choice of an exponentially decaying step size, known for its efficiency in
practice. Our analysis is based on the drift analysis of a discrete stochastic
process, which could also be interesting on its own.
\\ ( https://arxiv.org/abs/2403.01204 ,  1136kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01218
Date: Sat, 2 Mar 2024 14:22:40 GMT   (1432kb,D)

Title: Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense
  of Privacy
Authors: Jamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr Khalifa, Nicolas
  Papernot
Categories: cs.LG cs.CR
\\
  The high cost of model training makes it increasingly desirable to develop
techniques for unlearning. These techniques seek to remove the influence of a
training example without having to retrain the model from scratch. Intuitively,
once a model has unlearned, an adversary that interacts with the model should
no longer be able to tell whether the unlearned example was included in the
model's training set or not. In the privacy literature, this is known as
membership inference. In this work, we discuss adaptations of Membership
Inference Attacks (MIAs) to the setting of unlearning (leading to their
``U-MIA'' counterparts). We propose a categorization of existing U-MIAs into
``population U-MIAs'', where the same attacker is instantiated for all
examples, and ``per-example U-MIAs'', where a dedicated attacker is
instantiated for each example. We show that the latter category, wherein the
attacker tailors its membership prediction to each example under attack, is
significantly stronger. Indeed, our results show that the commonly used U-MIAs
in the unlearning literature overestimate the privacy protection afforded by
existing unlearning techniques on both vision and language models. Our
investigation reveals a large variance in the vulnerability of different
examples to per-example U-MIAs. In fact, several unlearning algorithms lead to
a reduced vulnerability for some, but not all, examples that we wish to
unlearn, at the expense of increasing it for other examples. Notably, we find
that the privacy protection for the remaining training examples may worsen as a
consequence of unlearning. We also discuss the fundamental difficulty of
equally protecting all examples using existing unlearning schemes, due to the
different rates at which examples are unlearned. We demonstrate that naive
attempts at tailoring unlearning stopping criteria to different examples fail
to alleviate these issues.
\\ ( https://arxiv.org/abs/2403.01218 ,  1432kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01221
Date: Sat, 2 Mar 2024 14:30:57 GMT   (33kb)

Title: A Two-Stage Algorithm for Cost-Efficient Multi-instance Counterfactual
  Explanations
Authors: Andr\'e Artelt and Andreas Gregoriades
Categories: cs.LG cs.AI
\\
  Counterfactual explanations constitute among the most popular methods for
analyzing the predictions of black-box systems since they can recommend
cost-efficient and actionable changes to the input to turn an undesired
system's output into a desired output. While most of the existing
counterfactual methods explain a single instance, several real-world use cases,
such as customer satisfaction, require the identification of a single
counterfactual that can satisfy multiple instances (e.g. customers)
simultaneously. In this work, we propose a flexible two-stage algorithm for
finding groups of instances along with cost-efficient multi-instance
counterfactual explanations. This is motivated by the fact that in most
previous works the aspect of finding such groups is not addressed.
\\ ( https://arxiv.org/abs/2403.01221 ,  33kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01232
Date: Sat, 2 Mar 2024 15:32:01 GMT   (472kb,D)

Title: Polynormer: Polynomial-Expressive Graph Transformer in Linear Time
Authors: Chenhui Deng, Zichao Yue, Zhiru Zhang
Categories: cs.LG cs.AI
Comments: Published as a conference paper at ICLR 2024
\\
  Graph transformers (GTs) have emerged as a promising architecture that is
theoretically more expressive than message-passing graph neural networks
(GNNs). However, typical GT models have at least quadratic complexity and thus
cannot scale to large graphs. While there are several linear GTs recently
proposed, they still lag behind GNN counterparts on several popular graph
datasets, which poses a critical concern on their practical expressivity. To
balance the trade-off between expressivity and scalability of GTs, we propose
Polynormer, a polynomial-expressive GT model with linear complexity. Polynormer
is built upon a novel base model that learns a high-degree polynomial on input
features. To enable the base model permutation equivariant, we integrate it
with graph topology and node features separately, resulting in local and global
equivariant attention models. Consequently, Polynormer adopts a linear
local-to-global attention scheme to learn high-degree equivariant polynomials
whose coefficients are controlled by attention scores. Polynormer has been
evaluated on $13$ homophilic and heterophilic datasets, including large graphs
with millions of nodes. Our extensive experiment results show that Polynormer
outperforms state-of-the-art GNN and GT baselines on most datasets, even
without the use of nonlinear activation functions.
\\ ( https://arxiv.org/abs/2403.01232 ,  472kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01234
Date: Sat, 2 Mar 2024 15:34:31 GMT   (3918kb)

Title: Active Deep Kernel Learning of Molecular Functionalities: Realizing
  Dynamic Structural Embeddings
Authors: Ayana Ghosh, Maxim Ziatdinov and, Sergei V. Kalinin
Categories: cs.LG physics.chem-ph physics.comp-ph physics.data-an
\\
  Exploring molecular spaces is crucial for advancing our understanding of
chemical properties and reactions, leading to groundbreaking innovations in
materials science, medicine, and energy. This paper explores an approach for
active learning in molecular discovery using Deep Kernel Learning (DKL), a
novel approach surpassing the limits of classical Variational Autoencoders
(VAEs). Employing the QM9 dataset, we contrast DKL with traditional VAEs, which
analyze molecular structures based on similarity, revealing limitations due to
sparse regularities in latent spaces. DKL, however, offers a more holistic
perspective by correlating structure with properties, creating latent spaces
that prioritize molecular functionality. This is achieved by recalculating
embedding vectors iteratively, aligning with the experimental availability of
target properties. The resulting latent spaces are not only better organized
but also exhibit unique characteristics such as concentrated maxima
representing molecular functionalities and a correlation between predictive
uncertainty and error. Additionally, the formation of exclusion regions around
certain compounds indicates unexplored areas with potential for groundbreaking
functionalities. This study underscores DKL's potential in molecular research,
offering new avenues for understanding and discovering molecular
functionalities beyond classical VAE limitations.
\\ ( https://arxiv.org/abs/2403.01234 ,  3918kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01242
Date: Sat, 2 Mar 2024 16:06:03 GMT   (1609kb,D)

Title: Augmenting Automation: Intent-Based User Instruction Classification with
  Machine Learning
Authors: Lochan Basyal, Bijay Gaudel
Categories: cs.LG cs.AI cs.CL cs.HC
Comments: 7 pages, 14 figures
\\
  Electric automation systems offer convenience and efficiency in controlling
electrical circuits and devices. Traditionally, these systems rely on
predefined commands for control, limiting flexibility and adaptability. In this
paper, we propose a novel approach to augment automation by introducing
intent-based user instruction classification using machine learning techniques.
Our system represents user instructions as intents, allowing for dynamic
control of electrical circuits without relying on predefined commands. Through
a machine learning model trained on a labeled dataset of user instructions, our
system classifies intents from user input, enabling a more intuitive and
adaptable control scheme. We present the design and implementation of our
intent-based electric automation system, detailing the development of the
machine learning model for intent classification. Experimental results
demonstrate the effectiveness of our approach in enhancing user experience and
expanding the capabilities of electric automation systems. Our work contributes
to the advancement of smart technologies by providing a more seamless
interaction between users and their environments.
\\ ( https://arxiv.org/abs/2403.01242 ,  1609kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01245
Date: Sat, 2 Mar 2024 16:11:58 GMT   (1129kb,D)

Title: AcME-AD: Accelerated Model Explanations for Anomaly Detection
Authors: Valentina Zaccaria, David Dandolo, Chiara Masiero, Gian Antonio Susto
Categories: cs.LG
\\
  Pursuing fast and robust interpretability in Anomaly Detection is crucial,
especially due to its significance in practical applications. Traditional
Anomaly Detection methods excel in outlier identification but are often
black-boxes, providing scant insights into their decision-making process. This
lack of transparency compromises their reliability and hampers their adoption
in scenarios where comprehending the reasons behind anomaly detection is vital.
At the same time, getting explanations quickly is paramount in practical
scenarios. To bridge this gap, we present AcME-AD, a novel approach rooted in
Explainable Artificial Intelligence principles, designed to clarify Anomaly
Detection models for tabular data. AcME-AD transcends the constraints of
model-specific or resource-heavy explainability techniques by delivering a
model-agnostic, efficient solution for interoperability. It offers local
feature importance scores and a what-if analysis tool, shedding light on the
factors contributing to each anomaly, thus aiding root cause analysis and
decision-making. This paper elucidates AcME-AD's foundation, its benefits over
existing methods, and validates its effectiveness with tests on both synthetic
and real datasets. AcME-AD's implementation and experiment replication code is
accessible in a public repository.
\\ ( https://arxiv.org/abs/2403.01245 ,  1129kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01267
Date: Sat, 2 Mar 2024 17:10:44 GMT   (14202kb,D)

Title: Dissecting Language Models: Machine Unlearning via Selective Pruning
Authors: Nicholas Pochinkov and Nandi Schoots
Categories: cs.LG cs.CL
\\
  Understanding and shaping the behaviour of Large Language Models (LLMs) is
increasingly important as applications become more powerful and more frequently
adopted. This paper introduces a machine unlearning method specifically
designed for LLMs. We introduce a selective pruning method for LLMs that
removes neurons based on their relative importance on a targeted capability
compared to overall network performance. This approach is a compute- and
data-efficient method for identifying and removing neurons that enable specific
behaviours. Our findings reveal that both feed-forward and attention neurons in
LLMs are specialized; that is, for specific tasks, certain neurons are more
crucial than others.
\\ ( https://arxiv.org/abs/2403.01267 ,  14202kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01268
Date: Sat, 2 Mar 2024 17:12:32 GMT   (3787kb,D)

Title: Defending Against Data Reconstruction Attacks in Federated Learning: An
  Information Theory Approach
Authors: Qi Tan, Qi Li, Yi Zhao, Zhuotao Liu, Xiaobing Guo, Ke Xu
Categories: cs.LG cs.CR cs.DC
Comments: Accepted by USENIX Security '24
\\
  Federated Learning (FL) trains a black-box and high-dimensional model among
different clients by exchanging parameters instead of direct data sharing,
which mitigates the privacy leak incurred by machine learning. However, FL
still suffers from membership inference attacks (MIA) or data reconstruction
attacks (DRA). In particular, an attacker can extract the information from
local datasets by constructing DRA, which cannot be effectively throttled by
existing techniques, e.g., Differential Privacy (DP).
  In this paper, we aim to ensure a strong privacy guarantee for FL under DRA.
We prove that reconstruction errors under DRA are constrained by the
information acquired by an attacker, which means that constraining the
transmitted information can effectively throttle DRA. To quantify the
information leakage incurred by FL, we establish a channel model, which depends
on the upper bound of joint mutual information between the local dataset and
multiple transmitted parameters. Moreover, the channel model indicates that the
transmitted information can be constrained through data space operation, which
can improve training efficiency and the model accuracy under constrained
information. According to the channel model, we propose algorithms to constrain
the information transmitted in a single round of local training. With a limited
number of training rounds, the algorithms ensure that the total amount of
transmitted information is limited. Furthermore, our channel model can be
applied to various privacy-enhancing techniques (such as DP) to enhance privacy
guarantees against DRA. Extensive experiments with real-world datasets validate
the effectiveness of our methods.
\\ ( https://arxiv.org/abs/2403.01268 ,  3787kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01272
Date: Sat, 2 Mar 2024 17:28:55 GMT   (592kb,D)

Title: Can a Confident Prior Replace a Cold Posterior?
Authors: Martin Marek, Brooks Paige, Pavel Izmailov
Categories: cs.LG stat.ML
\\
  Benchmark datasets used for image classification tend to have very low levels
of label noise. When Bayesian neural networks are trained on these datasets,
they often underfit, misrepresenting the aleatoric uncertainty of the data. A
common solution is to cool the posterior, which improves fit to the training
data but is challenging to interpret from a Bayesian perspective. We explore
whether posterior tempering can be replaced by a confidence-inducing prior
distribution. First, we introduce a "DirClip" prior that is practical to sample
and nearly matches the performance of a cold posterior. Second, we introduce a
"confidence prior" that directly approximates a cold likelihood in the limit of
decreasing temperature but cannot be easily sampled. Lastly, we provide several
general insights into confidence-inducing priors, such as when they might
diverge and how fine-tuning can mitigate numerical instability.
\\ ( https://arxiv.org/abs/2403.01272 ,  592kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01273
Date: Sat, 2 Mar 2024 17:29:22 GMT   (2266kb,D)

Title: NoMAD-Attention: Efficient LLM Inference on CPUs Through
  Multiply-add-free Attention
Authors: Tianyi Zhang, Jonah Wonkyu Yi, Bowen Yao, Zhaozhuo Xu, Anshumali
  Shrivastava
Categories: cs.LG cs.AI cs.CL
\\
  Large language model inference on Central Processing Units (CPU) is
challenging due to the vast quantities of expensive Multiply-Add (MAD) matrix
operations in the attention computations. In this paper, we argue that there is
a rare gem in modern CPUs, Single-Instruction-Multiple-Data (SIMD) registers,
which allow for ultra-low-latency lookups in batch. We leverage this unique
capability of CPUs to propose NoMAD-Attention, an efficient attention algorithm
that replaces MAD operations with in-register lookups. Through hardware-aware
algorithmic designs, NoMAD-Attention achieves the computation of attention
scores using repeated fast accesses to SIMD registers despite their highly
limited sizes. Moreover, NoMAD-Attention works with pre-trained attention-based
LLMs without model finetuning. Empirical evaluations demonstrate that
NoMAD-Attention maintains the quality of the original LLMs well, and speeds up
the 4-bit quantized LLaMA-7B-based model by up to 2$\times$ at 16k context
length. Our results are reproducible at
https://github.com/tonyzhang617/nomad-dist.
\\ ( https://arxiv.org/abs/2403.01273 ,  2266kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01306
Date: Sat, 2 Mar 2024 20:36:10 GMT   (55404kb,D)

Title: ICC: Quantifying Image Caption Concreteness for Multimodal Dataset
  Curation
Authors: Moran Yanuka, Morris Alper, Hadar Averbuch-Elor and Raja Giryes
Categories: cs.LG cs.CV
\\
  Web-scale training on paired text-image data is becoming increasingly central
to multimodal learning, but is challenged by the highly noisy nature of
datasets in the wild. Standard data filtering approaches succeed in removing
mismatched text-image pairs, but permit semantically related but highly
abstract or subjective text. These approaches lack the fine-grained ability to
isolate the most concrete samples that provide the strongest signal for
learning in a noisy dataset. In this work, we propose a new metric, image
caption concreteness, that evaluates caption text without an image reference to
measure its concreteness and relevancy for use in multimodal learning. Our
approach leverages strong foundation models for measuring visual-semantic
information loss in multimodal representations. We demonstrate that this
strongly correlates with human evaluation of concreteness in both single-word
and sentence-level texts. Moreover, we show that curation using ICC complements
existing approaches: It succeeds in selecting the highest quality samples from
multimodal web-scale datasets to allow for efficient training in
resource-constrained settings.
\\ ( https://arxiv.org/abs/2403.01306 ,  55404kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01315
Date: Sat, 2 Mar 2024 21:22:46 GMT   (68kb)

Title: Near-optimal Per-Action Regret Bounds for Sleeping Bandits
Authors: Quan Nguyen, Nishant A. Mehta
Categories: cs.LG stat.ML
Comments: Accepted to AISTATS 2024
\\
  We derive near-optimal per-action regret bounds for sleeping bandits, in
which both the sets of available arms and their losses in every round are
chosen by an adversary. In a setting with $K$ total arms and at most $A$
available arms in each round over $T$ rounds, the best known upper bound is
$O(K\sqrt{TA\ln{K}})$, obtained indirectly via minimizing internal sleeping
regrets. Compared to the minimax $\Omega(\sqrt{TA})$ lower bound, this upper
bound contains an extra multiplicative factor of $K\ln{K}$. We address this gap
by directly minimizing the per-action regret using generalized versions of
EXP3, EXP3-IX and FTRL with Tsallis entropy, thereby obtaining near-optimal
bounds of order $O(\sqrt{TA\ln{K}})$ and $O(\sqrt{T\sqrt{AK}})$. We extend our
results to the setting of bandits with advice from sleeping experts,
generalizing EXP4 along the way. This leads to new proofs for a number of
existing adaptive and tracking regret bounds for standard non-sleeping bandits.
Extending our results to the bandit version of experts that report their
confidences leads to new bounds for the confidence regret that depends
primarily on the sum of experts' confidences. We prove a lower bound, showing
that for any minimax optimal algorithms, there exists an action whose regret is
sublinear in $T$ but linear in the number of its active rounds.
\\ ( https://arxiv.org/abs/2403.01315 ,  68kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01317
Date: Sat, 2 Mar 2024 21:33:23 GMT   (5395kb,D)

Title: Less is More: Hop-Wise Graph Attention for Scalable and Generalizable
  Learning on Circuits
Authors: Chenhui Deng, Zichao Yue, Cunxi Yu, Gokce Sarar, Ryan Carey, Rajeev
  Jain, Zhiru Zhang
Categories: cs.LG cs.AR
Comments: Published as a conference paper at Design Automation Conference (DAC)
  2024
\\
  While graph neural networks (GNNs) have gained popularity for learning
circuit representations in various electronic design automation (EDA) tasks,
they face challenges in scalability when applied to large graphs and exhibit
limited generalizability to new designs. These limitations make them less
practical for addressing large-scale, complex circuit problems. In this work we
propose HOGA, a novel attention-based model for learning circuit
representations in a scalable and generalizable manner. HOGA first computes
hop-wise features per node prior to model training. Subsequently, the hop-wise
features are solely used to produce node representations through a gated
self-attention module, which adaptively learns important features among
different hops without involving the graph topology. As a result, HOGA is
adaptive to various structures across different circuits and can be efficiently
trained in a distributed manner. To demonstrate the efficacy of HOGA, we
consider two representative EDA tasks: quality of results (QoR) prediction and
functional reasoning. Our experimental results indicate that (1) HOGA reduces
estimation error over conventional GNNs by 46.76% for predicting QoR after
logic synthesis; (2) HOGA improves 10.0% reasoning accuracy over GNNs for
identifying functional blocks on unseen gate-level netlists after complex
technology mapping; (3) The training time for HOGA almost linearly decreases
with an increase in computing resources.
\\ ( https://arxiv.org/abs/2403.01317 ,  5395kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01329
Date: Sat, 2 Mar 2024 22:27:44 GMT   (26234kb,D)

Title: Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow
  Models
Authors: Neta Shaul, Uriel Singer, Ricky T. Q. Chen, Matthew Le, Ali Thabet,
  Albert Pumarola, Yaron Lipman
Categories: cs.LG cs.AI cs.CV
\\
  This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver
distillation approach to improve sample efficiency of Diffusion and Flow
models. BNS solvers are based on a family of non-stationary solvers that
provably subsumes existing numerical ODE solvers and consequently demonstrate
considerable improvement in sample approximation (PSNR) over these baselines.
Compared to model distillation, BNS solvers benefit from a tiny parameter space
($<$200 parameters), fast optimization (two orders of magnitude faster),
maintain diversity of samples, and in contrast to previous solver distillation
approaches nearly close the gap from standard distillation methods such as
Progressive Distillation in the low-medium NFE regime. For example, BNS solver
achieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64. We
experimented with BNS solvers for conditional image generation, text-to-image
generation, and text-2-audio generation showing significant improvement in
sample approximation (PSNR) in all.
\\ ( https://arxiv.org/abs/2403.01329 ,  26234kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01339
Date: Sat, 2 Mar 2024 23:19:10 GMT   (73kb)

Title: Uniform $\mathcal{C}^k$ Approximation of $G$-Invariant and Antisymmetric
  Functions, Embedding Dimensions, and Polynomial Representations
Authors: Soumya Ganguly, Khoa Tran, Rahul Sarkar
Categories: cs.LG math.RT
Comments: 38 pages
MSC-class: 05E10
ACM-class: I.2.4; I.2.6; I.2.0
\\
  For any subgroup $G$ of the symmetric group $\mathcal{S}_n$ on $n$ symbols,
we present results for the uniform $\mathcal{C}^k$ approximation of
$G$-invariant functions by $G$-invariant polynomials. For the case of totally
symmetric functions ($G = \mathcal{S}_n$), we show that this gives rise to the
sum-decomposition Deep Sets ansatz of Zaheer et al. (2018), where both the
inner and outer functions can be chosen to be smooth, and moreover, the inner
function can be chosen to be independent of the target function being
approximated. In particular, we show that the embedding dimension required is
independent of the regularity of the target function, the accuracy of the
desired approximation, as well as $k$. Next, we show that a similar procedure
allows us to obtain a uniform $\mathcal{C}^k$ approximation of antisymmetric
functions as a sum of $K$ terms, where each term is a product of a smooth
totally symmetric function and a smooth antisymmetric homogeneous polynomial of
degree at most $\binom{n}{2}$. We also provide upper and lower bounds on $K$
and show that $K$ is independent of the regularity of the target function, the
desired approximation accuracy, and $k$.
\\ ( https://arxiv.org/abs/2403.01339 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01344
Date: Sat, 2 Mar 2024 23:37:16 GMT   (3988kb,D)

Title: Mitigating the Bias in the Model for Continual Test-Time Adaptation
Authors: Inseop Chung, Kyomin Hwang, Jayeon Yoo, Nojun Kwak
Categories: cs.LG cs.CV
\\
  Continual Test-Time Adaptation (CTA) is a challenging task that aims to adapt
a source pre-trained model to continually changing target domains. In the CTA
setting, a model does not know when the target domain changes, thus facing a
drastic change in the distribution of streaming inputs during the test-time.
The key challenge is to keep adapting the model to the continually changing
target domains in an online manner. We find that a model shows highly biased
predictions as it constantly adapts to the chaining distribution of the target
data. It predicts certain classes more often than other classes, making
inaccurate over-confident predictions. This paper mitigates this issue to
improve performance in the CTA scenario. To alleviate the bias issue, we make
class-wise exponential moving average target prototypes with reliable target
samples and exploit them to cluster the target features class-wisely. Moreover,
we aim to align the target distributions to the source distribution by
anchoring the target feature to its corresponding source prototype. With
extensive experiments, our proposed method achieves noteworthy performance gain
when applied on top of existing CTA methods without substantial adaptation time
overhead.
\\ ( https://arxiv.org/abs/2403.01344 ,  3988kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01346
Date: Sat, 2 Mar 2024 23:53:24 GMT   (759kb,D)

Title: Improve Cost Efficiency of Active Learning over Noisy Dataset
Authors: Zan-Kai Chong, Hiroyuki Ohsaki, and Bryan Ng
Categories: cs.LG
Comments: 6 pages, 9 figures, conference
\\
  Active learning is a learning strategy whereby the machine learning algorithm
actively identifies and labels data points to optimize its learning. This
strategy is particularly effective in domains where an abundance of unlabeled
data exists, but the cost of labeling these data points is prohibitively
expensive. In this paper, we consider cases of binary classification, where
acquiring a positive instance incurs a significantly higher cost compared to
that of negative instances. For example, in the financial industry, such as in
money-lending businesses, a defaulted loan constitutes a positive event leading
to substantial financial loss. To address this issue, we propose a shifted
normal distribution sampling function that samples from a wider range than
typical uncertainty sampling. Our simulation underscores that our proposed
sampling function limits both noisy and positive label selection, delivering
between 20% and 32% improved cost efficiency over different test datasets.
\\ ( https://arxiv.org/abs/2403.01346 ,  759kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01348
Date: Sun, 3 Mar 2024 00:01:29 GMT   (990kb)

Title: SANGRIA: Stacked Autoencoder Neural Networks with Gradient Boosting for
  Indoor Localization
Authors: Danish Gufran, Saideep Tiku, Sudeep Pasricha
Categories: cs.LG cs.AI eess.SP
DOI: 10.1109/LES.2023.3279017
\\
  Indoor localization is a critical task in many embedded applications, such as
asset tracking, emergency response, and realtime navigation. In this article,
we propose a novel fingerprintingbased framework for indoor localization called
SANGRIA that uses stacked autoencoder neural networks with gradient boosted
trees. Our approach is designed to overcome the device heterogeneity challenge
that can create uncertainty in wireless signal measurements across embedded
devices used for localization. We compare SANGRIA to several state-of-the-art
frameworks and demonstrate 42.96% lower average localization error across
diverse indoor locales and heterogeneous devices.
\\ ( https://arxiv.org/abs/2403.01348 ,  990kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01352
Date: Sun, 3 Mar 2024 00:14:12 GMT   (2079kb)

Title: Improving Uncertainty Sampling with Bell Curve Weight Function
Authors: Zan-Kai Chong, Hiroyuki Ohsaki, and Bok-Min Goi
Categories: cs.LG
Comments: 9 pages, 9 figures, journal
Journal-ref: Zan-Kai Chong, Hiroyuki Ohsaki, Bok-Min Goi, "Improving
  Uncertainty Sampling with Bell Curve Weight Function", International Journal
  of Applied Physics and Mathematics, Vol. 13, No. 4, pp. 44-52, 2023
DOI: 10.17706/ijapm.2023.13.4.44-52
\\
  Typically, a supervised learning model is trained using passive learning by
randomly selecting unlabelled instances to annotate. This approach is effective
for learning a model, but can be costly in cases where acquiring labelled
instances is expensive. For example, it can be time-consuming to manually
identify spam mails (labelled instances) from thousands of emails (unlabelled
instances) flooding an inbox during initial data collection. Generally, we
answer the above scenario with uncertainty sampling, an active learning method
that improves the efficiency of supervised learning by using fewer labelled
instances than passive learning. Given an unlabelled data pool, uncertainty
sampling queries the labels of instances where the predicted probabilities, p,
fall into the uncertainty region, i.e., $p \approx 0.5$. The newly acquired
labels are then added to the existing labelled data pool to learn a new model.
Nonetheless, the performance of uncertainty sampling is susceptible to the area
of unpredictable responses (AUR) and the nature of the dataset. It is difficult
to determine whether to use passive learning or uncertainty sampling without
prior knowledge of a new dataset. To address this issue, we propose bell curve
sampling, which employs a bell curve weight function to acquire new labels.
With the bell curve centred at p=0.5, bell curve sampling selects instances
whose predicted values are in the uncertainty area most of the time without
neglecting the rest. Simulation results show that, most of the time bell curve
sampling outperforms uncertainty sampling and passive learning in datasets of
different natures and with AUR.
\\ ( https://arxiv.org/abs/2403.01352 ,  2079kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01361
Date: Sun, 3 Mar 2024 01:33:47 GMT   (865kb,D)

Title: Bandit Profit-maximization for Targeted Marketing
Authors: Joon Suk Huh, Ellen Vitercik, Kirthevasan Kandasamy
Categories: cs.LG cs.GT econ.GN q-fin.EC q-fin.GN
\\
  We study a sequential profit-maximization problem, optimizing for both price
and ancillary variables like marketing expenditures. Specifically, we aim to
maximize profit over an arbitrary sequence of multiple demand curves, each
dependent on a distinct ancillary variable, but sharing the same price. A
prototypical example is targeted marketing, where a firm (seller) wishes to
sell a product over multiple markets. The firm may invest different marketing
expenditures for different markets to optimize customer acquisition, but must
maintain the same price across all markets. Moreover, markets may have
heterogeneous demand curves, each responding to prices and marketing
expenditures differently. The firm's objective is to maximize its gross profit,
the total revenue minus marketing costs.
  Our results are near-optimal algorithms for this class of problems in an
adversarial bandit setting, where demand curves are arbitrary non-adaptive
sequences, and the firm observes only noisy evaluations of chosen points on the
demand curves. We prove a regret upper bound of
$\widetilde{\mathcal{O}}\big(nT^{3/4}\big)$ and a lower bound of
$\Omega\big((nT)^{3/4}\big)$ for monotonic demand curves, and a regret bound of
$\widetilde{\Theta}\big(nT^{2/3}\big)$ for demands curves that are monotonic in
price and concave in the ancillary variables.
\\ ( https://arxiv.org/abs/2403.01361 ,  865kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01384
Date: Sun, 3 Mar 2024 03:27:07 GMT   (738kb)

Title: On the Compressibility of Quantized Large Language Models
Authors: Yu Mao, Weilan Wang, Hongchao Du, Nan Guan, and Chun Jason Xue
Categories: cs.LG cs.AI cs.CL
\\
  Deploying Large Language Models (LLMs) on edge or mobile devices offers
significant benefits, such as enhanced data privacy and real-time processing
capabilities. However, it also faces critical challenges due to the substantial
memory requirement of LLMs. Quantization is an effective way of reducing the
model size while maintaining good performance. However, even after
quantization, LLMs may still be too big to fit entirely into the limited memory
of edge or mobile devices and have to be partially loaded from the storage to
complete the inference. In this case, the I/O latency of model loading becomes
the bottleneck of the LLM inference latency. In this work, we take a
preliminary step of studying applying data compression techniques to reduce
data movement and thus speed up the inference of quantized LLM on
memory-constrained devices. In particular, we discussed the compressibility of
quantized LLMs, the trade-off between the compressibility and performance of
quantized LLMs, and opportunities to optimize both of them jointly.
\\ ( https://arxiv.org/abs/2403.01384 ,  738kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01387
Date: Sun, 3 Mar 2024 03:52:27 GMT   (2331kb,D)

Title: A Comprehensive Survey of Federated Transfer Learning: Challenges,
  Methods and Applications
Authors: Wei Guo, Fuzhen Zhuang, Xiao Zhang, Yiqi Tong, Jin Dong
Categories: cs.LG cs.DC
\\
  Federated learning (FL) is a novel distributed machine learning paradigm that
enables participants to collaboratively train a centralized model with privacy
preservation by eliminating the requirement of data sharing. In practice, FL
often involves multiple participants and requires the third party to aggregate
global information to guide the update of the target participant. Therefore,
many FL methods do not work well due to the training and test data of each
participant may not be sampled from the same feature space and the same
underlying distribution. Meanwhile, the differences in their local devices
(system heterogeneity), the continuous influx of online data (incremental
data), and labeled data scarcity may further influence the performance of these
methods. To solve this problem, federated transfer learning (FTL), which
integrates transfer learning (TL) into FL, has attracted the attention of
numerous researchers. However, since FL enables a continuous share of knowledge
among participants with each communication round while not allowing local data
to be accessed by other participants, FTL faces many unique challenges that are
not present in TL. In this survey, we focus on categorizing and reviewing the
current progress on federated transfer learning, and outlining corresponding
solutions and applications. Furthermore, the common setting of FTL scenarios,
available datasets, and significant related research are summarized in this
survey.
\\ ( https://arxiv.org/abs/2403.01387 ,  2331kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01389
Date: Sun, 3 Mar 2024 04:21:21 GMT   (770kb,D)

Title: Fusion of Gaussian Processes Predictions with Monte Carlo Sampling
Authors: Marzieh Ajirak, Daniel Waxman, Fernando Llorente, Petar M. Djuric
Categories: cs.LG stat.ML
\\
  In science and engineering, we often work with models designed for accurate
prediction of variables of interest. Recognizing that these models are
approximations of reality, it becomes desirable to apply multiple models to the
same data and integrate their outcomes. In this paper, we operate within the
Bayesian paradigm, relying on Gaussian processes as our models. These models
generate predictive probability density functions (pdfs), and the objective is
to integrate them systematically, employing both linear and log-linear pooling.
We introduce novel approaches for log-linear pooling, determining
input-dependent weights for the predictive pdfs of the Gaussian processes. The
aggregation of the pdfs is realized through Monte Carlo sampling, drawing
samples of weights from their posterior. The performance of these methods, as
well as those based on linear pooling, is demonstrated using a synthetic
dataset.
\\ ( https://arxiv.org/abs/2403.01389 ,  770kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01400
Date: Sun, 3 Mar 2024 05:29:49 GMT   (960kb,D)

Title: Decoupling Weighing and Selecting for Integrating Multiple Graph
  Pre-training Tasks
Authors: Tianyu Fan, Lirong Wu, Yufei Huang, Haitao Lin, Cheng Tan, Zhangyang
  Gao, Stan Z. Li
Categories: cs.LG cs.AI
Comments: Published as a conference paper at ICLR 2024
\\
  Recent years have witnessed the great success of graph pre-training for graph
representation learning. With hundreds of graph pre-training tasks proposed,
integrating knowledge acquired from multiple pre-training tasks has become a
popular research topic. In this paper, we identify two important collaborative
processes for this topic: (1) select: how to select an optimal task combination
from a given task pool based on their compatibility, and (2) weigh: how to
weigh the selected tasks based on their importance. While there currently has
been a lot of work focused on weighing, comparatively little effort has been
devoted to selecting. This paper proposes a novel instance-level framework for
integrating multiple graph pre-training tasks, Weigh And Select (WAS), where
the two collaborative processes, weighing and selecting, are combined by
decoupled siamese networks. Specifically, it first adaptively learns an optimal
combination of tasks for each instance from a given task pool, based on which a
customized instance-level task weighing strategy is learned. Extensive
experiments on 16 graph datasets across node-level and graph-level downstream
tasks have demonstrated that by combining a few simple but classical tasks, WAS
can achieve comparable performance to other leading counterparts. The code is
available at https://github.com/TianyuFan0504/WAS.
\\ ( https://arxiv.org/abs/2403.01400 ,  960kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01417
Date: Sun, 3 Mar 2024 07:12:37 GMT   (32861kb,D)

Title: Asyn2F: An Asynchronous Federated Learning Framework with Bidirectional
  Model Aggregation
Authors: Tien-Dung Cao, Nguyen T. Vuong, Thai Q. Le, Hoang V.N. Dao, Tram
  Truong-Huu
Categories: cs.LG cs.DC
\\
  In federated learning, the models can be trained synchronously or
asynchronously. Many research works have focused on developing an aggregation
method for the server to aggregate multiple local models into the global model
with improved performance. They ignore the heterogeneity of the training
workers, which causes the delay in the training of the local models, leading to
the obsolete information issue. In this paper, we design and develop Asyn2F, an
Asynchronous Federated learning Framework with bidirectional model aggregation.
By bidirectional model aggregation, Asyn2F, on one hand, allows the server to
asynchronously aggregate multiple local models and results in a new global
model. On the other hand, it allows the training workers to aggregate the new
version of the global model into the local model, which is being trained even
in the middle of a training epoch. We develop Asyn2F considering the practical
implementation requirements such as using cloud services for model storage and
message queuing protocols for communications. Extensive experiments with
different datasets show that the models trained by Asyn2F achieve higher
performance compared to the state-of-the-art techniques. The experiments also
demonstrate the effectiveness, practicality, and scalability of Asyn2F, making
it ready for deployment in real scenarios.
\\ ( https://arxiv.org/abs/2403.01417 ,  32861kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01420
Date: Sun, 3 Mar 2024 07:38:24 GMT   (105kb,D)

Title: The Implicit Bias of Heterogeneity towards Invariance and Causality
Authors: Yang Xu, Yihong Gu, Cong Fang
Categories: cs.LG math.OC
\\
  It is observed empirically that the large language models (LLM), trained with
a variant of regression loss using numerous corpus from the Internet, can
unveil causal associations to some extent. This is contrary to the traditional
wisdom that ``association is not causation'' and the paradigm of traditional
causal inference in which prior causal knowledge should be carefully
incorporated into the design of methods. It is a mystery why causality, in a
higher layer of understanding, can emerge from the regression task that pursues
associations. In this paper, we claim the emergence of causality from
association-oriented training can be attributed to the coupling effects from
the heterogeneity of the source data, stochasticity of training algorithms, and
over-parameterization of the learning models. We illustrate such an intuition
using a simple but insightful model that learns invariance, a quasi-causality,
using regression loss. To be specific, we consider multi-environment low-rank
matrix sensing problems where the unknown r-rank ground-truth d*d matrices
diverge across the environments but contain a lower-rank invariant, causal
part. In this case, running pooled gradient descent will result in biased
solutions that only learn associations in general. We show that running
large-batch Stochastic Gradient Descent, whose each batch being linear
measurement samples randomly selected from a certain environment, can
successfully drive the solution towards the invariant, causal solution under
certain conditions. This step is related to the relatively strong heterogeneity
of the environments, the large step size and noises in the optimization
algorithm, and the over-parameterization of the model. In summary, we unveil
another implicit bias that is a result of the symbiosis between the
heterogeneity of data and modern algorithms, which is, to the best of our
knowledge, first in the literature.
\\ ( https://arxiv.org/abs/2403.01420 ,  105kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01426
Date: Sun, 3 Mar 2024 07:52:10 GMT   (14386kb,D)

Title: Introduction to Algogens
Authors: Amir Shachar
Categories: cs.LG
\\
  This book introduces the concept of Algogens, a promising integration of
generative AI with traditional algorithms aimed at improving problem-solving
techniques across various fields. It provides an accessible overview of how
Algogens combine AI's innovative potential with algorithms' reliability to
tackle complex challenges more effectively than either could alone.
  The text explores the basics of Algogens, their development, applications,
and advantages, such as better adaptability and efficiency. Through examples
and case studies, readers will learn about Algogens' practical uses today and
their potential for future cybersecurity, healthcare, and environmental science
innovation.
  Acknowledging new technologies' challenges and ethical considerations, the
book offers a balanced look at the prospects and obstacles facing Algogens. It
invites a broad audience, including experts and newcomers, to engage with the
topic and consider Algogens' role in advancing our problem-solving
capabilities.
  This work is presented as a starting point for anyone interested in the
intersection of AI and algorithms, encouraging further exploration and
discussion on this emerging field. It aims to spark curiosity and contribute to
the ongoing conversation about how technology can evolve to meet the complex
demands of the AI era.
\\ ( https://arxiv.org/abs/2403.01426 ,  14386kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01430
Date: Sun, 3 Mar 2024 07:56:55 GMT   (22321kb,D)

Title: On Diffusion Process in SE(3)-invariant Space
Authors: Zihan Zhou, Ruiying Liu, Jiachen Zheng, Xiaoxue Wang, Tianshu Yu
Categories: cs.LG
\\
  Sampling viable 3D structures (e.g., molecules and point clouds) with
SE(3)-invariance using diffusion-based models proved promising in a variety of
real-world applications, wherein SE(3)-invariant properties can be naturally
characterized by the inter-point distance manifold. However, due to the
non-trivial geometry, we still lack a comprehensive understanding of the
diffusion mechanism within such SE(3)-invariant space. This study addresses
this gap by mathematically delineating the diffusion mechanism under
SE(3)-invariance, via zooming into the interaction behavior between coordinates
and the inter-point distance manifold through the lens of differential
geometry. Upon this analysis, we propose accurate and projection-free diffusion
SDE and ODE accordingly. Such formulations enable enhancing the performance and
the speed of generation pathways; meanwhile offering valuable insights into
other systems incorporating SE(3)-invariance.
\\ ( https://arxiv.org/abs/2403.01430 ,  22321kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01438
Date: Sun, 3 Mar 2024 08:24:39 GMT   (3942kb,D)

Title: Privacy-Preserving Collaborative Split Learning Framework for Smart Grid
  Load Forecasting
Authors: Asif Iqbal, Prosanta Gope, Biplab Sikdar
Categories: cs.LG
\\
  Accurate load forecasting is crucial for energy management, infrastructure
planning, and demand-supply balancing. Smart meter data availability has led to
the demand for sensor-based load forecasting. Conventional ML allows training a
single global model using data from multiple smart meters requiring data
transfer to a central server, raising concerns for network requirements,
privacy, and security. We propose a split learning-based framework for load
forecasting to alleviate this issue. We split a deep neural network model into
two parts, one for each Grid Station (GS) responsible for an entire
neighbourhood's smart meters and the other for the Service Provider (SP).
Instead of sharing their data, client smart meters use their respective GSs'
model split for forward pass and only share their activations with the GS.
Under this framework, each GS is responsible for training a personalized model
split for their respective neighbourhoods, whereas the SP can train a single
global or personalized model for each GS. Experiments show that the proposed
models match or exceed a centrally trained model's performance and generalize
well. Privacy is analyzed by assessing information leakage between data and
shared activations of the GS model split. Additionally, differential privacy
enhances local data privacy while examining its impact on performance. A
transformer model is used as our base learner.
\\ ( https://arxiv.org/abs/2403.01438 ,  3942kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01460
Date: Sun, 3 Mar 2024 09:43:23 GMT   (871kb,D)

Title: One-Step Multi-View Clustering Based on Transition Probability
Authors: Wenhui Zhao, Quanxue Gao, Guangfei Li, Cheng Deng, Ming Yang
Categories: cs.LG
Comments: 8 pages
\\
  The large-scale multi-view clustering algorithms, based on the anchor graph,
have shown promising performance and efficiency and have been extensively
explored in recent years. Despite their successes, current methods lack
interpretability in the clustering process and do not sufficiently consider the
complementary information across different views. To address these
shortcomings, we introduce the One-Step Multi-View Clustering Based on
Transition Probability (OSMVC-TP). This method adopts a probabilistic approach,
which leverages the anchor graph, representing the transition probabilities
from samples to anchor points. Our method directly learns the transition
probabilities from anchor points to categories, and calculates the transition
probabilities from samples to categories, thus obtaining soft label matrices
for samples and anchor points, enhancing the interpretability of clustering.
Furthermore, to maintain consistency in labels across different views, we apply
a Schatten p-norm constraint on the tensor composed of the soft labels. This
approach effectively harnesses the complementary information among the views.
Extensive experiments have confirmed the effectiveness and robustness of
OSMVC-TP.
\\ ( https://arxiv.org/abs/2403.01460 ,  871kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01467
Date: Sun, 3 Mar 2024 10:23:08 GMT   (3524kb,D)

Title: Collaborate to Adapt: Source-Free Graph Domain Adaptation via
  Bi-directional Adaptation
Authors: Zhen Zhang, Meihan Liu, Anhui Wang, Hongyang Chen, Zhao Li, Jiajun Bu,
  Bingsheng He
Categories: cs.LG cs.AI
Comments: Accepted by WWW-2024
DOI: 10.1145/3589334.3645507
\\
  Unsupervised Graph Domain Adaptation (UGDA) has emerged as a practical
solution to transfer knowledge from a label-rich source graph to a completely
unlabelled target graph. However, most methods require a labelled source graph
to provide supervision signals, which might not be accessible in the real-world
settings due to regulations and privacy concerns. In this paper, we explore the
scenario of source-free unsupervised graph domain adaptation, which tries to
address the domain adaptation problem without accessing the labelled source
graph. Specifically, we present a novel paradigm called GraphCTA, which
performs model adaptation and graph adaptation collaboratively through a series
of procedures: (1) conduct model adaptation based on node's neighborhood
predictions in target graph considering both local and global information; (2)
perform graph adaptation by updating graph structure and node attributes via
neighborhood contrastive learning; and (3) the updated graph serves as an input
to facilitate the subsequent iteration of model adaptation, thereby
establishing a collaborative loop between model adaptation and graph
adaptation. Comprehensive experiments are conducted on various public datasets.
The experimental results demonstrate that our proposed model outperforms recent
source-free baselines by large margins.
\\ ( https://arxiv.org/abs/2403.01467 ,  3524kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01471
Date: Sun, 3 Mar 2024 10:35:46 GMT   (493kb,D)

Title: Preserving correlations: A statistical method for generating synthetic
  data
Authors: Nicklas J\"averg{\aa}rd, Rainey Lyons, Adrian Muntean and Jonas
  Forsman
Categories: cs.LG math.PR physics.data-an
\\
  We propose a method to generate statistically representative synthetic data.
The main goal is to be able to maintain in the synthetic dataset the
correlations of the features present in the original one, while offering a
comfortable privacy level that can be eventually tailored on specific customer
demands.
  We describe in detail our algorithm used both for the analysis of the
original dataset and for the generation of the synthetic data points. The
approach is tested using a large energy-related dataset. We obtain good results
both qualitatively (e.g. via vizualizing correlation maps) and quantitatively
(in terms of suitable $\ell^1$-type error norms used as evaluation metrics).
  The proposed methodology is general in the sense that it does not rely on the
used test dataset. We expect it to be applicable in a much broader context than
indicated here.
\\ ( https://arxiv.org/abs/2403.01471 ,  493kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01475
Date: Sun, 3 Mar 2024 10:59:16 GMT   (275kb,D)

Title: Representation Learning on Heterophilic Graph with Directional
  Neighborhood Attention
Authors: Qincheng Lu, Jiaqi Zhu, Sitao Luan, Xiao-Wen Chang
Categories: cs.LG cs.AI cs.SI
\\
  Graph Attention Network (GAT) is one of the most popular Graph Neural Network
(GNN) architecture, which employs the attention mechanism to learn edge weights
and has demonstrated promising performance in various applications. However,
since it only incorporates information from immediate neighborhood, it lacks
the ability to capture long-range and global graph information, leading to
unsatisfactory performance on some datasets, particularly on heterophilic
graphs. To address this limitation, we propose the Directional Graph Attention
Network (DGAT) in this paper. DGAT is able to combine the feature-based
attention with the global directional information extracted from the graph
topology. To this end, a new class of Laplacian matrices is proposed which can
provably reduce the diffusion distance between nodes. Based on the new
Laplacian, topology-guided neighbour pruning and edge adding mechanisms are
proposed to remove the noisy and capture the helpful long-range neighborhood
information. Besides, a global directional attention is designed to enable a
topological-aware information propagation. The superiority of the proposed DGAT
over the baseline GAT has also been verified through experiments on real-world
benchmarks and synthetic data sets. It also outperforms the state-of-the-art
(SOTA) models on 6 out of 7 real-world benchmark datasets.
\\ ( https://arxiv.org/abs/2403.01475 ,  275kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01493
Date: Sun, 3 Mar 2024 12:05:49 GMT   (2731kb,D)

Title: ConvTimeNet: A Deep Hierarchical Fully Convolutional Model for
  Multivariate Time Series Analysis
Authors: Mingyue Cheng, Jiqian Yang, Tingyue Pan, Qi Liu, Zhi Li
Categories: cs.LG
\\
  This paper introduces ConvTimeNet, a novel deep hierarchical fully
convolutional network designed to serve as a general-purpose model for time
series analysis. The key design of this network is twofold, designed to
overcome the limitations of traditional convolutional networks. Firstly, we
propose an adaptive segmentation of time series into sub-series level patches,
treating these as fundamental modeling units. This setting avoids the sparsity
semantics associated with raw point-level time steps. Secondly, we design a
fully convolutional block by skillfully integrating deepwise and pointwise
convolution operations, following the advanced building block style employed in
Transformer encoders. This backbone network allows for the effective capture of
both global sequence and cross-variable dependence, as it not only incorporates
the advancements of Transformer architecture but also inherits the inherent
properties of convolution. Furthermore, multi-scale representations of given
time series instances can be learned by controlling the kernel size flexibly.
Extensive experiments are conducted on both time series forecasting and
classification tasks. The results consistently outperformed strong baselines in
most situations in terms of effectiveness.The code is publicly available.
\\ ( https://arxiv.org/abs/2403.01493 ,  2731kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01499
Date: Sun, 3 Mar 2024 12:23:17 GMT   (2964kb,D)

Title: Normalising Flow-based Differentiable Particle Filters
Authors: Xiongjie Chen, Yunpeng Li
Categories: cs.LG eess.SP
Comments: 18 pages, 5 figures, submitted to IEEE Transactions on Signal
  Processing
\\
  Recently, there has been a surge of interest in incorporating neural networks
into particle filters, e.g. differentiable particle filters, to perform joint
sequential state estimation and model learning for non-linear non-Gaussian
state-space models in complex environments. Existing differentiable particle
filters are mostly constructed with vanilla neural networks that do not allow
density estimation. As a result, they are either restricted to a bootstrap
particle filtering framework or employ predefined distribution families (e.g.
Gaussian distributions), limiting their performance in more complex real-world
scenarios. In this paper we present a differentiable particle filtering
framework that uses (conditional) normalising flows to build its dynamic model,
proposal distribution, and measurement model. This not only enables valid
probability densities but also allows the proposed method to adaptively learn
these modules in a flexible way, without being restricted to predefined
distribution families. We derive the theoretical properties of the proposed
filters and evaluate the proposed normalising flow-based differentiable
particle filters' performance through a series of numerical experiments.
\\ ( https://arxiv.org/abs/2403.01499 ,  2964kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01501
Date: Sun, 3 Mar 2024 12:34:13 GMT   (3819kb,D)

Title: Applying Self-supervised Learning to Network Intrusion Detection for
  Network Flows with Graph Neural Network
Authors: Renjie Xu, Guangwei Wu, Weiping Wang, Xing Gao, An He, Zhengpeng Zhang
Categories: cs.LG cs.CR
Comments: 15pages,8figures
\\
  Graph Neural Networks (GNNs) have garnered intensive attention for Network
Intrusion Detection System (NIDS) due to their suitability for representing the
network traffic flows. However, most present GNN-based methods for NIDS are
supervised or semi-supervised. Network flows need to be manually annotated as
supervisory labels, a process that is time-consuming or even impossible, making
NIDS difficult to adapt to potentially complex attacks, especially in
large-scale real-world scenarios. The existing GNN-based self-supervised
methods focus on the binary classification of network flow as benign or not,
and thus fail to reveal the types of attack in practice. This paper studies the
application of GNNs to identify the specific types of network flows in an
unsupervised manner. We first design an encoder to obtain graph embedding, that
introduces the graph attention mechanism and considers the edge information as
the only essential factor. Then, a self-supervised method based on graph
contrastive learning is proposed. The method samples center nodes, and for each
center node, generates subgraph by it and its direct neighbor nodes, and
corresponding contrastive subgraph from the interpolated graph, and finally
constructs positive and negative samples from subgraphs. Furthermore, a
structured contrastive loss function based on edge features and graph local
topology is introduced. To the best of our knowledge, it is the first GNN-based
self-supervised method for the multiclass classification of network flows in
NIDS. Detailed experiments conducted on four real-world databases (NF-Bot-IoT,
NF-Bot-IoT-v2, NF-CSE-CIC-IDS2018, and NF-CSE-CIC-IDS2018-v2) systematically
compare our model with the state-of-the-art supervised and self-supervised
models, illustrating the considerable potential of our method. Our code is
accessible through https://github.com/renj-xu/NEGSC.
\\ ( https://arxiv.org/abs/2403.01501 ,  3819kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01533
Date: Sun, 3 Mar 2024 15:23:49 GMT   (260kb)

Title: Machine learning predicts long-term mortality after acute myocardial
  infarction using systolic time intervals and routinely collected clinical
  data
Authors: Bijan Roodini, Boshra Khajehpiri, amid Abrishami Moghaddam, Mohamad
  Forouzanfar
Categories: cs.LG cs.AI eess.SP
Comments: Accepted for publication in "Intelligent Medicine"
\\
  Precise estimation of cardiac patients' current and future comorbidities is
an important factor in prioritizing continuous physiological monitoring and new
therapies. ML models have shown satisfactory performance in short-term
mortality prediction of patients with heart disease, while their utility in
long-term predictions is limited. This study aims to investigate the
performance of tree-based ML models on long-term mortality prediction and the
effect of two recently introduced biomarkers on long-term mortality. This study
utilized publicly available data from CCHIA at the Ministry of Health and
Welfare, Taiwan, China. Medical records were used to gather demographic and
clinical data, including age, gender, BMI, percutaneous coronary intervention
(PCI) status, and comorbidities such as hypertension, dyslipidemia, ST-segment
elevation myocardial infarction (STEMI), and non-STEMI. Using medical and
demographic records as well as two recently introduced biomarkers, brachial
pre-ejection period (bPEP) and brachial ejection time (bET), collected from 139
patients with acute myocardial infarction, we investigated the performance of
advanced ensemble tree-based ML algorithms (random forest, AdaBoost, and
XGBoost) to predict all-cause mortality within 14 years. The developed ML
models achieved significantly better performance compared to the baseline LR
(C-Statistic, 0.80 for random forest, 0.79 for AdaBoost, and 0.78 for XGBoost,
vs 0.77 for LR) (P-RF<0.001, PAdaBoost<0.001, PXGBoost<0.05). Adding bPEP and
bET to our feature set significantly improved the algorithms' performance,
leading to an absolute increase in C-Statistic of up to 0.03 (C-Statistic, 0.83
for random forest, 0.82 for AdaBoost, and 0.80 for XGBoost, vs 0.74 for LR)
(P-RF<0.001, PAdaBoost<0.001, PXGBoost<0.05). This advancement may enable
better treatment prioritization for high-risk individuals.
\\ ( https://arxiv.org/abs/2403.01533 ,  260kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01535
Date: Sun, 3 Mar 2024 15:28:47 GMT   (92kb,D)

Title: Neural Graph Generator: Feature-Conditioned Graph Generation using
  Latent Diffusion Models
Authors: Iakovos Evdaimon, Giannis Nikolentzos, Michail Chatzianastasis, Hadi
  Abdine, Michalis Vazirgiannis
Categories: cs.LG cs.SI
\\
  Graph generation has emerged as a crucial task in machine learning, with
significant challenges in generating graphs that accurately reflect specific
properties. Existing methods often fall short in efficiently addressing this
need as they struggle with the high-dimensional complexity and varied nature of
graph properties. In this paper, we introduce the Neural Graph Generator (NGG),
a novel approach which utilizes conditioned latent diffusion models for graph
generation. NGG demonstrates a remarkable capacity to model complex graph
patterns, offering control over the graph generation process. NGG employs a
variational graph autoencoder for graph compression and a diffusion process in
the latent vector space, guided by vectors summarizing graph statistics. We
demonstrate NGG's versatility across various graph generation tasks, showing
its capability to capture desired graph properties and generalize to unseen
graphs. This work signifies a significant shift in graph generation
methodologies, offering a more practical and efficient solution for generating
diverse types of graphs with specific characteristics.
\\ ( https://arxiv.org/abs/2403.01535 ,  92kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01540
Date: Sun, 3 Mar 2024 15:40:24 GMT   (73kb,D)

Title: Quantized Hierarchical Federated Learning: A Robust Approach to
  Statistical Heterogeneity
Authors: Seyed Mohammad Azimi-Abarghouyi, Viktoria Fodor
Categories: cs.LG cs.IT math.IT
\\
  This paper presents a novel hierarchical federated learning algorithm within
multiple sets that incorporates quantization for communication-efficiency and
demonstrates resilience to statistical heterogeneity. Unlike conventional
hierarchical federated learning algorithms, our approach combines gradient
aggregation in intra-set iterations with model aggregation in inter-set
iterations. We offer a comprehensive analytical framework to evaluate its
optimality gap and convergence rate, comparing these aspects with those of
conventional algorithms. Additionally, we develop a problem formulation to
derive optimal system parameters in a closed-form solution. Our findings reveal
that our algorithm consistently achieves high learning accuracy over a range of
parameters and significantly outperforms other hierarchical algorithms,
particularly in scenarios with heterogeneous data distributions.
\\ ( https://arxiv.org/abs/2403.01540 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01554
Date: Sun, 3 Mar 2024 16:12:20 GMT   (4576kb,D)

Title: Transformers for Supervised Online Continual Learning
Authors: Jorg Bornschein, Yazhe Li, Amal Rannen-Triki
Categories: cs.LG
\\
  Transformers have become the dominant architecture for sequence modeling
tasks such as natural language processing or audio processing, and they are now
even considered for tasks that are not naturally sequential such as image
classification. Their ability to attend to and to process a set of tokens as
context enables them to develop in-context few-shot learning abilities.
However, their potential for online continual learning remains relatively
unexplored. In online continual learning, a model must adapt to a
non-stationary stream of data, minimizing the cumulative nextstep prediction
loss. We focus on the supervised online continual learning setting, where we
learn a predictor $x_t \rightarrow y_t$ for a sequence of examples $(x_t,
y_t)$. Inspired by the in-context learning capabilities of transformers and
their connection to meta-learning, we propose a method that leverages these
strengths for online continual learning. Our approach explicitly conditions a
transformer on recent observations, while at the same time online training it
with stochastic gradient descent, following the procedure introduced with
Transformer-XL. We incorporate replay to maintain the benefits of multi-epoch
training while adhering to the sequential protocol. We hypothesize that this
combination enables fast adaptation through in-context learning and sustained
longterm improvement via parametric learning. Our method demonstrates
significant improvements over previous state-of-the-art results on CLOC, a
challenging large-scale real-world benchmark for image geo-localization.
\\ ( https://arxiv.org/abs/2403.01554 ,  4576kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01582
Date: Sun, 3 Mar 2024 18:22:39 GMT   (16003kb,D)

Title: On the Model-Agnostic Multi-Source-Free Unsupervised Domain Adaptation
Authors: Jiangbo Pei, Ruizhe Li, Qingchao Chen
Categories: cs.LG
\\
  Multi-Source-Free Unsupervised Domain Adaptation (MSFDA) aims to transfer
knowledge from multiple well-labeled source domains to an unlabeled target
domain, using source models instead of source data. Existing MSFDA methods
limited that each source domain provides only a single model, with a uniform
structure. This paper introduces a new MSFDA setting: Model-Agnostic
Multi-Source-Free Unsupervised Domain Adaptation (MMDA), allowing diverse
source models with varying architectures, without quantitative restrictions.
While MMDA holds promising potential, incorporating numerous source models
poses a high risk of including undesired models, which highlights the source
model selection problem. To address it, we first provide a theoretical analysis
of this problem. We reveal two fundamental selection principles:
transferability principle and diversity principle, and introduce a selection
algorithm to integrate them. Then, considering the measure of transferability
is challenging, we propose a novel Source-Free Unsupervised Transferability
Estimation (SUTE). This novel formulation enables the assessment and comparison
of transferability across multiple source models with different architectures
in the context of domain shift, without requiring access to any target labels
or source data. Based on the above, we introduce a new framework to address
MMDA. Specifically, we first conduct source model selection based on the
proposed selection principles. Subsequently, we design two modules to aggregate
knowledge from included models and recycle useful knowledge from excluded
models. These modules enable us to leverage source knowledge efficiently and
effectively, thereby supporting us in learning a discriminative target model
via adaptation. We validate the effectiveness of our method through numerous
experimental results, and demonstrate that our approach achieves
state-of-the-art performance.
\\ ( https://arxiv.org/abs/2403.01582 ,  16003kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01590
Date: Sun, 3 Mar 2024 18:58:21 GMT   (12033kb,D)

Title: The Hidden Attention of Mamba Models
Authors: Ameen Ali, Itamar Zimerman, Lior Wolf
Categories: cs.LG
MSC-class: F.2.2, I.2.7
ACM-class: F.2.2; I.2.7
\\
  The Mamba layer offers an efficient selective state space model (SSM) that is
highly effective in modeling multiple domains including NLP, long-range
sequences processing, and computer vision. Selective SSMs are viewed as dual
models, in which one trains in parallel on the entire sequence via IO-aware
parallel scan, and deploys in an autoregressive manner. We add a third view and
show that such models can be viewed as attention-driven models. This new
perspective enables us to compare the underlying mechanisms to that of the
self-attention layers in transformers and allows us to peer inside the inner
workings of the Mamba model with explainability methods. Our code is publicly
available.
\\ ( https://arxiv.org/abs/2403.01590 ,  12033kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01605
Date: Sun, 3 Mar 2024 20:09:09 GMT   (4876kb,D)

Title: Towards Provable Log Density Policy Gradient
Authors: Pulkit Katdare, Anant Joshi and Katherine Driggs-Campbell
Categories: cs.LG cs.AI stat.ML
\\
  Policy gradient methods are a vital ingredient behind the success of modern
reinforcement learning. Modern policy gradient methods, although successful,
introduce a residual error in gradient estimation. In this work, we argue that
this residual term is significant and correcting for it could potentially
improve sample-complexity of reinforcement learning methods. To that end, we
propose log density gradient to estimate the policy gradient, which corrects
for this residual error term. Log density gradient method computes policy
gradient by utilising the state-action discounted distributional formulation.
We first present the equations needed to exactly find the log density gradient
for a tabular Markov Decision Processes (MDPs). For more complex environments,
we propose a temporal difference (TD) method that approximates log density
gradient by utilizing backward on-policy samples. Since backward sampling from
a Markov chain is highly restrictive we also propose a min-max optimization
that can approximate log density gradient using just on-policy samples. We also
prove uniqueness, and convergence under linear function approximation, for this
min-max optimization. Finally, we show that the sample complexity of our
min-max optimization to be of the order of $m^{-1/2}$, where $m$ is the number
of on-policy samples. We also demonstrate a proof-of-concept for our log
density gradient method on gridworld environment, and observe that our method
is able to improve upon the classical policy gradient method by a clear margin,
thus indicating a promising novel direction to develop reinforcement learning
algorithms that require fewer samples.
\\ ( https://arxiv.org/abs/2403.01605 ,  4876kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01607
Date: Sun, 3 Mar 2024 20:16:16 GMT   (6067kb,D)

Title: Respiratory motion forecasting with online learning of recurrent neural
  networks for safety enhancement in externally guided radiotherapy
Authors: Michel Pohl, Mitsuru Uesaka, Hiroyuki Takahashi, Kazuyuki Demachi, and
  Ritu Bhusal Chhatkuli
Categories: cs.LG cs.NE eess.IV eess.SP
Comments: 34 pages, 11 figures
\\
  In lung radiotherapy, infrared cameras can record the location of reflective
objects on the chest to infer the position of the tumor moving due to
breathing, but treatment system latencies hinder radiation beam precision.
Real-time recurrent learning (RTRL), is a potential solution as it can learn
patterns within non-stationary respiratory data but has high complexity. This
study assesses the capabilities of resource-efficient online RNN algorithms,
namely unbiased online recurrent optimization (UORO), sparse-1 step
approximation (SnAp-1), and decoupled neural interfaces (DNI) to forecast
respiratory motion during radiotherapy treatment accurately. We use time series
containing the 3D position of external markers on the chest of healthy
subjects. We propose efficient implementations for SnAp-1 and DNI based on
compression of the influence and immediate Jacobian matrices and an accurate
update of the linear coefficients used in credit assignment estimation,
respectively. The original sampling frequency was 10Hz; we performed resampling
at 3.33Hz and 30Hz. We use UORO, SnAp-1, and DNI to forecast each marker's 3D
position with horizons (the time interval in advance for which the prediction
is made) h<=2.1s and compare them with RTRL, least mean squares, and linear
regression. RNNs trained online achieved similar or better accuracy than most
previous works using larger training databases and deep learning, even though
we used only the first minute of each sequence to predict motion within that
exact sequence. SnAp-1 had the lowest normalized root mean square errors
(nRMSE) averaged over the horizon values considered, equal to 0.335 and 0.157,
at 3.33Hz and 10.0Hz, respectively. Similarly, UORO had the highest accuracy at
30Hz, with an nRMSE of 0.0897. DNI's inference time, equal to 6.8ms per time
step at 30Hz (Intel Core i7-13700 CPU), was the lowest among the RNN methods
examined.
\\ ( https://arxiv.org/abs/2403.01607 ,  6067kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01615
Date: Sun, 3 Mar 2024 21:04:36 GMT   (1008kb,D)

Title: Partial Federated Learning
Authors: Tiantian Feng, Anil Ramakrishna, Jimit Majmudar, Charith Peris, Jixuan
  Wang, Clement Chung, Richard Zemel, Morteza Ziyadi, Rahul Gupta
Categories: cs.LG cs.DC
\\
  Federated Learning (FL) is a popular algorithm to train machine learning
models on user data constrained to edge devices (for example, mobile phones)
due to privacy concerns. Typically, FL is trained with the assumption that no
part of the user data can be egressed from the edge. However, in many
production settings, specific data-modalities/meta-data are limited to be on
device while others are not. For example, in commercial SLU systems, it is
typically desired to prevent transmission of biometric signals (such as audio
recordings of the input prompt) to the cloud, but egress of locally (i.e. on
the edge device) transcribed text to the cloud may be possible. In this work,
we propose a new algorithm called Partial Federated Learning (PartialFL), where
a machine learning model is trained using data where a subset of data
modalities or their intermediate representations can be made available to the
server. We further restrict our model training by preventing the egress of data
labels to the cloud for better privacy, and instead use a contrastive learning
based model objective. We evaluate our approach on two different multi-modal
datasets and show promising results with our proposed approach.
\\ ( https://arxiv.org/abs/2403.01615 ,  1008kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01621
Date: Sun, 3 Mar 2024 21:42:55 GMT   (413kb)

Title: Machine Learning vs Deep Learning: The Generalization Problem
Authors: Yong Yi Bay and Kathleen A. Yearick
Categories: cs.LG cs.AI
Comments: 10 pages, 2 figures
ACM-class: I.2.6
\\
  The capacity to generalize beyond the range of training data is a pivotal
challenge, often synonymous with a model's utility and robustness. This study
investigates the comparative abilities of traditional machine learning (ML)
models and deep learning (DL) algorithms in terms of extrapolation -- a more
challenging aspect of generalization because it requires the model to make
inferences about data points that lie outside the domain it has been trained
on. We present an empirical analysis where both ML and DL models are trained on
an exponentially growing function and then tested on values outside the
training domain. The choice of this function allows us to distinctly showcase
the divergence in performance when models are required to predict beyond the
scope of their training data. Our findings suggest that deep learning models
possess inherent capabilities to generalize beyond the training scope, an
essential feature for real-world applications where data is often incomplete or
extends beyond the observed range. This paper argues for a nuanced
understanding of the structural differences between ML and DL models, with an
emphasis on the implications for both theoretical research and practical
deployment.
\\ ( https://arxiv.org/abs/2403.01621 ,  413kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01623
Date: Sun, 3 Mar 2024 22:10:21 GMT   (4090kb,D)

Title: ML4PhySim : Machine Learning for Physical Simulations Challenge (The
  airfoil design)
Authors: Mouadh Yagoubi, Milad Leyli-Abadi, David Danan, Jean-Patrick Brunet,
  Jocelyn Ahmed Mazari, Florent Bonnet, Asma Farjallah, Marc Schoenauer,
  Patrick Gallinari
Categories: cs.LG cs.CE
\\
  The use of machine learning (ML) techniques to solve complex physical
problems has been considered recently as a promising approach. However, the
evaluation of such learned physical models remains an important issue for
industrial use. The aim of this competition is to encourage the development of
new ML techniques to solve physical problems using a unified evaluation
framework proposed recently, called Learning Industrial Physical Simulations
(LIPS). We propose learning a task representing a well-known physical use case:
the airfoil design simulation, using a dataset called AirfRANS. The global
score calculated for each submitted solution is based on three main categories
of criteria covering different aspects, namely: ML-related,
Out-Of-Distribution, and physical compliance criteria. To the best of our
knowledge, this is the first competition addressing the use of ML-based
surrogate approaches to improve the trade-off computational cost/accuracy of
physical simulation.The competition is hosted by the Codabench platform with
online training and evaluation of all submitted solutions.
\\ ( https://arxiv.org/abs/2403.01623 ,  4090kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01628
Date: Sun, 3 Mar 2024 22:21:58 GMT   (143kb)

Title: Recent Advances, Applications, and Open Challenges in Machine Learning
  for Health: Reflections from Research Roundtables at ML4H 2023 Symposium
Authors: Hyewon Jeong, Sarah Jabbour, Yuzhe Yang, Rahul Thapta, Hussein
  Mozannar, William Jongwon Han, Nikita Mehandru, Michael Wornow, Vladislav
  Lialin, Xin Liu, Alejandro Lozano, Jiacheng Zhu, Rafal Dariusz Kocielnik,
  Keith Harrigian, Haoran Zhang, Edward Lee, Milos Vukadinovic, Aparna
  Balagopalan, Vincent Jeanselme, Katherine Matton, Ilker Demirel, Jason Fries,
  Parisa Rashidi, Brett Beaulieu-Jones, Xuhai Orson Xu, Matthew McDermott,
  Tristan Naumann, Monica Agrawal, Marinka Zitnik, Berk Ustun, Edward Choi,
  Kristen Yeom, Gamze Gursoy, Marzyeh Ghassemi, Emma Pierson, George Chen,
  Sanjat Kanjilal, Michael Oberst, Linying Zhang, Harvineet Singh, Tom
  Hartvigsen, Helen Zhou, Chinasa T. Okolo
Categories: cs.LG
Comments: ML4H 2023, Research Roundtables
\\
  The third ML4H symposium was held in person on December 10, 2023, in New
Orleans, Louisiana, USA. The symposium included research roundtable sessions to
foster discussions between participants and senior researchers on timely and
relevant topics for the \ac{ML4H} community. Encouraged by the successful
virtual roundtables in the previous year, we organized eleven in-person
roundtables and four virtual roundtables at ML4H 2022. The organization of the
research roundtables at the conference involved 17 Senior Chairs and 19 Junior
Chairs across 11 tables. Each roundtable session included invited senior chairs
(with substantial experience in the field), junior chairs (responsible for
facilitating the discussion), and attendees from diverse backgrounds with
interest in the session's topic. Herein we detail the organization process and
compile takeaways from these roundtable discussions, including recent advances,
applications, and open challenges for each topic. We conclude with a summary
and lessons learned across all roundtables. This document serves as a
comprehensive review paper, summarizing the recent advancements in machine
learning for healthcare as contributed by foremost researchers in the field.
\\ ( https://arxiv.org/abs/2403.01628 ,  143kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01632
Date: Sun, 3 Mar 2024 22:38:35 GMT   (2069kb,D)

Title: Improving LLM Code Generation with Grammar Augmentation
Authors: Shubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic, Gagandeep
  Singh
Categories: cs.LG cs.FL cs.PL cs.SE
\\
  We present SynCode a novel framework for efficient and general syntactical
decoding of code with large language models (LLMs). SynCode leverages the
grammar of a programming language, utilizing an offline-constructed efficient
lookup table called DFA mask store based on language grammar terminals. We
demonstrate SynCode's soundness and completeness given the context-free grammar
(CFG) of the programming language, presenting its ability to retain
syntactically valid tokens while rejecting invalid ones. The framework
seamlessly integrates with any language defined by CFG, as evidenced by
experiments on CFGs for Python and Go. The results underscore the significant
reduction of 96.07% of syntax errors achieved when SynCode is combined with
state-of-the-art LLMs, showcasing its substantial impact on enhancing
syntactical precision in code generation.
  Our code is available at https://github.com/uiuc-focal-lab/syncode.
\\ ( https://arxiv.org/abs/2403.01632 ,  2069kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01633
Date: Sun, 3 Mar 2024 22:43:47 GMT   (14254kb,D)

Title: Critical windows: non-asymptotic theory for feature emergence in
  diffusion models
Authors: Marvin Li and Sitan Chen
Categories: cs.LG cs.CV stat.ML
\\
  We develop theory to understand an intriguing property of diffusion models
for image generation that we term critical windows. Empirically, it has been
observed that there are narrow time intervals in sampling during which
particular features of the final image emerge, e.g. the image class or
background color (Ho et al., 2020b; Georgiev et al., 2023; Raya & Ambrogioni,
2023; Sclocchi et al., 2024; Biroli et al., 2024). While this is advantageous
for interpretability as it implies one can localize properties of the
generation to a small segment of the trajectory, it seems at odds with the
continuous nature of the diffusion. We propose a formal framework for studying
these windows and show that for data coming from a mixture of strongly
log-concave densities, these windows can be provably bounded in terms of
certain measures of inter- and intra-group separation. We also instantiate
these bounds for concrete examples like well-conditioned Gaussian mixtures.
Finally, we use our bounds to give a rigorous interpretation of diffusion
models as hierarchical samplers that progressively "decide" output features
over a discrete sequence of times. We validate our bounds with synthetic
experiments. Additionally, preliminary experiments on Stable Diffusion suggest
critical windows may serve as a useful tool for diagnosing fairness and privacy
violations in real-world diffusion models.
\\ ( https://arxiv.org/abs/2403.01633 ,  14254kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01639
Date: Sun, 3 Mar 2024 23:15:48 GMT   (8997kb,D)

Title: Theoretical Insights for Diffusion Guidance: A Case Study for Gaussian
  Mixture Models
Authors: Yuchen Wu, Minshuo Chen, Zihao Li, Mengdi Wang, Yuting Wei
Categories: cs.LG stat.ML
Comments: 41 pages, 12 figures
\\
  Diffusion models benefit from instillation of task-specific information into
the score function to steer the sample generation towards desired properties.
Such information is coined as guidance. For example, in text-to-image
synthesis, text input is encoded as guidance to generate semantically aligned
images. Proper guidance inputs are closely tied to the performance of diffusion
models. A common observation is that strong guidance promotes a tight alignment
to the task-specific information, while reducing the diversity of the generated
samples. In this paper, we provide the first theoretical study towards
understanding the influence of guidance on diffusion models in the context of
Gaussian mixture models. Under mild conditions, we prove that incorporating
diffusion guidance not only boosts classification confidence but also
diminishes distribution diversity, leading to a reduction in the differential
entropy of the output distribution. Our analysis covers the widely adopted
sampling schemes including DDPM and DDIM, and leverages comparison inequalities
for differential equations as well as the Fokker-Planck equation that
characterizes the evolution of probability density function, which may be of
independent theoretical interest.
\\ ( https://arxiv.org/abs/2403.01639 ,  8997kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01642
Date: Sun, 3 Mar 2024 23:38:37 GMT   (1715kb)

Title: Blue and Green-Mode Energy-Efficient Chemiresistive Sensor Array
  Realized by Rapid Ensemble Learning
Authors: Zeheng Wang, James Cooper, Muhammad Usman, and Timothy van der Laan
Categories: cs.LG cs.CE cs.SY eess.SY
Comments: First version before submission
\\
  The rapid advancement of Internet of Things (IoT) necessitates the
development of optimized Chemiresistive Sensor (CRS) arrays that are both
energy-efficient and capable. This study introduces a novel optimization
strategy that employs a rapid ensemble learning-based model committee approach
to achieve these goals. Utilizing machine learning models such as Elastic Net
Regression, Random Forests, and XGBoost, among others, the strategy identifies
the most impactful sensors in a CRS array for accurate classification: A
weighted voting mechanism is introduced to aggregate the models' opinions in
sensor selection, thereby setting up wo distinct working modes, termed "Blue"
and "Green". The Blue mode operates with all sensors for maximum detection
capability, while the Green mode selectively activates only key sensors,
significantly reducing energy consumption without compromising detection
accuracy. The strategy is validated through theoretical calculations and Monte
Carlo simulations, demonstrating its effectiveness and accuracy. The proposed
optimization strategy not only elevates the detection capability of CRS arrays
but also brings it closer to theoretical limits, promising significant
implications for the development of low-cost, easily fabricable next-generation
IoT sensor terminals.
\\ ( https://arxiv.org/abs/2403.01642 ,  1715kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01643
Date: Sun, 3 Mar 2024 23:40:35 GMT   (78kb,D)

Title: You Need to Pay Better Attention
Authors: Mehran Hosseini, Peyman Hosseini
Categories: cs.LG cs.AI cs.CL cs.CV
MSC-class: 68T07 (Primary) 68T45, 68T50, 68T10, 15A03, 15A04 (Secondary)
ACM-class: I.2.6; I.2.7; I.2.10; I.4.0; I.5.0; I.7.0
\\
  We introduce three new attention mechanisms that outperform standard
multi-head attention in terms of efficiency and learning capabilities, thereby
improving the performance and broader deployability of Transformer models. Our
first contribution is Optimised Attention, which performs similarly to standard
attention, but has 3/4 as many parameters and one matrix multiplication fewer
per head. Next, we introduce Efficient Attention, which performs on par with
standard attention with only 1/2 as many parameters as many parameters and two
matrix multiplications fewer per head and is up to twice as fast as standard
attention. Lastly, we introduce Super Attention, which surpasses standard
attention by a significant margin in both vision and natural language
processing tasks while having fewer parameters and matrix multiplications. In
addition to providing rigorous mathematical comparisons, we evaluate the
presented attention mechanisms on MNIST, CIFAR100, IMDB Movie Reviews, and
Amazon Reviews datasets.
\\ ( https://arxiv.org/abs/2403.01643 ,  78kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01653
Date: Mon, 4 Mar 2024 00:09:07 GMT   (1042kb,D)

Title: Day-ahead regional solar power forecasting with hierarchical temporal
  convolutional neural networks using historical power generation and weather
  data
Authors: Maneesha Perera, Julian De Hoog, Kasun Bandara, Damith Senanayake,
  Saman Halgamuge
Categories: cs.LG
Comments: 37 pages, 16 figures, Accepted to the journal of Applied Energy
\\
  Regional solar power forecasting, which involves predicting the total power
generation from all rooftop photovoltaic systems in a region holds significant
importance for various stakeholders in the energy sector. However, the vast
amount of solar power generation and weather time series from geographically
dispersed locations that need to be considered in the forecasting process makes
accurate regional forecasting challenging. Therefore, previous work has limited
the focus to either forecasting a single time series (i.e., aggregated time
series) which is the addition of all solar generation time series in a region,
disregarding the location-specific weather effects or forecasting solar
generation time series of each PV site (i.e., individual time series)
independently using location-specific weather data, resulting in a large number
of forecasting models. In this work, we propose two deep-learning-based
regional forecasting methods that can effectively leverage both types of time
series (aggregated and individual) with weather data in a region. We propose
two hierarchical temporal convolutional neural network architectures (HTCNN)
and two strategies to adapt HTCNNs for regional solar power forecasting. At
first, we explore generating a regional forecast using a single HTCNN. Next, we
divide the region into multiple sub-regions based on weather information and
train separate HTCNNs for each sub-region; the forecasts of each sub-region are
then added to generate a regional forecast. The proposed work is evaluated
using a large dataset collected over a year from 101 locations across Western
Australia to provide a day ahead forecast. We compare our approaches with
well-known alternative methods and show that the sub-region HTCNN requires
fewer individual networks and achieves a forecast skill score of 40.2% reducing
a statistically significant error by 6.5% compared to the best counterpart.
\\ ( https://arxiv.org/abs/2403.01653 ,  1042kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01660
Date: Mon, 4 Mar 2024 00:48:36 GMT   (911kb,D)

Title: Geometry and Stability of Supervised Learning Problems
Authors: Facundo M\'emoli, Brantley Vose, Robert C. Williamson
Categories: cs.LG math.MG
Comments: 87 pages
\\
  We introduce a notion of distance between supervised learning problems, which
we call the Risk distance. This optimal-transport-inspired distance facilitates
stability results; one can quantify how seriously issues like sampling bias,
noise, limited data, and approximations might change a given problem by
bounding how much these modifications can move the problem under the Risk
distance. With the distance established, we explore the geometry of the
resulting space of supervised learning problems, providing explicit geodesics
and proving that the set of classification problems is dense in a larger class
of problems. We also provide two variants of the Risk distance: one that
incorporates specified weights on a problem's predictors, and one that is more
sensitive to the contours of a problem's risk landscape.
\\ ( https://arxiv.org/abs/2403.01660 ,  911kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01666
Date: Mon, 4 Mar 2024 01:33:53 GMT   (6702kb,D)

Title: Improving Adversarial Energy-Based Model via Diffusion Process
Authors: Cong Geng, Tian Han, Peng-Tao Jiang, Hao Zhang, Jinwei Chen, S{\o}ren
  Hauberg, Bo Li
Categories: cs.LG cs.CV
\\
  Generative models have shown strong generation ability while efficient
likelihood estimation is less explored. Energy-based models~(EBMs) define a
flexible energy function to parameterize unnormalized densities efficiently but
are notorious for being difficult to train. Adversarial EBMs introduce a
generator to form a minimax training game to avoid expensive MCMC sampling used
in traditional EBMs, but a noticeable gap between adversarial EBMs and other
strong generative models still exists. Inspired by diffusion-based models, we
embedded EBMs into each denoising step to split a long-generated process into
several smaller steps. Besides, we employ a symmetric Jeffrey divergence and
introduce a variational posterior distribution for the generator's training to
address the main challenges that exist in adversarial EBMs. Our experiments
show significant improvement in generation compared to existing adversarial
EBMs, while also providing a useful energy function for efficient density
estimation.
\\ ( https://arxiv.org/abs/2403.01666 ,  6702kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01669
Date: Mon, 4 Mar 2024 01:44:19 GMT   (671kb,D)

Title: Quantifying and Predicting Residential Building Flexibility Using
  Machine Learning Methods
Authors: Patrick Salter, Qiuhua Huang, Paulo Cesar Tabares-Velasco
Categories: cs.LG cs.SY eess.SY
\\
  Residential buildings account for a significant portion (35\%) of the total
electricity consumption in the U.S. as of 2022. As more distributed energy
resources are installed in buildings, their potential to provide flexibility to
the grid increases. To tap into that flexibility provided by buildings,
aggregators or system operators need to quantify and forecast flexibility.
Previous works in this area primarily focused on commercial buildings, with
little work on residential buildings. To address the gap, this paper first
proposes two complementary flexibility metrics (i.e., power and energy
flexibility) and then investigates several mainstream machine learning-based
models for predicting the time-variant and sporadic flexibility of residential
buildings at four-hour and 24-hour forecast horizons. The
long-short-term-memory (LSTM) model achieves the best performance and can
predict power flexibility for up to 24 hours ahead with the average error
around 0.7 kW. However, for energy flexibility, the LSTM model is only
successful for loads with consistent operational patterns throughout the year
and faces challenges when predicting energy flexibility associated with HVAC
systems.
\\ ( https://arxiv.org/abs/2403.01669 ,  671kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01671
Date: Mon, 4 Mar 2024 01:49:23 GMT   (109kb,D)

Title: Permutation invariant functions: statistical tests, dimension reduction
  in metric entropy and estimation
Authors: Wee Chaimanowong, Ying Zhu
Categories: cs.LG
Comments: 24 pages, 3 figures
MSC-class: 62G07 62G10 62G07 (Primary), 62G08, 62G10 (Secondary)
\\
  Permutation invariance is among the most common symmetry that can be
exploited to simplify complex problems in machine learning (ML). There has been
a tremendous surge of research activities in building permutation invariant ML
architectures. However, less attention is given to how to statistically test
for permutation invariance of variables in a multivariate probability
distribution where the dimension is allowed to grow with the sample size. Also,
in terms of a statistical theory, little is known about how permutation
invariance helps with estimation in reducing dimensions. In this paper, we take
a step back and examine these questions in several fundamental problems: (i)
testing the assumption of permutation invariance of multivariate distributions;
(ii) estimating permutation invariant densities; (iii) analyzing the metric
entropy of smooth permutation invariant function classes and compare them with
their counterparts without imposing permutation invariance; (iv) kernel ridge
regression of permutation invariant functions in reproducing kernel Hilbert
space. In particular, our methods for (i) and (iv) are based on a sorting trick
and (ii) is based on an averaging trick. These tricks substantially simplify
the exploitation of permutation invariance.
\\ ( https://arxiv.org/abs/2403.01671 ,  109kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01695
Date: Mon, 4 Mar 2024 03:09:28 GMT   (5341kb,D)

Title: DyCE: Dynamic Configurable Exiting for Deep Learning Compression and
  Scaling
Authors: Qingyuan Wang, Barry Cardiff, Antoine Frapp\'e, Benoit Larras and
  Deepu John
Categories: cs.LG cs.AI
\\
  Modern deep learning (DL) models necessitate the employment of scaling and
compression techniques for effective deployment in resource-constrained
environments. Most existing techniques, such as pruning and quantization are
generally static. On the other hand, dynamic compression methods, such as early
exits, reduce complexity by recognizing the difficulty of input samples and
allocating computation as needed. Dynamic methods, despite their superior
flexibility and potential for co-existing with static methods, pose significant
challenges in terms of implementation due to any changes in dynamic parts will
influence subsequent processes. Moreover, most current dynamic compression
designs are monolithic and tightly integrated with base models, thereby
complicating the adaptation to novel base models. This paper introduces DyCE,
an dynamic configurable early-exit framework that decouples design
considerations from each other and from the base model. Utilizing this
framework, various types and positions of exits can be organized according to
predefined configurations, which can be dynamically switched in real-time to
accommodate evolving performance-complexity requirements. We also propose
techniques for generating optimized configurations based on any desired
trade-off between performance and computational complexity. This empowers
future researchers to focus on the improvement of individual exits without
latent compromise of overall system performance. The efficacy of this approach
is demonstrated through image classification tasks with deep CNNs. DyCE
significantly reduces the computational complexity by 23.5% of ResNet152 and
25.9% of ConvNextv2-tiny on ImageNet, with accuracy reductions of less than
0.5%. Furthermore, DyCE offers advantages over existing dynamic methods in
terms of real-time configuration and fine-grained performance tuning.
\\ ( https://arxiv.org/abs/2403.01695 ,  5341kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01718
Date: Mon, 4 Mar 2024 04:12:37 GMT   (744kb)

Title: L0 Regularization of Field-Aware Factorization Machine through Ising
  Model
Authors: Yasuharu Okamoto (1,2) ((1) Secure System Platform Research
  Laboratories, NEC Corporation, Nakahara-ku, Kawasaki, Kanagawa, Japan, (2)
  NEC-AIST Quantum Technology Cooperative Research Laboratories, Tsukuba,
  Ibaraki, Japan)
Categories: cs.LG cond-mat.mtrl-sci
Comments: 11 pages, 3 figures
\\
  We examined the use of the Ising model as an L0 regularization method for
field-aware factorization machines (FFM). This approach improves generalization
performance and has the advantage of simultaneously determining the best
feature combinations for each of several groups. We can deepen the
interpretation and understanding of the model from the similarities and
differences in the features selected in each group.
\\ ( https://arxiv.org/abs/2403.01718 ,  744kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01738
Date: Mon, 4 Mar 2024 05:31:29 GMT   (6101kb,D)

Title: ComS2T: A complementary spatiotemporal learning system for data-adaptive
  model evolution
Authors: Zhengyang Zhou, Qihe Huang, Binwu Wang, Jianpeng Hou, Kuo Yang, Yuxuan
  Liang, Yang Wang
Categories: cs.LG
\\
  Spatiotemporal (ST) learning has become a crucial technique to enable smart
cities and sustainable urban development. Current ST learning models capture
the heterogeneity via various spatial convolution and temporal evolution
blocks. However, rapid urbanization leads to fluctuating distributions in urban
data and city structures over short periods, resulting in existing methods
suffering generalization and data adaptation issues. Despite efforts, existing
methods fail to deal with newly arrived observations and those methods with
generalization capacity are limited in repeated training. Motivated by
complementary learning in neuroscience, we introduce a prompt-based
complementary spatiotemporal learning termed ComS2T, to empower the evolution
of models for data adaptation. ComS2T partitions the neural architecture into a
stable neocortex for consolidating historical memory and a dynamic hippocampus
for new knowledge update. We first disentangle two disjoint structures into
stable and dynamic weights, and then train spatial and temporal prompts by
characterizing distribution of main observations to enable prompts adaptive to
new data. This data-adaptive prompt mechanism, combined with a two-stage
training process, facilitates fine-tuning of the neural architecture
conditioned on prompts, thereby enabling efficient adaptation during testing.
Extensive experiments validate the efficacy of ComS2T in adapting to various
spatiotemporal out-of-distribution scenarios while maintaining efficient
inference capabilities.
\\ ( https://arxiv.org/abs/2403.01738 ,  6101kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01742
Date: Mon, 4 Mar 2024 05:39:23 GMT   (10138kb,D)

Title: Diffusion-TS: Interpretable Diffusion for General Time Series Generation
Authors: Xinyu Yuan and Yan Qiao
Categories: cs.LG cs.AI
\\
  Denoising diffusion probabilistic models (DDPMs) are becoming the leading
paradigm for generative models. It has recently shown breakthroughs in audio
synthesis, time series imputation and forecasting. In this paper, we propose
Diffusion-TS, a novel diffusion-based framework that generates multivariate
time series samples of high quality by using an encoder-decoder transformer
with disentangled temporal representations, in which the decomposition
technique guides Diffusion-TS to capture the semantic meaning of time series
while transformers mine detailed sequential information from the noisy model
input. Different from existing diffusion-based approaches, we train the model
to directly reconstruct the sample instead of the noise in each diffusion step,
combining a Fourier-based loss term. Diffusion-TS is expected to generate time
series satisfying both interpretablity and realness. In addition, it is shown
that the proposed Diffusion-TS can be easily extended to conditional generation
tasks, such as forecasting and imputation, without any model changes. This also
motivates us to further explore the performance of Diffusion-TS under irregular
settings. Finally, through qualitative and quantitative experiments, results
show that Diffusion-TS achieves the state-of-the-art results on various
realistic analyses of time series.
\\ ( https://arxiv.org/abs/2403.01742 ,  10138kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01759
Date: Mon, 4 Mar 2024 06:25:26 GMT   (3406kb,D)

Title: Open-world Machine Learning: A Review and New Outlooks
Authors: Fei Zhu, Shijie Ma, Zhen Cheng, Xu-Yao Zhang, Zhaoxiang Zhang,
  Cheng-Lin Liu
Categories: cs.LG cs.CV
\\
  Machine learning has achieved remarkable success in many applications.
However, existing studies are largely based on the closed-world assumption,
which assumes that the environment is stationary, and the model is fixed once
deployed. In many real-world applications, this fundamental and rather naive
assumption may not hold because an open environment is complex, dynamic, and
full of unknowns. In such cases, rejecting unknowns, discovering novelties, and
then incrementally learning them, could enable models to be safe and evolve
continually as biological systems do. This paper provides a holistic view of
open-world machine learning by investigating unknown rejection, novel class
discovery, and class-incremental learning in a unified paradigm. The
challenges, principles, and limitations of current methodologies are discussed
in detail. Finally, we discuss several potential directions for future
research. This paper aims to provide a comprehensive introduction to the
emerging open-world machine learning paradigm, to help researchers build more
powerful AI systems in their respective fields, and to promote the development
of artificial general intelligence.
\\ ( https://arxiv.org/abs/2403.01759 ,  3406kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01769
Date: Mon, 4 Mar 2024 06:55:57 GMT   (2675kb)

Title: A Safe Screening Rule with Bi-level Optimization of $\nu$ Support Vector
  Machine
Authors: Zhiji Yang, Wanyi Chen, Huan Zhang, Yitian Xu, Lei Shi, Jianhua Zhao
Categories: cs.LG cs.AI math.OC
\\
  Support vector machine (SVM) has achieved many successes in machine learning,
especially for a small sample problem. As a famous extension of the traditional
SVM, the $\nu$ support vector machine ($\nu$-SVM) has shown outstanding
performance due to its great model interpretability. However, it still faces
challenges in training overhead for large-scale problems. To address this
issue, we propose a safe screening rule with bi-level optimization for
$\nu$-SVM (SRBO-$\nu$-SVM) which can screen out inactive samples before
training and reduce the computational cost without sacrificing the prediction
accuracy. Our SRBO-$\nu$-SVM is strictly deduced by integrating the
Karush-Kuhn-Tucker (KKT) conditions, the variational inequalities of convex
problems and the $\nu$-property. Furthermore, we develop an efficient dual
coordinate descent method (DCDM) to further improve computational speed.
Finally, a unified framework for SRBO is proposed to accelerate many SVM-type
models, and it is successfully applied to one-class SVM. Experimental results
on 6 artificial data sets and 30 benchmark data sets have verified the
effectiveness and safety of our proposed methods in supervised and unsupervised
tasks.
\\ ( https://arxiv.org/abs/2403.01769 ,  2675kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01773
Date: Mon, 4 Mar 2024 07:03:10 GMT   (2396kb,D)

Title: Improving out-of-distribution generalization in graphs via hierarchical
  semantic environments
Authors: Yinhua Piao, Sangseon Lee, Yijingxiu Lu, Sun Kim
Categories: cs.LG cs.AI
Comments: Accepted by CVPR 2024
\\
  Out-of-distribution (OOD) generalization in the graph domain is challenging
due to complex distribution shifts and a lack of environmental contexts. Recent
methods attempt to enhance graph OOD generalization by generating flat
environments. However, such flat environments come with inherent limitations to
capture more complex data distributions. Considering the DrugOOD dataset, which
contains diverse training environments (e.g., scaffold, size, etc.), flat
contexts cannot sufficiently address its high heterogeneity. Thus, a new
challenge is posed to generate more semantically enriched environments to
enhance graph invariant learning for handling distribution shifts. In this
paper, we propose a novel approach to generate hierarchical semantic
environments for each graph. Firstly, given an input graph, we explicitly
extract variant subgraphs from the input graph to generate proxy predictions on
local environments. Then, stochastic attention mechanisms are employed to
re-extract the subgraphs for regenerating global environments in a hierarchical
manner. In addition, we introduce a new learning objective that guides our
model to learn the diversity of environments within the same hierarchy while
maintaining consistency across different hierarchies. This approach enables our
model to consider the relationships between environments and facilitates robust
graph invariant learning. Extensive experiments on real-world graph data have
demonstrated the effectiveness of our framework. Particularly, in the
challenging dataset DrugOOD, our method achieves up to 1.29\% and 2.83\%
improvement over the best baselines on IC50 and EC50 prediction tasks,
respectively.
\\ ( https://arxiv.org/abs/2403.01773 ,  2396kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01801
Date: Mon, 4 Mar 2024 07:45:29 GMT   (556kb,D)

Title: COLA: Cross-city Mobility Transformer for Human Trajectory Simulation
Authors: Yu Wang, Tongya Zheng, Yuxuan Liang, Shunyu Liu, Mingli Song
Categories: cs.LG cs.AI
Comments: Accepted by WWW 2024
DOI: 10.1145/3589334.3645469
\\
  Human trajectory data produced by daily mobile devices has proven its
usefulness in various substantial fields such as urban planning and epidemic
prevention. In terms of the individual privacy concern, human trajectory
simulation has attracted increasing attention from researchers, targeting at
offering numerous realistic mobility data for downstream tasks. Nevertheless,
the prevalent issue of data scarcity undoubtedly degrades the reliability of
existing deep learning models. In this paper, we are motivated to explore the
intriguing problem of mobility transfer across cities, grasping the universal
patterns of human trajectories to augment the powerful Transformer with
external mobility data. There are two crucial challenges arising in the
knowledge transfer across cities: 1) how to transfer the Transformer to adapt
for domain heterogeneity; 2) how to calibrate the Transformer to adapt for
subtly different long-tail frequency distributions of locations. To address
these challenges, we have tailored a Cross-city mObiLity trAnsformer (COLA)
with a dedicated model-agnostic transfer framework by effectively transferring
cross-city knowledge for human trajectory simulation. Firstly, COLA divides the
Transformer into the private modules for city-specific characteristics and the
shared modules for city-universal mobility patterns. Secondly, COLA leverages a
lightweight yet effective post-hoc adjustment strategy for trajectory
simulation, without disturbing the complex bi-level optimization of
model-agnostic knowledge transfer. Extensive experiments of COLA compared to
state-of-the-art single-city baselines and our implemented cross-city baselines
have demonstrated its superiority and effectiveness. The code is available at
https://github.com/Star607/Cross-city-Mobility-Transformer.
\\ ( https://arxiv.org/abs/2403.01801 ,  556kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01845
Date: Mon, 4 Mar 2024 08:51:38 GMT   (1159kb,D)

Title: NASH: Neural Architecture Search for Hardware-Optimized Machine Learning
  Models
Authors: Mengfei Ji, Zaid Al-Ars
Categories: cs.LG cs.AI cs.CV
\\
  As machine learning (ML) algorithms get deployed in an ever-increasing number
of applications, these algorithms need to achieve better trade-offs between
high accuracy, high throughput and low latency. This paper introduces NASH, a
novel approach that applies neural architecture search to machine learning
hardware. Using NASH, hardware designs can achieve not only high throughput and
low latency but also superior accuracy performance. We present four versions of
the NASH strategy in this paper, all of which show higher accuracy than the
original models. The strategy can be applied to various convolutional neural
networks, selecting specific model operations among many to guide the training
process toward higher accuracy. Experimental results show that applying NASH on
ResNet18 or ResNet34 achieves a top 1 accuracy increase of up to 3.1% and a top
5 accuracy increase of up to 2.2% compared to the non-NASH version when tested
on the ImageNet data set. We also integrated this approach into the FINN
hardware model synthesis tool to automate the application of our approach and
the generation of the hardware model. Results show that using FINN can achieve
a maximum throughput of 324.5 fps. In addition, NASH models can also result in
a better trade-off between accuracy and hardware resource utilization. The
accuracy-hardware (HW) Pareto curve shows that the models with the four NASH
versions represent the best trade-offs achieving the highest accuracy for a
given HW utilization. The code for our implementation is open-source and
publicly available on GitHub at https://github.com/MFJI/NASH.
\\ ( https://arxiv.org/abs/2403.01845 ,  1159kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01857
Date: Mon, 4 Mar 2024 09:13:14 GMT   (59kb)

Title: Reward Model Learning vs. Direct Policy Optimization: A Comparative
  Analysis of Learning from Human Preferences
Authors: Andi Nika, Debmalya Mandal, Parameswaran Kamalaruban, Georgios
  Tzannetos, Goran Radanovi\'c and Adish Singla
Categories: cs.LG
\\
  In this paper, we take a step towards a deeper understanding of learning from
human preferences by systematically comparing the paradigm of reinforcement
learning from human feedback (RLHF) with the recently proposed paradigm of
direct preference optimization (DPO). We focus our attention on the class of
loglinear policy parametrization and linear reward functions. In order to
compare the two paradigms, we first derive minimax statistical bounds on the
suboptimality gap induced by both RLHF and DPO, assuming access to an oracle
that exactly solves the optimization problems. We provide a detailed discussion
on the relative comparison between the two paradigms, simultaneously taking
into account the sample size, policy and reward class dimensions, and the
regularization temperature. Moreover, we extend our analysis to the approximate
optimization setting and derive exponentially decaying convergence rates for
both RLHF and DPO. Next, we analyze the setting where the ground-truth reward
is not realizable and find that, while RLHF incurs a constant additional error,
DPO retains its asymptotically decaying gap by just tuning the temperature
accordingly. Finally, we extend our comparison to the Markov decision process
setting, where we generalize our results with exact optimization. To the best
of our knowledge, we are the first to provide such a comparative analysis for
RLHF and DPO.
\\ ( https://arxiv.org/abs/2403.01857 ,  59kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01874
Date: Mon, 4 Mar 2024 09:30:35 GMT   (289kb,D)

Title: A Survey on Evaluation of Out-of-Distribution Generalization
Authors: Han Yu, Jiashuo Liu, Xingxuan Zhang, Jiayun Wu, Peng Cui
Categories: cs.LG
\\
  Machine learning models, while progressively advanced, rely heavily on the
IID assumption, which is often unfulfilled in practice due to inevitable
distribution shifts. This renders them susceptible and untrustworthy for
deployment in risk-sensitive applications. Such a significant problem has
consequently spawned various branches of works dedicated to developing
algorithms capable of Out-of-Distribution (OOD) generalization. Despite these
efforts, much less attention has been paid to the evaluation of OOD
generalization, which is also a complex and fundamental problem. Its goal is
not only to assess whether a model's OOD generalization capability is strong or
not, but also to evaluate where a model generalizes well or poorly. This
entails characterizing the types of distribution shifts that a model can
effectively address, and identifying the safe and risky input regions given a
model. This paper serves as the first effort to conduct a comprehensive review
of OOD evaluation. We categorize existing research into three paradigms: OOD
performance testing, OOD performance prediction, and OOD intrinsic property
characterization, according to the availability of test data. Additionally, we
briefly discuss OOD evaluation in the context of pretrained models. In closing,
we propose several promising directions for future research in OOD evaluation.
\\ ( https://arxiv.org/abs/2403.01874 ,  289kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01875
Date: Mon, 4 Mar 2024 09:31:56 GMT   (573kb,D)

Title: ICLN: Input Convex Loss Network for Decision Focused Learning
Authors: Haeun Jeon, Hyunglip Bae, Minsu Park, Chanyeong Kim, Woo Chang Kim
Categories: cs.LG cs.AI
\\
  In decision-making problem under uncertainty, predicting unknown parameters
is often considered independent of the optimization part. Decision-focused
Learning (DFL) is a task-oriented framework to integrate prediction and
optimization by adapting predictive model to give better decision for the
corresponding task. Here, an inevitable challenge arises when computing
gradients of the optimal decision with respect to the parameters. Existing
researches cope this issue by smoothly reforming surrogate optimization or
construct surrogate loss function that mimic task loss. However, they are
applied to restricted optimization domain or build functions in a local manner
leading a large computational time. In this paper, we propose Input Convex Loss
Network (ICLN), a novel global surrogate loss which can be implemented in a
general DFL paradigm. ICLN learns task loss via Input Convex Neural Networks
which is guaranteed to be convex for some inputs, while keeping the global
structure for the other inputs. This enables ICLN to admit general DFL through
only a single surrogate loss without any sense for choosing appropriate
parametric forms. We confirm effectiveness and flexibility of ICLN by
evaluating our proposed model with three stochastic decision-making problems.
\\ ( https://arxiv.org/abs/2403.01875 ,  573kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01895
Date: Mon, 4 Mar 2024 09:55:16 GMT   (207kb,D)

Title: Unsupervised Distance Metric Learning for Anomaly Detection Over
  Multivariate Time Series
Authors: Hanyang Yuan, Qinglin Cai, Keting Yin
Categories: cs.LG cs.AI
\\
  Distance-based time series anomaly detection methods are prevalent due to
their relative non-parametric nature and interpretability. However, the
commonly used Euclidean distance is sensitive to noise. While existing works
have explored dynamic time warping (DTW) for its robustness, they only support
supervised tasks over multivariate time series (MTS), leaving a scarcity of
unsupervised methods. In this work, we propose FCM-wDTW, an unsupervised
distance metric learning method for anomaly detection over MTS, which encodes
raw data into latent space and reveals normal dimension relationships through
cluster centers. FCM-wDTW introduces locally weighted DTW into fuzzy C-means
clustering and learns the optimal latent space efficiently, enabling anomaly
identification via data reconstruction. Experiments with 11 different types of
benchmarks demonstrate our method's competitive accuracy and efficiency.
\\ ( https://arxiv.org/abs/2403.01895 ,  207kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01896
Date: Mon, 4 Mar 2024 09:55:43 GMT   (531kb,D)

Title: Robustness Bounds on the Successful Adversarial Examples: Theory and
  Practice
Authors: Hiroaki Maeshima, Akira Otsuka
Categories: cs.LG cs.CR stat.ML
\\
  Adversarial example (AE) is an attack method for machine learning, which is
crafted by adding imperceptible perturbation to the data inducing
misclassification. In the current paper, we investigated the upper bound of the
probability of successful AEs based on the Gaussian Process (GP)
classification. We proved a new upper bound that depends on AE's perturbation
norm, the kernel function used in GP, and the distance of the closest pair with
different labels in the training dataset. Surprisingly, the upper bound is
determined regardless of the distribution of the sample dataset. We showed that
our theoretical result was confirmed through the experiment using ImageNet. In
addition, we showed that changing the parameters of the kernel function induces
a change of the upper bound of the probability of successful AEs.
\\ ( https://arxiv.org/abs/2403.01896 ,  531kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01919
Date: Mon, 4 Mar 2024 10:36:06 GMT   (1537kb,D)

Title: Matrix Completion with Convex Optimization and Column Subset Selection
Authors: Antonina Krajewska and Ewa Niewiadomska-Szynkiewicz
Categories: cs.LG
ACM-class: G.1.3; G.1.6
\\
  We introduce a two-step method for the matrix recovery problem. Our approach
combines the theoretical foundations of the Column Subset Selection and
Low-rank Matrix Completion problems. The proposed method, in each step, solves
a convex optimization task. We present two algorithms that implement our
Columns Selected Matrix Completion (CSMC) method, each dedicated to a different
size problem. We performed a formal analysis of the presented method, in which
we formulated the necessary assumptions and the probability of finding a
correct solution. In the second part of the paper, we present the results of
the experimental work. Numerical experiments verified the correctness and
performance of the algorithms. To study the influence of the matrix size, rank,
and the proportion of missing elements on the quality of the solution and the
computation time, we performed experiments on synthetic data. The presented
method was applied to two real-life problems problems: prediction of movie
rates in a recommendation system and image inpainting. Our thorough analysis
shows that CSMC provides solutions of comparable quality to matrix completion
algorithms, which are based on convex optimization. However, CSMC offers
notable savings in terms of runtime.
\\ ( https://arxiv.org/abs/2403.01919 ,  1537kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01922
Date: Mon, 4 Mar 2024 10:39:58 GMT   (232kb,D)

Title: FlowPrecision: Advancing FPGA-Based Real-Time Fluid Flow Estimation with
  Linear Quantization
Authors: Tianheng Ling, Julian Hoever, Chao Qian, Gregor Schiele
Categories: cs.LG physics.flu-dyn
Comments: 6 pages, 3 figures, The 22nd International Conference on Pervasive
  Computing and Communications (PerCom 2024), PerConAI Workshop
\\
  In industrial and environmental monitoring, achieving real-time and precise
fluid flow measurement remains a critical challenge. This study applies linear
quantization in FPGA-based soft sensors for fluid flow estimation,
significantly enhancing Neural Network model precision by overcoming the
limitations of traditional fixed-point quantization. Our approach achieves up
to a 10.10% reduction in Mean Squared Error and a notable 9.39% improvement in
inference speed through targeted hardware optimizations. Validated across
multiple data sets, our findings demonstrate that the optimized FPGA-based
quantized models can provide efficient, accurate real-time inference, offering
a viable alternative to cloud-based processing in pervasive autonomous systems.
\\ ( https://arxiv.org/abs/2403.01922 ,  232kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01942
Date: Mon, 4 Mar 2024 11:24:51 GMT   (1098kb,D)

Title: Mitigating Label Noise on Graph via Topological Sample Selection
Authors: Yuhao Wu, Jiangchao Yao, Xiaobo Xia, Jun Yu, Ruxin Wang, Bo Han,
  Tongliang Liu
Categories: cs.LG
\\
  Despite the success of the carefully-annotated benchmarks, the effectiveness
of existing graph neural networks (GNNs) can be considerably impaired in
practice when the real-world graph data is noisily labeled. Previous
explorations in sample selection have been demonstrated as an effective way for
robust learning with noisy labels, however, the conventional studies focus on
i.i.d data, and when moving to non-iid graph data and GNNs, two notable
challenges remain: (1) nodes located near topological class boundaries are very
informative for classification but cannot be successfully distinguished by the
heuristic sample selection. (2) there is no available measure that considers
the graph topological information to promote sample selection in a graph. To
address this dilemma, we propose a $\textit{Topological Sample Selection}$
(TSS) method that boosts the informative sample selection process in a graph by
utilising topological information. We theoretically prove that our procedure
minimizes an upper bound of the expected risk under target clean distribution,
and experimentally show the superiority of our method compared with
state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2403.01942 ,  1098kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01946
Date: Mon, 4 Mar 2024 11:32:18 GMT   (3850kb,D)

Title: A Generative Model of Symmetry Transformations
Authors: James Urquhart Allingham, Bruno Kacper Mlodozeniec, Shreyas Padhy,
  Javier Antor\'an, David Krueger, Richard E. Turner, Eric Nalisnick and Jos\'e
  Miguel Hern\'andez-Lobato
Categories: cs.LG
\\
  Correctly capturing the symmetry transformations of data can lead to
efficient models with strong generalization capabilities, though methods
incorporating symmetries often require prior knowledge. While recent
advancements have been made in learning those symmetries directly from the
dataset, most of this work has focused on the discriminative setting. In this
paper, we construct a generative model that explicitly aims to capture
symmetries in the data, resulting in a model that learns which symmetries are
present in an interpretable way. We provide a simple algorithm for efficiently
learning our generative model and demonstrate its ability to capture symmetries
under affine and color transformations. Combining our symmetry model with
existing generative models results in higher marginal test-log-likelihoods and
robustness to data sparsification.
\\ ( https://arxiv.org/abs/2403.01946 ,  3850kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02004
Date: Mon, 4 Mar 2024 12:57:26 GMT   (37kb)

Title: Error bounds for particle gradient descent, and extensions of the
  log-Sobolev and Talagrand inequalities
Authors: Rocco Caprio, Juan Kuntz, Samuel Power and Adam M. Johansen
Categories: cs.LG math.FA math.OC stat.CO stat.ML
\\
  We prove non-asymptotic error bounds for particle gradient descent
(PGD)~(Kuntz et al., 2023), a recently introduced algorithm for maximum
likelihood estimation of large latent variable models obtained by discretizing
a gradient flow of the free energy. We begin by showing that, for models
satisfying a condition generalizing both the log-Sobolev and the
Polyak--{\L}ojasiewicz inequalities (LSI and P{\L}I, respectively), the flow
converges exponentially fast to the set of minimizers of the free energy. We
achieve this by extending a result well-known in the optimal transport
literature (that the LSI implies the Talagrand inequality) and its counterpart
in the optimization literature (that the P{\L}I implies the so-called quadratic
growth condition), and applying it to our new setting. We also generalize the
Bakry--\'Emery Theorem and show that the LSI/P{\L}I generalization holds for
models with strongly concave log-likelihoods. For such models, we further
control PGD's discretization error, obtaining non-asymptotic error bounds.
While we are motivated by the study of PGD, we believe that the inequalities
and results we extend may be of independent interest.
\\ ( https://arxiv.org/abs/2403.02004 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02042
Date: Mon, 4 Mar 2024 13:47:33 GMT   (412kb,D)

Title: Deep Neural Network for Constraint Acquisition through Tailored Loss
  Function
Authors: Eduardo Vyhmeister, Rocio Paez, Gabriel Gonzalez
Categories: cs.LG cs.SC
\\
  The significance of learning constraints from data is underscored by its
potential applications in real-world problem-solving. While constraints are
popular for modeling and solving, the approaches to learning constraints from
data remain relatively scarce. Furthermore, the intricate task of modeling
demands expertise and is prone to errors, thus constraint acquisition methods
offer a solution by automating this process through learnt constraints from
examples or behaviours of solutions and non-solutions. This work introduces a
novel approach grounded in Deep Neural Network (DNN) based on Symbolic
Regression that, by setting suitable loss functions, constraints can be
extracted directly from datasets. Using the present approach, direct
formulation of constraints was achieved. Furthermore, given the broad
pre-developed architectures and functionalities of DNN, connections and
extensions with other frameworks could be foreseen.
\\ ( https://arxiv.org/abs/2403.02042 ,  412kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02107
Date: Mon, 4 Mar 2024 15:07:33 GMT   (5858kb,D)

Title: Iterated $Q$-Network: Beyond the One-Step Bellman Operator
Authors: Th\'eo Vincent, Daniel Palenicek, Boris Belousov, Jan Peters, Carlo
  D'Eramo
Categories: cs.LG cs.AI
Comments: Preprint
\\
  Value-based Reinforcement Learning (RL) methods rely on the application of
the Bellman operator, which needs to be approximated from samples. Most
approaches consist of an iterative scheme alternating the application of the
Bellman operator and a subsequent projection step onto a considered function
space. However, we observe that these algorithms can be improved by considering
multiple iterations of the Bellman operator at once. Thus, we introduce
iterated $Q$-Networks (iQN), a novel approach that learns a sequence of
$Q$-function approximations where each $Q$-function serves as the target for
the next one in a chain of consecutive Bellman iterations. We demonstrate that
iQN is theoretically sound and show how it can be seamlessly used in
value-based and actor-critic methods. We empirically demonstrate its advantages
on Atari $2600$ games and in continuous-control MuJoCo environments.
\\ ( https://arxiv.org/abs/2403.02107 ,  5858kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02116
Date: Mon, 4 Mar 2024 15:20:19 GMT   (3679kb,D)

Title: Inf2Guard: An Information-Theoretic Framework for Learning
  Privacy-Preserving Representations against Inference Attacks
Authors: Sayedeh Leila Noorbakhsh, Binghui Zhang, Yuan Hong, Binghui Wang
Categories: cs.LG cs.CR
Comments: Accepted by Usenix Security 2024
\\
  Machine learning (ML) is vulnerable to inference (e.g., membership inference,
property inference, and data reconstruction) attacks that aim to infer the
private information of training data or dataset. Existing defenses are only
designed for one specific type of attack and sacrifice significant utility or
are soon broken by adaptive attacks. We address these limitations by proposing
an information-theoretic defense framework, called Inf2Guard, against the three
major types of inference attacks. Our framework, inspired by the success of
representation learning, posits that learning shared representations not only
saves time/costs but also benefits numerous downstream tasks. Generally,
Inf2Guard involves two mutual information objectives, for privacy protection
and utility preservation, respectively. Inf2Guard exhibits many merits: it
facilitates the design of customized objectives against the specific inference
attack; it provides a general defense framework which can treat certain
existing defenses as special cases; and importantly, it aids in deriving
theoretical results, e.g., inherent utility-privacy tradeoff and guaranteed
privacy leakage. Extensive evaluations validate the effectiveness of Inf2Guard
for learning privacy-preserving representations against inference attacks and
demonstrate the superiority over the baselines.
\\ ( https://arxiv.org/abs/2403.02116 ,  3679kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02185
Date: Mon, 4 Mar 2024 16:27:21 GMT   (1047kb,D)

Title: Distilled ChatGPT Topic & Sentiment Modeling with Applications in
  Finance
Authors: Olivier Gandouet, Mouloud Belbahri, Armelle Jezequel, Yuriy Bodjov
Categories: cs.LG cs.CE cs.CL
Comments: Edge Intelligence Workshop at AAAI24
\\
  In this study, ChatGPT is utilized to create streamlined models that generate
easily interpretable features. These features are then used to evaluate
financial outcomes from earnings calls. We detail a training approach that
merges knowledge distillation and transfer learning, resulting in lightweight
topic and sentiment classification models without significant loss in accuracy.
These models are assessed through a dataset annotated by experts. The paper
also delves into two practical case studies, highlighting how the generated
features can be effectively utilized in quantitative investing scenarios.
\\ ( https://arxiv.org/abs/2403.02185 ,  1047kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02187
Date: Mon, 4 Mar 2024 16:28:04 GMT   (123kb)

Title: Mutual Information Estimation via Normalizing Flows
Authors: Ivan Butakov, Aleksander Tolmachev, Sofia Malanchuk, Anna
  Neopryatnaya, Alexey Frolov, Kirill Andreev
Categories: cs.LG cs.IT math.IT stat.ML
Comments: 15 pages, 5 figures
\\
  We propose a novel approach to the problem of mutual information (MI)
estimation via introducing normalizing flows-based estimator. The estimator
maps original data to the target distribution with known closed-form expression
for MI. We demonstrate that our approach yields MI estimates for the original
data. Experiments with high-dimensional data are provided to show the
advantages of the proposed estimator.
\\ ( https://arxiv.org/abs/2403.02187 ,  123kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02215
Date: Mon, 4 Mar 2024 17:02:23 GMT   (556kb,D)

Title: Joint Parameter and Parameterization Inference with Uncertainty
  Quantification through Differentiable Programming
Authors: Yongquan Qu, Mohamed Aziz Bhouri, Pierre Gentine
Categories: cs.LG math.DS nlin.CD physics.ao-ph
\\
  Accurate representations of unknown and sub-grid physical processes through
parameterizations (or closure) in numerical simulations with quantified
uncertainty are critical for resolving the coarse-grained partial differential
equations that govern many problems ranging from weather and climate prediction
to turbulence simulations. Recent advances have seen machine learning (ML)
increasingly applied to model these subgrid processes, resulting in the
development of hybrid physics-ML models through the integration with numerical
solvers. In this work, we introduce a novel framework for the joint estimation
and uncertainty quantification of physical parameters and machine learning
parameterizations in tandem, leveraging differentiable programming. Achieved
through online training and efficient Bayesian inference within a
high-dimensional parameter space, this approach is enabled by the capabilities
of differentiable programming. This proof of concept underscores the
substantial potential of differentiable programming in synergistically
combining machine learning with differential equations, thereby enhancing the
capabilities of hybrid physics-ML modeling.
\\ ( https://arxiv.org/abs/2403.02215 ,  556kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02221
Date: Mon, 4 Mar 2024 17:08:57 GMT   (805kb,D)

Title: TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language
  Models
Authors: Yilong Ren, Yue Chen, Shuai Liu, Boyue Wang, Haiyang Yu, and Zhiyong
  Cui
Categories: cs.LG
\\
  Traffic prediction constitutes a pivotal facet within the purview of
Intelligent Transportation Systems (ITS), and the attainment of highly precise
predictions holds profound significance for efficacious traffic management. The
precision of prevailing deep learning-driven traffic prediction models
typically sees an upward trend with a rise in the volume of training data.
However, the procurement of comprehensive spatiotemporal datasets for traffic
is often fraught with challenges, primarily stemming from the substantial costs
associated with data collection and retention. Consequently, developing a model
that can achieve accurate predictions and good generalization ability in areas
with limited historical traffic data is a challenging problem. It is noteworthy
that the rapidly advancing pretrained Large Language Models (LLMs) of recent
years have demonstrated exceptional proficiency in cross-modality knowledge
transfer and few-shot learning. Recognizing the sequential nature of traffic
data, similar to language, we introduce TPLLM, a novel traffic prediction
framework leveraging LLMs. In this framework, we construct a sequence embedding
layer based on Convolutional Neural Networks (CNNs) and a graph embedding layer
based on Graph Convolutional Networks (GCNs) to extract sequence features and
spatial features, respectively. These are subsequently integrated to form
inputs that are suitable for LLMs. A Low-Rank Adaptation (LoRA) fine-tuning
approach is applied to TPLLM, thereby facilitating efficient learning and
minimizing computational demands. Experiments on two real-world datasets
demonstrate that TPLLM exhibits commendable performance in both full-sample and
few-shot prediction scenarios, effectively supporting the development of ITS in
regions with scarce historical traffic data.
\\ ( https://arxiv.org/abs/2403.02221 ,  805kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02233
Date: Mon, 4 Mar 2024 17:24:03 GMT   (616kb,D)

Title: Transformers Provably Learn Feature-Position Correlations in Masked
  Image Modeling
Authors: Yu Huang, Zixin Wen, Yuejie Chi, Yingbin Liang
Categories: cs.LG math.OC stat.ML
Comments: 52 pages, 3 figures
\\
  Masked image modeling (MIM), which predicts randomly masked patches from
unmasked ones, has emerged as a promising approach in self-supervised vision
pretraining. However, the theoretical understanding of MIM is rather limited,
especially with the foundational architecture of transformers. In this paper,
to the best of our knowledge, we provide the first end-to-end theory of
learning one-layer transformers with softmax attention in MIM self-supervised
pretraining. On the conceptual side, we posit a theoretical mechanism of how
transformers, pretrained with MIM, produce empirically observed local and
diverse attention patterns on data distributions with spatial structures that
highlight feature-position correlations. On the technical side, our end-to-end
analysis of the training dynamics of softmax-based transformers accommodates
both input and position embeddings simultaneously, which is developed based on
a novel approach to track the interplay between the attention of
feature-position and position-wise correlations.
\\ ( https://arxiv.org/abs/2403.02233 ,  616kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02241
Date: Mon, 4 Mar 2024 17:33:20 GMT   (6018kb,D)

Title: Neural Redshift: Random Networks are not Random Functions
Authors: Damien Teney, Armand Nicolicioiu, Valentin Hartmann, Ehsan Abbasnejad
Categories: cs.LG cs.AI cs.CV
Journal-ref: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  2024
\\
  Our understanding of the generalization capabilities of neural networks (NNs)
is still incomplete. Prevailing explanations are based on implicit biases of
gradient descent (GD) but they cannot account for the capabilities of models
from gradient-free methods nor the simplicity bias recently observed in
untrained networks. This paper seeks other sources of generalization in NNs.
  Findings. To understand the inductive biases provided by architectures
independently from GD, we examine untrained, random-weight networks. Even
simple MLPs show strong inductive biases: uniform sampling in weight space
yields a very biased distribution of functions in terms of complexity. But
unlike common wisdom, NNs do not have an inherent "simplicity bias". This
property depends on components such as ReLUs, residual connections, and layer
normalizations. Alternative architectures can be built with a bias for any
level of complexity. Transformers also inherit all these properties from their
building blocks.
  Implications. We provide a fresh explanation for the success of deep learning
independent from gradient-based training. It points at promising avenues for
controlling the solutions implemented by trained models.
\\ ( https://arxiv.org/abs/2403.02241 ,  6018kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02243
Date: Mon, 4 Mar 2024 17:33:39 GMT   (2510kb,D)

Title: Better Schedules for Low Precision Training of Deep Neural Networks
Authors: Cameron R. Wolfe and Anastasios Kyrillidis
Categories: cs.LG cs.AI
Comments: 20 pages, 8 figures, 1 table, ACML 2023
ACM-class: I.2.6; I.2.10; I.4.0
Journal-ref: Wolfe, Cameron R., and Anastasios Kyrillidis. "Better schedules
  for low precision training of deep neural networks." Machine Learning (2024):
  1-19
DOI: 10.1007/s10994-023-06480-0
\\
  Low precision training can significantly reduce the computational overhead of
training deep neural networks (DNNs). Though many such techniques exist, cyclic
precision training (CPT), which dynamically adjusts precision throughout train-
ing according to a cyclic schedule, achieves particularly impressive
improvements in training efficiency, while actually improving DNN performance.
Existing CPT implementations take common learning rate schedules (e.g.,
cyclical cosine sched- ules) and use them for low precision training without
adequate comparisons to alternative scheduling options. We define a diverse
suite of CPT schedules and analyze their performance across a variety of DNN
training regimes, some of which are unexplored in the low precision training
literature (e.g., node classification with graph neural networks). From these
experiments, we discover alternative CPT schedules that offer further
improvements in training efficiency and model performance, as well as derive a
set of best practices for choosing CPT schedules. Going further, we find that a
correlation exists between model performance and training cost, and that
changing the underlying CPT schedule can control the tradeoff between these two
variables. To explain the direct correlation between model performance and
training cost, we draw a connection between quantized training and critical
learning periods, suggesting that aggressive quantization is a form of learning
impairment that can permanently damage model performance.
\\ ( https://arxiv.org/abs/2403.02243 ,  2510kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02289
Date: Mon, 4 Mar 2024 18:18:52 GMT   (4623kb,D)

Title: Physics-Informed Neural Networks with Skip Connections for Modeling and
  Control of Gas-Lifted Oil Wells
Authors: Jonas Ekeland Kittelsen, Eric Aislan Antonelo, Eduardo Camponogara,
  Lars Struen Imsland
Categories: cs.LG
\\
  Neural networks, while powerful, often lack interpretability.
Physics-Informed Neural Networks (PINNs) address this limitation by
incorporating physics laws into the loss function, making them applicable to
solving Ordinary Differential Equations (ODEs) and Partial Differential
Equations (PDEs). The recently introduced PINC framework extends PINNs to
control applications, allowing for open-ended long-range prediction and control
of dynamic systems. In this work, we enhance PINC for modeling highly nonlinear
systems such as gas-lifted oil wells. By introducing skip connections in the
PINC network and refining certain terms in the ODE, we achieve more accurate
gradients during training, resulting in an effective modeling process for the
oil well system. Our proposed improved PINC demonstrates superior performance,
reducing the validation prediction error by an average of 67% in the oil well
application and significantly enhancing gradient flow through the network
layers, increasing its magnitude by four orders of magnitude compared to the
original PINC. Furthermore, experiments showcase the efficacy of Model
Predictive Control (MPC) in regulating the bottom-hole pressure of the oil well
using the improved PINC model, even in the presence of noisy measurements.
\\ ( https://arxiv.org/abs/2403.02289 ,  4623kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02292
Date: Mon, 4 Mar 2024 18:21:56 GMT   (3928kb,D)

Title: A Decade of Privacy-Relevant Android App Reviews: Large Scale Trends
Authors: Omer Akgul, Sai Teja Peddinti, Nina Taft, Michelle L. Mazurek, Hamza
  Harkous, Animesh Srivastava, Benoit Seguin
Categories: cs.LG cs.HC
\\
  We present an analysis of 12 million instances of privacy-relevant reviews
publicly visible on the Google Play Store that span a 10 year period. By
leveraging state of the art NLP techniques, we can examine what users have been
writing about privacy along multiple dimensions: time, countries, app types,
diverse privacy topics, and even across a spectrum of emotions. We find
consistent growth of privacy-relevant reviews, and explore topics that are
trending (such as Data Deletion and Data Theft), as well as those on the
decline (such as privacy-relevant reviews on sensitive permissions). We find
that although privacy reviews come from more than 200 countries, 33 countries
provide 90% of privacy reviews. We conduct a comparison across countries by
examining the distribution of privacy topics a country's users write about, and
find that geographic proximity is not a reliable indicator that nearby
countries have similar privacy perspectives. We uncover some countries with
unique patterns and explore those herein. Surprisingly, we uncover that it is
not uncommon for reviews that discuss privacy to be positive (32%); many users
express pleasure about privacy features within apps or privacy-focused apps. We
also uncover some unexpected behaviors, such as the use of reviews to deliver
privacy disclaimers to developers. Finally, we demonstrate the value of
analyzing app reviews with our approach as a complement to existing methods for
understanding users' perspectives about privacy.
\\ ( https://arxiv.org/abs/2403.02292 ,  3928kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02310
Date: Mon, 4 Mar 2024 18:47:08 GMT   (412kb,D)

Title: Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve
Authors: Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun
  Kwatra, Bhargav S. Gulavani, Alexey Tumanov, Ramachandran Ramjee
Categories: cs.LG cs.DC
\\
  Each LLM serving request goes through two phases. The first is prefill which
processes the entire input prompt to produce one output token and the second is
decode which generates the rest of output tokens, one-at-a-time. Prefill
iterations have high latency but saturate GPU compute due to parallel
processing of the input prompt. In contrast, decode iterations have low latency
but also low compute utilization because a decode iteration processes only a
single token per request. This makes batching highly effective for decodes and
consequently for overall throughput. However, batching multiple requests leads
to an interleaving of prefill and decode iterations which makes it challenging
to achieve both high throughput and low latency.
  We introduce an efficient LLM inference scheduler Sarathi-Serve inspired by
the techniques we originally proposed for optimizing throughput in Sarathi.
Sarathi-Serve leverages chunked-prefills from Sarathi to create stall-free
schedules that can add new requests in a batch without pausing ongoing decodes.
Stall-free scheduling unlocks the opportunity to improve throughput with large
batch sizes while minimizing the effect of batching on latency. Our evaluation
shows that Sarathi-Serve improves serving throughput within desired latency
SLOs of Mistral-7B by up to 2.6x on a single A100 GPU and up to 6.9x for
Falcon-180B on 8 A100 GPUs over Orca and vLLM.
\\ ( https://arxiv.org/abs/2403.02310 ,  412kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02329
Date: Mon, 4 Mar 2024 18:57:11 GMT   (25026kb,D)

Title: COMMIT: Certifying Robustness of Multi-Sensor Fusion Systems against
  Semantic Attacks
Authors: Zijian Huang, Wenda Chu, Linyi Li, Chejian Xu, Bo Li
Categories: cs.LG cs.CR cs.CV
\\
  Multi-sensor fusion systems (MSFs) play a vital role as the perception module
in modern autonomous vehicles (AVs). Therefore, ensuring their robustness
against common and realistic adversarial semantic transformations, such as
rotation and shifting in the physical world, is crucial for the safety of AVs.
While empirical evidence suggests that MSFs exhibit improved robustness
compared to single-modal models, they are still vulnerable to adversarial
semantic transformations. Despite the proposal of empirical defenses, several
works show that these defenses can be attacked again by new adaptive attacks.
So far, there is no certified defense proposed for MSFs. In this work, we
propose the first robustness certification framework COMMIT certify robustness
of multi-sensor fusion systems against semantic attacks. In particular, we
propose a practical anisotropic noise mechanism that leverages randomized
smoothing with multi-modal data and performs a grid-based splitting method to
characterize complex semantic transformations. We also propose efficient
algorithms to compute the certification in terms of object detection accuracy
and IoU for large-scale MSF models. Empirically, we evaluate the efficacy of
COMMIT in different settings and provide a comprehensive benchmark of certified
robustness for different MSF models using the CARLA simulation platform. We
show that the certification for MSF models is at most 48.39% higher than that
of single-modal models, which validates the advantages of MSF models. We
believe our certification framework and benchmark will contribute an important
step towards certifiably robust AVs in practice.
\\ ( https://arxiv.org/abs/2403.02329 ,  25026kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02334
Date: Mon, 4 Mar 2024 18:58:46 GMT   (426kb,D)

Title: Gradient Correlation Subspace Learning against Catastrophic Forgetting
Authors: Tammuz Dubnov, Vishal Thengane
Categories: cs.LG cs.AI cs.CV
Comments: 5 figures; Code will be available here:
  https://github.com/vgthengane/GCSL
\\
  Efficient continual learning techniques have been a topic of significant
research over the last few years. A fundamental problem with such learning is
severe degradation of performance on previously learned tasks, known also as
catastrophic forgetting. This paper introduces a novel method to reduce
catastrophic forgetting in the context of incremental class learning called
Gradient Correlation Subspace Learning (GCSL). The method detects a subspace of
the weights that is least affected by previous tasks and projects the weights
to train for the new task into said subspace. The method can be applied to one
or more layers of a given network architectures and the size of the subspace
used can be altered from layer to layer and task to task. Code will be
available at
\href{https://github.com/vgthengane/GCSL}{https://github.com/vgthengane/GCSL}
\\ ( https://arxiv.org/abs/2403.02334 ,  426kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2403.00765 (*cross-listing*)
Date: Tue, 6 Feb 2024 12:08:01 GMT   (188kb,D)

Title: An Architecture for Unattended Containerized (Deep) Reinforcement
  Learning with Webots
Authors: Tobias Haubold, Petra Linke
Categories: cs.RO cs.AI cs.LG
Comments: Latex with llncs.cls, 17 pages, 5 figures
ACM-class: D.2.11
\\
  As data science applications gain adoption across industries, the tooling
landscape matures to facilitate the life cycle of such applications and provide
solutions to the challenges involved to boost the productivity of the people
involved. Reinforcement learning with agents in a 3D world could still face
challenges: the knowledge required to use a simulation software as well as the
utilization of a standalone simulation software in unattended training
pipelines.
  In this paper we review tools and approaches to train reinforcement learning
agents for robots in 3D worlds with respect to the robot Robotino and argue
that the separation of the simulation environment for creators of virtual
worlds and the model development environment for data scientists is not a well
covered topic. Often both are the same and data scientists require knowledge of
the simulation software to work directly with their APIs. Moreover, sometimes
creators of virtual worlds and data scientists even work on the same files. We
want to contribute to that topic by describing an approach where data
scientists don't require knowledge about the simulation software. Our approach
uses the standalone simulation software Webots, the Robot Operating System to
communicate with simulated robots as well as the simulation software itself and
container technology to separate the simulation from the model development
environment. We put emphasize on the APIs the data scientists work with and the
use of a standalone simulation software in unattended training pipelines. We
show the parts that are specific to the Robotino and the robot task to learn.
\\ ( https://arxiv.org/abs/2403.00765 ,  188kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00772 (*cross-listing*)
Date: Mon, 12 Feb 2024 10:04:54 GMT   (1635kb,D)

Title: Do Weibo platform experts perform better at predicting stock market?
Authors: Ziyuan Ma, Conor Ryan, Jim Buckley, and Muslim Chochlov
Categories: q-fin.ST cs.AI cs.LG cs.SI
Journal-ref: 2021, 22nd Engineering Applications of Neural Networks Conference
  (EANN 2021)
DOI: 10.1007/978-3-030-80568-5_40
\\
  Sentiment analysis can be used for stock market prediction. However, existing
research has not studied the impact of a user's financial background on
sentiment-based forecasting of the stock market using artificial neural
networks. In this work, a novel combination of neural networks is used for the
assessment of sentiment-based stock market prediction, based on the financial
background of the population that generated the sentiment. The state-of-the-art
language processing model Bidirectional Encoder Representations from
Transformers (BERT) is used to classify the sentiment and a Long-Short Term
Memory (LSTM) model is used for time-series based stock market prediction. For
evaluation, the Weibo social networking platform is used as a sentiment data
collection source. Weibo users (and their comments respectively) are divided
into Authorized Financial Advisor (AFA) and Unauthorized Financial Advisor
(UFA) groups according to their background information, as collected by Weibo.
The Hong Kong Hang Seng index is used to extract historical stock market change
data. The results indicate that stock market prediction learned from the AFA
group users is 39.67% more precise than that learned from the UFA group users
and shows the highest accuracy (87%) when compared to existing approaches.
\\ ( https://arxiv.org/abs/2403.00772 ,  1635kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00781 (*cross-listing*)
Date: Sun, 18 Feb 2024 06:07:17 GMT   (2508kb,D)

Title: ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender
  Chatbots through an LLM-Augmented Framework
Authors: Zhongqi Yang, Elahe Khatibi, Nitish Nagesh, Mahyar Abbasian, Iman
  Azimi, Ramesh Jain, Amir M. Rahmani
Categories: cs.IR cs.AI cs.LG cs.MM
Comments: Accepted by The IEEE/ACM international conference on Connected
  Health: Applications, Systems and Engineering Technologies (CHASE) 2024
\\
  The profound impact of food on health necessitates advanced
nutrition-oriented food recommendation services. Conventional methods often
lack the crucial elements of personalization, explainability, and
interactivity. While Large Language Models (LLMs) bring interpretability and
explainability, their standalone use falls short of achieving true
personalization. In this paper, we introduce ChatDiet, a novel LLM-powered
framework designed specifically for personalized nutrition-oriented food
recommendation chatbots. ChatDiet integrates personal and population models,
complemented by an orchestrator, to seamlessly retrieve and process pertinent
information. The result is a dynamic delivery of personalized and explainable
food recommendations, tailored to individual user preferences. Our evaluation
of ChatDiet includes a compelling case study, where we establish a causal
personal model to estimate individual nutrition effects. Our assessments,
including a food recommendation test showcasing a 92\% effectiveness rate,
coupled with illustrative dialogue examples, underscore ChatDiet's strengths in
explainability, personalization, and interactivity.
\\ ( https://arxiv.org/abs/2403.00781 ,  2508kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00782 (*cross-listing*)
Date: Sun, 18 Feb 2024 10:28:18 GMT   (2042kb,D)

Title: Ploutos: Towards interpretable stock movement prediction with financial
  large language model
Authors: Hanshuang Tong, Jun Li, Ning Wu, Ming Gong, Dongmei Zhang, Qi Zhang
Categories: q-fin.ST cs.AI cs.CL
Comments: 8 pages, 4 figures
\\
  Recent advancements in large language models (LLMs) have opened new pathways
for many domains. However, the full potential of LLMs in financial investments
remains largely untapped. There are two main challenges for typical deep
learning-based methods for quantitative finance. First, they struggle to fuse
textual and numerical information flexibly for stock movement prediction.
Second, traditional methods lack clarity and interpretability, which impedes
their application in scenarios where the justification for predictions is
essential. To solve the above challenges, we propose Ploutos, a novel financial
LLM framework that consists of PloutosGen and PloutosGPT. The PloutosGen
contains multiple primary experts that can analyze different modal data, such
as text and numbers, and provide quantitative strategies from different
perspectives. Then PloutosGPT combines their insights and predictions and
generates interpretable rationales. To generate accurate and faithful
rationales, the training strategy of PloutosGPT leverage rearview-mirror
prompting mechanism to guide GPT-4 to generate rationales, and a dynamic token
weighting mechanism to finetune LLM by increasing key tokens weight. Extensive
experiments show our framework outperforms the state-of-the-art methods on both
prediction accuracy and interpretability.
\\ ( https://arxiv.org/abs/2403.00782 ,  2042kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00784 (*cross-listing*)
Date: Sun, 18 Feb 2024 23:22:40 GMT   (1134kb,D)

Title: Utilizing BERT for Information Retrieval: Survey, Applications,
  Resources, and Challenges
Authors: Jiajia Wang, Jimmy X. Huang, Xinhui Tu, Junmei Wang, Angela J. Huang,
  Md Tahmid Rahman Laskar, Amran Bhuiyan
Categories: cs.IR cs.AI cs.CL
\\
  Recent years have witnessed a substantial increase in the use of deep
learning to solve various natural language processing (NLP) problems. Early
deep learning models were constrained by their sequential or unidirectional
nature, such that they struggled to capture the contextual relationships across
text inputs. The introduction of bidirectional encoder representations from
transformers (BERT) leads to a robust encoder for the transformer model that
can understand the broader context and deliver state-of-the-art performance
across various NLP tasks. This has inspired researchers and practitioners to
apply BERT to practical problems, such as information retrieval (IR). A survey
that focuses on a comprehensive analysis of prevalent approaches that apply
pretrained transformer encoders like BERT to IR can thus be useful for academia
and the industry. In light of this, we revisit a variety of BERT-based methods
in this survey, cover a wide range of techniques of IR, and group them into six
high-level categories: (i) handling long documents, (ii) integrating semantic
information, (iii) balancing effectiveness and efficiency, (iv) predicting the
weights of terms, (v) query expansion, and (vi) document expansion. We also
provide links to resources, including datasets and toolkits, for BERT-based IR
systems. A key highlight of our survey is the comparison between BERT's
encoder-based models and the latest generative Large Language Models (LLMs),
such as ChatGPT, which rely on decoders. Despite the popularity of LLMs, we
find that for specific tasks, finely tuned BERT encoders still outperform, and
at a lower deployment cost. Finally, we summarize the comprehensive outcomes of
the survey and suggest directions for future research in the area.
\\ ( https://arxiv.org/abs/2403.00784 ,  1134kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00790 (*cross-listing*)
Date: Thu, 22 Feb 2024 03:28:25 GMT   (5kb)

Title: Structuring Concept Space with the Musical Circle of Fifths by Utilizing
  Music Grammar Based Activations
Authors: Tofara Moyo
Categories: cs.SD cs.AI eess.AS
Comments: 3 pages
\\
  In this paper, we explore the intriguing similarities between the structure
of a discrete neural network, such as a spiking network, and the composition of
a piano piece. While both involve nodes or notes that are activated
sequentially or in parallel, the latter benefits from the rich body of music
theory to guide meaningful combinations. We propose a novel approach that
leverages musical grammar to regulate activations in a spiking neural network,
allowing for the representation of symbols as attractors. By applying rules for
chord progressions from music theory, we demonstrate how certain activations
naturally follow others, akin to the concept of attraction. Furthermore, we
introduce the concept of modulating keys to navigate different basins of
attraction within the network. Ultimately, we show that the map of concepts in
our model is structured by the musical circle of fifths, highlighting the
potential for leveraging music theory principles in deep learning algorithms.
\\ ( https://arxiv.org/abs/2403.00790 ,  5kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00796 (*cross-listing*)
Date: Fri, 23 Feb 2024 06:09:45 GMT   (578kb)

Title: Enhancing Mean-Reverting Time Series Prediction with Gaussian Processes:
  Functional and Augmented Data Structures in Financial Forecasting
Authors: Narayan Tondapu
Categories: q-fin.ST cs.AI cs.LG stat.ML
\\
  In this paper, we explore the application of Gaussian Processes (GPs) for
predicting mean-reverting time series with an underlying structure, using
relatively unexplored functional and augmented data structures. While many
conventional forecasting methods concentrate on the short-term dynamics of time
series data, GPs offer the potential to forecast not just the average
prediction but the entire probability distribution over a future trajectory.
This is particularly beneficial in financial contexts, where accurate
predictions alone may not suffice if incorrect volatility assessments lead to
capital losses. Moreover, in trade selection, GPs allow for the forecasting of
multiple Sharpe ratios adjusted for transaction costs, aiding in
decision-making. The functional data representation utilized in this study
enables longer-term predictions by leveraging information from previous years,
even as the forecast moves away from the current year's training data.
Additionally, the augmented representation enriches the training set by
incorporating multiple targets for future points in time, facilitating
long-term predictions. Our implementation closely aligns with the methodology
outlined in, which assessed effectiveness on commodity futures. However, our
testing methodology differs. Instead of real data, we employ simulated data
with similar characteristics. We construct a testing environment to evaluate
both data representations and models under conditions of increasing noise, fat
tails, and inappropriate kernels-conditions commonly encountered in practice.
By simulating data, we can compare our forecast distribution over time against
a full simulation of the actual distribution of our test set, thereby reducing
the inherent uncertainty in testing time series models on real data. We enable
feature prediction through augmentation and employ sub-sampling to ensure the
feasibility of GPs.
\\ ( https://arxiv.org/abs/2403.00796 ,  578kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00802 (*cross-listing*)
Date: Fri, 23 Feb 2024 21:11:55 GMT   (310kb,D)

Title: Towards a Theoretical Understanding of Two-Stage Recommender Systems
Authors: Amit Kumar Jaiswal
Categories: cs.IR cs.AI
Comments: 18 pages (including references and appendix), 1 figure, 2 tables
\\
  Production-grade recommender systems rely heavily on a large-scale corpus
used by online media services, including Netflix, Pinterest, and Amazon. These
systems enrich recommendations by learning users' and items' embeddings
projected in a low-dimensional space with two-stage models (two deep neural
networks), which facilitate their embedding constructs to predict users'
feedback associated with items. Despite its popularity for recommendations, its
theoretical behaviors remain comprehensively unexplored. We study the
asymptotic behaviors of the two-stage recommender that entail a strong
convergence to the optimal recommender system. We establish certain theoretical
properties and statistical assurance of the two-stage recommender. In addition
to asymptotic behaviors, we demonstrate that the two-stage recommender system
attains faster convergence by relying on the intrinsic dimensions of the input
features. Finally, we show numerically that the two-stage recommender enables
encapsulating the impacts of items' and users' attributes on ratings, resulting
in better performance compared to existing methods conducted using synthetic
and real-world data experiments.
\\ ( https://arxiv.org/abs/2403.00802 ,  310kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00803 (*cross-listing*)
Date: Fri, 23 Feb 2024 22:06:36 GMT   (624kb,D)

Title: LiMAML: Personalization of Deep Recommender Models via Meta Learning
Authors: Ruofan Wang, Prakruthi Prabhakar, Gaurav Srivastava, Tianqi Wang,
  Zeinab S. Jalali, Varun Bharill, Yunbo Ouyang, Aastha Nigam, Divya
  Venugopalan, Aman Gupta, Fedor Borisyuk, Sathiya Keerthi, Ajith Muralidharan
Categories: cs.IR cs.AI cs.LG
\\
  In the realm of recommender systems, the ubiquitous adoption of deep neural
networks has emerged as a dominant paradigm for modeling diverse business
objectives. As user bases continue to expand, the necessity of personalization
and frequent model updates have assumed paramount significance to ensure the
delivery of relevant and refreshed experiences to a diverse array of members.
In this work, we introduce an innovative meta-learning solution tailored to the
personalization of models for individual members and other entities, coupled
with the frequent updates based on the latest user interaction signals.
Specifically, we leverage the Model-Agnostic Meta Learning (MAML) algorithm to
adapt per-task sub-networks using recent user interaction data. Given the near
infeasibility of productionizing original MAML-based models in online
recommendation systems, we propose an efficient strategy to operationalize
meta-learned sub-networks in production, which involves transforming them into
fixed-sized vectors, termed meta embeddings, thereby enabling the seamless
deployment of models with hundreds of billions of parameters for online
serving. Through extensive experimentation on production data drawn from
various applications at LinkedIn, we demonstrate that the proposed solution
consistently outperforms the baseline models of those applications, including
strong baselines such as using wide-and-deep ID based personalization approach.
Our approach has enabled the deployment of a range of highly personalized AI
models across diverse LinkedIn applications, leading to substantial
improvements in business metrics as well as refreshed experience for our
members.
\\ ( https://arxiv.org/abs/2403.00803 ,  624kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00816 (*cross-listing*)
Date: Mon, 26 Feb 2024 01:17:50 GMT   (21506kb,D)

Title: CFRet-DVQA: Coarse-to-Fine Retrieval and Efficient Tuning for Document
  Visual Question Answering
Authors: Jinxu Zhang, Yongqi Yu, Yu Zhang
Categories: cs.IR cs.AI cs.CV
\\
  Document Visual Question Answering (DVQA) is a task that involves responding
to queries based on the content of images. Existing work is limited to locating
information within a single page and does not facilitate cross-page
question-and-answer interaction. Furthermore, the token length limitation
imposed on inputs to the model may lead to truncation of segments pertinent to
the answer. In this study, we introduce a simple but effective methodology
called CFRet-DVQA, which focuses on retrieval and efficient tuning to address
this critical issue effectively. For that, we initially retrieve multiple
segments from the document that correlate with the question at hand.
Subsequently, we leverage the advanced reasoning abilities of the large
language model (LLM), further augmenting its performance through instruction
tuning. This approach enables the generation of answers that align with the
style of the document labels. The experiments demonstrate that our methodology
achieved state-of-the-art or competitive results with both single-page and
multi-page documents in various fields.
\\ ( https://arxiv.org/abs/2403.00816 ,  21506kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00822 (*cross-listing*)
Date: Mon, 26 Feb 2024 17:47:57 GMT   (15376kb,D)

Title: InteraRec: Interactive Recommendations Using Multimodal Large Language
  Models
Authors: Saketh Reddy Karra, Theja Tulabandhula
Categories: cs.IR cs.AI
\\
  Weblogs, comprised of records detailing user activities on any website, offer
valuable insights into user preferences, behavior, and interests. Numerous
recommendation algorithms, employing strategies such as collaborative
filtering, content-based filtering, and hybrid methods, leverage the data mined
through these weblogs to provide personalized recommendations to users. Despite
the abundance of information available in these weblogs, identifying and
extracting pertinent information and key features necessitates extensive
engineering endeavors. The intricate nature of the data also poses a challenge
for interpretation, especially for non-experts. In this study, we introduce a
sophisticated and interactive recommendation framework denoted as InteraRec,
which diverges from conventional approaches that exclusively depend on weblogs
for recommendation generation. This framework captures high-frequency
screenshots of web pages as users navigate through a website. Leveraging
state-of-the-art multimodal large language models (MLLMs), it extracts valuable
insights into user preferences from these screenshots by generating a user
behavioral summary based on predefined keywords. Subsequently, this summary is
utilized as input to an LLM-integrated optimization setup to generate tailored
recommendations. Through our experiments, we demonstrate the effectiveness of
InteraRec in providing users with valuable and personalized offerings.
\\ ( https://arxiv.org/abs/2403.00822 ,  15376kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00832 (*cross-listing*)
Date: Wed, 28 Feb 2024 12:11:08 GMT   (12726kb,D)

Title: Explainable Session-based Recommendation via Path Reasoning
Authors: Yang Cao, Shuo Shang, Jun Wang, and Wei Zhang
Categories: cs.IR cs.AI
\\
  This paper explores providing explainability for session-based recommendation
(SR) by path reasoning. Current SR models emphasize accuracy but lack
explainability, while traditional path reasoning prioritizes knowledge graph
exploration, ignoring sequential patterns present in the session history.
Therefore, we propose a generalized hierarchical reinforcement learning
framework for SR, which improves the explainability of existing SR models via
Path Reasoning, namely PR4SR. Considering the different importance of items to
the session, we design the session-level agent to select the items in the
session as the starting point for path reasoning and the path-level agent to
perform path reasoning. In particular, we design a multi-target reward
mechanism to adapt to the skip behaviors of sequential patterns in SR, and
introduce path midpoint reward to enhance the exploration efficiency in
knowledge graphs. To improve the completeness of the knowledge graph and to
diversify the paths of explanation, we incorporate extracted feature
information from images into the knowledge graph. We instantiate PR4SR in five
state-of-the-art SR models (i.e., GRU4REC, NARM, GCSAN, SR-GNN, SASRec) and
compare it with other explainable SR frameworks, to demonstrate the
effectiveness of PR4SR for recommendation and explanation tasks through
extensive experiments with these approaches on four datasets.
\\ ( https://arxiv.org/abs/2403.00832 ,  12726kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00834 (*cross-listing*)
Date: Tue, 20 Feb 2024 17:48:01 GMT   (12146kb,D)

Title: Virtual Reality for Understanding Artificial-Intelligence-driven
  Scientific Discovery with an Application in Quantum Optics
Authors: Philipp Schmidt, S\"oren Arlt, Carlos Ruiz-Gonzalez, Xuemei Gu, Carla
  Rodr\'iguez, Mario Krenn
Categories: cs.HC cs.AI cs.GR quant-ph
Comments: 12 pages, 6 figures, comments welcome
\\
  Generative Artificial Intelligence (AI) models can propose solutions to
scientific problems beyond human capability. To truly make conceptual
contributions, researchers need to be capable of understanding the AI-generated
structures and extracting the underlying concepts and ideas. When algorithms
provide little explanatory reasoning alongside the output, scientists have to
reverse-engineer the fundamental insights behind proposals based solely on
examples. This task can be challenging as the output is often highly complex
and thus not immediately accessible to humans. In this work we show how
transferring part of the analysis process into an immersive Virtual Reality
(VR) environment can assist researchers in developing an understanding of
AI-generated solutions. We demonstrate the usefulness of VR in finding
interpretable configurations of abstract graphs, representing Quantum Optics
experiments. Thereby, we can manually discover new generalizations of
AI-discoveries as well as new understanding in experimental quantum optics.
Furthermore, it allows us to customize the search space in an informed way - as
a human-in-the-loop - to achieve significantly faster subsequent discovery
iterations. As concrete examples, with this technology, we discover a new
resource-efficient 3-dimensional entanglement swapping scheme, as well as a
3-dimensional 4-particle Greenberger-Horne-Zeilinger-state analyzer. Our
results show the potential of VR for increasing a human researcher's ability to
derive knowledge from graph-based generative AI that, which is a common
abstract data representation used in diverse fields of science.
\\ ( https://arxiv.org/abs/2403.00834 ,  12146kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00841 (*cross-listing*)
Date: Thu, 29 Feb 2024 11:36:48 GMT   (1598kb,D)

Title: Offline Fictitious Self-Play for Competitive Games
Authors: Jingxiao Chen, Weiji Xie, Weinan Zhang, Yong yu, Ying Wen
Categories: cs.MA cs.AI cs.GT cs.LG
\\
  Offline Reinforcement Learning (RL) has received significant interest due to
its ability to improve policies in previously collected datasets without online
interactions. Despite its success in the single-agent setting, offline
multi-agent RL remains a challenge, especially in competitive games. Firstly,
unaware of the game structure, it is impossible to interact with the opponents
and conduct a major learning paradigm, self-play, for competitive games.
Secondly, real-world datasets cannot cover all the state and action space in
the game, resulting in barriers to identifying Nash equilibrium (NE). To
address these issues, this paper introduces Off-FSP, the first practical
model-free offline RL algorithm for competitive games. We start by simulating
interactions with various opponents by adjusting the weights of the fixed
dataset with importance sampling. This technique allows us to learn best
responses to different opponents and employ the Offline Self-Play learning
framework. In this framework, we further implement Fictitious Self-Play (FSP)
to approximate NE. In partially covered real-world datasets, our methods show
the potential to approach NE by incorporating any single-agent offline RL
method. Experimental results in Leduc Hold'em Poker show that our method
significantly improves performances compared with state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2403.00841 ,  1598kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00843 (*cross-listing*)
Date: Thu, 29 Feb 2024 13:49:56 GMT   (2358kb,D)

Title: Enhancing Long-Term Recommendation with Bi-level Learnable Large
  Language Model Planning
Authors: Wentao Shi, Xiangnan He, Yang Zhang, Chongming Gao, Xinyue Li, Jizhi
  Zhang, Qifan Wang, Fuli Feng
Categories: cs.IR cs.AI cs.CL cs.LG
Comments: 11 pages, 5 figures
\\
  Traditional recommendation setting tends to excessively cater to users'
immediate interests and neglect their long-term engagement. To address it, it
is crucial to incorporate planning capabilities into the recommendation
decision-making process to develop policies that take into account both
immediate interests and long-term engagement. Despite Reinforcement Learning
(RL) can learn planning capacity by maximizing cumulative reward, the scarcity
of recommendation data presents challenges such as instability and
susceptibility to overfitting when training RL models from scratch.
  In this context, we propose to leverage the remarkable planning capabilities
over sparse data of Large Language Models (LLMs) for long-term recommendation.
The key lies in enabling a language model to understand and apply task-solving
principles effectively in personalized recommendation scenarios, as the model's
pre-training may not naturally encompass these principles, necessitating the
need to inspire or teach the model. To achieve this, we propose a Bi-level
Learnable LLM Planner framework, which combines macro-learning and
micro-learning through a hierarchical mechanism. The framework includes a
Planner and Reflector for acquiring high-level guiding principles and an
Actor-Critic component for planning personalization. Extensive experiments
validate the superiority of the framework in learning to plan for long-term
recommendations.
\\ ( https://arxiv.org/abs/2403.00843 ,  2358kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00854 (*cross-listing*)
Date: Thu, 29 Feb 2024 18:30:52 GMT   (671kb,D)

Title: Speaker-Independent Dysarthria Severity Classification using
  Self-Supervised Transformers and Multi-Task Learning
Authors: Lauren Stumpf and Balasundaram Kadirvelu and Sigourney Waibel and A.
  Aldo Faisal
Categories: q-bio.NC cs.AI cs.CL cs.LG cs.SD eess.AS
Comments: 17 pages, 2 tables, 4 main figures, 2 supplemental figures, prepared
  for journal submission
ACM-class: I.2.7; I.2.1; J.3
\\
  Dysarthria, a condition resulting from impaired control of the speech muscles
due to neurological disorders, significantly impacts the communication and
quality of life of patients. The condition's complexity, human scoring and
varied presentations make its assessment and management challenging. This study
presents a transformer-based framework for automatically assessing dysarthria
severity from raw speech data. It can offer an objective, repeatable,
accessible, standardised and cost-effective and compared to traditional methods
requiring human expert assessors. We develop a transformer framework, called
Speaker-Agnostic Latent Regularisation (SALR), incorporating a multi-task
learning objective and contrastive learning for speaker-independent multi-class
dysarthria severity classification. The multi-task framework is designed to
reduce reliance on speaker-specific characteristics and address the intrinsic
intra-class variability of dysarthric speech. We evaluated on the Universal
Access Speech dataset using leave-one-speaker-out cross-validation, our model
demonstrated superior performance over traditional machine learning approaches,
with an accuracy of $70.48\%$ and an F1 score of $59.23\%$. Our SALR model also
exceeded the previous benchmark for AI-based classification, which used support
vector machines, by $16.58\%$. We open the black box of our model by
visualising the latent space where we can observe how the model substantially
reduces speaker-specific cues and amplifies task-specific ones, thereby showing
its robustness. In conclusion, SALR establishes a new benchmark in
speaker-independent multi-class dysarthria severity classification using
generative AI. The potential implications of our findings for broader clinical
applications in automated dysarthria severity assessments.
\\ ( https://arxiv.org/abs/2403.00854 ,  671kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00863 (*cross-listing*)
Date: Thu, 29 Feb 2024 23:03:19 GMT   (1161kb,D)

Title: LLM-Ensemble: Optimal Large Language Model Ensemble Method for
  E-commerce Product Attribute Value Extraction
Authors: Chenhao Fang, Xiaohan Li, Zezhong Fan, Jianpeng Xu, Kaushiki Nag,
  Evren Korpeoglu, Sushant Kumar, Kannan Achan
Categories: cs.IR cs.AI cs.CL
\\
  Product attribute value extraction is a pivotal component in Natural Language
Processing (NLP) and the contemporary e-commerce industry. The provision of
precise product attribute values is fundamental in ensuring high-quality
recommendations and enhancing customer satisfaction. The recently emerging
Large Language Models (LLMs) have demonstrated state-of-the-art performance in
numerous attribute extraction tasks, without the need for domain-specific
training data. Nevertheless, varying strengths and weaknesses are exhibited by
different LLMs due to the diversity in data, architectures, and
hyperparameters. This variation makes them complementary to each other, with no
single LLM dominating all others. Considering the diverse strengths and
weaknesses of LLMs, it becomes necessary to develop an ensemble method that
leverages their complementary potentials. In this paper, we propose a novel
algorithm called LLM-ensemble to ensemble different LLMs' outputs for attribute
value extraction. We iteratively learn the weights for different LLMs to
aggregate the labels with weights to predict the final attribute value. Not
only can our proposed method be proven theoretically optimal, but it also
ensures efficient computation, fast convergence, and safe deployment. We have
also conducted extensive experiments with various state-of-the-art LLMs,
including Llama2-13B, Llama2-70B, PaLM-2, GPT-3.5, and GPT-4, on Walmart's
internal data. Our offline metrics demonstrate that the LLM-ensemble method
outperforms all the state-of-the-art single LLMs on Walmart's internal dataset.
This method has been launched in several production models, leading to improved
Gross Merchandise Volume (GMV), Click-Through Rate (CTR), Conversion Rate
(CVR), and Add-to-Cart Rate (ATC).
\\ ( https://arxiv.org/abs/2403.00863 ,  1161kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00865 (*cross-listing*)
Date: Fri, 1 Mar 2024 02:20:04 GMT   (1312kb,D)

Title: Fast and Efficient Local Search for Genetic Programming Based Loss
  Function Learning
Authors: Christian Raymond, Qi Chen, Bing Xue, and Mengjie Zhang
Categories: cs.NE cs.AI cs.CV cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2209.08907
\\
  In this paper, we develop upon the topic of loss function learning, an
emergent meta-learning paradigm that aims to learn loss functions that
significantly improve the performance of the models trained under them.
Specifically, we propose a new meta-learning framework for task and
model-agnostic loss function learning via a hybrid search approach. The
framework first uses genetic programming to find a set of symbolic loss
functions. Second, the set of learned loss functions is subsequently
parameterized and optimized via unrolled differentiation. The versatility and
performance of the proposed framework are empirically validated on a diverse
set of supervised learning tasks. Results show that the learned loss functions
bring improved convergence, sample efficiency, and inference performance on
tabulated, computer vision, and natural language processing problems, using a
variety of task-specific neural network architectures.
\\ ( https://arxiv.org/abs/2403.00865 ,  1312kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00867 (*cross-listing*)
Date: Fri, 1 Mar 2024 03:29:54 GMT   (1041kb,D)

Title: Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by
  Exploring Refusal Loss Landscapes
Authors: Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho
Categories: cs.CR cs.AI cs.CL cs.LG
Comments: Project page:
  https://huggingface.co/spaces/TrustSafeAI/GradientCuff-Jailbreak-Defense
\\
  Large Language Models (LLMs) are becoming a prominent generative AI tool,
where the user enters a query and the LLM generates an answer. To reduce harm
and misuse, efforts have been made to align these LLMs to human values using
advanced training techniques such as Reinforcement Learning from Human Feedback
(RLHF). However, recent studies have highlighted the vulnerability of LLMs to
adversarial jailbreak attempts aiming at subverting the embedded safety
guardrails. To address this challenge, this paper defines and investigates the
Refusal Loss of LLMs and then proposes a method called Gradient Cuff to detect
jailbreak attempts. Gradient Cuff exploits the unique properties observed in
the refusal loss landscape, including functional values and its smoothness, to
design an effective two-step detection strategy. Experimental results on two
aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak
attacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can
significantly improve the LLM's rejection capability for malicious jailbreak
queries, while maintaining the model's performance for benign user queries by
adjusting the detection threshold.
\\ ( https://arxiv.org/abs/2403.00867 ,  1041kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00871 (*cross-listing*)
Date: Fri, 1 Mar 2024 06:15:07 GMT   (616kb,D)

Title: Teach LLMs to Phish: Stealing Private Information from Language Models
Authors: Ashwinee Panda, Christopher A. Choquette-Choo, Zhengming Zhang,
  Yaoqing Yang, Prateek Mittal
Categories: cs.CR cs.AI cs.CL cs.LG
Comments: ICLR 2024
\\
  When large language models are trained on private data, it can be a
significant privacy risk for them to memorize and regurgitate sensitive
information. In this work, we propose a new practical data extraction attack
that we call "neural phishing". This attack enables an adversary to target and
extract sensitive or personally identifiable information (PII), e.g., credit
card numbers, from a model trained on user data with upwards of 10% attack
success rates, at times, as high as 50%. Our attack assumes only that an
adversary can insert as few as 10s of benign-appearing sentences into the
training dataset using only vague priors on the structure of the user data.
\\ ( https://arxiv.org/abs/2403.00871 ,  616kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00872 (*cross-listing*)
Date: Fri, 1 Mar 2024 07:14:45 GMT   (410kb,D)

Title: DFIN-SQL: Integrating Focused Schema with DIN-SQL for Superior Accuracy
  in Large-Scale Databases
Authors: Shai Volvovsky, Marco Marcassa, Mustafa Panbiharwala
Categories: cs.DB cs.AI
\\
  The task of converting natural language queries into SQL queries is
intricate, necessitating a blend of precise techniques for an accurate
translation. The DIN-SQL (Decomposed-In-Context SQL) methodology represents a
significant development in this domain. This paper introduces DFIN (Decomposed
Focused-In-Context), an innovative extension of DIN-SQL that enhances
Text-to-SQL conversion by addressing schema linking errors, which are a major
source of inaccuracies. DFIN uniquely alternates between prompting techniques
and Retrieval-Augmented Generation (RAG), adapting to the size and complexity
of the database schema. A preprocessing phase embeds database definitions and
leverages annotated files, akin to those in the BIRD dataset, facilitating the
runtime retrieval of pertinent schema information. This strategy significantly
reduces the token count for schema linking prompts, enabling the use of a
standard GPT-4 model over its larger context variant, thus handling large-scale
databases more effectively and economically. Our evaluation on the BIRD
dataset, a challenging real-world benchmark, demonstrates that DFIN not only
scales efficiently but also improves accuracy, achieving a score of 51.69. This
improvement surpasses DIN-SQL method (the current third-place), which is the
highest-ranked model employing in-context learning rather than fine-tuning,
previously scoring 50.72. The advancement of DFIN underscores the evolving
capabilities of in-context learning methodologies combined with advanced
language models, offering a promising avenue for future research in complex
Text-to-SQL conversion tasks.
\\ ( https://arxiv.org/abs/2403.00872 ,  410kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00875 (*cross-listing*)
Date: Fri, 1 Mar 2024 07:58:29 GMT   (1037kb,D)

Title: Enhancing Protein Predictive Models via Proteins Data Augmentation: A
  Benchmark and New Directions
Authors: Rui Sun, Lirong Wu, Haitao Lin, Yufei Huang, Stan Z. Li
Categories: q-bio.QM cs.AI cs.LG q-bio.BM
\\
  Augmentation is an effective alternative to utilize the small amount of
labeled protein data. However, most of the existing work focuses on design-ing
new architectures or pre-training tasks, and relatively little work has studied
data augmentation for proteins. This paper extends data augmentation techniques
previously used for images and texts to proteins and then benchmarks these
techniques on a variety of protein-related tasks, providing the first
comprehensive evaluation of protein augmentation. Furthermore, we propose two
novel semantic-level protein augmentation methods, namely Integrated Gradients
Substitution and Back Translation Substitution, which enable protein
semantic-aware augmentation through saliency detection and biological
knowledge. Finally, we integrate extended and proposed augmentations into an
augmentation pool and propose a simple but effective framework, namely
Automated Protein Augmentation (APA), which can adaptively select the most
suitable augmentation combinations for different tasks. Extensive experiments
have shown that APA enhances the performance of five protein related tasks by
an average of 10.55% across three architectures compared to vanilla
implementations without augmentation, highlighting its potential to make a
great impact on the field.
\\ ( https://arxiv.org/abs/2403.00875 ,  1037kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00878 (*cross-listing*)
Date: Fri, 1 Mar 2024 08:43:43 GMT   (903kb,D)

Title: Crimson: Empowering Strategic Reasoning in Cybersecurity through Large
  Language Models
Authors: Jiandong Jin, Bowen Tang, Mingxuan Ma, Xiao Liu, Yunfei Wang, Qingnan
  Lai, Jia Yang, Changling Zhou
Categories: cs.CR cs.AI
Comments: 9 pages, 7 figures
\\
  We introduces Crimson, a system that enhances the strategic reasoning
capabilities of Large Language Models (LLMs) within the realm of cybersecurity.
By correlating CVEs with MITRE ATT&CK techniques, Crimson advances threat
anticipation and strategic defense efforts. Our approach includes defining and
evaluating cybersecurity strategic tasks, alongside implementing a
comprehensive human-in-the-loop data-synthetic workflow to develop the
CVE-to-ATT&CK Mapping (CVEM) dataset. We further enhance LLMs' reasoning
abilities through a novel Retrieval-Aware Training (RAT) process and its
refined iteration, RAT-R.
  Our findings demonstrate that an LLM fine-tuned with our techniques,
possessing 7 billion parameters, approaches the performance level of GPT-4,
showing markedly lower rates of hallucination and errors, and surpassing other
models in strategic reasoning tasks. Moreover, domain-specific fine-tuning of
embedding models significantly improves performance within cybersecurity
contexts, underscoring the efficacy of our methodology. By leveraging Crimson
to convert raw vulnerability data into structured and actionable insights, we
bolster proactive cybersecurity defenses.
\\ ( https://arxiv.org/abs/2403.00878 ,  903kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00880 (*cross-listing*)
Date: Fri, 1 Mar 2024 08:50:27 GMT   (413kb,D)

Title: Dual-Granularity Medication Recommendation Based on Causal Inference
Authors: Shunpan Liang, Xiang Li, Xiang Li, Chen Li, Yu Lei, Yulei Hou, Tengfei
  Ma
Categories: cs.IR cs.AI
\\
  As medical demands grow and machine learning technology advances, AI-based
diagnostic and treatment systems are garnering increasing attention. Medication
recommendation aims to integrate patients' long-term health records with
medical knowledge, recommending accuracy and safe medication combinations for
specific conditions. However, most existing researches treat medication
recommendation systems merely as variants of traditional recommendation
systems, overlooking the heterogeneity between medications and diseases. To
address this challenge, we propose DGMed, a framework for medication
recommendation. DGMed utilizes causal inference to uncover the connections
among medical entities and presents an innovative feature alignment method to
tackle heterogeneity issues. Specifically, this study first applies causal
inference to analyze the quantified therapeutic effects of medications on
specific diseases from historical records, uncovering potential links between
medical entities. Subsequently, we integrate molecular-level knowledge,
aligning the embeddings of medications and diseases within the molecular space
to effectively tackle their heterogeneity. Ultimately, based on relationships
at the entity level, we adaptively adjust the recommendation probabilities of
medication and recommend medication combinations according to the patient's
current health condition. Experimental results on a real-world dataset show
that our method surpasses existing state-of-the-art baselines in four
evaluation metrics, demonstrating superior performance in both accuracy and
safety aspects. Compared to the sub-optimal model, our approach improved
accuracy by 4.40%, reduced the risk of side effects by 6.14%, and increased
time efficiency by 47.15%.
\\ ( https://arxiv.org/abs/2403.00880 ,  413kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00884 (*cross-listing*)
Date: Fri, 1 Mar 2024 10:01:36 GMT   (71kb,D)

Title: Text classification of column headers with a controlled vocabulary:
  leveraging LLMs for metadata enrichment
Authors: Margherita Martorana, Tobias Kuhn, Lise Stork, Jacco van Ossenbruggen
Categories: cs.DB cs.AI cs.IR
\\
  Traditional dataset retrieval systems index on metadata information rather
than on the data values. Thus relying primarily on manual annotations and
high-quality metadata, processes known to be labour-intensive and challenging
to automate. We propose a method to support metadata enrichment with topic
annotations of column headers using three Large Language Models (LLMs):
ChatGPT-3.5, GoogleBard and GoogleGemini. We investigate the LLMs ability to
classify column headers based on domain-specific topics from a controlled
vocabulary. We evaluate our approach by assessing the internal consistency of
the LLMs, the inter-machine alignment, and the human-machine agreement for the
topic classification task. Additionally, we investigate the impact of
contextual information (i.e. dataset description) on the classification
outcomes. Our results suggest that ChatGPT and GoogleGemini outperform
GoogleBard for internal consistency as well as LLM-human-alignment.
Interestingly, we found that context had no impact on the LLMs performances.
This work proposes a novel approach that leverages LLMs for text classification
using a controlled topic vocabulary, which has the potential to facilitate
automated metadata enrichment, thereby enhancing dataset retrieval and the
Findability, Accessibility, Interoperability and Reusability (FAIR) of research
data on the Web.
\\ ( https://arxiv.org/abs/2403.00884 ,  71kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00887 (*cross-listing*)
Date: Fri, 1 Mar 2024 11:28:37 GMT   (672kb,D)

Title: SEGAA: A Unified Approach to Predicting Age, Gender, and Emotion in
  Speech
Authors: Aron R, Indra Sigicharla, Chirag Periwal, Mohanaprasad K, Nithya
  Darisini P S, Sourabh Tiwari, Shivani Arora
Categories: eess.AS cs.AI cs.CL cs.LG cs.SD
\\
  The interpretation of human voices holds importance across various
applications. This study ventures into predicting age, gender, and emotion from
vocal cues, a field with vast applications. Voice analysis tech advancements
span domains, from improving customer interactions to enhancing healthcare and
retail experiences. Discerning emotions aids mental health, while age and
gender detection are vital in various contexts. Exploring deep learning models
for these predictions involves comparing single, multi-output, and sequential
models highlighted in this paper. Sourcing suitable data posed challenges,
resulting in the amalgamation of the CREMA-D and EMO-DB datasets. Prior work
showed promise in individual predictions, but limited research considered all
three variables simultaneously. This paper identifies flaws in an individual
model approach and advocates for our novel multi-output learning architecture
Speech-based Emotion Gender and Age Analysis (SEGAA) model. The experiments
suggest that Multi-output models perform comparably to individual models,
efficiently capturing the intricate relationships between variables and speech
inputs, all while achieving improved runtime.
\\ ( https://arxiv.org/abs/2403.00887 ,  672kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00890 (*cross-listing*)
Date: Fri, 1 Mar 2024 12:38:52 GMT   (745kb)

Title: Improving Android Malware Detection Through Data Augmentation Using
  Wasserstein Generative Adversarial Networks
Authors: Kawana Stalin, Mikias Berhanu Mekoya
Categories: cs.CR cs.AI
Comments: 19 pages
\\
  Generative Adversarial Networks (GANs) have demonstrated their versatility
across various applications, including data augmentation and malware detection.
This research explores the effectiveness of utilizing GAN-generated data to
train a model for the detection of Android malware. Given the considerable
storage requirements of Android applications, the study proposes a method to
synthetically represent data using GANs, thereby reducing storage demands. The
proposed methodology involves creating image representations of features
extracted from an existing dataset. A GAN model is then employed to generate a
more extensive dataset consisting of realistic synthetic grayscale images.
Subsequently, this synthetic dataset is utilized to train a Convolutional
Neural Network (CNN) designed to identify previously unseen Android malware
applications. The study includes a comparative analysis of the CNN's
performance when trained on real images versus synthetic images generated by
the GAN. Furthermore, the research explores variations in performance between
the Wasserstein Generative Adversarial Network (WGAN) and the Deep
Convolutional Generative Adversarial Network (DCGAN). The investigation extends
to studying the impact of image size and malware obfuscation on the
classification model's effectiveness. The data augmentation approach
implemented in this study resulted in a notable performance enhancement of the
classification model, ranging from 1.5% to 7%, depending on the dataset. The
achieved F1 score reached 97.5%. Keywords--Generative Adversarial Networks,
Android Malware, Data Augmentation, Wasserstein Generative Adversarial Network
\\ ( https://arxiv.org/abs/2403.00890 ,  745kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00894 (*cross-listing*)
Date: Fri, 1 Mar 2024 14:43:06 GMT   (7365kb,D)

Title: A systematic evaluation of large language models for generating
  programming code
Authors: Wenpin Hou and Zhicheng Ji
Categories: cs.SE cs.AI cs.CL cs.PL
\\
  We systematically evaluated the performance of seven large language models in
generating programming code using various prompt strategies, programming
languages, and task difficulties. GPT-4 substantially outperforms other large
language models, including Gemini Ultra and Claude 2. The coding performance of
GPT-4 varies considerably with different prompt strategies. In most LeetCode
and GeeksforGeeks coding contests evaluated in this study, GPT-4 employing the
optimal prompt strategy outperforms 85 percent of human participants.
Additionally, GPT-4 demonstrates strong capabilities in translating code
between different programming languages and in learning from past errors. The
computational efficiency of the code generated by GPT-4 is comparable to that
of human programmers. These results suggest that GPT-4 has the potential to
serve as a reliable assistant in programming code generation and software
development.
\\ ( https://arxiv.org/abs/2403.00894 ,  7365kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00895 (*cross-listing*)
Date: Fri, 1 Mar 2024 15:32:44 GMT   (551kb,D)

Title: End-to-end Graph-Sequential Representation Learning for Accurate
  Recommendations
Authors: Vladimir Baikalov, Evgeny Frolov
Categories: cs.IR cs.AI cs.LG
Comments: 4 pages, 1 figure, submitted to WWW'24, short-paper track
\\
  Many recent advancements in recommender systems have focused on developing
sequence-based and graph-based approaches. Both approaches proved useful in
modeling intricate relationships within behavioral data, leading to promising
outcomes in personalized ranking and next-item recommendation tasks while
maintaining good scalability. However, they capture very different signals from
data. While the former approach represents users directly through ordered
interactions with recent items, the latter one aims to capture indirect
dependencies across the interactions graph. This paper presents a novel
multi-representational learning framework that exploits the synergies between
these two paradigms. Our empirical evaluation on several datasets demonstrates
that mutual training of sequential and graph components with the proposed
framework significantly improves recommendations performance.
\\ ( https://arxiv.org/abs/2403.00895 ,  551kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00897 (*cross-listing*)
Date: Fri, 1 Mar 2024 16:27:33 GMT   (3833kb,D)

Title: VisRec: A Semi-Supervised Approach to Radio Interferometric Data
  Reconstruction
Authors: Ruoqi Wang, Haitao Wang, Qiong Luo, Feng Wang and Hejun Wu
Categories: eess.IV astro-ph.GA cs.AI cs.CV cs.LG
\\
  Radio telescopes produce visibility data about celestial objects, but these
data are sparse and noisy. As a result, images created on raw visibility data
are of low quality. Recent studies have used deep learning models to
reconstruct visibility data to get cleaner images. However, these methods rely
on a substantial amount of labeled training data, which requires significant
labeling effort from radio astronomers. Addressing this challenge, we propose
VisRec, a model-agnostic semi-supervised learning approach to the
reconstruction of visibility data. Specifically, VisRec consists of both a
supervised learning module and an unsupervised learning module. In the
supervised learning module, we introduce a set of data augmentation functions
to produce diverse training examples. In comparison, the unsupervised learning
module in VisRec augments unlabeled data and uses reconstructions from
non-augmented visibility data as pseudo-labels for training. This hybrid
approach allows VisRec to effectively leverage both labeled and unlabeled data.
This way, VisRec performs well even when labeled data is scarce. Our evaluation
results show that VisRec outperforms all baseline methods in reconstruction
quality, robustness against common observation perturbation, and
generalizability to different telescope configurations.
\\ ( https://arxiv.org/abs/2403.00897 ,  3833kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00929 (*cross-listing*)
Date: Fri, 1 Mar 2024 19:19:56 GMT   (3497kb,D)

Title: PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for
  Data-Efficient Imitation Learning
Authors: Tian Gao, Soroush Nasiriany, Huihan Liu, Quantao Yang, Yuke Zhu
Categories: cs.RO cs.AI cs.LG
\\
  Imitation learning has shown great potential for enabling robots to acquire
complex manipulation behaviors. However, these algorithms suffer from high
sample complexity in long-horizon tasks, where compounding errors accumulate
over the task horizons. We present PRIME (PRimitive-based IMitation with data
Efficiency), a behavior primitive-based framework designed for improving the
data efficiency of imitation learning. PRIME scaffolds robot tasks by
decomposing task demonstrations into primitive sequences, followed by learning
a high-level control policy to sequence primitives through imitation learning.
Our experiments demonstrate that PRIME achieves a significant performance
improvement in multi-stage manipulation tasks, with 10-34% higher success rates
in simulation over state-of-the-art baselines and 20-48% on physical hardware.
\\ ( https://arxiv.org/abs/2403.00929 ,  3497kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00965 (*cross-listing*)
Date: Fri, 1 Mar 2024 20:32:17 GMT   (2764kb)

Title: Binary Gaussian Copula Synthesis: A Novel Data Augmentation Technique to
  Advance ML-based Clinical Decision Support Systems for Early Prediction of
  Dialysis Among CKD Patients
Authors: Hamed Khosravi, Srinjoy Das, Abdullah Al-Mamun, Imtiaz Ahmed
Categories: stat.AP cs.AI cs.LG
\\
  The Center for Disease Control estimates that over 37 million US adults
suffer from chronic kidney disease (CKD), yet 9 out of 10 of these individuals
are unaware of their condition due to the absence of symptoms in the early
stages. It has a significant impact on patients' quality of life, particularly
when it progresses to the need for dialysis. Early prediction of dialysis is
crucial as it can significantly improve patient outcomes and assist healthcare
providers in making timely and informed decisions. However, developing an
effective machine learning (ML)-based Clinical Decision Support System (CDSS)
for early dialysis prediction poses a key challenge due to the imbalanced
nature of data. To address this challenge, this study evaluates various data
augmentation techniques to understand their effectiveness on real-world
datasets. We propose a new approach named Binary Gaussian Copula Synthesis
(BGCS). BGCS is tailored for binary medical datasets and excels in generating
synthetic minority data that mirrors the distribution of the original data.
BGCS enhances early dialysis prediction by outperforming traditional methods in
detecting dialysis patients. For the best ML model, Random Forest, BCGS
achieved a 72% improvement, surpassing the state-of-the-art augmentation
approaches. Also, we present a ML-based CDSS, designed to aid clinicians in
making informed decisions. CDSS, which utilizes decision tree models, is
developed to improve patient outcomes, identify critical variables, and thereby
enable clinicians to make proactive decisions, and strategize treatment plans
effectively for CKD patients who are more likely to require dialysis in the
near future. Through comprehensive feature analysis and meticulous data
preparation, we ensure that the CDSS's dialysis predictions are not only
accurate but also actionable, providing a valuable tool in the management and
treatment of CKD.
\\ ( https://arxiv.org/abs/2403.00965 ,  2764kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00994 (*cross-listing*)
Date: Fri, 1 Mar 2024 21:29:32 GMT   (2551kb,D)

Title: Leveraging Prompt-Based Large Language Models: Predicting Pandemic
  Health Decisions and Outcomes Through Social Media Language
Authors: Xiaohan Ding, Buse Carik, Uma Sushmitha Gunturi, Valerie Reyna, and
  Eugenia H. Rho
Categories: cs.HC cs.AI cs.CL cs.SI
Comments: 20 pages, 4 figures
DOI: 10.1145/3613904.3642117
\\
  We introduce a multi-step reasoning framework using prompt-based LLMs to
examine the relationship between social media language patterns and trends in
national health outcomes. Grounded in fuzzy-trace theory, which emphasizes the
importance of gists of causal coherence in effective health communication, we
introduce Role-Based Incremental Coaching (RBIC), a prompt-based LLM framework,
to identify gists at-scale. Using RBIC, we systematically extract gists from
subreddit discussions opposing COVID-19 health measures (Study 1). We then
track how these gists evolve across key events (Study 2) and assess their
influence on online engagement (Study 3). Finally, we investigate how the
volume of gists is associated with national health trends like vaccine uptake
and hospitalizations (Study 4). Our work is the first to empirically link
social media linguistic patterns to real-world public health trends,
highlighting the potential of prompt-based LLMs in identifying critical online
discussion patterns that can form the basis of public health communication
strategies.
\\ ( https://arxiv.org/abs/2403.00994 ,  2551kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01003 (*cross-listing*)
Date: Fri, 1 Mar 2024 22:00:44 GMT   (1205kb,D)

Title: FlaKat: A Machine Learning-Based Categorization Framework for Flaky
  Tests
Authors: Shizhe Lin, Ryan Zheng He Liu, Ladan Tahvildari
Categories: cs.SE cs.AI
\\
  Flaky tests can pass or fail non-deterministically, without alterations to a
software system. Such tests are frequently encountered by developers and hinder
the credibility of test suites. State-of-the-art research incorporates machine
learning solutions into flaky test detection and achieves reasonably good
accuracy. Moreover, the majority of automated flaky test repair solutions are
designed for specific types of flaky tests. This research work proposes a novel
categorization framework, called FlaKat, which uses machine-learning
classifiers for fast and accurate prediction of the category of a given flaky
test that reflects its root cause. Sampling techniques are applied to address
the imbalance between flaky test categories in the International Dataset of
Flaky Test (IDoFT). A new evaluation metric, called Flakiness Detection
Capacity (FDC), is proposed for measuring the accuracy of classifiers from the
perspective of information theory and provides proof for its effectiveness. The
final FDC results are also in agreement with F1 score regarding which
classifier yields the best flakiness classification.
\\ ( https://arxiv.org/abs/2403.01003 ,  1205kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01005 (*cross-listing*)
Date: Fri, 1 Mar 2024 22:03:22 GMT   (840kb,D)

Title: Policy Optimization for PDE Control with a Warm Start
Authors: Xiangyuan Zhang, Saviz Mowlavi, Mouhacine Benosman, Tamer Ba\c{s}ar
Categories: eess.SY cs.AI cs.SY math.OC
\\
  Dimensionality reduction is crucial for controlling nonlinear partial
differential equations (PDE) through a "reduce-then-design" strategy, which
identifies a reduced-order model and then implements model-based control
solutions. However, inaccuracies in the reduced-order modeling can
substantially degrade controller performance, especially in PDEs with chaotic
behavior. To address this issue, we augment the reduce-then-design procedure
with a policy optimization (PO) step. The PO step fine-tunes the model-based
controller to compensate for the modeling error from dimensionality reduction.
This augmentation shifts the overall strategy into
reduce-then-design-then-adapt, where the model-based controller serves as a
warm start for PO. Specifically, we study the state-feedback tracking control
of PDEs that aims to align the PDE state with a specific constant target
subject to a linear-quadratic cost. Through extensive experiments, we show that
a few iterations of PO can significantly improve the model-based controller
performance. Our approach offers a cost-effective alternative to PDE control
using end-to-end reinforcement learning.
\\ ( https://arxiv.org/abs/2403.01005 ,  840kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01024 (*cross-listing*)
Date: Fri, 1 Mar 2024 22:59:41 GMT   (3723kb,D)

Title: Reservoir Computing Using Measurement-Controlled Quantum Dynamics
Authors: A.H.Abbas and Ivan S.Maksymov
Categories: cs.NE cs.AI quant-ph
\\
  Physical reservoir computing (RC) is a machine learning algorithm that
employs the dynamics of a physical system to forecast highly nonlinear and
chaotic phenomena. In this paper, we introduce a quantum RC system that employs
the dynamics of a probed atom in a cavity. The atom experiences coherent
driving at a particular rate, leading to a measurement-controlled quantum
evolution. The proposed quantum reservoir can make fast and reliable forecasts
using a small number of artificial neurons compared with the traditional RC
algorithm. We theoretically validate the operation of the reservoir,
demonstrating its potential to be used in error-tolerant applications, where
approximate computing approaches may be used to make feasible forecasts in
conditions of limited computational and energy resources.
\\ ( https://arxiv.org/abs/2403.01024 ,  3723kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01038 (*cross-listing*)
Date: Sat, 2 Mar 2024 00:10:45 GMT   (741kb,D)

Title: AutoAttacker: A Large Language Model Guided System to Implement
  Automatic Cyber-attacks
Authors: Jiacen Xu, Jack W. Stokes, Geoff McDonald, Xuesong Bai, David
  Marshall, Siyue Wang, Adith Swaminathan, Zhou Li
Categories: cs.CR cs.AI
\\
  Large language models (LLMs) have demonstrated impressive results on natural
language tasks, and security researchers are beginning to employ them in both
offensive and defensive systems. In cyber-security, there have been multiple
research efforts that utilize LLMs focusing on the pre-breach stage of attacks
like phishing and malware generation. However, so far there lacks a
comprehensive study regarding whether LLM-based systems can be leveraged to
simulate the post-breach stage of attacks that are typically human-operated, or
"hands-on-keyboard" attacks, under various attack techniques and environments.
  As LLMs inevitably advance, they may be able to automate both the pre- and
post-breach attack stages. This shift may transform organizational attacks from
rare, expert-led events to frequent, automated operations requiring no
expertise and executed at automation speed and scale. This risks fundamentally
changing global computer security and correspondingly causing substantial
economic impacts, and a goal of this work is to better understand these risks
now so we can better prepare for these inevitable ever-more-capable LLMs on the
horizon. On the immediate impact side, this research serves three purposes.
First, an automated LLM-based, post-breach exploitation framework can help
analysts quickly test and continually improve their organization's network
security posture against previously unseen attacks. Second, an LLM-based
penetration test system can extend the effectiveness of red teams with a
limited number of human analysts. Finally, this research can help defensive
systems and teams learn to detect novel attack behaviors preemptively before
their use in the wild....
\\ ( https://arxiv.org/abs/2403.01038 ,  741kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01055 (*cross-listing*)
Date: Sat, 2 Mar 2024 01:11:35 GMT   (501kb,D)

Title: Towards Full Authorship with AI: Supporting Revision with AI-Generated
  Views
Authors: Jiho Kim, Ray C. Flanagan, Noelle E. Haviland, ZeAi Sun, Souad N.
  Yakubu, Edom A. Maru and Kenneth C. Arnold
Categories: cs.HC cs.AI cs.CY
Comments: 15 pages, 2 figures; Accepted to 5th Workshop on Human-AI Co-Creation
  with Generative Models (HAI-GEN) at ACM IUI 2024
ACM-class: H.5.2; I.7.1; I.2.7
\\
  Large language models (LLMs) are shaping a new user interface (UI) paradigm
in writing tools by enabling users to generate text through prompts. This
paradigm shifts some creative control from the user to the system, thereby
diminishing the user's authorship and autonomy in the writing process. To
restore autonomy, we introduce Textfocals, a UI prototype designed to
investigate a human-centered approach that emphasizes the user's role in
writing. Textfocals supports the writing process by providing LLM-generated
summaries, questions, and advice (i.e., LLM views) in a sidebar of a text
editor, encouraging reflection and self-driven revision in writing without
direct text generation. Textfocals' UI affordances, including contextually
adaptive views and scaffolding for prompt selection and customization, offer a
novel way to interact with LLMs where users maintain full authorship of their
writing. A formative user study with Textfocals showed promising evidence that
this approach might help users develop underdeveloped ideas, cater to the
rhetorical audience, and clarify their writing. However, the study also showed
interaction design challenges related to document navigation and scoping,
prompt engineering, and context management. Our work highlights the breadth of
the design space of writing support interfaces powered by generative AI that
maintain authorship integrity.
\\ ( https://arxiv.org/abs/2403.01055 ,  501kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01118 (*cross-listing*)
Date: Sat, 2 Mar 2024 08:03:42 GMT   (11153kb,D)

Title: Adversarial Testing for Visual Grounding via Image-Aware Property
  Reduction
Authors: Zhiyuan Chang, Mingyang Li, Junjie Wang, Cheng Li, Boyu Wu, Fanjiang
  Xu, Qing Wang
Categories: cs.CV cs.AI
Comments: 14pages, 6 figures
\\
  Due to the advantages of fusing information from various modalities,
multimodal learning is gaining increasing attention. Being a fundamental task
of multimodal learning, Visual Grounding (VG), aims to locate objects in images
through natural language expressions. Ensuring the quality of VG models
presents significant challenges due to the complex nature of the task. In the
black box scenario, existing adversarial testing techniques often fail to fully
exploit the potential of both modalities of information. They typically apply
perturbations based solely on either the image or text information,
disregarding the crucial correlation between the two modalities, which would
lead to failures in test oracles or an inability to effectively challenge VG
models. To this end, we propose PEELING, a text perturbation approach via
image-aware property reduction for adversarial testing of the VG model. The
core idea is to reduce the property-related information in the original
expression meanwhile ensuring the reduced expression can still uniquely
describe the original object in the image. To achieve this, PEELING first
conducts the object and properties extraction and recombination to generate
candidate property reduction expressions. It then selects the satisfied
expressions that accurately describe the original object while ensuring no
other objects in the image fulfill the expression, through querying the image
with a visual understanding technique. We evaluate PEELING on the
state-of-the-art VG model, i.e. OFA-VG, involving three commonly used datasets.
Results show that the adversarial tests generated by PEELING achieves 21.4% in
MultiModal Impact score (MMI), and outperforms state-of-the-art baselines for
images and texts by 8.2%--15.1%.
\\ ( https://arxiv.org/abs/2403.01118 ,  11153kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01131 (*cross-listing*)
Date: Sat, 2 Mar 2024 08:21:59 GMT   (4006kb,D)

Title: LLaMoCo: Instruction Tuning of Large Language Models for Optimization
  Code Generation
Authors: Zeyuan Ma, Hongshu Guo, Jiacheng Chen, Guojun Peng, Zhiguang Cao,
  Yining Ma, Yue-Jiao Gong
Categories: math.OC cs.AI cs.CL cs.LG cs.NE cs.SE
\\
  Recent research explores optimization using large language models (LLMs) by
either iteratively seeking next-step solutions from LLMs or directly prompting
LLMs for an optimizer. However, these approaches exhibit inherent limitations,
including low operational efficiency, high sensitivity to prompt design, and a
lack of domain-specific knowledge. We introduce LLaMoCo, the first
instruction-tuning framework designed to adapt LLMs for solving optimization
problems in a code-to-code manner. Specifically, we establish a comprehensive
instruction set containing well-described problem prompts and effective
optimization codes. We then develop a novel two-phase learning strategy that
incorporates a contrastive learning-based warm-up procedure before the
instruction-tuning phase to enhance the convergence behavior during model
fine-tuning. The experiment results demonstrate that a CodeGen (350M) model
fine-tuned by our LLaMoCo achieves superior optimization performance compared
to GPT-4 Turbo and the other competitors across both synthetic and realistic
problem sets. The fine-tuned model and the usage instructions are available at
https://anonymous.4open.science/r/LLaMoCo-722A.
\\ ( https://arxiv.org/abs/2403.01131 ,  4006kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01183 (*cross-listing*)
Date: Sat, 2 Mar 2024 11:44:14 GMT   (37789kb,D)

Title: Leveraging Self-Supervised Learning for Scene Recognition in Child
  Sexual Abuse Imagery
Authors: Pedro H. V. Valois, Jo\~ao Macedo, Leo S. F. Ribeiro, Jefersson A. dos
  Santos, Sandra Avila
Categories: cs.CV cs.AI cs.CY cs.LG
Comments: 13 pages, 5 figures, 4 tables. Under review
\\
  Crime in the 21st century is split into a virtual and real world. However,
the former has become a global menace to people's well-being and security in
the latter. The challenges it presents must be faced with unified global
cooperation, and we must rely more than ever on automated yet trustworthy tools
to combat the ever-growing nature of online offenses. Over 10 million child
sexual abuse reports are submitted to the US National Center for Missing &
Exploited Children every year, and over 80% originated from online sources.
Therefore, investigation centers and clearinghouses cannot manually process and
correctly investigate all imagery. In light of that, reliable automated tools
that can securely and efficiently deal with this data are paramount. In this
sense, the scene recognition task looks for contextual cues in the environment,
being able to group and classify child sexual abuse data without requiring to
be trained on sensitive material. The scarcity and limitations of working with
child sexual abuse images lead to self-supervised learning, a machine-learning
methodology that leverages unlabeled data to produce powerful representations
that can be more easily transferred to target tasks. This work shows that
self-supervised deep learning models pre-trained on scene-centric data can
reach 71.6% balanced accuracy on our indoor scene classification task and, on
average, 2.2 percentage points better performance than a fully supervised
version. We cooperate with Brazilian Federal Police experts to evaluate our
indoor classification model on actual child abuse material. The results
demonstrate a notable discrepancy between the features observed in widely used
scene datasets and those depicted on sensitive materials.
\\ ( https://arxiv.org/abs/2403.01183 ,  37789kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01210 (*cross-listing*)
Date: Sat, 2 Mar 2024 13:52:28 GMT   (3049kb,D)

Title: SAR-AE-SFP: SAR Imagery Adversarial Example in Real Physics domain with
  Target Scattering Feature Parameters
Authors: Jiahao Cui, Jiale Duan, Binyan Luo, Hang Cao, Wang Guo, Haifeng Li
Categories: cs.CV cs.AI
Comments: 10 pages, 9 figures, 2 tables
\\
  Deep neural network-based Synthetic Aperture Radar (SAR) target recognition
models are susceptible to adversarial examples. Current adversarial example
generation methods for SAR imagery primarily operate in the 2D digital domain,
known as image adversarial examples. Recent work, while considering SAR imaging
scatter mechanisms, fails to account for the actual imaging process, rendering
attacks in the three-dimensional physical domain infeasible, termed pseudo
physics adversarial examples. To address these challenges, this paper proposes
SAR-AE-SFP-Attack, a method to generate real physics adversarial examples by
altering the scattering feature parameters of target objects. Specifically, we
iteratively optimize the coherent energy accumulation of the target echo by
perturbing the reflection coefficient and scattering coefficient in the
scattering feature parameters of the three-dimensional target object, and
obtain the adversarial example after echo signal processing and imaging
processing in the RaySAR simulator. Experimental results show that compared to
digital adversarial attack methods, SAR-AE-SFP Attack significantly improves
attack efficiency on CNN-based models (over 30\%) and Transformer-based models
(over 13\%), demonstrating significant transferability of attack effects across
different models and perspectives.
\\ ( https://arxiv.org/abs/2403.01210 ,  3049kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01229 (*cross-listing*)
Date: Sat, 2 Mar 2024 15:14:58 GMT   (13945kb,D)

Title: REWIND Dataset: Privacy-preserving Speaking Status Segmentation from
  Multimodal Body Movement Signals in the Wild
Authors: Jose Vargas Quiros, Chirag Raman, Stephanie Tan, Ekin Gedik, Laura
  Cabrera-Quiros, Hayley Hung
Categories: cs.CV cs.AI cs.LG eess.SP
\\
  Recognizing speaking in humans is a central task towards understanding social
interactions. Ideally, speaking would be detected from individual voice
recordings, as done previously for meeting scenarios. However, individual voice
recordings are hard to obtain in the wild, especially in crowded mingling
scenarios due to cost, logistics, and privacy concerns. As an alternative,
machine learning models trained on video and wearable sensor data make it
possible to recognize speech by detecting its related gestures in an
unobtrusive, privacy-preserving way. These models themselves should ideally be
trained using labels obtained from the speech signal. However, existing
mingling datasets do not contain high quality audio recordings. Instead,
speaking status annotations have often been inferred by human annotators from
video, without validation of this approach against audio-based ground truth. In
this paper we revisit no-audio speaking status estimation by presenting the
first publicly available multimodal dataset with high-quality individual speech
recordings of 33 subjects in a professional networking event. We present three
baselines for no-audio speaking status segmentation: a) from video, b) from
body acceleration (chest-worn accelerometer), c) from body pose tracks. In all
cases we predict a 20Hz binary speaking status signal extracted from the audio,
a time resolution not available in previous datasets. In addition to providing
the signals and ground truth necessary to evaluate a wide range of speaking
status detection methods, the availability of audio in REWIND makes it suitable
for cross-modality studies not feasible with previous mingling datasets.
Finally, our flexible data consent setup creates new challenges for multimodal
systems under missing modalities.
\\ ( https://arxiv.org/abs/2403.01229 ,  13945kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01248 (*cross-listing*)
Date: Sat, 2 Mar 2024 16:16:26 GMT   (41902kb,D)

Title: SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code
Authors: Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David A.
  Ross, Cordelia Schmid, Alireza Fathi
Categories: cs.CV cs.AI cs.CL cs.LG
\\
  This paper introduces SceneCraft, a Large Language Model (LLM) Agent
converting text descriptions into Blender-executable Python scripts which
render complex scenes with up to a hundred 3D assets. This process requires
complex spatial planning and arrangement. We tackle these challenges through a
combination of advanced abstraction, strategic planning, and library learning.
SceneCraft first models a scene graph as a blueprint, detailing the spatial
relationships among assets in the scene. SceneCraft then writes Python scripts
based on this graph, translating relationships into numerical constraints for
asset layout. Next, SceneCraft leverages the perceptual strengths of
vision-language foundation models like GPT-V to analyze rendered images and
iteratively refine the scene. On top of this process, SceneCraft features a
library learning mechanism that compiles common script functions into a
reusable library, facilitating continuous self-improvement without expensive
LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses
existing LLM-based agents in rendering complex scenes, as shown by its
adherence to constraints and favorable human assessments. We also showcase the
broader application potential of SceneCraft by reconstructing detailed 3D
scenes from the Sintel movie and guiding a video generative model with
generated scenes as intermediary control signal.
\\ ( https://arxiv.org/abs/2403.01248 ,  41902kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01255 (*cross-listing*)
Date: Sat, 2 Mar 2024 16:25:42 GMT   (1125kb,D)

Title: Automatic Speech Recognition using Advanced Deep Learning Approaches: A
  survey
Authors: Hamza Kheddar, Mustapha Hemis, Yassine Himeur
Categories: cs.SD cs.AI eess.AS eess.SP
\\
  Recent advancements in deep learning (DL) have posed a significant challenge
for automatic speech recognition (ASR). ASR relies on extensive training
datasets, including confidential ones, and demands substantial computational
and storage resources. Enabling adaptive systems improves ASR performance in
dynamic environments. DL techniques assume training and testing data originate
from the same domain, which is not always true. Advanced DL techniques like
deep transfer learning (DTL), federated learning (FL), and reinforcement
learning (RL) address these issues. DTL allows high-performance models using
small yet related datasets, FL enables training on confidential data without
dataset possession, and RL optimizes decision-making in dynamic environments,
reducing computation costs. This survey offers a comprehensive review of DTL,
FL, and RL-based ASR frameworks, aiming to provide insights into the latest
developments and aid researchers and professionals in understanding the current
challenges. Additionally, transformers, which are advanced DL techniques
heavily used in proposed ASR frameworks, are considered in this survey for
their ability to capture extensive dependencies in the input ASR sequence. The
paper starts by presenting the background of DTL, FL, RL, and Transformers and
then adopts a well-designed taxonomy to outline the state-of-the-art
approaches. Subsequently, a critical analysis is conducted to identify the
strengths and weaknesses of each framework. Additionally, a comparative study
is presented to highlight the existing challenges, paving the way for future
research opportunities.
\\ ( https://arxiv.org/abs/2403.01255 ,  1125kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01277 (*cross-listing*)
Date: Sat, 2 Mar 2024 17:48:40 GMT   (13339kb,D)

Title: Optimal Integrated Task and Path Planning and Its Application to
  Multi-Robot Pickup and Delivery
Authors: Aman Aryan, Manan Modi, Indranil Saha, Rupak Majumdar and Swarup
  Mohalik
Categories: cs.RO cs.AI cs.MA
\\
  We propose a generic multi-robot planning mechanism that combines an optimal
task planner and an optimal path planner to provide a scalable solution for
complex multi-robot planning problems. The Integrated planner, through the
interaction of the task planner and the path planner, produces optimal
collision-free trajectories for the robots. We illustrate our general algorithm
on an object pick-and-drop planning problem in a warehouse scenario where a
group of robots is entrusted with moving objects from one location to another
in the workspace. We solve the task planning problem by reducing it into an
SMT-solving problem and employing the highly advanced SMT solver Z3 to solve
it. To generate collision-free movement of the robots, we extend the
state-of-the-art algorithm Conflict Based Search with Precedence Constraints
with several domain-specific constraints. We evaluate our integrated task and
path planner extensively on various instances of the object pick-and-drop
planning problem and compare its performance with a state-of-the-art
multi-robot classical planner. Experimental results demonstrate that our
planning mechanism can deal with complex planning problems and outperforms a
state-of-the-art classical planner both in terms of computation time and the
quality of the generated plan.
\\ ( https://arxiv.org/abs/2403.01277 ,  13339kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01281 (*cross-listing*)
Date: Sat, 2 Mar 2024 18:28:32 GMT   (130148kb,D)

Title: Fast Low-parameter Video Activity Localization in Collaborative Learning
  Environments
Authors: Venkatesh Jatla, Sravani Teeparthi, Ugesh Egala, Sylvia Celedon
  Pattichis, Marios S. Patticis
Categories: cs.CV cs.AI
\\
  Research on video activity detection has primarily focused on identifying
well-defined human activities in short video segments. The majority of the
research on video activity recognition is focused on the development of large
parameter systems that require training on large video datasets. This paper
develops a low-parameter, modular system with rapid inferencing capabilities
that can be trained entirely on limited datasets without requiring transfer
learning from large-parameter systems. The system can accurately detect and
associate specific activities with the students who perform the activities in
real-life classroom videos. Additionally, the paper develops an interactive
web-based application to visualize human activity maps over long real-life
classroom videos.
\\ ( https://arxiv.org/abs/2403.01281 ,  130148kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01286 (*cross-listing*)
Date: Sat, 2 Mar 2024 18:59:03 GMT   (4378kb,D)

Title: Summary Paper: Use Case on Building Collaborative Safe Autonomous
  Systems-A Robotdog for Guiding Visually Impaired People
Authors: Aman Malhotra and Selma Saidi
Categories: cs.RO cs.AI cs.DC cs.MA cs.SY eess.SY
\\
  This is a summary paper of a use case of a Robotdog dedicated to guide
visually impaired people in complex environment like a smart intersection. In
such scenarios, the Robotdog has to autonomously decide whether it is safe to
cross the intersection or not in order to further guide the human. We leverage
data sharing and collaboration between the Robotdog and other autonomous
systems operating in the same environment. We propose a system architecture for
autonomous systems through a separation of a collaborative decision layer, to
enable collective decision making processes, where data about the environment,
relevant to the Robotdog decision, together with evidences for trustworthiness
about other systems and the environment are shared.
\\ ( https://arxiv.org/abs/2403.01286 ,  4378kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01332 (*cross-listing*)
Date: Sat, 2 Mar 2024 22:38:01 GMT   (200kb,D)

Title: Chaining thoughts and LLMs to learn DNA structural biophysics
Authors: Tyler D. Ross, Ashwin Gopinath
Categories: q-bio.QM cs.AI cs.LG
\\
  The future development of an AI scientist, a tool that is capable of
integrating a variety of experimental data and generating testable hypotheses,
holds immense potential. So far, bespoke machine learning models have been
created to specialize in singular scientific tasks, but otherwise lack the
flexibility of a general purpose model. Here, we show that a general purpose
large language model, chatGPT 3.5-turbo, can be fine-tuned to learn the
structural biophysics of DNA. We find that both fine-tuning models to return
chain-of-thought responses and chaining together models fine-tuned for subtasks
have an enhanced ability to analyze and design DNA sequences and their
structures.
\\ ( https://arxiv.org/abs/2403.01332 ,  200kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01369 (*cross-listing*)
Date: Sun, 3 Mar 2024 02:05:17 GMT   (623kb,D)

Title: A Closer Look at Wav2Vec2 Embeddings for On-Device Single-Channel Speech
  Enhancement
Authors: Ravi Shankar, Ke Tan, Buye Xu, Anurag Kumar
Categories: eess.AS cs.AI cs.LG
Comments: 8 pages; Shorter form accepted in ICASSP 2024
\\
  Self-supervised learned models have been found to be very effective for
certain speech tasks such as automatic speech recognition, speaker
identification, keyword spotting and others. While the features are undeniably
useful in speech recognition and associated tasks, their utility in speech
enhancement systems is yet to be firmly established, and perhaps not properly
understood. In this paper, we investigate the uses of SSL representations for
single-channel speech enhancement in challenging conditions and find that they
add very little value for the enhancement task. Our constraints are designed
around on-device real-time speech enhancement -- model is causal, the compute
footprint is small. Additionally, we focus on low SNR conditions where such
models struggle to provide good enhancement. In order to systematically examine
how SSL representations impact performance of such enhancement models, we
propose a variety of techniques to utilize these embeddings which include
different forms of knowledge-distillation and pre-training.
\\ ( https://arxiv.org/abs/2403.01369 ,  623kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01407 (*cross-listing*)
Date: Sun, 3 Mar 2024 06:13:43 GMT   (5283kb,D)

Title: Region-Transformer: Self-Attention Region Based Class-Agnostic Point
  Cloud Segmentation
Authors: Dipesh Gyawali, Jian Zhang, BB Karki
Categories: cs.CV cs.AI cs.RO
Comments: 8 pages, 5 figures, 3 tables
Journal-ref: 19th International Joint Conference on Computer Vision, Imaging
  and Computer Graphics Theory and Applications, 2024
\\
  Point cloud segmentation, which helps us understand the environment of
specific structures and objects, can be performed in class-specific and
class-agnostic ways. We propose a novel region-based transformer model called
Region-Transformer for performing class-agnostic point cloud segmentation. The
model utilizes a region-growth approach and self-attention mechanism to
iteratively expand or contract a region by adding or removing points. It is
trained on simulated point clouds with instance labels only, avoiding semantic
labels. Attention-based networks have succeeded in many previous methods of
performing point cloud segmentation. However, a region-growth approach with
attention-based networks has yet to be used to explore its performance gain. To
our knowledge, we are the first to use a self-attention mechanism in a
region-growth approach. With the introduction of self-attention to
region-growth that can utilize local contextual information of neighborhood
points, our experiments demonstrate that the Region-Transformer model
outperforms previous class-agnostic and class-specific methods on indoor
datasets regarding clustering metrics. The model generalizes well to
large-scale scenes. Key advantages include capturing long-range dependencies
through self-attention, avoiding the need for semantic labels during training,
and applicability to a variable number of objects. The Region-Transformer model
represents a promising approach for flexible point cloud segmentation with
applications in robotics, digital twinning, and autonomous vehicles.
\\ ( https://arxiv.org/abs/2403.01407 ,  5283kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01413 (*cross-listing*)
Date: Sun, 3 Mar 2024 06:53:04 GMT   (6227kb,D)

Title: Exploring the Design of Generative AI in Supporting Music-based
  Reminiscence for Older Adults
Authors: Yucheng Jin, Wanling Cai, Li Chen, Yizhe Zhang, Gavin Doherty, Tonglin
  Jiang
Categories: cs.HC cs.AI
\\
  Music-based reminiscence has the potential to positively impact the
psychological well-being of older adults. However, the aging process and
physiological changes, such as memory decline and limited verbal communication,
may impede the ability of older adults to recall their memories and life
experiences. Given the advanced capabilities of generative artificial
intelligence (AI) systems, such as generated conversations and images, and
their potential to facilitate the reminiscing process, this study aims to
explore the design of generative AI to support music-based reminiscence in
older adults. This study follows a user-centered design approach incorporating
various stages, including detailed interviews with two social workers and two
design workshops (involving ten older adults). Our work contributes to an
in-depth understanding of older adults' attitudes toward utilizing generative
AI for supporting music-based reminiscence and identifies concrete design
considerations for the future design of generative AI to enhance the
reminiscence experience of older adults.
\\ ( https://arxiv.org/abs/2403.01413 ,  6227kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01437 (*cross-listing*)
Date: Sun, 3 Mar 2024 08:24:28 GMT   (923kb,D)

Title: GPTSee: Enhancing Moment Retrieval and Highlight Detection via
  Description-Based Similarity Features
Authors: Yunzhuo Sun, Yifang Xu, Zien Xie, Yukun Shu, and Sidan Du
Categories: cs.CV cs.AI
Comments: 5 pages, 3 figures
DOI: 10.1109/LSP.2023.3340103
\\
  Moment retrieval (MR) and highlight detection (HD) aim to identify relevant
moments and highlights in video from corresponding natural language query.
Large language models (LLMs) have demonstrated proficiency in various computer
vision tasks. However, existing methods for MR\&HD have not yet been integrated
with LLMs. In this letter, we propose a novel two-stage model that takes the
output of LLMs as the input to the second-stage transformer encoder-decoder.
First, MiniGPT-4 is employed to generate the detailed description of the video
frame and rewrite the query statement, fed into the encoder as new features.
Then, semantic similarity is computed between the generated description and the
rewritten queries. Finally, continuous high-similarity video frames are
converted into span anchors, serving as prior position information for the
decoder. Experiments demonstrate that our approach achieves a state-of-the-art
result, and by using only span anchors and similarity scores as outputs,
positioning accuracy outperforms traditional methods, like Moment-DETR.
\\ ( https://arxiv.org/abs/2403.01437 ,  923kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01489 (*cross-listing*)
Date: Sun, 3 Mar 2024 11:55:49 GMT   (10339kb,D)

Title: Regeneration Based Training-free Attribution of Fake Images Generated by
  Text-to-Image Generative Models
Authors: Meiling Li, Zhenxing Qian, Xinpeng Zhang
Categories: cs.CV cs.AI
\\
  Text-to-image generative models have recently garnered significant attention
due to their ability to generate images based on prompt descriptions. While
these models have shown promising performance, concerns have been raised
regarding the potential misuse of the generated fake images. In response to
this, we have presented a simple yet effective training-free method to
attribute fake images generated by text-to-image models to their source models.
Given a test image to be attributed, we first inverse the textual prompt of the
image, and then put the reconstructed prompt into different candidate models to
regenerate candidate fake images. By calculating and ranking the similarity of
the test image and the candidate images, we can determine the source of the
image. This attribution allows model owners to be held accountable for any
misuse of their models. Note that our approach does not limit the number of
candidate text-to-image generative models. Comprehensive experiments reveal
that (1) Our method can effectively attribute fake images to their source
models, achieving comparable attribution performance with the state-of-the-art
method; (2) Our method has high scalability ability, which is well adapted to
real-world attribution scenarios. (3) The proposed method yields satisfactory
robustness to common attacks, such as Gaussian blurring, JPEG compression, and
Resizing. We also analyze the factors that influence the attribution
performance, and explore the boost brought by the proposed method as a plug-in
to improve the performance of existing SOTA. We hope our work can shed some
light on the solutions to addressing the source of AI-generated images, as well
as to prevent the misuse of text-to-image generative models.
\\ ( https://arxiv.org/abs/2403.01489 ,  10339kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01510 (*cross-listing*)
Date: Sun, 3 Mar 2024 13:17:10 GMT   (14261kb,D)

Title: End-to-End Human Instance Matting
Authors: Qinglin Liu, Shengping Zhang, Quanling Meng, Bineng Zhong, Peiqiang
  Liu, Hongxun Yao
Categories: cs.CV cs.AI
Journal-ref: IEEE T-CSVT 2023
DOI: 10.1109/TCSVT.2023.3306400
\\
  Human instance matting aims to estimate an alpha matte for each human
instance in an image, which is extremely challenging and has rarely been
studied so far. Despite some efforts to use instance segmentation to generate a
trimap for each instance and apply trimap-based matting methods, the resulting
alpha mattes are often inaccurate due to inaccurate segmentation. In addition,
this approach is computationally inefficient due to multiple executions of the
matting method. To address these problems, this paper proposes a novel
End-to-End Human Instance Matting (E2E-HIM) framework for simultaneous multiple
instance matting in a more efficient manner. Specifically, a general perception
network first extracts image features and decodes instance contexts into latent
codes. Then, a united guidance network exploits spatial attention and semantics
embedding to generate united semantics guidance, which encodes the locations
and semantic correspondences of all instances. Finally, an instance matting
network decodes the image features and united semantics guidance to predict all
instance-level alpha mattes. In addition, we construct a large-scale human
instance matting dataset (HIM-100K) comprising over 100,000 human images with
instance alpha matte labels. Experiments on HIM-100K demonstrate the proposed
E2E-HIM outperforms the existing methods on human instance matting with 50%
lower errors and 5X faster speed (6 instances in a 640X640 image). Experiments
on the PPM-100, RWP-636, and P3M datasets demonstrate that E2E-HIM also
achieves competitive performance on traditional human matting.
\\ ( https://arxiv.org/abs/2403.01510 ,  14261kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01564 (*cross-listing*)
Date: Sun, 3 Mar 2024 17:00:28 GMT   (18557kb,D)

Title: ComTraQ-MPC: Meta-Trained DQN-MPC Integration for Trajectory Tracking
  with Limited Active Localization Updates
Authors: Gokul Puthumanaillam, Manav Vora and Melkior Ornik
Categories: cs.RO cs.AI cs.SY eess.SY
Comments: * Equal contribution
\\
  Optimal decision-making for trajectory tracking in partially observable,
stochastic environments where the number of active localization updates -- the
process by which the agent obtains its true state information from the sensors
-- are limited, presents a significant challenge. Traditional methods often
struggle to balance resource conservation, accurate state estimation and
precise tracking, resulting in suboptimal performance. This problem is
particularly pronounced in environments with large action spaces, where the
need for frequent, accurate state data is paramount, yet the capacity for
active localization updates is restricted by external limitations. This paper
introduces ComTraQ-MPC, a novel framework that combines Deep Q-Networks (DQN)
and Model Predictive Control (MPC) to optimize trajectory tracking with
constrained active localization updates. The meta-trained DQN ensures adaptive
active localization scheduling, while the MPC leverages available state
information to improve tracking. The central contribution of this work is their
reciprocal interaction: DQN's update decisions inform MPC's control strategy,
and MPC's outcomes refine DQN's learning, creating a cohesive, adaptive system.
Empirical evaluations in simulated and real-world settings demonstrate that
ComTraQ-MPC significantly enhances operational efficiency and accuracy,
providing a generalizable and approximately optimal solution for trajectory
tracking in complex partially observable environments.
\\ ( https://arxiv.org/abs/2403.01564 ,  18557kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01567 (*cross-listing*)
Date: Sun, 3 Mar 2024 17:14:40 GMT   (611kb,D)

Title: ReMatch: Retrieval Enhanced Schema Matching with LLMs
Authors: Eitam Sheetrit, Menachem Brief, Moshik Mishaeli, Oren Elisha
Categories: cs.DB cs.AI
\\
  Schema matching is a crucial task in data integration, involving the
alignment of a source database schema with a target schema to establish
correspondence between their elements. This task is challenging due to textual
and semantic heterogeneity, as well as differences in schema sizes. Although
machine-learning-based solutions have been explored in numerous studies, they
often suffer from low accuracy, require manual mapping of the schemas for model
training, or need access to source schema data which might be unavailable due
to privacy concerns. In this paper we present a novel method, named ReMatch,
for matching schemas using retrieval-enhanced Large Language Models (LLMs). Our
method avoids the need for predefined mapping, any model training, or access to
data in the source database. In the ReMatch method the tables of the target
schema and the attributes of the source schema are first represented as
structured passage-based documents. For each source attribute document, we
retrieve $J$ documents, representing target schema tables, according to their
semantic relevance. Subsequently, we create a prompt for every source table,
comprising all its attributes and their descriptions, alongside all attributes
from the set of top $J$ target tables retrieved previously. We employ LLMs
using this prompt for the matching task, yielding a ranked list of $K$
potential matches for each source attribute. Our experimental results on large
real-world schemas demonstrate that ReMatch significantly improves matching
capabilities and outperforms other machine learning approaches. By eliminating
the requirement for training data, ReMatch becomes a viable solution for
real-world scenarios.
\\ ( https://arxiv.org/abs/2403.01567 ,  611kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01569 (*cross-listing*)
Date: Sun, 3 Mar 2024 17:29:03 GMT   (25054kb,D)

Title: Kick Back & Relax++: Scaling Beyond Ground-Truth Depth with SlowTV &
  CribsTV
Authors: Jaime Spencer, Chris Russell, Simon Hadfield, Richard Bowden
Categories: cs.CV cs.AI cs.RO
\\
  Self-supervised learning is the key to unlocking generic computer vision
systems. By eliminating the reliance on ground-truth annotations, it allows
scaling to much larger data quantities. Unfortunately, self-supervised
monocular depth estimation (SS-MDE) has been limited by the absence of diverse
training data. Existing datasets have focused exclusively on urban driving in
densely populated cities, resulting in models that fail to generalize beyond
this domain.
  To address these limitations, this paper proposes two novel datasets: SlowTV
and CribsTV. These are large-scale datasets curated from publicly available
YouTube videos, containing a total of 2M training frames. They offer an
incredibly diverse set of environments, ranging from snowy forests to coastal
roads, luxury mansions and even underwater coral reefs. We leverage these
datasets to tackle the challenging task of zero-shot generalization,
outperforming every existing SS-MDE approach and even some state-of-the-art
supervised methods.
  The generalization capabilities of our models are further enhanced by a range
of components and contributions: 1) learning the camera intrinsics, 2) a
stronger augmentation regime targeting aspect ratio changes, 3) support frame
randomization, 4) flexible motion estimation, 5) a modern transformer-based
architecture. We demonstrate the effectiveness of each component in extensive
ablation experiments. To facilitate the development of future research, we make
the datasets, code and pretrained models available to the public at
https://github.com/jspenmar/slowtv_monodepth.
\\ ( https://arxiv.org/abs/2403.01569 ,  25054kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01575 (*cross-listing*)
Date: Sun, 3 Mar 2024 17:48:42 GMT   (5191kb,D)

Title: SARD: A Human-AI Collaborative Story Generation
Authors: Ahmed Y. Radwan, Khaled M. Alasmari, Omar A. Abdulbagi, Emad A.
  Alghamdi
Categories: cs.HC cs.AI
\\
  Generative artificial intelligence (GenAI) has ushered in a new era for
storytellers, providing a powerful tool to ignite creativity and explore
uncharted narrative territories. As technology continues to advance, the
synergy between human creativity and AI-generated content holds the potential
to redefine the landscape of storytelling. In this work, we propose SARD, a
drag-and-drop visual interface for generating a multi-chapter story using large
language models. Our evaluation of the usability of SARD and its creativity
support shows that while node-based visualization of the narrative may help
writers build a mental model, it exerts unnecessary mental overhead to the
writer and becomes a source of distraction as the story becomes more
elaborated. We also found that AI generates stories that are less lexically
diverse, irrespective of the complexity of the story. We identified some
patterns and limitations of our tool that can guide the development of future
human-AI co-writing tools.
\\ ( https://arxiv.org/abs/2403.01575 ,  5191kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01598 (*cross-listing*)
Date: Sun, 3 Mar 2024 19:52:43 GMT   (20353kb,D)

Title: APISR: Anime Production Inspired Real-World Anime Super-Resolution
Authors: Boyang Wang, Fengyu Yang, Xihang Yu, Chao Zhang, Hanbin Zhao
Categories: eess.IV cs.AI cs.CV
\\
  While real-world anime super-resolution (SR) has gained increasing attention
in the SR community, existing methods still adopt techniques from the
photorealistic domain. In this paper, we analyze the anime production workflow
and rethink how to use characteristics of it for the sake of the real-world
anime SR. First, we argue that video networks and datasets are not necessary
for anime SR due to the repetition use of hand-drawing frames. Instead, we
propose an anime image collection pipeline by choosing the least compressed and
the most informative frames from the video sources. Based on this pipeline, we
introduce the Anime Production-oriented Image (API) dataset. In addition, we
identify two anime-specific challenges of distorted and faint hand-drawn lines
and unwanted color artifacts. We address the first issue by introducing a
prediction-oriented compression module in the image degradation model and a
pseudo-ground truth preparation with enhanced hand-drawn lines. In addition, we
introduce the balanced twin perceptual loss combining both anime and
photorealistic high-level features to mitigate unwanted color artifacts and
increase visual clarity. We evaluate our method through extensive experiments
on the public benchmark, showing our method outperforms state-of-the-art
approaches by a large margin.
\\ ( https://arxiv.org/abs/2403.01598 ,  20353kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01599 (*cross-listing*)
Date: Sun, 3 Mar 2024 19:53:06 GMT   (1487kb,D)

Title: SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional
  Videos
Authors: Yulei Niu, Wenliang Guo, Long Chen, Xudong Lin, Shih-Fu Chang
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: Accepted by ICLR 2024
\\
  We study the problem of procedure planning in instructional videos, which
aims to make a goal-oriented sequence of action steps given partial visual
state observations. The motivation of this problem is to learn a structured and
plannable state and action space. Recent works succeeded in sequence modeling
of steps with only sequence-level annotations accessible during training, which
overlooked the roles of states in the procedures. In this work, we point out
that State CHangEs MAtter (SCHEMA) for procedure planning in instructional
videos. We aim to establish a more structured state space by investigating the
causal relations between steps and states in procedures. Specifically, we
explicitly represent each step as state changes and track the state changes in
procedures. For step representation, we leveraged the commonsense knowledge in
large language models (LLMs) to describe the state changes of steps via our
designed chain-of-thought prompting. For state change tracking, we align visual
state observations with language state descriptions via cross-modal contrastive
learning, and explicitly model the intermediate states of the procedure using
LLM-generated state descriptions. Experiments on CrossTask, COIN, and NIV
benchmark datasets demonstrate that our proposed SCHEMA model achieves
state-of-the-art performance and obtains explainable visualizations.
\\ ( https://arxiv.org/abs/2403.01599 ,  1487kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01600 (*cross-listing*)
Date: Sun, 3 Mar 2024 19:59:42 GMT   (1155kb,D)

Title: Can Poverty Be Reduced by Acting on Discrimination? An Agent-based Model
  for Policy Making
Authors: Alba Aguilera, Nieves Montes, Georgina Curto, Carles Sierra and
  Nardine Osman
Categories: cs.MA cs.AI
\\
  In the last decades, there has been a deceleration in the rates of poverty
reduction, suggesting that traditional redistributive approaches to poverty
mitigation could be losing effectiveness, and alternative insights to advance
the number one UN Sustainable Development Goal are required. The
criminalization of poor people has been denounced by several NGOs, and an
increasing number of voices suggest that discrimination against the poor (a
phenomenon known as \emph{aporophobia}) could be an impediment to mitigating
poverty. In this paper, we present the novel Aporophobia Agent-Based Model
(AABM) to provide evidence of the correlation between aporophobia and poverty
computationally. We present our use case built with real-world demographic data
and poverty-mitigation public policies (either enforced or under parliamentary
discussion) for the city of Barcelona. We classify policies as discriminatory
or non-discriminatory against the poor, with the support of specialized NGOs,
and we observe the results in the AABM in terms of the impact on wealth
inequality. The simulation provides evidence of the relationship between
aporophobia and the increase of wealth inequality levels, paving the way for a
new generation of poverty reduction policies that act on discrimination and
tackle poverty as a societal problem (not only a problem of the poor).
\\ ( https://arxiv.org/abs/2403.01600 ,  1155kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01606 (*cross-listing*)
Date: Sun, 3 Mar 2024 20:16:14 GMT   (1336kb,D)

Title: A Unified Model Selection Technique for Spectral Clustering Based Motion
  Segmentation
Authors: Yuxiang Huang, John Zelek
Categories: cs.CV cs.AI
Comments: Journal of Computational Vision and Imaging Systems
\\
  Motion segmentation is a fundamental problem in computer vision and is
crucial in various applications such as robotics, autonomous driving and action
recognition. Recently, spectral clustering based methods have shown impressive
results on motion segmentation in dynamic environments. These methods perform
spectral clustering on motion affinity matrices to cluster objects or point
trajectories in the scene into different motion groups. However, existing
methods often need the number of motions present in the scene to be known,
which significantly reduces their practicality. In this paper, we propose a
unified model selection technique to automatically infer the number of motion
groups for spectral clustering based motion segmentation methods by combining
different existing model selection techniques together. We evaluate our method
on the KT3DMoSeg dataset and achieve competitve results comparing to the
baseline where the number of clusters is given as ground truth information.
\\ ( https://arxiv.org/abs/2403.01606 ,  1336kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01649 (*cross-listing*)
Date: Mon, 4 Mar 2024 00:03:00 GMT   (155kb)

Title: Recommendations for Government Development and Use of Advanced Automated
  Systems to Make Decisions about Individuals
Authors: Susan Landau, James X. Dempsey, Ece Kamar, Steven M. Bellovin
Categories: cs.CY cs.AI
ACM-class: K.4; K.5; I.2
\\
  Contestability -- the ability to effectively challenge a decision -- is
critical to the implementation of fairness. In the context of governmental
decision making about individuals, contestability is often constitutionally
required as an element of due process; specific procedures may be required by
state or federal law relevant to a particular program. In addition,
contestability can be a valuable way to discover systemic errors, contributing
to ongoing assessments and system improvement.
  On January 24-25, 2024, with support from the National Science Foundation and
the William and Flora Hewlett Foundation, we convened a diverse group of
government officials, representatives of leading technology companies,
technology and policy experts from academia and the non-profit sector,
advocates, and stakeholders for a workshop on advanced automated decision
making, contestability, and the law. Informed by the workshop's rich and
wide-ranging discussion, we offer these recommendations. A full report
summarizing the discussion is in preparation.
\\ ( https://arxiv.org/abs/2403.01649 ,  155kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01673 (*cross-listing*)
Date: Mon, 4 Mar 2024 01:52:40 GMT   (3384kb,D)

Title: CATS: Enhancing Multivariate Time Series Forecasting by Constructing
  Auxiliary Time Series as Exogenous Variables
Authors: Jiecheng Lu, Xu Han, Yan Sun, Shihao Yang
Categories: stat.ML cs.AI cs.LG
\\
  For Multivariate Time Series Forecasting (MTSF), recent deep learning
applications show that univariate models frequently outperform multivariate
ones. To address the difficiency in multivariate models, we introduce a method
to Construct Auxiliary Time Series (CATS) that functions like a 2D
temporal-contextual attention mechanism, which generates Auxiliary Time Series
(ATS) from Original Time Series (OTS) to effectively represent and incorporate
inter-series relationships for forecasting. Key principles of ATS - continuity,
sparsity, and variability - are identified and implemented through different
modules. Even with a basic 2-layer MLP as core predictor, CATS achieves
state-of-the-art, significantly reducing complexity and parameters compared to
previous multivariate models, marking it an efficient and transferable MTSF
solution.
\\ ( https://arxiv.org/abs/2403.01673 ,  3384kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01693 (*cross-listing*)
Date: Mon, 4 Mar 2024 03:00:22 GMT   (18665kb,D)

Title: HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances
Authors: Supreeth Narasimhaswamy, Uttaran Bhattacharya, Xiang Chen, Ishita
  Dasgupta, Saayan Mitra, Minh Hoai
Categories: cs.CV cs.AI
\\
  Text-to-image generative models can generate high-quality humans, but realism
is lost when generating hands. Common artifacts include irregular hand poses,
shapes, incorrect numbers of fingers, and physically implausible finger
orientations. To generate images with realistic hands, we propose a novel
diffusion-based architecture called HanDiffuser that achieves realism by
injecting hand embeddings in the generative process. HanDiffuser consists of
two components: a Text-to-Hand-Params diffusion model to generate SMPL-Body and
MANO-Hand parameters from input text prompts, and a Text-Guided
Hand-Params-to-Image diffusion model to synthesize images by conditioning on
the prompts and hand parameters generated by the previous component. We
incorporate multiple aspects of hand representation, including 3D shapes and
joint-level finger positions, orientations and articulations, for robust
learning and reliable performance during inference. We conduct extensive
quantitative and qualitative experiments and perform user studies to
demonstrate the efficacy of our method in generating images with high-quality
hands.
\\ ( https://arxiv.org/abs/2403.01693 ,  18665kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01709 (*cross-listing*)
Date: Mon, 4 Mar 2024 03:56:14 GMT   (868kb,D)

Title: Can LLMs Generate Architectural Design Decisions? -An Exploratory
  Empirical study
Authors: Rudra Dhar, Karthik Vaidhyanathan, Vasudeva Varma
Categories: cs.SE cs.AI cs.LG
Comments: This paper has been accepted to IEEE ICSA 2024 (Main Track - Research
  Track)
\\
  Architectural Knowledge Management (AKM) involves the organized handling of
information related to architectural decisions and design within a project or
organization. An essential artifact of AKM is the Architecture Decision Records
(ADR), which documents key design decisions. ADRs are documents that capture
decision context, decision made and various aspects related to a design
decision, thereby promoting transparency, collaboration, and understanding.
Despite their benefits, ADR adoption in software development has been slow due
to challenges like time constraints and inconsistent uptake. Recent
advancements in Large Language Models (LLMs) may help bridge this adoption gap
by facilitating ADR generation. However, the effectiveness of LLM for ADR
generation or understanding is something that has not been explored. To this
end, in this work, we perform an exploratory study that aims to investigate the
feasibility of using LLM for the generation of ADRs given the decision context.
In our exploratory study, we utilize GPT and T5-based models with 0-shot,
few-shot, and fine-tuning approaches to generate the Decision of an ADR given
its Context. Our results indicate that in a 0-shot setting, state-of-the-art
models such as GPT-4 generate relevant and accurate Design Decisions, although
they fall short of human-level performance. Additionally, we observe that more
cost-effective models like GPT-3.5 can achieve similar outcomes in a few-shot
setting, and smaller models such as Flan-T5 can yield comparable results after
fine-tuning. To conclude, this exploratory study suggests that LLM can generate
Design Decisions, but further research is required to attain human-level
generation and establish standardized widespread adoption.
\\ ( https://arxiv.org/abs/2403.01709 ,  868kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01734 (*cross-listing*)
Date: Mon, 4 Mar 2024 05:20:57 GMT   (2952kb,D)

Title: Offline Goal-Conditioned Reinforcement Learning for Safety-Critical
  Tasks with Recovery Policy
Authors: Chenyang Cao, Zichen Yan, Renhao Lu, Junbo Tan, Xueqian Wang
Categories: cs.RO cs.AI cs.LG
Comments: Accepted by ICRA24
MSC-class: 68T40
\\
  Offline goal-conditioned reinforcement learning (GCRL) aims at solving
goal-reaching tasks with sparse rewards from an offline dataset. While prior
work has demonstrated various approaches for agents to learn near-optimal
policies, these methods encounter limitations when dealing with diverse
constraints in complex environments, such as safety constraints. Some of these
approaches prioritize goal attainment without considering safety, while others
excessively focus on safety at the expense of training efficiency. In this
paper, we study the problem of constrained offline GCRL and propose a new
method called Recovery-based Supervised Learning (RbSL) to accomplish
safety-critical tasks with various goals. To evaluate the method performance,
we build a benchmark based on the robot-fetching environment with a randomly
positioned obstacle and use expert or random policies to generate an offline
dataset. We compare RbSL with three offline GCRL algorithms and one offline
safe RL algorithm. As a result, our method outperforms the existing
state-of-the-art methods to a large extent. Furthermore, we validate the
practicality and effectiveness of RbSL by deploying it on a real Panda
manipulator. Code is available at https://github.com/Sunlighted/RbSL.git.
\\ ( https://arxiv.org/abs/2403.01734 ,  2952kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01768 (*cross-listing*)
Date: Mon, 4 Mar 2024 06:53:38 GMT   (3978kb,D)

Title: Canonical Form of Datatic Description in Control Systems
Authors: Guojian Zhan, Ziang Zheng, Shengbo Eben Li
Categories: cs.SY cs.AI eess.SY
\\
  The design of feedback controllers is undergoing a paradigm shift from
modelic (i.e., model-driven) control to datatic (i.e., data-driven) control.
Canonical form of state space model is an important concept in modelic control
systems, exemplified by Jordan form, controllable form and observable form,
whose purpose is to facilitate system analysis and controller synthesis. In the
realm of datatic control, there is a notable absence in the standardization of
data-based system representation. This paper for the first time introduces the
concept of canonical data form for the purpose of achieving more effective
design of datatic controllers. In a control system, the data sample in
canonical form consists of a transition component and an attribute component.
The former encapsulates the plant dynamics at the sampling time independently,
which is a tuple containing three elements: a state, an action and their
corresponding next state. The latter describes one or some artificial
characteristics of the current sample, whose calculation must be performed in
an online manner. The attribute of each sample must adhere to two requirements:
(1) causality, ensuring independence from any future samples; and (2) locality,
allowing dependence on historical samples but constrained to a finite
neighboring set. The purpose of adding attribute is to offer some kinds of
benefits for controller design in terms of effectiveness and efficiency. To
provide a more close-up illustration, we present two canonical data forms:
temporal form and spatial form, and demonstrate their advantages in reducing
instability and enhancing training efficiency in two datatic control systems.
\\ ( https://arxiv.org/abs/2403.01768 ,  3978kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01781 (*cross-listing*)
Date: Mon, 4 Mar 2024 07:21:07 GMT   (8658kb,D)

Title: Integrating Efficient Optimal Transport and Functional Maps For
  Unsupervised Shape Correspondence Learning
Authors: Tung Le, Khai Nguyen, Shanlin Sun, Nhat Ho, Xiaohui Xie
Categories: cs.CV cs.AI
Comments: accepted by CVPR 2024
\\
  In the realm of computer vision and graphics, accurately establishing
correspondences between geometric 3D shapes is pivotal for applications like
object tracking, registration, texture transfer, and statistical shape
analysis. Moving beyond traditional hand-crafted and data-driven feature
learning methods, we incorporate spectral methods with deep learning, focusing
on functional maps (FMs) and optimal transport (OT). Traditional OT-based
approaches, often reliant on entropy regularization OT in learning-based
framework, face computational challenges due to their quadratic cost. Our key
contribution is to employ the sliced Wasserstein distance (SWD) for OT, which
is a valid fast optimal transport metric in an unsupervised shape matching
framework. This unsupervised framework integrates functional map regularizers
with a novel OT-based loss derived from SWD, enhancing feature alignment
between shapes treated as discrete probability measures. We also introduce an
adaptive refinement process utilizing entropy regularized OT, further refining
feature alignments for accurate point-to-point correspondences. Our method
demonstrates superior performance in non-rigid shape matching, including
near-isometric and non-isometric scenarios, and excels in downstream tasks like
segmentation transfer. The empirical results on diverse datasets highlight our
framework's effectiveness and generalization capabilities, setting new
standards in non-rigid shape matching with efficient OT metrics and an adaptive
refinement module.
\\ ( https://arxiv.org/abs/2403.01781 ,  8658kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01791 (*cross-listing*)
Date: Mon, 4 Mar 2024 07:32:28 GMT   (490kb,D)

Title: Beyond Recommender: An Exploratory Study of the Effects of Different AI
  Roles in AI-Assisted Decision Making
Authors: Shuai Ma, Chenyi Zhang, Xinru Wang, Xiaojuan Ma, Ming Yin
Categories: cs.HC cs.AI
\\
  Artificial Intelligence (AI) is increasingly employed in various
decision-making tasks, typically as a Recommender, providing recommendations
that the AI deems correct. However, recent studies suggest this may diminish
human analytical thinking and lead to humans' inappropriate reliance on AI,
impairing the synergy in human-AI teams. In contrast, human advisors in group
decision-making perform various roles, such as analyzing alternative options or
criticizing decision-makers to encourage their critical thinking. This
diversity of roles has not yet been empirically explored in AI assistance. In
this paper, we examine three AI roles: Recommender, Analyzer, and Devil's
Advocate, and evaluate their effects across two AI performance levels. Our
results show each role's distinct strengths and limitations in task
performance, reliance appropriateness, and user experience. Notably, the
Recommender role is not always the most effective, especially if the AI
performance level is low, the Analyzer role may be preferable. These insights
offer valuable implications for designing AI assistants with adaptive
functional roles according to different situations.
\\ ( https://arxiv.org/abs/2403.01791 ,  490kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01818 (*cross-listing*)
Date: Mon, 4 Mar 2024 08:06:41 GMT   (14636kb,D)

Title: AllSpark: Reborn Labeled Features from Unlabeled in Transformer for
  Semi-Supervised Semantic Segmentation
Authors: Haonan Wang, Qixiang Zhang, Yi Li, Xiaomeng Li
Categories: cs.CV cs.AI
Comments: Accepted by CVPR 2024
\\
  Semi-supervised semantic segmentation (SSSS) has been proposed to alleviate
the burden of time-consuming pixel-level manual labeling, which leverages
limited labeled data along with larger amounts of unlabeled data. Current
state-of-the-art methods train the labeled data with ground truths and
unlabeled data with pseudo labels. However, the two training flows are
separate, which allows labeled data to dominate the training process, resulting
in low-quality pseudo labels and, consequently, sub-optimal results. To
alleviate this issue, we present AllSpark, which reborns the labeled features
from unlabeled ones with the channel-wise cross-attention mechanism. We further
introduce a Semantic Memory along with a Channel Semantic Grouping strategy to
ensure that unlabeled features adequately represent labeled features. The
AllSpark shed new light on the architecture level designs of SSSS rather than
framework level, which avoids increasingly complicated training pipeline
designs. It can also be regarded as a flexible bottleneck module that can be
seamlessly integrated into a general transformer-based segmentation model. The
proposed AllSpark outperforms existing methods across all evaluation protocols
on Pascal, Cityscapes and COCO benchmarks without bells-and-whistles. Code and
model weights are available at: https://github.com/xmed-lab/AllSpark.
\\ ( https://arxiv.org/abs/2403.01818 ,  14636kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01823 (*cross-listing*)
Date: Mon, 4 Mar 2024 08:16:11 GMT   (27027kb,D)

Title: RT-H: Action Hierarchies Using Language
Authors: Suneel Belkhale, Tianli Ding, Ted Xiao, Pierre Sermanet, Quon Vuong,
  Jonathan Tompson, Yevgen Chebotar, Debidatta Dwibedi, Dorsa Sadigh
Categories: cs.RO cs.AI
\\
  Language provides a way to break down complex concepts into digestible
pieces. Recent works in robot imitation learning use language-conditioned
policies that predict actions given visual observations and the high-level task
specified in language. These methods leverage the structure of natural language
to share data between semantically similar tasks (e.g., "pick coke can" and
"pick an apple") in multi-task datasets. However, as tasks become more
semantically diverse (e.g., "pick coke can" and "pour cup"), sharing data
between tasks becomes harder, so learning to map high-level tasks to actions
requires much more demonstration data. To bridge tasks and actions, our insight
is to teach the robot the language of actions, describing low-level motions
with more fine-grained phrases like "move arm forward". Predicting these
language motions as an intermediate step between tasks and actions forces the
policy to learn the shared structure of low-level motions across seemingly
disparate tasks. Furthermore, a policy that is conditioned on language motions
can easily be corrected during execution through human-specified language
motions. This enables a new paradigm for flexible policies that can learn from
human intervention in language. Our method RT-H builds an action hierarchy
using language motions: it first learns to predict language motions, and
conditioned on this and the high-level task, it predicts actions, using visual
context at all stages. We show that RT-H leverages this language-action
hierarchy to learn policies that are more robust and flexible by effectively
tapping into multi-task datasets. We show that these policies not only allow
for responding to language interventions, but can also learn from such
interventions and outperform methods that learn from teleoperated
interventions. Our website and videos are found at
https://rt-hierarchy.github.io.
\\ ( https://arxiv.org/abs/2403.01823 ,  27027kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01827 (*cross-listing*)
Date: Mon, 4 Mar 2024 08:22:29 GMT   (10927kb)

Title: Analysis and Fully Memristor-based Reservoir Computing for Temporal Data
  Classification
Authors: Ankur Singh, Sanghyeon Choi, Gunuk Wang, Maryaradhiya Daimari, and
  Byung-Geun Lee
Categories: cs.NE cs.AI
Comments: 22 pages, 20 figures, Journal
\\
  Reservoir computing (RC) offers a neuromorphic framework that is particularly
effective for processing spatiotemporal signals. Known for its temporal
processing prowess, RC significantly lowers training costs compared to
conventional recurrent neural networks. A key component in its hardware
deployment is the ability to generate dynamic reservoir states. Our research
introduces a novel dual-memory RC system, integrating a short-term memory via a
WOx-based memristor, capable of achieving 16 distinct states encoded over 4
bits, and a long-term memory component using a TiOx-based memristor within the
readout layer. We thoroughly examine both memristor types and leverage the RC
system to process temporal data sets. The performance of the proposed RC system
is validated through two benchmark tasks: isolated spoken digit recognition
with incomplete inputs and Mackey-Glass time series prediction. The system
delivered an impressive 98.84% accuracy in digit recognition and sustained a
low normalized root mean square error (NRMSE) of 0.036 in the time series
prediction task, underscoring its capability. This study illuminates the
adeptness of memristor-based RC systems in managing intricate temporal
challenges, laying the groundwork for further innovations in neuromorphic
computing.
\\ ( https://arxiv.org/abs/2403.01827 ,  10927kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01840 (*cross-listing*)
Date: Mon, 4 Mar 2024 08:38:15 GMT   (9843kb,D)

Title: FreeA: Human-object Interaction Detection using Free Annotation Labels
Authors: Yuxiao Wang, Zhenao Wei, Xinyu Jiang, Yu Lei, Weiying Xue, Jinxiu Liu,
  Qi Liu
Categories: cs.CV cs.AI
Comments: 11 pages, 7 figures, 6 tables
\\
  Recent human-object interaction (HOI) detection approaches rely on high cost
of manpower and require comprehensive annotated image datasets. In this paper,
we propose a novel self-adaption language-driven HOI detection method, termed
as FreeA, without labeling by leveraging the adaptability of CLIP to generate
latent HOI labels. To be specific, FreeA matches image features of human-object
pairs with HOI text templates, and a priori knowledge-based mask method is
developed to suppress improbable interactions. In addition, FreeA utilizes the
proposed interaction correlation matching method to enhance the likelihood of
actions related to a specified action, further refine the generated HOI labels.
Experiments on two benchmark datasets show that FreeA achieves state-of-the-art
performance among weakly supervised HOI models. Our approach is +8.58 mean
Average Precision (mAP) on HICO-DET and +1.23 mAP on V-COCO more accurate in
localizing and classifying the interactive actions than the newest weakly
model, and +1.68 mAP and +7.28 mAP than the latest weakly+ model, respectively.
Code will be available at https://drliuqi.github.io/.
\\ ( https://arxiv.org/abs/2403.01840 ,  9843kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01849 (*cross-listing*)
Date: Mon, 4 Mar 2024 08:59:32 GMT   (4106kb,D)

Title: One Prompt Word is Enough to Boost Adversarial Robustness for
  Pre-trained Vision-Language Models
Authors: Lin Li, Haoyan Guan, Jianing Qiu, Michael Spratling
Categories: cs.CV cs.AI cs.LG
Comments: CVPR2024
\\
  Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having
remarkable generalization ability, are highly vulnerable to adversarial
examples. This work studies the adversarial robustness of VLMs from the novel
perspective of the text prompt instead of the extensively studied model weights
(frozen in this work). We first show that the effectiveness of both adversarial
attack and defense are sensitive to the used text prompt. Inspired by this, we
propose a method to improve resilience to adversarial attacks by learning a
robust text prompt for VLMs. The proposed method, named Adversarial Prompt
Tuning (APT), is effective while being both computationally and data efficient.
Extensive experiments are conducted across 15 datasets and 4 data sparsity
schemes (from 1-shot to full training data settings) to show APT's superiority
over hand-engineered prompts and other state-of-the-art adaption methods. APT
demonstrated excellent abilities in terms of the in-distribution performance
and the generalization under input distribution shift and across datasets.
Surprisingly, by simply adding one learned word to the prompts, APT can
significantly boost the accuracy and robustness (epsilon=4/255) over the
hand-engineered prompts by +13% and +8.5% on average respectively. The
improvement further increases, in our most effective setting, to +26.4% for
accuracy and +16.7% for robustness. Code is available at
https://github.com/TreeLLi/APT.
\\ ( https://arxiv.org/abs/2403.01849 ,  4106kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01861 (*cross-listing*)
Date: Mon, 4 Mar 2024 09:18:13 GMT   (9335kb,D)

Title: AiSDF: Structure-aware Neural Signed Distance Fields in Indoor Scenes
Authors: Jaehoon Jang, Inha Lee, Minje Kim, Kyungdon Joo
Categories: cs.RO cs.AI cs.CV
Comments: 8 pages, 6 figures, Accepted to IEEE RA-L (First two authors
  contributed equally)
\\
  Indoor scenes we are living in are visually homogenous or textureless, while
they inherently have structural forms and provide enough structural priors for
3D scene reconstruction. Motivated by this fact, we propose a structure-aware
online signed distance fields (SDF) reconstruction framework in indoor scenes,
especially under the Atlanta world (AW) assumption. Thus, we dub this
incremental SDF reconstruction for AW as AiSDF. Within the online framework, we
infer the underlying Atlanta structure of a given scene and then estimate
planar surfel regions supporting the Atlanta structure. This Atlanta-aware
surfel representation provides an explicit planar map for a given scene. In
addition, based on these Atlanta planar surfel regions, we adaptively sample
and constrain the structural regularity in the SDF reconstruction, which
enables us to improve the reconstruction quality by maintaining a high-level
structure while enhancing the details of a given scene. We evaluate the
proposed AiSDF on the ScanNet and ReplicaCAD datasets, where we demonstrate
that the proposed framework is capable of reconstructing fine details of
objects implicitly, as well as structures explicitly in room-scale scenes.
\\ ( https://arxiv.org/abs/2403.01861 ,  9335kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01909 (*cross-listing*)
Date: Mon, 4 Mar 2024 10:18:38 GMT   (2459kb,D)

Title: Semi-Supervised Semantic Segmentation Based on Pseudo-Labels: A Survey
Authors: Lingyan Ran, Yali Li, Guoqiang Liang, and Yanning Zhang
Categories: cs.CV cs.AI
\\
  Semantic segmentation is an important and popular research area in computer
vision that focuses on classifying pixels in an image based on their semantics.
However, supervised deep learning requires large amounts of data to train
models and the process of labeling images pixel by pixel is time-consuming and
laborious. This review aims to provide a first comprehensive and organized
overview of the state-of-the-art research results on pseudo-label methods in
the field of semi-supervised semantic segmentation, which we categorize from
different perspectives and present specific methods for specific application
areas. In addition, we explore the application of pseudo-label technology in
medical and remote-sensing image segmentation. Finally, we also propose some
feasible future research directions to address the existing challenges.
\\ ( https://arxiv.org/abs/2403.01909 ,  2459kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01915 (*cross-listing*)
Date: Mon, 4 Mar 2024 10:29:58 GMT   (2666kb,D)

Title: xT: Nested Tokenization for Larger Context in Large Images
Authors: Ritwik Gupta, Shufan Li, Tyler Zhu, Jitendra Malik, Trevor Darrell,
  Karttikeya Mangalam
Categories: cs.CV cs.AI
\\
  Modern computer vision pipelines handle large images in one of two
sub-optimal ways: down-sampling or cropping. These two methods incur
significant losses in the amount of information and context present in an
image. There are many downstream applications in which global context matters
as much as high frequency details, such as in real-world satellite imagery; in
such cases researchers have to make the uncomfortable choice of which
information to discard. We introduce xT, a simple framework for vision
transformers which effectively aggregates global context with local details and
can model large images end-to-end on contemporary GPUs. We select a set of
benchmark datasets across classic vision tasks which accurately reflect a
vision model's ability to understand truly large images and incorporate fine
details over large scales and assess our method's improvement on them. By
introducing a nested tokenization scheme for large images in conjunction with
long-sequence length models normally used for natural language processing, we
are able to increase accuracy by up to 8.6% on challenging classification tasks
and $F_1$ score by 11.6 on context-dependent segmentation in large images.
\\ ( https://arxiv.org/abs/2403.01915 ,  2666kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01964 (*cross-listing*)
Date: Mon, 4 Mar 2024 12:07:28 GMT   (4361kb,D)

Title: The Heterogeneous Productivity Effects of Generative AI
Authors: David Kreitmeir and Paul A. Raschky
Categories: econ.GN cs.AI q-fin.EC
\\
  We analyse the individual productivity effects of Italy's ban on ChatGPT, a
generative pretrained transformer chatbot. We compile data on the daily coding
output quantity and quality of over 36,000 GitHub users in Italy and other
European countries and combine these data with the sudden announcement of the
ban in a difference-in-differences framework. Among the affected users in
Italy, we find a short-term increase in output quantity and quality for less
experienced users and a decrease in productivity on more routine tasks for
experienced users.
\\ ( https://arxiv.org/abs/2403.01964 ,  4361kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01977 (*cross-listing*)
Date: Mon, 4 Mar 2024 12:20:29 GMT   (5551kb,D)

Title: TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation
  under Visual Corruptions
Authors: Maytus Piriyajitakonkij, Mingfei Sun, Mengmi Zhang, Wei Pan
Categories: cs.RO cs.AI cs.CV
Comments: Under review. Code will be available soon
\\
  Robot navigation under visual corruption presents a formidable challenge. To
address this, we propose a Test-time Adaptation (TTA) method, named as TTA-Nav,
for point-goal navigation under visual corruptions. Our "plug-and-play" method
incorporates a top-down decoder to a pre-trained navigation model. Firstly, the
pre-trained navigation model gets a corrupted image and extracts features.
Secondly, the top-down decoder produces the reconstruction given the high-level
features extracted by the pre-trained model. Then, it feeds the reconstruction
of a corrupted image back to the pre-trained model. Finally, the pre-trained
model does forward pass again to output action. Despite being trained solely on
clean images, the top-down decoder can reconstruct cleaner images from
corrupted ones without the need for gradient-based adaptation. The pre-trained
navigation model with our top-down decoder significantly enhances navigation
performance across almost all visual corruptions in our benchmarks. Our method
improves the success rate of point-goal navigation from the state-of-the-art
result of 46% to 94% on the most severe corruption. This suggests its potential
for broader application in robotic visual navigation.
\\ ( https://arxiv.org/abs/2403.01977 ,  5551kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02014 (*cross-listing*)
Date: Mon, 4 Mar 2024 13:14:39 GMT   (1288kb,D)

Title: Unveiling Hidden Links Between Unseen Security Entities
Authors: Daniel Alfasi, Tal Shapira, Anat Bremler Barr
Categories: cs.CR cs.AI
\\
  The proliferation of software vulnerabilities poses a significant challenge
for security databases and analysts tasked with their timely identification,
classification, and remediation. With the National Vulnerability Database (NVD)
reporting an ever-increasing number of vulnerabilities, the traditional manual
analysis becomes untenably time-consuming and prone to errors. This paper
introduces VulnScopper, an innovative approach that utilizes multi-modal
representation learning, combining Knowledge Graphs (KG) and Natural Language
Processing (NLP), to automate and enhance the analysis of software
vulnerabilities. Leveraging ULTRA, a knowledge graph foundation model, combined
with a Large Language Model (LLM), VulnScopper effectively handles unseen
entities, overcoming the limitations of previous KG approaches. We evaluate
VulnScopper on two major security datasets, the NVD and the Red Hat CVE
database. Our method significantly improves the link prediction accuracy
between Common Vulnerabilities and Exposures (CVEs), Common Weakness
Enumeration (CWEs), and Common Platform Enumerations (CPEs). Our results show
that VulnScopper outperforms existing methods, achieving up to 78% Hits@10
accuracy in linking CVEs to CPEs and CWEs and presenting an 11.7% improvement
over large language models in predicting CWE labels based on the Red Hat
database. Based on the NVD, only 6.37% of the linked CPEs are being published
during the first 30 days; many of them are related to critical and high-risk
vulnerabilities which, according to multiple compliance frameworks (such as
CISA and PCI), should be remediated within 15-30 days. Our model can uncover
new products linked to vulnerabilities, reducing remediation time and improving
vulnerability management. We analyzed several CVEs from 2023 to showcase this
ability.
\\ ( https://arxiv.org/abs/2403.02014 ,  1288kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02018 (*cross-listing*)
Date: Mon, 4 Mar 2024 13:20:07 GMT   (1146kb,D)

Title: Cross Domain Policy Transfer with Effect Cycle-Consistency
Authors: Ruiqi Zhu, Tianhong Dai, Oya Celiktutan
Categories: cs.RO cs.AI
Comments: Accepted to International Conference on Robotics and Automation
  (ICRA), 2024
\\
  Training a robotic policy from scratch using deep reinforcement learning
methods can be prohibitively expensive due to sample inefficiency. To address
this challenge, transferring policies trained in the source domain to the
target domain becomes an attractive paradigm. Previous research has typically
focused on domains with similar state and action spaces but differing in other
aspects. In this paper, our primary focus lies in domains with different state
and action spaces, which has broader practical implications, i.e. transfer the
policy from robot A to robot B. Unlike prior methods that rely on paired data,
we propose a novel approach for learning the mapping functions between state
and action spaces across domains using unpaired data. We propose effect cycle
consistency, which aligns the effects of transitions across two domains through
a symmetrical optimization structure for learning these mapping functions. Once
the mapping functions are learned, we can seamlessly transfer the policy from
the source domain to the target domain. Our approach has been tested on three
locomotion tasks and two robotic manipulation tasks. The empirical results
demonstrate that our method can reduce alignment errors significantly and
achieve better performance compared to the state-of-the-art method.
\\ ( https://arxiv.org/abs/2403.02018 ,  1146kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02074 (*cross-listing*)
Date: Mon, 4 Mar 2024 14:21:51 GMT   (1743kb,D)

Title: Modality-Aware and Shift Mixer for Multi-modal Brain Tumor Segmentation
Authors: Zhongzhen Huang, Linda Wei, Shaoting Zhang, Xiaofan Zhang
Categories: cs.CV cs.AI
\\
  Combining images from multi-modalities is beneficial to explore various
information in computer vision, especially in the medical domain. As an
essential part of clinical diagnosis, multi-modal brain tumor segmentation aims
to delineate the malignant entity involving multiple modalities. Although
existing methods have shown remarkable performance in the task, the information
exchange for cross-scale and high-level representations fusion in spatial and
modality are limited in these methods. In this paper, we present a novel
Modality Aware and Shift Mixer that integrates intra-modality and
inter-modality dependencies of multi-modal images for effective and robust
brain tumor segmentation. Specifically, we introduce a Modality-Aware module
according to neuroimaging studies for modeling the specific modality pair
relationships at low levels, and a Modality-Shift module with specific mosaic
patterns is developed to explore the complex relationships across modalities at
high levels via the self-attention. Experimentally, we outperform previous
state-of-the-art approaches on the public Brain Tumor Segmentation (BraTS 2021
segmentation) dataset. Further qualitative experiments demonstrate the efficacy
and robustness of MASM.
\\ ( https://arxiv.org/abs/2403.02074 ,  1743kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02076 (*cross-listing*)
Date: Mon, 4 Mar 2024 14:22:02 GMT   (4513kb,D)

Title: VTG-GPT: Tuning-Free Zero-Shot Video Temporal Grounding with GPT
Authors: Yifang Xu, Yunzhuo Sun, Zien Xie, Benxiang Zhai, and Sidan Du
Categories: cs.CV cs.AI
Comments: 15 pages, 7 figures
DOI: 10.3390/app14051894
\\
  Video temporal grounding (VTG) aims to locate specific temporal segments from
an untrimmed video based on a linguistic query. Most existing VTG models are
trained on extensive annotated video-text pairs, a process that not only
introduces human biases from the queries but also incurs significant
computational costs. To tackle these challenges, we propose VTG-GPT, a
GPT-based method for zero-shot VTG without training or fine-tuning. To reduce
prejudice in the original query, we employ Baichuan2 to generate debiased
queries. To lessen redundant information in videos, we apply MiniGPT-v2 to
transform visual content into more precise captions. Finally, we devise the
proposal generator and post-processing to produce accurate segments from
debiased queries and image captions. Extensive experiments demonstrate that
VTG-GPT significantly outperforms SOTA methods in zero-shot settings and
surpasses unsupervised approaches. More notably, it achieves competitive
performance comparable to supervised methods. The code is available on
https://github.com/YoucanBaby/VTG-GPT
\\ ( https://arxiv.org/abs/2403.02076 ,  4513kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02118 (*cross-listing*)
Date: Mon, 4 Mar 2024 15:21:51 GMT   (20900kb,D)

Title: Position Paper: Towards Implicit Prompt For Text-To-Image Models
Authors: Yue Yang, Yuqi lin, Hong Liu, Wenqi Shao, Runjian Chen, Hailong Shang,
  Yu Wang, Yu Qiao, Kaipeng Zhang, Ping Luo
Categories: cs.CY cs.AI cs.CV
\\
  Recent text-to-image (T2I) models have had great success, and many benchmarks
have been proposed to evaluate their performance and safety. However, they only
consider explicit prompts while neglecting implicit prompts (hint at a target
without explicitly mentioning it). These prompts may get rid of safety
constraints and pose potential threats to the applications of these models.
This position paper highlights the current state of T2I models toward implicit
prompts. We present a benchmark named ImplicitBench and conduct an
investigation on the performance and impacts of implicit prompts with popular
T2I models. Specifically, we design and collect more than 2,000 implicit
prompts of three aspects: General Symbols, Celebrity Privacy, and
Not-Safe-For-Work (NSFW) Issues, and evaluate six well-known T2I models'
capabilities under these implicit prompts. Experiment results show that (1) T2I
models are able to accurately create various target symbols indicated by
implicit prompts; (2) Implicit prompts bring potential risks of privacy leakage
for T2I models. (3) Constraints of NSFW in most of the evaluated T2I models can
be bypassed with implicit prompts. We call for increased attention to the
potential and risks of implicit prompts in the T2I community and further
investigation into the capabilities and impacts of implicit prompts, advocating
for a balanced approach that harnesses their benefits while mitigating their
risks.
\\ ( https://arxiv.org/abs/2403.02118 ,  20900kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02127 (*cross-listing*)
Date: Mon, 4 Mar 2024 15:34:12 GMT   (16415kb,D)

Title: LOCR: Location-Guided Transformer for Optical Character Recognition
Authors: Yu Sun, Dongzhan Zhou, Chen Lin, Conghui He, Wanli Ouyang, Han-Sen
  Zhong
Categories: cs.CV cs.AI cs.CL
\\
  Academic documents are packed with texts, equations, tables, and figures,
requiring comprehensive understanding for accurate Optical Character
Recognition (OCR). While end-to-end OCR methods offer improved accuracy over
layout-based approaches, they often grapple with significant repetition issues,
especially with complex layouts in Out-Of-Domain (OOD) documents.To tackle this
issue, we propose LOCR, a model that integrates location guiding into the
transformer architecture during autoregression. We train the model on a dataset
comprising over 77M text-location pairs from 125K academic document pages,
including bounding boxes for words, tables and mathematical symbols. LOCR
adeptly handles various formatting elements and generates content in Markdown
language. It outperforms all existing methods in our test set constructed from
arXiv, as measured by edit distance, BLEU, METEOR and F-measure.LOCR also
reduces repetition frequency from 4.4% of pages to 0.5% in the arXiv dataset,
from 13.2% to 1.3% in OOD quantum physics documents and from 8.1% to 1.8% in
OOD marketing documents. Additionally, LOCR features an interactive OCR mode,
facilitating the generation of complex documents through a few location prompts
from human.
\\ ( https://arxiv.org/abs/2403.02127 ,  16415kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02131 (*cross-listing*)
Date: Mon, 4 Mar 2024 15:40:28 GMT   (10527kb,D)

Title: Deep Reinforcement Learning for Dynamic Algorithm Selection: A
  Proof-of-Principle Study on Differential Evolution
Authors: Hongshu Guo, Yining Ma, Zeyuan Ma, Jiacheng Chen, Xinglin Zhang,
  Zhiguang Cao, Jun Zhang, Yue-Jiao Gong
Categories: cs.NE cs.AI
Comments: Accepted by IEEE Transactions on Systems, Man, and Cybernetics:
  Systems at Thu, Feb 29, 2024
\\
  Evolutionary algorithms, such as Differential Evolution, excel in solving
real-parameter optimization challenges. However, the effectiveness of a single
algorithm varies across different problem instances, necessitating considerable
efforts in algorithm selection or configuration. This paper aims to address the
limitation by leveraging the complementary strengths of a group of algorithms
and dynamically scheduling them throughout the optimization progress for
specific problems. We propose a deep reinforcement learning-based dynamic
algorithm selection framework to accomplish this task. Our approach models the
dynamic algorithm selection a Markov Decision Process, training an agent in a
policy gradient manner to select the most suitable algorithm according to the
features observed during the optimization process. To empower the agent with
the necessary information, our framework incorporates a thoughtful design of
landscape and algorithmic features. Meanwhile, we employ a sophisticated deep
neural network model to infer the optimal action, ensuring informed algorithm
selections. Additionally, an algorithm context restoration mechanism is
embedded to facilitate smooth switching among different algorithms. These
mechanisms together enable our framework to seamlessly select and switch
algorithms in a dynamic online fashion. Notably, the proposed framework is
simple and generic, offering potential improvements across a broad spectrum of
evolutionary algorithms. As a proof-of-principle study, we apply this framework
to a group of Differential Evolution algorithms. The experimental results
showcase the remarkable effectiveness of the proposed framework, not only
enhancing the overall optimization performance but also demonstrating favorable
generalization ability across different problem classes.
\\ ( https://arxiv.org/abs/2403.02131 ,  10527kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02167 (*cross-listing*)
Date: Mon, 4 Mar 2024 16:13:39 GMT   (3620kb,D)

Title: Speech emotion recognition from voice messages recorded in the wild
Authors: Luc\'ia G\'omez-Zaragoz\'a, \'Oscar Valls, Roc\'io del Amor, Mar\'ia
  Jos\'e Castro-Bleda, Valery Naranjo, Mariano Alca\~niz Raya, Javier
  Mar\'in-Morales
Categories: eess.AS cs.AI cs.CL cs.SD
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
ACM-class: I.5.1; I.5.4
\\
  Emotion datasets used for Speech Emotion Recognition (SER) often contain
acted or elicited speech, limiting their applicability in real-world scenarios.
In this work, we used the Emotional Voice Messages (EMOVOME) database,
including spontaneous voice messages from conversations of 100 Spanish speakers
on a messaging app, labeled in continuous and discrete emotions by expert and
non-expert annotators. We created speaker-independent SER models using the
eGeMAPS features, transformer-based models and their combination. We compared
the results with reference databases and analyzed the influence of annotators
and gender fairness. The pre-trained Unispeech-L model and its combination with
eGeMAPS achieved the highest results, with 61.64% and 55.57% Unweighted
Accuracy (UA) for 3-class valence and arousal prediction respectively, a 10%
improvement over baseline models. For the emotion categories, 42.58% UA was
obtained. EMOVOME performed lower than the acted RAVDESS database. The elicited
IEMOCAP database also outperformed EMOVOME in the prediction of emotion
categories, while similar results were obtained in valence and arousal.
Additionally, EMOVOME outcomes varied with annotator labels, showing superior
results and better fairness when combining expert and non-expert annotations.
This study significantly contributes to the evaluation of SER models in
real-life situations, advancing in the development of applications for
analyzing spontaneous voice messages.
\\ ( https://arxiv.org/abs/2403.02167 ,  3620kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02227 (*cross-listing*)
Date: Mon, 4 Mar 2024 17:15:09 GMT   (89kb,D)

Title: Policy Space Response Oracles: A Survey
Authors: Ariyan Bighashdel, Yongzhao Wang, Stephen McAleer, Rahul Savani, Frans
  A. Oliehoek
Categories: cs.GT cs.AI cs.MA
Comments: Ariyan Bighashdel and Yongzhao Wang contributed equally
\\
  In game theory, a game refers to a model of interaction among rational
decision-makers or players, making choices with the goal of achieving their
individual objectives. Understanding their behavior in games is often referred
to as game reasoning. This survey provides a comprehensive overview of a
fast-developing game-reasoning framework for large games, known as Policy Space
Response Oracles (PSRO). We first motivate PSRO, provide historical context,
and position PSRO within game-reasoning approaches. We then focus on the
strategy exploration issue for PSRO, the challenge of assembling an effective
strategy portfolio for modeling the underlying game with minimum computational
cost. We also survey current research directions for enhancing the efficiency
of PSRO, and explore the applications of PSRO across various domains. We
conclude by discussing open questions and future research.
\\ ( https://arxiv.org/abs/2403.02227 ,  89kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02232 (*cross-listing*)
Date: Mon, 4 Mar 2024 17:22:43 GMT   (365kb)

Title: Comprehensive evaluation of Mal-API-2019 dataset by machine learning in
  malware detection
Authors: Zhenglin Li, Haibei Zhu, Houze Liu, Jintong Song, Qishuo Cheng
Categories: cs.CR cs.AI cs.LG
\\
  This study conducts a thorough examination of malware detection using machine
learning techniques, focusing on the evaluation of various classification
models using the Mal-API-2019 dataset. The aim is to advance cybersecurity
capabilities by identifying and mitigating threats more effectively. Both
ensemble and non-ensemble machine learning methods, such as Random Forest,
XGBoost, K Nearest Neighbor (KNN), and Neural Networks, are explored. Special
emphasis is placed on the importance of data pre-processing techniques,
particularly TF-IDF representation and Principal Component Analysis, in
improving model performance. Results indicate that ensemble methods,
particularly Random Forest and XGBoost, exhibit superior accuracy, precision,
and recall compared to others, highlighting their effectiveness in malware
detection. The paper also discusses limitations and potential future
directions, emphasizing the need for continuous adaptation to address the
evolving nature of malware. This research contributes to ongoing discussions in
cybersecurity and provides practical insights for developing more robust
malware detection systems in the digital era.
\\ ( https://arxiv.org/abs/2403.02232 ,  365kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02238 (*cross-listing*)
Date: Mon, 4 Mar 2024 17:29:57 GMT   (1296kb,D)

Title: Towards Intent-Based Network Management: Large Language Models for
  Intent Extraction in 5G Core Networks
Authors: Dimitrios Michael Manias and Ali Chouman and Abdallah Shami
Categories: cs.NI cs.AI
Comments: Submitted to: International Conference on the Design of Reliable
  Communication Networks 2024
\\
  The integration of Machine Learning and Artificial Intelligence (ML/AI) into
fifth-generation (5G) networks has made evident the limitations of network
intelligence with ever-increasing, strenuous requirements for current and
next-generation devices. This transition to ubiquitous intelligence demands
high connectivity, synchronicity, and end-to-end communication between users
and network operators, and will pave the way towards full network automation
without human intervention. Intent-based networking is a key factor in the
reduction of human actions, roles, and responsibilities while shifting towards
novel extraction and interpretation of automated network management. This paper
presents the development of a custom Large Language Model (LLM) for 5G and
next-generation intent-based networking and provides insights into future LLM
developments and integrations to realize end-to-end intent-based networking for
fully automated network intelligence.
\\ ( https://arxiv.org/abs/2403.02238 ,  1296kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02249 (*cross-listing*)
Date: Mon, 4 Mar 2024 17:34:59 GMT   (12043kb,D)

Title: Non-autoregressive Sequence-to-Sequence Vision-Language Models
Authors: Kunyu Shi, Qi Dong, Luis Goncalves, Zhuowen Tu, Stefano Soatto
Categories: cs.CV cs.AI
Comments: Accepted to CVPR 2024
\\
  Sequence-to-sequence vision-language models are showing promise, but their
applicability is limited by their inference latency due to their autoregressive
way of generating predictions. We propose a parallel decoding
sequence-to-sequence vision-language model, trained with a Query-CTC loss, that
marginalizes over multiple inference paths in the decoder. This allows us to
model the joint distribution of tokens, rather than restricting to conditional
distribution as in an autoregressive model. The resulting model, NARVL,
achieves performance on-par with its state-of-the-art autoregressive
counterpart, but is faster at inference time, reducing from the linear
complexity associated with the sequential generation of tokens to a paradigm of
constant time joint inference.
\\ ( https://arxiv.org/abs/2403.02249 ,  12043kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02253 (*cross-listing*)
Date: Mon, 4 Mar 2024 17:38:32 GMT   (3872kb,D)

Title: KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for
  Enhancing Reference-Based Phishing Detection
Authors: Yuexin Li, Chengyu Huang, Shumin Deng, Mei Lin Lock, Tri Cao, Nay Oo,
  Bryan Hooi, Hoon Wei Lim
Categories: cs.CR cs.AI cs.CL cs.LG
\\
  Phishing attacks have inflicted substantial losses on individuals and
businesses alike, necessitating the development of robust and efficient
automated phishing detection approaches. Reference-based phishing detectors
(RBPDs), which compare the logos on a target webpage to a known set of logos,
have emerged as the state-of-the-art approach. However, a major limitation of
existing RBPDs is that they rely on a manually constructed brand knowledge
base, making it infeasible to scale to a large number of brands, which results
in false negative errors due to the insufficient brand coverage of the
knowledge base. To address this issue, we propose an automated knowledge
collection pipeline, using which we collect and release a large-scale
multimodal brand knowledge base, KnowPhish, containing 20k brands with rich
information about each brand. KnowPhish can be used to boost the performance of
existing RBPDs in a plug-and-play manner. A second limitation of existing RBPDs
is that they solely rely on the image modality, ignoring useful textual
information present in the webpage HTML. To utilize this textual information,
we propose a Large Language Model (LLM)-based approach to extract brand
information of webpages from text. Our resulting multimodal phishing detection
approach, KnowPhish Detector (KPD), can detect phishing webpages with or
without logos. We evaluate KnowPhish and KPD on a manually validated dataset,
and on a field study under Singapore's local context, showing substantial
improvements in effectiveness and efficiency compared to state-of-the-art
baselines.
\\ ( https://arxiv.org/abs/2403.02253 ,  3872kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02302 (*cross-listing*)
Date: Mon, 4 Mar 2024 18:32:12 GMT   (17793kb,D)

Title: Beyond Specialization: Assessing the Capabilities of MLLMs in Age and
  Gender Estimation
Authors: Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh
Categories: cs.CV cs.AI cs.LG
ACM-class: I.2.0; I.4.0; I.4.9
\\
  Multimodal Large Language Models (MLLMs) have recently gained immense
popularity. Powerful commercial models like ChatGPT-4V and Gemini, as well as
open-source ones such as LLaVA, are essentially general-purpose models and are
applied to solve a wide variety of tasks, including those in computer vision.
These neural networks possess such strong general knowledge and reasoning
abilities that they have proven capable of working even on tasks for which they
were not specifically trained. We compared the capabilities of the most
powerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized task
of age and gender estimation with our state-of-the-art specialized model,
MiVOLO. We also updated MiVOLO and provide details and new metrics in this
article. This comparison has yielded some interesting results and insights
about the strengths and weaknesses of the participating models. Furthermore, we
attempted various ways to fine-tune the ShareGPT4V model for this specific
task, aiming to achieve state-of-the-art results in this particular challenge.
Although such a model would not be practical in production, as it is incredibly
expensive compared to a specialized model like MiVOLO, it could be very useful
in some tasks, like data annotation.
\\ ( https://arxiv.org/abs/2403.02302 ,  17793kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02325 (*cross-listing*)
Date: Mon, 4 Mar 2024 18:55:30 GMT   (7484kb,D)

Title: Contrastive Region Guidance: Improving Grounding in Vision-Language
  Models without Training
Authors: David Wan, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: Project website: https://contrastive-region-guidance.github.io/
\\
  Highlighting particularly relevant regions of an image can improve the
performance of vision-language models (VLMs) on various vision-language (VL)
tasks by guiding the model to attend more closely to these regions of interest.
For example, VLMs can be given a "visual prompt", where visual markers such as
bounding boxes delineate key image regions. However, current VLMs that can
incorporate visual guidance are either proprietary and expensive or require
costly training on curated data that includes visual prompts. We introduce
Contrastive Region Guidance (CRG), a training-free guidance method that enables
open-source VLMs to respond to visual prompts. CRG contrasts model outputs
produced with and without visual prompts, factoring out biases revealed by the
model when answering without the information required to produce a correct
answer (i.e., the model's prior). CRG achieves substantial improvements in a
wide variety of VL tasks: When region annotations are provided, CRG increases
absolute accuracy by up to 11.1% on ViP-Bench, a collection of six diverse
region-based tasks such as recognition, math, and object relationship
reasoning. We also show CRG's applicability to spatial reasoning, with 10%
improvement on What'sUp, as well as to compositional generalization --
improving accuracy by 11.5% and 7.5% on two challenging splits from SugarCrepe
-- and to image-text alignment for generated images, where we improve by up to
8.4 AUROC and 6.8 F1 points on SeeTRUE. When reference regions are absent, CRG
allows us to re-rank proposed regions in referring expression comprehension and
phrase grounding benchmarks like RefCOCO/+/g and Flickr30K Entities, with an
average gain of 3.2% in accuracy. Our analysis explores alternative masking
strategies for CRG, quantifies CRG's probability shift, and evaluates the role
of region guidance strength, empirically validating CRG's design choices.
\\ ( https://arxiv.org/abs/2403.02325 ,  7484kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02327 (*cross-listing*)
Date: Mon, 4 Mar 2024 18:55:50 GMT   (602kb,D)

Title: Model Lakes
Authors: Koyena Pal, David Bau, Ren\'ee J. Miller
Categories: cs.DB cs.AI
\\
  Given a set of deep learning models, it can be hard to find models
appropriate to a task, understand the models, and characterize how models are
different one from another. Currently, practitioners rely on manually-written
documentation to understand and choose models. However, not all models have
complete and reliable documentation. As the number of machine learning models
increases, this issue of finding, differentiating, and understanding models is
becoming more crucial. Inspired from research on data lakes, we introduce and
define the concept of model lakes. We discuss fundamental research challenges
in the management of large models. And we discuss what principled data
management techniques can be brought to bear on the study of large model
management.
\\ ( https://arxiv.org/abs/2403.02327 ,  602kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02336 (*cross-listing*)
Date: Mon, 4 Mar 2024 18:58:53 GMT   (8109kb,D)

Title: Brand Visibility in Packaging: A Deep Learning Approach for Logo
  Detection, Saliency-Map Prediction, and Logo Placement Analysis
Authors: Alireza Hosseini, Kiana Hooshanfar, Pouria Omrani, Reza Toosi, Ramin
  Toosi, Zahra Ebrahimian, Mohammad Ali Akhaee
Categories: cs.CV cs.AI
\\
  In the highly competitive area of product marketing, the visibility of brand
logos on packaging plays a crucial role in shaping consumer perception,
directly influencing the success of the product. This paper introduces a
comprehensive framework to measure the brand logo's attention on a packaging
design. The proposed method consists of three steps. The first step leverages
YOLOv8 for precise logo detection across prominent datasets, FoodLogoDet-1500
and LogoDet-3K. The second step involves modeling the user's visual attention
with a novel saliency prediction model tailored for the packaging context. The
proposed saliency model combines the visual elements with text maps employing a
transformers-based architecture to predict user attention maps. In the third
step, by integrating logo detection with a saliency map generation, the
framework provides a comprehensive brand attention score. The effectiveness of
the proposed method is assessed module by module, ensuring a thorough
evaluation of each component. Comparing logo detection and saliency map
prediction with state-of-the-art models shows the superiority of the proposed
methods. To investigate the robustness of the proposed brand attention score,
we collected a unique dataset to examine previous psychophysical hypotheses
related to brand visibility. the results show that the brand attention score is
in line with all previous studies. Also, we introduced seven new hypotheses to
check the impact of position, orientation, presence of person, and other visual
elements on brand attention. This research marks a significant stride in the
intersection of cognitive psychology, computer vision, and marketing, paving
the way for advanced, consumer-centric packaging designs.
\\ ( https://arxiv.org/abs/2403.02336 ,  8109kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02338 (*cross-listing*)
Date: Mon, 4 Mar 2024 18:59:30 GMT   (15272kb,D)

Title: Twisting Lids Off with Two Hands
Authors: Toru Lin, Zhao-Heng Yin, Haozhi Qi, Pieter Abbeel, Jitendra Malik
Categories: cs.RO cs.AI cs.CV cs.LG
Comments: Project page can be found at https://toruowo.github.io/bimanual-twist
\\
  Manipulating objects with two multi-fingered hands has been a long-standing
challenge in robotics, attributed to the contact-rich nature of many
manipulation tasks and the complexity inherent in coordinating a
high-dimensional bimanual system. In this work, we consider the problem of
twisting lids of various bottle-like objects with two hands, and demonstrate
that policies trained in simulation using deep reinforcement learning can be
effectively transferred to the real world. With novel engineering insights into
physical modeling, real-time perception, and reward design, the policy
demonstrates generalization capabilities across a diverse set of unseen
objects, showcasing dynamic and dexterous behaviors. Our findings serve as
compelling evidence that deep reinforcement learning combined with sim-to-real
transfer remains a promising approach for addressing manipulation problems of
unprecedented complexity.
\\ ( https://arxiv.org/abs/2403.02338 ,  15272kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00774 (*cross-listing*)
Date: Wed, 14 Feb 2024 02:33:17 GMT   (712kb)

Title: Regional inflation analysis using social network data
Authors: Vasilii Chsherbakov Ilia Karpov
Categories: q-fin.ST cs.CL cs.SI
\\
  Inflation is one of the most important macroeconomic indicators that have a
great impact on the population of any country and region. Inflation is
influenced by range of factors, one of which is inflation expectations. Many
central banks take this factor into consideration while implementing monetary
policy within the inflation targeting regime. Nowadays, a lot of people are
active users of the Internet, especially social networks. There is a hypothesis
that people search, read, and discuss mainly only those issues that are of
particular interest to them. It is logical to assume that the dynamics of
prices may also be in the focus of user discussions. So, such discussions could
be regarded as an alternative source of more rapid information about inflation
expectations. This study is based on unstructured data from Vkontakte social
network to analyze upward and downward inflationary trends (on the example of
the Omsk region). The sample of more than 8.5 million posts was collected
between January 2010 and May 2022. The authors used BERT neural networks to
solve the problem. These models demonstrated better results than the benchmarks
(e.g., logistic regression, decision tree classifier, etc.). It makes possible
to define pro-inflationary and disinflationary types of keywords in different
contexts and get their visualization with SHAP method. This analysis provides
additional operational information about inflationary processes at the regional
level The proposed approach can be scaled for other regions. At the same time
the limitation of the work is the time and power costs for the initial training
of similar models for all regions of Russia.
\\ ( https://arxiv.org/abs/2403.00774 ,  712kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00801 (*cross-listing*)
Date: Fri, 23 Feb 2024 18:45:35 GMT   (516kb,D)

Title: Self-Retrieval: Building an Information Retrieval System with One Large
  Language Model
Authors: Qiaoyu Tang, Jiawei Chen, Bowen Yu, Yaojie Lu, Cheng Fu, Haiyang Yu,
  Hongyu Lin, Fei Huang, Ben He, Xianpei Han, Le Sun, Yongbin Li
Categories: cs.IR cs.CL
\\
  The rise of large language models (LLMs) has transformed the role of
information retrieval (IR) systems in the way to humans accessing information.
Due to the isolated architecture and the limited interaction, existing IR
systems are unable to fully accommodate the shift from directly providing
information to humans to indirectly serving large language models. In this
paper, we propose Self-Retrieval, an end-to-end, LLM-driven information
retrieval architecture that can fully internalize the required abilities of IR
systems into a single LLM and deeply leverage the capabilities of LLMs during
IR process. Specifically, Self-retrieval internalizes the corpus to retrieve
into a LLM via a natural language indexing architecture. Then the entire
retrieval process is redefined as a procedure of document generation and
self-assessment, which can be end-to-end executed using a single large language
model. Experimental results demonstrate that Self-Retrieval not only
significantly outperforms previous retrieval approaches by a large margin, but
also can significantly boost the performance of LLM-driven downstream
applications like retrieval augumented generation.
\\ ( https://arxiv.org/abs/2403.00801 ,  516kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00806 (*cross-listing*)
Date: Sat, 24 Feb 2024 12:17:06 GMT   (989kb)

Title: Enhanced User Interaction in Operating Systems through Machine Learning
  Language Models
Authors: Chenwei Zhang, Wenran Lu, Chunhe Ni, Hongbo Wang, Jiang Wu
Categories: cs.IR cs.CE cs.CL cs.CV
\\
  With the large language model showing human-like logical reasoning and
understanding ability, whether agents based on the large language model can
simulate the interaction behavior of real users, so as to build a reliable
virtual recommendation A/B test scene to help the application of recommendation
research is an urgent, important and economic value problem. The combination of
interaction design and machine learning can provide a more efficient and
personalized user experience for products and services. This personalized
service can meet the specific needs of users and improve user satisfaction and
loyalty. Second, the interactive system can understand the user's views and
needs for the product by providing a good user interface and interactive
experience, and then use machine learning algorithms to improve and optimize
the product. This iterative optimization process can continuously improve the
quality and performance of the product to meet the changing needs of users. At
the same time, designers need to consider how these algorithms and tools can be
combined with interactive systems to provide a good user experience. This paper
explores the potential applications of large language models, machine learning
and interaction design for user interaction in recommendation systems and
operating systems. By integrating these technologies, more intelligent and
personalized services can be provided to meet user needs and promote continuous
improvement and optimization of products. This is of great value for both
recommendation research and user experience applications.
\\ ( https://arxiv.org/abs/2403.00806 ,  989kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00807 (*cross-listing*)
Date: Sat, 24 Feb 2024 12:31:22 GMT   (587kb)

Title: Enhancing Cloud-Based Large Language Model Processing with Elasticsearch
  and Transformer Models
Authors: Chunhe Ni, Jiang Wu, Hongbo Wang, Wenran Lu, Chenwei Zhang
Categories: cs.IR cs.CL cs.DC cs.DL
\\
  Large Language Models (LLMs) are a class of generative AI models built using
the Transformer network, capable of leveraging vast datasets to identify,
summarize, translate, predict, and generate language. LLMs promise to
revolutionize society, yet training these foundational models poses immense
challenges. Semantic vector search within large language models is a potent
technique that can significantly enhance search result accuracy and relevance.
Unlike traditional keyword-based search methods, semantic search utilizes the
meaning and context of words to grasp the intent behind queries and deliver
more precise outcomes. Elasticsearch emerges as one of the most popular tools
for implementing semantic search an exceptionally scalable and robust search
engine designed for indexing and searching extensive datasets. In this article,
we delve into the fundamentals of semantic search and explore how to harness
Elasticsearch and Transformer models to bolster large language model processing
paradigms. We gain a comprehensive understanding of semantic search principles
and acquire practical skills for implementing semantic search in real-world
model application scenarios.
\\ ( https://arxiv.org/abs/2403.00807 ,  587kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00820 (*cross-listing*)
Date: Mon, 26 Feb 2024 12:56:17 GMT   (578kb,D)

Title: Retrieval Augmented Generation Systems: Automatic Dataset Creation,
  Evaluation and Boolean Agent Setup
Authors: Tristan Kenneweg and Philip Kenneweg and Barbara Hammer
Categories: cs.IR cs.CL
Comments: Was handed in to IJCNN prior to preprint publication here. Was
  neither accepted nor rejected at date of publication here
\\
  Retrieval Augmented Generation (RAG) systems have seen huge popularity in
augmenting Large-Language Model (LLM) outputs with domain specific and time
sensitive data. Very recently a shift is happening from simple RAG setups that
query a vector database for additional information with every user input to
more sophisticated forms of RAG. However, different concrete approaches compete
on mostly anecdotal evidence at the moment. In this paper we present a rigorous
dataset creation and evaluation workflow to quantitatively compare different
RAG strategies. We use a dataset created this way for the development and
evaluation of a boolean agent RAG setup: A system in which a LLM can decide
whether to query a vector database or not, thus saving tokens on questions that
can be answered with internal knowledge. We publish our code and generated
dataset online.
\\ ( https://arxiv.org/abs/2403.00820 ,  578kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00923 (*cross-listing*)
Date: Fri, 1 Mar 2024 19:08:25 GMT   (806kb,D)

Title: An Interpretable Ensemble of Graph and Language Models for Improving
  Search Relevance in E-Commerce
Authors: Nurendra Choudhary, Edward W Huang, Karthik Subbian, Chandan K. Reddy
Categories: cs.IR cs.CL
Comments: Accepted to The Web Conference 2024 (Industry)
ACM-class: H.3.3; I.2.7; J.7
\\
  The problem of search relevance in the E-commerce domain is a challenging one
since it involves understanding the intent of a user's short nuanced query and
matching it with the appropriate products in the catalog. This problem has
traditionally been addressed using language models (LMs) and graph neural
networks (GNNs) to capture semantic and inter-product behavior signals,
respectively. However, the rapid development of new architectures has created a
gap between research and the practical adoption of these techniques. Evaluating
the generalizability of these models for deployment requires extensive
experimentation on complex, real-world datasets, which can be non-trivial and
expensive. Furthermore, such models often operate on latent space
representations that are incomprehensible to humans, making it difficult to
evaluate and compare the effectiveness of different models. This lack of
interpretability hinders the development and adoption of new techniques in the
field. To bridge this gap, we propose Plug and Play Graph LAnguage Model
(PP-GLAM), an explainable ensemble of plug and play models. Our approach uses a
modular framework with uniform data processing pipelines. It employs additive
explanation metrics to independently decide whether to include (i) language
model candidates, (ii) GNN model candidates, and (iii) inter-product behavioral
signals. For the task of search relevance, we show that PP-GLAM outperforms
several state-of-the-art baselines as well as a proprietary model on real-world
multilingual, multi-regional e-commerce datasets. To promote better model
comprehensibility and adoption, we also provide an analysis of the
explainability and computational complexity of our model. We also provide the
public codebase and provide a deployment strategy for practical implementation.
\\ ( https://arxiv.org/abs/2403.00923 ,  806kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01457 (*cross-listing*)
Date: Sun, 3 Mar 2024 09:22:21 GMT   (1167kb,D)

Title: Logic Rules as Explanations for Legal Case Retrieval
Authors: Zhongxiang Sun, Kepu Zhang, Weijie Yu, Haoyu Wang, Jun Xu
Categories: cs.IR cs.CL
Comments: accepted by lrec-coling 2024
\\
  In this paper, we address the issue of using logic rules to explain the
results from legal case retrieval. The task is critical to legal case retrieval
because the users (e.g., lawyers or judges) are highly specialized and require
the system to provide logical, faithful, and interpretable explanations before
making legal decisions. Recently, research efforts have been made to learn
explainable legal case retrieval models. However, these methods usually select
rationales (key sentences) from the legal cases as explanations, failing to
provide faithful and logically correct explanations. In this paper, we propose
Neural-Symbolic enhanced Legal Case Retrieval (NS-LCR), a framework that
explicitly conducts reasoning on the matching of legal cases through learning
case-level and law-level logic rules. The learned rules are then integrated
into the retrieval process in a neuro-symbolic manner. Benefiting from the
logic and interpretable nature of the logic rules, NS-LCR is equipped with
built-in faithful explainability. We also show that NS-LCR is a model-agnostic
framework that can be plugged in for multiple legal retrieval models. To
showcase NS-LCR's superiority, we enhance existing benchmarks by adding
manually annotated logic rules and introducing a novel explainability metric
using Large Language Models (LLMs). Our comprehensive experiments reveal
NS-LCR's effectiveness for ranking, alongside its proficiency in delivering
reliable explanations for legal case retrieval.
\\ ( https://arxiv.org/abs/2403.01457 ,  1167kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01472 (*cross-listing*)
Date: Sun, 3 Mar 2024 10:39:27 GMT   (15553kb,D)

Title: WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service
  Copyright Protection
Authors: Anudeex Shetty, Yue Teng, Ke He, Qiongkai Xu
Categories: cs.CR cs.CL cs.LG
Comments: Work in Progress
\\
  Embedding as a Service (EaaS) has become a widely adopted solution, which
offers feature extraction capabilities for addressing various downstream tasks
in Natural Language Processing (NLP). Prior studies have shown that EaaS can be
prone to model extraction attacks; nevertheless, this concern could be
mitigated by adding backdoor watermarks to the text embeddings and subsequently
verifying the attack models post-publication. Through the analysis of the
recent watermarking strategy for EaaS, EmbMarker, we design a novel CSE
(Clustering, Selection, Elimination) attack that removes the backdoor watermark
while maintaining the high utility of embeddings, indicating that the previous
watermarking approach can be breached. In response to this new threat, we
propose a new protocol to make the removal of watermarks more challenging by
incorporating multiple possible watermark directions. Our defense approach,
WARDEN, notably increases the stealthiness of watermarks and empirically has
been shown effective against CSE attack.
\\ ( https://arxiv.org/abs/2403.01472 ,  15553kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01747 (*cross-listing*)
Date: Mon, 4 Mar 2024 05:52:41 GMT   (3473kb,D)

Title: Towards Self-Contained Answers: Entity-Based Answer Rewriting in
  Conversational Search
Authors: Ivan Sekuli\'c, Krisztian Balog, Fabio Crestani
Categories: cs.IR cs.CL
DOI: 10.1145/3627508.3638300
\\
  Conversational information-seeking (CIS) is an emerging paradigm for
knowledge acquisition and exploratory search. Traditional web search interfaces
enable easy exploration of entities, but this is limited in conversational
settings due to the limited-bandwidth interface. This paper explore ways to
rewrite answers in CIS, so that users can understand them without having to
resort to external services or sources. Specifically, we focus on salient
entities -- entities that are central to understanding the answer. As our first
contribution, we create a dataset of conversations annotated with entities for
saliency. Our analysis of the collected data reveals that the majority of
answers contain salient entities. As our second contribution, we propose two
answer rewriting strategies aimed at improving the overall user experience in
CIS. One approach expands answers with inline definitions of salient entities,
making the answer self-contained. The other approach complements answers with
follow-up questions, offering users the possibility to learn more about
specific entities. Results of a crowdsourcing-based study indicate that
rewritten answers are clearly preferred over the original ones. We also find
that inline definitions tend to be favored over follow-up questions, but this
choice is highly subjective, thereby providing a promising future direction for
personalization.
\\ ( https://arxiv.org/abs/2403.01747 ,  3473kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02090 (*cross-listing*)
Date: Mon, 4 Mar 2024 14:46:58 GMT   (1352kb,D)

Title: Modeling Multimodal Social Interactions: New Challenges and Baselines
  with Densely Aligned Representations
Authors: Sangmin Lee, Bolin Lai, Fiona Ryan, Bikram Boote, James M. Rehg
Categories: cs.CV cs.CL cs.LG
Comments: CVPR 2024
\\
  Understanding social interactions involving both verbal and non-verbal cues
is essential to effectively interpret social situations. However, most prior
works on multimodal social cues focus predominantly on single-person behaviors
or rely on holistic visual representations that are not densely aligned to
utterances in multi-party environments. They are limited in modeling the
intricate dynamics of multi-party interactions. In this paper, we introduce
three new challenging tasks to model the fine-grained dynamics between multiple
people: speaking target identification, pronoun coreference resolution, and
mentioned player prediction. We contribute extensive data annotations to curate
these new challenges in social deduction game settings. Furthermore, we propose
a novel multimodal baseline that leverages densely aligned language-visual
representations by synchronizing visual features with their corresponding
utterances. This facilitates concurrently capturing verbal and non-verbal cues
pertinent to social reasoning. Experiments demonstrate the effectiveness of the
proposed approach with densely aligned multimodal representations in modeling
social interactions. We will release our benchmarks and source code to
facilitate further research.
\\ ( https://arxiv.org/abs/2403.02090 ,  1352kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00766 (*cross-listing*)
Date: Fri, 9 Feb 2024 07:25:07 GMT   (1625kb,D)

Title: Towards Fair and Firm Real-Time Scheduling in DNN Multi-Tenant
  Multi-Accelerator Systems via Reinforcement Learning
Authors: Enrico Russo, Francesco Giulio Blanco, Maurizio Palesi, Giuseppe
  Ascia, Davide Patti, Vincenzo Catania
Categories: cs.AR cs.DC cs.LG
\\
  This paper addresses the critical challenge of managing Quality of Service
(QoS) in cloud services, focusing on the nuances of individual tenant
expectations and varying Service Level Indicators (SLIs). It introduces a novel
approach utilizing Deep Reinforcement Learning for tenant-specific QoS
management in multi-tenant, multi-accelerator cloud environments. The chosen
SLI, deadline hit rate, allows clients to tailor QoS for each service request.
A novel online scheduling algorithm for Deep Neural Networks in
multi-accelerator systems is proposed, with a focus on guaranteeing
tenant-wise, model-specific QoS levels while considering real-time constraints.
\\ ( https://arxiv.org/abs/2403.00766 ,  1625kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00769 (*cross-listing*)
Date: Sun, 11 Feb 2024 11:41:25 GMT   (1058kb)

Title: Text mining in education
Authors: R. Ferreira-Mello, M. Andre, A. Pinheiro, E. Costa, and C. Romero
Categories: cs.IR cs.CY cs.LG
Journal-ref: Wiley Interdisciplinary Reviews: Data Mining and Knowledge
  Discovery (2019); 9(6):e1332
DOI: 10.1002/widm.1332
\\
  The explosive growth of online education environments is generating a massive
volume of data, specially in text format from forums, chats, social networks,
assessments, essays, among others. It produces exciting challenges on how to
mine text data in order to find useful knowledge for educational stakeholders.
Despite the increasing number of educational applications of text mining
published recently, we have not found any paper surveying them. In this line,
this work presents a systematic overview of the current status of the
Educational Text Mining field. Our final goal is to answer three main research
questions: Which are the text mining techniques most used in educational
environments? Which are the most used educational resources? And which are the
main applications or educational goals? Finally, we outline the conclusions and
the more interesting future trends.
\\ ( https://arxiv.org/abs/2403.00769 ,  1058kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00771 (*cross-listing*)
Date: Sun, 11 Feb 2024 21:57:49 GMT   (417kb)

Title: XProspeCT: CT Volume Generation from Paired X-Rays
Authors: Benjamin Paulson, Joshua Goldshteyn, Sydney Balboni, John Cisler,
  Andrew Crisler, Natalia Bukowski, Julia Kalish, Theodore Colwell
Categories: eess.IV cs.CV cs.LG physics.med-ph
Comments: Originally submitted as part of the MICS 2023 Undergraduate Paper
  Competition
\\
  Computed tomography (CT) is a beneficial imaging tool for diagnostic
purposes. CT scans provide detailed information concerning the internal
anatomic structures of a patient, but present higher radiation dose and costs
compared to X-ray imaging. In this paper, we build on previous research to
convert orthogonal X-ray images into simulated CT volumes by exploring larger
datasets and various model structures. Significant model variations include
UNet architectures, custom connections, activation functions, loss functions,
optimizers, and a novel back projection approach.
\\ ( https://arxiv.org/abs/2403.00771 ,  417kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00775 (*cross-listing*)
Date: Wed, 14 Feb 2024 14:17:56 GMT   (814kb)

Title: Detecting Anomalous Events in Object-centric Business Processes via
  Graph Neural Networks
Authors: Alessandro Niro and Michael Werner
Categories: q-fin.ST cs.DB cs.LG
Comments: 12 pages, 2 figures, to appear in the ICPM 2023 Workshops Proceedings
\\
  Detecting anomalies is important for identifying inefficiencies, errors, or
fraud in business processes. Traditional process mining approaches focus on
analyzing 'flattened', sequential, event logs based on a single case notion.
However, many real-world process executions exhibit a graph-like structure,
where events can be associated with multiple cases. Flattening event logs
requires selecting a single case identifier which creates a gap with the real
event data and artificially introduces anomalies in the event logs.
Object-centric process mining avoids these limitations by allowing events to be
related to different cases. This study proposes a novel framework for anomaly
detection in business processes that exploits graph neural networks and the
enhanced information offered by object-centric process mining. We first
reconstruct and represent the process dependencies of the object-centric event
logs as attributed graphs and then employ a graph convolutional autoencoder
architecture to detect anomalous events. Our results show that our approach
provides promising performance in detecting anomalies at the activity type and
attributes level, although it struggles to detect anomalies in the temporal
order of events.
\\ ( https://arxiv.org/abs/2403.00775 ,  814kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00777 (*cross-listing*)
Date: Wed, 14 Feb 2024 17:31:29 GMT   (767kb)

Title: Combating Financial Crimes with Unsupervised Learning Techniques:
  Clustering and Dimensionality Reduction for Anti-Money Laundering
Authors: Ahmed N. Bakry, Almohammady S. Alsharkawy, Mohamed S. Farag, and Kamal
  R. Raslan
Categories: q-fin.ST cs.LG
DOI: 10.58675/2636-3305.1664
\\
  Anti-Money Laundering (AML) is a crucial task in ensuring the integrity of
financial systems. One keychallenge in AML is identifying high-risk groups
based on their behavior. Unsupervised learning, particularly clustering, is a
promising solution for this task. However, the use of hundreds of features
todescribe behavior results in a highdimensional dataset that negatively
impacts clustering performance.In this paper, we investigate the effectiveness
of combining clustering method agglomerative hierarchicalclustering with four
dimensionality reduction techniques -Independent Component Analysis (ICA),
andKernel Principal Component Analysis (KPCA), Singular Value Decomposition
(SVD), Locality Preserving Projections (LPP)- to overcome the issue of
high-dimensionality in AML data and improve clusteringresults. This study aims
to provide insights into the most effective way of reducing the dimensionality
ofAML data and enhance the accuracy of clustering-based AML systems. The
experimental results demonstrate that KPCA outperforms other dimension
reduction techniques when combined with agglomerativehierarchical clustering.
This superiority is observed in the majority of situations, as confirmed by
threedistinct validation indices.
\\ ( https://arxiv.org/abs/2403.00777 ,  767kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00785 (*cross-listing*)
Date: Mon, 19 Feb 2024 02:43:55 GMT   (2116kb)

Title: Applying News and Media Sentiment Analysis for Generating Forex Trading
  Signals
Authors: Oluwafemi F Olaiyapo
Categories: q-fin.ST cs.LG
Journal-ref: Olaiyapo O.F. Applying News and Media Sentiment Analysis for
  Generating Forex Trading Signals. Review of Business and Economics Studies.
  2023;11(4):84-94
DOI: 10.26794/2308-944X-2023-11-4-84-94
\\
  The objective of this research is to examine how sentiment analysis can be
employed to generate trading signals for the Foreign Exchange (Forex) market.
The author assessed sentiment in social media posts and news articles
pertaining to the United States Dollar (USD) using a combination of methods:
lexicon-based analysis and the Naive Bayes machine learning algorithm. The
findings indicate that sentiment analysis proves valuable in forecasting market
movements and devising trading signals. Notably, its effectiveness is
consistent across different market conditions. The author concludes that by
analyzing sentiment expressed in news and social media, traders can glean
insights into prevailing market sentiments towards the USD and other pertinent
countries, thereby aiding trading decision-making. This study underscores the
importance of weaving sentiment analysis into trading strategies as a pivotal
tool for predicting market dynamics.
\\ ( https://arxiv.org/abs/2403.00785 ,  2116kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00793 (*cross-listing*)
Date: Thu, 22 Feb 2024 22:47:08 GMT   (3497kb,D)

Title: Ad Recommendation in a Collapsed and Entangled World
Authors: Junwei Pan, Wei Xue, Ximei Wang, Haibin Yu, Xun Liu, Shijie Quan,
  Xueming Qiu, Dapeng Liu, Lei Xiao, Jie Jiang
Categories: cs.IR cs.LG
\\
  In this paper, we present an industry ad recommendation system, paying
attention to the challenges and practices of learning appropriate
representations. Our study begins by showcasing our approaches to preserving
priors when encoding features of diverse types into embedding representations.
Specifically, we address sequence features, numeric features, pre-trained
embedding features, as well as sparse ID features. Moreover, we delve into two
pivotal challenges associated with feature representation: the dimensional
collapse of embeddings and the interest entanglement across various tasks or
scenarios. Subsequently, we propose several practical approaches to effectively
tackle these two challenges. We then explore several training techniques to
facilitate model optimization, reduce bias, and enhance exploration.
Furthermore, we introduce three analysis tools that enable us to
comprehensively study feature correlation, dimensional collapse, and interest
entanglement. This work builds upon the continuous efforts of Tencent's ads
recommendation team in the last decade. It not only summarizes general design
principles but also presents a series of off-the-shelf solutions and analysis
tools. The reported performance is based on our online advertising platform,
which handles hundreds of billions of requests daily, serving millions of ads
to billions of users.
\\ ( https://arxiv.org/abs/2403.00793 ,  3497kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00798 (*cross-listing*)
Date: Fri, 23 Feb 2024 15:00:46 GMT   (1189kb,D)

Title: Helen: Optimizing CTR Prediction Models with Frequency-wise Hessian
  Eigenvalue Regularization
Authors: Zirui Zhu, Yong Liu, Zangwei Zheng, Huifeng Guo, Yang You
Categories: cs.IR cs.LG
Comments: Proceedings of the ACM Web Conference 2024 (WWW '24)
DOI: 10.1145/3589334.3645463
\\
  Click-Through Rate (CTR) prediction holds paramount significance in online
advertising and recommendation scenarios. Despite the proliferation of recent
CTR prediction models, the improvements in performance have remained limited,
as evidenced by open-source benchmark assessments. Current researchers tend to
focus on developing new models for various datasets and settings, often
neglecting a crucial question: What is the key challenge that truly makes CTR
prediction so demanding?
  In this paper, we approach the problem of CTR prediction from an optimization
perspective. We explore the typical data characteristics and optimization
statistics of CTR prediction, revealing a strong positive correlation between
the top hessian eigenvalue and feature frequency. This correlation implies that
frequently occurring features tend to converge towards sharp local minima,
ultimately leading to suboptimal performance. Motivated by the recent
advancements in sharpness-aware minimization (SAM), which considers the
geometric aspects of the loss landscape during optimization, we present a
dedicated optimizer crafted for CTR prediction, named Helen. Helen incorporates
frequency-wise Hessian eigenvalue regularization, achieved through adaptive
perturbations based on normalized feature frequencies.
  Empirical results under the open-source benchmark framework underscore
Helen's effectiveness. It successfully constrains the top eigenvalue of the
Hessian matrix and demonstrates a clear advantage over widely used optimization
algorithms when applied to seven popular models across three public benchmark
datasets on BARS. Our code locates at github.com/NUS-HPC-AI-Lab/Helen.
\\ ( https://arxiv.org/abs/2403.00798 ,  1189kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00817 (*cross-listing*)
Date: Mon, 26 Feb 2024 05:08:52 GMT   (348kb,D)

Title: Doubly Calibrated Estimator for Recommendation on Data Missing Not At
  Random
Authors: Wonbin Kweon, Hwanjo Yu
Categories: cs.IR cs.LG
Comments: WWW 2024
DOI: 10.1145/3589334.3645417
\\
  Recommender systems often suffer from selection bias as users tend to rate
their preferred items. The datasets collected under such conditions exhibit
entries missing not at random and thus are not randomized-controlled trials
representing the target population. To address this challenge, a doubly robust
estimator and its enhanced variants have been proposed as they ensure
unbiasedness when accurate imputed errors or predicted propensities are
provided. However, we argue that existing estimators rely on miscalibrated
imputed errors and propensity scores as they depend on rudimentary models for
estimation. We provide theoretical insights into how miscalibrated imputation
and propensity models may limit the effectiveness of doubly robust estimators
and validate our theorems using real-world datasets. On this basis, we propose
a Doubly Calibrated Estimator that involves the calibration of both the
imputation and propensity models. To achieve this, we introduce calibration
experts that consider different logit distributions across users. Moreover, we
devise a tri-level joint learning framework, allowing the simultaneous
optimization of calibration experts alongside prediction and imputation models.
Through extensive experiments on real-world datasets, we demonstrate the
superiority of the Doubly Calibrated Estimator in the context of debiased
recommendation tasks.
\\ ( https://arxiv.org/abs/2403.00817 ,  348kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00844 (*cross-listing*)
Date: Thu, 29 Feb 2024 13:58:33 GMT   (3187kb,D)

Title: Lower-Left Partial AUC: An Effective and Efficient Optimization Metric
  for Recommendation
Authors: Wentao Shi, Chenxu Wang, Fuli Feng, Yang Zhang, Wenjie Wang, Junkang
  Wu, Xiangnan He
Categories: cs.IR cs.LG
Comments: WWW 2024; 15 pages
DOI: 10.1145/3589334.3645371
\\
  Optimization metrics are crucial for building recommendation systems at
scale. However, an effective and efficient metric for practical use remains
elusive. While Top-K ranking metrics are the gold standard for optimization,
they suffer from significant computational overhead. Alternatively, the more
efficient accuracy and AUC metrics often fall short of capturing the true
targets of recommendation tasks, leading to suboptimal performance. To overcome
this dilemma, we propose a new optimization metric, Lower-Left Partial AUC
(LLPAUC), which is computationally efficient like AUC but strongly correlates
with Top-K ranking metrics. Compared to AUC, LLPAUC considers only the partial
area under the ROC curve in the Lower-Left corner to push the optimization
focus on Top-K. We provide theoretical validation of the correlation between
LLPAUC and Top-K ranking metrics and demonstrate its robustness to noisy user
feedback. We further design an efficient point-wise recommendation loss to
maximize LLPAUC and evaluate it on three datasets, validating its effectiveness
and robustness.
\\ ( https://arxiv.org/abs/2403.00844 ,  3187kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00845 (*cross-listing*)
Date: Thu, 29 Feb 2024 14:10:26 GMT   (40kb)

Title: Improved Online Learning Algorithms for CTR Prediction in Ad Auctions
Authors: Zhe Feng, Christopher Liaw, Zixin Zhou
Categories: cs.IR cs.GT cs.LG
Comments: Appeared in ICML 2023
\\
  In this work, we investigate the online learning problem of revenue
maximization in ad auctions, where the seller needs to learn the click-through
rates (CTRs) of each ad candidate and charge the price of the winner through a
pay-per-click manner. We focus on two models of the advertisers' strategic
behaviors. First, we assume that the advertiser is completely myopic; i.e.~in
each round, they aim to maximize their utility only for the current round. In
this setting, we develop an online mechanism based on upper-confidence bounds
that achieves a tight $O(\sqrt{T})$ regret in the worst-case and negative
regret when the values are static across all the auctions and there is a gap
between the highest expected value (i.e.~value multiplied by their CTR) and
second highest expected value ad. Next, we assume that the advertiser is
non-myopic and cares about their long term utility. This setting is much more
complex since an advertiser is incentivized to influence the mechanism by
bidding strategically in earlier rounds. In this setting, we provide an
algorithm to achieve negative regret for the static valuation setting (with a
positive gap), which is in sharp contrast with the prior work that shows
$O(T^{2/3})$ regret when the valuation is generated by adversary.
\\ ( https://arxiv.org/abs/2403.00845 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00849 (*cross-listing*)
Date: Thu, 29 Feb 2024 16:10:21 GMT   (1467kb,D)

Title: NeuraLUT: Hiding Neural Network Density in Boolean Synthesizable
  Functions
Authors: Marta Andronic and George A. Constantinides
Categories: cs.AR cs.LG stat.ML
\\
  Field-Programmable Gate Array (FPGA) accelerators have proven successful in
handling latency- and resource-critical deep neural network (DNN) inference
tasks. Among the most computationally intensive operations in a neural network
(NN) is the dot product between the feature and weight vectors. Thus, some
previous FPGA acceleration works have proposed mapping neurons with quantized
inputs and outputs directly to lookup tables (LUTs) for hardware
implementation. In these works, the boundaries of the neurons coincide with the
boundaries of the LUTs. We propose relaxing these boundaries and mapping entire
sub-networks to a single LUT. As the sub-networks are absorbed within the LUT,
the NN topology and precision within a partition do not affect the size of the
lookup tables generated. Therefore, we utilize fully connected layers with
floating-point precision inside each partition, which benefit from being
universal function approximators, with rigid sparsity and quantization enforced
only between partitions, where the NN topology becomes exposed to the circuit
topology. Although cheap to implement, this approach can lead to very deep NNs,
and so to tackle challenges like vanishing gradients, we also introduce skip
connections inside the partitions. The resulting methodology can be seen as
training DNNs with a specific sparsity pattern that allows them to be mapped to
much shallower circuit-level networks, thereby significantly improving latency.
We validate our proposed method on a known latency-critical task, jet
substructure tagging, and on the classical computer vision task, the digit
classification using MNIST. Our approach allows for greater function
expressivity within the LUTs compared to existing work, leading to lower
latency NNs for the same accuracy.
\\ ( https://arxiv.org/abs/2403.00849 ,  1467kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00873 (*cross-listing*)
Date: Fri, 1 Mar 2024 07:41:05 GMT   (9263kb)

Title: Blockchain-empowered Federated Learning: Benefits, Challenges, and
  Solutions
Authors: Zeju Cai, Jianguo Chen, Yuting Fan, Zibin Zheng and Keqin Li
Categories: cs.CR cs.LG
\\
  Federated learning (FL) is a distributed machine learning approach that
protects user data privacy by training models locally on clients and
aggregating them on a parameter server. While effective at preserving privacy,
FL systems face limitations such as single points of failure, lack of
incentives, and inadequate security. To address these challenges, blockchain
technology is integrated into FL systems to provide stronger security,
fairness, and scalability. However, blockchain-empowered FL (BC-FL) systems
introduce additional demands on network, computing, and storage resources. This
survey provides a comprehensive review of recent research on BC-FL systems,
analyzing the benefits and challenges associated with blockchain integration.
We explore why blockchain is applicable to FL, how it can be implemented, and
the challenges and existing solutions for its integration. Additionally, we
offer insights on future research directions for the BC-FL system.
\\ ( https://arxiv.org/abs/2403.00873 ,  9263kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00889 (*cross-listing*)
Date: Fri, 1 Mar 2024 11:55:37 GMT   (7509kb,D)

Title: Time-bound Contextual Bio-ID Generation for Minimalist Wearables
Authors: Adiba Orzikulova, Diana A. Vasile, Fahim Kawsar, Chulhong Min
Categories: cs.CR cs.LG eess.SP
\\
  As wearable devices become increasingly miniaturized and powerful, a new
opportunity arises for instant and dynamic device-to-device collaboration and
human-to-device interaction. However, this progress presents a unique
challenge: these minimalist wearables lack inherent mechanisms for real-time
authentication, posing significant risks to data privacy and overall security.
To address this, we introduce Proteus that realizes an innovative concept of
time-bound contextual bio-IDs, which are generated from on-device sensor data
and embedded into a common latent space. These bio-IDs act as a time-bound
unique user identifier that can be used to identify the wearer in a certain
context. Proteus enables dynamic and contextual device collaboration as well as
robust human-to-device interaction. Our evaluations demonstrate the
effectiveness of our method, particularly in the context of minimalist
wearables.
\\ ( https://arxiv.org/abs/2403.00889 ,  7509kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00892 (*cross-listing*)
Date: Fri, 1 Mar 2024 13:47:39 GMT   (1280kb,D)

Title: PowerFlowMultiNet: Multigraph Neural Networks for Unbalanced Three-Phase
  Distribution Systems
Authors: Salah Ghamizi, Jun Cao, Aoxiang Ma, Pedro Rodriguez
Categories: eess.SY cs.LG cs.SY
\\
  Efficiently solving unbalanced three-phase power flow in distribution grids
is pivotal for grid analysis and simulation. There is a pressing need for
scalable algorithms capable of handling large-scale unbalanced power grids that
can provide accurate and fast solutions. To address this, deep learning
techniques, especially Graph Neural Networks (GNNs), have emerged. However,
existing literature primarily focuses on balanced networks, leaving a critical
gap in supporting unbalanced three-phase power grids. This letter introduces
PowerFlowMultiNet, a novel multigraph GNN framework explicitly designed for
unbalanced three-phase power grids. The proposed approach models each phase
separately in a multigraph representation, effectively capturing the inherent
asymmetry in unbalanced grids. A graph embedding mechanism utilizing message
passing is introduced to capture spatial dependencies within the power system
network. PowerFlowMultiNet outperforms traditional methods and other deep
learning approaches in terms of accuracy and computational speed. Rigorous
testing reveals significantly lower error rates and a notable hundredfold
increase in computational speed for large power networks compared to
model-based methods.
\\ ( https://arxiv.org/abs/2403.00892 ,  1280kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00935 (*cross-listing*)
Date: Fri, 1 Mar 2024 19:27:53 GMT   (218kb,D)

Title: Transfer Learning for Security: Challenges and Future Directions
Authors: Adrian Shuai Li, Arun Iyengar, Ashish Kundu, Elisa Bertino
Categories: cs.CR cs.LG
\\
  Many machine learning and data mining algorithms rely on the assumption that
the training and testing data share the same feature space and distribution.
However, this assumption may not always hold. For instance, there are
situations where we need to classify data in one domain, but we only have
sufficient training data available from a different domain. The latter data may
follow a distinct distribution. In such cases, successfully transferring
knowledge across domains can significantly improve learning performance and
reduce the need for extensive data labeling efforts. Transfer learning (TL) has
thus emerged as a promising framework to tackle this challenge, particularly in
security-related tasks. This paper aims to review the current advancements in
utilizing TL techniques for security. The paper includes a discussion of the
existing research gaps in applying TL in the security domain, as well as
exploring potential future research directions and issues that arise in the
context of TL-assisted security solutions.
\\ ( https://arxiv.org/abs/2403.00935 ,  218kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00961 (*cross-listing*)
Date: Fri, 1 Mar 2024 20:21:42 GMT   (4431kb,D)

Title: Data Science Education in Undergraduate Physics: Lessons Learned from a
  Community of Practice
Authors: Karan Shah, Julie Butler, Alexis Knaub, An{\i}l Zengino\u{g}lu,
  William Ratcliff, Mohammad Soltanieh-ha
Categories: physics.ed-ph cs.LG physics.data-an
Comments: 21 pages, 4 figures, 2 tables. The associated GItHub repository can
  be found at https://github.com/GDS-Education-Community-of-Practice/DSECOP
\\
  With the increasing availability of diverse datasets, ranging from
small-scale experimental data points to large and complex data repositories and
powerful data analysis tools, it is increasingly important that physics
educators equip their students with the skills to work with data effectively.
However, many educators may lack the necessary training and expertise in data
science to teach these skills. To address this gap, we created the Data Science
Education Community of Practice (DSECOP), bringing together graduate students
and physics educators from different institutions and backgrounds to share best
practices and lessons learned in integrating data science into undergraduate
physics education. In this article, we present insights and experiences from
this community of practice, highlighting key strategies and challenges in
incorporating data science into the introductory physics curriculum. Our goal
is to provide guidance and inspiration to educators who seek to integrate data
science into their teaching, helping to prepare the next generation of
physicists for a data-driven world.
\\ ( https://arxiv.org/abs/2403.00961 ,  4431kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00991 (*cross-listing*)
Date: Fri, 1 Mar 2024 21:27:03 GMT   (7300kb,D)

Title: SELFI: Autonomous Self-Improvement with Reinforcement Learning for
  Social Navigation
Authors: Noriaki Hirose, Dhruv Shah, Kyle Stachowicz, Ajay Sridhar and Sergey
  Levine
Categories: cs.RO cs.CV cs.LG
Comments: 11pages, 13 figures, 2 tables
\\
  Autonomous self-improving robots that interact and improve with experience
are key to the real-world deployment of robotic systems. In this paper, we
propose an online learning method, SELFI, that leverages online robot
experience to rapidly fine-tune pre-trained control policies efficiently. SELFI
applies online model-free reinforcement learning on top of offline model-based
learning to bring out the best parts of both learning paradigms. Specifically,
SELFI stabilizes the online learning process by incorporating the same
model-based learning objective from offline pre-training into the Q-values
learned with online model-free reinforcement learning. We evaluate SELFI in
multiple real-world environments and report improvements in terms of collision
avoidance, as well as more socially compliant behavior, measured by a human
user study. SELFI enables us to quickly learn useful robotic behaviors with
less human interventions such as pre-emptive behavior for the pedestrians,
collision avoidance for small and transparent objects, and avoiding travel on
uneven floor surfaces. We provide supplementary videos to demonstrate the
performance of our fine-tuned policy on our project page.
\\ ( https://arxiv.org/abs/2403.00991 ,  7300kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01022 (*cross-listing*)
Date: Fri, 1 Mar 2024 22:52:30 GMT   (2600kb,D)

Title: Autonomous Strike UAVs for Counterterrorism Missions: Challenges and
  Preliminary Solutions
Authors: Meshari Aljohani, Ravi Mukkamalai and Stephen Olariu
Categories: cs.RO cs.LG
Comments: 12 pages, 12 figures
\\
  Unmanned Aircraft Vehicles (UAVs) are becoming a crucial tool in modern
warfare, primarily due to their cost-effectiveness, risk reduction, and ability
to perform a wider range of activities. The use of autonomous UAVs to conduct
strike missions against highly valuable targets is the focus of this research.
Due to developments in ledger technology, smart contracts, and machine
learning, such activities formerly carried out by professionals or remotely
flown UAVs are now feasible. Our study provides the first in-depth analysis of
challenges and preliminary solutions for successful implementation of an
autonomous UAV mission. Specifically, we identify challenges that have to be
overcome and propose possible technical solutions for the challenges
identified. We also derive analytical expressions for the success probability
of an autonomous UAV mission, and describe a machine learning model to train
the UAV.
\\ ( https://arxiv.org/abs/2403.01022 ,  2600kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01023 (*cross-listing*)
Date: Fri, 1 Mar 2024 22:53:57 GMT   (150kb,D)

Title: Federated Learning via Lattice Joint Source-Channel Coding
Authors: Seyed Mohammad Azimi-Abarghouyi, Lav R. Varshney
Categories: cs.IT cs.LG math.IT
\\
  This paper introduces a universal federated learning framework that enables
over-the-air computation via digital communications, using a new joint
source-channel coding scheme. Without relying on channel state information at
devices, this scheme employs lattice codes to both quantize model parameters
and exploit interference from the devices. A novel two-layer receiver structure
at the server is designed to reliably decode an integer combination of the
quantized model parameters as a lattice point for the purpose of aggregation.
Numerical experiments validate the effectiveness of the proposed scheme. Even
with the challenges posed by channel conditions and device heterogeneity, the
proposed scheme markedly surpasses other over-the-air FL strategies.
\\ ( https://arxiv.org/abs/2403.01023 ,  150kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01058 (*cross-listing*)
Date: Sat, 2 Mar 2024 01:20:59 GMT   (7419kb,D)

Title: Neural Field Classifiers via Target Encoding and Classification Loss
Authors: Xindi Yang, Zeke Xie, Xiong Zhou, Boyu Liu, Buhua Liu, Yi Liu, Haoran
  Wang, Yunfeng Cai, Mingming Sun
Categories: cs.CV cs.LG
Comments: ICLR 2024 Main Conference; 17 pages; 11 figures; 13 tables
\\
  Neural field methods have seen great progress in various long-standing tasks
in computer vision and computer graphics, including novel view synthesis and
geometry reconstruction. As existing neural field methods try to predict some
coordinate-based continuous target values, such as RGB for Neural Radiance
Field (NeRF), all of these methods are regression models and are optimized by
some regression loss. However, are regression models really better than
classification models for neural field methods? In this work, we try to visit
this very fundamental but overlooked question for neural fields from a machine
learning perspective. We successfully propose a novel Neural Field Classifier
(NFC) framework which formulates existing neural field methods as
classification tasks rather than regression tasks. The proposed NFC can easily
transform arbitrary Neural Field Regressor (NFR) into its classification
variant via employing a novel Target Encoding module and optimizing a
classification loss. By encoding a continuous regression target into a
high-dimensional discrete encoding, we naturally formulate a multi-label
classification task. Extensive experiments demonstrate the impressive
effectiveness of NFC at the nearly free extra computational costs. Moreover,
NFC also shows robustness to sparse inputs, corrupted images, and dynamic
scenes.
\\ ( https://arxiv.org/abs/2403.01058 ,  7419kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01076 (*cross-listing*)
Date: Sat, 2 Mar 2024 03:03:29 GMT   (889kb,D)

Title: Extracting Usable Predictions from Quantized Networks through
  Uncertainty Quantification for OOD Detection
Authors: Rishi Singhal and Srinath Srinivasan
Categories: cs.CV cs.LG
\\
  OOD detection has become more pertinent with advances in network design and
increased task complexity. Identifying which parts of the data a given network
is misclassifying has become as valuable as the network's overall performance.
We can compress the model with quantization, but it suffers minor performance
loss. The loss of performance further necessitates the need to derive the
confidence estimate of the network's predictions. In line with this thinking,
we introduce an Uncertainty Quantification(UQ) technique to quantify the
uncertainty in the predictions from a pre-trained vision model. We subsequently
leverage this information to extract valuable predictions while ignoring the
non-confident predictions. We observe that our technique saves up to 80% of
ignored samples from being misclassified. The code for the same is available
here.
\\ ( https://arxiv.org/abs/2403.01076 ,  889kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01158 (*cross-listing*)
Date: Sat, 2 Mar 2024 10:18:37 GMT   (583kb)

Title: A Bayesian Committee Machine Potential for Oxygen-containing Organic
  Compounds
Authors: Seungwon Kim, D. ChangMo Yang, Soohaeng Yoo Willow, and Chang Woo
  Myung
Categories: cond-mat.mtrl-sci cs.LG
\\
  Understanding the pivotal role of oxygen-containing organic compounds in
serving as an energy source for living organisms and contributing to protein
formation is crucial in the field of biochemistry. This study addresses the
challenge of comprehending protein-protein interactions (PPI) and developing
predicitive models for proteins and organic compounds, with a specific focus on
quantifying their binding affinity. Here, we introduce the active Bayesian
Committee Machine (BCM) potential, specifically designed to predict
oxygen-containing organic compounds within eight groups of CHO. The BCM
potential adopts a committee-based approach to tackle scalability issues
associated with kernel regressors, particularly when dealing with large
datasets. Its adaptable structure allows for efficient and cost-effective
expansion, maintaing both transferability and scalability. Through systematic
benchmarking, we position the sparse BCM potential as a promising contender in
the pursuit of a universal machine learning potential.
\\ ( https://arxiv.org/abs/2403.01158 ,  583kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01192 (*cross-listing*)
Date: Sat, 2 Mar 2024 12:12:04 GMT   (315kb,D)

Title: A Composite Decomposition Method for Large-Scale Global Optimization
Authors: Maojiang Tian, Minyang Chen, Wei Du, Yang Tang, Yaochu Jin, Gary G.
  Yen
Categories: math.OC cs.LG cs.NE
\\
  Cooperative co-evolution (CC) algorithms, based on the divide-and-conquer
strategy, have emerged as the predominant approach to solving large-scale
global optimization (LSGO) problems. The efficiency and accuracy of the
grouping stage significantly impact the performance of the optimization
process. While the general separability grouping (GSG) method has overcome the
limitation of previous differential grouping (DG) methods by enabling the
decomposition of non-additively separable functions, it suffers from high
computational complexity. To address this challenge, this article proposes a
composite separability grouping (CSG) method, seamlessly integrating DG and GSG
into a problem decomposition framework to utilize the strengths of both
approaches. CSG introduces a step-by-step decomposition framework that
accurately decomposes various problem types using fewer computational
resources. By sequentially identifying additively, multiplicatively and
generally separable variables, CSG progressively groups non-separable variables
by recursively considering the interactions between each non-separable variable
and the formed non-separable groups. Furthermore, to enhance the efficiency and
accuracy of CSG, we introduce two innovative methods: a multiplicatively
separable variable detection method and a non-separable variable grouping
method. These two methods are designed to effectively detect multiplicatively
separable variables and efficiently group non-separable variables,
respectively. Extensive experimental results demonstrate that CSG achieves more
accurate variable grouping with lower computational complexity compared to GSG
and state-of-the-art DG series designs.
\\ ( https://arxiv.org/abs/2403.01192 ,  315kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01299 (*cross-listing*)
Date: Sat, 2 Mar 2024 19:44:19 GMT   (3373kb,D)

Title: A Photonic Physically Unclonable Function's Resilience to
  Multiple-Valued Machine Learning Attacks
Authors: Jessie M. Henderson, Elena R. Henderson, Clayton A. Harper, Hiva
  Shahoei, William V. Oxford, Eric C. Larson, Duncan L. MacFarlane, and
  Mitchell A. Thornton
Categories: cs.CR cs.LG
Comments: 6 pages, 4 figures
\\
  Physically unclonable functions (PUFs) identify integrated circuits using
nonlinearly-related challenge-response pairs (CRPs). Ideally, the relationship
between challenges and corresponding responses is unpredictable, even if a
subset of CRPs is known. Previous work developed a photonic PUF offering
improved security compared to non-optical counterparts. Here, we investigate
this PUF's susceptibility to Multiple-Valued-Logic-based machine learning
attacks. We find that approximately 1,000 CRPs are necessary to train models
that predict response bits better than random chance. Given the significant
challenge of acquiring a vast number of CRPs from a photonic PUF, our results
demonstrate photonic PUF resilience against such attacks.
\\ ( https://arxiv.org/abs/2403.01299 ,  3373kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01301 (*cross-listing*)
Date: Sat, 2 Mar 2024 19:55:38 GMT   (151kb,D)

Title: Supplier Recommendation in Online Procurement
Authors: Victor Coscrato and Derek Bridge
Categories: cs.IR cs.LG
\\
  Supply chain optimization is key to a healthy and profitable business. Many
companies use online procurement systems to agree contracts with suppliers. It
is vital that the most competitive suppliers are invited to bid for such
contracts. In this work, we propose a recommender system to assist with
supplier discovery in road freight online procurement. Our system is able to
provide personalized supplier recommendations, taking into account customer
needs and preferences. This is a novel application of recommender systems,
calling for design choices that fit the unique requirements of online
procurement. Our preliminary results, using real-world data, are promising.
\\ ( https://arxiv.org/abs/2403.01301 ,  151kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01318 (*cross-listing*)
Date: Sat, 2 Mar 2024 21:37:40 GMT   (70kb,D)

Title: High-Dimensional Tail Index Regression: with An Application to Text
  Analyses of Viral Posts in Social Media
Authors: Yuya Sasaki, Jing Tao, Yulong Wang
Categories: stat.ML cs.LG econ.EM
\\
  Motivated by the empirical power law of the distributions of credits (e.g.,
the number of "likes") of viral posts in social media, we introduce the
high-dimensional tail index regression and methods of estimation and inference
for its parameters. We propose a regularized estimator, establish its
consistency, and derive its convergence rate. To conduct inference, we propose
to debias the regularized estimate, and establish the asymptotic normality of
the debiased estimator. Simulation studies support our theory. These methods
are applied to text analyses of viral posts in X (formerly Twitter) concerning
LGBTQ+.
\\ ( https://arxiv.org/abs/2403.01318 ,  70kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01355 (*cross-listing*)
Date: Sun, 3 Mar 2024 00:58:27 GMT   (42kb)

Title: a-DCF: an architecture agnostic metric with application to
  spoofing-robust speaker verification
Authors: Hye-jin Shim, Jee-weon Jung, Tomi Kinnunen, Nicholas Evans,
  Jean-Francois Bonastre, Itshak Lapidot
Categories: eess.AS cs.LG
Comments: 8 pages, submitted to Speaker Odyssey 2024
\\
  Spoofing detection is today a mainstream research topic. Standard metrics can
be applied to evaluate the performance of isolated spoofing detection solutions
and others have been proposed to support their evaluation when they are
combined with speaker detection. These either have well-known deficiencies or
restrict the architectural approach to combine speaker and spoof detectors. In
this paper, we propose an architecture-agnostic detection cost function
(a-DCF). A generalisation of the original DCF used widely for the assessment of
automatic speaker verification (ASV), the a-DCF is designed for the evaluation
of spoofing-robust ASV. Like the DCF, the a-DCF reflects the cost of decisions
in a Bayes risk sense, with explicitly defined class priors and detection cost
model. We demonstrate the merit of the a-DCF through the benchmarking
evaluation of architecturally-heterogeneous spoofing-robust ASV solutions.
\\ ( https://arxiv.org/abs/2403.01355 ,  42kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01371 (*cross-listing*)
Date: Sun, 3 Mar 2024 02:19:49 GMT   (1350kb,D)

Title: Large-scale variational Gaussian state-space models
Authors: Matthew Dowling, Yuan Zhao, Il Memming Park
Categories: stat.ML cs.LG
\\
  We introduce an amortized variational inference algorithm and structured
variational approximation for state-space models with nonlinear dynamics driven
by Gaussian noise. Importantly, the proposed framework allows for efficient
evaluation of the ELBO and low-variance stochastic gradient estimates without
resorting to diagonal Gaussian approximations by exploiting (i) the low-rank
structure of Monte-Carlo approximations to marginalize the latent state through
the dynamics (ii) an inference network that approximates the update step with
low-rank precision matrix updates (iii) encoding current and future
observations into pseudo observations -- transforming the approximate smoothing
problem into an (easier) approximate filtering problem. Overall, the necessary
statistics and ELBO can be computed in $O(TL(Sr + S^2 + r^2))$ time where $T$
is the series length, $L$ is the state-space dimensionality, $S$ are the number
of samples used to approximate the predict step statistics, and $r$ is the rank
of the approximate precision matrix update in the update step (which can be
made of much lower dimension than $L$).
\\ ( https://arxiv.org/abs/2403.01371 ,  1350kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01423 (*cross-listing*)
Date: Sun, 3 Mar 2024 07:45:35 GMT   (1082kb,D)

Title: Collective Certified Robustness against Graph Injection Attacks
Authors: Yuni Lai, Bailin Pan, Kaihuang Chen, Yancheng Yuan, Kai Zhou
Categories: cs.CR cs.LG
\\
  We investigate certified robustness for GNNs under graph injection attacks.
Existing research only provides sample-wise certificates by verifying each node
independently, leading to very limited certifying performance. In this paper,
we present the first collective certificate, which certifies a set of target
nodes simultaneously. To achieve it, we formulate the problem as a binary
integer quadratic constrained linear programming (BQCLP). We further develop a
customized linearization technique that allows us to relax the BQCLP into
linear programming (LP) that can be efficiently solved. Through comprehensive
experiments, we demonstrate that our collective certification scheme
significantly improves certification performance with minimal computational
overhead. For instance, by solving the LP within 1 minute on the Citeseer
dataset, we achieve a significant increase in the certified ratio from 0.0% to
81.2% when the injected node number is 5% of the graph size. Our step marks a
crucial step towards making provable defense more practical.
\\ ( https://arxiv.org/abs/2403.01423 ,  1082kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01451 (*cross-listing*)
Date: Sun, 3 Mar 2024 09:08:41 GMT   (771kb,D)

Title: Enhancing Data Provenance and Model Transparency in Federated Learning
  Systems - A Database Approach
Authors: Michael Gu, Ramasoumya Naraparaju, Dongfang Zhao
Categories: cs.CR cs.DB cs.LG
Comments: 14 pages, 16 figures
\\
  Federated Learning (FL) presents a promising paradigm for training machine
learning models across decentralized edge devices while preserving data
privacy. Ensuring the integrity and traceability of data across these
distributed environments, however, remains a critical challenge. The ability to
create transparent artificial intelligence, such as detailing the training
process of a machine learning model, has become an increasingly prominent
concern due to the large number of sensitive (hyper)parameters it utilizes;
thus, it is imperative to strike a reasonable balance between openness and the
need to protect sensitive information.
  In this paper, we propose one of the first approaches to enhance data
provenance and model transparency in federated learning systems. Our
methodology leverages a combination of cryptographic techniques and efficient
model management to track the transformation of data throughout the FL process,
and seeks to increase the reproducibility and trustworthiness of a trained FL
model. We demonstrate the effectiveness of our approach through experimental
evaluations on diverse FL scenarios, showcasing its ability to tackle
accountability and explainability across the board.
  Our findings show that our system can greatly enhance data transparency in
various FL environments by storing chained cryptographic hashes and client
model snapshots in our proposed design for data decoupled FL. This is made
possible by also employing multiple optimization techniques which enables
comprehensive data provenance without imposing substantial computational loads.
Extensive experimental results suggest that integrating a database subsystem
into federated learning systems can improve data provenance in an efficient
manner, encouraging secure FL adoption in privacy-sensitive applications and
paving the way for future advancements in FL transparency and security
features.
\\ ( https://arxiv.org/abs/2403.01451 ,  771kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01485 (*cross-listing*)
Date: Sun, 3 Mar 2024 11:36:35 GMT   (2782kb,D)

Title: Approximations to the Fisher Information Metric of Deep Generative
  Models for Out-Of-Distribution Detection
Authors: Sam Dauncey, Chris Holmes, Christopher Williams and Fabian Falck
Categories: stat.ML cs.CV cs.LG
\\
  Likelihood-based deep generative models such as score-based diffusion models
and variational autoencoders are state-of-the-art machine learning models
approximating high-dimensional distributions of data such as images, text, or
audio. One of many downstream tasks they can be naturally applied to is
out-of-distribution (OOD) detection. However, seminal work by Nalisnick et al.
which we reproduce showed that deep generative models consistently infer higher
log-likelihoods for OOD data than data they were trained on, marking an open
problem. In this work, we analyse using the gradient of a data point with
respect to the parameters of the deep generative model for OOD detection, based
on the simple intuition that OOD data should have larger gradient norms than
training data. We formalise measuring the size of the gradient as approximating
the Fisher information metric. We show that the Fisher information matrix (FIM)
has large absolute diagonal values, motivating the use of chi-square
distributed, layer-wise gradient norms as features. We combine these features
to make a simple, model-agnostic and hyperparameter-free method for OOD
detection which estimates the joint density of the layer-wise gradient norms
for a given data point. We find that these layer-wise gradient norms are weakly
correlated, rendering their combined usage informative, and prove that the
layer-wise gradient norms satisfy the principle of (data representation)
invariance. Our empirical results indicate that this method outperforms the
Typicality test for most deep generative models and image dataset pairings.
\\ ( https://arxiv.org/abs/2403.01485 ,  2782kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01523 (*cross-listing*)
Date: Sun, 3 Mar 2024 14:50:15 GMT   (3201kb)

Title: Data-driven local operator finding for reduced-order modelling of plasma
  systems: I. Concept and verifications
Authors: Farbod Faraji, Maryam Reza, Aaron Knoll, and J. Nathan Kutz
Categories: physics.plasm-ph cs.LG physics.comp-ph
Comments: 27 pages, 18 figures
\\
  Reduced-order plasma models that can efficiently predict plasma behavior
across various settings and configurations are highly sought after yet elusive.
The demand for such models has surged in the past decade due to their potential
to facilitate scientific research and expedite the development of plasma
technologies. In line with the advancements in computational power and
data-driven methods, we introduce the "Phi Method" in this two-part article.
Part I presents this novel algorithm, which employs constrained regression on a
candidate term library informed by numerical discretization schemes to discover
discretized systems of differential equations. We demonstrate Phi Method's
efficacy in deriving reliable and robust reduced-order models (ROMs) for three
test cases: the Lorenz attractor, flow past a cylinder, and a 1D
Hall-thruster-representative plasma. Part II will delve into the method's
application for parametric dynamics discovery. Our results show that ROMs
derived from the Phi Method provide remarkably accurate predictions of systems'
behavior, whether derived from steady-state or transient-state data. This
underscores the method's potential for transforming plasma system modeling.
\\ ( https://arxiv.org/abs/2403.01523 ,  3201kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01532 (*cross-listing*)
Date: Sun, 3 Mar 2024 15:09:49 GMT   (3305kb)

Title: Data-driven local operator finding for reduced-order modelling of plasma
  systems: II. Application to parametric dynamics
Authors: Farbod Faraji, Maryam Reza, Aaron Knoll, J. Nathan Kutz
Categories: physics.plasm-ph cs.LG physics.comp-ph
Comments: 24 pages, 17 figures
\\
  Real-world systems often exhibit dynamics influenced by various parameters,
either inherent or externally controllable, necessitating models capable of
reliably capturing these parametric behaviors. Plasma technologies exemplify
such systems. For example, phenomena governing global dynamics in Hall
thrusters (a spacecraft propulsion technology) vary with various parameters,
such as the "self-sustained electric field". In this Part II, following on the
introduction of our novel data-driven local operator finding algorithm, Phi
Method, in Part I, we showcase the method's effectiveness in learning
parametric dynamics to predict system behavior across unseen parameter spaces.
We present two adaptations: the "parametric Phi Method" and the "ensemble Phi
Method", which are demonstrated through 2D fluid-flow-past-a-cylinder and 1D
Hall-thruster-plasma-discharge problems. Comparative evaluation against
parametric OPT-DMD in the fluid case demonstrates superior predictive
performance of the parametric Phi Method. Across both test cases, parametric
and ensemble Phi Method reliably recover governing parametric PDEs and offer
accurate predictions over test parameters. Ensemble ROM analysis underscores
Phi Method's robust learning of dominant dynamic coefficients with high
confidence.
\\ ( https://arxiv.org/abs/2403.01532 ,  3305kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01536 (*cross-listing*)
Date: Sun, 3 Mar 2024 15:30:31 GMT   (10745kb,D)

Title: Fast Ergodic Search with Kernel Functions
Authors: Muchen Sun, Ayush Gaggar, Peter Trautman, Todd Murphey
Categories: cs.RO cs.LG
\\
  Ergodic search enables optimal exploration of an information distribution
while guaranteeing the asymptotic coverage of the search space. However,
current methods typically have exponential computation complexity in the search
space dimension and are restricted to Euclidean space. We introduce a
computationally efficient ergodic search method. Our contributions are
two-fold. First, we develop a kernel-based ergodic metric and generalize it
from Euclidean space to Lie groups. We formally prove the proposed metric is
consistent with the standard ergodic metric while guaranteeing linear
complexity in the search space dimension. Secondly, we derive the first-order
optimality condition of the kernel ergodic metric for nonlinear systems, which
enables efficient trajectory optimization. Comprehensive numerical benchmarks
show that the proposed method is at least two orders of magnitude faster than
the state-of-the-art algorithm. Finally, we demonstrate the proposed algorithm
with a peg-in-hole insertion task. We formulate the problem as a coverage task
in the space of SE(3) and use a 30-second-long human demonstration as the prior
distribution for ergodic coverage. Ergodicity guarantees the asymptotic
solution of the peg-in-hole problem so long as the solution resides within the
prior information distribution, which is seen in the 100\% success rate.
\\ ( https://arxiv.org/abs/2403.01536 ,  10745kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01537 (*cross-listing*)
Date: Sun, 3 Mar 2024 15:30:59 GMT   (8609kb,D)

Title: Mixed-Strategy Nash Equilibrium for Crowd Navigation
Authors: Muchen Sun, Francesca Baldini, Peter Trautman, Todd Murphey
Categories: cs.RO cs.GT cs.LG
\\
  We address the problem of finding mixed-strategy Nash equilibrium for crowd
navigation. Mixed-strategy Nash equilibrium provides a rigorous model for the
robot to anticipate uncertain yet cooperative human behavior in crowds, but the
computation cost is often too high for scalable and real-time decision-making.
Here we prove that a simple iterative Bayesian updating scheme converges to the
Nash equilibrium of a mixed-strategy social navigation game. Furthermore, we
propose a data-driven framework to construct the game by initializing agent
strategies as Gaussian processes learned from human datasets. Based on the
proposed mixed-strategy Nash equilibrium model, we develop a sampling-based
crowd navigation framework that can be integrated into existing navigation
methods and runs in real-time on a laptop CPU. We evaluate our framework in
both simulated environments and real-world human datasets in unstructured
environments. Our framework consistently outperforms both non-learning and
learning-based methods on both safety and navigation efficiency and reaches
human-level crowd navigation performance on top of a meta-planner.
\\ ( https://arxiv.org/abs/2403.01537 ,  8609kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01546 (*cross-listing*)
Date: Sun, 3 Mar 2024 15:47:43 GMT   (32906kb,D)

Title: Hyperspectral Image Analysis in Single-Modal and Multimodal setting
  using Deep Learning Techniques
Authors: Shivam Pande
Categories: cs.CV cs.LG
Comments: 253 pages
\\
  Hyperspectral imaging provides precise classification for land use and cover
due to its exceptional spectral resolution. However, the challenges of high
dimensionality and limited spatial resolution hinder its effectiveness. This
study addresses these challenges by employing deep learning techniques to
efficiently process, extract features, and classify data in an integrated
manner. To enhance spatial resolution, we integrate information from
complementary modalities such as LiDAR and SAR data through multimodal
learning. Moreover, adversarial learning and knowledge distillation are
utilized to overcome issues stemming from domain disparities and missing
modalities. We also tailor deep learning architectures to suit the unique
characteristics of HSI data, utilizing 1D convolutional and recurrent neural
networks to handle its continuous spectral dimension. Techniques like visual
attention and feedback connections within the architecture bolster the
robustness of feature extraction. Additionally, we tackle the issue of limited
training samples through self-supervised learning methods, employing
autoencoders for dimensionality reduction and exploring semi-supervised
learning techniques that leverage unlabeled data. Our proposed approaches are
evaluated across various HSI datasets, consistently outperforming existing
state-of-the-art techniques.
\\ ( https://arxiv.org/abs/2403.01546 ,  32906kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01571 (*cross-listing*)
Date: Sun, 3 Mar 2024 17:36:42 GMT   (4596kb,D)

Title: Limits to classification performance by relating Kullback-Leibler
  divergence to Cohen's Kappa
Authors: L. Crow and S. J. Watts
Categories: stat.ML cs.LG physics.data-an
Comments: Presented at the American Statistical Association Symposium on Data
  Science and Statistics, St. Louis, USA, May 2023
\\
  The performance of machine learning classification algorithms are evaluated
by estimating metrics, often from the confusion matrix, using training data and
cross-validation. However, these do not prove that the best possible
performance has been achieved. Fundamental limits to error rates can be
estimated using information distance measures. To this end, the confusion
matrix has been formulated to comply with the Chernoff-Stein Lemma. This links
the error rates to the Kullback-Leibler divergences between the probability
density functions describing the two classes. This leads to a key result that
relates Cohen's Kappa to the Resistor Average Distance which is the parallel
resistor combination of the two Kullback-Leibler divergences. The Resistor
Average Distance has units of bits and is estimated from the same training data
used by the classification algorithm, using kNN estimates of the
KullBack-Leibler divergences. The classification algorithm gives the confusion
matrix and Kappa. Theory and methods are discussed in detail and then applied
to Monte Carlo data and real datasets. Four very different real datasets -
Breast Cancer, Coronary Heart Disease, Bankruptcy, and Particle Identification
- are analysed, with both continuous and discrete values, and their
classification performance compared to the expected theoretical limit. In all
cases this analysis shows that the algorithms could not have performed any
better due to the underlying probability density functions for the two classes.
Important lessons are learnt on how to predict the performance of algorithms
for imbalanced data using training datasets that are approximately balanced.
Machine learning is very powerful but classification performance ultimately
depends on the quality of the data and the relevance of the variables to the
problem.
\\ ( https://arxiv.org/abs/2403.01571 ,  4596kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01635 (*cross-listing*)
Date: Sun, 3 Mar 2024 22:55:39 GMT   (3402kb,D)

Title: Application of Neural Ordinary Differential Equations for Tokamak Plasma
  Dynamics Analysis
Authors: Zefang Liu, Weston M. Stacey
Categories: physics.plasm-ph cs.LG
Comments: 11 pages, 10 figures
\\
  In the quest for controlled thermonuclear fusion, tokamaks present complex
challenges in understanding burning plasma dynamics. This study introduces a
multi-region multi-timescale transport model, employing Neural Ordinary
Differential Equations (Neural ODEs) to simulate the intricate energy transfer
processes within tokamaks. Our methodology leverages Neural ODEs for the
numerical derivation of diffusivity parameters from DIII-D tokamak experimental
data, enabling the precise modeling of energy interactions between electrons
and ions across various regions, including the core, edge, and scrape-off
layer. These regions are conceptualized as distinct nodes, capturing the
critical timescales of radiation and transport processes essential for
efficient tokamak operation. Validation against DIII-D plasmas under various
auxiliary heating conditions demonstrates the model's effectiveness, ultimately
shedding light on ways to enhance tokamak performance with deep learning.
\\ ( https://arxiv.org/abs/2403.01635 ,  3402kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01636 (*cross-listing*)
Date: Sun, 3 Mar 2024 22:57:44 GMT   (1668kb,D)

Title: Sample Efficient Myopic Exploration Through Multitask Reinforcement
  Learning with Diverse Tasks
Authors: Ziping Xu, Zifan Xu, Runxuan Jiang, Peter Stone, Ambuj Tewari
Categories: stat.ML cs.LG
\\
  Multitask Reinforcement Learning (MTRL) approaches have gained increasing
attention for its wide applications in many important Reinforcement Learning
(RL) tasks. However, while recent advancements in MTRL theory have focused on
the improved statistical efficiency by assuming a shared structure across
tasks, exploration--a crucial aspect of RL--has been largely overlooked. This
paper addresses this gap by showing that when an agent is trained on a
sufficiently diverse set of tasks, a generic policy-sharing algorithm with
myopic exploration design like $\epsilon$-greedy that are inefficient in
general can be sample-efficient for MTRL. To the best of our knowledge, this is
the first theoretical demonstration of the "exploration benefits" of MTRL. It
may also shed light on the enigmatic success of the wide applications of myopic
exploration in practice. To validate the role of diversity, we conduct
experiments on synthetic robotic control environments, where the diverse task
set aligns with the task selection by automatic curriculum learning, which is
empirically shown to improve sample-efficiency.
\\ ( https://arxiv.org/abs/2403.01636 ,  1668kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01717 (*cross-listing*)
Date: Mon, 4 Mar 2024 04:10:24 GMT   (1907kb,D)

Title: Soft-constrained Schrodinger Bridge: a Stochastic Control Approach
Authors: Jhanvi Garg, Xianyang Zhang, Quan Zhou
Categories: stat.ML cs.LG math.OC stat.CO
Comments: 37 pages, 7 figures. Accepted by AISTATS 2024
\\
  Schr\"{o}dinger bridge can be viewed as a continuous-time stochastic control
problem where the goal is to find an optimally controlled diffusion process
with a pre-specified terminal distribution $\mu_T$. We propose to generalize
this stochastic control problem by allowing the terminal distribution to differ
from $\mu_T$ but penalizing the Kullback-Leibler divergence between the two
distributions. We call this new control problem soft-constrained
Schr\"{o}dinger bridge (SSB). The main contribution of this work is a
theoretical derivation of the solution to SSB, which shows that the terminal
distribution of the optimally controlled process is a geometric mixture of
$\mu_T$ and some other distribution. This result is further extended to a time
series setting. One application of SSB is the development of robust generative
diffusion models. We propose a score matching-based algorithm for sampling from
geometric mixtures and showcase its use via a numerical example for the MNIST
data set.
\\ ( https://arxiv.org/abs/2403.01717 ,  1907kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01723 (*cross-listing*)
Date: Mon, 4 Mar 2024 04:32:28 GMT   (2038kb,D)

Title: Statistical Mechanics of Dynamical System Identification
Authors: Andrei A. Klishin, Joseph Bakarji, J. Nathan Kutz, Krithika Manohar
Categories: cond-mat.stat-mech cs.LG math.OC physics.comp-ph
Comments: 21 RevTeX page, 9 figures
\\
  Recovering dynamical equations from observed noisy data is the central
challenge of system identification. We develop a statistical mechanical
approach to analyze sparse equation discovery algorithms, which typically
balance data fit and parsimony through a trial-and-error selection of
hyperparameters. In this framework, statistical mechanics offers tools to
analyze the interplay between complexity and fitness, in analogy to that done
between entropy and energy. To establish this analogy, we define the
optimization procedure as a two-level Bayesian inference problem that separates
variable selection from coefficient values and enables the computation of the
posterior parameter distribution in closed form. A key advantage of employing
statistical mechanical concepts, such as free energy and the partition
function, is in the quantification of uncertainty, especially in in the
low-data limit; frequently encountered in real-world applications. As the data
volume increases, our approach mirrors the thermodynamic limit, leading to
distinct sparsity- and noise-induced phase transitions that delineate correct
from incorrect identification. This perspective of sparse equation discovery,
is versatile and can be adapted to various other equation discovery algorithms.
\\ ( https://arxiv.org/abs/2403.01723 ,  2038kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01758 (*cross-listing*)
Date: Mon, 4 Mar 2024 06:24:24 GMT   (1389kb,D)

Title: AFBT GAN: enhanced explainability and diagnostic performance for
  cognitive decline by counterfactual generative adversarial network
Authors: Xiongri Shen, Zhenxi Song, Zhiguo Zhang
Categories: eess.IV cs.CV cs.LG
Comments: 10 pages, 5 figures
\\
  Existing explanation results of functional connectivity (FC) are normally
generated by using classification result labels and correlation analysis
methods such as Pearson's correlation or gradient backward. However, the
diagnostic model is still trained on the black box model and might lack the
attention of FCs in important regions during the training. To enhance the
explainability and improve diagnostic performance, providing prior knowledge on
neurodegeneration-related regions when healthy subjects (HC) develop into
subject cognitive decline (SCD) and mild cognitive impairment (MCI) for the
diagnostic model is a key step. To better determine the
neurodegeneration-related regions, we employ counterfactual reasoning to
generate the target label FC matrices derived from source label FC and then
subtract source label FC with target label FC. The counterfactual reasoning
architecture is constructed by adaptive forward and backward transformer
generative adversarial network (AFBT GAN), which is specifically designed by
network property in FC and inverse patch embedding operation in the
transformer. The specific design can make the model focus more on the current
network correlation and employ the global insight of the transformer to
reconstruct FC, which both help the generation of high-quality target label FC.
The validation experiments are conducted on both clinical and public datasets,
the generated attention map are both vital correlated to cognitive function and
the diagnostic performance is also significant. The code is available at
https://github.com/SXR3015/AFBT-GAN.
\\ ( https://arxiv.org/abs/2403.01758 ,  1389kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01776 (*cross-listing*)
Date: Mon, 4 Mar 2024 07:09:54 GMT   (7897kb,D)

Title: Hybrid data-driven and physics-informed regularized learning of cyclic
  plasticity with Neural Networks
Authors: Stefan Hildebrand and Sandra Klinge
Categories: cond-mat.mtrl-sci cs.LG nlin.AO physics.comp-ph
\\
  An extendable, efficient and explainable Machine Learning approach is
proposed to represent cyclic plasticity and replace conventional material
models based on the Radial Return Mapping algorithm. High accuracy and
stability by means of a limited amount of training data is achieved by
implementing physics-informed regularizations and the back stress information.
The off-loading of the Neural Network is applied to the maximal extent. The
proposed model architecture is simpler and more efficient compared to existing
solutions from the literature, while representing a complete three-dimensional
material model. The validation of the approach is carried out by means of
surrogate data obtained with the Armstrong-Frederick kinematic hardening model.
The Mean Squared Error is assumed as the loss function which stipulates several
restrictions: deviatoric character of internal variables, compliance with the
flow rule, the differentiation of elastic and plastic steps and the
associativity of the flow rule. The latter, however, has a minor impact on the
accuracy, which implies the generalizability of the model for a broad spectrum
of evolution laws for internal variables. Numerical tests simulating several
load cases are shown in detail and validated for accuracy and stability.
\\ ( https://arxiv.org/abs/2403.01776 ,  7897kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01798 (*cross-listing*)
Date: Mon, 4 Mar 2024 07:38:31 GMT   (1782kb,D)

Title: Towards Fair and Efficient Learning-based Congestion Control
Authors: Xudong Liao, Han Tian, Chaoliang Zeng, Xinchen Wan, Kai Chen
Categories: cs.NI cs.LG
\\
  Recent years have witnessed a plethora of learning-based solutions for
congestion control (CC) that demonstrate better performance over traditional
TCP schemes. However, they fail to provide consistently good convergence
properties, including {\em fairness}, {\em fast convergence} and {\em
stability}, due to the mismatch between their objective functions and these
properties. Despite being intuitive, integrating these properties into existing
learning-based CC is challenging, because: 1) their training environments are
designed for the performance optimization of single flow but incapable of
cooperative multi-flow optimization, and 2) there is no directly measurable
metric to represent these properties into the training objective function.
  We present Astraea, a new learning-based congestion control that ensures fast
convergence to fairness with stability. At the heart of Astraea is a
multi-agent deep reinforcement learning framework that explicitly optimizes
these convergence properties during the training process by enabling the
learning of interactive policy between multiple competing flows, while
maintaining high performance. We further build a faithful multi-flow
environment that emulates the competing behaviors of concurrent flows,
explicitly expressing convergence properties to enable their optimization
during training. We have fully implemented Astraea and our comprehensive
experiments show that Astraea can quickly converge to fairness point and
exhibit better stability than its counterparts. For example, \sys achieves
near-optimal bandwidth sharing (i.e., fairness) when multiple flows compete for
the same bottleneck, delivers up to 8.4$\times$ faster convergence speed and
2.8$\times$ smaller throughput deviation, while achieving comparable or even
better performance over prior solutions.
\\ ( https://arxiv.org/abs/2403.01798 ,  1782kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01805 (*cross-listing*)
Date: Mon, 4 Mar 2024 07:53:15 GMT   (407kb,D)

Title: Tsallis Entropy Regularization for Linearly Solvable MDP and Linear
  Quadratic Regulator
Authors: Yota Hashizume, Koshi Oishi, Kenji Kashima
Categories: math.OC cs.LG cs.SY eess.SY
Comments: 6 figures
\\
  Shannon entropy regularization is widely adopted in optimal control due to
its ability to promote exploration and enhance robustness, e.g., maximum
entropy reinforcement learning known as Soft Actor-Critic. In this paper,
Tsallis entropy, which is a one-parameter extension of Shannon entropy, is used
for the regularization of linearly solvable MDP and linear quadratic
regulators. We derive the solution for these problems and demonstrate its
usefulness in balancing between exploration and sparsity of the obtained
control law.
\\ ( https://arxiv.org/abs/2403.01805 ,  407kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01820 (*cross-listing*)
Date: Mon, 4 Mar 2024 08:10:42 GMT   (1269kb,D)

Title: Macroscopic auxiliary asymptotic preserving neural networks for the
  linear radiative transfer equations
Authors: Hongyan Li, Song Jiang, Wenjun Sun, Liwei Xu, Guanyu Zhou
Categories: math.NA cs.LG cs.NA
Comments: 24 pages, 29 figures
\\
  We develop a Macroscopic Auxiliary Asymptotic-Preserving Neural Network
(MA-APNN) method to solve the time-dependent linear radiative transfer
equations (LRTEs), which have a multi-scale nature and high dimensionality. To
achieve this, we utilize the Physics-Informed Neural Networks (PINNs) framework
and design a new adaptive exponentially weighted Asymptotic-Preserving (AP)
loss function, which incorporates the macroscopic auxiliary equation that is
derived from the original transfer equation directly and explicitly contains
the information of the diffusion limit equation. Thus, as the scale parameter
tends to zero, the loss function gradually transitions from the transport state
to the diffusion limit state. In addition, the initial data, boundary
conditions, and conservation laws serve as the regularization terms for the
loss. We present several numerical examples to demonstrate the effectiveness of
MA-APNNs.
\\ ( https://arxiv.org/abs/2403.01820 ,  1269kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01864 (*cross-listing*)
Date: Mon, 4 Mar 2024 09:20:05 GMT   (2719kb,D)

Title: RCoCo: Contrastive Collective Link Prediction across Multiplex Network
  in Riemannian Space
Authors: Li Sun, Mengjie Li, Yong Yang, Xiao Li, Lin Liu, Pengfei Zhang, Haohua
  Du
Categories: cs.SI cs.LG
Comments: Accepted by Springer International Journal of Machine Learning and
  Cybernetics (JMLC), 2024
\\
  Link prediction typically studies the probability of future interconnection
among nodes with the observation in a single social network. More often than
not, real scenario is presented as a multiplex network with common (anchor)
users active in multiple social networks. In the literature, most existing
works study either the intra-link prediction in a single network or inter-link
prediction among networks (a.k.a. network alignment), and consider two learning
tasks are independent from each other, which is still away from the fact. On
the representation space, the vast majority of existing methods are built upon
the traditional Euclidean space, unaware of the inherent geometry of social
networks. The third issue is on the scarce anchor users. Annotating anchor
users is laborious and expensive, and thus it is impractical to work with
quantities of anchor users. Herein, in light of the issues above, we propose to
study a challenging yet practical problem of Geometry-aware Collective Link
Prediction across Multiplex Network. To address this problem, we present a
novel contrastive model, RCoCo, which collaborates intra- and inter-network
behaviors in Riemannian spaces. In RCoCo, we design a curvature-aware graph
attention network ($\kappa-$GAT), conducting attention mechanism in Riemannian
manifold whose curvature is estimated by the Ricci curvatures over the network.
Thereafter, we formulate intra- and inter-contrastive loss in the manifolds, in
which we augment graphs by exploring the high-order structure of community and
information transfer on anchor users. Finally, we conduct extensive experiments
with 14 strong baselines on 8 real-world datasets, and show the effectiveness
of RCoCo.
\\ ( https://arxiv.org/abs/2403.01864 ,  2719kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01865 (*cross-listing*)
Date: Mon, 4 Mar 2024 09:21:10 GMT   (3875kb,D)

Title: Improving generalisation via anchor multivariate analysis
Authors: Homer Durand, Gherardo Varando, Gustau Camps-Valls, Nathan Mankovich
Categories: stat.ML cs.LG stat.AP stat.ME
Comments: 21 pages, 15 figures
MSC-class: 62Hxx
\\
  We introduce a causal regularisation extension to anchor regression (AR) for
improved out-of-distribution (OOD) generalisation. We present anchor-compatible
losses, aligning with the anchor framework to ensure robustness against
distribution shifts. Various multivariate analysis (MVA) algorithms, such as
(Orthonormalized) PLS, RRR, and MLR, fall within the anchor framework. We
observe that simple regularisation enhances robustness in OOD settings.
Estimators for selected algorithms are provided, showcasing consistency and
efficacy in synthetic and real-world climate science problems. The empirical
validation highlights the versatility of anchor regularisation, emphasizing its
compatibility with MVA approaches and its role in enhancing replicability while
guarding against distribution shifts. The extended AR framework advances causal
inference methodologies, addressing the need for reliable OOD generalisation.
\\ ( https://arxiv.org/abs/2403.01865 ,  3875kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01900 (*cross-listing*)
Date: Mon, 4 Mar 2024 09:59:11 GMT   (37kb,D)

Title: Universality of reservoir systems with recurrent neural networks
Authors: Hiroki Yasumoto and Toshiyuki Tanaka
Categories: cs.NE cs.LG
\\
  Approximation capability of reservoir systems whose reservoir is a recurrent
neural network (RNN) is discussed. In our problem setting, a reservoir system
approximates a set of functions just by adjusting its linear readout while the
reservoir is fixed. We will show what we call uniform strong universality of a
family of RNN reservoir systems for a certain class of functions to be
approximated. This means that, for any positive number, we can construct a
sufficiently large RNN reservoir system whose approximation error for each
function in the class of functions to be approximated is bounded from above by
the positive number. Such RNN reservoir systems are constructed via parallel
concatenation of RNN reservoirs.
\\ ( https://arxiv.org/abs/2403.01900 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01907 (*cross-listing*)
Date: Mon, 4 Mar 2024 10:10:23 GMT   (1159kb)

Title: Capacity of the Hebbian-Hopfield network associative memory
Authors: Mihailo Stojnic
Categories: stat.ML cond-mat.dis-nn cs.IT cs.LG math.IT math.PR
\\
  In \cite{Hop82}, Hopfield introduced a \emph{Hebbian} learning rule based
neural network model and suggested how it can efficiently operate as an
associative memory. Studying random binary patterns, he also uncovered that, if
a small fraction of errors is tolerated in the stored patterns retrieval, the
capacity of the network (maximal number of memorized patterns, $m$) scales
linearly with each pattern's size, $n$. Moreover, he famously predicted
$\alpha_c=\lim_{n\rightarrow\infty}\frac{m}{n}\approx 0.14$. We study this very
same scenario with two famous pattern's basins of attraction:
\textbf{\emph{(i)}} The AGS one from \cite{AmiGutSom85}; and
\textbf{\emph{(ii)}} The NLT one from
\cite{Newman88,Louk94,Louk94a,Louk97,Tal98}. Relying on the \emph{fully lifted
random duality theory} (fl RDT) from \cite{Stojnicflrdt23}, we obtain the
following explicit capacity characterizations on the first level of lifting:
  \begin{equation}
  \alpha_c^{(AGS,1)} = \left ( \max_{\delta\in \left ( 0,\frac{1}{2}\right )
}\frac{1-2\delta}{\sqrt{2} \mbox{erfinv} \left ( 1-2\delta\right )} -
\frac{2}{\sqrt{2\pi}} e^{-\left ( \mbox{erfinv}\left ( 1-2\delta \right )\right
)^2}\right )^2 \approx \mathbf{0.137906} \end{equation}
  \begin{equation}
  \alpha_c^{(NLT,1)} = \frac{\mbox{erf}(x)^2}{2x^2}-1+\mbox{erf}(x)^2 \approx
\mathbf{0.129490}, \quad 1-\mbox{erf}(x)^2-
\frac{2\mbox{erf}(x)e^{-x^2}}{\sqrt{\pi}x}+\frac{2e^{-2x^2}}{\pi}=0.
\end{equation}
  A substantial numerical work gives on the second level of lifting
$\alpha_c^{(AGS,2)} \approx \mathbf{0.138186}$ and $\alpha_c^{(NLT,2)} \approx
\mathbf{0.12979}$, effectively uncovering a remarkably fast lifting
convergence. Moreover, the obtained AGS characterizations exactly match the
replica symmetry based ones of \cite{AmiGutSom85} and the corresponding
symmetry breaking ones of \cite{SteKuh94}.
\\ ( https://arxiv.org/abs/2403.01907 ,  1159kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01918 (*cross-listing*)
Date: Mon, 4 Mar 2024 10:32:48 GMT   (3679kb,D)

Title: Towards Continuous Assurance Case Creation for ADS with the Evidential
  Tool Bus
Authors: Lev Sorokin, Radouane Bouchekir, Tewodros A. Beyene, Brian Hsuan-Cheng
  Liao, and Adam Molin
Categories: cs.SE cs.LG
Comments: Accepted at International SafeAutonomy Workshop at EDCC '24
\\
  An assurance case has become an integral component for the certification of
safety-critical systems. While manually defining assurance case patterns can be
not avoided, system-specific instantiations of assurance case patterns are both
costly and time-consuming. It becomes especially complex to maintain an
assurance case for a system when the requirements of the System-Under-Assurance
change, or an assurance claim becomes invalid due to, e.g., degradation of a
systems component, as common when deploying learning-enabled components. In
this paper, we report on our preliminary experience leveraging the tool
integration framework Evidential Tool Bus (ETB) for the construction and
continuous maintenance of an assurance case from a predefined assurance case
pattern. Specifically, we demonstrate the assurance process on an industrial
Automated Valet Parking system from the automotive domain. We present the
formalization of the provided assurance case pattern in the ETB processable
logical specification language of workflows. Our findings show that ETB is able
to create and maintain evidence required for the construction of an assurance
case.
\\ ( https://arxiv.org/abs/2403.01918 ,  3679kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01944 (*cross-listing*)
Date: Mon, 4 Mar 2024 11:30:02 GMT   (4764kb,D)

Title: Fourier-basis Functions to Bridge Augmentation Gap: Rethinking Frequency
  Augmentation in Image Classification
Authors: Puru Vaish, Shunxin Wang and Nicola Strisciuglio
Categories: cs.CV cs.LG
\\
  Computer vision models normally witness degraded performance when deployed in
real-world scenarios, due to unexpected changes in inputs that were not
accounted for during training. Data augmentation is commonly used to address
this issue, as it aims to increase data variety and reduce the distribution gap
between training and test data. However, common visual augmentations might not
guarantee extensive robustness of computer vision models. In this paper, we
propose Auxiliary Fourier-basis Augmentation (AFA), a complementary technique
targeting augmentation in the frequency domain and filling the augmentation gap
left by visual augmentations. We demonstrate the utility of augmentation via
Fourier-basis additive noise in a straightforward and efficient adversarial
setting. Our results show that AFA benefits the robustness of models against
common corruptions, OOD generalization, and consistency of performance of
models against increasing perturbations, with negligible deficit to the
standard performance of models. It can be seamlessly integrated with other
augmentation techniques to further boost performance.
\\ ( https://arxiv.org/abs/2403.01944 ,  4764kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01948 (*cross-listing*)
Date: Mon, 4 Mar 2024 11:34:12 GMT   (1326kb,D)

Title: On Fractional Moment Estimation from Polynomial Chaos Expansion
Authors: Luk\'a\v{s} Nov\'ak and Marcos Valdebenito and Matthias Faes
Categories: stat.ME cs.LG
\\
  Fractional statistical moments are utilized for various tasks of uncertainty
quantification, including the estimation of probability distributions. However,
an estimation of fractional statistical moments of costly mathematical models
by statistical sampling is challenging since it is typically not possible to
create a large experimental design due to limitations in computing capacity.
This paper presents a novel approach for the analytical estimation of
fractional moments, directly from polynomial chaos expansions. Specifically,
the first four statistical moments obtained from the deterministic PCE
coefficients are used for an estimation of arbitrary fractional moments via
H\"{o}lder's inequality. The proposed approach is utilized for an estimation of
statistical moments and probability distributions in three numerical examples
of increasing complexity. Obtained results show that the proposed approach
achieves a superior performance in estimating the distribution of the response,
in comparison to a standard Latin hypercube sampling in the presented examples.
\\ ( https://arxiv.org/abs/2403.01948 ,  1326kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02011 (*cross-listing*)
Date: Mon, 4 Mar 2024 13:12:02 GMT   (1260kb,D)

Title: Bipartite Graph Variational Auto-Encoder with Fair Latent Representation
  to Account for Sampling Bias in Ecological Networks
Authors: Emre Anakok, Pierre Barbillon, Colin Fontaine, Elisa Thebault
Categories: stat.ML cs.LG cs.SI
\\
  We propose a method to represent bipartite networks using graph embeddings
tailored to tackle the challenges of studying ecological networks, such as the
ones linking plants and pollinators, where many covariates need to be accounted
for, in particular to control for sampling bias. We adapt the variational graph
auto-encoder approach to the bipartite case, which enables us to generate
embeddings in a latent space where the two sets of nodes are positioned based
on their probability of connection. We translate the fairness framework
commonly considered in sociology in order to address sampling bias in ecology.
By incorporating the Hilbert-Schmidt independence criterion (HSIC) as an
additional penalty term in the loss we optimize, we ensure that the structure
of the latent space is independent of continuous variables, which are related
to the sampling process. Finally, we show how our approach can change our
understanding of ecological networks when applied to the Spipoll data set, a
citizen science monitoring program of plant-pollinator interactions to which
many observers contribute, making it prone to sampling bias.
\\ ( https://arxiv.org/abs/2403.02011 ,  1260kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02019 (*cross-listing*)
Date: Mon, 4 Mar 2024 13:20:52 GMT   (366kb,D)

Title: Active Learning of Mealy Machines with Timers
Authors: V\'eronique Bruy\`ere, Bharat Garhewal, Guillermo A. P\'erez, Ga\"etan
  Staquet, Frits W. Vaandrager
Categories: cs.FL cs.LG
Comments: 77 pages, 19 figures
MSC-class: 68Q45
ACM-class: F.4.3
\\
  We present the first algorithm for query learning of a general class of Mealy
machines with timers (MMTs) in a black-box context. Our algorithm is an
extension of the L# algorithm of Vaandrager et al. to a timed setting. Like the
algorithm for learning timed automata proposed by Waga, our algorithm is
inspired by ideas of Maler & Pnueli. Based on the elementary languages of, both
Waga's and our algorithm use symbolic queries, which are then implemented using
finitely many concrete queries. However, whereas Waga needs exponentially many
concrete queries to implement a single symbolic query, we only need a
polynomial number. This is because in order to learn a timed automaton, a
learner needs to determine the exact guard and reset for each transition (out
of exponentially many possibilities), whereas for learning an MMT a learner
only needs to figure out which of the preceding transitions caused a timeout.
As shown in our previous work, this can be done efficiently for a subclass of
MMTs that are race-avoiding: if a timeout is caused by a preceding input then a
slight change in the timing of this input will induce a corresponding change in
the timing of the timeout ("wiggling"). Experiments with a prototype
implementation, written in Rust, show that our algorithm is able to efficiently
learn realistic benchmarks.
\\ ( https://arxiv.org/abs/2403.02019 ,  366kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02035 (*cross-listing*)
Date: Mon, 4 Mar 2024 13:39:22 GMT   (30kb)

Title: Exponential Expressivity of ReLU$^k$ Neural Networks on Gevrey Classes
  with Point Singularities
Authors: Joost A. A. Opschoor, Christoph Schwab
Categories: math.NA cs.LG cs.NA
MSC-class: 65N30, 41A25
\\
  We analyze deep Neural Network emulation rates of smooth functions with point
singularities in bounded, polytopal domains $\mathrm{D} \subset \mathbb{R}^d$,
$d=2,3$. We prove exponential emulation rates in Sobolev spaces in terms of the
number of neurons and in terms of the number of nonzero coefficients for
Gevrey-regular solution classes defined in terms of weighted Sobolev scales in
$\mathrm{D}$, comprising the countably-normed spaces of I.M. Babu\v{s}ka and
B.Q. Guo.
  As intermediate result, we prove that continuous, piecewise polynomial high
order (``$p$-version'') finite elements with elementwise polynomial degree
$p\in\mathbb{N}$ on arbitrary, regular, simplicial partitions of polyhedral
domains $\mathrm{D} \subset \mathbb{R}^d$, $d\geq 2$ can be exactly emulated by
neural networks combining ReLU and ReLU$^2$ activations. On shape-regular,
simplicial partitions of polytopal domains $\mathrm{D}$, both the number of
neurons and the number of nonzero parameters are proportional to the number of
degrees of freedom of the finite element space, in particular for the
$hp$-Finite Element Method of I.M. Babu\v{s}ka and B.Q. Guo.
\\ ( https://arxiv.org/abs/2403.02035 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02051 (*cross-listing*)
Date: Mon, 4 Mar 2024 13:53:41 GMT   (63kb,D)

Title: Differential Privacy of Noisy (S)GD under Heavy-Tailed Perturbations
Authors: Umut \c{S}im\c{s}ekli, Mert G\"urb\"uzbalaban, Sinan
  Y{\i}ld{\i}r{\i}m, Lingjiong Zhu
Categories: stat.ML cs.CR cs.LG math.ST stat.TH
\\
  Injecting heavy-tailed noise to the iterates of stochastic gradient descent
(SGD) has received increasing attention over the past few years. While various
theoretical properties of the resulting algorithm have been analyzed mainly
from learning theory and optimization perspectives, their privacy preservation
properties have not yet been established. Aiming to bridge this gap, we provide
differential privacy (DP) guarantees for noisy SGD, when the injected noise
follows an $\alpha$-stable distribution, which includes a spectrum of
heavy-tailed distributions (with infinite variance) as well as the Gaussian
distribution. Considering the $(\epsilon, \delta)$-DP framework, we show that
SGD with heavy-tailed perturbations achieves $(0, \tilde{\mathcal{O}}(1/n))$-DP
for a broad class of loss functions which can be non-convex, where $n$ is the
number of data points. As a remarkable byproduct, contrary to prior work that
necessitates bounded sensitivity for the gradients or clipping the iterates,
our theory reveals that under mild assumptions, such a projection step is not
actually necessary. We illustrate that the heavy-tailed noising mechanism
achieves similar DP guarantees compared to the Gaussian case, which suggests
that it can be a viable alternative to its light-tailed counterparts.
\\ ( https://arxiv.org/abs/2403.02051 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02080 (*cross-listing*)
Date: Mon, 4 Mar 2024 14:26:52 GMT   (1327kb,D)

Title: Hybrid Quantum Neural Network Advantage for Radar-Based Drone Detection
  and Classification in Low Signal-to-Noise Ratio
Authors: Aiswariya Sweety Malarvanan
Categories: quant-ph cs.LG eess.SP physics.app-ph
Comments: 9 pages, 18 figures
\\
  In this paper, we investigate the performance of a Hybrid Quantum Neural
Network (HQNN) and a comparable classical Convolution Neural Network (CNN) for
detection and classification problem using a radar. Specifically, we take a
fairly complex radar time-series model derived from electromagnetic theory,
namely the Martin-Mulgrew model, that is used to simulate radar returns of
objects with rotating blades, such as drones. We find that when that
signal-to-noise ratio (SNR) is high, CNN outperforms the HQNN for detection and
classification. However, in the low SNR regime (which is of greatest interest
in practice) the performance of HQNN is found to be superior to that of the CNN
of a similar architecture.
\\ ( https://arxiv.org/abs/2403.02080 ,  1327kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02150 (*cross-listing*)
Date: Mon, 4 Mar 2024 16:00:35 GMT   (2437kb,AD)

Title: Recency-Weighted Temporally-Segmented Ensemble for Time-Series Modeling
Authors: P{\aa}l V. Johnsen, Eivind B{\o}hn, S{\o}lve Eidnes, Filippo Remonato
  and Signe Riemer-S{\o}rensen
Categories: stat.ML cs.LG
Comments: Main article with 23 pages including 12 figures and 4 tables.
  Supplementary File with 11 pages including 9 figures
\\
  Time-series modeling in process industries faces the challenge of dealing
with complex, multi-faceted, and evolving data characteristics. Conventional
single model approaches often struggle to capture the interplay of diverse
dynamics, resulting in suboptimal forecasts. Addressing this, we introduce the
Recency-Weighted Temporally-Segmented (ReWTS, pronounced `roots') ensemble
model, a novel chunk-based approach for multi-step forecasting. The key
characteristics of the ReWTS model are twofold: 1) It facilitates
specialization of models into different dynamics by segmenting the training
data into `chunks' of data and training one model per chunk. 2) During
inference, an optimization procedure assesses each model on the recent past and
selects the active models, such that the appropriate mixture of previously
learned dynamics can be recalled to forecast the future. This method not only
captures the nuances of each period, but also adapts more effectively to
changes over time compared to conventional `global' models trained on all data
in one go. We present a comparative analysis, utilizing two years of data from
a wastewater treatment plant and a drinking water treatment plant in Norway,
demonstrating the ReWTS ensemble's superiority. It consistently outperforms the
global model in terms of mean squared forecasting error across various model
architectures by 10-70\% on both datasets, notably exhibiting greater
resilience to outliers. This approach shows promise in developing automatic,
adaptable forecasting models for decision-making and control systems in process
industries and other complex systems.
\\ ( https://arxiv.org/abs/2403.02150 ,  2437kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02171 (*cross-listing*)
Date: Mon, 4 Mar 2024 16:17:43 GMT   (16454kb,D)

Title: Predicting large scale cosmological structure evolution with GAN-based
  autoencoders
Authors: Marion Ullmo, Nabila Aghnim, Aur\'elien Decelle, Miguel Aragon-Calvo
Categories: astro-ph.CO cs.LG
Comments: 11 pages, 11 figures
\\
  Cosmological simulations play a key role in the prediction and understanding
of large scale structure formation from initial conditions. We make use of
GAN-based Autoencoders (AEs) in an attempt to predict structure evolution
within simulations. The AEs are trained on images and cubes issued from
respectively 2D and 3D N-body simulations describing the evolution of the dark
matter (DM) field. We find that while the AEs can predict structure evolution
for 2D simulations of DM fields well, using only the density fields as input,
they perform significantly more poorly in similar conditions for 3D
simulations. However, additionally providing velocity fields as inputs greatly
improves results, with similar predictions regardless of time-difference
between input and target.
\\ ( https://arxiv.org/abs/2403.02171 ,  16454kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02251 (*cross-listing*)
Date: Mon, 4 Mar 2024 17:35:30 GMT   (523kb,D)

Title: A prediction rigidity formalism for low-cost uncertainties in trained
  neural networks
Authors: Filippo Bigi, Sanggyu Chong, Michele Ceriotti, and Federico Grasselli
Categories: stat.ML cs.LG
\\
  Regression methods are fundamental for scientific and technological
applications. However, fitted models can be highly unreliable outside of their
training domain, and hence the quantification of their uncertainty is crucial
in many of their applications. Based on the solution of a constrained
optimization problem, we propose "prediction rigidities" as a method to obtain
uncertainties of arbitrary pre-trained regressors. We establish a strong
connection between our framework and Bayesian inference, and we develop a
last-layer approximation that allows the new method to be applied to neural
networks. This extension affords cheap uncertainties without any modification
to the neural network itself or its training procedure. We show the
effectiveness of our method on a wide range of regression tasks, ranging from
simple toy models to applications in chemistry and meteorology.
\\ ( https://arxiv.org/abs/2403.02251 ,  523kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02274 (*cross-listing*)
Date: Mon, 4 Mar 2024 18:02:41 GMT   (14104kb,D)

Title: NatSGD: A Dataset with Speech, Gestures, and Demonstrations for Robot
  Learning in Natural Human-Robot Interaction
Authors: Snehesh Shrestha, Yantian Zha, Saketh Banagiri, Ge Gao, Yiannis
  Aloimonos, Cornelia Fermuller
Categories: cs.RO cs.LG
\\
  Recent advancements in multimodal Human-Robot Interaction (HRI) datasets have
highlighted the fusion of speech and gesture, expanding robots' capabilities to
absorb explicit and implicit HRI insights. However, existing speech-gesture HRI
datasets often focus on elementary tasks, like object pointing and pushing,
revealing limitations in scaling to intricate domains and prioritizing human
command data over robot behavior records. To bridge these gaps, we introduce
NatSGD, a multimodal HRI dataset encompassing human commands through speech and
gestures that are natural, synchronized with robot behavior demonstrations.
NatSGD serves as a foundational resource at the intersection of machine
learning and HRI research, and we demonstrate its effectiveness in training
robots to understand tasks through multimodal human commands, emphasizing the
significance of jointly considering speech and gestures. We have released our
dataset, simulator, and code to facilitate future research in human-robot
interaction system learning; access these resources at
https://www.snehesh.com/natsgd/
\\ ( https://arxiv.org/abs/2403.02274 ,  14104kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02300 (*cross-listing*)
Date: Mon, 4 Mar 2024 18:30:33 GMT   (43kb,D)

Title: Statistical Query Lower Bounds for Learning Truncated Gaussians
Authors: Ilias Diakonikolas, Daniel M. Kane, Thanasis Pittas, Nikos Zarifis
Categories: cs.DS cs.LG math.ST stat.ML stat.TH
\\
  We study the problem of estimating the mean of an identity covariance
Gaussian in the truncated setting, in the regime when the truncation set comes
from a low-complexity family $\mathcal{C}$ of sets. Specifically, for a fixed
but unknown truncation set $S \subseteq \mathbb{R}^d$, we are given access to
samples from the distribution $\mathcal{N}(\boldsymbol{ \mu}, \mathbf{ I})$
truncated to the set $S$. The goal is to estimate $\boldsymbol\mu$ within
accuracy $\epsilon>0$ in $\ell_2$-norm. Our main result is a Statistical Query
(SQ) lower bound suggesting a super-polynomial information-computation gap for
this task. In more detail, we show that the complexity of any SQ algorithm for
this problem is $d^{\mathrm{poly}(1/\epsilon)}$, even when the class
$\mathcal{C}$ is simple so that $\mathrm{poly}(d/\epsilon)$ samples
information-theoretically suffice. Concretely, our SQ lower bound applies when
$\mathcal{C}$ is a union of a bounded number of rectangles whose VC dimension
and Gaussian surface are small. As a corollary of our construction, it also
follows that the complexity of the previously known algorithm for this task is
qualitatively best possible.
\\ ( https://arxiv.org/abs/2403.02300 ,  43kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2107.04771
replaced with revised version Sat, 2 Mar 2024 08:46:51 GMT   (4661kb,D)

Title: Similar Cases Recommendation using Legal Knowledge Graphs
Authors: Jaspreet Singh Dhani, Ruchika Bhatt, Balaji Ganesan, Parikshet Sirohi,
  Vasudha Bhatnagar
Categories: cs.AI
Comments: 10 pages. 6 figures. 3rd Symposium on Artificial Intelligence and
  Law. SAIL 2023
\\ ( https://arxiv.org/abs/2107.04771 ,  4661kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13755
replaced with revised version Sun, 3 Mar 2024 14:23:21 GMT   (1408kb,D)

Title: Retrosynthetic Planning with Dual Value Networks
Authors: Guoqing Liu, Di Xue, Shufang Xie, Yingce Xia, Austin Tripp, Krzysztof
  Maziarz, Marwin Segler, Tao Qin, Zongzhang Zhang, Tie-Yan Liu
Categories: cs.AI cs.LG
Comments: Accepted to ICML 2023
\\ ( https://arxiv.org/abs/2301.13755 ,  1408kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03604
replaced with revised version Sun, 3 Mar 2024 04:59:28 GMT   (1318kb,D)

Title: Enabling Intelligent Interactions between an Agent and an LLM: A
  Reinforcement Learning Approach
Authors: Bin Hu, Chenyang Zhao, Pu Zhang, Zihao Zhou, Yuanhang Yang, Zenglin
  Xu, Bin Liu
Categories: cs.AI
Comments: 17 pages
\\ ( https://arxiv.org/abs/2306.03604 ,  1318kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09729
replaced with revised version Sat, 2 Mar 2024 15:02:33 GMT   (1445kb,D)

Title: MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large
  Language Models
Authors: Yilin Wen, Zifeng Wang, Jimeng Sun
Categories: cs.AI cs.CL cs.LG
Comments: 8 pages, 8 figures, 12 tables
\\ ( https://arxiv.org/abs/2308.09729 ,  1445kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10216
replaced with revised version Sat, 2 Mar 2024 15:49:21 GMT   (239kb,D)

Title: Safe POMDP Online Planning via Shielding
Authors: Shili Sheng, David Parker and Lu Feng
Categories: cs.AI
\\ ( https://arxiv.org/abs/2309.10216 ,  239kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05588
replaced with revised version Sun, 3 Mar 2024 09:11:38 GMT   (20421kb,D)

Title: Language-assisted Vision Model Debugger: A Sample-Free Approach to
  Finding and Fixing Bugs
Authors: Chaoquan Jiang, Jinqiang Wang, Rui Hu, Jitao Sang
Categories: cs.AI cs.CV
Comments: 10 pages,8 figures
\\ ( https://arxiv.org/abs/2312.05588 ,  20421kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09851
replaced with revised version Sat, 2 Mar 2024 03:24:06 GMT   (4444kb,D)

Title: Sophisticated Behavioral Simulation: A Possible Solution to Problems of
  Organized Complexity
Authors: Cheng Wang, Chuwen Wang, Yu Zhao, Wang Zhang, Shirong Zeng, Ronghui
  Ning, Changjun Jiang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2401.09851 ,  4444kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12624
replaced with revised version Sun, 3 Mar 2024 14:15:52 GMT   (2944kb,D)

Title: Knowledge Distillation from Language-Oriented to Emergent Communication
  for Multi-Agent Remote Control
Authors: Yongjun Kim, Sejin Seo, Jihong Park, Mehdi Bennis, Seong-Lyun Kim,
  Junil Choi
Categories: cs.AI cs.IT cs.LG cs.NI math.IT
\\ ( https://arxiv.org/abs/2401.12624 ,  2944kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01499
replaced with revised version Mon, 4 Mar 2024 12:52:13 GMT   (6089kb,D)

Title: Developing and Evaluating a Design Method for Positive Artificial
  Intelligence
Authors: Willem van der Maden, Derek Lomas, Paul Hekkert
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.01499 ,  6089kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03181
replaced with revised version Sun, 3 Mar 2024 18:13:54 GMT   (14509kb,D)

Title: C-RAG: Certified Generation Risks for Retrieval-Augmented Language
  Models
Authors: Mintong Kang, Nezihe Merve G\"urel, Ning Yu, Dawn Song, Bo Li
Categories: cs.AI cs.CL cs.IR
\\ ( https://arxiv.org/abs/2402.03181 ,  14509kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07234
replaced with revised version Sun, 3 Mar 2024 01:26:01 GMT   (701kb,D)

Title: CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for
  Chinese Public Security Domain
Authors: Xin Tong, Bo Jin, Zhi Lin, Binjun Wang, Ting Yu and Qiang Cheng
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.07234 ,  701kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07787
replaced with revised version Mon, 4 Mar 2024 08:42:32 GMT   (8194kb,D)

Title: Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment
  Analysis
Authors: Xiaowei Zhao, Yong Zhou, Xiujuan Xu, Yu Liu
Categories: cs.AI cs.CL
Comments: 8 pages, 4 figures
\\ ( https://arxiv.org/abs/2402.07787 ,  8194kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08128
replaced with revised version Sat, 2 Mar 2024 03:01:57 GMT   (159kb,D)

Title: Recursive Joint Simulation in Games
Authors: Vojtech Kovarik, Caspar Oesterheld, Vincent Conitzer
Categories: cs.AI cs.GT
\\ ( https://arxiv.org/abs/2402.08128 ,  159kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08939
replaced with revised version Mon, 4 Mar 2024 09:21:16 GMT   (7955kb,D)

Title: Premise Order Matters in Reasoning with Large Language Models
Authors: Xinyun Chen, Ryan A. Chi, Xuezhi Wang, Denny Zhou
Categories: cs.AI cs.CL
Comments: Xinyun and Ryan contribute equally
\\ ( https://arxiv.org/abs/2402.08939 ,  7955kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09099
replaced with revised version Mon, 4 Mar 2024 11:22:38 GMT   (25126kb,D)

Title: Exploring Neuron Interactions and Emergence in LLMs: From the
  Multifractal Analysis Perspective
Authors: Xiongye Xiao, Chenyu Zhou, Heng Ping, Defu Cao, Yaxing Li, Yizhuo
  Zhou, Shixuan Li, Paul Bogdan
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.09099 ,  25126kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11403
replaced with revised version Sun, 3 Mar 2024 22:07:50 GMT   (1226kb,D)

Title: An Empirical Evaluation of Neural and Neuro-symbolic Approaches to
  Real-time Multimodal Complex Event Detection
Authors: Liying Han, Mani B. Srivastava
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.11403 ,  1226kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15140
replaced with revised version Sat, 2 Mar 2024 04:59:36 GMT   (4116kb,D)

Title: A Relation-Interactive Approach for Message Passing in Hyper-relational
  Knowledge Graphs
Authors: Yonglin Jing
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.15140 ,  4116kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18679
replaced with revised version Mon, 4 Mar 2024 18:58:37 GMT   (36808kb,D)

Title: Data Interpreter: An LLM Agent For Data Science
Authors: Sirui Hong, Yizhang Lin, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi
  Chen, Jiayi Zhang, Jinlin Wang, Lingyao Zhang, Mingchen Zhuge, Taicheng Guo,
  Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang, Xiangtao Lu, Xinbing Liang,
  Yaying Fei, Yuheng Cheng, Zongze Xu, Chenglin Wu, Li Zhang, Min Yang, Xiawu
  Zheng
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.18679 ,  36808kb)
------------------------------------------------------------------------------
\\
arXiv:2208.00463
replaced with revised version Sun, 3 Mar 2024 11:27:26 GMT   (343kb,D)

Title: Mismatching-Aware Unsupervised Translation Quality Estimation For
  Low-Resource Languages
Authors: Fatemeh Azadi, Heshaam Faili, Mohammad Javad Dousti
Categories: cs.CL
Comments: Submitted to Language Resources and Evaluation
\\ ( https://arxiv.org/abs/2208.00463 ,  343kb)
------------------------------------------------------------------------------
\\
arXiv:2210.04870
replaced with revised version Mon, 4 Mar 2024 03:38:14 GMT   (408kb,D)

Title: SMiLE: Schema-augmented Multi-level Contrastive Learning for Knowledge
  Graph Link Prediction
Authors: Miao Peng, Ben Liu, Qianqian Xie, Wenjie Xu, Hua Wang, Min Peng
Categories: cs.CL cs.AI
Comments: Findings of EMNLP 2022
\\ ( https://arxiv.org/abs/2210.04870 ,  408kb)
------------------------------------------------------------------------------
\\
arXiv:2212.03827
replaced with revised version Sat, 2 Mar 2024 21:33:53 GMT   (5120kb,D)

Title: Discovering Latent Knowledge in Language Models Without Supervision
Authors: Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt
Categories: cs.CL cs.AI cs.LG
Comments: ICLR 2023
\\ ( https://arxiv.org/abs/2212.03827 ,  5120kb)
------------------------------------------------------------------------------
\\
arXiv:2303.07196
replaced with revised version Sat, 2 Mar 2024 19:19:44 GMT   (151kb,D)

Title: A Comprehensive Empirical Evaluation of Existing Word Embedding
  Approaches
Authors: Obaidullah Zaland, Muhammad Abulaish, Mohd. Fazil
Categories: cs.CL cs.NE
Comments: 28 pages, 3 figures and 10 tables
\\ ( https://arxiv.org/abs/2303.07196 ,  151kb)
------------------------------------------------------------------------------
\\
arXiv:2304.02313
replaced with revised version Mon, 4 Mar 2024 12:33:46 GMT   (31009kb,D)

Title: Personality-aware Human-centric Multimodal Reasoning: A New Task,
  Dataset and Baselines
Authors: Yaochen Zhu, Xiangqing Shen, Rui Xia
Categories: cs.CL
\\ ( https://arxiv.org/abs/2304.02313 ,  31009kb)
------------------------------------------------------------------------------
\\
arXiv:2305.02483
replaced with revised version Fri, 1 Mar 2024 23:41:24 GMT   (10699kb,D)

Title: Personalized Abstractive Summarization by Tri-agent Generation Pipeline
Authors: Wen Xiao, Yujia Xie, Giuseppe Carenini, Pengcheng He
Categories: cs.CL
Comments: Accepted at EACL 2024 Findings
\\ ( https://arxiv.org/abs/2305.02483 ,  10699kb)
------------------------------------------------------------------------------
\\
arXiv:2305.07912
replaced with revised version Mon, 4 Mar 2024 03:14:25 GMT   (7152kb,D)

Title: Pre-trained Language Model with Prompts for Temporal Knowledge Graph
  Completion
Authors: Wenjie Xu, Ben Liu, Miao Peng, Xu Jia, Min Peng
Categories: cs.CL cs.AI
Comments: Accepted to Findings of ACL 2023
ACM-class: I.2.4; I.2.7
\\ ( https://arxiv.org/abs/2305.07912 ,  7152kb)
------------------------------------------------------------------------------
\\
arXiv:2307.15337
replaced with revised version Sat, 2 Mar 2024 02:45:33 GMT   (1913kb,D)

Title: Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation
Authors: Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, Yu Wang
Categories: cs.CL cs.AI
Comments: In ICLR'24
\\ ( https://arxiv.org/abs/2307.15337 ,  1913kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05576
replaced with revised version Mon, 4 Mar 2024 14:24:10 GMT   (158kb,D)

Title: Do Language Models' Words Refer?
Authors: Matthew Mandelkern and Tal Linzen
Categories: cs.CL
\\ ( https://arxiv.org/abs/2308.05576 ,  158kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04550
replaced with revised version Sun, 3 Mar 2024 18:48:02 GMT   (346kb,D)

Title: Retrieving Evidence from EHRs with LLMs: Possibilities and Challenges
Authors: Hiba Ahsan, Denis Jered McInerney, Jisoo Kim, Christopher Potter,
  Geoffrey Young, Silvio Amir, Byron C. Wallace
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.04550 ,  346kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04739
replaced with revised version Sat, 2 Mar 2024 23:14:47 GMT   (73kb)

Title: Data Augmentation for Conversational AI
Authors: Heydar Soudani, Evangelos Kanoulas and Faegheh Hasibi
Categories: cs.CL cs.IR
DOI: 10.1145/3583780.3615291
\\ ( https://arxiv.org/abs/2309.04739 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04823
replaced with revised version Sat, 2 Mar 2024 14:46:27 GMT   (784kb,D)

Title: FaNS: a Facet-based Narrative Similarity Metric
Authors: Mousumi Akter, Shubhra Kanti Karmaker Santu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.04823 ,  784kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11852
replaced with revised version Sat, 2 Mar 2024 08:50:40 GMT   (6707kb,D)

Title: Knowledge Sanitization of Large Language Models
Authors: Yoichi Ishibashi, Hidetoshi Shimodaira
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.11852 ,  6707kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02954
replaced with revised version Sat, 2 Mar 2024 14:38:24 GMT   (2523kb,D)

Title: DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for
  In-Context Learning
Authors: Jing Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze
  Xie, Zhicheng Yang, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang,
  Chengming Li, Xiaodan Liang
Categories: cs.CL
Comments: Accepted in ICLR 2024
\\ ( https://arxiv.org/abs/2310.02954 ,  2523kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06474
replaced with revised version Mon, 4 Mar 2024 04:03:54 GMT   (703kb,D)

Title: Multilingual Jailbreak Challenges in Large Language Models
Authors: Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing
Categories: cs.CL
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.06474 ,  703kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09680
replaced with revised version Mon, 4 Mar 2024 04:37:35 GMT   (1599kb,D)

Title: Improved Contextual Recognition In Automatic Speech Recognition Systems
  By Semantic Lattice Rescoring
Authors: Ankitha Sudarshan, Vinay Samuel, Parth Patwa, Ibtihel Amara, Aman
  Chadha
Categories: cs.CL cs.AI cs.SD eess.AS
\\ ( https://arxiv.org/abs/2310.09680 ,  1599kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11053
replaced with revised version Mon, 4 Mar 2024 07:14:10 GMT   (1481kb,D)

Title: Denevil: Towards Deciphering and Navigating the Ethical Values of Large
  Language Models via Instruction Learning
Authors: Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, Ning Gu
Categories: cs.CL cs.AI cs.CY
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2310.11053 ,  1481kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05112
replaced with revised version Sun, 3 Mar 2024 01:15:36 GMT   (652kb,D)

Title: A Survey of Large Language Models in Medicine: Progress, Application,
  and Challenge
Authors: Hongjian Zhou, Fenglin Liu, Boyang Gu, Xinyu Zou, Jinfa Huang, Jinge
  Wu, Yiru Li, Sam S. Chen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng
  Mao, Chenyu You, Xian Wu, Yefeng Zheng, Lei Clifton, Zheng Li, Jiebo Luo,
  David A. Clifton
Categories: cs.CL cs.AI
Comments: Preprint. Version 4. 7 figures; 13 tables; 49 pages
\\ ( https://arxiv.org/abs/2311.05112 ,  652kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06534
replaced with revised version Sat, 2 Mar 2024 17:18:34 GMT   (295kb,D)

Title: Translating Legalese: Enhancing Public Understanding of Court Opinions
  with Legal Summarizers
Authors: Elliott Ash and Aniket Kesari and Suresh Naidu and Lena Song and
  Dominik Stammbach
Categories: cs.CL
Comments: published in proceedings of CSLAW 2024: Symposium on Computer Science
  and Law
DOI: 10.1145/3614407.3643700
\\ ( https://arxiv.org/abs/2311.06534 ,  295kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09033
replaced with revised version Sun, 3 Mar 2024 06:37:11 GMT   (677kb,D)

Title: MELA: Multilingual Evaluation of Linguistic Acceptability
Authors: Ziyin Zhang and Yikang Liu and Weifang Huang and Junyu Mao and Rui
  Wang and Hai Hu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2311.09033 ,  677kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09800
replaced with revised version Mon, 4 Mar 2024 14:35:59 GMT   (8543kb,D)

Title: $\textit{Dial BeInfo for Faithfulness}$: Improving Factuality of
  Information-Seeking Dialogue via Behavioural Fine-Tuning
Authors: Evgeniia Razumovskaia, Ivan Vuli\'c, Pavle Markovi\'c, Tomasz Cichy,
  Qian Zheng, Tsung-Hsien Wen, Pawe{\l} Budzianowski
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.09800 ,  8543kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01050
replaced with revised version Sat, 2 Mar 2024 21:53:37 GMT   (5649kb,D)

Title: Detection and Analysis of Stress-Related Posts in Reddit Acamedic
  Communities
Authors: Nazzere Oryngozha and Pakizar Shamoi and Ayan Igali
Categories: cs.CL
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
Journal-ref: IEEE Access, vol. 12, pp. 14932-14948, 2024
DOI: 10.1109/ACCESS.2024.3357662
\\ ( https://arxiv.org/abs/2312.01050 ,  5649kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01714
replaced with revised version Sun, 3 Mar 2024 06:12:44 GMT   (3841kb,D)

Title: Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large
  Language Models
Authors: Bingshuai Liu, Chenyang Lyu, Zijun Min, Zhanyu Wang, Jinsong Su,
  Longyue Wang
Categories: cs.CL
Comments: Work in progress
\\ ( https://arxiv.org/abs/2312.01714 ,  3841kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02436
replaced with revised version Mon, 4 Mar 2024 04:12:02 GMT   (2374kb,D)

Title: MUFFIN: Curating Multi-Faceted Instructions for Improving
  Instruction-Following
Authors: Renze Lou, Kai Zhang, Jian Xie, Yuxuan Sun, Janice Ahn, Hanzi Xu, Yu
  Su, Wenpeng Yin
Categories: cs.CL cs.AI
Comments: ICLR 2024. Data, model, and code are available at:
  https://renzelou.github.io/Muffin/
\\ ( https://arxiv.org/abs/2312.02436 ,  2374kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11361
replaced with revised version Mon, 4 Mar 2024 16:32:10 GMT   (477kb,D)

Title: NoMIRACL: Knowing When You Don't Know for Robust Multilingual
  Retrieval-Augmented Generation
Authors: Nandan Thakur, Luiz Bonifacio, Xinyu Zhang, Odunayo Ogundepo, Ehsan
  Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Boxing Chen, Mehdi
  Rezagholizadeh, Jimmy Lin
Categories: cs.CL cs.IR
\\ ( https://arxiv.org/abs/2312.11361 ,  477kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13103
replaced with revised version Sun, 3 Mar 2024 21:06:33 GMT   (711kb)

Title: Exploring Multimodal Large Language Models for Radiology Report
  Error-checking
Authors: Jinge Wu, Yunsoo Kim, Eva C. Keller, Jamie Chow, Adam P. Levine,
  Nikolas Pontikos, Zina Ibrahim, Paul Taylor, Michelle C. Williams, Honghan Wu
Categories: cs.CL cs.CV
\\ ( https://arxiv.org/abs/2312.13103 ,  711kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17242
replaced with revised version Mon, 4 Mar 2024 16:00:23 GMT   (951kb,D)

Title: Learning to Generate Text in Arbitrary Writing Styles
Authors: Aleem Khan, Andrew Wang, Sophia Hager, Nicholas Andrews
Categories: cs.CL
\\ ( https://arxiv.org/abs/2312.17242 ,  951kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06509
replaced with revised version Mon, 4 Mar 2024 13:16:53 GMT   (10742kb,D)

Title: AntEval: Quantitatively Evaluating Informativeness and Expressiveness of
  Agent Social Interactions
Authors: Yuanzhi Liang, Linchao Zhu, Yi Yang
Categories: cs.CL
Comments: Preliminary version of an ongoing work
\\ ( https://arxiv.org/abs/2401.06509 ,  10742kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06915
replaced with revised version Thu, 29 Feb 2024 19:55:14 GMT   (7703kb,D)

Title: DocFinQA: A Long-Context Financial Reasoning Dataset
Authors: Varshini Reddy, Rik Koncel-Kedziorski, Viet Dac Lai, Michael Krumdick,
  Charles Lovering, Chris Tanner
Categories: cs.CL cs.AI
Comments: 13 pages
\\ ( https://arxiv.org/abs/2401.06915 ,  7703kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02564
replaced with revised version Sat, 2 Mar 2024 16:33:32 GMT   (440kb,D)

Title: A Truly Joint Neural Architecture for Segmentation and Parsing
Authors: Danit Yshaayahu Levi and Reut Tsarfaty
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.02564 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03223
replaced with revised version Mon, 4 Mar 2024 15:31:48 GMT   (220kb,D)

Title: English Prompts are Better for NLI-based Zero-Shot Emotion
  Classification than Target-Language Prompts
Authors: Patrick Barrei{\ss} and Roman Klinger and Jeremy Barnes
Categories: cs.CL
Comments: accepted to the PromptEng workshop at The Web Conf
\\ ( https://arxiv.org/abs/2402.03223 ,  220kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05044
replaced with revised version Mon, 4 Mar 2024 07:20:31 GMT   (12089kb,D)

Title: SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large
  Language Models
Authors: Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin,
  Yu Qiao, Jing Shao
Categories: cs.CL cs.AI cs.CR cs.LG
Comments: fix institution typo
\\ ( https://arxiv.org/abs/2402.05044 ,  12089kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09282
replaced with revised version Sun, 3 Mar 2024 15:01:55 GMT   (1048kb,D)

Title: Distilling Large Language Models into Tiny Models for Named Entity
  Recognition
Authors: Yining Huang
Categories: cs.CL
Comments: 16 pages, 3 figures
\\ ( https://arxiv.org/abs/2402.09282 ,  1048kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11094
replaced with revised version Sat, 2 Mar 2024 14:01:04 GMT   (8846kb,D)

Title: Word Embeddings Revisited: Do LLMs Offer Something New?
Authors: Matthew Freestone and Shubhra Kanti Karmaker Santu
Categories: cs.CL
Comments: 7 pages, 4 figures
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2402.11094 ,  8846kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11924
replaced with revised version Sun, 3 Mar 2024 02:23:19 GMT   (1642kb,D)

Title: MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition
Authors: Jian Wu, Linyi Yang, Manabu Okumura, Yue Zhang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.11924 ,  1642kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12151
replaced with revised version Sun, 3 Mar 2024 20:06:24 GMT   (11260kb,D)

Title: Transformer-based Causal Language Models Perform Clustering
Authors: Xinbo Wu, Lav R. Varshney
Categories: cs.CL cs.AI
Comments: Added new experimental results and fixed some errors
\\ ( https://arxiv.org/abs/2402.12151 ,  11260kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15302
replaced with revised version Mon, 4 Mar 2024 18:50:19 GMT   (616kb,D)

Title: How (un)ethical are instruction-centric responses of LLMs? Unveiling the
  vulnerabilities of safety guardrails to harmful queries
Authors: Somnath Banerjee, Sayan Layek, Rima Hazra, Animesh Mukherjee
Categories: cs.CL cs.CR
Comments: Under review.
  {https://huggingface.co/datasets/SoftMINER-Group/TechHazardQA}
\\ ( https://arxiv.org/abs/2402.15302 ,  616kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15708
replaced with revised version Sun, 3 Mar 2024 09:18:07 GMT   (3507kb,D)

Title: Query Augmentation by Decoding Semantics from Brain Signals
Authors: Ziyi Ye, Jingtao Zhan, Qingyao Ai, Yiqun Liu, Maarten de Rijke,
  Christina Lioma, Tuukka Ruotsalo
Categories: cs.CL cs.AI cs.IR
\\ ( https://arxiv.org/abs/2402.15708 ,  3507kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16061
replaced with revised version Mon, 4 Mar 2024 13:37:48 GMT   (1348kb,D)

Title: How Large Language Models Encode Context Knowledge? A Layer-Wise Probing
  Study
Authors: Tianjie Ju, Weiwei Sun, Wei Du, Xinwei Yuan, Zhaochun Ren, Gongshen
  Liu
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024 (Long Paper)
\\ ( https://arxiv.org/abs/2402.16061 ,  1348kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16063
replaced with revised version Mon, 4 Mar 2024 01:53:35 GMT   (731kb,D)

Title: Citation-Enhanced Generation for LLM-based Chatbots
Authors: Weitao Li, Junkai Li, Weizhi Ma, Yang Liu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.16063 ,  731kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16107
replaced with revised version Sun, 3 Mar 2024 07:21:36 GMT   (243kb,D)

Title: FuseChat: Knowledge Fusion of Chat Models
Authors: Fanqi Wan, Ziyi Yang, Longguang Zhong, Xiaojun Quan, Xinting Huang,
  Wei Bi
Categories: cs.CL
Comments: Technical Report, work in progress
\\ ( https://arxiv.org/abs/2402.16107 ,  243kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16379
replaced with revised version Mon, 4 Mar 2024 03:14:11 GMT   (5006kb,D)

Title: Improving LLM-based Machine Translation with Systematic Self-Correction
Authors: Zhaopeng Feng, Yan Zhang, Hao Li, Wenqiang Liu, Jun Lang, Yang Feng,
  Jian Wu, Zuozhu Liu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.16379 ,  5006kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17256
replaced with revised version Mon, 4 Mar 2024 06:04:32 GMT   (1228kb,D)

Title: Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent
  Detection
Authors: Pei Wang, Keqing He, Yejie Wang, Xiaoshuai Song, Yutao Mou, Jingang
  Wang, Yunsen Xian, Xunliang Cai, Weiran Xu
Categories: cs.CL
Journal-ref: LREC-COLING 2024
\\ ( https://arxiv.org/abs/2402.17256 ,  1228kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17411
replaced with revised version Sat, 2 Mar 2024 14:08:06 GMT   (0kb,I)

Title: Consistency Matters: Explore LLMs Consistency From a Black-Box
  Perspective
Authors: Fufangchen Zhao, Guoqiang Jin, Jiaheng Huang, Rui Zhao and Fei Tan
Categories: cs.CL
Comments: This paper is not ready
\\ ( https://arxiv.org/abs/2402.17411 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17512
replaced with revised version Mon, 4 Mar 2024 12:21:52 GMT   (862kb,D)

Title: Latent Attention for Linear Time Transformers
Authors: Rares Dolga, Marius Cobzarenco, David Barber
Categories: cs.CL stat.ML
\\ ( https://arxiv.org/abs/2402.17512 ,  862kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17759
replaced with revised version Sun, 3 Mar 2024 15:07:50 GMT   (2172kb,D)

Title: Towards Optimal Learning of Language Models
Authors: Yuxian Gu, Li Dong, Yaru Hao, Qingxiu Dong, Minlie Huang, Furu Wei
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.17759 ,  2172kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17887
replaced with revised version Sat, 2 Mar 2024 09:03:18 GMT   (1792kb,D)

Title: JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning
  and Professional Question Answering Capability
Authors: Junda Wang, Zhichao Yang, Zonghai Yao, Hong Yu
Categories: cs.CL cs.IR
\\ ( https://arxiv.org/abs/2402.17887 ,  1792kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17897
replaced with revised version Mon, 4 Mar 2024 14:06:42 GMT   (300kb,D)

Title: A Language Model based Framework for New Concept Placement in Ontologies
Authors: Hang Dong, Jiaoyan Chen, Yuan He, Yongsheng Gao, Ian Horrocks
Categories: cs.CL cs.IR
Comments: 20 pages, 3 figures, accepted for ESWC 2024
ACM-class: I.2.7; I.2.4
\\ ( https://arxiv.org/abs/2402.17897 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18243
replaced with revised version Sat, 2 Mar 2024 08:28:14 GMT   (483kb,D)

Title: Learning or Self-aligning? Rethinking Instruction Fine-tuning
Authors: Mengjie Ren, Boxi Cao, Hongyu Lin, Cao Liu, Xianpei Han, Ke Zeng,
  Guanglu Wan, Xunliang Cai, Le Sun
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.18243 ,  483kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18284
replaced with revised version Sat, 2 Mar 2024 23:19:27 GMT   (6813kb,D)

Title: Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of
  Pre-trained Language Models with Proximal Policy Optimization
Authors: Shuo Yang and Gjergji Kasneci
Categories: cs.CL cs.AI
Comments: 12 pages, 2 figures
\\ ( https://arxiv.org/abs/2402.18284 ,  6813kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19116
replaced with revised version Mon, 4 Mar 2024 08:42:56 GMT   (1116kb,D)

Title: How to Understand "Support"? An Implicit-enhanced Causal Inference
  Approach for Weakly-supervised Phrase Grounding
Authors: Jiamin Luo, Jianing Zhao, Jingjing Wang, Guodong Zhou
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.19116 ,  1116kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19248
replaced with revised version Sat, 2 Mar 2024 04:37:37 GMT   (8119kb,D)

Title: Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question
  Answering Benchmark
Authors: Zhikun Xu, Yinghui Li, Ruixue Ding, Xinyu Wang, Boli Chen, Yong Jiang,
  Hai-Tao Zheng, Wenlian Lu, Pengjun Xie, Fei Huang
Categories: cs.CL
Comments: Work in progress!
\\ ( https://arxiv.org/abs/2402.19248 ,  8119kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19282
replaced with revised version Mon, 4 Mar 2024 12:30:10 GMT   (124kb,D)

Title: WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset
Authors: Jiantao Qiu, Haijun Lv, Zhenjiang Jin, Rui Wang, Wenchang Ning, Jia
  Yu, ChaoBin Zhang, Pei Chu, Yuan Qu, Jin Shi, Lindong Lu, Runyu Peng, Zhiyuan
  Zeng, Huanze Tang, Zhikai Lei, Jiawei Hong, Keyu Chen, Zhaoye Fei, Ruiliang
  Xu, Wei Li, Zhongyin Tu, Hang Yan and Conghui He
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.19282 ,  124kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19406
replaced with revised version Mon, 4 Mar 2024 11:35:02 GMT   (10140kb,D)

Title: On the Scaling Laws of Geographical Representation in Language Models
Authors: Nathan Godey, \'Eric de la Clergerie, Beno\^it Sagot
Categories: cs.CL cs.AI
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2402.19406 ,  10140kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00510
replaced with revised version Mon, 4 Mar 2024 06:36:01 GMT   (773kb,D)

Title: ROME: Memorization Insights from Text, Probability and Hidden State in
  Large Language Models
Authors: Bo Li and Qinghua Zhao and Lijie Wen
Categories: cs.CL cs.AI
Comments: Submitted to ACL, 2024
\\ ( https://arxiv.org/abs/2403.00510 ,  773kb)
------------------------------------------------------------------------------
\\
arXiv:2102.06202
replaced with revised version Sun, 3 Mar 2024 06:47:19 GMT   (724kb,D)

Title: Private Prediction Sets
Authors: Anastasios N. Angelopoulos and Stephen Bates and Tijana Zrnic and
  Michael I. Jordan
Categories: cs.LG cs.AI cs.CR stat.ME stat.ML
Comments: Code available at
  https://github.com/aangelopoulos/private_prediction_sets
Journal-ref: Harvard Data Science Review, 4(2). 2022
DOI: 10.1162/99608f92.16c71dad
\\ ( https://arxiv.org/abs/2102.06202 ,  724kb)
------------------------------------------------------------------------------
\\
arXiv:2102.09407
replaced with revised version Mon, 4 Mar 2024 15:22:32 GMT   (4870kb,D)

Title: Adaptive Rational Activations to Boost Deep Reinforcement Learning
Authors: Quentin Delfosse, Patrick Schramowski, Martin Mundt, Alejandro Molina
  and Kristian Kersting
Categories: cs.LG
Comments: Main paper: 9 pages, References: 4 pages, Appendix: 11 pages. Main
  paper: 5 figures, Appendix: 6 figures, 6 tables. Rational Activation
  Functions repository: https://github.com/k4ntz/activation-functions Rational
  Reinforcement Learning: https://github.com/ml-research/rational_rl
\\ ( https://arxiv.org/abs/2102.09407 ,  4870kb)
------------------------------------------------------------------------------
\\
arXiv:2105.13937
replaced with revised version Sat, 2 Mar 2024 12:55:56 GMT   (1895kb,D)

Title: Polygonal Unadjusted Langevin Algorithms: Creating stable and efficient
  adaptive algorithms for neural networks
Authors: Dong-Young Lim and Sotirios Sabanis
Categories: cs.LG math.OC math.PR stat.ML
\\ ( https://arxiv.org/abs/2105.13937 ,  1895kb)
------------------------------------------------------------------------------
\\
arXiv:2109.09307
replaced with revised version Sat, 2 Mar 2024 10:29:42 GMT   (10300kb,D)

Title: Assisted Learning for Organizations with Limited Imbalanced Data
Authors: Cheng Chen, Jiaying Zhou, Jie Ding, Yi Zhou
Categories: cs.LG
Comments: Published in Transactions on Machine Learning Research (TMLR)
  (05/2023)
Journal-ref: C. Chen, J. Zhou, J. Ding, and Y. Zhou, "Assisted Learning for
  Organizations with Limited Imbalanced Data," Transactions on Machine Learning
  Research (TMLR), 2023
\\ ( https://arxiv.org/abs/2109.09307 ,  10300kb)
------------------------------------------------------------------------------
\\
arXiv:2201.11967
replaced with revised version Mon, 4 Mar 2024 11:12:38 GMT   (2746kb,D)

Title: Pseudo-Differential Neural Operator: Generalized Fourier Neural Operator
  for Learning Solution Operators of Partial Differential Equations
Authors: Jin Young Shin, Jae Yong Lee, Hyung Ju Hwang
Categories: cs.LG cs.NA math.NA
Comments: 23 pages, 13 figures
MSC-class: 35S05, 47G30, 68U07
\\ ( https://arxiv.org/abs/2201.11967 ,  2746kb)
------------------------------------------------------------------------------
\\
arXiv:2204.00961
replaced with revised version Mon, 4 Mar 2024 05:38:44 GMT   (20193kb)

Title: Enhancing Digital Health Services: A Machine Learning Approach to
  Personalized Exercise Goal Setting
Authors: Ji Fang, Vincent CS Lee, Hao Ji, Haiyan Wang
Categories: cs.LG math.OC
DOI: 10.1177/20552076241233247
\\ ( https://arxiv.org/abs/2204.00961 ,  20193kb)
------------------------------------------------------------------------------
\\
arXiv:2207.07827
replaced with revised version Mon, 4 Mar 2024 05:48:45 GMT   (7453kb,D)

Title: CLMFormer: Mitigating Data Redundancy to Revitalize Transformer-based
  Long-Term Time Series Forecasting System
Authors: Mingjie Li, Rui Liu, Guangsi Shi, Mingfei Han, Changling Li, Lina Yao,
  Xiaojun Chang, and Ling Chen
Categories: cs.LG cs.CV
Comments: Tech report
\\ ( https://arxiv.org/abs/2207.07827 ,  7453kb)
------------------------------------------------------------------------------
\\
arXiv:2209.06356
replaced with revised version Sat, 2 Mar 2024 17:02:40 GMT   (3633kb,D)

Title: Using Forwards-Backwards Models to Approximate MDP Homomorphisms
Authors: Augustine N. Mavor-Parker, Matthew J. Sargent, Christian Pehle, Andrea
  Banino, Lewis D. Griffin, Caswell Barry
Categories: cs.LG cs.AI
Comments: Previously Presented at the Multi-disciplinary Conference on
  Reinforcement Learning and Decision Making (RLDM) 2022
\\ ( https://arxiv.org/abs/2209.06356 ,  3633kb)
------------------------------------------------------------------------------
\\
arXiv:2210.02419
replaced with revised version Mon, 4 Mar 2024 06:20:41 GMT   (14067kb,D)

Title: Boundary-Aware Uncertainty for Feature Attribution Explainers
Authors: Davin Hill, Aria Masoomi, Max Torop, Sandesh Ghimire, Jennifer Dy
Categories: cs.LG
Comments: Proceedings of the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024
\\ ( https://arxiv.org/abs/2210.02419 ,  14067kb)
------------------------------------------------------------------------------
\\
arXiv:2210.09475
replaced with revised version Sun, 3 Mar 2024 15:26:09 GMT   (3975kb,D)

Title: FIMP: Foundation Model-Informed Message Passing for Graph Neural
  Networks
Authors: Syed Asad Rizvi, Nhi Nguyen, Haoran Lyu, Benjamin Christensen, Josue
  Ortega Caro, Antonio H. O. Fonseca, Emanuele Zappala, Maryam Bagherian,
  Christopher Averill, Chadi G. Abdallah, Amin Karbasi, Rex Ying, Maria Brbic,
  Rahul Madhav Dhodapkar, David van Dijk
Categories: cs.LG
Comments: 16 pages (11 + 5 pages appendix). 5 figures and 7 tables
\\ ( https://arxiv.org/abs/2210.09475 ,  3975kb)
------------------------------------------------------------------------------
\\
arXiv:2211.10209
replaced with revised version Sun, 3 Mar 2024 14:58:54 GMT   (5091kb,D)

Title: Leveraging Algorithmic Fairness to Mitigate Blackbox Attribute Inference
  Attacks
Authors: Jan Aalmoes and Vasisht Duddu and Antoine Boutet
Categories: cs.LG cs.CR
Comments: arXiv admin note: text overlap with arXiv:2202.02242
\\ ( https://arxiv.org/abs/2211.10209 ,  5091kb)
------------------------------------------------------------------------------
\\
arXiv:2301.09308
replaced with revised version Sun, 3 Mar 2024 20:32:24 GMT   (747kb,D)

Title: On the Expressive Power of Geometric Graph Neural Networks
Authors: Chaitanya K. Joshi, Cristian Bodnar, Simon V. Mathis, Taco Cohen,
  Pietro Li\`o
Categories: cs.LG math.GR stat.ML
Comments: ICML 2023
Journal-ref: Proceedings of the 40th International Conference on Machine
  Learning, PMLR 202:15330-15355, 2023
\\ ( https://arxiv.org/abs/2301.09308 ,  747kb)
------------------------------------------------------------------------------
\\
arXiv:2302.04947
replaced with revised version Sun, 3 Mar 2024 04:33:34 GMT   (583kb,D)

Title: Gaussian Process-Gated Hierarchical Mixtures of Experts
Authors: Yuhao Liu, Marzieh Ajirak, Petar Djuric
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2302.04947 ,  583kb)
------------------------------------------------------------------------------
\\
arXiv:2302.11640
replaced with revised version Sat, 2 Mar 2024 21:17:13 GMT   (51kb)

Title: A critical look at the evaluation of GNNs under heterophily: Are we
  really making progress?
Authors: Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko,
  Liudmila Prokhorenkova
Categories: cs.LG
\\ ( https://arxiv.org/abs/2302.11640 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2303.15495
replaced with revised version Mon, 4 Mar 2024 15:42:31 GMT   (19271kb,D)

Title: Real-Time Bus Arrival Prediction: A Deep Learning Approach for Enhanced
  Urban Mobility
Authors: Narges Rashvand, Sanaz Sadat Hosseini, Mona Azarbayjani, Hamed Tabkhi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2303.15495 ,  19271kb)
------------------------------------------------------------------------------
\\
arXiv:2305.10738
replaced with revised version Sat, 2 Mar 2024 06:32:03 GMT   (512kb,D)

Title: Deep Temporal Graph Clustering
Authors: Meng Liu, Yue Liu, Ke Liang, Wenxuan Tu, Siwei Wang, Sihang Zhou,
  Xinwang Liu
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2305.10738 ,  512kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16147
replaced with revised version Sat, 2 Mar 2024 00:37:59 GMT   (1170kb,D)

Title: Learning Safety Constraints from Demonstrations with Unknown Rewards
Authors: David Lindner, Xin Chen, Sebastian Tschiatschek, Katja Hofmann,
  Andreas Krause
Categories: cs.LG cs.AI stat.ML
Comments: Presented at the International Conference on Artificial Intelligence
  and Statistics (AISTATS) 2024
\\ ( https://arxiv.org/abs/2305.16147 ,  1170kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00603
replaced with revised version Mon, 4 Mar 2024 14:20:17 GMT   (700kb,D)

Title: Safe Offline Reinforcement Learning with Real-Time Budget Constraints
Authors: Qian Lin, Bo Tang, Zifan Wu, Chao Yu, Shangqin Mao, Qianlong Xie,
  Xingxing Wang, Dong Wang
Categories: cs.LG cs.AI cs.RO
Comments: We propose a method to handle the constraint problem with dynamically
  determined safety budgets under the offline setting
\\ ( https://arxiv.org/abs/2306.00603 ,  700kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02010
replaced with revised version Sat, 2 Mar 2024 07:50:37 GMT   (142kb,D)

Title: Memorization Capacity of Multi-Head Attention in Transformers
Authors: Sadegh Mahdavi, Renjie Liao, Christos Thrampoulidis
Categories: cs.LG
Comments: ICLR 2024 (Spotlight)
\\ ( https://arxiv.org/abs/2306.02010 ,  142kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03928
replaced with revised version Sun, 3 Mar 2024 09:38:27 GMT   (4678kb,D)

Title: Designing Decision Support Systems Using Counterfactual Prediction Sets
Authors: Eleni Straitouri and Manuel Gomez Rodriguez
Categories: cs.LG cs.CY cs.HC stat.ME stat.ML
Comments: Best paper award in the ICML 2023 AI&HCI Workshop
\\ ( https://arxiv.org/abs/2306.03928 ,  4678kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04815
replaced with revised version Sun, 3 Mar 2024 00:28:01 GMT   (31098kb,D)

Title: Catapults in SGD: spikes in the training loss and their impact on
  generalization through feature learning
Authors: Libin Zhu, Chaoyue Liu, Adityanarayanan Radhakrishnan, Mikhail Belkin
Categories: cs.LG math.OC stat.ML
\\ ( https://arxiv.org/abs/2306.04815 ,  31098kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09910
replaced with revised version Fri, 1 Mar 2024 22:38:16 GMT   (561kb,D)

Title: LabelBench: A Comprehensive Framework for Benchmarking Adaptive
  Label-Efficient Learning
Authors: Jifan Zhang, Yifang Chen, Gregory Canal, Stephen Mussmann, Arnav M.
  Das, Gantavya Bhatt, Yinglun Zhu, Jeffrey Bilmes, Simon Shaolei Du, Kevin
  Jamieson, Robert D Nowak
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2306.09910 ,  561kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10356
replaced with revised version Sat, 2 Mar 2024 08:51:25 GMT   (1967kb,D)

Title: MATNet: Multi-Level Fusion Transformer-Based Model for Day-Ahead PV
  Generation Forecasting
Authors: Matteo Tortora, Francesco Conte, Gianluca Natrella, Paolo Soda
Categories: cs.LG cs.AI eess.SP
\\ ( https://arxiv.org/abs/2306.10356 ,  1967kb)
------------------------------------------------------------------------------
\\
arXiv:2306.15924
replaced with revised version Fri, 1 Mar 2024 22:01:50 GMT   (263kb)

Title: The Parametric Complexity of Operator Learning
Authors: Samuel Lanthaler and Andrew M. Stuart
Categories: cs.LG cs.NA math.NA
\\ ( https://arxiv.org/abs/2306.15924 ,  263kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02932
replaced with revised version Sat, 2 Mar 2024 16:44:00 GMT   (375kb,D)

Title: When No-Rejection Learning is Consistent for Regression with Rejection
Authors: Xiaocheng Li, Shang Liu, Chunlin Sun, Hanzhao Wang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2307.02932 ,  375kb)
------------------------------------------------------------------------------
\\
arXiv:2307.05435
replaced with revised version Mon, 4 Mar 2024 15:57:33 GMT   (4708kb,D)

Title: One-Versus-Others Attention: Scalable Multimodal Integration for
  Clinical Data
Authors: Michal Golovanevsky, Eva Schiller, Akira Nair, Ritambhara Singh,
  Carsten Eickhoff
Categories: cs.LG
\\ ( https://arxiv.org/abs/2307.05435 ,  4708kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12306
replaced with revised version Sun, 3 Mar 2024 12:03:05 GMT   (652kb,D)

Title: Tackling the Curse of Dimensionality with Physics-Informed Neural
  Networks
Authors: Zheyuan Hu, Khemraj Shukla, George Em Karniadakis, Kenji Kawaguchi
Categories: cs.LG cs.AI cs.NA math.DS math.NA stat.ML
Comments: 35 pages
MSC-class: 14J60
ACM-class: F.2.2; I.2.7
\\ ( https://arxiv.org/abs/2307.12306 ,  652kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13586
replaced with revised version Sun, 3 Mar 2024 07:09:47 GMT   (133kb)

Title: Settling the Sample Complexity of Online Reinforcement Learning
Authors: Zihan Zhang, Yuxin Chen, Jason D. Lee, Simon S. Du
Categories: cs.LG
\\ ( https://arxiv.org/abs/2307.13586 ,  133kb)
------------------------------------------------------------------------------
\\
arXiv:2308.03812
replaced with revised version Sun, 3 Mar 2024 11:15:35 GMT   (149kb,D)

Title: Noncompact uniform universal approximation
Authors: Teun D. H. van Nuland
Categories: cs.LG math.FA math.OA
Comments: 13 pages, 3 figures
MSC-class: 68T07, 46N10, 26B40
ACM-class: I.2.6
DOI: 10.1016/j.neunet.2024.106181
\\ ( https://arxiv.org/abs/2308.03812 ,  149kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07233
replaced with revised version Mon, 4 Mar 2024 02:39:58 GMT   (1779kb,D)

Title: A Unifying Generator Loss Function for Generative Adversarial Networks
Authors: Justin Veiner, Fady Alajaji, Bahman Gharesifard
Categories: cs.LG
Comments: 31 pages, 4 figures, 12 tables
\\ ( https://arxiv.org/abs/2308.07233 ,  1779kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00255
replaced with revised version Sun, 3 Mar 2024 05:26:03 GMT   (3078kb,D)

Title: SortedNet, a Place for Every Network and Every Network in its Place:
  Towards a Generalized Solution for Training Many-in-One Neural Networks
Authors: Mojtaba Valipour, Mehdi Rezagholizadeh, Hossein Rajabzadeh, Parsa
  Kavehzadeh, Marzieh Tahaei, Boxing Chen, and Ali Ghodsi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.00255 ,  3078kb)
------------------------------------------------------------------------------
\\
arXiv:2309.03720
replaced with revised version Mon, 4 Mar 2024 13:52:35 GMT   (5848kb,D)

Title: A Natural Gas Consumption Forecasting System for Continual Learning
  Scenarios based on Hoeffding Trees with Change Point Detection Mechanism
Authors: Radek Svoboda, Sebastian Basterrech, J\k{e}drzej Kozal, Jan
  Plato\v{s}, Micha{\l} Wo\'zniak
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2309.03720 ,  5848kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05019
replaced with revised version Mon, 4 Mar 2024 10:05:53 GMT   (33769kb,D)

Title: SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models
Authors: Shuchen Xue, Mingyang Yi, Weijian Luo, Shifeng Zhang, Jiacheng Sun,
  Zhenguo Li, Zhi-Ming Ma
Categories: cs.LG stat.ML
Comments: Accepted in Neurips 2023
\\ ( https://arxiv.org/abs/2309.05019 ,  33769kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06634
replaced with revised version Mon, 4 Mar 2024 12:08:53 GMT   (8728kb,D)

Title: $G$-Mapper: Learning a Cover in the Mapper Construction
Authors: Enrique Alvarado, Robin Belton, Emily Fischer, Kang-Ju Lee, Sourabh
  Palande, Sarah Percival, Emilie Purvine
Categories: cs.LG math.AT stat.ML
\\ ( https://arxiv.org/abs/2309.06634 ,  8728kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13150
replaced with revised version Sun, 3 Mar 2024 02:54:34 GMT   (4592kb,D)

Title: Pixel-wise Smoothing for Certified Robustness against Camera Motion
  Perturbations
Authors: Hanjiang Hu, Zuxin Liu, Linyi Li, Jiacheng Zhu, Ding Zhao
Categories: cs.LG cs.CV cs.RO
Comments: Camera-ready version of AISTATS 2024, 30 pages, 5 figures, 13 tables
\\ ( https://arxiv.org/abs/2309.13150 ,  4592kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15135
replaced with revised version Mon, 4 Mar 2024 05:05:24 GMT   (15652kb,D)

Title: Contrastive Continual Multi-view Clustering with Filtered Structural
  Fusion
Authors: Xinhang Wan, Jiyuan Liu, Hao Yu, Ao Li, Xinwang Liu, Ke Liang, Zhibin
  Dong, En Zhu
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2309.15135 ,  15652kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16739
replaced with revised version Mon, 4 Mar 2024 12:17:16 GMT   (5104kb,D)

Title: Pushing Large Language Models to the 6G Edge: Vision, Challenges, and
  Opportunities
Authors: Zheng Lin, Guanqiao Qu, Qiyuan Chen, Xianhao Chen, Zhe Chen and Kaibin
  Huang
Categories: cs.LG cs.AI
Comments: 7 pages, 5 figures
\\ ( https://arxiv.org/abs/2309.16739 ,  5104kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02207
replaced with revised version Mon, 4 Mar 2024 18:25:29 GMT   (6793kb,D)

Title: Language Models Represent Space and Time
Authors: Wes Gurnee, Max Tegmark
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2310.02207 ,  6793kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04418
replaced with revised version Sun, 3 Mar 2024 02:13:27 GMT   (254kb,D)

Title: Functional Interpolation for Relative Positions Improves Long Context
  Transformers
Authors: Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago
  Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, Srinadh
  Bhojanapalli
Categories: cs.LG
Comments: 26 pages; ICLR 2024 camera ready version
\\ ( https://arxiv.org/abs/2310.04418 ,  254kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05696
replaced with revised version Mon, 4 Mar 2024 16:31:46 GMT   (432kb,D)

Title: Protecting Sensitive Data through Federated Co-Training
Authors: Amr Abourayya and Jens Kleesiek and Kanishka Rao and Erman Ayday and
  Bharat Rao and Geoff Webb and Michael Kamp
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.05696 ,  432kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06543
replaced with revised version Sat, 2 Mar 2024 07:53:46 GMT   (14495kb,D)

Title: An Edge-Aware Graph Autoencoder Trained on Scale-Imbalanced Data for
  Traveling Salesman Problems
Authors: Shiqing Liu, Xueming Yan, Yaochu Jin
Categories: cs.LG
Comments: 37 pages, 7 figures. Accepted by Knowledge-based Systems
\\ ( https://arxiv.org/abs/2310.06543 ,  14495kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07171
replaced with revised version Sun, 3 Mar 2024 07:58:38 GMT   (1230kb,D)

Title: Advocating for the Silent: Enhancing Federated Generalization for
  Non-Participating Clients
Authors: Zheshun Wu, Zenglin Xu, Dun Zeng, Qifan Wang, Jie Liu
Categories: cs.LG cs.IT math.IT
\\ ( https://arxiv.org/abs/2310.07171 ,  1230kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08031
replaced with revised version Mon, 4 Mar 2024 02:57:11 GMT   (176kb,D)

Title: Local Graph Clustering with Noisy Labels
Authors: Artur Back de Luca, Kimon Fountoulakis, Shenghao Yang
Categories: cs.LG cs.SI stat.ML
Comments: 30 pages, 5 figures, 18 tables
\\ ( https://arxiv.org/abs/2310.08031 ,  176kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08224
replaced with revised version Fri, 1 Mar 2024 19:16:48 GMT   (526kb,D)

Title: Emergence of Latent Binary Encoding in Deep Neural Network Classifiers
Authors: Luigi Sbail\`o and Luca Ghiringhelli
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.08224 ,  526kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10012
replaced with revised version Mon, 4 Mar 2024 07:20:58 GMT   (10443kb,D)

Title: Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion
  Models?
Authors: Yu-Lin Tsai, Chia-Yi Hsu, Chulin Xie, Chih-Hsun Lin, Jia-You Chen, Bo
  Li, Pin-Yu Chen, Chia-Mu Yu, Chun-Ying Huang
Categories: cs.LG
Comments: This paper has already been accepted by ICLR 2024. This version is
  the camera-ready version
\\ ( https://arxiv.org/abs/2310.10012 ,  10443kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10207
replaced with revised version Sun, 3 Mar 2024 13:20:10 GMT   (35980kb,D)

Title: Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in
  the Real World
Authors: Rujie Wu, Xiaojian Ma, Zhenliang Zhang, Wei Wang, Qing Li, Song-Chun
  Zhu, Yizhou Wang
Categories: cs.LG
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2310.10207 ,  35980kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11684
replaced with revised version Sat, 2 Mar 2024 01:07:36 GMT   (207kb,D)

Title: Quantum Speedups in Regret Analysis of Infinite Horizon Average-Reward
  Markov Decision Processes
Authors: Bhargav Ganguly and Yang Xu and Vaneet Aggarwal
Categories: cs.LG cs.AI quant-ph
\\ ( https://arxiv.org/abs/2310.11684 ,  207kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11984
replaced with revised version Sun, 3 Mar 2024 09:19:51 GMT   (1672kb,D)

Title: From Interpolation to Extrapolation: Complete Length Generalization for
  Arithmetic Transformers
Authors: Shaoxiong Duan, Yining Shi, Wei Xu
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2310.11984 ,  1672kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13822
replaced with revised version Sun, 3 Mar 2024 01:38:40 GMT   (173kb,D)

Title: Adversarial Attacks on Fairness of Graph Neural Networks
Authors: Binchi Zhang, Yushun Dong, Chen Chen, Yada Zhu, Minnan Luo, Jundong Li
Categories: cs.LG
Comments: Accepted at ICLR 2024
\\ ( https://arxiv.org/abs/2310.13822 ,  173kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15516
replaced with revised version Sun, 3 Mar 2024 03:39:21 GMT   (76kb,D)

Title: Graph Attention-based Deep Reinforcement Learning for solving the
  Chinese Postman Problem with Load-dependent costs
Authors: Truong Son Hy, Cong Dao Tran
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.15516 ,  76kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18681
replaced with revised version Mon, 4 Mar 2024 10:29:34 GMT   (607kb,D)

Title: DySurv: Dynamic Deep Learning Model for Survival Prediction in the ICU
Authors: Munib Mesinovic, Peter Watkinson, Tingting Zhu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.18681 ,  607kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19527
replaced with revised version Sat, 2 Mar 2024 12:40:15 GMT   (34597kb,D)

Title: On the Theory of Risk-Aware Agents: Bridging Actor-Critic and Economics
Authors: Michal Nauman and Marek Cygan
Categories: cs.LG
Comments: Preprint
\\ ( https://arxiv.org/abs/2310.19527 ,  34597kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01047
replaced with revised version Sun, 3 Mar 2024 21:06:21 GMT   (844kb,D)

Title: Improving Robustness via Tilted Exponential Layer: A
  Communication-Theoretic Perspective
Authors: Bhagyashree Puranik, Ahmad Beirami, Yao Qin, Upamanyu Madhow
Categories: cs.LG cs.IT eess.SP math.IT
Comments: 27th International Conference on Artificial Intelligence and
  Statistics (AISTATS 2024)
\\ ( https://arxiv.org/abs/2311.01047 ,  844kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03764
replaced with revised version Sat, 2 Mar 2024 07:06:39 GMT   (2524kb,D)

Title: Neuro-GPT: Towards A Foundation Model for EEG
Authors: Wenhui Cui, Woojae Jeong, Philipp Th\"olke, Takfarinas Medani, Karim
  Jerbi, Anand A. Joshi, Richard M. Leahy
Categories: cs.LG eess.SP
Comments: Paper accepted by the 2024 IEEE International Symposium on Biomedical
  Imaging (ISBI)
\\ ( https://arxiv.org/abs/2311.03764 ,  2524kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07548
replaced with revised version Mon, 4 Mar 2024 00:24:53 GMT   (13537kb,D)

Title: Interpretable Fine-Tuning for Graph Neural Network Surrogate Models
Authors: Shivam Barwey and Romit Maulik
Categories: cs.LG physics.comp-ph physics.flu-dyn
\\ ( https://arxiv.org/abs/2311.07548 ,  13537kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14499
replaced with revised version Sun, 3 Mar 2024 06:22:50 GMT   (396kb,D)

Title: Hutchinson Trace Estimation for High-Dimensional and High-Order
  Physics-Informed Neural Networks
Authors: Zheyuan Hu, Zekun Shi, George Em Karniadakis, Kenji Kawaguchi
Categories: cs.LG cs.AI cs.NA math.DS math.NA stat.ML
Comments: Published in Computer Methods in Applied Mechanics and Engineering
MSC-class: 14J60
Journal-ref: Computer Methods in Applied Mechanics and Engineering, Volume 424,
  1 May 2024, 116883
DOI: 10.1016/j.cma.2024.116883
\\ ( https://arxiv.org/abs/2312.14499 ,  396kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00873
replaced with revised version Mon, 4 Mar 2024 09:24:35 GMT   (4843kb,D)

Title: A Bayesian Unification of Self-Supervised Clustering and Energy-Based
  Models
Authors: Emanuele Sansone and Robin Manhaeve
Categories: cs.LG cs.CV
Comments: Changes from previous version: added mean and standard deviations in
  experiments. Integral version of workshop paper arXiv:2309.15420. Improved
  GEDI version (from two stages to single stage training) arxiv:2212.13425
\\ ( https://arxiv.org/abs/2401.00873 ,  4843kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03790
replaced with revised version Sat, 2 Mar 2024 11:15:00 GMT   (10766kb,D)

Title: Inferring Properties of Graph Neural Networks
Authors: Dat Nguyen (1), Hieu M. Vu (2), Cong-Thanh Le (1), Bach Le (1), David
  Lo (3), ThanhVu Nguyen (4) Corina Pasareanu (5) ((1) University of Melbourne,
  (2) Independent Researcher, (3) Singapore Management University, (4) George
  Mason University, (5) Carnegie Mellon University)
Categories: cs.LG cs.CR cs.PL cs.SE
Comments: 20 pages main paper, 10 pages for appendix
\\ ( https://arxiv.org/abs/2401.03790 ,  10766kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04331
replaced with revised version Mon, 4 Mar 2024 05:57:06 GMT   (1421kb,D)

Title: Coupling Graph Neural Networks with Fractional Order Continuous
  Dynamics: A Robustness Study
Authors: Qiyu Kang, Kai Zhao, Yang Song, Yihang Xie, Yanan Zhao, Sijie Wang,
  Rui She, and Wee Peng Tay
Categories: cs.LG cs.AI
Comments: in Proc. AAAI Conference on Artificial Intelligence, Vancouver,
  Canada, Feb. 2024
\\ ( https://arxiv.org/abs/2401.04331 ,  1421kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04837
replaced with revised version Mon, 4 Mar 2024 18:56:12 GMT   (6478kb,D)

Title: T-PRIME: Transformer-based Protocol Identification for Machine-learning
  at the Edge
Authors: Mauro Belgiovine, Joshua Groen, Miquel Sirera, Chinenye Tassie, Ayberk
  Yark{\i}n Y{\i}ld{\i}z, Sage Trudeau, Stratis Ioannidis, Kaushik Chowdhury
Categories: cs.LG cs.NI cs.SY eess.SY
Comments: This is the extended version of the IEEE INFOCOM 2024 paper with the
  same title
\\ ( https://arxiv.org/abs/2401.04837 ,  6478kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05531
replaced with revised version Fri, 1 Mar 2024 21:49:23 GMT   (648kb)

Title: VI-PANN: Harnessing Transfer Learning and Uncertainty-Aware Variational
  Inference for Improved Generalization in Audio Pattern Recognition
Authors: John Fischer, Marko Orescanin, Eric Eckstrand
Categories: cs.LG cs.SD eess.AS
Comments: Published in IEEE Access
Journal-ref: IEEE Access (2024)
DOI: 10.1109/ACCESS.2024.3372423
\\ ( https://arxiv.org/abs/2401.05531 ,  648kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06040
replaced with revised version Mon, 4 Mar 2024 15:53:51 GMT   (477kb,D)

Title: Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for
  Traffic Forecasting
Authors: Qipeng Qian, Tanwi Mallick
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.06040 ,  477kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11202
replaced with revised version Mon, 4 Mar 2024 04:23:32 GMT   (1079kb,D)

Title: PartIR: Composing SPMD Partitioning Strategies for Machine Learning
Authors: Sami Alabed, Daniel Belov, Bart Chrzaszcz, Juliana Franco, Dominik
  Grewe, Dougal Maclaurin, James Molloy, Tom Natan, Tamara Norman, Xiaoyue Pan,
  Adam Paszke, Norman A. Rink, Michael Schaarschmidt, Timur Sitdikov, Agnieszka
  Swietlik, Dimitrios Vytiniotis, Joel Wee
Categories: cs.LG cs.DC cs.PL
\\ ( https://arxiv.org/abs/2401.11202 ,  1079kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11667
replaced with revised version Mon, 4 Mar 2024 03:08:53 GMT   (3461kb,D)

Title: INCPrompt: Task-Aware incremental Prompting for Rehearsal-Free
  Class-incremental Learning
Authors: Zhiyuan Wang, Xiaoyang Qu, Jing Xiao, Bokui Chen, Jianzong Wang
Categories: cs.LG
Comments: Accepted by the 49th IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP 2024)
\\ ( https://arxiv.org/abs/2401.11667 ,  3461kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14112
replaced with revised version Mon, 4 Mar 2024 02:30:21 GMT   (2147kb,D)

Title: FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric
  Algorithm-System Co-Design
Authors: Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen
  Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji
  Ruwase, Yuxiong He, Shuaiwen Leon Song
Categories: cs.LG cs.AI cs.AR
Comments: Adding URL link of the source code
\\ ( https://arxiv.org/abs/2401.14112 ,  2147kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15030
replaced with revised version Mon, 4 Mar 2024 15:23:16 GMT   (7535kb,D)

Title: On the generalization capacity of neural networks during generic
  multimodal reasoning
Authors: Takuya Ito, Soham Dan, Mattia Rigotti, James Kozloski, Murray Campbell
Categories: cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2401.15030 ,  7535kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15422
replaced with revised version Mon, 4 Mar 2024 16:55:15 GMT   (540kb,D)

Title: A Survey on Data Augmentation in Large Model Era
Authors: Yue Zhou, Chenlu Guo, Xu Wang, Yi Chang, and Yuan Wu
Categories: cs.LG cs.CL cs.CV
Comments: 33 pages; https://github.com/MLGroup-JLU/LLM-data-aug-survey
\\ ( https://arxiv.org/abs/2401.15422 ,  540kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16193
replaced with revised version Sat, 2 Mar 2024 08:27:52 GMT   (1786kb,D)

Title: Contributing Dimension Structure of Deep Feature for Coreset Selection
Authors: Zhijing Wan, Zhixiang Wang, Yuran Wang, Zheng Wang, Hongyuan Zhu,
  Shin'ichi Satoh
Categories: cs.LG cs.DB
Comments: 13 pages,11 figures, to be published in AAAI2024
\\ ( https://arxiv.org/abs/2401.16193 ,  1786kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16549
replaced with revised version Sun, 3 Mar 2024 19:47:58 GMT   (1345kb)

Title: Deep Learning for Multi-Label Learning: A Comprehensive Survey
Authors: Adane Nega Tarekegn, Mohib Ullah, Faouzi Alaya Cheikh
Categories: cs.LG cs.CV
Comments: 21 pages, 12 figures, 5 tables
\\ ( https://arxiv.org/abs/2401.16549 ,  1345kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18018
replaced with revised version Mon, 4 Mar 2024 06:31:21 GMT   (3015kb,D)

Title: On Prompt-Driven Safeguarding for Large Language Models
Authors: Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei
  Chang, Minlie Huang, Nanyun Peng
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2401.18018 ,  3015kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01440
replaced with revised version Sat, 2 Mar 2024 08:27:26 GMT   (866kb,D)

Title: Few-Shot Learning on Graphs: from Meta-learning to Pre-training and
  Prompting
Authors: Xingtong Yu, Yuan Fang, Zemin Liu, Yuxia Wu, Zhihao Wen, Jianyuan Bo,
  Xinming Zhang and Steven C.H. Hoi
Categories: cs.LG cs.AI cs.SI
\\ ( https://arxiv.org/abs/2402.01440 ,  866kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02042
replaced with revised version Sun, 3 Mar 2024 08:25:18 GMT   (53kb)

Title: Learning General Parameterized Policies for Infinite Horizon Average
  Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm
Authors: Qinbo Bai, Washim Uddin Mondal, Vaneet Aggarwal
Categories: cs.LG cs.AI
Comments: We fixed Lemma 6 in v2 which changed the final result
\\ ( https://arxiv.org/abs/2402.02042 ,  53kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02446
replaced with revised version Mon, 4 Mar 2024 12:18:47 GMT   (396kb,D)

Title: LQER: Low-Rank Quantization Error Reconstruction for LLMs
Authors: Cheng Zhang, Jianyi Cheng, George A. Constantinides, and Yiren Zhao
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2402.02446 ,  396kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03496
replaced with revised version Mon, 4 Mar 2024 03:21:47 GMT   (647kb,D)

Title: Can We Remove the Square-Root in Adaptive Gradient Methods? A
  Second-Order Perspective
Authors: Wu Lin, Felix Dangel, Runa Eschenhagen, Juhan Bae, Richard E. Turner,
  Alireza Makhzani
Categories: cs.LG math.OC
Comments: updated Sec. 3 & 4
\\ ( https://arxiv.org/abs/2402.03496 ,  647kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05011
replaced with revised version Sat, 2 Mar 2024 11:16:07 GMT   (3659kb,D)

Title: Navigating Complexity: Toward Lossless Graph Condensation via Expanding
  Window Matching
Authors: Yuchen Zhang and Tianle Zhang and Kai Wang and Ziyao Guo and Yuxuan
  Liang and Xavier Bresson and Wei Jin and Yang You
Categories: cs.LG
Comments: Lossless graph condensation method
\\ ( https://arxiv.org/abs/2402.05011 ,  3659kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05663
replaced with revised version Mon, 4 Mar 2024 12:01:53 GMT   (15849kb,D)

Title: Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave
  Prediction
Authors: Raphael Chekroun, Han Wang, Jonathan Lee, Marin Toromanoff, Sascha
  Hornauer, Fabien Moutarde, Maria Laura Delle Monache
Categories: cs.LG cs.AI cs.RO
\\ ( https://arxiv.org/abs/2402.05663 ,  15849kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05967
replaced with revised version Mon, 4 Mar 2024 11:39:55 GMT   (3630kb,D)

Title: The last Dance : Robust backdoor attack via diffusion models and
  bayesian approach
Authors: Orson Mengara
Categories: cs.LG cs.AI cs.CR eess.SP
Comments: Preprint: audio backdoor attack on Hugging Face's Transformer
  pre-trained models. This attack incorporates state-of-the-art Bayesian
  techniques, a modified Fokker-Planck equation, and a diffusion model
  approach. To alert the community to this danger!
\\ ( https://arxiv.org/abs/2402.05967 ,  3630kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06104
replaced with revised version Sun, 3 Mar 2024 19:04:08 GMT   (2136kb,D)

Title: Function Aligned Regression: A Method Explicitly Learns Functional
  Derivatives from Data
Authors: Dixian Zhu and Livnat Jerby
Categories: cs.LG cs.AI
Comments: 24 pages excluding references, 12 figures, 4 tables
\\ ( https://arxiv.org/abs/2402.06104 ,  2136kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09702
replaced with revised version Mon, 4 Mar 2024 17:32:32 GMT   (11046kb,D)

Title: Sparse and Faithful Explanations Without Sparse Models
Authors: Yiyang Sun, Zhi Chen, Vittorio Orlandi, Tong Wang, Cynthia Rudin
Categories: cs.LG stat.ML
Comments: Accepted in AISTATS 2024
\\ ( https://arxiv.org/abs/2402.09702 ,  11046kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09782
replaced with revised version Mon, 4 Mar 2024 06:10:09 GMT   (4811kb,D)

Title: MC-DBN: A Deep Belief Network-Based Model for Modality Completion
Authors: Zihong Luo, Kexin He, Chengzhi Liu, Zheng Tao
Categories: cs.LG cs.AI
Journal-ref: International Conference on Computer Supported Cooperative Work in
  Design 2024
\\ ( https://arxiv.org/abs/2402.09782 ,  4811kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09963
replaced with revised version Sun, 3 Mar 2024 08:42:07 GMT   (1572kb,D)

Title: Why are Sensitive Functions Hard for Transformers?
Authors: Michael Hahn, Mark Rofin
Categories: cs.LG
Comments: Fixed various errors
\\ ( https://arxiv.org/abs/2402.09963 ,  1572kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10991
replaced with revised version Mon, 4 Mar 2024 03:35:40 GMT   (388kb)

Title: Enhancing Convergence in Federated Learning: A Contribution-Aware
  Asynchronous Approach
Authors: Changxin Xu, Yuxin Qiao, Zhanxin Zhou, Fanghao Ni, and Jize Xiong
Categories: cs.LG cs.AI
Comments: 5 pages, 1 figures
\\ ( https://arxiv.org/abs/2402.10991 ,  388kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11253
replaced with revised version Sun, 3 Mar 2024 21:37:16 GMT   (370kb,D)

Title: Aligning Large Language Models by On-Policy Self-Judgment
Authors: Sangkyu Lee, Sungdong Kim, Ashkan Yousefpour, Minjoon Seo, Kang Min
  Yoo, Youngjae Yu
Categories: cs.LG cs.AI cs.CL
Comments: Preprint; Under review
\\ ( https://arxiv.org/abs/2402.11253 ,  370kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11837
replaced with revised version Sat, 2 Mar 2024 05:43:05 GMT   (2174kb,D)

Title: Self-Guided Robust Graph Structure Refinement
Authors: Yeonjun In, Kanghoon Yoon, Kibum Kim, Kijung Shin, and Chanyoung Park
Categories: cs.LG
Comments: This paper has been accepted by TheWebConf 2024 (Oral Presentation)
\\ ( https://arxiv.org/abs/2402.11837 ,  2174kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12694
replaced with revised version Mon, 4 Mar 2024 04:18:57 GMT   (10708kb,D)

Title: Revitalizing Multivariate Time Series Forecasting: Learnable
  Decomposition with Inter-Series Dependencies and Intra-Series Variations
  Modeling
Authors: Guoqi Yu, Jing Zou, Xiaowei Hu, Angelica I. Aviles-Rivero, Jing Qin
  and Shujun Wang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.12694 ,  10708kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13711
replaced with revised version Sun, 3 Mar 2024 08:01:54 GMT   (1321kb,D)

Title: DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based
  Graph Continual Learning
Authors: Seungyoon Choi, Wonjoong Kim, Sungwon Kim, Yeonjun In, Sein Kim,
  Chanyoung Park
Categories: cs.LG cs.AI
Comments: Accepted at ACM TheWebConf 2024 (WWW 2024) (Oral presentation)
DOI: 10.1145/3589334.3645561
\\ ( https://arxiv.org/abs/2402.13711 ,  1321kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14041
replaced with revised version Mon, 4 Mar 2024 14:01:08 GMT   (4322kb)

Title: E2USD: Efficient-yet-effective Unsupervised State Detection for
  Multivariate Time Series
Authors: Zhichen Lai, Huan Li, Dalin Zhang, Yan Zhao, Weizhu Qian, Christian S.
  Jensen
Categories: cs.LG cs.AI cs.DB
Comments: Accepted by The Web Conference 2024 (WWW 2024)
\\ ( https://arxiv.org/abs/2402.14041 ,  4322kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14139
replaced with revised version Mon, 4 Mar 2024 17:47:32 GMT   (1592kb,D)

Title: NeuroFlux: Memory-Efficient CNN Training Using Adaptive Local Learning
Authors: Dhananjay Saikumar and Blesson Varghese
Categories: cs.LG
Comments: Accepted to EuroSys 2024
\\ ( https://arxiv.org/abs/2402.14139 ,  1592kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14547
replaced with revised version Mon, 4 Mar 2024 16:54:25 GMT   (1889kb,D)

Title: OmniPred: Language Models as Universal Regressors
Authors: Xingyou Song, Oscar Li, Chansoo Lee, Bangding Yang, Daiyi Peng, Sagi
  Perel, Yutian Chen
Categories: cs.LG cs.AI cs.CL cs.DB
Comments: 24 pages, 10 figures. Code can be found in
  https://github.com/google-research/optformer/tree/main/optformer/omnipred
\\ ( https://arxiv.org/abs/2402.14547 ,  1889kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15883
replaced with revised version Mon, 4 Mar 2024 17:24:11 GMT   (48kb,D)

Title: Fusion Encoder Networks
Authors: Stephen Pasteris, Chris Hicks, Vasilios Mavroudis
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.15883 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17747
replaced with revised version Sun, 3 Mar 2024 02:06:42 GMT   (855kb,D)

Title: When Your AIs Deceive You: Challenges with Partial Observability of
  Human Evaluators in Reward Learning
Authors: Leon Lang, Davis Foote, Stuart Russell, Anca Dragan, Erik Jenner,
  Scott Emmons
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2402.17747 ,  855kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18007
replaced with revised version Sat, 2 Mar 2024 03:32:40 GMT   (870kb,D)

Title: Mixer is more than just a model
Authors: Qingfeng Ji, Yuxin Wang, Letong Sun
Categories: cs.LG cs.AI cs.SD eess.AS
\\ ( https://arxiv.org/abs/2402.18007 ,  870kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18607
replaced with revised version Mon, 4 Mar 2024 03:51:35 GMT   (5994kb,D)

Title: Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An
  Adversarial Perspective
Authors: Xinjian Luo, Yangfan Jiang, Fei Wei, Yuncheng Wu, Xiaokui Xiao, Beng
  Chin Ooi
Categories: cs.LG cs.AI cs.CR
\\ ( https://arxiv.org/abs/2402.18607 ,  5994kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18839
replaced with revised version Sun, 3 Mar 2024 05:28:11 GMT   (13468kb,D)

Title: Extended Flow Matching: a Method of Conditional Generation with
  Generalized Continuity Equation
Authors: Noboru Isobe, Masanori Koyama, Kohei Hayashi and Kenji Fukumizu
Categories: cs.LG math.AP math.FA math.OC math.PR
Comments: 15 pages, 4 figures
MSC-class: 68T07 (Primary), 49Q22 (Secondary)
\\ ( https://arxiv.org/abs/2402.18839 ,  13468kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19047
replaced with revised version Mon, 4 Mar 2024 11:09:49 GMT   (398kb,D)

Title: Theoretical Foundations of Deep Selective State-Space Models
Authors: Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, Cristopher Salvi
  and Terry Lyons
Categories: cs.LG math.DS
\\ ( https://arxiv.org/abs/2402.19047 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:1907.01743 (*cross-listing*)
replaced with revised version Sun, 3 Mar 2024 11:57:48 GMT   (5731kb,D)

Title: Deep Attentive Features for Prostate Segmentation in 3D Transrectal
  Ultrasound
Authors: Yi Wang, Haoran Dou, Xiaowei Hu, Lei Zhu, Xin Yang, Ming Xu, Jing Qin,
  Pheng-Ann Heng, Tianfu Wang, and Dong Ni
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: 11 pages, 10 figures, 2 tables. Accepted by IEEE transactions on
  Medical Imaging
DOI: 10.1109/TMI.2019.2913184
\\ ( https://arxiv.org/abs/1907.01743 ,  5731kb)
------------------------------------------------------------------------------
\\
arXiv:2203.15149
replaced with revised version Sun, 3 Mar 2024 15:32:30 GMT   (451kb,D)

Title: CMGAN: Conformer-based Metric GAN for Speech Enhancement
Authors: Ruizhe Cao, Sherif Abdulatif, Bin Yang
Categories: cs.SD cs.AI cs.LG eess.AS
Comments: 5 pages, 1 figure, 2 tables, published in INTERSPEECH 2022
DOI: 10.21437/Interspeech.2022-517
\\ ( https://arxiv.org/abs/2203.15149 ,  451kb)
------------------------------------------------------------------------------
\\
arXiv:2204.05798
replaced with revised version Mon, 4 Mar 2024 16:43:14 GMT   (11534kb,D)

Title: Multi-View Hypercomplex Learning for Breast Cancer Screening
Authors: Eleonora Lopez, Eleonora Grassucci, Martina Valleriani, Danilo
  Comminiello
Categories: cs.CV cs.AI cs.LG
Comments: This paper has been submitted to IEEE Transactions on Pattern
  Analysis and Machine Intelligence
\\ ( https://arxiv.org/abs/2204.05798 ,  11534kb)
------------------------------------------------------------------------------
\\
arXiv:2207.06492
replaced with revised version Sat, 2 Mar 2024 14:54:10 GMT   (11594kb,D)

Title: Approximate Nash Equilibrium Learning for n-Player Markov Games in
  Dynamic Pricing
Authors: Larkin Liu
Categories: cs.GT cs.AI
\\ ( https://arxiv.org/abs/2207.06492 ,  11594kb)
------------------------------------------------------------------------------
\\
arXiv:2211.07160
replaced with revised version Sat, 2 Mar 2024 11:52:16 GMT   (398kb,D)

Title: FedTracker: Furnishing Ownership Verification and Traceability for
  Federated Learning Model
Authors: Shuo Shao, Wenyuan Yang, Hanlin Gu, Zhan Qin, Lixin Fan, Qiang Yang
  and Kui Ren
Categories: cs.CR cs.AI cs.LG
\\ ( https://arxiv.org/abs/2211.07160 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2211.14298
replaced with revised version Sun, 3 Mar 2024 10:51:36 GMT   (21431kb,D)

Title: PIP: Positional-encoding Image Prior
Authors: Nimrod Shabtay, Eli Schwartz and Raja Giryes
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2211.14298 ,  21431kb)
------------------------------------------------------------------------------
\\
arXiv:2303.17386
replaced with revised version Mon, 4 Mar 2024 18:06:33 GMT   (47531kb,D)

Title: Complementary Random Masking for RGB-Thermal Semantic Segmentation
Authors: Ukcheol Shin, Kyunghyun Lee, In So Kweon, Jean Oh
Categories: cs.CV cs.AI cs.RO
Comments: ICRA 2024, Our source code is available at
  https://github.com/UkcheolShin/CRM_RGBTSeg
\\ ( https://arxiv.org/abs/2303.17386 ,  47531kb)
------------------------------------------------------------------------------
\\
arXiv:2304.06607
replaced with revised version Sat, 2 Mar 2024 07:14:47 GMT   (2520kb,D)

Title: False Claims against Model Ownership Resolution
Authors: Jian Liu, Rui Zhang, Sebastian Szyller, Kui Ren, N. Asokan
Categories: cs.CR cs.AI
Comments: 13pages,3 figures. To appear in the 33rd USENIX Security Symposium
  (USENIX Security '24)
\\ ( https://arxiv.org/abs/2304.06607 ,  2520kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15086
replaced with revised version Sat, 2 Mar 2024 12:47:22 GMT   (35486kb,D)

Title: Unpaired Image-to-Image Translation via Neural Schr\"odinger Bridge
Authors: Beomsu Kim, Gihyun Kwon, Kwanyoung Kim, Jong Chul Ye
Categories: cs.CV cs.AI cs.LG stat.ML
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2305.15086 ,  35486kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18352
replaced with revised version Mon, 4 Mar 2024 10:22:48 GMT   (2044kb,D)

Title: Multi-Objective Genetic Algorithm for Multi-View Feature Selection
Authors: Vandad Imani, Carlos Sevilla-Salcedo, Elaheh Moradi, Vittorio Fortino,
  and Jussi Tohka
Categories: cs.NE cs.AI cs.LG
\\ ( https://arxiv.org/abs/2305.18352 ,  2044kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05499
replaced with revised version Sat, 2 Mar 2024 09:12:23 GMT   (729kb,D)

Title: Prompt Injection attack against LLM-integrated Applications
Authors: Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang, Xiaofeng
  Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng and Yang Liu
Categories: cs.CR cs.AI cs.CL cs.SE
\\ ( https://arxiv.org/abs/2306.05499 ,  729kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06022
replaced with revised version Mon, 4 Mar 2024 06:11:51 GMT   (990kb)

Title: Dynamic Partial Computation Offloading for the Metaverse in In-Network
  Computing
Authors: Ibrahim Aliyu, Seungmin Oh, Namseok Ko, Tai-Won Um, Jinsul Kim
Categories: cs.DC cs.AI cs.GT
Comments: 14 pages, 9 figures
DOI: 10.1109/ACCESS.2023.3344817
\\ ( https://arxiv.org/abs/2306.06022 ,  990kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08018 (*cross-listing*)
replaced with revised version Mon, 4 Mar 2024 12:49:31 GMT   (17523kb,D)

Title: Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for
  Large Language Models
Authors: Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo
  Chen, Xiaohui Fan, Huajun Chen
Categories: q-bio.QM cs.AI cs.CE cs.CL cs.IR cs.LG
Comments: ICLR 2024. Project homepage:
  https://github.com/zjunlp/Mol-Instructions
\\ ( https://arxiv.org/abs/2306.08018 ,  17523kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08175 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 21:25:16 GMT   (113kb,D)

Title: DCTX-Conformer: Dynamic context carry-over for low latency unified
  streaming and non-streaming Conformer ASR
Authors: Goeric Huybrechts, Srikanth Ronanki, Xilai Li, Hadis Nosrati, Sravan
  Bodapati, Katrin Kirchhoff
Categories: eess.AS cs.AI cs.LG cs.SD
\\ ( https://arxiv.org/abs/2306.08175 ,  113kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02561
replaced with revised version Sun, 3 Mar 2024 08:12:36 GMT   (12155kb,D)

Title: Physically Grounded Vision-Language Models for Robotic Manipulation
Authors: Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian
  Ichter, Anirudha Majumdar, Dorsa Sadigh
Categories: cs.RO cs.AI cs.CV
Comments: Updated version for ICRA 2024
\\ ( https://arxiv.org/abs/2309.02561 ,  12155kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00433
replaced with revised version Mon, 4 Mar 2024 12:31:46 GMT   (24248kb,D)

Title: Active-Perceptive Motion Generation for Mobile Manipulation
Authors: Snehal Jauhri, Sophie Lueth, Georgia Chalvatzaki
Categories: cs.RO cs.AI
Comments: ICRA 2024. Project page: https://sites.google.com/view/actpermoma
\\ ( https://arxiv.org/abs/2310.00433 ,  24248kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03234 (*cross-listing*)
replaced with revised version Sun, 3 Mar 2024 18:59:18 GMT   (626kb,D)

Title: Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization
Authors: Quanqi Hu, Dixian Zhu, Tianbao Yang
Categories: math.OC cs.AI cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.03234 ,  626kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05207
replaced with revised version Mon, 4 Mar 2024 12:42:07 GMT   (1715kb,D)

Title: Boosting Facial Action Unit Detection Through Jointly Learning Facial
  Landmark Detection and Domain Separation and Reconstruction
Authors: Ziqiao Shang, Li Yu
Categories: cs.CV cs.AI cs.LG
Comments: 5 pages, 1 figure
\\ ( https://arxiv.org/abs/2310.05207 ,  1715kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05670
replaced with revised version Fri, 1 Mar 2024 19:20:18 GMT   (3786kb,D)

Title: Reinforcement learning for freeform robot design
Authors: Muhan Li, David Matthews, Sam Kriegman
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2310.05670 ,  3786kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09463
replaced with revised version Mon, 4 Mar 2024 04:32:11 GMT   (8166kb,D)

Title: HIO-SDF: Hierarchical Incremental Online Signed Distance Fields
Authors: Vasileios Vasilopoulos, Suveer Garg, Jinwook Huh, Bhoram Lee, Volkan
  Isler
Categories: cs.RO cs.AI
Comments: IEEE International Conference on Robotics and Automation (ICRA 2024)
  - 7 pages, 7 figures
\\ ( https://arxiv.org/abs/2310.09463 ,  8166kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00123 (*cross-listing*)
replaced with revised version Mon, 4 Mar 2024 15:59:43 GMT   (481kb,D)

Title: Q-Learning for Stochastic Control under General Information Structures
  and Non-Markovian Environments
Authors: Ali Devran Kara and Serdar Yuksel
Categories: math.OC cs.AI cs.SY eess.SY
Comments: 2 figures
\\ ( https://arxiv.org/abs/2311.00123 ,  481kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00968
replaced with revised version Mon, 4 Mar 2024 07:54:31 GMT   (5366kb,D)

Title: Video2Music: Suitable Music Generation from Videos using an Affective
  Multimodal Transformer model
Authors: Jaeyong Kang, Soujanya Poria, Dorien Herremans
Categories: cs.SD cs.AI eess.AS
\\ ( https://arxiv.org/abs/2311.00968 ,  5366kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10329
replaced with revised version Mon, 4 Mar 2024 07:28:07 GMT   (10955kb,D)

Title: High-fidelity Person-centric Subject-to-Image Synthesis
Authors: Yibin Wang and Weizhong Zhang and Jianwei Zheng and Cheng Jin
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2311.10329 ,  10955kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16119
replaced with revised version Sun, 3 Mar 2024 00:12:16 GMT   (4756kb,D)

Title: Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of
  LLMs through a Global Scale Prompt Hacking Competition
Authors: Sander Schulhoff, Jeremy Pinto, Anaum Khan, Louis-Fran\c{c}ois
  Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Liu Kost,
  Christopher Carnahan, Jordan Boyd-Graber
Categories: cs.CR cs.AI cs.CL
Comments: 34 pages, 8 figures Codebase:
  https://github.com/PromptLabs/hackaprompt Dataset:
  https://huggingface.co/datasets/hackaprompt/hackaprompt-dataset/blob/main/README.md
  Playground: https://huggingface.co/spaces/hackaprompt/playground
\\ ( https://arxiv.org/abs/2311.16119 ,  4756kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08519 (*cross-listing*)
replaced with revised version Sat, 2 Mar 2024 02:39:35 GMT   (4737kb)

Title: Reconciling Shared versus Context-Specific Information in a Neural
  Network Model of Latent Causes
Authors: Qihong Lu, Tan T. Nguyen, Qiong Zhang, Uri Hasson, Thomas L.
  Griffiths, Jeffrey M. Zacks, Samuel J. Gershman, Kenneth A. Norman
Categories: q-bio.NC cs.AI
\\ ( https://arxiv.org/abs/2312.08519 ,  4737kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04330
replaced with revised version Sun, 3 Mar 2024 08:39:20 GMT   (10368kb,D)

Title: BD-MSA: Body decouple VHR Remote Sensing Image Change Detection method
  guided by multi-scale feature information aggregation
Authors: Yonghui Tan, Xiaolong Li, Yishu Chen and Jinquan Ai
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2401.04330 ,  10368kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13782
replaced with revised version Sun, 3 Mar 2024 08:16:52 GMT   (7567kb,D)

Title: Tweets to Citations: Unveiling the Impact of Social Media Influencers on
  AI Research Visibility
Authors: Iain Xie Weissburg, Mehir Arora, Xinyi Wang, Liangming Pan, William
  Yang Wang
Categories: cs.DL cs.AI cs.CL cs.CV cs.LG cs.SI
Comments: 10 Pages, 14 Figures
\\ ( https://arxiv.org/abs/2401.13782 ,  7567kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02733
replaced with revised version Mon, 4 Mar 2024 08:21:48 GMT   (12463kb,D)

Title: ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer
Authors: Bumsoo Kim, Abdul Muqeet, Kyuchul Lee, Sanghyun Seo
Categories: cs.CV cs.AI cs.GR cs.LG cs.MM
Comments: 8 pages, 9 figures, 1 table
\\ ( https://arxiv.org/abs/2402.02733 ,  12463kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03246
replaced with revised version Sat, 2 Mar 2024 13:49:10 GMT   (39505kb,D)

Title: SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM
Authors: Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Hongyu Wang
Categories: cs.CV cs.AI cs.RO
\\ ( https://arxiv.org/abs/2402.03246 ,  39505kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03681
replaced with revised version Sat, 2 Mar 2024 00:19:06 GMT   (4731kb,D)

Title: RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model
  Feedback
Authors: Yufei Wang, Zhanyi Sun, Jesse Zhang, Zhou Xian, Erdem Biyik, David
  Held, Zackory Erickson
Categories: cs.RO cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.03681 ,  4731kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05448
replaced with revised version Sun, 3 Mar 2024 10:02:54 GMT   (7283kb,D)

Title: Minecraft-ify: Minecraft Style Image Generation with Text-guided Image
  Editing for In-Game Application
Authors: Bumsoo Kim, Sanghyun Byun, Yonghoon Jung, Wonseop Shin, Sareer UI
  Amin, Sanghyun Seo
Categories: cs.CV cs.AI cs.GR cs.LG cs.MM
Comments: 2 pages, 2 figures. Accepted as Spotlight to NeurIPS 2023 Workshop on
  Machine Learning for Creativity and Design
\\ ( https://arxiv.org/abs/2402.05448 ,  7283kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08640
replaced with revised version Sun, 3 Mar 2024 19:08:32 GMT   (4059kb,D)

Title: Forecasting high-impact research topics via machine learning on evolving
  knowledge graphs
Authors: Xuemei Gu, Mario Krenn
Categories: cs.DL cs.AI cs.LG
Comments: 11 pages, 7 figures, Comments welcome!
\\ ( https://arxiv.org/abs/2402.08640 ,  4059kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11871
replaced with revised version Mon, 4 Mar 2024 14:52:15 GMT   (13550kb,D)

Title: From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions,
  and Models for Planning from Raw Data
Authors: Naman Shah, Jayesh Nagpal, Pulkit Verma, Siddharth Srivastava
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2402.11871 ,  13550kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12721
replaced with revised version Sat, 2 Mar 2024 10:31:40 GMT   (3676kb,D)

Title: PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for
  Recognizing Low-Quality Images
Authors: Jinsung Jeon, Hyundong Jin, Jonghyun Choi, Sanghyun Hong, Dongeun Lee,
  Kookjin Lee, Noseong Park
Categories: cs.CV cs.AI
Comments: Accepted at ICLR 2024
\\ ( https://arxiv.org/abs/2402.12721 ,  3676kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12728
replaced with revised version Sun, 3 Mar 2024 04:51:28 GMT   (402kb,D)

Title: Modality-Aware Integration with Large Language Models for
  Knowledge-based Visual Question Answering
Authors: Junnan Dong, Qinggang Zhang, Huachi Zhou, Daochen Zha, Pai Zheng, Xiao
  Huang
Categories: cs.CV cs.AI cs.CL cs.IR cs.LG
Comments: 8 pages,3 figures and 1 page appendix; The processed graphs and codes
  will be avalibale
\\ ( https://arxiv.org/abs/2402.12728 ,  402kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13226 (*cross-listing*)
replaced with revised version Sat, 2 Mar 2024 15:46:20 GMT   (6165kb,D)

Title: NeRF Solves Undersampled MRI Reconstruction
Authors: Tae Jun Jang, Chang Min Hyun
Categories: eess.IV cs.AI cs.CE eess.SP
\\ ( https://arxiv.org/abs/2402.13226 ,  6165kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13809
replaced with revised version Sat, 2 Mar 2024 05:17:51 GMT   (0kb,I)

Title: NeuralDiffuser: Controllable fMRI Reconstruction with Primary Visual
  Feature Guided Diffusion
Authors: Haoyu Li, Hao Wu, Badong Chen
Categories: cs.NE cs.AI cs.CV
Comments: The implementation error lead to incorrect results in experiment
\\ ( https://arxiv.org/abs/2402.13809 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13929
replaced with revised version Sat, 2 Mar 2024 09:09:32 GMT   (16120kb,D)

Title: SDXL-Lightning: Progressive Adversarial Diffusion Distillation
Authors: Shanchuan Lin, Anran Wang, Xiao Yang
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.13929 ,  16120kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14498
replaced with revised version Mon, 4 Mar 2024 12:56:23 GMT   (6366kb,D)

Title: A Collision-Aware Cable Grasping Method in Cluttered Environment
Authors: Lei Zhang, Kaixin Bai, Qiang Li, Zhaopeng Chen, Jianwei Zhang
Categories: cs.RO cs.AI
Comments: 7 pages
\\ ( https://arxiv.org/abs/2402.14498 ,  6366kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15727
replaced with revised version Mon, 4 Mar 2024 05:37:40 GMT   (1711kb,D)

Title: LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A
  Vision Paper
Authors: Daoyuan Wu and Shuai Wang and Yang Liu and Ning Liu
Categories: cs.CR cs.AI
Comments: Fixed the bibliography reference issue in our LLM jailbreak defense
  vision paper submitted on 24 Feb 2024
\\ ( https://arxiv.org/abs/2402.15727 ,  1711kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15943
replaced with revised version Mon, 4 Mar 2024 04:22:37 GMT   (938kb)

Title: Rethinking Software Engineering in the Foundation Model Era: A Curated
  Catalogue of Challenges in the Development of Trustworthy FMware
Authors: Ahmed E. Hassan, Dayi Lin, Gopi Krishnan Rajbahadur, Keheliya Gallaba,
  Filipe R. Cogo, Boyuan Chen, Haoxiang Zhang, Kishanthan Thangarajah, Gustavo
  Ansaldi Oliva, Jiahuei Lin, Wali Mohammad Abdullah, Zhen Ming Jiang
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2402.15943 ,  938kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16906
replaced with revised version Sat, 2 Mar 2024 00:04:53 GMT   (782kb,D)

Title: LDB: A Large Language Model Debugger via Verifying Runtime Execution
  Step-by-step
Authors: Li Zhong, Zilong Wang, Jingbo Shang
Categories: cs.SE cs.AI cs.CL
Comments: Preprint
\\ ( https://arxiv.org/abs/2402.16906 ,  782kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17862
replaced with revised version Mon, 4 Mar 2024 05:59:58 GMT   (957kb,D)

Title: REPrune: Channel Pruning via Kernel Representative Selection
Authors: Mincheol Park, Dongjin Kim, Cheonjun Park, Yuna Park, Gyeong Eun Gong,
  Won Woo Ro, Suhyun Kim
Categories: cs.CV cs.AI
Comments: Published at AAAI2024
\\ ( https://arxiv.org/abs/2402.17862 ,  957kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18205
replaced with revised version Sat, 2 Mar 2024 03:47:13 GMT   (868kb,D)

Title: Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging
Authors: Wei Zhang, Hongcheng Guo, Anjie Le, Jian Yang, Jiaheng Liu, Zhoujun
  Li, Tieqiao Zheng, Shi Xu, Runqiang Zang, Liangfan Zheng, Bo Zhang
Categories: cs.SE cs.AI
Comments: 7 pages
\\ ( https://arxiv.org/abs/2402.18205 ,  868kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18590
replaced with revised version Sat, 2 Mar 2024 19:29:40 GMT   (143kb)

Title: Exploring the Impact of Large Language Models on Recommender Systems: An
  Extensive Review
Authors: Arpita Vats, Vinija Jain, Rahul Raja, Aman Chadha
Categories: cs.IR cs.AI
\\ ( https://arxiv.org/abs/2402.18590 ,  143kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18920
replaced with revised version Mon, 4 Mar 2024 15:43:55 GMT   (43303kb,D)

Title: Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation
Authors: Dongliang Cao, Marvin Eisenberger, Nafie El Amrani, Daniel Cremers,
  Florian Bernard
Categories: cs.CV cs.AI cs.CG
Comments: accepted by CVPR2024
\\ ( https://arxiv.org/abs/2402.18920 ,  43303kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00692 (*cross-listing*)
replaced with revised version Mon, 4 Mar 2024 04:47:46 GMT   (1006kb,D)

Title: Toward Autonomous Cooperation in Heterogeneous Nanosatellite
  Constellations Using Dynamic Graph Neural Networks
Authors: Guillem Casadesus-Vila, Joan-Adria Ruiz-de-Azua, Eduard Alarcon
Categories: eess.SP cs.AI cs.NI
Comments: 8 pages, 5 figures, conference
\\ ( https://arxiv.org/abs/2403.00692 ,  1006kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15068
replaced with revised version Mon, 4 Mar 2024 17:03:42 GMT   (345kb,D)

Title: Refining GPT-3 Embeddings with a Siamese Structure for Technical Post
  Duplicate Detection
Authors: Xingfang Wu, Heng Li, Nobukazu Yoshioka, Hironori Washizaki, Foutse
  Khomh
Categories: cs.SE cs.CL cs.LG
Comments: SANER 2024
\\ ( https://arxiv.org/abs/2312.15068 ,  345kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00231
replaced with revised version Mon, 4 Mar 2024 07:01:59 GMT   (5378kb,D)

Title: Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of
  Large Vision-Language Models
Authors: Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng
  Kong, Qi Liu
Categories: cs.CV cs.CL
Comments: Project page: https://mm-arxiv.github.io Fix typos
\\ ( https://arxiv.org/abs/2403.00231 ,  5378kb)
------------------------------------------------------------------------------
\\
arXiv:1910.04331 (*cross-listing*)
replaced with revised version Sun, 3 Mar 2024 12:01:18 GMT   (4946kb,D)

Title: Agent with Warm Start and Active Termination for Plane Localization in
  3D Ultrasound
Authors: Haoran Dou, Xin Yang, Jikuan Qian, Wufeng Xue, Hao Qin, Xu Wang,
  Lequan Yu, Shujun Wang, Yi Xiong, Pheng-Ann Heng, Dong Ni
Categories: eess.IV cs.CV cs.LG
Comments: 9 pages, 5 figures, 1 table. Accepted by MICCAI 2019 (oral)
\\ ( https://arxiv.org/abs/1910.04331 ,  4946kb)
------------------------------------------------------------------------------
\\
arXiv:1910.04935
replaced with revised version Sun, 3 Mar 2024 12:08:37 GMT   (4740kb,D)

Title: FetusMap: Fetal Pose Estimation in 3D Ultrasound
Authors: Xin Yang, Wenlong Shi, Haoran Dou, Jikuan Qian, Yi Wang, Wufeng Xue,
  Shengli Li, Dong Ni, Pheng-Ann Heng
Categories: cs.CV cs.LG eess.IV
Comments: 9 pages, 6 figures, 2 tables. Accepted by MICCAI 2019
\\ ( https://arxiv.org/abs/1910.04935 ,  4740kb)
------------------------------------------------------------------------------
\\
arXiv:2204.05275 (*cross-listing*)
replaced with revised version Sun, 3 Mar 2024 15:21:25 GMT   (3424kb,D)

Title: Settling the Sample Complexity of Model-Based Offline Reinforcement
  Learning
Authors: Gen Li and Laixi Shi and Yuxin Chen and Yuejie Chi and Yuting Wei
Categories: stat.ML cs.IT cs.LG cs.SY eess.SY math.IT math.ST stat.TH
Comments: accepted to the Annals of Statistics
\\ ( https://arxiv.org/abs/2204.05275 ,  3424kb)
------------------------------------------------------------------------------
\\
arXiv:2209.06177
replaced with revised version Sat, 2 Mar 2024 22:01:00 GMT   (500kb,D)

Title: Characterizing Graph Datasets for Node Classification:
  Homophily-Heterophily Dichotomy and Beyond
Authors: Oleg Platonov, Denis Kuznedelev, Artem Babenko, Liudmila Prokhorenkova
Categories: cs.SI cs.DM cs.LG math.PR
\\ ( https://arxiv.org/abs/2209.06177 ,  500kb)
------------------------------------------------------------------------------
\\
arXiv:2210.01607 (*cross-listing*)
replaced with revised version Mon, 4 Mar 2024 17:52:04 GMT   (1623kb,D)

Title: A Generative Shape Compositional Framework to Synthesise Populations of
  Virtual Chimaeras
Authors: Haoran Dou, Seppo Virtanen, Nishant Ravikumar, Alejandro F. Frangi
Categories: eess.IV cs.CV cs.LG
Comments: 15 pages, 4 figures, 4 tables. Accepted by IEEE Transactions on
  Neural Networks and Learning Systems
\\ ( https://arxiv.org/abs/2210.01607 ,  1623kb)
------------------------------------------------------------------------------
\\
arXiv:2211.00617 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 20:42:36 GMT   (83kb,D)

Title: Convergence of policy gradient methods for finite-horizon exploratory
  linear-quadratic control problems
Authors: Michael Giegrich, Christoph Reisinger, Yufei Zhang
Categories: math.OC cs.LG
Comments: To be published in SIAM Journal on Control and Optimization
MSC-class: 68Q25, 93E20
\\ ( https://arxiv.org/abs/2211.00617 ,  83kb)
------------------------------------------------------------------------------
\\
arXiv:2211.16943 (*cross-listing*)
replaced with revised version Sun, 3 Mar 2024 06:05:21 GMT   (1262kb,D)

Title: Predicting Properties of Quantum Systems with Conditional Generative
  Models
Authors: Haoxiang Wang, Maurice Weber, Josh Izaac, Cedric Yen-Yu Lin
Categories: quant-ph cs.LG
Comments: 10 pages, 14 figures, 5 pages appendix. Open-source code is available
  at https://github.com/PennyLaneAI/generative-quantum-states
\\ ( https://arxiv.org/abs/2211.16943 ,  1262kb)
------------------------------------------------------------------------------
\\
arXiv:2212.04672 (*cross-listing*)
replaced with revised version Sun, 3 Mar 2024 02:53:27 GMT   (322kb,D)

Title: Primal Dual Alternating Proximal Gradient Algorithms for Nonsmooth
  Nonconvex Minimax Problems with Coupled Linear Constraints
Authors: Huiling Zhang, Junlin Wang, Zi Xu, Yu-Hong Dai
Categories: math.OC cs.LG stat.ML
\\ ( https://arxiv.org/abs/2212.04672 ,  322kb)
------------------------------------------------------------------------------
\\
arXiv:2212.13992
replaced with revised version Sun, 3 Mar 2024 07:37:05 GMT   (813kb,D)

Title: Social-Aware Clustered Federated Learning with Customized Privacy
  Preservation
Authors: Yuntao Wang, Zhou Su, Yanghe Pan, Tom H Luan, Ruidong Li, and Shui Yu
Categories: cs.CR cs.LG
Comments: This paper has been accepted by IEEE/ACM Transactions on Networking
  in March 2024
\\ ( https://arxiv.org/abs/2212.13992 ,  813kb)
------------------------------------------------------------------------------
\\
arXiv:2301.12318
replaced with revised version Sat, 2 Mar 2024 22:56:23 GMT   (9588kb,D)

Title: Gradient Shaping: Enhancing Backdoor Attack Against Reverse Engineering
Authors: Rui Zhu, Di Tang, Siyuan Tang, Guanhong Tao, Shiqing Ma, Xiaofeng
  Wang, Haixu Tang
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2301.12318 ,  9588kb)
------------------------------------------------------------------------------
\\
arXiv:2302.01203
replaced with revised version Sat, 2 Mar 2024 17:26:22 GMT   (79kb)

Title: Online Learning under Budget and ROI Constraints via Weak Adaptivity
Authors: Matteo Castiglioni, Andrea Celli, Christian Kroer
Categories: cs.GT cs.LG
\\ ( https://arxiv.org/abs/2302.01203 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:2302.11517 (*cross-listing*)
replaced with revised version Sat, 2 Mar 2024 07:09:37 GMT   (9264kb,D)

Title: A Global and Patch-wise Contrastive Loss for Accurate Automated Exudate
  Detection
Authors: Wei Tang, Kangning Cui, and Raymond H. Chan
Categories: eess.IV cs.CV cs.LG
Comments: 8 pages, 3 figures, 2 tables. To appear in ISBI 2024
\\ ( https://arxiv.org/abs/2302.11517 ,  9264kb)
------------------------------------------------------------------------------
\\
arXiv:2302.14186 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 20:20:09 GMT   (589kb,D)

Title: Approximately optimal domain adaptation with Fisher's Linear
  Discriminant
Authors: Hayden S. Helm and Ashwin De Silva and Joshua T. Vogelstein and Carey
  E. Priebe and Weiwei Yang
Categories: eess.SP cs.LG stat.AP stat.ME stat.ML
\\ ( https://arxiv.org/abs/2302.14186 ,  589kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13318 (*cross-listing*)
replaced with revised version Mon, 4 Mar 2024 14:31:05 GMT   (28769kb,D)

Title: A stable deep adversarial learning approach for geological facies
  generation
Authors: Ferdinand Bhavsar, Nicolas Desassis, Fabien Ors, Thomas Romary
Categories: physics.geo-ph cs.LG
\\ ( https://arxiv.org/abs/2305.13318 ,  28769kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17583 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 19:30:15 GMT   (181kb,D)

Title: On Neural Networks as Infinite Tree-Structured Probabilistic Graphical
  Models
Authors: Boyao Li, Alexandar J. Thomson, Matthew M. Engelhard, David Page
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2305.17583 ,  181kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18436 (*cross-listing*)
replaced with revised version Sat, 2 Mar 2024 22:15:18 GMT   (877kb,D)

Title: Statistically Optimal K-means Clustering via Nonnegative Low-rank
  Semidefinite Programming
Authors: Yubo Zhuang, Xiaohui Chen, Yun Yang, Richard Y. Zhang
Categories: stat.ML cs.LG math.OC
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2305.18436 ,  877kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12803 (*cross-listing*)
replaced with revised version Mon, 4 Mar 2024 16:21:54 GMT   (334kb,D)

Title: Robust Statistical Comparison of Random Variables with Locally Varying
  Scale of Measurement
Authors: Christoph Jansen, Georg Schollmeyer, Hannah Blocher, Julian Rodemann,
  Thomas Augustin
Categories: stat.ML cs.LG math.ST stat.TH
Comments: Accepted for the 39th Conference on Uncertainty in Artificial
  Intelligence (UAI 2023)
MSC-class: 62G10, 62G35
\\ ( https://arxiv.org/abs/2306.12803 ,  334kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00494 (*cross-listing*)
replaced with revised version Sun, 3 Mar 2024 00:32:07 GMT   (3544kb,D)

Title: Improving Protein Optimization with Smoothed Fitness Landscapes
Authors: Andrew Kirjner, Jason Yim, Raman Samusevich, Shahar Bracha, Tommi
  Jaakkola, Regina Barzilay, Ila Fiete
Categories: q-bio.BM cs.LG q-bio.QM stat.ML
Comments: ICLR 2024. Code: https://github.com/kirjner/GGS
\\ ( https://arxiv.org/abs/2307.00494 ,  3544kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02799 (*cross-listing*)
replaced with revised version Sun, 3 Mar 2024 19:56:01 GMT   (25197kb,D)

Title: Few-shot Personalized Saliency Prediction Based on Inter-personnel Gaze
  Patterns
Authors: Yuya Moroto, Keisuke Maeda, Takahiro Ogawa and Miki Haseyama
Categories: eess.IV cs.LG
Comments: 5pages, 3 figures
\\ ( https://arxiv.org/abs/2307.02799 ,  25197kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07932 (*cross-listing*)
replaced with revised version Sun, 3 Mar 2024 08:38:32 GMT   (26006kb,D)

Title: A Novel Truncated Norm Regularization Method for Multi-channel Color
  Image Denoising
Authors: Yiwen Shan, Dong Hu, Zhi Wang
Categories: eess.IV cs.LG
\\ ( https://arxiv.org/abs/2307.07932 ,  26006kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11863 (*cross-listing*)
replaced with revised version Sat, 2 Mar 2024 07:14:02 GMT   (280kb,D)

Title: KinSPEAK: Improving speech recognition for Kinyarwanda via
  semi-supervised learning methods
Authors: Antoine Nzeyimana
Categories: eess.AS cs.LG cs.SD
Comments: 9 pages, 2 figures, 5 tables
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2308.11863 ,  280kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13888
replaced with revised version Sat, 2 Mar 2024 14:01:08 GMT   (47459kb,D)

Title: Neural Implicit Morphing of Face Images
Authors: Guilherme Schardong, Tiago Novello, Hallison Paz, Iurii Medvedev,
  Vin\'icius da Silva, Luiz Velho, Nuno Gon\c{c}alves
Categories: cs.CV cs.LG
Comments: 14 pages, 18 figures
ACM-class: I.4.8; I.4.10
\\ ( https://arxiv.org/abs/2308.13888 ,  47459kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00125 (*cross-listing*)
replaced with revised version Sun, 3 Mar 2024 23:49:48 GMT   (8937kb,D)

Title: Pure Differential Privacy for Functional Summaries via a Laplace-like
  Process
Authors: Haotian Lin, Matthew Reimherr
Categories: stat.ML cs.CR cs.LG
\\ ( https://arxiv.org/abs/2309.00125 ,  8937kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01829 (*cross-listing*)
replaced with revised version Sun, 3 Mar 2024 13:44:23 GMT   (593kb,D)

Title: A Post-Training Approach for Mitigating Overfitting in Quantum
  Convolutional Neural Networks
Authors: Aakash Ravindra Shinde, Charu Jain, and Amir Kalev
Categories: quant-ph cs.LG
Comments: 9 pages, 14 images, 6 tables
\\ ( https://arxiv.org/abs/2309.01829 ,  593kb)
------------------------------------------------------------------------------
\\
arXiv:2309.03891
replaced with revised version Sun, 3 Mar 2024 18:53:09 GMT   (21275kb,D)

Title: ArtiGrasp: Physically Plausible Synthesis of Bi-Manual Dexterous
  Grasping and Articulation
Authors: Hui Zhang, Sammy Christen, Zicong Fan, Luocheng Zheng, Jemin Hwangbo,
  Jie Song, Otmar Hilliges
Categories: cs.RO cs.CV cs.LG
Comments: 3DV-2024 camera ready. Project page:
  https://eth-ait.github.io/artigrasp/
\\ ( https://arxiv.org/abs/2309.03891 ,  21275kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08399
replaced with revised version Mon, 4 Mar 2024 12:24:52 GMT   (1641kb,D)

Title: Optimizing Modular Robot Composition: A Lexicographic Genetic Algorithm
  Approach
Authors: Jonathan K\"ulz and Matthias Althoff
Categories: cs.RO cs.LG cs.NE
Comments: \c{opyright} 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
\\ ( https://arxiv.org/abs/2309.08399 ,  1641kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13957 (*cross-listing*)
replaced with revised version Sun, 3 Mar 2024 16:23:00 GMT   (26286kb,D)

Title: Beam Enumeration: Probabilistic Explainability For Sample Efficient
  Self-conditioned Molecular Design
Authors: Jeff Guo, Philippe Schwaller
Categories: q-bio.BM cs.LG
\\ ( https://arxiv.org/abs/2309.13957 ,  26286kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01690 (*cross-listing*)
replaced with revised version Mon, 4 Mar 2024 14:37:22 GMT   (2082kb,D)

Title: Forecasting Tropical Cyclones with Cascaded Diffusion Models
Authors: Pritthijit Nath, Pancham Shukla, Shuai Wang, C\'esar Quilodr\'an-Casas
Categories: physics.ao-ph cs.LG
Comments: Accepted for poster presentation at the ICLR 2024 workshop on
  Tackling Climate Change with Machine Learning. 7 pages, 3 figures
\\ ( https://arxiv.org/abs/2310.01690 ,  2082kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07855
replaced with revised version Sun, 3 Mar 2024 09:57:57 GMT   (6188kb,D)

Title: CrIBo: Self-Supervised Learning via Cross-Image Object-Level
  Bootstrapping
Authors: Tim Lebailly, Thomas Stegm\"uller, Behzad Bozorgtabar, Jean-Philippe
  Thiran, Tinne Tuytelaars
Categories: cs.CV cs.LG
Comments: ICLR 2024 (spotlight)
\\ ( https://arxiv.org/abs/2310.07855 ,  6188kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07902
replaced with revised version Fri, 1 Mar 2024 19:13:56 GMT   (4766kb,D)

Title: Unraveling the Single Tangent Space Fallacy: An Analysis and
  Clarification for Applying Riemannian Geometry in Robot Learning
Authors: No\'emie Jaquier, Leonel Rozo, Tamim Asfour
Categories: cs.RO cs.LG
Comments: Accepted for publication in ICRA'24. 8 pages, 5 figures, 3 tables
\\ ( https://arxiv.org/abs/2310.07902 ,  4766kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11142
replaced with revised version Mon, 4 Mar 2024 09:07:44 GMT   (10638kb,D)

Title: BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian
  Inference
Authors: Siqi Kou, Lei Gan, Dequan Wang, Chongxuan Li, Zhijie Deng
Categories: cs.CV cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.11142 ,  10638kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11837 (*cross-listing*)
replaced with revised version Mon, 4 Mar 2024 11:52:38 GMT   (7025kb,D)

Title: Optimising Distributions with Natural Gradient Surrogates
Authors: Jonathan So, Richard E. Turner
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2310.11837 ,  7025kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19450 (*cross-listing*)
replaced with revised version Mon, 4 Mar 2024 11:51:50 GMT   (35000kb,D)

Title: Hodge-Compositional Edge Gaussian Processes
Authors: Maosheng Yang, Viacheslav Borovitskiy, Elvin Isufi
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2310.19450 ,  35000kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18274 (*cross-listing*)
replaced with revised version Mon, 4 Mar 2024 15:08:26 GMT   (3710kb,D)

Title: Semiparametric Efficient Inference in Adaptive Experiments
Authors: Thomas Cook, Alan Mishler, Aaditya Ramdas
Categories: stat.ML cs.LG stat.ME
Comments: 24 pages, 6 figures. To appear at CLeaR 2024
\\ ( https://arxiv.org/abs/2311.18274 ,  3710kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00038 (*cross-listing*)
replaced with revised version Mon, 4 Mar 2024 15:54:00 GMT   (5203kb,D)

Title: A Posteriori Evaluation of a Physics-Constrained Neural Ordinary
  Differential Equations Approach Coupled with CFD Solver for Modeling Stiff
  Chemical Kinetics
Authors: Tadbhagya Kumar, Anuj Kumar, Pinaki Pal
Categories: physics.comp-ph cs.LG physics.chem-ph physics.flu-dyn
\\ ( https://arxiv.org/abs/2312.00038 ,  5203kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01133 (*cross-listing*)
replaced with revised version Sun, 3 Mar 2024 08:58:36 GMT   (7765kb,D)

Title: $t^3$-Variational Autoencoder: Learning Heavy-tailed Data with Student's
  t and Power Divergence
Authors: Juno Kim, Jaehyuk Kwon, Mincheol Cho, Hyunjong Lee, Joong-Ho Won
Categories: stat.ML cs.LG
Comments: ICLR 2024; 27 pages, 7 figures, 8 tables
\\ ( https://arxiv.org/abs/2312.01133 ,  7765kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07358 (*cross-listing*)
replaced with revised version Mon, 4 Mar 2024 16:54:22 GMT   (660kb,D)

Title: Distributional Bellman Operators over Mean Embeddings
Authors: Li Kevin Wenliang, Gr\'egoire Del\'etang, Matthew Aitchison, Marcus
  Hutter, Anian Ruoss, Arthur Gretton, Mark Rowland
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2312.07358 ,  660kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11436 (*cross-listing*)
replaced with revised version Sun, 3 Mar 2024 16:31:58 GMT   (14340kb,D)

Title: Layerwise complexity-matched learning yields an improved model of
  cortical area V2
Authors: Nikhil Parthasarathy, Olivier J. H\'enaff, Eero P. Simoncelli
Categories: q-bio.NC cs.CV cs.LG
Comments: 28 pages, 12 figures
\\ ( https://arxiv.org/abs/2312.11436 ,  14340kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06182 (*cross-listing*)
replaced with revised version Sat, 2 Mar 2024 17:59:41 GMT   (5331kb,D)

Title: Prediction of Cellular Identities from Trajectory and Cell Fate
  Information
Authors: Baiyang Dai, Jiamin Yang, Hari Shroff, Patrick La Riviere
Categories: q-bio.QM cs.CV cs.LG eess.IV
\\ ( https://arxiv.org/abs/2401.06182 ,  5331kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06755 (*cross-listing*)
replaced with revised version Sun, 3 Mar 2024 17:46:05 GMT   (13992kb,D)

Title: Solving the Discretised Multiphase Flow Equations with Interface
  Capturing on Structured Grids Using Machine Learning Libraries
Authors: Boyang Chen, Claire E. Heaney, Jefferson L. M. A. Gomes, Omar K.
  Matar, Christopher C. Pain
Categories: physics.flu-dyn cs.LG
Comments: 34 pages, 18 figures, 4 tables
\\ ( https://arxiv.org/abs/2401.06755 ,  13992kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10652
replaced with revised version Sat, 2 Mar 2024 10:33:15 GMT   (1991kb,D)

Title: AutoChunk: Automated Activation Chunk for Memory-Efficient Long Sequence
  Inference
Authors: Xuanlei Zhao, Shenggan Cheng, Guangyang Lu, Jiarui Fang, Haotian Zhou,
  Bin Jia, Ziming Liu, Yang You
Categories: cs.PF cs.DC cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2401.10652 ,  1991kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07388 (*cross-listing*)
replaced with revised version Sat, 2 Mar 2024 03:36:09 GMT   (38kb)

Title: The Limits of Assumption-free Tests for Algorithm Performance
Authors: Yuetian Luo and Rina Foygel Barber
Categories: math.ST cs.LG stat.ML stat.TH
\\ ( https://arxiv.org/abs/2402.07388 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14264 (*cross-listing*)
replaced with revised version Sat, 2 Mar 2024 02:00:58 GMT   (125kb)

Title: Structure-agnostic Optimality of Doubly Robust Learning for Treatment
  Effect Estimation
Authors: Jikai Jin and Vasilis Syrgkanis
Categories: stat.ML cs.LG econ.EM math.ST stat.ME stat.TH
Comments: 31 pages
\\ ( https://arxiv.org/abs/2402.14264 ,  125kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16388 (*cross-listing*)
replaced with revised version Sat, 2 Mar 2024 13:40:04 GMT   (396kb,D)

Title: Uncertainty Quantification in Anomaly Detection with Cross-Conformal
  $p$-Values
Authors: Oliver Hennh\"ofer and Christine Preisach
Categories: stat.ML cs.LG
Comments: 16 pages, 1 figure, 3 tables (added code reference, typos)
\\ ( https://arxiv.org/abs/2402.16388 ,  396kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16991 (*cross-listing*)
replaced with revised version Mon, 4 Mar 2024 14:04:51 GMT   (6540kb,D)

Title: A Phase Transition in Diffusion Models Reveals the Hierarchical Nature
  of Data
Authors: Antonio Sclocchi, Alessandro Favero, Matthieu Wyart
Categories: stat.ML cond-mat.dis-nn cs.CV cs.LG
Comments: 21 pages, 16 figures
\\ ( https://arxiv.org/abs/2402.16991 ,  6540kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17926 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 19:48:53 GMT   (2081kb,D)

Title: Certain and Approximately Certain Models for Statistical Learning
Authors: Cheng Zhen, Nischal Aryal, Arash Termehchy, Alireza Aghasi, Amandeep
  Singh Chabada
Categories: stat.ML cs.DB cs.LG
Comments: A technical report for a paper to appear at SIGMOD 2024
\\ ( https://arxiv.org/abs/2402.17926 ,  2081kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18064
replaced with revised version Sun, 3 Mar 2024 23:04:59 GMT   (5306kb,D)

Title: Automated Testing of Spatially-Dependent Environmental Hypotheses
  through Active Transfer Learning
Authors: Nicholas Harrison, Nathan Wallace, Salah Sukkarieh
Categories: cs.RO cs.LG
Comments: Accepted for publication and presentation at ICRA 2024
\\ ( https://arxiv.org/abs/2402.18064 ,  5306kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18781
replaced with revised version Mon, 4 Mar 2024 02:23:51 GMT   (257kb)

Title: Conjectural Online Learning with First-order Beliefs in Asymmetric
  Information Stochastic Games
Authors: Tao Li, Kim Hammar, Rolf Stadler, and Quanyan Zhu
Categories: cs.GT cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2402.18781 ,  257kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18919
replaced with revised version Sat, 2 Mar 2024 14:57:12 GMT   (42058kb,D)

Title: Decompose-and-Compose: A Compositional Approach to Mitigating Spurious
  Correlation
Authors: Fahimeh Hosseini Noohdani, Parsa Hosseini, Aryan Yazdan Parast,
  Hamidreza Yaghoubi Araghi, Mahdieh Soleymani Baghshah
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2402.18919 ,  42058kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00033 (*cross-listing*)
replaced with revised version Mon, 4 Mar 2024 15:00:58 GMT   (6318kb,D)

Title: Identification of Craving Maps among Marijuana Users via Analysis of
  Functional Brain Networks with High-Order Attention Graph Neural Networks
Authors: Jun-En Ding, Shihao Yang, Anna Zilverstand, and Feng Liu
Categories: q-bio.NC cs.LG eess.SP
\\ ( https://arxiv.org/abs/2403.00033 ,  6318kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
