paper_240328.txt

Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 100011 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月28日 13:11
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Tue 26 Mar 24 18:00:00 GMT  to  Wed 27 Mar 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.18056
Date: Tue, 26 Mar 2024 19:19:16 GMT   (17004kb,D)

Title: Self-Clustering Hierarchical Multi-Agent Reinforcement Learning with
  Extensible Cooperation Graph
Authors: Qingxu Fu, Tenghai Qiu, Jianqiang Yi, Zhiqiang Pu, Xiaolin Ai
Categories: cs.AI
\\
  Multi-Agent Reinforcement Learning (MARL) has been successful in solving many
cooperative challenges. However, classic non-hierarchical MARL algorithms still
cannot address various complex multi-agent problems that require hierarchical
cooperative behaviors. The cooperative knowledge and policies learned in
non-hierarchical algorithms are implicit and not interpretable, thereby
restricting the integration of existing knowledge. This paper proposes a novel
hierarchical MARL model called Hierarchical Cooperation Graph Learning (HCGL)
for solving general multi-agent problems. HCGL has three components: a dynamic
Extensible Cooperation Graph (ECG) for achieving self-clustering cooperation; a
group of graph operators for adjusting the topology of ECG; and an MARL
optimizer for training these graph operators. HCGL's key distinction from other
MARL models is that the behaviors of agents are guided by the topology of ECG
instead of policy neural networks. ECG is a three-layer graph consisting of an
agent node layer, a cluster node layer, and a target node layer. To manipulate
the ECG topology in response to changing environmental conditions, four graph
operators are trained to adjust the edge connections of ECG dynamically. The
hierarchical feature of ECG provides a unique approach to merge primitive
actions (actions executed by the agents) and cooperative actions (actions
executed by the clusters) into a unified action space, allowing us to integrate
fundamental cooperative knowledge into an extensible interface. In our
experiments, the HCGL model has shown outstanding performance in multi-agent
benchmarks with sparse rewards. We also verify that HCGL can easily be
transferred to large-scale scenarios with high zero-shot transfer success
rates.
\\ ( https://arxiv.org/abs/2403.18056 ,  17004kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18057
Date: Tue, 26 Mar 2024 19:21:50 GMT   (32269kb,D)

Title: Prioritized League Reinforcement Learning for Large-Scale Heterogeneous
  Multiagent Systems
Authors: Qingxu Fu, Zhiqiang Pu, Min Chen, Tenghai Qiu, Jianqiang Yi
Categories: cs.AI
\\
  Large-scale heterogeneous multiagent systems feature various realistic
factors in the real world, such as agents with diverse abilities and overall
system cost. In comparison to homogeneous systems, heterogeneous systems offer
significant practical advantages. Nonetheless, they also present challenges for
multiagent reinforcement learning, including addressing the non-stationary
problem and managing an imbalanced number of agents with different types. We
propose a Prioritized Heterogeneous League Reinforcement Learning (PHLRL)
method to address large-scale heterogeneous cooperation problems. PHLRL
maintains a record of various policies that agents have explored during their
training and establishes a heterogeneous league consisting of diverse policies
to aid in future policy optimization. Furthermore, we design a prioritized
policy gradient approach to compensate for the gap caused by differences in the
number of different types of agents. Next, we use Unreal Engine to design a
large-scale heterogeneous cooperation benchmark named Large-Scale Multiagent
Operation (LSMO), which is a complex two-team competition scenario that
requires collaboration from both ground and airborne agents. We use experiments
to show that PHLRL outperforms state-of-the-art methods, including QTRAN and
QPLEX in LSMO.
\\ ( https://arxiv.org/abs/2403.18057 ,  32269kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18100
Date: Tue, 26 Mar 2024 20:59:48 GMT   (1158kb)

Title: Driving Intelligent IoT Monitoring and Control through Cloud Computing
  and Machine Learning
Authors: Hanzhe Li, Xiangxiang Wang, Yuan Feng, Yaqian Qi, Jingxiao Tian
Categories: cs.AI cs.LG
\\
  This article explores how to drive intelligent iot monitoring and control
through cloud computing and machine learning. As iot and the cloud continue to
generate large and diverse amounts of data as sensor devices in the network,
the collected data is sent to the cloud for statistical analysis, prediction,
and data analysis to achieve business objectives. However, because the cloud
computing model is limited by distance, it can be problematic in environments
where the quality of the Internet connection is not ideal for critical
operations. Therefore, edge computing, as a distributed computing architecture,
moves the location of processing applications, data and services from the
central node of the network to the logical edge node of the network to reduce
the dependence on cloud processing and analysis of data, and achieve near-end
data processing and analysis. The combination of iot and edge computing can
reduce latency, improve efficiency, and enhance security, thereby driving the
development of intelligent systems. The paper also introduces the development
of iot monitoring and control technology, the application of edge computing in
iot monitoring and control, and the role of machine learning in data analysis
and fault detection. Finally, the application and effect of intelligent
Internet of Things monitoring and control system in industry, agriculture,
medical and other fields are demonstrated through practical cases and
experimental studies.
\\ ( https://arxiv.org/abs/2403.18100 ,  1158kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18101
Date: Tue, 26 Mar 2024 21:00:06 GMT   (135kb,D)

Title: Towards Explainable Clustering: A Constrained Declarative based Approach
Authors: Mathieu Guilbert, Christel Vrain, Thi-Bich-Hanh Dao
Categories: cs.AI cs.LG
\\
  The domain of explainable AI is of interest in all Machine Learning fields,
and it is all the more important in clustering, an unsupervised task whose
result must be validated by a domain expert. We aim at finding a clustering
that has high quality in terms of classic clustering criteria and that is
explainable, and we argue that these two dimensions must be considered when
building the clustering. We consider that a good global explanation of a
clustering should give the characteristics of each cluster taking into account
their abilities to describe its objects (coverage) while distinguishing it from
the other clusters (discrimination). Furthermore, we aim at leveraging expert
knowledge, at different levels, on the structure of the expected clustering or
on its explanations. In our framework an explanation of a cluster is a set of
patterns, and we propose a novel interpretable constrained clustering method
called ECS for declarative clustering with Explainabilty-driven Cluster
Selection that integrates structural or domain expert knowledge expressed by
means of constraints. It is based on the notion of coverage and discrimination
that are formalized at different levels (cluster / clustering), each allowing
for exceptions through parameterized thresholds. Our method relies on four
steps: generation of a set of partitions, computation of frequent patterns for
each cluster, pruning clusters that violates some constraints, and selection of
clusters and associated patterns to build an interpretable clustering. This
last step is combinatorial and we have developed a Constraint-Programming (CP)
model to solve it. The method can integrate prior knowledge in the form of user
constraints, both before or in the CP model.
\\ ( https://arxiv.org/abs/2403.18101 ,  135kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18120
Date: Tue, 26 Mar 2024 22:01:13 GMT   (194kb,D)

Title: Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with
  Autoformalization
Authors: Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Q.
  Weinberger, Yuhuai Wu
Categories: cs.AI cs.CL cs.LG
Comments: ICLR 2024
\\
  Large language models (LLM), such as Google's Minerva and OpenAI's GPT
families, are becoming increasingly capable of solving mathematical
quantitative reasoning problems. However, they still make unjustified logical
and computational errors in their reasoning steps and answers. In this paper,
we leverage the fact that if the training corpus of LLMs contained sufficiently
many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving
environment), they can be prompted to translate i.e. autoformalize informal
mathematical statements into formal Isabelle code -- which can be verified
automatically for internal consistency. This provides a mechanism to
automatically reject solutions whose formalized versions are inconsistent
within themselves or with the formalized problem statement. We evaluate our
method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach
provides a consistently better heuristic than vanilla majority voting -- the
previously best method to identify correct answers, by more than 12% on GSM8K.
In our experiments it improves results consistently across all datasets and LLM
model sizes. The code can be found at https://github.com/jinpz/dtv.
\\ ( https://arxiv.org/abs/2403.18120 ,  194kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18145
Date: Tue, 26 Mar 2024 23:10:41 GMT   (7223kb,D)

Title: A Real-Time Rescheduling Algorithm for Multi-robot Plan Execution
Authors: Ying Feng, Adittyo Paul, Zhe Chen, Jiaoyang Li
Categories: cs.AI cs.MA cs.RO
Comments: ICAPS 2024
\\
  One area of research in multi-agent path finding is to determine how
replanning can be efficiently achieved in the case of agents being delayed
during execution. One option is to reschedule the passing order of agents,
i.e., the sequence in which agents visit the same location. In response, we
propose Switchable-Edge Search (SES), an A*-style algorithm designed to find
optimal passing orders. We prove the optimality of SES and evaluate its
efficiency via simulations. The best variant of SES takes less than 1 second
for small- and medium-sized problems and runs up to 4 times faster than
baselines for large-sized problems.
\\ ( https://arxiv.org/abs/2403.18145 ,  7223kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18183
Date: Wed, 27 Mar 2024 01:21:48 GMT   (854kb,D)

Title: Can AI Models Appreciate Document Aesthetics? An Exploration of
  Legibility and Layout Quality in Relation to Prediction Confidence
Authors: Hsiu-Wei Yang, Abhinav Agrawal, Pavlos Fragkogiannis, Shubham Nitin
  Mulay
Categories: cs.AI cs.IR
\\
  A well-designed document communicates not only through its words but also
through its visual eloquence. Authors utilize aesthetic elements such as
colors, fonts, graphics, and layouts to shape the perception of information.
Thoughtful document design, informed by psychological insights, enhances both
the visual appeal and the comprehension of the content. While state-of-the-art
document AI models demonstrate the benefits of incorporating layout and image
data, it remains unclear whether the nuances of document aesthetics are
effectively captured. To bridge the gap between human cognition and AI
interpretation of aesthetic elements, we formulated hypotheses concerning AI
behavior in document understanding tasks, specifically anchored in document
design principles. With a focus on legibility and layout quality, we tested
four aspects of aesthetic effects: noise, font-size contrast, alignment, and
complexity, on model confidence using correlational analysis. The results and
observations highlight the value of model analysis rooted in document design
theories. Our work serves as a trailhead for further studies and we advocate
for continued research in this topic to deepen our understanding of how AI
interprets document aesthetics.
\\ ( https://arxiv.org/abs/2403.18183 ,  854kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18203
Date: Wed, 27 Mar 2024 02:24:38 GMT   (7220kb,D)

Title: EndToEndML: An Open-Source End-to-End Pipeline for Machine Learning
  Applications
Authors: Nisha Pillai, Athish Ram Das, Moses Ayoola, Ganga Gireesan, Bindu
  Nanduri, Mahalingam Ramkumar
Categories: cs.AI
Comments: 2024 7th International Conference on Information and Computer
  Technologies (ICICT)
\\
  Artificial intelligence (AI) techniques are widely applied in the life
sciences. However, applying innovative AI techniques to understand and
deconvolute biological complexity is hindered by the learning curve for life
science scientists to understand and use computing languages. An open-source,
user-friendly interface for AI models, that does not require programming skills
to analyze complex biological data will be extremely valuable to the
bioinformatics community. With easy access to different sequencing technologies
and increased interest in different 'omics' studies, the number of biological
datasets being generated has increased and analyzing these high-throughput
datasets is computationally demanding. The majority of AI libraries today
require advanced programming skills as well as machine learning, data
preprocessing, and visualization skills. In this research, we propose a
web-based end-to-end pipeline that is capable of preprocessing, training,
evaluating, and visualizing machine learning (ML) models without manual
intervention or coding expertise. By integrating traditional machine learning
and deep neural network models with visualizations, our library assists in
recognizing, classifying, clustering, and predicting a wide range of
multi-modal, multi-sensor datasets, including images, languages, and
one-dimensional numerical data, for drug discovery, pathogen classification,
and medical diagnostics.
\\ ( https://arxiv.org/abs/2403.18203 ,  7220kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18205
Date: Wed, 27 Mar 2024 02:31:54 GMT   (173kb,D)

Title: Exploring the Privacy Protection Capabilities of Chinese Large Language
  Models
Authors: Yuqi Yang, Xiaowen Huang, Jitao Sang
Categories: cs.AI
Comments: 11 pages
\\
  Large language models (LLMs), renowned for their impressive capabilities in
various tasks, have significantly advanced artificial intelligence. Yet, these
advancements have raised growing concerns about privacy and security
implications. To address these issues and explain the risks inherent in these
models, we have devised a three-tiered progressive framework tailored for
evaluating privacy in language systems. This framework consists of
progressively complex and in-depth privacy test tasks at each tier. Our primary
objective is to comprehensively evaluate the sensitivity of large language
models to private information, examining how effectively they discern, manage,
and safeguard sensitive data in diverse scenarios. This systematic evaluation
helps us understand the degree to which these models comply with privacy
protection guidelines and the effectiveness of their inherent safeguards
against privacy breaches. Our observations indicate that existing Chinese large
language models universally show privacy protection shortcomings. It seems that
at the moment this widespread issue is unavoidable and may pose corresponding
privacy risks in applications based on these models.
\\ ( https://arxiv.org/abs/2403.18205 ,  173kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18218
Date: Wed, 27 Mar 2024 03:04:21 GMT   (161kb,D)

Title: Leveraging Large Language Models for Fuzzy String Matching in Political
  Science
Authors: Yu Wang
Categories: cs.AI
Comments: 7 pages, 2 figures, 1 table;
\\
  Fuzzy string matching remains a key issue when political scientists combine
data from different sources. Existing matching methods invariably rely on
string distances, such as Levenshtein distance and cosine similarity. As such,
they are inherently incapable of matching strings that refer to the same entity
with different names such as ''JP Morgan'' and ''Chase Bank'', ''DPRK'' and
''North Korea'', ''Chuck Fleischmann (R)'' and ''Charles Fleischmann (R)''. In
this letter, we propose to use large language models to entirely sidestep this
problem in an easy and intuitive manner. Extensive experiments show that our
proposed methods can improve the state of the art by as much as 39% in terms of
average precision while being substantially easier and more intuitive to use by
political scientists. Moreover, our results are robust against various
temperatures. We further note that enhanced prompting can lead to additional
performance improvements.
\\ ( https://arxiv.org/abs/2403.18218 ,  161kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18230
Date: Wed, 27 Mar 2024 03:33:32 GMT   (1027kb,D)

Title: Large Language Models Need Consultants for Reasoning: Becoming an Expert
  in a Complex Human System Through Behavior Simulation
Authors: Chuwen Wang, Shirong Zeng, Cheng Wang
Categories: cs.AI
\\
  Large language models (LLMs), in conjunction with various reasoning
reinforcement methodologies, have demonstrated remarkable capabilities
comparable to humans in fields such as mathematics, law, coding, common sense,
and world knowledge. In this paper, we delve into the reasoning abilities of
LLMs within complex human systems. We propose a novel reasoning framework,
termed ``Mosaic Expert Observation Wall'' (MEOW) exploiting
generative-agents-based simulation technique. In the MEOW framework, simulated
data are utilized to train an expert model concentrating ``experience'' about a
specific task in each independent time of simulation. It is the accumulated
``experience'' through the simulation that makes for an expert on a task in a
complex human system. We conduct the experiments within a communication game
that mirrors real-world security scenarios. The results indicate that our
proposed methodology can cooperate with existing methodologies to enhance the
reasoning abilities of LLMs in complex human systems.
\\ ( https://arxiv.org/abs/2403.18230 ,  1027kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18243
Date: Wed, 27 Mar 2024 04:20:18 GMT   (2968kb,D)

Title: Boosting Conversational Question Answering with Fine-Grained
  Retrieval-Augmentation and Self-Check
Authors: Linhao Ye, Zhikai Lei, Jianghao Yin, Qin Chen, Jie Zhou, Liang He
Categories: cs.AI
\\
  Retrieval-Augmented Generation (RAG) aims to generate more reliable and
accurate responses, by augmenting large language models (LLMs) with the
external vast and dynamic knowledge. Most previous work focuses on using RAG
for single-round question answering, while how to adapt RAG to the complex
conversational setting wherein the question is interdependent on the preceding
context is not well studied. In this paper, we propose a conversation-level RAG
approach, which incorporates fine-grained retrieval augmentation and self-check
for conversational question answering (CQA). In particular, our approach
consists of three components, namely conversational question refiner,
fine-grained retriever and self-check based response generator, which work
collaboratively for question understanding and relevant information acquisition
in conversational settings. Extensive experiments demonstrate the great
advantages of our approach over the state-of-the-art baselines. Moreover, we
also release a Chinese CQA dataset with new features including reformulated
question, extracted keyword, retrieved paragraphs and their helpfulness, which
facilitates further researches in RAG enhanced CQA.
\\ ( https://arxiv.org/abs/2403.18243 ,  2968kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18278
Date: Wed, 27 Mar 2024 06:13:39 GMT   (1532kb,D)

Title: Identification and Uses of Deep Learning Backbones via Pattern Mining
Authors: Michael Livanos and Ian Davidson
Categories: cs.AI
Comments: 9 pages, 6 figures, published SIAM SDM24
\\
  Deep learning is extensively used in many areas of data mining as a black-box
method with impressive results. However, understanding the core mechanism of
how deep learning makes predictions is a relatively understudied problem. Here
we explore the notion of identifying a backbone of deep learning for a given
group of instances. A group here can be instances of the same class or even
misclassified instances of the same class. We view each instance for a given
group as activating a subset of neurons and attempt to find a subgraph of
neurons associated with a given concept/group. We formulate this problem as a
set cover style problem and show it is intractable and presents a highly
constrained integer linear programming (ILP) formulation. As an alternative, we
explore a coverage-based heuristic approach related to pattern mining, and show
it converges to a Pareto equilibrium point of the ILP formulation.
Experimentally we explore these backbones to identify mistakes and improve
performance, explanation, and visualization. We demonstrate application-based
results using several challenging data sets, including Bird Audio Detection
(BAD) Challenge and Labeled Faces in the Wild (LFW), as well as the classic
MNIST data.
\\ ( https://arxiv.org/abs/2403.18278 ,  1532kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18338
Date: Wed, 27 Mar 2024 08:25:28 GMT   (127kb,D)

Title: mALBERT: Is a Compact Multilingual BERT Model Still Worth It?
Authors: Christophe Servan (ILES, STL), Sahar Ghannay (LISN), Sophie Rosset
  (LISN)
Categories: cs.AI
Comments: The 2024 Joint International Conference on Computational Linguistics,
  Language Resources and Evaluation, May 2024, Torino, Italy
\\
  Within the current trend of Pretained Language Models (PLM), emerge more and
more criticisms about the ethical andecological impact of such models. In this
article, considering these critical remarks, we propose to focus on
smallermodels, such as compact models like ALBERT, which are more ecologically
virtuous than these PLM. However,PLMs enable huge breakthroughs in Natural
Language Processing tasks, such as Spoken and Natural LanguageUnderstanding,
classification, Question--Answering tasks. PLMs also have the advantage of
being multilingual, and,as far as we know, a multilingual version of compact
ALBERT models does not exist. Considering these facts, wepropose the free
release of the first version of a multilingual compact ALBERT model,
pre-trained using Wikipediadata, which complies with the ethical aspect of such
a language model. We also evaluate the model against classicalmultilingual PLMs
in classical NLP tasks. Finally, this paper proposes a rare study on the
subword tokenizationimpact on language performances.
\\ ( https://arxiv.org/abs/2403.18338 ,  127kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18344
Date: Wed, 27 Mar 2024 08:34:55 GMT   (14933kb,D)

Title: LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions
  with Large Language Models
Authors: Mingxing Peng, Xusen Guo, Xianda Chen, Meixin Zhu, Kehua Chen, Hao
  (Frank) Yang, Xuesong Wang, and Yinhai Wang
Categories: cs.AI
\\
  To ensure safe driving in dynamic environments, autonomous vehicles should
possess the capability to accurately predict the lane change intentions of
surrounding vehicles in advance and forecast their future trajectories.
Existing motion prediction approaches have ample room for improvement,
particularly in terms of long-term prediction accuracy and interpretability. In
this paper, we address these challenges by proposing LC-LLM, an explainable
lane change prediction model that leverages the strong reasoning capabilities
and self-explanation abilities of Large Language Models (LLMs). Essentially, we
reformulate the lane change prediction task as a language modeling problem,
processing heterogeneous driving scenario information in natural language as
prompts for input into the LLM and employing a supervised fine-tuning technique
to tailor the LLM specifically for our lane change prediction task. This allows
us to utilize the LLM's powerful common sense reasoning abilities to understand
complex interactive information, thereby improving the accuracy of long-term
predictions. Furthermore, we incorporate explanatory requirements into the
prompts in the inference stage. Therefore, our LC-LLM model not only can
predict lane change intentions and trajectories but also provides explanations
for its predictions, enhancing the interpretability. Extensive experiments on
the large-scale highD dataset demonstrate the superior performance and
interpretability of our LC-LLM in lane change prediction task. To the best of
our knowledge, this is the first attempt to utilize LLMs for predicting lane
change behavior. Our study shows that LLMs can encode comprehensive interaction
information for driving behavior understanding.
\\ ( https://arxiv.org/abs/2403.18344 ,  14933kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18388
Date: Wed, 27 Mar 2024 09:25:20 GMT   (692kb,D)

Title: FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion
Authors: Xiaofeng Wu, Velibor Bojkovic, Bin Gu, Kun Suo, Kai Zou
Categories: cs.AI cs.CV
\\
  Spiking Neural Networks (SNNs) offer a promising avenue for energy-efficient
computing compared with Artificial Neural Networks (ANNs), closely mirroring
biological neural processes. However, this potential comes with inherent
challenges in directly training SNNs through spatio-temporal backpropagation --
stemming from the temporal dynamics of spiking neurons and their discrete
signal processing -- which necessitates alternative ways of training, most
notably through ANN-SNN conversion. In this work, we introduce a lightweight
Forward Temporal Bias Correction (FTBC) technique, aimed at enhancing
conversion accuracy without the computational overhead. We ground our method on
provided theoretical findings that through proper temporal bias calibration the
expected error of ANN-SNN conversion can be reduced to be zero after each time
step. We further propose a heuristic algorithm for finding the temporal bias
only in the forward pass, thus eliminating the computational burden of
backpropagation and we evaluate our method on CIFAR-10/100 and ImageNet
datasets, achieving a notable increase in accuracy on all datasets. Codes are
released at a GitHub repository.
\\ ( https://arxiv.org/abs/2403.18388 ,  692kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18405
Date: Wed, 27 Mar 2024 09:46:56 GMT   (1925kb,D)

Title: Leveraging Large Language Models for Relevance Judgments in Legal Case
  Retrieval
Authors: Shengjie Ma, Chong Chen, Qi Chu and Jiaxin Mao
Categories: cs.AI
\\
  Collecting relevant judgments for legal case retrieval is a challenging and
time-consuming task. Accurately judging the relevance between two legal cases
requires a considerable effort to read the lengthy text and a high level of
domain expertise to extract Legal Facts and make juridical judgments. With the
advent of advanced large language models, some recent studies have suggested
that it is promising to use LLMs for relevance judgment. Nonetheless, the
method of employing a general large language model for reliable relevance
judgments in legal case retrieval is yet to be thoroughly explored. To fill
this research gap, we devise a novel few-shot workflow tailored to the relevant
judgment of legal cases. The proposed workflow breaks down the annotation
process into a series of stages, imitating the process employed by human
annotators and enabling a flexible integration of expert reasoning to enhance
the accuracy of relevance judgments. By comparing the relevance judgments of
LLMs and human experts, we empirically show that we can obtain reliable
relevance judgments with the proposed workflow. Furthermore, we demonstrate the
capacity to augment existing legal case retrieval models through the synthesis
of data generated by the large language model.
\\ ( https://arxiv.org/abs/2403.18405 ,  1925kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18489
Date: Wed, 27 Mar 2024 12:01:51 GMT   (1317kb)

Title: Impact of Employing Weather Forecast Data as Input to the Estimation of
  Evapotranspiration by Deep Neural Network Models
Authors: Pedro J. Vaz, Gabriela Sch\"utz, Carlos Guerrero, and Pedro J. S.
  Cardoso
Categories: cs.AI cs.LG
Comments: A partial version of the work submitted to ESRE/INTERNATIONAL
  CONFERENCE ON ENVIRONMENTAL SCIENCES AND RENEWABLE ENERGY
\\
  Reference Evapotranspiration (ET0) is a key parameter for designing smart
irrigation scheduling, since it is related by a coefficient to the water needs
of a crop. The United Nations Food and Agriculture Organization, proposed a
standard method for ET0 computation (FAO56PM), based on the parameterization of
the Penman-Monteith equation, that is widely adopted in the literature. To
compute ET0 using the FAO56-PM method, four main weather parameters are needed:
temperature, humidity, wind, and solar radiation (SR). One way to make daily
ET0 estimations for future days is to use freely available weather forecast
services (WFSs), where many meteorological parameters are estimated up to the
next 15 days. A problem with this method is that currently, SR is not provided
as a free forecast parameter on most of those online services or, normally,
such forecasts present a financial cost penalty. For this reason, several ET0
estimation models using machine and deep learning were developed and presented
in the literature, that use as input features a reduced set of carefully
selected weather parameters, that are compatible with common freely available
WFSs. However, most studies on this topic have only evaluated model performance
using data from weather stations (WSs), without considering the effect of using
weather forecast data. In this study, the performance of authors' previous
models is evaluated when using weather forecast data from two online WFSs, in
the following scenarios: (i) direct ET0 estimation by an ANN model, and (ii)
estimate SR by ANN model, and then use that estimation for ET0 computation,
using the FAO56-PM method. Employing data collected from two WFSs and a WS
located in Vale do Lobo, Portugal, the latter approach achieved the best
result, with a coefficient of determination (R2) ranging between 0.893 and
0.667, when considering forecasts up to 15 days.
\\ ( https://arxiv.org/abs/2403.18489 ,  1317kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18537
Date: Wed, 27 Mar 2024 13:12:57 GMT   (432kb)

Title: A Path Towards Legal Autonomy: An interoperable and explainable approach
  to extracting, transforming, loading and computing legal information using
  large language models, expert systems and Bayesian networks
Authors: Axel Constant, Hannes Westermann, Bryan Wilson, Alex Kiefer, Ines
  Hipolito, Sylvain Pronovost, Steven Swanson, Mahault Albarracin, and Maxwell
  J.D. Ramstead
Categories: cs.AI cs.CL cs.CY cs.LO
\\
  Legal autonomy - the lawful activity of artificial intelligence agents - can
be achieved in one of two ways. It can be achieved either by imposing
constraints on AI actors such as developers, deployers and users, and on AI
resources such as data, or by imposing constraints on the range and scope of
the impact that AI agents can have on the environment. The latter approach
involves encoding extant rules concerning AI driven devices into the software
of AI agents controlling those devices (e.g., encoding rules about limitations
on zones of operations into the agent software of an autonomous drone device).
This is a challenge since the effectivity of such an approach requires a method
of extracting, loading, transforming and computing legal information that would
be both explainable and legally interoperable, and that would enable AI agents
to reason about the law. In this paper, we sketch a proof of principle for such
a method using large language models (LLMs), expert legal systems known as
legal decision paths, and Bayesian networks. We then show how the proposed
method could be applied to extant regulation in matters of autonomous cars,
such as the California Vehicle Code.
\\ ( https://arxiv.org/abs/2403.18537 ,  432kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18547
Date: Wed, 27 Mar 2024 13:25:43 GMT   (61kb,D)

Title: Neural Architecture Search for Sentence Classification with BERT
Authors: Philip Kenneweg, Sarah Schr\"oder, Barbara Hammer
Categories: cs.AI
DOI: 10.14428/esann/2022.ES2022-45
\\
  Pre training of language models on large text corpora is common practice in
Natural Language Processing. Following, fine tuning of these models is
performed to achieve the best results on a variety of tasks. In this paper we
question the common practice of only adding a single output layer as a
classification head on top of the network. We perform an AutoML search to find
architectures that outperform the current single layer at only a small compute
cost. We validate our classification architecture on a variety of NLP
benchmarks from the GLUE dataset.
\\ ( https://arxiv.org/abs/2403.18547 ,  61kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18659
Date: Wed, 27 Mar 2024 15:03:33 GMT   (118kb,D)

Title: INEXA: Interactive and Explainable Process Model Abstraction Through
  Object-Centric Process Mining
Authors: Janik-Vasily Benzin and Gyunam Park and Juergen Mangler and Stefanie
  Rinderle-Ma
Categories: cs.AI
\\
  Process events are recorded by multiple information systems at different
granularity levels. Based on the resulting event logs, process models are
discovered at different granularity levels, as well. Events stored at a
fine-grained granularity level, for example, may hinder the discovered process
model to be displayed due the high number of resulting model elements. The
discovered process model of a real-world manufacturing process, for example,
consists of 1,489 model elements and over 2,000 arcs. Existing process model
abstraction techniques could help reducing the size of the model, but would
disconnect it from the underlying event log. Existing event abstraction
techniques do neither support the analysis of mixed granularity levels, nor
interactive exploration of a suitable granularity level. To enable the
exploration of discovered process models at different granularity levels, we
propose INEXA, an interactive, explainable process model abstraction method
that keeps the link to the event log. As a starting point, INEXA aggregates
large process models to a "displayable" size, e.g., for the manufacturing use
case to a process model with 58 model elements. Then, the process analyst can
explore granularity levels interactively, while applied abstractions are
automatically traced in the event log for explainability.
\\ ( https://arxiv.org/abs/2403.18659 ,  118kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18725
Date: Wed, 27 Mar 2024 16:15:21 GMT   (472kb)

Title: Probabilistic Model Checking of Stochastic Reinforcement Learning
  Policies
Authors: Dennis Gross, Helge Spieker
Categories: cs.AI
\\
  We introduce a method to verify stochastic reinforcement learning (RL)
policies. This approach is compatible with any RL algorithm as long as the
algorithm and its corresponding environment collectively adhere to the Markov
property. In this setting, the future state of the environment should depend
solely on its current state and the action executed, independent of any
previous states or actions. Our method integrates a verification technique,
referred to as model checking, with RL, leveraging a Markov decision process, a
trained RL policy, and a probabilistic computation tree logic (PCTL) formula to
build a formal model that can be subsequently verified via the model checker
Storm. We demonstrate our method's applicability across multiple benchmarks,
comparing it to baseline methods called deterministic safety estimates and
naive monolithic model checking. Our results show that our method is suited to
verify stochastic RL policies.
\\ ( https://arxiv.org/abs/2403.18725 ,  472kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18731
Date: Wed, 27 Mar 2024 16:21:24 GMT   (2481kb,D)

Title: Enhancing Manufacturing Quality Prediction Models through the
  Integration of Explainability Methods
Authors: Dennis Gross, Helge Spieker, Arnaud Gotlieb, Ricardo Knoblauch
Categories: cs.AI cs.CV cs.CY cs.LG
\\
  This research presents a method that utilizes explainability techniques to
amplify the performance of machine learning (ML) models in forecasting the
quality of milling processes, as demonstrated in this paper through a
manufacturing use case. The methodology entails the initial training of ML
models, followed by a fine-tuning phase where irrelevant features identified
through explainability methods are eliminated. This procedural refinement
results in performance enhancements, paving the way for potential reductions in
manufacturing costs and a better understanding of the trained ML models. This
study highlights the usefulness of explainability techniques in both explaining
and optimizing predictive models in the manufacturing realm.
\\ ( https://arxiv.org/abs/2403.18731 ,  2481kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18018
Date: Tue, 26 Mar 2024 18:07:10 GMT   (834kb)

Title: DORE: A Dataset For Portuguese Definition Generation
Authors: Anna Beatriz Dimas Furtado, Tharindu Ranasinghe, Fr\'ed\'eric Blain,
  Ruslan Mitkov
Categories: cs.CL cs.LG
Comments: Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)
\\
  Definition modelling (DM) is the task of automatically generating a
dictionary definition for a specific word. Computational systems that are
capable of DM can have numerous applications benefiting a wide range of
audiences. As DM is considered a supervised natural language generation
problem, these systems require large annotated datasets to train the machine
learning (ML) models. Several DM datasets have been released for English and
other high-resource languages. While Portuguese is considered a
mid/high-resource language in most natural language processing tasks and is
spoken by more than 200 million native speakers, there is no DM dataset
available for Portuguese. In this research, we fill this gap by introducing
DORE; the first dataset for Definition MOdelling for PoRtuguEse containing more
than 100,000 definitions. We also evaluate several deep learning based DM
models on DORE and report the results. The dataset and the findings of this
paper will facilitate research and study of Portuguese in wider contexts.
\\ ( https://arxiv.org/abs/2403.18018 ,  834kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18024
Date: Tue, 26 Mar 2024 18:22:05 GMT   (544kb,D)

Title: Enriching Word Usage Graphs with Cluster Definitions
Authors: Mariia Fedorova, Andrey Kutuzov, Nikolay Arefyev, Dominik Schlechtweg
Categories: cs.CL
Comments: LREC-COLING 2024
\\
  We present a dataset of word usage graphs (WUGs), where the existing WUGs for
multiple languages are enriched with cluster labels functioning as sense
definitions. They are generated from scratch by fine-tuned encoder-decoder
language models. The conducted human evaluation has shown that these
definitions match the existing clusters in WUGs better than the definitions
chosen from WordNet by two baseline systems. At the same time, the method is
straightforward to use and easy to extend to new languages. The resulting
enriched datasets can be extremely helpful for moving on to explainable
semantic change modeling.
\\ ( https://arxiv.org/abs/2403.18024 ,  544kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18025
Date: Tue, 26 Mar 2024 18:23:16 GMT   (8270kb,D)

Title: Improving Pre-trained Language Model Sensitivity via Mask Specific
  losses: A case study on Biomedical NER
Authors: Micheal Abaho, Danushka Bollegala, Gary Leeming, Dan Joyce, Iain E
  Buchan
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: Paper alrerady accepted for publishing by the NAACL 2024 conference
  (main conference paper)
\\
  Adapting language models (LMs) to novel domains is often achieved through
fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning
introduces new knowledge into an LM, enabling it to comprehend and efficiently
perform a target domain task. Fine-tuning can however be inadvertently
insensitive if it ignores the wide array of disparities (e.g in word meaning)
between source and target domains. For instance, words such as chronic and
pressure may be treated lightly in social conversations, however, clinically,
these words are usually an expression of concern. To address insensitive
fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach
that efficiently acquires target domain knowledge by appropriately weighting
the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM
jointly masks DS-terms and generic words, then learns mask-specific losses by
ensuring LMs incur larger penalties for inaccurately predicting DS-terms
compared to generic words. Results of our analysis show that MSLM improves LMs
sensitivity and detection of DS-terms. We empirically show that an optimal
masking rate not only depends on the LM, but also on the dataset and the length
of sequences. Our proposed masking strategy outperforms advanced masking
strategies such as span- and PMI-based masking.
\\ ( https://arxiv.org/abs/2403.18025 ,  8270kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18031
Date: Tue, 26 Mar 2024 18:38:14 GMT   (9330kb,D)

Title: The Impact of Syntactic and Semantic Proximity on Machine Translation
  with Back-Translation
Authors: Nicolas Guerin, Shane Steinert-Threlkeld, Emmanuel Chemla
Categories: cs.CL
\\
  Unsupervised on-the-fly back-translation, in conjunction with multilingual
pretraining, is the dominant method for unsupervised neural machine
translation. Theoretically, however, the method should not work in general. We
therefore conduct controlled experiments with artificial languages to determine
what properties of languages make back-translation an effective training
method, covering lexical, syntactic, and semantic properties. We find, contrary
to popular belief, that (i) parallel word frequency distributions, (ii)
partially shared vocabulary, and (iii) similar syntactic structure across
languages are not sufficient to explain the success of back-translation. We
show however that even crude semantic signal (similar lexical fields across
languages) does improve alignment of two languages through back-translation. We
conjecture that rich semantic dependencies, parallel across languages, are at
the root of the success of unsupervised methods based on back-translation.
Overall, the success of unsupervised machine translation was far from being
analytically guaranteed. Instead, it is another proof that languages of the
world share deep similarities, and we hope to show how to identify which of
these similarities can serve the development of unsupervised, cross-linguistic
tools.
\\ ( https://arxiv.org/abs/2403.18031 ,  9330kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18051
Date: Tue, 26 Mar 2024 19:08:20 GMT   (8500kb,D)

Title: Supervisory Prompt Training
Authors: Jean Ghislain Billa, Min Oh, Liang Du
Categories: cs.CL cs.AI
\\
  The performance of Large Language Models (LLMs) relies heavily on the quality
of prompts, which are often manually engineered and task-specific, making them
costly and non-scalable. We propose a novel approach, Supervisory Prompt
Training (SPT). SPT automates the generation of highly effective prompts using
a dual LLM system. In this system, one LLM, the generator, performs a task
while the other, the corrector, provides feedback and generates improved
prompts. In contrast to earlier techniques, both the generator and corrector
collaboratively and continuously improve their prompts over time. We also
introduce the concept of \textit{impact scores} to measure the sentence-level
effectiveness of the prompts. Our method was tested on four benchmarks, testing
the level of hallucinations in LLMs. Notably, we were able to increase the
accuracy of GPT-4 on GSM8K from 65.8\% to 94.1\% (28.3\% increase). SPT
advances LLMs by refining prompts to enhance performance and reduce
hallucinations, offering an efficient and scalable alternative to traditional
model fine-tuning.
\\ ( https://arxiv.org/abs/2403.18051 ,  8500kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18058
Date: Tue, 26 Mar 2024 19:24:18 GMT   (7301kb,D)

Title: COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning
Authors: Yuelin Bai, Xinrun Du, Yiming Liang, Yonggang Jin, Ziqiang Liu,
  Junting Zhou, Tianyu Zheng, Xincheng Zhang, Nuo Ma, Zekun Wang, Ruibin Yuan,
  Haihong Wu, Hongquan Lin, Wenhao Huang, Jiajun Zhang, Wenhu Chen, Chenghua
  Lin, Jie Fu, Min Yang, Shiwen Ni, Ge Zhang
Categories: cs.CL cs.AI
\\
  Recently, there have been significant advancements in large language models
(LLMs), particularly focused on the English language. These advancements have
enabled these LLMs to understand and execute complex instructions with
unprecedented accuracy and fluency. However, despite these advancements, there
remains a noticeable gap in the development of Chinese instruction tuning. The
unique linguistic features and cultural depth of the Chinese language pose
challenges for instruction tuning tasks. Existing datasets are either derived
from English-centric LLMs or are ill-suited for aligning with the interaction
patterns of real-world Chinese users. To bridge this gap, we introduce
COIG-CQIA, a high-quality Chinese instruction tuning dataset. Our aim is to
build a diverse, wide-ranging instruction-tuning dataset to better align model
behavior with human interactions. To this end, we collect a high-quality
human-written corpus from various sources on the Chinese Internet, including
Q&A communities, Wikis, examinations, and existing NLP datasets. This corpus
was rigorously filtered and carefully processed to form the COIG-CQIA dataset.
Furthermore, we train models of various scales on different subsets of CQIA,
following in-depth evaluation and analyses. The findings from our experiments
offer valuable insights for selecting and developing Chinese instruction-tuning
datasets. We also find that models trained on CQIA-Subset achieve competitive
results in human assessment as well as knowledge and security benchmarks. Data
are available at https://huggingface.co/datasets/m-a-p/COIG-CQIA
\\ ( https://arxiv.org/abs/2403.18058 ,  7301kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18093
Date: Tue, 26 Mar 2024 20:25:53 GMT   (969kb,D)

Title: Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large
  Language Models
Authors: Hai-Long Nguyen, Duc-Minh Nguyen, Tan-Minh Nguyen, Ha-Thanh Nguyen,
  Thi-Hai-Yen Vuong, Ken Satoh
Categories: cs.CL cs.AI
Comments: JURISIN 2024
\\
  Large language models with billions of parameters, such as GPT-3.5, GPT-4,
and LLaMA, are increasingly prevalent. Numerous studies have explored effective
prompting techniques to harness the power of these LLMs for various research
problems. Retrieval, specifically in the legal data domain, poses a challenging
task for the direct application of Prompting techniques due to the large number
and substantial length of legal articles. This research focuses on maximizing
the potential of prompting by placing it as the final phase of the retrieval
system, preceded by the support of two phases: BM25 Pre-ranking and BERT-based
Re-ranking. Experiments on the COLIEE 2023 dataset demonstrate that integrating
prompting techniques on LLMs into the retrieval system significantly improves
retrieval accuracy. However, error analysis reveals several existing issues in
the retrieval system that still need resolution.
\\ ( https://arxiv.org/abs/2403.18093 ,  969kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18098
Date: Tue, 26 Mar 2024 20:47:32 GMT   (435kb,D)

Title: GPTs and Language Barrier: A Cross-Lingual Legal QA Examination
Authors: Ha-Thanh Nguyen, Hiroaki Yamada, Ken Satoh
Categories: cs.CL cs.AI
Comments: NLP 2024, Kobe, Japan
\\
  In this paper, we explore the application of Generative Pre-trained
Transformers (GPTs) in cross-lingual legal Question-Answering (QA) systems
using the COLIEE Task 4 dataset. In the COLIEE Task 4, given a statement and a
set of related legal articles that serve as context, the objective is to
determine whether the statement is legally valid, i.e., if it can be inferred
from the provided contextual articles or not, which is also known as an
entailment task. By benchmarking four different combinations of English and
Japanese prompts and data, we provide valuable insights into GPTs' performance
in multilingual legal QA scenarios, contributing to the development of more
efficient and accurate cross-lingual QA solutions in the legal domain.
\\ ( https://arxiv.org/abs/2403.18098 ,  435kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18105
Date: Tue, 26 Mar 2024 21:04:29 GMT   (1688kb,D)

Title: Large Language Models for Education: A Survey and Outlook
Authors: Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang, Jiliang
  Tang, Philip S. Yu, Qingsong Wen
Categories: cs.CL cs.AI
\\
  The advent of Large Language Models (LLMs) has brought in a new era of
possibilities in the realm of education. This survey paper summarizes the
various technologies of LLMs in educational settings from multifaceted
perspectives, encompassing student and teacher assistance, adaptive learning,
and commercial tools. We systematically review the technological advancements
in each perspective, organize related datasets and benchmarks, and identify the
risks and challenges associated with deploying LLMs in education. Furthermore,
we outline future research opportunities, highlighting the potential promising
directions. Our survey aims to provide a comprehensive technological picture
for educators, researchers, and policymakers to harness the power of LLMs to
revolutionize educational practices and foster a more effective personalized
learning environment.
\\ ( https://arxiv.org/abs/2403.18105 ,  1688kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18121
Date: Tue, 26 Mar 2024 22:01:13 GMT   (1698kb,D)

Title: ChatGPT Role-play Dataset: Analysis of User Motives and Model
  Naturalness
Authors: Yufei Tao, Ameeta Agrawal, Judit Dombi, Tetyana Sydorenko, Jung In Lee
Categories: cs.CL cs.HC
Comments: Accepted by LREC-COLING 2024
\\
  Recent advances in interactive large language models like ChatGPT have
revolutionized various domains; however, their behavior in natural and
role-play conversation settings remains underexplored. In our study, we address
this gap by deeply investigating how ChatGPT behaves during conversations in
different settings by analyzing its interactions in both a normal way and a
role-play setting. We introduce a novel dataset of broad range of human-AI
conversations annotated with user motives and model naturalness to examine (i)
how humans engage with the conversational AI model, and (ii) how natural are AI
model responses. Our study highlights the diversity of user motives when
interacting with ChatGPT and variable AI naturalness, showing not only the
nuanced dynamics of natural conversations between humans and AI, but also
providing new avenues for improving the effectiveness of human-AI
communication.
\\ ( https://arxiv.org/abs/2403.18121 ,  1698kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18125
Date: Tue, 26 Mar 2024 22:08:33 GMT   (23kb)

Title: For those who don't know (how) to ask: Building a dataset of technology
  questions for digital newcomers
Authors: Evan Lucas, Kelly S. Steelman, Leo C. Ureel, Charles Wallace
Categories: cs.CL
Comments: Presented at the AI4ED workshop at AAAI 2024
\\
  While the rise of large language models (LLMs) has created rich new
opportunities to learn about digital technology, many on the margins of this
technology struggle to gain and maintain competency due to lexical or
conceptual barriers that prevent them from asking appropriate questions.
Although there have been many efforts to understand factuality of LLM-created
content and ability of LLMs to answer questions, it is not well understood how
unclear or nonstandard language queries affect the model outputs. We propose
the creation of a dataset that captures questions of digital newcomers and
outsiders, utilizing data we have compiled from a decade's worth of one-on-one
tutoring. In this paper we lay out our planned efforts and some potential uses
of this dataset.
\\ ( https://arxiv.org/abs/2403.18125 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18140
Date: Tue, 26 Mar 2024 22:54:12 GMT   (383kb,D)

Title: Juru: Legal Brazilian Large Language Model from Reputable Sources
Authors: Roseval Malaquias Junior, Ramon Pires, Roseli Romero and Rodrigo
  Nogueira
Categories: cs.CL cs.AI
\\
  The high computational cost associated with pretraining large language models
limits their research. Two strategies have emerged to address this issue:
domain specialization and pretraining with high-quality data. To explore these
strategies, we specialized the Sabi\'a-2 Small model with 1.9 billion unique
tokens from reputable Brazilian legal sources and conducted few-shot
evaluations on legal and general knowledge exams. Our model, Juru, demonstrates
the benefits of domain specialization with a reduced amount of pretraining
data. However, this specialization comes at the expense of degrading
performance in other knowledge areas within the same language. This study
contributes to the growing body of scientific evidence showing that pretraining
data selection may enhance the performance of large language models, enabling
the exploration of these models at a lower cost.
\\ ( https://arxiv.org/abs/2403.18140 ,  383kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18148
Date: Tue, 26 Mar 2024 23:14:34 GMT   (1851kb,D)

Title: Large Language Models Produce Responses Perceived to be Empathic
Authors: Yoon Kyung Lee, Jina Suh, Hongli Zhan, Junyi Jessy Li, Desmond C. Ong
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) have demonstrated surprising performance on many
tasks, including writing supportive messages that display empathy. Here, we had
these models generate empathic messages in response to posts describing common
life experiences, such as workplace situations, parenting, relationships, and
other anxiety- and anger-eliciting situations. Across two studies (N=192, 202),
we showed human raters a variety of responses written by several models (GPT4
Turbo, Llama2, and Mistral), and had people rate these responses on how
empathic they seemed to be. We found that LLM-generated responses were
consistently rated as more empathic than human-written responses. Linguistic
analyses also show that these models write in distinct, predictable ``styles",
in terms of their use of punctuation, emojis, and certain words. These results
highlight the potential of using LLMs to enhance human peer support in contexts
where empathy is important.
\\ ( https://arxiv.org/abs/2403.18148 ,  1851kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18152
Date: Tue, 26 Mar 2024 23:32:52 GMT   (1111kb,D)

Title: Large Language Models as Financial Data Annotators: A Study on
  Effectiveness and Efficiency
Authors: Toyin Aguda, Suchetha Siddagangappa, Elena Kochkina, Simerjot Kaur,
  Dongsheng Wang, Charese Smiley, Sameena Shah
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  Collecting labeled datasets in finance is challenging due to scarcity of
domain experts and higher cost of employing them. While Large Language Models
(LLMs) have demonstrated remarkable performance in data annotation tasks on
general domain datasets, their effectiveness on domain specific datasets
remains underexplored. To address this gap, we investigate the potential of
LLMs as efficient data annotators for extracting relations in financial
documents. We compare the annotations produced by three LLMs (GPT-4, PaLM 2,
and MPT Instruct) against expert annotators and crowdworkers. We demonstrate
that the current state-of-the-art LLMs can be sufficient alternatives to
non-expert crowdworkers. We analyze models using various prompts and parameter
settings and find that customizing the prompts for each relation group by
providing specific examples belonging to those groups is paramount.
Furthermore, we introduce a reliability index (LLM-RelIndex) used to identify
outputs that may require expert attention. Finally, we perform an extensive
time, cost and error analysis and provide recommendations for the collection
and usage of automated annotations in domain-specific settings.
\\ ( https://arxiv.org/abs/2403.18152 ,  1111kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18167
Date: Wed, 27 Mar 2024 00:23:03 GMT   (2257kb,D)

Title: Mechanisms of non-factual hallucinations in language models
Authors: Lei Yu, Meng Cao, Jackie Chi Kit Cheung, Yue Dong
Categories: cs.CL cs.AI
\\
  State-of-the-art language models (LMs) sometimes generate non-factual
hallucinations that misalign with world knowledge. Despite extensive efforts to
detect and mitigate hallucinations, understanding their internal mechanisms
remains elusive. Our study investigates the mechanistic causes of
hallucination, specifically non-factual ones where the LM incorrectly predicts
object attributes in response to subject-relation queries. With causal
mediation analysis and embedding space projection, we identify two general
mechanistic causes of hallucinations shared across LMs of various scales and
designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and
2) failing to select the correct object attribute in upper layer attention
heads and MLPs. These two mechanisms exhibit varying degrees of subject-object
association, predictive uncertainty and perturbation robustness. Additionally,
we scrutinize LM pre-training checkpoints, revealing distinct learning dynamics
for the two mechanistic causes of hallucinations. We also highlight how
attribution features from our causal analysis can effectively construct
hallucination detectors. Our work proposes a mechanistic understanding of LM
factual errors.
\\ ( https://arxiv.org/abs/2403.18167 ,  2257kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18182
Date: Wed, 27 Mar 2024 01:19:23 GMT   (6065kb,D)

Title: ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech
  Corpus
Authors: Injy Hamed, Fadhl Eryani, David Palfreyman, Nizar Habash
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  We present ZAEBUC-Spoken, a multilingual multidialectal Arabic-English speech
corpus. The corpus comprises twelve hours of Zoom meetings involving multiple
speakers role-playing a work situation where Students brainstorm ideas for a
certain topic and then discuss it with an Interlocutor. The meetings cover
different topics and are divided into phases with different language setups.
The corpus presents a challenging set for automatic speech recognition (ASR),
including two languages (Arabic and English) with Arabic spoken in multiple
variants (Modern Standard Arabic, Gulf Arabic, and Egyptian Arabic) and English
used with various accents. Adding to the complexity of the corpus, there is
also code-switching between these languages and dialects. As part of our work,
we take inspiration from established sets of transcription guidelines to
present a set of guidelines handling issues of conversational speech,
code-switching and orthography of both languages. We further enrich the corpus
with two layers of annotations; (1) dialectness level annotation for the
portion of the corpus where mixing occurs between different variants of Arabic,
and (2) automatic morphological annotations, including tokenization,
lemmatization, and part-of-speech tagging.
\\ ( https://arxiv.org/abs/2403.18182 ,  6065kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18249
Date: Wed, 27 Mar 2024 04:39:18 GMT   (4899kb,D)

Title: Exploring the Deceptive Power of LLM-Generated Fake News: A Study of
  Real-World Detection Challenges
Authors: Yanshen Sun, Jianfeng He, Limeng Cui, Shuo Lei, Chang-Tien Lu
Categories: cs.CL cs.SI
\\
  Recent advancements in Large Language Models (LLMs) have enabled the creation
of fake news, particularly in complex fields like healthcare. Studies highlight
the gap in the deceptive power of LLM-generated fake news with and without
human assistance, yet the potential of prompting techniques has not been fully
explored. Thus, this work aims to determine whether prompting strategies can
effectively narrow this gap. Current LLM-based fake news attacks require human
intervention for information gathering and often miss details and fail to
maintain context consistency. Therefore, to better understand threat tactics,
we propose a strong fake news attack method called conditional
Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods,
VLPrompt eliminates the need for additional data collection while maintaining
contextual coherence and preserving the intricacies of the original text. To
propel future research on detecting VLPrompt attacks, we created a new dataset
named VLPrompt fake news (VLPFN) containing real and fake texts. Our
experiments, including various detection methods and novel human study metrics,
were conducted to assess their performance on our dataset, yielding numerous
findings.
\\ ( https://arxiv.org/abs/2403.18249 ,  4899kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18251
Date: Wed, 27 Mar 2024 04:47:10 GMT   (70kb,D)

Title: Since the Scientific Literature Is Multilingual, Our Models Should Be
  Too
Authors: Abteen Ebrahimi and Kenneth Church
Categories: cs.CL
\\
  English has long been assumed the $\textit{lingua franca}$ of scientific
research, and this notion is reflected in the natural language processing (NLP)
research involving scientific document representation. In this position piece,
we quantitatively show that the literature is largely multilingual and argue
that current models and benchmarks should reflect this linguistic diversity. We
provide evidence that text-based models fail to create meaningful
representations for non-English papers and highlight the negative user-facing
impacts of using English-only models non-discriminately across a multilingual
domain. We end with suggestions for the NLP community on how to improve
performance on non-English documents.
\\ ( https://arxiv.org/abs/2403.18251 ,  70kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18253
Date: Wed, 27 Mar 2024 04:51:42 GMT   (7824kb,D)

Title: MD-PK: Metaphor Detection via Prompt Learning and Knowledge Distillation
Authors: Kaidi Jia and Rongsheng Li
Categories: cs.CL
\\
  Metaphors are ubiquitous in daily life, yet detecting them poses a
significant challenge. Previous approaches often struggled with improper
application of language rules and overlooked the issue of data sparsity. To
address these challenges, we introduce knowledge distillation and prompt
learning into metaphor detection. Specifically, we devise a prompt learning
template tailored for the metaphor detection task. By masking target words and
providing relevant prompt information, we guide the model to accurately infer
the contextual meaning of these words. This approach not only mitigates the
interference from the literal meaning of target words but also ensures the
proper utilization of MIP language rules for metaphor detection. Moreover, we
employ a teacher model equipped with prior knowledge to generate meaningful
soft labels, guiding the optimization process of the student model. The
inclusion of soft labels, akin to label smoothing, helps alleviate the model's
tendency towards over-confidence and effectively addresses the challenge of
data sparsity. Experimental results demonstrate that our proposed model
achieves state-of-the-art performance across multiple datasets.
\\ ( https://arxiv.org/abs/2403.18253 ,  7824kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18277
Date: Wed, 27 Mar 2024 06:13:04 GMT   (1187kb,D)

Title: BlendX: Complex Multi-Intent Detection with Blended Patterns
Authors: Yejin Yoon, Jungyeon Lee, Kangsan Kim, Chanhee Park and Taeuk Kim
Categories: cs.CL
Comments: Accepted to LREC-COLING2024
\\
  Task-oriented dialogue (TOD) systems are commonly designed with the
presumption that each utterance represents a single intent. However, this
assumption may not accurately reflect real-world situations, where users
frequently express multiple intents within a single utterance. While there is
an emerging interest in multi-intent detection (MID), existing in-domain
datasets such as MixATIS and MixSNIPS have limitations in their formulation. To
address these issues, we present BlendX, a suite of refined datasets featuring
more diverse patterns than their predecessors, elevating both its complexity
and diversity. For dataset construction, we utilize both rule-based heuristics
as well as a generative tool -- OpenAI's ChatGPT -- which is augmented with a
similarity-driven strategy for utterance selection. To ensure the quality of
the proposed datasets, we also introduce three novel metrics that assess the
statistical properties of an utterance related to word count, conjunction use,
and pronoun usage. Extensive experiments on BlendX reveal that state-of-the-art
MID models struggle with the challenges posed by the new datasets, highlighting
the need to reexamine the current state of the MID field. The dataset is
available at https://github.com/HYU-NLP/BlendX.
\\ ( https://arxiv.org/abs/2403.18277 ,  1187kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18286
Date: Wed, 27 Mar 2024 06:25:40 GMT   (1091kb,D)

Title: Few-Shot Recalibration of Language Models
Authors: Xiang Lisa Li and Urvashi Khandelwal and Kelvin Guu
Categories: cs.CL cs.AI cs.LG
Comments: preprint
\\
  Recent work has uncovered promising ways to extract well-calibrated
confidence estimates from language models (LMs), where the model's confidence
score reflects how likely it is to be correct. However, while LMs may appear
well-calibrated over broad distributions, this often hides significant
miscalibration within narrower slices (e.g., systemic over-confidence in math
can balance out systemic under-confidence in history, yielding perfect
calibration in aggregate). To attain well-calibrated confidence estimates for
any slice of a distribution, we propose a new framework for few-shot
slice-specific recalibration. Specifically, we train a recalibration model that
takes in a few unlabeled examples from any given slice and predicts a curve
that remaps confidence scores to be more accurate for that slice. Our trained
model can recalibrate for arbitrary new slices, without using any labeled data
from that slice. This enables us to identify domain-specific confidence
thresholds above which the LM's predictions can be trusted, and below which it
should abstain. Experiments show that our few-shot recalibrator consistently
outperforms existing calibration methods, for instance improving calibration
error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.
\\ ( https://arxiv.org/abs/2403.18286 ,  1091kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18295
Date: Wed, 27 Mar 2024 06:43:58 GMT   (2309kb,D)

Title: Dual Instruction Tuning with Large Language Models for Mathematical
  Reasoning
Authors: Yongwei Zhou, Tiejun Zhao
Categories: cs.CL
\\
  Recent advancements highlight the success of instruction tuning with large
language models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical
reasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as
incorrect, missing, and redundant steps in CoT generation leading to
inaccuracies in answer predictions. To alleviate this problem, we propose a
dual instruction tuning strategy to meticulously model mathematical reasoning
from both forward and reverse directions. This involves introducing the
Intermediate Reasoning State Prediction task (forward reasoning) and the
Instruction Reconstruction task (reverse reasoning) to enhance the LLMs'
understanding and execution of instructions. Training instances for these tasks
are constructed based on existing mathematical instruction tuning datasets.
Subsequently, LLMs undergo multi-task fine-tuning using both existing
mathematical instructions and the newly created data. Comprehensive experiments
validate the effectiveness and domain generalization of the dual instruction
tuning strategy across various mathematical reasoning tasks.
\\ ( https://arxiv.org/abs/2403.18295 ,  2309kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18314
Date: Wed, 27 Mar 2024 07:34:44 GMT   (8446kb,D)

Title: Chinese Offensive Language Detection:Current Status and Future
  Directions
Authors: Yunze Xiao, Houda Bouamor and Wajdi Zaghouani
Categories: cs.CL cs.AI
\\
  Despite the considerable efforts being made to monitor and regulate
user-generated content on social media platforms, the pervasiveness of
offensive language, such as hate speech or cyberbullying, in the digital space
remains a significant challenge. Given the importance of maintaining a
civilized and respectful online environment, there is an urgent and growing
need for automatic systems capable of detecting offensive speech in real time.
However, developing effective systems for processing languages such as Chinese
presents a significant challenge, owing to the language's complex and nuanced
nature, which makes it difficult to process automatically. This paper provides
a comprehensive overview of offensive language detection in Chinese, examining
current benchmarks and approaches and highlighting specific models and tools
for addressing the unique challenges of detecting offensive language in this
complex language. The primary objective of this survey is to explore the
existing techniques and identify potential avenues for further research that
can address the cultural and linguistic complexities of Chinese.
\\ ( https://arxiv.org/abs/2403.18314 ,  8446kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18327
Date: Wed, 27 Mar 2024 08:08:00 GMT   (312kb,D)

Title: Can LLMs Converse Formally? Automatically Assessing LLMs in Translating
  and Interpreting Formal Specifications
Authors: Rushang Karia, Daksh Dobhal, Daniel Bramblett, Pulkit Verma, Siddharth
  Srivastava
Categories: cs.CL cs.AI
\\
  Stakeholders often describe system requirements using natural language which
are then converted to formal syntax by a domain-expert leading to increased
design costs. This paper assesses the capabilities of Large Language Models
(LLMs) in converting between natural language descriptions and formal
specifications. Existing work has evaluated the capabilities of LLMs in
generating formal syntax such as source code but such experiments are typically
hand-crafted and use problems that are likely to be in the training set of
LLMs, and often require human-annotated datasets. We propose an approach that
can use two copies of an LLM in conjunction with an off-the-shelf verifier to
automatically evaluate its translation abilities without any additional human
input. Our approach generates formal syntax using language grammars to
automatically generate a dataset. We conduct an empirical evaluation to measure
the accuracy of this translation task and show that SOTA LLMs cannot adequately
solve this task, limiting their current utility in the design of complex
systems.
\\ ( https://arxiv.org/abs/2403.18327 ,  312kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18336
Date: Wed, 27 Mar 2024 08:21:01 GMT   (945kb,D)

Title: A Dataset for Pharmacovigilance in German, French, and Japanese:
  Annotating Adverse Drug Reactions across Languages
Authors: Lisa Raithel, Hui-Syuan Yeh, Shuntaro Yada, Cyril Grouin, Thomas
  Lavergne, Aur\'elie N\'ev\'eol, Patrick Paroubek, Philippe Thomas, Tomohiro
  Nishiyama, Sebastian M\"oller, Eiji Aramaki, Yuji Matsumoto, Roland Roller,
  Pierre Zweigenbaum
Categories: cs.CL cs.LG
Comments: Accepted at LREC-COLING 2024
\\
  User-generated data sources have gained significance in uncovering Adverse
Drug Reactions (ADRs), with an increasing number of discussions occurring in
the digital world. However, the existing clinical corpora predominantly revolve
around scientific articles in English. This work presents a multilingual corpus
of texts concerning ADRs gathered from diverse sources, including patient fora,
social media, and clinical reports in German, French, and Japanese. Our corpus
contains annotations covering 12 entity types, four attribute types, and 13
relation types. It contributes to the development of real-world multilingual
language models for healthcare. We provide statistics to highlight certain
challenges associated with the corpus and conduct preliminary experiments
resulting in strong baselines for extracting entities and relations between
these entities, both within and across languages.
\\ ( https://arxiv.org/abs/2403.18336 ,  945kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18341
Date: Wed, 27 Mar 2024 08:32:19 GMT   (7865kb,D)

Title: IterAlign: Iterative Constitutional Alignment of Large Language Models
Authors: Xiusi Chen, Hongzhi Wen, Sreyashi Nag, Chen Luo, Qingyu Yin, Ruirui
  Li, Zheng Li, Wei Wang
Categories: cs.CL
Comments: NAACL 2024
\\
  With the rapid development of large language models (LLMs), aligning LLMs
with human values and societal norms to ensure their reliability and safety has
become crucial. Reinforcement learning with human feedback (RLHF) and
Constitutional AI (CAI) have been proposed for LLM alignment. However, these
methods require either heavy human annotations or explicitly pre-defined
constitutions, which are labor-intensive and resource-consuming. To overcome
these drawbacks, we study constitution-based LLM alignment and propose a
data-driven constitution discovery and self-alignment framework called
IterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM
and automatically discovers new constitutions using a stronger LLM. These
constitutions are then used to guide self-correction of the base LLM. Such a
constitution discovery pipeline can be run iteratively and automatically to
discover new constitutions that specifically target the alignment gaps in the
current LLM. Empirical results on several safety benchmark datasets and
multiple base LLMs show that IterAlign successfully improves truthfulness,
helpfulness, harmlessness and honesty, improving the LLM alignment by up to
$13.5\%$ in harmlessness.
\\ ( https://arxiv.org/abs/2403.18341 ,  7865kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18346
Date: Wed, 27 Mar 2024 08:38:49 GMT   (5557kb,D)

Title: Quantifying and Mitigating Unimodal Biases in Multimodal Large Language
  Models: A Causal Perspective
Authors: Meiqi Chen, Yixin Cao, Yan Zhang, and Chaochao Lu
Categories: cs.CL cs.CV
\\
  Recent advancements in Large Language Models (LLMs) have facilitated the
development of Multimodal LLMs (MLLMs). Despite their impressive capabilities,
MLLMs often suffer from an over-reliance on unimodal biases (e.g., language
bias and vision bias), leading to incorrect answers in complex multimodal
tasks. To investigate this issue, we propose a causal framework to interpret
the biases in Visual Question Answering (VQA) problems. Within our framework,
we devise a causal graph to elucidate the predictions of MLLMs on VQA problems,
and assess the causal effect of biases through an in-depth causal analysis.
Motivated by the causal graph, we introduce a novel MORE dataset, consisting of
12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities,
necessitating multi-hop reasoning and the surmounting of unimodal biases.
Furthermore, we propose two strategies to mitigate unimodal biases and enhance
MLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA)
framework for limited-access MLLMs and the refinement of open-source MLLMs
through fine-tuning. Extensive quantitative and qualitative experiments offer
valuable insights for future research.
\\ ( https://arxiv.org/abs/2403.18346 ,  5557kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18349
Date: Wed, 27 Mar 2024 08:39:56 GMT   (3955kb,D)

Title: Rejection Improves Reliability: Training LLMs to Refuse Unknown
  Questions Using RL from Knowledge Feedback
Authors: Hongshen Xu, Zichen Zhu, Da Ma, Situo Zhang, Shuai Fan, Lu Chen, Kai
  Yu
Categories: cs.CL
\\
  Large Language Models (LLMs) often generate erroneous outputs, known as
hallucinations, due to their limitations in discerning questions beyond their
knowledge scope. While addressing hallucination has been a focal point in
research, previous efforts primarily concentrate on enhancing correctness
without giving due consideration to the significance of rejection mechanisms.
In this paper, we conduct a comprehensive examination of the role of rejection,
introducing the notion of model reliability along with corresponding metrics.
These metrics measure the model's ability to provide accurate responses while
adeptly rejecting questions exceeding its knowledge boundaries, thereby
minimizing hallucinations. To improve the inherent reliability of LLMs, we
present a novel alignment framework called Reinforcement Learning from
Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically
determine the model's knowledge boundary and trains a reliable reward model to
encourage the refusal of out-of-knowledge questions. Experimental results on
mathematical questions affirm the substantial efficacy of RLKF in significantly
enhancing LLM reliability.
\\ ( https://arxiv.org/abs/2403.18349 ,  3955kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18350
Date: Wed, 27 Mar 2024 08:42:31 GMT   (46kb,D)

Title: Evaluation of Semantic Search and its Role in
  Retrieved-Augmented-Generation (RAG) for Arabic Language
Authors: Ali Mahboub, Muhy Eddin Za'ter, Bashar Alfrou, Yazan Estaitia, Adnan
  Jaljuli, Asma Hakouz
Categories: cs.CL
\\
  The latest advancements in machine learning and deep learning have brought
forth the concept of semantic similarity, which has proven immensely beneficial
in multiple applications and has largely replaced keyword search. However,
evaluating semantic similarity and conducting searches for a specific query
across various documents continue to be a complicated task. This complexity is
due to the multifaceted nature of the task, the lack of standard benchmarks,
whereas these challenges are further amplified for Arabic language. This paper
endeavors to establish a straightforward yet potent benchmark for semantic
search in Arabic. Moreover, to precisely evaluate the effectiveness of these
metrics and the dataset, we conduct our assessment of semantic search within
the framework of retrieval augmented generation (RAG).
\\ ( https://arxiv.org/abs/2403.18350 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18365
Date: Wed, 27 Mar 2024 08:57:21 GMT   (1419kb,D)

Title: BLADE: Enhancing Black-box Large Language Models with Small
  Domain-Specific Models
Authors: Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Zhijing Wu, Yiqun Liu,
  Chong Chen, Qi Tian
Categories: cs.CL
Comments: 11pages
\\
  Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable
of addressing a diverse range of tasks. However, general LLMs, which are
developed on open-domain data, may lack the domain-specific knowledge essential
for tasks in vertical domains, such as legal, medical, etc. To address this
issue, previous approaches either conduct continuous pre-training with
domain-specific data or employ retrieval augmentation to support general LLMs.
Unfortunately, these strategies are either cost-intensive or unreliable in
practical applications. To this end, we present a novel framework named BLADE,
which enhances Black-box LArge language models with small Domain-spEcific
models. BLADE consists of a black-box LLM and a small domain-specific LM. The
small LM preserves domain-specific knowledge and offers specialized insights,
while the general LLM contributes robust language comprehension and reasoning
capabilities. Specifically, our method involves three steps: 1) pre-training
the small LM with domain-specific data, 2) fine-tuning this model using
knowledge instruction data, and 3) joint Bayesian optimization of the general
LLM and the small LM. Extensive experiments conducted on public legal and
medical benchmarks reveal that BLADE significantly outperforms existing
approaches. This shows the potential of BLADE as an effective and
cost-efficient solution in adapting general LLMs for vertical domains.
\\ ( https://arxiv.org/abs/2403.18365 ,  1419kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18381
Date: Wed, 27 Mar 2024 09:19:13 GMT   (351kb,D)

Title: Improving Attributed Text Generation of Large Language Models via
  Preference Learning
Authors: Dongfang Li, Zetian Sun, Baotian Hu, Zhenyu Liu, Xinshuo Hu, Xuebo
  Liu, Min Zhang
Categories: cs.CL cs.AI
Comments: 23 pages, 15 tables, 2 figures
\\
  Large language models have been widely adopted in natural language
processing, yet they face the challenge of generating unreliable content.
Recent works aim to reduce misinformation and hallucinations by resorting to
attribution as a means to provide evidence (i.e., citations). However, current
attribution methods usually focus on the retrieval stage and automatic
evaluation that neglect mirroring the citation mechanisms in human scholarly
writing to bolster credibility. In this paper, we address these challenges by
modelling the attribution task as preference learning and introducing an
Automatic Preference Optimization (APO) framework. First, we create a curated
collection for post-training with 6,330 examples by collecting and filtering
from existing datasets. Second, considering the high cost of labelling
preference data, we further propose an automatic method to synthesize
attribution preference data resulting in 95,263 pairs. Moreover, inspired by
the human citation process, we further propose a progressive preference
optimization method by leveraging fine-grained information. Extensive
experiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate
that APO achieves state-of-the-art citation F1 with higher answer quality.
\\ ( https://arxiv.org/abs/2403.18381 ,  351kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18421
Date: Wed, 27 Mar 2024 10:18:21 GMT   (375kb,D)

Title: BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text
Authors: Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall,
  Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang,
  Michael Carbin, Christopher D. Manning
Categories: cs.CL cs.AI
Comments: 23 pages
\\
  Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance
on a wide variety of biomedical NLP tasks. However, these models have hundreds
of billions of parameters, are computationally expensive to run, require users
to send their input data over the internet, and are trained on unknown data
sources. Can smaller, more targeted models compete? To address this question,
we build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive
model trained exclusively on PubMed abstracts and full articles. When
fine-tuned, BioMedLM can produce strong multiple-choice biomedical
question-answering results competitive with much larger models, such as
achieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical
Genetics exam. BioMedLM can also be fine-tuned to produce useful answers to
patient questions on medical topics. This demonstrates that smaller models can
potentially serve as transparent, privacy-preserving, economical and
environmentally friendly foundations for particular NLP applications, such as
in biomedicine. The model is available on the Hugging Face Hub:
https://huggingface.co/stanford-crfm/BioMedLM.
\\ ( https://arxiv.org/abs/2403.18421 ,  375kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18423
Date: Wed, 27 Mar 2024 10:24:25 GMT   (2253kb,D)

Title: SemRoDe: Macro Adversarial Training to Learn Representations That are
  Robust to Word-Level Attacks
Authors: Brian Formento, Wenjie Feng, Chuan Sheng Foo, Luu Anh Tuan, See-Kiong
  Ng
Categories: cs.CL cs.LG
Comments: Published in NAACL 2024 (Main Track)
\\
  Language models (LMs) are indispensable tools for natural language processing
tasks, but their vulnerability to adversarial attacks remains a concern. While
current research has explored adversarial training techniques, their
improvements to defend against word-level attacks have been limited. In this
work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a
Macro Adversarial Training strategy to enhance the robustness of LMs. Drawing
inspiration from recent studies in the image domain, we investigate and later
confirm that in a discrete data setting such as language, adversarial samples
generated via word substitutions do indeed belong to an adversarial domain
exhibiting a high Wasserstein distance from the base domain. Our method learns
a robust representation that bridges these two domains. We hypothesize that if
samples were not projected into an adversarial domain, but instead to a domain
with minimal shift, it would improve attack robustness. We align the domains by
incorporating a new distance-based objective. With this, our model is able to
learn more generalized representations by aligning the model's high-level
output features and therefore better handling unseen adversarial samples. This
method can be generalized across word embeddings, even when they share minimal
overlap at both vocabulary and word-substitution levels. To evaluate the
effectiveness of our approach, we conduct experiments on BERT and RoBERTa
models on three datasets. The results demonstrate promising state-of-the-art
robustness.
\\ ( https://arxiv.org/abs/2403.18423 ,  2253kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18426
Date: Wed, 27 Mar 2024 10:27:28 GMT   (2159kb,D)

Title: TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions
Authors: Jamshid Mozafari, Anubhav Jangra, Adam Jatowt
Categories: cs.CL
Comments: Accepted at SIGIR 2024
\\
  Nowadays, individuals tend to engage in dialogues with Large Language Models,
seeking answers to their questions. In times when such answers are readily
accessible to anyone, the stimulation and preservation of human's cognitive
abilities, as well as the assurance of maintaining good reasoning skills by
humans becomes crucial. This study addresses such needs by proposing hints
(instead of final answers or before giving answers) as a viable solution. We
introduce a framework for the automatic hint generation for factoid questions,
employing it to construct TriviaHG, a novel large-scale dataset featuring
160,230 hints corresponding to 16,645 questions from the TriviaQA dataset.
Additionally, we present an automatic evaluation method that measures the
Convergence and Familiarity quality attributes of hints. To evaluate the
TriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals
to annotate 2,791 hints and tasked 6 humans with answering questions using the
provided hints. The effectiveness of hints varied, with success rates of 96%,
78%, and 36% for questions with easy, medium, and hard answers, respectively.
Moreover, the proposed automatic evaluation methods showed a robust correlation
with annotators' results. Conclusively, the findings highlight three key
insights: the facilitative role of hints in resolving unknown questions, the
dependence of hint quality on answer difficulty, and the feasibility of
employing automatic evaluation methods for hint assessment.
\\ ( https://arxiv.org/abs/2403.18426 ,  2159kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18430
Date: Wed, 27 Mar 2024 10:36:17 GMT   (936kb,D)

Title: Exploring language relations through syntactic distances and geographic
  proximity
Authors: Juan De Gregorio, Ra\'ul Toral, David S\'anchez
Categories: cs.CL physics.data-an physics.soc-ph stat.AP
Comments: 36 pages
\\
  Languages are grouped into families that share common linguistic traits.
While this approach has been successful in understanding genetic relations
between diverse languages, more analyses are needed to accurately quantify
their relatedness, especially in less studied linguistic levels such as syntax.
Here, we explore linguistic distances using series of parts of speech (POS)
extracted from the Universal Dependencies dataset. Within an
information-theoretic framework, we show that employing POS trigrams maximizes
the possibility of capturing syntactic variations while being at the same time
compatible with the amount of available data. Linguistic connections are then
established by assessing pairwise distances based on the POS distributions.
Intriguingly, our analysis reveals definite clusters that correspond to well
known language families and groups, with exceptions explained by distinct
morphological typologies. Furthermore, we obtain a significant correlation
between language similarity and geographic distance, which underscores the
influence of spatial proximity on language kinships.
\\ ( https://arxiv.org/abs/2403.18430 ,  936kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18447
Date: Wed, 27 Mar 2024 11:06:44 GMT   (2972kb,D)

Title: Can Language Beat Numerical Regression? Language-Based Multimodal
  Trajectory Prediction
Authors: Inhwan Bae and Junoh Lee and Hae-Gon Jeon
Categories: cs.CL cs.CV cs.LG cs.RO
Comments: Accepted at CVPR 2024
\\
  Language models have demonstrated impressive ability in context understanding
and generative performance. Inspired by the recent success of language
foundation models, in this paper, we propose LMTraj (Language-based Multimodal
Trajectory predictor), which recasts the trajectory prediction task into a sort
of question-answering problem. Departing from traditional numerical regression
models, which treat the trajectory coordinate sequence as continuous signals,
we consider them as discrete signals like text prompts. Specially, we first
transform an input space for the trajectory coordinate into the natural
language space. Here, the entire time-series trajectories of pedestrians are
converted into a text prompt, and scene images are described as text
information through image captioning. The transformed numerical and image data
are then wrapped into the question-answering template for use in a language
model. Next, to guide the language model in understanding and reasoning
high-level knowledge, such as scene context and social relationships between
pedestrians, we introduce an auxiliary multi-task question and answering. We
then train a numerical tokenizer with the prompt data. We encourage the
tokenizer to separate the integer and decimal parts well, and leverage it to
capture correlations between the consecutive numbers in the language model.
Lastly, we train the language model using the numerical tokenizer and all of
the question-answer prompts. Here, we propose a beam-search-based most-likely
prediction and a temperature-based multimodal prediction to implement both
deterministic and stochastic inferences. Applying our LMTraj, we show that the
language-based model can be a powerful pedestrian trajectory predictor, and
outperforms existing numerical-based predictor methods. Code is publicly
available at https://github.com/inhwanbae/LMTrajectory .
\\ ( https://arxiv.org/abs/2403.18447 ,  2972kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18504
Date: Wed, 27 Mar 2024 12:33:42 GMT   (461kb)

Title: AcTED: Automatic Acquisition of Typical Event Duration for
  Semi-supervised Temporal Commonsense QA
Authors: Felix Virgo, Fei Cheng, Lis Kanashiro Pereira, Masayuki Asahara,
  Ichiro Kobayashi and Sadao Kurohashi
Categories: cs.CL
\\
  We propose a voting-driven semi-supervised approach to automatically acquire
the typical duration of an event and use it as pseudo-labeled data. The human
evaluation demonstrates that our pseudo labels exhibit surprisingly high
accuracy and balanced coverage. In the temporal commonsense QA task,
experimental results show that using only pseudo examples of 400 events, we
achieve performance comparable to the existing BERT-based weakly supervised
approaches that require a significant amount of training examples. When
compared to the RoBERTa baselines, our best approach establishes
state-of-the-art performance with a 7% improvement in Exact Match.
\\ ( https://arxiv.org/abs/2403.18504 ,  461kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18542
Date: Wed, 27 Mar 2024 13:22:38 GMT   (2128kb,D)

Title: Attention-aware semantic relevance predicting Chinese sentence reading
Authors: Kun Sun
Categories: cs.CL cs.LG
\\
  In recent years, several influential computational models and metrics have
been proposed to predict how humans comprehend and process sentence. One
particularly promising approach is contextual semantic similarity. Inspired by
the attention algorithm in Transformer and human memory mechanisms, this study
proposes an ``attention-aware'' approach for computing contextual semantic
relevance. This new approach takes into account the different contributions of
contextual parts and the expectation effect, allowing it to incorporate
contextual information fully. The attention-aware approach also facilitates the
simulation of existing reading models and evaluate them. The resulting
``attention-aware'' metrics of semantic relevance can more accurately predict
fixation durations in Chinese reading tasks recorded in an eye-tracking corpus
than those calculated by existing approaches. The study's findings further
provide strong support for the presence of semantic preview benefits in Chinese
naturalistic reading. Furthermore, the attention-aware metrics of semantic
relevance, being memory-based, possess high interpretability from both
linguistic and cognitive standpoints, making them a valuable computational tool
for modeling eye-movements in reading and further gaining insight into the
process of language comprehension. Our approach underscores the potential of
these metrics to advance our comprehension of how humans understand and process
language, ultimately leading to a better understanding of language
comprehension and processing.
\\ ( https://arxiv.org/abs/2403.18542 ,  2128kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18555
Date: Wed, 27 Mar 2024 13:34:59 GMT   (926kb,D)

Title: Debiasing Sentence Embedders through Contrastive Word Pairs
Authors: Philip Kenneweg, Sarah Schr\"oder, Alexander Schulz, Barbara Hammer
Categories: cs.CL
DOI: 10.5220/0011615300003411
\\
  Over the last years, various sentence embedders have been an integral part in
the success of current machine learning approaches to Natural Language
Processing (NLP). Unfortunately, multiple sources have shown that the bias,
inherent in the datasets upon which these embedding methods are trained, is
learned by them. A variety of different approaches to remove biases in
embeddings exists in the literature. Most of these approaches are applicable to
word embeddings and in fewer cases to sentence embeddings. It is problematic
that most debiasing approaches are directly transferred from word embeddings,
therefore these approaches fail to take into account the nonlinear nature of
sentence embedders and the embeddings they produce. It has been shown in
literature that bias information is still present if sentence embeddings are
debiased using such methods. In this contribution, we explore an approach to
remove linear and nonlinear bias information for NLP solutions, without
impacting downstream performance. We compare our approach to common debiasing
methods on classical bias metrics and on bias metrics which take nonlinear
information into account.
\\ ( https://arxiv.org/abs/2403.18555 ,  926kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18647
Date: Wed, 27 Mar 2024 14:54:27 GMT   (5384kb,D)

Title: SDSAT: Accelerating LLM Inference through Speculative Decoding with
  Semantic Adaptive Tokens
Authors: Chengbo Liu, Yong Zhu
Categories: cs.CL
Comments: 12 pages, 7 figures
\\
  We propose an acceleration scheme for large language models (LLMs) through
Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary
objective of this design is to enhance the LLM model's ability to generate
draft tokens more accurately without compromising the model's accuracy. The
core strategies involve: 1) Fine-tune the model by incorporating semantic
adaptive tokens that possess flexible decoding capabilities without changing
its structure, allowing them to generate high-quality draft tokens. 2) By
employing a training method that does not affect the standard tokens, the model
can acquire parallel decoding abilities atop its original framework with
minimal training overhead. 3) We have designed the "two-step-draft-then-verify"
generation strategies using both greedy search and nucleus sampling.
Experiments conducted on the CodeLlama-13B and 7B models have yielded speed
increases of over 3.5X and 3.0X, respectively. Please refer to
https://github.com/hasuoshenyun/SDSAT.
\\ ( https://arxiv.org/abs/2403.18647 ,  5384kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18671
Date: Wed, 27 Mar 2024 15:15:14 GMT   (2525kb,D)

Title: Fact Checking Beyond Training Set
Authors: Payam Karisani, Heng Ji
Categories: cs.CL cs.LG
Comments: NAACL 2024
\\
  Evaluating the veracity of everyday claims is time consuming and in some
cases requires domain expertise. We empirically demonstrate that the commonly
used fact checking pipeline, known as the retriever-reader, suffers from
performance deterioration when it is trained on the labeled data from one
domain and used in another domain. Afterwards, we delve into each component of
the pipeline and propose novel algorithms to address this problem. We propose
an adversarial algorithm to make the retriever component robust against
distribution shift. Our core idea is to initially train a bi-encoder on the
labeled source data, and then, to adversarially train two separate document and
claim encoders using unlabeled target data. We then focus on the reader
component and propose to train it such that it is insensitive towards the order
of claims and evidence documents. Our empirical evaluations support the
hypothesis that such a reader shows a higher robustness against distribution
shift. To our knowledge, there is no publicly available multi-topic fact
checking dataset. Thus, we propose a simple automatic method to re-purpose two
well-known fact checking datasets. We then construct eight fact checking
scenarios from these datasets, and compare our model to a set of strong
baseline models, including recent domain adaptation models that use GPT4 for
generating synthetic data.
\\ ( https://arxiv.org/abs/2403.18671 ,  2525kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18680
Date: Wed, 27 Mar 2024 15:22:16 GMT   (282kb,D)

Title: NL-ITI: Optimizing Probing and Intervention for Improvement of ITI
  Method
Authors: Jakub Hoscilowicz, Adam Wiacek, Jan Chojnacki, Adam Cieslak, Leszek
  Michon, Vitalii Urbanevych, Artur Janicki
Categories: cs.CL cs.LG
Comments: Code is available at https://github.com/Samsung/NL-ITI
\\
  Large Language Models (LLM) are prone to returning false information. It
constitutes one of major challenges in the AI field. In our work, we explore
paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it
identifies attention heads, which contain the highest amount of desired type of
knowledge (e.g., truthful). Afterwards, during inference, LLM activations are
shifted for chosen subset of attention heads. We further improved the ITI
framework by introducing a nonlinear probing and multi-token intervention -
Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice
benchmarks, including TruthfulQA, on which we report around 14% MC1 metric
improvement with respect to the baseline ITI results. NL-ITI achieves also
encouraging results on other testsets - on Business Ethics subdomain of MMLU,
around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI
performs better while being less invasive in the behavior of LLM at the same
time (as measured by Kullback-Leibler divergence).
\\ ( https://arxiv.org/abs/2403.18680 ,  282kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18697
Date: Wed, 27 Mar 2024 15:46:25 GMT   (103kb,D)

Title: The Invalsi Benchmark: measuring Language Models Mathematical and
  Language understanding in Italian
Authors: Andrea Esuli and Giovanni Puccetti
Categories: cs.CL
\\
  While Italian is by all metrics a high resource language, currently, there
are isn't a Language Model pre-trained exclusively in this language. This
results in a lower number of available benchmarks to evaluate the performance
of language models in Italian.
  This work presents two new benchmarks to evaluate the models performance on
mathematical understanding and language understanding in Italian. These
benchmarks are based on real tests that are undertaken by students of age
between 11 and 18 within the Italian school system and have therefore been
validated by several experts in didactics and pedagogy.
  To validate this dataset we evaluate the performance of 9 language models
that are the best performing when writing in Italian, including our own
fine-tuned models. We show that this is a challenging benchmark where current
language models are bound by 60\% accuracy.
  We believe that the release of this dataset paves the way for improving
future models mathematical and language understanding in Italian.
\\ ( https://arxiv.org/abs/2403.18697 ,  103kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18769
Date: Wed, 27 Mar 2024 17:13:38 GMT   (496kb)

Title: Improved Neural Protoform Reconstruction via Reflex Prediction
Authors: Liang Lu, Jingzhi Wang, David R. Mortensen
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  Protolanguage reconstruction is central to historical linguistics. The
comparative method, one of the most influential theoretical and methodological
frameworks in the history of the language sciences, allows linguists to infer
protoforms (reconstructed ancestral words) from their reflexes (related modern
words) based on the assumption of regular sound change. Not surprisingly,
numerous computational linguists have attempted to operationalize comparative
reconstruction through various computational models, the most successful of
which have been supervised encoder-decoder models, which treat the problem of
predicting protoforms given sets of reflexes as a sequence-to-sequence problem.
We argue that this framework ignores one of the most important aspects of the
comparative method: not only should protoforms be inferable from cognate sets
(sets of related reflexes) but the reflexes should also be inferable from the
protoforms. Leveraging another line of research -- reflex prediction -- we
propose a system in which candidate protoforms from a reconstruction model are
reranked by a reflex prediction model. We show that this more complete
implementation of the comparative method allows us to surpass state-of-the-art
protoform reconstruction methods on three of four Chinese and Romance datasets.
\\ ( https://arxiv.org/abs/2403.18769 ,  496kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18771
Date: Wed, 27 Mar 2024 17:20:39 GMT   (2242kb,D)

Title: CheckEval: Robust Evaluation Framework using Large Language Model via
  Checklist
Authors: Yukyung Lee, Joonghoon Kim, Jaehee Kim, Hyowon Cho, Pilsung Kang
Categories: cs.CL
Comments: HEAL at CHI 2024
\\
  We introduce CheckEval, a novel evaluation framework using Large Language
Models, addressing the challenges of ambiguity and inconsistency in current
evaluation methods. CheckEval addresses these challenges by dividing evaluation
criteria into detailed sub-aspects and constructing a checklist of Boolean
questions for each, simplifying the evaluation. This approach not only renders
the process more interpretable but also significantly enhances the robustness
and reliability of results by focusing on specific evaluation dimensions.
Validated through a focused case study using the SummEval benchmark, CheckEval
indicates a strong correlation with human judgments. Furthermore, it
demonstrates a highly consistent Inter-Annotator Agreement. These findings
highlight the effectiveness of CheckEval for objective, flexible, and precise
evaluations. By offering a customizable and interactive framework, CheckEval
sets a new standard for the use of LLMs in evaluation, responding to the
evolving needs of the field and establishing a clear method for future
LLM-based evaluation.
\\ ( https://arxiv.org/abs/2403.18771 ,  2242kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18783
Date: Wed, 27 Mar 2024 17:31:39 GMT   (1223kb,D)

Title: Towards a World-English Language Model for On-Device Virtual Assistants
Authors: Rricha Jalota, Lyan Verwimp, Markus Nussbaum-Thom, Amr Mousa, Arturo
  Argueta and Youssef Oualil
Categories: cs.CL
Comments: Accepted in ICASSP 2024
DOI: 10.1109/ICASSP48485.2024.10448018
\\
  Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are
generally language-, region-, and in some cases, device-dependent, which
increases the effort to scale and maintain them. Combining NNLMs for one or
more of the categories is one way to improve scalability. In this work, we
combine regional variants of English to build a ``World English'' NNLM for
on-device VAs. In particular, we investigate the application of adapter
bottlenecks to model dialect-specific characteristics in our existing
production NNLMs {and enhance the multi-dialect baselines}. We find that
adapter modules are more effective in modeling dialects than specializing
entire sub-networks. Based on this insight and leveraging the design of our
production models, we introduce a new architecture for World English NNLM that
meets the accuracy, latency, and memory constraints of our single-dialect
models.
\\ ( https://arxiv.org/abs/2403.18783 ,  1223kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18802
Date: Wed, 27 Mar 2024 17:48:55 GMT   (236kb,D)

Title: Long-form factuality in large language models
Authors: Jerry Wei and Chengrun Yang and Xinying Song and Yifeng Lu and Nathan
  Hu and Dustin Tran and Daiyi Peng and Ruibo Liu and Da Huang and Cosmo Du and
  Quoc V. Le
Categories: cs.CL cs.AI cs.LG
\\
  Large language models (LLMs) often generate content that contains factual
errors when responding to fact-seeking prompts on open-ended topics. To
benchmark a model's long-form factuality in open domains, we first use GPT-4 to
generate LongFact, a prompt set comprising thousands of questions spanning 38
topics. We then propose that LLM agents can be used as automated evaluators for
long-form factuality through a method which we call Search-Augmented Factuality
Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into
a set of individual facts and to evaluate the accuracy of each fact using a
multi-step reasoning process comprising sending search queries to Google Search
and determining whether a fact is supported by the search results. Furthermore,
we propose extending F1 score as an aggregated metric for long-form factuality.
To do so, we balance the percentage of supported facts in a response
(precision) with the percentage of provided facts relative to a hyperparameter
representing a user's preferred response length (recall).
  Empirically, we demonstrate that LLM agents can achieve superhuman rating
performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced
human annotators 72% of the time, and on a random subset of 100 disagreement
cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times
cheaper than human annotators. We also benchmark thirteen language models on
LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding
that larger language models generally achieve better long-form factuality.
LongFact, SAFE, and all experimental code are available at
https://github.com/google-deepmind/long-form-factuality.
\\ ( https://arxiv.org/abs/2403.18802 ,  236kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18803
Date: Wed, 27 Mar 2024 17:49:31 GMT   (137kb,D)

Title: Projective Methods for Mitigating Gender Bias in Pre-trained Language
  Models
Authors: Hillary Dawkins, Isar Nejadgholi, Daniel Gillis, and Judi McCuaig
Categories: cs.CL
\\
  Mitigation of gender bias in NLP has a long history tied to debiasing static
word embeddings. More recently, attention has shifted to debiasing pre-trained
language models. We study to what extent the simplest projective debiasing
methods, developed for word embeddings, can help when applied to BERT's
internal representations. Projective methods are fast to implement, use a small
number of saved parameters, and make no updates to the existing model
parameters. We evaluate the efficacy of the methods in reducing both intrinsic
bias, as measured by BERT's next sentence prediction task, and in mitigating
observed bias in a downstream setting when fine-tuned. To this end, we also
provide a critical analysis of a popular gender-bias assessment test for
quantifying intrinsic bias, resulting in an enhanced test set and new bias
measures. We find that projective methods can be effective at both intrinsic
bias and downstream bias mitigation, but that the two outcomes are not
necessarily correlated. This finding serves as a warning that intrinsic bias
test sets, based either on language modeling tasks or next sentence prediction,
should not be the only benchmark in developing a debiased language model.
\\ ( https://arxiv.org/abs/2403.18803 ,  137kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18804
Date: Wed, 27 Mar 2024 17:50:00 GMT   (169kb,D)

Title: Is Modularity Transferable? A Case Study through the Lens of Knowledge
  Distillation
Authors: Mateusz Klimaszewski, Piotr Andruszkiewicz, Alexandra Birch
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  The rise of Modular Deep Learning showcases its potential in various Natural
Language Processing applications. Parameter-efficient fine-tuning (PEFT)
modularity has been shown to work for various use cases, from domain adaptation
to multilingual setups. However, all this work covers the case where the
modular components are trained and deployed within one single Pre-trained
Language Model (PLM). This model-specific setup is a substantial limitation on
the very modularity that modular architectures are trying to achieve. We ask
whether current modular approaches are transferable between models and whether
we can transfer the modules from more robust and larger PLMs to smaller ones.
In this work, we aim to fill this gap via a lens of Knowledge Distillation,
commonly used for model compression, and present an extremely straightforward
approach to transferring pre-trained, task-specific PEFT modules between
same-family PLMs. Moreover, we propose a method that allows the transfer of
modules between incompatible PLMs without any change in the inference
complexity. The experiments on Named Entity Recognition, Natural Language
Inference, and Paraphrase Identification tasks over multiple languages and PEFT
methods showcase the initial potential of transferable modularity.
\\ ( https://arxiv.org/abs/2403.18804 ,  169kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17954
Date: Sun, 10 Mar 2024 16:49:04 GMT   (1772kb,D)

Title: Sort & Slice: A Simple and Superior Alternative to Hash-Based Folding
  for Extended-Connectivity Fingerprints
Authors: Markus Dablander, Thierry Hanser, Renaud Lambiotte, Garrett M. Morris
Categories: cs.LG physics.chem-ph q-bio.BM
Comments: Submitted to Journal of Cheminformatics
\\
  Extended-connectivity fingerprints (ECFPs) are a ubiquitous tool in current
cheminformatics and molecular machine learning, and one of the most prevalent
molecular feature extraction techniques used for chemical prediction. Atom
features learned by graph neural networks can be aggregated to compound-level
representations using a large spectrum of graph pooling methods; in contrast,
sets of detected ECFP substructures are by default transformed into bit vectors
using only a simple hash-based folding procedure. We introduce a general
mathematical framework for the vectorisation of structural fingerprints via a
formal operation called substructure pooling that encompasses hash-based
folding, algorithmic substructure-selection, and a wide variety of other
potential techniques. We go on to describe Sort & Slice, an easy-to-implement
and bit-collision-free alternative to hash-based folding for the pooling of
ECFP substructures. Sort & Slice first sorts ECFP substructures according to
their relative prevalence in a given set of training compounds and then slices
away all but the $L$ most frequent substructures which are subsequently used to
generate a binary fingerprint of desired length, $L$. We computationally
compare the performance of hash-based folding, Sort & Slice, and two advanced
supervised substructure-selection schemes (filtering and mutual-information
maximisation) for ECFP-based molecular property prediction. Our results
indicate that, despite its technical simplicity, Sort & Slice robustly (and at
times substantially) outperforms traditional hash-based folding as well as the
other investigated methods across prediction tasks, data splitting techniques,
machine-learning models and ECFP hyperparameters. We thus recommend that Sort &
Slice canonically replace hash-based folding as the default
substructure-pooling technique to vectorise ECFPs for supervised molecular
machine learning.
\\ ( https://arxiv.org/abs/2403.17954 ,  1772kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17958
Date: Tue, 12 Mar 2024 22:45:05 GMT   (7844kb,D)

Title: Deep Generative Domain Adaptation with Temporal Attention for Cross-User
  Activity Recognition
Authors: Xiaozhou Ye, Kevin I-Kai Wang
Categories: cs.LG cs.AI cs.CV cs.HC
\\
  In Human Activity Recognition (HAR), a predominant assumption is that the
data utilized for training and evaluation purposes are drawn from the same
distribution. It is also assumed that all data samples are independent and
identically distributed ($\displaystyle i.i.d.$). Contrarily, practical
implementations often challenge this notion, manifesting data distribution
discrepancies, especially in scenarios such as cross-user HAR. Domain
adaptation is the promising approach to address these challenges inherent in
cross-user HAR tasks. However, a clear gap in domain adaptation techniques is
the neglect of the temporal relation embedded within time series data during
the phase of aligning data distributions. Addressing this oversight, our
research presents the Deep Generative Domain Adaptation with Temporal Attention
(DGDATA) method. This novel method uniquely recognises and integrates temporal
relations during the domain adaptation process. By synergizing the capabilities
of generative models with the Temporal Relation Attention mechanism, our method
improves the classification performance in cross-user HAR. A comprehensive
evaluation has been conducted on three public sensor-based HAR datasets
targeting different scenarios and applications to demonstrate the efficacy of
the proposed DGDATA method.
\\ ( https://arxiv.org/abs/2403.17958 ,  7844kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17993
Date: Tue, 26 Mar 2024 12:45:52 GMT   (23026kb,D)

Title: Mixing Artificial and Natural Intelligence: From Statistical Mechanics
  to AI and Back to Turbulence
Authors: Michael (Misha) Chertkov
Categories: cs.LG cond-mat.stat-mech cs.AI physics.flu-dyn
Comments: 35 pages, 9 figures
\\
  The paper reflects on the future role of AI in scientific research, with a
special focus on turbulence studies, and examines the evolution of AI,
particularly through Diffusion Models rooted in non-equilibrium statistical
mechanics. It underscores the significant impact of AI on advancing reduced,
Lagrangian models of turbulence through innovative use of deep neural networks.
Additionally, the paper reviews various other AI applications in turbulence
research and outlines potential challenges and opportunities in the concurrent
advancement of AI and statistical hydrodynamics. This discussion sets the stage
for a future where AI and turbulence research are intricately intertwined,
leading to more profound insights and advancements in both fields.
\\ ( https://arxiv.org/abs/2403.17993 ,  23026kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18028
Date: Tue, 26 Mar 2024 18:29:39 GMT   (3687kb,D)

Title: Predicting species occurrence patterns from partial observations
Authors: Hager Radi Abdelwahed, M\'elisande Teng, David Rolnick
Categories: cs.LG cs.AI cs.CV q-bio.PE
Comments: Tackling Climate Change with Machine Learning workshop at ICLR 2024
\\
  To address the interlinked biodiversity and climate crises, we need an
understanding of where species occur and how these patterns are changing.
However, observational data on most species remains very limited, and the
amount of data available varies greatly between taxonomic groups. We introduce
the problem of predicting species occurrence patterns given (a) satellite
imagery, and (b) known information on the occurrence of other species. To
evaluate algorithms on this task, we introduce SatButterfly, a dataset of
satellite images, environmental data and observational data for butterflies,
which is designed to pair with the existing SatBird dataset of bird
observational data. To address this task, we propose a general model, R-Tran,
for predicting species occurrence patterns that enables the use of partial
observational data wherever found. We find that R-Tran outperforms other
methods in predicting species encounter rates with partial information both
within a taxon (birds) and across taxa (birds and butterflies). Our approach
opens new perspectives to leveraging insights from species with abundant data
to other species with scarce data, by modelling the ecosystems in which they
co-occur.
\\ ( https://arxiv.org/abs/2403.18028 ,  3687kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18035
Date: Tue, 26 Mar 2024 18:40:36 GMT   (13322kb,D)

Title: Bidirectional Consistency Models
Authors: Liangchen Li and Jiajun He
Categories: cs.LG cs.CV
Comments: 40 pages, 25 figures
\\
  Diffusion models (DMs) are capable of generating remarkably high-quality
samples by iteratively denoising a random vector, a process that corresponds to
moving along the probability flow ordinary differential equation (PF ODE).
Interestingly, DMs can also invert an input image to noise by moving backward
along the PF ODE, a key operation for downstream tasks such as interpolation
and image editing. However, the iterative nature of this process restricts its
speed, hindering its broader application. Recently, Consistency Models (CMs)
have emerged to address this challenge by approximating the integral of the PF
ODE, thereby bypassing the need to iterate. Yet, the absence of an explicit ODE
solver complicates the inversion process. To resolve this, we introduce the
Bidirectional Consistency Model (BCM), which learns a single neural network
that enables both forward and backward traversal along the PF ODE, efficiently
unifying generation and inversion tasks within one framework. Notably, our
proposed method enables one-step generation and inversion while also allowing
the use of additional steps to enhance generation quality or reduce
reconstruction error. Furthermore, by leveraging our model's bidirectional
consistency, we introduce a sampling strategy that can enhance FID while
preserving the generated image content. We further showcase our model's
capabilities in several downstream tasks, such as interpolation and inpainting,
and present demonstrations of potential applications, including blind
restoration of compressed images and defending black-box adversarial attacks.
\\ ( https://arxiv.org/abs/2403.18035 ,  13322kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18103
Date: Tue, 26 Mar 2024 21:01:41 GMT   (3221kb,D)

Title: Tutorial on Diffusion Models for Imaging and Vision
Authors: Stanley H. Chan
Categories: cs.LG cs.CV
\\
  The astonishing growth of generative tools in recent years has empowered many
exciting applications in text-to-image generation and text-to-video generation.
The underlying principle behind these generative tools is the concept of
diffusion, a particular sampling mechanism that has overcome some shortcomings
that were deemed difficult in the previous approaches. The goal of this
tutorial is to discuss the essential ideas underlying the diffusion models. The
target audience of this tutorial includes undergraduate and graduate students
who are interested in doing research on diffusion models or applying these
models to solve other problems.
\\ ( https://arxiv.org/abs/2403.18103 ,  3221kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18127
Date: Tue, 26 Mar 2024 22:15:47 GMT   (8kb)

Title: A Correction of Pseudo Log-Likelihood Method
Authors: Shi Feng, Nuoya Xiong, Zhijie Zhang, Wei Chen
Categories: cs.LG math.ST stat.ML stat.TH
Comments: 7 pages
\\
  Pseudo log-likelihood is a type of maximum likelihood estimation (MLE) method
used in various fields including contextual bandits, influence maximization of
social networks, and causal bandits. However, in previous literature
\citep{li2017provably, zhang2022online, xiong2022combinatorial,
feng2023combinatorial1, feng2023combinatorial2}, the log-likelihood function
may not be bounded, which may result in the algorithm they proposed not
well-defined. In this paper, we give a counterexample that the maximum pseudo
log-likelihood estimation fails and then provide a solution to correct the
algorithms in \citep{li2017provably, zhang2022online, xiong2022combinatorial,
feng2023combinatorial1, feng2023combinatorial2}.
\\ ( https://arxiv.org/abs/2403.18127 ,  8kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18128
Date: Tue, 26 Mar 2024 22:17:01 GMT   (998kb,D)

Title: HealthGAT: Node Classifications in Electronic Health Records using Graph
  Attention Networks
Authors: Fahmida Liza Piya and Mehak Gupta and Rahmatollah Beheshti
Categories: cs.LG cs.CY
\\
  While electronic health records (EHRs) are widely used across various
applications in healthcare, most applications use the EHRs in their raw
(tabular) format. Relying on raw or simple data pre-processing can greatly
limit the performance or even applicability of downstream tasks using EHRs. To
address this challenge, we present HealthGAT, a novel graph attention network
framework that utilizes a hierarchical approach to generate embeddings from
EHR, surpassing traditional graph-based methods. Our model iteratively refines
the embeddings for medical codes, resulting in improved EHR data analysis. We
also introduce customized EHR-centric auxiliary pre-training tasks to leverage
the rich medical knowledge embedded within the data. This approach provides a
comprehensive analysis of complex medical relationships and offers significant
advancement over standard data representation techniques. HealthGAT has
demonstrated its effectiveness in various healthcare scenarios through
comprehensive evaluations against established methodologies. Specifically, our
model shows outstanding performance in node classification and downstream tasks
such as predicting readmissions and diagnosis classifications.
  Our code is available at https://github.com/healthylaife/HealthGAT
\\ ( https://arxiv.org/abs/2403.18128 ,  998kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18132
Date: Tue, 26 Mar 2024 22:26:39 GMT   (31318kb,D)

Title: Recommendation of data-free class-incremental learning algorithms by
  simulating future data
Authors: Eva Feillet, Adrian Popescu, C\'eline Hudelot
Categories: cs.LG cs.AI cs.CV
\\
  Class-incremental learning deals with sequential data streams composed of
batches of classes. Various algorithms have been proposed to address the
challenging case where samples from past classes cannot be stored. However,
selecting an appropriate algorithm for a user-defined setting is an open
problem, as the relative performance of these algorithms depends on the
incremental settings. To solve this problem, we introduce an algorithm
recommendation method that simulates the future data stream. Given an initial
set of classes, it leverages generative models to simulate future classes from
the same visual domain. We evaluate recent algorithms on the simulated stream
and recommend the one which performs best in the user-defined incremental
setting. We illustrate the effectiveness of our method on three large datasets
using six algorithms and six incremental settings. Our method outperforms
competitive baselines, and performance is close to that of an oracle choosing
the best algorithm in each setting. This work contributes to facilitate the
practical deployment of incremental learning.
\\ ( https://arxiv.org/abs/2403.18132 ,  31318kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18133
Date: Tue, 26 Mar 2024 22:28:43 GMT   (977kb,D)

Title: AE SemRL: Learning Semantic Association Rules with Autoencoders
Authors: Erkan Karabulut, Victoria Degeler, Paul Groth
Categories: cs.LG cs.AI
\\
  Association Rule Mining (ARM) is the task of learning associations among data
features in the form of logical rules. Mining association rules from
high-dimensional numerical data, for example, time series data from a large
number of sensors in a smart environment, is a computationally intensive task.
In this study, we propose an Autoencoder-based approach to learn and extract
association rules from time series data (AE SemRL). Moreover, we argue that in
the presence of semantic information related to time series data sources,
semantics can facilitate learning generalizable and explainable association
rules. Despite enriching time series data with additional semantic features, AE
SemRL makes learning association rules from high-dimensional data feasible. Our
experiments show that semantic association rules can be extracted from a latent
representation created by an Autoencoder and this method has in the order of
hundreds of times faster execution time than state-of-the-art ARM approaches in
many scenarios. We believe that this study advances a new way of extracting
associations from representations and has the potential to inspire more
research in this field.
\\ ( https://arxiv.org/abs/2403.18133 ,  977kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18136
Date: Tue, 26 Mar 2024 22:41:41 GMT   (2457kb,D)

Title: Securing GNNs: Explanation-Based Identification of Backdoored Training
  Graphs
Authors: Jane Downer, Ren Wang, and Binghui Wang
Categories: cs.LG cs.AI
\\
  Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet
they are vulnerable to backdoor attacks that can compromise their performance
and ethical application. The detection of these attacks is crucial for
maintaining the reliability and security of GNN classification tasks, but
effective detection techniques are lacking. Following an initial investigation,
we observed that while graph-level explanations can offer limited insights,
their effectiveness in detecting backdoor triggers is inconsistent and
incomplete. To bridge this gap, we extract and transform secondary outputs of
GNN explanation mechanisms, designing seven novel metrics that more effectively
detect backdoor attacks. Additionally, we develop an adaptive attack to
rigorously evaluate our approach. We test our method on multiple benchmark
datasets and examine its efficacy against various attack models. Our results
show that our method can achieve high detection performance, marking a
significant advancement in safeguarding GNNs against backdoor attacks.
\\ ( https://arxiv.org/abs/2403.18136 ,  2457kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18142
Date: Tue, 26 Mar 2024 23:03:06 GMT   (2890kb,D)

Title: HERTA: A High-Efficiency and Rigorous Training Algorithm for Unfolded
  Graph Neural Networks
Authors: Yongyi Yang, Jiaming Yang, Wei Hu, Micha{\l} Derezi\'nski
Categories: cs.LG
\\
  As a variant of Graph Neural Networks (GNNs), Unfolded GNNs offer enhanced
interpretability and flexibility over traditional designs. Nevertheless, they
still suffer from scalability challenges when it comes to the training cost.
Although many methods have been proposed to address the scalability issues,
they mostly focus on per-iteration efficiency, without worst-case convergence
guarantees. Moreover, those methods typically add components to or modify the
original model, thus possibly breaking the interpretability of Unfolded GNNs.
In this paper, we propose HERTA: a High-Efficiency and Rigorous Training
Algorithm for Unfolded GNNs that accelerates the whole training process,
achieving a nearly-linear time worst-case training guarantee. Crucially, HERTA
converges to the optimum of the original model, thus preserving the
interpretability of Unfolded GNNs. Additionally, as a byproduct of HERTA, we
propose a new spectral sparsification method applicable to normalized and
regularized graph Laplacians that ensures tighter bounds for our algorithm than
existing spectral sparsifiers do. Experiments on real-world datasets verify the
superiority of HERTA as well as its adaptability to various loss functions and
optimizers.
\\ ( https://arxiv.org/abs/2403.18142 ,  2890kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18147
Date: Tue, 26 Mar 2024 23:14:15 GMT   (1233kb,D)

Title: Divide, Conquer, Combine Bayesian Decision Tree Sampling
Authors: Jodie A. Cochrane, Adrian Wills, Sarah J. Johnson
Categories: cs.LG
Comments: 38 pages, 5 figures
\\
  Decision trees are commonly used predictive models due to their flexibility
and interpretability. This paper is directed at quantifying the uncertainty of
decision tree predictions by employing a Bayesian inference approach. This is
challenging because these approaches need to explore both the tree structure
space and the space of decision parameters associated with each tree structure.
This has been handled by using Markov Chain Monte Carlo (MCMC) methods, where a
Markov Chain is constructed to provide samples from the desired Bayesian
estimate. Importantly, the structure and the decision parameters are tightly
coupled; small changes in the tree structure can demand vastly different
decision parameters to provide accurate predictions. A challenge for existing
MCMC approaches is proposing joint changes in both the tree structure and the
decision parameters that result in efficient sampling. This paper takes a
different approach, where each distinct tree structure is associated with a
unique set of decision parameters. The proposed approach, entitled DCC-Tree, is
inspired by the work in Zhou et al. [23] for probabilistic programs and
Cochrane et al. [4] for Hamiltonian Monte Carlo (HMC) based sampling for
decision trees. Results show that DCC-Tree performs comparably to other
HMC-based methods and better than existing Bayesian tree methods while
improving on consistency and reducing the per-proposal complexity.
\\ ( https://arxiv.org/abs/2403.18147 ,  1233kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18159
Date: Tue, 26 Mar 2024 23:51:44 GMT   (236kb,D)

Title: Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal
  Propagation Analysis for Large Language Models
Authors: Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Kyunggeun
  Lee, Jun Ma, Harris Teague
Categories: cs.LG cs.AI cs.CL
Comments: Accepted at Practical ML for Low Resource Settings Workshop at ICLR
  2024
\\
  Large generative models, such as large language models (LLMs) and diffusion
models have as revolutionized the fields of NLP and computer vision
respectively. However, their slow inference, high computation and memory
requirement makes it challenging to deploy them on edge devices. In this study,
we propose a light-weight quantization aware fine tuning technique using
knowledge distillation (KD-QAT) to improve the performance of 4-bit weight
quantized LLMs using commonly available datasets to realize a popular language
use case, on device chat applications. To improve this paradigm of finetuning,
as main contributions, we provide insights into stability of KD-QAT by
empirically studying the gradient propagation during training to better
understand the vulnerabilities of KD-QAT based approaches to low-bit
quantization errors. Based on our insights, we propose ov-freeze, a simple
technique to stabilize the KD-QAT process. Finally, we experiment with the
popular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that
ov-freeze results in near float-point precision performance, i.e., less than
0.7% loss of accuracy on Commonsense Reasoning benchmarks.
\\ ( https://arxiv.org/abs/2403.18159 ,  236kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18176
Date: Wed, 27 Mar 2024 01:05:45 GMT   (2900kb,D)

Title: Mistake, Manipulation and Margin Guarantees in Online Strategic
  Classification
Authors: Lingqing Shen, Nam Ho-Nguyen, Khanh-Hung Giang-Tran, Fatma
  K{\i}l{\i}n\c{c}-Karzan
Categories: cs.LG cs.GT math.OC
\\
  We consider an online strategic classification problem where each arriving
agent can manipulate their true feature vector to obtain a positive predicted
label, while incurring a cost that depends on the amount of manipulation. The
learner seeks to predict the agent's true label given access to only the
manipulated features. After the learner releases their prediction, the agent's
true label is revealed. Previous algorithms such as the strategic perceptron
guarantee finitely many mistakes under a margin assumption on agents' true
feature vectors. However, these are not guaranteed to encourage agents to be
truthful. Promoting truthfulness is intimately linked to obtaining adequate
margin on the predictions, thus we provide two new algorithms aimed at
recovering the maximum margin classifier in the presence of strategic agent
behavior. We prove convergence, finite mistake and finite manipulation
guarantees for a variety of agent cost structures. We also provide generalized
versions of the strategic perceptron with mistake guarantees for different
costs. Our numerical study on real and synthetic data demonstrates that the new
algorithms outperform previous ones in terms of margin, number of manipulation
and number of mistakes.
\\ ( https://arxiv.org/abs/2403.18176 ,  2900kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18181
Date: Wed, 27 Mar 2024 01:18:00 GMT   (565kb)

Title: Compression of the Koopman matrix for nonlinear physical models via
  hierarchical clustering
Authors: Tomoya Nishikata and Jun Ohkubo
Categories: cs.LG math.DS
Comments: 9 pages, 10 figures
\\
  Machine learning methods allow the prediction of nonlinear dynamical systems
from data alone. The Koopman operator is one of them, which enables us to
employ linear analysis for nonlinear dynamical systems. The linear
characteristics of the Koopman operator are hopeful to understand the nonlinear
dynamics and perform rapid predictions. The extended dynamic mode decomposition
(EDMD) is one of the methods to approximate the Koopman operator as a
finite-dimensional matrix. In this work, we propose a method to compress the
Koopman matrix using hierarchical clustering. Numerical demonstrations for the
cart-pole model and comparisons with the conventional singular value
decomposition (SVD) are shown; the results indicate that the hierarchical
clustering performs better than the naive SVD compressions.
\\ ( https://arxiv.org/abs/2403.18181 ,  565kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18192
Date: Wed, 27 Mar 2024 02:00:18 GMT   (7505kb,D)

Title: Multi-Label Adaptive Batch Selection by Highlighting Hard and Imbalanced
  Samples
Authors: Ao Zhou, Bin Liu, Jin Wang, Grigorios Tsoumakas
Categories: cs.LG
\\
  Deep neural network models have demonstrated their effectiveness in
classifying multi-label data from various domains. Typically, they employ a
training mode that combines mini-batches with optimizers, where each sample is
randomly selected with equal probability when constructing mini-batches.
However, the intrinsic class imbalance in multi-label data may bias the model
towards majority labels, since samples relevant to minority labels may be
underrepresented in each mini-batch. Meanwhile, during the training process, we
observe that instances associated with minority labels tend to induce greater
losses. Existing heuristic batch selection methods, such as priority selection
of samples with high contribution to the objective function, i.e., samples with
high loss, have been proven to accelerate convergence while reducing the loss
and test error in single-label data. However, batch selection methods have not
yet been applied and validated in multi-label data. In this study, we introduce
a simple yet effective adaptive batch selection algorithm tailored to
multi-label deep learning models. It adaptively selects each batch by
prioritizing hard samples related to minority labels. A variant of our method
also takes informative label correlations into consideration. Comprehensive
experiments combining five multi-label deep learning models on thirteen
benchmark datasets show that our method converges faster and performs better
than random batch selection.
\\ ( https://arxiv.org/abs/2403.18192 ,  7505kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18196
Date: Wed, 27 Mar 2024 02:13:20 GMT   (37kb)

Title: Looking Beyond What You See: An Empirical Analysis on Subgroup
  Intersectional Fairness for Multi-label Chest X-ray Classification Using
  Social Determinants of Racial Health Inequities
Authors: Dana Moukheiber, Saurabh Mahindre, Lama Moukheiber, Mira Moukheiber,
  Mingchen Gao
Categories: cs.LG cs.AI cs.CV cs.CY
Comments: ICCV CVAMD 2023
\\
  There has been significant progress in implementing deep learning models in
disease diagnosis using chest X- rays. Despite these advancements, inherent
biases in these models can lead to disparities in prediction accuracy across
protected groups. In this study, we propose a framework to achieve accurate
diagnostic outcomes and ensure fairness across intersectional groups in
high-dimensional chest X- ray multi-label classification. Transcending
traditional protected attributes, we consider complex interactions within
social determinants, enabling a more granular benchmark and evaluation of
fairness. We present a simple and robust method that involves retraining the
last classification layer of pre-trained models using a balanced dataset across
groups. Additionally, we account for fairness constraints and integrate
class-balanced fine-tuning for multi-label settings. The evaluation of our
method on the MIMIC-CXR dataset demonstrates that our framework achieves an
optimal tradeoff between accuracy and fairness compared to baseline methods.
\\ ( https://arxiv.org/abs/2403.18196 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18209
Date: Wed, 27 Mar 2024 02:41:52 GMT   (7711kb,D)

Title: Long and Short-Term Constraints Driven Safe Reinforcement Learning for
  Autonomous Driving
Authors: Xuemin Hu, Pan Chen, Yijun Wen, Bo Tang, Long Chen
Categories: cs.LG cs.AI cs.RO
\\
  Reinforcement learning (RL) has been widely used in decision-making tasks,
but it cannot guarantee the agent's safety in the training process due to the
requirements of interaction with the environment, which seriously limits its
industrial applications such as autonomous driving. Safe RL methods are
developed to handle this issue by constraining the expected safety violation
costs as a training objective, but they still permit unsafe state occurrence,
which is unacceptable in autonomous driving tasks. Moreover, these methods are
difficult to achieve a balance between the cost and return expectations, which
leads to learning performance degradation for the algorithms. In this paper, we
propose a novel algorithm based on the long and short-term constraints (LSTC)
for safe RL. The short-term constraint aims to guarantee the short-term state
safety that the vehicle explores, while the long-term constraint ensures the
overall safety of the vehicle throughout the decision-making process. In
addition, we develop a safe RL method with dual-constraint optimization based
on the Lagrange multiplier to optimize the training process for end-to-end
autonomous driving. Comprehensive experiments were conducted on the MetaDrive
simulator. Experimental results demonstrate that the proposed method achieves
higher safety in continuous state and action tasks, and exhibits higher
exploration performance in long-distance decision-making tasks compared with
state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.18209 ,  7711kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18219
Date: Wed, 27 Mar 2024 03:07:18 GMT   (485kb)

Title: From Two-Dimensional to Three-Dimensional Environment with Q-Learning:
  Modeling Autonomous Navigation with Reinforcement Learning and no Libraries
Authors: Ergon Cugler de Moraes Silva
Categories: cs.LG cs.AI stat.CO
\\
  Reinforcement learning (RL) algorithms have become indispensable tools in
artificial intelligence, empowering agents to acquire optimal decision-making
policies through interactions with their environment and feedback mechanisms.
This study explores the performance of RL agents in both two-dimensional (2D)
and three-dimensional (3D) environments, aiming to research the dynamics of
learning across different spatial dimensions. A key aspect of this
investigation is the absence of pre-made libraries for learning, with the
algorithm developed exclusively through computational mathematics. The
methodological framework centers on RL principles, employing a Q-learning agent
class and distinct environment classes tailored to each spatial dimension. The
research aims to address the question: How do reinforcement learning agents
adapt and perform in environments of varying spatial dimensions, particularly
in 2D and 3D settings? Through empirical analysis, the study evaluates agents'
learning trajectories and adaptation processes, revealing insights into the
efficacy of RL algorithms in navigating complex, multi-dimensional spaces.
Reflections on the findings prompt considerations for future research,
particularly in understanding the dynamics of learning in higher-dimensional
environments.
\\ ( https://arxiv.org/abs/2403.18219 ,  485kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18266
Date: Wed, 27 Mar 2024 05:38:48 GMT   (4606kb,D)

Title: Branch-Tuning: Balancing Stability and Plasticity for Continual
  Self-Supervised Learning
Authors: Wenzhuo Liu, Fei Zhu, Cheng-Lin Liu
Categories: cs.LG cs.CV
\\
  Self-supervised learning (SSL) has emerged as an effective paradigm for
deriving general representations from vast amounts of unlabeled data. However,
as real-world applications continually integrate new content, the high
computational and resource demands of SSL necessitate continual learning rather
than complete retraining. This poses a challenge in striking a balance between
stability and plasticity when adapting to new information. In this paper, we
employ Centered Kernel Alignment for quantitatively analyzing model stability
and plasticity, revealing the critical roles of batch normalization layers for
stability and convolutional layers for plasticity. Motivated by this, we
propose Branch-tuning, an efficient and straightforward method that achieves a
balance between stability and plasticity in continual SSL. Branch-tuning
consists of branch expansion and compression, and can be easily applied to
various SSL methods without the need of modifying the original methods,
retaining old data or models. We validate our method through incremental
experiments on various benchmark datasets, demonstrating its effectiveness and
practical value in real-world scenarios. We hope our work offers new insights
for future continual self-supervised learning research. The code will be made
publicly available.
\\ ( https://arxiv.org/abs/2403.18266 ,  4606kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18267
Date: Wed, 27 Mar 2024 05:41:50 GMT   (240kb,D)

Title: DSF-GAN: DownStream Feedback Generative Adversarial Network
Authors: Oriel Perets, Nadav Rappoport
Categories: cs.LG cs.AI
ACM-class: I.2
\\
  Utility and privacy are two crucial measurements of the quality of synthetic
tabular data. While significant advancements have been made in privacy
measures, generating synthetic samples with high utility remains challenging.
To enhance the utility of synthetic samples, we propose a novel architecture
called the DownStream Feedback Generative Adversarial Network (DSF-GAN). This
approach incorporates feedback from a downstream prediction model during
training to augment the generator's loss function with valuable information.
Thus, DSF-GAN utilizes a downstream prediction task to enhance the utility of
synthetic samples. To evaluate our method, we tested it using two popular
datasets. Our experiments demonstrate improved model performance when training
on synthetic samples generated by DSF-GAN, compared to those generated by the
same GAN architecture without feedback. The evaluation was conducted on the
same validation set comprising real samples. All code and datasets used in this
research will be made openly available for ease of reproduction.
\\ ( https://arxiv.org/abs/2403.18267 ,  240kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18296
Date: Wed, 27 Mar 2024 06:46:59 GMT   (680kb,D)

Title: GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic
  Communication Paradigm
Authors: Chunhang Zheng, Kechao Cai
Categories: cs.LG cs.AI eess.SP
\\
  Traditional approaches to semantic communication tasks rely on the knowledge
of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these
methods necessitate training under specific SNR conditions, entailing
considerable time and computational resources. In this paper, we propose GeNet,
a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at
combating noise, thereby facilitating Task-Oriented Communication (TOC). We
propose a novel approach where we first transform the input data image into
graph structures. Then we leverage a GNN-based encoder to extract semantic
information from the source data. This extracted semantic information is then
transmitted through the channel. At the receiver's end, a GNN-based decoder is
utilized to reconstruct the relevant semantic information from the source data
for TOC. Through experimental evaluation, we show GeNet's effectiveness in
anti-noise TOC while decoupling the SNR dependency. We further evaluate GeNet's
performance by varying the number of nodes, revealing its versatility as a new
paradigm for semantic communication. Additionally, we show GeNet's robustness
to geometric transformations by testing it with different rotation angles,
without resorting to data augmentation.
\\ ( https://arxiv.org/abs/2403.18296 ,  680kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18301
Date: Wed, 27 Mar 2024 06:55:23 GMT   (3766kb,D)

Title: Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives
Authors: Shrinivas Ramasubramanian, Harsh Rangwani, Sho Takemori, Kunal
  Samanta, Yuhei Umeda, Venkatesh Babu Radhakrishnan
Categories: cs.LG cs.AI cs.CV stat.ML
Comments: ICLR 2024 SpotLight
\\
  The rise in internet usage has led to the generation of massive amounts of
data, resulting in the adoption of various supervised and semi-supervised
machine learning algorithms, which can effectively utilize the colossal amount
of data to train models. However, before deploying these models in the real
world, these must be strictly evaluated on performance measures like worst-case
recall and satisfy constraints such as fairness. We find that current
state-of-the-art empirical techniques offer sub-optimal performance on these
practical, non-decomposable performance objectives. On the other hand, the
theoretical techniques necessitate training a new model from scratch for each
performance objective. To bridge the gap, we propose SelMix, a selective
mixup-based inexpensive fine-tuning technique for pre-trained models, to
optimize for the desired objective. The core idea of our framework is to
determine a sampling distribution to perform a mixup of features between
samples from particular classes such that it optimizes the given objective. We
comprehensively evaluate our technique against the existing empirical and
theoretically principled methods on standard benchmark datasets for imbalanced
classification. We find that proposed SelMix fine-tuning significantly improves
the performance for various practical non-decomposable objectives across
benchmarks.
\\ ( https://arxiv.org/abs/2403.18301 ,  3766kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18310
Date: Wed, 27 Mar 2024 07:22:32 GMT   (3529kb)

Title: A thermodynamically consistent physics-informed deep learning material
  model for short fiber/polymer nanocomposites
Authors: Betim Bahtiri, Behrouz Arash, Sven Scheffler, Maximilian Jux, Raimund
  Rolfes
Categories: cs.LG cs.AI cs.CE cs.NA math.NA
Comments: arXiv admin note: text overlap with arXiv:2305.08102
\\
  This work proposes a physics-informed deep learning (PIDL)-based constitutive
model for investigating the viscoelastic-viscoplastic behavior of short
fiber-reinforced nanoparticle-filled epoxies under various ambient conditions.
The deep-learning model is trained to enforce thermodynamic principles, leading
to a thermodynamically consistent constitutive model. To accomplish this, a
long short-term memory network is combined with a feed-forward neural network
to predict internal variables required for characterizing the internal
dissipation of the nanocomposite materials. In addition, another feed-forward
neural network is used to indicate the free-energy function, which enables
defining the thermodynamic state of the entire system. The PIDL model is
initially developed for the three-dimensional case by generating synthetic data
from a classical constitutive model. The model is then trained by extracting
the data directly from cyclic loading-unloading experimental tests. Numerical
examples show that the PIDL model can accurately predict the mechanical
behavior of epoxy-based nanocomposites for different volume fractions of fibers
and nanoparticles under various hygrothermal conditions.
\\ ( https://arxiv.org/abs/2403.18310 ,  3529kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18316
Date: Wed, 27 Mar 2024 07:38:36 GMT   (137kb,D)

Title: Multi-Modal Contrastive Learning for Online Clinical Time-Series
  Applications
Authors: Fabian Baldenweg, Manuel Burger, Gunnar R\"atsch, Rita Kuznetsova
Categories: cs.LG
Comments: Accepted as a Workshop Paper at TS4H@ICLR2024
\\
  Electronic Health Record (EHR) datasets from Intensive Care Units (ICU)
contain a diverse set of data modalities. While prior works have successfully
leveraged multiple modalities in supervised settings, we apply advanced
self-supervised multi-modal contrastive learning techniques to ICU data,
specifically focusing on clinical notes and time-series for clinically relevant
online prediction tasks. We introduce a loss function Multi-Modal Neighborhood
Contrastive Loss (MM-NCL), a soft neighborhood function, and showcase the
excellent linear probe and zero-shot performance of our approach.
\\ ( https://arxiv.org/abs/2403.18316 ,  137kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18321
Date: Wed, 27 Mar 2024 07:50:45 GMT   (5365kb)

Title: Implementation of the Principal Component Analysis onto High-Performance
  Computer Facilities for Hyperspectral Dimensionality Reduction: Results and
  Comparisons
Authors: E. Martel, R. Lazcano, J. Lopez, D. Madro\~nal, R. Salvador, S. Lopez,
  E. Juarez, R. Guerra, C. Sanz, R. Sarmiento
Categories: cs.LG cs.CV
Comments: 30 pages, 10 figures
DOI: 10.3390/rs10060864
\\
  Dimensionality reduction represents a critical preprocessing step in order to
increase the efficiency and the performance of many hyperspectral imaging
algorithms. However, dimensionality reduction algorithms, such as the Principal
Component Analysis (PCA), suffer from their computationally demanding nature,
becoming advisable for their implementation onto high-performance computer
architectures for applications under strict latency constraints. This work
presents the implementation of the PCA algorithm onto two different
high-performance devices, namely, an NVIDIA Graphics Processing Unit (GPU) and
a Kalray manycore, uncovering a highly valuable set of tips and tricks in order
to take full advantage of the inherent parallelism of these high-performance
computing platforms, and hence, reducing the time that is required to process a
given hyperspectral image. Moreover, the achieved results obtained with
different hyperspectral images have been compared with the ones that were
obtained with a field programmable gate array (FPGA)-based implementation of
the PCA algorithm that has been recently published, providing, for the first
time in the literature, a comprehensive analysis in order to highlight the pros
and cons of each option.
\\ ( https://arxiv.org/abs/2403.18321 ,  5365kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18322
Date: Wed, 27 Mar 2024 07:52:10 GMT   (125kb,D)

Title: Quantum Algorithms: A New Frontier in Financial Crime Prevention
Authors: Abraham Itzhak Weinberg, Alessio Faccia
Categories: cs.LG cs.ET
\\
  Financial crimes fast proliferation and sophistication require novel
approaches that provide robust and effective solutions. This paper explores the
potential of quantum algorithms in combating financial crimes. It highlights
the advantages of quantum computing by examining traditional and Machine
Learning (ML) techniques alongside quantum approaches. The study showcases
advanced methodologies such as Quantum Machine Learning (QML) and Quantum
Artificial Intelligence (QAI) as powerful solutions for detecting and
preventing financial crimes, including money laundering, financial crime
detection, cryptocurrency attacks, and market manipulation. These quantum
approaches leverage the inherent computational capabilities of quantum
computers to overcome limitations faced by classical methods. Furthermore, the
paper illustrates how quantum computing can support enhanced financial risk
management analysis. Financial institutions can improve their ability to
identify and mitigate risks, leading to more robust risk management strategies
by exploiting the quantum advantage. This research underscores the
transformative impact of quantum algorithms on financial risk management. By
embracing quantum technologies, organisations can enhance their capabilities to
combat evolving threats and ensure the integrity and stability of financial
systems.
\\ ( https://arxiv.org/abs/2403.18322 ,  125kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18337
Date: Wed, 27 Mar 2024 08:21:41 GMT   (1766kb)

Title: Macroscale fracture surface segmentation via semi-supervised learning
  considering the structural similarity
Authors: Johannes Rosenberger, Johannes Tlatlik, Sebastian M\"unstermann
Categories: cs.LG
Comments: During review title changed to: Deep learning based initial crack
  size measurements utilizing macroscale fracture surface segmentation
ACM-class: I.m
Journal-ref: Engineering Fracture Mechanics, Volume 293, 1st December 2023,
  109868
DOI: 10.1016/j.engfracmech.2023.109686
\\
  To this date the safety assessment of materials, used for example in the
nuclear power sector, commonly relies on a fracture mechanical analysis
utilizing macroscopic concepts, where a global load quantity K or J is compared
to the materials fracture toughness curve. Part of the experimental effort
involved in these concepts is dedicated to the quantitative analysis of
fracture surfaces. Within the scope of this study a methodology for the
semi-supervised training of deep learning models for fracture surface
segmentation on a macroscopic level was established. Therefore, three distinct
and unique datasets were created to analyze the influence of structural
similarity on the segmentation capability. The structural similarity differs
due to the assessed materials and specimen, as well as imaging-induced variance
due to fluctuations in image acquisition in different laboratories. The
datasets correspond to typical isolated laboratory conditions, complex
real-world circumstances, and a curated subset of the two. We implemented a
weak-to-strong consistency regularization for semi-supervised learning. On the
heterogeneous dataset we were able to train robust and well-generalizing models
that learned feature representations from images across different domains
without observing a significant drop in prediction quality. Furthermore, our
approach reduced the number of labeled images required for training by a factor
of 6. To demonstrate the success of our method and the benefit of our approach
for the fracture mechanics assessment, we utilized the models for initial crack
size measurements with the area average method. For the laboratory setting, the
deep learning assisted measurements proved to have the same quality as manual
measurements. For models trained on the heterogeneous dataset, very good
measurement accuracies with mean deviations smaller than 1 % could be
achieved...
\\ ( https://arxiv.org/abs/2403.18337 ,  1766kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18343
Date: Wed, 27 Mar 2024 08:34:39 GMT   (1515kb,D)

Title: The Artificial Neural Twin -- Process Optimization and Continual
  Learning in Distributed Process Chains
Authors: Johannes Emmert, Ronald Mendez, Houman Mirzaalian Dastjerdi,
  Christopher Syben, Andreas Maier
Categories: cs.LG
Comments: 20 pages, 11 figures
ACM-class: I.2.11; J.2; F.2.2
\\
  Industrial process optimization and control is crucial to increase economic
and ecologic efficiency. However, data sovereignty, differing goals, or the
required expert knowledge for implementation impede holistic implementation.
Further, the increasing use of data-driven AI-methods in process models and
industrial sensory often requires regular fine-tuning to accommodate
distribution drifts. We propose the Artificial Neural Twin, which combines
concepts from model predictive control, deep learning, and sensor networks to
address these issues. Our approach introduces differentiable data fusion to
estimate the state of distributed process steps and their dependence on input
data. By treating the interconnected process steps as a quasi neural-network,
we can backpropagate loss gradients for process optimization or model
fine-tuning to process parameters or AI models respectively. The concept is
demonstrated on a virtual machine park simulated in Unity, consisting of bulk
material processes in plastic recycling.
\\ ( https://arxiv.org/abs/2403.18343 ,  1515kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18375
Date: Wed, 27 Mar 2024 09:14:36 GMT   (3272kb,D)

Title: Stragglers-Aware Low-Latency Synchronous Federated Learning via
  Layer-Wise Model Updates
Authors: Natalie Lang, Alejandro Cohen, and Nir Shlezinger
Categories: cs.LG eess.SP
\\
  Synchronous federated learning (FL) is a popular paradigm for collaborative
edge learning. It typically involves a set of heterogeneous devices locally
training neural network (NN) models in parallel with periodic centralized
aggregations. As some of the devices may have limited computational resources
and varying availability, FL latency is highly sensitive to stragglers.
Conventional approaches discard incomplete intra-model updates done by
stragglers, alter the amount of local workload and architecture, or resort to
asynchronous settings; which all affect the trained model performance under
tight training latency constraints. In this work, we propose straggler-aware
layer-wise federated learning (SALF) that leverages the optimization procedure
of NNs via backpropagation to update the global model in a layer-wise fashion.
SALF allows stragglers to synchronously convey partial gradients, having each
layer of the global model be updated independently with a different
contributing set of users. We provide a theoretical analysis, establishing
convergence guarantees for the global model under mild assumptions on the
distribution of the participating devices, revealing that SALF converges at the
same asymptotic rate as FL with no timing limitations. This insight is matched
with empirical observations, demonstrating the performance gains of SALF
compared to alternative mechanisms mitigating the device heterogeneity gap in
FL.
\\ ( https://arxiv.org/abs/2403.18375 ,  3272kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18379
Date: Wed, 27 Mar 2024 09:17:50 GMT   (254kb,D)

Title: IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining
  Useful Life Prediction
Authors: Guangzai Ye, Li Feng, Jianlan Guo, Yuqiang Chen
Categories: cs.LG cs.AI
\\
  Accurately estimating the Remaining Useful Life (RUL) of lithium-ion
batteries is crucial for maintaining the safe and stable operation of
rechargeable battery management systems. However, this task is often
challenging due to the complex temporal dynamics involved. Recently,
attention-based networks, such as Transformers and Informer, have been the
popular architecture in time series forecasting. Despite their effectiveness,
these models with abundant parameters necessitate substantial training time to
unravel temporal patterns. To tackle these challenges, we propose a simple
MLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which
is an architecture based exclusively on multi-layer perceptrons (MLPs),
extracting information by mixing operations along both intra-patch and
inter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer
comprises parallel dual-head mixer layers: the intra-patch mixing MLP,
capturing local temporal patterns in the short-term period, and the inter-patch
mixing MLP, capturing global temporal patterns in the long-term period.
Notably, to address the varying importance of features in RUL prediction, we
introduce a weighted loss function in the MLP-Mixer-based architecture, marking
the first time such an approach has been employed. Our experiments demonstrate
that IIP-Mixer achieves competitive performance in battery RUL prediction,
outperforming other popular time-series frameworks
\\ ( https://arxiv.org/abs/2403.18379 ,  254kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18393
Date: Wed, 27 Mar 2024 09:30:50 GMT   (1465kb,D)

Title: Tensor-based Graph Learning with Consistency and Specificity for
  Multi-view Clustering
Authors: Long Shi and Lei Cao and Yunshan Ye and Yu Zhao and Badong Chen
Categories: cs.LG
\\
  Graph learning is widely recognized as a crucial technique in multi-view
clustering. Existing graph learning methods typically involve constructing an
adaptive neighbor graph based on probabilistic neighbors and then learning a
consensus graph to for clustering, however, they are confronted with two
limitations. Firstly, they often rely on Euclidean distance to measure
similarity when constructing the adaptive neighbor graph, which proves
inadequate in capturing the intrinsic structure among data points in many
real-world scenarios. Secondly, most of these methods focus solely on consensus
graph, ignoring view-specific graph information. In response to the
aforementioned drawbacks, we in this paper propose a novel tensor-based graph
learning framework that simultaneously considers consistency and specificity
for multi-view clustering. Specifically, we calculate the similarity distance
on the Stiefel manifold to preserve the intrinsic structure among data points.
By making an assumption that the learned neighbor graph of each view comprises
both a consistent graph and a view-specific graph, we formulate a new
tensor-based target graph learning paradigm. Owing to the benefits of tensor
singular value decomposition (t-SVD) in uncovering high-order correlations,
this model is capable of achieving a complete understanding of the target
graph. Furthermore, we develop an iterative algorithm to solve the proposed
objective optimization problem. Experiments conducted on real-world datasets
have demonstrated the superior performance of the proposed method over some
state-of-the-art multi-view clustering methods. The source code has been
released on https://github.com/lshi91/CSTGL-Code.
\\ ( https://arxiv.org/abs/2403.18393 ,  1465kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18402
Date: Wed, 27 Mar 2024 09:44:50 GMT   (1957kb,D)

Title: On Spectrogram Analysis in a Multiple Classifier Fusion Framework for
  Power Grid Classification Using Electric Network Frequency
Authors: Georgios Tzolopoulos, Christos Korgialas and Constantine Kotropoulos
Categories: cs.LG
Comments: 13th International Conference on Pattern Recognition Applications and
  Methods (ICPRAM)
DOI: 10.5220/0012418400003654
\\
  The Electric Network Frequency (ENF) serves as a unique signature inherent to
power distribution systems. Here, a novel approach for power grid
classification is developed, leveraging ENF. Spectrograms are generated from
audio and power recordings across different grids, revealing distinctive ENF
patterns that aid in grid classification through a fusion of classifiers. Four
traditional machine learning classifiers plus a Convolutional Neural Network
(CNN), optimized using Neural Architecture Search, are developed for One-vs-All
classification. This process generates numerous predictions per sample, which
are then compiled and used to train a shallow multi-label neural network
specifically designed to model the fusion process, ultimately leading to the
conclusive class prediction for each sample. Experimental findings reveal that
both validation and testing accuracy outperform those of current
state-of-the-art classifiers, underlining the effectiveness and robustness of
the proposed methodology.
\\ ( https://arxiv.org/abs/2403.18402 ,  1957kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18415
Date: Wed, 27 Mar 2024 10:06:33 GMT   (59kb)

Title: The Topos of Transformer Networks
Authors: Mattia Jacopo Villani and Peter McBurney
Categories: cs.LG math.CT
\\
  The transformer neural network has significantly out-shined all other neural
network architectures as the engine behind large language models. We provide a
theoretical analysis of the expressivity of the transformer architecture
through the lens of topos theory. From this viewpoint, we show that many common
neural network architectures, such as the convolutional, recurrent and graph
convolutional networks, can be embedded in a pretopos of piecewise-linear
functions, but that the transformer necessarily lives in its topos completion.
In particular, this suggests that the two network families instantiate
different fragments of logic: the former are first order, whereas transformers
are higher-order reasoners. Furthermore, we draw parallels with architecture
search and gradient descent, integrating our analysis in the framework of
cybernetic agents.
\\ ( https://arxiv.org/abs/2403.18415 ,  59kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18436
Date: Wed, 27 Mar 2024 10:40:27 GMT   (414kb,D)

Title: Collaborative Active Learning in Conditional Trust Environment
Authors: Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng
Categories: cs.LG
Comments: 5 pages, 9 figures, conference
\\
  In this paper, we investigate collaborative active learning, a paradigm in
which multiple collaborators explore a new domain by leveraging their combined
machine learning capabilities without disclosing their existing data and
models. Instead, the collaborators share prediction results from the new domain
and newly acquired labels. This collaboration offers several advantages: (a) it
addresses privacy and security concerns by eliminating the need for direct
model and data disclosure; (b) it enables the use of different data sources and
insights without direct data exchange; and (c) it promotes cost-effectiveness
and resource efficiency through shared labeling costs. To realize these
benefits, we introduce a collaborative active learning framework designed to
fulfill the aforementioned objectives. We validate the effectiveness of the
proposed framework through simulations. The results demonstrate that
collaboration leads to higher AUC scores compared to independent efforts,
highlighting the framework's ability to overcome the limitations of individual
models. These findings support the use of collaborative approaches in active
learning, emphasizing their potential to enhance outcomes through collective
expertise and shared resources. Our work provides a foundation for further
research on collaborative active learning and its practical applications in
various domains where data privacy, cost efficiency, and model performance are
critical considerations.
\\ ( https://arxiv.org/abs/2403.18436 ,  414kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18438
Date: Wed, 27 Mar 2024 10:45:16 GMT   (890kb,D)

Title: Global Vegetation Modeling with Pre-Trained Weather Transformers
Authors: Pascal Janetzky, Florian Gallusser, Simon Hentschel, Andreas Hotho,
  Anna Krause
Categories: cs.LG
Comments: Tackling Climate Change with Machine Learning Workshop @ ICLR 2024
\\
  Accurate vegetation models can produce further insights into the complex
interaction between vegetation activity and ecosystem processes. Previous
research has established that long-term trends and short-term variability of
temperature and precipitation affect vegetation activity. Motivated by the
recent success of Transformer-based Deep Learning models for medium-range
weather forecasting, we adapt the publicly available pre-trained FourCastNet to
model vegetation activity while accounting for the short-term dynamics of
climate variability. We investigate how the learned global representation of
the atmosphere's state can be transferred to model the normalized difference
vegetation index (NDVI). Our model globally estimates vegetation activity at a
resolution of \SI{0.25}{\degree} while relying only on meteorological data. We
demonstrate that leveraging pre-trained weather models improves the NDVI
estimates compared to learning an NDVI model from scratch. Additionally, we
compare our results to other recent data-driven NDVI modeling approaches from
machine learning and ecology literature. We further provide experimental
evidence on how much data and training time is necessary to turn FourCastNet
into an effective vegetation model. Code and models will be made available upon
publication.
\\ ( https://arxiv.org/abs/2403.18438 ,  890kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18439
Date: Wed, 27 Mar 2024 10:47:06 GMT   (5027kb,D)

Title: Generalized Policy Learning for Smart Grids: FL TRPO Approach
Authors: Yunxiang Li, Nicolas Mauricio Cuadrado, Samuel Horv\'ath, Martin
  Tak\'a\v{c}
Categories: cs.LG
Comments: ICLR 2024 Workshop: Tackling Climate Change with Machine Learning
\\
  The smart grid domain requires bolstering the capabilities of existing energy
management systems; Federated Learning (FL) aligns with this goal as it
demonstrates a remarkable ability to train models on heterogeneous datasets
while maintaining data privacy, making it suitable for smart grid applications,
which often involve disparate data distributions and interdependencies among
features that hinder the suitability of linear models. This paper introduces a
framework that combines FL with a Trust Region Policy Optimization (FL TRPO)
aiming to reduce energy-associated emissions and costs. Our approach reveals
latent interconnections and employs personalized encoding methods to capture
unique insights, understanding the relationships between features and optimal
strategies, allowing our model to generalize to previously unseen data.
Experimental results validate the robustness of our approach, affirming its
proficiency in effectively learning policy models for smart grid challenges.
\\ ( https://arxiv.org/abs/2403.18439 ,  5027kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18444
Date: Wed, 27 Mar 2024 11:00:53 GMT   (853kb,D)

Title: FRESCO: Federated Reinforcement Energy System for Cooperative
  Optimization
Authors: Nicolas Mauricio Cuadrado, Roberto Alejandro Gutierrez, Martin
  Tak\'a\v{c}
Categories: cs.LG
Comments: Tiny Paper at ICLR 2023
\\
  The rise in renewable energy is creating new dynamics in the energy grid that
promise to create a cleaner and more participative energy grid, where
technology plays a crucial part in making the required flexibility to achieve
the vision of the next-generation grid. This work presents FRESCO, a framework
that aims to ease the implementation of energy markets using a hierarchical
control architecture of reinforcement learning agents trained using federated
learning. The core concept we are proving is that having greedy agents subject
to changing conditions from a higher level agent creates a cooperative setup
that will allow for fulfilling all the individual objectives. This paper
presents a general overview of the framework, the current progress, and some
insights we obtained from the recent results.
\\ ( https://arxiv.org/abs/2403.18444 ,  853kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18451
Date: Wed, 27 Mar 2024 11:11:06 GMT   (1964kb,D)

Title: CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in
  Resource-Constrained CPS and IoT
Authors: Yi Hu, Jinhang Zuo, Alanis Zhao, Bob Iannucci, Carlee Joe-Wong
Categories: cs.LG cs.AI
Comments: accepted and to be published in 2024 IEEE International Workshop on
  Foundation Models for Cyber-Physical Systems & Internet of Things (FMSys)
\\
  Foundation models (FMs) emerge as a promising solution to harness distributed
and diverse environmental data by leveraging prior knowledge to understand the
complicated temporal and spatial correlations within heterogeneous datasets.
Unlike distributed learning frameworks such as federated learning, which often
struggle with multimodal data, FMs can transform diverse inputs into
embeddings. This process facilitates the integration of information from
various modalities and the application of prior learning to new domains.
However, deploying FMs in resource-constrained edge systems poses significant
challenges. To this end, we introduce CoRAST, a novel learning framework that
utilizes FMs for enhanced analysis of distributed, correlated heterogeneous
data. Utilizing a server-based FM, CoRAST can exploit existing environment
information to extract temporal, spatial, and cross-modal correlations among
sensor data. This enables CoRAST to offer context-aware insights for localized
client tasks through FM-powered global representation learning. Our evaluation
on real-world weather dataset demonstrates CoRAST's ability to exploit
correlated heterogeneous data through environmental representation learning to
reduce the forecast errors by up to 50.3% compared to the baselines.
\\ ( https://arxiv.org/abs/2403.18451 ,  1964kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18486
Date: Wed, 27 Mar 2024 11:58:45 GMT   (470kb,D)

Title: Synthesizing EEG Signals from Event-Related Potential Paradigms with
  Conditional Diffusion Models
Authors: Guido Klein, Pierre Guetschel, Gianluigi Silvestri, Michael Tangermann
Categories: cs.LG cs.AI eess.SP
Comments: submitted to 9th Graz BCI conference, 6 pages, 3 figures, first
  figure is split into two subfigures, 1 table
ACM-class: I.2.6; G.3; I.5.4; J.3
\\
  Data scarcity in the brain-computer interface field can be alleviated through
the use of generative models, specifically diffusion models. While diffusion
models have previously been successfully applied to electroencephalogram (EEG)
data, existing models lack flexibility w.r.t.~sampling or require alternative
representations of the EEG data. To overcome these limitations, we introduce a
novel approach to conditional diffusion models that utilizes classifier-free
guidance to directly generate subject-, session-, and class-specific EEG data.
In addition to commonly used metrics, domain-specific metrics are employed to
evaluate the specificity of the generated samples. The results indicate that
the proposed model can generate EEG data that resembles real data for each
subject, session, and class.
\\ ( https://arxiv.org/abs/2403.18486 ,  470kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18494
Date: Wed, 27 Mar 2024 12:10:30 GMT   (21378kb,D)

Title: Learning in PINNs: Phase transition, total diffusion, and generalization
Authors: Sokratis J. Anagnostopoulos, Juan Diego Toscano, Nikolaos
  Stergiopulos, George Em Karniadakis
Categories: cs.LG
\\
  We investigate the learning dynamics of fully-connected neural networks
through the lens of gradient signal-to-noise ratio (SNR), examining the
behavior of first-order optimizers like Adam in non-convex objectives. By
interpreting the drift/diffusion phases in the information bottleneck theory,
focusing on gradient homogeneity, we identify a third phase termed ``total
diffusion", characterized by equilibrium in the learning rates and homogeneous
gradients. This phase is marked by an abrupt SNR increase, uniform residuals
across the sample space and the most rapid training convergence. We propose a
residual-based re-weighting scheme to accelerate this diffusion in quadratic
loss functions, enhancing generalization. We also explore the information
compression phenomenon, pinpointing a significant saturation-induced
compression of activations at the total diffusion phase, with deeper layers
experiencing negligible information loss. Supported by experimental data on
physics-informed neural networks (PINNs), which underscore the importance of
gradient homogeneity due to their PDE-based sample inter-dependence, our
findings suggest that recognizing phase transitions could refine ML
optimization strategies for improved generalization.
\\ ( https://arxiv.org/abs/2403.18494 ,  21378kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18506
Date: Wed, 27 Mar 2024 12:35:23 GMT   (8404kb,D)

Title: Faster Convergence for Transformer Fine-tuning with Line Search Methods
Authors: Philip Kenneweg, Leonardo Galli, Tristan Kenneweg, Barbara Hammer
Categories: cs.LG cs.AI
DOI: 10.1109/IJCNN54540.2023.10192001
\\
  Recent works have shown that line search methods greatly increase performance
of traditional stochastic gradient descent methods on a variety of datasets and
architectures [1], [2]. In this work we succeed in extending line search
methods to the novel and highly popular Transformer architecture and dataset
domains in natural language processing. More specifically, we combine the
Armijo line search with the Adam optimizer and extend it by subdividing the
networks architecture into sensible units and perform the line search
separately on these local units. Our optimization method outperforms the
traditional Adam optimizer and achieves significant performance improvements
for small data sets or small training budgets, while performing equal or better
for other tested cases. Our work is publicly available as a python package,
which provides a hyperparameter-free pytorch optimizer that is compatible with
arbitrary network architectures.
\\ ( https://arxiv.org/abs/2403.18506 ,  8404kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18517
Date: Wed, 27 Mar 2024 12:49:14 GMT   (2012kb,D)

Title: Efficient Algorithms for Regularized Nonnegative Scale-invariant
  Low-rank Approximation Models
Authors: Jeremy E. Cohen and Valentin Leplat
Categories: cs.LG cs.NA math.NA math.OC
\\
  Regularized nonnegative low-rank approximations such as sparse Nonnegative
Matrix Factorization or sparse Nonnegative Tucker Decomposition are an
important branch of dimensionality reduction models with enhanced
interpretability. However, from a practical perspective, the choice of
regularizers and regularization coefficients, as well as the design of
efficient algorithms, is challenging because of the multifactor nature of these
models and the lack of theory to back these choices. This paper aims at
improving upon these issues. By studying a more general model called the
Homogeneous Regularized Scale-Invariant, we prove that the scale-invariance
inherent to low-rank approximation models causes an implicit regularization
with both unexpected beneficial and detrimental effects. This observation
allows to better understand the effect of regularization functions in low-rank
approximation models, to guide the choice of the regularization
hyperparameters, and to design balancing strategies to enhance the convergence
speed of dedicated optimization algorithms. Some of these results were already
known but restricted to specific instances of regularized low-rank
approximations. We also derive a generic Majorization Minimization algorithm
that handles many regularized nonnegative low-rank approximations, with
convergence guarantees. We showcase our contributions on sparse Nonnegative
Matrix Factorization, ridge-regularized Canonical Polyadic decomposition and
sparse Nonnegative Tucker Decomposition.
\\ ( https://arxiv.org/abs/2403.18517 ,  2012kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18519
Date: Wed, 27 Mar 2024 12:50:27 GMT   (17916kb,D)

Title: Improving Line Search Methods for Large Scale Neural Network Training
Authors: Philip Kenneweg, Tristan Kenneweg, Barbara Hammer
Categories: cs.LG cs.AI
DOI: 10.1109/ACDSA59508.2024.10467724
\\
  In recent studies, line search methods have shown significant improvements in
the performance of traditional stochastic gradient descent techniques,
eliminating the need for a specific learning rate schedule. In this paper, we
identify existing issues in state-of-the-art line search methods, propose
enhancements, and rigorously evaluate their effectiveness. We test these
methods on larger datasets and more complex data domains than before.
Specifically, we improve the Armijo line search by integrating the momentum
term from ADAM in its search direction, enabling efficient large-scale
training, a task that was previously prone to failure using Armijo line search
methods. Our optimization approach outperforms both the previous Armijo
implementation and tuned learning rate schedules for Adam. Our evaluation
focuses on Transformers and CNNs in the domains of NLP and image data. Our work
is publicly available as a Python package, which provides a hyperparameter free
Pytorch optimizer.
\\ ( https://arxiv.org/abs/2403.18519 ,  17916kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18539
Date: Wed, 27 Mar 2024 13:14:29 GMT   (656kb,D)

Title: Safe and Robust Reinforcement-Learning: Principles and Practice
Authors: Taku Yamagata, Raul Santos-Rodriguez
Categories: cs.LG cs.SY eess.SY
\\
  Reinforcement Learning (RL) has shown remarkable success in solving
relatively complex tasks, yet the deployment of RL systems in real-world
scenarios poses significant challenges related to safety and robustness. This
paper aims to identify and further understand those challenges thorough the
exploration of the main dimensions of the safe and robust RL landscape,
encompassing algorithmic, ethical, and practical considerations. We conduct a
comprehensive review of methodologies and open problems that summarizes the
efforts in recent years to address the inherent risks associated with RL
applications.
  After discussing and proposing definitions for both safe and robust RL, the
paper categorizes existing research works into different algorithmic approaches
that enhance the safety and robustness of RL agents. We examine techniques such
as uncertainty estimation, optimisation methodologies, exploration-exploitation
trade-offs, and adversarial training. Environmental factors, including
sim-to-real transfer and domain adaptation, are also scrutinized to understand
how RL systems can adapt to diverse and dynamic surroundings. Moreover, human
involvement is an integral ingredient of the analysis, acknowledging the broad
set of roles that humans can take in this context.
  Importantly, to aid practitioners in navigating the complexities of safe and
robust RL implementation, this paper introduces a practical checklist derived
from the synthesized literature. The checklist encompasses critical aspects of
algorithm design, training environment considerations, and ethical guidelines.
It will serve as a resource for developers and policymakers alike to ensure the
responsible deployment of RL systems in many application domains.
\\ ( https://arxiv.org/abs/2403.18539 ,  656kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18569
Date: Wed, 27 Mar 2024 13:50:13 GMT   (2600kb,D)

Title: PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop
  Prediction
Authors: Yuxiang Zhao, Zhuomin Chai, Xun Jiang, Yibo Lin, Runsheng Wang, Ru
  Huang
Categories: cs.LG cs.AI
\\
  IR drop on the power delivery network (PDN) is closely related to PDN's
configuration and cell current consumption. As the integrated circuit (IC)
design is growing larger, dynamic IR drop simulation becomes computationally
unaffordable and machine learning based IR drop prediction has been explored as
a promising solution. Although CNN-based methods have been adapted to IR drop
prediction task in several works, the shortcomings of overlooking PDN
configuration is non-negligible. In this paper, we consider not only how to
properly represent cell-PDN relation, but also how to model IR drop following
its physical nature in the feature aggregation procedure. Thus, we propose a
novel graph structure, PDNGraph, to unify the representations of the PDN
structure and the fine-grained cell-PDN relation. We further propose a
dual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN
branches to favorably capture the above features during the learning process.
Several key designs are presented to make the dynamic IR drop prediction highly
effective and interpretable. We are the first work to apply graph structure to
deep-learning based dynamic IR drop prediction method. Experiments show that
PDNNet outperforms the state-of-the-art CNN-based methods by up to 39.3%
reduction in prediction error and achieves 545x speedup compared to the
commercial tool, which demonstrates the superiority of our method.
\\ ( https://arxiv.org/abs/2403.18569 ,  2600kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18570
Date: Wed, 27 Mar 2024 13:51:26 GMT   (818kb,D)

Title: Physics-Informed Graph Neural Networks for Water Distribution Systems
Authors: Inaam Ashraf, Janine Strotherm, Luca Hermes, Barbara Hammer
Categories: cs.LG cs.AI
Comments: Extended version of the paper with the same title published at
  Proceedings of the AAAI Conference on Artificial Intelligence 2024
\\
  Water distribution systems (WDS) are an integral part of critical
infrastructure which is pivotal to urban development. As 70% of the world's
population will likely live in urban environments in 2050, efficient simulation
and planning tools for WDS play a crucial role in reaching UN's sustainable
developmental goal (SDG) 6 - "Clean water and sanitation for all". In this
realm, we propose a novel and efficient machine learning emulator, more
precisely, a physics-informed deep learning (DL) model, for hydraulic state
estimation in WDS. Using a recursive approach, our model only needs a few graph
convolutional neural network (GCN) layers and employs an innovative algorithm
based on message passing. Unlike conventional machine learning tasks, the model
uses hydraulic principles to infer two additional hydraulic state features in
the process of reconstructing the available ground truth feature in an
unsupervised manner. To the best of our knowledge, this is the first DL
approach to emulate the popular hydraulic simulator EPANET, utilizing no
additional information. Like most DL models and unlike the hydraulic simulator,
our model demonstrates vastly faster emulation times that do not increase
drastically with the size of the WDS. Moreover, we achieve high accuracy on the
ground truth and very similar results compared to the hydraulic simulator as
demonstrated through experiments on five real-world WDS datasets.
\\ ( https://arxiv.org/abs/2403.18570 ,  818kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18579
Date: Wed, 27 Mar 2024 13:59:09 GMT   (289kb,D)

Title: On Optimizing Hyperparameters for Quantum Neural Networks
Authors: Sabrina Herbst, Vincenzo De Maio and Ivona Brandic
Categories: cs.LG cs.ET
\\
  The increasing capabilities of Machine Learning (ML) models go hand in hand
with an immense amount of data and computational power required for training.
Therefore, training is usually outsourced into HPC facilities, where we have
started to experience limits in scaling conventional HPC hardware, as theorized
by Moore's law. Despite heavy parallelization and optimization efforts, current
state-of-the-art ML models require weeks for training, which is associated with
an enormous $CO_2$ footprint. Quantum Computing, and specifically Quantum
Machine Learning (QML), can offer significant theoretical speed-ups and
enhanced expressive power. However, training QML models requires tuning various
hyperparameters, which is a nontrivial task and suboptimal choices can highly
affect the trainability and performance of the models. In this study, we
identify the most impactful hyperparameters and collect data about the
performance of QML models. We compare different configurations and provide
researchers with performance data and concrete suggestions for hyperparameter
selection.
\\ ( https://arxiv.org/abs/2403.18579 ,  289kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18613
Date: Wed, 27 Mar 2024 14:28:44 GMT   (352kb)

Title: Scalable Lipschitz Estimation for CNNs
Authors: Yusuf Sulehman, Tingting Mu
Categories: cs.LG
\\
  Estimating the Lipschitz constant of deep neural networks is of growing
interest as it is useful for informing on generalisability and adversarial
robustness. Convolutional neural networks (CNNs) in particular, underpin much
of the recent success in computer vision related applications. However,
although existing methods for estimating the Lipschitz constant can be tight,
they have limited scalability when applied to CNNs. To tackle this, we propose
a novel method to accelerate Lipschitz constant estimation for CNNs. The core
idea is to divide a large convolutional block via a joint layer and width-wise
partition, into a collection of smaller blocks. We prove an upper-bound on the
Lipschitz constant of the larger block in terms of the Lipschitz constants of
the smaller blocks. Through varying the partition factor, the resulting method
can be adjusted to prioritise either accuracy or scalability and permits
parallelisation. We demonstrate an enhanced scalability and comparable accuracy
to existing baselines through a range of experiments.
\\ ( https://arxiv.org/abs/2403.18613 ,  352kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18631
Date: Wed, 27 Mar 2024 14:38:02 GMT   (1424kb,D)

Title: First Experiences with the Identification of People at Risk for Diabetes
  in Argentina using Machine Learning Techniques
Authors: Enzo Rucci and Gonzalo Tittarelli and Franco Ronchetti and Jorge F.
  Elgart and Laura Lanzarini and Juan Jos\'e Gagliardino
Categories: cs.LG
Comments: Accepted for publication in Computer Science - CACIC 2023
\\
  Detecting Type 2 Diabetes (T2D) and Prediabetes (PD) is a real challenge for
medicine due to the absence of pathogenic symptoms and the lack of known
associated risk factors. Even though some proposals for machine learning models
enable the identification of people at risk, the nature of the condition makes
it so that a model suitable for one population may not necessarily be suitable
for another. In this article, the development and assessment of predictive
models to identify people at risk for T2D and PD specifically in Argentina are
discussed. First, the database was thoroughly preprocessed and three specific
datasets were generated considering a compromise between the number of records
and the amount of available variables. After applying 5 different
classification models, the results obtained show that a very good performance
was observed for two datasets with some of these models. In particular, RF, DT,
and ANN demonstrated great classification power, with good values for the
metrics under consideration. Given the lack of this type of tool in Argentina,
this work represents the first step towards the development of more
sophisticated models.
\\ ( https://arxiv.org/abs/2403.18631 ,  1424kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18635
Date: Wed, 27 Mar 2024 14:40:25 GMT   (467kb,D)

Title: Fusion approaches for emotion recognition from speech using acoustic and
  text-based features
Authors: Leonardo Pepino and Pablo Riera and Luciana Ferrer and Agustin Gravano
Categories: cs.LG cs.SD eess.AS
Comments: 5 pages. Accepted in ICASSP 2020
DOI: 10.1109/ICASSP40776.2020.9054709
\\
  In this paper, we study different approaches for classifying emotions from
speech using acoustic and text-based features. We propose to obtain
contextualized word embeddings with BERT to represent the information contained
in speech transcriptions and show that this results in better performance than
using Glove embeddings. We also propose and compare different strategies to
combine the audio and text modalities, evaluating them on IEMOCAP and
MSP-PODCAST datasets. We find that fusing acoustic and text-based systems is
beneficial on both datasets, though only subtle differences are observed across
the evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect
that the criteria used to define the cross-validation folds have on results. In
particular, the standard way of creating folds for this dataset results in a
highly optimistic estimation of performance for the text-based system,
suggesting that some previous works may overestimate the advantage of
incorporating transcriptions.
\\ ( https://arxiv.org/abs/2403.18635 ,  467kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18668
Date: Wed, 27 Mar 2024 15:11:07 GMT   (1922kb)

Title: Aiming for Relevance
Authors: Bar Eini Porat, Danny Eytan, Uri Shalit
Categories: cs.LG cs.AI cs.HC stat.ML
Comments: 10 pages, 9 figures, AMIA Informatics 2024
\\
  Vital signs are crucial in intensive care units (ICUs). They are used to
track the patient's state and to identify clinically significant changes.
Predicting vital sign trajectories is valuable for early detection of adverse
events. However, conventional machine learning metrics like RMSE often fail to
capture the true clinical relevance of such predictions. We introduce novel
vital sign prediction performance metrics that align with clinical contexts,
focusing on deviations from clinical norms, overall trends, and trend
deviations. These metrics are derived from empirical utility curves obtained in
a previous study through interviews with ICU clinicians. We validate the
metrics' usefulness using simulated and real clinical datasets (MIMIC and
eICU). Furthermore, we employ these metrics as loss functions for neural
networks, resulting in models that excel in predicting clinically significant
events. This research paves the way for clinically relevant machine learning
model evaluation and optimization, promising to improve ICU patient care. 10
pages, 9 figures.
\\ ( https://arxiv.org/abs/2403.18668 ,  1922kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18681
Date: Wed, 27 Mar 2024 15:24:54 GMT   (5039kb,D)

Title: TransFusion: Contrastive Learning with Transformers
Authors: Huanran Li, Daniel Pimentel-Alarc\'on
Categories: cs.LG cs.AI
Comments: 17 pages, 4 figures,
\\
  This paper proposes a novel framework, TransFusion, designed to make the
process of contrastive learning more analytical and explainable. TransFusion
consists of attention blocks whose softmax being replaced by ReLU, and its
final block's weighted-sum operation is truncated to leave the adjacency matrix
as the output. The model is trained by minimizing the Jensen-Shannon Divergence
between its output and the target affinity matrix, which indicates whether each
pair of samples belongs to the same or different classes. The main contribution
of TransFusion lies in defining a theoretical limit for answering two
fundamental questions in the field: the maximum level of data augmentation and
the minimum batch size required for effective contrastive learning.
Furthermore, experimental results indicate that TransFusion successfully
extracts features that isolate clusters from complex real-world data, leading
to improved classification accuracy in downstream tasks.
\\ ( https://arxiv.org/abs/2403.18681 ,  5039kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18687
Date: Wed, 27 Mar 2024 15:34:27 GMT   (132kb)

Title: InceptionTime vs. Wavelet -- A comparison for time series classification
Authors: Daniel Klenkert, Daniel Schaeffer, Julian Stauch
Categories: cs.LG
Comments: 4 pages, 1 figure
ACM-class: I.5.4; J.2
\\
  Neural networks were used to classify infrasound data. Two different
approaches were compared. One based on the direct classification of time series
data, using a custom implementation of the InceptionTime network. For the other
approach, we generated 2D images of the wavelet transformation of the signals,
which were subsequently classified using a ResNet implementation. Choosing
appropriate hyperparameter settings, both achieve a classification accuracy of
above 90 %, with the direct approach reaching 95.2 %.
\\ ( https://arxiv.org/abs/2403.18687 ,  132kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18699
Date: Wed, 27 Mar 2024 15:48:16 GMT   (2092kb,D)

Title: Contrastive Learning with Orthonormal Anchors (CLOA)
Authors: Huanran Li, Daniel Pimentel-Alarc\'on
Categories: cs.LG cs.AI
Comments: 11 pages, 4 figures
\\
  This study focuses on addressing the instability issues prevalent in
contrastive learning, specifically examining the InfoNCE loss function and its
derivatives. We reveal a critical observation that these loss functions exhibit
a restrictive behavior, leading to a convergence phenomenon where embeddings
tend to merge into a singular point. This "over-fusion" effect detrimentally
affects classification accuracy in subsequent supervised-learning tasks.
Through theoretical analysis, we demonstrate that embeddings, when equalized or
confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In
response to this challenge, our research introduces an innovative strategy that
leverages the same or fewer labeled data than typically used in the fine-tuning
phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to
disentangle embedding clusters, significantly enhancing the distinctiveness of
each embedding while simultaneously ensuring their aggregation into dense,
well-defined clusters. Our method demonstrates remarkable improvements with
just a fraction of the conventional label requirements, as evidenced by our
results on CIFAR10 and CIFAR100 datasets.
\\ ( https://arxiv.org/abs/2403.18699 ,  2092kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18705
Date: Wed, 27 Mar 2024 15:54:55 GMT   (4303kb,D)

Title: Conditional Wasserstein Distances with Applications in Bayesian OT Flow
  Matching
Authors: Jannis Chemseddine, Paul Hagemann, Christian Wald, Gabriele Steidl
Categories: cs.LG math.OC
Comments: This paper supersedes arXiv:2310.13433
\\
  In inverse problems, many conditional generative models approximate the
posterior measure by minimizing a distance between the joint measure and its
learned approximation. While this approach also controls the distance between
the posterior measures in the case of the Kullback--Leibler divergence, this is
in general not hold true for the Wasserstein distance. In this paper, we
introduce a conditional Wasserstein distance via a set of restricted couplings
that equals the expected Wasserstein distance of the posteriors. Interestingly,
the dual formulation of the conditional Wasserstein-1 flow resembles losses in
the conditional Wasserstein GAN literature in a quite natural way. We derive
theoretical properties of the conditional Wasserstein distance, characterize
the corresponding geodesics and velocity fields as well as the flow ODEs.
Subsequently, we propose to approximate the velocity fields by relaxing the
conditional Wasserstein distance. Based on this, we propose an extension of OT
Flow Matching for solving Bayesian inverse problems and demonstrate its
numerical advantages on an inverse problem and class-conditional image
generation.
\\ ( https://arxiv.org/abs/2403.18705 ,  4303kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18710
Date: Wed, 27 Mar 2024 15:57:42 GMT   (484kb,D)

Title: Deep Learning for Traffic Flow Prediction using Cellular Automata-based
  Model and CNN-LSTM architecture
Authors: Zhaohui Yang, Kshitij Jerath
Categories: cs.LG
\\
  Recent works have attempted to use deep learning to predict future states of
traffic flow, but have met with mixed results. These approaches face two key
challenges. First, training deep learning neural networks requires large
amounts of training data which are not yet easily available for traffic flow
systems. Second, even when data is available, the neural networks require
access to historical data that covers most possible traffic flow dynamics to
successfully predict future traffic states. Specifically, these deep learning
approaches do not fully leverage domain-knowledge about traffic flow dynamics,
despite a significant existing knowledge-base. In this work, we propose to
solve both issues using a Convolutional Neural Network (CNNs) with Long Short
Term Memory (LSTM) deep learning architecture to successfully predict traffic
flow, while leveraging a cellular automata-based statistical mechanics model of
traffic flow to generate training and test data. Another major contribution of
this paper is the insight that training data for a large traffic system can
actually be sampled from the simulations of a much smaller traffic system. This
is achieved through observing that the normalized energy distribution of the
statistical mechanics model is scale invariant, which significantly eases the
burden of data generation for large scale traffic systems. The resulting
simulations indicate good agreement between the predicted and the true traffic
flow dynamics.
\\ ( https://arxiv.org/abs/2403.18710 ,  484kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18717
Date: Wed, 27 Mar 2024 16:06:37 GMT   (808kb,D)

Title: Semi-Supervised Learning for Deep Causal Generative Models
Authors: Yasin Ibrahim, Hermione Warr, Konstantinos Kamnitsas
Categories: cs.LG cs.AI cs.CV stat.ML
\\
  Developing models that can answer questions of the form "How would $x$ change
if $y$ had been $z$?" is fundamental for advancing medical image analysis.
Training causal generative models that address such counterfactual questions,
though, currently requires that all relevant variables have been observed and
that corresponding labels are available in training data. However, clinical
data may not have complete records for all patients and state of the art causal
generative models are unable to take full advantage of this. We thus develop,
for the first time, a semi-supervised deep causal generative model that
exploits the causal relationships between variables to maximise the use of all
available data. We explore this in the setting where each sample is either
fully labelled or fully unlabelled, as well as the more clinically realistic
case of having different labels missing for each sample. We leverage techniques
from causal inference to infer missing values and subsequently generate
realistic counterfactuals, even for samples with incomplete labels.
\\ ( https://arxiv.org/abs/2403.18717 ,  808kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18735
Date: Wed, 27 Mar 2024 16:24:26 GMT   (710kb,D)

Title: Nonlinear model reduction for operator learning
Authors: Hamidreza Eivazi, Stefan Wittek, Andreas Rausch
Categories: cs.LG cs.NA math.NA
Comments: Published as a Tiny Paper at ICLR 2024 (Notable)
\\
  Operator learning provides methods to approximate mappings between
infinite-dimensional function spaces. Deep operator networks (DeepONets) are a
notable architecture in this field. Recently, an extension of DeepONet based on
model reduction and neural networks, proper orthogonal decomposition
(POD)-DeepONet, has been able to outperform other architectures in terms of
accuracy for several benchmark tests. We extend this idea towards nonlinear
model order reduction by proposing an efficient framework that combines neural
networks with kernel principal component analysis (KPCA) for operator learning.
Our results demonstrate the superior performance of KPCA-DeepONet over
POD-DeepONet.
\\ ( https://arxiv.org/abs/2403.18735 ,  710kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18739
Date: Wed, 27 Mar 2024 16:32:32 GMT   (207kb,D)

Title: Usage-Specific Survival Modeling Based on Operational Data and Neural
  Networks
Authors: Olov Holmer, Mattias Krysander, and Erik Frisk
Categories: cs.LG cs.SY eess.SY stat.ML
Comments: 7 pages
\\
  Accurate predictions of when a component will fail are crucial when planning
maintenance, and by modeling the distribution of these failure times, survival
models have shown to be particularly useful in this context. The presented
methodology is based on conventional neural network-based survival models that
are trained using data that is continuously gathered and stored at specific
times, called snapshots. An important property of this type of training data is
that it can contain more than one snapshot from a specific individual which
results in that standard maximum likelihood training can not be directly
applied since the data is not independent. However, the papers show that if the
data is in a specific format where all snapshot times are the same for all
individuals, called homogeneously sampled, maximum likelihood training can be
applied and produce desirable results. In many cases, the data is not
homogeneously sampled and in this case, it is proposed to resample the data to
make it homogeneously sampled. How densely the dataset is sampled turns out to
be an important parameter; it should be chosen large enough to produce good
results, but this also increases the size of the dataset which makes training
slow. To reduce the number of samples needed during training, the paper also
proposes a technique to, instead of resampling the dataset once before the
training starts, randomly resample the dataset at the start of each epoch
during the training. The proposed methodology is evaluated on both a simulated
dataset and an experimental dataset of starter battery failures. The results
show that if the data is homogeneously sampled the methodology works as
intended and produces accurate survival models. The results also show that
randomly resampling the dataset on each epoch is an effective way to reduce the
size of the training data.
\\ ( https://arxiv.org/abs/2403.18739 ,  207kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18742
Date: Wed, 27 Mar 2024 16:39:28 GMT   (2295kb,D)

Title: Understanding the Learning Dynamics of Alignment with Human Feedback
Authors: Shawn Im, Yixuan Li
Categories: cs.LG cs.AI
\\
  Aligning large language models (LLMs) with human intentions has become a
critical task for safely deploying models in real-world systems. While existing
alignment approaches have seen empirical success, theoretically understanding
how these methods affect model behavior remains an open question. Our work
provides an initial attempt to theoretically analyze the learning dynamics of
human preference alignment. We formally show how the distribution of preference
datasets influences the rate of model updates and provide rigorous guarantees
on the training accuracy. Our theory also reveals an intricate phenomenon where
the optimization is prone to prioritizing certain behaviors with higher
preference distinguishability. We empirically validate our findings on
contemporary LLMs and alignment tasks, reinforcing our theoretical insights and
shedding light on considerations for future alignment approaches. Disclaimer:
This paper contains potentially offensive text; reader discretion is advised.
\\ ( https://arxiv.org/abs/2403.18742 ,  2295kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18766
Date: Wed, 27 Mar 2024 17:05:03 GMT   (57kb)

Title: Superior Parallel Big Data Clustering through Competitive Stochastic
  Sample Size Optimization in Big-means
Authors: Rustam Mussabayev, Ravil Mussabayev
Categories: cs.LG cs.AI cs.DC cs.IR
\\
  This paper introduces a novel K-means clustering algorithm, an advancement on
the conventional Big-means methodology. The proposed method efficiently
integrates parallel processing, stochastic sampling, and competitive
optimization to create a scalable variant designed for big data applications.
It addresses scalability and computation time challenges typically faced with
traditional techniques. The algorithm adjusts sample sizes dynamically for each
worker during execution, optimizing performance. Data from these sample sizes
are continually analyzed, facilitating the identification of the most efficient
configuration. By incorporating a competitive element among workers using
different sample sizes, efficiency within the Big-means algorithm is further
stimulated. In essence, the algorithm balances computational time and
clustering quality by employing a stochastic, competitive sampling strategy in
a parallel computing setting.
\\ ( https://arxiv.org/abs/2403.18766 ,  57kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2403.17608 (*cross-listing*)
Date: Tue, 26 Mar 2024 11:39:00 GMT   (28601kb,D)

Title: Fake or JPEG? Revealing Common Biases in Generated Image Detection
  Datasets
Authors: Patrick Grommelt, Louis Weiss, Franz-Josef Pfreundt, Janis Keuper
Categories: cs.CV cs.AI cs.LG
\\
  The widespread adoption of generative image models has highlighted the urgent
need to detect artificial content, which is a crucial step in combating
widespread manipulation and misinformation. Consequently, numerous detectors
and associated datasets have emerged. However, many of these datasets
inadvertently introduce undesirable biases, thereby impacting the effectiveness
and evaluation of detectors. In this paper, we emphasize that many datasets for
AI-generated image detection contain biases related to JPEG compression and
image size. Using the GenImage dataset, we demonstrate that detectors indeed
learn from these undesired factors. Furthermore, we show that removing the
named biases substantially increases robustness to JPEG compression and
significantly alters the cross-generator performance of evaluated detectors.
Specifically, it leads to more than 11 percentage points increase in
cross-generator performance for ResNet50 and Swin-T detectors on the GenImage
dataset, achieving state-of-the-art results.
  We provide the dataset and source codes of this paper on the anonymous
website: https://www.unbiased-genimage.org
\\ ( https://arxiv.org/abs/2403.17608 ,  28601kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17942 (*cross-listing*)
Date: Fri, 2 Feb 2024 06:17:49 GMT   (959kb,D)

Title: A Note On Lookahead In Real Life And Computing
Authors: Burle Sharma, Rakesh Mohanty, Sucheta Panda
Categories: cs.DS cs.AI cs.LG
\\
  Past, Present and Future are considered to be three temporal and logical
concepts which are well defined by human beings for their existence and growth.
We, as human beings, have the privilege of using our intelligence to mentally
execute an activity before physical occurrence of the same in the real world.
Knowledge of the past, aplomb of present and visualisation for the future
correspond to three concepts such as look-back, look-at and look-ahead
respectively in real life as well as in diversified domains of computing.
Look-Ahead(LA) deals with the future prediction of information and processing
of input to produce the output in advance. In this article, our main objective
is to learn, understand and explore the concept of LA and design novel models
as solution for real world problems. We present three well known algorithmic
frameworks used in practice based on availability of input information such as
offline, online and semi-online. We introduce interesting real life
applications and well known computing problems where LA plays a significant
role for making a process, system or algorithm efficient. We define new types
of LA and propose a taxonomy for LA based on literature review for designing
novel LA models in future. Using the concept of LA, We identify and present
many interesting and non-trivial research challenges as future potential
research directions. Intuitively, we observe that LA can be used as a powerful
tool and framework for future researchers in design of efficient computational
models and algorithms for solving non-trivial and challenging optimization
problems.
\\ ( https://arxiv.org/abs/2403.17942 ,  959kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17978 (*cross-listing*)
Date: Sat, 23 Mar 2024 15:49:13 GMT   (5944kb,D)

Title: Holographic Global Convolutional Networks for Long-Range Prediction
  Tasks in Malware Detection
Authors: Mohammad Mahmudul Alam, Edward Raff, Stella Biderman, Tim Oates, James
  Holt
Categories: cs.CR cs.AI cs.LG stat.ML
Comments: To appear in Proceedings of the 27th International Conference on
  Artificial Intelligence and Statistics (AISTATS) 2024, Valencia, Spain
\\
  Malware detection is an interesting and valuable domain to work in because it
has significant real-world impact and unique machine-learning challenges. We
investigate existing long-range techniques and benchmarks and find that they're
not very suitable in this problem area. In this paper, we introduce Holographic
Global Convolutional Networks (HGConv) that utilize the properties of
Holographic Reduced Representations (HRR) to encode and decode features from
sequence elements. Unlike other global convolutional methods, our method does
not require any intricate kernel computation or crafted kernel design. HGConv
kernels are defined as simple parameters learned through backpropagation. The
proposed method has achieved new SOTA results on Microsoft Malware
Classification Challenge, Drebin, and EMBER malware benchmarks. With log-linear
complexity in sequence length, the empirical results demonstrate substantially
faster run-time by HGConv compared to other methods achieving far more
efficient scaling even with sequence length $\geq 100,000$.
\\ ( https://arxiv.org/abs/2403.17978 ,  5944kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17992 (*cross-listing*)
Date: Tue, 26 Mar 2024 12:20:10 GMT   (4516kb,D)

Title: Interpretable cancer cell detection with phonon microscopy using
  multi-task conditional neural networks for inter-batch calibration
Authors: Yijie Zheng, Rafael Fuentes-Dominguez, Matt Clark, George S.D. Gordon,
  Fernando Perez-Cota
Categories: q-bio.QM cs.AI cs.LG eess.IV eess.SP
\\
  Advances in artificial intelligence (AI) show great potential in revealing
underlying information from phonon microscopy (high-frequency ultrasound) data
to identify cancerous cells. However, this technology suffers from the 'batch
effect' that comes from unavoidable technical variations between each
experiment, creating confounding variables that the AI model may inadvertently
learn. We therefore present a multi-task conditional neural network framework
to simultaneously achieve inter-batch calibration, by removing confounding
variables, and accurate cell classification of time-resolved phonon-derived
signals. We validate our approach by training and validating on different
experimental batches, achieving a balanced precision of 89.22% and an average
cross-validated precision of 89.07% for classifying background, healthy and
cancerous regions. Classification can be performed in 0.5 seconds with only
simple prior batch information required for multiple batch corrections.
Further, we extend our model to reconstruct denoised signals, enabling physical
interpretation of salient features indicating disease state including sound
velocity, sound attenuation and cell-adhesion to substrate.
\\ ( https://arxiv.org/abs/2403.17992 ,  4516kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17995 (*cross-listing*)
Date: Tue, 26 Mar 2024 14:47:05 GMT   (8154kb,D)

Title: Semi-Supervised Image Captioning Considering Wasserstein Graph Matching
Authors: Yang Yang
Categories: cs.CV cs.AI cs.LG
\\
  Image captioning can automatically generate captions for the given images,
and the key challenge is to learn a mapping function from visual features to
natural language features. Existing approaches are mostly supervised ones,
i.e., each image has a corresponding sentence in the training set. However,
considering that describing images always requires a huge of manpower, we
usually have limited amount of described images (i.e., image-text pairs) and a
large number of undescribed images in real-world applications. Thereby, a
dilemma is the "Semi-Supervised Image Captioning". To solve this problem, we
propose a novel Semi-Supervised Image Captioning method considering Wasserstein
Graph Matching (SSIC-WGM), which turns to adopt the raw image inputs to
supervise the generated sentences. Different from traditional single modal
semi-supervised methods, the difficulty of semi-supervised cross-modal learning
lies in constructing intermediately comparable information among heterogeneous
modalities. In this paper, SSIC-WGM adopts the successful scene graphs as
intermediate information, and constrains the generated sentences from two
aspects: 1) inter-modal consistency. SSIC-WGM constructs the scene graphs of
the raw image and generated sentence respectively, then employs the wasserstein
distance to better measure the similarity between region embeddings of
different graphs. 2) intra-modal consistency. SSIC-WGM takes the data
augmentation techniques for the raw images, then constrains the consistency
among augmented images and generated sentences. Consequently, SSIC-WGM combines
the cross-modal pseudo supervision and structure invariant measure for
efficiently using the undescribed images, and learns more reasonable mapping
function.
\\ ( https://arxiv.org/abs/2403.17995 ,  8154kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18062 (*cross-listing*)
Date: Tue, 26 Mar 2024 19:26:53 GMT   (9838kb,D)

Title: ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large Language Models
  through Geometric Decomposition
Authors: Samuel Li, Sarthak Bhagat, Joseph Campbell, Yaqi Xie, Woojun Kim,
  Katia Sycara, Simon Stepputtis
Categories: cs.RO cs.AI
Comments: 8 pages
\\
  Task-oriented grasping of unfamiliar objects is a necessary skill for robots
in dynamic in-home environments. Inspired by the human capability to grasp such
objects through intuition about their shape and structure, we present a novel
zero-shot task-oriented grasping method leveraging a geometric decomposition of
the target object into simple, convex shapes that we represent in a graph
structure, including geometric attributes and spatial relationships. Our
approach employs minimal essential information - the object's name and the
intended task - to facilitate zero-shot task-oriented grasping. We utilize the
commonsense reasoning capabilities of large language models to dynamically
assign semantic meaning to each decomposed part and subsequently reason over
the utility of each part for the intended task. Through extensive experiments
on a real-world robotics platform, we demonstrate that our grasping approach's
decomposition and reasoning pipeline is capable of selecting the correct part
in 92% of the cases and successfully grasping the object in 82% of the tasks we
evaluate. Additional videos, experiments, code, and data are available on our
project website: https://shapegrasp.github.io/.
\\ ( https://arxiv.org/abs/2403.18062 ,  9838kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18063 (*cross-listing*)
Date: Tue, 26 Mar 2024 19:29:21 GMT   (3137kb,D)

Title: Spectral Convolutional Transformer: Harmonizing Real vs. Complex
  Multi-View Spectral Operators for Vision Transformer
Authors: Badri N. Patro, Vinay P. Namboodiri, Vijay S. Agneeswaran
Categories: cs.CV cs.AI cs.CL cs.LG cs.MM
\\
  Transformers used in vision have been investigated through diverse
architectures - ViT, PVT, and Swin. These have worked to improve the attention
mechanism and make it more efficient. Differently, the need for including local
information was felt, leading to incorporating convolutions in transformers
such as CPVT and CvT. Global information is captured using a complex Fourier
basis to achieve global token mixing through various methods, such as AFNO,
GFNet, and Spectformer. We advocate combining three diverse views of data -
local, global, and long-range dependence. We also investigate the simplest
global representation using only the real domain spectral representation -
obtained through the Hartley transform. We use a convolutional operator in the
initial layers to capture local information. Through these two contributions,
we are able to optimize and obtain a spectral convolution transformer (SCT)
that provides improved performance over the state-of-the-art methods while
reducing the number of parameters. Through extensive experiments, we show that
SCT-C-small gives state-of-the-art performance on the ImageNet dataset and
reaches 84.5\% top-1 accuracy, while SCT-C-Large reaches 85.9\% and SCT-C-Huge
reaches 86.4\%. We evaluate SCT on transfer learning on datasets such as
CIFAR-10, CIFAR-100, Oxford Flower, and Stanford Car. We also evaluate SCT on
downstream tasks i.e. instance segmentation on the MSCOCO dataset. The project
page is available on this webpage.\url{https://github.com/badripatro/sct}
\\ ( https://arxiv.org/abs/2403.18063 ,  3137kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18067 (*cross-listing*)
Date: Tue, 26 Mar 2024 19:36:50 GMT   (3870kb,D)

Title: State of the art applications of deep learning within tracking and
  detecting marine debris: A survey
Authors: Zoe Moorton, Dr. Zeyneb Kurt, Dr. Wai Lok Woo
Categories: cs.CV cs.AI cs.LG
Comments: Review paper, 60 pages including references, 1 figure, 3 tables, 1
  supplementary data
\\
  Deep learning techniques have been explored within the marine litter problem
for approximately 20 years but the majority of the research has developed
rapidly in the last five years. We provide an in-depth, up to date, summary and
analysis of 28 of the most recent and significant contributions of deep
learning in marine debris. From cross referencing the research paper results,
the YOLO family significantly outperforms all other methods of object detection
but there are many respected contributions to this field that have
categorically agreed that a comprehensive database of underwater debris is not
currently available for machine learning. Using a small dataset curated and
labelled by us, we tested YOLOv5 on a binary classification task and found the
accuracy was low and the rate of false positives was high; highlighting the
importance of a comprehensive database. We conclude this survey with over 40
future research recommendations and open challenges.
\\ ( https://arxiv.org/abs/2403.18067 ,  3870kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18079 (*cross-listing*)
Date: Tue, 26 Mar 2024 19:58:39 GMT   (47kb)

Title: Paths to Equilibrium in Normal-Form Games
Authors: Bora Yongacoglu, G\"urdal Arslan, Lacra Pavel, Serdar Y\"uksel
Categories: cs.GT cs.AI cs.LG
\\
  In multi-agent reinforcement learning (MARL), agents repeatedly interact
across time and revise their strategies as new data arrives, producing a
sequence of strategy profiles. This paper studies sequences of strategies
satisfying a pairwise constraint inspired by policy updating in reinforcement
learning, where an agent who is best responding in period $t$ does not switch
its strategy in the next period $t+1$. This constraint merely requires that
optimizing agents do not switch strategies, but does not constrain the other
non-optimizing agents in any way, and thus allows for exploration. Sequences
with this property are called satisficing paths, and arise naturally in many
MARL algorithms. A fundamental question about strategic dynamics is such: for a
given game and initial strategy profile, is it always possible to construct a
satisficing path that terminates at an equilibrium strategy? The resolution of
this question has implications about the capabilities or limitations of a class
of MARL algorithms. We answer this question in the affirmative for mixed
extensions of finite normal-form games.%
\\ ( https://arxiv.org/abs/2403.18079 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18116 (*cross-listing*)
Date: Tue, 26 Mar 2024 21:45:29 GMT   (539kb,D)

Title: QuakeSet: A Dataset and Low-Resource Models to Monitor Earthquakes
  through Sentinel-1
Authors: Daniele Rege Cambrin, Paolo Garza
Categories: cs.CV cs.AI
Comments: Accepted at ISCRAM 2024
\\
  Earthquake monitoring is necessary to promptly identify the affected areas,
the severity of the events, and, finally, to estimate damages and plan the
actions needed for the restoration process. The use of seismic stations to
monitor the strength and origin of earthquakes is limited when dealing with
remote areas (we cannot have global capillary coverage). Identification and
analysis of all affected areas is mandatory to support areas not monitored by
traditional stations. Using social media images in crisis management has proven
effective in various situations. However, they are still limited by the
possibility of using communication infrastructures in case of an earthquake and
by the presence of people in the area. Moreover, social media images and
messages cannot be used to estimate the actual severity of earthquakes and
their characteristics effectively. The employment of satellites to monitor
changes around the globe grants the possibility of exploiting instrumentation
that is not limited by the visible spectrum, the presence of land
infrastructures, and people in the affected areas. In this work, we propose a
new dataset composed of images taken from Sentinel-1 and a new series of tasks
to help monitor earthquakes from a new detailed view. Coupled with the data, we
provide a series of traditional machine learning and deep learning models as
baselines to assess the effectiveness of ML-based models in earthquake
analysis.
\\ ( https://arxiv.org/abs/2403.18116 ,  539kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18195 (*cross-listing*)
Date: Wed, 27 Mar 2024 02:08:12 GMT   (3797kb,D)

Title: SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly
  Network
Authors: Yuxuan Wan, Kaichen Zhou, jinhong Chen, Hao Dong
Categories: cs.RO cs.AI
\\
  Autonomous assembly in robotics and 3D vision presents significant
challenges, particularly in ensuring assembly correctness. Presently,
predominant methods such as MEPNet focus on assembling components based on
manually provided images. However, these approaches often fall short in
achieving satisfactory results for tasks requiring long-term planning.
Concurrently, we observe that integrating a self-correction module can
partially alleviate such issues. Motivated by this concern, we introduce the
single-step assembly error correction task, which involves identifying and
rectifying misassembled components. To support research in this area, we
present the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising
manual images for assembly steps and instances of assembly failures.
Additionally, we propose the Self-Correct Assembly Network (SCANet), a novel
method to address this task. SCANet treats assembled components as queries,
determining their correctness in manual images and providing corrections when
necessary. Finally, we utilize SCANet to correct the assembly results of
MEPNet. Experimental results demonstrate that SCANet can identify and correct
MEPNet's misassembled results, significantly improving the correctness of
assembly. Our code and dataset are available at
https://github.com/Yaser-wyx/SCANet.
\\ ( https://arxiv.org/abs/2403.18195 ,  3797kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18208 (*cross-listing*)
Date: Wed, 27 Mar 2024 02:39:23 GMT   (385kb,D)

Title: An Evolutionary Network Architecture Search Framework with Adaptive
  Multimodal Fusion for Hand Gesture Recognition
Authors: Yizhang Xia, Shihao Song, Zhanglu Hou, Junwen Xu, Juan Zou, Yuan Liu
  and Shengxiang Yang
Categories: cs.CV cs.AI cs.NE
\\
  Hand gesture recognition (HGR) based on multimodal data has attracted
considerable attention owing to its great potential in applications. Various
manually designed multimodal deep networks have performed well in multimodal
HGR (MHGR), but most of existing algorithms require a lot of expert experience
and time-consuming manual trials. To address these issues, we propose an
evolutionary network architecture search framework with the adaptive multimodel
fusion (AMF-ENAS). Specifically, we design an encoding space that
simultaneously considers fusion positions and ratios of the multimodal data,
allowing for the automatic construction of multimodal networks with different
architectures through decoding. Additionally, we consider three input streams
corresponding to intra-modal surface electromyography (sEMG), intra-modal
accelerometer (ACC), and inter-modal sEMG-ACC. To automatically adapt to
various datasets, the ENAS framework is designed to automatically search a MHGR
network with appropriate fusion positions and ratios. To the best of our
knowledge, this is the first time that ENAS has been utilized in MHGR to tackle
issues related to the fusion position and ratio of multimodal data.
Experimental results demonstrate that AMF-ENAS achieves state-of-the-art
performance on the Ninapro DB2, DB3, and DB7 datasets.
\\ ( https://arxiv.org/abs/2403.18208 ,  385kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18212 (*cross-listing*)
Date: Wed, 27 Mar 2024 02:46:09 GMT   (2228kb,D)

Title: Preference-Based Planning in Stochastic Environments: From
  Partially-Ordered Temporal Goals to Most Preferred Policies
Authors: Hazhar Rahmani, Abhishek N. Kulkarni and Jie Fu
Categories: cs.RO cs.AI cs.FL cs.LO
Comments: arXiv admin note: substantial text overlap with arXiv:2209.12267
\\
  Human preferences are not always represented via complete linear orders: It
is natural to employ partially-ordered preferences for expressing incomparable
outcomes. In this work, we consider decision-making and probabilistic planning
in stochastic systems modeled as Markov decision processes (MDPs), given a
partially ordered preference over a set of temporally extended goals.
Specifically, each temporally extended goal is expressed using a formula in
Linear Temporal Logic on Finite Traces (LTL$_f$). To plan with the partially
ordered preference, we introduce order theory to map a preference over temporal
goals to a preference over policies for the MDP. Accordingly, a most preferred
policy under a stochastic ordering induces a stochastic nondominated
probability distribution over the finite paths in the MDP. To synthesize a most
preferred policy, our technical approach includes two key steps. In the first
step, we develop a procedure to transform a partially ordered preference over
temporal goals into a computational model, called preference automaton, which
is a semi-automaton with a partial order over acceptance conditions. In the
second step, we prove that finding a most preferred policy is equivalent to
computing a Pareto-optimal policy in a multi-objective MDP that is constructed
from the original MDP, the preference automaton, and the chosen stochastic
ordering relation. Throughout the paper, we employ running examples to
illustrate the proposed preference specification and solution approaches. We
demonstrate the efficacy of our algorithm using these examples, providing
detailed analysis, and then discuss several potential future directions.
\\ ( https://arxiv.org/abs/2403.18212 ,  2228kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18223 (*cross-listing*)
Date: Wed, 27 Mar 2024 03:25:45 GMT   (123kb,D)

Title: A Transformer-Based Framework for Payload Malware Detection and
  Classification
Authors: Kyle Stein, Arash Mahyari, Guillermo Francia III, Eman El-Sheikh
Categories: cs.CR cs.AI cs.LG
\\
  As malicious cyber threats become more sophisticated in breaching computer
networks, the need for effective intrusion detection systems (IDSs) becomes
crucial. Techniques such as Deep Packet Inspection (DPI) have been introduced
to allow IDSs analyze the content of network packets, providing more context
for identifying potential threats. IDSs traditionally rely on using
anomaly-based and signature-based detection techniques to detect unrecognized
and suspicious activity. Deep learning techniques have shown great potential in
DPI for IDSs due to their efficiency in learning intricate patterns from the
packet content being transmitted through the network. In this paper, we propose
a revolutionary DPI algorithm based on transformers adapted for the purpose of
detecting malicious traffic with a classifier head. Transformers learn the
complex content of sequence data and generalize them well to similar scenarios
thanks to their self-attention mechanism. Our proposed method uses the raw
payload bytes that represent the packet contents and is deployed as
man-in-the-middle. The payload bytes are used to detect malicious packets and
classify their types. Experimental results on the UNSW-NB15 and CIC-IOT23
datasets demonstrate that our transformer-based model is effective in
distinguishing malicious from benign traffic in the test dataset, attaining an
average accuracy of 79\% using binary classification and 72\% on the
multi-classification experiment, both using solely payload bytes.
\\ ( https://arxiv.org/abs/2403.18223 ,  123kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18241 (*cross-listing*)
Date: Wed, 27 Mar 2024 04:09:34 GMT   (12040kb,D)

Title: NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,
  Reconstruction, and Generation
Authors: Ruikai Cui, Weizhe Liu, Weixuan Sun, Senbo Wang, Taizhang Shang, Yang
  Li, Xibin Song, Han Yan, Zhennan Wu, Shenzhou Chen, Hongdong Li, Pan Ji
Categories: cs.CV cs.AI cs.GR cs.LG
\\
  3D shape generation aims to produce innovative 3D content adhering to
specific conditions and constraints. Existing methods often decompose 3D shapes
into a sequence of localized components, treating each element in isolation
without considering spatial consistency. As a result, these approaches exhibit
limited versatility in 3D data representation and shape generation, hindering
their ability to generate highly diverse 3D shapes that comply with the
specified constraints. In this paper, we introduce a novel spatial-aware 3D
shape generation framework that leverages 2D plane representations for enhanced
3D shape modeling. To ensure spatial coherence and reduce memory usage, we
incorporate a hybrid shape representation technique that directly learns a
continuous signed distance field representation of the 3D shape using
orthogonal 2D planes. Additionally, we meticulously enforce spatial
correspondences across distinct planes using a transformer-based autoencoder
structure, promoting the preservation of spatial relationships in the generated
3D shapes. This yields an algorithm that consistently outperforms
state-of-the-art 3D shape generation methods on various tasks, including
unconditional shape generation, multi-modal shape completion, single-view
reconstruction, and text-to-shape synthesis.
\\ ( https://arxiv.org/abs/2403.18241 ,  12040kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18252 (*cross-listing*)
Date: Wed, 27 Mar 2024 04:49:23 GMT   (6697kb,D)

Title: Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models
Authors: Yiwu Zhong, Zi-Yuan Hu, Michael R. Lyu, Liwei Wang
Categories: cs.CV cs.AI cs.CL cs.LG cs.MM
Comments: Project page: https://github.com/LaVi-Lab/Visual-Table
\\
  Visual representation learning has been a cornerstone in computer vision,
evolving from supervised learning with human-annotated labels to aligning
image-text pairs from the Internet. Despite recent advancements in multi-modal
large language models (MLLMs), the visual representations they rely on, such as
CLIP embeddings, often lack access to external world knowledge critical for
real-world visual reasoning. In this work, we propose Visual Table, a novel
visual representation tailored for MLLMs. It provides hierarchical text
descriptions of holistic visual scenes, consisting of a scene description and
multiple object-centric descriptions that encompass categories, attributes, and
knowledge at instance level. We further develop a scalable generator for visual
table generation and train it on small-scale annotations from GPT4V. Extensive
evaluations demonstrate that, with generated visual tables as additional visual
representations, our model can consistently outperform the state-of-the-art
(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone
visual representations, our model can closely match or even beat the SOTA MLLMs
that are built on CLIP visual embeddings. Our code is available at
https://github.com/LaVi-Lab/Visual-Table.
\\ ( https://arxiv.org/abs/2403.18252 ,  6697kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18256 (*cross-listing*)
Date: Wed, 27 Mar 2024 04:56:48 GMT   (4222kb,D)

Title: Manipulating Neural Path Planners via Slight Perturbations
Authors: Zikang Xiong and Suresh Jagannathan
Categories: cs.RO cs.AI
\\
  Data-driven neural path planners are attracting increasing interest in the
robotics community. However, their neural network components typically come as
black boxes, obscuring their underlying decision-making processes. Their
black-box nature exposes them to the risk of being compromised via the
insertion of hidden malicious behaviors. For example, an attacker may hide
behaviors that, when triggered, hijack a delivery robot by guiding it to a
specific (albeit wrong) destination, trapping it in a predefined region, or
inducing unnecessary energy expenditure by causing the robot to repeatedly
circle a region. In this paper, we propose a novel approach to specify and
inject a range of hidden malicious behaviors, known as backdoors, into neural
path planners. Our approach provides a concise but flexible way to define these
behaviors, and we show that hidden behaviors can be triggered by slight
perturbations (e.g., inserting a tiny unnoticeable object), that can
nonetheless significantly compromise their integrity. We also discuss potential
techniques to identify these backdoors aimed at alleviating such risks. We
demonstrate our approach on both sampling-based and search-based neural path
planners.
\\ ( https://arxiv.org/abs/2403.18256 ,  4222kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18258 (*cross-listing*)
Date: Wed, 27 Mar 2024 05:10:38 GMT   (23533kb,D)

Title: Enhancing Generative Class Incremental Learning Performance with Model
  Forgetting Approach
Authors: Taro Togo, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama
Categories: cs.CV cs.AI
\\
  This study presents a novel approach to Generative Class Incremental Learning
(GCIL) by introducing the forgetting mechanism, aimed at dynamically managing
class information for better adaptation to streaming data. GCIL is one of the
hot topics in the field of computer vision, and this is considered one of the
crucial tasks in society, specifically the continual learning of generative
models. The ability to forget is a crucial brain function that facilitates
continual learning by selectively discarding less relevant information for
humans. However, in the field of machine learning models, the concept of
intentionally forgetting has not been extensively investigated. In this study
we aim to bridge this gap by incorporating the forgetting mechanisms into GCIL,
thereby examining their impact on the models' ability to learn in continual
learning. Through our experiments, we have found that integrating the
forgetting mechanisms significantly enhances the models' performance in
acquiring new knowledge, underscoring the positive role that strategic
forgetting plays in the process of continual learning.
\\ ( https://arxiv.org/abs/2403.18258 ,  23533kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18305 (*cross-listing*)
Date: Wed, 27 Mar 2024 06:59:39 GMT   (4569kb,D)

Title: A Recommender System for NFT Collectibles with Item Feature
Authors: Minjoo Choi, Seonmi Kim, Yejin Kim, Youngbin Lee, Joohwan Hong,
  Yongjae Lee
Categories: cs.IR cs.AI
Comments: Presented at the AAAI 2023 Bridge on AI for Financial Services
  (https://sites.google.com/view/aaai-ai-fin/home)
\\
  Recommender systems have been actively studied and applied in various domains
to deal with information overload. Although there are numerous studies on
recommender systems for movies, music, and e-commerce, comparatively less
attention has been paid to the recommender system for NFTs despite the
continuous growth of the NFT market. This paper presents a recommender system
for NFTs that utilizes a variety of data sources, from NFT transaction records
to external item features, to generate precise recommendations that cater to
individual preferences. We develop a data-efficient graph-based recommender
system to efficiently capture the complex relationship between each item and
users and generate node(item) embeddings which incorporate both node feature
information and graph structure. Furthermore, we exploit inputs beyond
user-item interactions, such as image feature, text feature, and price feature.
Numerical experiments verify the performance of the graph-based recommender
system improves significantly after utilizing all types of item features as
side information, thereby outperforming all other baselines.
\\ ( https://arxiv.org/abs/2403.18305 ,  4569kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18347 (*cross-listing*)
Date: Wed, 27 Mar 2024 08:38:56 GMT   (4158kb,D)

Title: A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal
  Holes
Authors: Sanmoy Bandyopadhyay, Suman Kundu
Categories: astro-ph.SR cs.AI cs.CV cs.LG
Comments: 14 pages, 5 figures, 3 tables
\\
  The detection and analysis of the solar coronal holes (CHs) is an important
field of study in the domain of solar physics. Mainly, it is required for the
proper prediction of the geomagnetic storms which directly or indirectly affect
various space and ground-based systems. For the detection of CHs till date, the
solar scientist depends on manual hand-drawn approaches. However, with the
advancement of image processing technologies, some automated image segmentation
methods have been used for the detection of CHs. In-spite of this, fast and
accurate detection of CHs are till a major issues. Here in this work, a novel
quantum computing-based fast fuzzy c-mean technique has been developed for fast
detection of the CHs region. The task has been carried out in two stages, in
first stage the solar image has been segmented using a quantum computing based
fast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted
out from the segmented image based on image morphological operation. In the
work, quantum computing has been used to optimize the cost function of the fast
fuzzy c-mean (FFCM) algorithm, where quantum approximate optimization algorithm
(QAOA) has been used to optimize the quadratic part of the cost function. The
proposed method has been tested for 193 \AA{} SDO/AIA full-disk solar image
datasets and has been compared with the existing techniques. The outcome shows
the comparable performance of the proposed method with the existing one within
a very lesser time.
\\ ( https://arxiv.org/abs/2403.18347 ,  4158kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18351 (*cross-listing*)
Date: Wed, 27 Mar 2024 08:42:47 GMT   (24668kb,D)

Title: Generating Diverse Agricultural Data for Vision-Based Farming
  Applications
Authors: Mikolaj Cieslak and Umabharathi Govindarajan and Alejandro Garcia and
  Anuradha Chandrashekar and Torsten H\"adrich and Aleksander Mendoza-Drosik
  and Dominik L. Michels and S\"oren Pirk and Chia-Chun Fu and Wojciech
  Pa{\l}ubicki
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: 10 pages, 8 figures, 3 tables
MSC-class: 68T07, 68T45
ACM-class: I.2.10; I.4.6
\\
  We present a specialized procedural model for generating synthetic
agricultural scenes, focusing on soybean crops, along with various weeds. This
model is capable of simulating distinct growth stages of these plants, diverse
soil conditions, and randomized field arrangements under varying lighting
conditions. The integration of real-world textures and environmental factors
into the procedural generation process enhances the photorealism and
applicability of the synthetic data. Our dataset includes 12,000 images with
semantic labels, offering a comprehensive resource for computer vision tasks in
precision agriculture, such as semantic segmentation for autonomous weed
control. We validate our model's effectiveness by comparing the synthetic data
against real agricultural images, demonstrating its potential to significantly
augment training data for machine learning models in agriculture. This approach
not only provides a cost-effective solution for generating high-quality,
diverse data but also addresses specific needs in agricultural vision tasks
that are not fully covered by general-purpose models.
\\ ( https://arxiv.org/abs/2403.18351 ,  24668kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18364 (*cross-listing*)
Date: Wed, 27 Mar 2024 08:57:15 GMT   (8566kb,D)

Title: Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR
Authors: Salwa Mostafa, Mateus P. Mota, Alvaro Valcarce, and Mehdi Bennis
Categories: cs.IT cs.AI cs.LG math.IT
\\
  We investigate the problem of supporting Industrial Internet of Things user
equipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and
random traffic arrival. A deep reinforcement learning (DRL) based centralized
dynamic scheduler for time-frequency resources is proposed to learn how to
schedule the available communication resources among the IIoT UEs. The proposed
scheduler leverages an RL framework to adapt to the dynamic changes in the
wireless communication system and traffic arrivals. Moreover, a graph-based
reduction scheme is proposed to reduce the state and action space of the RL
framework to allow fast convergence and a better learning strategy. Simulation
results demonstrate the effectiveness of the proposed intelligent scheduler in
guaranteeing the expressed intent of IIoT UEs compared to several traditional
scheduling schemes, such as round-robin, semi-static, and heuristic approaches.
The proposed scheduler also outperforms the contention-free and
contention-based schemes in maximizing the number of successfully computed
tasks.
\\ ( https://arxiv.org/abs/2403.18364 ,  8566kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18383 (*cross-listing*)
Date: Wed, 27 Mar 2024 09:21:07 GMT   (38276kb,D)

Title: Generative Multi-modal Models are Good Class-Incremental Learners
Authors: Xusheng Cao, Haori Lu, Linlan Huang, Xialei Liu, Ming-Ming Cheng
Categories: cs.CV cs.AI cs.LG
Comments: Accepted at CVPR 2024
\\
  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic
forgetting caused by the classifier's bias towards the current task has long
posed a significant challenge. It is mainly caused by the characteristic of
discriminative models. With the growing popularity of the generative
multi-modal models, we would explore replacing discriminative models with
generative ones for CIL. However, transitioning from discriminative to
generative models requires addressing two key challenges. The primary challenge
lies in transferring the generated textual information into the classification
of distinct categories. Additionally, it requires formulating the task of CIL
within a generative framework. To this end, we propose a novel generative
multi-modal model (GMM) framework for class-incremental learning. Our approach
directly generates labels for images using an adapted generative model. After
obtaining the detailed text, we use a text encoder to extract text features and
employ feature matching to determine the most similar label as the
classification prediction. In the conventional CIL settings, we achieve
significantly better results in long-sequence task scenarios. Under the
Few-shot CIL setting, we have improved by at least 14\% accuracy over all the
current state-of-the-art methods with significantly less forgetting. Our code
is available at \url{https://github.com/DoubleClass/GMM}.
\\ ( https://arxiv.org/abs/2403.18383 ,  38276kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18397 (*cross-listing*)
Date: Wed, 27 Mar 2024 09:35:56 GMT   (5954kb)

Title: Colour and Brush Stroke Pattern Recognition in Abstract Art using
  Modified Deep Convolutional Generative Adversarial Networks
Authors: Srinitish Srinivasan and Varenya Pathak
Categories: cs.CV cs.AI cs.LG
Comments: 28 pages, 5 tables, 7 figures
\\
  Abstract Art is an immensely popular, discussed form of art that often has
the ability to depict the emotions of an artist. Many researchers have made
attempts to study abstract art in the form of edge detection, brush stroke and
emotion recognition algorithms using machine and deep learning. This papers
describes the study of a wide distribution of abstract paintings using
Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and
reproduce a distribution enabling researchers and scientists to effectively
explore and study the generated image space. However, the challenge lies in
developing an efficient GAN architecture that overcomes common training
pitfalls. This paper addresses this challenge by introducing a modified-DCGAN
(mDCGAN) specifically designed for high-quality artwork generation. The
approach involves a thorough exploration of the modifications made, delving
into the intricate workings of DCGANs, optimisation techniques, and
regularisation methods aimed at improving stability and realism in art
generation enabling effective study of generated patterns. The proposed mDCGAN
incorporates meticulous adjustments in layer configurations and architectural
choices, offering tailored solutions to the unique demands of art generation
while effectively combating issues like mode collapse and gradient vanishing.
Further this paper explores the generated latent space by performing random
walks to understand vector relationships between brush strokes and colours in
the abstract art space and a statistical analysis of unstable outputs after a
certain period of GAN training and compare its significant difference. These
findings validate the effectiveness of the proposed approach, emphasising its
potential to revolutionise the field of digital art generation and digital art
ecosystem.
\\ ( https://arxiv.org/abs/2403.18397 ,  5954kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18406 (*cross-listing*)
Date: Wed, 27 Mar 2024 09:48:23 GMT   (16566kb,D)

Title: An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering
  Using a VLM
Authors: Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: Our code is available at https://github.com/imagegridworth/IG-VLM
\\
  Stimulated by the sophisticated reasoning capabilities of recent Large
Language Models (LLMs), a variety of strategies for bridging video modality
have been devised. A prominent strategy involves Video Language Models
(VideoLMs), which train a learnable interface with video data to connect
advanced vision encoders with LLMs. Recently, an alternative strategy has
surfaced, employing readily available foundation models, such as VideoLMs and
LLMs, across multiple stages for modality bridging. In this study, we introduce
a simple yet novel strategy where only a single Vision Language Model (VLM) is
utilized. Our starting point is the plain insight that a video comprises a
series of images, or frames, interwoven with temporal information. The essence
of video comprehension lies in adeptly managing the temporal aspects along with
the spatial details of each frame. Initially, we transform a video into a
single composite image by arranging multiple frames in a grid layout. The
resulting single image is termed as an image grid. This format, while
maintaining the appearance of a solitary image, effectively retains temporal
information within the grid structure. Therefore, the image grid approach
enables direct application of a single high-performance VLM without
necessitating any video-data training. Our extensive experimental analysis
across ten zero-shot video question answering benchmarks, including five
open-ended and five multiple-choice benchmarks, reveals that the proposed Image
Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out
of ten benchmarks.
\\ ( https://arxiv.org/abs/2403.18406 ,  16566kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18407 (*cross-listing*)
Date: Wed, 27 Mar 2024 09:49:37 GMT   (1590kb,D)

Title: A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is
  Critical for Semi-supervised Classification
Authors: Jiaqi Wu, Junbiao Pang, Baochang Zhang, Qingming Huang
Categories: cs.CV cs.AI
\\
  Semi-supervised learning (SSL) is a practical challenge in computer vision.
Pseudo-label (PL) methods, e.g., FixMatch and FreeMatch, obtain the State Of
The Art (SOTA) performances in SSL. These approaches employ a
threshold-to-pseudo-label (T2L) process to generate PLs by truncating the
confidence scores of unlabeled data predicted by the self-training method.
However, self-trained models typically yield biased and high-variance
predictions, especially in the scenarios when a little labeled data are
supplied. To address this issue, we propose a lightweight channel-based
ensemble method to effectively consolidate multiple inferior PLs into the
theoretically guaranteed unbiased and low-variance one. Importantly, our
approach can be readily extended to any SSL framework, such as FixMatch or
FreeMatch. Experimental results demonstrate that our method significantly
outperforms state-of-the-art techniques on CIFAR10/100 in terms of
effectiveness and efficiency.
\\ ( https://arxiv.org/abs/2403.18407 ,  1590kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18425 (*cross-listing*)
Date: Wed, 27 Mar 2024 10:26:42 GMT   (34928kb,D)

Title: U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models
Authors: Ilias Mitsouras, Eleftherios Tsonis, Paraskevi Tzouveli, Athanasios
  Voulodimos
Categories: cs.CV cs.AI cs.LG
\\
  Diffusion models have demonstrated remarkable performance in text-to-image
synthesis, producing realistic and high resolution images that faithfully
adhere to the corresponding text-prompts. Despite their great success, they
still fall behind in sketch-to-image synthesis tasks, where in addition to
text-prompts, the spatial layout of the generated images has to closely follow
the outlines of certain reference sketches. Employing an MLP latent edge
predictor to guide the spatial layout of the synthesized image by predicting
edge maps at each denoising step has been recently proposed. Despite yielding
promising results, the pixel-wise operation of the MLP does not take into
account the spatial layout as a whole, and demands numerous denoising
iterations to produce satisfactory images, leading to time inefficiency. To
this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge
predictor, which is capable of efficiently capturing both local and global
features, as well as spatial correlations between pixels. Moreover, we propose
the addition of a sketch simplification network that offers the user the choice
of preprocessing and simplifying input sketches for enhanced outputs. The
experimental results, corroborated by user feedback, demonstrate that our
proposed U-Net latent edge predictor leads to more realistic results, that are
better aligned with the spatial outlines of the reference sketches, while
drastically reducing the number of required denoising steps and, consequently,
the overall execution time.
\\ ( https://arxiv.org/abs/2403.18425 ,  34928kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18459 (*cross-listing*)
Date: Wed, 27 Mar 2024 11:18:01 GMT   (1929kb,D)

Title: CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration
Authors: Marina Ionova and Jan Kristof Behrens
Categories: cs.RO cs.AI
Comments: 7 pages, 8 figures
\\
  Assembly processes involving humans and robots are challenging scenarios
because the individual activities and access to shared workspace have to be
coordinated. Fixed robot programs leave no room to diverge from a fixed
protocol. Working on such a process can be stressful for the user and lead to
ineffective behavior or failure. We propose a novel approach of online
constraint-based scheduling in a reactive execution control framework
facilitating behavior trees called CoBOS. This allows the robot to adapt to
uncertain events such as delayed activity completions and activity selection
(by the human). The user will experience less stress as the robotic coworkers
adapt their behavior to best complement the human-selected activities to
complete the common task. In addition to the improved working conditions, our
algorithm leads to increased efficiency, even in highly uncertain scenarios. We
evaluate our algorithm using a probabilistic simulation study with 56000
experiments. We outperform all baselines by a margin of 4-10%. Initial real
robot experiments using a Franka Emika Panda robot and human tracking based on
HTC Vive VR gloves look promising.
\\ ( https://arxiv.org/abs/2403.18459 ,  1929kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18469 (*cross-listing*)
Date: Wed, 27 Mar 2024 11:28:57 GMT   (7631kb,D)

Title: Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain
  Adaptive Segmentation of 3D Point Clouds
Authors: Zhimin Yuan, Wankang Zeng, Yanfei Su, Weiquan Liu, Ming Cheng, Yulan
  Guo, Cheng Wang
Categories: cs.CV cs.AI
Comments: CVPR2024
\\
  3D synthetic-to-real unsupervised domain adaptive segmentation is crucial to
annotating new domains. Self-training is a competitive approach for this task,
but its performance is limited by different sensor sampling patterns (i.e.,
variations in point density) and incomplete training strategies. In this work,
we propose a density-guided translator (DGT), which translates point density
between domains, and integrates it into a two-stage self-training pipeline
named DGT-ST. First, in contrast to existing works that simultaneously conduct
data generation and feature/output alignment within unstable adversarial
training, we employ the non-learnable DGT to bridge the domain gap at the input
level. Second, to provide a well-initialized model for self-training, we
propose a category-level adversarial network in stage one that utilizes the
prototype to prevent negative transfer. Finally, by leveraging the designs
above, a domain-mixed self-training method with source-aware consistency loss
is proposed in stage two to narrow the domain gap further. Experiments on two
synthetic-to-real segmentation tasks (SynLiDAR $\rightarrow$ semanticKITTI and
SynLiDAR $\rightarrow$ semanticPOSS) demonstrate that DGT-ST outperforms
state-of-the-art methods, achieving 9.4$\%$ and 4.3$\%$ mIoU improvements,
respectively. Code is available at \url{https://github.com/yuan-zm/DGT-ST}.
\\ ( https://arxiv.org/abs/2403.18469 ,  7631kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18536 (*cross-listing*)
Date: Wed, 27 Mar 2024 13:12:41 GMT   (617kb)

Title: A Novel Behavior-Based Recommendation System for E-commerce
Authors: Reza Barzegar Nozari, Mahdi Divsalar, Sepehr Akbarzadeh Abkenar,
  Mohammadreza Fadavi Amiri, Ali Divsalar
Categories: cs.IR cs.AI cs.HC
\\
  The majority of existing recommender systems rely on user ratings, which are
limited by the lack of user collaboration and the sparsity problem. To address
these issues, this study proposes a behavior-based recommender system that
leverages customers' natural behaviors, such as browsing and clicking, on
e-commerce platforms. The proposed recommendation system involves clustering
active customers, determining neighborhoods, collecting similar users,
calculating product reputation based on similar users, and recommending
high-reputation products. To overcome the complexity of customer behaviors and
traditional clustering methods, an unsupervised clustering approach based on
product categories is developed to enhance the recommendation methodology. This
study makes notable contributions in several aspects. Firstly, a groundbreaking
behavior-based recommendation methodology is developed, incorporating customer
behavior to generate accurate and tailored recommendations leading to improved
customer satisfaction and engagement. Secondly, an original unsupervised
clustering method, focusing on product categories, enables more precise
clustering and facilitates accurate recommendations. Finally, an approach to
determine neighborhoods for active customers within clusters is established,
ensuring grouping of customers with similar behavioral patterns to enhance
recommendation accuracy and relevance. The proposed recommendation methodology
and clustering method contribute to improved recommendation performance,
offering valuable insights for researchers and practitioners in the field of
e-commerce recommendation systems. Additionally, the proposed method
outperforms benchmark methods in experiments conducted using a behavior dataset
from the well-known e-commerce site Alibaba.
\\ ( https://arxiv.org/abs/2403.18536 ,  617kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18546 (*cross-listing*)
Date: Wed, 27 Mar 2024 13:24:58 GMT   (10078kb,D)

Title: Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes
Authors: Siang Chen, Wei Tang, Pengwei Xie, Wenming Yang, Guijin Wang
Categories: cs.RO cs.AI cs.CV
Comments: Extensive results on GraspNet-1B dataset
\\
  Fast and robust object grasping in clutter is a crucial component of
robotics. Most current works resort to the whole observed point cloud for 6-Dof
grasp generation, ignoring the guidance information excavated from global
semantics, thus limiting high-quality grasp generation and real-time
performance. In this work, we show that the widely used heatmaps are
underestimated in the efficiency of 6-Dof grasp generation. Therefore, we
propose an effective local grasp generator combined with grasp heatmaps as
guidance, which infers in a global-to-local semantic-to-point way.
Specifically, Gaussian encoding and the grid-based strategy are applied to
predict grasp heatmaps as guidance to aggregate local points into graspable
regions and provide global semantic information. Further, a novel non-uniform
anchor sampling mechanism is designed to improve grasp accuracy and diversity.
Benefiting from the high-efficiency encoding in the image space and focusing on
points in local graspable regions, our framework can perform high-quality grasp
detection in real-time and achieve state-of-the-art results. In addition, real
robot experiments demonstrate the effectiveness of our method with a success
rate of 94% and a clutter completion rate of 100%. Our code is available at
https://github.com/THU-VCLab/HGGD.
\\ ( https://arxiv.org/abs/2403.18546 ,  10078kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18593 (*cross-listing*)
Date: Wed, 27 Mar 2024 14:18:09 GMT   (1282kb,D)

Title: Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote
  Sensing Image Understanding
Authors: Run Shao, Zhaoyang Zhang, Chao Tao, Yunsheng Zhang, Chengli Peng,
  Haifeng Li
Categories: cs.CV cs.AI
Comments: 20 pages, 8 figures, 6 tables
\\
  The tokenizer, as one of the fundamental components of large models, has long
been overlooked or even misunderstood in visual tasks. One key factor of the
great comprehension power of the large language model is that natural language
tokenizers utilize meaningful words or subwords as the basic elements of
language. In contrast, mainstream visual tokenizers, represented by patch-based
methods such as Patch Embed, rely on meaningless rectangular patches as basic
elements of vision, which cannot serve as effectively as words or subwords in
language. Starting from the essence of the tokenizer, we defined semantically
independent regions (SIRs) for vision. We designed a simple HOmogeneous visual
tOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception
Module (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity,
the OPM splits the image into 4*4 pixel seeds and then utilizes the attention
mechanism to perceive SIRs. The OVM employs cross-attention to merge seeds
within the same SIR. To achieve adaptability, the OVM defines a variable number
of learnable vectors as cross-attention queries, allowing for the adjustment of
token quantity. We conducted experiments on the NWPU-RESISC45, WHU-RS19
classification dataset, and GID5 segmentation dataset for sparse and dense
tasks. The results demonstrate that the visual tokens obtained by HOOK
correspond to individual objects, which demonstrates homogeneity. HOOK
outperformed Patch Embed by 6\% and 10\% in the two tasks and achieved
state-of-the-art performance compared to the baselines used for comparison.
Compared to Patch Embed, which requires more than one hundred tokens for one
image, HOOK requires only 6 and 8 tokens for sparse and dense tasks,
respectively, resulting in efficiency improvements of 1.5 to 2.8 times. The
code is available at https://github.com/GeoX-Lab/Hook.
\\ ( https://arxiv.org/abs/2403.18593 ,  1282kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18600 (*cross-listing*)
Date: Wed, 27 Mar 2024 14:22:40 GMT   (14547kb,D)

Title: RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in
  Instructional Videos
Authors: Ali Zare, Yulei Niu, Hammad Ayyubi, Shih-fu Chang
Categories: cs.CV cs.AI cs.RO
Comments: 23 pages, 6 figures, 12 tables
\\
  Procedure Planning in instructional videos entails generating a sequence of
action steps based on visual observations of the initial and target states.
Despite the rapid progress in this task, there remain several critical
challenges to be solved: (1) Adaptive procedures: Prior works hold an
unrealistic assumption that the number of action steps is known and fixed,
leading to non-generalizable models in real-world scenarios where the sequence
length varies. (2) Temporal relation: Understanding the step temporal relation
knowledge is essential in producing reasonable and executable plans. (3)
Annotation cost: Annotating instructional videos with step-level labels (i.e.,
timestamp) or sequence-level labels (i.e., action category) is demanding and
labor-intensive, limiting its generalizability to large-scale datasets.In this
work, we propose a new and practical setting, called adaptive procedure
planning in instructional videos, where the procedure length is not fixed or
pre-determined. To address these challenges we introduce Retrieval-Augmented
Planner (RAP) model. Specifically, for adaptive procedures, RAP adaptively
determines the conclusion of actions using an auto-regressive model
architecture. For temporal relation, RAP establishes an external memory module
to explicitly retrieve the most relevant state-action pairs from the training
videos and revises the generated procedures. To tackle high annotation cost,
RAP utilizes a weakly-supervised learning manner to expand the training dataset
to other task-relevant, unannotated videos by generating pseudo labels for
action steps. Experiments on CrossTask and COIN benchmarks show the superiority
of RAP over traditional fixed-length models, establishing it as a strong
baseline solution for adaptive procedure planning.
\\ ( https://arxiv.org/abs/2403.18600 ,  14547kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18607 (*cross-listing*)
Date: Wed, 27 Mar 2024 14:25:02 GMT   (1706kb,D)

Title: Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic
  Learning over Low-power Devices
Authors: Hanqing Fu, Gaolei Li, Jun Wu, Jianhua Li, Xi Lin, Kai Zhou, Yuchen
  Liu
Categories: cs.CR cs.AI eess.SP
\\
  Federated neuromorphic learning (FedNL) leverages event-driven spiking neural
networks and federated learning frameworks to effectively execute intelligent
analysis tasks over amounts of distributed low-power devices but also perform
vulnerability to poisoning attacks. The threat of backdoor attacks on
traditional deep neural networks typically comes from time-invariant data.
However, in FedNL, unknown threats may be hidden in time-varying spike signals.
In this paper, we start to explore a novel vulnerability of FedNL-based systems
with the concept of time division multiplexing, termed Spikewhisper, which
allows attackers to evade detection as much as possible, as multiple malicious
clients can imperceptibly poison with different triggers at different
timeslices. In particular, the stealthiness of Spikewhisper is derived from the
time-domain divisibility of global triggers, in which each malicious client
pastes only one local trigger to a certain timeslice in the neuromorphic
sample, and also the polarity and motion of each local trigger can be
configured by attackers. Extensive experiments based on two different
neuromorphic datasets demonstrate that the attack success rate of Spikewispher
is higher than the temporally centralized attacks. Besides, it is validated
that the effect of Spikewispher is sensitive to the trigger duration.
\\ ( https://arxiv.org/abs/2403.18607 ,  1706kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18690 (*cross-listing*)
Date: Wed, 27 Mar 2024 15:41:23 GMT   (41985kb,D)

Title: Annolid: Annotate, Segment, and Track Anything You Need
Authors: Chen Yang, Thomas A. Cleland
Categories: cs.CV cs.AI
\\
  Annolid is a deep learning-based software package designed for the
segmentation, labeling, and tracking of research targets within video files,
focusing primarily on animal behavior analysis. Based on state-of-the-art
instance segmentation methods, Annolid now harnesses the Cutie video object
segmentation model to achieve resilient, markerless tracking of multiple
animals from single annotated frames, even in environments in which they may be
partially or entirely concealed by environmental features or by one another.
Our integration of Segment Anything and Grounding-DINO strategies additionally
enables the automatic masking and segmentation of recognizable animals and
objects by text command, removing the need for manual annotation. Annolid's
comprehensive approach to object segmentation flexibly accommodates a broad
spectrum of behavior analysis applications, enabling the classification of
diverse behavioral states such as freezing, digging, pup huddling, and social
interactions in addition to the tracking of animals and their body parts.
\\ ( https://arxiv.org/abs/2403.18690 ,  41985kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18711 (*cross-listing*)
Date: Wed, 27 Mar 2024 15:58:25 GMT   (1599kb,D)

Title: SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable
  Transient-Free 3D reconstruction from Satellite Imagery
Authors: Camille Billouard, Dawa Derksen, Emmanuelle Sarrazin, Bruno Vallet
Categories: cs.CV cs.AI
Comments: 5 pages, 3 figures, 1 table; Accepted to International Geoscience and
  Remote Sensing Symposium (IGARSS) 2024; Code available at
  https://github.com/Ellimac0/SAT-NGP
\\
  Current stereo-vision pipelines produce high accuracy 3D reconstruction when
using multiple pairs or triplets of satellite images. However, these pipelines
are sensitive to the changes between images that can occur as a result of
multi-date acquisitions. Such variations are mainly due to variable shadows,
reflexions and transient objects (cars, vegetation). To take such changes into
account, Neural Radiance Fields (NeRF) have recently been applied to multi-date
satellite imagery. However, Neural methods are very compute-intensive, taking
dozens of hours to learn, compared with minutes for standard stereo-vision
pipelines. Following the ideas of Instant Neural Graphics Primitives we propose
to use an efficient sampling strategy and multi-resolution hash encoding to
accelerate the learning. Our model, Satellite Neural Graphics Primitives
(SAT-NGP) decreases the learning time to 15 minutes while maintaining the
quality of the 3D reconstruction.
\\ ( https://arxiv.org/abs/2403.18711 ,  1599kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18715 (*cross-listing*)
Date: Wed, 27 Mar 2024 16:04:47 GMT   (7851kb,D)

Title: Mitigating Hallucinations in Large Vision-Language Models with
  Instruction Contrastive Decoding
Authors: Xintong Wang, Jingheng Pan, Liang Ding, Chris Biemann
Categories: cs.CV cs.AI cs.CL cs.MM
\\
  Large Vision-Language Models (LVLMs) are increasingly adept at generating
contextually detailed and coherent responses from visual inputs. However, their
application in multimodal decision-making and open-ended generation is hindered
by a notable rate of hallucinations, where generated text inaccurately
represents the visual contents. To address this issue, this paper introduces
the Instruction Contrastive Decoding (ICD) method, a novel approach designed to
reduce hallucinations during LVLM inference. Our method is inspired by our
observation that what we call disturbance instructions significantly exacerbate
hallucinations in multimodal fusion modules. ICD contrasts distributions from
standard and instruction disturbance, thereby increasing alignment uncertainty
and effectively subtracting hallucinated concepts from the original
distribution. Through comprehensive experiments on discriminative benchmarks
(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that
ICD significantly mitigates both object-level and attribute-level
hallucinations. Moreover, our method not only addresses hallucinations but also
significantly enhances the general perception and recognition capabilities of
LVLMs.
\\ ( https://arxiv.org/abs/2403.18715 ,  7851kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18755 (*cross-listing*)
Date: Wed, 27 Mar 2024 16:54:45 GMT   (1737kb,D)

Title: Many-Objective Evolutionary Influence Maximization: Balancing Spread,
  Budget, Fairness, and Time
Authors: Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca
Categories: cs.NE cs.AI cs.SI
Comments: To appear in Genetic and Evolutionary Computation Conference (GECCO
  24 Companion), July 14 18, 2024, Melbourne, VIC, Australia. ACM, New York,
  NY, USA
DOI: 10.1145/3638530.3654161
\\
  The Influence Maximization (IM) problem seeks to discover the set of nodes in
a graph that can spread the information propagation at most. This problem is
known to be NP-hard, and it is usually studied by maximizing the influence
(spread) and, optionally, optimizing a second objective, such as minimizing the
seed set size or maximizing the influence fairness. However, in many practical
scenarios multiple aspects of the IM problem must be optimized at the same
time. In this work, we propose a first case study where several IM-specific
objective functions, namely budget, fairness, communities, and time, are
optimized on top of the maximization of influence and minimization of the seed
set size. To this aim, we introduce MOEIM (Many-Objective Evolutionary
Algorithm for Influence Maximization) a Multi-Objective Evolutionary Algorithm
(MOEA) based on NSGA-II incorporating graph-aware operators and a smart
initialization. We compare MOEIM in two experimental settings, including a
total of nine graph datasets, two heuristic methods, a related MOEA, and a
state-of-the-art Deep Learning approach. The experiments show that MOEIM
overall outperforms the competitors in most of the tested many-objective
settings. To conclude, we also investigate the correlation between the
objectives, leading to novel insights into the topic. The codebase is available
at https://github.com/eliacunegatti/MOEIM.
\\ ( https://arxiv.org/abs/2403.18755 ,  1737kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18756 (*cross-listing*)
Date: Wed, 27 Mar 2024 16:56:14 GMT   (4396kb)

Title: Detection of subclinical atherosclerosis by image-based deep learning on
  chest x-ray
Authors: Guglielmo Gallone, Francesco Iodice, Alberto Presta, Davide Tore,
  Ovidio de Filippo, Michele Visciano, Carlo Alberto Barbano, Alessandro
  Serafini, Paola Gorrini, Alessandro Bruno, Walter Grosso Marra, James Hughes,
  Mario Iannaccone, Paolo Fonio, Attilio Fiandrotti, Alessandro Depaoli, Marco
  Grangetto, Gaetano Maria de Ferrari and Fabrizio D'Ascenzo
Categories: cs.CV cs.AI cs.LG
Comments: Submitted to European Heart Journal - Cardiovascular Imaging Added
  also the additional material 44 pages (30 main paper, 14 additional
  material), 14 figures (5 main manuscript, 9 additional material)
\\
  Aims. To develop a deep-learning based system for recognition of subclinical
atherosclerosis on a plain frontal chest x-ray. Methods and Results. A
deep-learning algorithm to predict coronary artery calcium (CAC) score (the
AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%
internal validation cohort) of primary prevention patients (58.4% male, median
age 63 [51-74] years) with available paired chest x-ray and chest computed
tomography (CT) indicated for any clinical reason and performed within 3
months. The CAC score calculated on chest CT was used as ground truth. The
model was validated on an temporally-independent cohort of 90 patients from the
same institution (external validation). The diagnostic accuracy of the AI-CAC
model assessed by the area under the curve (AUC) was the primary outcome.
Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.
AUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation
cohort and 0.77 in the external validation cohort. Sensitivity was consistently
above 92% in both cohorts. In the overall cohort (n=540), among patients with
AI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with
AI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events
(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to
accurately detect subclinical atherosclerosis on chest x-ray with elevated
sensitivity, and to predict ASCVD events with elevated negative predictive
value. Adoption of the AI-CAC model to refine CV risk stratification or as an
opportunistic screening tool requires prospective evaluation.
\\ ( https://arxiv.org/abs/2403.18756 ,  4396kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18762 (*cross-listing*)
Date: Wed, 27 Mar 2024 17:01:10 GMT   (8921kb,D)

Title: ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place
  Recognition
Authors: Weidong Xie, Lun Luo, Nanfei Ye, Yi Ren, Shaoyi Du, Minhang Wang,
  Jintao Xu, Rui Ai, Weihao Gu, Xieyuanli Chen
Categories: cs.CV cs.AI cs.RO
Comments: 8 pages, 11 figures, conference
\\
  Place recognition is an important task for robots and autonomous cars to
localize themselves and close loops in pre-built maps. While single-modal
sensor-based methods have shown satisfactory performance, cross-modal place
recognition that retrieving images from a point-cloud database remains a
challenging problem. Current cross-modal methods transform images into 3D
points using depth estimation for modality conversion, which are usually
computationally intensive and need expensive labeled data for depth
supervision. In this work, we introduce a fast and lightweight framework to
encode images and point clouds into place-distinctive descriptors. We propose
an effective Field of View (FoV) transformation module to convert point clouds
into an analogous modality as images. This module eliminates the necessity for
depth estimation and helps subsequent modules achieve real-time performance. We
further design a non-negative factorization-based encoder to extract mutually
consistent semantic features between point clouds and images. This encoder
yields more distinctive global descriptors for retrieval. Experimental results
on the KITTI dataset show that our proposed methods achieve state-of-the-art
performance while running in real time. Additional evaluation on the HAOMO
dataset covering a 17 km trajectory further shows the practical generalization
capabilities. We have released the implementation of our methods as open source
at: https://github.com/haomo-ai/ModaLink.git.
\\ ( https://arxiv.org/abs/2403.18762 ,  8921kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18775 (*cross-listing*)
Date: Wed, 27 Mar 2024 17:23:39 GMT   (8021kb,D)

Title: ImageNet-D: Benchmarking Neural Network Robustness on Diffusion
  Synthetic Object
Authors: Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, Chengzhi Mao
Categories: cs.CV cs.AI cs.LG
Comments: Accepted at CVPR 2024
\\
  We establish rigorous benchmarks for visual perception robustness. Synthetic
images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific
type of evaluation over synthetic corruptions, backgrounds, and textures, yet
those robustness benchmarks are restricted in specified variations and have low
synthetic quality. In this work, we introduce generative model as a data source
for synthesizing hard images that benchmark deep models' robustness. Leveraging
diffusion models, we are able to generate images with more diversified
backgrounds, textures, and materials than any prior work, where we term this
benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a
significant accuracy drop to a range of vision models, from the standard ResNet
visual classifier to the latest foundation models like CLIP and MiniGPT-4,
significantly reducing their accuracy by up to 60\%. Our work suggests that
diffusion models can be an effective source to test vision models. The code and
dataset are available at https://github.com/chenshuang-zhang/imagenet_d.
\\ ( https://arxiv.org/abs/2403.18775 ,  8021kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18795 (*cross-listing*)
Date: Wed, 27 Mar 2024 17:40:14 GMT   (4538kb,D)

Title: Gamba: Marry Gaussian Splatting with Mamba for single view 3D
  reconstruction
Authors: Qiuhong Shen, Xuanyu Yi, Zike Wu, Pan Zhou, Hanwang Zhang, Shuicheng
  Yan, Xinchao Wang
Categories: cs.CV cs.AI
\\
  We tackle the challenge of efficiently reconstructing a 3D asset from a
single image with growing demands for automated 3D content creation pipelines.
Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural
Radiance Fields (NeRF). Despite their significant success, these approaches
encounter practical limitations due to lengthy optimization and considerable
memory usage. In this report, we introduce Gamba, an end-to-end amortized 3D
reconstruction model from single-view images, emphasizing two main insights:
(1) 3D representation: leveraging a large number of 3D Gaussians for an
efficient 3D Gaussian splatting process; (2) Backbone design: introducing a
Mamba-based sequential network that facilitates context-dependent reasoning and
linear scalability with the sequence (token) length, accommodating a
substantial number of Gaussians. Gamba incorporates significant advancements in
data preprocessing, regularization design, and training methodologies. We
assessed Gamba against existing optimization-based and feed-forward 3D
generation approaches using the real-world scanned OmniObject3D dataset. Here,
Gamba demonstrates competitive generation capabilities, both qualitatively and
quantitatively, while achieving remarkable speed, approximately 0.6 second on a
single NVIDIA A100 GPU.
\\ ( https://arxiv.org/abs/2403.18795 ,  4538kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18807 (*cross-listing*)
Date: Wed, 27 Mar 2024 17:53:30 GMT   (8994kb,D)

Title: ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth
  Estimation
Authors: Suraj Patni, Aradhye Agarwal, Chetan Arora
Categories: cs.CV cs.AI cs.LG
Comments: Accepted at IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2024
\\
  In the absence of parallax cues, a learning-based single image depth
estimation (SIDE) model relies heavily on shading and contextual cues in the
image. While this simplicity is attractive, it is necessary to train such
models on large and varied datasets, which are difficult to capture. It has
been shown that using embeddings from pre-trained foundational models, such as
CLIP, improves zero shot transfer in several applications. Taking inspiration
from this, in our paper we explore the use of global image priors generated
from a pre-trained ViT model to provide more detailed contextual information.
We argue that the embedding vector from a ViT model, pre-trained on a large
dataset, captures greater relevant information for SIDE than the usual route of
generating pseudo image captions, followed by CLIP based text embeddings. Based
on this idea, we propose a new SIDE model using a diffusion backbone which is
conditioned on ViT embeddings. Our proposed design establishes a new
state-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of
0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on
KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to
0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model
trained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)
over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,
18%, 45%, 9%) by ZoeDepth. The code is available at
https://github.com/Aradhye2002/EcoDepth.
\\ ( https://arxiv.org/abs/2403.18807 ,  8994kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18814 (*cross-listing*)
Date: Wed, 27 Mar 2024 17:59:04 GMT   (8457kb,D)

Title: Mini-Gemini: Mining the Potential of Multi-modality Vision Language
  Models
Authors: Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen,
  Ruihang Chu, Shaoteng Liu, Jiaya Jia
Categories: cs.CV cs.AI cs.CL
Comments: Code and models are available at
  https://github.com/dvlab-research/MiniGemini
\\
  In this work, we introduce Mini-Gemini, a simple and effective framework
enhancing multi-modality Vision Language Models (VLMs). Despite the
advancements in VLMs facilitating basic visual dialog and reasoning, a
performance gap persists compared to advanced models like GPT-4 and Gemini. We
try to narrow the gap by mining the potential of VLMs for better performance
and any-to-any workflow from three aspects, i.e., high-resolution visual
tokens, high-quality data, and VLM-guided generation. To enhance visual tokens,
we propose to utilize an additional visual encoder for high-resolution
refinement without increasing the visual token count. We further construct a
high-quality dataset that promotes precise image comprehension and
reasoning-based generation, expanding the operational scope of current VLMs. In
general, Mini-Gemini further mines the potential of VLMs and empowers current
frameworks with image understanding, reasoning, and generation simultaneously.
Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs)
from 2B to 34B. It is demonstrated to achieve leading performance in several
zero-shot benchmarks and even surpasses the developed private models. Code and
models are available at https://github.com/dvlab-research/MiniGemini.
\\ ( https://arxiv.org/abs/2403.18814 ,  8457kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18260 (*cross-listing*)
Date: Wed, 27 Mar 2024 05:22:06 GMT   (7454kb,D)

Title: Toward Interactive Regional Understanding in Vision-Large Language
  Models
Authors: Jungbeom Lee, Sanghyuk Chun, Sangdoo Yun
Categories: cs.CV cs.CL
Comments: NAACL 2024 Main Conference
\\
  Recent Vision-Language Pre-training (VLP) models have demonstrated
significant advancements. Nevertheless, these models heavily rely on image-text
pairs that capture only coarse and global information of an image, leading to a
limitation in their regional understanding ability. In this work, we introduce
\textbf{RegionVLM}, equipped with explicit regional modeling capabilities,
allowing them to understand user-indicated image regions. To achieve this, we
design a simple yet innovative architecture, requiring no modifications to the
model architecture or objective function. Additionally, we leverage a dataset
that contains a novel source of information, namely Localized Narratives, which
has been overlooked in previous VLP research. Our experiments demonstrate that
our single generalist model not only achieves an interactive dialogue system
but also exhibits superior performance on various zero-shot region
understanding tasks, without compromising its ability for global image
understanding.
\\ ( https://arxiv.org/abs/2403.18260 ,  7454kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18276 (*cross-listing*)
Date: Wed, 27 Mar 2024 06:07:05 GMT   (349kb,D)

Title: RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era
  of Transformers
Authors: Zhichao Xu
Categories: cs.IR cs.CL
\\
  Transformer structure has achieved great success in multiple applied machine
learning communities, such as natural language processing (NLP), computer
vision (CV) and information retrieval (IR). Transformer architecture's core
mechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$
time complexity in inference. Many works have been proposed to improve the
attention mechanism's scalability, such as Flash Attention and Multi-query
Attention. A different line of work aims to design new mechanisms to replace
attention. Recently, a notable model structure -- Mamba, which is based on
state space models, has achieved transformer-equivalent performance in multiple
sequence modeling tasks.
  In this work, we examine \mamba's efficacy through the lens of a classical IR
task -- document ranking. A reranker model takes a query and a document as
input, and predicts a scalar relevance score. This task demands the language
model's ability to comprehend lengthy contextual inputs and to capture the
interaction between query and document tokens. We find that (1) Mamba models
achieve competitive performance compared to transformer-based models with the
same training recipe; (2) but also have a lower training throughput in
comparison to efficient transformer implementations such as flash attention. We
hope this study can serve as a starting point to explore Mamba models in other
classical IR tasks. Our code implementation and trained checkpoints are made
public to facilitate
reproducibility.\footnote{https://github.com/zhichaoxu-shufe/RankMamba}.
\\ ( https://arxiv.org/abs/2403.18276 ,  349kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18435 (*cross-listing*)
Date: Wed, 27 Mar 2024 10:40:14 GMT   (2805kb,D)

Title: DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via
  Structural Word Alignment
Authors: Haitao Li, Qingyao Ai, Xinyan Han, Jia Chen, Qian Dong, Yiqun Liu,
  Chong Chen, Qi Tian
Categories: cs.IR cs.CL
Comments: 11 pages
\\
  Recent research demonstrates the effectiveness of using pre-trained language
models for legal case retrieval. Most of the existing works focus on improving
the representation ability for the contextualized embedding of the [CLS] token
and calculate relevance using textual semantic similarity. However, in the
legal domain, textual semantic similarity does not always imply that the cases
are relevant enough. Instead, relevance in legal cases primarily depends on the
similarity of key facts that impact the final judgment. Without proper
treatments, the discriminative ability of learned representations could be
limited since legal cases are lengthy and contain numerous non-key facts. To
this end, we introduce DELTA, a discriminative model designed for legal case
retrieval. The basic idea involves pinpointing key facts in legal cases and
pulling the contextualized embedding of the [CLS] token closer to the key facts
while pushing away from the non-key facts, which can warm up the case embedding
space in an unsupervised manner. To be specific, this study brings the word
alignment mechanism to the contextual masked auto-encoder. First, we leverage
shallow decoders to create information bottlenecks, aiming to enhance the
representation ability. Second, we employ the deep decoder to enable
translation between different structures, with the goal of pinpointing key
facts to enhance discriminative ability. Comprehensive experiments conducted on
publicly available legal benchmarks show that our approach can outperform
existing state-of-the-art methods in legal case retrieval. It provides a new
perspective on the in-depth understanding and processing of legal case
documents.
\\ ( https://arxiv.org/abs/2403.18435 ,  2805kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18525 (*cross-listing*)
Date: Wed, 27 Mar 2024 12:59:44 GMT   (5208kb,D)

Title: Language Plays a Pivotal Role in the Object-Attribute Compositional
  Generalization of CLIP
Authors: Reza Abbasi, Mohammad Samiei, Mohammad Hossein Rohban, Mahdieh
  Soleymani Baghshah
Categories: cs.CV cs.CL cs.LG
Comments: Oral accepted at OODCV 2023(http://www.ood-cv.org)
\\
  Vision-language models, such as CLIP, have shown promising
Out-of-Distribution (OoD) generalization under various types of distribution
shifts. Recent studies attempted to investigate the leading cause of this
capability. In this work, we follow the same path, but focus on a specific type
of OoD data - images with novel compositions of attribute-object pairs - and
study whether such models can successfully classify those images into
composition classes. We carefully designed an authentic image test dataset
called ImageNet-AO, consisting of attributes for objects that are unlikely
encountered in the CLIP training sets. We found that CLIPs trained with large
datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude
improvement in effective compositional OoD generalization compared to both
supervised models and CLIPs trained with smaller datasets, such as CC-12M and
YFCC-15M. Our results provide evidence that the scale and diversity of training
data and language supervision play a key role in unlocking the compositional
generalization abilities of vision-language models.
\\ ( https://arxiv.org/abs/2403.18525 ,  5208kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18609 (*cross-listing*)
Date: Wed, 27 Mar 2024 14:26:41 GMT   (167kb)

Title: A survey on learning models of spiking neural membrane systems and
  spiking neural networks
Authors: Prithwineel Paul, Petr Sosik, Lucie Ciencialova
Categories: cs.NE cs.CL
\\
  Spiking neural networks (SNN) are a biologically inspired model of neural
networks with certain brain-like properties. In the past few decades, this
model has received increasing attention in computer science community, owing
also to the successful phenomenon of deep learning. In SNN, communication
between neurons takes place through the spikes and spike trains. This
differentiates these models from the ``standard'' artificial neural networks
(ANN) where the frequency of spikes is replaced by real-valued signals. Spiking
neural P systems (SNPS) can be considered a branch of SNN based more on the
principles of formal automata, with many variants developed within the
framework of the membrane computing theory. In this paper, we first briefly
compare structure and function, advantages and drawbacks of SNN and SNPS. A key
part of the article is a survey of recent results and applications of machine
learning and deep learning models of both SNN and SNPS formalisms.
\\ ( https://arxiv.org/abs/2403.18609 ,  167kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18624 (*cross-listing*)
Date: Wed, 27 Mar 2024 14:34:29 GMT   (661kb,D)

Title: Vulnerability Detection with Code Language Models: How Far Are We?
Authors: Yangruibo Ding, Yanjun Fu, Omniyyah Ibrahim, Chawin Sitawarin, Xinyun
  Chen, Basel Alomair, David Wagner, Baishakhi Ray, Yizheng Chen
Categories: cs.SE cs.CL
\\
  In the context of the rising interest in code language models (code LMs) and
vulnerability detection, we study the effectiveness of code LMs for detecting
vulnerabilities. Our analysis reveals significant shortcomings in existing
vulnerability datasets, including poor data quality, low label accuracy, and
high duplication rates, leading to unreliable model performance in realistic
vulnerability detection scenarios. Additionally, the evaluation methods used
with these datasets are not representative of real-world vulnerability
detection.
  To address these challenges, we introduce PrimeVul, a new dataset for
training and evaluating code LMs for vulnerability detection. PrimeVul
incorporates a novel set of data labeling techniques that achieve comparable
label accuracy to human-verified benchmarks while significantly expanding the
dataset. It also implements a rigorous data de-duplication and chronological
data splitting strategy to mitigate data leakage issues, alongside introducing
more realistic evaluation metrics and settings. This comprehensive approach
aims to provide a more accurate assessment of code LMs' performance in
real-world conditions.
  Evaluating code LMs on PrimeVul reveals that existing benchmarks
significantly overestimate the performance of these models. For instance, a
state-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on
PrimeVul. Attempts to improve performance through advanced training techniques
and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin
to random guessing in the most stringent settings. These findings underscore
the considerable gap between current capabilities and the practical
requirements for deploying code LMs in security roles, highlighting the need
for more innovative research in this domain.
\\ ( https://arxiv.org/abs/2403.18624 ,  661kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18667 (*cross-listing*)
Date: Wed, 27 Mar 2024 15:11:00 GMT   (1785kb,D)

Title: Improving Content Recommendation: Knowledge Graph-Based Semantic
  Contrastive Learning for Diversity and Cold-Start Users
Authors: Yejin Kim, Scott Rome, Kevin Foley, Mayur Nankani, Rimon Melamed,
  Javier Morales, Abhay Yadav, Maria Peifer, Sardar Hamidian and H. Howie Huang
Categories: cs.IR cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  Addressing the challenges related to data sparsity, cold-start problems, and
diversity in recommendation systems is both crucial and demanding. Many current
solutions leverage knowledge graphs to tackle these issues by combining both
item-based and user-item collaborative signals. A common trend in these
approaches focuses on improving ranking performance at the cost of escalating
model complexity, reducing diversity, and complicating the task. It is
essential to provide recommendations that are both personalized and diverse,
rather than solely relying on achieving high rank-based performance, such as
Click-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task
learning approach, training on user-item and item-item interactions. We apply
item-based contrastive learning on descriptive text, sampling positive and
negative pairs based on item metadata. Our approach allows the model to better
understand the relationships between entities within the knowledge graph by
utilizing semantic information from text. It leads to more accurate, relevant,
and diverse user recommendations and a benefit that extends even to cold-start
users who have few interactions with items. We perform extensive experiments on
two widely used datasets to validate the effectiveness of our approach. Our
findings demonstrate that jointly training user-item interactions and
item-based signals using synopsis text is highly effective. Furthermore, our
results provide evidence that item-based contrastive learning enhances the
quality of entity embeddings, as indicated by metrics such as uniformity and
alignment.
\\ ( https://arxiv.org/abs/2403.18667 ,  1785kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18684 (*cross-listing*)
Date: Wed, 27 Mar 2024 15:27:36 GMT   (658kb,D)

Title: Scaling Laws For Dense Retrieval
Authors: Yan Fang, Jingtao Zhan, Qingyao Ai, Jiaxin Mao, Weihang Su, Jia Chen,
  Yiqun Liu
Categories: cs.IR cs.CL
Comments: Accepted at SIGIR 2024
\\
  Scaling up neural models has yielded significant advancements in a wide array
of tasks, particularly in language generation. Previous studies have found that
the performance of neural models frequently adheres to predictable scaling
laws, correlated with factors such as training set size and model size. This
insight is invaluable, especially as large-scale experiments grow increasingly
resource-intensive. Yet, such scaling law has not been fully explored in dense
retrieval due to the discrete nature of retrieval metrics and complex
relationships between training data and model sizes in retrieval tasks. In this
study, we investigate whether the performance of dense retrieval models follows
the scaling law as other neural models. We propose to use contrastive
log-likelihood as the evaluation metric and conduct extensive experiments with
dense retrieval models implemented with different numbers of parameters and
trained with different amounts of annotated data. Results indicate that, under
our settings, the performance of dense retrieval models follows a precise
power-law scaling related to the model size and the number of annotations.
Additionally, we examine scaling with prevalent data augmentation methods to
assess the impact of annotation quality, and apply the scaling law to find the
best resource allocation strategy under a budget constraint. We believe that
these insights will significantly contribute to understanding the scaling
effect of dense retrieval models and offer meaningful guidance for future
research endeavors.
\\ ( https://arxiv.org/abs/2403.18684 ,  658kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18746 (*cross-listing*)
Date: Wed, 27 Mar 2024 16:45:02 GMT   (3738kb,D)

Title: CYCLE: Learning to Self-Refine the Code Generation
Authors: Yangruibo Ding, Marcus J. Min, Gail Kaiser, Baishakhi Ray
Categories: cs.SE cs.CL
Comments: Camera-ready for OOPSLA'24
\\
  Pre-trained code language models have achieved promising performance in code
generation and improved the programming efficiency of human developers.
However, their self-refinement capability is typically overlooked by the
existing evaluations of code LMs, which focus only on the accuracy of the
one-time prediction. For the cases when code LMs fail to implement the correct
program, developers actually find it hard to debug and fix the faulty
prediction since it is not written by the developers themselves. Unfortunately,
our study reveals that code LMs cannot efficiently self-refine their faulty
generations as well.
  In this paper, we propose CYCLE framework, learning to self-refine the faulty
generation according to the available feedback, such as the execution results
reported by the test suites. We evaluate CYCLE on three popular code generation
benchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE
successfully maintains, sometimes improves, the quality of one-time code
generation, while significantly improving the self-refinement capability of
code LMs. We implement four variants of CYCLE with varied numbers of parameters
across 350M, 1B, 2B, and 3B, and the experiments show that CYCLE consistently
boosts the code generation performance, by up to 63.5%, across benchmarks and
varied model sizes. We also notice that CYCLE outperforms code LMs that have
3$\times$ more parameters in self-refinement.
\\ ( https://arxiv.org/abs/2403.18746 ,  3738kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17980 (*cross-listing*)
Date: Sun, 24 Mar 2024 04:09:48 GMT   (3691kb,D)

Title: EG-ConMix: An Intrusion Detection Method based on Graph Contrastive
  Learning
Authors: Lijin Wu, Shanshan Lei, Feilong Liao, Yuanjun Zheng, Yuxin Liu, Wentao
  Fu, Hao Song, Jiajun Zhou
Categories: cs.CR cs.LG
\\
  As the number of IoT devices increases, security concerns become more
prominent. The impact of threats can be minimized by deploying Network
Intrusion Detection System (NIDS) by monitoring network traffic, detecting and
discovering intrusions, and issuing security alerts promptly. Most intrusion
detection research in recent years has been directed towards the pair of
traffic itself without considering the interrelationships among them, thus
limiting the monitoring of complex IoT network attack events. Besides,
anomalous traffic in real networks accounts for only a small fraction, which
leads to a severe imbalance problem in the dataset that makes algorithmic
learning and prediction extremely difficult. In this paper, we propose an
EG-ConMix method based on E-GraphSAGE, incorporating a data augmentation module
to fix the problem of data imbalance. In addition, we incorporate contrastive
learning to discern the difference between normal and malicious traffic
samples, facilitating the extraction of key features. Extensive experiments on
two publicly available datasets demonstrate the superior intrusion detection
performance of EG-ConMix compared to state-of-the-art methods. Remarkably, it
exhibits significant advantages in terms of training speed and accuracy for
large-scale graphs.
\\ ( https://arxiv.org/abs/2403.17980 ,  3691kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17982 (*cross-listing*)
Date: Sun, 24 Mar 2024 17:52:36 GMT   (704kb)

Title: Markov chain models for inspecting response dynamics in psychological
  testing
Authors: Andrea Bosco
Categories: stat.ME cs.LG math.PR
Comments: 20 pages, 1 figure, 3 tables, 25 equations/matrices. Part of this
  paper was presented to the XXIX AIP Congress, Experimental Psychology
  Section. September 18th-20th 2023, Lucca, Italy. Title of the talk:
  "Differentiating students with signs of ADHD or OCD based on hysteresis in
  responses to a mind-wandering test. A Study of Markov Chain Test Response
  Sequences"
ACM-class: G.3; J.4
\\
  The importance of considering contextual probabilities in shaping response
patterns within psychological testing is underscored, despite the ubiquitous
nature of order effects discussed extensively in methodological literature.
Drawing from concepts such as path-dependency, first-order autocorrelation,
state-dependency, and hysteresis, the present study is an attempt to address
how earlier responses serve as an anchor for subsequent answers in tests,
surveys, and questionnaires. Introducing the notion of non-commuting
observables derived from quantum physics, I highlight their role in
characterizing psychological processes and the impact of measurement
instruments on participants' responses. We advocate for the utilization of
first-order Markov chain modeling to capture and forecast sequential
dependencies in survey and test responses. The employment of the first-order
Markov chain model lies in individuals' propensity to exhibit partial focus to
preceding responses, with recent items most likely exerting a substantial
influence on subsequent response selection. This study contributes to advancing
our understanding of the dynamics inherent in sequential data within
psychological research and provides a methodological framework for conducting
longitudinal analyses of response patterns of test and questionnaire.
\\ ( https://arxiv.org/abs/2403.17982 ,  704kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17983 (*cross-listing*)
Date: Sun, 24 Mar 2024 21:41:29 GMT   (1051kb,D)

Title: Is Watermarking LLM-Generated Code Robust?
Authors: Tarun Suresh, Shubham Ugare, Gagandeep Singh, Sasa Misailovic
Categories: cs.CR cs.LG
\\
  We present the first study of the robustness of existing watermarking
techniques on Python code generated by large language models. Although existing
works showed that watermarking can be robust for natural language, we show that
it is easy to remove these watermarks on code by semantic-preserving
transformations.
\\ ( https://arxiv.org/abs/2403.17983 ,  1051kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17994 (*cross-listing*)
Date: Tue, 26 Mar 2024 13:50:39 GMT   (3032kb,D)

Title: Solution for Point Tracking Task of ICCV 1st Perception Test Challenge
  2023
Authors: Hongpeng Pan, Yang Yang, Zhongtian Fu, Yuxuan Zhang, Shian Du, Yi Xu,
  Xiangyang Ji
Categories: cs.CV cs.LG
\\
  This report proposes an improved method for the Tracking Any Point (TAP)
task, which tracks any physical surface through a video. Several existing
approaches have explored the TAP by considering the temporal relationships to
obtain smooth point motion trajectories, however, they still suffer from the
cumulative error caused by temporal prediction. To address this issue, we
propose a simple yet effective approach called TAP with confident static points
(TAPIR+), which focuses on rectifying the tracking of the static point in the
videos shot by a static camera. To clarify, our approach contains two key
components: (1) Multi-granularity Camera Motion Detection, which could identify
the video sequence by the static camera shot. (2) CMR-based point trajectory
prediction with one moving object segmentation approach to isolate the static
point from the moving object. Our approach ranked first in the final test with
a score of 0.46.
\\ ( https://arxiv.org/abs/2403.17994 ,  3032kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18026 (*cross-listing*)
Date: Tue, 26 Mar 2024 18:23:31 GMT   (1548kb)

Title: Cross-system biological image quality enhancement based on the
  generative adversarial network as a foundation for establishing a
  multi-institute microscopy cooperative network
Authors: Dominik Panek, Carina Rz\k{a}ca, Maksymilian Szczypior, Joanna Sorysz,
  Krzysztof Misztal, Zbigniew Baster, Zenon Rajfur
Categories: eess.IV cs.LG q-bio.QM
Comments: 15 Pages, 5 Figures, 1 Table, 3 pages Supplementary Materials
\\
  High-quality fluorescence imaging of biological systems is limited by
processes like photobleaching and phototoxicity, and also in many cases, by
limited access to the latest generations of microscopes. Moreover, low temporal
resolution can lead to a motion blur effect in living systems. Our work
presents a deep learning (DL) generative-adversarial approach to the problem of
obtaining high-quality (HQ) images based on their low-quality (LQ) equivalents.
We propose a generative-adversarial network (GAN) for contrast transfer between
two different separate microscopy systems: a confocal microscope (producing HQ
images) and a wide-field fluorescence microscope (producing LQ images). Our
model proves that such transfer is possible, allowing us to receive
HQ-generated images characterized by low mean squared error (MSE) values, high
structural similarity index (SSIM), and high peak signal-to-noise ratio (PSNR)
values. For our best model in the case of comparing HQ-generated images and
HQ-ground truth images, the median values of the metrics are 6x10-4, 0.9413,
and 31.87, for MSE, SSIM, and PSNR, respectively. In contrast, in the case of
comparison between LQ and HQ ground truth median values of the metrics are
equal to 0.0071, 0.8304, and 21.48 for MSE, SSIM, and PSNR respectively.
Therefore, we observe a significant increase ranging from 14% to 49% for SSIM
and PSNR respectively. These results, together with other single-system
cross-modality studies, provide proof of concept for further implementation of
a cross-system biological image quality enhancement.
\\ ( https://arxiv.org/abs/2403.18026 ,  1548kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18044 (*cross-listing*)
Date: Tue, 26 Mar 2024 18:57:56 GMT   (776kb,D)

Title: Deep polytopic autoencoders for low-dimensional linear parameter-varying
  approximations and nonlinear feedback design
Authors: Jan Heiland, Yongho Kim, Steffen W. R. Werner
Categories: math.OC cs.LG cs.NA math.DS math.NA physics.flu-dyn
Comments: 9 pages, 6 figures, 2 tables
\\
  Polytopic autoencoders provide low-dimensional parametrizations of states in
a polytope. For nonlinear PDEs, this is readily applied to low-dimensional
linear parameter-varying (LPV) approximations as they have been exploited for
efficient nonlinear controller design via series expansions of the solution to
the state-dependent Riccati equation. In this work, we develop a polytopic
autoencoder for control applications and show how it outperforms standard
linear approaches in view of LPV approximations of nonlinear systems and how
the particular architecture enables higher order series expansions at little
extra computational effort. We illustrate the properties and potentials of this
approach to computational nonlinear controller design for large-scale systems
with a thorough numerical study.
\\ ( https://arxiv.org/abs/2403.18044 ,  776kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18052 (*cross-listing*)
Date: Tue, 26 Mar 2024 19:10:08 GMT   (5630kb,D)

Title: R2D2 image reconstruction with model uncertainty quantification in radio
  astronomy
Authors: Amir Aghabiglou, Chung San Chu, Arwa Dabbech, Yves Wiaux
Categories: astro-ph.IM cs.LG eess.IV eess.SP
Comments: submitted to IEEE EUSIPCO 2024
\\
  The ``Residual-to-Residual DNN series for high-Dynamic range imaging'' (R2D2)
approach was recently introduced for Radio-Interferometric (RI) imaging in
astronomy. R2D2's reconstruction is formed as a series of residual images,
iteratively estimated as outputs of Deep Neural Networks (DNNs) taking the
previous iteration's image estimate and associated data residual as inputs. In
this work, we investigate the robustness of the R2D2 image estimation process,
by studying the uncertainty associated with its series of learned models.
Adopting an ensemble averaging approach, multiple series can be trained,
arising from different random DNN initializations of the training process at
each iteration. The resulting multiple R2D2 instances can also be leveraged to
generate ``R2D2 samples'', from which empirical mean and standard deviation
endow the algorithm with a joint estimation and uncertainty quantification
functionality. Focusing on RI imaging, and adopting a telescope-specific
approach, multiple R2D2 instances were trained to encompass the most general
observation setting of the Very Large Array (VLA). Simulations and real-data
experiments confirm that: (i) R2D2's image estimation capability is superior to
that of the state-of-the-art algorithms; (ii) its ultra-fast reconstruction
capability (arising from series with only few DNNs) makes the computation of
multiple reconstruction samples and of uncertainty maps practical even at large
image dimension; (iii) it is characterized by a very low model uncertainty.
\\ ( https://arxiv.org/abs/2403.18052 ,  5630kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18072 (*cross-listing*)
Date: Tue, 26 Mar 2024 19:49:58 GMT   (859kb,D)

Title: Goal-Oriented Bayesian Optimal Experimental Design for Nonlinear Models
  using Markov Chain Monte Carlo
Authors: Shijie Zhong, Wanggang Shen, Tommie Catanach, Xun Huan
Categories: stat.CO cs.LG stat.ME stat.ML
MSC-class: 62K05, 62F15, 62B15
\\
  Optimal experimental design (OED) provides a systematic approach to quantify
and maximize the value of experimental data. Under a Bayesian approach,
conventional OED maximizes the expected information gain (EIG) on model
parameters. However, we are often interested in not the parameters themselves,
but predictive quantities of interest (QoIs) that depend on the parameters in a
nonlinear manner. We present a computational framework of predictive
goal-oriented OED (GO-OED) suitable for nonlinear observation and prediction
models, which seeks the experimental design providing the greatest EIG on the
QoIs. In particular, we propose a nested Monte Carlo estimator for the QoI EIG,
featuring Markov chain Monte Carlo for posterior sampling and kernel density
estimation for evaluating the posterior-predictive density and its
Kullback-Leibler divergence from the prior-predictive. The GO-OED design is
then found by maximizing the EIG over the design space using Bayesian
optimization. We demonstrate the effectiveness of the overall nonlinear GO-OED
method, and illustrate its differences versus conventional non-GO-OED, through
various test problems and an application of sensor placement for source
inversion in a convection-diffusion field.
\\ ( https://arxiv.org/abs/2403.18072 ,  859kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18094 (*cross-listing*)
Date: Tue, 26 Mar 2024 20:30:55 GMT   (14855kb,D)

Title: A Personalized Video-Based Hand Taxonomy: Application for Individuals
  with Spinal Cord Injury
Authors: Mehdy Dousty, David J. Fleet, Jos\'e Zariffa
Categories: cs.CV cs.LG
\\
  Hand function is critical for our interactions and quality of life. Spinal
cord injuries (SCI) can impair hand function, reducing independence. A
comprehensive evaluation of function in home and community settings requires a
hand grasp taxonomy for individuals with impaired hand function. Developing
such a taxonomy is challenging due to unrepresented grasp types in standard
taxonomies, uneven data distribution across injury levels, and limited data.
This study aims to automatically identify the dominant distinct hand grasps in
egocentric video using semantic clustering. Egocentric video recordings
collected in the homes of 19 individual with cervical SCI were used to cluster
grasping actions with semantic significance. A deep learning model integrating
posture and appearance data was employed to create a personalized hand
taxonomy. Quantitative analysis reveals a cluster purity of 67.6% +- 24.2% with
with 18.0% +- 21.8% redundancy. Qualitative assessment revealed meaningful
clusters in video content. This methodology provides a flexible and effective
strategy to analyze hand function in the wild. It offers researchers and
clinicians an efficient tool for evaluating hand function, aiding sensitive
assessments and tailored intervention plans.
\\ ( https://arxiv.org/abs/2403.18094 ,  14855kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18104 (*cross-listing*)
Date: Tue, 26 Mar 2024 21:04:18 GMT   (10843kb,D)

Title: Mathematical Foundation and Corrections for Full Range Head Pose
  Estimation
Authors: Huei-Chung Hu, Xuyang Wu, Yuan Wang, Yi Fang, Hsin-Tai Wu
Categories: cs.CV cs.LG
\\
  Numerous works concerning head pose estimation (HPE) offer algorithms or
proposed neural network-based approaches for extracting Euler angles from
either facial key points or directly from images of the head region. However,
many works failed to provide clear definitions of the coordinate systems and
Euler or Tait-Bryan angles orders in use. It is a well-known fact that rotation
matrices depend on coordinate systems, and yaw, roll, and pitch angles are
sensitive to their application order. Without precise definitions, it becomes
challenging to validate the correctness of the output head pose and drawing
routines employed in prior works. In this paper, we thoroughly examined the
Euler angles defined in the 300W-LP dataset, head pose estimation such as
3DDFA-v2, 6D-RepNet, WHENet, etc, and the validity of their drawing routines of
the Euler angles. When necessary, we infer their coordinate system and sequence
of yaw, roll, pitch from provided code. This paper presents (1) code and
algorithms for inferring coordinate system from provided source code, code for
Euler angle application order and extracting precise rotation matrices and the
Euler angles, (2) code and algorithms for converting poses from one rotation
system to another, (3) novel formulae for 2D augmentations of the rotation
matrices, and (4) derivations and code for the correct drawing routines for
rotation matrices and poses. This paper also addresses the feasibility of
defining rotations with right-handed coordinate system in Wikipedia and SciPy,
which makes the Euler angle extraction much easier for full-range head pose
research.
\\ ( https://arxiv.org/abs/2403.18104 ,  10843kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18211 (*cross-listing*)
Date: Wed, 27 Mar 2024 02:42:52 GMT   (16311kb,D)

Title: NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual
  Pretraining and Multi-level Modulation
Authors: Jingyang Huo, Yikai Wang, Xuelin Qian, Yun Wang, Chong Li, Jianfeng
  Feng, Yanwei Fu
Categories: cs.CV cs.LG
\\
  Recent fMRI-to-image approaches mainly focused on associating fMRI signals
with specific conditions of pre-trained diffusion models. These approaches,
while producing high-quality images, capture only a limited aspect of the
complex information in fMRI signals and offer little detailed control over
image creation. In contrast, this paper proposes to directly modulate the
generation process of diffusion models using fMRI signals. Our approach,
NeuroPictor, divides the fMRI-to-image process into three steps: i) fMRI
calibrated-encoding, to tackle multi-individual pre-training for a shared
latent space to minimize individual difference and enable the subsequent
cross-subject training; ii) fMRI-to-image cross-subject pre-training,
perceptually learning to guide diffusion model with high- and low-level
conditions across different individuals; iii) fMRI-to-image single-subject
refining, similar with step ii but focus on adapting to particular individual.
NeuroPictor extracts high-level semantic features from fMRI signals that
characterizing the visual stimulus and incrementally fine-tunes the diffusion
model with a low-level manipulation network to provide precise structural
instructions. By training with over 60,000 fMRI-image pairs from various
individuals, our model enjoys superior fMRI-to-image decoding capacity,
particularly in the within-subject setting, as evidenced in benchmark datasets.
Project page: https://jingyanghuo.github.io/neuropictor/.
\\ ( https://arxiv.org/abs/2403.18211 ,  16311kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18216 (*cross-listing*)
Date: Wed, 27 Mar 2024 02:59:04 GMT   (970kb,D)

Title: Minimax Optimal Fair Classification with Bounded Demographic Disparity
Authors: Xianli Zeng, Guang Cheng and Edgar Dobriban
Categories: stat.ML cs.CY cs.LG math.ST stat.TH
\\
  Mitigating the disparate impact of statistical machine learning methods is
crucial for ensuring fairness. While extensive research aims to reduce
disparity, the effect of using a \emph{finite dataset} -- as opposed to the
entire population -- remains unclear. This paper explores the statistical
foundations of fair binary classification with two protected groups, focusing
on controlling demographic disparity, defined as the difference in acceptance
rates between the groups. Although fairness may come at the cost of accuracy
even with infinite data, we show that using a finite sample incurs additional
costs due to the need to estimate group-specific acceptance thresholds. We
study the minimax optimal classification error while constraining demographic
disparity to a user-specified threshold. To quantify the impact of fairness
constraints, we introduce a novel measure called \emph{fairness-aware excess
risk} and derive a minimax lower bound on this measure that all classifiers
must satisfy. Furthermore, we propose FairBayes-DDP+, a group-wise thresholding
method with an offset that we show attains the minimax lower bound. Our lower
bound proofs involve several innovations. Experiments support that
FairBayes-DDP+ controls disparity at the user-specified level, while being
faster and having a more favorable fairness-accuracy tradeoff than several
baselines.
\\ ( https://arxiv.org/abs/2403.18216 ,  970kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18222 (*cross-listing*)
Date: Wed, 27 Mar 2024 03:19:36 GMT   (4794kb,D)

Title: Uncertainty-Aware Deployment of Pre-trained Language-Conditioned
  Imitation Learning Policies
Authors: Bo Wu, Bruce D. Lee, Kostas Daniilidis, Bernadette Bucher, Nikolai
  Matni
Categories: cs.RO cs.LG
Comments: 8 pages, 7 figures
\\
  Large-scale robotic policies trained on data from diverse tasks and robotic
platforms hold great promise for enabling general-purpose robots; however,
reliable generalization to new environment conditions remains a major
challenge. Toward addressing this challenge, we propose a novel approach for
uncertainty-aware deployment of pre-trained language-conditioned imitation
learning agents. Specifically, we use temperature scaling to calibrate these
models and exploit the calibrated model to make uncertainty-aware decisions by
aggregating the local information of candidate actions. We implement our
approach in simulation using three such pre-trained models, and showcase its
potential to significantly enhance task completion rates. The accompanying code
is accessible at the link:
https://github.com/BobWu1998/uncertainty_quant_all.git
\\ ( https://arxiv.org/abs/2403.18222 ,  4794kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18228 (*cross-listing*)
Date: Wed, 27 Mar 2024 03:31:16 GMT   (5736kb,D)

Title: Fourier or Wavelet bases as counterpart self-attention in spikformer for
  efficient visual classification
Authors: Qingyu Wang, Duzhen Zhang, Tilelin Zhang, Bo Xu
Categories: cs.CV cs.LG cs.NE
Comments: 18 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:2308.02557
\\
  Energy-efficient spikformer has been proposed by integrating the biologically
plausible spiking neural network (SNN) and artificial Transformer, whereby the
Spiking Self-Attention (SSA) is used to achieve both higher accuracy and lower
computational cost. However, it seems that self-attention is not always
necessary, especially in sparse spike-form calculation manners. In this paper,
we innovatively replace vanilla SSA (using dynamic bases calculating from Query
and Key) with spike-form Fourier Transform, Wavelet Transform, and their
combinations (using fixed triangular or wavelets bases), based on a key
hypothesis that both of them use a set of basis functions for information
transformation. Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is
proposed and verified in visual classification tasks, including both static
image and event-based video datasets. The FWformer can achieve comparable or
even higher accuracies ($0.4\%$-$1.5\%$), higher running speed ($9\%$-$51\%$
for training and $19\%$-$70\%$ for inference), reduced theoretical energy
consumption ($20\%$-$25\%$), and reduced GPU memory usage ($4\%$-$26\%$),
compared to the standard spikformer. Our result indicates the continuous
refinement of new Transformers, that are inspired either by biological
discovery (spike-form), or information theory (Fourier or Wavelet Transform),
is promising.
\\ ( https://arxiv.org/abs/2403.18228 ,  5736kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18233 (*cross-listing*)
Date: Wed, 27 Mar 2024 03:39:57 GMT   (554kb,D)

Title: Benchmarking Image Transformers for Prostate Cancer Detection from
  Ultrasound Data
Authors: Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar, Amoon Jamzad,
  Mahdi Gilany, Minh Nguyen Nhat To, Brian Wodlinger, Purang Abolmaesumi,
  Parvin Mousavi
Categories: eess.IV cs.CV cs.LG q-bio.TO
Comments: early draft, 7 pages; Accepted to SPIE Medical Imaging 2024
\\
  PURPOSE: Deep learning methods for classifying prostate cancer (PCa) in
ultrasound images typically employ convolutional networks (CNNs) to detect
cancer in small regions of interest (ROI) along a needle trace region. However,
this approach suffers from weak labelling, since the ground-truth
histopathology labels do not describe the properties of individual ROIs.
Recently, multi-scale approaches have sought to mitigate this issue by
combining the context awareness of transformers with a CNN feature extractor to
detect cancer from multiple ROIs using multiple-instance learning (MIL). In
this work, we present a detailed study of several image transformer
architectures for both ROI-scale and multi-scale classification, and a
comparison of the performance of CNNs and transformers for ultrasound-based
prostate cancer classification. We also design a novel multi-objective learning
strategy that combines both ROI and core predictions to further mitigate label
noise. METHODS: We evaluate 3 image transformers on ROI-scale cancer
classification, then use the strongest model to tune a multi-scale classifier
with MIL. We train our MIL models using our novel multi-objective learning
strategy and compare our results to existing baselines. RESULTS: We find that
for both ROI-scale and multi-scale PCa detection, image transformer backbones
lag behind their CNN counterparts. This deficit in performance is even more
noticeable for larger models. When using multi-objective learning, we can
improve performance of MIL, with a 77.9% AUROC, a sensitivity of 75.9%, and a
specificity of 66.3%. CONCLUSION: Convolutional networks are better suited for
modelling sparse datasets of prostate ultrasounds, producing more robust
features than transformers in PCa detection. Multi-scale methods remain the
best architecture for this task, with multi-objective learning presenting an
effective way to improve performance.
\\ ( https://arxiv.org/abs/2403.18233 ,  554kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18269 (*cross-listing*)
Date: Wed, 27 Mar 2024 05:50:23 GMT   (295kb,D)

Title: Clustering Change Sign Detection by Fusing Mixture Complexity
Authors: Kento Urano, Ryo Yuki and Kenji Yamanishi
Categories: stat.ML cs.IT cs.LG math.IT
Comments: 23 pages
\\
  This paper proposes an early detection method for cluster structural changes.
Cluster structure refers to discrete structural characteristics, such as the
number of clusters, when data are represented using finite mixture models, such
as Gaussian mixture models. We focused on scenarios in which the cluster
structure gradually changed over time. For finite mixture models, the concept
of mixture complexity (MC) measures the continuous cluster size by considering
the cluster proportion bias and overlap between clusters. In this paper, we
propose MC fusion as an extension of MC to handle situations in which multiple
mixture numbers are possible in a finite mixture model. By incorporating the
fusion of multiple models, our approach accurately captured the cluster
structure during transitional periods of gradual change. Moreover, we introduce
a method for detecting changes in the cluster structure by examining the
transition of MC fusion. We demonstrate the effectiveness of our method through
empirical analysis using both artificial and real-world datasets.
\\ ( https://arxiv.org/abs/2403.18269 ,  295kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18302 (*cross-listing*)
Date: Wed, 27 Mar 2024 06:58:01 GMT   (3727kb,D)

Title: Super-Resolution of SOHO/MDI Magnetograms of Solar Active Regions Using
  SDO/HMI Data and an Attention-Aided Convolutional Neural Network
Authors: Chunhui Xu, Jason T. L. Wang, Haimin Wang, Haodi Jiang, Qin Li, Yasser
  Abduallah, Yan Xu
Categories: astro-ph.SR cs.LG
Comments: 17 pages, 7 figures
\\
  Image super-resolution has been an important subject in image processing and
recognition. Here, we present an attention-aided convolutional neural network
(CNN) for solar image super-resolution. Our method, named SolarCNN, aims to
enhance the quality of line-of-sight (LOS) magnetograms of solar active regions
(ARs) collected by the Michelson Doppler Imager (MDI) on board the Solar and
Heliospheric Observatory (SOHO). The ground-truth labels used for training
SolarCNN are the LOS magnetograms collected by the Helioseismic and Magnetic
Imager (HMI) on board the Solar Dynamics Observatory (SDO). Solar ARs consist
of strong magnetic fields in which magnetic energy can suddenly be released to
produce extreme space weather events, such as solar flares, coronal mass
ejections, and solar energetic particles. SOHO/MDI covers Solar Cycle 23, which
is stronger with more eruptive events than Cycle 24. Enhanced SOHO/MDI
magnetograms allow for better understanding and forecasting of violent events
of space weather. Experimental results show that SolarCNN improves the quality
of SOHO/MDI magnetograms in terms of the structural similarity index measure
(SSIM), Pearson's correlation coefficient (PCC), and the peak signal-to-noise
ratio (PSNR).
\\ ( https://arxiv.org/abs/2403.18302 ,  3727kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18326 (*cross-listing*)
Date: Wed, 27 Mar 2024 08:07:07 GMT   (162kb)

Title: Privacy-Preserving Distributed Nonnegative Matrix Factorization
Authors: Ehsan Lari, Reza Arablouei, Stefan Werner
Categories: cs.CR cs.DC cs.LG eess.SP
Comments: 5 pages, 1 figure, submitted to EUSIPCO 2024 conference
\\
  Nonnegative matrix factorization (NMF) is an effective data representation
tool with numerous applications in signal processing and machine learning.
However, deploying NMF in a decentralized manner over ad-hoc networks
introduces privacy concerns due to the conventional approach of sharing raw
data among network agents. To address this, we propose a privacy-preserving
algorithm for fully-distributed NMF that decomposes a distributed large data
matrix into left and right matrix factors while safeguarding each agent's local
data privacy. It facilitates collaborative estimation of the left matrix factor
among agents and enables them to estimate their respective right factors
without exposing raw data. To ensure data privacy, we secure information
exchanges between neighboring agents utilizing the Paillier cryptosystem, a
probabilistic asymmetric algorithm for public-key cryptography that allows
computations on encrypted data without decryption. Simulation results conducted
on synthetic and real-world datasets demonstrate the effectiveness of the
proposed algorithm in achieving privacy-preserving distributed NMF over ad-hoc
networks.
\\ ( https://arxiv.org/abs/2403.18326 ,  162kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18330 (*cross-listing*)
Date: Wed, 27 Mar 2024 08:11:25 GMT   (3093kb,D)

Title: Tracking-Assisted Object Detection with Event Cameras
Authors: Ting-Kang Yen, Igor Morawski, Shusil Dangi, Kai He, Chung-Yi Lin,
  Jia-Fong Yeh, Hung-Ting Su, Winston Hsu
Categories: cs.CV cs.LG
\\
  Event-based object detection has recently garnered attention in the computer
vision community due to the exceptional properties of event cameras, such as
high dynamic range and no motion blur. However, feature asynchronism and
sparsity cause invisible objects due to no relative motion to the camera,
posing a significant challenge in the task. Prior works have studied various
memory mechanisms to preserve as many features as possible at the current time,
guided by temporal clues. While these implicit-learned memories retain some
short-term information, they still struggle to preserve long-term features
effectively. In this paper, we consider those invisible objects as
pseudo-occluded objects and aim to reveal their features. Firstly, we introduce
visibility attribute of objects and contribute an auto-labeling algorithm to
append additional visibility labels on an existing event camera dataset.
Secondly, we exploit tracking strategies for pseudo-occluded objects to
maintain their permanence and retain their bounding boxes, even when features
have not been available for a very long time. These strategies can be treated
as an explicit-learned memory guided by the tracking objective to record the
displacements of objects across frames. Lastly, we propose a spatio-temporal
feature aggregation module to enrich the latent features and a consistency loss
to increase the robustness of the overall pipeline. We conduct comprehensive
experiments to verify our method's effectiveness where still objects are
retained but real occluded objects are discarded. The results demonstrate that
(1) the additional visibility labels can assist in supervised training, and (2)
our method outperforms state-of-the-art approaches with a significant
improvement of 7.9% absolute mAP.
\\ ( https://arxiv.org/abs/2403.18330 ,  3093kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18355 (*cross-listing*)
Date: Wed, 27 Mar 2024 08:48:16 GMT   (7189kb,D)

Title: Supervised Multiple Kernel Learning approaches for multi-omics data
  integration
Authors: Mitja Briscik (IMT), Gabriele Tazza, Marie-Agnes Dillies, L\'aszl\'o
  Vid\'acs, S\'ebastien Dejean (IMT)
Categories: stat.ML cs.LG stat.AP
\\
  Advances in high-throughput technologies have originated an ever-increasing
availability of omics datasets. The integration of multiple heterogeneous data
sources is currently an issue for biology and bioinformatics. Multiple kernel
learning (MKL) has shown to be a flexible and valid approach to consider the
diverse nature of multi-omics inputs, despite being an underused tool in
genomic data mining.We provide novel MKL approaches based on different kernel
fusion strategies.To learn from the meta-kernel of input kernels, we
adaptedunsupervised integration algorithms for supervised tasks with support
vector machines.We also tested deep learning architectures for kernel fusion
and classification.The results show that MKL-based models can compete with more
complex, state-of-the-art, supervised multi-omics integrative approaches.
Multiple kernel learning offers a natural framework for predictive models in
multi-omics genomic data. Our results offer a direction for bio-data mining
research and further development of methods for heterogeneous data integration.
\\ ( https://arxiv.org/abs/2403.18355 ,  7189kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18370 (*cross-listing*)
Date: Wed, 27 Mar 2024 09:06:36 GMT   (31297kb,D)

Title: Ship in Sight: Diffusion Models for Ship-Image Super Resolution
Authors: Luigi Sigillo, Riccardo Fosco Gramaccioni, Alessandro Nicolosi, Danilo
  Comminiello
Categories: cs.CV cs.LG
Comments: Accepted at 2024 International Joint Conference on Neural Networks
  (IJCNN)
\\
  In recent years, remarkable advancements have been achieved in the field of
image generation, primarily driven by the escalating demand for high-quality
outcomes across various image generation subtasks, such as inpainting,
denoising, and super resolution. A major effort is devoted to exploring the
application of super-resolution techniques to enhance the quality of
low-resolution images. In this context, our method explores in depth the
problem of ship image super resolution, which is crucial for coastal and port
surveillance. We investigate the opportunity given by the growing interest in
text-to-image diffusion models, taking advantage of the prior knowledge that
such foundation models have already learned. In particular, we present a
diffusion-model-based architecture that leverages text conditioning during
training while being class-aware, to best preserve the crucial details of the
ships during the generation of the super-resoluted image. Since the specificity
of this task and the scarcity availability of off-the-shelf data, we also
introduce a large labeled ship dataset scraped from online ship images, mostly
from ShipSpotting\footnote{\url{www.shipspotting.com}} website. Our method
achieves more robust results than other deep learning models previously
employed for super resolution, as proven by the multiple experiments performed.
Moreover, we investigate how this model can benefit downstream tasks, such as
classification and object detection, thus emphasizing practical implementation
in a real-world scenario. Experimental results show flexibility, reliability,
and impressive performance of the proposed framework over state-of-the-art
methods for different tasks. The code is available at:
https://github.com/LuigiSigillo/ShipinSight .
\\ ( https://arxiv.org/abs/2403.18370 ,  31297kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18452 (*cross-listing*)
Date: Wed, 27 Mar 2024 11:11:08 GMT   (2197kb,D)

Title: SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model
Authors: Inhwan Bae and Young-Jae Park and Hae-Gon Jeon
Categories: cs.CV cs.LG cs.RO
Comments: Accepted at CVPR 2024
\\
  There are five types of trajectory prediction tasks: deterministic,
stochastic, domain adaptation, momentary observation, and few-shot. These
associated tasks are defined by various factors, such as the length of input
paths, data split and pre-processing methods. Interestingly, even though they
commonly take sequential coordinates of observations as input and infer future
paths in the same coordinates as output, designing specialized architectures
for each task is still necessary. For the other task, generality issues can
lead to sub-optimal performances. In this paper, we propose SingularTrajectory,
a diffusion-based universal trajectory prediction framework to reduce the
performance gap across the five tasks. The core of SingularTrajectory is to
unify a variety of human dynamics representations on the associated tasks. To
do this, we first build a Singular space to project all types of motion
patterns from each task into one embedding space. We next propose an adaptive
anchor working in the Singular space. Unlike traditional fixed anchor methods
that sometimes yield unacceptable paths, our adaptive anchor enables correct
anchors, which are put into a wrong location, based on a traversability map.
Finally, we adopt a diffusion-based predictor to further enhance the prototype
paths using a cascaded denoising process. Our unified framework ensures the
generality across various benchmark settings such as input modality, and
trajectory lengths. Extensive experiments on five public benchmarks demonstrate
that SingularTrajectory substantially outperforms existing models, highlighting
its effectiveness in estimating general dynamics of human movements. Code is
publicly available at https://github.com/inhwanbae/SingularTrajectory .
\\ ( https://arxiv.org/abs/2403.18452 ,  2197kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18495 (*cross-listing*)
Date: Wed, 27 Mar 2024 12:15:22 GMT   (21926kb,D)

Title: Direct mineral content prediction from drill core images via transfer
  learning
Authors: Romana Boiger and Sergey V. Churakov and Ignacio Ballester Llagaria
  and Georg Kosakowski and Raphael W\"ust and Nikolaos I. Prasianakis
Categories: cs.CV cs.LG eess.IV
\\
  Deep subsurface exploration is important for mining, oil and gas industries,
as well as in the assessment of geological units for the disposal of chemical
or nuclear waste, or the viability of geothermal energy systems. Typically,
detailed examinations of subsurface formations or units are performed on
cuttings or core materials extracted during drilling campaigns, as well as on
geophysical borehole data, which provide detailed information about the
petrophysical properties of the rocks. Depending on the volume of rock samples
and the analytical program, the laboratory analysis and diagnostics can be very
time-consuming. This study investigates the potential of utilizing machine
learning, specifically convolutional neural networks (CNN), to assess the
lithology and mineral content solely from analysis of drill core images, aiming
to support and expedite the subsurface geological exploration. The paper
outlines a comprehensive methodology, encompassing data preprocessing, machine
learning methods, and transfer learning techniques. The outcome reveals a
remarkable 96.7% accuracy in the classification of drill core segments into
distinct formation classes. Furthermore, a CNN model was trained for the
evaluation of mineral content using a learning data set from multidimensional
log analysis data (silicate, total clay, carbonate). When benchmarked against
laboratory XRD measurements on samples from the cores, both the advanced
multidimensional log analysis model and the neural network approach developed
here provide equally good performance. This work demonstrates that deep
learning and particularly transfer learning can support extracting
petrophysical properties, including mineral content and formation
classification, from drill core images, thus offering a road map for enhancing
model performance and data set quality in image-based analysis of drill cores.
\\ ( https://arxiv.org/abs/2403.18495 ,  21926kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18509 (*cross-listing*)
Date: Wed, 27 Mar 2024 12:39:16 GMT   (360kb)

Title: Distributed Maximum Consensus over Noisy Links
Authors: Ehsan Lari, Reza Arablouei, Naveen K. D. Venkategowda, Stefan Werner
Categories: cs.DC cs.LG eess.SP
Comments: 5 pages, 7 figures, submitted to EUSIPCO 2024 conference
\\
  We introduce a distributed algorithm, termed noise-robust distributed maximum
consensus (RD-MC), for estimating the maximum value within a multi-agent
network in the presence of noisy communication links. Our approach entails
redefining the maximum consensus problem as a distributed optimization problem,
allowing a solution using the alternating direction method of multipliers.
Unlike existing algorithms that rely on multiple sets of noise-corrupted
estimates, RD-MC employs a single set, enhancing both robustness and
efficiency. To further mitigate the effects of link noise and improve
robustness, we apply moving averaging to the local estimates. Through extensive
simulations, we demonstrate that RD-MC is significantly more robust to
communication link noise compared to existing maximum-consensus algorithms.
\\ ( https://arxiv.org/abs/2403.18509 ,  360kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18514 (*cross-listing*)
Date: Wed, 27 Mar 2024 12:44:57 GMT   (1316kb,D)

Title: CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection
  of Pathological Pulmonary CT scans
Authors: Aissam Djahnine, Alexandre Popoff, Emilien Jupin-Delevaux, Vincent
  Cottin, Olivier Nempont, Loic Boussel
Categories: eess.IV cs.CV cs.LG
\\
  Unsupervised pathology detection can be implemented by training a model on
healthy data only and measuring the deviation from the training set upon
inference, for example with CNN-based feature extraction and one-class
classifiers, or reconstruction-score-based methods such as AEs, GANs and
Diffusion models. Normalizing Flows (NF) have the ability to directly learn the
probability distribution of training examples through an invertible
architecture. We leverage this property in a novel 3D NF-based model named
CT-3DFlow, specifically tailored for patient-level pulmonary pathology
detection in chest CT data. Our model is trained unsupervised on healthy 3D
pulmonary CT patches, and detects deviations from its log-likelihood
distribution as anomalies. We aggregate patches-level likelihood values from a
patient's CT scan to provide a patient-level 'normal'/'abnormal' prediction.
Out-of-distribution detection performance is evaluated using expert annotations
on a separate chest CT test dataset, outperforming other state-of-the-art
methods.
\\ ( https://arxiv.org/abs/2403.18514 ,  1316kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18535 (*cross-listing*)
Date: Wed, 27 Mar 2024 13:11:34 GMT   (1211kb,D)

Title: Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs
Authors: Yichi Zhang, Zhihao Duan, Yuning Huang, Fengqing Zhu
Categories: eess.IV cs.LG
Comments: 2024 IEEE International Conference on Multimedia and Expo (ICME2024)
\\
  Recent studies reveal a significant theoretical link between variational
autoencoders (VAEs) and rate-distortion theory, notably in utilizing VAEs to
estimate the theoretical upper bound of the information rate-distortion
function of images. Such estimated theoretical bounds substantially exceed the
performance of existing neural image codecs (NICs). To narrow this gap, we
propose a theoretical bound-guided hierarchical VAE (BG-VAE) for NIC. The
proposed BG-VAE leverages the theoretical bound to guide the NIC model towards
enhanced performance. We implement the BG-VAE using Hierarchical VAEs and
demonstrate its effectiveness through extensive experiments. Along with
advanced neural network blocks, we provide a versatile, variable-rate NIC that
outperforms existing methods when considering both rate-distortion performance
and computational complexity. The code is available at BG-VAE.
\\ ( https://arxiv.org/abs/2403.18535 ,  1211kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18540 (*cross-listing*)
Date: Wed, 27 Mar 2024 13:17:15 GMT   (129kb,D)

Title: skscope: Fast Sparsity-Constrained Optimization in Python
Authors: Zezhi Wang, Jin Zhu, Peng Chen, Huiyang Peng, Xiaoke Zhang, Anran
  Wang, Yu Zheng, Junxian Zhu, Xueqin Wang
Categories: stat.ML cs.LG stat.CO
Comments: 4 pages
\\
  Applying iterative solvers on sparsity-constrained optimization (SCO)
requires tedious mathematical deduction and careful programming/debugging that
hinders these solvers' broad impact. In the paper, the library skscope is
introduced to overcome such an obstacle. With skscope, users can solve the SCO
by just programming the objective function. The convenience of skscope is
demonstrated through two examples in the paper, where sparse linear regression
and trend filtering are addressed with just four lines of code. More
importantly, skscope's efficient implementation allows state-of-the-art solvers
to quickly attain the sparse solution regardless of the high dimensionality of
parameter space. Numerical experiments reveal the available solvers in skscope
can achieve up to 80x speedup on the competing relaxation solutions obtained
via the benchmarked convex solver. skscope is published on the Python Package
Index (PyPI) and Conda, and its source code is available at:
https://github.com/abess-team/skscope.
\\ ( https://arxiv.org/abs/2403.18540 ,  129kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18560 (*cross-listing*)
Date: Wed, 27 Mar 2024 13:42:14 GMT   (3111kb,D)

Title: Noise-Robust Keyword Spotting through Self-supervised Pretraining
Authors: Jacob M{\o}rk, Holger Severin Bovbjerg, Gergely Kiss, Zheng-Hua Tan
Categories: eess.AS cs.LG cs.SD
MSC-class: 68T10
ACM-class: I.2.6
\\
  Voice assistants are now widely available, and to activate them a keyword
spotting (KWS) algorithm is used. Modern KWS systems are mainly trained using
supervised learning methods and require a large amount of labelled data to
achieve a good performance. Leveraging unlabelled data through self-supervised
learning (SSL) has been shown to increase the accuracy in clean conditions.
This paper explores how SSL pretraining such as Data2Vec can be used to enhance
the robustness of KWS models in noisy conditions, which is under-explored.
  Models of three different sizes are pretrained using different pretraining
approaches and then fine-tuned for KWS. These models are then tested and
compared to models trained using two baseline supervised learning methods, one
being standard training using clean data and the other one being multi-style
training (MTR). The results show that pretraining and fine-tuning on clean data
is superior to supervised learning on clean data across all testing conditions,
and superior to supervised MTR for testing conditions of SNR above 5 dB. This
indicates that pretraining alone can increase the model's robustness. Finally,
it is found that using noisy data for pretraining models, especially with the
Data2Vec-denoising approach, significantly enhances the robustness of KWS
models in noisy conditions.
\\ ( https://arxiv.org/abs/2403.18560 ,  3111kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18578 (*cross-listing*)
Date: Wed, 27 Mar 2024 13:59:05 GMT   (1618kb,D)

Title: SteinGen: Generating Fidelitous and Diverse Graph Samples
Authors: Gesine Reinert and Wenkai Xu
Categories: stat.ML cs.LG
\\
  Generating graphs that preserve characteristic structures while promoting
sample diversity can be challenging, especially when the number of graph
observations is small. Here, we tackle the problem of graph generation from
only one observed graph. The classical approach of graph generation from
parametric models relies on the estimation of parameters, which can be
inconsistent or expensive to compute due to intractable normalisation
constants. Generative modelling based on machine learning techniques to
generate high-quality graph samples avoids parameter estimation but usually
requires abundant training samples. Our proposed generating procedure,
SteinGen, which is phrased in the setting of graphs as realisations of
exponential random graph models, combines ideas from Stein's method and MCMC by
employing Markovian dynamics which are based on a Stein operator for the target
model. SteinGen uses the Glauber dynamics associated with an estimated Stein
operator to generate a sample, and re-estimates the Stein operator from the
sample after every sampling step. We show that on a class of exponential random
graph models this novel "estimation and re-estimation" generation strategy
yields high distributional similarity (high fidelity) to the original data,
combined with high sample diversity.
\\ ( https://arxiv.org/abs/2403.18578 ,  1618kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18582 (*cross-listing*)
Date: Wed, 27 Mar 2024 14:03:41 GMT   (3894kb,D)

Title: One flow to correct them all: improving simulations in high-energy
  physics with a single normalising flow and a switch
Authors: Caio Cesar Daumann, Mauro Donega, Johannes Erdmann, Massimiliano
  Galli, Jan Lukas Sp\"ah, Davide Valsecchi
Categories: hep-ph cs.LG hep-ex physics.data-an
Comments: 19 pages, 12 figures
\\
  Simulated events are key ingredients in almost all high-energy physics
analyses. However, imperfections in the simulation can lead to sizeable
differences between the observed data and simulated events. The effects of such
mismodelling on relevant observables must be corrected either effectively via
scale factors, with weights or by modifying the distributions of the
observables and their correlations. We introduce a correction method that
transforms one multidimensional distribution (simulation) into another one
(data) using a simple architecture based on a single normalising flow with a
boolean condition. We demonstrate the effectiveness of the method on a
physics-inspired toy dataset with non-trivial mismodelling of several
observables and their correlations.
\\ ( https://arxiv.org/abs/2403.18582 ,  3894kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18587 (*cross-listing*)
Date: Wed, 27 Mar 2024 14:11:23 GMT   (1145kb,D)

Title: The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency
  Attacks in Computer Vision
Authors: Andreas M\"uller and Erwin Quiring
Categories: cs.CR cs.CV cs.LG
Comments: Accepted at the DLSP 2024
\\
  Resource efficiency plays an important role for machine learning nowadays.
The energy and decision latency are two critical aspects to ensure a
sustainable and practical application. Unfortunately, the energy consumption
and decision latency are not robust against adversaries. Researchers have
recently demonstrated that attackers can compute and submit so-called sponge
examples at inference time to increase the energy consumption and decision
latency of neural networks. In computer vision, the proposed strategy crafts
inputs with less activation sparsity which could otherwise be used to
accelerate the computation. In this paper, we analyze the mechanism how these
energy-latency attacks reduce activation sparsity. In particular, we find that
input uniformity is a key enabler. A uniform image, that is, an image with
mostly flat, uniformly colored surfaces, triggers more activations due to a
specific interplay of convolution, batch normalization, and ReLU activation.
Based on these insights, we propose two new simple, yet effective strategies
for crafting sponge examples: sampling images from a probability distribution
and identifying dense, yet inconspicuous inputs in natural datasets. We
empirically examine our findings in a comprehensive evaluation with multiple
image classification models and show that our attack achieves the same sparsity
effect as prior sponge-example methods, but at a fraction of computation
effort. We also show that our sponge examples transfer between different neural
networks. Finally, we discuss applications of our findings for the good by
improving efficiency by increasing sparsity.
\\ ( https://arxiv.org/abs/2403.18587 ,  1145kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18597 (*cross-listing*)
Date: Wed, 27 Mar 2024 14:20:11 GMT   (29860kb,D)

Title: Heterogeneous Peridynamic Neural Operators: Discover Biotissue
  Constitutive Law and Microstructure From Digital Image Correlation
  Measurements
Authors: Siavash Jafarzadeh, Stewart Silling, Lu Zhang, Colton Ross, Chung-Hao
  Lee, S. M. Rakibur Rahman, Shuodao Wang, Yue Yu
Categories: cond-mat.mtrl-sci cs.LG
\\
  Human tissues are highly organized structures with specific collagen fiber
arrangements varying from point to point. The effects of such heterogeneity
play an important role for tissue function, and hence it is of critical to
discover and understand the distribution of such fiber orientations from
experimental measurements, such as the digital image correlation data. To this
end, we introduce the heterogeneous peridynamic neural operator (HeteroPNO)
approach, for data-driven constitutive modeling of heterogeneous anisotropic
materials. The goal is to learn both a nonlocal constitutive law together with
the material microstructure, in the form of a heterogeneous fiber orientation
field, from loading field-displacement field measurements. To this end, we
propose a two-phase learning approach. Firstly, we learn a homogeneous
constitutive law in the form of a neural network-based kernel function and a
nonlocal bond force, to capture complex homogeneous material responses from
data. Then, in the second phase we reinitialize the learnt bond force and the
kernel function, and training them together with a fiber orientation field for
each material point. Owing to the state-based peridynamic skeleton, our
HeteroPNO-learned material models are objective and have the balance of linear
and angular momentum guaranteed. Moreover, the effects from heterogeneity and
nonlinear constitutive relationship are captured by the kernel function and the
bond force respectively, enabling physical interpretability. As a result, our
HeteroPNO architecture can learn a constitutive model for a biological tissue
with anisotropic heterogeneous response undergoing large deformation regime.
Moreover, the framework is capable to provide displacement and stress field
predictions for new and unseen loading instances.
\\ ( https://arxiv.org/abs/2403.18597 ,  29860kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18637 (*cross-listing*)
Date: Wed, 27 Mar 2024 14:42:08 GMT   (5208kb,D)

Title: Transformers-based architectures for stroke segmentation: A review
Authors: Yalda Zafari-Ghadim, Essam A. Rashed, and Mohamed Mabrok
Categories: eess.IV cs.CV cs.LG
\\
  Stroke remains a significant global health concern, necessitating precise and
efficient diagnostic tools for timely intervention and improved patient
outcomes. The emergence of deep learning methodologies has transformed the
landscape of medical image analysis. Recently, Transformers, initially designed
for natural language processing, have exhibited remarkable capabilities in
various computer vision applications, including medical image analysis. This
comprehensive review aims to provide an in-depth exploration of the
cutting-edge Transformer-based architectures applied in the context of stroke
segmentation. It commences with an exploration of stroke pathology, imaging
modalities, and the challenges associated with accurate diagnosis and
segmentation. Subsequently, the review delves into the fundamental ideas of
Transformers, offering detailed insights into their architectural intricacies
and the underlying mechanisms that empower them to effectively capture complex
spatial information within medical images. The existing literature is
systematically categorized and analyzed, discussing various approaches that
leverage Transformers for stroke segmentation. A critical assessment is
provided, highlighting the strengths and limitations of these methods,
including considerations of performance and computational efficiency.
Additionally, this review explores potential avenues for future research and
development
\\ ( https://arxiv.org/abs/2403.18637 ,  5208kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18639 (*cross-listing*)
Date: Mon, 5 Feb 2024 13:54:11 GMT   (407kb,D)

Title: Dependency Aware Incident Linking in Large Cloud Systems
Authors: Supriyo Ghosh, Karish Grover, Jimmy Wong, Chetan Bansal, Rakesh
  Namineni, Mohit Verma and Saravan Rajmohan
Categories: cs.DC cs.LG
\\
  Despite significant reliability efforts, large-scale cloud services
inevitably experience production incidents that can significantly impact
service availability and customer's satisfaction. Worse, in many cases one
incident can lead to multiple downstream failures due to cascading effects that
creates several related incidents across different dependent services. Often
time On-call Engineers (OCEs) examine these incidents in silos that lead to
significant amount of manual toil and increase the overall time-to-mitigate
incidents. Therefore, developing efficient incident linking models is of
paramount importance for grouping related incidents into clusters so as to
quickly resolve major outages and reduce on-call fatigue. Existing incident
linking methods mostly leverages textual and contextual information of
incidents (e.g., title, description, severity, impacted components), thus
failing to leverage the inter-dependencies between services. In this paper, we
propose the dependency-aware incident linking (DiLink) framework which
leverages both textual and service dependency graph information to improve the
accuracy and coverage of incident links not only coming from same service, but
also from different services and workloads. Furthermore, we propose a novel
method to align the embeddings of multi-modal (i.e., textual and graphical)
data using Orthogonal Procrustes. Extensive experimental results on real-world
incidents from 5 workloads of Microsoft demonstrate that our alignment method
has an F1-score of 0.96 (14% gain over current state-of-the-art methods). We
are also in the process of deploying this solution across 610 services from
these 5 workloads for continuously supporting OCEs improving incident
management and reducing manual toil.
\\ ( https://arxiv.org/abs/2403.18639 ,  407kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18664 (*cross-listing*)
Date: Wed, 27 Mar 2024 15:08:00 GMT   (212kb,D)

Title: Neural Network-Based Piecewise Survival Models
Authors: Olov Holmer, Erik Frisk, and Mattias Krysander
Categories: stat.ML cs.LG cs.SY eess.SY
Comments: 7 pages
\\
  In this paper, a family of neural network-based survival models is presented.
The models are specified based on piecewise definitions of the hazard function
and the density function on a partitioning of the time; both constant and
linear piecewise definitions are presented, resulting in a family of four
models. The models can be seen as an extension of the commonly used
discrete-time and piecewise exponential models and thereby add flexibility to
this set of standard models. Using a simulated dataset the models are shown to
perform well compared to the highly expressive, state-of-the-art energy-based
model, while only requiring a fraction of the computation time.
\\ ( https://arxiv.org/abs/2403.18664 ,  212kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18685 (*cross-listing*)
Date: Wed, 27 Mar 2024 15:29:08 GMT   (64kb,D)

Title: Representatividad Muestral en la Incertidumbre Sim\'etrica Multivariada
  para la Selecci\'on de Atributos
Authors: Gustavo Sosa-Cabrera
Categories: cs.IT cs.LG math.IT math.ST stat.TH
Comments: 52 pages, in Spanish. Advisors: Miguel Garc\'ia-Torres, Santiago
  G\'omez-Guerrero, Christian E. Schaerer Serra
\\
  In this work, we analyze the behavior of the multivariate symmetric
uncertainty (MSU) measure through the use of statistical simulation techniques
under various mixes of informative and non-informative randomly generated
features. Experiments show how the number of attributes, their cardinalities,
and the sample size affect the MSU. In this thesis, through observation of
results, it is proposed an heuristic condition that preserves good quality in
the MSU under different combinations of these three factors, providing a new
useful criterion to help drive the process of dimension reduction.
  --
  En el presente trabajo hemos analizado el comportamiento de una versi\'on
multivariada de la incertidumbre sim\'etrica a trav\'es de t\'ecnicas de
simulaci\'on estad\'isticas sobre varias combinaciones de atributos
informativos y no-informativos generados de forma aleatoria. Los experimentos
muestran como el n\'umero de atributos, sus cardinalidades y el tama\~no
muestral afectan al MSU como medida. En esta tesis, mediante la observaci\'on
de resultados hemos propuesto una condici\'on que preserva una buena calidad en
el MSU bajo diferentes combinaciones de los tres factores mencionados, lo cual
provee un nuevo y valioso criterio para llevar a cabo el proceso de reducci\'on
de dimensionalidad.
\\ ( https://arxiv.org/abs/2403.18685 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18703 (*cross-listing*)
Date: Wed, 27 Mar 2024 15:52:54 GMT   (1774kb,D)

Title: Fpga-Based Neural Thrust Controller for UAVs
Authors: Sharif Azem, David Scheunert, Mengguang Li, Jonas Gehrunger, Kai Cui,
  Christian Hochberger and Heinz Koepp
Categories: eess.SY cs.LG cs.SY
\\
  The advent of unmanned aerial vehicles (UAVs) has improved a variety of
fields by providing a versatile, cost-effective and accessible platform for
implementing state-of-the-art algorithms. To accomplish a broader range of
tasks, there is a growing need for enhanced on-board computing to cope with
increasing complexity and dynamic environmental conditions. Recent advances
have seen the application of Deep Neural Networks (DNNs), particularly in
combination with Reinforcement Learning (RL), to improve the adaptability and
performance of UAVs, especially in unknown environments. However, the
computational requirements of DNNs pose a challenge to the limited computing
resources available on many UAVs. This work explores the use of Field
Programmable Gate Arrays (FPGAs) as a viable solution to this challenge,
offering flexibility, high performance, energy and time efficiency. We propose
a novel hardware board equipped with an Artix-7 FPGA for a popular open-source
micro-UAV platform. We successfully validate its functionality by implementing
an RL-based low-level controller using real-world experiments.
\\ ( https://arxiv.org/abs/2403.18703 ,  1774kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18765 (*cross-listing*)
Date: Wed, 27 Mar 2024 17:03:31 GMT   (11633kb,D)

Title: CaT: Constraints as Terminations for Legged Locomotion Reinforcement
  Learning
Authors: Elliot Chane-Sane, Pierre-Alexandre Leziart, Thomas Flayols, Olivier
  Stasse, Philippe Sou\`eres, Nicolas Mansard
Categories: cs.RO cs.LG
Comments: Project webpage: https://constraints-as-terminations.github.io
\\
  Deep Reinforcement Learning (RL) has demonstrated impressive results in
solving complex robotic tasks such as quadruped locomotion. Yet, current
solvers fail to produce efficient policies respecting hard constraints. In this
work, we advocate for integrating constraints into robot learning and present
Constraints as Terminations (CaT), a novel constrained RL algorithm. Departing
from classical constrained RL formulations, we reformulate constraints through
stochastic terminations during policy learning: any violation of a constraint
triggers a probability of terminating potential future rewards the RL agent
could attain. We propose an algorithmic approach to this formulation, by
minimally modifying widely used off-the-shelf RL algorithms in robot learning
(such as Proximal Policy Optimization). Our approach leads to excellent
constraint adherence without introducing undue complexity and computational
overhead, thus mitigating barriers to broader adoption. Through empirical
evaluation on the real quadruped robot Solo crossing challenging obstacles, we
demonstrate that CaT provides a compelling solution for incorporating
constraints into RL frameworks. Videos and code are available at
https://constraints-as-terminations.github.io.
\\ ( https://arxiv.org/abs/2403.18765 ,  11633kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18774 (*cross-listing*)
Date: Tue, 23 Jan 2024 22:00:49 GMT   (16494kb,D)

Title: RAW: A Robust and Agile Plug-and-Play Watermark Framework for
  AI-Generated Images with Provable Guarantees
Authors: Xun Xian, Ganghua Wang, Xuan Bi, Jayanth Srinivasa, Ashish Kundu,
  Mingyi Hong, and Jie Ding
Categories: cs.CV cs.CR cs.LG
\\
  Safeguarding intellectual property and preventing potential misuse of
AI-generated images are of paramount importance. This paper introduces a robust
and agile plug-and-play watermark detection framework, dubbed as RAW. As a
departure from traditional encoder-decoder methods, which incorporate fixed
binary codes as watermarks within latent representations, our approach
introduces learnable watermarks directly into the original image data.
Subsequently, we employ a classifier that is jointly trained with the watermark
to detect the presence of the watermark. The proposed framework is compatible
with various generative architectures and supports on-the-fly watermark
injection after training. By incorporating state-of-the-art smoothing
techniques, we show that the framework provides provable guarantees regarding
the false positive rate for misclassifying a watermarked image, even in the
presence of certain adversarial attacks targeting watermark removal.
Experiments on a diverse range of images generated by state-of-the-art
diffusion models reveal substantial performance enhancements compared to
existing approaches. For instance, our method demonstrates a notable increase
in AUROC, from 0.48 to 0.82, when compared to state-of-the-art approaches in
detecting watermarked images under adversarial attacks, while maintaining image
quality, as indicated by closely aligned FID and CLIP scores.
\\ ( https://arxiv.org/abs/2403.18774 ,  16494kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18810 (*cross-listing*)
Date: Thu, 8 Feb 2024 21:56:43 GMT   (2649kb,D)

Title: LightningNet: Distributed Graph-based Cellular Network Performance
  Forecasting for the Edge
Authors: Konstantinos Zacharopoulos, Georgios Koutroumpas, Ioannis Arapakis,
  Konstantinos Georgopoulos, Javad Khangosstar, Sotiris Ioannidis
Categories: cs.NI cs.LG
\\
  The cellular network plays a pivotal role in providing Internet access, since
it is the only global-scale infrastructure with ubiquitous mobility support. To
manage and maintain large-scale networks, mobile network operators require
timely information, or even accurate performance forecasts. In this paper, we
propose LightningNet, a lightweight and distributed graph-based framework for
forecasting cellular network performance, which can capture spatio-temporal
dependencies that arise in the network traffic. LightningNet achieves a steady
performance increase over state-of-the-art forecasting techniques, while
maintaining a similar resource usage profile. Our architecture ideology also
excels in the respect that it is specifically designed to support IoT and edge
devices, giving us an even greater step ahead of the current state-of-the-art,
as indicated by our performance experiments with NVIDIA Jetson.
\\ ( https://arxiv.org/abs/2403.18810 ,  2649kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2306.12609
replaced with revised version Wed, 27 Mar 2024 07:11:30 GMT   (113kb,D)

Title: Towards Regulatable AI Systems: Technical Gaps and Policy Opportunities
Authors: Xudong Shen, Hannah Brown, Jiashu Tao, Martin Strobel, Yao Tong,
  Akshay Narayan, Harold Soh, Finale Doshi-Velez
Categories: cs.AI cs.CY
Comments: scheduled for publication in the Communications of the ACM, titled
  "Directions of Technical Innovation for Regulatable AI Systems"
\\ ( https://arxiv.org/abs/2306.12609 ,  113kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03325
replaced with revised version Wed, 27 Mar 2024 08:54:35 GMT   (2140kb,D)

Title: Learning Concept-Based Causal Transition and Symbolic Reasoning for
  Visual Planning
Authors: Yilue Qian, Peiyu Yu, Ying Nian Wu, Yao Su, Wei Wang, Lifeng Fan
Categories: cs.AI cs.CV cs.LG
\\ ( https://arxiv.org/abs/2310.03325 ,  2140kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17072
replaced with revised version Wed, 27 Mar 2024 07:04:58 GMT   (19638kb,D)

Title: MMP++: Motion Manifold Primitives with Parametric Curve Models
Authors: Yonghyeon Lee
Categories: cs.AI cs.LG cs.RO
Comments: 12 pages. This work has been submitted to the IEEE for possible
  publication
\\ ( https://arxiv.org/abs/2310.17072 ,  19638kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09654
replaced with revised version Tue, 26 Mar 2024 20:12:18 GMT   (4861kb,D)

Title: GPT-4's assessment of its performance in a USMLE-based case study
Authors: Uttam Dhakal, Aniket Kumar Singh, Suman Devkota, Yogesh Sapkota,
  Bishal Lamichhane, Suprinsa Paudyal, Chandra Dhakal
Categories: cs.AI cs.CL cs.HC cs.MA stat.ML
\\ ( https://arxiv.org/abs/2402.09654 ,  4861kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17574
replaced with revised version Wed, 27 Mar 2024 17:34:57 GMT   (2598kb,D)

Title: Agent-Pro: Learning to Evolve via Policy-Level Reflection and
  Optimization
Authors: Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang
  Hou, Zeqi Tan, Peng Li, Yueting Zhuang, Weiming Lu
Categories: cs.AI cs.CL
Comments: LLM-based Agent
\\ ( https://arxiv.org/abs/2402.17574 ,  2598kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16427
replaced with revised version Wed, 27 Mar 2024 03:27:24 GMT   (1129kb,D)

Title: Re2LLM: Reflective Reinforcement Large Language Model for Session-based
  Recommendation
Authors: Ziyan Wang, Yingpeng Du, Zhu Sun, Haoyan Chua, Kaidong Feng, Wenya
  Wang, Jie Zhang
Categories: cs.AI
Comments: 11 pages, 4 figures
\\ ( https://arxiv.org/abs/2403.16427 ,  1129kb)
------------------------------------------------------------------------------
\\
arXiv:2302.12468
replaced with revised version Wed, 27 Mar 2024 06:46:56 GMT   (314kb,D)

Title: Adapting Knowledge for Few-shot Table-to-Text Generation
Authors: Zhixin Guo, Minyxuan Yan, Jiexing Qi, Jianping Zhou, Ziwei He, Guanjie
  Zheng, and Xinbing Wang
Categories: cs.CL
Comments: arXiv admin note: substantial text overlap with arXiv:2302.04415
\\ ( https://arxiv.org/abs/2302.12468 ,  314kb)
------------------------------------------------------------------------------
\\
arXiv:2304.03544
replaced with revised version Wed, 27 Mar 2024 10:53:42 GMT   (423kb,D)

Title: InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual
  Topic Modeling
Authors: Xiaobao Wu, Xinshuai Dong, Thong Nguyen, Chaoqun Liu, Liangming Pan,
  Anh Tuan Luu
Categories: cs.CL
Comments: Accepted to AAAI2023 conference. Code is available at
  https://github.com/BobXWu/InfoCTM
\\ ( https://arxiv.org/abs/2304.03544 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2305.02151
replaced with revised version Wed, 27 Mar 2024 08:43:28 GMT   (6666kb,D)

Title: Identifying the Correlation Between Language Distance and Cross-Lingual
  Transfer in a Multilingual Representation Space
Authors: Fred Philippy, Siwen Guo, Shohreh Haddadan
Categories: cs.CL cs.AI cs.LG
Comments: SIGTYP Workshop 2023 (co-located with EACL 2023)
DOI: 10.18653/v1/2023.sigtyp-1.3
\\ ( https://arxiv.org/abs/2305.02151 ,  6666kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14718
replaced with revised version Tue, 26 Mar 2024 18:07:01 GMT   (3584kb,D)

Title: Leftover-Lunch: Advantage-based Offline Reinforcement Learning for
  Language Models
Authors: Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap,
  Mark Riedl
Categories: cs.CL
Comments: published at ICLR 2024
\\ ( https://arxiv.org/abs/2305.14718 ,  3584kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14965
replaced with revised version Wed, 27 Mar 2024 04:38:44 GMT   (7055kb,D)

Title: Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting
  Jailbreaks
Authors: Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, Monojit
  Choudhury
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024 - The 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
\\ ( https://arxiv.org/abs/2305.14965 ,  7055kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03997
replaced with revised version Tue, 26 Mar 2024 21:32:51 GMT   (4091kb,D)

Title: Sentiment Analysis in Finance: From Transformers Back to eXplainable
  Lexicons (XLex)
Authors: Maryan Rizinski, Hristijan Peshov, Kostadin Mishev, Milos Jovanovik,
  Dimitar Trajanov
Categories: cs.CL
Comments: Published by IEEE Access DOI: 10.1109/ACCESS.2024.3349970 Link:
  https://ieeexplore.ieee.org/document/10380556
\\ ( https://arxiv.org/abs/2306.03997 ,  4091kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04357
replaced with revised version Wed, 27 Mar 2024 03:06:13 GMT   (176kb,D)

Title: Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue
  Systems
Authors: Zhenpeng Su and Xing Wu and Wei Zhou and Guangyuan Ma and Songlin Hu
Categories: cs.CL cs.AI
Comments: This paper has been accepted by NAACL 2024
\\ ( https://arxiv.org/abs/2306.04357 ,  176kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05882
replaced with revised version Tue, 26 Mar 2024 22:54:48 GMT   (1344kb)

Title: Good, but not always Fair: An Evaluation of Gender Bias for three
  commercial Machine Translation Systems
Authors: Silvia Alma Piazzolla, Beatrice Savoldi, Luisa Bentivogli
Categories: cs.CL
Journal-ref: Hermes Journal of Language and Communication in Business no 63
  2023
\\ ( https://arxiv.org/abs/2306.05882 ,  1344kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16071
replaced with revised version Wed, 27 Mar 2024 08:56:01 GMT   (822kb,D)

Title: \`{I}r\`{o}y\`{i}nSpeech: A multi-purpose Yor\`{u}b\'{a} Speech Corpus
Authors: Tolulope Ogunremi, Kola Tubosun, Anuoluwapo Aremu, Iroro Orife, David
  Ifeoluwa Adelani
Categories: cs.CL cs.SD eess.AS
Comments: Accepted to LREC-COLING 2024
\\ ( https://arxiv.org/abs/2307.16071 ,  822kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12531
replaced with revised version Wed, 27 Mar 2024 13:46:37 GMT   (193kb,D)

Title: CARE: Co-Attention Network for Joint Entity and Relation Extraction
Authors: Wenjun Kong and Yamei Xia
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\ ( https://arxiv.org/abs/2308.12531 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13320
replaced with revised version Wed, 27 Mar 2024 14:57:29 GMT   (452kb,D)

Title: GlotScript: A Resource and Tool for Low Resource Writing System
  Identification
Authors: Amir Hossein Kargaran, Fran\c{c}ois Yvon, Hinrich Sch\"utze
Categories: cs.CL
Comments: LREC-COLING 2024
\\ ( https://arxiv.org/abs/2309.13320 ,  452kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13322
replaced with revised version Wed, 27 Mar 2024 10:50:24 GMT   (8611kb,D)

Title: From Text to Source: Results in Detecting Large Language Model-Generated
  Content
Authors: Wissam Antoun, Beno\^it Sagot, Djam\'e Seddah
Categories: cs.CL
Comments: Accepted to COLING-LREC 2024
\\ ( https://arxiv.org/abs/2309.13322 ,  8611kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19055
replaced with revised version Tue, 26 Mar 2024 22:59:36 GMT   (133kb,D)

Title: A Few-Shot Learning Focused Survey on Recent Named Entity Recognition
  and Relation Classification Methods
Authors: Sakher Khalil Alqaaidi, Elika Bozorgi, Afsaneh Shams, Krzysztof Kochut
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.19055 ,  133kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07838
replaced with revised version Wed, 27 Mar 2024 11:36:46 GMT   (321kb,D)

Title: LLatrieval: LLM-Verified Retrieval for Verifiable Generation
Authors: Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin, Tianxiang Sun,
  Xipeng Qiu
Categories: cs.CL cs.AI cs.IR
Comments: Accepted by NAACL 2024 (Main Conference)
\\ ( https://arxiv.org/abs/2311.07838 ,  321kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08268
replaced with revised version Wed, 27 Mar 2024 13:29:31 GMT   (9519kb,D)

Title: A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can
  Fool Large Language Models Easily
Authors: Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen,
  Shujian Huang
Categories: cs.CL
Comments: Acccepted by NAACL 2024, 18 pages, 7 figures, 13 tables
\\ ( https://arxiv.org/abs/2311.08268 ,  9519kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08590
replaced with revised version Wed, 27 Mar 2024 05:53:58 GMT   (7525kb,D)

Title: PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language
  Models
Authors: HyunJin Kim, Young Jin Kim, JinYeong Bak
Categories: cs.CL
Comments: Accepted to NAACL 2024
\\ ( https://arxiv.org/abs/2311.08590 ,  7525kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10997
replaced with revised version Wed, 27 Mar 2024 09:16:57 GMT   (1955kb,D)

Title: Retrieval-Augmented Generation for Large Language Models: A Survey
Authors: Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi,
  Yi Dai, Jiawei Sun, Meng Wang and Haofen Wang
Categories: cs.CL cs.AI
Comments: Ongoing Work
\\ ( https://arxiv.org/abs/2312.10997 ,  1955kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02009
replaced with revised version Wed, 27 Mar 2024 17:24:47 GMT   (2777kb,D)

Title: Self-Contrast: Better Reflection Through Inconsistent Solving
  Perspectives
Authors: Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang,
  Yueting Zhuang, Weiming Lu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.02009 ,  2777kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06201
replaced with revised version Wed, 27 Mar 2024 06:31:42 GMT   (1214kb,D)

Title: EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction
Authors: Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren
  Kan, Dongsheng Li, Deqing Yang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.06201 ,  1214kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06712
replaced with revised version Wed, 27 Mar 2024 13:38:35 GMT   (113kb,D)

Title: Few-Shot Detection of Machine-Generated Text using Style Representations
Authors: Rafael Rivera Soto, Kailin Koch, Aleem Khan, Barry Chen, Marcus
  Bishop, and Nicholas Andrews
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2401.06712 ,  113kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12492
replaced with revised version Tue, 26 Mar 2024 19:28:15 GMT   (7901kb,D)

Title: Comparing Pre-trained Human Language Models: Is it Better with Human
  Context as Groups, Individual Traits, or Both?
Authors: Nikita Soni, Niranjan Balasubramanian, H. Andrew Schwartz, and Dirk
  Hovy
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.12492 ,  7901kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01739
replaced with revised version Wed, 27 Mar 2024 10:21:24 GMT   (755kb,D)

Title: OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models
Authors: Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu
  Zhou, Yang You
Categories: cs.CL cs.AI cs.DC cs.LG
\\ ( https://arxiv.org/abs/2402.01739 ,  755kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09283
replaced with revised version Wed, 27 Mar 2024 13:55:14 GMT   (8472kb,D)

Title: Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey
Authors: Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao
Categories: cs.CL cs.AI cs.CY cs.LG
Comments: Accepted to NAACL 2024
\\ ( https://arxiv.org/abs/2402.09283 ,  8472kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15764
replaced with revised version Wed, 27 Mar 2024 01:23:58 GMT   (1128kb,D)

Title: Look Before You Leap: Problem Elaboration Prompting Improves
  Mathematical Reasoning in Large Language Models
Authors: Haoran Liao, Jidong Tian, Shaohua Hu, Hao He, Yaohui Jin
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.15764 ,  1128kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17304
replaced with revised version Wed, 27 Mar 2024 02:59:57 GMT   (850kb)

Title: Probing Multimodal Large Language Models for Global and Local Semantic
  Representations
Authors: Mingxu Tao, Quzhe Huang, Kun Xu, Liwei Chen, Yansong Feng, Dongyan
  Zhao
Categories: cs.CL cs.AI
Comments: Accepted by LREC-COLING 2024 as a short paper (Camera Ready)
\\ ( https://arxiv.org/abs/2402.17304 ,  850kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00868
replaced with revised version Wed, 27 Mar 2024 03:03:00 GMT   (1078kb,D)

Title: SoftTiger: A Clinical Foundation Model for Healthcare Workflows
Authors: Ye Chen, Igor Couto, Wei Cai, Cong Fu, Bruno Dorneles
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.00868 ,  1078kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04507
replaced with revised version Wed, 27 Mar 2024 14:50:56 GMT   (2050kb,D)

Title: NLPre: a revised approach towards language-centric benchmarking of
  Natural Language Preprocessing systems
Authors: Martyna Wi\k{a}cek, Piotr Rybak, {\L}ukasz Pszenny, Alina Wr\'oblewska
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2403.04507 ,  2050kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09131
replaced with revised version Wed, 27 Mar 2024 05:02:55 GMT   (311kb,D)

Title: ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate
  Professional and Non-Professional Styled Text
Authors: Chang Zong, Yuyan Chen, Weiming Lu, Jian Shao, Yueting Zhuang
Categories: cs.CL cs.AI
Comments: 8 pages
MSC-class: 68T50
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2403.09131 ,  311kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09887
replaced with revised version Tue, 26 Mar 2024 23:52:35 GMT   (3374kb,D)

Title: Sabi\'a-2: A New Generation of Portuguese Large Language Models
Authors: Thales Sales Almeida, Hugo Abonizio, Rodrigo Nogueira and Ramon Pires
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.09887 ,  3374kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11128
replaced with revised version Wed, 27 Mar 2024 15:22:53 GMT   (1293kb,D)

Title: Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants'
  API Invocation Capabilities
Authors: Honglin Mu, Yang Xu, Yunlong Feng, Xiaofeng Han, Yitong Li, Yutai Hou,
  Wanxiang Che
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2403.11128 ,  1293kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11399
replaced with revised version Wed, 27 Mar 2024 07:05:22 GMT   (12284kb,D)

Title: X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment
Authors: Dongjae Shin, Hyunseok Lim, Inho Won, Changsu Choi, Minjun Kim,
  Seungwoo Song, Hangyeol Yoo, Sangmin Kim, Kyungtae Lim
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.11399 ,  12284kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14814
replaced with revised version Tue, 26 Mar 2024 18:10:10 GMT   (2437kb)

Title: The opportunities and risks of large language models in mental health
Authors: Hannah R. Lawrence, Renee A. Schneider, Susan B. Rubin, Maja J.
  Mataric, Daniel J. McDuff, and Megan Jones Bell
Categories: cs.CL cs.AI cs.CY cs.HC cs.LG
Comments: 12 pages, 2 tables, 4 figures
\\ ( https://arxiv.org/abs/2403.14814 ,  2437kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16432
replaced with revised version Wed, 27 Mar 2024 11:37:58 GMT   (7544kb,D)

Title: $\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on
  Prompt-based Language Models
Authors: Yue Xu, Wenjie Wang
Categories: cs.CL cs.AI
Comments: Accepted to the main conference of NAACL2024
\\ ( https://arxiv.org/abs/2403.16432 ,  7544kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16512
replaced with revised version Wed, 27 Mar 2024 06:25:10 GMT   (10057kb,D)

Title: LLMs Are Few-Shot In-Context Low-Resource Language Learners
Authors: Samuel Cahyawijaya, Holy Lovenia, Pascale Fung
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.16512 ,  10057kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16516
replaced with revised version Wed, 27 Mar 2024 12:32:31 GMT   (16909kb,D)

Title: Visually Guided Generative Text-Layout Pre-training for Document
  Intelligence
Authors: Zhiming Mao, Haoli Bai, Lu Hou, Jiansheng Wei, Xin Jiang, Qun Liu,
  Kam-Fai Wong
Categories: cs.CL cs.CV
Comments: Accepted to NAACL 2024 main conference. The first version of this
  paper was submitted to OpenReview
  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023
\\ ( https://arxiv.org/abs/2403.16516 ,  16909kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17143
replaced with revised version Wed, 27 Mar 2024 15:15:16 GMT   (8280kb,D)

Title: Guided Distant Supervision for Multilingual Relation Extraction Data:
  Adapting to a New Language
Authors: Alistair Plum, Tharindu Ranasinghe, Christoph Purschke
Categories: cs.CL cs.LG
Comments: Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)
\\ ( https://arxiv.org/abs/2403.17143 ,  8280kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17636
replaced with revised version Wed, 27 Mar 2024 05:55:35 GMT   (7259kb,D)

Title: Mix-Initiative Response Generation with Dynamic Prefix Tuning
Authors: Yuxiang Nie, Heyan Huang, Xian-Ling Mao, Lizi Liao
Categories: cs.CL
Comments: Accepted to the main conference of NAACL 2024
\\ ( https://arxiv.org/abs/2403.17636 ,  7259kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17647
replaced with revised version Wed, 27 Mar 2024 10:07:59 GMT   (10130kb,D)

Title: Intrinsic Subgraph Generation for Interpretable Graph based Visual
  Question Answering
Authors: Pascal Tilli, Ngoc Thang Vu
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2403.17647 ,  10130kb)
------------------------------------------------------------------------------
\\
arXiv:2102.12920
replaced with revised version Wed, 27 Mar 2024 09:07:29 GMT   (76kb)

Title: Emerging Trends in Federated Learning: From Model Fusion to Federated X
  Learning
Authors: Shaoxiong Ji and Yue Tan and Teemu Saravirta and Zhiqin Yang and Yixin
  Liu and Lauri Vasankari and Shirui Pan and Guodong Long and Anwar Walid
Categories: cs.LG cs.DC
Comments: To appear in the International Journal of Machine Learning and
  Cybernetics
\\ ( https://arxiv.org/abs/2102.12920 ,  76kb)
------------------------------------------------------------------------------
\\
arXiv:2301.11104
replaced with revised version Wed, 27 Mar 2024 03:47:20 GMT   (4802kb,D)

Title: Discovering and Mitigating Visual Biases through Keyword Explanation
Authors: Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, Jinwoo
  Shin
Categories: cs.LG cs.CV
Comments: CVPR 2024. First two authors contributed equally
\\ ( https://arxiv.org/abs/2301.11104 ,  4802kb)
------------------------------------------------------------------------------
\\
arXiv:2302.06912
replaced with revised version Wed, 27 Mar 2024 06:57:30 GMT   (840kb,D)

Title: Regret-Based Defense in Adversarial Reinforcement Learning
Authors: Roman Belaire, Pradeep Varakantham, Thanh Nguyen, David Lo
Categories: cs.LG cs.AI
Comments: Accepted at AAMAS 2024
\\ ( https://arxiv.org/abs/2302.06912 ,  840kb)
------------------------------------------------------------------------------
\\
arXiv:2302.13483
replaced with revised version Wed, 27 Mar 2024 17:38:27 GMT   (2015kb,D)

Title: CrystalBox: Future-Based Explanations for Input-Driven Deep RL Systems
Authors: Sagar Patel, Sangeetha Abdu Jyothi, Nina Narodytska
Categories: cs.LG cs.NI cs.SY eess.SY
\\ ( https://arxiv.org/abs/2302.13483 ,  2015kb)
------------------------------------------------------------------------------
\\
arXiv:2303.10365
replaced with revised version Wed, 27 Mar 2024 12:53:12 GMT   (678kb,D)

Title: CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label
  Learning
Authors: Shiyu Tian, Hongxin Wei, Yiqun Wang, Lei Feng
Categories: cs.LG cs.AI
Comments: Accepted by CVPR 2024
ACM-class: I.2
\\ ( https://arxiv.org/abs/2303.10365 ,  678kb)
------------------------------------------------------------------------------
\\
arXiv:2303.12091
replaced with revised version Wed, 27 Mar 2024 15:44:25 GMT   (2097kb,D)

Title: Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised
  Learning
Authors: Yang Yu, Danruo Deng, Furui Liu, Yueming Jin, Qi Dou, Guangyong Chen,
  Pheng-Ann Heng
Categories: cs.LG cs.AI cs.CV
Comments: Accepted by AAAI2024
\\ ( https://arxiv.org/abs/2303.12091 ,  2097kb)
------------------------------------------------------------------------------
\\
arXiv:2304.01973
replaced with revised version Tue, 26 Mar 2024 22:46:10 GMT   (15554kb,D)

Title: ERM++: An Improved Baseline for Domain Generalization
Authors: Piotr Teterwak, Kuniaki Saito, Theodoros Tsiligkaridis, Kate Saenko,
  Bryan A. Plummer
Categories: cs.LG cs.CV
Comments: An improved baseline for Domain Generalization
\\ ( https://arxiv.org/abs/2304.01973 ,  15554kb)
------------------------------------------------------------------------------
\\
arXiv:2304.06427
replaced with revised version Wed, 27 Mar 2024 02:58:26 GMT   (3102kb,D)

Title: In-Distribution and Out-of-Distribution Self-supervised ECG
  Representation Learning for Arrhythmia Detection
Authors: Sahar Soltanieh, Javad Hashemi, Ali Etemad
Categories: cs.LG cs.AI eess.SP
Comments: This paper has been published in the IEEE Journal of Biomedical and
  Health Informatics (JBHI). Copyright IEEE. Please cite as: S. Soltanieh, J.
  Hashemi and A. Etemad, "In-Distribution and Out-of-Distribution
  Self-Supervised ECG Representation Learning for Arrhythmia Detection," in
  IEEE Journal of Biomedical and Health Informatics, vol. 28, no. 2, pp.
  789-800, Feb. 2024
DOI: 10.1109/JBHI.2023.3331626
\\ ( https://arxiv.org/abs/2304.06427 ,  3102kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13525
replaced with revised version Wed, 27 Mar 2024 17:47:56 GMT   (1718kb,D)

Title: A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs
Authors: Siddharth Singh, Prajwal Singhania, Aditya K. Ranjan, Zack Sating,
  Abhinav Bhatele
Categories: cs.LG cs.AI cs.DC cs.PF
\\ ( https://arxiv.org/abs/2305.13525 ,  1718kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14258
replaced with revised version Wed, 27 Mar 2024 05:45:37 GMT   (580kb,D)

Title: Weakly Supervised AUC Optimization: A Unified Partial AUC Approach
Authors: Zheng Xie, Yu Liu, Hao-Yuan He, Ming Li, Zhi-Hua Zhou
Categories: cs.LG cs.AI
Comments: Accepted by IEEE TPAMI
DOI: 10.1109/TPAMI.2024.3357814
\\ ( https://arxiv.org/abs/2305.14258 ,  580kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09459
replaced with revised version Wed, 27 Mar 2024 14:02:58 GMT   (3076kb,D)

Title: Recurrent Action Transformer with Memory
Authors: Alexey Staroverov and Egor Cherepanov and Dmitry Yudin and Alexey K.
  Kovalev and Aleksandr I. Panov
Categories: cs.LG cs.AI
Comments: 15 pages, 11 figures
\\ ( https://arxiv.org/abs/2306.09459 ,  3076kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13352
replaced with revised version Wed, 27 Mar 2024 09:04:04 GMT   (96kb,D)

Title: High Dimensional Distributed Gradient Descent with Arbitrary Number of
  Byzantine Attackers
Authors: Puning Zhao, Zhiguo Wan
Categories: cs.LG cs.CR cs.DC
\\ ( https://arxiv.org/abs/2307.13352 ,  96kb)
------------------------------------------------------------------------------
\\
arXiv:2308.06822
replaced with revised version Tue, 26 Mar 2024 19:39:23 GMT   (1384kb,D)

Title: Approximate and Weighted Data Reconstruction Attack in Federated
  Learning
Authors: Yongcun Song, Ziqi Wang, Enrique Zuazua
Categories: cs.LG cs.AI cs.CR math.OC
\\ ( https://arxiv.org/abs/2308.06822 ,  1384kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04381
replaced with revised version Wed, 27 Mar 2024 17:07:47 GMT   (589kb,D)

Title: Generalization Bounds: Perspectives from Information Theory and
  PAC-Bayes
Authors: Fredrik Hellstr\"om, Giuseppe Durisi, Benjamin Guedj, Maxim Raginsky
Categories: cs.LG cs.AI cs.IT math.IT math.ST stat.ML stat.TH
Comments: 228 pages
\\ ( https://arxiv.org/abs/2309.04381 ,  589kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11427
replaced with revised version Wed, 27 Mar 2024 14:02:57 GMT   (7484kb,D)

Title: Generative Pre-Training of Time-Series Data for Unsupervised Fault
  Detection in Semiconductor Manufacturing
Authors: Sewoong Lee, JinKyou Choi and Min Su Kim
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2309.11427 ,  7484kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05723
replaced with revised version Wed, 27 Mar 2024 09:48:34 GMT   (2254kb,D)

Title: Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement
  Learning
Authors: Trevor McInroe, Adam Jelley, Stefano V. Albrecht, Amos Storkey
Categories: cs.LG
Comments: 10 pages, 17 figures, preprint
\\ ( https://arxiv.org/abs/2310.05723 ,  2254kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01191
replaced with revised version Wed, 27 Mar 2024 10:12:31 GMT   (402kb,D)

Title: VIGraph: Generative Self-supervised Learning for Class-Imbalanced Node
  Classification
Authors: Yulan Hu, Sheng Ouyang, Zhirui Yang, Yong Liu
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2311.01191 ,  402kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01483
replaced with revised version Wed, 27 Mar 2024 16:56:23 GMT   (43788kb,D)

Title: FedSN: A Novel Federated Learning Framework over LEO Satellite Networks
Authors: Zheng Lin, Zhe Chen, Zihan Fang, Xianhao Chen, Xiong Wang, and Yue Gao
Categories: cs.LG cs.AI cs.DC
Comments: 14 pages, 17 figures
\\ ( https://arxiv.org/abs/2311.01483 ,  43788kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03683
replaced with revised version Wed, 27 Mar 2024 16:44:22 GMT   (453kb,D)

Title: Preventing Arbitrarily High Confidence on Far-Away Data in
  Point-Estimated Discriminative Neural Networks
Authors: Ahmad Rashid, Serena Hacker, Guojun Zhang, Agustinus Kristiadi and
  Pascal Poupart
Categories: cs.LG
Comments: Accepted at AISTATS 2024
\\ ( https://arxiv.org/abs/2311.03683 ,  453kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04698
replaced with revised version Wed, 27 Mar 2024 12:24:17 GMT   (5868kb,D)

Title: Challenging Common Paradigms in Multi-Task Learning
Authors: Cathrin Elich, Lukas Kirchdorfer, Jan M. K\"ohler, Lukas Schott
Categories: cs.LG cs.AI cs.CV
Comments: -
\\ ( https://arxiv.org/abs/2311.04698 ,  5868kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03256
replaced with revised version Wed, 27 Mar 2024 03:14:14 GMT   (3948kb,D)

Title: CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale
  Recommendation Models
Authors: Hailin Zhang, Zirui Liu, Boxuan Chen, Yikai Zhao, Tong Zhao, Tong
  Yang, Bin Cui
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.03256 ,  3948kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05677
replaced with revised version Tue, 26 Mar 2024 22:53:56 GMT   (319kb,D)

Title: Batched Low-Rank Adaptation of Foundation Models
Authors: Yeming Wen, Swarat Chaudhuri
Categories: cs.LG cs.AI cs.CL
Comments: 16 pages, 3 figures
\\ ( https://arxiv.org/abs/2312.05677 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07950
replaced with revised version Wed, 27 Mar 2024 04:51:51 GMT   (2564kb,D)

Title: CBQ: Cross-Block Quantization for Large Language Models
Authors: Xin Ding, Xiaoyu Liu, Zhijun Tu, Yun Zhang, Wei Li, Jie Hu, Hanting
  Chen, Yehui Tang, Zhiwei Xiong, Baoqun Yin, Yunhe Wang
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2312.07950 ,  2564kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08533
replaced with revised version Wed, 27 Mar 2024 09:11:48 GMT   (36647kb,D)

Title: World Models via Policy-Guided Trajectory Diffusion
Authors: Marc Rigter, Jun Yamada, Ingmar Posner
Categories: cs.LG cs.AI
Comments: Published in TMLR, March 2024
\\ ( https://arxiv.org/abs/2312.08533 ,  36647kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10812
replaced with revised version Wed, 27 Mar 2024 00:15:16 GMT   (7911kb,D)

Title: Learning to Act without Actions
Authors: Dominik Schmidt, Minqi Jiang
Categories: cs.LG cs.AI
Comments: Accepted at ICLR 2024 (spotlight). The code can be found at
  http://github.com/schmidtdominik/LAPO
\\ ( https://arxiv.org/abs/2312.10812 ,  7911kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12558
replaced with revised version Wed, 27 Mar 2024 05:48:21 GMT   (687kb,D)

Title: Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge
Authors: Meshal Alharbi, Mardavij Roozbehani, Munther Dahleh
Categories: cs.LG math.OC stat.ML
Comments: Published in the 38th Annual AAAI Conference on Artificial
  Intelligence
DOI: 10.1609/aaai.v38i10.28953
\\ ( https://arxiv.org/abs/2312.12558 ,  687kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07494
replaced with revised version Wed, 27 Mar 2024 16:06:34 GMT   (12286kb,D)

Title: Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering
  Tasks
Authors: Zihao Wang, P S Pravin, Zhe Wu
Categories: cs.LG cs.CE cs.SY eess.SY
\\ ( https://arxiv.org/abs/2401.07494 ,  12286kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16025
replaced with revised version Wed, 27 Mar 2024 04:36:17 GMT   (18029kb,D)

Title: Simple Policy Optimization
Authors: Zhengpeng Xie
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.16025 ,  18029kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02561
replaced with revised version Wed, 27 Mar 2024 05:23:40 GMT   (267kb,D)

Title: Foundation Model Makes Clustering A Better Initialization For Cold-Start
  Active Learning
Authors: Han Yuan and Chuan Hong
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2402.02561 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11800
replaced with revised version Wed, 27 Mar 2024 15:48:29 GMT   (803kb,D)

Title: Stochastic Approximation with Delayed Updates: Finite-Time Rates under
  Markovian Sampling
Authors: Arman Adibi, Nicolo Dal Fabbro, Luca Schenato, Sanjeev Kulkarni, H.
  Vincent Poor, George J. Pappas, Hamed Hassani, Aritra Mitra
Categories: cs.LG cs.AI cs.MA cs.SY eess.SY math.OC
Comments: Accepted to the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024!
\\ ( https://arxiv.org/abs/2402.11800 ,  803kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08579
replaced with revised version Wed, 27 Mar 2024 12:28:02 GMT   (1003kb,D)

Title: Machine Learning Optimized Orthogonal Basis Piecewise Polynomial
  Approximation
Authors: Hannes Waclawek, Stefan Huber
Categories: cs.LG
Comments: Submitted to LION18
\\ ( https://arxiv.org/abs/2403.08579 ,  1003kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10158
replaced with revised version Wed, 27 Mar 2024 08:57:20 GMT   (754kb,D)

Title: Functional Graph Convolutional Networks: A unified multi-task and
  multi-modal learning framework to facilitate health and social-care insights
Authors: Tobia Boschi, Francesca Bonin, Rodrigo Ordonez-Hurtado, C\'ecile
  Rousseau, Alessandra Pascale, and John Dinsmore
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.10158 ,  754kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13374
replaced with revised version Wed, 27 Mar 2024 14:57:54 GMT   (1126kb,D)

Title: Byzantine-resilient Federated Learning With Adaptivity to Data
  Heterogeneity
Authors: Shiyuan Zuo, Xingrun Yan, Rongfei Fan, Han Hu, Hangguan Shan, Tony Q.
  S. Quek
Categories: cs.LG cs.AI cs.CR
\\ ( https://arxiv.org/abs/2403.13374 ,  1126kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14623
replaced with revised version Wed, 27 Mar 2024 16:49:35 GMT   (16363kb,D)

Title: Simplified Diffusion Schr\"odinger Bridge
Authors: Zhicong Tang, Tiankai Hang, Shuyang Gu, Dong Chen, Baining Guo
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2403.14623 ,  16363kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16451
replaced with revised version Wed, 27 Mar 2024 14:36:21 GMT   (30954kb,D)

Title: DeepMachining: Online Prediction of Machining Errors of Lathe Machines
Authors: Xiang-Li Lu, Hwai-Jung Hsu, Che-Wei Chou, H. T. Kung, and Chen-Hsin
  Lee
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.16451 ,  30954kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17456
replaced with revised version Wed, 27 Mar 2024 02:39:26 GMT   (6178kb,D)

Title: Imitating Cost-Constrained Behaviors in Reinforcement Learning
Authors: Qian Shao, Pradeep Varakantham, Shih-Fen Cheng
Categories: cs.LG cs.AI
Comments: Accepted to the 34th International Conference on Automated Planning
  and Scheduling (ICAPS-24)
\\ ( https://arxiv.org/abs/2403.17456 ,  6178kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17878
replaced with revised version Wed, 27 Mar 2024 16:01:00 GMT   (929kb,D)

Title: Empowering Data Mesh with Federated Learning
Authors: Haoyuan Li and Salman Toor
Categories: cs.LG cs.DC
\\ ( https://arxiv.org/abs/2403.17878 ,  929kb)
------------------------------------------------------------------------------
\\
arXiv:2201.06180
replaced with revised version Wed, 27 Mar 2024 16:45:26 GMT   (1117kb,D)

Title: Nonlinear Control Allocation: A Learning Based Approach
Authors: Hafiz Zeeshan Iqbal Khan, Surrayya Mobeen, Jahanzeb Rajput, Jamshed
  Riaz
Categories: eess.SY cs.AI cs.SY math.OC
Comments: submitted to IEEE Conference on Decision and Control (CDC), 2024
\\ ( https://arxiv.org/abs/2201.06180 ,  1117kb)
------------------------------------------------------------------------------
\\
arXiv:2302.01421 (*cross-listing*)
replaced with revised version Wed, 27 Mar 2024 00:38:33 GMT   (627kb,D)

Title: Follower Agnostic Methods for Stackelberg Games
Authors: Chinmay Maheshwari and James Cheng and S. Shankar Sasty and Lillian
  Ratliff and Eric Mazumdar
Categories: math.OC cs.AI cs.GT math.DS
Comments: 31 pages
MSC-class: 91A65
\\ ( https://arxiv.org/abs/2302.01421 ,  627kb)
------------------------------------------------------------------------------
\\
arXiv:2303.09618
replaced with revised version Tue, 26 Mar 2024 22:59:52 GMT   (8632kb,D)

Title: HIVE: Harnessing Human Feedback for Instructional Visual Editing
Authors: Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu,
  Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong and Ran
  Xu
Categories: cs.CV cs.AI cs.CL cs.HC cs.LG
Comments: In CVPR, 2024
\\ ( https://arxiv.org/abs/2303.09618 ,  8632kb)
------------------------------------------------------------------------------
\\
arXiv:2303.13300
replaced with revised version Wed, 27 Mar 2024 02:23:29 GMT   (1065kb)

Title: The Innovation Paradox: Concept Space Expansion with Diminishing
  Originality and the Promise of Creative AI
Authors: Serhad Sarica, Jianxi Luo
Categories: cs.SI cs.AI
Comments: Forthcoming on the Design Science
\\ ( https://arxiv.org/abs/2303.13300 ,  1065kb)
------------------------------------------------------------------------------
\\
arXiv:2303.17251
replaced with revised version Wed, 27 Mar 2024 14:48:48 GMT   (68kb,D)

Title: Demystifying Misconceptions in Social Bots Research
Authors: Stefano Cresci, Kai-Cheng Yang, Angelo Spognardi, Roberto Di Pietro,
  Filippo Menczer, Marinella Petrocchi
Categories: cs.SI cs.AI cs.CY cs.LG
\\ ( https://arxiv.org/abs/2303.17251 ,  68kb)
------------------------------------------------------------------------------
\\
arXiv:2305.03123
replaced with revised version Wed, 27 Mar 2024 16:03:32 GMT   (1861kb)

Title: ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A Review
Authors: Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, and Lewis
  Nkenyereye
Categories: cs.CY cs.AI cs.CL cs.LG
Comments: 29 pages, 8 figures, 4 tables
\\ ( https://arxiv.org/abs/2305.03123 ,  1861kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16772
replaced with revised version Tue, 26 Mar 2024 18:04:33 GMT   (31402kb,D)

Title: Learning from Synthetic Human Group Activities
Authors: Che-Jui Chang, Danrui Li, Deep Patel, Parth Goel, Honglu Zhou,
  Seonghyeon Moon, Samuel S. Sohn, Sejong Yoon, Vladimir Pavlovic, Mubbasir
  Kapadia
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2306.16772 ,  31402kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09136
replaced with revised version Wed, 27 Mar 2024 07:16:28 GMT   (681kb,D)

Title: The Effects of Mixed Sample Data Augmentation are Class Dependent
Authors: Haeil Lee, Hansang Lee, Junmo Kim
Categories: cs.CV cs.AI
Comments: 21 pages, 18 figures, Overall Revision
\\ ( https://arxiv.org/abs/2307.09136 ,  681kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00117
replaced with revised version Wed, 27 Mar 2024 13:38:00 GMT   (5170kb,D)

Title: ABScribe: Rapid Exploration & Organization of Multiple Writing
  Variations in Human-AI Co-Writing Tasks using Large Language Models
Authors: Mohi Reza, Nathan Laundry, Ilya Musabirov, Peter Dushniku, Zhi Yuan
  "Michael" Yu, Kashish Mittal, Tovi Grossman, Michael Liut, Anastasia
  Kuzminykh, Joseph Jay Williams
Categories: cs.HC cs.AI cs.LG
Comments: CHI 2024
DOI: 10.1145/3613904.3641899
\\ ( https://arxiv.org/abs/2310.00117 ,  5170kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10319
replaced with revised version Wed, 27 Mar 2024 17:41:50 GMT   (32975kb,D)

Title: Shifting to Machine Supervision: Annotation-Efficient Semi and
  Self-Supervised Learning for Automatic Medical Image Segmentation and
  Classification
Authors: Pranav Singh, Raviteja Chukkapalli, Shravan Chaudhari, Luoyao Chen,
  Mei Chen, Jinqian Pan, Craig Smuda and Jacopo Cirrone
Categories: cs.CV cs.AI
Comments: Seventeen pages (incl. references), five figures, and one table.
  (Under Review)
\\ ( https://arxiv.org/abs/2311.10319 ,  32975kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10522
replaced with revised version Wed, 27 Mar 2024 11:18:51 GMT   (17843kb,D)

Title: Enhancing Object Coherence in Layout-to-Image Synthesis
Authors: Yibin Wang and Weizhong Zhang and Jianwei Zheng and Cheng Jin
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2311.10522 ,  17843kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12028
replaced with revised version Wed, 27 Mar 2024 11:43:28 GMT   (8862kb,D)

Title: Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose
  Estimation
Authors: Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Jialun Cai, Nicu Sebe
Categories: cs.CV cs.AI cs.LG
Comments: Accepted by CVPR 2024, Open Sourced
\\ ( https://arxiv.org/abs/2311.12028 ,  8862kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02126
replaced with revised version Tue, 26 Mar 2024 21:20:57 GMT   (10463kb,D)

Title: SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM
Authors: Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan
  Yang, Sebastian Scherer, Deva Ramanan, Jonathon Luiten
Categories: cs.CV cs.AI cs.RO
Comments: CVPR 2024. Website: https://spla-tam.github.io/
\\ ( https://arxiv.org/abs/2312.02126 ,  10463kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08344
replaced with revised version Tue, 26 Mar 2024 19:25:53 GMT   (5986kb,D)

Title: FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects
Authors: Bowen Wen, Wei Yang, Jan Kautz, Stan Birchfield
Categories: cs.CV cs.AI cs.RO
\\ ( https://arxiv.org/abs/2312.08344 ,  5986kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03244 (*cross-listing*)
replaced with revised version Tue, 26 Mar 2024 20:35:45 GMT   (3017kb,D)

Title: Artificial Intelligence for Operations Research: Revolutionizing the
  Operations Research Process
Authors: Zhenan Fan, Bissan Ghaddar, Xinglu Wang, Linzi Xing, Yong Zhang, Zirui
  Zhou
Categories: math.OC cs.AI
\\ ( https://arxiv.org/abs/2401.03244 ,  3017kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15120
replaced with revised version Wed, 27 Mar 2024 15:49:52 GMT   (3053kb,D)

Title: Incorporating simulated spatial context information improves the
  effectiveness of contrastive learning models
Authors: Lizhen Zhu and James Z. Wang and Wonseuk Lee and Brad Wyble
Categories: cs.CV cs.AI
DOI: 10.1016/j.patter.2024.100964
\\ ( https://arxiv.org/abs/2401.15120 ,  3053kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13284
replaced with revised version Wed, 27 Mar 2024 14:30:44 GMT   (1495kb,D)

Title: Structure Guided Large Language Model for SQL Generation
Authors: Qinggang Zhang, Junnan Dong, Hao Chen, Wentao Li, Feiran Huang, Xiao
  Huang
Categories: cs.DB cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.13284 ,  1495kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18920
replaced with revised version Wed, 27 Mar 2024 07:16:21 GMT   (43301kb,D)

Title: Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation
Authors: Dongliang Cao, Marvin Eisenberger, Nafie El Amrani, Daniel Cremers,
  Florian Bernard
Categories: cs.CV cs.AI cs.CG
Comments: accepted by CVPR2024
\\ ( https://arxiv.org/abs/2402.18920 ,  43301kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00154
replaced with revised version Wed, 27 Mar 2024 02:21:03 GMT   (1625kb,D)

Title: LLMs in Political Science: Heralding a New Era of Visual Analysis
Authors: Yu Wang
Categories: cs.CV cs.AI
Comments: 7 pages, 3 tables
\\ ( https://arxiv.org/abs/2403.00154 ,  1625kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03100 (*cross-listing*)
replaced with revised version Wed, 27 Mar 2024 16:14:34 GMT   (893kb,D)

Title: NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and
  Diffusion Models
Authors: Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang,
  Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin,
  Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, Sheng Zhao
Categories: eess.AS cs.AI cs.CL cs.LG cs.SD
Comments: Achieving human-level quality and naturalness on multi-speaker
  datasets (e.g., LibriSpeech) in a zero-shot way
\\ ( https://arxiv.org/abs/2403.03100 ,  893kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05465
replaced with revised version Tue, 26 Mar 2024 18:43:35 GMT   (5649kb,D)

Title: Algorithm-Hardware Co-Design of Distribution-Aware Logarithmic-Posit
  Encodings for Efficient DNN Inference
Authors: Akshat Ramachandran, Zishen Wan, Geonhwa Jeong, John Gustafson, Tushar
  Krishna
Categories: cs.AR cs.AI cs.LG cs.NE
Comments: 2024 61st IEEE/ACM Design Automation Conference (DAC)
\\ ( https://arxiv.org/abs/2403.05465 ,  5649kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06054 (*cross-listing*)
replaced with revised version Wed, 27 Mar 2024 17:06:10 GMT   (13986kb,D)

Title: Decoupled Data Consistency with Diffusion Purification for Image
  Restoration
Authors: Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishankar,
  Qing Qu
Categories: eess.IV cs.AI cs.CV cs.LG eess.SP
\\ ( https://arxiv.org/abs/2403.06054 ,  13986kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07711
replaced with revised version Wed, 27 Mar 2024 06:02:38 GMT   (798kb,D)

Title: SSM Meets Video Diffusion Models: Efficient Video Generation with
  Structured State Spaces
Authors: Yuta Oshima, Shohei Taniguchi, Masahiro Suzuki, Yutaka Matsuo
Categories: cs.CV cs.AI
Comments: Accepted as workshop paper at ICLR 2024
\\ ( https://arxiv.org/abs/2403.07711 ,  798kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09700
replaced with revised version Wed, 27 Mar 2024 13:42:25 GMT   (6954kb,D)

Title: Shapley Values-Powered Framework for Fair Reward Split in Content
  Produced by GenAI
Authors: Alex Glinsky, Alexey Sokolsky
Categories: cs.CV cs.AI
Comments: 36 pages, 32 figures
MSC-class: 91A12, 68T05, 91B32
ACM-class: I.2.6; I.3.3; I.2.0; J.5; J.7
\\ ( https://arxiv.org/abs/2403.09700 ,  6954kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14864
replaced with revised version Wed, 27 Mar 2024 09:24:55 GMT   (11936kb,D)

Title: Learning Quadruped Locomotion Using Differentiable Simulation
Authors: Yunlong Song, Sangbae Kim, Davide Scaramuzza
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2403.14864 ,  11936kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15114
replaced with revised version Wed, 27 Mar 2024 12:13:42 GMT   (4377kb,D)

Title: Solving a Real-World Package Delivery Routing Problem Using Quantum
  Annealers
Authors: Eneko Osaba, Esther Villar-Rodriguez and Ant\'on Asla
Categories: cs.ET cs.AI
Comments: 15 pages, 11 figures and 4 tables. Paper submitted for review in
  Scientific Reports
\\ ( https://arxiv.org/abs/2403.15114 ,  4377kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15472
replaced with revised version Wed, 27 Mar 2024 06:22:41 GMT   (1103kb,D)

Title: Enhancing Programming Education with ChatGPT: A Case Study on Student
  Perceptions and Interactions in a Python Course
Authors: Boxaun Ma, Li Chen and Shin'ichi Konomi
Categories: cs.CY cs.AI cs.PL
\\ ( https://arxiv.org/abs/2403.15472 ,  1103kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16915
replaced with revised version Wed, 27 Mar 2024 01:53:36 GMT   (84kb,D)

Title: Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language
  Models
Authors: Atsushi Keyaki and Ribeka Keyaki
Categories: cs.IR cs.AI cs.CL cs.LG
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2403.16915 ,  84kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17219
replaced with revised version Wed, 27 Mar 2024 15:08:31 GMT   (3476kb,D)

Title: SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental
  Health Sensing Studies
Authors: Akshat Choube, Vedant Das Swain, Varun Mishra
Categories: cs.HC cs.AI cs.CY
\\ ( https://arxiv.org/abs/2403.17219 ,  3476kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17421
replaced with revised version Wed, 27 Mar 2024 06:28:53 GMT   (3065kb,D)

Title: MA4DIV: Multi-Agent Reinforcement Learning for Search Result
  Diversification
Authors: Yiqun Chen, Jiaxin Mao, Yi Zhang, Dehong Ma, Long Xia, Jun Fan,
  Daiting Shi, Zhicong Cheng, Simiu Gu, Dawei Yin
Categories: cs.IR cs.AI
\\ ( https://arxiv.org/abs/2403.17421 ,  3065kb)
------------------------------------------------------------------------------
\\
arXiv:2301.10856
replaced with revised version Wed, 27 Mar 2024 00:47:21 GMT   (9286kb,D)

Title: Partial Mobilization: Tracking Multilingual Information Flows Amongst
  Russian Media Outlets and Telegram
Authors: Hans W. A. Hanley and Zakir Durumeric
Categories: cs.CY cs.CL cs.LG cs.SI
Comments: Accepted to ICWSM 2024
\\ ( https://arxiv.org/abs/2301.10856 ,  9286kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11138 (*cross-listing*)
replaced with revised version Wed, 27 Mar 2024 00:29:33 GMT   (557kb)

Title: NLP-based detection of systematic anomalies among the narratives of
  consumer complaints
Authors: Peiheng Gao, Ning Sun, Xuefeng Wang, Chen Yang, Ri\v{c}ardas Zitikis
Categories: stat.ME cs.CL q-fin.RM stat.ML
\\ ( https://arxiv.org/abs/2308.11138 ,  557kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12997
replaced with revised version Wed, 27 Mar 2024 13:59:57 GMT   (267kb,D)

Title: Towards Trustworthy Reranking: A Simple yet Effective Abstention
  Mechanism
Authors: Hippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe,
  C\'eline Hudelot, Pierre Colombo
Categories: cs.IR cs.CL
\\ ( https://arxiv.org/abs/2402.12997 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15837
replaced with revised version Wed, 27 Mar 2024 08:54:06 GMT   (3154kb,D)

Title: Centered Masking for Language-Image Pre-Training
Authors: Mingliang Liang, Martha Larson
Categories: cs.CV cs.CL cs.LG
\\ ( https://arxiv.org/abs/2403.15837 ,  3154kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17343
replaced with revised version Wed, 27 Mar 2024 02:49:16 GMT   (2127kb,D)

Title: Language Models are Free Boosters for Biomedical Imaging Tasks
Authors: Zhixin Lai, Jing Wu, Suiyao Chen, Yucheng Zhou, Naira Hovakimyan
Categories: cs.CV cs.CL cs.LG
\\ ( https://arxiv.org/abs/2403.17343 ,  2127kb)
------------------------------------------------------------------------------
\\
arXiv:2306.15328 (*cross-listing*)
replaced with revised version Tue, 26 Mar 2024 18:19:54 GMT   (30kb)

Title: Simulating counterfactuals
Authors: Juha Karvanen, Santtu Tikka, Matti Vihola
Categories: stat.ML cs.CY cs.LG stat.CO
\\ ( https://arxiv.org/abs/2306.15328 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07572 (*cross-listing*)
replaced with revised version Tue, 26 Mar 2024 20:50:44 GMT   (14095kb,D)

Title: High-Rate Phase Association with Travel Time Neural Fields
Authors: Cheng Shi, Maarten V. de Hoop, and Ivan Dokmani\'c
Categories: physics.geo-ph cs.LG eess.SP
\\ ( https://arxiv.org/abs/2307.07572 ,  14095kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02396 (*cross-listing*)
replaced with revised version Tue, 26 Mar 2024 23:17:24 GMT   (1443kb,D)

Title: HOOD: Real-Time Human Presence and Out-of-Distribution Detection Using
  FMCW Radar
Authors: Sabri Mustafa Kahya, Muhammet Sami Yavuz, Eckehard Steinbach
Categories: eess.SP cs.CV cs.LG
Comments: 10 pages, 2 figures, project page: https://muskahya.github.io/HOOD
\\ ( https://arxiv.org/abs/2308.02396 ,  1443kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12882
replaced with revised version Wed, 27 Mar 2024 14:47:41 GMT   (1962kb,D)

Title: LCANets++: Robust Audio Classification using Multi-layer Neural Networks
  with Lateral Competition
Authors: Sayanton V. Dibbo, Juston S. Moore, Garrett T. Kenyon, Michael A. Teti
Categories: cs.SD cs.CR cs.LG eess.AS
Comments: Accepted at 2024 IEEE International Conference on Acoustics, Speech
  and Signal Processing Workshops (ICASSPW)
\\ ( https://arxiv.org/abs/2308.12882 ,  1962kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06075 (*cross-listing*)
replaced with revised version Wed, 27 Mar 2024 09:51:15 GMT   (22838kb,D)

Title: A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel
  Segmentation via Two-Phase Training Angiography-to-Venography Translation
Authors: Francesco Galati, Daniele Falcetta, Rosa Cortese, Barbara Casolla,
  Ferran Prados, Ninon Burgos, Maria A. Zuluaga
Categories: eess.IV cs.CV cs.LG
Comments: Accepted at the 34th British Machine Vision Conference (BMVC)
\\ ( https://arxiv.org/abs/2309.06075 ,  22838kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11798
replaced with revised version Wed, 27 Mar 2024 16:12:18 GMT   (676kb,D)

Title: A Comprehensive Review of Community Detection in Graphs
Authors: Jiakang Li, Songning Lai, Zhihao Shuai, Yuan Tan, Yifan Jia, Mianyang
  Yu, Zichen Song, Xiaokang Peng, Ziyang Xu, Yongxin Ni, Haifeng Qiu, Jiayu
  Yang, Yutong Liu, Yonggang Lu
Categories: cs.SI cs.LG
\\ ( https://arxiv.org/abs/2309.11798 ,  676kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12370
replaced with revised version Wed, 27 Mar 2024 13:44:21 GMT   (167kb,D)

Title: No-Regret Learning in Bilateral Trade via Global Budget Balance
Authors: Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Federico Fusco
Categories: cs.GT cs.LG
Comments: Accepted at STOC 2024
\\ ( https://arxiv.org/abs/2310.12370 ,  167kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10842
replaced with revised version Tue, 26 Mar 2024 19:45:15 GMT   (250kb,D)

Title: Compositional Inductive Invariant Based Verification of Neural Network
  Controlled Systems
Authors: Yuhao Zhou, Stavros Tripakis
Categories: cs.LO cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2312.10842 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17098
replaced with revised version Wed, 27 Mar 2024 00:46:26 GMT   (963kb)

Title: Deep Learning-Driven Approach for Handwritten Chinese Character
  Classification
Authors: Boris Kriuk, Fedor Kriuk
Categories: cs.CV cs.LG
Comments: 30 pages, 9 figures, 2 tables, preprint v2
\\ ( https://arxiv.org/abs/2401.17098 ,  963kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07868 (*cross-listing*)
replaced with revised version Wed, 27 Mar 2024 16:12:43 GMT   (74kb)

Title: Nesting Particle Filters for Experimental Design in Dynamical Systems
Authors: Sahel Iqbal, Adrien Corenflos, Simo S\"arkk\"a, Hany Abdulsamad
Categories: stat.ML cs.LG stat.ME
\\ ( https://arxiv.org/abs/2402.07868 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09267 (*cross-listing*)
replaced with revised version Wed, 27 Mar 2024 11:11:02 GMT   (5403kb,D)

Title: Deep Limit Order Book Forecasting
Authors: Antonio Briola, Silvia Bartolucci, Tomaso Aste
Categories: q-fin.TR cs.LG
Comments: 43 pages, 14 figures, 12 Tables
\\ ( https://arxiv.org/abs/2403.09267 ,  5403kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12820
replaced with revised version Wed, 27 Mar 2024 07:35:47 GMT   (0kb,I)

Title: A Physics-embedded Deep Learning Framework for Cloth Simulation
Authors: Zhiwei Zhao
Categories: cs.GR cs.LG
Comments: A derivation is incomplete, and updations are being processed
ACM-class: I.2.0; I.3.7
\\ ( https://arxiv.org/abs/2403.12820 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16335 (*cross-listing*)
replaced with revised version Tue, 26 Mar 2024 23:29:49 GMT   (3566kb,D)

Title: MEDDAP: Medical Dataset Enhancement via Diversified Augmentation
  Pipeline
Authors: Yasamin Medghalchi, Niloufar Zakariaei, Arman Rahmim, Ilker
  Hacihaliloglu
Categories: eess.IV cs.CV cs.LG
Comments: submitted to miccai 2024 submitted to miccai 2024 Submitted to
  MICCAI-2024
\\ ( https://arxiv.org/abs/2403.16335 ,  3566kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16967
replaced with revised version Tue, 26 Mar 2024 22:00:27 GMT   (28573kb,D)

Title: Visual Whole-Body Control for Legged Loco-Manipulation
Authors: Minghuan Liu, Zixuan Chen, Xuxin Cheng, Yandong Ji, Ruihan Yang,
  Xiaolong Wang
Categories: cs.RO cs.CV cs.LG
Comments: The first two authors contribute equally. Project page:
  https://wholebody-b1.github.io
\\ ( https://arxiv.org/abs/2403.16967 ,  28573kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17458
replaced with revised version Wed, 27 Mar 2024 04:54:59 GMT   (258kb)

Title: Expectations Versus Reality: Evaluating Intrusion Detection Systems in
  Practice
Authors: Jake Hesford, Daniel Cheng, Alan Wan, Larry Huynh, Seungho Kim,
  Hyoungshick Kim, Jin B. Hong
Categories: cs.CR cs.LG
Comments: 10 pages
MSC-class: 68M25, 68M20
ACM-class: C.4; D.m
\\ ( https://arxiv.org/abs/2403.17458 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17767 (*cross-listing*)
replaced with revised version Wed, 27 Mar 2024 08:49:19 GMT   (104kb)

Title: Asymptotic Bayes risk of semi-supervised learning with uncertain
  labeling
Authors: Victor Leger and Romain Couillet
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2403.17767 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17905 (*cross-listing*)
replaced with revised version Wed, 27 Mar 2024 09:07:02 GMT   (5114kb,D)

Title: Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2
Authors: Yiwei Chen, Chao Tang, Amir Aghabiglou, Chung San Chu and Yves Wiaux
Categories: eess.IV cs.CV cs.LG eess.SP
Comments: submitted to IEEE EUSIPCO 2024
\\ ( https://arxiv.org/abs/2403.17905 ,  5114kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
