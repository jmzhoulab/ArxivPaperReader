paper_240313.txt


Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 80021 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月13日 12:30
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Mon 11 Mar 24 18:00:00 GMT  to  Tue 12 Mar 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.06995
Date: Sun, 3 Mar 2024 07:50:29 GMT   (738kb)

Title: Exact algorithms and heuristics for capacitated covering salesman
  problems
Authors: Lucas Porto Maziero, F\'abio Luiz Usberti, Celso Cavellucci
Categories: cs.AI
\\
  This paper introduces the Capacitated Covering Salesman Problem (CCSP),
approaching the notion of service by coverage in capacitated vehicle routing
problems. In CCSP, locations where vehicles can transit are provided, some of
which have customers with demands. The objective is to service customers
through a fleet of vehicles based in a depot, minimizing the total distance
traversed by the vehicles. CCSP is unique in the sense that customers, to be
serviced, do not need to be visited by a vehicle. Instead, they can be serviced
if they are within a coverage area of the vehicle. This assumption is motivated
by applications in which some customers are unreachable (e.g., forbidden access
to vehicles) or visiting every customer is impractical. In this work,
optimization methodologies are proposed for the CCSP based on ILP (Integer
Linear Programming) and BRKGA (Biased Random-Key Genetic Algorithm)
metaheuristic. Computational experiments conducted on a benchmark of instances
for the CCSP evaluate the performance of the methodologies with respect to
primal bounds. Furthermore, our ILP formulation is extended in order to create
a novel MILP (Mixed Integer Linear Programming) for the Multi-Depot Covering
Tour Vehicle Routing Problem (MDCTVRP). Computational experiments show that the
extended MILP formulation outperformed the previous state-of-the-art exact
approach with respect to optimality gaps. In particular, optimal solutions were
obtained for several previously unsolved instances.
\\ ( https://arxiv.org/abs/2403.06995 ,  738kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06996
Date: Sun, 3 Mar 2024 10:38:57 GMT   (27634kb,D)

Title: On the stochastics of human and artificial creativity
Authors: Solve S{\ae}b{\o} and Helge Brovold
Categories: cs.AI
Comments: 40 pages, 1 figure with 2 sub-figures
MSC-class: 34, 37, 60, 62
ACM-class: G.1.7; G.3; I.2; J.4; J.5
\\
  What constitutes human creativity, and is it possible for computers to
exhibit genuine creativity? We argue that achieving human-level intelligence in
computers, or so-called Artificial General Intelligence, necessitates attaining
also human-level creativity. We contribute to this discussion by developing a
statistical representation of human creativity, incorporating prior insights
from stochastic theory, psychology, philosophy, neuroscience, and chaos theory.
This highlights the stochastic nature of the human creative process, which
includes both a bias guided, random proposal step, and an evaluation step
depending on a flexible or transformable bias structure. The acquired
representation of human creativity is subsequently used to assess the
creativity levels of various contemporary AI systems. Our analysis includes
modern AI algorithms such as reinforcement learning, diffusion models, and
large language models, addressing to what extent they measure up to human level
creativity. We conclude that these technologies currently lack the capability
for autonomous creative action at a human level.
\\ ( https://arxiv.org/abs/2403.06996 ,  27634kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07003
Date: Thu, 7 Mar 2024 12:10:19 GMT   (26469kb,D)

Title: Evacuation Management Framework towards Smart City-wide Intelligent
  Emergency Interactive Response System
Authors: Anuj Abraham and Yi Zhang and Shitala Prasad
Categories: cs.AI cs.CY cs.LG cs.NI
\\
  A smart city solution toward future 6G network deployment allows small and
medium sized enterprises (SMEs), industry, and government entities to connect
with the infrastructures and play a crucial role in enhancing emergency
preparedness with advanced sensors. The objective of this work is to propose a
set of coordinated technological solutions to transform an existing emergency
response system into an intelligent interactive system, thereby improving the
public services and the quality of life for residents at home, on road, in
hospitals, transport hubs, etc. In this context, we consider a city wide view
from three different application scenes that are closely related to peoples
daily life, to optimize the actions taken at relevant departments. Therefore,
using artificial intelligence (AI) and machine learning (ML) techniques to
enable the next generation connected vehicle experiences, we specifically focus
on accidents happening in indoor households, urban roads, and at large public
facilities. This smart interactive response system will benefit from advanced
sensor fusion and AI by formulating a real time dynamic model.
\\ ( https://arxiv.org/abs/2403.07003 ,  26469kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07004
Date: Thu, 7 Mar 2024 13:14:21 GMT   (129kb)

Title: Convergence of Some Convex Message Passing Algorithms to a Fixed Point
Authors: Vaclav Voracek, Tomas Werner
Categories: cs.AI cs.LG math.OC stat.ML
\\
  A popular approach to the MAP inference problem in graphical models is to
minimize an upper bound obtained from a dual linear programming or Lagrangian
relaxation by (block-)coordinate descent. Examples of such algorithms are
max-sum diffusion and sequential tree-reweighted message passing. Convergence
properties of these methods are currently not fully understood. They have been
proved to converge to the set characterized by local consistency of active
constraints, with unknown convergence rate; however, it was not clear if the
iterates converge at all (to any single point). We prove a stronger result
(which was conjectured before but never proved): the iterates converge to a
fixed point of the algorithm. Moreover, we show that they achieve precision
$\varepsilon>0$ in $\mathcal{O}(1/\varepsilon)$ iterations.
  We first prove this for a version of coordinate descent applied to a general
piecewise-affine convex objective, using a novel proof technique. Then we
demonstrate the generality of this approach by reducing some popular
coordinate-descent algorithms to this problem. Finally we show that, in
contrast to our main result, a similar version of coordinate descent applied to
a constrained optimization problem need not converge.
\\ ( https://arxiv.org/abs/2403.07004 ,  129kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07005
Date: Fri, 8 Mar 2024 06:38:22 GMT   (2049kb,D)

Title: Multi-Agent Reinforcement Learning with a Hierarchy of Reward Machines
Authors: Xuejing Zheng, Chao Yu
Categories: cs.AI cs.LG cs.MA
\\
  In this paper, we study the cooperative Multi-Agent Reinforcement Learning
(MARL) problems using Reward Machines (RMs) to specify the reward functions
such that the prior knowledge of high-level events in a task can be leveraged
to facilitate the learning efficiency. Unlike the existing work that RMs have
been incorporated into MARL for task decomposition and policy learning in
relatively simple domains or with an assumption of independencies among the
agents, we present Multi-Agent Reinforcement Learning with a Hierarchy of RMs
(MAHRM) that is capable of dealing with more complex scenarios when the events
among agents can occur concurrently and the agents are highly interdependent.
  MAHRM exploits the relationship of high-level events to decompose a task into
a hierarchy of simpler subtasks that are assigned to a small group of agents,
so as to reduce the overall computational complexity.
  Experimental results in three cooperative MARL domains show that MAHRM
outperforms other MARL methods using the same prior knowledge of high-level
events.
\\ ( https://arxiv.org/abs/2403.07005 ,  2049kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07010
Date: Sat, 9 Mar 2024 04:19:50 GMT   (767kb)

Title: On Globular T-Spherical Fuzzy (G-TSF) Sets with Application to G-TSF
  Multi-Criteria Group Decision-Making
Authors: Miin-Shen Yang, Yasir Akhtar, Mehboob Ali
Categories: cs.AI
\\
  In this paper, we give the concept of Globular T-Spherical Fuzzy (G-TSF) Sets
(G-TSFSs) as an innovative extension of T-Spherical Fuzzy Sets (TSFSs) and
Circular Spherical Fuzzy Sets (C-SFSs). G-TSFSs represent membership,
indeterminacy, and non-membership degrees using a globular/sphere bound that
can offer a more accurate portrayal of vague, ambiguous, and imprecise
information. By employing a structured representation of data points on a
sphere with a specific center and radius, this model enhances decision-making
processes by enabling a more comprehensive evaluation of objects within a
flexible region. Following the newly defined G-TSFSs, we establish some basic
set operations and introduce fundamental algebraic operations for G-TSF Values
(G-TSFVs). These operations expand the evaluative capabilities of
decision-makers, facilitating more sensitive decision-making processes in a
broader region. To quantify a similarity measure (SM) between GTSFVs, the SM is
defined based on the radius of G-TSFSs. Additionally, Hamming distance and
Euclidean distance are introduced for G-TSFSs. We also present theorems and
examples to elucidate computational mechanisms. Furthermore, we give the G-TSF
Weighted Average (G-TSFWA) and G-TSF Weighted Geometric (G-TSFWG) operators.
Leveraging our proposed SM, a Multi-Criteria Group Decision-Making (MCGDM)
scheme for G-TSFSs, named G-TSF MCGDM (G-TSFMCGDM), is developed to address
group decision-making problems. The applicability and effectiveness of the
proposed G-TSFMCGDM method are demonstrated by applying it to solve the
selection problem of the best venue for professional development training
sessions in a firm. The analysis results affirm the suitability and utility of
the proposed method for resolving MCGDM problems, establishing its
effectiveness in practical decision-making scenarios.
\\ ( https://arxiv.org/abs/2403.07010 ,  767kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07131
Date: Mon, 11 Mar 2024 19:55:08 GMT   (881kb,D)

Title: Bigraph Matching Weighted with Learnt Incentive Function for Multi-Robot
  Task Allocation
Authors: Steve Paul, Nathan Maurer, Souma Chowdhury
Categories: cs.AI cs.MA
Comments: This paper was accepted for presentation in proceedings of IEEE
  International Conference on Robotics and Automation 2024
\\
  Most real-world Multi-Robot Task Allocation (MRTA) problems require fast and
efficient decision-making, which is often achieved using heuristics-aided
methods such as genetic algorithms, auction-based methods, and bipartite graph
matching methods. These methods often assume a form that lends better
explainability compared to an end-to-end (learnt) neural network based policy
for MRTA. However, deriving suitable heuristics can be tedious, risky and in
some cases impractical if problems are too complex. This raises the question:
can these heuristics be learned? To this end, this paper particularly develops
a Graph Reinforcement Learning (GRL) framework to learn the heuristics or
incentives for a bipartite graph matching approach to MRTA. Specifically a
Capsule Attention policy model is used to learn how to weight task/robot
pairings (edges) in the bipartite graph that connects the set of tasks to the
set of robots. The original capsule attention network architecture is
fundamentally modified by adding encoding of robots' state graph, and two
Multihead Attention based decoders whose output are used to construct a
LogNormal distribution matrix from which positive bigraph weights can be drawn.
The performance of this new bigraph matching approach augmented with a
GRL-derived incentive is found to be at par with the original bigraph matching
approach that used expert-specified heuristics, with the former offering
notable robustness benefits. During training, the learned incentive policy is
found to get initially closer to the expert-specified incentive and then
slightly deviate from its trend.
\\ ( https://arxiv.org/abs/2403.07131 ,  881kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07363
Date: Tue, 12 Mar 2024 06:52:24 GMT   (860kb)

Title: A New Random Forest Ensemble of Intuitionistic Fuzzy Decision Trees
Authors: Yingtao Ren, Xiaomin Zhu, Kaiyuan Bai, Runtong Zhang
Categories: cs.AI
Journal-ref: IEEE Transactions on Fuzzy Systems 31.5 (2023): 1729-1741
DOI: 10.1109/TFUZZ.2022.3215725
\\
  Classification is essential to the applications in the field of data mining,
artificial intelligence, and fault detection. There exists a strong need in
developing accurate, suitable, and efficient classification methods and
algorithms with broad applicability. Random forest is a general algorithm that
is often used for classification under complex conditions. Although it has been
widely adopted, its combination with diverse fuzzy theory is still worth
exploring. In this paper, we propose the intuitionistic fuzzy random forest
(IFRF), a new random forest ensemble of intuitionistic fuzzy decision trees
(IFDT). Such trees in forest use intuitionistic fuzzy information gain to
select features and consider hesitation in information transmission. The
proposed method enjoys the power of the randomness from bootstrapped sampling
and feature selection, the flexibility of fuzzy logic and fuzzy sets, and the
robustness of multiple classifier systems. Extensive experiments demonstrate
that the IFRF has competitative and superior performance compared to other
state-of-the-art fuzzy and ensemble algorithms. IFDT is more suitable for
ensemble learning with outstanding classification accuracy. This study is the
first to propose a random forest ensemble based on the intuitionistic fuzzy
theory.
\\ ( https://arxiv.org/abs/2403.07363 ,  860kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07510
Date: Tue, 12 Mar 2024 10:45:45 GMT   (490kb,D)

Title: Relevance Score: A Landmark-Like Heuristic for Planning
Authors: Oliver Kim and Mohan Sridharan
Categories: cs.AI
Comments: 12 Pages, 3 figures
ACM-class: I.2.8
\\
  Landmarks are facts or actions that appear in all valid solutions of a
planning problem. They have been used successfully to calculate heuristics that
guide the search for a plan. We investigate an extension to this concept by
defining a novel "relevance score" that helps identify facts or actions that
appear in most but not all plans to achieve any given goal. We describe an
approach to compute this relevance score and use it as a heuristic in the
search for a plan. We experimentally compare the performance of our approach
with that of a state of the art landmark-based heuristic planning approach
using benchmark planning problems. While the original landmark-based heuristic
leads to better performance on problems with well-defined landmarks, our
approach substantially improves performance on problems that lack non-trivial
landmarks.
\\ ( https://arxiv.org/abs/2403.07510 ,  490kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07548
Date: Tue, 12 Mar 2024 11:33:48 GMT   (12912kb,D)

Title: Online Continual Learning For Interactive Instruction Following Agents
Authors: Byeonghwi Kim, Minhyuk Seo, Jonghyun Choi
Categories: cs.AI cs.LG cs.RO
Comments: ICLR 2024 (Project page:
  $\href{https://bhkim94.github.io/projects/CL-ALFRED>}{\text{https}}$)
\\
  In learning an embodied agent executing daily tasks via language directives,
the literature largely assumes that the agent learns all training data at the
beginning. We argue that such a learning scenario is less realistic since a
robotic agent is supposed to learn the world continuously as it explores and
perceives it. To take a step towards a more realistic embodied agent learning
scenario, we propose two continual learning setups for embodied agents;
learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new
environments (Environment Incremental Learning, Environment-IL) For the tasks,
previous 'data prior' based continual learning methods maintain logits for the
past tasks. However, the stored information is often insufficiently learned
information and requires task boundary information, which might not always be
available. Here, we propose to update them based on confidence scores without
task boundary information during training (i.e., task-free) in a moving average
fashion, named Confidence-Aware Moving Average (CAMA). In the proposed
Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior state
of the art in our empirical validations by noticeable margins. The project page
including codes is https://github.com/snumprlab/cl-alfred.
\\ ( https://arxiv.org/abs/2403.07548 ,  12912kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07566
Date: Tue, 12 Mar 2024 11:53:00 GMT   (298kb,D)

Title: An Improved Strategy for Blood Glucose Control Using Multi-Step Deep
  Reinforcement Learning
Authors: Weiwei Gu and Senquan Wang
Categories: cs.AI
\\
  Blood Glucose (BG) control involves keeping an individual's BG within a
healthy range through extracorporeal insulin injections is an important task
for people with type 1 diabetes. However,traditional patient self-management is
cumbersome and risky. Recent research has been devoted to exploring
individualized and automated BG control approaches, among which Deep
Reinforcement Learning (DRL) shows potential as an emerging approach. In this
paper, we use an exponential decay model of drug concentration to convert the
formalization of the BG control problem, which takes into account the delay and
prolongedness of drug effects, from a PAE-POMDP (Prolonged Action
Effect-Partially Observable Markov Decision Process) to a MDP, and we propose a
novel multi-step DRL-based algorithm to solve the problem. The Prioritized
Experience Replay (PER) sampling method is also used in it. Compared to
single-step bootstrapped updates, multi-step learning is more efficient and
reduces the influence from biasing targets. Our proposed method converges
faster and achieves higher cumulative rewards compared to the benchmark in the
same training environment, and improves the time-in-range (TIR), the percentage
of time the patient's BG is within the target range, in the evaluation phase.
Our work validates the effectiveness of multi-step reinforcement learning in BG
control, which may help to explore the optimal glycemic control measure and
improve the survival of diabetic patients.
\\ ( https://arxiv.org/abs/2403.07566 ,  298kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07587
Date: Tue, 12 Mar 2024 12:18:20 GMT   (2171kb,D)

Title: Perennial Semantic Data Terms of Use for Decentralized Web
Authors: Rui Zhao, Jun Zhao
Categories: cs.AI cs.CY cs.LO
Comments: This paper is accepted by International World Wide Web Conference
  2024 (WWW 2024 / The Web Conf 2024)
DOI: 10.1145/3589334.3645631
\\
  In today's digital landscape, the Web has become increasingly centralized,
raising concerns about user privacy violations. Decentralized Web
architectures, such as Solid, offer a promising solution by empowering users
with better control over their data in their personal `Pods'. However, a
significant challenge remains: users must navigate numerous applications to
decide which application can be trusted with access to their data Pods. This
often involves reading lengthy and complex Terms of Use agreements, a process
that users often find daunting or simply ignore. This compromises user autonomy
and impedes detection of data misuse. We propose a novel formal description of
Data Terms of Use (DToU), along with a DToU reasoner. Users and applications
specify their own parts of the DToU policy with local knowledge, covering
permissions, requirements, prohibitions and obligations. Automated reasoning
verifies compliance, and also derives policies for output data. This
constitutes a ``perennial'' DToU language, where the policy authoring only
occurs once, and we can conduct ongoing automated checks across users,
applications and activity cycles. Our solution is built on Turtle, Notation 3
and RDF Surfaces, for the language and the reasoning engine. It ensures
seamless integration with other semantic tools for enhanced interoperability.
We have successfully integrated this language into the Solid framework, and
conducted performance benchmark. We believe this work demonstrates a
practicality of a perennial DToU language and the potential of a paradigm shift
to how users interact with data and applications in a decentralized Web,
offering both improved privacy and usability.
\\ ( https://arxiv.org/abs/2403.07587 ,  2171kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07769
Date: Tue, 12 Mar 2024 15:56:10 GMT   (558kb)

Title: Transforming Competition into Collaboration: The Revolutionary Role of
  Multi-Agent Systems and Language Models in Modern Organizations
Authors: Carlos Jose Xavier Cruz
Categories: cs.AI cs.CL cs.CY cs.MA
\\
  This article explores the dynamic influence of computational entities based
on multi-agent systems theory (SMA) combined with large language models (LLM),
which are characterized by their ability to simulate complex human
interactions, as a possibility to revolutionize human user interaction from the
use of specialized artificial agents to support everything from operational
organizational processes to strategic decision making based on applied
knowledge and human orchestration. Previous investigations reveal that there
are limitations, particularly in the autonomous approach of artificial agents,
especially when dealing with new challenges and pragmatic tasks such as
inducing logical reasoning and problem solving. It is also considered that
traditional techniques, such as the stimulation of chains of thoughts, require
explicit human guidance. In our approach we employ agents developed from large
language models (LLM), each with distinct prototyping that considers behavioral
elements, driven by strategies that stimulate the generation of knowledge based
on the use case proposed in the scenario (role-play) business, using a
discussion approach between agents (guided conversation). We demonstrate the
potential of developing agents useful for organizational strategies, based on
multi-agent system theories (SMA) and innovative uses based on large language
models (LLM based), offering a differentiated and adaptable experiment to
different applications, complexities, domains, and capabilities from LLM.
\\ ( https://arxiv.org/abs/2403.07769 ,  558kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07087
Date: Mon, 11 Mar 2024 18:25:01 GMT   (814kb)

Title: LSTM-Based Text Generation: A Study on Historical Datasets
Authors: Mustafa Abbas Hussein Hussein, Serkan Sava\c{s}
Categories: cs.CL cs.AI
Report-no: ISBN: 978-625-6879-50-8
Journal-ref: 16th International Istanbul Scientific Research Congress on Life,
  Engineering, Architecture, and Mathematical Sciences Proceedings Book, Pages:
  42-49, 2024
DOI: 10.5281/zenodo.10776102
\\
  This paper presents an exploration of Long Short-Term Memory (LSTM) networks
in the realm of text generation, focusing on the utilization of historical
datasets for Shakespeare and Nietzsche. LSTMs, known for their effectiveness in
handling sequential data, are applied here to model complex language patterns
and structures inherent in historical texts. The study demonstrates that
LSTM-based models, when trained on historical datasets, can not only generate
text that is linguistically rich and contextually relevant but also provide
insights into the evolution of language patterns over time. The finding
presents models that are highly accurate and efficient in predicting text from
works of Nietzsche, with low loss values and a training time of 100 iterations.
The accuracy of the model is 0.9521, indicating high accuracy. The loss of the
model is 0.2518, indicating its effectiveness. The accuracy of the model in
predicting text from the work of Shakespeare is 0.9125, indicating a low error
rate. The training time of the model is 100, mirroring the efficiency of the
Nietzsche dataset. This efficiency demonstrates the effectiveness of the model
design and training methodology, especially when handling complex literary
texts. This research contributes to the field of natural language processing by
showcasing the versatility of LSTM networks in text generation and offering a
pathway for future explorations in historical linguistics and beyond.
\\ ( https://arxiv.org/abs/2403.07087 ,  814kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07088
Date: Mon, 11 Mar 2024 18:26:02 GMT   (1663kb,D)

Title: SPA: Towards A Computational Friendly Cloud-Base and On-Devices
  Collaboration Seq2seq Personalized Generation
Authors: Yanming Liu, Xinyue Peng, Jiannan Cao, Le Dai, Xingzu Liu, Weihao Liu,
  Mingbang Wang
Categories: cs.CL
Comments: 11 pages, first version of SPA(Side Plugin Adaption)
\\
  Large language models(LLMs) have shown its outperforming ability on various
tasks and question answering. However, LLMs require high computation cost and
large memory cost. At the same time, LLMs may cause privacy leakage when
training or prediction procedure contains sensitive information. In this paper,
we propose SPA(Side Plugin Adaption), a lightweight architecture for fast
on-devices inference and privacy retaining on the constraints of strict
on-devices computation and memory constraints. Compared with other on-devices
seq2seq generation, SPA could make a fast and stable inference on low-resource
constraints, allowing it to obtain cost effiency. Our method establish an
interaction between a pretrained LLMs on-cloud and additive parameters
on-devices, which could provide the knowledge on both pretrained LLMs and
private personal feature.Further more, SPA provides a framework to keep
feature-base parameters on private guaranteed but low computational devices
while leave the parameters containing general information on the high
computational devices.
\\ ( https://arxiv.org/abs/2403.07088 ,  1663kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07118
Date: Mon, 11 Mar 2024 19:19:59 GMT   (1331kb,D)

Title: Narrating Causal Graphs with Large Language Models
Authors: Atharva Phatak, Vijay K. Mago, Ameeta Agrawal, Aravind Inbasekaran,
  Philippe J. Giabbanelli
Categories: cs.CL
Comments: HICSS '24
\\
  The use of generative AI to create text descriptions from graphs has mostly
focused on knowledge graphs, which connect concepts using facts. In this work
we explore the capability of large pretrained language models to generate text
from causal graphs, where salient concepts are represented as nodes and
causality is represented via directed, typed edges. The causal reasoning
encoded in these graphs can support applications as diverse as healthcare or
marketing. Using two publicly available causal graph datasets, we empirically
investigate the performance of four GPT-3 models under various settings. Our
results indicate that while causal text descriptions improve with training
data, compared to fact-based graphs, they are harder to generate under
zero-shot settings. Results further suggest that users of generative AI can
deploy future applications faster since similar performances are obtained when
training a model with only a few examples as compared to fine-tuning via a
large curated dataset.
\\ ( https://arxiv.org/abs/2403.07118 ,  1331kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07144
Date: Mon, 11 Mar 2024 20:28:27 GMT   (1928kb,D)

Title: Thought Graph: Generating Thought Process for Biological Reasoning
Authors: Chi-Yang Hsu, Kyle Cox, Jiawei Xu, Zhen Tan, Tianhua Zhai, Mengzhou
  Hu, Dexter Pratt, Tianlong Chen, Ziniu Hu, Ying Ding
Categories: cs.CL
Comments: 4 pages. Accepted by Web Conf 2024
DOI: 10.1145/3589335.3651572
\\
  We present the Thought Graph as a novel framework to support complex
reasoning and use gene set analysis as an example to uncover semantic
relationships between biological processes. Our framework stands out for its
ability to provide a deeper understanding of gene sets, significantly
surpassing GSEA by 40.28% and LLM baselines by 5.38% based on cosine similarity
to human annotations. Our analysis further provides insights into future
directions of biological processes naming, and implications for bioinformatics
and precision medicine.
\\ ( https://arxiv.org/abs/2403.07144 ,  1928kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07175
Date: Mon, 11 Mar 2024 21:33:05 GMT   (571kb,D)

Title: Rebuilding ROME : Resolving Model Collapse during Sequential Model
  Editing
Authors: Akshat Gupta, Gopala Anumanchipalli
Categories: cs.CL cs.AI
\\
  Recent work on model editing using Rank-One Model Editing (ROME), a popular
model editing method, has shown that there are certain facts that the algorithm
is unable to edit without breaking the model. Such edits have previously been
called disabling edits. These disabling edits cause immediate model collapse
and limits the use of ROME for sequential editing. In this paper, we make two
main contributions. Firstly, we show that model collapse with ROME only happens
when making edits using the CounterFact dataset and does not happen when using
the zsRE dataset. Secondly, we find that disabling edits are an artifact of the
original implementation of ROME. With this paper, we provide a more stable
implementation ROME, which we call r-ROME and show that we no longer observe
model collapse when making large scale sequential edits with ROME.
\\ ( https://arxiv.org/abs/2403.07175 ,  571kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07183
Date: Mon, 11 Mar 2024 21:51:39 GMT   (570kb,D)

Title: Monitoring AI-Modified Content at Scale: A Case Study on the Impact of
  ChatGPT on AI Conference Peer Reviews
Authors: Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao,
  Xuandong Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, Daniel A.
  McFarland, James Y. Zou
Categories: cs.CL cs.AI cs.LG cs.SI
Comments: 42 pages, 30 figures
ACM-class: I.2.7
\\
  We present an approach for estimating the fraction of text in a large corpus
which is likely to be substantially modified or produced by a large language
model (LLM). Our maximum likelihood model leverages expert-written and
AI-generated reference texts to accurately and efficiently examine real-world
LLM-use at the corpus level. We apply this approach to a case study of
scientific peer review in AI conferences that took place after the release of
ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest
that between 6.5% and 16.9% of text submitted as peer reviews to these
conferences could have been substantially modified by LLMs, i.e. beyond
spell-checking or minor writing updates. The circumstances in which generated
text occurs offer insight into user behavior: the estimated fraction of
LLM-generated text is higher in reviews which report lower confidence, were
submitted close to the deadline, and from reviewers who are less likely to
respond to author rebuttals. We also observe corpus-level trends in generated
text which may be too subtle to detect at the individual level, and discuss the
implications of such trends on peer review. We call for future
interdisciplinary work to examine how LLM use is changing our information and
knowledge practices.
\\ ( https://arxiv.org/abs/2403.07183 ,  570kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07193
Date: Mon, 11 Mar 2024 22:27:16 GMT   (873kb)

Title: CuentosIE: can a chatbot about "tales with a message" help to teach
  emotional intelligence?
Authors: Antonio Ferr\'andez, Roc\'io Lavigne-Cerv\'an, Jes\'us Peral, Ignasi
  Navarro-Soria, \'Angel Lloret, David Gil, Carmen Rocamora
Categories: cs.CL cs.AI
Comments: 26 pages
ACM-class: I.2.0
Journal-ref: PeerJ Computer Science, Volume 10, February 2024, ID e1866
DOI: 10.7717/peerj-cs.1866
\\
  In this article, we present CuentosIE (TalesEI: chatbot of tales with a
message to develop Emotional Intelligence), an educational chatbot on emotions
that also provides teachers and psychologists with a tool to monitor their
students/patients through indicators and data compiled by CuentosIE. The use of
"tales with a message" is justified by their simplicity and easy understanding,
thanks to their moral or associated metaphors. The main contributions of
CuentosIE are the selection, collection, and classification of a set of highly
specialized tales, as well as the provision of tools (searching, reading
comprehension, chatting, recommending, and classifying) that are useful for
both educating users about emotions and monitoring their emotional development.
The preliminary evaluation of the tool has obtained encouraging results, which
provides an affirmative answer to the question posed in the title of the
article.
\\ ( https://arxiv.org/abs/2403.07193 ,  873kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07202
Date: Mon, 11 Mar 2024 22:58:58 GMT   (444kb,D)

Title: SPAWNing Structural Priming Predictions from a Cognitively Motivated
  Parser
Authors: Grusha Prasad and Tal Linzen
Categories: cs.CL
\\
  Structural priming is a widely used psycholinguistic paradigm to study human
sentence representations. In this work we propose a framework for using
empirical priming patterns to build a theory characterizing the structural
representations humans construct when processing sentences. This framework uses
a new cognitively motivated parser, SPAWN, to generate quantitative priming
predictions from theoretical syntax and evaluate these predictions with
empirical human behavior. As a case study, we apply this framework to study
reduced relative clause representations in English. We use SPAWN to generate
priming predictions from two theoretical accounts which make different
assumptions about the structure of relative clauses. We find that the
predictions from only one of these theories (Participial-Phase) align with
empirical priming patterns, thus highlighting which assumptions about relative
clause better capture human sentence representations.
\\ ( https://arxiv.org/abs/2403.07202 ,  444kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07230
Date: Tue, 12 Mar 2024 00:58:19 GMT   (821kb,D)

Title: Curry-DPO: Enhancing Alignment using Curriculum Learning & Ranked
  Preferences
Authors: Pulkit Pattnaik and Rishabh Maheshwary and Kelechi Ogueji and Vikas
  Yadav and Sathwik Tejaswi Madhusudhan
Categories: cs.CL cs.AI cs.LG
Comments: Work in progress
\\
  Direct Preference Optimization (DPO) is an effective technique that leverages
pairwise preference data (usually one chosen and rejected response pair per
user prompt) to align LLMs to human preferences. In practice, multiple
responses can exist for a given prompt with varying quality relative to each
other. With availability of such quality ratings for multiple responses, we
propose utilizing these responses to create multiple preference pairs for a
given prompt. Our work focuses on systematically using the constructed multiple
preference pair in DPO training via curriculum learning methodology. In
particular, we order these multiple pairs of preference data from easy to hard
(emulating curriculum training) according to various criteria. We show detailed
comparisons of our proposed approach to the standard single-pair DPO setting.
Our method, which we call Curry-DPO consistently shows increased performance
gains on MTbench, Vicuna, WizardLM, and the UltraFeedback test set,
highlighting its effectiveness. More specifically, Curry-DPO achieves a score
of 7.43 on MT-bench with Zephy-7B model outperforming majority of existing LLMs
with similar parameter size. Curry-DPO also achieves the highest adjusted win
rates on Vicuna, WizardLM, and UltraFeedback test datasets (90.7%, 87.1%, and
87.9% respectively) in our experiments, with notable gains of upto 7.5% when
compared to standard DPO technique.
\\ ( https://arxiv.org/abs/2403.07230 ,  821kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07260
Date: Tue, 12 Mar 2024 02:37:11 GMT   (784kb,D)

Title: CKERC : Joint Large Language Models with Commonsense Knowledge for
  Emotion Recognition in Conversation
Authors: Yumeng Fu
Categories: cs.CL
\\
  Emotion recognition in conversation (ERC) is a task which predicts the
emotion of an utterance in the context of a conversation. It tightly depends on
dialogue context, speaker identity information, multiparty dialogue scenario
and so on. However, the state-of-the-art method (instructERC) solely
identifying speaker, and ignores commonsense knowledge(i.e., reaction of the
listeners and intention of the speaker, etc.) behind speakers during a
conversation, which can deeply mine speaker information. To this end, we
propose a novel joint large language models with commonsense knowledge
framework for emotion recognition in conversation, namely CKERC.We design
prompts to generate interlocutors' commonsense based on historical utterances
with large language model. And we use the interlocutor commonsense
identification task for LLM pre-training to fine-tune speaker implicit clues
information.By solving above challenge, our method achieve state-of-the-art.We
extensive experiment on three widely-used datasets, i.e., IEMOCAP, MELD,
EmoryNLP, demonstrate our method superiority. Also, we conduct in-depth
analysis and further demonstrate the effectiveness of commonsense knowledge in
ERC task in large language model.
\\ ( https://arxiv.org/abs/2403.07260 ,  784kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07279
Date: Tue, 12 Mar 2024 03:17:59 GMT   (3097kb,D)

Title: A Survey of Explainable Knowledge Tracing
Authors: Yanhong Bai, Jiabao Zhao, Tingjiang Wei, Qing Cai, Liang He
Categories: cs.CL
\\
  With the long term accumulation of high quality educational data, artificial
intelligence has shown excellent performance in knowledge tracing. However, due
to the lack of interpretability and transparency of some algorithms, this
approach will result in reduced stakeholder trust and a decreased acceptance of
intelligent decisions. Therefore, algorithms need to achieve high accuracy, and
users need to understand the internal operating mechanism and provide reliable
explanations for decisions. This paper thoroughly analyzes the interpretability
of KT algorithms. First, the concepts and common methods of explainable
artificial intelligence and knowledge tracing are introduced. Next, explainable
knowledge tracing models are classified into two categories: transparent models
and black box models. Then, the interpretable methods used are reviewed from
three stages: ante hoc interpretable methods, post hoc interpretable methods,
and other dimensions. It is worth noting that current evaluation methods for
explainable knowledge tracing are lacking. Hence, contrast and deletion
experiments are conducted to explain the prediction results of the deep
knowledge tracing model on the ASSISTment2009 by using three XAI methods.
Moreover, this paper offers some insights into evaluation methods from the
perspective of educational stakeholders. This paper provides a detailed and
comprehensive review of the research on explainable knowledge tracing, aiming
to offer some basis and inspiration for researchers interested in the
interpretability of knowledge tracing.
\\ ( https://arxiv.org/abs/2403.07279 ,  3097kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07311
Date: Tue, 12 Mar 2024 04:47:29 GMT   (897kb,D)

Title: Knowledge Graph Large Language Model (KG-LLM) for Link Prediction
Authors: Dong Shu, Tianle Chen, Mingyu Jin, Yiting Zhang, Mengnan Du, Yongfeng
  Zhang
Categories: cs.CL cs.LG
Comments: 24 pages, 3 figures, submit to ECML PKDD 2024
\\
  The task of predicting multiple links within knowledge graphs (KGs) stands as
a challenge in the field of knowledge graph analysis, a challenge increasingly
resolvable due to advancements in natural language processing (NLP) and KG
embedding techniques. This paper introduces a novel methodology, the Knowledge
Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP
paradigms, including chain-of-thought (CoT) prompting and in-context learning
(ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a
CoT prompt, our framework is designed to discern and learn the latent
representations of entities and their interrelations. To show the efficacy of
the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs)
within this framework, employing both non-ICL and ICL tasks for a comprehensive
evaluation. Further, we explore the framework's potential to provide LLMs with
zero-shot capabilities for handling previously unseen prompts. Our experimental
findings discover that integrating ICL and CoT not only augments the
performance of our approach but also significantly boosts the models'
generalization capacity, thereby ensuring more precise predictions in
unfamiliar scenarios.
\\ ( https://arxiv.org/abs/2403.07311 ,  897kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07321
Date: Tue, 12 Mar 2024 05:15:21 GMT   (363kb,D)

Title: GPT-generated Text Detection: Benchmark Dataset and Tensor-based
  Detection Method
Authors: Zubair Qazi, William Shiao, and Evangelos E. Papalexakis
Categories: cs.CL
Comments: 4 pages, 2 figures, published in the WWW 2024 Short Papers Track
\\
  As natural language models like ChatGPT become increasingly prevalent in
applications and services, the need for robust and accurate methods to detect
their output is of paramount importance. In this paper, we present GPT Reddit
Dataset (GRiD), a novel Generative Pretrained Transformer (GPT)-generated text
detection dataset designed to assess the performance of detection models in
identifying generated responses from ChatGPT. The dataset consists of a diverse
collection of context-prompt pairs based on Reddit, with human-generated and
ChatGPT-generated responses. We provide an analysis of the dataset's
characteristics, including linguistic diversity, context complexity, and
response quality. To showcase the dataset's utility, we benchmark several
detection methods on it, demonstrating their efficacy in distinguishing between
human and ChatGPT-generated responses. This dataset serves as a resource for
evaluating and advancing detection techniques in the context of ChatGPT and
contributes to the ongoing efforts to ensure responsible and trustworthy
AI-driven communication on the internet. Finally, we propose GpTen, a novel
tensor-based GPT text detection method that is semi-supervised in nature since
it only has access to human-generated text and performs on par with
fully-supervised baselines.
\\ ( https://arxiv.org/abs/2403.07321 ,  363kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07342
Date: Tue, 12 Mar 2024 06:01:04 GMT   (2887kb,D)

Title: Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive
  Learning
Authors: Qiao Sun, Liujia Yang, Minghao Ma, Nanyang Ye, Qinying Gu
Categories: cs.CL cs.AI
\\
  Aspect Sentiment Triplet Extraction (ASTE) is a burgeoning subtask of
fine-grained sentiment analysis, aiming to extract structured sentiment
triplets from unstructured textual data. Existing approaches to ASTE often
complicate the task with additional structures or external data. In this
research, we propose a novel tagging scheme and employ a contrastive learning
approach to mitigate these challenges. The proposed approach demonstrates
comparable or superior performance in comparison to state-of-the-art
techniques, while featuring a more compact design and reduced computational
overhead. Notably, even in the era of Large Language Models (LLMs), our method
exhibits superior efficacy compared to GPT 3.5 and GPT 4 in a few-shot learning
scenarios. This study also provides valuable insights for the advancement of
ASTE techniques within the paradigm of large language models.
\\ ( https://arxiv.org/abs/2403.07342 ,  2887kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07350
Date: Tue, 12 Mar 2024 06:16:33 GMT   (254kb)

Title: KEBench: A Benchmark on Knowledge Editing for Large Vision-Language
  Models
Authors: Han Huang, Haitian Zhong, Qiang Liu, Shu Wu, Liang Wang, Tieniu Tan
Categories: cs.CL cs.AI cs.CV
Comments: 13 pages
\\
  Currently, little research has been done on knowledge editing for Large
Vision-Language Models (LVLMs). Editing LVLMs faces the challenge of
effectively integrating diverse modalities (image and text) while ensuring
coherent and contextually relevant modifications. An existing benchmark has
three metrics (Reliability, Locality and Generality) to measure knowledge
editing for LVLMs. However, the benchmark falls short in the quality of
generated images used in evaluation and cannot assess whether models
effectively utilize edited knowledge in relation to the associated content. We
adopt different data collection methods to construct a new benchmark,
$\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive
evaluation. Leveraging a multimodal knowledge graph, our image data exhibits
clear directionality towards entities. This directional aspect can be further
utilized to extract entity-related knowledge and form editing data. We
conducted experiments of different editing methods on five LVLMs, and
thoroughly analyze how these methods impact the models. The results reveal
strengths and deficiencies of these methods and, hopefully, provide insights
into potential avenues for future research.
\\ ( https://arxiv.org/abs/2403.07350 ,  254kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07378
Date: Tue, 12 Mar 2024 07:31:18 GMT   (318kb,D)

Title: SVD-LLM: Truncation-aware Singular Value Decomposition for Large
  Language Model Compression
Authors: Xin Wang, Yu Zheng, Zhongwei Wan, Mi Zhang
Categories: cs.CL cs.LG
Comments: Under Review
\\
  The advancements in Large Language Models (LLMs) have been hindered by their
substantial sizes, which necessitate LLM compression methods for practical
deployment. Singular Value Decomposition (SVD) offers a promising solution for
LLM compression. However, state-of-the-art SVD-based LLM compression methods
have two key limitations: truncating smaller singular values may lead to higher
compression loss, and the lack of update on the remaining model parameters
after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM
compression method that addresses the limitations of existing methods. SVD-LLM
incorporates a truncation-aware data whitening strategy to ensure a direct
mapping between singular values and compression loss. Moreover, SVD-LLM adopts
a layer-wise closed-form model parameter update strategy to compensate for
accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total
of 11 datasets and seven models from three different LLM families at four
different scales. Our results demonstrate the superiority of SVD-LLM over
state-of-the-arts, especially at high model compression ratios. The source code
is available at https://github.com/AIoT-MLSys-Lab/SVD-LLM.
\\ ( https://arxiv.org/abs/2403.07378 ,  318kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07384
Date: Tue, 12 Mar 2024 07:45:33 GMT   (195kb,D)

Title: SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large
  Language Models by Summarizing Training Trajectories of Small Models
Authors: Yu Yang, Siddhartha Mishra, Jeffrey N Chiang, Baharan Mirzasoleiman
Categories: cs.CL cs.AI cs.LG
\\
  Despite the effectiveness of data selection for large language models (LLMs)
during pretraining and instruction fine-tuning phases, improving data
efficiency in supervised fine-tuning (SFT) for specialized domains poses
significant challenges due to the complexity of fine-tuning data. To bridge
this gap, we introduce an effective and scalable data selection method for SFT,
SmallToLarge (S2L), which leverages training trajectories from small models to
guide the data selection for larger models. We demonstrate through extensive
experiments that S2L significantly improves data efficiency in SFT for
mathematical problem-solving, reducing the training data to just 11% of the
original MathInstruct dataset (Yue et al., 2023) to match full dataset
performance while outperforming state-of-the-art data selection algorithms by
an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably,
selecting only 50K data for SFT, S2L achieves a 32.7% accuracy on the most
challenging MATH (Hendrycks et al., 2021) benchmark, improving Phi-2 (Li et
al., 2023b) by 16.6%. In clinical text summarization on the MIMIC-III dataset
(Johnson et al., 2016), S2L again outperforms training on the full dataset
using only 50% of the data. Notably, S2L can perform data selection using a
reference model 40x smaller than the target model, proportionally reducing the
cost of data selection.
\\ ( https://arxiv.org/abs/2403.07384 ,  195kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07398
Date: Tue, 12 Mar 2024 08:13:52 GMT   (1010kb,D)

Title: Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs
Authors: Tianqing Fang, Zeming Chen, Yangqiu Song, Antoine Bosselut
Categories: cs.CL cs.AI
Comments: 19 pages
\\
  Event commonsense reasoning requires the ability to reason about the
relationship between events, as well as infer implicit context underlying that
relationship. However, data scarcity makes it challenging for language models
to learn to generate commonsense inferences for contexts and questions
involving interactions between complex events. To address this demand, we
present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop
logical queries (e.g., the joint effect or cause of both event A and B, or the
effect of the effect of event C) from an existing commonsense knowledge graph
(CSKG), and verbalizing them using handcrafted rules and large language models
into multiple-choice and text generation questions. Our experiments show that
language models trained on COM2 exhibit significant improvements in complex
reasoning ability, resulting in enhanced zero-shot performance in both
in-domain and out-of-domain tasks for question answering and generative
commonsense reasoning, without expensive human annotations.
\\ ( https://arxiv.org/abs/2403.07398 ,  1010kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07440
Date: Tue, 12 Mar 2024 09:32:25 GMT   (1122kb,D)

Title: Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A
  Brain-Inspired Method for Parameter-Efficient Fine-Tuning
Authors: Yao Liang, Yuwei Wang, Yi Zeng
Categories: cs.CL cs.AI
\\
  Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have
been proven to significantly enhance model performance on a variety of
downstream tasks and effectively control the output behaviors of LPLMs. Recent
studies have proposed numerous methods for fine-tuning a small number of
parameters based on open-source LPLMs, reducing the demand for computational
and storage resources. Among these, reparameterization fine-tuning methods
represented by LoRA (Low-Rank Adaptation) have gained popularity. We find that
although these methods perform well in many aspects, there is still
considerable room for improvement in terms of complex task adaptability,
performance, stability, and algorithm complexity. In response to this, inspired
by the idea that the functions of the brain are shaped by its geometric
structure, this paper integrates this idea into LoRA technology and proposes a
new matrix transformation-based reparameterization method for efficient
fine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA).
MTLoRA aims to dynamically alter its spatial geometric structure by applying a
transformation-matrix T to perform linear transformations, such as rotation,
scaling, and translation, on the task-specific parameter matrix, generating new
matrix feature patterns (eigenvectors) to mimic the fundamental influence of
complex geometric structure feature patterns in the brain on functions, thereby
enhancing the model's performance in downstream tasks. In Natural Language
Understanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and
the results reveal that MTLoRA achieves an overall performance increase of
about 1.0% across eight tasks; in Natural Language Generation (NLG) tasks,
MTLoRA improves performance by an average of 0.95% and 0.31% in the DART and
WebNLG tasks, respectively.
\\ ( https://arxiv.org/abs/2403.07440 ,  1122kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07544
Date: Tue, 12 Mar 2024 11:32:30 GMT   (9135kb,D)

Title: MAMMOTH: Massively Multilingual Modular Open Translation @ Helsinki
Authors: Timothee Mickus, Stig-Arne Gr\"onroos, Joseph Attieh, Michele Boggia,
  Ona De Gibert, Shaoxiong Ji, Niki Andreas Lopi, Alessandro Raganato, Ra\'ul
  V\'azquez, J\"org Tiedemann
Categories: cs.CL
Comments: Presented as a demo at EACL 2024
\\
  NLP in the age of monolithic large language models is approaching its limits
in terms of size and information that can be handled. The trend goes to
modularization, a necessary step into the direction of designing smaller
sub-networks and components with specialized functionality. In this paper, we
present the MAMMOTH toolkit: a framework designed for training massively
multilingual modular machine translation systems at scale, initially derived
from OpenNMT-py and then adapted to ensure efficient training across
computation clusters. We showcase its efficiency across clusters of A100 and
V100 NVIDIA GPUs, and discuss our design philosophy and plans for future
information. The toolkit is publicly available online.
\\ ( https://arxiv.org/abs/2403.07544 ,  9135kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07556
Date: Tue, 12 Mar 2024 11:40:44 GMT   (3847kb,D)

Title: Truth-Aware Context Selection: Mitigating the Hallucinations of Large
  Language Models Being Misled by Untruthful Contexts
Authors: Tian Yu, Shaolei Zhang and Yang Feng
Categories: cs.CL
Comments: Code is available at: https://github.com/ictnlp/TACS
\\
  Although large language models (LLMs) have demonstrated impressive text
generation capabilities, they are easily misled by the untruthful context
provided by users or knowledge argumentation tools, thereby producing
hallucinations. To alleviate the LLMs from being misled by untruthful
information and take advantage of knowledge argumentation, we propose
Truth-Aware Context Selection (TACS), a lightweight method to shield untruthful
context from the inputs. TACS begins by performing truth detection on the input
context, leveraging the parameterized knowledge within the LLM. Subsequently,
it constructs a corresponding attention mask based on the truthfulness of each
position, selecting the truthful context and discarding the untruthful context.
Additionally, we introduce a new evaluation metric, Disturbance Adaption Rate,
to further study the LLMs' ability to accept truthful information and resist
untruthful information. Experimental results show that TACS can effectively
filter information in context and significantly improve the overall quality of
LLMs' responses when presented with misleading information.
\\ ( https://arxiv.org/abs/2403.07556 ,  3847kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07557
Date: Tue, 12 Mar 2024 11:41:51 GMT   (6967kb,D)

Title: SIFiD: Reassess Summary Factual Inconsistency Detection with LLM
Authors: Jiuding Yang, Hui Liu, Weidong Guo, Zhuwei Rao, Yu Xu, Di Niu
Categories: cs.CL cs.LG
\\
  Ensuring factual consistency between the summary and the original document is
paramount in summarization tasks. Consequently, considerable effort has been
dedicated to detecting inconsistencies. With the advent of Large Language
Models (LLMs), recent studies have begun to leverage their advanced language
understanding capabilities for inconsistency detection. However, early attempts
have shown that LLMs underperform traditional models due to their limited
ability to follow instructions and the absence of an effective detection
methodology. In this study, we reassess summary inconsistency detection with
LLMs, comparing the performances of GPT-3.5 and GPT-4. To advance research in
LLM-based inconsistency detection, we propose SIFiD (Summary Inconsistency
Detection with Filtered Document) that identify key sentences within documents
by either employing natural language inference or measuring semantic similarity
between summaries and documents.
\\ ( https://arxiv.org/abs/2403.07557 ,  6967kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07567
Date: Tue, 12 Mar 2024 11:53:27 GMT   (9520kb,D)

Title: Triples-to-isiXhosa (T2X): Addressing the Challenges of Low-Resource
  Agglutinative Data-to-Text Generation
Authors: Francois Meyer and Jan Buys
Categories: cs.CL
\\
  Most data-to-text datasets are for English, so the difficulties of modelling
data-to-text for low-resource languages are largely unexplored. In this paper
we tackle data-to-text for isiXhosa, which is low-resource and agglutinative.
We introduce Triples-to-isiXhosa (T2X), a new dataset based on a subset of
WebNLG, which presents a new linguistic context that shifts modelling demands
to subword-driven techniques. We also develop an evaluation framework for T2X
that measures how accurately generated text describes the data. This enables
future users of T2X to go beyond surface-level metrics in evaluation. On the
modelling side we explore two classes of methods - dedicated data-to-text
models trained from scratch and pretrained language models (PLMs). We propose a
new dedicated architecture aimed at agglutinative data-to-text, the Subword
Segmental Pointer Generator (SSPG). It jointly learns to segment words and copy
entities, and outperforms existing dedicated models for 2 agglutinative
languages (isiXhosa and Finnish). We investigate pretrained solutions for T2X,
which reveals that standard PLMs come up short. Fine-tuning machine translation
models emerges as the best method overall. These findings underscore the
distinct challenge presented by T2X: neither well-established data-to-text
architectures nor customary pretrained methodologies prove optimal. We conclude
with a qualitative analysis of generation errors and an ablation study.
\\ ( https://arxiv.org/abs/2403.07567 ,  9520kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07581
Date: Tue, 12 Mar 2024 12:10:18 GMT   (653kb,D)

Title: LLMvsSmall Model? Large Language Model Based Text Augmentation Enhanced
  Personality Detection Model
Authors: Linmei Hu, Hongyu He, Duokang Wang, Ziwang Zhao, Yingxia Shao, Liqiang
  Nie
Categories: cs.CL
\\
  Personality detection aims to detect one's personality traits underlying in
social media posts. One challenge of this task is the scarcity of ground-truth
personality traits which are collected from self-report questionnaires. Most
existing methods learn post features directly by fine-tuning the pre-trained
language models under the supervision of limited personality labels. This leads
to inferior quality of post features and consequently affects the performance.
In addition, they treat personality traits as one-hot classification labels,
overlooking the semantic information within them. In this paper, we propose a
large language model (LLM) based text augmentation enhanced personality
detection model, which distills the LLM's knowledge to enhance the small model
for personality detection, even when the LLM fails in this task. Specifically,
we enable LLM to generate post analyses (augmentations) from the aspects of
semantic, sentiment, and linguistic, which are critical for personality
detection. By using contrastive learning to pull them together in the embedding
space, the post encoder can better capture the psycho-linguistic information
within the post representations, thus improving personality detection.
Furthermore, we utilize the LLM to enrich the information of personality labels
for enhancing the detection performance. Experimental results on the benchmark
datasets demonstrate that our model outperforms the state-of-the-art methods on
personality detection.
\\ ( https://arxiv.org/abs/2403.07581 ,  653kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07678
Date: Tue, 12 Mar 2024 14:12:59 GMT   (983kb,D)

Title: MoralBERT: Detecting Moral Values in Social Discourse
Authors: Vjosa Preniqi, Iacopo Ghinassi, Kyriaki Kalimeri, Charalampos Saitis
Categories: cs.CL cs.CY
\\
  Morality plays a fundamental role in how we perceive information while
greatly influencing our decisions and judgements. Controversial topics,
including vaccination, abortion, racism, and sexuality, often elicit opinions
and attitudes that are not solely based on evidence but rather reflect moral
worldviews. Recent advances in natural language processing have demonstrated
that moral values can be gauged in human-generated textual content. Here, we
design a range of language representation models fine-tuned to capture exactly
the moral nuances in text, called MoralBERT. We leverage annotated moral data
from three distinct sources: Twitter, Reddit, and Facebook user-generated
content covering various socially relevant topics. This approach broadens
linguistic diversity and potentially enhances the models' ability to comprehend
morality in various contexts. We also explore a domain adaptation technique and
compare it to the standard fine-tuned BERT model, using two different
frameworks for moral prediction: single-label and multi-label. We compare
in-domain approaches with conventional models relying on lexicon-based
techniques, as well as a Machine Learning classifier with Word2Vec
representation. Our results showed that in-domain prediction models
significantly outperformed traditional models. While the single-label setting
reaches a higher accuracy than previously achieved for the task when using BERT
pretrained models. Experiments in an out-of-domain setting, instead, suggest
that further work is needed for existing domain adaptation techniques to
generalise between different social media platforms, especially for the
multi-label task. The investigations and outcomes from this study pave the way
for further exploration, enabling a more profound comprehension of moral
narratives about controversial social issues.
\\ ( https://arxiv.org/abs/2403.07678 ,  983kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07691
Date: Tue, 12 Mar 2024 14:34:08 GMT   (8428kb,D)

Title: Reference-free Monolithic Preference Optimization with Odds Ratio
Authors: Jiwoo Hong, Noah Lee, James Thorne
Categories: cs.CL cs.AI
Comments: Preprint
\\
  While recent preference alignment algorithms for language models have
demonstrated promising results, supervised fine-tuning (SFT) remains imperative
for achieving successful convergence. In this paper, we study the crucial role
of SFT within the context of preference alignment, emphasizing that a minor
penalty for the disfavored generation style is sufficient for
preference-aligned SFT. Building on this foundation, we introduce a
straightforward and innovative reference model-free monolithic odds ratio
preference optimization algorithm, ORPO, eliminating the necessity for an
additional preference alignment phase. We demonstrate, both empirically and
theoretically, that the odds ratio is a sensible choice for contrasting favored
and disfavored styles during SFT across the diverse sizes from 125M to 7B.
Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with
ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art
language models with more than 7B and 13B parameters: achieving up to 12.20% on
$\text{AlpacaEval}_{2.0}$ and 7.32 in MT-Bench, as shown in Figures 1 and 12.
We release code and model checkpoints for Mistral-ORPO-$\alpha$ (7B) and
Mistral-ORPO-$\beta$ (7B).
\\ ( https://arxiv.org/abs/2403.07691 ,  8428kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07693
Date: Tue, 12 Mar 2024 14:37:03 GMT   (1507kb,D)

Title: Large, Small or Both: A Novel Data Augmentation Framework Based on
  Language Models for Debiasing Opinion Summarization
Authors: Yanyue Zhang, Pengfei Li, Yilong Lai and Deyu Zhou
Categories: cs.CL cs.AI
\\
  As more than 70$\%$ of reviews in the existing opinion summary data set are
positive, current opinion summarization approaches are reluctant to generate
negative summaries given the input of negative texts. To address such sentiment
bias, a direct approach without the over-reliance on a specific framework is to
generate additional data based on large language models to balance the
emotional distribution of the dataset. However, data augmentation based on
large language models faces two disadvantages: 1) the potential issues or
toxicity in the augmented data; 2) the expensive costs. Therefore, in this
paper, we propose a novel data augmentation framework based on both large and
small language models for debiasing opinion summarization. In specific, a small
size of synthesized negative reviews is obtained by rewriting the positive text
via a large language model. Then, a disentangle reconstruction model is trained
based on the generated data. After training, a large amount of synthetic data
can be obtained by decoding the new representation obtained from the
combination of different sample representations and filtering based on
confusion degree and sentiment classification. Experiments have proved that our
framework can effectively alleviate emotional bias same as using only large
models, but more economically.
\\ ( https://arxiv.org/abs/2403.07693 ,  1507kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07708
Date: Tue, 12 Mar 2024 14:51:57 GMT   (339kb,D)

Title: Improving Reinforcement Learning from Human Feedback Using Contrastive
  Rewards
Authors: Wei Shen, Xiaoying Zhang, Yuanshun Yao, Rui Zheng, Hongyi Guo, Yang
  Liu
Categories: cs.CL cs.AI
\\
  Reinforcement learning from human feedback (RLHF) is the mainstream paradigm
used to align large language models (LLMs) with human preferences. Yet existing
RLHF heavily relies on accurate and informative reward models, which are
vulnerable and sensitive to noise from various sources, e.g. human labeling
errors, making the pipeline fragile. In this work, we improve the effectiveness
of the reward model by introducing a penalty term on the reward, named as
\textit{contrastive rewards}. %Contrastive rewards Our approach involves two
steps: (1) an offline sampling step to obtain responses to prompts that serve
as baseline calculation and (2) a contrastive reward calculated using the
baseline responses and used in the Proximal Policy Optimization (PPO) step. We
show that contrastive rewards enable the LLM to penalize reward uncertainty,
improve robustness, encourage improvement over baselines, calibrate according
to task difficulty, and reduce variance in PPO. We show empirically contrastive
rewards can improve RLHF substantially, evaluated by both GPTs and humans, and
our method consistently outperforms strong baselines.
\\ ( https://arxiv.org/abs/2403.07708 ,  339kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07714
Date: Tue, 12 Mar 2024 14:57:40 GMT   (765kb,D)

Title: StableToolBench: Towards Stable Large-Scale Benchmarking on Tool
  Learning of Large Language Models
Authors: Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li,
  Zhiyuan Liu, Maosong Sun, Yang Liu
Categories: cs.CL
\\
  Large Language Models (LLMs) have witnessed remarkable advancements in recent
years, prompting the exploration of tool learning, which integrates LLMs with
external tools to address diverse real-world challenges. Assessing the
capability of LLMs to utilise tools necessitates large-scale and stable
benchmarks. However, previous works relied on either hand-crafted online tools
with limited scale, or large-scale real online APIs suffering from instability
of API status. To address this problem, we introduce StableToolBench, a
benchmark evolving from ToolBench, proposing a virtual API server and stable
evaluation system. The virtual API server contains a caching system and API
simulators which are complementary to alleviate the change in API status.
Meanwhile, the stable evaluation system designs solvable pass and win rates
using GPT-4 as the automatic evaluator to eliminate the randomness during
evaluation. Experimental results demonstrate the stability of StableToolBench,
and further discuss the effectiveness of API simulators, the caching system,
and the evaluator system.
\\ ( https://arxiv.org/abs/2403.07714 ,  765kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07726
Date: Tue, 12 Mar 2024 15:06:22 GMT   (8830kb,D)

Title: SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and
  Related Observable Overgeneration Mistakes
Authors: Timothee Mickus, Elaine Zosa, Ra\'ul V\'azquez, Teemu Vahtola, J\"org
  Tiedemann, Vincent Segonne, Alessandro Raganato, Marianna Apidianaki
Categories: cs.CL
Comments: SemEval 2024 shared task. Pre-review version
\\
  This paper presents the results of the SHROOM, a shared task focused on
detecting hallucinations: outputs from natural language generation (NLG)
systems that are fluent, yet inaccurate. Such cases of overgeneration put in
jeopardy many NLG applications, where correctness is often mission-critical.
The shared task was conducted with a newly constructed dataset of 4000 model
outputs labeled by 5 annotators each, spanning 3 NLP tasks: machine
translation, paraphrase generation and definition modeling.
  The shared task was tackled by a total of 58 different users grouped in 42
teams, out of which 27 elected to write a system description paper;
collectively, they submitted over 300 prediction sets on both tracks of the
shared task. We observe a number of key trends in how this approach was tackled
-- many participants rely on a handful of model, and often rely either on
synthetic data for fine-tuning or zero-shot prompting strategies. While a
majority of the teams did outperform our proposed baseline system, the
performances of top-scoring systems are still consistent with a random handling
of the more challenging items.
\\ ( https://arxiv.org/abs/2403.07726 ,  8830kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07747
Date: Tue, 12 Mar 2024 15:32:39 GMT   (1022kb,D)

Title: FineMath: A Fine-Grained Mathematical Evaluation Benchmark for Chinese
  Large Language Models
Authors: Yan Liu, Renren Jin, Lin Shi, Zheng Yao, Deyi Xiong
Categories: cs.CL cs.AI
\\
  To thoroughly assess the mathematical reasoning abilities of Large Language
Models (LLMs), we need to carefully curate evaluation datasets covering diverse
mathematical concepts and mathematical problems at different difficulty levels.
In pursuit of this objective, we propose FineMath in this paper, a fine-grained
mathematical evaluation benchmark dataset for assessing Chinese LLMs. FineMath
is created to cover the major key mathematical concepts taught in elementary
school math, which are further divided into 17 categories of math word
problems, enabling in-depth analysis of mathematical reasoning abilities of
LLMs. All the 17 categories of math word problems are manually annotated with
their difficulty levels according to the number of reasoning steps required to
solve these problems. We conduct extensive experiments on a wide range of LLMs
on FineMath and find that there is still considerable room for improvements in
terms of mathematical reasoning capability of Chinese LLMs. We also carry out
an in-depth analysis on the evaluation process and methods that have been
overlooked previously. These two factors significantly influence the model
results and our understanding of their mathematical reasoning capabilities. The
dataset will be publicly available soon.
\\ ( https://arxiv.org/abs/2403.07747 ,  1022kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07794
Date: Tue, 12 Mar 2024 16:33:30 GMT   (1526kb,D)

Title: Fine-tuning Large Language Models with Sequential Instructions
Authors: Hanxu Hu, Pinzhen Chen, Edoardo M. Ponti
Categories: cs.CL
Comments: 11pages, 3 figures
\\
  Large language models (LLMs) struggle to follow a sequence of instructions in
a single query as they may ignore or misinterpret part of it. This impairs
their performance in complex problems whose solution requires multiple
intermediate steps, such as multilingual (translate then answer) and multimodal
(caption then answer) tasks. We empirically verify this with open-source LLMs
as large as LLaMA-2 70B and Mixtral-8x7B. Targeting the scarcity of sequential
instructions in present-day data, we propose sequential instruction tuning, a
simple yet effective strategy to automatically augment instruction tuning data
and equip LLMs with the ability to execute multiple sequential instructions.
After exploring interleaving instructions in existing datasets, such as Alpaca,
with a wide range of intermediate tasks, we find that sequential
instruction-tuned models consistently outperform the conventional
instruction-tuned baselines in downstream tasks involving reasoning,
multilingual, and multimodal abilities. To shed further light on our technique,
we analyse how adversarial intermediate texts, unseen tasks, prompt
verbalization, number of tasks, and prompt length affect SIT. We hope that this
method will open new research avenues on instruction tuning for complex tasks.
\\ ( https://arxiv.org/abs/2403.07794 ,  1526kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07805
Date: Tue, 12 Mar 2024 16:42:44 GMT   (8762kb,D)

Title: Beyond Memorization: The Challenge of Random Memory Access in Language
  Models
Authors: Tongyao Zhu, Qian Liu, Liang Pang, Zhengbao Jiang, Min-Yen Kan, Min
  Lin
Categories: cs.CL cs.AI
Comments: 8 pages, 4 figures
\\
  Recent developments in Language Models (LMs) have shown their effectiveness
in NLP tasks, particularly in knowledge-intensive tasks. However, the
mechanisms underlying knowledge storage and memory access within their
parameters remain elusive. In this paper, we investigate whether a generative
LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through
carefully-designed synthetic tasks, covering the scenarios of full recitation,
selective recitation and grounded question answering, we reveal that LMs manage
to sequentially access their memory while encountering challenges in randomly
accessing memorized content. We find that techniques including recitation and
permutation improve the random memory access capability of LMs. Furthermore, by
applying this intervention to realistic scenarios of open-domain question
answering, we validate that enhancing random access by recitation leads to
notable improvements in question answering. The code to reproduce our
experiments can be found at https://github.
com/sail-sg/lm-random-memory-access.
\\ ( https://arxiv.org/abs/2403.07805 ,  8762kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07816
Date: Tue, 12 Mar 2024 16:54:58 GMT   (946kb,D)

Title: Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM
Authors: Sainbayar Sukhbaatar, Olga Golovneva, Vasu Sharma, Hu Xu, Xi Victoria
  Lin, Baptiste Rozi\`ere, Jacob Kahn, Daniel Li, Wen-tau Yih, Jason Weston,
  Xian Li
Categories: cs.CL cs.AI
\\
  We investigate efficient methods for training Large Language Models (LLMs) to
possess capabilities in multiple specialized domains, such as coding, math
reasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts
from a seed model, which is branched to train experts in embarrassingly
parallel fashion with high throughput and reduced communication cost. After
individual experts are asynchronously trained, BTX brings together their
feedforward parameters as experts in Mixture-of-Expert (MoE) layers and
averages the remaining parameters, followed by an MoE-finetuning stage to learn
token-level routing. BTX generalizes two special cases, the Branch-Train-Merge
method, which does not have the MoE finetuning stage to learn routing, and
sparse upcycling, which omits the stage of training experts asynchronously.
Compared to alternative approaches, BTX achieves the best accuracy-efficiency
tradeoff.
\\ ( https://arxiv.org/abs/2403.07816 ,  946kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07825
Date: Tue, 12 Mar 2024 17:04:28 GMT   (11626kb,D)

Title: The Missing Piece in Model Editing: A Deep Dive into the Hidden Damage
  Brought By Model Editing
Authors: Jianchen Wang, Zhouhong Gu, Zhuozhi Xiong, Hongwei Feng, Yanghua Xiao
Categories: cs.CL
\\
  Large Language Models have revolutionized numerous tasks with their
remarkable efficacy.However, the editing of these models, crucial for
rectifying outdated or erroneous information, often leads to a complex issue
known as the ripple effect in the hidden space. This effect, while difficult to
detect, can significantly impede the efficacy of model editing tasks and
deteriorate model performance.This paper addresses this scientific challenge by
proposing a novel evaluation methodology, Graphical Outlier Relation based
Assessment(GORA), which quantitatively evaluates the adaptations of the model
and the subsequent impact of editing. Furthermore, we introduce the Selective
Outlier Re-Editing Approach(SORA), a model editing method designed to mitigate
this ripple effect. Our comprehensive evaluations reveal that the ripple effect
in the hidden space is a significant issue in all current model editing
methods. However, our proposed methods, GORA and SORA, effectively identify and
alleviate this issue, respectively, contributing to the advancement of LLM
editing techniques.
\\ ( https://arxiv.org/abs/2403.07825 ,  11626kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07865
Date: Tue, 12 Mar 2024 17:55:38 GMT   (8284kb,D)

Title: Exploring Safety Generalization Challenges of Large Language Models via
  Code
Authors: Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Wai Lam,
  Lizhuang Ma
Categories: cs.CL cs.AI cs.CR cs.LG cs.SE
\\
  The rapid advancement of Large Language Models (LLMs) has brought about
remarkable capabilities in natural language processing but also raised concerns
about their potential misuse. While strategies like supervised fine-tuning and
reinforcement learning from human feedback have enhanced their safety, these
methods primarily focus on natural languages, which may not generalize to other
domains. This paper introduces CodeAttack, a framework that transforms natural
language inputs into code inputs, presenting a novel environment for testing
the safety generalization of LLMs. Our comprehensive studies on
state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a
common safety vulnerability of these models against code input: CodeAttack
consistently bypasses the safety guardrails of all models more than 80\% of the
time. Furthermore, we find that a larger distribution gap between CodeAttack
and natural language leads to weaker safety generalization, such as encoding
natural language input with data structures or using less popular programming
languages. These findings highlight new safety risks in the code domain and the
need for more robust safety alignment algorithms to match the code capabilities
of LLMs.
\\ ( https://arxiv.org/abs/2403.07865 ,  8284kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07872
Date: Tue, 12 Mar 2024 17:59:48 GMT   (11812kb,D)

Title: Rethinking Generative Large Language Model Evaluation for Semantic
  Comprehension
Authors: Fangyun Wei, Xi Chen, Lin Luo
Categories: cs.CL
\\
  Despite their sophisticated capabilities, large language models (LLMs)
encounter a major hurdle in effective assessment. This paper first revisits the
prevalent evaluation method-multiple choice question answering (MCQA), which
allows for straightforward accuracy measurement. Through a comprehensive
evaluation of 24 models across 11 benchmarks, we highlight several potential
drawbacks of MCQA, for instance, the inconsistency between the MCQA evaluation
and the generation of open-ended responses in practical scenarios. In response,
we introduce an RWQ-Elo rating system, engaging 24 LLMs such as GPT-4, GPT-3.5,
Google-Gemini-Pro and LLaMA-1/-2, in a two-player competitive format, with
GPT-4 serving as the judge. Each LLM receives an Elo rating thereafter. This
system is designed to mirror real-world usage, and for this purpose, we have
compiled a new benchmark called ``Real-world questions'' (RWQ), comprising
20,772 authentic user inquiries. Additionally, we thoroughly analyze the
characteristics of our system and compare it with prior leaderboards like
AlpacaEval and MT-Bench. Our analysis reveals the stability of our RWQ-Elo
system, the feasibility of registering new models, and its potential to reshape
LLM leaderboards.
\\ ( https://arxiv.org/abs/2403.07872 ,  11812kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06988
Date: Wed, 7 Feb 2024 13:36:02 GMT   (128kb,D)

Title: Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation
Authors: Luca Beurer-Kellner, Marc Fischer, Martin Vechev
Categories: cs.LG cs.CL
\\
  To ensure that text generated by large language models (LLMs) is in an
expected format, constrained decoding proposes to enforce strict formal
language constraints during generation. However, as we show in this work, not
only do such methods incur performance overhead during generation, but many of
them also significantly impair task accuracy, if they do not correctly align
the underlying LLM sub-word vocabularies with external constraints. To address
this, we present a novel decoding algorithm, DOMINO, that can enforce
constraints in a fully subword-aligned fashion, while leveraging
pre-computation and speculative decoding to achieve virtually no overhead and
in some cases even almost 2$\times$ speedup over unconstrained decoding --
thereby outperforming existing approaches by a wide margin.
\\ ( https://arxiv.org/abs/2403.06988 ,  128kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06999
Date: Mon, 4 Mar 2024 10:46:02 GMT   (1517kb)

Title: Survival modeling using deep learning, machine learning and statistical
  methods: A comparative analysis for predicting mortality after hospital
  admission
Authors: Ziwen Wang, Jin Wee Lee, Tanujit Chakraborty, Yilin Ning, Mingxuan
  Liu, Feng Xie, Marcus Eng Hock Ong, Nan Liu
Categories: cs.LG cs.AI cs.CY
\\
  Survival analysis is essential for studying time-to-event outcomes and
providing a dynamic understanding of the probability of an event occurring over
time. Various survival analysis techniques, from traditional statistical models
to state-of-the-art machine learning algorithms, support healthcare
intervention and policy decisions. However, there remains ongoing discussion
about their comparative performance. We conducted a comparative study of
several survival analysis methods, including Cox proportional hazards (CoxPH),
stepwise CoxPH, elastic net penalized Cox model, Random Survival Forests (RSF),
Gradient Boosting machine (GBM) learning, AutoScore-Survival, DeepSurv,
time-dependent Cox model based on neural network (CoxTime), and DeepHit
survival neural network. We applied the concordance index (C-index) for model
goodness-of-fit, and integral Brier scores (IBS) for calibration, and
considered the model interpretability. As a case study, we performed a
retrospective analysis of patients admitted through the emergency department of
a tertiary hospital from 2017 to 2019, predicting 90-day all-cause mortality
based on patient demographics, clinicopathological features, and historical
data. The results of the C-index indicate that deep learning achieved
comparable performance, with DeepSurv producing the best discrimination
(DeepSurv: 0.893; CoxTime: 0.892; DeepHit: 0.891). The calibration of DeepSurv
(IBS: 0.041) performed the best, followed by RSF (IBS: 0.042) and GBM (IBS:
0.0421), all using the full variables. Moreover, AutoScore-Survival, using a
minimal variable subset, is easy to interpret, and can achieve good
discrimination and calibration (C-index: 0.867; IBS: 0.044). While all models
were satisfactory, DeepSurv exhibited the best discrimination and calibration.
In addition, AutoScore-Survival offers a more parsimonious model and excellent
interpretability.
\\ ( https://arxiv.org/abs/2403.06999 ,  1517kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07008
Date: Sat, 9 Mar 2024 02:47:11 GMT   (1094kb,D)

Title: AutoEval Done Right: Using Synthetic Data for Model Evaluation
Authors: Pierre Boyeau, Anastasios N. Angelopoulos, Nir Yosef, Jitendra Malik,
  Michael I. Jordan
Categories: cs.LG cs.AI cs.CL stat.ME
\\
  The evaluation of machine learning models using human-labeled validation data
can be expensive and time-consuming. AI-labeled synthetic data can be used to
decrease the number of human annotations required for this purpose in a process
called autoevaluation. We suggest efficient and statistically principled
algorithms for this purpose that improve sample efficiency while remaining
unbiased. These algorithms increase the effective human-labeled sample size by
up to 50% on experiments with GPT-4.
\\ ( https://arxiv.org/abs/2403.07008 ,  1094kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07012
Date: Sat, 9 Mar 2024 10:01:49 GMT   (1716kb,D)

Title: Non-Intrusive Load Monitoring with Missing Data Imputation Based on
  Tensor Decomposition
Authors: DengYu Shi
Categories: cs.LG
\\
  With the widespread adoption of Non-Intrusive Load Monitoring (NILM) in
building energy management, ensuring the high quality of NILM data has become
imperative. However, practical applications of NILM face challenges associated
with data loss, significantly impacting accuracy and reliability in energy
management. This paper addresses the issue of NILM data loss by introducing an
innovative tensor completion(TC) model- Proportional-Integral-Derivative
(PID)-incorporated Non-negative Latent Factorization of Tensors (PNLFT) with
twofold ideas: 1) To tackle the issue of slow convergence in Latent
Factorization of Tensors (LFT) using Stochastic Gradient Descent (SGD), a
Proportional-Integral-Derivative controller is introduced during the learning
process. The PID controller utilizes historical and current information to
control learning residuals. 2) Considering the characteristics of NILM data,
non-negative update rules are proposed in the model's learning scheme.
Experimental results on three datasets demonstrate that, compared to
state-of-the-art models, the proposed model exhibits noteworthy enhancements in
both convergence speed and accuracy.
\\ ( https://arxiv.org/abs/2403.07012 ,  1716kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07015
Date: Sat, 9 Mar 2024 16:47:42 GMT   (2007kb,D)

Title: Adaptive Hyperparameter Optimization for Continual Learning Scenarios
Authors: Rudy Semola, Julio Hurtado, Vincenzo Lomonaco, Davide Bacciu
Categories: cs.LG
\\
  Hyperparameter selection in continual learning scenarios is a challenging and
underexplored aspect, especially in practical non-stationary environments.
Traditional approaches, such as grid searches with held-out validation data
from all tasks, are unrealistic for building accurate lifelong learning
systems. This paper aims to explore the role of hyperparameter selection in
continual learning and the necessity of continually and automatically tuning
them according to the complexity of the task at hand. Hence, we propose
leveraging the nature of sequence task learning to improve Hyperparameter
Optimization efficiency. By using the functional analysis of variance-based
techniques, we identify the most crucial hyperparameters that have an impact on
performance. We demonstrate empirically that this approach, agnostic to
continual scenarios and strategies, allows us to speed up hyperparameters
optimization continually across tasks and exhibit robustness even in the face
of varying sequential task orders. We believe that our findings can contribute
to the advancement of continual learning methodologies towards more efficient,
robust and adaptable models for real-world applications.
\\ ( https://arxiv.org/abs/2403.07015 ,  2007kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07022
Date: Sun, 10 Mar 2024 02:34:44 GMT   (11707kb,D)

Title: A Unified Model for Spatio-Temporal Prediction Queries with Arbitrary
  Modifiable Areal Units
Authors: Liyue Chen, Jiangyi Fang, Tengfei Liu, Shaosheng Cao, Leye Wang
Categories: cs.LG cs.AI
Comments: Accepted by ICDE 2024
\\
  Spatio-Temporal (ST) prediction is crucial for making informed decisions in
urban location-based applications like ride-sharing. However, existing ST
models often require region partition as a prerequisite, resulting in two main
pitfalls. Firstly, location-based services necessitate ad-hoc regions for
various purposes, requiring multiple ST models with varying scales and zones,
which can be costly to support. Secondly, different ST models may produce
conflicting outputs, resulting in confusing predictions. In this paper, we
propose One4All-ST, a framework that can conduct ST prediction for arbitrary
modifiable areal units using only one model. To reduce the cost of getting
multi-scale predictions, we design an ST network with hierarchical spatial
modeling and scale normalization modules to efficiently and equally learn
multi-scale representations. To address prediction inconsistencies across
scales, we propose a dynamic programming scheme to solve the formulated optimal
combination problem, minimizing predicted error through theoretical analysis.
Besides, we suggest using an extended quad-tree to index the optimal
combinations for quick response to arbitrary modifiable areal units in
practical online scenarios. Extensive experiments on two real-world datasets
verify the efficiency and effectiveness of One4All-ST in ST prediction for
arbitrary modifiable areal units. The source codes and data of this work are
available at https://github.com/uctb/One4All-ST.
\\ ( https://arxiv.org/abs/2403.07022 ,  11707kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07027
Date: Sun, 10 Mar 2024 19:20:55 GMT   (187kb)

Title: FWin transformer for dengue prediction under climate and ocean influence
Authors: Nhat Thanh Tran, Jack Xin, Guofa Zhou
Categories: cs.LG
\\
  Dengue fever is one of the most deadly mosquito-born tropical infectious
diseases. Detailed long range forecast model is vital in controlling the spread
of disease and making mitigation efforts. In this study, we examine methods
used to forecast dengue cases for long range predictions. The dataset consists
of local climate/weather in addition to global climate indicators of Singapore
from 2000 to 2019. We utilize newly developed deep neural networks to learn the
intricate relationship between the features. The baseline models in this study
are in the class of recent transformers for long sequence forecasting tasks. We
found that a Fourier mixed window attention (FWin) based transformer performed
the best in terms of both the mean square error and the maximum absolute error
on the long range dengue forecast up to 60 weeks.
\\ ( https://arxiv.org/abs/2403.07027 ,  187kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07028
Date: Mon, 11 Mar 2024 02:17:42 GMT   (7759kb,D)

Title: An Efficient Learning-based Solver Comparable to Metaheuristics for the
  Capacitated Arc Routing Problem
Authors: Runze Guo, Feng Xue, Anlong Ming, Nicu Sebe
Categories: cs.LG cs.AI math.OC
\\
  Recently, neural networks (NN) have made great strides in combinatorial
optimization. However, they face challenges when solving the capacitated arc
routing problem (CARP) which is to find the minimum-cost tour covering all
required edges on a graph, while within capacity constraints. In tackling CARP,
NN-based approaches tend to lag behind advanced metaheuristics, since they lack
directed arc modeling and efficient learning methods tailored for complex CARP.
In this paper, we introduce an NN-based solver to significantly narrow the gap
with advanced metaheuristics while exhibiting superior efficiency. First, we
propose the direction-aware attention model (DaAM) to incorporate
directionality into the embedding process, facilitating more effective
one-stage decision-making. Second, we design a supervised reinforcement
learning scheme that involves supervised pre-training to establish a robust
initial policy for subsequent reinforcement fine-tuning. It proves particularly
valuable for solving CARP that has a higher complexity than the node routing
problems (NRPs). Finally, a path optimization method is proposed to adjust the
depot return positions within the path generated by DaAM. Experiments
illustrate that our approach surpasses heuristics and achieves decision quality
comparable to state-of-the-art metaheuristics for the first time while
maintaining superior efficiency.
\\ ( https://arxiv.org/abs/2403.07028 ,  7759kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07030
Date: Mon, 11 Mar 2024 03:34:14 GMT   (1092kb,D)

Title: AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge
  Distillation
Authors: Zihao Tang, Zheqi Lv, Shengyu Zhang, Yifan Zhou, Xinyu Duan, Fei Wu,
  Kun Kuang
Categories: cs.LG cs.CV
Comments: Accepted to ICLR 2024
\\
  Due to privacy or patent concerns, a growing number of large models are
released without granting access to their training data, making transferring
their knowledge inefficient and problematic. In response, Data-Free Knowledge
Distillation (DFKD) methods have emerged as direct solutions. However, simply
adopting models derived from DFKD for real-world applications suffers
significant performance degradation, due to the discrepancy between teachers'
training data and real-world scenarios (student domain). The degradation stems
from the portions of teachers' knowledge that are not applicable to the student
domain. They are specific to the teacher domain and would undermine students'
performance. Hence, selectively transferring teachers' appropriate knowledge
becomes the primary challenge in DFKD. In this work, we propose a simple but
effective method AuG-KD. It utilizes an uncertainty-guided and sample-specific
anchor to align student-domain data with the teacher domain and leverages a
generative method to progressively trade off the learning process between OOD
knowledge distillation and domain-specific information learning via mixup
learning. Extensive experiments in 3 datasets and 8 settings demonstrate the
stability and superiority of our approach. Code available at
https://github.com/IshiKura-a/AuG-KD .
\\ ( https://arxiv.org/abs/2403.07030 ,  1092kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07031
Date: Mon, 11 Mar 2024 04:19:05 GMT   (894kb,D)

Title: The Cram Method for Efficient Simultaneous Learning and Evaluation
Authors: Zeyang Jia, Kosuke Imai, Michael Lingzhi Li
Categories: cs.LG stat.CO stat.ME stat.ML
\\
  We introduce the "cram" method, a general and efficient approach to
simultaneous learning and evaluation using a generic machine learning (ML)
algorithm. In a single pass of batched data, the proposed method repeatedly
trains an ML algorithm and tests its empirical performance. Because it utilizes
the entire sample for both learning and evaluation, cramming is significantly
more data-efficient than sample-splitting. The cram method also naturally
accommodates online learning algorithms, making its implementation
computationally efficient. To demonstrate the power of the cram method, we
consider the standard policy learning setting where cramming is applied to the
same data to both develop an individualized treatment rule (ITR) and estimate
the average outcome that would result if the learned ITR were to be deployed.
We show that under a minimal set of assumptions, the resulting crammed
evaluation estimator is consistent and asymptotically normal. While our
asymptotic results require a relatively weak stabilization condition of ML
algorithm, we develop a simple, generic method that can be used with any policy
learning algorithm to satisfy this condition. Our extensive simulation studies
show that, when compared to sample-splitting, cramming reduces the evaluation
standard error by more than 40% while improving the performance of learned
policy. We also apply the cram method to a randomized clinical trial to
demonstrate its applicability to real-world problems. Finally, we briefly
discuss future extensions of the cram method to other learning and evaluation
settings.
\\ ( https://arxiv.org/abs/2403.07031 ,  894kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07033
Date: Mon, 11 Mar 2024 05:47:07 GMT   (3424kb,D)

Title: Interpreting What Typical Fault Signals Look Like via Prototype-matching
Authors: Qian Chen and Xingjian Dong and Zhike Peng
Categories: cs.LG cs.AI
Comments: 17 pages, 12 figures, 6 tables
\\
  Neural networks, with powerful nonlinear mapping and classification
capabilities, are widely applied in mechanical fault diagnosis to ensure
safety. However, being typical black-box models, their application is limited
in high-reliability-required scenarios. To understand the classification logic
and explain what typical fault signals look like, the prototype matching
network (PMN) is proposed by combining the human-inherent prototype-matching
with autoencoder (AE). The PMN matches AE-extracted feature with each prototype
and selects the most similar prototype as the prediction result. It has three
interpreting paths on classification logic, fault prototypes, and matching
contributions. Conventional diagnosis and domain generalization experiments
demonstrate its competitive diagnostic performance and distinguished advantages
in representation learning. Besides, the learned typical fault signals (i.e.,
sample-level prototypes) showcase the ability for denoising and extracting
subtle key features that experts find challenging to capture. This ability
broadens human understanding and provides a promising solution from
interpretability research to AI-for-Science.
\\ ( https://arxiv.org/abs/2403.07033 ,  3424kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07036
Date: Mon, 11 Mar 2024 08:13:42 GMT   (1207kb,D)

Title: A Converting Autoencoder Toward Low-latency and Energy-efficient DNN
  Inference at the Edge
Authors: Hasanul Mahmud, Peng Kang, Kevin Desai, Palden Lama, Sushil Prasad
Categories: cs.LG cs.CV cs.DC
Comments: 8 Pages, 8 Figures
\\
  Reducing inference time and energy usage while maintaining prediction
accuracy has become a significant concern for deep neural networks (DNN)
inference on resource-constrained edge devices. To address this problem, we
propose a novel approach based on "converting" autoencoder and lightweight
DNNs. This improves upon recent work such as early-exiting framework and DNN
partitioning. Early-exiting frameworks spend different amounts of computation
power for different input data depending upon their complexity. However, they
can be inefficient in real-world scenarios that deal with many hard image
samples. On the other hand, DNN partitioning algorithms that utilize the
computation power of both the cloud and edge devices can be affected by network
delays and intermittent connections between the cloud and the edge. We present
CBNet, a low-latency and energy-efficient DNN inference framework tailored for
edge devices. It utilizes a "converting" autoencoder to efficiently transform
hard images into easy ones, which are subsequently processed by a lightweight
DNN for inference. To the best of our knowledge, such autoencoder has not been
proposed earlier. Our experimental results using three popular
image-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and
an instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x
speedup in inference latency and 79% reduction in energy usage compared to
competing techniques while maintaining similar or higher accuracy.
\\ ( https://arxiv.org/abs/2403.07036 ,  1207kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07038
Date: Mon, 11 Mar 2024 09:54:35 GMT   (2204kb,D)

Title: Leveraging graph neural networks for supporting Automatic Triage of
  Patients
Authors: Annamaria Defilippo and Pierangelo Veltri and Pietro Lio' and Pietro
  Hiram Guzzi
Categories: cs.LG
\\
  Patient triage plays a crucial role in emergency departments, ensuring timely
and appropriate care based on correctly evaluating the emergency grade of
patient conditions.
  Triage methods are generally performed by human operator based on her own
experience and information that are gathered from the patient management
process.
  Thus, it is a process that can generate errors in emergency level
associations. Recently, Traditional triage methods heavily rely on human
decisions, which can be subjective and prone to errors.
  Recently, a growing interest has been focused on leveraging artificial
intelligence (AI) to develop algorithms able to maximize information gathering
and minimize errors in patient triage processing.
  We define and implement an AI based module to manage patients emergency code
assignments in emergency departments. It uses emergency department historical
data to train the medical decision process. Data containing relevant patient
information, such as vital signs, symptoms, and medical history, are used to
accurately classify patients into triage categories. Experimental results
demonstrate that the proposed algorithm achieved high accuracy outperforming
traditional triage methods. By using the proposed method we claim that
healthcare professionals can predict severity index to guide patient management
processing and resource allocation.
\\ ( https://arxiv.org/abs/2403.07038 ,  2204kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07040
Date: Mon, 11 Mar 2024 16:04:58 GMT   (1072kb,D)

Title: All in One: Multi-Task Prompting for Graph Neural Networks (Extended
  Abstract)
Authors: Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, Jihong Guan
Categories: cs.LG cs.AI
Comments: submitted to IJCAI 2024 Sister Conferences Track. The original paper
  can be seen at arXiv:2307.01504
\\
  This paper is an extended abstract of our original work published in KDD23,
where we won the best research paper award (Xiangguo Sun, Hong Cheng, Jia Li,
Bo Liu, and Jihong Guan. All in one: Multi-task prompting for graph neural
networks. KDD 23) The paper introduces a novel approach to bridging the gap
between pre-trained graph models and the diverse tasks they're applied to,
inspired by the success of prompt learning in NLP. Recognizing the challenge of
aligning pre-trained models with varied graph tasks (node level, edge level,
and graph level), which can lead to negative transfer and poor performance, we
propose a multi-task prompting method for graphs. This method involves unifying
graph and language prompt formats, enabling NLP's prompting strategies to be
adapted for graph tasks. By analyzing the task space of graph applications, we
reformulate problems to fit graph-level tasks and apply meta-learning to
improve prompt initialization for multiple tasks. Experiments show our method's
effectiveness in enhancing model performance across different graph tasks.
  Beyond the original work, in this extended abstract, we further discuss the
graph prompt from a bigger picture and provide some of the latest work toward
this area.
\\ ( https://arxiv.org/abs/2403.07040 ,  1072kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07041
Date: Mon, 11 Mar 2024 16:26:06 GMT   (1078kb,D)

Title: Ant Colony Sampling with GFlowNets for Combinatorial Optimization
Authors: Minsu Kim, Sanghyeok Choi, Jiwoo Son, Hyeonah Kim, Jinkyoo Park,
  Yoshua Bengio
Categories: cs.LG cs.NE
Comments: 19 pages, 6 figures
\\
  This paper introduces the Generative Flow Ant Colony Sampler (GFACS), a novel
neural-guided meta-heuristic algorithm for combinatorial optimization. GFACS
integrates generative flow networks (GFlowNets) with the ant colony
optimization (ACO) methodology. GFlowNets, a generative model that learns a
constructive policy in combinatorial spaces, enhance ACO by providing an
informed prior distribution of decision variables conditioned on input graph
instances. Furthermore, we introduce a novel combination of training tricks,
including search-guided local exploration, energy normalization, and energy
shaping to improve GFACS. Our experimental results demonstrate that GFACS
outperforms baseline ACO algorithms in seven CO tasks and is competitive with
problem-specific heuristics for vehicle routing problems. The source code is
available at \url{https://github.com/ai4co/gfacs}.
\\ ( https://arxiv.org/abs/2403.07041 ,  1078kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07072
Date: Mon, 11 Mar 2024 18:03:02 GMT   (784kb,D)

Title: Explainable Learning with Gaussian Processes
Authors: Kurt Butler, Guanchao Feng, Petar M. Djuric
Categories: cs.LG eess.SP
Comments: 38 pages, 7 figures
MSC-class: 60G15
\\
  The field of explainable artificial intelligence (XAI) attempts to develop
methods that provide insight into how complicated machine learning methods make
predictions. Many methods of explanation have focused on the concept of feature
attribution, a decomposition of the model's prediction into individual
contributions corresponding to each input feature. In this work, we explore the
problem of feature attribution in the context of Gaussian process regression
(GPR). We take a principled approach to defining attributions under model
uncertainty, extending the existing literature. We show that although GPR is a
highly flexible and non-parametric approach, we can derive interpretable,
closed-form expressions for the feature attributions. When using integrated
gradients as an attribution method, we show that the attributions of a GPR
model also follow a Gaussian process distribution, which quantifies the
uncertainty in attribution arising from uncertainty in the model. We
demonstrate, both through theory and experimentation, the versatility and
robustness of this approach. We also show that, when applicable, the exact
expressions for GPR attributions are both more accurate and less
computationally expensive than the approximations currently used in practice.
The source code for this project is freely available under MIT license at
https://github.com/KurtButler/2024_attributions_paper.
\\ ( https://arxiv.org/abs/2403.07072 ,  784kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07078
Date: Mon, 11 Mar 2024 18:11:00 GMT   (2816kb)

Title: Improving deep learning with prior knowledge and cognitive models: A
  survey on enhancing explainability, adversarial robustness and zero-shot
  learning
Authors: Fuseinin Mumuni and Alhassan Mumuni
Categories: cs.LG cs.AI cs.CV
Journal-ref: Cognitive Systems Research, 84 (2024)
DOI: 10.1016/j.cogsys.2023.101188
\\
  We review current and emerging knowledge-informed and brain-inspired
cognitive systems for realizing adversarial defenses, eXplainable Artificial
Intelligence (XAI), and zero-shot or few-short learning. Data-driven deep
learning models have achieved remarkable performance and demonstrated
capabilities surpassing human experts in many applications. Yet, their
inability to exploit domain knowledge leads to serious performance limitations
in practical applications. In particular, deep learning systems are exposed to
adversarial attacks, which can trick them into making glaringly incorrect
decisions. Moreover, complex data-driven models typically lack interpretability
or explainability, i.e., their decisions cannot be understood by human
subjects. Furthermore, models are usually trained on standard datasets with a
closed-world assumption. Hence, they struggle to generalize to unseen cases
during inference in practical open-world environments, thus, raising the zero-
or few-shot generalization problem. Although many conventional solutions exist,
explicit domain knowledge, brain-inspired neural network and cognitive
architectures offer powerful new dimensions towards alleviating these problems.
Prior knowledge is represented in appropriate forms and incorporated in deep
learning frameworks to improve performance. Brain-inspired cognition methods
use computational models that mimic the human mind to enhance intelligent
behavior in artificial agents and autonomous robots. Ultimately, these models
achieve better explainability, higher adversarial robustness and data-efficient
learning, and can, in turn, provide insights for cognitive science and
neuroscience-that is, to deepen human understanding on how the brain works in
general, and how it handles these problems.
\\ ( https://arxiv.org/abs/2403.07078 ,  2816kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07094
Date: Mon, 11 Mar 2024 18:40:47 GMT   (1042kb,D)

Title: FALCON: FLOP-Aware Combinatorial Optimization for Neural Network Pruning
Authors: Xiang Meng, Wenyu Chen, Riade Benbaki, Rahul Mazumder
Categories: cs.LG
\\
  The increasing computational demands of modern neural networks present
deployment challenges on resource-constrained devices. Network pruning offers a
solution to reduce model size and computational cost while maintaining
performance. However, most current pruning methods focus primarily on improving
sparsity by reducing the number of nonzero parameters, often neglecting other
deployment costs such as inference time, which are closely related to the
number of floating-point operations (FLOPs). In this paper, we propose FALCON,
a novel combinatorial-optimization-based framework for network pruning that
jointly takes into account model accuracy (fidelity), FLOPs, and sparsity
constraints. A main building block of our approach is an integer linear program
(ILP) that simultaneously handles FLOP and sparsity constraints. We present a
novel algorithm to approximately solve the ILP. We propose a novel first-order
method for our optimization framework which makes use of our ILP solver. Using
problem structure (e.g., the low-rank structure of approx. Hessian), we can
address instances with millions of parameters. Our experiments demonstrate that
FALCON achieves superior accuracy compared to other pruning approaches within a
fixed FLOP budget. For instance, for ResNet50 with 20% of the total FLOPs
retained, our approach improves the accuracy by 48% relative to
state-of-the-art. Furthermore, in gradual pruning settings with re-training
between pruning steps, our framework outperforms existing pruning methods,
emphasizing the significance of incorporating both FLOP and sparsity
constraints for effective network pruning.
\\ ( https://arxiv.org/abs/2403.07094 ,  1042kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07095
Date: Mon, 11 Mar 2024 18:44:36 GMT   (1597kb,D)

Title: Overcoming the Paradox of Certified Training with Gaussian Smoothing
Authors: Stefan Balauca, Mark Niklas M\"uller, Yuhao Mao, Maximilian Baader,
  Marc Fischer, Martin Vechev
Categories: cs.LG
\\
  Training neural networks with high certified accuracy against adversarial
examples remains an open problem despite significant efforts. While
certification methods can effectively leverage tight convex relaxations for
bound computation, in training, these methods perform worse than looser
relaxations. Prior work hypothesized that this is caused by the discontinuity
and perturbation sensitivity of the loss surface induced by these tighter
relaxations. In this work, we show theoretically that Gaussian Loss Smoothing
can alleviate both of these issues. We confirm this empirically by proposing a
certified training method combining PGPE, an algorithm computing gradients of a
smoothed loss, with different convex relaxations. When using this training
method, we observe that tighter bounds indeed lead to strictly better networks
that can outperform state-of-the-art methods on the same network. While scaling
PGPE-based training remains challenging due to high computational cost, our
results clearly demonstrate the promise of Gaussian Loss Smoothing for training
certifiably robust neural networks.
\\ ( https://arxiv.org/abs/2403.07095 ,  1597kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07134
Date: Mon, 11 Mar 2024 20:04:03 GMT   (667kb,D)

Title: COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization
Authors: Aozhong Zhang, Zi Yang, Naigang Wang, Yingyong Qin, Jack Xin, Xin Li,
  Penghang Yin
Categories: cs.LG cs.CV
\\
  Post-training quantization (PTQ) has emerged as a practical approach to
compress large neural networks, making them highly efficient for deployment.
However, effectively reducing these models to their low-bit counterparts
without compromising the original accuracy remains a key challenge. In this
paper, we propose an innovative PTQ algorithm termed COMQ, which sequentially
conducts coordinate-wise minimization of the layer-wise reconstruction errors.
We consider the widely used integer quantization, where every quantized weight
can be decomposed into a shared floating-point scalar and an integer bit-code.
Within a fixed layer, COMQ treats all the scaling factor(s) and bit-codes as
the variables of the reconstruction error. Every iteration improves this error
along a single coordinate while keeping all other variables constant. COMQ is
easy to use and requires no hyper-parameter tuning. It instead involves only
dot products and rounding operations. We update these variables in a carefully
designed greedy order, significantly enhancing the accuracy. COMQ achieves
remarkable results in quantizing 4-bit Vision Transformers, with a negligible
loss of less than 1% in Top-1 accuracy. In 4-bit INT quantization of
convolutional neural networks, COMQ maintains near-lossless accuracy with a
minimal drop of merely 0.3% in Top-1 accuracy.
\\ ( https://arxiv.org/abs/2403.07134 ,  667kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07136
Date: Mon, 11 Mar 2024 20:05:48 GMT   (37kb)

Title: On the Limited Representational Power of Value Functions and its Links
  to Statistical (In)Efficiency
Authors: David Cheikhi, Daniel Russo
Categories: cs.LG cs.AI stat.ML
\\
  Identifying the trade-offs between model-based and model-free methods is a
central question in reinforcement learning. Value-based methods offer
substantial computational advantages and are sometimes just as statistically
efficient as model-based methods. However, focusing on the core problem of
policy evaluation, we show information about the transition dynamics may be
impossible to represent in the space of value functions. We explore this
through a series of case studies focused on structures that arises in many
important problems. In several, there is no information loss and value-based
methods are as statistically efficient as model based ones. In other
closely-related examples, information loss is severe and value-based methods
are severely outperformed. A deeper investigation points to the limitations of
the representational power as the driver of the inefficiency, as opposed to
failure in algorithm design.
\\ ( https://arxiv.org/abs/2403.07136 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07151
Date: Mon, 11 Mar 2024 20:39:32 GMT   (612kb,D)

Title: Don't Forget What I did?: Assessing Client Contributions in Federated
  Learning
Authors: Bishwamittra Ghosh, Debabrota Basu, Fu Huazhu, Wang Yuan, Renuga
  Kanagavelu, Jiang Jin Peng, Liu Yong, Goh Siow Mong Rick, and Wei Qingsong
Categories: cs.LG cs.AI cs.CR
Comments: Under submission
\\
  Federated Learning (FL) is a collaborative machine learning (ML) approach,
where multiple clients participate in training an ML model without exposing the
private data. Fair and accurate assessment of client contributions is an
important problem in FL to facilitate incentive allocation and encouraging
diverse clients to participate in a unified model training. Existing methods
for assessing client contribution adopts co-operative game-theoretic concepts,
such as Shapley values, but under simplified assumptions. In this paper, we
propose a history-aware game-theoretic framework, called FLContrib, to assess
client contributions when a subset of (potentially non-i.i.d.) clients
participate in each epoch of FL training. By exploiting the FL training process
and linearity of Shapley value, we develop FLContrib that yields a historical
timeline of client contributions as FL training progresses over epochs.
Additionally, to assess client contribution under limited computational budget,
we propose a scheduling procedure that considers a two-sided fairness criteria
to perform expensive Shapley value computation only in a subset of training
epochs. In experiments, we demonstrate a controlled trade-off between the
correctness and efficiency of client contributions assessed via FLContrib. To
demonstrate the benefits of history-aware client contributions, we apply
FLContrib to detect dishonest clients conducting data poisoning in FL training.
\\ ( https://arxiv.org/abs/2403.07151 ,  612kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07179
Date: Mon, 11 Mar 2024 21:44:54 GMT   (985kb,D)

Title: 3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of
  Molecular Graphs
Authors: Huaisheng Zhu, Teng Xiao, Vasant G Honavar
Categories: cs.LG cs.CL q-bio.BM
\\
  Generating molecules with desired properties is a critical task with broad
applications in drug discovery and materials design. Inspired by recent
advances in large language models, there is a growing interest in using natural
language descriptions of molecules to generate molecules with the desired
properties. Most existing methods focus on generating molecules that precisely
match the text description. However, practical applications call for methods
that generate diverse, and ideally novel, molecules with the desired
properties. We propose 3M-Diffusion, a novel multi-modal molecular graph
generation method, to address this challenge. 3M-Diffusion first encodes
molecular graphs into a graph latent space aligned with text descriptions. It
then reconstructs the molecular structure and atomic attributes based on the
given text descriptions using the molecule decoder. It then learns a
probabilistic mapping from the text space to the latent molecular graph space
using a diffusion model. The results of our extensive experiments on several
datasets demonstrate that 3M-Diffusion can generate high-quality, novel and
diverse molecular graphs that semantically match the textual description
provided.
\\ ( https://arxiv.org/abs/2403.07179 ,  985kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07185
Date: Mon, 11 Mar 2024 21:54:52 GMT   (224kb,D)

Title: Uncertainty in Graph Neural Networks: A Survey
Authors: Fangxin Wang, Yuqing Liu, Kay Liu, Yibo Wang, Sourav Medya, Philip S.
  Yu
Categories: cs.LG stat.ML
Comments: 13 main pages, 3 figures, 1 table. Under review
\\
  Graph Neural Networks (GNNs) have been extensively used in various real-world
applications. However, the predictive uncertainty of GNNs stemming from diverse
sources such as inherent randomness in data and model training errors can lead
to unstable and erroneous predictions. Therefore, identifying, quantifying, and
utilizing uncertainty are essential to enhance the performance of the model for
the downstream tasks as well as the reliability of the GNN predictions. This
survey aims to provide a comprehensive overview of the GNNs from the
perspective of uncertainty with an emphasis on its integration in graph
learning. We compare and summarize existing graph uncertainty theory and
methods, alongside the corresponding downstream tasks. Thereby, we bridge the
gap between theory and practice, meanwhile connecting different GNN
communities. Moreover, our work provides valuable insights into promising
directions in this field.
\\ ( https://arxiv.org/abs/2403.07185 ,  224kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07187
Date: Mon, 11 Mar 2024 22:00:39 GMT   (3086kb,D)

Title: UPS: Towards Foundation Models for PDE Solving via Cross-Modal
  Adaptation
Authors: Junhong Shen, Tanya Marwah, Ameet Talwalkar
Categories: cs.LG
\\
  We introduce UPS (Unified PDE Solver), an effective and data-efficient
approach to solve diverse spatiotemporal PDEs defined over various domains,
dimensions, and resolutions. UPS unifies different PDEs into a consistent
representation space and processes diverse collections of PDE data using a
unified network architecture that combines LLMs with domain-specific neural
operators. We train the network via a two-stage cross-modal adaptation process,
leveraging ideas of modality alignment and multi-task learning. By adapting
from pretrained LLMs and exploiting text-form meta information, we are able to
use considerably fewer training samples than previous methods while obtaining
strong empirical results. UPS outperforms existing baselines, often by a large
margin, on a wide range of 1D and 2D datasets in PDEBench, achieving
state-of-the-art results on 8 of 10 tasks considered. Meanwhile, it is capable
of few-shot transfer to different PDE families, coefficients, and resolutions.
\\ ( https://arxiv.org/abs/2403.07187 ,  3086kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07191
Date: Mon, 11 Mar 2024 22:24:14 GMT   (8928kb,D)

Title: $\mathbf{(N,K)}$-Puzzle: A Cost-Efficient Testbed for Benchmarking
  Reinforcement Learning Algorithms in Generative Language Model
Authors: Yufeng Zhang, Liyu Chen, Boyi Liu, Yingxiang Yang, Qiwen Cui, Yunzhe
  Tao, Hongxia Yang
Categories: cs.LG cs.AI cs.CL
Comments: 8 pages
\\
  Recent advances in reinforcement learning (RL) algorithms aim to enhance the
performance of language models at scale. Yet, there is a noticeable absence of
a cost-effective and standardized testbed tailored to evaluating and comparing
these algorithms. To bridge this gap, we present a generalized version of the
24-Puzzle: the $(N,K)$-Puzzle, which challenges language models to reach a
target value $K$ with $N$ integers. We evaluate the effectiveness of
established RL algorithms such as Proximal Policy Optimization (PPO), alongside
novel approaches like Identity Policy Optimization (IPO) and Direct Policy
Optimization (DPO).
\\ ( https://arxiv.org/abs/2403.07191 ,  8928kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07201
Date: Mon, 11 Mar 2024 22:58:11 GMT   (1143kb)

Title: A multi-cohort study on prediction of acute brain dysfunction states
  using selective state space models
Authors: Brandon Silva, Miguel Contreras, Sabyasachi Bandyopadhyay, Yuanfang
  Ren, Ziyuan Guan, Jeremy Balch, Kia Khezeli, Tezcan Ozrazgat Baslanti, Ben
  Shickel, Azra Bihorac, Parisa Rashidi
Categories: cs.LG cs.AI stat.AP
Comments: 22 pages, 8 figures, To be published
\\
  Assessing acute brain dysfunction (ABD), including delirium and coma in the
intensive care unit (ICU), is a critical challenge due to its prevalence and
severe implications for patient outcomes. Current diagnostic methods rely on
infrequent clinical observations, which can only determine a patient's ABD
status after onset. Our research attempts to solve these problems by harnessing
Electronic Health Records (EHR) data to develop automated methods for ABD
prediction for patients in the ICU. Existing models solely predict a single
state (e.g., either delirium or coma), require at least 24 hours of observation
data to make predictions, do not dynamically predict fluctuating ABD conditions
during ICU stay (typically a one-time prediction), and use small sample size,
proprietary single-hospital datasets. Our research fills these gaps in the
existing literature by dynamically predicting delirium, coma, and mortality for
12-hour intervals throughout an ICU stay and validating on two public datasets.
Our research also introduces the concept of dynamically predicting critical
transitions from non-ABD to ABD and between different ABD states in real time,
which could be clinically more informative for the hospital staff. We compared
the predictive performance of two state-of-the-art neural network models, the
MAMBA selective state space model and the Longformer Transformer model. Using
the MAMBA model, we achieved a mean area under the receiving operator
characteristic curve (AUROC) of 0.95 on outcome prediction of ABD for 12-hour
intervals. The model achieves a mean AUROC of 0.79 when predicting transitions
between ABD states. Our study uses a curated dataset from the University of
Florida Health Shands Hospital for internal validation and two publicly
available datasets, MIMIC-IV and eICU, for external validation, demonstrating
robustness across ICU stays from 203 hospitals and 140,945 patients.
\\ ( https://arxiv.org/abs/2403.07201 ,  1143kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07213
Date: Mon, 11 Mar 2024 23:52:46 GMT   (6322kb,D)

Title: Which LLM to Play? Convergence-Aware Online Model Selection with
  Time-Increasing Bandits
Authors: Yu Xia, Fang Kong, Tong Yu, Liya Guo, Ryan A. Rossi, Sungchul Kim,
  Shuai Li
Categories: cs.LG stat.ML
Comments: Accepted by WWW'24 (Oral)
\\
  Web-based applications such as chatbots, search engines and news
recommendations continue to grow in scale and complexity with the recent surge
in the adoption of LLMs. Online model selection has thus garnered increasing
attention due to the need to choose the best model among a diverse set while
balancing task reward and exploration cost. Organizations faces decisions like
whether to employ a costly API-based LLM or a locally finetuned small LLM,
weighing cost against performance. Traditional selection methods often evaluate
every candidate model before choosing one, which are becoming impractical given
the rising costs of training and finetuning LLMs. Moreover, it is undesirable
to allocate excessive resources towards exploring poor-performing models. While
some recent works leverage online bandit algorithm to manage such
exploration-exploitation trade-off in model selection, they tend to overlook
the increasing-then-converging trend in model performances as the model is
iteratively finetuned, leading to less accurate predictions and suboptimal
model selections.
  In this paper, we propose a time-increasing bandit algorithm TI-UCB, which
effectively predicts the increase of model performances due to finetuning and
efficiently balances exploration and exploitation in model selection. To
further capture the converging points of models, we develop a change detection
mechanism by comparing consecutive increase predictions. We theoretically prove
that our algorithm achieves a logarithmic regret upper bound in a typical
increasing bandit setting, which implies a fast convergence rate. The advantage
of our method is also empirically validated through extensive experiments on
classification model selection and online selection of LLMs. Our results
highlight the importance of utilizing increasing-then-converging pattern for
more efficient and economic model selection in the deployment of LLMs.
\\ ( https://arxiv.org/abs/2403.07213 ,  6322kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07221
Date: Tue, 12 Mar 2024 00:26:16 GMT   (2102kb,D)

Title: LookupFFN: Making Transformers Compute-lite for CPU inference
Authors: Zhanpeng Zeng, Michael Davies, Pranav Pulijala, Karthikeyan
  Sankaralingam, Vikas Singh
Categories: cs.LG
Comments: ICML 2023
\\
  While GPU clusters are the de facto choice for training large deep neural
network (DNN) models today, several reasons including ease of workflow,
security and cost have led to efforts investigating whether CPUs may be viable
for inference in routine use in many sectors of the industry. But the imbalance
between the compute capabilities of GPUs and CPUs is huge. Motivated by these
considerations, we study a module which is a workhorse within modern DNN
architectures, GEMM based Feed Forward Networks (FFNs), and assess the extent
to which it can be made compute- (or FLOP-) lite. Specifically, we propose an
alternative formulation (we call it LookupFFN) to GEMM based FFNs inspired by
the recent studies of using Locality Sensitive Hashing (LSH) to approximate
FFNs. Our formulation recasts most essential operations as a memory look-up,
leveraging the trade-off between the two resources on any platform: compute and
memory (since CPUs offer it in abundance). For RoBERTa language model
pretraining, our formulation achieves similar performance compared to GEMM
based FFNs, while dramatically reducing the required FLOP. Our development is
complemented with a detailed hardware profiling of strategies that will
maximize efficiency -- not just on contemporary hardware but on products that
will be offered in the near/medium term future. Code is avaiable at
\url{https://github.com/mlpen/LookupFFN}.
\\ ( https://arxiv.org/abs/2403.07221 ,  2102kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07245
Date: Tue, 12 Mar 2024 02:05:06 GMT   (2410kb,D)

Title: Dataset Condensation for Time Series Classification via Dual Domain
  Matching
Authors: Zhanyu Liu, Ke Hao, Guanjie Zheng, Yanwei Yu
Categories: cs.LG
Comments: Under Review
\\
  Time series data has been demonstrated to be crucial in various research
fields. The management of large quantities of time series data presents
challenges in terms of deep learning tasks, particularly for training a deep
neural network. Recently, a technique named \textit{Dataset Condensation} has
emerged as a solution to this problem. This technique generates a smaller
synthetic dataset that has comparable performance to the full real dataset in
downstream tasks such as classification. However, previous methods are
primarily designed for image and graph datasets, and directly adapting them to
the time series dataset leads to suboptimal performance due to their inability
to effectively leverage the rich information inherent in time series data,
particularly in the frequency domain. In this paper, we propose a novel
framework named Dataset \textit{\textbf{Cond}}ensation for
\textit{\textbf{T}}ime \textit{\textbf{S}}eries
\textit{\textbf{C}}lassification via Dual Domain Matching (\textbf{CondTSC})
which focuses on the time series classification dataset condensation task.
Different from previous methods, our proposed framework aims to generate a
condensed dataset that matches the surrogate objectives in both the time and
frequency domains. Specifically, CondTSC incorporates multi-view data
augmentation, dual domain training, and dual surrogate objectives to enhance
the dataset condensation process in the time and frequency domains. Through
extensive experiments, we demonstrate the effectiveness of our proposed
framework, which outperforms other baselines and learns a condensed synthetic
dataset that exhibits desirable characteristics such as conforming to the
distribution of the original data.
\\ ( https://arxiv.org/abs/2403.07245 ,  2410kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07261
Date: Tue, 12 Mar 2024 02:38:36 GMT   (3468kb,D)

Title: Disentangling Policy from Offline Task Representation Learning via
  Adversarial Data Augmentation
Authors: Chengxing Jia, Fuxiang Zhang, Yi-Chen Li, Chen-Xiao Gao, Xu-Hui Liu,
  Lei Yuan, Zongzhang Zhang, Yang Yu
Categories: cs.LG cs.AI
\\
  Offline meta-reinforcement learning (OMRL) proficiently allows an agent to
tackle novel tasks while solely relying on a static dataset. For precise and
efficient task identification, existing OMRL research suggests learning
separate task representations that be incorporated with policy input, thus
forming a context-based meta-policy. A major approach to train task
representations is to adopt contrastive learning using multi-task offline data.
The dataset typically encompasses interactions from various policies (i.e., the
behavior policies), thus providing a plethora of contextual information
regarding different tasks. Nonetheless, amassing data from a substantial number
of policies is not only impractical but also often unattainable in realistic
settings. Instead, we resort to a more constrained yet practical scenario,
where multi-task data collection occurs with a limited number of policies. We
observed that learned task representations from previous OMRL methods tend to
correlate spuriously with the behavior policy instead of reflecting the
essential characteristics of the task, resulting in unfavorable
out-of-distribution generalization. To alleviate this issue, we introduce a
novel algorithm to disentangle the impact of behavior policy from task
representation learning through a process called adversarial data augmentation.
Specifically, the objective of adversarial data augmentation is not merely to
generate data analogous to offline data distribution; instead, it aims to
create adversarial examples designed to confound learned task representations
and lead to incorrect task identification. Our experiments show that learning
from such adversarial samples significantly enhances the robustness and
effectiveness of the task identification process and realizes satisfactory
out-of-distribution generalization.
\\ ( https://arxiv.org/abs/2403.07261 ,  3468kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07262
Date: Tue, 12 Mar 2024 02:43:41 GMT   (2785kb,D)

Title: Advantage-Aware Policy Optimization for Offline Reinforcement Learning
Authors: Yunpeng Qing, Shunyu liu, Jingyuan Cong, Kaixuan Chen, Yihe Zhou,
  Mingli Song
Categories: cs.LG cs.AI
\\
  Offline Reinforcement Learning (RL) endeavors to leverage offline datasets to
craft effective agent policy without online interaction, which imposes proper
conservative constraints with the support of behavior policies to tackle the
Out-Of-Distribution (OOD) problem. However, existing works often suffer from
the constraint conflict issue when offline datasets are collected from multiple
behavior policies, i.e., different behavior policies may exhibit inconsistent
actions with distinct returns across the state space. To remedy this issue,
recent Advantage-Weighted (AW) methods prioritize samples with high advantage
values for agent training while inevitably leading to overfitting on these
samples. In this paper, we introduce a novel Advantage-Aware Policy
Optimization (A2PO) method to explicitly construct advantage-aware policy
constraints for offline learning under mixed-quality datasets. Specifically,
A2PO employs a Conditional Variational Auto-Encoder (CVAE) to disentangle the
action distributions of intertwined behavior policies by modeling the advantage
values of all training data as conditional variables. Then the agent can follow
such disentangled action distribution constraints to optimize the
advantage-aware policy towards high advantage values. Extensive experiments
conducted on both the single-quality and mixed-quality datasets of the D4RL
benchmark demonstrate that A2PO yields results superior to state-of-the-art
counterparts. Our code will be made publicly available.
\\ ( https://arxiv.org/abs/2403.07262 ,  2785kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07282
Date: Tue, 12 Mar 2024 03:26:58 GMT   (113kb,D)

Title: Enhancing Transfer Learning with Flexible Nonparametric Posterior
  Sampling
Authors: Hyungi Lee, Giung Nam, Edwin Fong, Juho Lee
Categories: cs.LG
Comments: ICLR 2024
\\
  Transfer learning has recently shown significant performance across various
tasks involving deep neural networks. In these transfer learning scenarios, the
prior distribution for downstream data becomes crucial in Bayesian model
averaging (BMA). While previous works proposed the prior over the neural
network parameters centered around the pre-trained solution, such strategies
have limitations when dealing with distribution shifts between upstream and
downstream data. This paper introduces nonparametric transfer learning (NPTL),
a flexible posterior sampling method to address the distribution shift issue
within the context of nonparametric learning. The nonparametric learning (NPL)
method is a recent approach that employs a nonparametric prior for posterior
sampling, efficiently accounting for model misspecification scenarios, which is
suitable for transfer learning scenarios that may involve the distribution
shift between upstream and downstream tasks. Through extensive empirical
validations, we demonstrate that our approach surpasses other baselines in BMA
performance.
\\ ( https://arxiv.org/abs/2403.07282 ,  113kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07294
Date: Tue, 12 Mar 2024 03:54:25 GMT   (2367kb,D)

Title: Graph Data Condensation via Self-expressive Graph Structure
  Reconstruction
Authors: Zhanyu Liu, Chaolv Zeng, Guanjie Zheng
Categories: cs.LG cs.AI cs.SI
\\
  With the increasing demands of training graph neural networks (GNNs) on
large-scale graphs, graph data condensation has emerged as a critical technique
to relieve the storage and time costs during the training phase. It aims to
condense the original large-scale graph to a much smaller synthetic graph while
preserving the essential information necessary for efficiently training a
downstream GNN. However, existing methods concentrate either on optimizing node
features exclusively or endeavor to independently learn node features and the
graph structure generator. They could not explicitly leverage the information
of the original graph structure and failed to construct an interpretable graph
structure for the synthetic dataset. To address these issues, we introduce a
novel framework named \textbf{G}raph Data \textbf{C}ondensation via
\textbf{S}elf-expressive Graph Structure \textbf{R}econstruction
(\textbf{GCSR}). Our method stands out by (1) explicitly incorporating the
original graph structure into the condensing process and (2) capturing the
nuanced interdependencies between the condensed nodes by reconstructing an
interpretable self-expressive graph structure. Extensive experiments and
comprehensive analysis validate the efficacy of the proposed method across
diverse GNN models and datasets. Our code is available at
https://www.dropbox.com/scl/fi/2aonyp5ln5gisdqtjimu8/GCSR.zip?rlkey=11cuwfpsf54wxiiktu0klud0x&dl=0
\\ ( https://arxiv.org/abs/2403.07294 ,  2367kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07300
Date: Tue, 12 Mar 2024 04:04:38 GMT   (5925kb,D)

Title: Taming Pre-trained LLMs for Generalised Time Series Forecasting via
  Cross-modal Knowledge Distillation
Authors: Peiyuan Liu, Hang Guo, Tao Dai, Naiqi Li, Jigang Bao, Xudong Ren, Yong
  Jiang, Shu-Tao Xia
Categories: cs.LG cs.CL
\\
  Multivariate time series forecasting has recently gained great success with
the rapid growth of deep learning models. However, existing approaches usually
train models from scratch using limited temporal data, preventing their
generalization. Recently, with the surge of the Large Language Models (LLMs),
several works have attempted to introduce LLMs into time series forecasting.
Despite promising results, these methods directly take time series as the input
to LLMs, ignoring the inherent modality gap between temporal and text data. In
this work, we propose a novel Large Language Models and time series alignment
framework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time
series forecasting challenge. Based on cross-modal knowledge distillation, the
proposed method exploits both input-agnostic static knowledge and
input-dependent dynamic knowledge in pre-trained LLMs. In this way, it empowers
the forecasting model with favorable performance as well as strong
generalization abilities. Extensive experiments demonstrate the proposed method
establishes a new state of the art for both long- and short-term forecasting.
Code is available at \url{https://github.com/Hank0626/LLaTA}.
\\ ( https://arxiv.org/abs/2403.07300 ,  5925kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07308
Date: Tue, 12 Mar 2024 04:29:43 GMT   (1696kb,D)

Title: Verification-Aided Learning of Neural Network Barrier Functions with
  Termination Guarantees
Authors: Shaoru Chen, Lekan Molu, Mahyar Fazlyab
Categories: cs.LG cs.AI cs.SY eess.SY
Comments: This is an online extended version of the same paper accepted to
  American Control Conference 2024
\\
  Barrier functions are a general framework for establishing a safety guarantee
for a system. However, there is no general method for finding these functions.
To address this shortcoming, recent approaches use self-supervised learning
techniques to learn these functions using training data that are periodically
generated by a verification procedure, leading to a verification-aided learning
framework. Despite its immense potential in automating barrier function
synthesis, the verification-aided learning framework does not have termination
guarantees and may suffer from a low success rate of finding a valid barrier
function in practice. In this paper, we propose a holistic approach to address
these drawbacks. With a convex formulation of the barrier function synthesis,
we propose to first learn an empirically well-behaved NN basis function and
then apply a fine-tuning algorithm that exploits the convexity and
counterexamples from the verification failure to find a valid barrier function
with finite-step termination guarantees: if there exist valid barrier
functions, the fine-tuning algorithm is guaranteed to find one in a finite
number of iterations. We demonstrate that our fine-tuning method can
significantly boost the performance of the verification-aided learning
framework on examples of different scales and using various neural network
verifiers.
\\ ( https://arxiv.org/abs/2403.07308 ,  1696kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07309
Date: Tue, 12 Mar 2024 04:36:41 GMT   (6299kb,D)

Title: Reinforced Sequential Decision-Making for Sepsis Treatment: The POSNEGDM
  Framework with Mortality Classifier and Transformer
Authors: Dipesh Tamboli and Jiayu Chen and Kiran Pranesh Jotheeswaran and Denny
  Yu and Vaneet Aggarwal
Categories: cs.LG cs.AI cs.CY
Comments: Accepted to IEEE Journal of Biomedical and Health Informatics, Mar
  2024
\\
  Sepsis, a life-threatening condition triggered by the body's exaggerated
response to infection, demands urgent intervention to prevent severe
complications. Existing machine learning methods for managing sepsis struggle
in offline scenarios, exhibiting suboptimal performance with survival rates
below 50%. This paper introduces the POSNEGDM -- ``Reinforcement Learning with
Positive and Negative Demonstrations for Sequential Decision-Making" framework
utilizing an innovative transformer-based model and a feedback reinforcer to
replicate expert actions while considering individual patient characteristics.
A mortality classifier with 96.7\% accuracy guides treatment decisions towards
positive outcomes. The POSNEGDM framework significantly improves patient
survival, saving 97.39% of patients, outperforming established machine learning
algorithms (Decision Transformer and Behavioral Cloning) with survival rates of
33.4% and 43.5%, respectively. Additionally, ablation studies underscore the
critical role of the transformer-based decision maker and the integration of a
mortality classifier in enhancing overall survival rates. In summary, our
proposed approach presents a promising avenue for enhancing sepsis treatment
outcomes, contributing to improved patient care and reduced healthcare costs.
\\ ( https://arxiv.org/abs/2403.07309 ,  6299kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07329
Date: Tue, 12 Mar 2024 05:29:48 GMT   (9376kb,D)

Title: Unknown Domain Inconsistency Minimization for Domain Generalization
Authors: Seungjae Shin, HeeSun Bae, Byeonghu Na, Yoon-Yeong Kim and Il-Chul
  Moon
Categories: cs.LG
Comments: 25 pages, 7 figures, Accepted to the twelfth International Conference
  on Learninig Representations (ICLR 24)
\\
  The objective of domain generalization (DG) is to enhance the transferability
of the model learned from a source domain to unobserved domains. To prevent
overfitting to a specific domain, Sharpness-Aware Minimization (SAM) reduces
source domain's loss sharpness. Although SAM variants have delivered
significant improvements in DG, we highlight that there's still potential for
improvement in generalizing to unknown domains through the exploration on data
space. This paper introduces an objective rooted in both parameter and data
perturbed regions for domain generalization, coined Unknown Domain
Inconsistency Minimization (UDIM). UDIM reduces the loss landscape
inconsistency between source domain and unknown domains. As unknown domains are
inaccessible, these domains are empirically crafted by perturbing instances
from the source domain dataset. In particular, by aligning the loss landscape
acquired in the source domain to the loss landscape of perturbed domains, we
expect to achieve generalization grounded on these flat minima for the unknown
domains. Theoretically, we validate that merging SAM optimization with the UDIM
objective establishes an upper bound for the true objective of the DG task. In
an empirical aspect, UDIM consistently outperforms SAM variants across multiple
DG benchmark datasets. Notably, UDIM shows statistically significant
improvements in scenarios with more restrictive domain information,
underscoring UDIM's generalization capability in unseen domains. Our code is
available at \url{https://github.com/SJShin-AI/UDIM}.
\\ ( https://arxiv.org/abs/2403.07329 ,  9376kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07339
Date: Tue, 12 Mar 2024 05:44:27 GMT   (863kb,D)

Title: IM-Unpack: Training and Inference with Arbitrarily Low Precision
  Integers
Authors: Zhanpeng Zeng, Karthikeyan Sankaralingam, Vikas Singh
Categories: cs.LG cs.CL cs.CV
\\
  GEneral Matrix Multiply (GEMM) is a central operation in deep learning and
corresponds to the largest chunk of the compute footprint. Therefore, improving
its efficiency is an active topic of ongoing research. A popular strategy is
the use of low bit-width integers to approximate the original entries in a
matrix. This allows efficiency gains, but often requires sophisticated
techniques to control the rounding error incurred. In this work, we first
verify/check that when the low bit-width restriction is removed, for a variety
of Transformer-based models, whether integers are sufficient for all GEMMs need
-- for {\em both} training and inference stages, and can achieve parity with
floating point counterparts. No sophisticated techniques are needed. We find
that while a large majority of entries in matrices (encountered in such models)
can be easily represented by {\em low} bit-width integers, the existence of a
few heavy hitter entries make it difficult to achieve efficiency gains via the
exclusive use of low bit-width GEMMs alone. To address this issue, we develop a
simple algorithm, Integer Matrix Unpacking (IM-Unpack), to {\em unpack} a
matrix with large integer entries into a larger matrix whose entries all lie
within the representable range of arbitrarily low bit-width integers. This
allows {\em equivalence} with the original GEMM, i.e., the exact result can be
obtained using purely low bit-width integer GEMMs. This comes at the cost of
additional operations -- we show that for many popular models, this overhead is
quite small.
\\ ( https://arxiv.org/abs/2403.07339 ,  863kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07353
Date: Tue, 12 Mar 2024 06:22:10 GMT   (951kb,D)

Title: Graph Unlearning with Efficient Partial Retraining
Authors: Jiahao Zhang, Lin Wang, Shijie Wang, Wenqi Fan
Categories: cs.LG cs.CR
Comments: 8 pages, 3 figures, accepted by The Web Conference 2024
\\
  Graph Neural Networks (GNNs) have achieved remarkable success in various
real-world applications. However, GNNs may be trained on undesirable graph
data, which can degrade their performance and reliability. To enable trained
GNNs to efficiently unlearn unwanted data, a desirable solution is
retraining-based graph unlearning, which partitions the training graph into
subgraphs and trains sub-models on them, allowing fast unlearning through
partial retraining. However, the graph partition process causes information
loss in the training graph, resulting in the low model utility of sub-GNN
models. In this paper, we propose GraphRevoker, a novel graph unlearning
framework that better maintains the model utility of unlearnable GNNs.
Specifically, we preserve the graph property with graph property-aware sharding
and effectively aggregate the sub-GNN models for prediction with graph
contrastive sub-model aggregation. We conduct extensive experiments to
demonstrate the superiority of our proposed approach.
\\ ( https://arxiv.org/abs/2403.07353 ,  951kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07362
Date: Tue, 12 Mar 2024 06:50:32 GMT   (27366kb,D)

Title: Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine
  Unlearning
Authors: Chongyu Fan, Jiancheng Liu, Alfred Hero, Sijia Liu
Categories: cs.LG cs.AI cs.CV
\\
  The trustworthy machine learning (ML) community is increasingly recognizing
the crucial need for models capable of selectively 'unlearning' data points
after training. This leads to the problem of machine unlearning (MU), aiming to
eliminate the influence of chosen data points on model performance, while still
maintaining the model's utility post-unlearning. Despite various MU methods for
data influence erasure, evaluations have largely focused on random data
forgetting, ignoring the vital inquiry into which subset should be chosen to
truly gauge the authenticity of unlearning performance. To tackle this issue,
we introduce a new evaluative angle for MU from an adversarial viewpoint. We
propose identifying the data subset that presents the most significant
challenge for influence erasure, i.e., pinpointing the worst-case forget set.
Utilizing a bi-level optimization principle, we amplify unlearning challenges
at the upper optimization level to emulate worst-case scenarios, while
simultaneously engaging in standard training and unlearning at the lower level,
achieving a balance between data influence erasure and model utility. Our
proposal offers a worst-case evaluation of MU's resilience and effectiveness.
Through extensive experiments across different datasets (including CIFAR-10,
100, CelebA, Tiny ImageNet, and ImageNet) and models (including both image
classifiers and generative models), we expose critical pros and cons in
existing (approximate) unlearning strategies. Our results illuminate the
complex challenges of MU in practice, guiding the future development of more
accurate and robust unlearning algorithms. The code is available at
https://github.com/OPTML-Group/Unlearn-WorstCase.
\\ ( https://arxiv.org/abs/2403.07362 ,  27366kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07379
Date: Tue, 12 Mar 2024 07:32:47 GMT   (15321kb,D)

Title: Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The
  Lengths, Bends, and Dead Ends
Authors: Sidak Pal Singh, Bobby He, Thomas Hofmann, Bernhard Sch\"olkopf
Categories: cs.LG cs.CL stat.ML
Comments: Preprint, 51 pages
\\
  We propose a fresh take on understanding the mechanisms of neural networks by
analyzing the rich structure of parameters contained within their optimization
trajectories. Towards this end, we introduce some natural notions of the
complexity of optimization trajectories, both qualitative and quantitative,
which reveal the inherent nuance and interplay involved between various
optimization choices, such as momentum, weight decay, and batch size. We use
them to provide key hallmarks about the nature of optimization in deep neural
networks: when it goes right, and when it finds itself in a dead end. Further,
thanks to our trajectory perspective, we uncover an intertwined behaviour of
momentum and weight decay that promotes directional exploration, as well as a
directional regularization behaviour of some others. We perform experiments
over large-scale vision and language settings, including large language models
(LLMs) with up to 12 billion parameters, to demonstrate the value of our
approach.
\\ ( https://arxiv.org/abs/2403.07379 ,  15321kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07404
Date: Tue, 12 Mar 2024 08:33:26 GMT   (3603kb,D)

Title: Accelerated Inference and Reduced Forgetting: The Dual Benefits of
  Early-Exit Networks in Continual Learning
Authors: Filip Szatkowski, Fei Yang, Bart{\l}omiej Twardowski, Tomasz
  Trzci\'nski, Joost van de Weijer
Categories: cs.LG cs.AI
\\
  Driven by the demand for energy-efficient employment of deep neural networks,
early-exit methods have experienced a notable increase in research attention.
These strategies allow for swift predictions by making decisions early in the
network, thereby conserving computation time and resources. However, so far the
early-exit networks have only been developed for stationary data distributions,
which restricts their application in real-world scenarios with continuous
non-stationary data. This study aims to explore the continual learning of the
early-exit networks. We adapt existing continual learning methods to fit with
early-exit architectures and investigate their behavior in the continual
setting. We notice that early network layers exhibit reduced forgetting and can
outperform standard networks even when using significantly fewer resources.
Furthermore, we analyze the impact of task-recency bias on early-exit inference
and propose Task-wise Logits Correction (TLC), a simple method that equalizes
this bias and improves the network performance for every given compute budget
in the class-incremental setting. We assess the accuracy and computational cost
of various continual learning techniques enhanced with early-exits and TLC
across standard class-incremental learning benchmarks such as 10 split CIFAR100
and ImageNetSubset and show that TLC can achieve the accuracy of the standard
methods using less than 70\% of their computations. Moreover, at full
computational budget, our method outperforms the accuracy of the standard
counterparts by up to 15 percentage points. Our research underscores the
inherent synergy between early-exit networks and continual learning,
emphasizing their practical utility in resource-constrained environments.
\\ ( https://arxiv.org/abs/2403.07404 ,  3603kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07413
Date: Tue, 12 Mar 2024 08:40:21 GMT   (66kb)

Title: Learning-Augmented Algorithms with Explicit Predictors
Authors: Marek Elias and Haim Kaplan and Yishay Mansour and Shay Moran
Categories: cs.LG cs.DS
\\
  Recent advances in algorithmic design show how to utilize predictions
obtained by machine learning models from past and present data. These
approaches have demonstrated an enhancement in performance when the predictions
are accurate, while also ensuring robustness by providing worst-case guarantees
when predictions fail. In this paper we focus on online problems; prior
research in this context was focused on a paradigm where the predictor is
pre-trained on past data and then used as a black box (to get the predictions
it was trained for). In contrast, in this work, we unpack the predictor and
integrate the learning problem it gives rise for within the algorithmic
challenge. In particular we allow the predictor to learn as it receives larger
parts of the input, with the ultimate goal of designing online learning
algorithms specifically tailored for the algorithmic task at hand. Adopting
this perspective, we focus on a number of fundamental problems, including
caching and scheduling, which have been well-studied in the black-box setting.
For each of the problems we consider, we introduce new algorithms that take
advantage of explicit learning algorithms which we carefully design towards
optimizing the overall performance. We demonstrate the potential of our
approach by deriving performance bounds which improve over those established in
previous work.
\\ ( https://arxiv.org/abs/2403.07413 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07442
Date: Tue, 12 Mar 2024 09:32:41 GMT   (1005kb,D)

Title: Proxy Methods for Domain Adaptation
Authors: Katherine Tsai, Stephen R. Pfohl, Olawale Salaudeen, Nicole Chiou,
  Matt J. Kusner, Alexander D'Amour, Sanmi Koyejo, Arthur Gretton
Categories: cs.LG stat.ML
\\
  We study the problem of domain adaptation under distribution shift, where the
shift is due to a change in the distribution of an unobserved, latent variable
that confounds both the covariates and the labels. In this setting, neither the
covariate shift nor the label shift assumptions apply. Our approach to
adaptation employs proximal causal learning, a technique for estimating causal
effects in settings where proxies of unobserved confounders are available. We
demonstrate that proxy variables allow for adaptation to distribution shift
without explicitly recovering or modeling latent variables. We consider two
settings, (i) Concept Bottleneck: an additional ''concept'' variable is
observed that mediates the relationship between the covariates and labels; (ii)
Multi-domain: training data from multiple source domains is available, where
each source domain exhibits a different distribution over the latent
confounder. We develop a two-stage kernel estimation approach to adapt to
complex distribution shifts in both settings. In our experiments, we show that
our approach outperforms other methods, notably those which explicitly recover
the latent confounder.
\\ ( https://arxiv.org/abs/2403.07442 ,  1005kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07456
Date: Tue, 12 Mar 2024 09:51:05 GMT   (2188kb,D)

Title: A tutorial on multi-view autoencoders using the multi-view-AE library
Authors: Ana Lawry Aguila, Andre Altmann
Categories: cs.LG stat.ML
\\
  There has been a growing interest in recent years in modelling multiple
modalities (or views) of data to for example, understand the relationship
between modalities or to generate missing data. Multi-view autoencoders have
gained significant traction for their adaptability and versatility in modelling
multi-modal data, demonstrating an ability to tailor their approach to suit the
characteristics of the data at hand. However, most multi-view autoencoders have
inconsistent notation and are often implemented using different coding
frameworks. To address this, we present a unified mathematical framework for
multi-view autoencoders, consolidating their formulations. Moreover, we offer
insights into the motivation and theoretical advantages of each model. To
facilitate accessibility and practical use, we extend the documentation and
functionality of the previously introduced \texttt{multi-view-AE} library. This
library offers Python implementations of numerous multi-view autoencoder
models, presented within a user-friendly framework. Through benchmarking
experiments, we evaluate our implementations against previous ones,
demonstrating comparable or superior performance. This work aims to establish a
cohesive foundation for multi-modal modelling, serving as a valuable
educational resource in the field.
\\ ( https://arxiv.org/abs/2403.07456 ,  2188kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07460
Date: Tue, 12 Mar 2024 09:57:45 GMT   (770kb,D)

Title: Experimental Comparison of Ensemble Methods and Time-to-Event Analysis
  Models Through Integrated Brier Score and Concordance Index
Authors: Camila Fernandez (LPSM), Chung Shue Chen, Chen Pierre Gaillard, Alonso
  Silva
Categories: cs.LG
\\
  Time-to-event analysis is a branch of statistics that has increased in
popularity during the last decades due to its many application fields, such as
predictive maintenance, customer churn prediction and population lifetime
estimation. In this paper, we review and compare the performance of several
prediction models for time-to-event analysis. These consist of semi-parametric
and parametric statistical models, in addition to machine learning approaches.
Our study is carried out on three datasets and evaluated in two different
scores (the integrated Brier score and concordance index). Moreover, we show
how ensemble methods, which surprisingly have not yet been much studied in
time-to-event analysis, can improve the prediction accuracy and enhance the
robustness of the prediction performance. We conclude the analysis with a
simulation experiment in which we evaluate the factors influencing the
performance ranking of the methods using both scores.
\\ ( https://arxiv.org/abs/2403.07460 ,  770kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07472
Date: Tue, 12 Mar 2024 10:08:36 GMT   (70kb,D)

Title: Imbalance-aware Presence-only Loss Function for Species Distribution
  Modeling
Authors: Robin Zbinden, Nina van Tiel, Marc Ru{\ss}wurm, Devis Tuia
Categories: cs.LG
Comments: Tackling Climate Change with Machine Learning at ICLR 2024
\\
  In the face of significant biodiversity decline, species distribution models
(SDMs) are essential for understanding the impact of climate change on species
habitats by connecting environmental conditions to species occurrences.
Traditionally limited by a scarcity of species observations, these models have
significantly improved in performance through the integration of larger
datasets provided by citizen science initiatives. However, they still suffer
from the strong class imbalance between species within these datasets, often
resulting in the penalization of rare species--those most critical for
conservation efforts. To tackle this issue, this study assesses the
effectiveness of training deep learning models using a balanced presence-only
loss function on large citizen science-based datasets. We demonstrate that this
imbalance-aware loss function outperforms traditional loss functions across
various datasets and tasks, particularly in accurately modeling rare species
with limited observations.
\\ ( https://arxiv.org/abs/2403.07472 ,  70kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07483
Date: Tue, 12 Mar 2024 10:18:59 GMT   (574kb,D)

Title: A Deep Learning Approach to Diabetes Diagnosis
Authors: Zeyu Zhang, Khandaker Asif Ahmed, Md Rakibul Hasan, Tom Gedeon, Md
  Zakir Hossain
Categories: cs.LG cs.AI
Comments: Accepted to ACIIDS 2024
\\
  Diabetes, resulting from inadequate insulin production or utilization, causes
extensive harm to the body. Existing diagnostic methods are often invasive and
come with drawbacks, such as cost constraints. Although there are machine
learning models like Classwise k Nearest Neighbor (CkNN) and General Regression
Neural Network (GRNN), they struggle with imbalanced data and result in
under-performance. Leveraging advancements in sensor technology and machine
learning, we propose a non-invasive diabetes diagnosis using a Back Propagation
Neural Network (BPNN) with batch normalization, incorporating data re-sampling
and normalization for class balancing. Our method addresses existing challenges
such as limited performance associated with traditional machine learning.
Experimental results on three datasets show significant improvements in overall
accuracy, sensitivity, and specificity compared to traditional methods.
Notably, we achieve accuracies of 89.81% in Pima diabetes dataset, 75.49% in
CDC BRFSS2015 dataset, and 95.28% in Mesra Diabetes dataset. This underscores
the potential of deep learning models for robust diabetes diagnosis. See
project website https://steve-zeyu-zhang.github.io/DiabetesDiagnosis/
\\ ( https://arxiv.org/abs/2403.07483 ,  574kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07486
Date: Tue, 12 Mar 2024 10:21:31 GMT   (1609kb,D)

Title: XpertAI: uncovering model strategies for sub-manifolds
Authors: Simon Letzgus, Klaus-Robert M\"uller, and Gr\'egoire Montavon
Categories: cs.LG
\\
  In recent years, Explainable AI (XAI) methods have facilitated profound
validation and knowledge extraction from ML models. While extensively studied
for classification, few XAI solutions have addressed the challenges specific to
regression models. In regression, explanations need to be precisely formulated
to address specific user queries (e.g.\ distinguishing between `Why is the
output above 0?' and `Why is the output above 50?'). They should furthermore
reflect the model's behavior on the relevant data sub-manifold. In this paper,
we introduce XpertAI, a framework that disentangles the prediction strategy
into multiple range-specific sub-strategies and allows the formulation of
precise queries about the model (the `explanandum') as a linear combination of
those sub-strategies. XpertAI is formulated generally to work alongside popular
XAI attribution techniques, based on occlusion, gradient integration, or
reverse propagation. Qualitative and quantitative results, demonstrate the
benefits of our approach.
\\ ( https://arxiv.org/abs/2403.07486 ,  1609kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07501
Date: Tue, 12 Mar 2024 10:38:54 GMT   (935kb,D)

Title: Detecting Security-Relevant Methods using Multi-label Machine Learning
Authors: Oshando Johnson, Goran Piskachev, Ranjith Krishnamurthy, Eric Bodden
Categories: cs.LG
Comments: 6 pages, 3 figures, The IDE Workshop
DOI: 10.1145/3643796.3648464
\\
  To detect security vulnerabilities, static analysis tools need to be
configured with security-relevant methods. Current approaches can automatically
identify such methods using binary relevance machine learning approaches.
However, they ignore dependencies among security-relevant methods,
over-generalize and perform poorly in practice. Additionally, users have to
nevertheless manually configure static analysis tools using the detected
methods. Based on feedback from users and our observations, the excessive
manual steps can often be tedious, error-prone and counter-intuitive.
  In this paper, we present Dev-Assist, an IntelliJ IDEA plugin that detects
security-relevant methods using a multi-label machine learning approach that
considers dependencies among labels. The plugin can automatically generate
configurations for static analysis tools, run the static analysis, and show the
results in IntelliJ IDEA. Our experiments reveal that Dev-Assist's machine
learning approach has a higher F1-Measure than related approaches. Moreover,
the plugin reduces and simplifies the manual effort required when configuring
and using static analysis tools.
\\ ( https://arxiv.org/abs/2403.07501 ,  935kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07503
Date: Tue, 12 Mar 2024 10:42:32 GMT   (2433kb,D)

Title: Constrained Optimal Fuel Consumption of HEV: A Constrained Reinforcement
  Learning Approach
Authors: Shuchang Yan
Categories: cs.LG
\\
  Hybrid electric vehicles (HEVs) are becoming increasingly popular because
they can better combine the working characteristics of internal combustion
engines and electric motors. However, the minimum fuel consumption of an HEV
for a battery electrical balance case under a specific assembly condition and a
specific speed curve still needs to be clarified in academia and industry.
Regarding this problem, this work provides the mathematical expression of
constrained optimal fuel consumption (COFC) from the perspective of constrained
reinforcement learning (CRL) for the first time globally. Also, two mainstream
approaches of CRL, constrained variational policy optimization (CVPO) and
Lagrangian-based approaches, are utilized for the first time to obtain the
vehicle's minimum fuel consumption under the battery electrical balance
condition. We conduct case studies on the well-known Prius TOYOTA hybrid system
(THS) under the NEDC condition; we give vital steps to implement CRL approaches
and compare the performance between the CVPO and Lagrangian-based approaches.
Our case study found that CVPO and Lagrangian-based approaches can obtain the
lowest fuel consumption while maintaining the SOC balance constraint. The CVPO
approach converges stable, but the Lagrangian-based approach can obtain the
lowest fuel consumption at 3.95 L/100km, though with more significant
oscillations. This result verifies the effectiveness of our proposed CRL
approaches to the COFC problem.
\\ ( https://arxiv.org/abs/2403.07503 ,  2433kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07586
Date: Tue, 12 Mar 2024 12:16:40 GMT   (1323kb,D)

Title: Federated Learning of Socially Appropriate Agent Behaviours in Simulated
  Home Environments
Authors: Saksham Checker and Nikhil Churamani and Hatice Gunes
Categories: cs.LG cs.AI cs.CY cs.RO
Comments: Accepted at the Workshop on Lifelong Learning and Personalization in
  Long-Term Human-Robot Interaction (LEAP-HRI) at the 19th ACM/IEEE
  International Conference on Human-Robot Interaction (HRI), 2024
\\
  As social robots become increasingly integrated into daily life, ensuring
their behaviours align with social norms is crucial. For their widespread
open-world application, it is important to explore Federated Learning (FL)
settings where individual robots can learn about their unique environments
while also learning from each others' experiences. In this paper, we present a
novel FL benchmark that evaluates different strategies, using multi-label
regression objectives, where each client individually learns to predict the
social appropriateness of different robot actions while also sharing their
learning with others. Furthermore, splitting the training data by different
contexts such that each client incrementally learns across contexts, we present
a novel Federated Continual Learning (FCL) benchmark that adapts FL-based
methods to use state-of-the-art Continual Learning (CL) methods to continually
learn socially appropriate agent behaviours under different contextual
settings. Federated Averaging (FedAvg) of weights emerges as a robust FL
strategy while rehearsal-based FCL enables incrementally learning the social
appropriateness of robot actions, across contextual splits.
\\ ( https://arxiv.org/abs/2403.07586 ,  1323kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07588
Date: Tue, 12 Mar 2024 12:18:55 GMT   (18756kb,D)

Title: Visual Privacy Auditing with Diffusion Models
Authors: Kristian Schwethelm, Johannes Kaiser, Moritz Knolle, Daniel Rueckert,
  Georgios Kaissis, Alexander Ziller
Categories: cs.LG cs.CR
\\
  Image reconstruction attacks on machine learning models pose a significant
risk to privacy by potentially leaking sensitive information. Although
defending against such attacks using differential privacy (DP) has proven
effective, determining appropriate DP parameters remains challenging. Current
formal guarantees on data reconstruction success suffer from overly theoretical
assumptions regarding adversary knowledge about the target data, particularly
in the image domain. In this work, we empirically investigate this discrepancy
and find that the practicality of these assumptions strongly depends on the
domain shift between the data prior and the reconstruction target. We propose a
reconstruction attack based on diffusion models (DMs) that assumes adversary
access to real-world image priors and assess its implications on privacy
leakage under DP-SGD. We show that (1) real-world data priors significantly
influence reconstruction success, (2) current reconstruction bounds do not
model the risk posed by data priors well, and (3) DMs can serve as effective
auditing tools for visualizing privacy leakage.
\\ ( https://arxiv.org/abs/2403.07588 ,  18756kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07591
Date: Tue, 12 Mar 2024 12:24:11 GMT   (776kb,D)

Title: Robustifying and Boosting Training-Free Neural Architecture Search
Authors: Zhenfeng He, Yao Shu, Zhongxiang Dai, Bryan Kian Hsiang Low
Categories: cs.LG
Comments: Accepted by ICLR 2024. Code available at
  https://github.com/hzf1174/RoBoT
\\
  Neural architecture search (NAS) has become a key component of AutoML and a
standard tool to automate the design of deep neural networks. Recently,
training-free NAS as an emerging paradigm has successfully reduced the search
costs of standard training-based NAS by estimating the true architecture
performance with only training-free metrics. Nevertheless, the estimation
ability of these metrics typically varies across different tasks, making it
challenging to achieve robust and consistently good search performance on
diverse tasks with only a single training-free metric. Meanwhile, the
estimation gap between training-free metrics and the true architecture
performances limits training-free NAS to achieve superior performance. To
address these challenges, we propose the robustifying and boosting
training-free NAS (RoBoT) algorithm which (a) employs the optimized combination
of existing training-free metrics explored from Bayesian optimization to
develop a robust and consistently better-performing metric on diverse tasks,
and (b) applies greedy search, i.e., the exploitation, on the newly developed
metric to bridge the aforementioned gap and consequently to boost the search
performance of standard training-free NAS further. Remarkably, the expected
performance of our RoBoT can be theoretically guaranteed, which improves over
the existing training-free NAS under mild conditions with additional
interesting insights. Our extensive experiments on various NAS benchmark tasks
yield substantial empirical evidence to support our theoretical results.
\\ ( https://arxiv.org/abs/2403.07591 ,  776kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07603
Date: Tue, 12 Mar 2024 12:40:23 GMT   (1739kb,D)

Title: ProPML: Probability Partial Multi-label Learning
Authors: {\L}ukasz Struski, Adam Pardyl, Jacek Tabor, Bartosz Zieli\'nski
Categories: cs.LG
Comments: Accepted to the International Conference on Data Science and Advanced
  Analytics (DSAA 2023)
\\
  Partial Multi-label Learning (PML) is a type of weakly supervised learning
where each training instance corresponds to a set of candidate labels, among
which only some are true. In this paper, we introduce \our{}, a novel
probabilistic approach to this problem that extends the binary cross entropy to
the PML setup. In contrast to existing methods, it does not require suboptimal
disambiguation and, as such, can be applied to any deep architecture.
Furthermore, experiments conducted on artificial and real-world datasets
indicate that \our{} outperforms existing approaches, especially for high noise
in a candidate set.
\\ ( https://arxiv.org/abs/2403.07603 ,  1739kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07611
Date: Tue, 12 Mar 2024 12:49:47 GMT   (337kb,D)

Title: Efficient Knowledge Deletion from Trained Models through Layer-wise
  Partial Machine Unlearning
Authors: Vinay Chakravarthi Gogineni and Esmaeil S. Nadimi
Categories: cs.LG cs.AI
Comments: 16pages, 4 figures
\\
  Machine unlearning has garnered significant attention due to its ability to
selectively erase knowledge obtained from specific training data samples in an
already trained machine learning model. This capability enables data holders to
adhere strictly to data protection regulations. However, existing unlearning
techniques face practical constraints, often causing performance degradation,
demanding brief fine-tuning post unlearning, and requiring significant storage.
In response, this paper introduces a novel class of machine unlearning
algorithms. First method is partial amnesiac unlearning, integration of
layer-wise pruning with amnesiac unlearning. In this method, updates made to
the model during training are pruned and stored, subsequently used to forget
specific data from trained model. The second method assimilates layer-wise
partial-updates into label-flipping and optimization-based unlearning to
mitigate the adverse effects of data deletion on model efficacy. Through a
detailed experimental evaluation, we showcase the effectiveness of proposed
unlearning methods. Experimental results highlight that the partial amnesiac
unlearning not only preserves model efficacy but also eliminates the necessity
for brief post fine-tuning, unlike conventional amnesiac unlearning. Moreover,
employing layer-wise partial updates in label-flipping and optimization-based
unlearning techniques demonstrates superiority in preserving model efficacy
compared to their naive counterparts.
\\ ( https://arxiv.org/abs/2403.07611 ,  337kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07632
Date: Tue, 12 Mar 2024 13:12:24 GMT   (2194kb)

Title: CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs
  for Reduced hERG Liability
Authors: Gregory W. Kyro, Matthew T. Martin, Eric D. Watt, Victor S. Batista
Categories: cs.LG q-bio.BM
\\
  Drug-induced cardiotoxicity is a major health concern which can lead to
serious adverse effects including life-threatening cardiac arrhythmias via the
blockade of the voltage-gated hERG potassium ion channel. It is therefore of
tremendous interest to develop advanced methods to identify hERG-active
compounds in early stages of drug development, as well as to optimize
commercially available drugs for reduced hERG activity. In this work, we
present CardioGenAI, a machine learning-based framework for re-engineering both
developmental and marketed drugs for reduced hERG activity while preserving
their pharmacological activity. The framework incorporates novel
state-of-the-art discriminative models for predicting hERG channel activity, as
well as activity against the voltage-gated NaV1.5 and CaV1.2 channels due to
their potential implications in modulating the arrhythmogenic potential induced
by hERG channel blockade. These models can also serve independently as
effective components of a virtual screening pipeline. We applied the complete
framework to pimozide, an FDA-approved antipsychotic agent that demonstrates
high affinity to the hERG channel, and generated 100 refined candidates.
Remarkably, among the candidates is fluspirilene, a compound which is of the
same class of drugs (diphenylmethanes) as pimozide and therefore has similar
pharmacological activity, yet exhibits over 700-fold weaker binding to hERG. We
have made all of our software open-source to facilitate integration of the
CardioGenAI framework for molecular hypothesis generation into drug discovery
workflows.
\\ ( https://arxiv.org/abs/2403.07632 ,  2194kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07652
Date: Tue, 12 Mar 2024 13:41:15 GMT   (7836kb,D)

Title: Harder Tasks Need More Experts: Dynamic Routing in MoE Models
Authors: Quzhe Huang, Zhenwei An, Nan Zhuang, Mingxu Tao, Chen Zhang, Yang Jin,
  Kun Xu, Kun Xu, Liwei Chen, Songfang Huang, Yansong Feng
Categories: cs.LG cs.CL
\\
  In this paper, we introduce a novel dynamic expert selection framework for
Mixture of Experts (MoE) models, aiming to enhance computational efficiency and
model performance by adjusting the number of activated experts based on input
difficulty. Unlike traditional MoE approaches that rely on fixed Top-K routing,
which activates a predetermined number of experts regardless of the input's
complexity, our method dynamically selects experts based on the confidence
level in expert selection for each input. This allows for a more efficient
utilization of computational resources, activating more experts for complex
tasks requiring advanced reasoning and fewer for simpler tasks. Through
extensive evaluations, our dynamic routing method demonstrates substantial
improvements over conventional Top-2 routing across various benchmarks,
achieving an average improvement of 0.7% with less than 90% activated
parameters. Further analysis shows our model dispatches more experts to tasks
requiring complex reasoning skills, like BBH, confirming its ability to
dynamically allocate computational resources in alignment with the input's
complexity. Our findings also highlight a variation in the number of experts
needed across different layers of the transformer model, offering insights into
the potential for designing heterogeneous MoE frameworks. The code and models
are available at https://github.com/ZhenweiAn/Dynamic_MoE.
\\ ( https://arxiv.org/abs/2403.07652 ,  7836kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07657
Date: Tue, 12 Mar 2024 13:47:50 GMT   (11758kb,D)

Title: Scalable Spatiotemporal Prediction with Bayesian Neural Fields
Authors: Feras Saad, Jacob Burnim, Colin Carroll, Brian Patton, Urs K\"oster,
  Rif A. Saurous, Matthew Hoffman
Categories: cs.LG cs.AI stat.AP stat.ME
Comments: 22 pages, 6 figures, 3 tables
\\
  Spatiotemporal datasets, which consist of spatially-referenced time series,
are ubiquitous in many scientific and business-intelligence applications, such
as air pollution monitoring, disease tracking, and cloud-demand forecasting. As
modern datasets continue to increase in size and complexity, there is a growing
need for new statistical methods that are flexible enough to capture complex
spatiotemporal dynamics and scalable enough to handle large prediction
problems. This work presents the Bayesian Neural Field (BayesNF), a
domain-general statistical model for inferring rich probability distributions
over a spatiotemporal domain, which can be used for data-analysis tasks
including forecasting, interpolation, and variography. BayesNF integrates a
novel deep neural network architecture for high-capacity function estimation
with hierarchical Bayesian inference for robust uncertainty quantification. By
defining the prior through a sequence of smooth differentiable transforms,
posterior inference is conducted on large-scale data using variationally
learned surrogates trained via stochastic gradient descent. We evaluate BayesNF
against prominent statistical and machine-learning baselines, showing
considerable improvements on diverse prediction problems from climate and
public health datasets that contain tens to hundreds of thousands of
measurements. The paper is accompanied with an open-source software package
(https://github.com/google/bayesnf) that is easy-to-use and compatible with
modern GPU and TPU accelerators on the JAX machine learning platform.
\\ ( https://arxiv.org/abs/2403.07657 ,  11758kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07669
Date: Tue, 12 Mar 2024 14:00:50 GMT   (384kb)

Title: Machine Learning for Soccer Match Result Prediction
Authors: Rory Bunker, Calvin Yeung, Keisuke Fujii
Categories: cs.LG
\\
  Machine learning has become a common approach to predicting the outcomes of
soccer matches, and the body of literature in this domain has grown
substantially in the past decade and a half. This chapter discusses available
datasets, the types of models and features, and ways of evaluating model
performance in this application domain. The aim of this chapter is to give a
broad overview of the current state and potential future developments in
machine learning for soccer match results prediction, as a resource for those
interested in conducting future studies in the area. Our main findings are that
while gradient-boosted tree models such as CatBoost, applied to soccer-specific
ratings such as pi-ratings, are currently the best-performing models on
datasets containing only goals as the match features, there needs to be a more
thorough comparison of the performance of deep learning models and Random
Forest on a range of datasets with different types of features. Furthermore,
new rating systems using both player- and team-level information and
incorporating additional information from, e.g., spatiotemporal tracking and
event data, could be investigated further. Finally, the interpretability of
match result prediction models needs to be enhanced for them to be more useful
for team management.
\\ ( https://arxiv.org/abs/2403.07669 ,  384kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07688
Date: Tue, 12 Mar 2024 14:28:06 GMT   (1217kb,D)

Title: Maxwell's Demon at Work: Efficient Pruning by Leveraging Saturation of
  Neurons
Authors: Simon Dufort-Labb\'e, Pierluca D'Oro, Evgenii Nikishin, Razvan
  Pascanu, Pierre-Luc Bacon, Aristide Baratin
Categories: cs.LG cs.AI
\\
  When training deep neural networks, the phenomenon of $\textit{dying
neurons}$ $\unicode{x2013}$units that become inactive or saturated, output zero
during training$\unicode{x2013}$ has traditionally been viewed as undesirable,
linked with optimization challenges, and contributing to plasticity loss in
continual learning scenarios. In this paper, we reassess this phenomenon,
focusing on sparsity and pruning. By systematically exploring the impact of
various hyperparameter configurations on dying neurons, we unveil their
potential to facilitate simple yet effective structured pruning algorithms. We
introduce $\textit{Demon Pruning}$ (DemP), a method that controls the
proliferation of dead neurons, dynamically leading to network sparsity.
Achieved through a combination of noise injection on active units and a
one-cycled schedule regularization strategy, DemP stands out for its simplicity
and broad applicability. Experiments on CIFAR10 and ImageNet datasets
demonstrate that DemP surpasses existing structured pruning techniques,
showcasing superior accuracy-sparsity tradeoffs and training speedups. These
findings suggest a novel perspective on dying neurons as a valuable resource
for efficient model compression and optimization.
\\ ( https://arxiv.org/abs/2403.07688 ,  1217kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07704
Date: Tue, 12 Mar 2024 14:49:19 GMT   (13317kb,D)

Title: Symmetric Q-learning: Reducing Skewness of Bellman Error in Online
  Reinforcement Learning
Authors: Motoki Omura, Takayuki Osa, Yusuke Mukuta, Tatsuya Harada
Categories: cs.LG cs.AI
Comments: Accepted at AAAI 2024: The 38th Annual AAAI Conference on Artificial
  Intelligence (Main Tech Track)
\\
  In deep reinforcement learning, estimating the value function to evaluate the
quality of states and actions is essential. The value function is often trained
using the least squares method, which implicitly assumes a Gaussian error
distribution. However, a recent study suggested that the error distribution for
training the value function is often skewed because of the properties of the
Bellman operator, and violates the implicit assumption of normal error
distribution in the least squares method. To address this, we proposed a method
called Symmetric Q-learning, in which the synthetic noise generated from a
zero-mean distribution is added to the target values to generate a Gaussian
error distribution. We evaluated the proposed method on continuous control
benchmark tasks in MuJoCo. It improved the sample efficiency of a
state-of-the-art reinforcement learning method by reducing the skewness of the
error distribution.
\\ ( https://arxiv.org/abs/2403.07704 ,  13317kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07718
Date: Tue, 12 Mar 2024 14:58:45 GMT   (4544kb,D)

Title: WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work
  Tasks?
Authors: Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji,
  Manuel Del Verme, Tom Marty, L\'eo Boisvert, Megh Thakkar, Quentin Cappart,
  David Vazquez, Nicolas Chapados, Alexandre Lacoste
Categories: cs.LG cs.AI
Comments: 27 pages, 10 figures, preprint
\\
  We study the use of large language model-based agents for interacting with
software via web browsers. Unlike prior work, we focus on measuring the agents'
ability to perform tasks that span the typical daily work of knowledge workers
utilizing enterprise software systems. To this end, we propose WorkArena, a
remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow
platform. We also introduce BrowserGym, an environment for the design and
evaluation of such agents, offering a rich set of actions as well as multimodal
observations. Our empirical evaluation reveals that while current agents show
promise on WorkArena, there remains a considerable gap towards achieving full
task automation. Notably, our analysis uncovers a significant performance
disparity between open and closed-source LLMs, highlighting a critical area for
future exploration and development in the field.
\\ ( https://arxiv.org/abs/2403.07718 ,  4544kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07723
Date: Tue, 12 Mar 2024 15:01:17 GMT   (65kb)

Title: On the Last-Iterate Convergence of Shuffling Gradient Methods
Authors: Zijian Liu, Zhengyuan Zhou
Categories: cs.LG math.OC stat.ML
Comments: In submission (comments welcome)
\\
  Shuffling gradient methods, which are also known as stochastic gradient
descent (SGD) without replacement, are widely implemented in practice,
particularly including three popular algorithms: Random Reshuffle (RR), Shuffle
Once (SO), and Incremental Gradient (IG). Compared to the empirical success,
the theoretical guarantee of shuffling gradient methods was not
well-understanding for a long time. Until recently, the convergence rates had
just been established for the average iterate for convex functions and the last
iterate for strongly convex problems (using squared distance as the metric).
However, when using the function value gap as the convergence criterion,
existing theories cannot interpret the good performance of the last iterate in
different settings (e.g., constrained optimization). To bridge this gap between
practice and theory, we prove last-iterate convergence rates for shuffling
gradient methods with respect to the objective value even without strong
convexity. Our new results either (nearly) match the existing last-iterate
lower bounds or are as fast as the previous best upper bounds for the average
iterate.
\\ ( https://arxiv.org/abs/2403.07723 ,  65kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07724
Date: Tue, 12 Mar 2024 15:01:27 GMT   (17987kb,D)

Title: Balancing Fairness and Accuracy in Data-Restricted Binary Classification
Authors: Zachary McBride Lazri, Danial Dervovic, Antigoni Polychroniadou, Ivan
  Brugere, Dana Dachman-Soled, and Min Wu
Categories: cs.LG cs.AI cs.CY stat.ML
\\
  Applications that deal with sensitive information may have restrictions
placed on the data available to a machine learning (ML) classifier. For
example, in some applications, a classifier may not have direct access to
sensitive attributes, affecting its ability to produce accurate and fair
decisions. This paper proposes a framework that models the trade-off between
accuracy and fairness under four practical scenarios that dictate the type of
data available for analysis. Prior works examine this trade-off by analyzing
the outputs of a scoring function that has been trained to implicitly learn the
underlying distribution of the feature vector, class label, and sensitive
attribute of a dataset. In contrast, our framework directly analyzes the
behavior of the optimal Bayesian classifier on this underlying distribution by
constructing a discrete approximation it from the dataset itself. This approach
enables us to formulate multiple convex optimization problems, which allow us
to answer the question: How is the accuracy of a Bayesian classifier affected
in different data restricting scenarios when constrained to be fair? Analysis
is performed on a set of fairness definitions that include group and individual
fairness. Experiments on three datasets demonstrate the utility of the proposed
framework as a tool for quantifying the trade-offs among different fairness
notions and their distributional dependencies.
\\ ( https://arxiv.org/abs/2403.07724 ,  17987kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07797
Date: Tue, 12 Mar 2024 16:34:07 GMT   (366kb,D)

Title: Joint Selection: Adaptively Incorporating Public Information for Private
  Synthetic Data
Authors: Miguel Fuentes, Brett Mullins, Ryan McKenna, Gerome Miklau, Daniel
  Sheldon
Categories: cs.LG cs.AI
\\
  Mechanisms for generating differentially private synthetic data based on
marginals and graphical models have been successful in a wide range of
settings. However, one limitation of these methods is their inability to
incorporate public data. Initializing a data generating model by pre-training
on public data has shown to improve the quality of synthetic data, but this
technique is not applicable when model structure is not determined a priori. We
develop the mechanism jam-pgm, which expands the adaptive measurements
framework to jointly select between measuring public data and private data.
This technique allows for public data to be included in a graphical-model-based
mechanism. We show that jam-pgm is able to outperform both publicly assisted
and non publicly assisted synthetic data generation mechanisms even when the
public data distribution is biased.
\\ ( https://arxiv.org/abs/2403.07797 ,  366kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07809
Date: Tue, 12 Mar 2024 16:46:54 GMT   (1242kb,D)

Title: pyvene: A Library for Understanding and Improving PyTorch Models via
  Interventions
Authors: Zhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang,
  Noah D. Goodman, Christopher D. Manning, Christopher Potts
Categories: cs.LG cs.CL
Comments: 8 pages, 3 figures
\\
  Interventions on model-internal states are fundamental operations in many
areas of AI, including model editing, steering, robustness, and
interpretability. To facilitate such research, we introduce $\textbf{pyvene}$,
an open-source Python library that supports customizable interventions on a
range of different PyTorch modules. $\textbf{pyvene}$ supports complex
intervention schemes with an intuitive configuration format, and its
interventions can be static or include trainable parameters. We show how
$\textbf{pyvene}$ provides a unified and extensible framework for performing
interventions on neural models and sharing the intervened upon models with
others. We illustrate the power of the library via interpretability analyses
using causal abstraction and knowledge localization. We publish our library
through Python Package Index (PyPI) and provide code, documentation, and
tutorials at https://github.com/stanfordnlp/pyvene.
\\ ( https://arxiv.org/abs/2403.07809 ,  1242kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07815
Date: Tue, 12 Mar 2024 16:53:54 GMT   (1128kb,D)

Title: Chronos: Learning the Language of Time Series
Authors: Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro
  Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian
  Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix, Michael
  W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider,
  Yuyang Wang
Categories: cs.LG cs.AI
Comments: Inference code and model checkpoints available at
  https://github.com/amazon-science/chronos-forecasting
\\
  We introduce Chronos, a simple yet effective framework for pretrained
probabilistic time series models. Chronos tokenizes time series values using
scaling and quantization into a fixed vocabulary and trains existing
transformer-based language model architectures on these tokenized time series
via the cross-entropy loss. We pretrained Chronos models based on the T5 family
(ranging from 20M to 710M parameters) on a large collection of publicly
available datasets, complemented by a synthetic dataset that we generated via
Gaussian processes to improve generalization. In a comprehensive benchmark
consisting of 42 datasets, and comprising both classical local models and deep
learning methods, we show that Chronos models: (a) significantly outperform
other methods on datasets that were part of the training corpus; and (b) have
comparable and occasionally superior zero-shot performance on new datasets,
relative to methods that were trained specifically on them. Our results
demonstrate that Chronos models can leverage time series data from diverse
domains to improve zero-shot accuracy on unseen forecasting tasks, positioning
pretrained models as a viable tool to greatly simplify forecasting pipelines.
\\ ( https://arxiv.org/abs/2403.07815 ,  1128kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07842
Date: Tue, 12 Mar 2024 17:27:49 GMT   (15834kb,D)

Title: Quantifying and Mitigating Privacy Risks for Tabular Generative Models
Authors: Chaoyi Zhu, Jiayi Tang, Hans Brouwer, Juan F. P\'erez, Marten van
  Dijk, Lydia Y. Chen
Categories: cs.LG cs.CR
\\
  Synthetic data from generative models emerges as the privacy-preserving
data-sharing solution. Such a synthetic data set shall resemble the original
data without revealing identifiable private information. The backbone
technology of tabular synthesizers is rooted in image generative models,
ranging from Generative Adversarial Networks (GANs) to recent diffusion models.
Recent prior work sheds light on the utility-privacy tradeoff on tabular data,
revealing and quantifying privacy risks on synthetic data. We first conduct an
exhaustive empirical analysis, highlighting the utility-privacy tradeoff of
five state-of-the-art tabular synthesizers, against eight privacy attacks, with
a special focus on membership inference attacks. Motivated by the observation
of high data quality but also high privacy risk in tabular diffusion, we
propose DP-TLDM, Differentially Private Tabular Latent Diffusion Model, which
is composed of an autoencoder network to encode the tabular data and a latent
diffusion model to synthesize the latent tables. Following the emerging f-DP
framework, we apply DP-SGD to train the auto-encoder in combination with batch
clipping and use the separation value as the privacy metric to better capture
the privacy gain from DP algorithms. Our empirical evaluation demonstrates that
DP-TLDM is capable of achieving a meaningful theoretical privacy guarantee
while also significantly enhancing the utility of synthetic data. Specifically,
compared to other DP-protected tabular generative models, DP-TLDM improves the
synthetic quality by an average of 35% in data resemblance, 15% in the utility
for downstream tasks, and 50% in data discriminability, all while preserving a
comparable level of privacy risk.
\\ ( https://arxiv.org/abs/2403.07842 ,  15834kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07843
Date: Tue, 12 Mar 2024 17:32:52 GMT   (554kb,D)

Title: A Machine learning and Empirical Bayesian Approach for Predictive Buying
  in B2B E-commerce
Authors: Tuhin Subhra De and Pranjal Singh and Alok Patel
Categories: cs.LG
Comments: Published at the 8th International Conference on Machine Learning and
  Soft Computing (ICMLSC 2024), Singapore
DOI: 10.1145/3647750.3647754
\\
  In the context of developing nations like India, traditional business to
business (B2B) commerce heavily relies on the establishment of robust
relationships, trust, and credit arrangements between buyers and sellers.
Consequently, ecommerce enterprises frequently. Established in 2016 with a
vision to revolutionize trade in India through technology, Udaan is the
countrys largest business to business ecommerce platform. Udaan operates across
diverse product categories, including lifestyle, electronics, home and employ
telecallers to cultivate buyer relationships, streamline order placement
procedures, and promote special promotions. The accurate anticipation of buyer
order placement behavior emerges as a pivotal factor for attaining sustainable
growth, heightening competitiveness, and optimizing the efficiency of these
telecallers. To address this challenge, we have employed an ensemble approach
comprising XGBoost and a modified version of Poisson Gamma model to predict
customer order patterns with precision. This paper provides an in-depth
exploration of the strategic fusion of machine learning and an empirical
Bayesian approach, bolstered by the judicious selection of pertinent features.
This innovative approach has yielded a remarkable 3 times increase in customer
order rates, show casing its potential for transformative impact in the
ecommerce industry.
\\ ( https://arxiv.org/abs/2403.07843 ,  554kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07849
Date: Tue, 12 Mar 2024 17:41:27 GMT   (2178kb,D)

Title: Iterative Graph Neural Network Enhancement via Frequent Subgraph Mining
  of Explanations
Authors: Harish G. Naik and Jan Polster and Raj Shekhar and Tam\'as Horv\'ath
  and Gy\"orgy Tur\'an
Categories: cs.LG
\\
  We formulate an XAI-based model improvement approach for Graph Neural
Networks (GNNs) for node classification, called Explanation Enhanced Graph
Learning (EEGL). The goal is to improve predictive performance of GNN using
explanations. EEGL is an iterative self-improving algorithm, which starts with
a learned "vanilla" GNN, and repeatedly uses frequent subgraph mining to find
relevant patterns in explanation subgraphs. These patterns are then filtered
further to obtain application-dependent features corresponding to the presence
of certain subgraphs in the node neighborhoods. Giving an application-dependent
algorithm for such a subgraph-based extension of the Weisfeiler-Leman (1-WL)
algorithm has previously been posed as an open problem. We present experimental
evidence, with synthetic and real-world data, which show that EEGL outperforms
related approaches in predictive performance and that it has a
node-distinguishing power beyond that of vanilla GNNs. We also analyze EEGL's
training dynamics.
\\ ( https://arxiv.org/abs/2403.07849 ,  2178kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07851
Date: Tue, 12 Mar 2024 17:43:20 GMT   (245kb,D)

Title: 12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning
Authors: Yoga Esa Wibowo, Cristian Cioflan, Thorir Mar Ingolfsson, Michael
  Hersche, Leo Zhao, Abbas Rahimi, Luca Benini
Categories: cs.LG cs.CV
Comments: 6 pages, 4 tables, 3 figures. Accepted at IEEE DATE 2024
\\
  Few-Shot Class-Incremental Learning (FSCIL) enables machine learning systems
to expand their inference capabilities to new classes using only a few labeled
examples, without forgetting the previously learned classes. Classical
backpropagation-based learning and its variants are often unsuitable for
battery-powered, memory-constrained systems at the extreme edge. In this work,
we introduce Online Few-Shot Class-Incremental Learning (O-FSCIL), based on a
lightweight model consisting of a pretrained and metalearned feature extractor
and an expandable explicit memory storing the class prototypes. The
architecture is pretrained with a novel feature orthogonality regularization
and metalearned with a multi-margin loss. For learning a new class, our
approach extends the explicit memory with novel class prototypes, while the
remaining architecture is kept frozen. This allows learning previously unseen
classes based on only a few examples with one single pass (hence online).
O-FSCIL obtains an average accuracy of 68.62% on the FSCIL CIFAR100 benchmark,
achieving state-of-the-art results. Tailored for ultra-low-power platforms, we
implement O-FSCIL on the 60 mW GAP9 microcontroller, demonstrating online
learning capabilities within just 12 mJ per new class.
\\ ( https://arxiv.org/abs/2403.07851 ,  245kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07856
Date: Tue, 12 Mar 2024 17:46:38 GMT   (453kb,D)

Title: Quantum Support Vector Machine for Prostate Cancer Detection: A
  Performance Analysis
Authors: Walid El Maouaki, Taoufik Said, Mohamed Bennai
Categories: cs.LG quant-ph
Comments: 14 pages, 7 figures, 2 tables
\\
  This study addresses the urgent need for improved prostate cancer detection
methods by harnessing the power of advanced technological solutions. We
introduce the application of Quantum Support Vector Machine (QSVM) to this
critical healthcare challenge, showcasing an enhancement in diagnostic
performance over the classical Support Vector Machine (SVM) approach. Our study
not only outlines the remarkable improvements in diagnostic performance made by
QSVM over the classic SVM technique, but it delves into the advancements
brought about by the quantum feature map architecture, which has been carefully
identified and evaluated, ensuring it aligns seamlessly with the unique
characteristics of our prostate cancer dataset. This architecture succeded in
creating a distinct feature space, enabling the detection of complex,
non-linear patterns in the data. The findings reveal not only a comparable
accuracy with classical SVM ($92\%$) but also a $7.14\%$ increase in
sensitivity and a notably high F1-Score ($93.33\%$). This study's important
combination of quantum computing in medical diagnostics marks a pivotal step
forward in cancer detection, offering promising implications for the future of
healthcare technology.
\\ ( https://arxiv.org/abs/2403.07856 ,  453kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07857
Date: Tue, 12 Mar 2024 17:48:08 GMT   (7136kb,D)

Title: Fairness Feedback Loops: Training on Synthetic Data Amplifies Bias
Authors: Sierra Wyllie, Ilia Shumailov, Nicolas Papernot
Categories: cs.LG
\\
  Model-induced distribution shifts (MIDS) occur as previous model outputs
pollute new model training sets over generations of models. This is known as
model collapse in the case of generative models, and performative prediction or
unfairness feedback loops for supervised models. When a model induces a
distribution shift, it also encodes its mistakes, biases, and unfairnesses into
the ground truth of its data ecosystem. We introduce a framework that allows us
to track multiple MIDS over many generations, finding that they can lead to
loss in performance, fairness, and minoritized group representation, even in
initially unbiased datasets. Despite these negative consequences, we identify
how models might be used for positive, intentional, interventions in their data
ecosystems, providing redress for historical discrimination through a framework
called algorithmic reparation (AR). We simulate AR interventions by curating
representative training batches for stochastic gradient descent to demonstrate
how AR can improve upon the unfairnesses of models and data ecosystems subject
to other MIDS. Our work takes an important step towards identifying,
mitigating, and taking accountability for the unfair feedback loops enabled by
the idea that ML systems are inherently neutral and objective.
\\ ( https://arxiv.org/abs/2403.07857 ,  7136kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2403.06993 (*cross-listing*)
Date: Wed, 28 Feb 2024 12:34:04 GMT   (1831kb)

Title: Automatic driving lane change safety prediction model based on LSTM
Authors: Wenjian Sun, Linying Pan, Jingyu Xu, Weixiang Wan, Yong Wang
Categories: cs.RO cs.AI cs.LG cs.SY eess.IV eess.SY
\\
  Autonomous driving technology can improve traffic safety and reduce traffic
accidents. In addition, it improves traffic flow, reduces congestion, saves
energy and increases travel efficiency. In the relatively mature automatic
driving technology, the automatic driving function is divided into several
modules: perception, decision-making, planning and control, and a reasonable
division of labor can improve the stability of the system. Therefore,
autonomous vehicles need to have the ability to predict the trajectory of
surrounding vehicles in order to make reasonable decision planning and safety
measures to improve driving safety. By using deep learning method, a
safety-sensitive deep learning model based on short term memory (LSTM) network
is proposed. This model can alleviate the shortcomings of current automatic
driving trajectory planning, and the output trajectory not only ensures high
accuracy but also improves safety. The cell state simulation algorithm
simulates the trackability of the trajectory generated by this model. The
research results show that compared with the traditional model-based method,
the trajectory prediction method based on LSTM network has obvious advantages
in predicting the trajectory in the long time domain. The intention recognition
module considering interactive information has higher prediction and accuracy,
and the algorithm results show that the trajectory is very smooth based on the
premise of safe prediction and efficient lane change. And autonomous vehicles
can efficiently and safely complete lane changes.
\\ ( https://arxiv.org/abs/2403.06993 ,  1831kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06994 (*cross-listing*)
Date: Thu, 29 Feb 2024 07:50:06 GMT   (7984kb,D)

Title: Physics Sensor Based Deep Learning Fall Detection System
Authors: Zeyuan Qu and Tiange Huang and Yuxin Ji and Yongjun Li
Categories: eess.SP cs.AI cs.LG
\\
  Fall detection based on embedded sensor is a practical and popular research
direction in recent years. In terms of a specific application: fall detection
methods based upon physics sensors such as [gyroscope and accelerator] have
been exploited using traditional hand crafted features and feed them in machine
learning models like Markov chain or just threshold based classification
methods. In this paper, we build a complete system named TSFallDetect including
data receiving device based on embedded sensor, mobile deep-learning model
deploying platform, and a simple server, which will be used to gather models
and data for future expansion. On the other hand, we exploit the sequential
deep-learning methods to address this falling motion prediction problem based
on data collected by inertial and film pressure sensors. We make a empirical
study based on existing datasets and our datasets collected from our system
separately, which shows that the deep-learning model has more potential
advantage than other traditional methods, and we proposed a new deep-learning
model based on the time series data to predict the fall, and it may be superior
to other sequential models in this particular field.
\\ ( https://arxiv.org/abs/2403.06994 ,  7984kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07017 (*cross-listing*)
Date: Sat, 9 Mar 2024 17:36:54 GMT   (694kb,D)

Title: Mathematics of multi-agent learning systems at the interface of game
  theory and artificial intelligence
Authors: Long Wang, Feng Fu, Xingru Chen
Categories: physics.soc-ph cs.AI cs.GT cs.MA
Comments: 8 pages, 1 figure
\\
  Evolutionary Game Theory (EGT) and Artificial Intelligence (AI) are two
fields that, at first glance, might seem distinct, but they have notable
connections and intersections. The former focuses on the evolution of behaviors
(or strategies) in a population, where individuals interact with others and
update their strategies based on imitation (or social learning). The more
successful a strategy is, the more prevalent it becomes over time. The latter,
meanwhile, is centered on machine learning algorithms and (deep) neural
networks. It is often from a single-agent perspective but increasingly involves
multi-agent environments, in which intelligent agents adjust their strategies
based on feedback and experience, somewhat akin to the evolutionary process yet
distinct in their self-learning capacities. In light of the key components
necessary to address real-world problems, including (i) learning and
adaptation, (ii) cooperation and competition, (iii) robustness and stability,
and altogether (iv) population dynamics of individual agents whose strategies
evolve, the cross-fertilization of ideas between both fields will contribute to
the advancement of mathematics of multi-agent learning systems, in particular,
to the nascent domain of ``collective cooperative intelligence'' bridging
evolutionary dynamics and multi-agent reinforcement learning.
\\ ( https://arxiv.org/abs/2403.07017 ,  694kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07032 (*cross-listing*)
Date: Mon, 11 Mar 2024 04:56:10 GMT   (2608kb,D)

Title: STARFlow: Spatial Temporal Feature Re-embedding with Attentive Learning
  for Real-world Scene Flow
Authors: Zhiyang Lu and Qinghan Chen and Ming Cheng
Categories: cs.CV cs.AI
Comments: 10 pages, 8 figures, CVPR template
\\
  Scene flow prediction is a crucial underlying task in understanding dynamic
scenes as it offers fundamental motion information. However, contemporary scene
flow methods encounter three major challenges. Firstly, flow estimation solely
based on local receptive fields lacks long-dependency matching of point pairs.
To address this issue, we propose global attentive flow embedding to match
all-to-all point pairs in both feature space and Euclidean space, providing
global initialization before local refinement. Secondly, there are deformations
existing in non-rigid objects after warping, which leads to variations in the
spatiotemporal relation between the consecutive frames. For a more precise
estimation of residual flow, a spatial temporal feature re-embedding module is
devised to acquire the sequence features after deformation. Furthermore,
previous methods perform poor generalization due to the significant domain gap
between the synthesized and LiDAR-scanned datasets. We leverage novel domain
adaptive losses to effectively bridge the gap of motion inference from
synthetic to real-world. Experiments demonstrate that our approach achieves
state-of-the-art performance across various datasets, with particularly
outstanding results on real-world LiDAR-scanned datasets. Our code is available
at https://github.com/O-VIGIA/StarFlow.
\\ ( https://arxiv.org/abs/2403.07032 ,  2608kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07039 (*cross-listing*)
Date: Mon, 11 Mar 2024 09:57:16 GMT   (669kb)

Title: From English to ASIC: Hardware Implementation with Large Language Model
Authors: Emil Goh, Maoyang Xiang, I-Chyn Wey, T. Hui Teo
Categories: cs.AR cs.AI cs.PL
Comments: 15 pages, 1 figure
\\
  In the realm of ASIC engineering, the landscape has been significantly
reshaped by the rapid development of LLM, paralleled by an increase in the
complexity of modern digital circuits. This complexity has escalated the
requirements for HDL coding, necessitating a higher degree of precision and
sophistication. However, challenges have been faced due to the
less-than-optimal performance of modern language models in generating hardware
description code, a situation further exacerbated by the scarcity of the
corresponding high-quality code datasets. These challenges have highlighted the
gap between the potential of LLMs to revolutionize digital circuit design and
their current capabilities in accurately interpreting and implementing hardware
specifications. To address these challenges, a strategy focusing on the
fine-tuning of the leading-edge nature language model and the reshuffling of
the HDL code dataset has been developed. The fine-tuning aims to enhance
models' proficiency in generating precise and efficient ASIC design, while the
dataset reshuffling is intended to broaden the scope and improve the quality of
training material. The model demonstrated significant improvements compared to
the base model, with approximately 10% to 20% increase in accuracy across a
wide range of temperature for the pass@1 metric. This approach is expected to
facilitate a simplified and more efficient LLM-assisted framework for complex
circuit design, leveraging their capabilities to meet the sophisticated demands
of HDL coding and thus streamlining the ASIC development process.
\\ ( https://arxiv.org/abs/2403.07039 ,  669kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07090 (*cross-listing*)
Date: Mon, 11 Mar 2024 18:33:56 GMT   (565kb,D)

Title: Time Series Analysis of Key Societal Events as Reflected in Complex
  Social Media Data Streams
Authors: Andy Skumanich, Han Kyul Kim
Categories: cs.IR cs.AI
Comments: AAAI2024 Workshop on AI for Time Series Analysis (AI4TS)
\\
  Social media platforms hold valuable insights, yet extracting essential
information can be challenging. Traditional top-down approaches often struggle
to capture critical signals in rapidly changing events. As global events evolve
swiftly, social media narratives, including instances of disinformation, become
significant sources of insights. To address the need for an inductive strategy,
we explore a niche social media platform GAB and an established messaging
service Telegram, to develop methodologies applicable on a broader scale. This
study investigates narrative evolution on these platforms using quantitative
corpus-based discourse analysis techniques. Our approach is a novel mode to
study multiple social media domains to distil key information which may be
obscured otherwise, allowing for useful and actionable insights. The paper
details the technical and methodological aspects of gathering and preprocessing
GAB and Telegram data for a keyness (Log Ratio) metric analysis, identifying
crucial nouns and verbs for deeper exploration. Empirically, this approach is
applied to a case study of a well defined event that had global impact: the
2023 Wagner mutiny. The main findings are: (1) the time line can be
deconstructed to provide useful data features allowing for improved
interpretation; (2) a methodology is applied which provides a basis for
generalization. The key contribution is an approach, that in some cases,
provides the ability to capture the dynamic narrative shifts over time with
elevated confidence. The approach can augment near-real-time assessment of key
social movements, allowing for informed governance choices. This research is
important because it lays out a useful methodology for time series relevant
info-culling, which can enable proactive modes for positive social engagement.
\\ ( https://arxiv.org/abs/2403.07090 ,  565kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07194 (*cross-listing*)
Date: Sat, 10 Feb 2024 09:31:39 GMT   (536kb)

Title: Improving prediction of students' performance in intelligent tutoring
  systems using attribute selection and ensembles of different multimodal data
  sources
Authors: W. Chango, R. Cerezo, M. Sanchez-Santillan, R. Azevedo, and C. Romero
Categories: cs.CY cs.AI cs.HC cs.LG
Journal-ref: Journal of Computing in Higher Education,2021, 33, 614-634
DOI: 10.1007/s12528-021-09298-8
\\
  The aim of this study was to predict university students' learning
performance using different sources of data from an Intelligent Tutoring
System. We collected and preprocessed data from 40 students from different
multimodal sources: learning strategies from system logs, emotions from face
recording videos, interaction zones from eye tracking, and test performance
from final knowledge evaluation. Our objective was to test whether the
prediction could be improved by using attribute selection and classification
ensembles. We carried out three experiments by applying six classification
algorithms to numerical and discretized preprocessed multimodal data. The
results show that the best predictions were produced using ensembles and
selecting the best attributes approach with numerical data.
\\ ( https://arxiv.org/abs/2403.07194 ,  536kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07255 (*cross-listing*)
Date: Tue, 12 Mar 2024 02:24:37 GMT   (2327kb,D)

Title: Deep Learning-Assisted Parallel Interference Cancellation for Grant-Free
  NOMA in Machine-Type Communication
Authors: Yongjeong Oh, Jaehong Jo, Byonghyo Shim, and Yo-Seb Jeon
Categories: eess.SP cs.AI cs.LG
\\
  In this paper, we present a novel approach for joint activity detection (AD),
channel estimation (CE), and data detection (DD) in uplink grant-free
non-orthogonal multiple access (NOMA) systems. Our approach employs an
iterative and parallel interference removal strategy inspired by parallel
interference cancellation (PIC), enhanced with deep learning to jointly tackle
the AD, CE, and DD problems. Based on this approach, we develop three PIC
frameworks, each of which is designed for either coherent or non-coherence
schemes. The first framework performs joint AD and CE using received pilot
signals in the coherent scheme. Building upon this framework, the second
framework utilizes both the received pilot and data signals for CE, further
enhancing the performances of AD, CE, and DD in the coherent scheme. The third
framework is designed to accommodate the non-coherent scheme involving a small
number of data bits, which simultaneously performs AD and DD. Through joint
loss functions and interference cancellation modules, our approach supports
end-to-end training, contributing to enhanced performances of AD, CE, and DD
for both coherent and non-coherent schemes. Simulation results demonstrate the
superiority of our approach over traditional techniques, exhibiting enhanced
performances of AD, CE, and DD while maintaining lower computational
complexity.
\\ ( https://arxiv.org/abs/2403.07255 ,  2327kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07271 (*cross-listing*)
Date: Tue, 12 Mar 2024 03:00:15 GMT   (3020kb,D)

Title: Anderson acceleration for iteratively reweighted $\ell_1$ algorithm
Authors: Kexin Li
Categories: math.OC cs.AI cs.LG eess.SP
\\
  Iteratively reweighted L1 (IRL1) algorithm is a common algorithm for solving
sparse optimization problems with nonconvex and nonsmooth regularization. The
development of its acceleration algorithm, often employing Nesterov
acceleration, has sparked significant interest. Nevertheless, the convergence
and complexity analysis of these acceleration algorithms consistently poses
substantial challenges. Recently, Anderson acceleration has gained prominence
owing to its exceptional performance for speeding up fixed-point iteration,
with numerous recent studies applying it to gradient-based algorithms.
Motivated by the powerful impact of Anderson acceleration, we propose an
Anderson-accelerated IRL1 algorithm and establish its local linear convergence
rate. We extend this convergence result, typically observed in smooth settings,
to a nonsmooth scenario. Importantly, our theoretical results do not depend on
the Kurdyka-Lojasiewicz condition, a necessary condition in existing Nesterov
acceleration-based algorithms. Furthermore, to ensure global convergence, we
introduce a globally convergent Anderson accelerated IRL1 algorithm by
incorporating a classical nonmonotone line search condition. Experimental
results indicate that our algorithm outperforms existing Nesterov
acceleration-based algorithms.
\\ ( https://arxiv.org/abs/2403.07271 ,  3020kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07277 (*cross-listing*)
Date: Tue, 12 Mar 2024 03:15:08 GMT   (39023kb,D)

Title: A Bayesian Approach to OOD Robustness in Image Classification
Authors: Prakhar Kaushik and Adam Kortylewski and Alan Yuille
Categories: cs.CV cs.AI
Comments: CVPR 2024
\\
  An important and unsolved problem in computer vision is to ensure that the
algorithms are robust to changes in image domains. We address this problem in
the scenario where we have access to images from the target domains but no
annotations. Motivated by the challenges of the OOD-CV benchmark where we
encounter real world Out-of-Domain (OOD) nuisances and occlusion, we introduce
a novel Bayesian approach to OOD robustness for object classification. Our work
extends Compositional Neural Networks (CompNets), which have been shown to be
robust to occlusion but degrade badly when tested on OOD data. We exploit the
fact that CompNets contain a generative head defined over feature vectors
represented by von Mises-Fisher (vMF) kernels, which correspond roughly to
object parts, and can be learned without supervision. We obverse that some vMF
kernels are similar between different domains, while others are not. This
enables us to learn a transitional dictionary of vMF kernels that are
intermediate between the source and target domains and train the generative
model on this dictionary using the annotations on the source domain, followed
by iterative refinement. This approach, termed Unsupervised Generative
Transition (UGT), performs very well in OOD scenarios even when occlusion is
present. UGT is evaluated on different OOD benchmarks including the OOD-CV
dataset, several popular datasets (e.g., ImageNet-C [9]), artificial image
corruptions (including adding occluders), and synthetic-to-real domain
transfer, and does well in all scenarios outperforming SOTA alternatives (e.g.
up to 10% top-1 accuracy on Occluded OOD-CV dataset).
\\ ( https://arxiv.org/abs/2403.07277 ,  39023kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07292 (*cross-listing*)
Date: Tue, 12 Mar 2024 03:50:57 GMT   (27114kb,D)

Title: Continual All-in-One Adverse Weather Removal with Knowledge Replay on a
  Unified Network Structure
Authors: De Cheng, Yanling Ji, Dong Gong, Yan Li, Nannan Wang, Junwei Han,
  Dingwen Zhang
Categories: cs.CV cs.AI
\\
  In real-world applications, image degeneration caused by adverse weather is
always complex and changes with different weather conditions from days and
seasons. Systems in real-world environments constantly encounter adverse
weather conditions that are not previously observed. Therefore, it practically
requires adverse weather removal models to continually learn from incrementally
collected data reflecting various degeneration types. Existing adverse weather
removal approaches, for either single or multiple adverse weathers, are mainly
designed for a static learning paradigm, which assumes that the data of all
types of degenerations to handle can be finely collected at one time before a
single-phase learning process. They thus cannot directly handle the incremental
learning requirements. To address this issue, we made the earliest effort to
investigate the continual all-in-one adverse weather removal task, in a setting
closer to real-world applications. Specifically, we develop a novel continual
learning framework with effective knowledge replay (KR) on a unified network
structure. Equipped with a principal component projection and an effective
knowledge distillation mechanism, the proposed KR techniques are tailored for
the all-in-one weather removal task. It considers the characteristics of the
image restoration task with multiple degenerations in continual learning, and
the knowledge for different degenerations can be shared and accumulated in the
unified network structure. Extensive experimental results demonstrate the
effectiveness of the proposed method to deal with this challenging task, which
performs competitively to existing dedicated or joint training image
restoration methods. Our code is available at
https://github.com/xiaojihh/CL_all-in-one.
\\ ( https://arxiv.org/abs/2403.07292 ,  27114kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07322 (*cross-listing*)
Date: Tue, 12 Mar 2024 05:15:42 GMT   (2001kb,D)

Title: A Question-centric Multi-experts Contrastive Learning Framework for
  Improving the Accuracy and Interpretability of Deep Sequential Knowledge
  Tracing Models
Authors: Hengyuan Zhang, Zitao Liu, Chenming Shang, Dawei Li, Yong Jiang
Categories: cs.CY cs.AI cs.LG
Comments: 24 pages, 8 figures
\\
  Knowledge tracing (KT) plays a crucial role in predicting students' future
performance by analyzing their historical learning processes. Deep neural
networks (DNNs) have shown great potential in solving the KT problem. However,
there still exist some important challenges when applying deep learning
techniques to model the KT process. The first challenge lies in taking the
individual information of the question into modeling. This is crucial because,
despite questions sharing the same knowledge component (KC), students'
knowledge acquisition on homogeneous questions can vary significantly. The
second challenge lies in interpreting the prediction results from existing deep
learning-based KT models. In real-world applications, while it may not be
necessary to have complete transparency and interpretability of the model
parameters, it is crucial to present the model's prediction results in a manner
that teachers find interpretable. This makes teachers accept the rationale
behind the prediction results and utilize them to design teaching activities
and tailored learning strategies for students. However, the inherent black-box
nature of deep learning techniques often poses a hurdle for teachers to fully
embrace the model's prediction results. To address these challenges, we propose
a Question-centric Multi-experts Contrastive Learning framework for KT called
Q-MCKT.
\\ ( https://arxiv.org/abs/2403.07322 ,  2001kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07332 (*cross-listing*)
Date: Tue, 12 Mar 2024 05:34:51 GMT   (2121kb,D)

Title: Large Window-based Mamba UNet for Medical Image Segmentation: Beyond
  Convolution and Self-attention
Authors: Jinhong Wang, Jintai Chen, Danny Chen and Jian Wu
Categories: cs.CV cs.AI
\\
  In clinical practice, medical image segmentation provides useful information
on the contours and dimensions of target organs or tissues, facilitating
improved diagnosis, analysis, and treatment. In the past few years,
convolutional neural networks (CNNs) and Transformers have dominated this area,
but they still suffer from either limited receptive fields or costly long-range
modeling. Mamba, a State Space Sequence Model (SSM), recently emerged as a
promising paradigm for long-range dependency modeling with linear complexity.
In this paper, we introduce a Large Window-based Mamba U}-shape Network, or
LMa-UNet, for 2D and 3D medical image segmentation. A distinguishing feature of
our LMa-UNet is its utilization of large windows, excelling in locally spatial
modeling compared to small kernel-based CNNs and small window-based
Transformers, while maintaining superior efficiency in global modeling compared
to self-attention with quadratic complexity. Additionally, we design a novel
hierarchical and bidirectional Mamba block to further enhance the global and
neighborhood spatial modeling capability of Mamba. Comprehensive experiments
demonstrate the effectiveness and efficiency of our method and the feasibility
of using large window size to achieve large receptive fields. Codes are
available at https://github.com/wjh892521292/LMa-UNet.
\\ ( https://arxiv.org/abs/2403.07332 ,  2121kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07355 (*cross-listing*)
Date: Tue, 12 Mar 2024 06:28:41 GMT   (811kb)

Title: Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO
  Systems
Authors: Junyong Shin, Yujin Kang, Yo-Seb Jeon
Categories: eess.SP cs.AI cs.CV
\\
  This paper presents a finite-rate deep-learning (DL)-based channel state
information (CSI) feedback method for massive multiple-input multiple-output
(MIMO) systems. The presented method provides a finite-bit representation of
the latent vector based on a vector-quantized variational autoencoder (VQ-VAE)
framework while reducing its computational complexity based on shape-gain
vector quantization. In this method, the magnitude of the latent vector is
quantized using a non-uniform scalar codebook with a proper transformation
function, while the direction of the latent vector is quantized using a
trainable Grassmannian codebook. A multi-rate codebook design strategy is also
developed by introducing a codeword selection rule for a nested codebook along
with the design of a loss function. Simulation results demonstrate that the
proposed method reduces the computational complexity associated with VQ-VAE
while improving CSI reconstruction performance under a given feedback overhead.
\\ ( https://arxiv.org/abs/2403.07355 ,  811kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07376 (*cross-listing*)
Date: Tue, 12 Mar 2024 07:27:02 GMT   (15541kb,D)

Title: NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning
  Disentangled Reasoning
Authors: Bingqian Lin, Yunshuang Nie, Ziming Wei, Jiaqi Chen, Shikui Ma,
  Jianhua Han, Hang Xu, Xiaojun Chang, Xiaodan Liang
Categories: cs.CV cs.AI cs.CL cs.RO
\\
  Vision-and-Language Navigation (VLN), as a crucial research problem of
Embodied AI, requires an embodied agent to navigate through complex 3D
environments following natural language instructions. Recent research has
highlighted the promising capacity of large language models (LLMs) in VLN by
improving navigational reasoning accuracy and interpretability. However, their
predominant use in an offline manner usually suffers from substantial domain
gap between the VLN task and the LLM training corpus. This paper introduces a
novel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill
parameter-efficient in-domain training to enable self-guided navigational
decision, leading to a significant mitigation of the domain gap in a
cost-effective manner. Specifically, at each timestep, the LLM is prompted to
forecast the navigational chain-of-thought by: 1) acting as a world model to
imagine the next observation according to the instruction, 2) selecting the
candidate observation that best aligns with the imagination, and 3) determining
the action based on the reasoning from the prior steps. Through constructing
formalized labels for training, the LLM can learn to generate desired and
reasonable chain-of-thought outputs for improving the action decision.
Experimental results across various training settings and popular VLN
benchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room
(R4R)) show the significant superiority of NavCoT over the direct action
prediction variants. Through simple parameter-efficient finetuning, our NavCoT
outperforms a recent GPT4-based approach with ~7% relative improvement on the
R2R dataset. We believe that NavCoT will help unlock more task-adaptive and
scalable LLM-based embodied agents, which are helpful for developing real-world
robotics applications. Code is available at
https://github.com/expectorlin/NavCoT.
\\ ( https://arxiv.org/abs/2403.07376 ,  15541kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07380 (*cross-listing*)
Date: Tue, 12 Mar 2024 07:41:51 GMT   (466kb,D)

Title: Gabor-guided transformer for single image deraining
Authors: Sijin He, Guangfeng Lin
Categories: cs.CV cs.AI
\\
  Image deraining have have gained a great deal of attention in order to
address the challenges posed by the effects of harsh weather conditions on
visual tasks. While convolutional neural networks (CNNs) are popular, their
limitations in capturing global information may result in ineffective rain
removal. Transformer-based methods with self-attention mechanisms have
improved, but they tend to distort high-frequency details that are crucial for
image fidelity. To solve this problem, we propose the Gabor-guided tranformer
(Gabformer) for single image deraining. The focus on local texture features is
enhanced by incorporating the information processed by the Gabor filter into
the query vector, which also improves the robustness of the model to noise due
to the properties of the filter. Extensive experiments on the benchmarks
demonstrate that our method outperforms state-of-the-art approaches.
\\ ( https://arxiv.org/abs/2403.07380 ,  466kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07389 (*cross-listing*)
Date: Tue, 12 Mar 2024 07:57:33 GMT   (6634kb,D)

Title: Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from
  Duplex to Monoplex IHC Images
Authors: Nicolas Brieu, Nicolas Triltsch, Philipp Wortmann, Dominik Winter,
  Shashank Saran, Marlon Rebelatto, G\"unter Schmidt
Categories: cs.CV cs.AI eess.IV
Comments: 4 pages, 2 figures
MSC-class: I.2.10, J.3, I.4.6
\\
  Generative models enable the translation from a source image domain where
readily trained models are available to a target domain unseen during training.
While Cycle Generative Adversarial Networks (GANs) are well established, the
associated cycle consistency constrain relies on that an invertible mapping
exists between the two domains. This is, however, not the case for the
translation between images stained with chromogenic monoplex and duplex
immunohistochemistry (IHC) assays. Focusing on the translation from the latter
to the first, we propose - through the introduction of a novel training design,
an alternative constrain leveraging a set of immunofluorescence (IF) images as
an auxiliary unpaired image domain. Quantitative and qualitative results on a
downstream segmentation task show the benefit of the proposed method in
comparison to baseline approaches.
\\ ( https://arxiv.org/abs/2403.07389 ,  6634kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07403 (*cross-listing*)
Date: Tue, 12 Mar 2024 08:32:23 GMT   (18285kb,D)

Title: From Canteen Food to Daily Meals: Generalizing Food Recognition to More
  Practical Scenarios
Authors: Guoshan Liu, Yang Jiao, Jingjing Chen, Bin Zhu, Yu-Gang Jiang
Categories: cs.CV cs.AI
\\
  The precise recognition of food categories plays a pivotal role for
intelligent health management, attracting significant research attention in
recent years. Prominent benchmarks, such as Food-101 and VIREO Food-172,
provide abundant food image resources that catalyze the prosperity of research
in this field. Nevertheless, these datasets are well-curated from canteen
scenarios and thus deviate from food appearances in daily life. This
discrepancy poses great challenges in effectively transferring classifiers
trained on these canteen datasets to broader daily-life scenarios encountered
by humans. Toward this end, we present two new benchmarks, namely DailyFood-172
and DailyFood-16, specifically designed to curate food images from everyday
meals. These two datasets are used to evaluate the transferability of
approaches from the well-curated food image domain to the everyday-life food
image domain. In addition, we also propose a simple yet effective baseline
method named Multi-Cluster Reference Learning (MCRL) to tackle the
aforementioned domain gap. MCRL is motivated by the observation that food
images in daily-life scenarios exhibit greater intra-class appearance variance
compared with those in well-curated benchmarks. Notably, MCRL can be seamlessly
coupled with existing approaches, yielding non-trivial performance
enhancements. We hope our new benchmarks can inspire the community to explore
the transferability of food recognition models trained on well-curated datasets
toward practical real-life applications.
\\ ( https://arxiv.org/abs/2403.07403 ,  18285kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07500 (*cross-listing*)
Date: Tue, 12 Mar 2024 10:38:03 GMT   (8102kb,D)

Title: Block-wise LoRA: Revisiting Fine-grained LoRA for Effective
  Personalization and Stylization in Text-to-Image Generation
Authors: Likun Li, Haoqi Zeng, Changpeng Yang, Haozhe Jia, Di Xu
Categories: cs.CV cs.AI
\\
  The objective of personalization and stylization in text-to-image is to
instruct a pre-trained diffusion model to analyze new concepts introduced by
users and incorporate them into expected styles. Recently, parameter-efficient
fine-tuning (PEFT) approaches have been widely adopted to address this task and
have greatly propelled the development of this field. Despite their popularity,
existing efficient fine-tuning methods still struggle to achieve effective
personalization and stylization in T2I generation. To address this issue, we
propose block-wise Low-Rank Adaptation (LoRA) to perform fine-grained
fine-tuning for different blocks of SD, which can generate images faithful to
input prompts and target identity and also with desired style. Extensive
experiments demonstrate the effectiveness of the proposed method.
\\ ( https://arxiv.org/abs/2403.07500 ,  8102kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07540 (*cross-listing*)
Date: Tue, 12 Mar 2024 11:26:58 GMT   (6083kb,D)

Title: WannaLaugh: A Configurable Ransomware Emulator -- Learning to Mimic
  Malicious Storage Traces
Authors: Dionysios Diamantopolous and Roman Pletka and Slavisa Sarafijanovic
  and A.L. Narasimha Reddy and Haris Pozidis
Categories: cs.CR cs.AI
\\
  Ransomware, a fearsome and rapidly evolving cybersecurity threat, continues
to inflict severe consequences on individuals and organizations worldwide.
Traditional detection methods, reliant on static signatures and application
behavioral patterns, are challenged by the dynamic nature of these threats.
This paper introduces three primary contributions to address this challenge.
First, we introduce a ransomware emulator. This tool is designed to safely
mimic ransomware attacks without causing actual harm or spreading malware,
making it a unique solution for studying ransomware behavior. Second, we
demonstrate how we use this emulator to create storage I/O traces. These traces
are then utilized to train machine-learning models. Our results show that these
models are effective in detecting ransomware, highlighting the practical
application of our emulator in developing responsible cybersecurity tools.
Third, we show how our emulator can be used to mimic the I/O behavior of
existing ransomware thereby enabling safe trace collection. Both the emulator
and its application represent significant steps forward in ransomware detection
in the era of machine-learning-driven cybersecurity.
\\ ( https://arxiv.org/abs/2403.07540 ,  6083kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07553 (*cross-listing*)
Date: Tue, 12 Mar 2024 11:39:18 GMT   (3692kb,D)

Title: The future of document indexing: GPT and Donut revolutionize table of
  content processing
Authors: Degaga Wolde Feyisa, Haylemicheal Berihun, Amanuel Zewdu, Mahsa
  Najimoghadam, Marzieh Zare
Categories: cs.IR cs.AI cs.CV
Comments: Document AI, Document Classification, Information extraction, Large
  Language Models, OCR Models, Visual Document Understanding
\\
  Industrial projects rely heavily on lengthy, complex specification documents,
making tedious manual extraction of structured information a major bottleneck.
This paper introduces an innovative approach to automate this process,
leveraging the capabilities of two cutting-edge AI models: Donut, a model that
extracts information directly from scanned documents without OCR, and OpenAI
GPT-3.5 Turbo, a robust large language model. The proposed methodology is
initiated by acquiring the table of contents (ToCs) from construction
specification documents and subsequently structuring the ToCs text into JSON
data. Remarkable accuracy is achieved, with Donut reaching 85% and GPT-3.5
Turbo reaching 89% in effectively organizing the ToCs. This landmark
achievement represents a significant leap forward in document indexing,
demonstrating the immense potential of AI to automate information extraction
tasks across diverse document types, boosting efficiency and liberating
critical resources in various industries.
\\ ( https://arxiv.org/abs/2403.07553 ,  3692kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07559 (*cross-listing*)
Date: Tue, 12 Mar 2024 11:47:12 GMT   (771kb,D)

Title: Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding
Authors: Huijie Tang, Federico Berto, Jinkyoo Park
Categories: cs.MA cs.AI cs.LG cs.RO
\\
  Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding
(MAPF) has recently gained attention due to its efficiency and scalability.
Several MARL-MAPF methods choose to use communication to enrich the information
one agent can perceive. However, existing works still struggle in structured
environments with high obstacle density and a high number of agents. To further
improve the performance of the communication-based MARL-MAPF solvers, we
propose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first
propose a selective communication block to gather richer information for better
agent coordination within multi-agent environments and train the model with a
Q-learning-based algorithm. We further introduce three advanced inference
strategies aimed at bolstering performance during the execution phase. First,
we hybridize the neural policy with single-agent expert guidance for navigating
conflict-free zones. Secondly, we propose Q value-based methods for prioritized
resolution of conflicts as well as deadlock situations. Finally, we introduce a
robust ensemble method that can efficiently collect the best out of multiple
possible solutions. We empirically evaluate EPH in complex multi-agent
environments and demonstrate competitive performance against state-of-the-art
neural methods for MAPF.
\\ ( https://arxiv.org/abs/2403.07559 ,  771kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07573 (*cross-listing*)
Date: Tue, 12 Mar 2024 12:03:16 GMT   (6941kb,D)

Title: Towards a Dynamic Future with Adaptable Computing and Network
  Convergence (ACNC)
Authors: Masoud Shokrnezhad, Hao Yu, Tarik Taleb, Richard Li, Kyunghan Lee,
  Jaeseung Song, and Cedric Westphal
Categories: cs.NI cs.AI cs.DC cs.ET cs.LG
\\
  In the context of advancing 6G, a substantial paradigm shift is anticipated,
highlighting comprehensive everything-to-everything interactions characterized
by numerous connections and stringent adherence to Quality of
Service/Experience (QoS/E) prerequisites. The imminent challenge stems from
resource scarcity, prompting a deliberate transition to Computing-Network
Convergence (CNC) as an auspicious approach for joint resource orchestration.
While CNC-based mechanisms have garnered attention, their effectiveness in
realizing future services, particularly in use cases like the Metaverse, may
encounter limitations due to the continually changing nature of users,
services, and resources. Hence, this paper presents the concept of Adaptable
CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for
the joint orchestration of computing and network resources, catering to dynamic
and voluminous user requests with stringent requirements. ACNC encompasses two
primary functionalities: state recognition and context detection. Given the
intricate nature of the user-service-computing-network space, the paper employs
dimension reduction to generate live, holistic, abstract system states in a
hierarchical structure. To address the challenges posed by dynamic changes,
Continual Learning (CL) is employed, classifying the system state into contexts
controlled by dedicated ML agents, enabling them to operate efficiently. These
two functionalities are intricately linked within a closed loop overseen by the
End-to-End (E2E) orchestrator to allocate resources. The paper introduces the
components of ACNC, proposes a Metaverse scenario to exemplify ACNC's role in
resource provisioning with Segment Routing v6 (SRv6), outlines ACNC's workflow,
details a numerical analysis for efficiency assessment, and concludes with
discussions on relevant challenges and potential avenues for future research.
\\ ( https://arxiv.org/abs/2403.07573 ,  6941kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07605 (*cross-listing*)
Date: Tue, 12 Mar 2024 12:44:34 GMT   (2312kb,D)

Title: Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in
  Text-To-Image Generation
Authors: Michael Ogezi and Ning Shi
Categories: cs.CV cs.AI cs.LG
\\
  In text-to-image generation, using negative prompts, which describe
undesirable image characteristics, can significantly boost image quality.
However, producing good negative prompts is manual and tedious. To address
this, we propose NegOpt, a novel method for optimizing negative prompt
generation toward enhanced image generation, using supervised fine-tuning and
reinforcement learning. Our combined approach results in a substantial increase
of 25% in Inception Score compared to other approaches and surpasses
ground-truth negative prompts from the test set. Furthermore, with NegOpt we
can preferentially optimize the metrics most important to us. Finally, we
construct Negative Prompts DB, a dataset of negative prompts.
\\ ( https://arxiv.org/abs/2403.07605 ,  2312kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07608 (*cross-listing*)
Date: Tue, 12 Mar 2024 12:47:32 GMT   (1012kb,D)

Title: Couler: Unified Machine Learning Workflow Optimization in Cloud
Authors: Xiaoda Wang, Yuan Tang, Tengda Guo, Bo Sang, Jingji Wu, Jian Sha, Ke
  Zhang, Jiang Qian, Mingjie Tang
Categories: cs.DB cs.AI cs.LG
\\
  Machine Learning (ML) has become ubiquitous, fueling data-driven applications
across various organizations. Contrary to the traditional perception of ML in
research, ML workflows can be complex, resource-intensive, and time-consuming.
Expanding an ML workflow to encompass a wider range of data infrastructure and
data types may lead to larger workloads and increased deployment costs.
Currently, numerous workflow engines are available (with over ten being widely
recognized). This variety poses a challenge for end-users in terms of mastering
different engine APIs. While efforts have primarily focused on optimizing ML
Operations (MLOps) for a specific workflow engine, current methods largely
overlook workflow optimization across different engines.
  In this work, we design and implement Couler, a system designed for unified
ML workflow optimization in the cloud. Our main insight lies in the ability to
generate an ML workflow using natural language (NL) descriptions. We integrate
Large Language Models (LLMs) into workflow generation, and provide a unified
programming interface for various workflow engines. This approach alleviates
the need to understand various workflow engines' APIs. Moreover, Couler
enhances workflow computation efficiency by introducing automated caching at
multiple stages, enabling large workflow auto-parallelization and automatic
hyperparameters tuning. These enhancements minimize redundant computational
costs and improve fault tolerance during deep learning workflow training.
Couler is extensively deployed in real-world production scenarios at Ant Group,
handling approximately 22k workflows daily, and has successfully improved the
CPU/Memory utilization by more than 15% and the workflow completion rate by
around 17%.
\\ ( https://arxiv.org/abs/2403.07608 ,  1012kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07622 (*cross-listing*)
Date: Tue, 12 Mar 2024 13:05:51 GMT   (19627kb,D)

Title: Multiple Latent Space Mapping for Compressed Dark Image Enhancement
Authors: Yi Zeng, Zhengning Wang, Yuxuan Liu, Tianjiao Zeng, Xuhang Liu,
  Xinglong Luo, Shuaicheng Liu, Shuyuan Zhu and Bing Zeng
Categories: cs.CV cs.AI eess.IV
\\
  Dark image enhancement aims at converting dark images to normal-light images.
Existing dark image enhancement methods take uncompressed dark images as inputs
and achieve great performance. However, in practice, dark images are often
compressed before storage or transmission over the Internet. Current methods
get poor performance when processing compressed dark images. Artifacts hidden
in the dark regions are amplified by current methods, which results in
uncomfortable visual effects for observers. Based on this observation, this
study aims at enhancing compressed dark images while avoiding compression
artifacts amplification. Since texture details intertwine with compression
artifacts in compressed dark images, detail enhancement and blocking artifacts
suppression contradict each other in image space. Therefore, we handle the task
in latent space. To this end, we propose a novel latent mapping network based
on variational auto-encoder (VAE). Firstly, different from previous VAE-based
methods with single-resolution features only, we exploit multiple latent spaces
with multi-resolution features, to reduce the detail blur and improve image
fidelity. Specifically, we train two multi-level VAEs to project compressed
dark images and normal-light images into their latent spaces respectively.
Secondly, we leverage a latent mapping network to transform features from
compressed dark space to normal-light space. Specifically, since the
degradation models of darkness and compression are different from each other,
the latent mapping process is divided mapping into enlightening branch and
deblocking branch. Comprehensive experiments demonstrate that the proposed
method achieves state-of-the-art performance in compressed dark image
enhancement.
\\ ( https://arxiv.org/abs/2403.07622 ,  19627kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07630 (*cross-listing*)
Date: Tue, 12 Mar 2024 13:11:58 GMT   (6780kb,D)

Title: Hunting Attributes: Context Prototype-Aware Learning for Weakly
  Supervised Semantic Segmentation
Authors: Feilong Tang, Zhongxing Xu, Zhaojun Qu, Wei Feng, Xingjian Jiang,
  Zongyuan Ge
Categories: cs.CV cs.AI
\\
  Recent weakly supervised semantic segmentation (WSSS) methods strive to
incorporate contextual knowledge to improve the completeness of class
activation maps (CAM). In this work, we argue that the knowledge bias between
instances and contexts affects the capability of the prototype to sufficiently
understand instance semantics. Inspired by prototype learning theory, we
propose leveraging prototype awareness to capture diverse and fine-grained
feature attributes of instances. The hypothesis is that contextual prototypes
might erroneously activate similar and frequently co-occurring object
categories due to this knowledge bias. Therefore, we propose to enhance the
prototype representation ability by mitigating the bias to better capture
spatial coverage in semantic object regions. With this goal, we present a
Context Prototype-Aware Learning (CPAL) strategy, which leverages semantic
context to enrich instance comprehension. The core of this method is to
accurately capture intra-class variations in object features through
context-aware prototypes, facilitating the adaptation to the semantic
attributes of various instances. We design feature distribution alignment to
optimize prototype awareness, aligning instance feature distributions with
dense features. In addition, a unified training framework is proposed to
combine label-guided classification supervision and prototypes-guided
self-supervision. Experimental results on PASCAL VOC 2012 and MS COCO 2014 show
that CPAL significantly improves off-the-shelf methods and achieves
state-of-the-art performance. The project is available at
https://github.com/Barrett-python/CPAL.
\\ ( https://arxiv.org/abs/2403.07630 ,  6780kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07687 (*cross-listing*)
Date: Tue, 12 Mar 2024 14:27:17 GMT   (11363kb,D)

Title: Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model
  Performance and Annotation Cost
Authors: Oana Ignat, Longju Bai, Joan Nwatu, Rada Mihalcea
Categories: cs.CV cs.AI cs.CL
Comments: accepted at COLING 2024
\\
  Current foundation models have shown impressive performance across various
tasks. However, several studies have revealed that these models are not
effective for everyone due to the imbalanced geographical and economic
representation of the data used in the training process. Most of this data
comes from Western countries, leading to poor results for underrepresented
countries. To address this issue, more data needs to be collected from these
countries, but the cost of annotation can be a significant bottleneck. In this
paper, we propose methods to identify the data to be annotated to balance model
performance and annotation costs. Our approach first involves finding the
countries with images of topics (objects and actions) most visually distinct
from those already in the training datasets used by current large
vision-language foundation models. Next, we identify countries with higher
visual similarity for these topics and show that using data from these
countries to supplement the training data improves model performance and
reduces annotation costs. The resulting lists of countries and corresponding
topics are made available at
https://github.com/MichiganNLP/visual_diversity_budget.
\\ ( https://arxiv.org/abs/2403.07687 ,  11363kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07711 (*cross-listing*)
Date: Tue, 12 Mar 2024 14:53:56 GMT   (679kb,D)

Title: SSM Meets Video Diffusion Models: Efficient Video Generation with
  Structured State Spaces
Authors: Yuta Oshima, Shohei Taniguchi, Masahiro Suzuki, Yutaka Matsuo
Categories: cs.CV cs.AI
Comments: Accepted as workshop paper at ICLR 2024
\\
  Given the remarkable achievements in image generation through diffusion
models, the research community has shown increasing interest in extending these
models to video generation. Recent diffusion models for video generation have
predominantly utilized attention layers to extract temporal features. However,
attention layers are limited by their memory consumption, which increases
quadratically with the length of the sequence. This limitation presents
significant challenges when attempting to generate longer video sequences using
diffusion models. To overcome this challenge, we propose leveraging state-space
models (SSMs). SSMs have recently gained attention as viable alternatives due
to their linear memory consumption relative to sequence length. In the
experiments, we first evaluate our SSM-based model with UCF101, a standard
benchmark of video generation. In addition, to investigate the potential of
SSMs for longer video generation, we perform an experiment using the MineRL
Navigate dataset, varying the number of frames to 64 and 150. In these
settings, our SSM-based model can considerably save memory consumption for
longer sequences, while maintaining competitive FVD scores to the
attention-based models. Our codes are available at
https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.
\\ ( https://arxiv.org/abs/2403.07711 ,  679kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07720 (*cross-listing*)
Date: Tue, 12 Mar 2024 14:58:52 GMT   (1771kb,D)

Title: Multi-modal Auto-regressive Modeling via Visual Words
Authors: Tianshuo Peng, Zuchao Li, Lefei Zhang, Hai Zhao, Ping Wang, and Bo Du
Categories: cs.CV cs.AI
\\
  Large Language Models (LLMs), benefiting from the auto-regressive modelling
approach performed on massive unannotated texts corpora, demonstrates powerful
perceptual and reasoning capabilities. However, as for extending
auto-regressive modelling to multi-modal scenarios to build Large Multi-modal
Models (LMMs), there lies a great difficulty that the image information is
processed in the LMM as continuous visual embeddings, which cannot obtain
discrete supervised labels for classification. In this paper, we successfully
perform multi-modal auto-regressive modeling with a unified objective for the
first time. Specifically, we propose the concept of visual words, which maps
the visual features to probability distributions over LLM's vocabulary,
providing supervision information for visual modelling. We further explore the
distribution of visual features in the semantic space within LMM and the
possibility of using text embeddings to represent visual information.
Experimental results and ablation studies on 5 VQA tasks and 4 benchmark
toolkits validate the powerful performance of our proposed approach.
\\ ( https://arxiv.org/abs/2403.07720 ,  1771kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07733 (*cross-listing*)
Date: Tue, 12 Mar 2024 15:13:12 GMT   (27216kb,D)

Title: DSEG-LIME -- Improving Image Explanation by Hierarchical Data-Driven
  Segmentation
Authors: Patrick Knab, Sascha Marton, Christian Bartelt
Categories: cs.CV cs.AI
\\
  Explainable Artificial Intelligence is critical in unraveling decision-making
processes in complex machine learning models. LIME (Local Interpretable
Model-agnostic Explanations) is a well-known XAI framework for image analysis.
It utilizes image segmentation to create features to identify relevant areas
for classification. Consequently, poor segmentation can compromise the
consistency of the explanation and undermine the importance of the segments,
affecting the overall interpretability. Addressing these challenges, we
introduce DSEG-LIME (Data-Driven Segmentation LIME), featuring: i) a
data-driven segmentation for human-recognized feature generation, and ii) a
hierarchical segmentation procedure through composition. We benchmark DSEG-LIME
on pre-trained models with images from the ImageNet dataset - scenarios without
domain-specific knowledge. The analysis includes a quantitative evaluation
using established XAI metrics, complemented by a qualitative assessment through
a user study. Our findings demonstrate that DSEG outperforms in most of the XAI
metrics and enhances the alignment of explanations with human-recognized
concepts, significantly improving interpretability. The code is available
under: https://github. com/patrick-knab/DSEG-LIME
\\ ( https://arxiv.org/abs/2403.07733 ,  27216kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07741 (*cross-listing*)
Date: Tue, 12 Mar 2024 15:19:25 GMT   (2839kb,D)

Title: Uncertainty Quantification with Deep Ensembles for 6D Object Pose
  Estimation
Authors: Kira Wursthorn, Markus Hillemann, Markus Ulrich
Categories: cs.CV cs.AI
Comments: 8 pages
\\
  The estimation of 6D object poses is a fundamental task in many computer
vision applications. Particularly, in high risk scenarios such as human-robot
interaction, industrial inspection, and automation, reliable pose estimates are
crucial. In the last years, increasingly accurate and robust
deep-learning-based approaches for 6D object pose estimation have been
proposed. Many top-performing methods are not end-to-end trainable but consist
of multiple stages. In the context of deep uncertainty quantification, deep
ensembles are considered as state of the art since they have been proven to
produce well-calibrated and robust uncertainty estimates. However, deep
ensembles can only be applied to methods that can be trained end-to-end. In
this work, we propose a method to quantify the uncertainty of multi-stage 6D
object pose estimation approaches with deep ensembles. For the implementation,
we choose SurfEmb as representative, since it is one of the top-performing 6D
object pose estimation approaches in the BOP Challenge 2022. We apply
established metrics and concepts for deep uncertainty quantification to
evaluate the results. Furthermore, we propose a novel uncertainty calibration
score for regression tasks to quantify the quality of the estimated
uncertainty.
\\ ( https://arxiv.org/abs/2403.07741 ,  2839kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07743 (*cross-listing*)
Date: Tue, 12 Mar 2024 15:22:05 GMT   (54386kb,D)

Title: Equipping Computational Pathology Systems with Artifact Processing
  Pipelines: A Showcase for Computation and Performance Trade-offs
Authors: Neel Kanwal, Farbod Khoraminia, Umay Kiraz, Andres Mosquera-Zamudio,
  Carlos Monteagudo, Emiel A.M. Janssen, Tahlita C.M. Zuiverloon, Chunmig Rong,
  and Kjersti Engan
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: Submitted to BMC Medical Informatics and Decision Making Journal
\\
  Histopathology is a gold standard for cancer diagnosis under a microscopic
examination. However, histological tissue processing procedures result in
artifacts, which are ultimately transferred to the digitized version of glass
slides, known as whole slide images (WSIs). Artifacts are diagnostically
irrelevant areas and may result in wrong deep learning (DL) algorithms
predictions. Therefore, detecting and excluding artifacts in the computational
pathology (CPATH) system is essential for reliable automated diagnosis. In this
paper, we propose a mixture of experts (MoE) scheme for detecting five notable
artifacts, including damaged tissue, blur, folded tissue, air bubbles, and
histologically irrelevant blood from WSIs. First, we train independent binary
DL models as experts to capture particular artifact morphology. Then, we
ensemble their predictions using a fusion mechanism. We apply probabilistic
thresholding over the final probability distribution to improve the sensitivity
of the MoE. We developed DL pipelines using two MoEs and two multiclass models
of state-of-the-art deep convolutional neural networks (DCNNs) and vision
transformers (ViTs). DCNNs-based MoE and ViTs-based MoE schemes outperformed
simpler multiclass models and were tested on datasets from different hospitals
and cancer types, where MoE using DCNNs yielded the best results. The proposed
MoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retaining
less computational cost for inference than MoE using ViTs. This best
performance of MoEs comes with relatively higher computational trade-offs than
multiclass models. The proposed artifact detection pipeline will not only
ensure reliable CPATH predictions but may also provide quality control.
\\ ( https://arxiv.org/abs/2403.07743 ,  54386kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07745 (*cross-listing*)
Date: Tue, 12 Mar 2024 15:28:21 GMT   (193kb,D)

Title: Probabilistic Easy Variational Causal Effect
Authors: Usef Faghihi and Amir Saki
Categories: stat.ML cs.AI cs.LG
Comments: 45 pages, 9 Figures
MSC-class: 26A45, 6008, 68T37, 68T20, 68T27, 68U99, 46N30, 62R10
ACM-class: G.3; I.2.3
\\
  Let $X$ and $Z$ be random vectors, and $Y=g(X,Z)$. In this paper, on the one
hand, for the case that $X$ and $Z$ are continuous, by using the ideas from the
total variation and the flux of $g$, we develop a point of view in causal
inference capable of dealing with a broad domain of causal problems. Indeed, we
focus on a function, called Probabilistic Easy Variational Causal Effect
(PEACE), which can measure the direct causal effect of $X$ on $Y$ with respect
to continuously and interventionally changing the values of $X$ while keeping
the value of $Z$ constant. PEACE is a function of $d\ge 0$, which is a degree
managing the strengths of probability density values $f(x|z)$. On the other
hand, we generalize the above idea for the discrete case and show its
compatibility with the continuous case. Further, we investigate some properties
of PEACE using measure theoretical concepts. Furthermore, we provide some
identifiability criteria and several examples showing the generic capability of
PEACE. We note that PEACE can deal with the causal problems for which
micro-level or just macro-level changes in the value of the input variables are
important. Finally, PEACE is stable under small changes in $\partial
g_{in}/\partial x$ and the joint distribution of $X$ and $Z$, where $g_{in}$ is
obtained from $g$ by removing all functional relationships defining $X$ and
$Z$.
\\ ( https://arxiv.org/abs/2403.07745 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07748 (*cross-listing*)
Date: Tue, 12 Mar 2024 15:33:09 GMT   (29kb)

Title: Ariadne and Theseus: Exploration and Rendezvous with Two Mobile Agents
  in an Unknown Graph
Authors: Romain Cosson
Categories: cs.MA cs.AI
\\
  We investigate two fundamental problems in mobile computing: exploration and
rendezvous, with two distinct mobile agents in an unknown graph. The agents can
read and write information on whiteboards that are located at all nodes. They
both move along one adjacent edge at every time-step. In the exploration
problem, both agents start from the same node of the graph and must traverse
all of its edges. We show that a simple variant of depth-first search achieves
collective exploration in $m$ synchronous time-steps, where $m$ is the number
of edges of the graph. This improves the competitive ratio of collective graph
exploration. In the rendezvous problem, the agents start from different nodes
of the graph and must meet as fast as possible. We introduce an algorithm
guaranteeing rendezvous in at most $\frac{3}{2}m$ time-steps. This improves
over the so-called `wait for Mommy' algorithm which requires $2m$ time-steps.
All our guarantees are derived from a more general asynchronous setting in
which the speeds of the agents are controlled by an adversary at all times. Our
guarantees also generalize to weighted graphs, if the number of edges $m$ is
replaced by the sum of all edge lengths.
\\ ( https://arxiv.org/abs/2403.07748 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07750 (*cross-listing*)
Date: Tue, 12 Mar 2024 15:36:42 GMT   (5027kb,D)

Title: Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and
  Image Embeddings
Authors: Sahand Sharifzadeh, Christos Kaplanis, Shreya Pathak, Dharshan
  Kumaran, Anastasija Ilic, Jovana Mitrovic, Charles Blundell, Andrea Banino
Categories: cs.CV cs.AI
Comments: 9 pages, 6 figures
\\
  The creation of high-quality human-labeled image-caption datasets presents a
significant bottleneck in the development of Visual-Language Models (VLMs). We
propose a novel approach that leverages the strengths of Large Language Models
(LLMs) and image generation models to create synthetic image-text pairs for
efficient and effective VLM training. Our method employs pretraining a
text-to-image model to synthesize image embeddings starting from captions
generated by an LLM. These synthetic pairs are then used to train a VLM.
Extensive experiments demonstrate that the VLM trained with synthetic data
exhibits comparable performance on image captioning, while requiring a fraction
of the data used by models trained solely on human-annotated data. In
particular, we outperform the baseline by 17% through augmentation with a
synthetic dataset. Furthermore, we show that synthesizing in the image
embedding space is 25% faster than in the pixel space. This research introduces
a promising technique for generating large-scale, customizable image datasets,
leading to enhanced VLM performance and wider applicability across various
domains, all with improved data efficiency and resource utilization.
\\ ( https://arxiv.org/abs/2403.07750 ,  5027kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07788 (*cross-listing*)
Date: Tue, 12 Mar 2024 16:23:49 GMT   (17395kb,D)

Title: DexCap: Scalable and Portable Mocap Data Collection System for Dexterous
  Manipulation
Authors: Chen Wang, Haochen Shi, Weizhuo Wang, Ruohan Zhang, Li Fei-Fei, C.
  Karen Liu
Categories: cs.RO cs.AI cs.CV cs.LG
\\
  Imitation learning from human hand motion data presents a promising avenue
for imbuing robots with human-like dexterity in real-world manipulation tasks.
Despite this potential, substantial challenges persist, particularly with the
portability of existing hand motion capture (mocap) systems and the difficulty
of translating mocap data into effective control policies. To tackle these
issues, we introduce DexCap, a portable hand motion capture system, alongside
DexIL, a novel imitation algorithm for training dexterous robot skills directly
from human hand mocap data. DexCap offers precise, occlusion-resistant tracking
of wrist and finger motions based on SLAM and electromagnetic field together
with 3D observations of the environment. Utilizing this rich dataset, DexIL
employs inverse kinematics and point cloud-based imitation learning to
replicate human actions with robot hands. Beyond learning from human motion,
DexCap also offers an optional human-in-the-loop correction mechanism to refine
and further improve robot performance. Through extensive evaluation across six
dexterous manipulation tasks, our approach not only demonstrates superior
performance but also showcases the system's capability to effectively learn
from in-the-wild mocap data, paving the way for future data collection methods
for dexterous manipulation. More details can be found at
https://dex-cap.github.io
\\ ( https://arxiv.org/abs/2403.07788 ,  17395kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07818 (*cross-listing*)
Date: Tue, 12 Mar 2024 16:57:56 GMT   (1419kb,D)

Title: Label Dropout: Improved Deep Learning Echocardiography Segmentation
  Using Multiple Datasets With Domain Shift and Partial Labelling
Authors: Iman Islam (1), Esther Puyol-Ant\'on (1), Bram Ruijsink (1), Andrew J.
  Reader (1), Andrew P. King (1) ((1) King's College London)
Categories: cs.CV cs.AI cs.LG
Comments: 10 pages, 5 figures, submitted to MICCAI conference
\\
  Echocardiography (echo) is the first imaging modality used when assessing
cardiac function. The measurement of functional biomarkers from echo relies
upon the segmentation of cardiac structures and deep learning models have been
proposed to automate the segmentation process. However, in order to translate
these tools to widespread clinical use it is important that the segmentation
models are robust to a wide variety of images (e.g. acquired from different
scanners, by operators with different levels of expertise etc.). To achieve
this level of robustness it is necessary that the models are trained with
multiple diverse datasets. A significant challenge faced when training with
multiple diverse datasets is the variation in label presence, i.e. the combined
data are often partially-labelled. Adaptations of the cross entropy loss
function have been proposed to deal with partially labelled data. In this paper
we show that training naively with such a loss function and multiple diverse
datasets can lead to a form of shortcut learning, where the model associates
label presence with domain characteristics, leading to a drop in performance.
To address this problem, we propose a novel label dropout scheme to break the
link between domain characteristics and the presence or absence of labels. We
demonstrate that label dropout improves echo segmentation Dice score by 62% and
25% on two cardiac structures when training using multiple diverse partially
labelled datasets.
\\ ( https://arxiv.org/abs/2403.07818 ,  1419kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07839 (*cross-listing*)
Date: Tue, 12 Mar 2024 17:24:26 GMT   (3099kb,D)

Title: MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with
  Module-wise Pruning Error Metric
Authors: Haokun Lin, Haoli Bai, Zhili Liu, Lu Hou, Muyi Sun, Linqi Song, Ying
  Wei, Zhenan Sun
Categories: cs.CV cs.AI cs.MM
Comments: 18 pages, 8 figures, Published in CVPR2024
Journal-ref: In Proc. IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2024
\\
  Vision-language pre-trained models have achieved impressive performance on
various downstream tasks. However, their large model sizes hinder their
utilization on platforms with limited computational resources. We find that
directly using smaller pre-trained models and applying magnitude-based pruning
on CLIP models leads to inflexibility and inferior performance. Recent efforts
for VLP compression either adopt uni-modal compression metrics resulting in
limited performance or involve costly mask-search processes with learnable
masks. In this paper, we first propose the Module-wise Pruning Error (MoPE)
metric, accurately assessing CLIP module importance by performance decline on
cross-modal tasks. Using the MoPE metric, we introduce a unified pruning
framework applicable to both pre-training and task-specific fine-tuning
compression stages. For pre-training, MoPE-CLIP effectively leverages knowledge
from the teacher model, significantly reducing pre-training costs while
maintaining strong zero-shot capabilities. For fine-tuning, consecutive pruning
from width to depth yields highly competitive task-specific models. Extensive
experiments in two stages demonstrate the effectiveness of the MoPE metric, and
MoPE-CLIP outperforms previous state-of-the-art VLP compression methods.
\\ ( https://arxiv.org/abs/2403.07839 ,  3099kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07869 (*cross-listing*)
Date: Tue, 12 Mar 2024 17:58:01 GMT   (2772kb,D)

Title: TeleMoMa: A Modular and Versatile Teleoperation System for Mobile
  Manipulation
Authors: Shivin Dass, Wensi Ai, Yuqian Jiang, Samik Singh, Jiaheng Hu, Ruohan
  Zhang, Peter Stone, Ben Abbatematteo, Roberto Martin-Martin
Categories: cs.RO cs.AI cs.LG
\\
  A critical bottleneck limiting imitation learning in robotics is the lack of
data. This problem is more severe in mobile manipulation, where collecting
demonstrations is harder than in stationary manipulation due to the lack of
available and easy-to-use teleoperation interfaces. In this work, we
demonstrate TeleMoMa, a general and modular interface for whole-body
teleoperation of mobile manipulators. TeleMoMa unifies multiple human
interfaces including RGB and depth cameras, virtual reality controllers,
keyboard, joysticks, etc., and any combination thereof. In its more accessible
version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering
the entry bar for humans to provide mobile manipulation demonstrations. We
demonstrate the versatility of TeleMoMa by teleoperating several existing
mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and
the real world. We demonstrate the quality of the demonstrations collected with
TeleMoMa by training imitation learning policies for mobile manipulation tasks
involving synchronized whole-body motion. Finally, we also show that TeleMoMa's
teleoperation channel enables teleoperation on site, looking at the robot, or
remote, sending commands and observations through a computer network, and
perform user studies to evaluate how easy it is for novice users to learn to
collect demonstrations with different combinations of human interfaces enabled
by our system. We hope TeleMoMa becomes a helpful tool for the community
enabling researchers to collect whole-body mobile manipulation demonstrations.
For more information and video results,
https://robin-lab.cs.utexas.edu/telemoma-web.
\\ ( https://arxiv.org/abs/2403.07869 ,  2772kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02684 (*cross-listing*)
Date: Sun, 5 Nov 2023 15:48:29 GMT   (8784kb,D)

Title: Octavius: Mitigating Task Interference in MLLMs via MoE
Authors: Zeren Chen, Ziqin Wang, Zhen Wang, Huayang Liu, Zhenfei Yin, Si Liu,
  Lu Sheng, Wanli Ouyang, Yu Qiao, Jing Shao
Categories: cs.CV cs.CL
Comments: 20 pages, 11 figures
\\
  Recent studies have demonstrated Large Language Models (LLMs) can extend
their zero-shot generalization capabilities to multimodal learning through
instruction tuning. As more modalities and downstream tasks are introduced,
negative conflicts and interference may have a worse impact on performance.
While this phenomenon has been overlooked in previous work, we propose a novel
and extensible framework, called \mname, for comprehensive studies and
experimentation on multimodal learning with Multimodal Large Language Models
(MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and
one of the representative PEFT techniques, \emph{i.e.,} LoRA, designing a novel
LLM-based decoder, called LoRA-MoE, for multimodal learning. The experimental
results (about 20\% improvement) have shown the effectiveness and versatility
of our design in various 2D and 3D downstream tasks. Code and corresponding
dataset will be available soon.
\\ ( https://arxiv.org/abs/2311.02684 ,  8784kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07142 (*cross-listing*)
Date: Mon, 11 Mar 2024 20:23:59 GMT   (7580kb,D)

Title: One Category One Prompt: Dataset Distillation using Diffusion Models
Authors: Ali Abbasi, Ashkan Shahbazi, Hamed Pirsiavash, Soheil Kolouri
Categories: cs.CV cs.CL cs.LG
\\
  The extensive amounts of data required for training deep neural networks pose
significant challenges on storage and transmission fronts. Dataset distillation
has emerged as a promising technique to condense the information of massive
datasets into a much smaller yet representative set of synthetic samples.
However, traditional dataset distillation approaches often struggle to scale
effectively with high-resolution images and more complex architectures due to
the limitations in bi-level optimization. Recently, several works have proposed
exploiting knowledge distillation with decoupled optimization schemes to scale
up dataset distillation. Although these methods effectively address the
scalability issue, they rely on extensive image augmentations requiring the
storage of soft labels for augmented images. In this paper, we introduce
Dataset Distillation using Diffusion Models (D3M) as a novel paradigm for
dataset distillation, leveraging recent advancements in generative
text-to-image foundation models. Our approach utilizes textual inversion, a
technique for fine-tuning text-to-image generative models, to create concise
and informative representations for large datasets. By employing these learned
text prompts, we can efficiently store and infer new samples for introducing
data variability within a fixed memory budget. We show the effectiveness of our
method through extensive experiments across various computer vision benchmark
datasets with different memory budgets.
\\ ( https://arxiv.org/abs/2403.07142 ,  7580kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07283 (*cross-listing*)
Date: Tue, 12 Mar 2024 03:30:04 GMT   (8297kb,D)

Title: A Framework for Cost-Effective and Self-Adaptive LLM Shaking and
  Recovery Mechanism
Authors: Zhiyu Chen, Yu Li, Suochao Zhang, Jingbo Zhou, Jiwen Zhou, Chenfu Bao,
  Dianhai Yu
Categories: cs.CR cs.CL cs.LG
Comments: 9 pages
\\
  As Large Language Models (LLMs) gain great success in real-world
applications, an increasing number of users are seeking to develop and deploy
their customized LLMs through cloud services. Nonetheless, in some specific
domains, there are still concerns regarding cost and trade-offs between privacy
issues and accuracy. In this study, we introduce a cost-effective and
self-adaptive LLM shaking tuning and recovery mechanism, named CypherTalk. With
carefully designed horizontal and vertical shaking operators, we can achieve
comparable accuracy results with SOTA privacy-preserving LLM schemes using
Cryptography-based or Differential Privacy-based methods. Experiments also show
that with the CypherTalk framework, users can achieve reliable accuracy when
using optimized shaking operator settings. To our best knowledge, this is the
first work that considers cost, and trade-off between model utility and privacy
in LLM scenarios.
\\ ( https://arxiv.org/abs/2403.07283 ,  8297kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07690 (*cross-listing*)
Date: Tue, 12 Mar 2024 14:33:53 GMT   (536kb,D)

Title: SATDAUG -- A Balanced and Augmented Dataset for Detecting Self-Admitted
  Technical Debt
Authors: Edi Sutoyo, Andrea Capiluppi
Categories: cs.SE cs.CL
Comments: Accepted to be published at the 21st IEEE/ACM International
  Conference on Mining Software Repositories (MSR 2024)
\\
  Self-admitted technical debt (SATD) refers to a form of technical debt in
which developers explicitly acknowledge and document the existence of technical
shortcuts, workarounds, or temporary solutions within the codebase. Over recent
years, researchers have manually labeled datasets derived from various software
development artifacts: source code comments, messages from the issue tracker
and pull request sections, and commit messages. These datasets are designed for
training, evaluation, performance validation, and improvement of machine
learning and deep learning models to accurately identify SATD instances.
However, class imbalance poses a serious challenge across all the existing
datasets, particularly when researchers are interested in categorizing the
specific types of SATD. In order to address the scarcity of labeled data for
SATD \textit{identification} (i.e., whether an instance is SATD or not) and
\textit{categorization} (i.e., which type of SATD is being classified) in
existing datasets, we share the \textit{SATDAUG} dataset, an augmented version
of existing SATD datasets, including source code comments, issue tracker, pull
requests, and commit messages. These augmented datasets have been balanced in
relation to the available artifacts and provide a much richer source of labeled
data for training machine learning or deep learning models.
\\ ( https://arxiv.org/abs/2403.07690 ,  536kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06992 (*cross-listing*)
Date: Wed, 28 Feb 2024 06:03:55 GMT   (1960kb,D)

Title: Phase autoencoder for limit-cycle oscillators
Authors: Koichiro Yawata and Kai Fukami and Kunihiko Taira and Hiroya Nakao
Categories: nlin.AO cs.LG nlin.CD
Comments: 12 pages, 16 figures
\\
  We present a phase autoencoder that encodes the asymptotic phase of a
limit-cycle oscillator, a fundamental quantity characterizing its
synchronization dynamics. This autoencoder is trained in such a way that its
latent variables directly represent the asymptotic phase of the oscillator. The
trained autoencoder can perform two functions without relying on the
mathematical model of the oscillator: first, it can evaluate the asymptotic
phase and phase sensitivity function of the oscillator; second, it can
reconstruct the oscillator state on the limit cycle in the original space from
the phase value as an input. Using several examples of limit-cycle oscillators,
we demonstrate that the asymptotic phase and phase sensitivity function can be
estimated only from time-series data by the trained autoencoder. We also
present a simple method for globally synchronizing two oscillators as an
application of the trained autoencoder.
\\ ( https://arxiv.org/abs/2403.06992 ,  1960kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07013 (*cross-listing*)
Date: Sat, 9 Mar 2024 11:54:58 GMT   (673kb,D)

Title: AdaNovo: Adaptive \emph{De Novo} Peptide Sequencing with Conditional
  Mutual Information
Authors: Jun Xia, Shaorong Chen, Jingbo Zhou, Tianze Lin, Wenjie Du, Sizhe Liu,
  Stan Z. Li
Categories: q-bio.QM cs.LG q-bio.BM
\\
  Tandem mass spectrometry has played a pivotal role in advancing proteomics,
enabling the analysis of protein composition in biological samples. Despite the
development of various deep learning methods for identifying amino acid
sequences (peptides) responsible for observed spectra, challenges persist in
\emph{de novo} peptide sequencing. Firstly, prior methods struggle to identify
amino acids with post-translational modifications (PTMs) due to their lower
frequency in training data compared to canonical amino acids, further resulting
in decreased peptide-level identification precision. Secondly, diverse types of
noise and missing peaks in mass spectra reduce the reliability of training data
(peptide-spectrum matches, PSMs). To address these challenges, we propose
AdaNovo, a novel framework that calculates conditional mutual information (CMI)
between the spectrum and each amino acid/peptide, using CMI for adaptive model
training. Extensive experiments demonstrate AdaNovo's state-of-the-art
performance on a 9-species benchmark, where the peptides in the training set
are almost completely disjoint from the peptides of the test sets. Moreover,
AdaNovo excels in identifying amino acids with PTMs and exhibits robustness
against data noise. The supplementary materials contain the official code.
\\ ( https://arxiv.org/abs/2403.07013 ,  673kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07025 (*cross-listing*)
Date: Sun, 10 Mar 2024 15:35:41 GMT   (349kb,D)

Title: Enhancing Quantum Variational Algorithms with Zero Noise Extrapolation
  via Neural Networks
Authors: Subhasree Bhattacharjee, Soumyadip Sarkar, Kunal Das, Bikramjit Sarkar
Categories: quant-ph cs.LG
\\
  In the emergent realm of quantum computing, the Variational Quantum
Eigensolver (VQE) stands out as a promising algorithm for solving complex
quantum problems, especially in the noisy intermediate-scale quantum (NISQ)
era. However, the ubiquitous presence of noise in quantum devices often limits
the accuracy and reliability of VQE outcomes. This research introduces a novel
approach to ameliorate this challenge by utilizing neural networks for zero
noise extrapolation (ZNE) in VQE computations. By employing the Qiskit
framework, we crafted parameterized quantum circuits using the RY-RZ ansatz and
examined their behavior under varying levels of depolarizing noise. Our
investigations spanned from determining the expectation values of a
Hamiltonian, defined as a tensor product of Z operators, under different noise
intensities to extracting the ground state energy. To bridge the observed
outcomes under noise with the ideal noise-free scenario, we trained a Feed
Forward Neural Network on the error probabilities and their associated
expectation values. Remarkably, our model proficiently predicted the VQE
outcome under hypothetical noise-free conditions. By juxtaposing the simulation
results with real quantum device executions, we unveiled the discrepancies
induced by noise and showcased the efficacy of our neural network-based ZNE
technique in rectifying them. This integrative approach not only paves the way
for enhanced accuracy in VQE computations on NISQ devices but also underlines
the immense potential of hybrid quantum-classical paradigms in circumventing
the challenges posed by quantum noise. Through this research, we envision a
future where quantum algorithms can be reliably executed on noisy devices,
bringing us one step closer to realizing the full potential of quantum
computing.
\\ ( https://arxiv.org/abs/2403.07025 ,  349kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07026 (*cross-listing*)
Date: Sun, 10 Mar 2024 15:45:39 GMT   (732kb,D)

Title: Whiteness-based bilevel learning of regularization parameters in imaging
Authors: Carlo Santambrogio, Monica Pragliola, Alessandro Lanza, Marco
  Donatelli, Luca Calatroni
Categories: math.OC cs.LG eess.IV
\\
  We consider an unsupervised bilevel optimization strategy for learning
regularization parameters in the context of imaging inverse problems in the
presence of additive white Gaussian noise. Compared to supervised and
semi-supervised metrics relying either on the prior knowledge of reference data
and/or on some (partial) knowledge on the noise statistics, the proposed
approach optimizes the whiteness of the residual between the observed data and
the observation model with no need of ground-truth data.We validate the
approach on standard Total Variation-regularized image deconvolution problems
which show that the proposed quality metric provides estimates close to the
mean-square error oracle and to discrepancy-based principles.
\\ ( https://arxiv.org/abs/2403.07026 ,  732kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07035 (*cross-listing*)
Date: Mon, 11 Mar 2024 08:05:01 GMT   (912kb,D)

Title: Multiple Population Alternate Evolution Neural Architecture Search
Authors: Juan Zou, Han Chu, Yizhang Xia, Junwen Xu, Yuan Liu, Zhanglu Hou
Categories: cs.NE cs.LG
\\
  The effectiveness of Evolutionary Neural Architecture Search (ENAS) is
influenced by the design of the search space. Nevertheless, common methods
including the global search space, scalable search space and hierarchical
search space have certain limitations. Specifically, the global search space
requires a significant amount of computational resources and time, the scalable
search space sacrifices the diversity of network structures and the
hierarchical search space increases the search cost in exchange for network
diversity. To address above limitation, we propose a novel paradigm of
searching neural network architectures and design the Multiple Population
Alternate Evolution Neural Architecture Search (MPAE), which can achieve module
diversity with a smaller search cost. MPAE converts the search space into L
interconnected units and sequentially searches the units, then the above search
of the entire network be cycled several times to reduce the impact of previous
units on subsequent units. To accelerate the population evolution process, we
also propose the the population migration mechanism establishes an excellent
migration archive and transfers the excellent knowledge and experience in the
migration archive to new populations. The proposed method requires only 0.3 GPU
days to search a neural network on the CIFAR dataset and achieves the
state-of-the-art results.
\\ ( https://arxiv.org/abs/2403.07035 ,  912kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07059 (*cross-listing*)
Date: Mon, 11 Mar 2024 18:00:06 GMT   (8528kb,D)

Title: Better than classical? The subtle art of benchmarking quantum machine
  learning models
Authors: Joseph Bowles, Shahnawaz Ahmed, Maria Schuld
Categories: quant-ph cs.LG
\\
  Benchmarking models via classical simulations is one of the main ways to
judge ideas in quantum machine learning before noise-free hardware is
available. However, the huge impact of the experimental design on the results,
the small scales within reach today, as well as narratives influenced by the
commercialisation of quantum technologies make it difficult to gain robust
insights. To facilitate better decision-making we develop an open-source
package based on the PennyLane software framework and use it to conduct a
large-scale study that systematically tests 12 popular quantum machine learning
models on 6 binary classification tasks used to create 160 individual datasets.
We find that overall, out-of-the-box classical machine learning models
outperform the quantum classifiers. Moreover, removing entanglement from a
quantum model often results in as good or better performance, suggesting that
"quantumness" may not be the crucial ingredient for the small learning tasks
considered here. Our benchmarks also unlock investigations beyond simplistic
leaderboard comparisons, and we identify five important questions for quantum
model design that follow from our results.
\\ ( https://arxiv.org/abs/2403.07059 ,  8528kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07066 (*cross-listing*)
Date: Mon, 11 Mar 2024 18:00:47 GMT   (25190kb,D)

Title: Re-Simulation-based Self-Supervised Learning for Pre-Training Foundation
  Models
Authors: Philip Harris, Michael Kagan, Jeffrey Krupa, Benedikt Maier, Nathaniel
  Woodward
Categories: hep-ph cs.LG hep-ex
Comments: 24 pages, 9 figures
\\
  Self-Supervised Learning (SSL) is at the core of training modern large
machine learning models, providing a scheme for learning powerful
representations that can be used in a variety of downstream tasks. However, SSL
strategies must be adapted to the type of training data and downstream tasks
required. We propose RS3L, a novel simulation-based SSL strategy that employs a
method of re-simulation to drive data augmentation for contrastive learning. By
intervening in the middle of the simulation process and re-running simulation
components downstream of the intervention, we generate multiple realizations of
an event, thus producing a set of augmentations covering all physics-driven
variations available in the simulator. Using experiments from high-energy
physics, we explore how this strategy may enable the development of a
foundation model; we show how R3SL pre-training enables powerful performance in
downstream tasks such as discrimination of a variety of objects and uncertainty
mitigation. In addition to our results, we make the RS3L dataset publicly
available for further studies on how to improve SSL strategies.
\\ ( https://arxiv.org/abs/2403.07066 ,  25190kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07092 (*cross-listing*)
Date: Mon, 11 Mar 2024 18:36:55 GMT   (381kb,D)

Title: A cascaded deep network for automated tumor detection and segmentation
  in clinical PET imaging of diffuse large B-cell lymphoma
Authors: Shadab Ahamed, Natalia Dubljevic, Ingrid Bloise, Claire Gowdy, Patrick
  Martineau, Don Wilson, Carlos F. Uribe, Arman Rahmim, and Fereshteh
  Yousefirizi
Categories: eess.IV cs.CV cs.LG physics.med-ph
Comments: 8 pages, 3 figures, 3 tables
Journal-ref: Proc. SPIE 12032, Medical Imaging 2022: Image Processing, 120323M
  (4 April 2022)
DOI: 10.1117/12.2612684
\\
  Accurate detection and segmentation of diffuse large B-cell lymphoma (DLBCL)
from PET images has important implications for estimation of total metabolic
tumor volume, radiomics analysis, surgical intervention and radiotherapy.
Manual segmentation of tumors in whole-body PET images is time-consuming,
labor-intensive and operator-dependent. In this work, we develop and validate a
fast and efficient three-step cascaded deep learning model for automated
detection and segmentation of DLBCL tumors from PET images. As compared to a
single end-to-end network for segmentation of tumors in whole-body PET images,
our three-step model is more effective (improves 3D Dice score from 58.9% to
78.1%) since each of its specialized modules, namely the slice classifier, the
tumor detector and the tumor segmentor, can be trained independently to a high
degree of skill to carry out a specific task, rather than a single network with
suboptimal performance on overall segmentation.
\\ ( https://arxiv.org/abs/2403.07092 ,  381kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07105 (*cross-listing*)
Date: Mon, 11 Mar 2024 18:57:45 GMT   (22120kb,D)

Title: A slice classification neural network for automated classification of
  axial PET/CT slices from a multi-centric lymphoma dataset
Authors: Shadab Ahamed, Yixi Xu, Ingrid Bloise, Joo H. O, Carlos F. Uribe,
  Rahul Dodhia, Juan L. Ferres, and Arman Rahmim
Categories: eess.IV cs.CV cs.LG physics.med-ph
Comments: 10 pages, 6 figures, 2 tables
Journal-ref: Proc. SPIE 12464, Medical Imaging 2023: Image Processing, 124641Q
  (3 April 2023)
DOI: 10.1117/12.2652947
\\
  Automated slice classification is clinically relevant since it can be
incorporated into medical image segmentation workflows as a preprocessing step
that would flag slices with a higher probability of containing tumors, thereby
directing physicians attention to the important slices. In this work, we train
a ResNet-18 network to classify axial slices of lymphoma PET/CT images
(collected from two institutions) depending on whether the slice intercepted a
tumor (positive slice) in the 3D image or if the slice did not (negative
slice). Various instances of the network were trained on 2D axial datasets
created in different ways: (i) slice-level split and (ii) patient-level split;
inputs of different types were used: (i) only PET slices and (ii) concatenated
PET and CT slices; and different training strategies were employed: (i)
center-aware (CAW) and (ii) center-agnostic (CAG). Model performances were
compared using the area under the receiver operating characteristic curve
(AUROC) and the area under the precision-recall curve (AUPRC), and various
binary classification metrics. We observe and describe a performance
overestimation in the case of slice-level split as compared to the
patient-level split training. The model trained using patient-level split data
with the network input containing only PET slices in the CAG training regime
was the best performing/generalizing model on a majority of metrics. Our models
were additionally more closely compared using the sensitivity metric on the
positive slices from their respective test sets.
\\ ( https://arxiv.org/abs/2403.07105 ,  22120kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07113 (*cross-listing*)
Date: Mon, 11 Mar 2024 19:06:04 GMT   (4290kb,D)

Title: Class Imbalance in Object Detection: An Experimental Diagnosis and Study
  of Mitigation Strategies
Authors: Nieves Crasto
Categories: cs.CV cs.LG
\\
  Object detection, a pivotal task in computer vision, is frequently hindered
by dataset imbalances, particularly the under-explored issue of
foreground-foreground class imbalance. This lack of attention to
foreground-foreground class imbalance becomes even more pronounced in the
context of single-stage detectors. This study introduces a benchmarking
framework utilizing the YOLOv5 single-stage detector to address the problem of
foreground-foreground class imbalance. We crafted a novel 10-class long-tailed
dataset from the COCO dataset, termed COCO-ZIPF, tailored to reflect common
real-world detection scenarios with a limited number of object classes. Against
this backdrop, we scrutinized three established techniques: sampling, loss
weighing, and data augmentation. Our comparative analysis reveals that sampling
and loss reweighing methods, while shown to be beneficial in two-stage detector
settings, do not translate as effectively in improving YOLOv5's performance on
the COCO-ZIPF dataset. On the other hand, data augmentation methods,
specifically mosaic and mixup, significantly enhance the model's mean Average
Precision (mAP), by introducing more variability and complexity into the
training data. (Code available:
https://github.com/craston/object_detection_cib)
\\ ( https://arxiv.org/abs/2403.07113 ,  4290kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07128 (*cross-listing*)
Date: Mon, 11 Mar 2024 19:51:01 GMT   (150kb,D)

Title: FAX: Scalable and Differentiable Federated Primitives in JAX
Authors: Keith Rush, Zachary Charles, Zachary Garrett
Categories: cs.DC cs.LG
\\
  We present FAX, a JAX-based library designed to support large-scale
distributed and federated computations in both data center and cross-device
applications. FAX leverages JAX's sharding mechanisms to enable native
targeting of TPUs and state-of-the-art JAX runtimes, including Pathways. FAX
embeds building blocks for federated computations as primitives in JAX. This
enables three key benefits. First, FAX computations can be translated to XLA
HLO. Second, FAX provides a full implementation of federated automatic
differentiation, greatly simplifying the expression of federated computations.
Last, FAX computations can be interpreted out to existing production
cross-device federated compute systems. We show that FAX provides an easily
programmable, performant, and scalable framework for federated computations in
the data center. FAX is available at
https://github.com/google-research/google-research/tree/master/fax .
\\ ( https://arxiv.org/abs/2403.07128 ,  150kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07132 (*cross-listing*)
Date: Mon, 11 Mar 2024 20:02:17 GMT   (597kb,D)

Title: A New Machine Learning Dataset of Bulldog Nostril Images for Stenosis
  Degree Classification
Authors: Gabriel Toshio Hirokawa Higa, Joyce Katiuccia Medeiros Ramos Carvalho,
  Paolo Brito Pascoalini Zanoni, Gisele Braziliano de Andrade, Hemerson Pistori
Categories: eess.IV cs.CV cs.LG
\\
  Brachycephaly, a conformation trait in some dog breeds, causes BOAS, a
respiratory disorder that affects the health and welfare of the dogs with
various symptoms. In this paper, a new annotated dataset composed of 190 images
of bulldogs' nostrils is presented. Three degrees of stenosis are approximately
equally represented in the dataset: mild, moderate and severe stenosis. The
dataset also comprises a small quantity of non stenotic nostril images. To the
best of our knowledge, this is the first image dataset addressing this problem.
Furthermore, deep learning is investigated as an alternative to automatically
infer stenosis degree using nostril images. In this work, several neural
networks were tested: ResNet50, MobileNetV3, DenseNet201, SwinV2 and MaxViT.
For this evaluation, the problem was modeled in two different ways: first, as a
three-class classification problem (mild or open, moderate, and severe);
second, as a binary classification problem, with severe stenosis as target. For
the multiclass classification, a maximum median f-score of 53.77\% was achieved
by the MobileNetV3. For binary classification, a maximum median f-score of
72.08\% has been reached by ResNet50, indicating that the problem is
challenging but possibly tractable.
\\ ( https://arxiv.org/abs/2403.07132 ,  597kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07137 (*cross-listing*)
Date: Mon, 11 Mar 2024 20:07:05 GMT   (10106kb,D)

Title: Exploring Cluster Analysis in Nelore Cattle Visual Score Attribution
Authors: Alexandre de Oliveira Bezerra, Rodrigo Goncalves Mateus, Vanessa Ap.
  de Moraes Weber, Fabricio de Lima Weber, Yasmin Alves de Arruda, Rodrigo da
  Costa Gomes, Gabriel Toshio Hirokawa Higa, Hemerson Pistori
Categories: eess.IV cs.CV cs.LG
\\
  Assessing the biotype of cattle through human visual inspection is a very
common and important practice in precision cattle breeding. This paper presents
the results of a correlation analysis between scores produced by humans for
Nelore cattle and a variety of measurements that can be derived from images or
other instruments. It also presents a study using the k-means algorithm to
generate new ways of clustering a batch of cattle using the measurements that
most correlate with the animal's body weight and visual scores.
\\ ( https://arxiv.org/abs/2403.07137 ,  10106kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07143 (*cross-listing*)
Date: Mon, 11 Mar 2024 20:28:23 GMT   (63kb)

Title: New Perspectives in Online Contract Design: Heterogeneous, Homogeneous,
  Non-myopic Agents and Team Production
Authors: Shiliang Zuo
Categories: cs.GT cs.LG
\\
  This work studies the repeated principal-agent problem from an online
learning perspective. The principal's goal is to learn the optimal contract
that maximizes her utility through repeated interactions, without prior
knowledge of the agent's type (i.e., the agent's cost and production
functions).
  I study three different settings when the principal contracts with a
$\textit{single}$ agent each round: 1. The agents are heterogeneous; 2. the
agents are homogenous; 3. the principal interacts with the same agent and the
agent is non-myopic. I present different approaches and techniques for
designing learning algorithms in each setting. For heterogeneous agent types, I
identify a condition that allows the problem to be reduced to Lipschitz bandits
directly. For identical agents, I give a polynomial sample complexity scheme to
learn the optimal contract based on inverse game theory. For strategic
non-myopic agents, I design a low strategic-regret mechanism. Also, I identify
a connection between linear contracts and posted-price auctions, showing the
two can be reduced to one another, and give a regret lower bound on learning
the optimal linear contract based on this observation.
  I also study a $\textit{team production}$ model. I identify a condition under
which the principal's learning problem can be reformulated as solving a family
of convex programs, thereby showing the optimal contract can be found
efficiently.
\\ ( https://arxiv.org/abs/2403.07143 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07148 (*cross-listing*)
Date: Mon, 11 Mar 2024 20:35:52 GMT   (417kb,D)

Title: Stochastic Extragradient with Random Reshuffling: Improved Convergence
  for Variational Inequalities
Authors: Konstantinos Emmanouilidis, Ren\'e Vidal, Nicolas Loizou
Categories: math.OC cs.GT cs.LG stat.ML
\\
  The Stochastic Extragradient (SEG) method is one of the most popular
algorithms for solving finite-sum min-max optimization and variational
inequality problems (VIPs) appearing in various machine learning tasks.
However, existing convergence analyses of SEG focus on its with-replacement
variants, while practical implementations of the method randomly reshuffle
components and sequentially use them. Unlike the well-studied with-replacement
variants, SEG with Random Reshuffling (SEG-RR) lacks established theoretical
guarantees. In this work, we provide a convergence analysis of SEG-RR for three
classes of VIPs: (i) strongly monotone, (ii) affine, and (iii) monotone. We
derive conditions under which SEG-RR achieves a faster convergence rate than
the uniform with-replacement sampling SEG. In the monotone setting, our
analysis of SEG-RR guarantees convergence to an arbitrary accuracy without
large batch sizes, a strong requirement needed in the classical
with-replacement SEG. As a byproduct of our results, we provide convergence
guarantees for Shuffle Once SEG (shuffles the data only at the beginning of the
algorithm) and the Incremental Extragradient (does not shuffle the data). We
supplement our analysis with experiments validating empirically the superior
performance of SEG-RR over the classical with-replacement sampling SEG.
\\ ( https://arxiv.org/abs/2403.07148 ,  417kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07207 (*cross-listing*)
Date: Mon, 11 Mar 2024 23:21:26 GMT   (400kb,D)

Title: Tracking Dynamic Gaussian Density with a Theoretically Optimal Sliding
  Window Approach
Authors: Yinsong Wang, Yu Ding, Shahin Shahrampour
Categories: stat.ML cs.LG
\\
  Dynamic density estimation is ubiquitous in many applications, including
computer vision and signal processing. One popular method to tackle this
problem is the "sliding window" kernel density estimator. There exist various
implementations of this method that use heuristically defined weight sequences
for the observed data. The weight sequence, however, is a key aspect of the
estimator affecting the tracking performance significantly. In this work, we
study the exact mean integrated squared error (MISE) of "sliding window"
Gaussian Kernel Density Estimators for evolving Gaussian densities. We provide
a principled guide for choosing the optimal weight sequence by theoretically
characterizing the exact MISE, which can be formulated as constrained quadratic
programming. We present empirical evidence with synthetic datasets to show that
our weighting scheme indeed improves the tracking performance compared to
heuristic approaches.
\\ ( https://arxiv.org/abs/2403.07207 ,  400kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07216 (*cross-listing*)
Date: Tue, 12 Mar 2024 00:08:54 GMT   (375kb,D)

Title: Adaptive Gain Scheduling using Reinforcement Learning for Quadcopter
  Control
Authors: Mike Timmerman, Aryan Patel, Tim Reinhart
Categories: eess.SY cs.LG cs.RO cs.SY
\\
  The paper presents a technique using reinforcement learning (RL) to adapt the
control gains of a quadcopter controller. Specifically, we employed Proximal
Policy Optimization (PPO) to train a policy which adapts the gains of a
cascaded feedback controller in-flight. The primary goal of this controller is
to minimize tracking error while following a specified trajectory. The paper's
key objective is to analyze the effectiveness of the adaptive gain policy and
compare it to the performance of a static gain control algorithm, where the
Integral Squared Error and Integral Time Squared Error are used as metrics. The
results show that the adaptive gain scheme achieves over 40$\%$ decrease in
tracking error as compared to the static gain controller.
\\ ( https://arxiv.org/abs/2403.07216 ,  375kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07218 (*cross-listing*)
Date: Tue, 12 Mar 2024 00:25:14 GMT   (4163kb,D)

Title: SoK: Can Trajectory Generation Combine Privacy and Utility?
Authors: Erik Buchholz and Alsharif Abuadbba and Shuo Wang and Surya Nepal and
  Salil S. Kanhere
Categories: cs.CR cs.LG
Comments: Accepted at PETS'24 Issue 3. DOI to be added
\\
  While location trajectories represent a valuable data source for analyses and
location-based services, they can reveal sensitive information, such as
political and religious preferences. Differentially private publication
mechanisms have been proposed to allow for analyses under rigorous privacy
guarantees. However, the traditional protection schemes suffer from a limiting
privacy-utility trade-off and are vulnerable to correlation and reconstruction
attacks. Synthetic trajectory data generation and release represent a promising
alternative to protection algorithms. While initial proposals achieve
remarkable utility, they fail to provide rigorous privacy guarantees. This
paper proposes a framework for designing a privacy-preserving trajectory
publication approach by defining five design goals, particularly stressing the
importance of choosing an appropriate Unit of Privacy. Based on this framework,
we briefly discuss the existing trajectory protection approaches, emphasising
their shortcomings. This work focuses on the systematisation of the
state-of-the-art generative models for trajectories in the context of the
proposed framework. We find that no existing solution satisfies all
requirements. Thus, we perform an experimental study evaluating the
applicability of six sequential generative models to the trajectory domain.
Finally, we conclude that a generative trajectory model providing semantic
guarantees remains an open research question and propose concrete next steps
for future research.
\\ ( https://arxiv.org/abs/2403.07218 ,  4163kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07232 (*cross-listing*)
Date: Tue, 12 Mar 2024 01:00:52 GMT   (3615kb,D)

Title: Tractable Joint Prediction and Planning over Discrete Behavior Modes for
  Urban Driving
Authors: Adam Villaflor, Brian Yang, Huangyuan Su, Katerina Fragkiadaki, John
  Dolan, Jeff Schneider
Categories: cs.RO cs.LG
\\
  Significant progress has been made in training multimodal trajectory
forecasting models for autonomous driving. However, effectively integrating
these models with downstream planners and model-based control approaches is
still an open problem. Although these models have conventionally been evaluated
for open-loop prediction, we show that they can be used to parameterize
autoregressive closed-loop models without retraining. We consider recent
trajectory prediction approaches which leverage learned anchor embeddings to
predict multiple trajectories, finding that these anchor embeddings can
parameterize discrete and distinct modes representing high-level driving
behaviors. We propose to perform fully reactive closed-loop planning over these
discrete latent modes, allowing us to tractably model the causal interactions
between agents at each step. We validate our approach on a suite of more
dynamic merging scenarios, finding that our approach avoids the $\textit{frozen
robot problem}$ which is pervasive in conventional planners. Our approach also
outperforms the previous state-of-the-art in CARLA on challenging dense traffic
scenarios when evaluated at realistic speeds.
\\ ( https://arxiv.org/abs/2403.07232 ,  3615kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07241 (*cross-listing*)
Date: Tue, 12 Mar 2024 01:47:17 GMT   (6949kb,D)

Title: Calibrating Multi-modal Representations: A Pursuit of Group Robustness
  without Annotations
Authors: Chenyu You, Yifei Min, Weicheng Dai, Jasjeet S. Sekhon, Lawrence
  Staib, James S. Duncan
Categories: cs.CV cs.LG
Comments: Accepted by CVPR 2024
\\
  Fine-tuning pre-trained vision-language models, like CLIP, has yielded
success on diverse downstream tasks. However, several pain points persist for
this paradigm: (i) directly tuning entire pre-trained models becomes both
time-intensive and computationally costly. Additionally, these tuned models
tend to become highly specialized, limiting their practicality for real-world
deployment; (ii) recent studies indicate that pre-trained vision-language
classifiers may overly depend on spurious features -- patterns that correlate
with the target in training data, but are not related to the true labeling
function; and (iii) existing studies on mitigating the reliance on spurious
features, largely based on the assumption that we can identify such features,
does not provide definitive assurance for real-world applications. As a
piloting study, this work focuses on exploring mitigating the reliance on
spurious features for CLIP without using any group annotation. To this end, we
systematically study the existence of spurious correlation on CLIP and
CILP+ERM. We first, following recent work on Deep Feature Reweighting (DFR),
verify that last-layer retraining can greatly improve group robustness on
pretrained CLIP. In view of them, we advocate a lightweight representation
calibration method for fine-tuning CLIP, by first generating a calibration set
using the pretrained CLIP, and then calibrating representations of samples
within this set through contrastive learning, all without the need for group
labels. Extensive experiments and in-depth visualizations on several benchmarks
validate the effectiveness of our proposals, largely reducing reliance and
significantly boosting the model generalization.
\\ ( https://arxiv.org/abs/2403.07241 ,  6949kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07247 (*cross-listing*)
Date: Tue, 12 Mar 2024 02:09:39 GMT   (3923kb,D)

Title: GuideGen: A Text-guided Framework for Joint CT Volume and Anatomical
  structure Generation
Authors: Linrui Dai, Rongzhao Zhang, Zhongzhen Huang, Xiaofan Zhang
Categories: eess.IV cs.CV cs.LG
Comments: submitted to MICCAI2024
\\
  The annotation burden and extensive labor for gathering a large medical
dataset with images and corresponding labels are rarely cost-effective and
highly intimidating. This results in a lack of abundant training data that
undermines downstream tasks and partially contributes to the challenge image
analysis faces in the medical field. As a workaround, given the recent success
of generative neural models, it is now possible to synthesize image datasets at
a high fidelity guided by external constraints. This paper explores this
possibility and presents \textbf{GuideGen}: a pipeline that jointly generates
CT images and tissue masks for abdominal organs and colorectal cancer
conditioned on a text prompt. Firstly, we introduce Volumetric Mask Sampler to
fit the discrete distribution of mask labels and generate low-resolution 3D
tissue masks. Secondly, our Conditional Image Generator autoregressively
generates CT slices conditioned on a corresponding mask slice to incorporate
both style information and anatomical guidance. This pipeline guarantees high
fidelity and variability as well as exact alignment between generated CT
volumes and tissue masks. Both qualitative and quantitative experiments on 3D
abdominal CTs demonstrate a high performance of our proposed pipeline, thereby
proving our method can serve as a dataset generator and provide potential
benefits to downstream tasks. It is hoped that our work will offer a promising
solution on the multimodality generation of CT and its anatomical mask. Our
source code is publicly available at
https://github.com/OvO1111/JointImageGeneration.
\\ ( https://arxiv.org/abs/2403.07247 ,  3923kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07263 (*cross-listing*)
Date: Tue, 12 Mar 2024 02:45:24 GMT   (20020kb,D)

Title: Adaptive Bounding Box Uncertainties via Two-Step Conformal Prediction
Authors: Alexander Timans, Christoph-Nikolas Straehle, Kaspar Sakmann, Eric
  Nalisnick
Categories: cs.CV cs.LG stat.ML
Comments: 38 pages, 14 figures, 6 tables (incl. appendix)
\\
  Quantifying a model's predictive uncertainty is essential for safety-critical
applications such as autonomous driving. We consider quantifying such
uncertainty for multi-object detection. In particular, we leverage conformal
prediction to obtain uncertainty intervals with guaranteed coverage for object
bounding boxes. One challenge in doing so is that bounding box predictions are
conditioned on the object's class label. Thus, we develop a novel two-step
conformal approach that propagates uncertainty in predicted class labels into
the uncertainty intervals for the bounding boxes. This broadens the validity of
our conformal coverage guarantees to include incorrectly classified objects,
ensuring their usefulness when maximal safety assurances are required.
Moreover, we investigate novel ensemble and quantile regression formulations to
ensure the bounding box intervals are adaptive to object size, leading to a
more balanced coverage across sizes. Validating our two-step approach on
real-world datasets for 2D bounding box localization, we find that desired
coverage levels are satisfied with actionably tight predictive uncertainty
intervals.
\\ ( https://arxiv.org/abs/2403.07263 ,  20020kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07264 (*cross-listing*)
Date: Tue, 12 Mar 2024 02:47:00 GMT   (3074kb,D)

Title: Near-Interpolators: Rapid Norm Growth and the Trade-Off between
  Interpolation and Generalization
Authors: Yutong Wang, Rishi Sonthalia, Wei Hu
Categories: stat.ML cs.LG
Comments: AISTATS 2024
\\
  We study the generalization capability of nearly-interpolating linear
regressors: $\boldsymbol{\beta}$'s whose training error $\tau$ is positive but
small, i.e., below the noise floor. Under a random matrix theoretic assumption
on the data distribution and an eigendecay assumption on the data covariance
matrix $\boldsymbol{\Sigma}$, we demonstrate that any near-interpolator
exhibits rapid norm growth: for $\tau$ fixed, $\boldsymbol{\beta}$ has squared
$\ell_2$-norm $\mathbb{E}[\|{\boldsymbol{\beta}}\|_{2}^{2}] =
\Omega(n^{\alpha})$ where $n$ is the number of samples and $\alpha >1$ is the
exponent of the eigendecay, i.e., $\lambda_i(\boldsymbol{\Sigma}) \sim
i^{-\alpha}$. This implies that existing data-independent norm-based bounds are
necessarily loose. On the other hand, in the same regime we precisely
characterize the asymptotic trade-off between interpolation and generalization.
Our characterization reveals that larger norm scaling exponents $\alpha$
correspond to worse trade-offs between interpolation and generalization. We
verify empirically that a similar phenomenon holds for nearly-interpolating
shallow neural networks.
\\ ( https://arxiv.org/abs/2403.07264 ,  3074kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07310 (*cross-listing*)
Date: Tue, 12 Mar 2024 04:38:05 GMT   (4426kb,D)

Title: How does promoting the minority fraction affect generalization? A
  theoretical study of the one-hidden-layer neural network on group imbalance
Authors: Hongkang Li, Shuai Zhang, Yihua Zhang, Meng Wang, Sijia Liu, Pin-Yu
  Chen
Categories: stat.ML cs.LG
\\
  Group imbalance has been a known problem in empirical risk minimization
(ERM), where the achieved high average accuracy is accompanied by low accuracy
in a minority group. Despite algorithmic efforts to improve the minority group
accuracy, a theoretical generalization analysis of ERM on individual groups
remains elusive. By formulating the group imbalance problem with the Gaussian
Mixture Model, this paper quantifies the impact of individual groups on the
sample complexity, the convergence rate, and the average and group-level
testing performance. Although our theoretical framework is centered on binary
classification using a one-hidden-layer neural network, to the best of our
knowledge, we provide the first theoretical analysis of the group-level
generalization of ERM in addition to the commonly studied average
generalization performance. Sample insights of our theoretical results include
that when all group-level co-variance is in the medium regime and all mean are
close to zero, the learning performance is most desirable in the sense of a
small sample complexity, a fast training rate, and a high average and
group-level testing accuracy. Moreover, we show that increasing the fraction of
the minority group in the training data does not necessarily improve the
generalization performance of the minority group. Our theoretical results are
validated on both synthetic and empirical datasets, such as CelebA and CIFAR-10
in image classification.
\\ ( https://arxiv.org/abs/2403.07310 ,  4426kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07314 (*cross-listing*)
Date: Tue, 12 Mar 2024 05:00:38 GMT   (981kb)

Title: Customizable Avatars with Dynamic Facial Action Coded Expressions
  (CADyFACE) for Improved User Engagement
Authors: Megan A. Witherow, Crystal Butler, Winston J. Shields, Furkan Ilgin,
  Norou Diawara, Janice Keener, John W. Harrington, and Khan M. Iftekharuddin
Categories: cs.HC cs.CV cs.LG
Comments: 12 pages, 8 figures
\\
  Customizable 3D avatar-based facial expression stimuli may improve user
engagement in behavioral biomarker discovery and therapeutic intervention for
autism, Alzheimer's disease, facial palsy, and more. However, there is a lack
of customizable avatar-based stimuli with Facial Action Coding System (FACS)
action unit (AU) labels. Therefore, this study focuses on (1) FACS-labeled,
customizable avatar-based expression stimuli for maintaining subjects'
engagement, (2) learning-based measurements that quantify subjects' facial
responses to such stimuli, and (3) validation of constructs represented by
stimulus-measurement pairs. We propose Customizable Avatars with Dynamic Facial
Action Coded Expressions (CADyFACE) labeled with AUs by a certified FACS
expert. To measure subjects' AUs in response to CADyFACE, we propose a novel
Beta-guided Correlation and Multi-task Expression learning neural network
(BeCoME-Net) for multi-label AU detection. The beta-guided correlation loss
encourages feature correlation with AUs while discouraging correlation with
subject identities for improved generalization. We train BeCoME-Net for
unilateral and bilateral AU detection and compare with state-of-the-art
approaches. To assess construct validity of CADyFACE and BeCoME-Net, twenty
healthy adult volunteers complete expression recognition and mimicry tasks in
an online feasibility study while webcam-based eye-tracking and video are
collected. We test validity of multiple constructs, including face preference
during recognition and AUs during mimicry.
\\ ( https://arxiv.org/abs/2403.07314 ,  981kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07320 (*cross-listing*)
Date: Tue, 12 Mar 2024 05:09:25 GMT   (45315kb,D)

Title: Approaching Rate-Distortion Limits in Neural Compression with Lattice
  Transform Coding
Authors: Eric Lei, Hamed Hassani, Shirin Saeedi Bidokhti
Categories: cs.IT cs.LG eess.SP math.IT
\\
  Neural compression has brought tremendous progress in designing lossy
compressors with good rate-distortion (RD) performance at low complexity. Thus
far, neural compression design involves transforming the source to a latent
vector, which is then rounded to integers and entropy coded. While this
approach has been shown to be optimal in a one-shot sense on certain sources,
we show that it is highly sub-optimal on i.i.d. sequences, and in fact always
recovers scalar quantization of the original source sequence. We demonstrate
that the sub-optimality is due to the choice of quantization scheme in the
latent space, and not the transform design. By employing lattice quantization
instead of scalar quantization in the latent space, we demonstrate that Lattice
Transform Coding (LTC) is able to recover optimal vector quantization at
various dimensions and approach the asymptotically-achievable rate-distortion
function at reasonable complexity. On general vector sources, LTC improves upon
standard neural compressors in one-shot coding performance. LTC also enables
neural compressors that perform block coding on i.i.d. vector sources, which
yields coding gain over optimal one-shot coding.
\\ ( https://arxiv.org/abs/2403.07320 ,  45315kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07356 (*cross-listing*)
Date: Tue, 12 Mar 2024 06:29:54 GMT   (1128kb,D)

Title: Premonition: Using Generative Models to Preempt Future Data Changes in
  Continual Learning
Authors: Mark D. McDonnell, Dong Gong, Ehsan Abbasnejad and Anton van den
  Hengel
Categories: cs.CV cs.LG
Comments: 31 pages total (14 main paper, 5 references, 12 appendices)
\\
  Continual learning requires a model to adapt to ongoing changes in the data
distribution, and often to the set of tasks to be performed. It is rare,
however, that the data and task changes are completely unpredictable. Given a
description of an overarching goal or data theme, which we call a realm, humans
can often guess what concepts are associated with it. We show here that the
combination of a large language model and an image generation model can
similarly provide useful premonitions as to how a continual learning challenge
might develop over time. We use the large language model to generate text
descriptions of semantically related classes that might potentially appear in
the data stream in future. These descriptions are then rendered using Stable
Diffusion to generate new labelled image samples. The resulting synthetic
dataset is employed for supervised pre-training, but is discarded prior to
commencing continual learning, along with the pre-training classification head.
We find that the backbone of our pre-trained networks can learn representations
useful for the downstream continual learning problem, thus becoming a valuable
input to any existing continual learning method. Although there are
complexities arising from the domain gap between real and synthetic images, we
show that pre-training models in this manner improves multiple Class Incremenal
Learning (CIL) methods on fine-grained image classification benchmarks.
Supporting code can be found at https://github.com/cl-premonition/premonition.
\\ ( https://arxiv.org/abs/2403.07356 ,  1128kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07366 (*cross-listing*)
Date: Tue, 12 Mar 2024 07:01:57 GMT   (5833kb,D)

Title: Entropy is not Enough for Test-Time Adaptation: From the Perspective of
  Disentangled Factors
Authors: Jonghyun Lee, Dahuin Jung, Saehyung Lee, Junsung Park, Juhyeon Shin,
  Uiwon Hwang, Sungroh Yoon
Categories: cs.CV cs.LG
Comments: ICLR 2024 Spotlight; 26 pages, 9 figures, 20 tables;
\\
  Test-time adaptation (TTA) fine-tunes pre-trained deep neural networks for
unseen test data. The primary challenge of TTA is limited access to the entire
test dataset during online updates, causing error accumulation. To mitigate it,
TTA methods have utilized the model output's entropy as a confidence metric
that aims to determine which samples have a lower likelihood of causing error.
Through experimental studies, however, we observed the unreliability of entropy
as a confidence metric for TTA under biased scenarios and theoretically
revealed that it stems from the neglect of the influence of latent disentangled
factors of data on predictions. Building upon these findings, we introduce a
novel TTA method named Destroy Your Object (DeYO), which leverages a newly
proposed confidence metric named Pseudo-Label Probability Difference (PLPD).
PLPD quantifies the influence of the shape of an object on prediction by
measuring the difference between predictions before and after applying an
object-destructive transformation. DeYO consists of sample selection and sample
weighting, which employ entropy and PLPD concurrently. For robust adaptation,
DeYO prioritizes samples that dominantly incorporate shape information when
making predictions. Our extensive experiments demonstrate the consistent
superiority of DeYO over baseline methods across various scenarios, including
biased and wild. Project page is publicly available at
https://whitesnowdrop.github.io/DeYO/.
\\ ( https://arxiv.org/abs/2403.07366 ,  5833kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07431 (*cross-listing*)
Date: Tue, 12 Mar 2024 09:15:12 GMT   (2236kb,D)

Title: Knowledge Transfer across Multiple Principal Component Analysis Studies
Authors: Zeyu Li and Kangxiang Qin and Yong He and Wang Zhou and Xinsheng Zhang
Categories: stat.ML cs.LG
\\
  Transfer learning has aroused great interest in the statistical community. In
this article, we focus on knowledge transfer for unsupervised learning tasks in
contrast to the supervised learning tasks in the literature. Given the
transferable source populations, we propose a two-step transfer learning
algorithm to extract useful information from multiple source principal
component analysis (PCA) studies, thereby enhancing estimation accuracy for the
target PCA task. In the first step, we integrate the shared subspace
information across multiple studies by a proposed method named as Grassmannian
barycenter, instead of directly performing PCA on the pooled dataset. The
proposed Grassmannian barycenter method enjoys robustness and computational
advantages in more general cases. Then the resulting estimator for the shared
subspace from the first step is further utilized to estimate the target private
subspace in the second step. Our theoretical analysis credits the gain of
knowledge transfer between PCA studies to the enlarged eigenvalue gap, which is
different from the existing supervised transfer learning tasks where sparsity
plays the central role. In addition, we prove that the bilinear forms of the
empirical spectral projectors have asymptotic normality under weaker eigenvalue
gap conditions after knowledge transfer. When the set of informativesources is
unknown, we endow our algorithm with the capability of useful dataset selection
by solving a rectified optimization problem on the Grassmann manifold, which in
turn leads to a computationally friendly rectified Grassmannian K-means
procedure. In the end, extensive numerical simulation results and a real data
case concerning activity recognition are reported to support our theoretical
claims and to illustrate the empirical usefulness of the proposed transfer
learning methods.
\\ ( https://arxiv.org/abs/2403.07431 ,  2236kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07447 (*cross-listing*)
Date: Tue, 12 Mar 2024 09:37:22 GMT   (3622kb,D)

Title: Ab-initio variational wave functions for the time-dependent
  many-electron Schr\"odinger equation
Authors: Jannes Nys, Gabriel Pescia, Giuseppe Carleo
Categories: cond-mat.str-el cs.LG physics.chem-ph physics.comp-ph quant-ph
\\
  Describing the dynamics of many-electron quantum systems is crucial for
applications such as predicting electronic structures in quantum chemistry, the
properties of condensed matter systems, and the behaviors of complex materials.
However, the real-time evolution of non-equilibrium quantum electronic systems
poses a significant challenge for theoretical and computational approaches, due
to the system's exploration of a vast configuration space. This work introduces
a variational approach for fermionic time-dependent wave functions, surpassing
mean-field approximations by capturing many-body correlations. The proposed
methodology involves parameterizing the time-evolving quantum state, enabling
the approximation of the state's evolution. To account for electron
correlations, we employ time-dependent Jastrow factors and backflow
transformations. We also show that we can incorporate neural networks to
parameterize these functions. The time-dependent variational Monte Carlo
technique is employed to efficiently compute the optimal time-dependent
parameters. The approach is demonstrated in three distinct systems: the
solvable harmonic interaction model, the dynamics of a diatomic molecule in
intense laser fields, and a quenched quantum dot. In all cases, we show clear
signatures of many-body correlations in the dynamics not captured by mean-field
methods. The results showcase the ability of our variational approach to
accurately capture the time evolution of quantum states, providing insight into
the quantum dynamics of interacting electronic systems, beyond the capabilities
of mean-field.
\\ ( https://arxiv.org/abs/2403.07447 ,  3622kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07454 (*cross-listing*)
Date: Tue, 12 Mar 2024 09:48:17 GMT   (35824kb,D)

Title: Fast, accurate and lightweight sequential simulation-based inference
  using Gaussian locally linear mappings
Authors: Henrik H\"aggstr\"om, Pedro L. C. Rodrigues, Geoffroy Oudoumanessah,
  Florence Forbes, Umberto Picchini
Categories: stat.ML cs.LG
Comments: 58 pages, 55 figures
\\
  Bayesian inference for complex models with an intractable likelihood can be
tackled using algorithms performing many calls to computer simulators. These
approaches are collectively known as "simulation-based inference" (SBI). Recent
SBI methods have made use of neural networks (NN) to provide approximate, yet
expressive constructs for the unavailable likelihood function and the posterior
distribution. However, they do not generally achieve an optimal trade-off
between accuracy and computational demand. In this work, we propose an
alternative that provides both approximations to the likelihood and the
posterior distribution, using structured mixtures of probability distributions.
Our approach produces accurate posterior inference when compared to
state-of-the-art NN-based SBI methods, while exhibiting a much smaller
computational footprint. We illustrate our results on several benchmark models
from the SBI literature.
\\ ( https://arxiv.org/abs/2403.07454 ,  35824kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07465 (*cross-listing*)
Date: Tue, 12 Mar 2024 10:00:06 GMT   (1447kb,D)

Title: One for All and All for One: GNN-based Control-Flow Attestation for
  Embedded Devices
Authors: Marco Chilese, Richard Mitev, Meni Orenbach, Robert Thorburn, Ahmad
  Atamli, Ahmad-Reza Sadeghi
Categories: cs.CR cs.LG
\\
  Control-Flow Attestation (CFA) is a security service that allows an entity
(verifier) to verify the integrity of code execution on a remote computer
system (prover). Existing CFA schemes suffer from impractical assumptions, such
as requiring access to the prover's internal state (e.g., memory or code), the
complete Control-Flow Graph (CFG) of the prover's software, large sets of
measurements, or tailor-made hardware. Moreover, current CFA schemes are
inadequate for attesting embedded systems due to their high computational
overhead and resource usage.
  In this paper, we overcome the limitations of existing CFA schemes for
embedded devices by introducing RAGE, a novel, lightweight CFA approach with
minimal requirements. RAGE can detect Code Reuse Attacks (CRA), including
control- and non-control-data attacks. It efficiently extracts features from
one execution trace and leverages Unsupervised Graph Neural Networks (GNNs) to
identify deviations from benign executions. The core intuition behind RAGE is
to exploit the correspondence between execution trace, execution graph, and
execution embeddings to eliminate the unrealistic requirement of having access
to a complete CFG.
  We evaluate RAGE on embedded benchmarks and demonstrate that (i) it detects
40 real-world attacks on embedded software; (ii) Further, we stress our scheme
with synthetic return-oriented programming (ROP) and data-oriented programming
(DOP) attacks on the real-world embedded software benchmark Embench, achieving
98.03% (ROP) and 91.01% (DOP) F1-Score while maintaining a low False Positive
Rate of 3.19%; (iii) Additionally, we evaluate RAGE on OpenSSL, used by
millions of devices and achieve 97.49% and 84.42% F1-Score for ROP and DOP
attack detection, with an FPR of 5.47%.
\\ ( https://arxiv.org/abs/2403.07465 ,  1447kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07471 (*cross-listing*)
Date: Tue, 12 Mar 2024 10:06:48 GMT   (27kb)

Title: On the nonconvexity of some push-forward constraints and its
  consequences in machine learning
Authors: Lucas de Lara (UT3, IMT), Mathis Deronzier (UT3, IMT), Alberto
  Gonz\'alez-Sanz, Virgile Foy (UT3, IMT)
Categories: stat.ML cs.LG math.PR
\\
  The push-forward operation enables one to redistribute a probability measure
through a deterministic map. It plays a key role in statistics and
optimization: many learning problems (notably from optimal transport,
generative modeling, and algorithmic fairness) include constraints or penalties
framed as push-forward conditions on the model. However, the literature lacks
general theoretical insights on the (non)convexity of such constraints and its
consequences on the associated learning problems. This paper aims at filling
this gap. In a first part, we provide a range of sufficient and necessary
conditions for the (non)convexity of two sets of functions: the maps
transporting one probability measure to another; the maps inducing equal output
distributions across distinct probability measures. This highlights that for
most probability measures, these push-forward constraints are not convex. In a
second time, we show how this result implies critical limitations on the design
of convex optimization problems for learning generative models or group-fair
predictors. This work will hopefully help researchers and practitioners have a
better understanding of the critical impact of push-forward conditions onto
convexity.
\\ ( https://arxiv.org/abs/2403.07471 ,  27kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07478 (*cross-listing*)
Date: Tue, 12 Mar 2024 10:12:59 GMT   (940kb,D)

Title: Towards Graph Foundation Models for Personalization
Authors: Andreas Damianou, Francesco Fabbri, Paul Gigioli, Marco De Nadai,
  Alice Wang, Enrico Palumbo, Mounia Lalmas
Categories: cs.IR cs.LG
\\
  In the realm of personalization, integrating diverse information sources such
as consumption signals and content-based representations is becoming
increasingly critical to build state-of-the-art solutions. In this regard, two
of the biggest trends in research around this subject are Graph Neural Networks
(GNNs) and Foundation Models (FMs). While GNNs emerged as a popular solution in
industry for powering personalization at scale, FMs have only recently caught
attention for their promising performance in personalization tasks like ranking
and retrieval. In this paper, we present a graph-based foundation modeling
approach tailored to personalization. Central to this approach is a
Heterogeneous GNN (HGNN) designed to capture multi-hop content and consumption
relationships across a range of recommendable item types. To ensure the
generality required from a Foundation Model, we employ a Large Language Model
(LLM) text-based featurization of nodes that accommodates all item types, and
construct the graph using co-interaction signals, which inherently transcend
content specificity. To facilitate practical generalization, we further couple
the HGNN with an adaptation mechanism based on a two-tower (2T) architecture,
which also operates agnostically to content type. This multi-stage approach
ensures high scalability; while the HGNN produces general purpose embeddings,
the 2T component models in a continuous space the sheer size of user-item
interaction data. Our comprehensive approach has been rigorously tested and
proven effective in delivering recommendations across a diverse array of
products within a real-world, industrial audio streaming platform.
\\ ( https://arxiv.org/abs/2403.07478 ,  940kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07485 (*cross-listing*)
Date: Tue, 12 Mar 2024 10:21:21 GMT   (1861kb,D)

Title: PMBO: Enhancing Black-Box Optimization through Multivariate Polynomial
  Surrogates
Authors: Janina Schreiber, Pau Batlle, Damar Wicaksono, Michael Hecht
Categories: math.OC cs.LG cs.MS
Comments: 16 pages, 6 figures
\\
  We introduce a surrogate-based black-box optimization method, termed
Polynomial-model-based optimization (PMBO). The algorithm alternates polynomial
approximation with Bayesian optimization steps, using Gaussian processes to
model the error between the objective and its polynomial fit. We describe the
algorithmic design of PMBO and compare the results of the performance of PMBO
with several optimization methods for a set of analytic test functions.
  The results show that PMBO outperforms the classic Bayesian optimization and
is robust with respect to the choice of its correlation function family and its
hyper-parameter setting, which, on the contrary, need to be carefully tuned in
classic Bayesian optimization. Remarkably, PMBO performs comparably with
state-of-the-art evolutionary algorithms such as the Covariance Matrix
Adaptation -- Evolution Strategy (CMA-ES). This finding suggests that PMBO
emerges as the pivotal choice among surrogate-based optimization methods when
addressing low-dimensional optimization problems. Hereby, the simple nature of
polynomials opens the opportunity for interpretation and analysis of the
inferred surrogate model, providing a macroscopic perspective on the landscape
of the objective function.
\\ ( https://arxiv.org/abs/2403.07485 ,  1861kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07493 (*cross-listing*)
Date: Tue, 12 Mar 2024 10:32:35 GMT   (4581kb,D)

Title: Signed graphs in data sciences via communicability geometry
Authors: Fernando Diaz-Diaz and Ernesto Estrada
Categories: math.MG cs.DM cs.LG math.CO physics.soc-ph
\\
  Signed graphs are an emergent way of representing data in a variety of
contexts were conflicting interactions exist. These include data from
biological, ecological, and social systems. Here we propose the concept of
communicability geometry for signed graphs, proving that metrics in this space,
such as the communicability distance and angles, are Euclidean and spherical.
We then apply these metrics to solve several problems in data analysis of
signed graphs in a unified way. They include the partitioning of signed graphs,
dimensionality reduction, finding hierarchies of alliances in signed networks
as well as the quantification of the degree of polarization between the
existing factions in systems represented by this type of graphs.
\\ ( https://arxiv.org/abs/2403.07493 ,  4581kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07507 (*cross-listing*)
Date: Tue, 12 Mar 2024 10:43:52 GMT   (4375kb,D)

Title: Reconstructions of Jupiter's magnetic field using physics informed
  neural networks
Authors: Philip W. Livermore, Leyuan Wu, Longwei Chen, Sjoerd A.L. de Ridder
Categories: astro-ph.EP cs.LG
\\
  Magnetic sounding using data collected from the Juno mission can be used to
provide constraints on Jupiter's interior. However, inwards continuation of
reconstructions assuming zero electrical conductivity and a representation in
spherical harmonics are limited by the enhancement of noise at small scales. In
this paper we describe new reconstructions of Jupiter's internal magnetic field
based on physics-informed neural networks and either the first 33 (PINN33) or
the first 50 (PINN50) of Juno's orbits. The method can resolve local
structures, and allows for weak ambient electrical currents. Compared with
other methods, our reconstructions of Jupiter's magnetic field both on and
above the surface are similar, and we achieve a similar fit to the Juno data.
However, our models are not hampered by noise at depth, and so offer a much
clearer picture of the interior structure. We estimate that the dynamo boundary
is at a fractional radius of 0.8. At this depth, the magnetic field is arranged
into longitudinal bands, and the great blue spot appears to be rooted in
neighbouring structures of oppositely signed flux.
\\ ( https://arxiv.org/abs/2403.07507 ,  4375kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07526 (*cross-listing*)
Date: Tue, 12 Mar 2024 11:05:05 GMT   (20171kb,D)

Title: Physics-Transfer Learning for Material Strength Screening
Authors: Yingjie Zhao and Zian Zhang and Zhiping Xu
Categories: cond-mat.mtrl-sci cs.LG physics.comp-ph
\\
  The strength of materials, like many problems in the natural sciences, spans
multiple length and time scales, and the solution has to balance accuracy and
performance. Peierls stress is one of the central concepts in crystal
plasticity that measures the strength through the resistance of a dislocation
to plastic flow. The determination of Peierls stress involves a multiscale
nature depending on both elastic lattice responses and the energy landscape of
crystal slips. Material screening by strength via the Peierls stress from
first-principles calculations is computationally intractable for the nonlocal
characteristics of dislocations, and not included in the state-of-the-art
computational material databases. In this work, we propose a physics-transfer
framework to learn the physics of crystal plasticity from empirical atomistic
simulations and then predict the Peierls stress from chemically accurate
density functional theory-based calculations of material parameters. Notably,
the strengths of single-crystalline metals can be predicted from a few
single-point calculations for the deformed lattice and on the {\gamma} surface,
allowing efficient, high-throughput screening for material discovery.
Uncertainty quantification is carried out to assess the accuracy of models and
sources of errors, showing reduced physical and system uncertainties in the
predictions by elevating the fidelity of training models. This physics-transfer
framework can be generalized to other problems facing the accuracy-performance
dilemma, by harnessing the hierarchy of physics in the multiscale models of
materials science.
\\ ( https://arxiv.org/abs/2403.07526 ,  20171kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07536 (*cross-listing*)
Date: Tue, 12 Mar 2024 11:19:46 GMT   (10611kb)

Title: LaB-GATr: geometric algebra transformers for large biomedical surface
  and volume meshes
Authors: Julian Suk, Baris Imre, Jelmer M. Wolterink
Categories: cs.CV cs.LG
\\
  Many anatomical structures can be described by surface or volume meshes.
Machine learning is a promising tool to extract information from these 3D
models. However, high-fidelity meshes often contain hundreds of thousands of
vertices, which creates unique challenges in building deep neural network
architectures. Furthermore, patient-specific meshes may not be canonically
aligned which limits the generalisation of machine learning algorithms. We
propose LaB-GATr, a transfomer neural network with geometric tokenisation that
can effectively learn with large-scale (bio-)medical surface and volume meshes
through sequence compression and interpolation. Our method extends the recently
proposed geometric algebra transformer (GATr) and thus respects all Euclidean
symmetries, i.e. rotation, translation and reflection, effectively mitigating
the problem of canonical alignment between patients. LaB-GATr achieves
state-of-the-art results on three tasks in cardiovascular hemodynamics
modelling and neurodevelopmental phenotype prediction, featuring meshes of up
to 200,000 vertices. Our results demonstrate that LaB-GATr is a powerful
architecture for learning with high-fidelity meshes which has the potential to
enable interesting downstream applications. Our implementation is publicly
available.
\\ ( https://arxiv.org/abs/2403.07536 ,  10611kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07542 (*cross-listing*)
Date: Tue, 12 Mar 2024 11:29:40 GMT   (186kb,D)

Title: A Survey of Vision Transformers in Autonomous Driving: Current Trends
  and Future Directions
Authors: Quoc-Vinh Lai-Dang
Categories: cs.CV cs.LG
Comments: 9 pages, 3 figures
\\
  This survey explores the adaptation of visual transformer models in
Autonomous Driving, a transition inspired by their success in Natural Language
Processing. Surpassing traditional Recurrent Neural Networks in tasks like
sequential image processing and outperforming Convolutional Neural Networks in
global context capture, as evidenced in complex scene recognition, Transformers
are gaining traction in computer vision. These capabilities are crucial in
Autonomous Driving for real-time, dynamic visual scene processing. Our survey
provides a comprehensive overview of Vision Transformer applications in
Autonomous Driving, focusing on foundational concepts such as self-attention,
multi-head attention, and encoder-decoder architecture. We cover applications
in object detection, segmentation, pedestrian detection, lane detection, and
more, comparing their architectural merits and limitations. The survey
concludes with future research directions, highlighting the growing role of
Vision Transformers in Autonomous Driving.
\\ ( https://arxiv.org/abs/2403.07542 ,  186kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07562 (*cross-listing*)
Date: Tue, 12 Mar 2024 11:50:47 GMT   (361kb,D)

Title: A Flexible Cell Classification for ML Projects in Jupyter Notebooks
Authors: Miguel Perez and Selin Aydin and Horst Lichter
Categories: cs.SE cs.LG
Comments: 9 pages, 3 figures
\\
  Jupyter Notebook is an interactive development environment commonly used for
rapid experimentation of machine learning (ML) solutions. Describing the ML
activities performed along code cells improves the readability and
understanding of Notebooks. Manual annotation of code cells is time-consuming
and error-prone. Therefore, tools have been developed that classify the cells
of a notebook concerning the ML activity performed in them. However, the
current tools are not flexible, as they work based on look-up tables that have
been created, which map function calls of commonly used ML libraries to ML
activities. These tables must be manually adjusted to account for new or
changed libraries.
  This paper presents a more flexible approach to cell classification based on
a hybrid classification approach that combines a rule-based and a decision tree
classifier. We discuss the design rationales and describe the developed
classifiers in detail. We implemented the new flexible cell classification
approach in a tool called JupyLabel. Its evaluation and the obtained metric
scores regarding precision, recall, and F1-score are discussed. Additionally,
we compared JupyLabel with HeaderGen, an existing cell classification tool. We
were able to show that the presented flexible cell classification approach
outperforms this tool significantly.
\\ ( https://arxiv.org/abs/2403.07562 ,  361kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07563 (*cross-listing*)
Date: Tue, 12 Mar 2024 11:51:55 GMT   (11715kb,D)

Title: Learning Generalizable Feature Fields for Mobile Manipulation
Authors: Ri-Zhao Qiu, Yafei Hu, Ge Yang, Yuchen Song, Yang Fu, Jianglong Ye,
  Jiteng Mu, Ruihan Yang, Nikolay Atanasov, Sebastian Scherer, Xiaolong Wang
Categories: cs.RO cs.CV cs.LG
Comments: Preprint. Project website is at: https://geff-b1.github.io/
\\
  An open problem in mobile manipulation is how to represent objects and scenes
in a unified manner, so that robots can use it both for navigating in the
environment and manipulating objects. The latter requires capturing intricate
geometry while understanding fine-grained semantics, whereas the former
involves capturing the complexity inherit to an expansive physical scale. In
this work, we present GeFF (Generalizable Feature Fields), a scene-level
generalizable neural feature field that acts as a unified representation for
both navigation and manipulation that performs in real-time. To do so, we treat
generative novel view synthesis as a pre-training task, and then align the
resulting rich scene priors with natural language via CLIP feature
distillation. We demonstrate the effectiveness of this approach by deploying
GeFF on a quadrupedal robot equipped with a manipulator. We evaluate GeFF's
ability to generalize to open-set objects as well as running time, when
performing open-vocabulary mobile manipulation in dynamic scenes.
\\ ( https://arxiv.org/abs/2403.07563 ,  11715kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07569 (*cross-listing*)
Date: Tue, 12 Mar 2024 11:56:50 GMT   (5042kb,D)

Title: Exploring Challenges in Deep Learning of Single-Station Ground Motion
  Records
Authors: \"Umit Mert \c{C}a\u{g}lar, Baris Yilmaz, Melek T\"urkmen, Erdem
  Akag\"und\"uz, Salih Tileylioglu
Categories: eess.SP cs.CV cs.LG
Comments: 9 Pages, 12 Figures, 5 Tables
\\
  Contemporary deep learning models have demonstrated promising results across
various applications within seismology and earthquake engineering. These models
rely primarily on utilizing ground motion records for tasks such as earthquake
event classification, localization, earthquake early warning systems, and
structural health monitoring. However, the extent to which these models
effectively learn from these complex time-series signals has not been
thoroughly analyzed. In this study, our objective is to evaluate the degree to
which auxiliary information, such as seismic phase arrival times or seismic
station distribution within a network, dominates the process of deep learning
from ground motion records, potentially hindering its effectiveness. We perform
a hyperparameter search on two deep learning models to assess their
effectiveness in deep learning from ground motion records while also examining
the impact of auxiliary information on model performance. Experimental results
reveal a strong reliance on the highly correlated P and S phase arrival
information. Our observations highlight a potential gap in the field,
indicating an absence of robust methodologies for deep learning of
single-station ground motion recordings independent of any auxiliary
information.
\\ ( https://arxiv.org/abs/2403.07569 ,  5042kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07585 (*cross-listing*)
Date: Tue, 12 Mar 2024 12:15:57 GMT   (654kb,D)

Title: Communication Optimization for Distributed Training: Architecture,
  Advances, and Opportunities
Authors: Yunze Wei, Tianshuo Hu, Cong Liang, Yong Cui
Categories: cs.DC cs.LG
\\
  The past few years have witnessed the flourishing of large-scale deep neural
network models with ever-growing parameter numbers. Training such large-scale
models typically requires massive memory and computing resources that exceed
those of a single GPU, necessitating distributed training. As GPU performance
has rapidly evolved in recent years, computation time has shrunk, thereby
increasing the proportion of communication in the overall training time.
Therefore, optimizing communication for distributed training has become an
urgent issue. In this article, we briefly introduce the general architecture of
distributed deep neural network training and analyze relationships among
Parallelization Strategy, Collective Communication Library, and Network from
the perspective of communication optimization, which forms a three-layer
paradigm. We then review current representative research advances with this
three-layer paradigm. We find that layers in the current three-layer paradigm
are relatively independent, but there is a rich design space for cross-layer
collaborative optimization in distributed training scenarios. Therefore, we
further advocate a communication-efficient five-layer paradigm underlining
opportunities for collaboration designs and look forward to the perspectives of
"Vertical", "Horizontal", "Intra-Inter" and "Host-Net" collaboration designs.
We hope this article can shed some light on future research on communication
optimization for distributed training.
\\ ( https://arxiv.org/abs/2403.07585 ,  654kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07627 (*cross-listing*)
Date: Tue, 12 Mar 2024 13:09:15 GMT   (4834kb,D)

Title: generAItor: Tree-in-the-Loop Text Generation for Language Model
  Explainability and Adaptation
Authors: Thilo Spinner, Rebecca Kehlbeck, Rita Sevastjanova, Tobias St\"ahle,
  Daniel A. Keim, Oliver Deussen, Mennatallah El-Assady
Categories: cs.HC cs.LG
Comments: 24 pages paper, 4 pages references, 3 pages appendix, 8 figures
ACM-class: I.2.7; H.5.2
DOI: 10.1145/3652028
\\
  Large language models (LLMs) are widely deployed in various downstream tasks,
e.g., auto-completion, aided writing, or chat-based text generation. However,
the considered output candidates of the underlying search algorithm are
under-explored and under-explained. We tackle this shortcoming by proposing a
tree-in-the-loop approach, where a visual representation of the beam search
tree is the central component for analyzing, explaining, and adapting the
generated outputs. To support these tasks, we present generAItor, a visual
analytics technique, augmenting the central beam search tree with various
task-specific widgets, providing targeted visualizations and interaction
possibilities. Our approach allows interactions on multiple levels and offers
an iterative pipeline that encompasses generating, exploring, and comparing
output candidates, as well as fine-tuning the model based on adapted data. Our
case study shows that our tool generates new insights in gender bias analysis
beyond state-of-the-art template-based methods. Additionally, we demonstrate
the applicability of our approach in a qualitative user study. Finally, we
quantitatively evaluate the adaptability of the model to few samples, as
occurring in text-generation use cases.
\\ ( https://arxiv.org/abs/2403.07627 ,  4834kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07648 (*cross-listing*)
Date: Tue, 12 Mar 2024 13:31:14 GMT   (2710kb,D)

Title: Characterization of Large Language Model Development in the Datacenter
Authors: Qinghao Hu, Zhisheng Ye, Zerui Wang, Guoteng Wang, Meng Zhang,
  Qiaoling Chen, Peng Sun, Dahua Lin, Xiaolin Wang, Yingwei Luo, Yonggang Wen,
  Tianwei Zhang
Categories: cs.DC cs.LG
\\
  Large Language Models (LLMs) have presented impressive performance across
several transformative tasks. However, it is non-trivial to efficiently utilize
large-scale cluster resources to develop LLMs, often riddled with numerous
challenges such as frequent hardware failures, intricate parallelization
strategies, and imbalanced resource utilization. In this paper, we present an
in-depth characterization study of a six-month LLM development workload trace
collected from our GPU datacenter Acme. Specifically, we investigate
discrepancies between LLMs and prior task-specific Deep Learning (DL)
workloads, explore resource utilization patterns, and identify the impact of
various job failures. Our analysis summarizes hurdles we encountered and
uncovers potential opportunities to optimize systems tailored for LLMs.
Furthermore, we introduce our system efforts: (1) fault-tolerant pretraining,
which enhances fault tolerance through LLM-involved failure diagnosis and
automatic recovery. (2) decoupled scheduling for evaluation, which achieves
timely performance feedback via trial decomposition and scheduling
optimization.
\\ ( https://arxiv.org/abs/2403.07648 ,  2710kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07706 (*cross-listing*)
Date: Tue, 12 Mar 2024 14:51:23 GMT   (5390kb,D)

Title: Fast and Simple Explainability for Point Cloud Networks
Authors: Meir Yossef Levi and Guy Gilboa
Categories: cs.CV cs.LG
\\
  We propose a fast and simple explainable AI (XAI) method for point cloud
data. It computes pointwise importance with respect to a trained network
downstream task. This allows better understanding of the network properties,
which is imperative for safety-critical applications. In addition to debugging
and visualization, our low computational complexity facilitates online feedback
to the network at inference. This can be used to reduce uncertainty and to
increase robustness. In this work, we introduce \emph{Feature Based
Interpretability} (FBI), where we compute the features' norm, per point, before
the bottleneck. We analyze the use of gradients and post- and pre-bottleneck
strategies, showing pre-bottleneck is preferred, in terms of smoothness and
ranking. We obtain at least three orders of magnitude speedup, compared to
current XAI methods, thus, scalable for big point clouds or large-scale
architectures. Our approach achieves SOTA results, in terms of classification
explainability. We demonstrate how the proposed measure is helpful in analyzing
and characterizing various aspects of 3D learning, such as rotation invariance,
robustness to out-of-distribution (OOD) outliers or domain shift and dataset
bias.
\\ ( https://arxiv.org/abs/2403.07706 ,  5390kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07728 (*cross-listing*)
Date: Tue, 12 Mar 2024 15:07:20 GMT   (208kb,D)

Title: CAS: A General Algorithm for Online Selective Conformal Prediction with
  FCR Control
Authors: Yajie Bao, Yuyang Huo, Haojie Ren, Changliang Zou
Categories: stat.ML cs.LG stat.ME
\\
  We study the problem of post-selection predictive inference in an online
fashion. To avoid devoting resources to unimportant units, a preliminary
selection of the current individual before reporting its prediction interval is
common and meaningful in online predictive tasks. Since the online selection
causes a temporal multiplicity in the selected prediction intervals, it is
important to control the real-time false coverage-statement rate (FCR) to
measure the averaged miscoverage error. We develop a general framework named
CAS (Calibration after Adaptive Selection) that can wrap around any prediction
model and online selection rule to output post-selection prediction intervals.
If the current individual is selected, we first perform an adaptive selection
on historical data to construct a calibration set, then output a conformal
prediction interval for the unobserved label. We provide tractable
constructions for the calibration set for popular online selection rules. We
proved that CAS can achieve an exact selection-conditional coverage guarantee
in the finite-sample and distribution-free regimes. For the decision-driven
selection rule, including most online multiple-testing procedures, CAS can
exactly control the real-time FCR below the target level without any
distributional assumptions. For the online selection with symmetric thresholds,
we establish the error bound for the control gap of FCR under mild
distributional assumptions. To account for the distribution shift in online
data, we also embed CAS into some recent dynamic conformal prediction methods
and examine the long-run FCR control. Numerical results on both synthetic and
real data corroborate that CAS can effectively control FCR around the target
level and yield more narrowed prediction intervals over existing baselines
across various settings.
\\ ( https://arxiv.org/abs/2403.07728 ,  208kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07735 (*cross-listing*)
Date: Tue, 12 Mar 2024 15:13:21 GMT   (24kb)

Title: The Minimax Rate of HSIC Estimation for Translation-Invariant Kernels
Authors: Florian Kalinke and Zoltan Szabo
Categories: math.ST cs.IT cs.LG math.IT stat.ML stat.TH
MSC-class: 62C20, 46E22, 47B32, 94A15, 62G10
ACM-class: G.3; H.1.1; I.2.6
\\
  Kernel techniques are among the most influential approaches in data science
and statistics. Under mild conditions, the reproducing kernel Hilbert space
associated to a kernel is capable of encoding the independence of $M\ge 2$
random variables. Probably the most widespread independence measure relying on
kernels is the so-called Hilbert-Schmidt independence criterion (HSIC; also
referred to as distance covariance in the statistics literature). Despite
various existing HSIC estimators designed since its introduction close to two
decades ago, the fundamental question of the rate at which HSIC can be
estimated is still open. In this work, we prove that the minimax optimal rate
of HSIC estimation on $\mathbb R^d$ for Borel measures containing the Gaussians
with continuous bounded translation-invariant characteristic kernels is
$\mathcal O\!\left(n^{-1/2}\right)$. Specifically, our result implies the
optimality in the minimax sense of many of the most-frequently used estimators
(including the U-statistic, the V-statistic, and the Nystr\"om-based one) on
$\mathbb R^d$.
\\ ( https://arxiv.org/abs/2403.07735 ,  24kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07767 (*cross-listing*)
Date: Tue, 12 Mar 2024 15:54:32 GMT   (77kb)

Title: Beyond the Labels: Unveiling Text-Dependency in Paralinguistic Speech
  Recognition Datasets
Authors: Jan Pe\v{s}\'an, Santosh Kesiraju, Luk\'a\v{s} Burget and Jan
  ''Honza'' \v{C}ernock\'y
Categories: eess.AS cs.LG eess.SP
\\
  Paralinguistic traits like cognitive load and emotion are increasingly
recognized as pivotal areas in speech recognition research, often examined
through specialized datasets like CLSE and IEMOCAP. However, the integrity of
these datasets is seldom scrutinized for text-dependency. This paper critically
evaluates the prevalent assumption that machine learning models trained on such
datasets genuinely learn to identify paralinguistic traits, rather than merely
capturing lexical features. By examining the lexical overlap in these datasets
and testing the performance of machine learning models, we expose significant
text-dependency in trait-labeling. Our results suggest that some machine
learning models, especially large pre-trained models like HuBERT, might
inadvertently focus on lexical characteristics rather than the intended
paralinguistic features. The study serves as a call to action for the research
community to reevaluate the reliability of existing datasets and methodologies,
ensuring that machine learning models genuinely learn what they are designed to
recognize.
\\ ( https://arxiv.org/abs/2403.07767 ,  77kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07780 (*cross-listing*)
Date: Tue, 12 Mar 2024 16:08:47 GMT   (714kb,D)

Title: FairRR: Pre-Processing for Group Fairness through Randomized Response
Authors: Xianli Zeng, Joshua Ward, Guang Cheng
Categories: stat.ML cs.LG
\\
  The increasing usage of machine learning models in consequential
decision-making processes has spurred research into the fairness of these
systems. While significant work has been done to study group fairness in the
in-processing and post-processing setting, there has been little that
theoretically connects these results to the pre-processing domain. This paper
proposes that achieving group fairness in downstream models can be formulated
as finding the optimal design matrix in which to modify a response variable in
a Randomized Response framework. We show that measures of group fairness can be
directly controlled for with optimal model utility, proposing a pre-processing
algorithm called FairRR that yields excellent downstream model utility and
fairness.
\\ ( https://arxiv.org/abs/2403.07780 ,  714kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07802 (*cross-listing*)
Date: Tue, 12 Mar 2024 16:41:31 GMT   (206kb,D)

Title: Boosting keyword spotting through on-device learnable user speech
  characteristics
Authors: Cristian Cioflan, Lukas Cavigelli, Luca Benini
Categories: cs.SD cs.LG eess.AS
Comments: 5 pages, 3 tables, 2 figures. Accepted as a full paper by the tinyML
  Research Symposium 2024
\\
  Keyword spotting systems for always-on TinyML-constrained applications
require on-site tuning to boost the accuracy of offline trained classifiers
when deployed in unseen inference conditions. Adapting to the speech
peculiarities of target users requires many in-domain samples, often
unavailable in real-world scenarios. Furthermore, current on-device learning
techniques rely on computationally intensive and memory-hungry backbone update
schemes, unfit for always-on, battery-powered devices. In this work, we propose
a novel on-device learning architecture, composed of a pretrained backbone and
a user-aware embedding learning the user's speech characteristics. The
so-generated features are fused and used to classify the input utterance. For
domain shifts generated by unseen speakers, we measure error rate reductions of
up to 19% from 30.1% to 24.3% based on the 35-class problem of the Google
Speech Commands dataset, through the inexpensive update of the user
projections. We moreover demonstrate the few-shot learning capabilities of our
proposed architecture in sample- and class-scarce learning conditions. With
23.7 kparameters and 1 MFLOP per epoch required for on-device training, our
system is feasible for TinyML applications aimed at battery-powered
microcontrollers.
\\ ( https://arxiv.org/abs/2403.07802 ,  206kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07822 (*cross-listing*)
Date: Tue, 12 Mar 2024 17:03:07 GMT   (8217kb,D)

Title: Fusing Climate Data Products using a Spatially Varying Autoencoder
Authors: Jacob A. Johnson, Matthew J. Heaton, William F. Christensen, Lynsie R.
  Warr, and Summer B. Rupper
Categories: stat.AP cs.LG
Comments: 13 pages, 7 figures
\\
  Autoencoders are powerful machine learning models used to compress
information from multiple data sources. However, autoencoders, like all
artificial neural networks, are often unidentifiable and uninterpretable. This
research focuses on creating an identifiable and interpretable autoencoder that
can be used to meld and combine climate data products. The proposed autoencoder
utilizes a Bayesian statistical framework, allowing for probabilistic
interpretations while also varying spatially to capture useful spatial patterns
across the various data products. Constraints are placed on the autoencoder as
it learns patterns in the data, creating an interpretable consensus that
includes the important features from each input. We demonstrate the utility of
the autoencoder by combining information from multiple precipitation products
in High Mountain Asia.
\\ ( https://arxiv.org/abs/2403.07822 ,  8217kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07854 (*cross-listing*)
Date: Tue, 12 Mar 2024 17:44:45 GMT   (2030kb,D)

Title: Distilling the Knowledge in Data Pruning
Authors: Emanuel Ben-Baruch, Adam Botach, Igor Kviatkovsky, Manoj Aggarwal,
  G\'erard Medioni
Categories: cs.CV cs.LG
\\
  With the increasing size of datasets used for training neural networks, data
pruning becomes an attractive field of research. However, most current data
pruning algorithms are limited in their ability to preserve accuracy compared
to models trained on the full data, especially in high pruning regimes. In this
paper we explore the application of data pruning while incorporating knowledge
distillation (KD) when training on a pruned subset. That is, rather than
relying solely on ground-truth labels, we also use the soft predictions from a
teacher network pre-trained on the complete data. By integrating KD into
training, we demonstrate significant improvement across datasets, pruning
methods, and on all pruning fractions. We first establish a theoretical
motivation for employing self-distillation to improve training on pruned data.
Then, we empirically make a compelling and highly practical observation: using
KD, simple random pruning is comparable or superior to sophisticated pruning
methods across all pruning regimes. On ImageNet for example, we achieve
superior accuracy despite training on a random subset of only 50% of the data.
Additionally, we demonstrate a crucial connection between the pruning factor
and the optimal knowledge distillation weight. This helps mitigate the impact
of samples with noisy labels and low-quality images retained by typical pruning
algorithms. Finally, we make an intriguing observation: when using lower
pruning fractions, larger teachers lead to accuracy degradation, while
surprisingly, employing teachers with a smaller capacity than the student's may
improve results. Our code will be made available.
\\ ( https://arxiv.org/abs/2403.07854 ,  2030kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2209.00953
replaced with revised version Tue, 12 Mar 2024 02:43:40 GMT   (629kb,D)

Title: SATformer: Transformer-Based UNSAT Core Learning
Authors: Zhengyuan Shi (1), Min Li (1), Yi Liu (1), Sadaf Khan (1), Junhua
  Huang (2), Hui-Ling Zhen (2), Mingxuan Yuan (2), Qiang Xu (1) ((1) The
  Chinese University of Hong Kong, (2) Huawei Noah's Ark Lab)
Categories: cs.AI cs.LG cs.LO
Journal-ref: In 2023 IEEE/ACM International Conference on Computer Aided Design
  (ICCAD) 2023 Oct 28 (pp. 1-4). IEEE
DOI: 10.1109/ICCAD57390.2023.10323731
\\ ( https://arxiv.org/abs/2209.00953 ,  629kb)
------------------------------------------------------------------------------
\\
arXiv:2302.09582
replaced with revised version Tue, 12 Mar 2024 14:55:29 GMT   (6341kb)

Title: Language-Specific Representation of Emotion-Concept Knowledge Causally
  Supports Emotion Inference
Authors: Ming Li, Yusheng Su, Hsiu-Yuan Huang, Jiali Cheng, Xin Hu, Xinmiao
  Zhang, Huadong Wang, Yujia Qin, Xiaozhi Wang, Kristen A. Lindquist, Zhiyuan
  Liu, Dan Zhang
Categories: cs.AI cs.CL
Comments: 44 pages, 14 figures, 2 tables
\\ ( https://arxiv.org/abs/2302.09582 ,  6341kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11432
replaced with revised version Tue, 12 Mar 2024 09:51:35 GMT   (4914kb,D)

Title: A Survey on Large Language Model based Autonomous Agents
Authors: Lei Wang and Chen Ma and Xueyang Feng and Zeyu Zhang and Hao Yang and
  Jingsen Zhang and Zhiyuan Chen and Jiakai Tang and Xu Chen and Yankai Lin and
  Wayne Xin Zhao and Zhewei Wei and Ji-Rong Wen
Categories: cs.AI cs.CL
Comments: 35 pages, 5 figures, 3 tables, has been accepted by frontiers of
  computer science (FCS), doi={10.1007/s11704-024-40231-1}
\\ ( https://arxiv.org/abs/2308.11432 ,  4914kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14363
replaced with revised version Tue, 12 Mar 2024 02:17:03 GMT   (9017kb,D)

Title: Mobile Foundation Model as Firmware
Authors: Jinliang Yuan, Chen Yang, Dongqi Cai, Shihe Wang, Xin Yuan, Zeling
  Zhang, Xiang Li, Dingge Zhang, Hanzi Mei, Xianqing Jia, Shangguang Wang,
  Mengwei Xu
Categories: cs.AI
Comments: 17 pages, 15 figures, published to ACM MobiCom'24
Journal-ref: The 30th Annual International Conference on Mobile Computing and
  Networking, 2024
DOI: 10.1145/3636534.3649361
\\ ( https://arxiv.org/abs/2308.14363 ,  9017kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02099
replaced with revised version Tue, 12 Mar 2024 16:13:59 GMT   (5216kb,D)

Title: A Safe Preference Learning Approach for Personalization with
  Applications to Autonomous Vehicles
Authors: Ruya Karagulle and Nikos Arechiga and Andrew Best and Jonathan
  DeCastro and Necmiye Ozay
Categories: cs.AI cs.SY eess.SY
Comments: 9 pages, 3 figures, 2 tables. This work has been submitted to the
  IEEE for possible publication. Copyright may be transferred without notice,
  after which this version may no longer be accessible
\\ ( https://arxiv.org/abs/2311.02099 ,  5216kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12403
replaced with revised version Tue, 12 Mar 2024 10:09:50 GMT   (40kb,D)

Title: On Alternating-Time Temporal Logic, Hyperproperties, and Strategy
  Sharing
Authors: Raven Beutner, Bernd Finkbeiner
Categories: cs.AI cs.LO cs.MA
Comments: AAAI 2024
\\ ( https://arxiv.org/abs/2312.12403 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10568
replaced with revised version Tue, 12 Mar 2024 08:24:37 GMT   (10017kb,D)

Title: CivRealm: A Learning and Reasoning Odyssey in Civilization for
  Decision-Making Agents
Authors: Siyuan Qi, Shuo Chen, Yexin Li, Xiangyu Kong, Junqi Wang, Bangcheng
  Yang, Pring Wong, Yifan Zhong, Xiaoyuan Zhang, Zhaowei Zhang, Nian Liu, Wei
  Wang, Yaodong Yang, Song-Chun Zhu
Categories: cs.AI
\\ ( https://arxiv.org/abs/2401.10568 ,  10017kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05359
replaced with revised version Mon, 11 Mar 2024 23:15:10 GMT   (585kb,D)

Title: Guiding Large Language Models with Divide-and-Conquer Program for
  Discerning Problem Solving
Authors: Yizhou Zhang, Lun Du, Defu Cao, Qiang Fu, Yan Liu
Categories: cs.AI cs.CL cs.LG
Comments: Preprint
\\ ( https://arxiv.org/abs/2402.05359 ,  585kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10011
replaced with revised version Tue, 12 Mar 2024 12:38:09 GMT   (164kb,D)

Title: Clifford Group Equivariant Simplicial Message Passing Networks
Authors: Cong Liu, David Ruhe, Floor Eijkelboom, Patrick Forr\'e
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.10011 ,  164kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18679
replaced with revised version Tue, 12 Mar 2024 17:26:53 GMT   (12998kb,D)

Title: Data Interpreter: An LLM Agent For Data Science
Authors: Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Danyang
  Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Li Zhang, Lingyao Zhang, Min Yang,
  Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang,
  Xiangtao Lu, Xiawu Zheng, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zongze Xu,
  Chenglin Wu
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.18679 ,  12998kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02760
replaced with revised version Tue, 12 Mar 2024 11:29:07 GMT   (397kb)

Title: Emerging Synergies Between Large Language Models and Machine Learning in
  Ecommerce Recommendations
Authors: Xiaonan Xu, Yichao Wu, Penghao Liang, Yuhang He, Han Wang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2403.02760 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04964
replaced with revised version Mon, 11 Mar 2024 18:41:29 GMT   (290kb)

Title: Tell me the truth: A system to measure the trustworthiness of Large
  Language Models
Authors: Carlo Lipizzi
Categories: cs.AI cs.CL cs.CY
\\ ( https://arxiv.org/abs/2403.04964 ,  290kb)
------------------------------------------------------------------------------
\\
arXiv:2211.04118
replaced with revised version Tue, 12 Mar 2024 08:29:41 GMT   (1052kb,D)

Title: ConsPrompt: Exploiting Contrastive Samples for Fewshot Prompt Learning
Authors: Jinta Weng and Yifan Deng and d Donghao Li and Hao You and Yue Hu and
  Heyan Huang
Categories: cs.CL cs.AI
Comments: 2 figures
Journal-ref: ICASSP2024
\\ ( https://arxiv.org/abs/2211.04118 ,  1052kb)
------------------------------------------------------------------------------
\\
arXiv:2212.07249
replaced with revised version Tue, 12 Mar 2024 13:30:16 GMT   (9859kb,D)

Title: APOLLO: An Optimized Training Approach for Long-form Numerical Reasoning
Authors: Jiashuo Sun, Hang Zhang, Chen Lin, Xiangdong Su, Yeyun Gong, Jian Guo
Categories: cs.CL cs.LG
Comments: Accepted by COLING 2024
\\ ( https://arxiv.org/abs/2212.07249 ,  9859kb)
------------------------------------------------------------------------------
\\
arXiv:2303.16421
replaced with revised version Tue, 12 Mar 2024 03:14:18 GMT   (210kb,D)

Title: ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of
  Commonsense Problem in Large Language Models
Authors: Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie Lu, Ben He,
  Shanshan Jiang, Bin Dong
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\ ( https://arxiv.org/abs/2303.16421 ,  210kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13040
replaced with revised version Tue, 12 Mar 2024 08:52:02 GMT   (8341kb,D)

Title: SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented
  Dialogue Agents
Authors: Shuzheng Si, Wentao Ma, Haoyu Gao, Yuchuan Wu, Ting-En Lin, Yinpei
  Dai, Hangyu Li, Rui Yan, Fei Huang, Yongbin Li
Categories: cs.CL cs.AI
Comments: NeurIPS 2023
\\ ( https://arxiv.org/abs/2305.13040 ,  8341kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08543
replaced with revised version Tue, 12 Mar 2024 16:15:19 GMT   (309kb,D)

Title: Knowledge Distillation of Large Language Models
Authors: Yuxian Gu, Li Dong, Furu Wei, Minlie Huang
Categories: cs.CL cs.AI
Comments: Published as a conference paper in ICLR 2024
\\ ( https://arxiv.org/abs/2306.08543 ,  309kb)
------------------------------------------------------------------------------
\\
arXiv:2308.06259
replaced with revised version Tue, 12 Mar 2024 05:22:46 GMT   (2904kb,D)

Title: Self-Alignment with Instruction Backtranslation
Authors: Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke
  Zettlemoyer, Jason Weston, Mike Lewis
Categories: cs.CL
Comments: ICLR2024 camera ready
\\ ( https://arxiv.org/abs/2308.06259 ,  2904kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00770
replaced with revised version Tue, 12 Mar 2024 00:50:00 GMT   (826kb,D)

Title: Bias and Fairness in Large Language Models: A Survey
Authors: Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim,
  Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K. Ahmed
Categories: cs.CL cs.AI cs.CY cs.LG
\\ ( https://arxiv.org/abs/2309.00770 ,  826kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10691
replaced with revised version Tue, 12 Mar 2024 15:53:06 GMT   (1415kb,D)

Title: MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language
  Feedback
Authors: Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao
  Peng, Heng Ji
Categories: cs.CL cs.AI cs.LG
Comments: ICLR 2024. Code is available on our project website:
  https://xingyaoww.github.io/mint-bench
\\ ( https://arxiv.org/abs/2309.10691 ,  1415kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11911
replaced with revised version Tue, 12 Mar 2024 12:54:36 GMT   (1177kb,D)

Title: InstructERC: Reforming Emotion Recognition in Conversation with a
  Retrieval Multi-task LLMs Framework
Authors: Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, Sirui Wang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.11911 ,  1177kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16319
replaced with revised version Tue, 12 Mar 2024 03:43:57 GMT   (1553kb,D)

Title: Augmenting Transformers with Recursively Composed Multi-grained
  Representations
Authors: Xiang Hu, Qingyang Zhu, Kewei Tu, Wei Wu
Categories: cs.CL cs.AI
Comments: ICLR 2024 poster
\\ ( https://arxiv.org/abs/2309.16319 ,  1553kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02129
replaced with revised version Tue, 12 Mar 2024 16:58:53 GMT   (447kb,D)

Title: Unveiling the Pitfalls of Knowledge Editing for Large Language Models
Authors: Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen
Categories: cs.CL cs.AI cs.CV cs.DB cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.02129 ,  447kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01070
replaced with revised version Tue, 12 Mar 2024 14:50:30 GMT   (652kb,D)

Title: Multilingual DistilWhisper: Efficient Distillation of Multi-task Speech
  Models via Language-Specific Experts
Authors: Thomas Palmeira Ferraz, Marcely Zanon Boito, Caroline Brun, Vassilina
  Nikoulina
Categories: cs.CL cs.SD eess.AS
Comments: Accepted to IEEE ICASSP 2024
\\ ( https://arxiv.org/abs/2311.01070 ,  652kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06602
replaced with revised version Tue, 12 Mar 2024 16:54:57 GMT   (8825kb,D)

Title: BizBench: A Quantitative Reasoning Benchmark for Business and Finance
Authors: Rik Koncel-Kedziorski, Michael Krumdick, Viet Lai, Varshini Reddy,
  Charles Lovering, Chris Tanner
Categories: cs.CL
Comments: Work in progress
\\ ( https://arxiv.org/abs/2311.06602 ,  8825kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11624
replaced with revised version Tue, 12 Mar 2024 04:38:53 GMT   (183kb,D)

Title: In-context Learning with Retrieved Demonstrations for Language Models: A
  Survey
Authors: Man Luo, Xin Xu, Yue Liu, Panupong Pasupat, Mehran Kazemi
Categories: cs.CL cs.AI cs.IR
\\ ( https://arxiv.org/abs/2401.11624 ,  183kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11725
replaced with revised version Tue, 12 Mar 2024 15:48:17 GMT   (2721kb,D)

Title: Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language
  Conversion for Language Models
Authors: Yile Wang, Sijie Cheng, Zixin Sun, Peng Li, Yang Liu
Categories: cs.CL
Comments: ICLR AGI Workshop 2024
\\ ( https://arxiv.org/abs/2401.11725 ,  2721kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13165
replaced with revised version Tue, 12 Mar 2024 17:41:13 GMT   (282kb)

Title: Misgendering and Assuming Gender in Machine Translation when Working
  with Low-Resource Languages
Authors: Sourojit Ghosh, Srishti Chatterjee
Categories: cs.CL
Comments: Upcoming Publication, Gendered Technology in Translation and
  Interpreting Centering Rights in the Development of Language Technology,
  Routledge 2024
\\ ( https://arxiv.org/abs/2401.13165 ,  282kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00746
replaced with revised version Tue, 12 Mar 2024 00:16:10 GMT   (1333kb,D)

Title: Health-LLM: Personalized Retrieval-Augmented Disease Prediction System
Authors: Mingyu Jin, Qinkai Yu, Dong Shu, Chong Zhang, Suiyuan Zhu, Mengnan Du,
  Yanda Meng, Yongfeng Zhang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.00746 ,  1333kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12243
replaced with revised version Tue, 12 Mar 2024 13:52:13 GMT   (705kb,D)

Title: Understanding the Effects of Noise in Text-to-SQL: An Examination of the
  BIRD-Bench Benchmark
Authors: Niklas Wretblad, Fredrik Gordh Riseby, Rahul Biswas, Amin Ahmadi,
  Oskar Holmstr\"om
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.12243 ,  705kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19282
replaced with revised version Tue, 12 Mar 2024 12:27:52 GMT   (107kb,D)

Title: WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset
Authors: Jiantao Qiu, Haijun Lv, Zhenjiang Jin, Rui Wang, Wenchang Ning, Jia
  Yu, ChaoBin Zhang, Zhenxiang Li, Pei Chu, Yuan Qu, Jin Shi, Lindong Lu, Runyu
  Peng, Zhiyuan Zeng, Huanze Tang, Zhikai Lei, Jiawei Hong, Keyu Chen, Zhaoye
  Fei, Ruiliang Xu, Wei Li, Zhongying Tu, Hang Yan and Conghui He
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.19282 ,  107kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01548
replaced with revised version Tue, 12 Mar 2024 09:49:28 GMT   (1339kb,D)

Title: In-Context Sharpness as Alerts: An Inner Representation Perspective for
  Hallucination Mitigation
Authors: Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang
  Gao, Junxian He
Categories: cs.CL cs.AI cs.LG
Comments: code repo is available at:
  https://github.com/hkust-nlp/Activation_decoding.git
\\ ( https://arxiv.org/abs/2403.01548 ,  1339kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01841
replaced with revised version Tue, 12 Mar 2024 07:34:28 GMT   (1475kb,D)

Title: Making Pre-trained Language Models Great on Tabular Prediction
Authors: Jiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Z. Chen, Jimeng
  Sun, Jian Wu, Jintai Chen
Categories: cs.CL cs.LG
Comments: Accepted to ICLR 2024 as spotlight presentation (Notable Top 5%).
  OpenReview link is https://openreview.net/forum?id=anzIzGZuLi, codes will be
  available at https://github.com/jyansir/tp-berta
\\ ( https://arxiv.org/abs/2403.01841 ,  1475kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03102
replaced with revised version Tue, 12 Mar 2024 05:33:16 GMT   (11338kb,D)

Title: "In Dialogues We Learn": Towards Personalized Dialogue Without
  Pre-defined Profiles through In-Dialogue Learning
Authors: Chuanqi Cheng, Quan Tu, Wei Wu, Shuo Shang, Cunli Mao, Zhengtao Yu,
  Rui Yan
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.03102 ,  11338kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05326
replaced with revised version Tue, 12 Mar 2024 12:12:36 GMT   (1927kb,D)

Title: ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in
  Dialogues
Authors: Yiding Liu and Jingjing Wang and Jiamin Luo and Tao Zeng and Guodong
  Zhou
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.05326 ,  1927kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06412
replaced with revised version Tue, 12 Mar 2024 10:33:06 GMT   (8186kb,D)

Title: CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in
  Korean
Authors: Eunsu Kim, Juyoung Suk, Philhoon Oh, Haneul Yoo, James Thorne, Alice
  Oh
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.06412 ,  8186kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06914
replaced with revised version Tue, 12 Mar 2024 15:52:14 GMT   (339kb,D)

Title: MEND: Meta dEmonstratioN Distillation for Efficient and Effective
  In-Context Learning
Authors: Yichuan Li, Xiyao Ma, Sixing Lu, Kyumin Lee, Xiaohu Liu, Chenlei Guo
Categories: cs.CL cs.AI
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2403.06914 ,  339kb)
------------------------------------------------------------------------------
\\
arXiv:2006.14347
replaced with revised version Tue, 12 Mar 2024 13:00:14 GMT   (1542kb,D)

Title: Epoch-evolving Gaussian Process Guided Learning
Authors: Jiabao Cui, Xuewei Li, Bin Li, Hanbin Zhao, Bourahla Omar, and Xi Li
Categories: cs.LG cs.CV stat.ML
\\ ( https://arxiv.org/abs/2006.14347 ,  1542kb)
------------------------------------------------------------------------------
\\
arXiv:2106.10866
replaced with revised version Tue, 12 Mar 2024 08:14:31 GMT   (387kb,D)

Title: Customizing Graph Neural Networks using Path Reweighting
Authors: Jianpeng Chen and Yujing Wang and Ming Zeng and Zongyi Xiang and Bitan
  Hou and Yunhai Tong and Ole J. Mengshoel and Yazhou Ren
Categories: cs.LG
Comments: 25 pages with 14 figures
MSC-class: 68T07, 68T30, 68R99
ACM-class: I.2.0; I.2.4
\\ ( https://arxiv.org/abs/2106.10866 ,  387kb)
------------------------------------------------------------------------------
\\
arXiv:2110.03423
replaced with revised version Tue, 12 Mar 2024 12:12:29 GMT   (2038kb,D)

Title: Efficient GPU implementation of randomized SVD and its applications
Authors: {\L}ukasz Struski, Pawe{\l} Morkisz, Przemys{\l}aw Spurek, Samuel
  Rodriguez Bernabeu, Tomasz Trzci\'nski
Categories: cs.LG cs.PF
Journal-ref: Expert Systems with Applications, Volume 248, 15 August 2024
DOI: 10.1016/j.eswa.2024.123462
\\ ( https://arxiv.org/abs/2110.03423 ,  2038kb)
------------------------------------------------------------------------------
\\
arXiv:2111.13800
replaced with revised version Tue, 12 Mar 2024 17:25:35 GMT   (1239kb,D)

Title: A Two-Stage Feature Selection Approach for Robust Evaluation of
  Treatment Effects in High-Dimensional Observational Data
Authors: Md Saiful Islam, Sahil Shikalgar, Md. Noor-E-Alam
Categories: cs.LG cs.AI stat.ME
\\ ( https://arxiv.org/abs/2111.13800 ,  1239kb)
------------------------------------------------------------------------------
\\
arXiv:2112.15411
replaced with revised version Tue, 12 Mar 2024 16:16:00 GMT   (13822kb,D)

Title: Disjoint Contrastive Regression Learning for Multi-Sourced Annotations
Authors: Xiaoqian Ruan, Gaoang Wang
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2112.15411 ,  13822kb)
------------------------------------------------------------------------------
\\
arXiv:2202.03580
replaced with revised version Tue, 12 Mar 2024 09:27:38 GMT   (182kb,D)

Title: Convolutional Neural Networks on Graphs with Chebyshev Approximation,
  Revisited
Authors: Mingguo He, Zhewei Wei, Ji-Rong Wen
Categories: cs.LG cs.AI
Comments: NeurIPS 2022
\\ ( https://arxiv.org/abs/2202.03580 ,  182kb)
------------------------------------------------------------------------------
\\
arXiv:2207.06316
replaced with revised version Tue, 12 Mar 2024 11:29:58 GMT   (631kb)

Title: Majorization-minimization for Sparse Nonnegative Matrix Factorization
  with the $\beta$-divergence
Authors: Arthur Marmin, Jos\'e Henrique de Morais Goulart, C\'edric F\'evotte
Categories: cs.LG math.OC
Journal-ref: IEEE Transactions on Signal Processing 2023
DOI: 10.1109/TSP.2023.3266939
\\ ( https://arxiv.org/abs/2207.06316 ,  631kb)
------------------------------------------------------------------------------
\\
arXiv:2211.07864
replaced with revised version Tue, 12 Mar 2024 05:23:54 GMT   (1429kb,D)

Title: Federated Adaptive Prompt Tuning for Multi-Domain Collaborative Learning
Authors: Shangchao Su and Mingzhao Yang and Bin Li and Xiangyang Xue
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2211.07864 ,  1429kb)
------------------------------------------------------------------------------
\\
arXiv:2211.14302
replaced with revised version Tue, 12 Mar 2024 14:02:49 GMT   (6167kb,D)

Title: Neural DAEs: Constrained neural networks
Authors: Tue Boesen, Eldad Haber, Uri Michael Ascher
Categories: cs.LG physics.comp-ph
Comments: Extended the paper to PDEs, added a third experiment denoising a
  vector field and updated the introduction to make the distinction between
  this work and physics informed neural networks more clear
MSC-class: 70H99, 34A09
\\ ( https://arxiv.org/abs/2211.14302 ,  6167kb)
------------------------------------------------------------------------------
\\
arXiv:2211.14611
replaced with revised version Tue, 12 Mar 2024 16:07:12 GMT   (1158kb)

Title: The Principles of Data-Centric AI (DCAI)
Authors: Mohammad Hossein Jarrahi, Ali Memariani, Shion Guha
Categories: cs.LG cs.AI cs.HC
ACM-class: E.0; I.2
Journal-ref: Communications of the ACM (2023)
DOI: 10.1145/3571724
\\ ( https://arxiv.org/abs/2211.14611 ,  1158kb)
------------------------------------------------------------------------------
\\
arXiv:2302.00252
replaced with revised version Mon, 11 Mar 2024 23:11:40 GMT   (5726kb,D)

Title: QLABGrad: a Hyperparameter-Free and Convergence-Guaranteed Scheme for
  Deep Learning
Authors: Minghan Fu, Fang-Xiang Wu
Categories: cs.LG math.OC
Comments: Accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2302.00252 ,  5726kb)
------------------------------------------------------------------------------
\\
arXiv:2302.02224
replaced with revised version Mon, 11 Mar 2024 23:32:07 GMT   (2092kb,D)

Title: TAP: The Attention Patch for Cross-Modal Knowledge Transfer from
  Unlabeled Modality
Authors: Yinsong Wang, Shahin Shahrampour
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2302.02224 ,  2092kb)
------------------------------------------------------------------------------
\\
arXiv:2303.08115
replaced with revised version Mon, 11 Mar 2024 22:08:41 GMT   (2774kb,D)

Title: Human-Inspired Framework to Accelerate Reinforcement Learning
Authors: Ali Beikmohammadi and Sindri Magn\'usson
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2303.08115 ,  2774kb)
------------------------------------------------------------------------------
\\
arXiv:2304.13339
replaced with revised version Tue, 12 Mar 2024 07:21:04 GMT   (562kb,D)

Title: OpenBox: A Python Toolkit for Generalized Black-box Optimization
Authors: Huaijun Jiang, Yu Shen, Yang Li, Beicheng Xu, Sixian Du, Wentao Zhang,
  Ce Zhang and Bin Cui
Categories: cs.LG
\\ ( https://arxiv.org/abs/2304.13339 ,  562kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14550
replaced with revised version Mon, 11 Mar 2024 21:22:22 GMT   (768kb,D)

Title: When should we prefer Decision Transformers for Offline Reinforcement
  Learning?
Authors: Prajjwal Bhargava, Rohan Chitnis, Alborz Geramifard, Shagun Sodhani,
  Amy Zhang
Categories: cs.LG cs.AI
Comments: International Conference on Learning Representations (ICLR) 2024
\\ ( https://arxiv.org/abs/2305.14550 ,  768kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14585
replaced with revised version Mon, 11 Mar 2024 21:45:30 GMT   (17421kb,D)

Title: Faithful and Efficient Explanations for Neural Networks via Neural
  Tangent Kernel Surrogate Models
Authors: Andrew Engel, Zhichao Wang, Natalie S. Frank, Ioana Dumitriu, Sutanay
  Choudhury, Anand Sarwate, Tony Chiang
Categories: cs.LG
Comments: 9 pages, 2 figures, 3 tables Updated 3/11/2024 various
  additions/clarifications after ICLR review. Accepted as a Spotlight paper at
  ICLR 2024
\\ ( https://arxiv.org/abs/2305.14585 ,  17421kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08762
replaced with revised version Mon, 11 Mar 2024 19:13:04 GMT   (110kb,D)

Title: Theoretical Hardness and Tractability of POMDPs in RL with Partial
  Online State Information
Authors: Ming Shi, Yingbin Liang, and Ness Shroff
Categories: cs.LG cs.AI
Comments: Submitted for publication
\\ ( https://arxiv.org/abs/2306.08762 ,  110kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14114
replaced with revised version Tue, 12 Mar 2024 12:39:03 GMT   (1632kb,D)

Title: TNPAR: Topological Neural Poisson Auto-Regressive Model for Learning
  Granger Causal Structure from Event Sequences
Authors: Yuequn Liu, Ruichu Cai, Wei Chen, Jie Qiao, Yuguang Yan, Zijian Li,
  Keli Zhang, Zhifeng Hao
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2306.14114 ,  1632kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00543
replaced with revised version Tue, 12 Mar 2024 13:44:55 GMT   (11075kb,D)

Title: Defending Against Poisoning Attacks in Federated Learning with
  Blockchain
Authors: Nanqing Dong, Zhipeng Wang, Jiahao Sun, Michael Kampffmeyer, William
  Knottenbelt, Eric Xing
Categories: cs.LG cs.AI cs.CR cs.GT
Comments: Accepted by IEEE Transactions on Artificial Intelligence
\\ ( https://arxiv.org/abs/2307.00543 ,  11075kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09476
replaced with revised version Tue, 12 Mar 2024 07:00:02 GMT   (487kb,D)

Title: Overthinking the Truth: Understanding how Language Models Process False
  Demonstrations
Authors: Danny Halawi, Jean-Stanislas Denain, Jacob Steinhardt
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2307.09476 ,  487kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09883
replaced with revised version Tue, 12 Mar 2024 12:20:37 GMT   (2347kb,D)

Title: Symmetric Equilibrium Learning of VAEs
Authors: Boris Flach and Dmitrij Schlesinger and Alexander Shekhovtsov
Categories: cs.LG
Comments: 13 pages, 6 figures, accepted for AISTATS 2024
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2307.09883 ,  2347kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08079
replaced with revised version Tue, 12 Mar 2024 17:46:10 GMT   (2752kb)

Title: Rigid Transformations for Stabilized Lower Dimensional Space to Support
  Subsurface Uncertainty Quantification and Interpretation
Authors: Ademide O. Mabadeje and Michael J. Pyrcz
Categories: cs.LG
Comments: 30 pages, 17 figures
Journal-ref: Comput Geosci (2024)
DOI: 10.1007/s10596-024-10278-x
\\ ( https://arxiv.org/abs/2308.08079 ,  2752kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13212
replaced with revised version Tue, 12 Mar 2024 07:49:44 GMT   (3219kb,D)

Title: SEGNO: Generalizing Equivariant Graph Neural Networks with Physical
  Inductive Biases
Authors: Yang Liu, Jiashun Cheng, Haihong Zhao, Tingyang Xu, Peilin Zhao, Fugee
  Tsung, Jia Li, Yu Rong
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2308.13212 ,  3219kb)
------------------------------------------------------------------------------
\\
arXiv:2308.15006
replaced with revised version Mon, 11 Mar 2024 23:32:33 GMT   (318kb,D)

Title: Directional Optimism for Safe Linear Bandits
Authors: Spencer Hutchinson, Berkay Turan, Mahnoosh Alizadeh
Categories: cs.LG
Comments: 37 pages, 4 figures
\\ ( https://arxiv.org/abs/2308.15006 ,  318kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05436
replaced with revised version Tue, 12 Mar 2024 10:18:09 GMT   (1987kb,D)

Title: Quantized Fourier and Polynomial Features for more Expressive Tensor
  Network Models
Authors: Frederiek Wesel, Kim Batselier
Categories: cs.LG
Comments: 9 pages, 4 figures. Reviewed version after peer-review. To be
  published in the proceedings of the 27th International Conference on
  Artificial Intelligence and Statistics (AISTATS)
ACM-class: I.5.0
\\ ( https://arxiv.org/abs/2309.05436 ,  1987kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17130
replaced with revised version Tue, 12 Mar 2024 08:30:39 GMT   (176kb,D)

Title: GRANDE: Gradient-Based Decision Tree Ensembles for Tabular Data
Authors: Sascha Marton, Stefan L\"udtke, Christian Bartelt, Heiner
  Stuckenschmidt
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.17130 ,  176kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02832
replaced with revised version Mon, 11 Mar 2024 18:18:41 GMT   (901kb,D)

Title: Out-of-Distribution Detection by Leveraging Between-Layer Transformation
  Smoothness
Authors: Fran Jeleni\'c, Josip Juki\'c, Martin Tutek, Mate Puljiz, Jan
  \v{S}najder
Categories: cs.LG cs.CL
Comments: International Conference on Learning Representations: ICLR 2024
\\ ( https://arxiv.org/abs/2310.02832 ,  901kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03646
replaced with revised version Tue, 12 Mar 2024 13:38:31 GMT   (155kb,D)

Title: TRAM: Bridging Trust Regions and Sharpness Aware Minimization
Authors: Tom Sherborne, Naomi Saphra, Pradeep Dasigi, Hao Peng
Categories: cs.LG cs.CL
Comments: Camera Ready for ICLR 2024 (Accepted as Spotlight). 21 pages, 14
  tables, 2 figures
\\ ( https://arxiv.org/abs/2310.03646 ,  155kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06117
replaced with revised version Tue, 12 Mar 2024 04:38:27 GMT   (763kb,D)

Title: Take a Step Back: Evoking Reasoning via Abstraction in Large Language
  Models
Authors: Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed
  H. Chi, Quoc V Le and Denny Zhou
Categories: cs.LG cs.AI cs.CL
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.06117 ,  763kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10207
replaced with revised version Tue, 12 Mar 2024 10:57:49 GMT   (35980kb,D)

Title: Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in
  the Real World
Authors: Rujie Wu, Xiaojian Ma, Zhenliang Zhang, Wei Wang, Qing Li, Song-Chun
  Zhu, Yizhou Wang
Categories: cs.LG
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2310.10207 ,  35980kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11401
replaced with revised version Tue, 12 Mar 2024 15:56:41 GMT   (7053kb,D)

Title: Enhancing Group Fairness in Online Settings Using Oblique Decision
  Forests
Authors: Somnath Basu Roy Chowdhury, Nicholas Monath, Ahmad Beirami, Rahul
  Kidambi, Avinava Dubey, Amr Ahmed, Snigdha Chaturvedi
Categories: cs.LG
Comments: ICLR 2024 (Spotlight)
\\ ( https://arxiv.org/abs/2310.11401 ,  7053kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12553
replaced with revised version Mon, 11 Mar 2024 19:14:33 GMT   (4973kb,D)

Title: Explanation-based Training with Differentiable Insertion/Deletion
  Metric-aware Regularizers
Authors: Yuya Yoshikawa, Tomoharu Iwata
Categories: cs.LG cs.CV stat.ML
Comments: Accepted to AISTATS 2024
\\ ( https://arxiv.org/abs/2310.12553 ,  4973kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14992
replaced with revised version Tue, 12 Mar 2024 08:19:09 GMT   (1487kb,D)

Title: Bayesian Regression Markets
Authors: Thomas Falconer and Jalal Kazempour and Pierre Pinson
Categories: cs.LG
Comments: 41 pages, 11 figures, 2 tables
\\ ( https://arxiv.org/abs/2310.14992 ,  1487kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05061
replaced with revised version Mon, 11 Mar 2024 18:55:33 GMT   (20734kb,D)

Title: Efficient Compression of Overparameterized Deep Models through
  Low-Dimensional Learning Dynamics
Authors: Soo Min Kwon, Zekai Zhang, Dogyoon Song, Laura Balzano, Qing Qu
Categories: cs.LG stat.ML
Comments: International Conference on Artificial Intelligence and Statistics
  (AISTATS 2024)
\\ ( https://arxiv.org/abs/2311.05061 ,  20734kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11227
replaced with revised version Tue, 12 Mar 2024 05:26:45 GMT   (4554kb,D)

Title: FedRA: A Random Allocation Strategy for Federated Tuning to Unleash the
  Power of Heterogeneous Clients
Authors: Shangchao Su, Bin Li, Xiangyang Xue
Categories: cs.LG cs.AI cs.DC
\\ ( https://arxiv.org/abs/2311.11227 ,  4554kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04985
replaced with revised version Tue, 12 Mar 2024 11:35:08 GMT   (3810kb,D)

Title: SparQ Attention: Bandwidth-Efficient LLM Inference
Authors: Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo
  Luschi, Douglas Orr
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.04985 ,  3810kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08531
replaced with revised version Tue, 12 Mar 2024 03:53:46 GMT   (58kb)

Title: Revisiting the Last-Iterate Convergence of Stochastic Gradient Methods
Authors: Zijian Liu, Zhengyuan Zhou
Categories: cs.LG math.OC stat.ML
Comments: The preliminary version has been accepted at ICLR 2024. This extended
  version was finished in November 2023 and revised in March 2024 with fixed
  typos
\\ ( https://arxiv.org/abs/2312.08531 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01981
replaced with revised version Tue, 12 Mar 2024 01:41:06 GMT   (40844kb,D)

Title: Beyond Regrets: Geometric Metrics for Bayesian Optimization
Authors: Jungtaek Kim
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2401.01981 ,  40844kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06187
replaced with revised version Tue, 12 Mar 2024 02:38:33 GMT   (6409kb,D)

Title: Scissorhands: Scrub Data Influence via Connection Sensitivity in
  Networks
Authors: Jing Wu and Mehrtash Harandi
Categories: cs.LG cs.CV
Comments: Machine Unlearning, Deep Learning
\\ ( https://arxiv.org/abs/2401.06187 ,  6409kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08961
replaced with revised version Tue, 12 Mar 2024 15:55:52 GMT   (1347kb,D)

Title: Cascading Reinforcement Learning
Authors: Yihan Du, R. Srikant, Wei Chen
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.08961 ,  1347kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11237
replaced with revised version Tue, 12 Mar 2024 01:58:18 GMT   (987kb,D)

Title: Closing the Gap between TD Learning and Supervised Learning -- A
  Generalisation Point of View
Authors: Raj Ghugare, Matthieu Geist, Glen Berseth, Benjamin Eysenbach
Categories: cs.LG
Comments: ICLR 2024, Project code:
  https://github.com/RajGhugare19/stitching-is-combinatorial-generalisation
\\ ( https://arxiv.org/abs/2401.11237 ,  987kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15879
replaced with revised version Tue, 12 Mar 2024 07:53:36 GMT   (4763kb,D)

Title: lil'HDoC: An Algorithm for Good Arm Identification under Small Threshold
  Gap
Authors: Tzu-Hsien Tsai, Yun-Da Tsai, Shou-De Lin
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2401.15879 ,  4763kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07999
replaced with revised version Mon, 11 Mar 2024 21:32:41 GMT   (451kb,D)

Title: NetInfoF Framework: Measuring and Exploiting Network Usable Information
Authors: Meng-Chieh Lee, Haiyang Yu, Jian Zhang, Vassilis N. Ioannidis, Xiang
  Song, Soji Adeshina, Da Zheng, Christos Faloutsos
Categories: cs.LG cs.SI
Comments: Accepted to ICLR 2024 (Spotlight)
\\ ( https://arxiv.org/abs/2402.07999 ,  451kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08156
replaced with revised version Tue, 12 Mar 2024 17:37:26 GMT   (150kb)

Title: Group Decision-Making among Privacy-Aware Agents
Authors: Marios Papachristou, M. Amin Rahimian
Categories: cs.LG cs.AI cs.CR cs.MA stat.ML
\\ ( https://arxiv.org/abs/2402.08156 ,  150kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12177
replaced with revised version Tue, 12 Mar 2024 16:04:23 GMT   (60kb)

Title: Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning
Authors: Mingtian Zhang, Shawn Lan, Peter Hayes, David Barber
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.12177 ,  60kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13388
replaced with revised version Tue, 12 Mar 2024 00:32:05 GMT   (90kb,D)

Title: Transformer tricks: Precomputing the first layer
Authors: Nils Graef
Categories: cs.LG
Comments: 5 pages, 2 figures
\\ ( https://arxiv.org/abs/2402.13388 ,  90kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15441
replaced with revised version Tue, 12 Mar 2024 07:44:00 GMT   (1249kb,D)

Title: Active Few-Shot Fine-Tuning
Authors: Jonas H\"ubotter and Bhavya Sukhija and Lenart Treven and Yarden As
  and Andreas Krause
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.15441 ,  1249kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15898
replaced with revised version Tue, 12 Mar 2024 07:37:03 GMT   (2457kb,D)

Title: Information-based Transductive Active Learning
Authors: Jonas H\"ubotter, Bhavya Sukhija, Lenart Treven, Yarden As, Andreas
  Krause
Categories: cs.LG cs.AI
Comments: arXiv admin note: substantial text overlap with arXiv:2402.15441
\\ ( https://arxiv.org/abs/2402.15898 ,  2457kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17257
replaced with revised version Tue, 12 Mar 2024 04:48:46 GMT   (25025kb,D)

Title: RIME: Robust Preference-based Reinforcement Learning with Noisy
  Preferences
Authors: Jie Cheng, Gang Xiong, Xingyuan Dai, Qinghai Miao, Yisheng Lv, Fei-Yue
  Wang
Categories: cs.LG cs.AI cs.RO
\\ ( https://arxiv.org/abs/2402.17257 ,  25025kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18603
replaced with revised version Tue, 12 Mar 2024 16:35:25 GMT   (946kb,D)

Title: MMSR: Symbolic Regression is a Multimodal Task
Authors: Yanjie Li, Jingyi Liu, Weijun Li, Lina Yu, Min Wu, Wenqiang Li, Meilan
  Hao, Su Wei, Yusong Deng
Categories: cs.LG cs.AI cs.CL
Comments: 12 page
\\ ( https://arxiv.org/abs/2402.18603 ,  946kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00012
replaced with revised version Tue, 12 Mar 2024 12:59:45 GMT   (653kb,D)

Title: PreRoutGNN for Timing Prediction with Order Preserving Partition: Global
  Circuit Pre-training, Local Delay Learning and Attentional Cell Modeling
Authors: Ruizhe Zhong, Junjie Ye, Zhentao Tang, Shixiong Kai, Mingxuan Yuan,
  Jianye Hao, Junchi Yan
Categories: cs.LG cs.AR
Comments: 13 pages, 5 figures, The 38th Annual AAAI Conference on Artificial
  Intelligence (AAAI 2024)
\\ ( https://arxiv.org/abs/2403.00012 ,  653kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00673
replaced with revised version Tue, 12 Mar 2024 12:20:59 GMT   (25260kb,D)

Title: Snapshot Reinforcement Learning: Leveraging Prior Trajectories for
  Efficiency
Authors: Yanxiao Zhao, Yangge Qian, Tianyi Wang, Jingyang Shan, Xiaolin Qin
Categories: cs.LG
Comments: Under review
\\ ( https://arxiv.org/abs/2403.00673 ,  25260kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01438
replaced with revised version Tue, 12 Mar 2024 05:37:07 GMT   (2648kb,D)

Title: Privacy-Preserving Collaborative Split Learning Framework for Smart Grid
  Load Forecasting
Authors: Asif Iqbal, Prosanta Gope, Biplab Sikdar
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.01438 ,  2648kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02624
replaced with revised version Tue, 12 Mar 2024 06:28:39 GMT   (11944kb,D)

Title: Pareto-Optimal Estimation and Policy Learning on Short-term and
  Long-term Treatment Effects
Authors: Yingrong Wang, Anpeng Wu, Haoxuan Li, Weiming Liu, Qiaowei Miao,
  Ruoxuan Xiong, Fei Wu, Kun Kuang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.02624 ,  11944kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04558
replaced with revised version Tue, 12 Mar 2024 11:42:06 GMT   (1167kb,D)

Title: Reducing self-supervised learning complexity improves weakly-supervised
  classification performance in computational pathology
Authors: Tim Lenz, Omar S. M. El Nahhas, Marta Ligero, Jakob Nikolas Kather
Categories: cs.LG cs.AI cs.CV
Comments: Submitted to MICCAI 2024
\\ ( https://arxiv.org/abs/2403.04558 ,  1167kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04778
replaced with revised version Tue, 12 Mar 2024 13:47:52 GMT   (1668kb,D)

Title: An Efficient Difference-of-Convex Solver for Privacy Funnel
Authors: Teng-Hui Huang and Hesham El Gamal
Categories: cs.LG cs.CR cs.IT math.IT
\\ ( https://arxiv.org/abs/2403.04778 ,  1668kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05385
replaced with revised version Tue, 12 Mar 2024 16:01:02 GMT   (135kb,D)

Title: Switching the Loss Reduces the Cost in Batch Reinforcement Learning
Authors: Alex Ayoub, Kaiwen Wang, Vincent Liu, Samuel Robertson, James
  McInerney, Dawen Liang, Nathan Kallus, and Csaba Szepesv\'ari
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.05385 ,  135kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05527
replaced with revised version Mon, 11 Mar 2024 18:55:40 GMT   (2567kb,D)

Title: GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
  Generative Inference of LLM
Authors: Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu,
  Tushar Krishna, Tuo Zhao
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2403.05527 ,  2567kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05818
replaced with revised version Tue, 12 Mar 2024 08:55:00 GMT   (1309kb)

Title: PR-NET: Leveraging Pathway Refined Network Structures for Prostate
  Cancer Patient Condition Prediction
Authors: R. Li, J. Liu, X.L. Deng, X. Liu, J.C. Guo, W.Y. Wu, L. Yang
Categories: cs.LG q-bio.QM
\\ ( https://arxiv.org/abs/2403.05818 ,  1309kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05918
replaced with revised version Tue, 12 Mar 2024 02:45:48 GMT   (2409kb)

Title: SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to
  Imbalanced Data
Authors: Ming Zheng, Yang Yang, Zhi-Hang Zhao, Shan-Chao Gan, Yang Chen, Si-Kai
  Ni and Yang Lu
Categories: cs.LG cs.AI
Comments: None
\\ ( https://arxiv.org/abs/2403.05918 ,  2409kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06397
replaced with revised version Tue, 12 Mar 2024 02:13:51 GMT   (4693kb,D)

Title: DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe
  Multi-Agent Reinforcement Learning
Authors: Xuefeng Wang, Henglin Pu, Hyung Jun Kim and Husheng Li
Categories: cs.LG cs.AI cs.SY eess.SY
Comments: 8 pages, 5 figures
\\ ( https://arxiv.org/abs/2403.06397 ,  4693kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06569
replaced with revised version Tue, 12 Mar 2024 11:40:33 GMT   (1061kb,D)

Title: Enhancing Joint Motion Prediction for Individuals with Limb Loss Through
  Model Reprogramming
Authors: Sharmita Dey, Sarath R. Nair
Categories: cs.LG cs.RO
Journal-ref: ICLR 2024 Workshop: Learning from Time Series for Health
\\ ( https://arxiv.org/abs/2403.06569 ,  1061kb)
------------------------------------------------------------------------------
\\
arXiv:2104.12582
replaced with revised version Mon, 11 Mar 2024 18:58:33 GMT   (35kb)

Title: Understanding and Avoiding AI Failures: A Practical Guide
Authors: Heather M. Williams, Roman V. Yampolskiy
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2104.12582 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2108.00408
replaced with revised version Mon, 11 Mar 2024 23:20:07 GMT   (785kb)

Title: CSC-Unet: A Novel Convolutional Sparse Coding Strategy Based Neural
  Network for Semantic Segmentation
Authors: Haitong Tang, Shuang He, Mengduo Yang, Xia Lu, Qin Yu, Kaiyue Liu,
  Hongjie Yan and Nizhuan Wang
Categories: cs.CV cs.AI cs.LG
Journal-ref: IEEE Access,2024
DOI: 10.1109/ACCESS.2024.3373619
\\ ( https://arxiv.org/abs/2108.00408 ,  785kb)
------------------------------------------------------------------------------
\\
arXiv:2109.00031
replaced with revised version Mon, 11 Mar 2024 18:11:50 GMT   (7124kb)

Title: Deep DNA Storage: Scalable and Robust DNA Storage via Coding Theory and
  Deep Learning
Authors: Daniella Bar-Lev, Itai Orr, Omer Sabary, Tuvi Etzion, Eitan Yaakobi
Categories: cs.IT cs.AI math.IT
\\ ( https://arxiv.org/abs/2109.00031 ,  7124kb)
------------------------------------------------------------------------------
\\
arXiv:2201.13224
replaced with revised version Tue, 12 Mar 2024 15:46:43 GMT   (1007kb,D)

Title: Evaluating a Methodology for Increasing AI Transparency: A Case Study
Authors: David Piorkowski, John Richards, Michael Hind
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2201.13224 ,  1007kb)
------------------------------------------------------------------------------
\\
arXiv:2203.02158 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 13:57:45 GMT   (1109kb,D)

Title: Transformations in Learned Image Compression from a Modulation
  Perspective
Authors: Youneng Bao, Fangyang Meng, Wen Tan, Chao Li, Yonghong Tian and
  Yongsheng Liang
Categories: eess.IV cs.AI cs.CV
Comments: 10 pages, 8 figures
\\ ( https://arxiv.org/abs/2203.02158 ,  1109kb)
------------------------------------------------------------------------------
\\
arXiv:2210.09292
replaced with revised version Tue, 12 Mar 2024 02:08:37 GMT   (3889kb,D)

Title: Efficient Diffusion Models for Vision: A Survey
Authors: Anwaar Ulhaq and Naveed Akhtar
Categories: cs.CV cs.AI
Comments: 14 Pages, 5 Figures (in progress)
\\ ( https://arxiv.org/abs/2210.09292 ,  3889kb)
------------------------------------------------------------------------------
\\
arXiv:2211.02695
replaced with revised version Tue, 12 Mar 2024 15:12:55 GMT   (1757kb,D)

Title: WaveNets: Wavelet Channel Attention Networks
Authors: Hadi Salman, Caleb Parks, Shi Yin Hong, Justin Zhan
Categories: cs.CV cs.AI
Comments: IEEE BigData2022 conference
DOI: 10.1109/BigData55660.2022.10020665
\\ ( https://arxiv.org/abs/2211.02695 ,  1757kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03403
replaced with revised version Tue, 12 Mar 2024 12:41:04 GMT   (19721kb,D)

Title: SGAT4PASS: Spherical Geometry-Aware Transformer for PAnoramic Semantic
  Segmentation
Authors: Xuewei Li, Tao Wu, Zhongang Qi, Gaoang Wang, Ying Shan, Xi Li
Categories: cs.CV cs.AI cs.LG cs.MM
Comments: Accepted by IJCAI 2023
\\ ( https://arxiv.org/abs/2306.03403 ,  19721kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16140
replaced with revised version Tue, 12 Mar 2024 07:23:51 GMT   (17457kb,D)

Title: Fully $1\times1$ Convolutional Network for Lightweight Image
  Super-Resolution
Authors: Gang Wu, Junjun Jiang, Kui Jiang, Xianming Liu
Categories: cs.CV cs.AI
Comments: Accepted by Machine Intelligence Research, DOI:
  10.1007/s11633-024-1401-z
DOI: 10.1007/s11633-024-1401-z
\\ ( https://arxiv.org/abs/2307.16140 ,  17457kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02409 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 12:25:24 GMT   (1542kb,D)

Title: Mental Workload Estimation with Electroencephalogram Signals by
  Combining Multi-Space Deep Models
Authors: Hong-Hai Nguyen, Ngumimi Karen Iyortsuun, Seungwon Kim, Hyung-Jeong
  Yang, and Soo-Hyung Kim
Categories: eess.SP cs.AI cs.LG
Comments: 16 pages, 5 figures
\\ ( https://arxiv.org/abs/2308.02409 ,  1542kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04522
replaced with revised version Tue, 12 Mar 2024 00:16:38 GMT   (6291kb,D)

Title: Deep Learning for Steganalysis of Diverse Data Types: A review of
  methods, taxonomy, challenges and future directions
Authors: Hamza Kheddar, Mustapha Hemis, Yassine Himeur, David Meg\'ias, Abbes
  Amira
Categories: cs.CR cs.AI cs.LG cs.MM cs.SD eess.AS eess.IV
Journal-ref: Neurocomputing, Elsevier, 2024
DOI: 10.1016/j.neucom.2024.127528
\\ ( https://arxiv.org/abs/2308.04522 ,  6291kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02001
replaced with revised version Mon, 11 Mar 2024 18:31:09 GMT   (52kb,D)

Title: Analyzing domain shift when using additional data for the MICCAI KiTS23
  Challenge
Authors: George Stoica, Mihaela Breaban and Vlad Barbu
Categories: cs.CV cs.AI cs.LG
Comments: This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this contribution is
  published in https://link.springer.com/book/10.1007/978-3-031-54806-2, and is
  available online at https://doi.org/10.1007/978-3-031-54806-2_4
Journal-ref: Kidney and Kidney Tumor Segmentation. KiTS 2023. Lecture Notes in
  Computer Science, vol 14540
DOI: 10.1007/978-3-031-54806-2_4
\\ ( https://arxiv.org/abs/2309.02001 ,  52kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18511
replaced with revised version Tue, 12 Mar 2024 11:52:42 GMT   (23149kb,D)

Title: 3DCoMPaT$^{++}$: An improved Large-scale 3D Vision Dataset for
  Compositional Recognition
Authors: Habib Slim, Xiang Li, Yuchen Li, Mahmoud Ahmed, Mohamed Ayman, Ujjwal
  Upadhyay, Ahmed Abdelreheem, Arpit Prajapati, Suhail Pothigara, Peter Wonka,
  Mohamed Elhoseiny
Categories: cs.CV cs.AI
Comments: https://3dcompat-dataset.org/v2/
\\ ( https://arxiv.org/abs/2310.18511 ,  23149kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16480
replaced with revised version Tue, 12 Mar 2024 12:07:39 GMT   (2749kb,D)

Title: WsiCaption: Multiple Instance Generation of Pathology Reports for
  Gigapixel Whole-Slide Images
Authors: Pingyi Chen, Honglin Li, Chenglu Zhu, Sunyi Zheng, Zhongyi Shui, Lin
  Yang
Categories: cs.CV cs.AI cs.CL
\\ ( https://arxiv.org/abs/2311.16480 ,  2749kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03781
replaced with revised version Tue, 12 Mar 2024 08:13:01 GMT   (41069kb,D)

Title: Lite-Mind: Towards Efficient and Robust Brain Representation Network
Authors: Zixuan Gong, Qi Zhang, Duoqian Miao, Guangyin Bao, Liang Hu
Categories: cs.CV cs.AI
Comments: 17 pages
\\ ( https://arxiv.org/abs/2312.03781 ,  41069kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09982
replaced with revised version Mon, 11 Mar 2024 19:24:41 GMT   (1672kb,D)

Title: ACPO: AI-Enabled Compiler-Driven Program Optimization
Authors: Amir H. Ashouri, Muhammad Asif Manzoor, Duc Minh Vu, Raymond Zhang,
  Ziwen Wang, Angel Zhang, Bryan Chan, Tomasz S. Czajkowski and Yaoqing Gao
Categories: cs.PL cs.AI cs.LG cs.PF
Comments: Preprint version of ACPO (12 pages)
ACM-class: I.2.5; D.3.0; I.2.6
\\ ( https://arxiv.org/abs/2312.09982 ,  1672kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15020
replaced with revised version Tue, 12 Mar 2024 07:12:38 GMT   (1881kb,D)

Title: Automated Approaches to Detect Self-Admitted Technical Debt: A
  Systematic Literature Review
Authors: Edi Sutoyo, Andrea Capiluppi
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2312.15020 ,  1881kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16145
replaced with revised version Mon, 11 Mar 2024 18:13:54 GMT   (48209kb,D)

Title: One-Dimensional Adapter to Rule Them All: Concepts, Diffusion Models and
  Erasing Applications
Authors: Mengyao Lyu, Yuhong Yang, Haiwen Hong, Hui Chen, Xuan Jin, Yuan He,
  Hui Xue, Jungong Han, Guiguang Ding
Categories: cs.CV cs.AI cs.LG
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2312.16145 ,  48209kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03227
replaced with revised version Tue, 12 Mar 2024 11:28:20 GMT   (3069kb,D)

Title: IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of
  brain MR images
Authors: Vincent Roca, Gr\'egory Kuchcinski, Jean-Pierre Pruvo, Dorian
  Manouvriez, Renaud Lopes
Categories: cs.CV cs.AI cs.LG
Comments: 23 pages, 8 figures; typos corrected
\\ ( https://arxiv.org/abs/2402.03227 ,  3069kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09430 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 11:48:02 GMT   (11432kb,D)

Title: WiMANS: A Benchmark Dataset for WiFi-based Multi-user Activity Sensing
Authors: Shuokang Huang, Kaihan Li, Di You, Yichong Chen, Arvin Lin, Siying
  Liu, Xiaohui Li, Julie A. McCann
Categories: eess.SP cs.AI cs.CV cs.MM
Comments: We present WiMANS, to our knowledge, the first dataset for multi-user
  activity sensing based on WiFi
\\ ( https://arxiv.org/abs/2402.09430 ,  11432kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09442 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 11:14:15 GMT   (574kb)

Title: Progress in artificial intelligence applications based on the
  combination of self-driven sensors and deep learning
Authors: Weixiang Wan, Wenjian Sun, Qiang Zeng, Linying Pan, Jingyu Xu, Bo Liu
Categories: eess.SP cs.AI
Comments: This aticle was accepted by ieee conference
\\ ( https://arxiv.org/abs/2402.09442 ,  574kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09786
replaced with revised version Tue, 12 Mar 2024 13:36:23 GMT   (29299kb,D)

Title: Examining Pathological Bias in a Generative Adversarial Network
  Discriminator: A Case Study on a StyleGAN3 Model
Authors: Alvin Grissom II, Ryan F. Lei, Matt Gusdorff, Jeova Farias Sales Rocha
  Neto, Bailey Lin, Ryan Trotter
Categories: cs.CV cs.AI cs.CY cs.LG
\\ ( https://arxiv.org/abs/2402.09786 ,  29299kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10885
replaced with revised version Mon, 11 Mar 2024 22:05:00 GMT   (28406kb,D)

Title: 3D Diffuser Actor: Policy Diffusion with 3D Scene Representations
Authors: Tsung-Wei Ke, Nikolaos Gkanatsios, Katerina Fragkiadaki
Categories: cs.RO cs.AI cs.CV cs.LG
Comments: First two authors contributed equally
\\ ( https://arxiv.org/abs/2402.10885 ,  28406kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13254
replaced with revised version Tue, 12 Mar 2024 17:59:56 GMT   (10188kb,D)

Title: CounterCurate: Enhancing Physical and Semantic Visio-Linguistic
  Compositional Reasoning via Counterfactual Examples
Authors: Jianrui Zhang, Mu Cai, Tengyang Xie, Yong Jae Lee
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: 13 pages, 6 figures, 8 tables, Project Page:
  https://countercurate.github.io/
\\ ( https://arxiv.org/abs/2402.13254 ,  10188kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06025
replaced with revised version Tue, 12 Mar 2024 17:35:29 GMT   (13284kb,D)

Title: CarbonNet: How Computer Vision Plays a Role in Climate Change?
  Application: Learning Geomechanics from Subsurface Geometry of CCS to
  Mitigate Global Warming
Authors: Wei Chen, Yunan Li and Yuan Tian
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.06025 ,  13284kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06054 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 03:22:13 GMT   (13981kb,D)

Title: Decoupled Data Consistency with Diffusion Purification for Image
  Restoration
Authors: Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishanka, Qing
  Qu
Categories: eess.IV cs.AI cs.CV cs.LG eess.SP
\\ ( https://arxiv.org/abs/2403.06054 ,  13981kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06670
replaced with revised version Tue, 12 Mar 2024 03:04:15 GMT   (6526kb,D)

Title: CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar
  Class-Incremental Learning
Authors: Xinyuan Gao, Songlin Dong, Yuhang He, Xing Wei, Yihong Gong
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.06670 ,  6526kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09618 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 04:14:18 GMT   (15305kb,D)

Title: Simulating Opinion Dynamics with Networks of LLM-based Agents
Authors: Yun-Shiuan Chuang, Agam Goyal, Nikunj Harlalka, Siddharth Suresh,
  Robert Hawkins, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers
Categories: physics.soc-ph cs.CL
\\ ( https://arxiv.org/abs/2311.09618 ,  15305kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15479
replaced with revised version Mon, 11 Mar 2024 23:37:36 GMT   (3565kb,D)

Title: Navigating the Post-API Dilemma Search Engine Results Pages Present a
  Biased View of Social Media Data
Authors: Amrit Poudel, Tim Weninger
Categories: cs.IR cs.CL cs.SI
Comments: Proceedings of the ACM Web Conference 2024 (WWW '24)
\\ ( https://arxiv.org/abs/2401.15479 ,  3565kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16315
replaced with revised version Mon, 11 Mar 2024 18:49:40 GMT   (28563kb,D)

Title: Finer: Investigating and Enhancing Fine-Grained Visual Concept
  Recognition in Large Vision Language Models
Authors: Jeonghwan Kim and Heng Ji
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2402.16315 ,  28563kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05820
replaced with revised version Tue, 12 Mar 2024 11:26:07 GMT   (277kb,D)

Title: An Audio-textual Diffusion Model For Converting Speech Signals Into
  Ultrasound Tongue Imaging Data
Authors: Yudong Yang, Rongfeng Su, Xiaokang Liu, Nan Yan, and Lan Wang
Categories: cs.SD cs.CL eess.AS
Comments: ICASSP2024 Accept
\\ ( https://arxiv.org/abs/2403.05820 ,  277kb)
------------------------------------------------------------------------------
\\
arXiv:2006.15524
replaced with revised version Tue, 12 Mar 2024 12:53:14 GMT   (1823kb,D)

Title: MgSvF: Multi-Grained Slow vs. Fast Framework for Few-Shot
  Class-Incremental Learning
Authors: Hanbin Zhao, Yongjian Fu, Mintong Kang, Qi Tian, Fei Wu, Xi Li
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2006.15524 ,  1823kb)
------------------------------------------------------------------------------
\\
arXiv:2109.06911 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 21:28:38 GMT   (150kb,D)

Title: Learning and Decision-Making with Data: Optimal Formulations and Phase
  Transitions
Authors: Amine Bennouna and Bart P.G. Van Parys
Categories: stat.ML cs.LG math.OC math.ST stat.TH
\\ ( https://arxiv.org/abs/2109.06911 ,  150kb)
------------------------------------------------------------------------------
\\
arXiv:2112.12989
replaced with revised version Tue, 12 Mar 2024 14:47:47 GMT   (16856kb,D)

Title: Domain-Aware Continual Zero-Shot Learning
Authors: Kai Yi, Paul Janson, Wenxuan Zhang, Mohamed Elhoseiny
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2112.12989 ,  16856kb)
------------------------------------------------------------------------------
\\
arXiv:2202.08876 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 18:38:23 GMT   (2180kb,D)

Title: An alternative approach to train neural networks using monotone
  variational inequality
Authors: Chen Xu, Xiuyuan Cheng, Yao Xie
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2202.08876 ,  2180kb)
------------------------------------------------------------------------------
\\
arXiv:2212.11851 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 15:31:01 GMT   (14813kb,D)

Title: StoRM: A Diffusion-based Stochastic Regeneration Model for Speech
  Enhancement and Dereverberation
Authors: Jean-Marie Lemercier and Julius Richter and Simon Welker and Timo
  Gerkmann
Categories: eess.AS cs.LG cs.SD
Comments: Published in IEEE/ACM Transactions on Audio, Speech and Language
  Processing, 2023
DOI: 10.1109/TASLP.2023.3294692
\\ ( https://arxiv.org/abs/2212.11851 ,  14813kb)
------------------------------------------------------------------------------
\\
arXiv:2303.17823 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 23:28:13 GMT   (1137kb,D)

Title: An interpretable neural network-based non-proportional odds model for
  ordinal regression
Authors: Akifumi Okuno, Kazuharu Harada
Categories: stat.ME cs.LG stat.ML
Comments: 35 pages, 18 figures, accepted to Journal of Computational and
  Graphical Statistics
\\ ( https://arxiv.org/abs/2303.17823 ,  1137kb)
------------------------------------------------------------------------------
\\
arXiv:2305.10015 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 07:26:45 GMT   (2659kb,D)

Title: Utility Theory of Synthetic Data Generation
Authors: Shirong Xu and Will Wei Sun and Guang Cheng
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2305.10015 ,  2659kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12522
replaced with revised version Mon, 11 Mar 2024 18:26:10 GMT   (42256kb,D)

Title: P-NOC: adversarial training of CAM generating networks for robust weakly
  supervised semantic segmentation priors
Authors: Lucas David, Helio Pedrini, and Zanoni Dias
Categories: cs.CV cs.LG
Comments: 19 pages, 10 figures
\\ ( https://arxiv.org/abs/2305.12522 ,  42256kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17283 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 05:54:22 GMT   (3852kb,D)

Title: Sharpened Lazy Incremental Quasi-Newton Method
Authors: Aakash Lahoti, Spandan Senapati, Ketan Rajawat, Alec Koppel
Categories: math.OC cs.LG
Comments: 36 pages, 2 figures; Accepted to AISTATS 2024
\\ ( https://arxiv.org/abs/2305.17283 ,  3852kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03117 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 19:54:30 GMT   (16414kb,D)

Title: Str2Str: A Score-based Framework for Zero-shot Protein Conformation
  Sampling
Authors: Jiarui Lu, Bozitao Zhong, Zuobai Zhang, Jian Tang
Categories: q-bio.QM cs.LG q-bio.BM
Comments: Published as a conference paper at ICLR 2024, see
  https://openreview.net/forum?id=C4BikKsgmK
\\ ( https://arxiv.org/abs/2306.03117 ,  16414kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10535 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 11:57:00 GMT   (1326kb,D)

Title: ProMIL: Probabilistic Multiple Instance Learning for Medical Imaging
Authors: {\L}ukasz Struski, Dawid Rymarczyk, Arkadiusz Lewicki, Robert
  Sabiniewicz, Jacek Tabor, Bartosz Zieli\'nski
Categories: eess.IV cs.CV cs.LG
Comments: Accepted Paper to European Conference on Artificial Intelligence
  (ECAI 2023)
\\ ( https://arxiv.org/abs/2306.10535 ,  1326kb)
------------------------------------------------------------------------------
\\
arXiv:2307.05881 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 14:09:56 GMT   (6739kb,D)

Title: tdCoxSNN: Time-Dependent Cox Survival Neural Network for Continuous-time
  Dynamic Prediction
Authors: Lang Zeng, Jipeng Zhang, Wei Chen, Ying Ding
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2307.05881 ,  6739kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05525
replaced with revised version Tue, 12 Mar 2024 15:05:23 GMT   (4713kb,D)

Title: Robustifying Point Cloud Networks by Refocusing
Authors: Meir Yossef Levi, Guy Gilboa
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2308.05525 ,  4713kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10547 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 10:58:02 GMT   (257kb,D)

Title: Decentralized Riemannian Conjugate Gradient Method on the Stiefel
  Manifold
Authors: Jun Chen, Haishan Ye, Mengmeng Wang, Tianxin Huang, Guang Dai, Ivor
  W.Tsang, Yong Liu
Categories: math.OC cs.LG cs.SY eess.SY
Journal-ref: International Conference on Learning Representations, 2024
\\ ( https://arxiv.org/abs/2308.10547 ,  257kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04349
replaced with revised version Tue, 12 Mar 2024 15:22:05 GMT   (5593kb,D)

Title: Toward a Plug-and-Play Vision-Based Grasping Module for Robotics
Authors: Fran\c{c}ois H\'el\'enon, Johann Huber, Fa\"iz Ben Amar and St\'ephane
  Doncieux
Categories: cs.RO cs.LG
Comments: 6 pages, 9 figures
\\ ( https://arxiv.org/abs/2310.04349 ,  5593kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07079
replaced with revised version Mon, 11 Mar 2024 21:52:12 GMT   (659kb,D)

Title: Secure Decentralized Learning with Blockchain
Authors: Xiaoxue Zhang, Yifan Hua and Chen Qian
Categories: cs.CR cs.LG
ACM-class: I.2.11; C.2.4
\\ ( https://arxiv.org/abs/2310.07079 ,  659kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07990 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 15:34:13 GMT   (903kb)

Title: Multi-View Variational Autoencoder for Missing Value Imputation in
  Untargeted Metabolomics
Authors: Chen Zhao, Kuan-Jui Su, Chong Wu, Xuewei Cao, Qiuying Sha, Wu Li, Zhe
  Luo, Tian Qin, Chuan Qiu, Lan Juan Zhao, Anqi Liu, Lindong Jiang, Xiao Zhang,
  Hui Shen, Weihua Zhou, Hong-Wen Deng
Categories: q-bio.GN cs.IR cs.LG stat.AP
Comments: 19 pages, 3 figures
\\ ( https://arxiv.org/abs/2310.07990 ,  903kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02058
replaced with revised version Tue, 12 Mar 2024 17:23:55 GMT   (2772kb,D)

Title: LOTUS: Continual Imitation Learning for Robot Manipulation Through
  Unsupervised Skill Discovery
Authors: Weikang Wan, Yifeng Zhu, Rutav Shah, Yuke Zhu
Categories: cs.RO cs.CV cs.LG
Comments: ICRA 2024
\\ ( https://arxiv.org/abs/2311.02058 ,  2772kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04417
replaced with revised version Mon, 11 Mar 2024 20:52:30 GMT   (2836kb,D)

Title: Evaluating Emerging AI/ML Accelerators: IPU, RDU, and NVIDIA/AMD GPUs
Authors: Hongwu Peng and Caiwen Ding and Tong Geng and Sutanay Choudhury and
  Kevin Barker and Ang Li
Categories: cs.AR cs.DC cs.LG cs.PF
ACM-class: C.4
\\ ( https://arxiv.org/abs/2311.04417 ,  2836kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06295 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 07:36:05 GMT   (2574kb,D)

Title: Gradual Optimization Learning for Conformational Energy Minimization
Authors: Artem Tsypin, Leonid Ugadiarov, Kuzma Khrabrov, Alexander Telepov,
  Egor Rumiantsev, Alexey Skrynnik, Aleksandr I. Panov, Dmitry Vetrov, Elena
  Tutubalina and Artur Kadurin
Categories: physics.chem-ph cs.LG
Comments: Published as a conference paper at ICLR2024 (Poster)
\\ ( https://arxiv.org/abs/2311.06295 ,  2574kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09354 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 22:12:25 GMT   (10653kb)

Title: Nondestructive, quantitative viability analysis of 3D tissue cultures
  using machine learning image segmentation
Authors: Kylie J. Trettner, Jeremy Hsieh, Weikun Xiao, Jerry S.H. Lee, Andrea
  M. Armani
Categories: q-bio.QM cs.LG eess.IV
Comments: 52 total pages, Main text and SI included, 35 figures (5 main text,
  30 supplemental), 9 tables, 6 datasets (provided on linked GitHub), linked
  image files on Zenodo
\\ ( https://arxiv.org/abs/2311.09354 ,  10653kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12163 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 23:51:32 GMT   (2833kb,D)

Title: Quantum Inception Score
Authors: Akira Sone and Akira Tanji and Naoki Yamamoto
Categories: quant-ph cond-mat.stat-mech cs.LG stat.ML
Comments: v2: 11+5 pages, 6 figures. We added the application of quantum
  inception score in evaluating the quantum generative models for the phase
  classification problem in the many-body physics, and improved our proofs for
  the derived theorems
\\ ( https://arxiv.org/abs/2311.12163 ,  2833kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12028
replaced with revised version Mon, 11 Mar 2024 20:29:50 GMT   (15472kb,D)

Title: EyePreserve: Identity-Preserving Iris Synthesis
Authors: Siamul Karim Khan, Patrick Tinsley, Mahsa Mitcheff, Patrick Flynn,
  Kevin W. Bowyer, Adam Czajka
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2312.12028 ,  15472kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04079 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 17:47:59 GMT   (774kb,D)

Title: RudolfV: A Foundation Model by Pathologists for Pathologists
Authors: Jonas Dippel, Barbara Feulner, Tobias Winterhoff, Simon Schallenberg,
  Gabriel Dernbach, Andreas Kunft, Stephan Tietz, Timo Milbich, Simon Heinke,
  Marie-Lisa Eich, Julika Ribbat-Idel, Rosemarie Krupar, Philipp Jurmeister,
  David Horst, Lukas Ruff, Klaus-Robert M\"uller, Frederick Klauschen,
  Maximilian Alber
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2401.04079 ,  774kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05482 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 14:44:58 GMT   (1385kb,D)

Title: A Non-Intrusive Neural Quality Assessment Model for Surface
  Electromyography Signals
Authors: Cho-Yuan Lee, Kuan-Chen Wang, Kai-Chun Liu, Xugang Lu, Ping-Cheng Yeh,
  and Yu Tsao
Categories: eess.SP cs.LG
Comments: 5 pages, 4 figures
\\ ( https://arxiv.org/abs/2402.05482 ,  1385kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10433 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 20:20:16 GMT   (729kb,D)

Title: Fusing Neural and Physical: Augment Protein Conformation Sampling with
  Tractable Simulations
Authors: Jiarui Lu, Zuobai Zhang, Bozitao Zhong, Chence Shi, Jian Tang
Categories: q-bio.BM cs.LG q-bio.QM
Comments: Published at the GEM workshop, ICLR 2024
\\ ( https://arxiv.org/abs/2402.10433 ,  729kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14892 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 17:34:16 GMT   (25651kb,D)

Title: Novelty Detection on Radio Astronomy Data using Signatures
Authors: Paola Arrubarrena, Maud Lemercier, Bojan Nikolic, Terry Lyons, Thomas
  Cass
Categories: astro-ph.IM cs.LG
MSC-class: 60L10, 60L20
\\ ( https://arxiv.org/abs/2402.14892 ,  25651kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00892
replaced with revised version Tue, 12 Mar 2024 09:36:27 GMT   (9234kb,D)

Title: PowerFlowMultiNet: Multigraph Neural Networks for Unbalanced Three-Phase
  Distribution Systems
Authors: Salah Ghamizi, Jun Cao, Aoxiang Ma, Pedro Rodriguez
Categories: eess.SY cs.LG cs.SY
\\ ( https://arxiv.org/abs/2403.00892 ,  9234kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03229 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 16:03:03 GMT   (1481kb,D)

Title: Embracing Uncertainty Flexibility: Harnessing a Supervised Tree Kernel
  to Empower Ensemble Modelling for 2D Echocardiography-Based Prediction of
  Right Ventricular Volume
Authors: Tuan A. Bohoran, Polydoros N. Kampaktsis, Laura McLaughlin, Jay Leb,
  Gerry P. McCann, Archontis Giannakidis
Categories: q-bio.TO cs.LG eess.IV math.AP
Comments: In the Proceedings of the 16th International Conference of Machine
  Vision (ICMV 2023), November 15-18, Yerevan, Armenia
\\ ( https://arxiv.org/abs/2403.03229 ,  1481kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04259 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 04:02:49 GMT   (873kb,D)

Title: Decentralized and Equitable Optimal Transport
Authors: Ivan Lau, Shiqian Ma, C\'esar A. Uribe
Categories: math.OC cs.LG
Comments: Accepted to ACC 2024
\\ ( https://arxiv.org/abs/2403.04259 ,  873kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04990 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 06:35:49 GMT   (13122kb,D)

Title: Jet Discrimination with Quantum Complete Graph Neural Network
Authors: Yi-An Chen, Kai-Feng Chen
Categories: hep-ph cs.LG quant-ph
\\ ( https://arxiv.org/abs/2403.04990 ,  13122kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05949
replaced with revised version Tue, 12 Mar 2024 03:23:45 GMT   (14157kb,D)

Title: General surgery vision transformer: A video pre-trained foundation model
  for general surgery
Authors: Samuel Schmidgall, Ji Woong Kim, Jeffery Jopling, Axel Krieger
Categories: cs.CV cs.LG q-bio.TO
\\ ( https://arxiv.org/abs/2403.05949 ,  14157kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06279 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 16:54:34 GMT   (17kb)

Title: Fine-tuning of diffusion models via stochastic control: entropy
  regularization and beyond
Authors: Wenpin Tang
Categories: math.OC cs.LG
Comments: 15 pages
\\ ( https://arxiv.org/abs/2403.06279 ,  17kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
