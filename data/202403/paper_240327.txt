paper_240327.txt

Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 80011 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月27日 12:49
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Mon 25 Mar 24 18:00:00 GMT  to  Tue 26 Mar 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.17040
Date: Mon, 25 Mar 2024 12:15:10 GMT   (1633kb)

Title: Enhancing Graph Representation Learning with Attention-Driven Spiking
  Neural Networks
Authors: Huifeng Yin, Mingkun Xu, Jing Pei, Lei Deng
Categories: cs.AI cs.LG cs.NE
\\
  Graph representation learning has become a crucial task in machine learning
and data mining due to its potential for modeling complex structures such as
social networks, chemical compounds, and biological systems. Spiking neural
networks (SNNs) have recently emerged as a promising alternative to traditional
neural networks for graph learning tasks, benefiting from their ability to
efficiently encode and process temporal and spatial information. In this paper,
we propose a novel approach that integrates attention mechanisms with SNNs to
improve graph representation learning. Specifically, we introduce an attention
mechanism for SNN that can selectively focus on important nodes and
corresponding features in a graph during the learning process. We evaluate our
proposed method on several benchmark datasets and show that it achieves
comparable performance compared to existing graph learning techniques.
\\ ( https://arxiv.org/abs/2403.17040 ,  1633kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17101
Date: Mon, 25 Mar 2024 18:38:54 GMT   (1010kb)

Title: AI Consciousness is Inevitable: A Theoretical Computer Science
  Perspective
Authors: Lenore Blum and Manuel Blum
Categories: cs.AI
MSC-class: 68T01
ACM-class: F.1; I.2
\\
  We look at consciousness through the lens of Theoretical Computer Science, a
branch of mathematics that studies computation under resource limitations. From
this perspective, we develop a formal machine model for consciousness. The
model is inspired by Alan Turing's simple yet powerful model of computation and
Bernard Baars' theater model of consciousness. Though extremely simple, the
model aligns at a high level with many of the major scientific theories of
human and animal consciousness, supporting our claim that machine consciousness
is inevitable.
\\ ( https://arxiv.org/abs/2403.17101 ,  1010kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17108
Date: Mon, 25 Mar 2024 18:46:13 GMT   (1458kb,D)

Title: Graph Protection under Multiple Simultaneous Attacks: A Heuristic
  Approach
Authors: Marko Djukanovic, Stefan Kapunac, Aleksandar Kartelj, Dragan Matic
Categories: cs.AI
Comments: 32 pages, 10 figures
\\
  This work focuses on developing an effective meta-heuristic approach to
protect against simultaneous attacks on nodes of a network modeled using a
graph. Specifically, we focus on the $k$-strong Roman domination problem, a
generalization of the well-known Roman domination problem on graphs. This
general problem is about assigning integer weights to nodes that represent the
number of field armies stationed at each node in order to satisfy the
protection constraints while minimizing the total weights. These constraints
concern the protection of a graph against any simultaneous attack consisting of
$k \in \mathbb{N}$ nodes. An attack is considered repelled if each node labeled
0 can be defended by borrowing an army from one of its neighboring nodes,
ensuring that the neighbor retains at least one army for self-defense. The
$k$-SRD problem has practical applications in various areas, such as developing
counter-terrorism strategies or managing supply chain disruptions. The solution
to this problem is notoriously difficult to find, as even checking the
feasibility of the proposed solution requires an exponential number of steps.
We propose a variable neighborhood search algorithm in which the feasibility of
the solution is checked by introducing the concept of quasi-feasibility, which
is realized by careful sampling within the set of all possible attacks.
Extensive experimental evaluations show the scalability and robustness of the
proposed approach compared to the two exact approaches from the literature.
Experiments are conducted with random networks from the literature and newly
introduced random wireless networks as well as with real-world networks. A
practical application scenario, using real-world networks, involves applying
our approach to graphs extracted from GeoJSON files containing geographic
features of hundreds of cities or larger regions.
\\ ( https://arxiv.org/abs/2403.17108 ,  1458kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17209
Date: Mon, 25 Mar 2024 21:37:30 GMT   (1372kb)

Title: Generation of Asset Administration Shell with Large Language Model
  Agents: Interoperability in Digital Twins with Semantic Node
Authors: Yuchen Xia, Zhewen Xiao, Nasser Jazdi and Michael Weyrich
Categories: cs.AI cs.IR cs.MA cs.SE
Comments: Pre-print, submitted to IEEE ACCESS, under peer-review
\\
  This research introduces a novel approach for assisting the creation of Asset
Administration Shell (AAS) instances for digital twin modeling within the
context of Industry 4.0, aiming to enhance interoperability in smart
manufacturing and reduce manual effort. We construct a "semantic node" data
structure to capture the semantic essence of textual data. Then, a system
powered by large language models is designed and implemented to process
"semantic node" and generate AAS instance models from textual technical data.
Our evaluation demonstrates a 62-79% effective generation rate, indicating a
substantial proportion of manual creation effort can be converted into easier
validation effort, thereby reducing the time and cost in creating AAS instance
models. In our evaluation, a comparative analysis of different LLMs and an
in-depth ablation study of Retrieval-Augmented Generation (RAG) mechanisms
provide insights into the effectiveness of LLM systems for interpreting
technical concepts. Our findings emphasize LLMs' capability in automating AAS
instance creation, enhancing semantic interoperability, and contributing to the
broader field of semantic interoperability for digital twins in industrial
applications. The prototype implementation and evaluation results are released
on our GitHub Repository with the link: https://github.com/YuchenXia/AASbyLLM
\\ ( https://arxiv.org/abs/2403.17209 ,  1372kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17234
Date: Mon, 25 Mar 2024 22:21:23 GMT   (3631kb,D)

Title: Speeding Up Path Planning via Reinforcement Learning in MCTS for
  Automated Parking
Authors: Xinlong Zheng, Xiaozhou Zhang, Donghao Xu
Categories: cs.AI cs.RO
\\
  In this paper, we address a method that integrates reinforcement learning
into the Monte Carlo tree search to boost online path planning under fully
observable environments for automated parking tasks. Sampling-based planning
methods under high-dimensional space can be computationally expensive and
time-consuming. State evaluation methods are useful by leveraging the prior
knowledge into the search steps, making the process faster in a real-time
system. Given the fact that automated parking tasks are often executed under
complex environments, a solid but lightweight heuristic guidance is challenging
to compose in a traditional analytical way. To overcome this limitation, we
propose a reinforcement learning pipeline with a Monte Carlo tree search under
the path planning framework. By iteratively learning the value of a state and
the best action among samples from its previous cycle's outcomes, we are able
to model a value estimator and a policy generator for given states. By doing
that, we build up a balancing mechanism between exploration and exploitation,
speeding up the path planning process while maintaining its quality without
using human expert driver data.
\\ ( https://arxiv.org/abs/2403.17234 ,  3631kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17246
Date: Mon, 25 Mar 2024 22:47:13 GMT   (173kb,D)

Title: TwoStep: Multi-agent Task Planning using Classical Planners and Large
  Language Models
Authors: Ishika Singh, David Traum, Jesse Thomason
Categories: cs.AI cs.CL cs.MA cs.RO
Comments: 12 pages
\\
  Classical planning formulations like the Planning Domain Definition Language
(PDDL) admit action sequences guaranteed to achieve a goal state given an
initial state if any are possible. However, reasoning problems defined in PDDL
do not capture temporal aspects of action taking, for example that two agents
in the domain can execute an action simultaneously if postconditions of each do
not interfere with preconditions of the other. A human expert can decompose a
goal into largely independent constituent parts and assign each agent to one of
these subgoals to take advantage of simultaneous actions for faster execution
of plan steps, each using only single agent planning. By contrast, large
language models (LLMs) used for directly inferring plan steps do not guarantee
execution success, but do leverage commonsense reasoning to assemble action
sequences. We combine the strengths of classical planning and LLMs by
approximating human intuitions for two-agent planning goal decomposition. We
demonstrate that LLM-based goal decomposition leads to faster planning times
than solving multi-agent PDDL problems directly while simultaneously achieving
fewer plan execution steps than a single agent plan alone and preserving
execution success. Additionally, we find that LLM-based approximations of
subgoals can achieve similar multi-agent execution steps than those specified
by human experts. Website and resources at https://glamor-usc.github.io/twostep
\\ ( https://arxiv.org/abs/2403.17246 ,  173kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17247
Date: Mon, 25 Mar 2024 22:49:56 GMT   (433kb,D)

Title: DASA: Delay-Adaptive Multi-Agent Stochastic Approximation
Authors: Nicolo Dal Fabbro, Arman Adibi, H. Vincent Poor, Sanjeev R. Kulkarni,
  Aritra Mitra and George J. Pappas
Categories: cs.AI cs.RO cs.SY eess.SY math.OC stat.ML
\\
  We consider a setting in which $N$ agents aim to speedup a common Stochastic
Approximation (SA) problem by acting in parallel and communicating with a
central server. We assume that the up-link transmissions to the server are
subject to asynchronous and potentially unbounded time-varying delays. To
mitigate the effect of delays and stragglers while reaping the benefits of
distributed computation, we propose \texttt{DASA}, a Delay-Adaptive algorithm
for multi-agent Stochastic Approximation. We provide a finite-time analysis of
\texttt{DASA} assuming that the agents' stochastic observation processes are
independent Markov chains. Significantly advancing existing results,
\texttt{DASA} is the first algorithm whose convergence rate depends only on the
mixing time $\tmix$ and on the average delay $\tau_{avg}$ while jointly
achieving an $N$-fold convergence speedup under Markovian sampling. Our work is
relevant for various SA applications, including multi-agent and distributed
temporal difference (TD) learning, Q-learning and stochastic optimization with
correlated data.
\\ ( https://arxiv.org/abs/2403.17247 ,  433kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17306
Date: Tue, 26 Mar 2024 01:28:42 GMT   (25437kb,D)

Title: Visual Hallucination: Definition, Quantification, and Prescriptive
  Remediations
Authors: Vipula Rawte, Anku Rani, Harshad Sharma, Neeraj Anand, Krishnav
  Rajbangshi, Amit Sheth, Amitava Das
Categories: cs.AI
\\
  The troubling rise of hallucination presents perhaps the most significant
impediment to the advancement of responsible AI. In recent times, considerable
research has focused on detecting and mitigating hallucination in Large
Language Models (LLMs). However, it's worth noting that hallucination is also
quite prevalent in Vision-Language models (VLMs). In this paper, we offer a
fine-grained discourse on profiling VLM hallucination based on two tasks: i)
image captioning, and ii) Visual Question Answering (VQA). We delineate eight
fine-grained orientations of visual hallucination: i) Contextual Guessing, ii)
Identity Incongruity, iii) Geographical Erratum, iv) Visual Illusion, v) Gender
Anomaly, vi) VLM as Classifier, vii) Wrong Reading, and viii) Numeric
Discrepancy. We curate Visual HallucInation eLiciTation (VHILT), a publicly
available dataset comprising 2,000 samples generated using eight VLMs across
two tasks of captioning and VQA along with human annotations for the categories
as mentioned earlier.
\\ ( https://arxiv.org/abs/2403.17306 ,  25437kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17312
Date: Tue, 26 Mar 2024 01:46:34 GMT   (2972kb,D)

Title: ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV
  Caching
Authors: Youpeng Zhao, Di Wu, Jun Wang
Categories: cs.AI cs.LG cs.PF
Comments: ISCA 2024
\\
  The Transformer architecture has significantly advanced natural language
processing (NLP) and has been foundational in developing large language models
(LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP
tasks. Despite their superior accuracy, LLMs present unique challenges in
practical inference, concerning the compute and memory-intensive nature. Thanks
to the autoregressive characteristic of LLM inference, KV caching for the
attention layers in Transformers can effectively accelerate LLM inference by
substituting quadratic-complexity computation with linear-complexity memory
accesses. Yet, this approach requires increasing memory as demand grows for
processing longer sequences. The overhead leads to reduced throughput due to
I/O bottlenecks and even out-of-memory errors, particularly on
resource-constrained systems like a single commodity GPU. In this paper, we
propose ALISA, a novel algorithm-system co-design solution to address the
challenges imposed by KV caching. On the algorithm level, ALISA prioritizes
tokens that are most important in generating a new token via a Sparse Window
Attention (SWA) algorithm. SWA introduces high sparsity in attention layers and
reduces the memory footprint of KV caching at negligible accuracy loss. On the
system level, ALISA employs three-phase token-level dynamical scheduling and
optimizes the trade-off between caching and recomputation, thus maximizing the
overall performance in resource-constrained systems. In a single GPU-CPU
system, we demonstrate that under varying workloads, ALISA improves the
throughput of baseline systems such as FlexGen and vLLM by up to 3X and 1.9X,
respectively.
\\ ( https://arxiv.org/abs/2403.17312 ,  2972kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17328
Date: Tue, 26 Mar 2024 02:22:08 GMT   (289kb,D)

Title: Learning Traffic Signal Control via Genetic Programming
Authors: Xiao-Cheng Liao, Yi Mei and Mengjie Zhang
Categories: cs.AI cs.NE
\\
  The control of traffic signals is crucial for improving transportation
efficiency. Recently, learning-based methods, especially Deep Reinforcement
Learning (DRL), garnered substantial success in the quest for more efficient
traffic signal control strategies. However, the design of rewards in DRL highly
demands domain knowledge to converge to an effective policy, and the final
policy also presents difficulties in terms of explainability. In this work, a
new learning-based method for signal control in complex intersections is
proposed. In our approach, we design a concept of phase urgency for each signal
phase. During signal transitions, the traffic light control strategy selects
the next phase to be activated based on the phase urgency. We then proposed to
represent the urgency function as an explainable tree structure. The urgency
function can calculate the phase urgency for a specific phase based on the
current road conditions. Genetic programming is adopted to perform
gradient-free optimization of the urgency function. We test our algorithm on
multiple public traffic signal control datasets. The experimental results
indicate that the tree-shaped urgency function evolved by genetic programming
outperforms the baselines, including a state-of-the-art method in the
transportation field and a well-known DRL-based method.
\\ ( https://arxiv.org/abs/2403.17328 ,  289kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17333
Date: Tue, 26 Mar 2024 02:33:36 GMT   (2471kb,D)

Title: The Pursuit of Fairness in Artificial Intelligence Models: A Survey
Authors: Tahsin Alamgir Kheya and Mohamed Reda Bouadjenek and Sunil Aryal
Categories: cs.AI cs.CY cs.LG
Comments: 37 pages, 6 figures
\\
  Artificial Intelligence (AI) models are now being utilized in all facets of
our lives such as healthcare, education and employment. Since they are used in
numerous sensitive environments and make decisions that can be life altering,
potential biased outcomes are a pressing matter. Developers should ensure that
such models don't manifest any unexpected discriminatory practices like
partiality for certain genders, ethnicities or disabled people. With the
ubiquitous dissemination of AI systems, researchers and practitioners are
becoming more aware of unfair models and are bound to mitigate bias in them.
Significant research has been conducted in addressing such issues to ensure
models don't intentionally or unintentionally perpetuate bias. This survey
offers a synopsis of the different ways researchers have promoted fairness in
AI systems. We explore the different definitions of fairness existing in the
current literature. We create a comprehensive taxonomy by categorizing
different types of bias and investigate cases of biased AI in different
application domains. A thorough study is conducted of the approaches and
techniques employed by researchers to mitigate bias in AI models. Moreover, we
also delve into the impact of biased models on user experience and the ethical
considerations to contemplate when developing and deploying such models. We
hope this survey helps researchers and practitioners understand the intricate
details of fairness and bias in AI systems. By sharing this thorough survey, we
aim to promote additional discourse in the domain of equitable and responsible
AI.
\\ ( https://arxiv.org/abs/2403.17333 ,  2471kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17350
Date: Tue, 26 Mar 2024 03:28:02 GMT   (38915kb,D)

Title: The Solution of the Zodiac Killer's 340-Character Cipher
Authors: David Oranchak, Sam Blake, Jarl Van Eycke
Categories: cs.AI cs.CR
\\
  The case of the Zodiac Killer is one of the most widely known unsolved serial
killer cases in history. The unidentified killer murdered five known victims
and terrorized the state of California. He also communicated extensively with
the press and law enforcement. Besides his murders, Zodiac was known for his
use of ciphers. The first Zodiac cipher was solved within a week of its
publication, while the second cipher was solved by the authors after 51 years,
when it was discovered to be a transposition and homophonic substitution cipher
with unusual qualities. In this paper, we detail the historical significance of
this cipher and the numerous efforts which culminated in its solution.
\\ ( https://arxiv.org/abs/2403.17350 ,  38915kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17358
Date: Tue, 26 Mar 2024 03:46:33 GMT   (139kb,D)

Title: Addressing Myopic Constrained POMDP Planning with Recursive Dual Ascent
Authors: Paula Stocco, Suhas Chundi, Arec Jamgochian, Mykel J. Kochenderfer
Categories: cs.AI
Comments: Accepted to the 2024 International Conference on Automated Planning
  and Scheduling (ICAPS)
\\
  Lagrangian-guided Monte Carlo tree search with global dual ascent has been
applied to solve large constrained partially observable Markov decision
processes (CPOMDPs) online. In this work, we demonstrate that these global dual
parameters can lead to myopic action selection during exploration, ultimately
leading to suboptimal decision making. To address this, we introduce
history-dependent dual variables that guide local action selection and are
optimized with recursive dual ascent. We empirically compare the performance of
our approach on a motivating toy example and two large CPOMDPs, demonstrating
improved exploration, and ultimately, safer outcomes.
\\ ( https://arxiv.org/abs/2403.17358 ,  139kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17384
Date: Tue, 26 Mar 2024 05:10:47 GMT   (1554kb,D)

Title: Explainable Graph Neural Networks for Observation Impact Analysis in
  Atmospheric State Estimation
Authors: Hyeon-Ju Jeon, Jeon-Ho Kang, In-Hyuk Kwon, O-Joun Lee
Categories: cs.AI cs.CY
\\
  This paper investigates the impact of observations on atmospheric state
estimation in weather forecasting systems using graph neural networks (GNNs)
and explainability methods. We integrate observation and Numerical Weather
Prediction (NWP) points into a meteorological graph, extracting $k$-hop
subgraphs centered on NWP points. Self-supervised GNNs are employed to estimate
the atmospheric state by aggregating data within these $k$-hop radii. The study
applies gradient-based explainability methods to quantify the significance of
different observations in the estimation process. Evaluated with data from 11
satellite and land-based observations, the results highlight the effectiveness
of visualizing the importance of observation types, enhancing the understanding
and optimization of observational data in weather forecasting.
\\ ( https://arxiv.org/abs/2403.17384 ,  1554kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17395
Date: Tue, 26 Mar 2024 05:25:01 GMT   (226kb,D)

Title: An Open-source End-to-End Logic Optimization Framework for Large-scale
  Boolean Network with Reinforcement Learning
Authors: Zhen Li, Kaixiang Zhu, Xuegong Zhou, Lingli Wang
Categories: cs.AI
Comments: 5 pages, 4 figures, 1 table
\\
  We propose an open-source end-to-end logic optimization framework for
large-scale boolean network with reinforcement learning.
\\ ( https://arxiv.org/abs/2403.17395 ,  226kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17419
Date: Tue, 26 Mar 2024 06:18:42 GMT   (8kb)

Title: AI Safety: Necessary, but insufficient and possibly problematic
Authors: Deepak P
Categories: cs.AI cs.CY
Journal-ref: P., Deepak, 'AI safety: necessary, but insufficient and possibly
  problematic'. AI & Soc (2024)
DOI: 10.1007/s00146-024-01899-y
\\
  This article critically examines the recent hype around AI safety. We first
start with noting the nature of the AI safety hype as being dominated by
governments and corporations, and contrast it with other avenues within AI
research on advancing social good. We consider what 'AI safety' actually means,
and outline the dominant concepts that the digital footprint of AI safety
aligns with. We posit that AI safety has a nuanced and uneasy relationship with
transparency and other allied notions associated with societal good, indicating
that it is an insufficient notion if the goal is that of societal good in a
broad sense. We note that the AI safety debate has already influenced some
regulatory efforts in AI, perhaps in not so desirable directions. We also share
our concerns on how AI safety may normalize AI that advances structural harm
through providing exploitative and harmful AI with a veneer of safety.
\\ ( https://arxiv.org/abs/2403.17419 ,  8kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17426
Date: Tue, 26 Mar 2024 06:47:17 GMT   (298kb,D)

Title: Knowledge-Powered Recommendation for an Improved Diet Water Footprint
Authors: Saurav Joshi, Filip Ilievski, Jay Pujara
Categories: cs.AI
Comments: 3 pages, 1 figure, AAAI'24
\\
  According to WWF, 1.1 billion people lack access to water, and 2.7 billion
experience water scarcity at least one month a year. By 2025, two-thirds of the
world's population may be facing water shortages. This highlights the urgency
of managing water usage efficiently, especially in water-intensive sectors like
food. This paper proposes a recommendation engine, powered by knowledge graphs,
aiming to facilitate sustainable and healthy food consumption. The engine
recommends ingredient substitutes in user recipes that improve nutritional
value and reduce environmental impact, particularly water footprint. The system
architecture includes source identification, information extraction, schema
alignment, knowledge graph construction, and user interface development. The
research offers a promising tool for promoting healthier eating habits and
contributing to water conservation efforts.
\\ ( https://arxiv.org/abs/2403.17426 ,  298kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17428
Date: Tue, 26 Mar 2024 06:50:04 GMT   (1486kb,D)

Title: Aligning Large Language Models for Enhancing Psychiatric Interviews
  through Symptom Delineation and Summarization
Authors: Jae-hee So, Joonhwan Chang, Eunji Kim, Junho Na, JiYeon Choi, Jy-yong
  Sohn, Byung-Hoon Kim, Sang Hui Chu
Categories: cs.AI cs.CL
\\
  Recent advancements in Large Language Models (LLMs) have accelerated their
usage in various domains. Given the fact that psychiatric interviews are
goal-oriented and structured dialogues between the professional interviewer and
the interviewee, it is one of the most underexplored areas where LLMs can
contribute substantial value. Here, we explore the use of LLMs for enhancing
psychiatric interviews, by analyzing counseling data from North Korean
defectors with traumatic events and mental health issues. Specifically, we
investigate whether LLMs can (1) delineate the part of the conversation that
suggests psychiatric symptoms and name the symptoms, and (2) summarize
stressors and symptoms, based on the interview dialogue transcript. Here, the
transcript data was labeled by mental health experts for training and
evaluation of LLMs. Our experimental results show that appropriately prompted
LLMs can achieve high performance on both the symptom delineation task and the
summarization task. This research contributes to the nascent field of applying
LLMs to psychiatric interview and demonstrates their potential effectiveness in
aiding mental health practitioners.
\\ ( https://arxiv.org/abs/2403.17428 ,  1486kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17532
Date: Tue, 26 Mar 2024 09:36:59 GMT   (304kb,D)

Title: KC-GenRe: A Knowledge-constrained Generative Re-ranking Method Based on
  Large Language Models for Knowledge Graph Completion
Authors: Yilin Wang, Minghao Hu, Zhen Huang, Dongsheng Li, Dong Yang, Xicheng
  Lu
Categories: cs.AI
Comments: This paper has been accepted for publication in the proceedings of
  LREC-COLING 2024
\\
  The goal of knowledge graph completion (KGC) is to predict missing facts
among entities. Previous methods for KGC re-ranking are mostly built on
non-generative language models to obtain the probability of each candidate.
Recently, generative large language models (LLMs) have shown outstanding
performance on several tasks such as information extraction and dialog systems.
Leveraging them for KGC re-ranking is beneficial for leveraging the extensive
pre-trained knowledge and powerful generative capabilities. However, it may
encounter new problems when accomplishing the task, namely mismatch,
misordering and omission. To this end, we introduce KC-GenRe, a
knowledge-constrained generative re-ranking method based on LLMs for KGC. To
overcome the mismatch issue, we formulate the KGC re-ranking task as a
candidate identifier sorting generation problem implemented by generative LLMs.
To tackle the misordering issue, we develop a knowledge-guided interactive
training method that enhances the identification and ranking of candidates. To
address the omission issue, we design a knowledge-augmented constrained
inference method that enables contextual prompting and controlled generation,
so as to obtain valid rankings. Experimental results show that KG-GenRe
achieves state-of-the-art performance on four datasets, with gains of up to
6.7% and 7.7% in the MRR and Hits@1 metric compared to previous methods, and
9.0% and 11.1% compared to that without re-ranking. Extensive analysis
demonstrates the effectiveness of components in KG-GenRe.
\\ ( https://arxiv.org/abs/2403.17532 ,  304kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17549
Date: Tue, 26 Mar 2024 09:55:49 GMT   (503kb)

Title: Practical Applications of Advanced Cloud Services and Generative AI
  Systems in Medical Image Analysis
Authors: Jingyu Xu, Binbin Wu, Jiaxin Huang, Yulu Gong, Yifan Zhang, Bo Liu
Categories: cs.AI cs.CV
\\
  The medical field is one of the important fields in the application of
artificial intelligence technology. With the explosive growth and
diversification of medical data, as well as the continuous improvement of
medical needs and challenges, artificial intelligence technology is playing an
increasingly important role in the medical field. Artificial intelligence
technologies represented by computer vision, natural language processing, and
machine learning have been widely penetrated into diverse scenarios such as
medical imaging, health management, medical information, and drug research and
development, and have become an important driving force for improving the level
and quality of medical services.The article explores the transformative
potential of generative AI in medical imaging, emphasizing its ability to
generate syntheticACM-2 data, enhance images, aid in anomaly detection, and
facilitate image-to-image translation. Despite challenges like model
complexity, the applications of generative models in healthcare, including
Med-PaLM 2 technology, show promising results. By addressing limitations in
dataset size and diversity, these models contribute to more accurate diagnoses
and improved patient outcomes. However, ethical considerations and
collaboration among stakeholders are essential for responsible implementation.
Through experiments leveraging GANs to augment brain tumor MRI datasets, the
study demonstrates how generative AI can enhance image quality and diversity,
ultimately advancing medical diagnostics and patient care.
\\ ( https://arxiv.org/abs/2403.17549 ,  503kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17601
Date: Tue, 26 Mar 2024 11:13:35 GMT   (2551kb,D)

Title: LASIL: Learner-Aware Supervised Imitation Learning For Long-term
  Microscopic Traffic Simulation
Authors: Ke Guo, Zhenwei Miao, Wei Jing, Weiwei Liu, Weizi Li, Dayang Hao, Jia
  Pan
Categories: cs.AI cs.LG
Comments: accepted by cvpr 2024. arXiv admin note: text overlap with
  arXiv:2306.06401
\\
  Microscopic traffic simulation plays a crucial role in transportation
engineering by providing insights into individual vehicle behavior and overall
traffic flow. However, creating a realistic simulator that accurately
replicates human driving behaviors in various traffic conditions presents
significant challenges. Traditional simulators relying on heuristic models
often fail to deliver accurate simulations due to the complexity of real-world
traffic environments. Due to the covariate shift issue, existing imitation
learning-based simulators often fail to generate stable long-term simulations.
In this paper, we propose a novel approach called learner-aware supervised
imitation learning to address the covariate shift problem in multi-agent
imitation learning. By leveraging a variational autoencoder simultaneously
modeling the expert and learner state distribution, our approach augments
expert states such that the augmented state is aware of learner state
distribution. Our method, applied to urban traffic simulation, demonstrates
significant improvements over existing state-of-the-art baselines in both
short-term microscopic and long-term macroscopic realism when evaluated on the
real-world dataset pNEUMA.
\\ ( https://arxiv.org/abs/2403.17601 ,  2551kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17607
Date: Tue, 26 Mar 2024 11:38:39 GMT   (12104kb,D)

Title: Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs
Authors: Kai Yuan, Christoph Bauinger, Xiangyi Zhang, Pascal Baehr, Matthias
  Kirchhart, Darius Dabert, Adrien Tousnakhoff, Pierre Boudier, Michael
  Paulitsch
Categories: cs.AI
\\
  This paper presents a SYCL implementation of Multi-Layer Perceptrons (MLPs),
which targets and is optimized for the Intel Data Center GPU Max 1550. To
increase the performance, our implementation minimizes the slow global memory
accesses by maximizing the data reuse within the general register file and the
shared local memory by fusing the operations in each layer of the MLP. We show
with a simple roofline model that this results in a significant increase in the
arithmetic intensity, leading to improved performance, especially for
inference. We compare our approach to a similar CUDA implementation for MLPs
and show that our implementation on the Intel Data Center GPU outperforms the
CUDA implementation on Nvidia's H100 GPU by a factor up to 2.84 in inference
and 1.75 in training. The paper also showcases the efficiency of our SYCL
implementation in three significant areas: Image Compression, Neural Radiance
Fields, and Physics-Informed Machine Learning. In all cases, our implementation
outperforms the off-the-shelf Intel Extension for PyTorch (IPEX) implementation
on the same Intel GPU by up to a factor of 30 and the CUDA PyTorch version on
Nvidia's H100 GPU by up to a factor 19. The code can be found at
https://github.com/intel/tiny-dpcpp-nn.
\\ ( https://arxiv.org/abs/2403.17607 ,  12104kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17632
Date: Tue, 26 Mar 2024 12:08:05 GMT   (23196kb,D)

Title: Data-driven Energy Consumption Modelling for Electric Micromobility
  using an Open Dataset
Authors: Yue Ding, Sen Yan, Maqsood Hussain Shah, Hongyuan Fang, Ji Li,
  Mingming Liu
Categories: cs.AI cs.CY cs.LG
Comments: 7 pages, 5 figures, 4 tables. This manuscript has been accepted by
  the IEEE ITEC 2024
\\
  The escalating challenges of traffic congestion and environmental degradation
underscore the critical importance of embracing E-Mobility solutions in urban
spaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes,
play a pivotal role in this transition, offering sustainable alternatives for
urban commuters. However, the energy consumption patterns for these tools are a
critical aspect that impacts their effectiveness in real-world scenarios and is
essential for trip planning and boosting user confidence in using these. To
this effect, recent studies have utilised physical models customised for
specific mobility tools and conditions, but these models struggle with
generalization and effectiveness in real-world scenarios due to a notable
absence of open datasets for thorough model evaluation and verification. To
fill this gap, our work presents an open dataset, collected in Dublin, Ireland,
specifically designed for energy modelling research related to E-Scooters and
E-Bikes. Furthermore, we provide a comprehensive analysis of energy consumption
modelling based on the dataset using a set of representative machine learning
algorithms and compare their performance against the contemporary mathematical
models as a baseline. Our results demonstrate a notable advantage for
data-driven models in comparison to the corresponding mathematical models for
estimating energy consumption. Specifically, data-driven models outperform
physical models in accuracy by up to 83.83% for E-Bikes and 82.16% for
E-Scooters based on an in-depth analysis of the dataset under certain
assumptions.
\\ ( https://arxiv.org/abs/2403.17632 ,  23196kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17643
Date: Tue, 26 Mar 2024 12:23:34 GMT   (2052kb,D)

Title: S+t-SNE - Bringing dimensionality reduction to data streams
Authors: Pedro C. Vieira, Jo\~ao P. Montrezol, Jo\~ao T. Vieira, Jo\~ao Gama
Categories: cs.AI cs.IR
Comments: This preprint has not undergone peer review or any post-submission
  improvements or corrections. We will soon add a link to the final version of
  this contribution that underwent peer-review and post-acceptance improvements
  and was presented at IDA2024 (https://ida2024.org/)
\\
  We present S+t-SNE, an adaptation of the t-SNE algorithm designed to handle
infinite data streams. The core idea behind S+t-SNE is to update the t-SNE
embedding incrementally as new data arrives, ensuring scalability and
adaptability to handle streaming scenarios. By selecting the most important
points at each step, the algorithm ensures scalability while keeping
informative visualisations. Employing a blind method for drift management
adjusts the embedding space, facilitating continuous visualisation of evolving
data dynamics. Our experimental evaluations demonstrate the effectiveness and
efficiency of S+t-SNE. The results highlight its ability to capture patterns in
a streaming scenario. We hope our approach offers researchers and practitioners
a real-time tool for understanding and interpreting high-dimensional data.
\\ ( https://arxiv.org/abs/2403.17643 ,  2052kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17653
Date: Tue, 26 Mar 2024 12:36:11 GMT   (2206kb,D)

Title: An Extension-based Approach for Computing and Verifying Preferences in
  Abstract Argumentation
Authors: Quratul-ain Mahesar, Nir Oren, Wamberto W. Vasconcelos
Categories: cs.AI
\\
  We present an extension-based approach for computing and verifying
preferences in an abstract argumentation system. Although numerous
argumentation semantics have been developed previously for identifying
acceptable sets of arguments from an argumentation framework, there is a lack
of justification behind their acceptability based on implicit argument
preferences. Preference-based argumentation frameworks allow one to determine
what arguments are justified given a set of preferences. Our research considers
the inverse of the standard reasoning problem, i.e., given an abstract
argumentation framework and a set of justified arguments, we compute what the
possible preferences over arguments are. Furthermore, there is a need to verify
(i.e., assess) that the computed preferences would lead to the acceptable sets
of arguments. This paper presents a novel approach and algorithm for
exhaustively computing and enumerating all possible sets of preferences
(restricted to three identified cases) for a conflict-free set of arguments in
an abstract argumentation framework. We prove the soundness, completeness and
termination of the algorithm. The research establishes that preferences are
determined using an extension-based approach after the evaluation phase
(acceptability of arguments) rather than stated beforehand. In this work, we
focus our research study on grounded, preferred and stable semantics. We show
that the complexity of computing sets of preferences is exponential in the
number of arguments, and thus, describe an approximate approach and algorithm
to compute the preferences. Furthermore, we present novel algorithms for
verifying (i.e., assessing) the computed preferences. We provide details of the
implementation of the algorithms (source code has been made available), various
experiments performed to evaluate the algorithms and the analysis of the
results.
\\ ( https://arxiv.org/abs/2403.17653 ,  2206kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17683
Date: Tue, 26 Mar 2024 13:14:18 GMT   (3448kb,D)

Title: Solution for Emotion Prediction Competition of Workshop on Emotionally
  and Culturally Intelligent AI
Authors: Shengdong Xu,Zhouyang Chi,Yang Yang
Categories: cs.AI
\\
  This report provide a detailed description of the method that we explored and
proposed in the WECIA Emotion Prediction Competition (EPC), which predicts a
person's emotion through an artistic work with a comment. The dataset of this
competition is ArtELingo, designed to encourage work on diversity across
languages and cultures. The dataset has two main challenges, namely modal
imbalance problem and language-cultural differences problem. In order to
address this issue, we propose a simple yet effective approach called
single-multi modal with Emotion-Cultural specific prompt(ECSP), which focuses
on using the single modal message to enhance the performance of multimodal
models and a well-designed prompt to reduce cultural differences problem. To
clarify, our approach contains two main blocks:
(1)XLM-R\cite{conneau2019unsupervised} based unimodal model and
X$^2$-VLM\cite{zeng2022x} based multimodal model (2) Emotion-Cultural specific
prompt. Our approach ranked first in the final test with a score of 0.627.
\\ ( https://arxiv.org/abs/2403.17683 ,  3448kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17726
Date: Tue, 26 Mar 2024 14:14:30 GMT   (9379kb,D)

Title: Tiny Models are the Computational Saver for Large Models
Authors: Qingyuan Wang, Barry Cardiff, Antoine Frapp\'e, Benoit Larras, Deepu
  John
Categories: cs.AI
\\
  This paper introduces TinySaver, an early-exit-like dynamic model compression
approach which employs tiny models to substitute large models adaptively.
Distinct from traditional compression techniques, dynamic methods like
TinySaver can leverage the difficulty differences to allow certain inputs to
complete their inference processes early, thereby conserving computational
resources. Most existing early exit designs are implemented by attaching
additional network branches to the model's backbone. Our study, however,
reveals that completely independent tiny models can replace a substantial
portion of the larger models' job with minimal impact on performance. Employing
them as the first exit can remarkably enhance computational efficiency. By
searching and employing the most appropriate tiny model as the computational
saver for a given large model, the proposed approaches work as a novel and
generic method to model compression. This finding will help the research
community in exploring new compression methods to address the escalating
computational demands posed by rapidly evolving AI models. Our evaluation of
this approach in ImageNet-1k classification demonstrates its potential to
reduce the number of compute operations by up to 90%, with only negligible
losses in performance, across various modern vision models. The code of this
work will be available.
\\ ( https://arxiv.org/abs/2403.17726 ,  9379kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17735
Date: Tue, 26 Mar 2024 14:24:01 GMT   (283kb,D)

Title: Out-of-distribution Rumor Detection via Test-Time Adaptation
Authors: Xiang Tao,Mingqing Zhang,Qiang Liu,Shu Wu,Liang Wang
Categories: cs.AI
\\
  Due to the rapid spread of rumors on social media, rumor detection has become
an extremely important challenge. Existing methods for rumor detection have
achieved good performance, as they have collected enough corpus from the same
data distribution for model training. However, significant distribution shifts
between the training data and real-world test data occur due to differences in
news topics, social media platforms, languages and the variance in propagation
scale caused by news popularity. This leads to a substantial decline in the
performance of these existing methods in Out-Of-Distribution (OOD) situations.
To address this problem, we propose a simple and efficient method named
Test-time Adaptation for Rumor Detection under distribution shifts (TARD). This
method models the propagation of news in the form of a propagation graph, and
builds propagation graph test-time adaptation framework, enhancing the model's
adaptability and robustness when facing OOD problems. Extensive experiments
conducted on two group datasets collected from real-world social platforms
demonstrate that our framework outperforms the state-of-the-art methods in
performance.
\\ ( https://arxiv.org/abs/2403.17735 ,  283kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17742
Date: Tue, 26 Mar 2024 14:30:23 GMT   (12057kb,D)

Title: Using Stratified Sampling to Improve LIME Image Explanations
Authors: Muhammad Rashid, Elvio G. Amparore, Enrico Ferrari, Damiano Verda
Categories: cs.AI
\\
  We investigate the use of a stratified sampling approach for LIME Image, a
popular model-agnostic explainable AI method for computer vision tasks, in
order to reduce the artifacts generated by typical Monte Carlo sampling. Such
artifacts are due to the undersampling of the dependent variable in the
synthetic neighborhood around the image being explained, which may result in
inadequate explanations due to the impossibility of fitting a linear regressor
on the sampled data. We then highlight a connection with the Shapley theory,
where similar arguments about undersampling and sample relevance were suggested
in the past. We derive all the formulas and adjustment factors required for an
unbiased stratified sampling estimator. Experiments show the efficacy of the
proposed approach.
\\ ( https://arxiv.org/abs/2403.17742 ,  12057kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17755
Date: Tue, 26 Mar 2024 14:44:51 GMT   (2309kb,D)

Title: DataCook: Crafting Anti-Adversarial Examples for Healthcare Data
  Copyright Protection
Authors: Sihan Shang and Jiancheng Yang and Zhenglong Sun and Pascal Fua
Categories: cs.AI cs.CR cs.CV
\\
  In the realm of healthcare, the challenges of copyright protection and
unauthorized third-party misuse are increasingly significant. Traditional
methods for data copyright protection are applied prior to data distribution,
implying that models trained on these data become uncontrollable. This paper
introduces a novel approach, named DataCook, designed to safeguard the
copyright of healthcare data during the deployment phase. DataCook operates by
"cooking" the raw data before distribution, enabling the development of models
that perform normally on this processed data. However, during the deployment
phase, the original test data must be also "cooked" through DataCook to ensure
normal model performance. This process grants copyright holders control over
authorization during the deployment phase. The mechanism behind DataCook is by
crafting anti-adversarial examples (AntiAdv), which are designed to enhance
model confidence, as opposed to standard adversarial examples (Adv) that aim to
confuse models. Similar to Adv, AntiAdv introduces imperceptible perturbations,
ensuring that the data processed by DataCook remains easily understandable. We
conducted extensive experiments on MedMNIST datasets, encompassing both 2D/3D
data and the high-resolution variants. The outcomes indicate that DataCook
effectively meets its objectives, preventing models trained on AntiAdv from
analyzing unauthorized data effectively, without compromising the validity and
accuracy of the data in legitimate scenarios. Code and data are available at
https://github.com/MedMNIST/DataCook.
\\ ( https://arxiv.org/abs/2403.17755 ,  2309kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17778
Date: Tue, 26 Mar 2024 15:11:18 GMT   (115kb,D)

Title: Towards a FAIR Documentation of Workflows and Models in Applied
  Mathematics
Authors: Marco Reidelbach, Bj\"orn Schembera, Marcus Weber
Categories: cs.AI cs.DB cs.DL
ACM-class: H.3.3; H.3.7; E.0
\\
  Modeling-Simulation-Optimization workflows play a fundamental role in applied
mathematics. The Mathematical Research Data Initiative, MaRDI, responded to
this by developing a FAIR and machine-interpretable template for a
comprehensive documentation of such workflows. MaRDMO, a Plugin for the
Research Data Management Organiser, enables scientists from diverse fields to
document and publish their workflows on the MaRDI Portal seamlessly using the
MaRDI template. Central to these workflows are mathematical models. MaRDI
addresses them with the MathModDB ontology, offering a structured formal model
description. Here, we showcase the interaction between MaRDMO and the MathModDB
Knowledge Graph through an algebraic modeling workflow from the Digital
Humanities. This demonstration underscores the versatility of both services
beyond their original numerical domain.
\\ ( https://arxiv.org/abs/2403.17778 ,  115kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17787
Date: Tue, 26 Mar 2024 15:20:49 GMT   (2366kb,D)

Title: Evaluating the Efficacy of Prompt-Engineered Large Multimodal Models
  Versus Fine-Tuned Vision Transformers in Image-Based Security Applications
Authors: Fouad Trad and Ali Chehab
Categories: cs.AI cs.CR cs.CV
\\
  The success of Large Language Models (LLMs) has led to a parallel rise in the
development of Large Multimodal Models (LMMs), such as Gemini-pro, which have
begun to transform a variety of applications. These sophisticated multimodal
models are designed to interpret and analyze complex data, integrating both
textual and visual information on a scale previously unattainable, opening new
avenues for a range of applications. This paper investigates the applicability
and effectiveness of prompt-engineered Gemini-pro LMMs versus fine-tuned Vision
Transformer (ViT) models in addressing critical security challenges. We focus
on two distinct tasks: a visually evident task of detecting simple triggers,
such as small squares in images, indicative of potential backdoors, and a
non-visually evident task of malware classification through visual
representations. Our results highlight a significant divergence in performance,
with Gemini-pro falling short in accuracy and reliability when compared to
fine-tuned ViT models. The ViT models, on the other hand, demonstrate
exceptional accuracy, achieving near-perfect performance on both tasks. This
study not only showcases the strengths and limitations of prompt-engineered
LMMs in cybersecurity applications but also emphasizes the unmatched efficacy
of fine-tuned ViT models for precise and dependable tasks.
\\ ( https://arxiv.org/abs/2403.17787 ,  2366kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17814
Date: Tue, 26 Mar 2024 15:52:36 GMT   (2653kb,D)

Title: D-PAD: Deep-Shallow Multi-Frequency Patterns Disentangling for Time
  Series Forecasting
Authors: Xiaobing Yuan and Ling Chen
Categories: cs.AI
\\
  In time series forecasting, effectively disentangling intricate temporal
patterns is crucial. While recent works endeavor to combine decomposition
techniques with deep learning, multiple frequencies may still be mixed in the
decomposed components, e.g., trend and seasonal. Furthermore, frequency domain
analysis methods, e.g., Fourier and wavelet transforms, have limitations in
resolution in the time domain and adaptability. In this paper, we propose
D-PAD, a deep-shallow multi-frequency patterns disentangling neural network for
time series forecasting. Specifically, a multi-component decomposing (MCD)
block is introduced to decompose the series into components with different
frequency ranges, corresponding to the "shallow" aspect. A
decomposition-reconstruction-decomposition (D-R-D) module is proposed to
progressively extract the information of frequencies mixed in the components,
corresponding to the "deep" aspect. After that, an interaction and fusion (IF)
module is used to further analyze the components. Extensive experiments on
seven real-world datasets demonstrate that D-PAD achieves the state-of-the-art
performance, outperforming the best baseline by an average of 9.48% and 7.15%
in MSE and MAE, respectively.
\\ ( https://arxiv.org/abs/2403.17814 ,  2653kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17826
Date: Tue, 26 Mar 2024 16:06:33 GMT   (66kb)

Title: On the Computational Complexity of Stackelberg Planning and
  Meta-Operator Verification: Technical Report
Authors: Gregor Behnke, Marcel Steinmetz
Categories: cs.AI
Comments: Presented at ICAPS24
\\
  Stackelberg planning is a recently introduced single-turn two-player
adversarial planning model, where two players are acting in a joint classical
planning task, the objective of the first player being hampering the second
player from achieving its goal. This places the Stackelberg planning problem
somewhere between classical planning and general combinatorial two-player
games. But, where exactly? All investigations of Stackelberg planning so far
focused on practical aspects. We close this gap by conducting the first
theoretical complexity analysis of Stackelberg planning. We show that in
general Stackelberg planning is actually no harder than classical planning.
Under a polynomial plan-length restriction, however, Stackelberg planning is a
level higher up in the polynomial complexity hierarchy, suggesting that
compilations into classical planning come with a worst-case exponential
plan-length increase. In attempts to identify tractable fragments, we further
study its complexity under various planning task restrictions, showing that
Stackelberg planning remains intractable where classical planning is not. We
finally inspect the complexity of meta-operator verification, a problem that
has been recently connected to Stackelberg planning.
\\ ( https://arxiv.org/abs/2403.17826 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17873
Date: Tue, 26 Mar 2024 17:02:42 GMT   (41kb)

Title: Addressing Social Misattributions of Large Language Models: An
  HCXAI-based Approach
Authors: Andrea Ferrario, Alberto Termine, Alessandro Facchini
Categories: cs.AI
Comments: Extended version of the manuscript accepted for the ACM CHI Workshop
  on Human-Centered Explainable AI 2024 (HCXAI24)
\\
  Human-centered explainable AI (HCXAI) advocates for the integration of social
aspects into AI explanations. Central to the HCXAI discourse is the Social
Transparency (ST) framework, which aims to make the socio-organizational
context of AI systems accessible to their users. In this work, we suggest
extending the ST framework to address the risks of social misattributions in
Large Language Models (LLMs), particularly in sensitive areas like mental
health. In fact LLMs, which are remarkably capable of simulating roles and
personas, may lead to mismatches between designers' intentions and users'
perceptions of social attributes, risking to promote emotional manipulation and
dangerous behaviors, cases of epistemic injustice, and unwarranted trust. To
address these issues, we propose enhancing the ST framework with a fifth
'W-question' to clarify the specific social attributions assigned to LLMs by
its designers and users. This addition aims to bridge the gap between LLM
capabilities and user perceptions, promoting the ethically responsible
development and use of LLM-based technology.
\\ ( https://arxiv.org/abs/2403.17873 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17914
Date: Tue, 26 Mar 2024 17:51:06 GMT   (2181kb,D)

Title: Hierarchical Multi-label Classification for Fine-level Event Extraction
  from Aviation Accident Reports
Authors: Xinyu Zhao, Hao Yan, Yongming Liu
Categories: cs.AI
Comments: Accepted in INFORMS Journal of Data Science
\\
  A large volume of accident reports is recorded in the aviation domain, which
greatly values improving aviation safety. To better use those reports, we need
to understand the most important events or impact factors according to the
accident reports. However, the increasing number of accident reports requires
large efforts from domain experts to label those reports. In order to make the
labeling process more efficient, many researchers have started developing
algorithms to identify the underlying events from accident reports
automatically. This article argues that we can identify the events more
accurately by leveraging the event taxonomy. More specifically, we consider the
problem a hierarchical classification task where we first identify the
coarse-level information and then predict the fine-level information. We
achieve this hierarchical classification process by incorporating a novel
hierarchical attention module into BERT. To further utilize the information
from event taxonomy, we regularize the proposed model according to the
relationship and distribution among labels. The effectiveness of our framework
is evaluated with the data collected by National Transportation Safety Board
(NTSB). It has been shown that fine-level prediction accuracy is highly
improved, and the regularization term can be beneficial to the rare event
identification problem.
\\ ( https://arxiv.org/abs/2403.17914 ,  2181kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17918
Date: Tue, 26 Mar 2024 17:54:15 GMT   (2926kb,D)

Title: AgentStudio: A Toolkit for Building General Virtual Agents
Authors: Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang, Bo An,
  Shuicheng Yan
Categories: cs.AI
\\
  Creating autonomous virtual agents capable of using arbitrary software on any
digital device remains a major challenge for artificial intelligence. Two key
obstacles hinder progress: insufficient infrastructure for building virtual
agents in real-world environments, and the need for in-the-wild evaluation of
fundamental agent abilities. To address this, we introduce AgentStudio, an
online, realistic, and multimodal toolkit that covers the entire lifecycle of
agent development. This includes environment setups, data collection, agent
evaluation, and visualization. The observation and action spaces are highly
generic, supporting both function calling and human-computer interfaces. This
versatility is further enhanced by AgentStudio's graphical user interfaces,
which allow efficient development of datasets and benchmarks in real-world
settings. To illustrate, we introduce a visual grounding dataset and a
real-world benchmark suite, both created with our graphical interfaces.
Furthermore, we present several actionable insights derived from AgentStudio,
e.g., general visual grounding, open-ended tool creation, learning from videos,
etc. We have open-sourced the environments, datasets, benchmarks, and
interfaces to promote research towards developing general virtual agents for
the future.
\\ ( https://arxiv.org/abs/2403.17918 ,  2926kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17104
Date: Mon, 25 Mar 2024 18:41:47 GMT   (8448kb,D)

Title: Attribute First, then Generate: Locally-attributable Grounded Text
  Generation
Authors: Aviv Slobodkin, Eran Hirsch, Arie Cattan, Tal Schuster, Ido Dagan
Categories: cs.CL
\\
  Recent efforts to address hallucinations in Large Language Models (LLMs) have
focused on attributed text generation, which supplements generated texts with
citations of supporting sources for post-generation fact-checking and
corrections. Yet, these citations often point to entire documents or
paragraphs, burdening users with extensive verification work. In this paper, we
introduce a locally-attributable text generation approach, prioritizing concise
attributions. Our method, named ``Attribute First, then Generate'', breaks down
the conventional end-to-end generation process into three intuitive steps:
content selection, sentence planning, and sequential sentence generation. By
initially identifying relevant source segments (``select first'') and then
conditioning the generation process on them (``then generate''), we ensure
these segments also act as the output's fine-grained attributions (``select''
becomes ``attribute''). Tested on Multi-document Summarization and Long-form
Question-answering, our method not only yields more concise citations than the
baselines but also maintains - and in some cases enhances - both generation
quality and attribution accuracy. Furthermore, it significantly reduces the
time required for fact verification by human assessors.
\\ ( https://arxiv.org/abs/2403.17104 ,  8448kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17125
Date: Mon, 25 Mar 2024 19:07:32 GMT   (3962kb,D)

Title: The Strong Pull of Prior Knowledge in Large Language Models and Its
  Impact on Emotion Recognition
Authors: Georgios Chochlakis, Alexandros Potamianos, Kristina Lerman, Shrikanth
  Narayanan
Categories: cs.CL cs.AI
Comments: 30 pages, 27 figures
\\
  In-context Learning (ICL) has emerged as a powerful paradigm for performing
natural language tasks with Large Language Models (LLM) without updating the
models' parameters, in contrast to the traditional gradient-based finetuning.
The promise of ICL is that the LLM can adapt to perform the present task at a
competitive or state-of-the-art level at a fraction of the cost. The ability of
LLMs to perform tasks in this few-shot manner relies on their background
knowledge of the task (or task priors). However, recent work has found that,
unlike traditional learning, LLMs are unable to fully integrate information
from demonstrations that contrast task priors. This can lead to performance
saturation at suboptimal levels, especially for subjective tasks such as
emotion recognition, where the mapping from text to emotions can differ widely
due to variability in human annotations. In this work, we design experiments
and propose measurements to explicitly quantify the consistency of proxies of
LLM priors and their pull on the posteriors. We show that LLMs have strong yet
inconsistent priors in emotion recognition that ossify their predictions. We
also find that the larger the model, the stronger these effects become. Our
results suggest that caution is needed when using ICL with larger LLMs for
affect-centered tasks outside their pre-training domain and when interpreting
ICL results.
\\ ( https://arxiv.org/abs/2403.17125 ,  3962kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17135
Date: Mon, 25 Mar 2024 19:17:59 GMT   (990kb)

Title: Exploring the Generalization of Cancer Clinical Trial Eligibility
  Classifiers Across Diseases
Authors: Yumeng Yang, Ashley Gilliam, Ethan B Ludmir, Kirk Roberts
Categories: cs.CL cs.LG q-bio.QM
\\
  Clinical trials are pivotal in medical research, and NLP can enhance their
success, with application in recruitment. This study aims to evaluate the
generalizability of eligibility classification across a broad spectrum of
clinical trials. Starting with phase 3 cancer trials, annotated with seven
eligibility exclusions, then to determine how well models can generalize to
non-cancer and non-phase 3 trials. To assess this, we have compiled eligibility
criteria data for five types of trials: (1) additional phase 3 cancer trials,
(2) phase 1 and 2 cancer trials, (3) heart disease trials, (4) type 2 diabetes
trials, and (5) observational trials for any disease, comprising 2,490
annotated eligibility criteria across seven exclusion types. Our results show
that models trained on the extensive cancer dataset can effectively handle
criteria commonly found in non-cancer trials, such as autoimmune diseases.
However, they struggle with criteria disproportionately prevalent in cancer
trials, like prior malignancy. We also experiment with few-shot learning,
demonstrating that a limited number of disease-specific examples can partially
overcome this performance gap. We are releasing this new dataset of annotated
eligibility statements to promote the development of cross-disease
generalization in clinical trial classification.
\\ ( https://arxiv.org/abs/2403.17135 ,  990kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17141
Date: Mon, 25 Mar 2024 19:28:10 GMT   (4568kb,D)

Title: MetaAligner: Conditional Weak-to-Strong Correction for Generalizable
  Multi-Objective Alignment of Language Models
Authors: Kailai Yang, Zhiwei Liu, Qianqian Xie, Tianlin Zhang, Nirui Song,
  Jimin Huang, Ziyan Kuang, Sophia Ananiadou
Categories: cs.CL cs.AI
Comments: Work in progress, more general experimental results to come
\\
  Recent advancements in large language models (LLMs) aim to tackle
heterogeneous human expectations and values via multi-objective preference
alignment. However, existing methods are parameter-adherent to the policy
model, leading to two key limitations: (1) the high-cost repetition of their
alignment algorithms for each new target model; (2) they cannot expand to
unseen objectives due to their static alignment objectives. In this work, we
propose Meta-Objective Aligner (MetaAligner), a model that performs conditional
weak-to-strong correction for weak responses to approach strong responses.
MetaAligner is the first policy-agnostic and generalizable method for
multi-objective preference alignment, which enables plug-and-play alignment by
decoupling parameter updates from the policy models and facilitates zero-shot
preference alignment for unseen objectives via in-context learning.
Experimental results show that MetaAligner achieves significant and balanced
improvements in multi-objective alignments on 11 policy models with up to 63x
more parameters, and outperforms previous alignment methods with down to 22.27x
less computational resources. The model also accurately aligns with unseen
objectives, marking the first step towards generalizable multi-objective
preference alignment.
\\ ( https://arxiv.org/abs/2403.17141 ,  4568kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17143
Date: Mon, 25 Mar 2024 19:40:26 GMT   (8280kb)

Title: Guided Distant Supervision for Multilingual Relation Extraction Data:
  Adapting to a New Language
Authors: Alistair Plum, Tharindu Ranasinghe, Christoph Purschke
Categories: cs.CL cs.LG
Comments: Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)
\\
  Relation extraction is essential for extracting and understanding
biographical information in the context of digital humanities and related
subjects. There is a growing interest in the community to build datasets
capable of training machine learning models to extract relationships. However,
annotating such datasets can be expensive and time-consuming, in addition to
being limited to English. This paper applies guided distant supervision to
create a large biographical relationship extraction dataset for German. Our
dataset, composed of more than 80,000 instances for nine relationship types, is
the largest biographical German relationship extraction dataset. We also create
a manually annotated dataset with 2000 instances to evaluate the models and
release it together with the dataset compiled using guided distant supervision.
We train several state-of-the-art machine learning models on the automatically
created dataset and release them as well. Furthermore, we experiment with
multilingual and cross-lingual experiments that could benefit many low-resource
languages.
\\ ( https://arxiv.org/abs/2403.17143 ,  8280kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17146
Date: Mon, 25 Mar 2024 19:44:06 GMT   (3160kb,D)

Title: Outcome-Constrained Large Language Models for Countering Hate Speech
Authors: Lingzi Hong, Pengcheng Luo, Eduardo Blanco, Xiaoying Song
Categories: cs.CL
\\
  Counterspeech that challenges or responds to hate speech has been seen as an
alternative to mitigate the negative impact of hate speech and foster
productive online communications. Research endeavors have been directed to
using language models for the automatic generation of counterspeech to assist
efforts in combating online hate. Existing research focuses on the generation
of counterspeech with certain linguistic attributes, such as being polite,
informative, and intent-driven. However, it remains unclear what impact the
counterspeech might have in an online environment. We first explore methods
that utilize large language models (LLM) to generate counterspeech constrained
by potential conversation outcomes. We build two conversation outcome
classifiers that predict the incivility level and the hater reentry behavior
following replies to hate with Reddit data, then propose four methods to
incorporate the desired outcomes, i.e., low conversation incivility and
non-hateful hater reentry, into the text generation process, including Prompt
with Instructions, Prompt and Select, LLM finetune, and LLM transformer
reinforcement learning (TRL). Evaluation results show effective strategies to
generate outcome-constrained counterspeech and the linguistic characteristics
of texts generated by different methods.
\\ ( https://arxiv.org/abs/2403.17146 ,  3160kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17155
Date: Mon, 25 Mar 2024 20:12:02 GMT   (1598kb,D)

Title: Task-Agnostic Detector for Insertion-Based Backdoor Attacks
Authors: Weimin Lyu, Xiao Lin, Songzhu Zheng, Lu Pang, Haibin Ling, Susmit Jha,
  Chao Chen
Categories: cs.CL cs.CR
Comments: Findings of NAACL 2024
\\
  Textual backdoor attacks pose significant security threats. Current detection
approaches, typically relying on intermediate feature representation or
reconstructing potential triggers, are task-specific and less effective beyond
sentence classification, struggling with tasks like question answering and
named entity recognition. We introduce TABDet (Task-Agnostic Backdoor
Detector), a pioneering task-agnostic method for backdoor detection. TABDet
leverages final layer logits combined with an efficient pooling technique,
enabling unified logit representation across three prominent NLP tasks. TABDet
can jointly learn from diverse task-specific models, demonstrating superior
detection efficacy over traditional task-specific methods.
\\ ( https://arxiv.org/abs/2403.17155 ,  1598kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17158
Date: Mon, 25 Mar 2024 20:16:14 GMT   (239kb,D)

Title: Reflecting the Male Gaze: Quantifying Female Objectification in 19th and
  20th Century Novels
Authors: Kexin Luo, Yue Mao, Bei Zhang, Sophie Hao
Categories: cs.CL
Comments: To appear in LREC-COLING 2024
\\
  Inspired by the concept of the male gaze (Mulvey, 1975) in literature and
media studies, this paper proposes a framework for analyzing gender bias in
terms of female objectification: the extent to which a text portrays female
individuals as objects of visual pleasure. Our framework measures female
objectification along two axes. First, we compute an agency bias score that
indicates whether male entities are more likely to appear in the text as
grammatical agents than female entities. Next, by analyzing the word embedding
space induced by a text (Caliskan et al., 2017), we compute an appearance bias
score that indicates whether female entities are more closely associated with
appearance-related words than male entities. Applying our framework to 19th and
20th century novels reveals evidence of female objectification in literature:
we find that novels written from a male perspective systematically objectify
female characters, while novels written from a female perspective do not
exhibit statistically significant objectification of any gender.
\\ ( https://arxiv.org/abs/2403.17158 ,  239kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17169
Date: Mon, 25 Mar 2024 20:36:03 GMT   (3423kb,D)

Title: NUMTEMP: A real-world benchmark to verify claims with statistical and
  temporal expressions
Authors: Venktesh V, Abhijit Anand, Avishek Anand, Vinay Setty
Categories: cs.CL cs.AI
Comments: 17 pages, 1 figure
\\
  Automated fact checking has gained immense interest to tackle the growing
misinformation in the digital era. Existing systems primarily focus on
synthetic claims on Wikipedia, and noteworthy progress has also been made on
real-world claims. In this work, we release Numtemp, a diverse, multi-domain
dataset focused exclusively on numerical claims, encompassing temporal,
statistical and diverse aspects with fine-grained metadata and an evidence
collection without leakage. This addresses the challenge of verifying
real-world numerical claims, which are complex and often lack precise
information, not addressed by existing works that mainly focus on synthetic
claims. We evaluate and quantify the limitations of existing solutions for the
task of verifying numerical claims. We also evaluate claim decomposition based
methods, numerical understanding based models and our best baselines achieves a
macro-F1 of 58.32. This demonstrates that Numtemp serves as a challenging
evaluation set for numerical claim verification.
\\ ( https://arxiv.org/abs/2403.17169 ,  3423kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17196
Date: Mon, 25 Mar 2024 21:17:14 GMT   (313kb)

Title: GPT-4 Understands Discourse at Least as Well as Humans Do
Authors: Thomas Shultz, Jamie Wise, Ardavan Salehi Nobandegani
Categories: cs.CL
Comments: 7 pages, 1 figure, 3 tables
\\
  We test whether a leading AI system GPT-4 understands discourse as well as
humans do, using a standardized test of discourse comprehension. Participants
are presented with brief stories and then answer eight yes/no questions probing
their comprehension of the story. The questions are formatted to assess the
separate impacts of directness (stated vs. implied) and salience (main idea vs.
details). GPT-4 performs slightly, but not statistically significantly, better
than humans given the very high level of human performance. Both GPT-4 and
humans exhibit a strong ability to make inferences about information that is
not explicitly stated in a story, a critical test of understanding.
\\ ( https://arxiv.org/abs/2403.17196 ,  313kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17199
Date: Mon, 25 Mar 2024 21:19:50 GMT   (402kb,D)

Title: Extracting Social Support and Social Isolation Information from Clinical
  Psychiatry Notes: Comparing a Rule-based NLP System and a Large Language
  Model
Authors: Braja Gopal Patra, Lauren A. Lepow, Praneet Kasi Reddy Jagadeesh
  Kumar, Veer Vekaria, Mohit Manoj Sharma, Prakash Adekkanattu, Brian Fennessy,
  Gavin Hynes, Isotta Landi, Jorge A. Sanchez-Ruiz, Euijung Ryu, Joanna M.
  Biernacka, Girish N. Nadkarni, Ardesheer Talati, Myrna Weissman, Mark Olfson,
  J. John Mann, Alexander W. Charney, and Jyotishman Pathak
Categories: cs.CL
Comments: 2 figures, 3 tables
\\
  Background: Social support (SS) and social isolation (SI) are social
determinants of health (SDOH) associated with psychiatric outcomes. In
electronic health records (EHRs), individual-level SS/SI is typically
documented as narrative clinical notes rather than structured coded data.
Natural language processing (NLP) algorithms can automate the otherwise
labor-intensive process of data extraction.
  Data and Methods: Psychiatric encounter notes from Mount Sinai Health System
(MSHS, n=300) and Weill Cornell Medicine (WCM, n=225) were annotated and
established a gold standard corpus. A rule-based system (RBS) involving
lexicons and a large language model (LLM) using FLAN-T5-XL were developed to
identify mentions of SS and SI and their subcategories (e.g., social network,
instrumental support, and loneliness).
  Results: For extracting SS/SI, the RBS obtained higher macro-averaged
f-scores than the LLM at both MSHS (0.89 vs. 0.65) and WCM (0.85 vs. 0.82). For
extracting subcategories, the RBS also outperformed the LLM at both MSHS (0.90
vs. 0.62) and WCM (0.82 vs. 0.81).
  Discussion and Conclusion: Unexpectedly, the RBS outperformed the LLMs across
all metrics. Intensive review demonstrates that this finding is due to the
divergent approach taken by the RBS and LLM. The RBS were designed and refined
to follow the same specific rules as the gold standard annotations. Conversely,
the LLM were more inclusive with categorization and conformed to common
English-language understanding. Both approaches offer advantages and are made
available open-source for future testing.
\\ ( https://arxiv.org/abs/2403.17199 ,  402kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17216
Date: Mon, 25 Mar 2024 21:46:35 GMT   (7713kb,D)

Title: Ontology Completion with Natural Language Inference and Concept
  Embeddings: An Analysis
Authors: Na Li, Thomas Bailleux, Zied Bouraoui, Steven Schockaert
Categories: cs.CL
\\
  We consider the problem of finding plausible knowledge that is missing from a
given ontology, as a generalisation of the well-studied taxonomy expansion
task. One line of work treats this task as a Natural Language Inference (NLI)
problem, thus relying on the knowledge captured by language models to identify
the missing knowledge. Another line of work uses concept embeddings to identify
what different concepts have in common, taking inspiration from cognitive
models for category based induction. These two approaches are intuitively
complementary, but their effectiveness has not yet been compared. In this
paper, we introduce a benchmark for evaluating ontology completion methods and
thoroughly analyse the strengths and weaknesses of both approaches. We find
that both approaches are indeed complementary, with hybrid strategies achieving
the best overall results. We also find that the task is highly challenging for
Large Language Models, even after fine-tuning.
\\ ( https://arxiv.org/abs/2403.17216 ,  7713kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17220
Date: Mon, 25 Mar 2024 21:48:36 GMT   (493kb,D)

Title: Making Sentence Embeddings Robust to User-Generated Content
Authors: Lydia Nishimwe, Beno\^it Sagot, Rachel Bawden
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  NLP models have been known to perform poorly on user-generated content (UGC),
mainly because it presents a lot of lexical variations and deviates from the
standard texts on which most of these models were trained. In this work, we
focus on the robustness of LASER, a sentence embedding model, to UGC data. We
evaluate this robustness by LASER's ability to represent non-standard sentences
and their standard counterparts close to each other in the embedding space.
Inspired by previous works extending LASER to other languages and modalities,
we propose RoLASER, a robust English encoder trained using a teacher-student
approach to reduce the distances between the representations of standard and
UGC sentences. We show that with training only on standard and synthetic
UGC-like data, RoLASER significantly improves LASER's robustness to both
natural and artificial UGC data by achieving up to 2x and 11x better scores. We
also perform a fine-grained analysis on artificial UGC data and find that our
model greatly outperforms LASER on its most challenging UGC phenomena such as
keyboard typos and social media abbreviations. Evaluation on downstream tasks
shows that RoLASER performs comparably to or better than LASER on standard
data, while consistently outperforming it on UGC data.
\\ ( https://arxiv.org/abs/2403.17220 ,  493kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17240
Date: Mon, 25 Mar 2024 22:42:19 GMT   (6708kb,D)

Title: The Role of $n$-gram Smoothing in the Age of Neural Networks
Authors: Luca Malagutti, Andrius Buinovskij, Anej Svete, Clara Meister, Afra
  Amini, Ryan Cotterell
Categories: cs.CL
\\
  For nearly three decades, language models derived from the $n$-gram
assumption held the state of the art on the task. The key to their success lay
in the application of various smoothing techniques that served to combat
overfitting. However, when neural language models toppled $n$-gram models as
the best performers, $n$-gram smoothing techniques became less relevant.
Indeed, it would hardly be an understatement to suggest that the line of
inquiry into $n$-gram smoothing techniques became dormant. This paper re-opens
the role classical $n$-gram smoothing techniques may play in the age of neural
language models. First, we draw a formal equivalence between label smoothing, a
popular regularization technique for neural language models, and add-$\lambda$
smoothing. Second, we derive a generalized framework for converting \emph{any}
$n$-gram smoothing technique into a regularizer compatible with neural language
models. Our empirical results find that our novel regularizers are comparable
to and, indeed, sometimes outperform label smoothing on language modeling and
machine translation.
\\ ( https://arxiv.org/abs/2403.17240 ,  6708kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17245
Date: Mon, 25 Mar 2024 22:46:16 GMT   (7898kb,D)

Title: SPLICE: A Singleton-Enhanced PipeLIne for Coreference REsolution
Authors: Yilun Zhu, Siyao Peng, Sameer Pradhan, Amir Zeldes
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  Singleton mentions, i.e.~entities mentioned only once in a text, are
important to how humans understand discourse from a theoretical perspective.
However previous attempts to incorporate their detection in end-to-end neural
coreference resolution for English have been hampered by the lack of singleton
mention spans in the OntoNotes benchmark. This paper addresses this limitation
by combining predicted mentions from existing nested NER systems and features
derived from OntoNotes syntax trees. With this approach, we create a near
approximation of the OntoNotes dataset with all singleton mentions, achieving
~94% recall on a sample of gold singletons. We then propose a two-step neural
mention and coreference resolution system, named SPLICE, and compare its
performance to the end-to-end approach in two scenarios: the OntoNotes test set
and the out-of-domain (OOD) OntoGUM corpus. Results indicate that reconstructed
singleton training yields results comparable to end-to-end systems for
OntoNotes, while improving OOD stability (+1.1 avg. F1). We conduct error
analysis for mention detection and delve into its impact on coreference
clustering, revealing that precision improvements deliver more substantial
benefits than increases in recall for resolving coreference chains.
\\ ( https://arxiv.org/abs/2403.17245 ,  7898kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17254
Date: Mon, 25 Mar 2024 23:02:33 GMT   (1214kb,D)

Title: A Hybrid Approach To Aspect Based Sentiment Analysis Using Transfer
  Learning
Authors: Gaurav Negi, Rajdeep Sarkar, Omnia Zayed and Paul Buitelaar
Categories: cs.CL
\\
  Aspect-Based Sentiment Analysis (ABSA) aims to identify terms or multiword
expressions (MWEs) on which sentiments are expressed and the sentiment
polarities associated with them. The development of supervised models has been
at the forefront of research in this area. However, training these models
requires the availability of manually annotated datasets which is both
expensive and time-consuming. Furthermore, the available annotated datasets are
tailored to a specific domain, language, and text type. In this work, we
address this notable challenge in current state-of-the-art ABSA research. We
propose a hybrid approach for Aspect Based Sentiment Analysis using transfer
learning. The approach focuses on generating weakly-supervised annotations by
exploiting the strengths of both large language models (LLM) and traditional
syntactic dependencies. We utilise syntactic dependency structures of sentences
to complement the annotations generated by LLMs, as they may overlook
domain-specific aspect terms. Extensive experimentation on multiple datasets is
performed to demonstrate the efficacy of our hybrid method for the tasks of
aspect term extraction and aspect sentiment classification.
  Keywords: Aspect Based Sentiment Analysis, Syntactic Parsing, large language
model (LLM)
\\ ( https://arxiv.org/abs/2403.17254 ,  1214kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17281
Date: Tue, 26 Mar 2024 00:09:38 GMT   (946kb,D)

Title: Automate Knowledge Concept Tagging on Math Questions with LLMs
Authors: Hang Li, Tianlong Xu, Jiliang Tang, Qingsong Wen
Categories: cs.CL
Comments: 7 pages, 2 figures
\\
  Knowledge concept tagging for questions plays a crucial role in contemporary
intelligent educational applications, including learning progress diagnosis,
practice question recommendations, and course content organization.
Traditionally, these annotations have been conducted manually with help from
pedagogical experts, as the task requires not only a strong semantic
understanding of both question stems and knowledge definitions but also deep
insights into connecting question-solving logic with corresponding knowledge
concepts. In this paper, we explore automating the tagging task using Large
Language Models (LLMs), in response to the inability of prior manual methods to
meet the rapidly growing demand for concept tagging in questions posed by
advanced educational applications. Moreover, the zero/few-shot learning
capability of LLMs makes them well-suited for application in educational
scenarios, which often face challenges in collecting large-scale,
expertise-annotated datasets. By conducting extensive experiments with a
variety of representative LLMs, we demonstrate that LLMs are a promising tool
for concept tagging in math questions. Furthermore, through case studies
examining the results from different LLMs, we draw some empirical conclusions
about the key factors for success in applying LLMs to the automatic concept
tagging task.
\\ ( https://arxiv.org/abs/2403.17281 ,  946kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17284
Date: Tue, 26 Mar 2024 00:25:01 GMT   (4009kb,D)

Title: Common Ground Tracking in Multimodal Dialogue
Authors: Ibrahim Khebour, Kenneth Lai, Mariah Bradford, Yifan Zhu, Richard
  Brutti, Christopher Tam, Jingxuan Tu, Benjamin Ibarra, Nathaniel Blanchard,
  Nikhil Krishnaswamy, and James Pustejovsky
Categories: cs.CL
\\
  Within Dialogue Modeling research in AI and NLP, considerable attention has
been spent on ``dialogue state tracking'' (DST), which is the ability to update
the representations of the speaker's needs at each turn in the dialogue by
taking into account the past dialogue moves and history. Less studied but just
as important to dialogue modeling, however, is ``common ground tracking''
(CGT), which identifies the shared belief space held by all of the participants
in a task-oriented dialogue: the task-relevant propositions all participants
accept as true. In this paper we present a method for automatically identifying
the current set of shared beliefs and ``questions under discussion'' (QUDs) of
a group with a shared goal. We annotate a dataset of multimodal interactions in
a shared physical space with speech transcriptions, prosodic features,
gestures, actions, and facets of collaboration, and operationalize these
features for use in a deep neural model to predict moves toward construction of
common ground. Model outputs cascade into a set of formal closure rules derived
from situated evidence and belief axioms and update operations. We empirically
assess the contribution of each feature type toward successful construction of
common ground relative to ground truth, establishing a benchmark in this novel,
challenging task.
\\ ( https://arxiv.org/abs/2403.17284 ,  4009kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17297
Date: Tue, 26 Mar 2024 00:53:24 GMT   (3291kb,D)

Title: InternLM2 Technical Report
Authors: Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen,
  Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan,
  Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo,
  Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao,
  Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li,
  Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu,
  Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma,
  Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang,
  Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze
  Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang,
  Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu,
  Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying,
  Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang,
  Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang,
  Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe
  Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, Dahua Lin
Categories: cs.CL cs.AI
\\
  The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has
sparked discussions on the advent of Artificial General Intelligence (AGI).
However, replicating such advancements in open-source models has been
challenging. This paper introduces InternLM2, an open-source LLM that
outperforms its predecessors in comprehensive evaluations across 6 dimensions
and 30 benchmarks, long-context modeling, and open-ended subjective evaluations
through innovative pre-training and optimization techniques. The pre-training
process of InternLM2 is meticulously detailed, highlighting the preparation of
diverse data types including text, code, and long-context data. InternLM2
efficiently captures long-term dependencies, initially trained on 4k tokens
before advancing to 32k tokens in pre-training and fine-tuning stages,
exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack" test.
InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel
Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF)
strategy that addresses conflicting human preferences and reward hacking. By
releasing InternLM2 models in different training stages and model sizes, we
provide the community with insights into the model's evolution.
\\ ( https://arxiv.org/abs/2403.17297 ,  3291kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17299
Date: Tue, 26 Mar 2024 00:56:06 GMT   (10803kb,D)

Title: Decoding Probing: Revealing Internal Linguistic Structures in Neural
  Language Models using Minimal Pairs
Authors: Linyang He, Peili Chen, Ercong Nie, Yuanning Li, Jonathan R. Brennan
Categories: cs.CL q-bio.NC
Comments: Accepted by LREC-COLING 2024
\\
  Inspired by cognitive neuroscience studies, we introduce a novel `decoding
probing' method that uses minimal pairs benchmark (BLiMP) to probe internal
linguistic characteristics in neural language models layer by layer. By
treating the language model as the `brain' and its representations as `neural
activations', we decode grammaticality labels of minimal pairs from the
intermediate layers' representations. This approach reveals: 1) Self-supervised
language models capture abstract linguistic structures in intermediate layers
that GloVe and RNN language models cannot learn. 2) Information about syntactic
grammaticality is robustly captured through the first third layers of GPT-2 and
also distributed in later layers. As sentence complexity increases, more layers
are required for learning grammatical capabilities. 3) Morphological and
semantics/syntax interface-related features are harder to capture than syntax.
4) For Transformer-based models, both embeddings and attentions capture
grammatical features but show distinct patterns. Different attention heads
exhibit similar tendencies toward various linguistic phenomena, but with varied
contributions.
\\ ( https://arxiv.org/abs/2403.17299 ,  10803kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17307
Date: Tue, 26 Mar 2024 01:29:17 GMT   (8803kb,D)

Title: HILL: Hierarchy-aware Information Lossless Contrastive Learning for
  Hierarchical Text Classification
Authors: He Zhu, Junran Wu, Ruomei Liu, Yue Hou, Ze Yuan, Shangzhe Li, Yicheng
  Pan, Ke Xu
Categories: cs.CL cs.IT math.IT
Comments: Accepted by NAACL 2024
\\
  Existing self-supervised methods in natural language processing (NLP),
especially hierarchical text classification (HTC), mainly focus on
self-supervised contrastive learning, extremely relying on human-designed
augmentation rules to generate contrastive samples, which can potentially
corrupt or distort the original information. In this paper, we tend to
investigate the feasibility of a contrastive learning scheme in which the
semantic and syntactic information inherent in the input sample is adequately
reserved in the contrastive samples and fused during the learning process.
Specifically, we propose an information lossless contrastive learning strategy
for HTC, namely \textbf{H}ierarchy-aware \textbf{I}nformation \textbf{L}ossless
contrastive \textbf{L}earning (HILL), which consists of a text encoder
representing the input document, and a structure encoder directly generating
the positive sample. The structure encoder takes the document embedding as
input, extracts the essential syntactic information inherent in the label
hierarchy with the principle of structural entropy minimization, and injects
the syntactic information into the text representation via hierarchical
representation learning. Experiments on three common datasets are conducted to
verify the superiority of HILL.
\\ ( https://arxiv.org/abs/2403.17307 ,  8803kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17308
Date: Tue, 26 Mar 2024 01:29:46 GMT   (15259kb,D)

Title: Neural Multimodal Topic Modeling: A Comprehensive Evaluation
Authors: Felipe Gonz\'alez-Pizarro, Giuseppe Carenini
Categories: cs.CL cs.AI cs.LG
Comments: Camera-Ready for LREC-COLING 2024 (Long Paper)
ACM-class: I.2.7
\\
  Neural topic models can successfully find coherent and diverse topics in
textual data. However, they are limited in dealing with multimodal datasets
(e.g., images and text). This paper presents the first systematic and
comprehensive evaluation of multimodal topic modeling of documents containing
both text and images. In the process, we propose two novel topic modeling
solutions and two novel evaluation metrics. Overall, our evaluation on an
unprecedented rich and diverse collection of datasets indicates that both of
our models generate coherent and diverse topics. Nevertheless, the extent to
which one method outperforms the other depends on the metrics and dataset
combinations, which suggests further exploration of hybrid solutions in the
future. Notably, our succinct human evaluation aligns with the outcomes
determined by our proposed metrics. This alignment not only reinforces the
credibility of our metrics but also highlights the potential for their
application in guiding future multimodal topic modeling endeavors.
\\ ( https://arxiv.org/abs/2403.17308 ,  15259kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17314
Date: Tue, 26 Mar 2024 01:52:59 GMT   (10340kb,D)

Title: Project MOSLA: Recording Every Moment of Second Language Acquisition
Authors: Masato Hagiwara, Joshua Tanner
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  Second language acquisition (SLA) is a complex and dynamic process. Many SLA
studies that have attempted to record and analyze this process have typically
focused on a single modality (e.g., textual output of learners), covered only a
short period of time, and/or lacked control (e.g., failed to capture every
aspect of the learning process). In Project MOSLA (Moments of Second Language
Acquisition), we have created a longitudinal, multimodal, multilingual, and
controlled dataset by inviting participants to learn one of three target
languages (Arabic, Spanish, and Chinese) from scratch over a span of two years,
exclusively through online instruction, and recording every lesson using Zoom.
The dataset is semi-automatically annotated with speaker/language IDs and
transcripts by both human annotators and fine-tuned state-of-the-art speech
models. Our experiments reveal linguistic insights into learners' proficiency
development over time, as well as the potential for automatically detecting the
areas of focus on the screen purely from the unannotated multimodal data. Our
dataset is freely available for research purposes and can serve as a valuable
resource for a wide range of applications, including but not limited to SLA,
proficiency assessment, language and speech processing, pedagogy, and
multimodal learning analytics.
\\ ( https://arxiv.org/abs/2403.17314 ,  10340kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17319
Date: Tue, 26 Mar 2024 02:01:18 GMT   (1091kb,D)

Title: JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue
  Dataset
Authors: Atsumoto Ohashi, Ryu Hirai, Shinya Iizuka, Ryuichiro Higashinaka
Categories: cs.CL cs.AI
Comments: Accepted by LREC-COLING 2024
\\
  Dialogue datasets are crucial for deep learning-based task-oriented dialogue
system research. While numerous English language multi-domain task-oriented
dialogue datasets have been developed and contributed to significant
advancements in task-oriented dialogue systems, such a dataset does not exist
in Japanese, and research in this area is limited compared to that in English.
In this study, towards the advancement of research and development of
task-oriented dialogue systems in Japanese, we constructed JMultiWOZ, the first
Japanese language large-scale multi-domain task-oriented dialogue dataset.
Using JMultiWOZ, we evaluated the dialogue state tracking and response
generation capabilities of the state-of-the-art methods on the existing major
English benchmark dataset MultiWOZ2.2 and the latest large language model
(LLM)-based methods. Our evaluation results demonstrated that JMultiWOZ
provides a benchmark that is on par with MultiWOZ2.2. In addition, through
evaluation experiments of interactive dialogues with the models and human
participants, we identified limitations in the task completion capabilities of
LLMs in Japanese.
\\ ( https://arxiv.org/abs/2403.17319 ,  1091kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17359
Date: Tue, 26 Mar 2024 03:51:01 GMT   (9888kb,D)

Title: Chain-of-Action: Faithful and Multimodal Question Answering through
  Large Language Models
Authors: Zhenyu Pan, Haozheng Luo, Manling Li, Han Liu
Categories: cs.CL
\\
  We present a Chain-of-Action (CoA) framework for multimodal and
retrieval-augmented Question-Answering (QA). Compared to the literature, CoA
overcomes two major challenges of current QA applications: (i) unfaithful
hallucination that is inconsistent with real-time or domain facts and (ii) weak
reasoning performance over compositional information. Our key contribution is a
novel reasoning-retrieval mechanism that decomposes a complex question into a
reasoning chain via systematic prompting and pre-designed actions.
Methodologically, we propose three types of domain-adaptable `Plug-and-Play'
actions for retrieving real-time information from heterogeneous sources. We
also propose a multi-reference faith score (MRFS) to verify and resolve
conflicts in the answers. Empirically, we exploit both public benchmarks and a
Web3 case study to demonstrate the capability of CoA over other methods.
\\ ( https://arxiv.org/abs/2403.17359 ,  9888kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17361
Date: Tue, 26 Mar 2024 03:54:25 GMT   (168kb,D)

Title: Bridging Textual and Tabular Worlds for Fact Verification: A
  Lightweight, Attention-Based Model
Authors: Shirin Dabbaghi Varnosfaderani, Canasai Kruengkrai, Ramin Yahyapour
  and Junichi Yamagishi
Categories: cs.CL cs.AI
Comments: Accepted for a presentation at LREC-COLING 2024 - The 2024 Joint
  International Conference on Computational Linguistics, Language Resources and
  Evaluation
\\
  FEVEROUS is a benchmark and research initiative focused on fact extraction
and verification tasks involving unstructured text and structured tabular data.
In FEVEROUS, existing works often rely on extensive preprocessing and utilize
rule-based transformations of data, leading to potential context loss or
misleading encodings. This paper introduces a simple yet powerful model that
nullifies the need for modality conversion, thereby preserving the original
evidence's context. By leveraging pre-trained models on diverse text and
tabular datasets and by incorporating a lightweight attention-based mechanism,
our approach efficiently exploits latent connections between different data
types, thereby yielding comprehensive and reliable verdict predictions. The
model's modular structure adeptly manages multi-modal information, ensuring the
integrity and authenticity of the original evidence are uncompromised.
Comparative analyses reveal that our approach exhibits competitive performance,
aligning itself closely with top-tier models on the FEVEROUS benchmark.
\\ ( https://arxiv.org/abs/2403.17361 ,  168kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17363
Date: Tue, 26 Mar 2024 03:58:52 GMT   (975kb,D)

Title: Extracting Biomedical Entities from Noisy Audio Transcripts
Authors: Nima Ebadi, Kellen Morgan, Adrian Tan, Billy Linares, Sheri Osborn,
  Emma Majors, Jeremy Davis, Anthony Rios
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  Automatic Speech Recognition (ASR) technology is fundamental in transcribing
spoken language into text, with considerable applications in the clinical
realm, including streamlining medical transcription and integrating with
Electronic Health Record (EHR) systems. Nevertheless, challenges persist,
especially when transcriptions contain noise, leading to significant drops in
performance when Natural Language Processing (NLP) models are applied. Named
Entity Recognition (NER), an essential clinical task, is particularly affected
by such noise, often termed the ASR-NLP gap. Prior works have primarily studied
ASR's efficiency in clean recordings, leaving a research gap concerning the
performance in noisy environments. This paper introduces a novel dataset,
BioASR-NER, designed to bridge the ASR-NLP gap in the biomedical domain,
focusing on extracting adverse drug reactions and mentions of entities from the
Brief Test of Adult Cognition by Telephone (BTACT) exam. Our dataset offers a
comprehensive collection of almost 2,000 clean and noisy recordings. In
addressing the noise challenge, we present an innovative transcript-cleaning
method using GPT4, investigating both zero-shot and few-shot methodologies. Our
study further delves into an error analysis, shedding light on the types of
errors in transcription software, corrections by GPT4, and the challenges GPT4
faces. This paper aims to foster improved understanding and potential solutions
for the ASR-NLP gap, ultimately supporting enhanced healthcare documentation
practices.
\\ ( https://arxiv.org/abs/2403.17363 ,  975kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17368
Date: Tue, 26 Mar 2024 04:07:08 GMT   (1191kb,D)

Title: ChatGPT Rates Natural Language Explanation Quality Like Humans: But on
  Which Scales?
Authors: Fan Huang, Haewoon Kwak, Kunwoo Park, Jisun An
Categories: cs.CL cs.AI
Comments: Accpeted by LREC-COLING 2024 main conference, long paper
\\
  As AI becomes more integral in our lives, the need for transparency and
responsibility grows. While natural language explanations (NLEs) are vital for
clarifying the reasoning behind AI decisions, evaluating them through human
judgments is complex and resource-intensive due to subjectivity and the need
for fine-grained ratings. This study explores the alignment between ChatGPT and
human assessments across multiple scales (i.e., binary, ternary, and 7-Likert
scale). We sample 300 data instances from three NLE datasets and collect 900
human annotations for both informativeness and clarity scores as the text
quality measurement. We further conduct paired comparison experiments under
different ranges of subjectivity scores, where the baseline comes from 8,346
human annotations. Our results show that ChatGPT aligns better with humans in
more coarse-grained scales. Also, paired comparisons and dynamic prompting
(i.e., providing semantically similar examples in the prompt) improve the
alignment. This research advances our understanding of large language models'
capabilities to assess the text explanation quality in different configurations
for responsible AI development.
\\ ( https://arxiv.org/abs/2403.17368 ,  1191kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17385
Date: Tue, 26 Mar 2024 05:11:51 GMT   (1572kb,D)

Title: ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity
  Recognition
Authors: Haris Riaz, Razvan-Gabriel Dumitru and Mihai Surdeanu
Categories: cs.CL cs.AI
Comments: Accepted to LREC-COLING 2024
\\
  In this work, we revisit the problem of semi-supervised named entity
recognition (NER) focusing on extremely light supervision, consisting of a
lexicon containing only 10 examples per class. We introduce ELLEN, a simple,
fully modular, neuro-symbolic method that blends fine-tuned language models
with linguistic rules. These rules include insights such as ''One Sense Per
Discourse'', using a Masked Language Model as an unsupervised NER, leveraging
part-of-speech tags to identify and eliminate unlabeled entities as false
negatives, and other intuitions about classifier confidence scores in local and
global context. ELLEN achieves very strong performance on the CoNLL-2003
dataset when using the minimal supervision from the lexicon above. It also
outperforms most existing (and considerably more complex) semi-supervised NER
methods under the same supervision settings commonly used in the literature
(i.e., 5% of the training data). Further, we evaluate our CoNLL-2003 model in a
zero-shot scenario on WNUT-17 where we find that it outperforms GPT-3.5 and
achieves comparable performance to GPT-4. In a zero-shot setting, ELLEN also
achieves over 75% of the performance of a strong, fully supervised model
trained on gold data. Our code is available at:
https://github.com/hriaz17/ELLEN.
\\ ( https://arxiv.org/abs/2403.17385 ,  1572kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17407
Date: Tue, 26 Mar 2024 05:55:21 GMT   (2083kb,D)

Title: Transcribing Bengali Text with Regional Dialects to IPA using District
  Guided Tokens
Authors: S M Jishanul Islam, Sadia Ahmmed and Sahid Hossain Mustakim
Categories: cs.CL cs.AI cs.LG
Comments: This work became the champion of the Bhashamul challenge
ACM-class: F.2.2; I.2.7
\\
  Accurate transcription of Bengali text to the International Phonetic Alphabet
(IPA) is a challenging task due to the complex phonology of the language and
context-dependent sound changes. This challenge is even more for regional
Bengali dialects due to unavailability of standardized spelling conventions for
these dialects, presence of local and foreign words popular in those regions
and phonological diversity across different regions. This paper presents an
approach to this sequence-to-sequence problem by introducing the District
Guided Tokens (DGT) technique on a new dataset spanning six districts of
Bangladesh. The key idea is to provide the model with explicit information
about the regional dialect or "district" of the input text before generating
the IPA transcription. This is achieved by prepending a district token to the
input sequence, effectively guiding the model to understand the unique phonetic
patterns associated with each district. The DGT technique is applied to
fine-tune several transformer-based models, on this new dataset. Experimental
results demonstrate the effectiveness of DGT, with the ByT5 model achieving
superior performance over word-based models like mT5, BanglaT5, and umT5. This
is attributed to ByT5's ability to handle a high percentage of
out-of-vocabulary words in the test set. The proposed approach highlights the
importance of incorporating regional dialect information into ubiquitous
natural language processing systems for languages with diverse phonological
variations. The following work was a result of the "Bhashamul" challenge, which
is dedicated to solving the problem of Bengali text with regional dialects to
IPA transcription https://www.kaggle.com/competitions/regipa/. The training and
inference notebooks are available through the competition link.
\\ ( https://arxiv.org/abs/2403.17407 ,  2083kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17411
Date: Tue, 26 Mar 2024 06:11:07 GMT   (8989kb,D)

Title: PCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large
  Language Models
Authors: Jinyi Li, Yihuai Lan, Lei Wang, Hao Wang
Categories: cs.CL
Comments: For open-source repository, see
  https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression
\\
  Prompt compression is an innovative method for efficiently condensing input
prompts while preserving essential information. To facilitate quick-start
services, user-friendly interfaces, and compatibility with common datasets and
metrics, we present the Prompt Compression Toolkit (PCToolkit). This toolkit is
a unified plug-and-play solution for compressing prompts in Large Language
Models (LLMs), featuring cutting-edge prompt compressors, diverse datasets, and
metrics for comprehensive performance evaluation. PCToolkit boasts a modular
design, allowing for easy integration of new datasets and metrics through
portable and user-friendly interfaces. In this paper, we outline the key
components and functionalities of PCToolkit. We conducted evaluations of the
compressors within PCToolkit across various natural language tasks, including
reconstruction, summarization, mathematical problem-solving, question
answering, few-shot learning, synthetic tasks, code completion, boolean
expressions, multiple choice questions, and lies recognition.
\\ ( https://arxiv.org/abs/2403.17411 ,  8989kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17413
Date: Tue, 26 Mar 2024 06:12:21 GMT   (1542kb,D)

Title: LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error
  Correction
Authors: Yixuan Wang, Baoxin Wang, Yijun Liu, Dayong Wu and Wanxiang Che
Categories: cs.CL
Comments: Accepted to COLING 2024
\\
  Over-correction is a critical problem in Chinese grammatical error correction
(CGEC) task. Recent work using model ensemble methods based on voting can
effectively mitigate over-correction and improve the precision of the GEC
system. However, these methods still require the output of several GEC systems
and inevitably lead to reduced error recall. In this light, we propose the
LM-Combiner, a rewriting model that can directly modify the over-correction of
GEC system outputs without a model ensemble. Specifically, we train the model
on an over-correction dataset constructed through the proposed K-fold cross
inference method, which allows it to directly generate filtered sentences by
combining the original and the over-corrected text. In the inference stage, we
directly take the original sentences and the output results of other systems as
input and then obtain the filtered sentences through LM-Combiner. Experiments
on the FCGEC dataset show that our proposed method effectively alleviates the
over-correction of the original system (+18.2 Precision) while ensuring the
error recall remains unchanged. Besides, we find that LM-Combiner still has a
good rewriting performance even with small parameters and few training data,
and thus can cost-effectively mitigate the over-correction of black-box GEC
systems (e.g., ChatGPT).
\\ ( https://arxiv.org/abs/2403.17413 ,  1542kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17431
Date: Tue, 26 Mar 2024 06:57:23 GMT   (231kb,D)

Title: Robust and Scalable Model Editing for Large Language Models
Authors: Yingfa Chen, Zhengyan Zhang, Xu Han, Chaojun Xiao, Zhiyuan Liu, Chen
  Chen, Kuai Li, Tao Yang, Maosong Sun
Categories: cs.CL cs.LG
Comments: LREC-COLING 2024 paper, 16 pages, 4 figures
\\
  Large language models (LLMs) can make predictions using parametric
knowledge--knowledge encoded in the model weights--or contextual
knowledge--knowledge presented in the context. In many scenarios, a desirable
behavior is that LLMs give precedence to contextual knowledge when it conflicts
with the parametric knowledge, and fall back to using their parametric
knowledge when the context is irrelevant. This enables updating and correcting
the model's knowledge by in-context editing instead of retraining. Previous
works have shown that LLMs are inclined to ignore contextual knowledge and fail
to reliably fall back to parametric knowledge when presented with irrelevant
context. In this work, we discover that, with proper prompting methods,
instruction-finetuned LLMs can be highly controllable by contextual knowledge
and robust to irrelevant context. Utilizing this feature, we propose EREN (Edit
models by REading Notes) to improve the scalability and robustness of LLM
editing. To better evaluate the robustness of model editors, we collect a new
dataset, that contains irrelevant questions that are more challenging than the
ones in existing datasets. Empirical results show that our method outperforms
current state-of-the-art methods by a large margin. Unlike existing techniques,
it can integrate knowledge from multiple edits, and correctly respond to
syntactically similar but semantically unrelated inputs (and vice versa). The
source code can be found at https://github.com/thunlp/EREN.
\\ ( https://arxiv.org/abs/2403.17431 ,  231kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17486
Date: Tue, 26 Mar 2024 08:32:39 GMT   (2457kb,D)

Title: KDMCSE: Knowledge Distillation Multimodal Sentence Embeddings with
  Adaptive Angular margin Contrastive Learning
Authors: Cong-Duy Nguyen, Thong Nguyen, Xiaobao Wu, Anh Tuan Luu
Categories: cs.CL
Comments: Accepted to NAACL 2024
\\
  Previous work on multimodal sentence embedding has proposed multimodal
contrastive learning and achieved promising results. However, by taking the
rest of the batch as negative samples without reviewing when forming
contrastive pairs, those studies encountered many suspicious and noisy negative
examples, significantly affecting the methods' overall performance. In this
work, we propose KDMCSE (Knowledge Distillation Multimodal contrastive learning
of Sentence Embeddings), a novel approach that enhances the discrimination and
generalizability of multimodal representation and inherits the knowledge from
the teacher model to learn the difference between positive and negative
instances and via that, can detect noisy and wrong negative samples effectively
before they are calculated in the contrastive objective. Furthermore, to
overcome the limitation of modeling the variation within negative pairs, we
introduce a new contrastive objective, AdapACSE (Adaptive Angular Margin
Supervised Contrastive Learning for Multimodal sentence embeddings), that
enhances the discriminative representation by strengthening the margin within
the angular space while capturing varying semantics within the negative.
Experimental results on widely used Semantic Textual Similarity (STS)
benchmarks demonstrate the effectiveness of our approach.
\\ ( https://arxiv.org/abs/2403.17486 ,  2457kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17491
Date: Tue, 26 Mar 2024 08:47:23 GMT   (1660kb,D)

Title: DGoT: Dynamic Graph of Thoughts for Scientific Abstract Generation
Authors: Xinyu Ning and Yutong Zhao and Yitong Liu and Hongwen Yang
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\
  The method of training language models based on domain datasets has obtained
significant achievements in the task of generating scientific paper abstracts.
However, such models face problems of generalization and expensive training
costs. The use of large language models (LLMs) to solve the task of generating
paper abstracts saves the cost of model training. However, due to the
hallucination problem of LLM, it is often necessary to improve the reliability
of the results through multi-round query prompt approach such as Graph of
Thoughts (GoT), which also brings additional reasoning costs. In this paper, we
propose a Dynamic Graph of Thought (DGoT). It not only inherits the advantages
of the existing GoT prompt approach, but also dynamically adjust the graph
structure according to data characteristics while reducing model reasoning
cost. Experimental results show that our method's cost-effectiveness in
abstract generation tasks is only 43.7% to 56.4% of other multi-round query
prompt approaches. Our code is available at https://github.com/JayceNing/DGoT.
\\ ( https://arxiv.org/abs/2403.17491 ,  1660kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17497
Date: Tue, 26 Mar 2024 08:58:28 GMT   (615kb,D)

Title: Sharing the Cost of Success: A Game for Evaluating and Learning
  Collaborative Multi-Agent Instruction Giving and Following Policies
Authors: Philipp Sadler, Sherzod Hakimov and David Schlangen
Categories: cs.CL cs.CV
Comments: 9 pages, Accepted at LREC-COLING 2024
\\
  In collaborative goal-oriented settings, the participants are not only
interested in achieving a successful outcome, but do also implicitly negotiate
the effort they put into the interaction (by adapting to each other). In this
work, we propose a challenging interactive reference game that requires two
players to coordinate on vision and language observations. The learning signal
in this game is a score (given after playing) that takes into account the
achieved goal and the players' assumed efforts during the interaction. We show
that a standard Proximal Policy Optimization (PPO) setup achieves a high
success rate when bootstrapped with heuristic partner behaviors that implement
insights from the analysis of human-human interactions. And we find that a
pairing of neural partners indeed reduces the measured joint effort when
playing together repeatedly. However, we observe that in comparison to a
reasonable heuristic pairing there is still room for improvement -- which
invites further research in the direction of cost-sharing in collaborative
interactions.
\\ ( https://arxiv.org/abs/2403.17497 ,  615kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17516
Date: Tue, 26 Mar 2024 09:18:59 GMT   (8623kb,D)

Title: MapGuide: A Simple yet Effective Method to Reconstruct Continuous
  Language from Brain Activities
Authors: Xinpei Zhao, Jingyuan Sun, Shaonan Wang, Jing Ye, Xiaohan Zhang,
  Chengqing Zong
Categories: cs.CL cs.AI
Comments: Accepted to NAACL 2024 main conference
\\
  Decoding continuous language from brain activity is a formidable yet
promising field of research. It is particularly significant for aiding people
with speech disabilities to communicate through brain signals. This field
addresses the complex task of mapping brain signals to text. The previous best
attempt reverse-engineered this process in an indirect way: it began by
learning to encode brain activity from text and then guided text generation by
aligning with predicted brain responses. In contrast, we propose a simple yet
effective method that guides text reconstruction by directly comparing them
with the predicted text embeddings mapped from brain activities. Comprehensive
experiments reveal that our method significantly outperforms the current
state-of-the-art model, showing average improvements of 77% and 54% on BLEU and
METEOR scores. We further validate the proposed modules through detailed
ablation studies and case analyses and highlight a critical correlation: the
more precisely we map brain activities to text embeddings, the better the text
reconstruction results. Such insight can simplify the task of reconstructing
language from brain activities for future work, emphasizing the importance of
improving brain-to-text-embedding mapping techniques.
\\ ( https://arxiv.org/abs/2403.17516 ,  8623kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17528
Date: Tue, 26 Mar 2024 09:31:55 GMT   (233kb,D)

Title: Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual
  Applications
Authors: Chihiro Yano, Akihiko Fukuchi, Shoko Fukasawa, Hideyuki Tachibana,
  Yotaro Watanabe
Categories: cs.CL
Comments: Accepted in LREC-COLING 2024
\\
  Prior work on multilingual sentence embedding has demonstrated that the
efficient use of natural language inference (NLI) data to build
high-performance models can outperform conventional methods. However, the
potential benefits from the recent ``exponential'' growth of language models
with billions of parameters have not yet been fully explored. In this paper, we
introduce Multilingual Sentence T5 (m-ST5), as a larger model of NLI-based
multilingual sentence embedding, by extending Sentence T5, an existing
monolingual model. By employing the low-rank adaptation (LoRA) technique, we
have achieved a successful scaling of the model's size to 5.7 billion
parameters. We conducted experiments to evaluate the performance of sentence
embedding and verified that the method outperforms the NLI-based prior
approach. Furthermore, we also have confirmed a positive correlation between
the size of the model and its performance. It was particularly noteworthy that
languages with fewer resources or those with less linguistic similarity to
English benefited more from the parameter increase. Our model is available at
https://huggingface.co/pkshatech/m-ST5.
\\ ( https://arxiv.org/abs/2403.17528 ,  233kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17534
Date: Tue, 26 Mar 2024 09:39:53 GMT   (865kb,D)

Title: Sparse Logistic Regression with High-order Features for Automatic
  Grammar Rule Extraction from Treebanks
Authors: Santiago Herrera, Caio Corro, Sylvain Kahane
Categories: cs.CL
Comments: Published in LREC-Coling 2024 proceedings
\\
  Descriptive grammars are highly valuable, but writing them is time-consuming
and difficult. Furthermore, while linguists typically use corpora to create
them, grammar descriptions often lack quantitative data. As for formal
grammars, they can be challenging to interpret. In this paper, we propose a new
method to extract and explore significant fine-grained grammar patterns and
potential syntactic grammar rules from treebanks, in order to create an
easy-to-understand corpus-based grammar. More specifically, we extract
descriptions and rules across different languages for two linguistic phenomena,
agreement and word order, using a large search space and paying special
attention to the ranking order of the extracted rules. For that, we use a
linear classifier to extract the most salient features that predict the
linguistic phenomena under study. We associate statistical information to each
rule, and we compare the ranking of the model's results to those of other
quantitative and statistical measures. Our method captures both well-known and
less well-known significant grammar rules in Spanish, French, and Wolof.
\\ ( https://arxiv.org/abs/2403.17534 ,  865kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17536
Date: Tue, 26 Mar 2024 09:41:21 GMT   (1155kb,D)

Title: ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent
  Classifier and Slot Filler
Authors: Paramita Mirza, Viju Sudhi, Soumya Ranjan Sahoo, Sinchana Ramakanth
  Bhat
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  State-of-the-art intent classification (IC) and slot filling (SF) methods
often rely on data-intensive deep learning models, limiting their practicality
for industry applications. Large language models on the other hand,
particularly instruction-tuned models (Instruct-LLMs), exhibit remarkable
zero-shot performance across various natural language tasks. This study
evaluates Instruct-LLMs on popular benchmark datasets for IC and SF,
emphasizing their capacity to learn from fewer examples. We introduce
ILLUMINER, an approach framing IC and SF as language generation tasks for
Instruct-LLMs, with a more efficient SF-prompting method compared to prior
work. A comprehensive comparison with multiple baselines shows that our
approach, using the FLAN-T5 11B model, outperforms the state-of-the-art joint
IC+SF method and in-context learning with GPT3.5 (175B), particularly in slot
filling by 11.1--32.2 percentage points. Additionally, our in-depth ablation
study demonstrates that parameter-efficient fine-tuning requires less than 6%
of training data to yield comparable performance with traditional full-weight
fine-tuning.
\\ ( https://arxiv.org/abs/2403.17536 ,  1155kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17540
Date: Tue, 26 Mar 2024 09:43:15 GMT   (673kb,D)

Title: Large Language Models Are State-of-the-Art Evaluator for Grammatical
  Error Correction
Authors: Masamune Kobayashi, Masato Mita, Mamoru Komachi
Categories: cs.CL
\\
  Large Language Models (LLMs) have been reported to outperform existing
automatic evaluation metrics in some tasks, such as text summarization and
machine translation. However, there has been a lack of research on LLMs as
evaluators in grammatical error correction (GEC). In this study, we investigate
the performance of LLMs in GEC evaluation by employing prompts designed to
incorporate various evaluation criteria inspired by previous research. Our
extensive experimental results demonstrate that GPT-4 achieved Kendall's rank
correlation of 0.662 with human judgments, surpassing all existing methods.
Furthermore, in recent GEC evaluations, we have underscored the significance of
the LLMs scale and particularly emphasized the importance of fluency among
evaluation criteria.
\\ ( https://arxiv.org/abs/2403.17540 ,  673kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17545
Date: Tue, 26 Mar 2024 09:49:35 GMT   (2003kb,D)

Title: A Gaze-grounded Visual Question Answering Dataset for Clarifying
  Ambiguous Japanese Questions
Authors: Shun Inadumi, Seiya Kawano, Akishige Yuguchi, Yasutomo Kawanishi,
  Koichiro Yoshino
Categories: cs.CL cs.CV
Comments: LREC-COLING 2024
\\
  Situated conversations, which refer to visual information as visual question
answering (VQA), often contain ambiguities caused by reliance on directive
information. This problem is exacerbated because some languages, such as
Japanese, often omit subjective or objective terms. Such ambiguities in
questions are often clarified by the contexts in conversational situations,
such as joint attention with a user or user gaze information. In this study, we
propose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous
questions using gaze information by focusing on a clarification process
complemented by gaze information. We also propose a method that utilizes gaze
target estimation results to improve the accuracy of GazeVQA tasks. Our
experimental results showed that the proposed method improved the performance
in some cases of a VQA system on GazeVQA and identified some typical problems
of GazeVQA tasks that need to be improved.
\\ ( https://arxiv.org/abs/2403.17545 ,  2003kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17546
Date: Tue, 26 Mar 2024 09:51:43 GMT   (2266kb)

Title: Decoding excellence: Mapping the demand for psychological traits of
  operations and supply chain professionals through text mining
Authors: S. Di Luozzo, A. Fronzetti Colladon, M. M. Schiraldi
Categories: cs.CL cs.SI econ.GN physics.soc-ph q-fin.EC
ACM-class: I.2.7; J.4; H.4.0
\\
  The current study proposes an innovative methodology for the profiling of
psychological traits of Operations Management (OM) and Supply Chain Management
(SCM) professionals. We use innovative methods and tools of text mining and
social network analysis to map the demand for relevant skills from a set of job
descriptions, with a focus on psychological characteristics. The proposed
approach aims to evaluate the market demand for specific traits by combining
relevant psychological constructs, text mining techniques, and an innovative
measure, namely, the Semantic Brand Score. We apply the proposed methodology to
a dataset of job descriptions for OM and SCM professionals, with the objective
of providing a mapping of their relevant required skills, including
psychological characteristics. In addition, the analysis is then detailed by
considering the region of the organization that issues the job description, its
organizational size, and the seniority level of the open position in order to
understand their nuances. Finally, topic modeling is used to examine key
components and their relative significance in job descriptions. By employing a
novel methodology and considering contextual factors, we provide an innovative
understanding of the attitudinal traits that differentiate professionals. This
research contributes to talent management, recruitment practices, and
professional development initiatives, since it provides new figures and
perspectives to improve the effectiveness and success of Operations Management
and Supply Chain Management professionals.
\\ ( https://arxiv.org/abs/2403.17546 ,  2266kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17552
Date: Tue, 26 Mar 2024 09:59:45 GMT   (2959kb,D)

Title: Naive Bayes-based Context Extension for Large Language Models
Authors: Jianlin Su, Murtadha Ahmed, Wenbo, Luo Ao, Mingren Zhu, Yunfeng Liu
Categories: cs.CL
Comments: Accepted to main NAACL 2024
\\
  Large Language Models (LLMs) have shown promising in-context learning
abilities. However, conventional In-Context Learning (ICL) approaches are often
impeded by length limitations of transformer architecture, which pose
challenges when attempting to effectively integrate supervision from a
substantial number of demonstration examples. In this paper, we introduce a
novel framework, called Naive Bayes-based Context Extension (NBCE), to enable
existing LLMs to perform ICL with an increased number of demonstrations by
significantly expanding their context size. Importantly, this expansion does
not require fine-tuning or dependence on particular model architectures, all
the while preserving linear efficiency. NBCE initially splits the context into
equal-sized windows fitting the target LLM's maximum length. Then, it
introduces a voting mechanism to select the most relevant window, regarded as
the posterior context. Finally, it employs Bayes' theorem to generate the test
task. Our experimental results demonstrate that NBCE substantially enhances
performance, particularly as the number of demonstration examples increases,
consistently outperforming alternative methods. The NBCE code will be made
publicly accessible. The code NBCE is available at:
https://github.com/amurtadha/NBCE-master
\\ ( https://arxiv.org/abs/2403.17552 ,  2959kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17553
Date: Tue, 26 Mar 2024 10:01:01 GMT   (908kb,D)

Title: RuBia: A Russian Language Bias Detection Dataset
Authors: Veronika Grigoreva, Anastasiia Ivanova, Ilseyar Alimova, Ekaterina
  Artemova
Categories: cs.CL
Comments: accepted to LREC-COLING 2024
\\
  Warning: this work contains upsetting or disturbing content.
  Large language models (LLMs) tend to learn the social and cultural biases
present in the raw pre-training data. To test if an LLM's behavior is fair,
functional datasets are employed, and due to their purpose, these datasets are
highly language and culture-specific. In this paper, we address a gap in the
scope of multilingual bias evaluation by presenting a bias detection dataset
specifically designed for the Russian language, dubbed as RuBia. The RuBia
dataset is divided into 4 domains: gender, nationality, socio-economic status,
and diverse, each of the domains is further divided into multiple fine-grained
subdomains. Every example in the dataset consists of two sentences with the
first reinforcing a potentially harmful stereotype or trope and the second
contradicting it. These sentence pairs were first written by volunteers and
then validated by native-speaking crowdsourcing workers. Overall, there are
nearly 2,000 unique sentence pairs spread over 19 subdomains in RuBia. To
illustrate the dataset's purpose, we conduct a diagnostic evaluation of
state-of-the-art or near-state-of-the-art LLMs and discuss the LLMs'
predisposition to social biases.
\\ ( https://arxiv.org/abs/2403.17553 ,  908kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17556
Date: Tue, 26 Mar 2024 10:04:24 GMT   (8351kb,D)

Title: m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt
Authors: Jian Yang, Hongcheng Guo, Yuwei Yin, Jiaqi Bai, Bing Wang, Jiaheng
  Liu, Xinnian Liang, Linzheng Cahi, Liqun Yang, Zhoujun Li
Categories: cs.CL cs.AI
Comments: COLING 2024
\\
  Multilingual translation supports multiple translation directions by
projecting all languages in a shared space, but the translation quality is
undermined by the difference between languages in the text-only modality,
especially when the number of languages is large. To bridge this gap, we
introduce visual context as the universal language-independent representation
to facilitate multilingual translation. In this paper, we propose a framework
to leverage the multimodal prompt to guide the Multimodal Multilingual neural
Machine Translation (m3P), which aligns the representations of different
languages with the same meaning and generates the conditional vision-language
memory for translation. We construct a multilingual multimodal instruction
dataset (InstrMulti102) to support 102 languages. Our method aims to minimize
the representation distance of different languages by regarding the image as a
central language. Experimental results show that m3P outperforms previous
text-only baselines and multilingual multimodal methods by a large margin.
Furthermore, the probing experiments validate the effectiveness of our method
in enhancing translation under the low-resource and massively multilingual
scenario.
\\ ( https://arxiv.org/abs/2403.17556 ,  8351kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17564
Date: Tue, 26 Mar 2024 10:14:12 GMT   (855kb,D)

Title: Task-Oriented Paraphrase Analytics
Authors: Marcel Gohsen and Matthias Hagen and Martin Potthast and Benno Stein
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  Since paraphrasing is an ill-defined task, the term "paraphrasing" covers
text transformation tasks with different characteristics. Consequently,
existing paraphrasing studies have applied quite different (explicit and
implicit) criteria as to when a pair of texts is to be considered a paraphrase,
all of which amount to postulating a certain level of semantic or lexical
similarity. In this paper, we conduct a literature review and propose a
taxonomy to organize the 25~identified paraphrasing (sub-)tasks. Using
classifiers trained to identify the tasks that a given paraphrasing instance
fits, we find that the distributions of task-specific instances in the known
paraphrase corpora vary substantially. This means that the use of these
corpora, without the respective paraphrase conditions being clearly defined
(which is the normal case), must lead to incomparable and misleading results.
\\ ( https://arxiv.org/abs/2403.17564 ,  855kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17582
Date: Tue, 26 Mar 2024 10:45:11 GMT   (3376kb,D)

Title: Towards a Zero-Data, Controllable, Adaptive Dialog System
Authors: Dirk V\"ath, Lindsey Vanderlyn, Ngoc Thang Vu
Categories: cs.CL cs.AI cs.LG
\\
  Conversational Tree Search (V\"ath et al., 2023) is a recent approach to
controllable dialog systems, where domain experts shape the behavior of a
Reinforcement Learning agent through a dialog tree. The agent learns to
efficiently navigate this tree, while adapting to information needs, e.g.,
domain familiarity, of different users. However, the need for additional
training data hinders deployment in new domains. To address this, we explore
approaches to generate this data directly from dialog trees. We improve the
original approach, and show that agents trained on synthetic data can achieve
comparable dialog success to models trained on human data, both when using a
commercial Large Language Model for generation, or when using a smaller
open-source model, running on a single GPU. We further demonstrate the
scalability of our approach by collecting and testing on two new datasets:
ONBOARD, a new domain helping foreign residents moving to a new city, and the
medical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and
head symptoms. Finally, we perform human testing, where no statistically
significant differences were found in either objective or subjective measures
between models trained on human and generated data.
\\ ( https://arxiv.org/abs/2403.17582 ,  3376kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17611
Date: Tue, 26 Mar 2024 11:44:49 GMT   (542kb,D)

Title: Denoising Table-Text Retrieval for Open-Domain Question Answering
Authors: Deokhyung Kang, Baikjin Jung, Yunsu Kim and Gary Geunbae Lee
Categories: cs.CL cs.AI
Comments: Accepted to LREC-COLING 2024
\\
  In table-text open-domain question answering, a retriever system retrieves
relevant evidence from tables and text to answer questions. Previous studies in
table-text open-domain question answering have two common challenges: firstly,
their retrievers can be affected by false-positive labels in training datasets;
secondly, they may struggle to provide appropriate evidence for questions that
require reasoning across the table. To address these issues, we propose
Denoised Table-Text Retriever (DoTTeR). Our approach involves utilizing a
denoised training dataset with fewer false positive labels by discarding
instances with lower question-relevance scores measured through a false
positive detection model. Subsequently, we integrate table-level ranking
information into the retriever to assist in finding evidence for questions that
demand reasoning across the table. To encode this ranking information, we
fine-tune a rank-aware column encoder to identify minimum and maximum values
within a column. Experimental results demonstrate that DoTTeR significantly
outperforms strong baselines on both retrieval recall and downstream QA tasks.
Our code is available at https://github.com/deokhk/DoTTeR.
\\ ( https://arxiv.org/abs/2403.17611 ,  542kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17612
Date: Tue, 26 Mar 2024 11:45:22 GMT   (51kb,D)

Title: "You are an expert annotator": Automatic Best-Worst-Scaling Annotations
  for Emotion Intensity Modeling
Authors: Christopher Bagdon and Prathamesh Karmalker and Harsha Gurulingappa
  and Roman Klinger
Categories: cs.CL
Comments: accepted for publication in NAACL 2024
\\
  Labeling corpora constitutes a bottleneck to create models for new tasks or
domains. Large language models mitigate the issue with automatic corpus
labeling methods, particularly for categorical annotations. Some NLP tasks such
as emotion intensity prediction, however, require text regression, but there is
no work on automating annotations for continuous label assignments. Regression
is considered more challenging than classification: The fact that humans
perform worse when tasked to choose values from a rating scale lead to
comparative annotation methods, including best-worst scaling. This raises the
question if large language model-based annotation methods show similar
patterns, namely that they perform worse on rating scale annotation tasks than
on comparative annotation tasks. To study this, we automate emotion intensity
predictions and compare direct rating scale predictions, pairwise comparisons
and best-worst scaling. We find that the latter shows the highest reliability.
A transformer regressor fine-tuned on these data performs nearly on par with a
model trained on the original manual annotations.
\\ ( https://arxiv.org/abs/2403.17612 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17636
Date: Tue, 26 Mar 2024 12:11:29 GMT   (7259kb,D)

Title: Mix-Initiative Response Generation with Dynamic Prefix Tuning
Authors: Yuxiang Nie, Heyan Huang, Xian-Ling Mao, Lizi Liao
Categories: cs.CL
Comments: Accepted to the main conference of NAACL 2024
\\
  Mixed initiative serves as one of the key factors in controlling conversation
directions. For a speaker, responding passively or leading proactively would
result in rather different responses. However, most dialogue systems focus on
training a holistic response generation model without any distinction among
different initiatives. It leads to the cross-contamination problem, where the
model confuses different initiatives and generates inappropriate responses.
Moreover, obtaining plenty of human annotations for initiative labels can be
expensive. To address this issue, we propose a general mix-Initiative Dynamic
Prefix Tuning framework (IDPT) to decouple different initiatives from the
generation model, which learns initiative-aware prefixes in both supervised and
unsupervised settings. Specifically, IDPT decouples initiative factors into
different prefix parameters and uses the attention mechanism to adjust the
selection of initiatives in guiding generation dynamically. The prefix
parameters can be tuned towards accurate initiative prediction as well as
mix-initiative response generation. Extensive experiments on two public
dialogue datasets show that the proposed IDPT outperforms previous baselines on
both automatic metrics and human evaluations. It also manages to generate
appropriate responses with manipulated initiatives.
\\ ( https://arxiv.org/abs/2403.17636 ,  7259kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17640
Date: Tue, 26 Mar 2024 12:21:51 GMT   (1011kb,D)

Title: REFeREE: A REference-FREE Model-Based Metric for Text Simplification
Authors: Yichen Huang, Ekaterina Kochmar
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  Text simplification lacks a universal standard of quality, and annotated
reference simplifications are scarce and costly. We propose to alleviate such
limitations by introducing REFeREE, a reference-free model-based metric with a
3-stage curriculum. REFeREE leverages an arbitrarily scalable pretraining stage
and can be applied to any quality standard as long as a small number of human
annotations are available. Our experiments show that our metric outperforms
existing reference-based metrics in predicting overall ratings and reaches
competitive and consistent performance in predicting specific ratings while
requiring no reference simplifications at inference time.
\\ ( https://arxiv.org/abs/2403.17640 ,  1011kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17645
Date: Tue, 26 Mar 2024 12:27:32 GMT   (668kb)

Title: DANCER: Entity Description Augmented Named Entity Corrector for
  Automatic Speech Recognition
Authors: Yi-Cheng Wang, Hsin-Wei Wang, Bi-Cheng Yan, Chi-Han Lin and Berlin
  Chen
Categories: cs.CL
\\
  End-to-end automatic speech recognition (E2E ASR) systems often suffer from
mistranscription of domain-specific phrases, such as named entities, sometimes
leading to catastrophic failures in downstream tasks. A family of fast and
lightweight named entity correction (NEC) models for ASR have recently been
proposed, which normally build on phonetic-level edit distance algorithms and
have shown impressive NEC performance. However, as the named entity (NE) list
grows, the problems of phonetic confusion in the NE list are exacerbated; for
example, homophone ambiguities increase substantially. In view of this, we
proposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER),
which leverages entity descriptions to provide additional information to
facilitate mitigation of phonetic confusion for NEC on ASR transcription. To
this end, an efficient entity description augmented masked language model
(EDA-MLM) comprised of a dense retrieval model is introduced, enabling MLM to
adapt swiftly to domain-specific entities for the NEC task. A series of
experiments conducted on the AISHELL-1 and Homophone datasets confirm the
effectiveness of our modeling approach. DANCER outperforms a strong baseline,
the phonetic edit-distance-based NEC model (PED-NEC), by a character error rate
(CER) reduction of about 7% relatively on AISHELL-1 for named entities. More
notably, when tested on Homophone that contain named entities of high phonetic
confusion, DANCER offers a more pronounced CER reduction of 46% relatively over
PED-NEC for named entities.
\\ ( https://arxiv.org/abs/2403.17645 ,  668kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17647
Date: Tue, 26 Mar 2024 12:29:18 GMT   (10130kb,D)

Title: Intrinsic Subgraph Generation for Interpretable Graph based Visual
  Question Answering
Authors: Pascal Tilli, Ngoc Thang Vu
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  The large success of deep learning based methods in Visual Question Answering
(VQA) has concurrently increased the demand for explainable methods. Most
methods in Explainable Artificial Intelligence (XAI) focus on generating
post-hoc explanations rather than taking an intrinsic approach, the latter
characterizing an interpretable model. In this work, we introduce an
interpretable approach for graph-based VQA and demonstrate competitive
performance on the GQA dataset. This approach bridges the gap between
interpretability and performance. Our model is designed to intrinsically
produce a subgraph during the question-answering process as its explanation,
providing insight into the decision making. To evaluate the quality of these
generated subgraphs, we compare them against established post-hoc
explainability methods for graph neural networks, and perform a human
evaluation. Moreover, we present quantitative metrics that correlate with the
evaluations of human assessors, acting as automatic metrics for the generated
explanatory subgraphs. Our implementation is available at
https://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.
\\ ( https://arxiv.org/abs/2403.17647 ,  10130kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17661
Date: Tue, 26 Mar 2024 12:47:39 GMT   (14172kb,D)

Title: Language Models for Text Classification: Is In-Context Learning Enough?
Authors: Aleksandra Edwards and Jose Camacho-Collados
Categories: cs.CL cs.AI
Comments: Accepted at LREC-COLING 2024
\\
  Recent foundational language models have shown state-of-the-art performance
in many NLP tasks in zero- and few-shot settings. An advantage of these models
over more standard approaches based on fine-tuning is the ability to understand
instructions written in natural language (prompts), which helps them generalise
better to different tasks and domains without the need for specific training
data. This makes them suitable for addressing text classification problems for
domains with limited amounts of annotated instances. However, existing research
is limited in scale and lacks understanding of how text generation models
combined with prompting techniques compare to more established methods for text
classification such as fine-tuning masked language models. In this paper, we
address this research gap by performing a large-scale evaluation study for 16
text classification datasets covering binary, multiclass, and multilabel
problems. In particular, we compare zero- and few-shot approaches of large
language models to fine-tuning smaller language models. We also analyse the
results by prompt, classification type, domain, and number of labels. In
general, the results show how fine-tuning smaller and more efficient language
models can still outperform few-shot approaches of larger language models,
which have room for improvement when it comes to text classification.
\\ ( https://arxiv.org/abs/2403.17661 ,  14172kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17706
Date: Tue, 26 Mar 2024 13:50:34 GMT   (1108kb,D)

Title: Enhanced Short Text Modeling: Leveraging Large Language Models for Topic
  Refinement
Authors: Shuyu Chang,Rui Wang,Peng Ren,Haiping Huang
Categories: cs.CL cs.AI
Comments: 6 pages, 4 figures
\\
  Crafting effective topic models for brief texts, like tweets and news
headlines, is essential for capturing the swift shifts in social dynamics.
Traditional topic models, however, often fall short in accurately representing
the semantic intricacies of short texts due to their brevity and lack of
contextual data. In our study, we harness the advanced capabilities of Large
Language Models (LLMs) to introduce a novel approach termed "Topic Refinement".
This approach does not directly involve itself in the initial modeling of
topics but focuses on improving topics after they have been mined. By employing
prompt engineering, we direct LLMs to eliminate off-topic words within a given
topic, ensuring that only contextually relevant words are preserved or
substituted with ones that fit better semantically. This method emulates
human-like scrutiny and improvement of topics, thereby elevating the semantic
quality of the topics generated by various models. Our comprehensive evaluation
across three unique datasets has shown that our topic refinement approach
significantly enhances the semantic coherence of topics.
\\ ( https://arxiv.org/abs/2403.17706 ,  1108kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17733
Date: Tue, 26 Mar 2024 14:20:42 GMT   (1268kb,D)

Title: Continual Few-shot Event Detection via Hierarchical Augmentation
  Networks
Authors: Chenlong Zhang, Pengfei Cao, Yubo Chen, Kang Liu, Zhiqiang Zhang,
  Mengshu Sun, Jun Zhao
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  Traditional continual event detection relies on abundant labeled data for
training, which is often impractical to obtain in real-world applications. In
this paper, we introduce continual few-shot event detection (CFED), a more
commonly encountered scenario when a substantial number of labeled samples are
not accessible. The CFED task is challenging as it involves memorizing previous
event types and learning new event types with few-shot samples. To mitigate
these challenges, we propose a memory-based framework: Hierarchical
Augmentation Networks (HANet). To memorize previous event types with limited
memory, we incorporate prototypical augmentation into the memory set. For the
issue of learning new event types in few-shot scenarios, we propose a
contrastive augmentation module for token representations. Despite comparing
with previous state-of-the-art methods, we also conduct comparisons with
ChatGPT. Experiment results demonstrate that our method significantly
outperforms all of these methods in multiple continual few-shot event detection
tasks.
\\ ( https://arxiv.org/abs/2403.17733 ,  1268kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17748
Date: Tue, 26 Mar 2024 14:40:10 GMT   (7988kb,D)

Title: UCxn: Typologically Informed Annotation of Constructions Atop Universal
  Dependencies
Authors: Leonie Weissweiler, Nina B\"obel, Kirian Guiller, Santiago Herrera,
  Wesley Scivetti, Arthur Lorenzi, Nurit Melnik, Archna Bhatia, Hinrich
  Sch\"utze, Lori Levin, Amir Zeldes, Joakim Nivre, William Croft, Nathan
  Schneider
Categories: cs.CL
Comments: LREC-COLING 2024
\\
  The Universal Dependencies (UD) project has created an invaluable collection
of treebanks with contributions in over 140 languages. However, the UD
annotations do not tell the full story. Grammatical constructions that convey
meaning through a particular combination of several morphosyntactic elements --
for example, interrogative sentences with special markers and/or word orders --
are not labeled holistically. We argue for (i) augmenting UD annotations with a
'UCxn' annotation layer for such meaning-bearing grammatical constructions, and
(ii) approaching this in a typologically informed way so that morphosyntactic
strategies can be compared across languages. As a case study, we consider five
construction families in ten languages, identifying instances of each
construction in UD treebanks through the use of morphosyntactic patterns. In
addition to findings regarding these particular constructions, our study yields
important insights on methodology for describing and identifying constructions
in language-general and language-particular ways, and lays the foundation for
future constructional enrichment of UD treebanks.
\\ ( https://arxiv.org/abs/2403.17748 ,  7988kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17752
Date: Tue, 26 Mar 2024 14:43:48 GMT   (12812kb,D)

Title: Can multiple-choice questions really be useful in detecting the
  abilities of LLMs?
Authors: Wangyue Li, Liangzhi Li, Tong Xiang, Xiao Liu, Wei Deng, Noa Garcia
Categories: cs.CL
\\
  Multiple-choice questions (MCQs) are widely used in the evaluation of large
language models (LLMs) due to their simplicity and efficiency. However, there
are concerns about whether MCQs can truly measure LLM's capabilities,
particularly in knowledge-intensive scenarios where long-form generation (LFG)
answers are required. The misalignment between the task and the evaluation
method demands a thoughtful analysis of MCQ's efficacy, which we undertake in
this paper by evaluating nine LLMs on four question-answering (QA) datasets in
two languages: Chinese and English. We identify a significant issue: LLMs
exhibit an order sensitivity in bilingual MCQs, favoring answers located at
specific positions, i.e., the first position. We further quantify the gap
between MCQs and long-form generation questions (LFGQs) by comparing their
direct outputs, token logits, and embeddings. Our results reveal a relatively
low correlation between answers from MCQs and LFGQs for identical questions.
Additionally, we propose two methods to quantify the consistency and confidence
of LLMs' output, which can be generalized to other QA evaluation benchmarks.
Notably, our analysis challenges the idea that the higher the consistency, the
greater the accuracy. We also find MCQs to be less reliable than LFGQs in terms
of expected calibration error. Finally, the misalignment between MCQs and LFGQs
is not only reflected in the evaluation performance but also in the embedding
space. Our code and models can be accessed at
https://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs.
\\ ( https://arxiv.org/abs/2403.17752 ,  12812kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17760
Date: Tue, 26 Mar 2024 14:51:12 GMT   (9009kb,D)

Title: Constructions Are So Difficult That Even Large Language Models Get Them
  Right for the Wrong Reasons
Authors: Shijia Zhou, Leonie Weissweiler, Taiqi He, Hinrich Sch\"utze, David R.
  Mortensen, Lori Levin
Categories: cs.CL
Comments: LREC-COLING 2024
\\
  In this paper, we make a contribution that can be understood from two
perspectives: from an NLP perspective, we introduce a small challenge dataset
for NLI with large lexical overlap, which minimises the possibility of models
discerning entailment solely based on token distinctions, and show that GPT-4
and Llama 2 fail it with strong bias. We then create further challenging
sub-tasks in an effort to explain this failure. From a Computational
Linguistics perspective, we identify a group of constructions with three
classes of adjectives which cannot be distinguished by surface features. This
enables us to probe for LLM's understanding of these constructions in various
ways, and we find that they fail in a variety of ways to distinguish between
them, suggesting that they don't adequately represent their meaning or capture
the lexical properties of phrasal heads.
\\ ( https://arxiv.org/abs/2403.17760 ,  9009kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17768
Date: Tue, 26 Mar 2024 14:54:48 GMT   (2784kb,D)

Title: SciNews: From Scholarly Complexities to Public Narratives -- A Dataset
  for Scientific News Report Generation
Authors: Dongqi Pu, Yifan Wang, Jia Loy, Vera Demberg
Categories: cs.CL cs.AI cs.LG
Comments: LREC-COLING 2024 Main Conference Paper
\\
  Scientific news reports serve as a bridge, adeptly translating complex
research articles into reports that resonate with the broader public. The
automated generation of such narratives enhances the accessibility of scholarly
insights. In this paper, we present a new corpus to facilitate this paradigm
development. Our corpus comprises a parallel compilation of academic
publications and their corresponding scientific news reports across nine
disciplines. To demonstrate the utility and reliability of our dataset, we
conduct an extensive analysis, highlighting the divergences in readability and
brevity between scientific news narratives and academic manuscripts. We
benchmark our dataset employing state-of-the-art text generation models. The
evaluation process involves both automatic and human evaluation, which lays the
groundwork for future explorations into the automated generation of scientific
news reports. The dataset and code related to this work are available at
https://dongqi.me/projects/SciNews.
\\ ( https://arxiv.org/abs/2403.17768 ,  2784kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17816
Date: Tue, 26 Mar 2024 15:53:02 GMT   (15022kb,D)

Title: Graph Language Model (GLM): A new graph-based approach to detect social
  instabilities
Authors: Wallyson Lemes de Oliveira, Vahid Shamsaddini, Ali Ghofrani, Rahul
  Singh Inda, Jithendra Sai Veeramaneni, \'Etienne Voutaz
Categories: cs.CL
\\
  This scientific report presents a novel methodology for the early prediction
of important political events using News datasets. The methodology leverages
natural language processing, graph theory, clique analysis, and semantic
relationships to uncover hidden predictive signals within the data. Initially,
we designed a preliminary version of the method and tested it on a few events.
This analysis revealed limitations in the initial research phase. We then
enhanced the model in two key ways: first, we added a filtration step to only
consider politically relevant news before further processing; second, we
adjusted the input features to make the alert system more sensitive to
significant spikes in the data. After finalizing the improved methodology, we
tested it on eleven events including US protests, the Ukraine war, and French
protests. Results demonstrate the superiority of our approach compared to
baseline methods. Through targeted refinements, our model can now provide
earlier and more accurate predictions of major political events based on subtle
patterns in news data.
\\ ( https://arxiv.org/abs/2403.17816 ,  15022kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17848
Date: Tue, 26 Mar 2024 16:37:54 GMT   (2086kb,D)

Title: ArabicaQA: A Comprehensive Dataset for Arabic Question Answering
Authors: Abdelrahman Abdallah, Mahmoud Kasem, Mahmoud Abdalla, Mohamed Mahmoud,
  Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt
Categories: cs.CL cs.IR
Comments: Accepted at SIGIR 2024
\\
  In this paper, we address the significant gap in Arabic natural language
processing (NLP) resources by introducing ArabicaQA, the first large-scale
dataset for machine reading comprehension and open-domain question answering in
Arabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701
unanswerable questions created by crowdworkers to look similar to answerable
ones, along with additional labels of open-domain questions marks a crucial
advancement in Arabic NLP resources. We also present AraDPR, the first dense
passage retrieval model trained on the Arabic Wikipedia corpus, specifically
designed to tackle the unique challenges of Arabic text retrieval. Furthermore,
our study includes extensive benchmarking of large language models (LLMs) for
Arabic question answering, critically evaluating their performance in the
Arabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking
of LLMs in Arabic question answering offer significant advancements in the
field of Arabic NLP. The dataset and code are publicly accessible for further
research https://github.com/DataScienceUIBK/ArabicaQA.
\\ ( https://arxiv.org/abs/2403.17848 ,  2086kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17853
Date: Tue, 26 Mar 2024 16:42:30 GMT   (3380kb,D)

Title: Using Domain Knowledge to Guide Dialog Structure Induction via Neural
  Probabilistic Soft Logic
Authors: Connor Pryor, Quan Yuan, Jeremiah Liu, Mehran Kazemi, Deepak
  Ramachandran, Tania Bedrax-Weiss, Lise Getoor
Categories: cs.CL cs.LG
\\
  Dialog Structure Induction (DSI) is the task of inferring the latent dialog
structure (i.e., a set of dialog states and their temporal transitions) of a
given goal-oriented dialog. It is a critical component for modern dialog system
design and discourse analysis. Existing DSI approaches are often purely
data-driven, deploy models that infer latent states without access to domain
knowledge, underperform when the training corpus is limited/noisy, or have
difficulty when test dialogs exhibit distributional shifts from the training
domain. This work explores a neural-symbolic approach as a potential solution
to these problems. We introduce Neural Probabilistic Soft Logic Dialogue
Structure Induction (NEUPSL DSI), a principled approach that injects symbolic
knowledge into the latent space of a generative neural model. We conduct a
thorough empirical investigation on the effect of NEUPSL DSI learning on hidden
representation quality, few-shot learning, and out-of-domain generalization
performance. Over three dialog structure induction datasets and across
unsupervised and semi-supervised settings for standard and cross-domain
generalization, the injection of symbolic knowledge using NEUPSL DSI provides a
consistent boost in performance over the canonical baselines.
\\ ( https://arxiv.org/abs/2403.17853 ,  3380kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17856
Date: Tue, 26 Mar 2024 16:45:27 GMT   (2686kb,D)

Title: Verbing Weirds Language (Models): Evaluation of English Zero-Derivation
  in Five LLMs
Authors: David R. Mortensen, Valentina Izrailevitch, Yunze Xiao, Hinrich
  Sch\"utze, Leonie Weissweiler
Categories: cs.CL
Comments: LREC-COLING 2024
\\
  Lexical-syntactic flexibility, in the form of conversion (or zero-derivation)
is a hallmark of English morphology. In conversion, a word with one part of
speech is placed in a non-prototypical context, where it is coerced to behave
as if it had a different part of speech. However, while this process affects a
large part of the English lexicon, little work has been done to establish the
degree to which language models capture this type of generalization. This paper
reports the first study on the behavior of large language models with reference
to conversion. We design a task for testing lexical-syntactic flexibility --
the degree to which models can generalize over words in a construction with a
non-prototypical part of speech. This task is situated within a natural
language inference paradigm. We test the abilities of five language models --
two proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral
7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task,
followed by GPT-3.5, but that the open source language models are also able to
perform it and that the 7B parameter Mistral displays as little difference
between its baseline performance on the natural language inference task and the
non-prototypical syntactic category task, as the massive GPT-4.
\\ ( https://arxiv.org/abs/2403.17856 ,  2686kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17859
Date: Tue, 26 Mar 2024 16:48:13 GMT   (19023kb,D)

Title: ChroniclingAmericaQA: A Large-scale Question Answering Dataset based on
  Historical American Newspaper Pages
Authors: Bhawna Piryani, Jamshid Mozafari, Adam Jatowt
Categories: cs.CL
Comments: Accepted at SIGIR 2024
\\
  Question answering (QA) and Machine Reading Comprehension (MRC) tasks have
significantly advanced in recent years due to the rapid development of deep
learning techniques and, more recently, large language models. At the same
time, many benchmark datasets have become available for QA and MRC tasks.
However, most existing large-scale benchmark datasets have been created
predominantly using synchronous document collections like Wikipedia or the Web.
Archival document collections, such as historical newspapers, contain valuable
information from the past that is still not widely used to train large language
models. To further contribute to advancing QA and MRC tasks and to overcome the
limitation of previous datasets, we introduce ChroniclingAmericaQA, a
large-scale dataset with 485K question-answer pairs created based on the
historical newspaper collection Chronicling America. Our dataset is constructed
from a subset of the Chronicling America newspaper collection spanning 120
years. One of the significant challenges for utilizing digitized historical
newspaper collections is the low quality of OCR text. Therefore, to enable
realistic testing of QA models, our dataset can be used in three different
ways: answering questions from raw and noisy content, answering questions from
cleaner, corrected version of the content, as well as answering questions from
scanned images of newspaper pages. This and the fact that ChroniclingAmericaQA
spans the longest time period among available QA datasets make it quite a
unique and useful resource.
\\ ( https://arxiv.org/abs/2403.17859 ,  19023kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17860
Date: Tue, 26 Mar 2024 16:49:25 GMT   (286kb,D)

Title: Exploring LLMs as a Source of Targeted Synthetic Textual Data to
  Minimize High Confidence Misclassifications
Authors: Philip Lippmann, Matthijs Spaan, Jie Yang
Categories: cs.CL
\\
  Natural Language Processing (NLP) models optimized for predictive performance
often make high confidence errors and suffer from vulnerability to adversarial
and out-of-distribution data. Existing work has mainly focused on mitigation of
such errors using either humans or an automated approach. In this study, we
explore the usage of large language models (LLMs) for data augmentation as a
potential solution to the issue of NLP models making wrong predictions with
high confidence during classification tasks. We compare the effectiveness of
synthetic data generated by LLMs with that of human data obtained via the same
procedure. For mitigation, humans or LLMs provide natural language
characterizations of high confidence misclassifications to generate synthetic
data, which are then used to extend the training set. We conduct an extensive
evaluation of our approach on three classification tasks and demonstrate its
effectiveness in reducing the number of high confidence misclassifications
present in the model, all while maintaining the same level of accuracy.
Moreover, we find that the cost gap between humans and LLMs surpasses an order
of magnitude, as LLMs attain human-like performance while being more scalable.
\\ ( https://arxiv.org/abs/2403.17860 ,  286kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17887
Date: Tue, 26 Mar 2024 17:20:04 GMT   (1539kb,D)

Title: The Unreasonable Ineffectiveness of the Deeper Layers
Authors: Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso,
  Daniel A. Roberts
Categories: cs.CL cs.LG stat.ML
Comments: 12 + 10 pages, 5 + 4 figures
Report-no: MIT-CTP/5694
\\
  We empirically study a simple layer-pruning strategy for popular families of
open-weight pretrained LLMs, finding minimal degradation of performance on
different question-answering benchmarks until after a large fraction (up to
half) of the layers are removed. To prune these models, we identify the optimal
block of layers to prune by considering similarity across layers; then, to
"heal" the damage, we perform a small amount of finetuning. In particular, we
use parameter-efficient finetuning (PEFT) methods, specifically quantization
and Low Rank Adapters (QLoRA), such that each of our experiments can be
performed on a single A100 GPU. From a practical perspective, these results
suggest that layer pruning methods can complement other PEFT strategies to
further reduce computational resources of finetuning on the one hand, and can
improve the memory and latency of inference on the other hand. From a
scientific perspective, the robustness of these LLMs to the deletion of layers
implies either that current pretraining methods are not properly leveraging the
parameters in the deeper layers of the network or that the shallow layers play
a critical role in storing knowledge.
\\ ( https://arxiv.org/abs/2403.17887 ,  1539kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17011
Date: Tue, 2 Jan 2024 18:12:03 GMT   (1947kb,D)

Title: SUDO: a framework for evaluating clinical artificial intelligence
  systems without ground-truth annotations
Authors: Dani Kiyasseh, Aaron Cohen, Chengsheng Jiang, Nicholas Altieri
Categories: cs.LG cs.AI cs.CY
\\
  A clinical artificial intelligence (AI) system is often validated on a
held-out set of data which it has not been exposed to before (e.g., data from a
different hospital with a distinct electronic health record system). This
evaluation process is meant to mimic the deployment of an AI system on data in
the wild; those which are currently unseen by the system yet are expected to be
encountered in a clinical setting. However, when data in the wild differ from
the held-out set of data, a phenomenon referred to as distribution shift, and
lack ground-truth annotations, it becomes unclear the extent to which AI-based
findings can be trusted on data in the wild. Here, we introduce SUDO, a
framework for evaluating AI systems without ground-truth annotations. SUDO
assigns temporary labels to data points in the wild and directly uses them to
train distinct models, with the highest performing model indicative of the most
likely label. Through experiments with AI systems developed for dermatology
images, histopathology patches, and clinical reports, we show that SUDO can be
a reliable proxy for model performance and thus identify unreliable
predictions. We also demonstrate that SUDO informs the selection of models and
allows for the previously out-of-reach assessment of algorithmic bias for data
in the wild without ground-truth annotations. The ability to triage unreliable
predictions for further inspection and assess the algorithmic bias of AI
systems can improve the integrity of research findings and contribute to the
deployment of ethical AI systems in medicine.
\\ ( https://arxiv.org/abs/2403.17011 ,  1947kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17031
Date: Sun, 24 Mar 2024 02:59:27 GMT   (5106kb,D)

Title: The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR
  Summarization
Authors: Shengyi Huang, Michael Noukhovitch, Arian Hosseini, Kashif Rasul,
  Weixun Wang, Lewis Tunstall
Categories: cs.LG
\\
  This work is the first to openly reproduce the Reinforcement Learning from
Human Feedback (RLHF) scaling behaviors reported in OpenAI's seminal TL;DR
summarization work. We create an RLHF pipeline from scratch, enumerate over 20
key implementation details, and share key insights during the reproduction. Our
RLHF-trained Pythia models demonstrate significant gains in response quality
that scale with model size, with our 2.8B, 6.9B models outperforming OpenAI's
released 1.3B checkpoint. We publicly release the trained model checkpoints and
code to facilitate further research and accelerate progress in the field
(\url{https://github.com/vwxyzjn/summarize_from_feedback_details}).
\\ ( https://arxiv.org/abs/2403.17031 ,  5106kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17032
Date: Sun, 24 Mar 2024 06:52:37 GMT   (1507kb)

Title: Stochastic parameter reduced-order model based on hybrid machine
  learning approaches
Authors: Cheng Fang, Jinqiao Duan
Categories: cs.LG
\\
  Establishing appropriate mathematical models for complex systems in natural
phenomena not only helps deepen our understanding of nature but can also be
used for state estimation and prediction. However, the extreme complexity of
natural phenomena makes it extremely challenging to develop full-order models
(FOMs) and apply them to studying many quantities of interest. In contrast,
appropriate reduced-order models (ROMs) are favored due to their high
computational efficiency and ability to describe the key dynamics and
statistical characteristics of natural phenomena. Taking the viscous Burgers
equation as an example, this paper constructs a Convolutional
Autoencoder-Reservoir Computing-Normalizing Flow algorithm framework, where the
Convolutional Autoencoder is used to construct latent space representations,
and the Reservoir Computing-Normalizing Flow framework is used to characterize
the evolution of latent state variables. In this way, a data-driven stochastic
parameter reduced-order model is constructed to describe the complex system and
its dynamic behavior.
\\ ( https://arxiv.org/abs/2403.17032 ,  1507kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17091
Date: Mon, 25 Mar 2024 18:28:45 GMT   (11395kb)

Title: Offline Reinforcement Learning: Role of State Aggregation and Trajectory
  Data
Authors: Zeyu Jia, Alexander Rakhlin, Ayush Sekhari, Chen-Yu Wei
Categories: cs.LG cs.AI stat.ML
\\
  We revisit the problem of offline reinforcement learning with value function
realizability but without Bellman completeness. Previous work by Xie and Jiang
(2021) and Foster et al. (2022) left open the question whether a bounded
concentrability coefficient along with trajectory-based offline data admits a
polynomial sample complexity. In this work, we provide a negative answer to
this question for the task of offline policy evaluation. In addition to
addressing this question, we provide a rather complete picture for offline
policy evaluation with only value function realizability. Our primary findings
are threefold: 1) The sample complexity of offline policy evaluation is
governed by the concentrability coefficient in an aggregated Markov Transition
Model jointly determined by the function class and the offline data
distribution, rather than that in the original MDP. This unifies and
generalizes the ideas of Xie and Jiang (2021) and Foster et al. (2022), 2) The
concentrability coefficient in the aggregated Markov Transition Model may grow
exponentially with the horizon length, even when the concentrability
coefficient in the original MDP is small and the offline data is admissible
(i.e., the data distribution equals the occupancy measure of some policy), 3)
Under value function realizability, there is a generic reduction that can
convert any hard instance with admissible data to a hard instance with
trajectory data, implying that trajectory data offers no extra benefits over
admissible data. These three pieces jointly resolve the open problem, though
each of them could be of independent interest.
\\ ( https://arxiv.org/abs/2403.17091 ,  11395kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17093
Date: Mon, 25 Mar 2024 18:32:22 GMT   (1221kb,D)

Title: Enhancing UAV Security Through Zero Trust Architecture: An Advanced Deep
  Learning and Explainable AI Analysis
Authors: Ekramul Haque, Kamrul Hasan, Imtiaz Ahmed, Md. Sahabul Alam, Tariqul
  Islam
Categories: cs.LG eess.SP
Comments: 6 pages, 5 figures
\\
  In the dynamic and ever-changing domain of Unmanned Aerial Vehicles (UAVs),
the utmost importance lies in guaranteeing resilient and lucid security
measures. This study highlights the necessity of implementing a Zero Trust
Architecture (ZTA) to enhance the security of unmanned aerial vehicles (UAVs),
hence departing from conventional perimeter defences that may expose
vulnerabilities. The Zero Trust Architecture (ZTA) paradigm requires a rigorous
and continuous process of authenticating all network entities and
communications. The accuracy of our methodology in detecting and identifying
unmanned aerial vehicles (UAVs) is 84.59\%. This is achieved by utilizing Radio
Frequency (RF) signals within a Deep Learning framework, a unique method.
Precise identification is crucial in Zero Trust Architecture (ZTA), as it
determines network access. In addition, the use of eXplainable Artificial
Intelligence (XAI) tools such as SHapley Additive exPlanations (SHAP) and Local
Interpretable Model-agnostic Explanations (LIME) contributes to the improvement
of the model's transparency and interpretability. Adherence to Zero Trust
Architecture (ZTA) standards guarantees that the classifications of unmanned
aerial vehicles (UAVs) are verifiable and comprehensible, enhancing security
within the UAV field.
\\ ( https://arxiv.org/abs/2403.17093 ,  1221kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17105
Date: Mon, 25 Mar 2024 18:43:58 GMT   (928kb,D)

Title: Stochastic Gradient Langevin Unlearning
Authors: Eli Chien, Haoyu Wang, Ziang Chen, Pan Li
Categories: cs.LG cs.CR
Comments: arXiv admin note: substantial text overlap with arXiv:2401.10371
\\
  ``The right to be forgotten'' ensured by laws for user data privacy becomes
increasingly important. Machine unlearning aims to efficiently remove the
effect of certain data points on the trained model parameters so that it can be
approximately the same as if one retrains the model from scratch. This work
proposes stochastic gradient Langevin unlearning, the first unlearning
framework based on noisy stochastic gradient descent (SGD) with privacy
guarantees for approximate unlearning problems under convexity assumption. Our
results show that mini-batch gradient updates provide a superior
privacy-complexity trade-off compared to the full-batch counterpart. There are
numerous algorithmic benefits of our unlearning approach, including complexity
saving compared to retraining, and supporting sequential and batch unlearning.
To examine the privacy-utility-complexity trade-off of our method, we conduct
experiments on benchmark datasets compared against prior works. Our approach
achieves a similar utility under the same privacy constraint while using $2\%$
and $10\%$ of the gradient computations compared with the state-of-the-art
gradient-based approximate unlearning methods for mini-batch and full-batch
settings, respectively.
\\ ( https://arxiv.org/abs/2403.17105 ,  928kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17130
Date: Mon, 25 Mar 2024 19:15:19 GMT   (2280kb,D)

Title: Exploring the potential of prototype-based soft-labels data distillation
  for imbalanced data classification
Authors: Radu-Andrei Rosu, Mihaela-Elena Breaban, Henri Luchian
Categories: cs.LG cs.AI
Journal-ref: 24th International Symposium on Symbolic and Numeric Algorithms
  for Scientific Computing (SYNASC), pp. 173-180, 2022. IEEE
\\
  Dataset distillation aims at synthesizing a dataset by a small number of
artificially generated data items, which, when used as training data, reproduce
or approximate a machine learning (ML) model as if it were trained on the
entire original dataset. Consequently, data distillation methods are usually
tied to a specific ML algorithm. While recent literature deals mainly with
distillation of large collections of images in the context of neural network
models, tabular data distillation is much less represented and mainly focused
on a theoretical perspective. The current paper explores the potential of a
simple distillation technique previously proposed in the context of
Less-than-one shot learning. The main goal is to push further the performance
of prototype-based soft-labels distillation in terms of classification
accuracy, by integrating optimization steps in the distillation process. The
analysis is performed on real-world data sets with various degrees of
imbalance. Experimental studies trace the capability of the method to distill
the data, but also the opportunity to act as an augmentation method, i.e. to
generate new data that is able to increase model accuracy when used in
conjunction with - as opposed to instead of - the original data.
\\ ( https://arxiv.org/abs/2403.17130 ,  2280kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17159
Date: Mon, 25 Mar 2024 20:16:16 GMT   (4458kb,D)

Title: Less Is More - On the Importance of Sparsification for Transformers and
  Graph Neural Networks for TSP
Authors: Attila Lischka, Jiaming Wu, Rafael Basso, Morteza Haghir Chehreghani,
  Bal\'azs Kulcs\'ar
Categories: cs.LG cs.AI
Comments: 14 pages, 6 figures
\\
  Most of the recent studies tackling routing problems like the Traveling
Salesman Problem (TSP) with machine learning use a transformer or Graph Neural
Network (GNN) based encoder architecture. However, many of them apply these
encoders naively by allowing them to aggregate information over the whole TSP
instances. We, on the other hand, propose a data preprocessing method that
allows the encoders to focus on the most relevant parts of the TSP instances
only. In particular, we propose graph sparsification for TSP graph
representations passed to GNNs and attention masking for TSP instances passed
to transformers where the masks correspond to the adjacency matrices of the
sparse TSP graph representations. Furthermore, we propose ensembles of
different sparsification levels allowing models to focus on the most promising
parts while also allowing information flow between all nodes of a TSP instance.
In the experimental studies, we show that for GNNs appropriate sparsification
and ensembles of different sparsification levels lead to substantial
performance increases of the overall architecture. We also design a new,
state-of-the-art transformer encoder with ensembles of attention masking. These
transformers increase model performance from a gap of $0.16\%$ to $0.10\%$ for
TSP instances of size 100 and from $0.02\%$ to $0.00\%$ for TSP instances of
size 50.
\\ ( https://arxiv.org/abs/2403.17159 ,  4458kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17174
Date: Mon, 25 Mar 2024 20:43:17 GMT   (257kb)

Title: Belief Samples Are All You Need For Social Learning
Authors: Mahyar JafariNodeh, Amir Ajorlou, Ali Jadbabaie
Categories: cs.LG cs.SI cs.SY eess.SY math.DS math.OC
Comments: 6 pages
\\
  In this paper, we consider the problem of social learning, where a group of
agents embedded in a social network are interested in learning an underlying
state of the world. Agents have incomplete, noisy, and heterogeneous sources of
information, providing them with recurring private observations of the
underlying state of the world. Agents can share their learning experience with
their peers by taking actions observable to them, with values from a finite
feasible set of states. Actions can be interpreted as samples from the beliefs
which agents may form and update on what the true state of the world is.
Sharing samples, in place of full beliefs, is motivated by the limited
communication, cognitive, and information-processing resources available to
agents especially in large populations. Previous work (Salhab et al.) poses the
question as to whether learning with probability one is still achievable if
agents are only allowed to communicate samples from their beliefs. We provide a
definite positive answer to this question, assuming a strongly connected
network and a ``collective distinguishability'' assumption, which are both
required for learning even in full-belief-sharing settings. In our proposed
belief update mechanism, each agent's belief is a normalized weighted geometric
interpolation between a fully Bayesian private belief -- aggregating
information from the private source -- and an ensemble of empirical
distributions of the samples shared by her neighbors over time. By carefully
constructing asymptotic almost-sure lower/upper bounds on the frequency of
shared samples matching the true state/or not, we rigorously prove the
convergence of all the beliefs to the true state, with probability one.
\\ ( https://arxiv.org/abs/2403.17174 ,  257kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17210
Date: Mon, 25 Mar 2024 21:37:31 GMT   (382kb,D)

Title: CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug
  Interactions
Authors: Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Serbetar Karlo,
  Dong-Kyu Chae
Categories: cs.LG cs.AI cs.IR q-bio.BM q-bio.MN
Comments: 8 Pages, 4 Figures; In review in IEEE/ACM Transactions on
  Computational Biology and Bioinformatics
\\
  Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process
of drug development. DDIs occur when one drug's properties are affected by the
inclusion of other drugs. Detecting favorable DDIs has the potential to pave
the way for creating and advancing innovative medications applicable in
practical settings. However, existing DDI prediction models continue to face
challenges related to generalization in extreme cases, robust feature
extraction, and real-life application possibilities. We aim to address these
challenges by leveraging the effectiveness of context-aware deep graph learning
by introducing a novel framework named CADGL. Based on a customized variational
graph autoencoder (VGAE), we capture critical structural and physio-chemical
information using two context preprocessors for feature extraction from two
different perspectives: local neighborhood and molecular context, in a
heterogeneous graphical structure. Our customized VGAE consists of a graph
encoder, a latent information encoder, and an MLP decoder. CADGL surpasses
other state-of-the-art DDI prediction models, excelling in predicting
clinically valuable novel DDIs, supported by rigorous case studies.
\\ ( https://arxiv.org/abs/2403.17210 ,  382kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17212
Date: Mon, 25 Mar 2024 21:39:33 GMT   (270kb,D)

Title: Sanity Checks for Explanation Uncertainty
Authors: Matias Valdenegro-Toro and Mihir Mulye
Categories: cs.LG cs.AI
Comments: 15 pages, 7 figures, 3 tables
\\
  Explanations for machine learning models can be hard to interpret or be
wrong. Combining an explanation method with an uncertainty estimation method
produces explanation uncertainty. Evaluating explanation uncertainty is
difficult. In this paper we propose sanity checks for uncertainty explanation
methods, where a weight and data randomization tests are defined for
explanations with uncertainty, allowing for quick tests to combinations of
uncertainty and explanation methods. We experimentally show the validity and
effectiveness of these tests on the CIFAR10 and California Housing datasets,
noting that Ensembles seem to consistently pass both tests with Guided
Backpropagation, Integrated Gradients, and LIME explanations.
\\ ( https://arxiv.org/abs/2403.17212 ,  270kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17224
Date: Mon, 25 Mar 2024 21:56:02 GMT   (653kb,D)

Title: Uncertainty Quantification for Gradient-based Explanations in Neural
  Networks
Authors: Mihir Mulye and Matias Valdenegro-Toro
Categories: cs.LG cs.AI
Comments: 24 pages, 11 figures
\\
  Explanation methods help understand the reasons for a model's prediction.
These methods are increasingly involved in model debugging, performance
optimization, and gaining insights into the workings of a model. With such
critical applications of these methods, it is imperative to measure the
uncertainty associated with the explanations generated by these methods. In
this paper, we propose a pipeline to ascertain the explanation uncertainty of
neural networks by combining uncertainty estimation methods and explanation
methods. We use this pipeline to produce explanation distributions for the
CIFAR-10, FER+, and California Housing datasets. By computing the coefficient
of variation of these distributions, we evaluate the confidence in the
explanation and determine that the explanations generated using Guided
Backpropagation have low uncertainty associated with them. Additionally, we
compute modified pixel insertion/deletion metrics to evaluate the quality of
the generated explanations.
\\ ( https://arxiv.org/abs/2403.17224 ,  653kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17236
Date: Mon, 25 Mar 2024 22:26:09 GMT   (380kb,D)

Title: Neural Image Compression with Quantization Rectifier
Authors: Wei Luo, Bo Chen
Categories: cs.LG
Comments: Published at International Conference on Machine Learning (ICML)
  Neural Compression Workshop 2023, Honolulu, Hawaii, USA. PMLR 202, 2023.
  Copyright 2023 by the authors
\\
  Neural image compression has been shown to outperform traditional image
codecs in terms of rate-distortion performance. However, quantization
introduces errors in the compression process, which can degrade the quality of
the compressed image. Existing approaches address the train-test mismatch
problem incurred during quantization, the random impact of quantization on the
expressiveness of image features is still unsolved. This paper presents a novel
quantization rectifier (QR) method for image compression that leverages image
feature correlation to mitigate the impact of quantization. Our method designs
a neural network architecture that predicts unquantized features from the
quantized ones, preserving feature expressiveness for better image
reconstruction quality. We develop a soft-to-predictive training technique to
integrate QR into existing neural image codecs. In evaluation, we integrate QR
into state-of-the-art neural image codecs and compare enhanced models and
baselines on the widely-used Kodak benchmark. The results show consistent
coding efficiency improvement by QR with a negligible increase in the running
time.
\\ ( https://arxiv.org/abs/2403.17236 ,  380kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17239
Date: Mon, 25 Mar 2024 22:39:47 GMT   (1900kb,D)

Title: Manufacturing Service Capability Prediction with Graph Neural Networks
Authors: Yunqing Li, Xiaorui Liu and Binil Starly
Categories: cs.LG cs.SI
\\
  In the current landscape, the predominant methods for identifying
manufacturing capabilities from manufacturers rely heavily on keyword matching
and semantic matching. However, these methods often fall short by either
overlooking valuable hidden information or misinterpreting critical data.
Consequently, such approaches result in an incomplete identification of
manufacturers' capabilities. This underscores the pressing need for data-driven
solutions to enhance the accuracy and completeness of manufacturing capability
identification. To address the need, this study proposes a Graph Neural
Network-based method for manufacturing service capability identification over a
knowledge graph. To enhance the identification performance, this work
introduces a novel approach that involves aggregating information from the
graph nodes' neighborhoods as well as oversampling the graph data, which can be
effectively applied across a wide range of practical scenarios. Evaluations
conducted on a Manufacturing Service Knowledge Graph and subsequent ablation
studies demonstrate the efficacy and robustness of the proposed approach. This
study not only contributes a innovative method for inferring manufacturing
service capabilities but also significantly augments the quality of
Manufacturing Service Knowledge Graphs.
\\ ( https://arxiv.org/abs/2403.17239 ,  1900kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17259
Date: Mon, 25 Mar 2024 23:07:31 GMT   (2465kb)

Title: Diffusion-based Negative Sampling on Graphs for Link Prediction
Authors: Trung-Kien Nguyen, Yuan Fang
Categories: cs.LG cs.SI
Comments: Accepted in the TheWebConf 2024
\\
  Link prediction is a fundamental task for graph analysis with important
applications on the Web, such as social network analysis and recommendation
systems, etc. Modern graph link prediction methods often employ a contrastive
approach to learn robust node representations, where negative sampling is
pivotal. Typical negative sampling methods aim to retrieve hard examples based
on either predefined heuristics or automatic adversarial approaches, which
might be inflexible or difficult to control. Furthermore, in the context of
link prediction, most previous methods sample negative nodes from existing
substructures of the graph, missing out on potentially more optimal samples in
the latent space. To address these issues, we investigate a novel strategy of
multi-level negative sampling that enables negative node generation with
flexible and controllable ``hardness'' levels from the latent space. Our
method, called Conditional Diffusion-based Multi-level Negative Sampling
(DMNS), leverages the Markov chain property of diffusion models to generate
negative nodes in multiple levels of variable hardness and reconcile them for
effective graph link prediction. We further demonstrate that DMNS follows the
sub-linear positivity principle for robust negative sampling. Extensive
experiments on several benchmark datasets demonstrate the effectiveness of
DMNS.
\\ ( https://arxiv.org/abs/2403.17259 ,  2465kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17287
Date: Tue, 26 Mar 2024 00:33:49 GMT   (178kb,D)

Title: Not All Federated Learning Algorithms Are Created Equal: A Performance
  Evaluation Study
Authors: Gustav A. Baumgart, Jaemin Shin, Ali Payani, Myungjin Lee, Ramana Rao
  Kompella
Categories: cs.LG cs.DC
\\
  Federated Learning (FL) emerged as a practical approach to training a model
from decentralized data. The proliferation of FL led to the development of
numerous FL algorithms and mechanisms. Many prior efforts have given their
primary focus on accuracy of those approaches, but there exists little
understanding of other aspects such as computational overheads, performance and
training stability, etc. To bridge this gap, we conduct extensive performance
evaluation on several canonical FL algorithms (FedAvg, FedProx, FedYogi,
FedAdam, SCAFFOLD, and FedDyn) by leveraging an open-source federated learning
framework called Flame. Our comprehensive measurement study reveals that no
single algorithm works best across different performance metrics. A few key
observations are: (1) While some state-of-the-art algorithms achieve higher
accuracy than others, they incur either higher computation overheads (FedDyn)
or communication overheads (SCAFFOLD). (2) Recent algorithms present smaller
standard deviation in accuracy across clients than FedAvg, indicating that the
advanced algorithms' performances are stable. (3) However, algorithms such as
FedDyn and SCAFFOLD are more prone to catastrophic failures without the support
of additional techniques such as gradient clipping. We hope that our empirical
study can help the community to build best practices in evaluating FL
algorithms.
\\ ( https://arxiv.org/abs/2403.17287 ,  178kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17329
Date: Tue, 26 Mar 2024 02:24:32 GMT   (7652kb,D)

Title: Deep Support Vectors
Authors: Junhoo Lee, Hyunho Lee, Kyomin Hwang, Nojun Kwak
Categories: cs.LG cs.AI
\\
  While the success of deep learning is commonly attributed to its theoretical
equivalence with Support Vector Machines (SVM), the practical implications of
this relationship have not been thoroughly explored. This paper pioneers an
exploration in this domain, specifically focusing on the identification of Deep
Support Vectors (DSVs) within deep learning models. We introduce the concept of
DeepKKT conditions, an adaptation of the traditional Karush-Kuhn-Tucker (KKT)
conditions tailored for deep learning. Through empirical investigations, we
illustrate that DSVs exhibit similarities to support vectors in SVM, offering a
tangible method to interpret the decision-making criteria of models.
Additionally, our findings demonstrate that models can be effectively
reconstructed using DSVs, resembling the process in SVM. The code will be
available.
\\ ( https://arxiv.org/abs/2403.17329 ,  7652kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17351
Date: Tue, 26 Mar 2024 03:29:42 GMT   (1141kb,D)

Title: Learn from Heterophily: Heterophilous Information-enhanced Graph Neural
  Network
Authors: Yilun Zheng, Jiahao Xu and Lihui Chen
Categories: cs.LG cs.SI
\\
  Under circumstances of heterophily, where nodes with different labels tend to
be connected based on semantic meanings, Graph Neural Networks (GNNs) often
exhibit suboptimal performance. Current studies on graph heterophily mainly
focus on aggregation calibration or neighbor extension and address the
heterophily issue by utilizing node features or structural information to
improve GNN representations. In this paper, we propose and demonstrate that the
valuable semantic information inherent in heterophily can be utilized
effectively in graph learning by investigating the distribution of neighbors
for each individual node within the graph. The theoretical analysis is carried
out to demonstrate the efficacy of the idea in enhancing graph learning. Based
on this analysis, we propose HiGNN, an innovative approach that constructs an
additional new graph structure, that integrates heterophilous information by
leveraging node distribution to enhance connectivity between nodes that share
similar semantic characteristics. We conduct empirical assessments on node
classification tasks using both homophilous and heterophilous benchmark
datasets and compare HiGNN to popular GNN baselines and SoTA methods,
confirming the effectiveness in improving graph representations. In addition,
by incorporating heterophilous information, we demonstrate a notable
enhancement in existing GNN-based approaches, and the homophily degree across
real-world datasets, thus affirming the efficacy of our approach.
\\ ( https://arxiv.org/abs/2403.17351 ,  1141kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17381
Date: Tue, 26 Mar 2024 04:59:27 GMT   (466kb,D)

Title: Application-Driven Innovation in Machine Learning
Authors: David Rolnick, Alan Aspuru-Guzik, Sara Beery, Bistra Dilkina, Priya L.
  Donti, Marzyeh Ghassemi, Hannah Kerner, Claire Monteleoni, Esther Rolf,
  Milind Tambe, Adam White
Categories: cs.LG cs.AI
Comments: 12 pages, 3 figures
\\
  As applications of machine learning proliferate, innovative algorithms
inspired by specific real-world challenges have become increasingly important.
Such work offers the potential for significant impact not merely in domains of
application but also in machine learning itself. In this paper, we describe the
paradigm of application-driven research in machine learning, contrasting it
with the more standard paradigm of methods-driven research. We illustrate the
benefits of application-driven machine learning and how this approach can
productively synergize with methods-driven work. Despite these benefits, we
find that reviewing, hiring, and teaching practices in machine learning often
hold back application-driven innovation. We outline how these processes may be
improved.
\\ ( https://arxiv.org/abs/2403.17381 ,  466kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17404
Date: Tue, 26 Mar 2024 05:48:02 GMT   (85kb)

Title: Generalization Error Analysis for Sparse Mixture-of-Experts: A
  Preliminary Study
Authors: Jinze Zhao, Peihao Wang, Zhangyang Wang
Categories: cs.LG
\\
  Mixture-of-Experts (MoE) represents an ensemble methodology that amalgamates
predictions from several specialized sub-models (referred to as experts). This
fusion is accomplished through a router mechanism, dynamically assigning
weights to each expert's contribution based on the input data. Conventional MoE
mechanisms select all available experts, incurring substantial computational
costs. In contrast, Sparse Mixture-of-Experts (Sparse MoE) selectively engages
only a limited number, or even just one expert, significantly reducing
computation overhead while empirically preserving, and sometimes even
enhancing, performance. Despite its wide-ranging applications and these
advantageous characteristics, MoE's theoretical underpinnings have remained
elusive. In this paper, we embark on an exploration of Sparse MoE's
generalization error concerning various critical factors. Specifically, we
investigate the impact of the number of data samples, the total number of
experts, the sparsity in expert selection, the complexity of the routing
mechanism, and the complexity of individual experts. Our analysis sheds light
on \textit{how \textbf{sparsity} contributes to the MoE's generalization},
offering insights from the perspective of classical learning theory.
\\ ( https://arxiv.org/abs/2403.17404 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17410
Date: Tue, 26 Mar 2024 06:06:01 GMT   (2209kb,D)

Title: On permutation-invariant neural networks
Authors: Masanari Kimura, Ryotaro Shimizu, Yuki Hirakawa, Ryosuke Goto, Yuki
  Saito
Categories: cs.LG cs.AI stat.ML
\\
  Conventional machine learning algorithms have traditionally been designed
under the assumption that input data follows a vector-based format, with an
emphasis on vector-centric paradigms. However, as the demand for tasks
involving set-based inputs has grown, there has been a paradigm shift in the
research community towards addressing these challenges. In recent years, the
emergence of neural network architectures such as Deep Sets and Transformers
has presented a significant advancement in the treatment of set-based data.
These architectures are specifically engineered to naturally accommodate sets
as input, enabling more effective representation and processing of set
structures. Consequently, there has been a surge of research endeavors
dedicated to exploring and harnessing the capabilities of these architectures
for various tasks involving the approximation of set functions. This
comprehensive survey aims to provide an overview of the diverse problem
settings and ongoing research efforts pertaining to neural networks that
approximate set functions. By delving into the intricacies of these approaches
and elucidating the associated challenges, the survey aims to equip readers
with a comprehensive understanding of the field. Through this comprehensive
perspective, we hope that researchers can gain valuable insights into the
potential applications, inherent limitations, and future directions of
set-based neural networks. Indeed, from this survey we gain two insights: i)
Deep Sets and its variants can be generalized by differences in the aggregation
function, and ii) the behavior of Deep Sets is sensitive to the choice of the
aggregation function. From these observations, we show that Deep Sets, one of
the well-known permutation-invariant neural networks, can be generalized in the
sense of a quasi-arithmetic mean.
\\ ( https://arxiv.org/abs/2403.17410 ,  2209kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17445
Date: Tue, 26 Mar 2024 07:23:46 GMT   (270kb,D)

Title: Incorporating Exponential Smoothing into MLP: A Simple but Effective
  Sequence Model
Authors: Jiqun Chu, Zuoquan Lin
Categories: cs.LG cs.AI cs.CL
Comments: 12 pages, 5 tables, 3 figures
\\
  Modeling long-range dependencies in sequential data is a crucial step in
sequence learning. A recently developed model, the Structured State Space (S4),
demonstrated significant effectiveness in modeling long-range sequences.
However, It is unclear whether the success of S4 can be attributed to its
intricate parameterization and HiPPO initialization or simply due to State
Space Models (SSMs). To further investigate the potential of the deep SSMs, we
start with exponential smoothing (ETS), a simple SSM, and propose a stacked
architecture by directly incorporating it into an element-wise MLP. We augment
simple ETS with additional parameters and complex field to reduce the inductive
bias. Despite increasing less than 1\% of parameters of element-wise MLP, our
models achieve comparable results to S4 on the LRA benchmark.
\\ ( https://arxiv.org/abs/2403.17445 ,  270kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17447
Date: Tue, 26 Mar 2024 07:26:00 GMT   (436kb,D)

Title: Chain of Compression: A Systematic Approach to Combinationally Compress
  Convolutional Neural Networks
Authors: Yingtao Shen, Minqing Sun, Jie Zhao, An Zou
Categories: cs.LG cs.CV cs.NE
Comments: 10 pages, 15 figures
\\
  Convolutional neural networks (CNNs) have achieved significant popularity,
but their computational and memory intensity poses challenges for
resource-constrained computing systems, particularly with the prerequisite of
real-time performance. To release this burden, model compression has become an
important research focus. Many approaches like quantization, pruning, early
exit, and knowledge distillation have demonstrated the effect of reducing
redundancy in neural networks. Upon closer examination, it becomes apparent
that each approach capitalizes on its unique features to compress the neural
network, and they can also exhibit complementary behavior when combined. To
explore the interactions and reap the benefits from the complementary features,
we propose the Chain of Compression, which works on the combinational sequence
to apply these common techniques to compress the neural network. Validated on
the image-based regression and classification networks across different data
sets, our proposed Chain of Compression can significantly compress the
computation cost by 100-1000 times with ignorable accuracy loss compared with
the baseline model.
\\ ( https://arxiv.org/abs/2403.17447 ,  436kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17456
Date: Tue, 26 Mar 2024 07:41:54 GMT   (45669kb,D)

Title: Imitating Cost-Constrained Behaviors in Reinforcement Learning
Authors: Qian Shao, Pradeep Varakantham, Shih-Fen Cheng
Categories: cs.LG cs.AI
Comments: Accepted to the 34th International Conference on Automated Planning
  and Scheduling (ICAPS-24)
\\
  Complex planning and scheduling problems have long been solved using various
optimization or heuristic approaches. In recent years, imitation learning that
aims to learn from expert demonstrations has been proposed as a viable
alternative to solving these problems. Generally speaking, imitation learning
is designed to learn either the reward (or preference) model or directly the
behavioral policy by observing the behavior of an expert. Existing work in
imitation learning and inverse reinforcement learning has focused on imitation
primarily in unconstrained settings (e.g., no limit on fuel consumed by the
vehicle). However, in many real-world domains, the behavior of an expert is
governed not only by reward (or preference) but also by constraints. For
instance, decisions on self-driving delivery vehicles are dependent not only on
the route preferences/rewards (depending on past demand data) but also on the
fuel in the vehicle and the time available. In such problems, imitation
learning is challenging as decisions are not only dictated by the reward model
but are also dependent on a cost-constrained model. In this paper, we provide
multiple methods that match expert distributions in the presence of trajectory
cost constraints through (a) Lagrangian-based method; (b) Meta-gradients to
find a good trade-off between expected return and minimizing constraint
violation; and (c) Cost-violation-based alternating gradient. We empirically
show that leading imitation learning approaches imitate cost-constrained
behaviors poorly and our meta-gradient-based approach achieves the best
performance.
\\ ( https://arxiv.org/abs/2403.17456 ,  45669kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17467
Date: Tue, 26 Mar 2024 07:55:45 GMT   (472kb,D)

Title: A Unified Kernel for Neural Network Learning
Authors: Shao-Qun Zhang, Zong-Yi Chen, Yong-Ming Tian, Xun Lu
Categories: cs.LG cs.AI
\\
  Past decades have witnessed a great interest in the distinction and
connection between neural network learning and kernel learning. Recent
advancements have made theoretical progress in connecting infinite-wide neural
networks and Gaussian processes. Two predominant approaches have emerged: the
Neural Network Gaussian Process (NNGP) and the Neural Tangent Kernel (NTK). The
former, rooted in Bayesian inference, represents a zero-order kernel, while the
latter, grounded in the tangent space of gradient descents, is a first-order
kernel. In this paper, we present the Unified Neural Kernel (UNK), which
characterizes the learning dynamics of neural networks with gradient descents
and parameter initialization. The proposed UNK kernel maintains the limiting
properties of both NNGP and NTK, exhibiting behaviors akin to NTK with a finite
learning step and converging to NNGP as the learning step approaches infinity.
Besides, we also theoretically characterize the uniform tightness and learning
convergence of the UNK kernel, providing comprehensive insights into this
unified kernel. Experimental results underscore the effectiveness of our
proposed method.
\\ ( https://arxiv.org/abs/2403.17467 ,  472kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17500
Date: Tue, 26 Mar 2024 08:59:37 GMT   (250kb,D)

Title: Variational Graph Auto-Encoder Based Inductive Learning Method for
  Semi-Supervised Classification
Authors: Hanxuan Yang, Zhaoxin Yu, Qingchao Kong, Wei Liu, Wenji Mao
Categories: cs.LG
\\
  Graph representation learning is a fundamental research issue in various
domains of applications, of which the inductive learning problem is
particularly challenging as it requires models to generalize to unseen graph
structures during inference. In recent years, graph neural networks (GNNs) have
emerged as powerful graph models for inductive learning tasks such as node
classification, whereas they typically heavily rely on the annotated nodes
under a fully supervised training setting. Compared with the GNN-based methods,
variational graph auto-encoders (VGAEs) are known to be more generalizable to
capture the internal structural information of graphs independent of node
labels and have achieved prominent performance on multiple unsupervised
learning tasks. However, so far there is still a lack of work focusing on
leveraging the VGAE framework for inductive learning, due to the difficulties
in training the model in a supervised manner and avoiding over-fitting the
proximity information of graphs. To solve these problems and improve the model
performance of VGAEs for inductive graph representation learning, in this work,
we propose the Self-Label Augmented VGAE model. To leverage the label
information for training, our model takes node labels as one-hot encoded inputs
and then performs label reconstruction in model training. To overcome the
scarcity problem of node labels for semi-supervised settings, we further
propose the Self-Label Augmentation Method (SLAM), which uses pseudo labels
generated by our model with a node-wise masking approach to enhance the label
information. Experiments on benchmark inductive learning graph datasets verify
that our proposed model archives promising results on node classification with
particular superiority under semi-supervised learning settings.
\\ ( https://arxiv.org/abs/2403.17500 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17503
Date: Tue, 26 Mar 2024 09:04:18 GMT   (912kb,D)

Title: DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free
  Class-Incremental Learning
Authors: Huiping Zhuang, Run He, Kai Tong, Ziqian Zeng, Cen Chen and Zhiping
  Lin
Categories: cs.LG cs.CV
Comments: Accepted in AAAI 2024
\\
  Class-incremental learning (CIL) under an exemplar-free constraint has
presented a significant challenge. Existing methods adhering to this constraint
are prone to catastrophic forgetting, far more so than replay-based techniques
that retain access to past samples. In this paper, to solve the exemplar-free
CIL problem, we propose a Dual-Stream Analytic Learning (DS-AL) approach. The
DS-AL contains a main stream offering an analytical (i.e., closed-form) linear
solution, and a compensation stream improving the inherent under-fitting
limitation due to adopting linear mapping. The main stream redefines the CIL
problem into a Concatenated Recursive Least Squares (C-RLS) task, allowing an
equivalence between the CIL and its joint-learning counterpart. The
compensation stream is governed by a Dual-Activation Compensation (DAC) module.
This module re-activates the embedding with a different activation function
from the main stream one, and seeks fitting compensation by projecting the
embedding to the null space of the main stream's linear mapping. Empirical
results demonstrate that the DS-AL, despite being an exemplar-free technique,
delivers performance comparable with or better than that of replay-based
methods across various datasets, including CIFAR-100, ImageNet-100 and
ImageNet-Full. Additionally, the C-RLS' equivalent property allows the DS-AL to
execute CIL in a phase-invariant manner. This is evidenced by a
never-before-seen 500-phase CIL ImageNet task, which performs on a level
identical to a 5-phase one. Our codes are available at
https://github.com/ZHUANGHP/Analytic-continual-learning.
\\ ( https://arxiv.org/abs/2403.17503 ,  912kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17507
Date: Tue, 26 Mar 2024 09:09:40 GMT   (242kb,D)

Title: EL-MLFFs: Ensemble Learning of Machine Leaning Force Fields
Authors: Bangchen Yin, Yue Yin, Yuda W. Tang and Hai Xiao
Categories: cs.LG physics.chem-ph
Comments: 12 pages, 3 figures
\\
  Machine learning force fields (MLFFs) have emerged as a promising approach to
bridge the accuracy of quantum mechanical methods and the efficiency of
classical force fields. However, the abundance of MLFF models and the challenge
of accurately predicting atomic forces pose significant obstacles in their
practical application. In this paper, we propose a novel ensemble learning
framework, EL-MLFFs, which leverages the stacking method to integrate
predictions from diverse MLFFs and enhance force prediction accuracy. By
constructing a graph representation of molecular structures and employing a
graph neural network (GNN) as the meta-model, EL-MLFFs effectively captures
atomic interactions and refines force predictions. We evaluate our approach on
two distinct datasets: methane molecules and methanol adsorbed on a Cu(100)
surface. The results demonstrate that EL-MLFFs significantly improves force
prediction accuracy compared to individual MLFFs, with the ensemble of all
eight models yielding the best performance. Moreover, our ablation study
highlights the crucial roles of the residual network and graph attention layers
in the model's architecture. The EL-MLFFs framework offers a promising solution
to the challenges of model selection and force prediction accuracy in MLFFs,
paving the way for more reliable and efficient molecular simulations.
\\ ( https://arxiv.org/abs/2403.17507 ,  242kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17520
Date: Tue, 26 Mar 2024 09:22:37 GMT   (5848kb,D)

Title: Boosting Adversarial Training via Fisher-Rao Norm-based Regularization
Authors: Xiangyu Yin, Wenjie Ruan
Categories: cs.LG cs.CV
Comments: This paper has been accepted to CVPR2024
\\
  Adversarial training is extensively utilized to improve the adversarial
robustness of deep neural networks. Yet, mitigating the degradation of standard
generalization performance in adversarial-trained models remains an open
problem. This paper attempts to resolve this issue through the lens of model
complexity. First, We leverage the Fisher-Rao norm, a geometrically invariant
metric for model complexity, to establish the non-trivial bounds of the
Cross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer
Perceptron. Then we generalize a complexity-related variable, which is
sensitive to the changes in model width and the trade-off factors in
adversarial training. Moreover, intensive empirical evidence validates that
this variable highly correlates with the generalization gap of Cross-Entropy
loss between adversarial-trained and standard-trained models, especially during
the initial and final phases of the training process. Building upon this
observation, we propose a novel regularization framework, called Logit-Oriented
Adversarial Training (LOAT), which can mitigate the trade-off between
robustness and accuracy while imposing only a negligible increase in
computational overhead. Our extensive experiments demonstrate that the proposed
regularization strategy can boost the performance of the prevalent adversarial
training algorithms, including PGD-AT, TRADES, TRADES (LSE), MART, and DM-AT,
across various network architectures. Our code will be available at
https://github.com/TrustAI/LOAT.
\\ ( https://arxiv.org/abs/2403.17520 ,  5848kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17533
Date: Tue, 26 Mar 2024 09:39:21 GMT   (6862kb,D)

Title: BVR Gym: A Reinforcement Learning Environment for Beyond-Visual-Range
  Air Combat
Authors: Edvards Scukins, Markus Klein, Lars Kroon, and Petter \"Ogren
Categories: cs.LG
Comments: 8 pages, 8 figures
\\
  Creating new air combat tactics and discovering novel maneuvers can require
numerous hours of expert pilots' time. Additionally, for each different combat
scenario, the same strategies may not work since small changes in equipment
performance may drastically change the air combat outcome. For this reason, we
created a reinforcement learning environment to help investigate potential air
combat tactics in the field of beyond-visual-range (BVR) air combat: the BVR
Gym. This type of air combat is important since long-range missiles are often
the first weapon to be used in aerial combat. Some existing environments
provide high-fidelity simulations but are either not open source or are not
adapted to the BVR air combat domain. Other environments are open source but
use less accurate simulation models. Our work provides a high-fidelity
environment based on the open-source flight dynamics simulator JSBSim and is
adapted to the BVR air combat domain. This article describes the building
blocks of the environment and some use cases.
\\ ( https://arxiv.org/abs/2403.17533 ,  6862kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17542
Date: Tue, 26 Mar 2024 09:44:57 GMT   (556kb,D)

Title: VDSC: Enhancing Exploration Timing with Value Discrepancy and State
  Counts
Authors: Marius Captari, Remo Sasso, Matthia Sabatelli
Categories: cs.LG cs.AI
\\
  Despite the considerable attention given to the questions of \textit{how
much} and \textit{how to} explore in deep reinforcement learning, the
investigation into \textit{when} to explore remains relatively less researched.
While more sophisticated exploration strategies can excel in specific, often
sparse reward environments, existing simpler approaches, such as
$\epsilon$-greedy, persist in outperforming them across a broader spectrum of
domains. The appeal of these simpler strategies lies in their ease of
implementation and generality across a wide range of domains. The downside is
that these methods are essentially a blind switching mechanism, which
completely disregards the agent's internal state. In this paper, we propose to
leverage the agent's internal state to decide \textit{when} to explore,
addressing the shortcomings of blind switching mechanisms. We present Value
Discrepancy and State Counts through homeostasis (VDSC), a novel approach for
efficient exploration timing. Experimental results on the Atari suite
demonstrate the superiority of our strategy over traditional methods such as
$\epsilon$-greedy and Boltzmann, as well as more sophisticated techniques like
Noisy Nets.
\\ ( https://arxiv.org/abs/2403.17542 ,  556kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17561
Date: Tue, 26 Mar 2024 10:10:53 GMT   (1990kb,D)

Title: A Survey on Deep Learning and State-of-the-arts Applications
Authors: Mohd Halim Mohd Noor and Ayokunle Olalekan Ige
Categories: cs.LG
\\
  Deep learning, a branch of artificial intelligence, is a computational model
that uses multiple layers of interconnected units (neurons) to learn intricate
patterns and representations directly from raw input data. Empowered by this
learning capability, it has become a powerful tool for solving complex problems
and is the core driver of many groundbreaking technologies and innovations.
Building a deep learning model is a challenging task due to the algorithm`s
complexity and the dynamic nature of real-world problems. Several studies have
reviewed deep learning concepts and applications. However, the studies mostly
focused on the types of deep learning models and convolutional neural network
architectures, offering limited coverage of the state-of-the-art of deep
learning models and their applications in solving complex problems across
different domains. Therefore, motivated by the limitations, this study aims to
comprehensively review the state-of-the-art deep learning models in computer
vision, natural language processing, time series analysis and pervasive
computing. We highlight the key features of the models and their effectiveness
in solving the problems within each domain. Furthermore, this study presents
the fundamentals of deep learning, various deep learning model types and
prominent convolutional neural network architectures. Finally, challenges and
future directions in deep learning research are discussed to offer a broader
perspective for future researchers.
\\ ( https://arxiv.org/abs/2403.17561 ,  1990kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17572
Date: Tue, 26 Mar 2024 10:25:21 GMT   (482kb,D)

Title: Enhancing Privacy in Federated Learning through Local Training
Authors: Nicola Bastianello, Changxin Liu, Karl H. Johansson
Categories: cs.LG math.OC
\\
  In this paper we propose the federated private local training algorithm
(Fed-PLT) for federated learning, to overcome the challenges of (i) expensive
communications and (ii) privacy preservation. We address (i) by allowing for
both partial participation and local training, which significantly reduce the
number of communication rounds between the central coordinator and computing
agents. The algorithm matches the state of the art in the sense that the use of
local training demonstrably does not impact accuracy. Additionally, agents have
the flexibility to choose from various local training solvers, such as
(stochastic) gradient descent and accelerated gradient descent. Further, we
investigate how employing local training can enhance privacy, addressing point
(ii). In particular, we derive differential privacy bounds and highlight their
dependence on the number of local training epochs. We assess the effectiveness
of the proposed algorithm by comparing it to alternative techniques,
considering both theoretical analysis and numerical results from a
classification task.
\\ ( https://arxiv.org/abs/2403.17572 ,  482kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17588
Date: Tue, 26 Mar 2024 10:54:07 GMT   (473kb)

Title: Forest-ORE: Mining Optimal Rule Ensemble to interpret Random Forest
  models
Authors: Haddouchi Maissae and Berrado Abdelaziz
Categories: cs.LG
Comments: 48 pages, 11 figures
\\
  Random Forest (RF) is well-known as an efficient ensemble learning method in
terms of predictive performance. It is also considered a Black Box because of
its hundreds of deep decision trees. This lack of interpretability can be a
real drawback for acceptance of RF models in several real-world applications,
especially those affecting one's lives, such as in healthcare, security, and
law. In this work, we present Forest-ORE, a method that makes RF interpretable
via an optimized rule ensemble (ORE) for local and global interpretation.
Unlike other rule-based approaches aiming at interpreting the RF model, this
method simultaneously considers several parameters that influence the choice of
an interpretable rule ensemble. Existing methods often prioritize predictive
performance over interpretability coverage and do not provide information about
existing overlaps or interactions between rules. Forest-ORE uses a
mixed-integer optimization program to build an ORE that considers the trade-off
between predictive performance, interpretability coverage, and model size (size
of the rule ensemble, rule lengths, and rule overlaps). In addition to
providing an ORE competitive in predictive performance with RF, this method
enriches the ORE through other rules that afford complementary information. It
also enables monitoring of the rule selection process and delivers various
metrics that can be used to generate a graphical representation of the final
model. This framework is illustrated through an example, and its robustness is
assessed through 36 benchmark datasets. A comparative analysis of well-known
methods shows that Forest-ORE provides an excellent trade-off between
predictive performance, interpretability coverage, and model size.
\\ ( https://arxiv.org/abs/2403.17588 ,  473kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17592
Date: Tue, 26 Mar 2024 11:01:53 GMT   (1090kb,D)

Title: On the Benefits of Over-parameterization for Out-of-Distribution
  Generalization
Authors: Yifan Hao, Yong Lin, Difan Zou, Tong Zhang
Categories: cs.LG stat.ML
\\
  In recent years, machine learning models have achieved success based on the
independently and identically distributed assumption. However, this assumption
can be easily violated in real-world applications, leading to the
Out-of-Distribution (OOD) problem. Understanding how modern over-parameterized
DNNs behave under non-trivial natural distributional shifts is essential, as
current theoretical understanding is insufficient. Existing theoretical works
often provide meaningless results for over-parameterized models in OOD
scenarios or even contradict empirical findings. To this end, we are
investigating the performance of the over-parameterized model in terms of OOD
generalization under the general benign overfitting conditions. Our analysis
focuses on a random feature model and examines non-trivial natural
distributional shifts, where the benign overfitting estimators demonstrate a
constant excess OOD loss, despite achieving zero excess in-distribution (ID)
loss. We demonstrate that in this scenario, further increasing the model's
parameterization can significantly reduce the OOD loss. Intuitively, the
variance term of ID loss remains low due to orthogonality of long-tail
features, meaning overfitting noise during training generally doesn't raise
testing loss. However, in OOD cases, distributional shift increases the
variance term. Thankfully, the inherent shift is unrelated to individual x,
maintaining the orthogonality of long-tail features. Expanding the hidden
dimension can additionally improve this orthogonality by mapping the features
into higher-dimensional spaces, thereby reducing the variance term. We further
show that model ensembles also improve OOD loss, akin to increasing model
capacity. These insights explain the empirical phenomenon of enhanced OOD
generalization through model ensembles, supported by consistent simulations
with theoretical results.
\\ ( https://arxiv.org/abs/2403.17592 ,  1090kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17637
Date: Tue, 26 Mar 2024 12:12:44 GMT   (2847kb,D)

Title: PeersimGym: An Environment for Solving the Task Offloading Problem with
  Reinforcement Learning
Authors: Frederico Metelo,Stevo Rackovi\'c,Pedro \'Akos,Cl\'audia Soares
Categories: cs.LG cs.AI
\\
  Task offloading, crucial for balancing computational loads across devices in
networks such as the Internet of Things, poses significant optimization
challenges, including minimizing latency and energy usage under strict
communication and storage constraints. While traditional optimization falls
short in scalability; and heuristic approaches lack in achieving optimal
outcomes, Reinforcement Learning (RL) offers a promising avenue by enabling the
learning of optimal offloading strategies through iterative interactions.
However, the efficacy of RL hinges on access to rich datasets and
custom-tailored, realistic training environments. To address this, we introduce
PeersimGym, an open-source, customizable simulation environment tailored for
developing and optimizing task offloading strategies within computational
networks. PeersimGym supports a wide range of network topologies and
computational constraints and integrates a \textit{PettingZoo}-based interface
for RL agent deployment in both solo and multi-agent setups. Furthermore, we
demonstrate the utility of the environment through experiments with Deep
Reinforcement Learning agents, showcasing the potential of RL-based approaches
to significantly enhance offloading strategies in distributed computing
settings. PeersimGym thus bridges the gap between theoretical RL models and
their practical applications, paving the way for advancements in efficient task
offloading methodologies.
\\ ( https://arxiv.org/abs/2403.17637 ,  2847kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17646
Date: Tue, 26 Mar 2024 12:28:04 GMT   (775kb,D)

Title: Uncertainty-aware Distributional Offline Reinforcement Learning
Authors: Xiaocong Chen and Siyu Wang and Tong Yu and Lina Yao
Categories: cs.LG
\\
  Offline reinforcement learning (RL) presents distinct challenges as it relies
solely on observational data. A central concern in this context is ensuring the
safety of the learned policy by quantifying uncertainties associated with
various actions and environmental stochasticity. Traditional approaches
primarily emphasize mitigating epistemic uncertainty by learning risk-averse
policies, often overlooking environmental stochasticity. In this study, we
propose an uncertainty-aware distributional offline RL method to simultaneously
address both epistemic uncertainty and environmental stochasticity. We propose
a model-free offline RL algorithm capable of learning risk-averse policies and
characterizing the entire distribution of discounted cumulative rewards, as
opposed to merely maximizing the expected value of accumulated discounted
returns. Our method is rigorously evaluated through comprehensive experiments
in both risk-sensitive and risk-neutral benchmarks, demonstrating its superior
performance.
\\ ( https://arxiv.org/abs/2403.17646 ,  775kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17660
Date: Tue, 26 Mar 2024 12:47:04 GMT   (124kb,D)

Title: CANOS: A Fast and Scalable Neural AC-OPF Solver Robust To N-1
  Perturbations
Authors: Luis Piloto, Sofia Liguori, Sephora Madjiheurem, Miha Zgubic, Sean
  Lovett, Hamish Tomlinson, Sophie Elster, Chris Apps and Sims Witherspoon
Categories: cs.LG
\\
  Optimal Power Flow (OPF) refers to a wide range of related optimization
problems with the goal of operating power systems efficiently and securely. In
the simplest setting, OPF determines how much power to generate in order to
minimize costs while meeting demand for power and satisfying physical and
operational constraints. In even the simplest case, power grid operators use
approximations of the AC-OPF problem because solving the exact problem is
prohibitively slow with state-of-the-art solvers. These approximations
sacrifice accuracy and operational feasibility in favor of speed. This
trade-off leads to costly "uplift payments" and increased carbon emissions,
especially for large power grids. In the present work, we train a deep learning
system (CANOS) to predict near-optimal solutions (within 1% of the true AC-OPF
cost) without compromising speed (running in as little as 33--65 ms).
Importantly, CANOS scales to realistic grid sizes with promising empirical
results on grids containing as many as 10,000 buses. Finally, because CANOS is
a Graph Neural Network, it is robust to changes in topology. We show that CANOS
is accurate across N-1 topological perturbations of a base grid typically used
in security-constrained analysis. This paves the way for more efficient
optimization of more complex OPF problems which alter grid connectivity such as
unit commitment, topology optimization and security-constrained OPF.
\\ ( https://arxiv.org/abs/2403.17660 ,  124kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17673
Date: Tue, 26 Mar 2024 13:02:43 GMT   (684kb,D)

Title: How Private is DP-SGD?
Authors: Lynn Chua, Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi,
  Amer Sinha, Chiyuan Zhang
Categories: cs.LG cs.CR cs.DS
\\
  We demonstrate a substantial gap between the privacy guarantees of the
Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch
sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of
Differentially Private Stochastic Gradient Descent (DP-SGD) follows by
interpreting it as a post-processing of ABLQ. While shuffling based DP-SGD is
more commonly used in practical implementations, it is neither analytically nor
numerically amenable to easy privacy analysis. On the other hand, Poisson
subsampling based DP-SGD is challenging to scalably implement, but has a
well-understood privacy analysis, with multiple open-source numerically tight
privacy accountants available. This has led to a common practice of using
shuffling based DP-SGD in practice, but using the privacy analysis for the
corresponding Poisson subsampling version. Our result shows that there can be a
substantial gap between the privacy analysis when using the two types of batch
sampling, and thus advises caution in reporting privacy parameters for DP-SGD.
\\ ( https://arxiv.org/abs/2403.17673 ,  684kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17698
Date: Tue, 26 Mar 2024 13:38:06 GMT   (2159kb,D)

Title: MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding
  Length Extrapolation
Authors: Weiguo Gao
Categories: cs.LG cs.AI
\\
  When the predicted sequence length exceeds the length seen during training,
the transformer's inference accuracy diminishes. Existing relative position
encoding methods, such as those based on the ALiBi technique, address the
length extrapolation challenge exclusively through the implementation of a
single kernel function, which introduces a constant bias to every post-softmax
attention scores according to their distance. These approaches do not
investigate or employ multiple kernel functions to address the extrapolation
challenge. Drawing on the ALiBi approach, this study proposes a novel relative
positional encoding method, called MEP, which employs a weighted average to
combine distinct kernel functions(such as the exponential kernel and the
Gaussian kernel) to generate a bias that is applied to post-softmax attention
scores. Initially, the framework utilizes various kernel functions to construct
multiple kernel functions. Each kernel function adheres to a consistent mean
weight coefficient, harnessing the synergistic advantages of different kernels
to formulate an innovative bias function. Subsequently, specific slopes are
tailored for each kernel function, applying penalties at varying rates, to
enhance the model's extrapolation capabilities. Finally, this bias is
seamlessly incorporated as a penalty to the post-softmax scores. We present two
distinct versions of our method: a parameter-free variant that requires no new
learnable parameters, which enhances length extrapolation capabilities without
compromising training efficiency, and a parameterized variant capable of
integrating state-of-the-art techniques. Empirical evaluations across diverse
datasets have demonstrated that both variants of our method achieve
state-of-the-art performance, outperforming traditional parameter-free and
parameterized approaches.
\\ ( https://arxiv.org/abs/2403.17698 ,  2159kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17728
Date: Tue, 26 Mar 2024 14:17:01 GMT   (3527kb,D)

Title: Masked Autoencoders are PDE Learners
Authors: Anthony Zhou and Amir Barati Farimani
Categories: cs.LG
Comments: 10 pages, 3 figures
\\
  Neural solvers for partial differential equations (PDEs) have great
potential, yet their practicality is currently limited by their
generalizability. PDEs evolve over broad scales and exhibit diverse behaviors;
predicting these phenomena will require learning representations across a wide
variety of inputs, which may encompass different coefficients, geometries, or
equations. As a step towards generalizable PDE modeling, we adapt masked
pretraining for PDEs. Through self-supervised learning across PDEs, masked
autoencoders can learn useful latent representations for downstream tasks. In
particular, masked pretraining can improve coefficient regression and
timestepping performance of neural solvers on unseen equations. We hope that
masked pretraining can emerge as a unifying method across large, unlabeled, and
heterogeneous datasets to learn latent physics at scale.
\\ ( https://arxiv.org/abs/2403.17728 ,  3527kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17745
Date: Tue, 26 Mar 2024 14:36:22 GMT   (437kb,D)

Title: Leave No Patient Behind: Enhancing Medication Recommendation for Rare
  Disease Patients
Authors: Zihao Zhao, Yi Jing, Fuli Feng, Jiancan Wu, Chongming Gao and Xiangnan
  He
Categories: cs.LG
\\
  Medication recommendation systems have gained significant attention in
healthcare as a means of providing tailored and effective drug combinations
based on patients' clinical information. However, existing approaches often
suffer from fairness issues, as recommendations tend to be more accurate for
patients with common diseases compared to those with rare conditions. In this
paper, we propose a novel model called Robust and Accurate REcommendations for
Medication (RAREMed), which leverages the pretrain-finetune learning paradigm
to enhance accuracy for rare diseases. RAREMed employs a transformer encoder
with a unified input sequence approach to capture complex relationships among
disease and procedure codes. Additionally, it introduces two self-supervised
pre-training tasks, namely Sequence Matching Prediction (SMP) and Self
Reconstruction (SR), to learn specialized medication needs and interrelations
among clinical codes. Experimental results on two real-world datasets
demonstrate that RAREMed provides accurate drug sets for both rare and common
disease patients, thereby mitigating unfairness in medication recommendation
systems.
\\ ( https://arxiv.org/abs/2403.17745 ,  437kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17753
Date: Tue, 26 Mar 2024 14:43:57 GMT   (31494kb,D)

Title: CCDSReFormer: Traffic Flow Prediction with a Criss-Crossed Dual-Stream
  Enhanced Rectified Transformer Model
Authors: Zhiqi Shao, Michael G.H. Bell, Ze Wang, D. Glenn Geers, Xusheng Yao,
  and Junbin Gao
Categories: cs.LG
Comments: 18 pages
ACM-class: I.2.0
\\
  Accurate, and effective traffic forecasting is vital for smart traffic
systems, crucial in urban traffic planning and management. Current
Spatio-Temporal Transformer models, despite their prediction capabilities,
struggle with balancing computational efficiency and accuracy, favoring global
over local information, and handling spatial and temporal data separately,
limiting insight into complex interactions. We introduce the Criss-Crossed
Dual-Stream Enhanced Rectified Transformer model (CCDSReFormer), which includes
three innovative modules: Enhanced Rectified Spatial Self-attention (ReSSA),
Enhanced Rectified Delay Aware Self-attention (ReDASA), and Enhanced Rectified
Temporal Self-attention (ReTSA). These modules aim to lower computational needs
via sparse attention, focus on local information for better traffic dynamics
understanding, and merge spatial and temporal insights through a unique
learning method. Extensive tests on six real-world datasets highlight
CCDSReFormer's superior performance. An ablation study also confirms the
significant impact of each component on the model's predictive accuracy,
showcasing our model's ability to forecast traffic flow effectively.
\\ ( https://arxiv.org/abs/2403.17753 ,  31494kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17775
Date: Tue, 26 Mar 2024 15:07:58 GMT   (476kb)

Title: Secure Aggregation is Not Private Against Membership Inference Attacks
Authors: Khac-Hoang Ngo, Johan \"Ostman, Giuseppe Durisi, and Alexandre Graell
  i Amat
Categories: cs.LG cs.CR
\\
  Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in
federated learning, affording the server access only to the aggregate of model
updates while safeguarding the confidentiality of individual updates. Despite
widespread claims regarding SecAgg's privacy-preserving capabilities, a formal
analysis of its privacy is lacking, making such presumptions unjustified. In
this paper, we delve into the privacy implications of SecAgg by treating it as
a local differential privacy (LDP) mechanism for each local update. We design a
simple attack wherein an adversarial server seeks to discern which update
vector a client submitted, out of two possible ones, in a single training round
of federated learning under SecAgg. By conducting privacy auditing, we assess
the success probability of this attack and quantify the LDP guarantees provided
by SecAgg. Our numerical results unveil that, contrary to prevailing claims,
SecAgg offers weak privacy against membership inference attacks even in a
single training round. Indeed, it is difficult to hide a local update by adding
other independent local updates when the updates are of high dimension. Our
findings underscore the imperative for additional privacy-enhancing mechanisms,
such as noise injection, in federated learning.
\\ ( https://arxiv.org/abs/2403.17775 ,  476kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17806
Date: Tue, 26 Mar 2024 15:44:58 GMT   (6403kb,D)

Title: Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding
  Model Mechanisms
Authors: Michael Hanna, Sandro Pezzelle, Yonatan Belinkov
Categories: cs.LG cs.CL
\\
  Many recent language model (LM) interpretability studies have adopted the
circuits framework, which aims to find the minimal computational subgraph, or
circuit, that explains LM behavior on a given task. Most studies determine
which edges belong in a LM's circuit by performing causal interventions on each
edge independently, but this scales poorly with model size. Edge attribution
patching (EAP), gradient-based approximation to interventions, has emerged as a
scalable but imperfect solution to this problem. In this paper, we introduce a
new method - EAP with integrated gradients (EAP-IG) - that aims to better
maintain a core property of circuits: faithfulness. A circuit is faithful if
all model edges outside the circuit can be ablated without changing the model's
performance on the task; faithfulness is what justifies studying circuits,
rather than the full model. Our experiments demonstrate that circuits found
using EAP are less faithful than those found using EAP-IG, even though both
have high node overlap with circuits found previously using causal
interventions. We conclude more generally that when using circuits to compare
the mechanisms models use to solve tasks, faithfulness, not overlap, is what
should be measured.
\\ ( https://arxiv.org/abs/2403.17806 ,  6403kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17811
Date: Tue, 26 Mar 2024 15:50:37 GMT   (106kb,D)

Title: Are Compressed Language Models Less Subgroup Robust?
Authors: Leonidas Gee, Andrea Zugarini, Novi Quadrianto
Categories: cs.LG cs.CL
Comments: The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)
Journal-ref: Proceedings of the 2023 Conference on Empirical Methods in Natural
  Language Processing: Main Track
DOI: 10.18653/v1/2023.emnlp-main.983
\\
  To reduce the inference cost of large language models, model compression is
increasingly used to create smaller scalable models. However, little is known
about their robustness to minority subgroups defined by the labels and
attributes of a dataset. In this paper, we investigate the effects of 18
different compression methods and settings on the subgroup robustness of BERT
language models. We show that worst-group performance does not depend on model
size alone, but also on the compression method used. Additionally, we find that
model compression does not always worsen the performance on minority subgroups.
Altogether, our analysis serves to further research into the subgroup
robustness of model compression.
\\ ( https://arxiv.org/abs/2403.17811 ,  106kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17831
Date: Tue, 26 Mar 2024 16:13:55 GMT   (177kb,D)

Title: Learning the Optimal Power Flow: Environment Design Matters
Authors: Thomas Wolgast and Astrid Nie{\ss}e
Categories: cs.LG cs.SY eess.SY
\\
  To solve the optimal power flow (OPF) problem, reinforcement learning (RL)
emerges as a promising new approach. However, the RL-OPF literature is strongly
divided regarding the exact formulation of the OPF problem as an RL
environment. In this work, we collect and implement diverse environment design
decisions from the literature regarding training data, observation space,
episode definition, and reward function choice. In an experimental analysis, we
show the significant impact of these environment design options on RL-OPF
training performance. Further, we derive some first recommendations regarding
the choice of these design decisions. The created environment framework is
fully open-source and can serve as a benchmark for future research in the
RL-OPF field.
\\ ( https://arxiv.org/abs/2403.17831 ,  177kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17833
Date: Tue, 26 Mar 2024 16:14:43 GMT   (10364kb,D)

Title: GPFL: A Gradient Projection-Based Client Selection Framework for
  Efficient Federated Learning
Authors: Shijie Na, Yuzhi Liang and Siu-Ming Yiu
Categories: cs.LG cs.DC
Comments: 8 pages, 5 figures
\\
  Federated learning client selection is crucial for determining participant
clients while balancing model accuracy and communication efficiency. Existing
methods have limitations in handling data heterogeneity, computational burdens,
and independent client treatment. To address these challenges, we propose GPFL,
which measures client value by comparing local and global descent directions.
We also employ an Exploit-Explore mechanism to enhance performance.
Experimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL
outperforms baselines in Non-IID scenarios, achieving over 9\% improvement in
FEMINST test accuracy. Moreover, GPFL exhibits shorter computation times
through pre-selection and parameter reuse in federated learning.
\\ ( https://arxiv.org/abs/2403.17833 ,  10364kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17844
Date: Tue, 26 Mar 2024 16:33:12 GMT   (1124kb,D)

Title: Mechanistic Design and Scaling of Hybrid Architectures
Authors: Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Bj\"orn
  Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon,
  Christopher R\'e, Ce Zhang, Stefano Massaroli
Categories: cs.LG
\\
  The development of deep learning architectures is a resource-demanding
process, due to a vast design space, long prototyping times, and high compute
costs associated with at-scale model training and evaluation. We set out to
simplify this process by grounding it in an end-to-end mechanistic architecture
design (MAD) pipeline, encompassing small-scale capability unit tests
predictive of scaling laws. Through a suite of synthetic token manipulation
tasks such as compression and recall, designed to probe capabilities, we
identify and test new hybrid architectures constructed from a variety of
computational primitives. We experimentally validate the resulting
architectures via an extensive compute-optimal and a new state-optimal scaling
law analysis, training over 500 language models between 70M to 7B parameters.
Surprisingly, we find MAD synthetics to correlate with compute-optimal
perplexity, enabling accurate evaluation of new architectures via isolated
proxy tasks. The new architectures found via MAD, based on simple ideas such as
hybridization and sparsity, outperform state-of-the-art Transformer,
convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in
scaling, both at compute-optimal budgets and in overtrained regimes. Overall,
these results provide evidence that performance on curated synthetic tasks can
be predictive of scaling laws, and that an optimal architecture should leverage
specialized layers via a hybrid topology.
\\ ( https://arxiv.org/abs/2403.17844 ,  1124kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17845
Date: Tue, 26 Mar 2024 16:34:05 GMT   (18539kb,D)

Title: TractOracle: towards an anatomically-informed reward function for
  RL-based tractography
Authors: Antoine Th\'eberge and Maxime Descoteaux and Pierre-Marc Jodoin
Categories: cs.LG
\\
  Reinforcement learning (RL)-based tractography is a competitive alternative
to machine learning and classical tractography algorithms due to its high
anatomical accuracy obtained without the need for any annotated data. However,
the reward functions so far used to train RL agents do not encapsulate
anatomical knowledge which causes agents to generate spurious false positives
tracts. In this paper, we propose a new RL tractography system, TractOracle,
which relies on a reward network trained for streamline classification. This
network is used both as a reward function during training as well as a mean for
stopping the tracking process early and thus reduce the number of false
positive streamlines. This makes our system a unique method that evaluates and
reconstructs WM streamlines at the same time. We report an improvement of true
positive ratios by almost 20\% and a reduction of 3x of false positive ratios
on one dataset and an increase between 2x and 7x in the number true positive
streamlines on another dataset.
\\ ( https://arxiv.org/abs/2403.17845 ,  18539kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17847
Date: Tue, 26 Mar 2024 16:36:50 GMT   (5198kb,D)

Title: Climate Downscaling: A Deep-Learning Based Super-resolution Model of
  Precipitation Data with Attention Block and Skip Connections
Authors: Chia-Hao Chiang, Zheng-Han Huang, Liwen Liu, Hsin-Chien Liang, Yi-Chi
  Wang, Wan-Ling Tseng, Chao Wang, Che-Ta Chen, Ko-Chih Wang
Categories: cs.LG cs.AI
\\
  Human activities accelerate consumption of fossil fuels and produce
greenhouse gases, resulting in urgent issues today: global warming and the
climate change. These indirectly cause severe natural disasters, plenty of
lives suffering and huge losses of agricultural properties. To mitigate impacts
on our lands, scientists are developing renewable, reusable, and clean energies
and climatologists are trying to predict the extremes. Meanwhile, governments
are publicizing resource-saving policies for a more eco-friendly society and
arousing environment awareness. One of the most influencing factors is the
precipitation, bringing condensed water vapor onto lands. Water resources are
the most significant but basic needs in society, not only supporting our
livings, but also economics. In Taiwan, although the average annual
precipitation is up to 2,500 millimeter (mm), the water allocation for each
person is lower than the global average due to drastically geographical
elevation changes and uneven distribution through the year. Thus, it is crucial
to track and predict the rainfall to make the most use of it and to prevent the
floods. However, climate models have limited resolution and require intensive
computational power for local-scale use. Therefore, we proposed a deep
convolutional neural network with skip connections, attention blocks, and
auxiliary data concatenation, in order to downscale the low-resolution
precipitation data into high-resolution one. Eventually, we compare with other
climate downscaling methods and show better performance in metrics of Mean
Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson Correlation,
structural similarity index (SSIM), and forecast indicators.
\\ ( https://arxiv.org/abs/2403.17847 ,  5198kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17852
Date: Tue, 26 Mar 2024 16:40:08 GMT   (3851kb,D)

Title: Counterfactual Fairness through Transforming Data Orthogonal to Bias
Authors: Shuyi Chen, Shixiang Zhu
Categories: cs.LG stat.ML
\\
  Machine learning models have shown exceptional prowess in solving complex
issues across various domains. Nonetheless, these models can sometimes exhibit
biased decision-making, leading to disparities in treatment across different
groups. Despite the extensive research on fairness, the nuanced effects of
multivariate and continuous sensitive variables on decision-making outcomes
remain insufficiently studied. We introduce a novel data pre-processing
algorithm, Orthogonal to Bias (OB), designed to remove the influence of a group
of continuous sensitive variables, thereby facilitating counterfactual fairness
in machine learning applications. Our approach is grounded in the assumption of
a jointly normal distribution within a structural causal model (SCM), proving
that counterfactual fairness can be achieved by ensuring the data is
uncorrelated with sensitive variables. The OB algorithm is model-agnostic,
catering to a wide array of machine learning models and tasks, and includes a
sparse variant to enhance numerical stability through regularization. Through
empirical evaluation on simulated and real-world datasets - including the adult
income and the COMPAS recidivism datasets - our methodology demonstrates its
capacity to enable fairer outcomes without compromising accuracy.
\\ ( https://arxiv.org/abs/2403.17852 ,  3851kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17878
Date: Tue, 26 Mar 2024 17:10:15 GMT   (929kb,D)

Title: Empowering Data Mesh with Federated Learning
Authors: Haoyuan Li and Salman Toor
Categories: cs.LG cs.DC
Comments: In Proceedings of ACM Knowledge Discovery and Data Mining, Barcelona,
  Spain, 25th - 29th August, 2024 (Conference acronym KDD), 9 pages
\\
  The evolution of data architecture has seen the rise of data lakes, aiming to
solve the bottlenecks of data management and promote intelligent
decision-making. However, this centralized architecture is limited by the
proliferation of data sources and the growing demand for timely analysis and
processing. A new data paradigm, Data Mesh, is proposed to overcome these
challenges. Data Mesh treats domains as a first-class concern by distributing
the data ownership from the central team to each data domain, while keeping the
federated governance to monitor domains and their data products. Many
multi-million dollar organizations like Paypal, Netflix, and Zalando have
already transformed their data analysis pipelines based on this new
architecture. In this decentralized architecture where data is locally
preserved by each domain team, traditional centralized machine learning is
incapable of conducting effective analysis across multiple domains, especially
for security-sensitive organizations. To this end, we introduce a pioneering
approach that incorporates Federated Learning into Data Mesh. To the best of
our knowledge, this is the first open-source applied work that represents a
critical advancement toward the integration of federated learning methods into
the Data Mesh paradigm, underscoring the promising prospects for
privacy-preserving and decentralized data analysis strategies within Data Mesh
architecture.
\\ ( https://arxiv.org/abs/2403.17878 ,  929kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17886
Date: Tue, 26 Mar 2024 17:19:23 GMT   (4072kb,D)

Title: Compressed Multi-task embeddings for Data-Efficient Downstream training
  and inference in Earth Observation
Authors: Carlos Gomes and Thomas Brunschwiler
Categories: cs.LG
Comments: Published at IGARSS 2024
\\
  As repositories of large scale data in earth observation (EO) have grown, so
have transfer and storage costs for model training and inference, expending
significant resources. We introduce Neural Embedding Compression (NEC), based
on the transfer of compressed embeddings to data consumers instead of raw data.
We adapt foundation models (FM) through learned neural compression to generate
multi-task embeddings while navigating the tradeoff between compression rate
and embedding utility. We update only a small fraction of the FM parameters
(10%) for a short training period (1% of the iterations of pre-training). We
evaluate NEC on two EO tasks: scene classification and semantic segmentation.
Compared with applying traditional compression to the raw data, NEC achieves
similar accuracy with a 75% to 90% reduction in data. Even at 99.7%
compression, performance drops by only 5% on the scene classification task.
Overall, NEC is a data-efficient yet performant approach for multi-task EO
modelling.
\\ ( https://arxiv.org/abs/2403.17886 ,  4072kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17891
Date: Tue, 26 Mar 2024 17:22:29 GMT   (868kb,D)

Title: Image-based Novel Fault Detection with Deep Learning Classifiers using
  Hierarchical Labels
Authors: Nurettin Sergin, Jiayu Huang, Tzyy-Shuh Chang, Hao Yan
Categories: cs.LG cs.AI
Comments: Accepted in IISE Transaction
\\
  One important characteristic of modern fault classification systems is the
ability to flag the system when faced with previously unseen fault types. This
work considers the unknown fault detection capabilities of deep neural
network-based fault classifiers. Specifically, we propose a methodology on how,
when available, labels regarding the fault taxonomy can be used to increase
unknown fault detection performance without sacrificing model performance. To
achieve this, we propose to utilize soft label techniques to improve the
state-of-the-art deep novel fault detection techniques during the training
process and novel hierarchically consistent detection statistics for online
novel fault detection. Finally, we demonstrated increased detection performance
on novel fault detection in inspection images from the hot steel rolling
process, with results well replicated across multiple scenarios and baseline
detection methods.
\\ ( https://arxiv.org/abs/2403.17891 ,  868kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17919
Date: Tue, 26 Mar 2024 17:55:02 GMT   (2185kb,D)

Title: LISA: Layerwise Importance Sampling for Memory-Efficient Large Language
  Model Fine-Tuning
Authors: Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han,
  Tong Zhang
Categories: cs.LG cs.AI cs.CL math.OC
\\
  The machine learning community has witnessed impressive advancements since
the first appearance of large language models (LLMs), yet their huge memory
consumption has become a major roadblock to large-scale training. Parameter
Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been
proposed to alleviate this problem, but their performance still fails to match
full parameter training in most large-scale fine-tuning settings. Attempting to
complement this deficiency, we investigate layerwise properties of LoRA on
fine-tuning tasks and observe an uncommon skewness of weight norms across
different layers. Utilizing this key observation, a surprisingly simple
training strategy is discovered, which outperforms both LoRA and full parameter
training in a wide range of settings with memory costs as low as LoRA. We name
it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,
which applies the idea of importance sampling to different layers in LLMs and
randomly freeze most middle layers during optimization. Experimental results
show that with similar or less GPU memory consumption, LISA surpasses LoRA or
even full parameter tuning in downstream fine-tuning tasks, where LISA
consistently outperforms LoRA by over $11\%$-$37\%$ in terms of MT-Bench
scores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or
better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating
its effectiveness across different domains.
\\ ( https://arxiv.org/abs/2403.17919 ,  2185kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17921
Date: Tue, 26 Mar 2024 17:55:58 GMT   (8609kb,D)

Title: The Need for Speed: Pruning Transformers with One Recipe
Authors: Samir Khaki and Konstantinos N. Plataniotis
Categories: cs.LG
Comments: Accepted in the International Conference on Learning Representations
  (ICLR) 2024
\\
  We introduce the $\textbf{O}$ne-shot $\textbf{P}$runing $\textbf{T}$echnique
for $\textbf{I}$nterchangeable $\textbf{N}$etworks ($\textbf{OPTIN}$) framework
as a tool to increase the efficiency of pre-trained transformer architectures
$\textit{without requiring re-training}$. Recent works have explored improving
transformer efficiency, however often incur computationally expensive
re-training procedures or depend on architecture-specific characteristics, thus
impeding practical wide-scale adoption. To address these shortcomings, the
OPTIN framework leverages intermediate feature distillation, capturing the
long-range dependencies of model parameters (coined $\textit{trajectory}$), to
produce state-of-the-art results on natural language, image classification,
transfer learning, and semantic segmentation tasks $\textit{without
re-training}$. Given a FLOP constraint, the OPTIN framework will compress the
network while maintaining competitive accuracy performance and improved
throughput. Particularly, we show a $\leq 2$% accuracy degradation from NLP
baselines and a $0.5$% improvement from state-of-the-art methods on image
classification at competitive FLOPs reductions. We further demonstrate the
generalization of tasks and architecture with comparative performance using
Mask2Former for semantic segmentation and cnn-style networks. OPTIN presents
one of the first one-shot efficient frameworks for compressing transformer
architectures that generalizes well across different class domains, in
particular: natural language and image-related tasks, without
$\textit{re-training}$.
\\ ( https://arxiv.org/abs/2403.17921 ,  8609kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2403.17012 (*cross-listing*)
Date: Sun, 11 Feb 2024 18:27:29 GMT   (147kb)

Title: Evolution and Efficiency in Neural Architecture Search: Bridging the Gap
  Between Expert Design and Automated Optimization
Authors: Fanfei Meng, Chen-Ao Wang, Alexander Brown
Categories: cs.NE cs.AI
Comments: 11 pages
\\
  The paper provides a comprehensive overview of Neural Architecture Search
(NAS), emphasizing its evolution from manual design to automated,
computationally-driven approaches. It covers the inception and growth of NAS,
highlighting its application across various domains, including medical imaging
and natural language processing. The document details the shift from
expert-driven design to algorithm-driven processes, exploring initial
methodologies like reinforcement learning and evolutionary algorithms. It also
discusses the challenges of computational demands and the emergence of
efficient NAS methodologies, such as Differentiable Architecture Search and
hardware-aware NAS. The paper further elaborates on NAS's application in
computer vision, NLP, and beyond, demonstrating its versatility and potential
for optimizing neural network architectures across different tasks. Future
directions and challenges, including computational efficiency and the
integration with emerging AI domains, are addressed, showcasing NAS's dynamic
nature and its continued evolution towards more sophisticated and efficient
architecture search methods.
\\ ( https://arxiv.org/abs/2403.17012 ,  147kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17064 (*cross-listing*)
Date: Mon, 25 Mar 2024 18:00:42 GMT   (35468kb,D)

Title: Continuous, Subject-Specific Attribute Control in T2I Models by
  Identifying Semantic Directions
Authors: Stefan Andreas Baumann and Felix Krause and Michael Neumayr and Nick
  Stracke and Vincent Tao Hu and Bj\"orn Ommer
Categories: cs.CV cs.AI cs.LG
Comments: Project page: https://compvis.github.io/attribute-control
\\
  In recent years, advances in text-to-image (T2I) diffusion models have
substantially elevated the quality of their generated images. However,
achieving fine-grained control over attributes remains a challenge due to the
limitations of natural language prompts (such as no continuous set of
intermediate descriptions existing between ``person'' and ``old person''). Even
though many methods were introduced that augment the model or generation
process to enable such control, methods that do not require a fixed reference
image are limited to either enabling global fine-grained attribute expression
control or coarse attribute expression control localized to specific subjects,
not both simultaneously. We show that there exist directions in the commonly
used token-level CLIP text embeddings that enable fine-grained subject-specific
control of high-level attributes in text-to-image models. Based on this
observation, we introduce one efficient optimization-free and one robust
optimization-based method to identify these directions for specific attributes
from contrastive text prompts. We demonstrate that these directions can be used
to augment the prompt text input with fine-grained control over attributes of
specific subjects in a compositional manner (control over multiple attributes
of a single subject) without having to adapt the diffusion model. Project page:
https://compvis.github.io/attribute-control. Code is available at
https://github.com/CompVis/attribute-control.
\\ ( https://arxiv.org/abs/2403.17064 ,  35468kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17083 (*cross-listing*)
Date: Mon, 25 Mar 2024 18:16:34 GMT   (577kb,D)

Title: A Study in Dataset Pruning for Image Super-Resolution
Authors: Brian B. Moser, Federico Raue and Andreas Dengel
Categories: eess.IV cs.AI cs.CV cs.GR cs.LG
\\
  In image Super-Resolution (SR), relying on large datasets for training is a
double-edged sword. While offering rich training material, they also demand
substantial computational and storage resources. In this work, we analyze
dataset pruning as a solution to these challenges. We introduce a novel
approach that reduces a dataset to a core-set of training samples, selected
based on their loss values as determined by a simple pre-trained SR model. By
focusing the training on just 50% of the original dataset, specifically on the
samples characterized by the highest loss values, we achieve results comparable
to or even surpassing those obtained from training on the entire dataset.
Interestingly, our analysis reveals that the top 5% of samples with the highest
loss values negatively affect the training process. Excluding these samples and
adjusting the selection to favor easier samples further enhances training
outcomes. Our work opens new perspectives to the untapped potential of dataset
pruning in image SR. It suggests that careful selection of training data based
on loss-value metrics can lead to better SR models, challenging the
conventional wisdom that more data inevitably leads to better performance.
\\ ( https://arxiv.org/abs/2403.17083 ,  577kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17089 (*cross-listing*)
Date: Mon, 25 Mar 2024 18:25:10 GMT   (2832kb,D)

Title: GOLF: Goal-Oriented Long-term liFe tasks supported by human-AI
  collaboration
Authors: Ben Wang
Categories: cs.HC cs.AI cs.IR
\\
  The advent of ChatGPT and similar large language models (LLMs) has
revolutionized the human-AI interaction and information-seeking process.
Leveraging LLMs as an alternative to search engines, users can now access
summarized information tailored to their queries, significantly reducing the
cognitive load associated with navigating vast information resources. This
shift underscores the potential of LLMs in redefining information access
paradigms. Drawing on the foundation of task-focused information retrieval and
LLMs' task planning ability, this research extends the scope of LLM
capabilities beyond routine task automation to support users in navigating
long-term and significant life tasks. It introduces the GOLF framework
(Goal-Oriented Long-term liFe tasks), which focuses on enhancing LLMs' ability
to assist in significant life decisions through goal orientation and long-term
planning. The methodology encompasses a comprehensive simulation study to test
the framework's efficacy, followed by model and human evaluations to develop a
dataset benchmark for long-term life tasks, and experiments across different
models and settings. By shifting the focus from short-term tasks to the broader
spectrum of long-term life goals, this research underscores the transformative
potential of LLMs in enhancing human decision-making processes and task
management, marking a significant step forward in the evolution of human-AI
collaboration.
\\ ( https://arxiv.org/abs/2403.17089 ,  2832kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17124 (*cross-listing*)
Date: Mon, 25 Mar 2024 19:04:59 GMT   (13131kb,D)

Title: Grounding Language Plans in Demonstrations Through Counterfactual
  Perturbations
Authors: Yanwei Wang, Tsun-Hsuan Wang, Jiayuan Mao, Michael Hagenow, Julie Shah
Categories: cs.RO cs.AI cs.CL cs.LG
\\
  Grounding the common-sense reasoning of Large Language Models in physical
domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior
works have focused on leveraging LLMs directly for planning in symbolic spaces,
this work uses LLMs to guide the search of task structures and constraints
implicit in multi-step demonstrations. Specifically, we borrow from
manipulation planning literature the concept of mode families, which group
robot configurations by specific motion constraints, to serve as an abstraction
layer between the high-level language representations of an LLM and the
low-level physical trajectories of a robot. By replaying a few human
demonstrations with synthetic perturbations, we generate coverage over the
demonstrations' state space with additional successful executions as well as
counterfactuals that fail the task. Our explanation-based learning framework
trains an end-to-end differentiable neural network to predict successful
trajectories from failures and as a by-product learns classifiers that ground
low-level states and images in mode families without dense labeling. The
learned grounding classifiers can further be used to translate language plans
into reactive policies in the physical domain in an interpretable manner. We
show our approach improves the interpretability and reactivity of imitation
learning through 2D navigation and simulated and real robot manipulation tasks.
Website: https://sites.google.com/view/grounding-plans
\\ ( https://arxiv.org/abs/2403.17124 ,  13131kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17134 (*cross-listing*)
Date: Mon, 25 Mar 2024 19:17:43 GMT   (1154kb,D)

Title: RepairAgent: An Autonomous, LLM-Based Agent for Program Repair
Authors: Islem Bouzenia, Premkumar Devanbu, Michael Pradel
Categories: cs.SE cs.AI
\\
  Automated program repair has emerged as a powerful technique to mitigate the
impact of software bugs on system reliability and user experience. This paper
introduces RepairAgent, the first work to address the program repair challenge
through an autonomous agent based on a large language model (LLM). Unlike
existing deep learning-based approaches, which prompt a model with a fixed
prompt or in a fixed feedback loop, our work treats the LLM as an agent capable
of autonomously planning and executing actions to fix bugs by invoking suitable
tools. RepairAgent freely interleaves gathering information about the bug,
gathering repair ingredients, and validating fixes, while deciding which tools
to invoke based on the gathered information and feedback from previous fix
attempts. Key contributions that enable RepairAgent include a set of tools that
are useful for program repair, a dynamically updated prompt format that allows
the LLM to interact with these tools, and a finite state machine that guides
the agent in invoking the tools. Our evaluation on the popular Defects4J
dataset demonstrates RepairAgent's effectiveness in autonomously repairing 164
bugs, including 39 bugs not fixed by prior techniques. Interacting with the LLM
imposes an average cost of 270,000 tokens per bug, which, under the current
pricing of OpenAI's GPT-3.5 model, translates to 14 cents of USD per bug. To
the best of our knowledge, this work is the first to present an autonomous,
LLM-based agent for program repair, paving the way for future agent-based
techniques in software engineering.
\\ ( https://arxiv.org/abs/2403.17134 ,  1154kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17147 (*cross-listing*)
Date: Mon, 25 Mar 2024 19:50:07 GMT   (24510kb,D)

Title: Hearing the shape of an arena with spectral swarm robotics
Authors: Leo Cazenille, Nicolas Lobato-Dauzier, Alessia Loi, Mika Ito, Olivier
  Marchal, Nathanael Aubert-Kato, Nicolas Bredeche, Anthony J. Genot
Categories: cs.RO cs.AI cs.CG
\\
  Swarm robotics promises adaptability to unknown situations and robustness
against failures. However, it still struggles with global tasks that require
understanding the broader context in which the robots operate, such as
identifying the shape of the arena in which the robots are embedded. Biological
swarms, such as shoals of fish, flocks of birds, and colonies of insects,
routinely solve global geometrical problems through the diffusion of local
cues. This paradigm can be explicitly described by mathematical models that
could be directly computed and exploited by a robotic swarm. Diffusion over a
domain is mathematically encapsulated by the Laplacian, a linear operator that
measures the local curvature of a function. Crucially the geometry of a domain
can generally be reconstructed from the eigenspectrum of its Laplacian. Here we
introduce spectral swarm robotics where robots diffuse information to their
neighbors to emulate the Laplacian operator - enabling them to "hear" the
spectrum of their arena. We reveal a universal scaling that links the optimal
number of robots (a global parameter) with their optimal radius of interaction
(a local parameter). We validate experimentally spectral swarm robotics under
challenging conditions with the one-shot classification of arena shapes using a
sparse swarm of Kilobots. Spectral methods can assist with challenging tasks
where robots need to build an emergent consensus on their environment, such as
adaptation to unknown terrains, division of labor, or quorum sensing. Spectral
methods may extend beyond robotics to analyze and coordinate swarms of agents
of various natures, such as traffic or crowds, and to better understand the
long-range dynamics of natural systems emerging from short-range interactions.
\\ ( https://arxiv.org/abs/2403.17147 ,  24510kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17164 (*cross-listing*)
Date: Mon, 25 Mar 2024 20:29:04 GMT   (10322kb,D)

Title: Multi-Objective Quality-Diversity for Crystal Structure Prediction
Authors: Hannah Janmohamed, Marta Wolinska, Shikha Surana, Thomas Pierrot, Aron
  Walsh, Antoine Cully
Categories: cs.NE cs.AI cs.LG
Comments: Accepted GECCO 2024
DOI: 10.1145/3638529.3654048
\\
  Crystal structures are indispensable across various domains, from batteries
to solar cells, and extensive research has been dedicated to predicting their
properties based on their atomic configurations. However, prevailing Crystal
Structure Prediction methods focus on identifying the most stable solutions
that lie at the global minimum of the energy function. This approach overlooks
other potentially interesting materials that lie in neighbouring local minima
and have different material properties such as conductivity or resistance to
deformation. By contrast, Quality-Diversity algorithms provide a promising
avenue for Crystal Structure Prediction as they aim to find a collection of
high-performing solutions that have diverse characteristics. However, it may
also be valuable to optimise for the stability of crystal structures alongside
other objectives such as magnetism or thermoelectric efficiency. Therefore, in
this work, we harness the power of Multi-Objective Quality-Diversity algorithms
in order to find crystal structures which have diverse features and achieve
different trade-offs of objectives. We analyse our approach on 5 crystal
systems and demonstrate that it is not only able to re-discover known real-life
structures, but also find promising new ones. Moreover, we propose a method for
illuminating the objective space to gain an understanding of what trade-offs
can be achieved.
\\ ( https://arxiv.org/abs/2403.17164 ,  10322kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17214 (*cross-listing*)
Date: Mon, 25 Mar 2024 21:41:31 GMT   (716kb,D)

Title: Exploring the Impact of the Output Format on the Evaluation of Large
  Language Models for Code Translation
Authors: Marcos Macedo, Yuan Tian, Filipe R. Cogo, Bram Adams
Categories: cs.SE cs.AI
Comments: Accepted into 2024 IEEE/ACM First International Conference on AI
  Foundation Models and Software Engineering (Forge)
DOI: 10.1145/3650105.3652301
\\
  Code translation between programming languages is a long-existing and
critical task in software engineering, facilitating the modernization of legacy
systems, ensuring cross-platform compatibility, and enhancing software
performance. With the recent advances in large language models (LLMs) and their
applications to code translation, there is an increasing need for comprehensive
evaluation of these models. In this study, we empirically analyze the generated
outputs of eleven popular instruct-tuned LLMs with parameters ranging from 1B
up to 46.7B on 3,820 translation pairs across five languages, including C, C++,
Go, Java, and Python. Our analysis found that between 26.4% and 73.7% of code
translations produced by our evaluated LLMs necessitate post-processing, as
these translations often include a mix of code, quotes, and text rather than
being purely source code. Overlooking the output format of these models can
inadvertently lead to underestimation of their actual performance. This is
particularly evident when evaluating them with execution-based metrics such as
Computational Accuracy (CA). Our results demonstrate that a strategic
combination of prompt engineering and regular expression can effectively
extract the source code from the model generation output. In particular, our
method can help eleven selected models achieve an average Code Extraction
Success Rate (CSR) of 92.73%. Our findings shed light on and motivate future
research to conduct more reliable benchmarks of LLMs for code translation.
\\ ( https://arxiv.org/abs/2403.17214 ,  716kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17217 (*cross-listing*)
Date: Mon, 25 Mar 2024 21:46:53 GMT   (29890kb,D)

Title: DiffusionAct: Controllable Diffusion Autoencoder for One-shot Face
  Reenactment
Authors: Stella Bounareli, Christos Tzelepis, Vasileios Argyriou, Ioannis
  Patras and Georgios Tzimiropoulos
Categories: cs.CV cs.AI
Comments: Project page: https://stelabou.github.io/diffusionact/
\\
  Video-driven neural face reenactment aims to synthesize realistic facial
images that successfully preserve the identity and appearance of a source face,
while transferring the target head pose and facial expressions. Existing
GAN-based methods suffer from either distortions and visual artifacts or poor
reconstruction quality, i.e., the background and several important appearance
details, such as hair style/color, glasses and accessories, are not faithfully
reconstructed. Recent advances in Diffusion Probabilistic Models (DPMs) enable
the generation of high-quality realistic images. To this end, in this paper we
present DiffusionAct, a novel method that leverages the photo-realistic image
generation of diffusion models to perform neural face reenactment.
Specifically, we propose to control the semantic space of a Diffusion
Autoencoder (DiffAE), in order to edit the facial pose of the input images,
defined as the head pose orientation and the facial expressions. Our method
allows one-shot, self, and cross-subject reenactment, without requiring
subject-specific fine-tuning. We compare against state-of-the-art GAN-,
StyleGAN2-, and diffusion-based methods, showing better or on-par reenactment
performance.
\\ ( https://arxiv.org/abs/2403.17217 ,  29890kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17219 (*cross-listing*)
Date: Mon, 25 Mar 2024 21:48:22 GMT   (3476kb,D)

Title: SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental
  Health Sensing Studies
Authors: Akshat Choube, Vedant Das Swain, Varun Mishra
Categories: cs.HC cs.AI cs.CY
\\
  Advances in mobile and wearable technologies have enabled the potential to
passively monitor a person's mental, behavioral, and affective health. These
approaches typically rely on longitudinal collection of self-reported outcomes,
e.g., depression, stress, and anxiety, to train machine learning (ML) models.
However, the need to continuously self-report adds a significant burden on the
participants, often resulting in attrition, missing labels, or insincere
responses. In this work, we introduce the Scale Scores Simulation using Mental
Models (SeSaMe) framework to alleviate participants' burden in digital mental
health studies. By leveraging pre-trained large language models (LLMs), SeSaMe
enables the simulation of participants' responses on psychological scales. In
SeSaMe, researchers can prompt LLMs with information on participants' internal
behavioral dispositions, enabling LLMs to construct mental models of
participants to simulate their responses on psychological scales. We
demonstrate an application of SeSaMe, where we use GPT-4 to simulate responses
on one scale using responses from another as behavioral information. We also
evaluate the alignment between human and SeSaMe-simulated responses to
psychological scales. Then, we present experiments to inspect the utility of
SeSaMe-simulated responses as ground truth in training ML models by replicating
established depression and anxiety screening tasks from a previous study. Our
results indicate SeSaMe to be a promising approach, but its alignment may vary
across scales and specific prediction objectives. We also observed that model
performance with simulated data was on par with using the real data for
training in most evaluation scenarios. We conclude by discussing the potential
implications of SeSaMe in addressing some challenges researchers face with
ground-truth collection in passive sensing studies.
\\ ( https://arxiv.org/abs/2403.17219 ,  3476kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17223 (*cross-listing*)
Date: Mon, 25 Mar 2024 21:53:36 GMT   (307kb)

Title: Co-Occurring of Object Detection and Identification towards unlabeled
  object discovery
Authors: Binay Kumar Singh, Niels Da Vitoria Lobo
Categories: cs.CV cs.AI cs.LG
Comments: 6 pages, 2 figures,
\\
  In this paper, we propose a novel deep learning based approach for
identifying co-occurring objects in conjunction with base objects in multilabel
object categories. Nowadays, with the advancement in computer vision based
techniques we need to know about co-occurring objects with respect to base
object for various purposes. The pipeline of the proposed work is composed of
two stages: in the first stage of the proposed model we detect all the bounding
boxes present in the image and their corresponding labels, then in the second
stage we perform co-occurrence matrix analysis. In co-occurrence matrix
analysis, we set base classes based on the maximum occurrences of the labels
and build association rules and generate frequent patterns. These frequent
patterns will show base classes and their corresponding co-occurring classes.
We performed our experiments on two publicly available datasets: Pascal VOC and
MS-COCO. The experimental results on public benchmark dataset is reported in
Sec 4. Further we extend this work by considering all frequently objects as
unlabeled and what if they are occluded as well.
\\ ( https://arxiv.org/abs/2403.17223 ,  307kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17237 (*cross-listing*)
Date: Mon, 25 Mar 2024 22:34:05 GMT   (19053kb,D)

Title: DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric
  Diffusion
Authors: Yuanze Lin, Ronald Clark, Philip Torr
Categories: cs.CV cs.AI cs.GR
Comments: Project webpage: https://yuanze-lin.me/DreamPolisher_page/
\\
  We present DreamPolisher, a novel Gaussian Splatting based method with
geometric guidance, tailored to learn cross-view consistency and intricate
detail from textual descriptions. While recent progress on text-to-3D
generation methods have been promising, prevailing methods often fail to ensure
view-consistency and textural richness. This problem becomes particularly
noticeable for methods that work with text input alone. To address this, we
propose a two-stage Gaussian Splatting based approach that enforces geometric
consistency among views. Initially, a coarse 3D generation undergoes refinement
via geometric optimization. Subsequently, we use a ControlNet driven refiner
coupled with the geometric consistency term to improve both texture fidelity
and overall consistency of the generated 3D asset. Empirical evaluations across
diverse textual prompts spanning various object categories demonstrate the
efficacy of DreamPolisher in generating consistent and realistic 3D objects,
aligning closely with the semantics of the textual instructions.
\\ ( https://arxiv.org/abs/2403.17237 ,  19053kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17266 (*cross-listing*)
Date: Mon, 25 Mar 2024 23:19:19 GMT   (926kb)

Title: Exploring CausalWorld: Enhancing robotic manipulation via knowledge
  transfer and curriculum learning
Authors: Xinrui Wang, Yan Jin
Categories: cs.RO cs.AI cs.LG
\\
  This study explores a learning-based tri-finger robotic arm manipulating
task, which requires complex movements and coordination among the fingers. By
employing reinforcement learning, we train an agent to acquire the necessary
skills for proficient manipulation. To enhance the efficiency and effectiveness
of the learning process, two knowledge transfer strategies, fine-tuning and
curriculum learning, were utilized within the soft actor-critic architecture.
Fine-tuning allows the agent to leverage pre-trained knowledge and adapt it to
new tasks. Several variations like model transfer, policy transfer, and
across-task transfer were implemented and evaluated. To eliminate the need for
pretraining, curriculum learning decomposes the advanced task into simpler,
progressive stages, mirroring how humans learn. The number of learning stages,
the context of the sub-tasks, and the transition timing were found to be the
critical design parameters. The key factors of two learning strategies and
corresponding effects were explored in context-aware and context-unaware
scenarios, enabling us to identify the scenarios where the methods demonstrate
optimal performance, derive conclusive insights, and contribute to a broader
range of learning-based engineering applications.
\\ ( https://arxiv.org/abs/2403.17266 ,  926kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17338 (*cross-listing*)
Date: Tue, 26 Mar 2024 02:49:08 GMT   (1436kb,D)

Title: Reinforcement Learning-based Receding Horizon Control using Adaptive
  Control Barrier Functions for Safety-Critical Systems
Authors: Ehsan Sabouni, H.M. Sabbir Ahmad, Vittorio Giammarino, Christos G.
  Cassandras, Ioannis Ch. Paschalidis, Wenchao Li
Categories: eess.SY cs.AI cs.SY
\\
  Optimal control methods provide solutions to safety-critical problems but
easily become intractable. Control Barrier Functions (CBFs) have emerged as a
popular technique that facilitates their solution by provably guaranteeing
safety, through their forward invariance property, at the expense of some
performance loss. This approach involves defining a performance objective
alongside CBF-based safety constraints that must always be enforced.
Unfortunately, both performance and solution feasibility can be significantly
impacted by two key factors: (i) the selection of the cost function and
associated parameters, and (ii) the calibration of parameters within the
CBF-based constraints, which capture the trade-off between performance and
conservativeness. %as well as infeasibility. To address these challenges, we
propose a Reinforcement Learning (RL)-based Receding Horizon Control (RHC)
approach leveraging Model Predictive Control (MPC) with CBFs (MPC-CBF). In
particular, we parameterize our controller and use bilevel optimization, where
RL is used to learn the optimal parameters while MPC computes the optimal
control input. We validate our method by applying it to the challenging
automated merging control problem for Connected and Automated Vehicles (CAVs)
at conflicting roadways. Results demonstrate improved performance and a
significant reduction in the number of infeasible cases compared to traditional
heuristic approaches used for tuning CBF-based controllers, showcasing the
effectiveness of the proposed method.
\\ ( https://arxiv.org/abs/2403.17338 ,  1436kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17342 (*cross-listing*)
Date: Tue, 26 Mar 2024 03:03:50 GMT   (993kb,D)

Title: The Solution for the ICCV 2023 1st Scientific Figure Captioning
  Challenge
Authors: Dian Chao, Xin Song, Shupeng Zhong, Boyuan Wang, Xiangyu Wu, Chen Zhu,
  Yang Yang
Categories: cs.CV cs.AI
\\
  In this paper, we propose a solution for improving the quality of captions
generated for figures in papers. We adopt the approach of summarizing the
textual content in the paper to generate image captions. Throughout our study,
we encounter discrepancies in the OCR information provided in the official
dataset. To rectify this, we employ the PaddleOCR toolkit to extract OCR
information from all images. Moreover, we observe that certain textual content
in the official paper pertains to images that are not relevant for captioning,
thereby introducing noise during caption generation. To mitigate this issue, we
leverage LLaMA to extract image-specific information by querying the textual
content based on image mentions, effectively filtering out extraneous
information. Additionally, we recognize a discrepancy between the primary use
of maximum likelihood estimation during text generation and the evaluation
metrics such as ROUGE employed to assess the quality of generated captions. To
bridge this gap, we integrate the BRIO model framework, enabling a more
coherent alignment between the generation and evaluation processes. Our
approach ranked first in the final test with a score of 4.49.
\\ ( https://arxiv.org/abs/2403.17342 ,  993kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17357 (*cross-listing*)
Date: Tue, 26 Mar 2024 03:44:51 GMT   (1744kb,D)

Title: MESIA: Understanding and Leveraging Supplementary Nature of Method-level
  Comments for Automatic Comment Generation
Authors: Xinglu Pan, Chenxiao Liu, Yanzhen Zou, Tao Xie, Bing Xie
Categories: cs.SE cs.AI
Comments: In 32nd IEEE/ACM International Conference on Program Comprehension
  (ICPC'24)
\\
  Code comments are important for developers in program comprehension. In
scenarios of comprehending and reusing a method, developers expect code
comments to provide supplementary information beyond the method signature.
However, the extent of such supplementary information varies a lot in different
code comments. In this paper, we raise the awareness of the supplementary
nature of method-level comments and propose a new metric named MESIA (Mean
Supplementary Information Amount) to assess the extent of supplementary
information that a code comment can provide. With the MESIA metric, we conduct
experiments on a popular code-comment dataset and three common types of neural
approaches to generate method-level comments. Our experimental results
demonstrate the value of our proposed work with a number of findings. (1)
Small-MESIA comments occupy around 20% of the dataset and mostly fall into only
the WHAT comment category. (2) Being able to provide various kinds of essential
information, large-MESIA comments in the dataset are difficult for existing
neural approaches to generate. (3) We can improve the capability of existing
neural approaches to generate large-MESIA comments by reducing the proportion
of small-MESIA comments in the training set. (4) The retrained model can
generate large-MESIA comments that convey essential meaningful supplementary
information for methods in the small-MESIA test set, but will get a lower BLEU
score in evaluation. These findings indicate that with good training data,
auto-generated comments can sometimes even surpass human-written reference
comments, and having no appropriate ground truth for evaluation is an issue
that needs to be addressed by future work on automatic comment generation.
\\ ( https://arxiv.org/abs/2403.17357 ,  1744kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17373 (*cross-listing*)
Date: Tue, 26 Mar 2024 04:27:56 GMT   (6816kb,D)

Title: AIDE: An Automatic Data Engine for Object Detection in Autonomous
  Driving
Authors: Mingfu Liang, Jong-Chyi Su, Samuel Schulter, Sparsh Garg, Shiyu Zhao,
  Ying Wu, Manmohan Chandraker
Categories: cs.CV cs.AI cs.LG
Comments: Accepted by CVPR-2024
\\
  Autonomous vehicle (AV) systems rely on robust perception models as a
cornerstone of safety assurance. However, objects encountered on the road
exhibit a long-tailed distribution, with rare or unseen categories posing
challenges to a deployed perception model. This necessitates an expensive
process of continuously curating and annotating data with significant human
effort. We propose to leverage recent advances in vision-language and large
language models to design an Automatic Data Engine (AIDE) that automatically
identifies issues, efficiently curates data, improves the model through
auto-labeling, and verifies the model through generation of diverse scenarios.
This process operates iteratively, allowing for continuous self-improvement of
the model. We further establish a benchmark for open-world detection on AV
datasets to comprehensively evaluate various learning paradigms, demonstrating
our method's superior performance at a reduced cost.
\\ ( https://arxiv.org/abs/2403.17373 ,  6816kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17377 (*cross-listing*)
Date: Tue, 26 Mar 2024 04:49:11 GMT   (41596kb,D)

Title: Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance
Authors: Donghoon Ahn, Hyoungwon Cho, Jaewon Min, Wooseok Jang, Jungwoo Kim,
  SeonHwa Kim, Hyun Hee Park, Kyong Hwan Jin, Seungryong Kim
Categories: cs.CV cs.AI cs.LG
Comments: Project page is available at
  https://ku-cvlab.github.io/Perturbed-Attention-Guidance
\\
  Recent studies have demonstrated that diffusion models are capable of
generating high-quality samples, but their quality heavily depends on sampling
guidance techniques, such as classifier guidance (CG) and classifier-free
guidance (CFG). These techniques are often not applicable in unconditional
generation or in various downstream tasks such as image restoration. In this
paper, we propose a novel sampling guidance, called Perturbed-Attention
Guidance (PAG), which improves diffusion sample quality across both
unconditional and conditional settings, achieving this without requiring
additional training or the integration of external modules. PAG is designed to
progressively enhance the structure of samples throughout the denoising
process. It involves generating intermediate samples with degraded structure by
substituting selected self-attention maps in diffusion U-Net with an identity
matrix, by considering the self-attention mechanisms' ability to capture
structural information, and guiding the denoising process away from these
degraded samples. In both ADM and Stable Diffusion, PAG surprisingly improves
sample quality in conditional and even unconditional scenarios. Moreover, PAG
significantly improves the baseline performance in various downstream tasks
where existing guidances such as CG or CFG cannot be fully utilized, including
ControlNet with empty prompts and image restoration such as inpainting and
deblurring.
\\ ( https://arxiv.org/abs/2403.17377 ,  41596kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17379 (*cross-listing*)
Date: Thu, 22 Feb 2024 22:34:06 GMT   (4723kb,D)

Title: Exploring and Applying Audio-Based Sentiment Analysis in Music
Authors: Etash Jhanji
Categories: cs.SD cs.AI cs.LG eess.AS
Comments: 5 pages, 7 figures, 2 tables. For source code, see
  https://github.com/etashj/Exploring-and-Applying-Audio-Based-Sentiment-Analysis
\\
  Sentiment analysis is a continuously explored area of text processing that
deals with the computational analysis of opinions, sentiments, and subjectivity
of text. However, this idea is not limited to text and speech, in fact, it
could be applied to other modalities. In reality, humans do not express
themselves in text as deeply as they do in music. The ability of a
computational model to interpret musical emotions is largely unexplored and
could have implications and uses in therapy and musical queuing. In this paper,
two individual tasks are addressed. This study seeks to (1) predict the emotion
of a musical clip over time and (2) determine the next emotion value after the
music in a time series to ensure seamless transitions. Utilizing data from the
Emotions in Music Database, which contains clips of songs selected from the
Free Music Archive annotated with levels of valence and arousal as reported on
Russel's circumplex model of affect by multiple volunteers, models are trained
for both tasks. Overall, the performance of these models reflected that they
were able to perform the tasks they were designed for effectively and
accurately.
\\ ( https://arxiv.org/abs/2403.17379 ,  4723kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17421 (*cross-listing*)
Date: Tue, 26 Mar 2024 06:34:23 GMT   (3065kb,D)

Title: MA4DIV: Multi-Agent Reinforcement Learning for Search Result
  Diversification
Authors: Yiqun Chen, Jiaxin Mao, Yi Zhang, Dehong MA, Long Xia, Jun Fan,
  Daiting Shi, Zhicong Cheng, Dawei Yin
Categories: cs.IR cs.AI
\\
  The objective of search result diversification (SRD) is to ensure that
selected documents cover as many different subtopics as possible. Existing
methods primarily utilize a paradigm of "greedy selection", i.e., selecting one
document with the highest diversity score at a time. These approaches tend to
be inefficient and are easily trapped in a suboptimal state. In addition, some
other methods aim to approximately optimize the diversity metric, such as
$\alpha$-NDCG, but the results still remain suboptimal. To address these
challenges, we introduce Multi-Agent reinforcement learning (MARL) for search
result DIVersity, which called MA4DIV. In this approach, each document is an
agent and the search result diversification is modeled as a cooperative task
among multiple agents. This approach allows for directly optimizing the
diversity metrics, such as $\alpha$-NDCG, while achieving high training
efficiency. We conducted preliminary experiments on public TREC datasets to
demonstrate the effectiveness and potential of MA4DIV. Considering the limited
number of queries in public TREC datasets, we construct a large-scale dataset
from industry sources and show that MA4DIV achieves substantial improvements in
both effectiveness and efficiency than existing baselines on a industrial scale
dataset.
\\ ( https://arxiv.org/abs/2403.17421 ,  3065kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17465 (*cross-listing*)
Date: Tue, 26 Mar 2024 07:55:16 GMT   (3763kb,D)

Title: LaRE^2: Latent Reconstruction Error Based Method for Diffusion-Generated
  Image Detection
Authors: Yunpeng Luo, Junlong Du, Ke Yan, Shouhong Ding
Categories: cs.CV cs.AI
Comments: CVPR 2024
\\
  The evolution of Diffusion Models has dramatically improved image generation
quality, making it increasingly difficult to differentiate between real and
generated images. This development, while impressive, also raises significant
privacy and security concerns. In response to this, we propose a novel Latent
REconstruction error guided feature REfinement method (LaRE^2) for detecting
the diffusion-generated images. We come up with the Latent Reconstruction Error
(LaRE), the first reconstruction-error based feature in the latent space for
generated image detection. LaRE surpasses existing methods in terms of feature
extraction efficiency while preserving crucial cues required to differentiate
between the real and the fake. To exploit LaRE, we propose an Error-Guided
feature REfinement module (EGRE), which can refine the image feature guided by
LaRE to enhance the discriminativeness of the feature. Our EGRE utilizes an
align-then-refine mechanism, which effectively refines the image feature for
generated-image detection from both spatial and channel perspectives. Extensive
experiments on the large-scale GenImage benchmark demonstrate the superiority
of our LaRE^2, which surpasses the best SoTA method by up to 11.9%/12.1%
average ACC/AP across 8 different image generators. LaRE also surpasses
existing methods in terms of feature extraction cost, delivering an impressive
speed enhancement of 8 times.
\\ ( https://arxiv.org/abs/2403.17465 ,  3763kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17479 (*cross-listing*)
Date: Tue, 26 Mar 2024 08:19:29 GMT   (1199kb,D)

Title: Natural Language Requirements Testability Measurement Based on
  Requirement Smells
Authors: Morteza Zakeri-Nasrabadi and Saeed Parsa
Categories: cs.SE cs.AI cs.LG
Comments: 45 pages, 16 figures, and 13 tables; submitted as a journal paper
\\
  Requirements form the basis for defining software systems' obligations and
tasks. Testable requirements help prevent failures, reduce maintenance costs,
and make it easier to perform acceptance tests. However, despite the importance
of measuring and quantifying requirements testability, no automatic approach
for measuring requirements testability has been proposed based on the
requirements smells, which are at odds with the requirements testability. This
paper presents a mathematical model to evaluate and rank the natural language
requirements testability based on an extensive set of nine requirements smells,
detected automatically, and acceptance test efforts determined by requirement
length and its application domain. Most of the smells stem from uncountable
adjectives, context-sensitive, and ambiguous words. A comprehensive dictionary
is required to detect such words. We offer a neural word-embedding technique to
generate such a dictionary automatically. Using the dictionary, we could
automatically detect Polysemy smell (domain-specific ambiguity) for the first
time in 10 application domains. Our empirical study on nearly 1000 software
requirements from six well-known industrial and academic projects demonstrates
that the proposed smell detection approach outperforms Smella, a
state-of-the-art tool, in detecting requirements smells. The precision and
recall of smell detection are improved with an average of 0.03 and 0.33,
respectively, compared to the state-of-the-art. The proposed requirement
testability model measures the testability of 985 requirements with a mean
absolute error of 0.12 and a mean squared error of 0.03, demonstrating the
model's potential for practical use.
\\ ( https://arxiv.org/abs/2403.17479 ,  1199kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17515 (*cross-listing*)
Date: Tue, 26 Mar 2024 09:18:50 GMT   (1011kb,D)

Title: Prediction-sharing During Training and Inference
Authors: Yotam Gafni, Ronen Gradwohl, Moshe Tennenholtz
Categories: econ.TH cs.AI cs.GT cs.LG cs.MA
\\
  Two firms are engaged in a competitive prediction task. Each firm has two
sources of data -- labeled historical data and unlabeled inference-time data --
and uses the former to derive a prediction model, and the latter to make
predictions on new instances. We study data-sharing contracts between the
firms. The novelty of our study is to introduce and highlight the differences
between contracts that share prediction models only, contracts to share
inference-time predictions only, and contracts to share both. Our analysis
proceeds on three levels. First, we develop a general Bayesian framework that
facilitates our study. Second, we narrow our focus to two natural settings
within this framework: (i) a setting in which the accuracy of each firm's
prediction model is common knowledge, but the correlation between the
respective models is unknown; and (ii) a setting in which two hypotheses exist
regarding the optimal predictor, and one of the firms has a structural
advantage in deducing it. Within these two settings we study optimal contract
choice. More specifically, we find the individually rational and Pareto-optimal
contracts for some notable cases, and describe specific settings where each of
the different sharing contracts emerge as optimal. Finally, in the third level
of our analysis we demonstrate the applicability of our concepts in a synthetic
simulation using real loan data.
\\ ( https://arxiv.org/abs/2403.17515 ,  1011kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17525 (*cross-listing*)
Date: Tue, 26 Mar 2024 09:26:12 GMT   (9000kb,D)

Title: Equipping Sketch Patches with Context-Aware Positional Encoding for
  Graphic Sketch Representation
Authors: Sicong Zang, Zhijun Fang
Categories: cs.CV cs.AI
\\
  The drawing order of a sketch records how it is created stroke-by-stroke by a
human being. For graphic sketch representation learning, recent studies have
injected sketch drawing orders into graph edge construction by linking each
patch to another in accordance to a temporal-based nearest neighboring
strategy. However, such constructed graph edges may be unreliable, since a
sketch could have variants of drawings. In this paper, we propose a
variant-drawing-protected method by equipping sketch patches with context-aware
positional encoding (PE) to make better use of drawing orders for learning
graphic sketch representation. Instead of injecting sketch drawings into graph
edges, we embed these sequential information into graph nodes only. More
specifically, each patch embedding is equipped with a sinusoidal absolute PE to
highlight the sequential position in the drawing order. And its neighboring
patches, ranked by the values of self-attention scores between patch
embeddings, are equipped with learnable relative PEs to restore the contextual
positions within a neighborhood. During message aggregation via graph
convolutional networks, a node receives both semantic contents from patch
embeddings and contextual patterns from PEs by its neighbors, arriving at
drawing-order-enhanced sketch representations. Experimental results indicate
that our method significantly improves sketch healing and controllable sketch
synthesis.
\\ ( https://arxiv.org/abs/2403.17525 ,  9000kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17530 (*cross-listing*)
Date: Tue, 26 Mar 2024 09:36:20 GMT   (1907kb,D)

Title: Boosting Few-Shot Learning with Disentangled Self-Supervised Learning
  and Meta-Learning for Medical Image Classification
Authors: Eva Pachetti, Sotirios A. Tsaftaris, Sara Colantonio
Categories: cs.CV cs.AI
Comments: 20 pages, 4 figures, 4 tables. Submitted to Elsevier on 25 March 2024
ACM-class: I.2; I.4; I.5; J.3
\\
  Background and objective: Employing deep learning models in critical domains
such as medical imaging poses challenges associated with the limited
availability of training data. We present a strategy for improving the
performance and generalization capabilities of models trained in low-data
regimes. Methods: The proposed method starts with a pre-training phase, where
features learned in a self-supervised learning setting are disentangled to
improve the robustness of the representations for downstream tasks. We then
introduce a meta-fine-tuning step, leveraging related classes between
meta-training and meta-testing phases but varying the granularity level. This
approach aims to enhance the model's generalization capabilities by exposing it
to more challenging classification tasks during meta-training and evaluating it
on easier tasks but holding greater clinical relevance during meta-testing. We
demonstrate the effectiveness of the proposed approach through a series of
experiments exploring several backbones, as well as diverse pre-training and
fine-tuning schemes, on two distinct medical tasks, i.e., classification of
prostate cancer aggressiveness from MRI data and classification of breast
cancer malignity from microscopic images. Results: Our results indicate that
the proposed approach consistently yields superior performance w.r.t. ablation
experiments, maintaining competitiveness even when a distribution shift between
training and evaluation data occurs. Conclusion: Extensive experiments
demonstrate the effectiveness and wide applicability of the proposed approach.
We hope that this work will add another solution to the arsenal of addressing
learning issues in data-scarce imaging domains.
\\ ( https://arxiv.org/abs/2403.17530 ,  1907kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17587 (*cross-listing*)
Date: Tue, 26 Mar 2024 10:53:25 GMT   (60kb,D)

Title: Parameterized Analysis of Bribery in Challenge the Champ Tournaments
Authors: Juhi Chaudhary and Hendrik Molter and Meirav Zehavi
Categories: cs.DS cs.AI
\\
  Challenge the champ tournaments are one of the simplest forms of competition,
where a (initially selected) champ is repeatedly challenged by other players.
If a player beats the champ, then that player is considered the new (current)
champ. Each player in the competition challenges the current champ once in a
fixed order. The champ of the last round is considered the winner of the
tournament. We investigate a setting where players can be bribed to lower their
winning probability against the initial champ. The goal is to maximize the
probability of the initial champ winning the tournament by bribing the other
players, while not exceeding a given budget for the bribes. Mattei et al.
[Journal of Applied Logic, 2015] showed that the problem can be solved in
pseudo-polynomial time, and that it is in XP when parameterized by the number
of players.
  We show that the problem is weakly NP-hard and W[1]-hard when parameterized
by the number of players. On the algorithmic side, we show that the problem is
fixed-parameter tractable when parameterized either by the number of different
bribe values or the number of different probability values. To this end, we
establish several results that are of independent interest. In particular, we
show that the product knapsack problem is W[1]-hard when parameterized by the
number of items in the knapsack, and that constructive bribery for cup
tournaments is W[1]-hard when parameterized by the number of players.
Furthermore, we present a novel way of designing mixed integer linear programs,
ensuring optimal solutions where all variables are integers.
\\ ( https://arxiv.org/abs/2403.17587 ,  60kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17589 (*cross-listing*)
Date: Tue, 26 Mar 2024 10:54:07 GMT   (347kb,D)

Title: Dual Memory Networks: A Versatile Adaptation Approach for
  Vision-Language Models
Authors: Yabin Zhang, Wenjie Zhu, Hui Tang, Zhiyuan Ma, Kaiyang Zhou, Lei Zhang
Categories: cs.CV cs.AI cs.LG cs.MM
Comments: CVPR2024; Codes are available at \url{https://github.com/YBZh/DMN}
\\
  With the emergence of pre-trained vision-language models like CLIP, how to
adapt them to various downstream classification tasks has garnered significant
attention in recent research. The adaptation strategies can be typically
categorized into three paradigms: zero-shot adaptation, few-shot adaptation,
and the recently-proposed training-free few-shot adaptation. Most existing
approaches are tailored for a specific setting and can only cater to one or two
of these paradigms. In this paper, we introduce a versatile adaptation approach
that can effectively work under all three settings. Specifically, we propose
the dual memory networks that comprise dynamic and static memory components.
The static memory caches training data knowledge, enabling training-free
few-shot adaptation, while the dynamic memory preserves historical test
features online during the testing process, allowing for the exploration of
additional data insights beyond the training set. This novel capability
enhances model performance in the few-shot setting and enables model usability
in the absence of training data. The two memory networks employ the same
flexible memory interactive strategy, which can operate in a training-free mode
and can be further enhanced by incorporating learnable projection layers. Our
approach is tested across 11 datasets under the three task settings.
Remarkably, in the zero-shot scenario, it outperforms existing methods by over
3\% and even shows superior results against methods utilizing external training
data. Additionally, our method exhibits robust performance against natural
distribution shifts. Codes are available at \url{https://github.com/YBZh/DMN}.
\\ ( https://arxiv.org/abs/2403.17589 ,  347kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17633 (*cross-listing*)
Date: Tue, 26 Mar 2024 12:08:14 GMT   (4622kb,D)

Title: UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object
  Detection with Sparse LiDAR and Large Domain Gaps
Authors: Maciej K Wozniak, Mattias Hansson, Marko Thiel, Patric Jensfelt
Categories: cs.CV cs.AI cs.RO
\\
  In this study, we address a gap in existing unsupervised domain adaptation
approaches on LiDAR-based 3D object detection, which have predominantly
concentrated on adapting between established, high-density autonomous driving
datasets. We focus on sparser point clouds, capturing scenarios from different
perspectives: not just from vehicles on the road but also from mobile robots on
sidewalks, which encounter significantly different environmental conditions and
sensor configurations. We introduce Unsupervised Adversarial Domain Adaptation
for 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source
models or teacher-student architectures. Instead, it uses an adversarial
approach to directly learn domain-invariant features. We demonstrate its
efficacy in various adaptation scenarios, showing significant improvements in
both self-driving car and mobile robot domains. Our code is open-source and
will be available soon.
\\ ( https://arxiv.org/abs/2403.17633 ,  4622kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17656 (*cross-listing*)
Date: Tue, 26 Mar 2024 12:39:02 GMT   (2899kb,D)

Title: SGHormer: An Energy-Saving Graph Transformer Driven by Spikes
Authors: Huizhe Zhang, Jintang Li, Liang Chen, Zibin Zheng
Categories: cs.NE cs.AI cs.LG
Comments: 9 pages, 3 figures
\\
  Graph Transformers (GTs) with powerful representation learning ability make a
huge success in wide range of graph tasks. However, the costs behind
outstanding performances of GTs are higher energy consumption and computational
overhead. The complex structure and quadratic complexity during attention
calculation in vanilla transformer seriously hinder its scalability on the
large-scale graph data. Though existing methods have made strides in
simplifying combinations among blocks or attention-learning paradigm to improve
GTs' efficiency, a series of energy-saving solutions originated from
biologically plausible structures are rarely taken into consideration when
constructing GT framework. To this end, we propose a new spiking-based graph
transformer (SGHormer). It turns full-precision embeddings into sparse and
binarized spikes to reduce memory and computational costs. The spiking graph
self-attention and spiking rectify blocks in SGHormer explicitly capture global
structure information and recover the expressive power of spiking embeddings,
respectively. In experiments, SGHormer achieves comparable performances to
other full-precision GTs with extremely low computational energy consumption.
The results show that SGHomer makes a remarkable progress in the field of
low-energy GTs.
\\ ( https://arxiv.org/abs/2403.17656 ,  2899kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17674 (*cross-listing*)
Date: Tue, 26 Mar 2024 13:02:46 GMT   (1609kb,D)

Title: Depending on yourself when you should: Mentoring LLM with RL agents to
  become the master in cybersecurity games
Authors: Yikuan Yan, Yaolun Zhang, Keman Huang
Categories: cs.CR cs.AI cs.MA
Comments: 10 pages
\\
  Integrating LLM and reinforcement learning (RL) agent effectively to achieve
complementary performance is critical in high stake tasks like cybersecurity
operations. In this study, we introduce SecurityBot, a LLM agent mentored by
pre-trained RL agents, to support cybersecurity operations. In particularly,
the LLM agent is supported with a profile module to generated behavior
guidelines, a memory module to accumulate local experiences, a reflection
module to re-evaluate choices, and an action module to reduce action space.
Additionally, it adopts the collaboration mechanism to take suggestions from
pre-trained RL agents, including a cursor for dynamic suggestion taken, an
aggregator for multiple mentors' suggestions ranking and a caller for proactive
suggestion asking. Building on the CybORG experiment framework, our experiences
show that SecurityBot demonstrates significant performance improvement compared
with LLM or RL standalone, achieving the complementary performance in the
cybersecurity games.
\\ ( https://arxiv.org/abs/2403.17674 ,  1609kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17677 (*cross-listing*)
Date: Tue, 26 Mar 2024 13:05:02 GMT   (383kb,D)

Title: Onboard deep lossless and near-lossless predictive coding of
  hyperspectral images with line-based attention
Authors: Diego Valsesia, Tiziano Bianchi, Enrico Magli
Categories: eess.IV cs.AI
\\
  Deep learning methods have traditionally been difficult to apply to
compression of hyperspectral images onboard of spacecrafts, due to the large
computational complexity needed to achieve adequate representational power, as
well as the lack of suitable datasets for training and testing. In this paper,
we depart from the traditional autoencoder approach and we design a predictive
neural network, called LineRWKV, that works recursively line-by-line to limit
memory consumption. In order to achieve that, we adopt a novel hybrid
attentive-recursive operation that combines the representational advantages of
Transformers with the linear complexity and recursive implementation of
recurrent neural networks. The compression algorithm performs prediction of
each pixel using LineRWKV, followed by entropy coding of the residual.
Experiments on the HySpecNet-11k dataset and PRISMA images show that LineRWKV
is the first deep-learning method to outperform CCSDS-123.0-B-2 at lossless and
near-lossless compression. Promising throughput results are also evaluated on a
7W embedded system.
\\ ( https://arxiv.org/abs/2403.17677 ,  383kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17693 (*cross-listing*)
Date: Tue, 26 Mar 2024 13:34:21 GMT   (10515kb,D)

Title: ExpressEdit: Video Editing with Natural Language and Sketching
Authors: Bekzat Tilekbay, Saelyne Yang, Michal Lewkowicz, Alex Suryapranata,
  Juho Kim
Categories: cs.HC cs.AI
Comments: 22 pages, 5 figures, to be published in ACM IUI 2024
DOI: 10.1145/3640543.3645164
\\
  Informational videos serve as a crucial source for explaining conceptual and
procedural knowledge to novices and experts alike. When producing informational
videos, editors edit videos by overlaying text/images or trimming footage to
enhance the video quality and make it more engaging. However, video editing can
be difficult and time-consuming, especially for novice video editors who often
struggle with expressing and implementing their editing ideas. To address this
challenge, we first explored how multimodality$-$natural language (NL) and
sketching, which are natural modalities humans use for expression$-$can be
utilized to support video editors in expressing video editing ideas. We
gathered 176 multimodal expressions of editing commands from 10 video editors,
which revealed the patterns of use of NL and sketching in describing edit
intents. Based on the findings, we present ExpressEdit, a system that enables
editing videos via NL text and sketching on the video frame. Powered by LLM and
vision models, the system interprets (1) temporal, (2) spatial, and (3)
operational references in an NL command and spatial references from sketching.
The system implements the interpreted edits, which then the user can iterate
on. An observational study (N=10) showed that ExpressEdit enhanced the ability
of novice video editors to express and implement their edit ideas. The system
allowed participants to perform edits more efficiently and generate more ideas
by generating edits based on user's multimodal edit commands and supporting
iterations on the editing commands. This work offers insights into the design
of future multimodal interfaces and AI-based pipelines for video editing.
\\ ( https://arxiv.org/abs/2403.17693 ,  10515kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17710 (*cross-listing*)
Date: Tue, 26 Mar 2024 13:58:00 GMT   (364kb,D)

Title: Optimization-based Prompt Injection Attack to LLM-as-a-Judge
Authors: Jiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun,
  Neil Zhenqiang Gong
Categories: cs.CR cs.AI
\\
  LLM-as-a-Judge is a novel solution that can assess textual information with
large language models (LLMs). Based on existing research studies, LLMs
demonstrate remarkable performance in providing a compelling alternative to
traditional human assessment. However, the robustness of these systems against
prompt injection attacks remains an open question. In this work, we introduce
JudgeDeceiver, a novel optimization-based prompt injection attack tailored to
LLM-as-a-Judge. Our method formulates a precise optimization objective for
attacking the decision-making process of LLM-as-a-Judge and utilizes an
optimization algorithm to efficiently automate the generation of adversarial
sequences, achieving targeted and effective manipulation of model evaluations.
Compared to handcraft prompt injection attacks, our method demonstrates
superior efficacy, posing a significant challenge to the current security
paradigms of LLM-based judgment systems. Through extensive experiments, we
showcase the capability of JudgeDeceiver in altering decision outcomes across
various cases, highlighting the vulnerability of LLM-as-a-Judge systems to the
optimization-based prompt injection attack.
\\ ( https://arxiv.org/abs/2403.17710 ,  364kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17740 (*cross-listing*)
Date: Tue, 26 Mar 2024 14:29:34 GMT   (4865kb,D)

Title: All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating
  Prediction
Authors: Shuheng Fang, Kangfei Zhao, Yu Rong, Zhixun Li and Jeffrey Xu Yu
Categories: cs.IR cs.AI
Comments: 14 pages, 9 figures
\\
  Cold-start rating prediction is a fundamental problem in recommender systems
that has been extensively studied. Many methods have been proposed that exploit
explicit relations among existing data, such as collaborative filtering, social
recommendations and heterogeneous information network, to alleviate the data
insufficiency issue for cold-start users and items. However, the explicit
relations constructed based on data between different roles may be unreliable
and irrelevant, which limits the performance ceiling of the specific
recommendation task. Motivated by this, in this paper, we propose a flexible
framework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not
solely rely on the pre-defined interaction pattern or the manually constructed
heterogeneous information network. Instead, we devise a Heterogeneous
Interaction Module (HIM) to jointly model the heterogeneous interactions and
directly infer the important interactions via the observed data. In the
experiments, we evaluate our model under three cold-start settings on three
real-world datasets. The experimental results show that HIRE outperforms other
baselines by a large margin. Furthermore, we visualize the inferred
interactions of HIRE to confirm the contribution of our model.
\\ ( https://arxiv.org/abs/2403.17740 ,  4865kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17784 (*cross-listing*)
Date: Tue, 26 Mar 2024 15:16:14 GMT   (7007kb,D)

Title: SciCapenter: Supporting Caption Composition for Scientific Figures with
  Machine-Generated Captions and Ratings
Authors: Ting-Yao Hsu, Chieh-Yang Huang, Shih-Hong Huang, Ryan Rossi, Sungchul
  Kim, Tong Yu, C. Lee Giles, Ting-Hao K. Huang
Categories: cs.HC cs.AI
Comments: CHI EA '24: Extended Abstracts of the 2024 CHI Conference on Human
  Factors in Computing Systems
DOI: 10.1145/3613905.3650738
\\
  Crafting effective captions for figures is important. Readers heavily depend
on these captions to grasp the figure's message. However, despite a
well-developed set of AI technologies for figures and captions, these have
rarely been tested for usefulness in aiding caption writing. This paper
introduces SciCapenter, an interactive system that puts together cutting-edge
AI technologies for scientific figure captions to aid caption composition.
SciCapenter generates a variety of captions for each figure in a scholarly
article, providing scores and a comprehensive checklist to assess caption
quality across multiple critical aspects, such as helpfulness, OCR mention, key
takeaways, and visual properties reference. Users can directly edit captions in
SciCapenter, resubmit for revised evaluations, and iteratively refine them. A
user study with Ph.D. students indicates that SciCapenter significantly lowers
the cognitive load of caption writing. Participants' feedback further offers
valuable design insights for future systems aiming to enhance caption writing.
\\ ( https://arxiv.org/abs/2403.17784 ,  7007kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17819 (*cross-listing*)
Date: Tue, 26 Mar 2024 15:54:48 GMT   (1549kb,D)

Title: Accelerating Radio Spectrum Regulation Workflows with Large Language
  Models (LLMs)
Authors: Amir Ghasemi, Paul Guinand
Categories: cs.NI cs.AI
\\
  Wireless spectrum regulation is a complex and demanding process due to the
rapid pace of technological progress, increasing demand for spectrum, and a
multitude of stakeholders with potentially conflicting interests, alongside
significant economic implications. To navigate this, regulators must engage
effectively with all parties, keep pace with global technology trends, conduct
technical evaluations, issue licenses in a timely manner, and comply with
various legal and policy frameworks.
  In light of these challenges, this paper demonstrates example applications of
Large Language Models (LLMs) to expedite spectrum regulatory processes. We
explore various roles that LLMs can play in this context while identifying some
of the challenges to address. The paper also offers practical case studies and
insights, with appropriate experiments, highlighting the transformative
potential of LLMs in spectrum management.
\\ ( https://arxiv.org/abs/2403.17819 ,  1549kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17827 (*cross-listing*)
Date: Tue, 26 Mar 2024 16:06:42 GMT   (24893kb,D)

Title: DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from
  Textual Descriptions
Authors: Sammy Christen and Shreyas Hampali and Fadime Sener and Edoardo
  Remelli and Tomas Hodan and Eric Sauser and Shugao Ma and Bugra Tekin
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: Project Page: https://diffh2o.github.io/
\\
  Generating natural hand-object interactions in 3D is challenging as the
resulting hand and object motions are expected to be physically plausible and
semantically meaningful. Furthermore, generalization to unseen objects is
hindered by the limited scale of available hand-object interaction datasets. We
propose DiffH2O, a novel method to synthesize realistic, one or two-handed
object interactions from provided text prompts and geometry of the object. The
method introduces three techniques that enable effective learning from limited
data. First, we decompose the task into a grasping stage and a text-based
interaction stage and use separate diffusion models for each. In the grasping
stage, the model only generates hand motions, whereas in the interaction phase
both hand and object poses are synthesized. Second, we propose a compact
representation that tightly couples hand and object poses. Third, we propose
two different guidance schemes to allow more control of the generated motions:
grasp guidance and detailed textual guidance. Grasp guidance takes a single
target grasping pose and guides the diffusion model to reach this grasp at the
end of the grasping stage, which provides control over the grasping pose. Given
a grasping motion from this stage, multiple different actions can be prompted
in the interaction phase. For textual guidance, we contribute comprehensive
text descriptions to the GRAB dataset and show that they enable our method to
have more fine-grained control over hand-object interactions. Our quantitative
and qualitative evaluation demonstrates that the proposed method outperforms
baseline methods and leads to natural hand-object motions. Moreover, we
demonstrate the practicality of our framework by utilizing a hand pose estimate
from an off-the-shelf pose estimator for guidance, and then sampling multiple
different actions in the interaction stage.
\\ ( https://arxiv.org/abs/2403.17827 ,  24893kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17839 (*cross-listing*)
Date: Tue, 26 Mar 2024 16:27:37 GMT   (6558kb,D)

Title: ReMamber: Referring Image Segmentation with Mamba Twister
Authors: Yuhuan Yang, Chaofan Ma, Jiangchao Yao, Zhun Zhong, Ya Zhang and
  Yanfeng Wang
Categories: cs.CV cs.AI
\\
  Referring Image Segmentation (RIS) leveraging transformers has achieved great
success on the interpretation of complex visual-language tasks. However, the
quadratic computation cost makes it resource-consuming in capturing long-range
visual-language dependencies. Fortunately, Mamba addresses this with efficient
linear complexity in processing. However, directly applying Mamba to
multi-modal interactions presents challenges, primarily due to inadequate
channel interactions for the effective fusion of multi-modal data. In this
paper, we propose ReMamber, a novel RIS architecture that integrates the power
of Mamba with a multi-modal Mamba Twister block. The Mamba Twister explicitly
models image-text interaction, and fuses textual and visual features through
its unique channel and spatial twisting mechanism. We achieve the
state-of-the-art on three challenging benchmarks. Moreover, we conduct thorough
analyses of ReMamber and discuss other fusion designs using Mamba. These
provide valuable perspectives for future research.
\\ ( https://arxiv.org/abs/2403.17839 ,  6558kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17846 (*cross-listing*)
Date: Tue, 26 Mar 2024 16:36:43 GMT   (37891kb,D)

Title: Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot
  Navigation
Authors: Abdelrhman Werby, Chenguang Huang, Martin B\"uchner, Abhinav Valada,
  Wolfram Burgard
Categories: cs.RO cs.AI cs.CL cs.CV cs.LG
Comments: Code and video are available at http://hovsg.github.io/
\\
  Recent open-vocabulary robot mapping methods enrich dense geometric maps with
pre-trained visual-language features. While these maps allow for the prediction
of point-wise saliency maps when queried for a certain language concept,
large-scale environments and abstract queries beyond the object level still
pose a considerable hurdle, ultimately limiting language-grounded robotic
navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D
scene graph mapping approach for language-grounded robot navigation. Leveraging
open-vocabulary vision foundation models, we first obtain state-of-the-art
open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene
graph hierarchy consisting of floor, room, and object concepts, each enriched
with open-vocabulary features. Our approach is able to represent multi-story
buildings and allows robotic traversal of those using a cross-floor Voronoi
graph. HOV-SG is evaluated on three distinct datasets and surpasses previous
baselines in open-vocabulary semantic accuracy on the object, room, and floor
level while producing a 75% reduction in representation size compared to dense
open-vocabulary maps. In order to prove the efficacy and generalization
capabilities of HOV-SG, we showcase successful long-horizon
language-conditioned robot navigation within real-world multi-storage
environments. We provide code and trial video data at http://hovsg.github.io/.
\\ ( https://arxiv.org/abs/2403.17846 ,  37891kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17916 (*cross-listing*)
Date: Tue, 26 Mar 2024 17:53:27 GMT   (1825kb,D)

Title: CMP: Cooperative Motion Prediction with Multi-Agent Communication
Authors: Zhuoyuan Wu, Yuping Wang, Hengbo Ma, Zhaowei Li, Hang Qiu, Jiachen Li
Categories: cs.RO cs.AI cs.CV cs.LG cs.MA
\\
  The confluence of the advancement of Autonomous Vehicles (AVs) and the
maturity of Vehicle-to-Everything (V2X) communication has enabled the
capability of cooperative connected and automated vehicles (CAVs). Building on
top of cooperative perception, this paper explores the feasibility and
effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR
signals as input to enhance tracking and prediction capabilities. Unlike
previous work that focuses separately on either cooperative perception or
motion prediction, our framework, to the best of our knowledge, is the first to
address the unified problem where CAVs share information in both perception and
prediction modules. Incorporated into our design is the unique capability to
tolerate realistic V2X bandwidth limitations and transmission delays, while
dealing with bulky perception representations. We also propose a prediction
aggregation module, which unifies the predictions obtained by different CAVs
and generates the final prediction. Through extensive experiments and ablation
studies, we demonstrate the effectiveness of our method in cooperative
perception, tracking, and motion prediction tasks. In particular, CMP reduces
the average prediction error by 17.2\% with fewer missing detections compared
with the no cooperation setting. Our work marks a significant step forward in
the cooperative capabilities of CAVs, showcasing enhanced performance in
complex scenarios.
\\ ( https://arxiv.org/abs/2403.17916 ,  1825kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17924 (*cross-listing*)
Date: Tue, 26 Mar 2024 17:57:05 GMT   (46869kb,D)

Title: AID: Attention Interpolation of Text-to-Image Diffusion
Authors: Qiyuan He, Jinghao Wang, Ziwei Liu, Angela Yao
Categories: cs.CV cs.AI
\\
  Conditional diffusion models can create unseen images in various settings,
aiding image interpolation. Interpolation in latent spaces is well-studied, but
interpolation with specific conditions like text or poses is less understood.
Simple approaches, such as linear interpolation in the space of conditions,
often result in images that lack consistency, smoothness, and fidelity. To that
end, we introduce a novel training-free technique named Attention Interpolation
via Diffusion (AID). Our key contributions include 1) proposing an inner/outer
interpolated attention layer; 2) fusing the interpolated attention with
self-attention to boost fidelity; and 3) applying beta distribution to
selection to increase smoothness. We also present a variant, Prompt-guided
Attention Interpolation via Diffusion (PAID), that considers interpolation as a
condition-dependent generative process. This method enables the creation of new
images with greater consistency, smoothness, and efficiency, and offers control
over the exact path of interpolation. Our approach demonstrates effectiveness
for conceptual and spatial interpolation. Code and demo are available at
https://github.com/QY-H00/attention-interpolation-diffusion.
\\ ( https://arxiv.org/abs/2403.17924 ,  46869kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17927 (*cross-listing*)
Date: Tue, 26 Mar 2024 17:57:57 GMT   (1202kb,D)

Title: MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution
Authors: Wei Tao, Yucheng Zhou, Wenqiang Zhang, Yu Cheng
Categories: cs.SE cs.AI
Comments: work in progress
\\
  In software evolution, resolving the emergent issues within GitHub
repositories is a complex challenge that involves not only the incorporation of
new code but also the maintenance of existing functionalities. Large Language
Models (LLMs) have shown promise in code generation and understanding but face
difficulties in code change, particularly at the repository level. To overcome
these challenges, we empirically study the reason why LLMs mostly fail to
resolve GitHub issues and analyze some impact factors. Motivated by the
empirical findings, we propose a novel LLM-based Multi-Agent framework for
GitHub Issue reSolution, MAGIS, consisting of four kinds of agents customized
for the software evolution: Manager, Repository Custodian, Developer, and
Quality Assurance Engineer agents. This framework leverages the collaboration
of various agents in the planning and coding process to unlock the potential of
LLMs to resolve GitHub issues. In experiments, we employ the SWE-bench
benchmark to compare MAGIS with popular LLMs, including GPT-3.5, GPT-4, and
Claude-2. MAGIS can resolve 13.94% GitHub issues, which significantly
outperforms the baselines. Specifically, MAGIS achieves an eight-fold increase
in resolved ratio over the direct application of GPT-4, the based LLM of our
method. We also analyze the factors for improving GitHub issue resolution
rates, such as line location, task allocation, etc.
\\ ( https://arxiv.org/abs/2403.17927 ,  1202kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17933 (*cross-listing*)
Date: Tue, 26 Mar 2024 17:58:29 GMT   (3349kb,D)

Title: SLEDGE: Synthesizing Simulation Environments for Driving Agents with
  Generative Models
Authors: Kashyap Chitta, Daniel Dauner, Andreas Geiger
Categories: cs.RO cs.AI cs.CV cs.LG
\\
  SLEDGE is the first generative simulator for vehicle motion planning trained
on real-world driving logs. Its core component is a learned model that is able
to generate agent bounding boxes and lane graphs. The model's outputs serve as
an initial state for traffic simulation. The unique properties of the entities
to be generated for SLEDGE, such as their connectivity and variable count per
scene, render the naive application of most modern generative models to this
task non-trivial. Therefore, together with a systematic study of existing lane
graph representations, we introduce a novel raster-to-vector autoencoder
(RVAE). It encodes agents and the lane graph into distinct channels in a
rasterized latent map. This facilitates both lane-conditioned agent generation
and combined generation of lanes and agents with a Diffusion Transformer. Using
generated entities in SLEDGE enables greater control over the simulation, e.g.
upsampling turns or increasing traffic density. Further, SLEDGE can support
500m long routes, a capability not found in existing data-driven simulators
like nuPlan. It presents new challenges for planning algorithms, evidenced by
failure rates of over 40% for PDM, the winner of the 2023 nuPlan challenge,
when tested on hard routes and dense traffic generated by our model. Compared
to nuPlan, SLEDGE requires 500$\times$ less storage to set up (<4GB), making it
a more accessible option and helping with democratizing future research in this
field.
\\ ( https://arxiv.org/abs/2403.17933 ,  3349kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17336 (*cross-listing*)
Date: Tue, 26 Mar 2024 02:47:42 GMT   (937kb,D)

Title: Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of
  Large Language Models
Authors: Zhiyuan Yu and Xiaogeng Liu and Shunning Liang and Zach Cameron and
  Chaowei Xiao and Ning Zhang
Categories: cs.CR cs.CL
Comments: Accepted by USENIX Security 2024
\\
  Recent advancements in generative AI have enabled ubiquitous access to large
language models (LLMs). Empowered by their exceptional capabilities to
understand and generate human-like text, these models are being increasingly
integrated into our society. At the same time, there are also concerns on the
potential misuse of this powerful technology, prompting defensive measures from
service providers. To overcome such protection, jailbreaking prompts have
recently emerged as one of the most effective mechanisms to circumvent security
restrictions and elicit harmful content originally designed to be prohibited.
  Due to the rapid development of LLMs and their ease of access via natural
languages, the frontline of jailbreak prompts is largely seen in online forums
and among hobbyists. To gain a better understanding of the threat landscape of
semantically meaningful jailbreak prompts, we systemized existing prompts and
measured their jailbreak effectiveness empirically. Further, we conducted a
user study involving 92 participants with diverse backgrounds to unveil the
process of manually creating jailbreak prompts. We observed that users often
succeeded in jailbreak prompts generation regardless of their expertise in
LLMs. Building on the insights from the user study, we also developed a system
using AI as the assistant to automate the process of jailbreak prompt
generation.
\\ ( https://arxiv.org/abs/2403.17336 ,  937kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17343 (*cross-listing*)
Date: Tue, 26 Mar 2024 03:05:20 GMT   (2127kb,D)

Title: Language Models are Free Boosters for Biomedical Imaging Tasks
Authors: Zhixin Lai, Jing Wu, Suiyao Chen, Yucheng Zhou, Anna Hovakimyan, Naira
  Hovakimyan
Categories: cs.CV cs.CL cs.LG
\\
  In this study, we uncover the unexpected efficacy of residual-based large
language models (LLMs) as part of encoders for biomedical imaging tasks, a
domain traditionally devoid of language or textual data. The approach diverges
from established methodologies by utilizing a frozen transformer block,
extracted from pre-trained LLMs, as an innovative encoder layer for the direct
processing of visual tokens. This strategy represents a significant departure
from the standard multi-modal vision-language frameworks, which typically hinge
on language-driven prompts and inputs. We found that these LLMs could boost
performance across a spectrum of biomedical imaging applications, including
both 2D and 3D visual classification tasks, serving as plug-and-play boosters.
More interestingly, as a byproduct, we found that the proposed framework
achieved superior performance, setting new state-of-the-art results on
extensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we
aim to open new avenues for employing LLMs in biomedical imaging and enriching
the understanding of their potential in this specialized domain.
\\ ( https://arxiv.org/abs/2403.17343 ,  2127kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17344 (*cross-listing*)
Date: Tue, 26 Mar 2024 03:07:32 GMT   (629kb,D)

Title: Disambiguate Entity Matching through Relation Discovery with Large
  Language Models
Authors: Zezhou Huang
Categories: cs.DB cs.CL
\\
  Entity matching is a critical challenge in data integration and cleaning,
central to tasks like fuzzy joins and deduplication. Traditional approaches
have focused on overcoming fuzzy term representations through methods such as
edit distance, Jaccard similarity, and more recently, embeddings and deep
neural networks, including advancements from large language models (LLMs) like
GPT. However, the core challenge in entity matching extends beyond term
fuzziness to the ambiguity in defining what constitutes a "match," especially
when integrating with external databases. This ambiguity arises due to varying
levels of detail and granularity among entities, complicating exact matches. We
propose a novel approach that shifts focus from purely identifying semantic
similarities to understanding and defining the "relations" between entities as
crucial for resolving ambiguities in matching. By predefining a set of
relations relevant to the task at hand, our method allows analysts to navigate
the spectrum of similarity more effectively, from exact matches to conceptually
related entities.
\\ ( https://arxiv.org/abs/2403.17344 ,  629kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17524 (*cross-listing*)
Date: Tue, 26 Mar 2024 09:25:57 GMT   (2804kb,D)

Title: Provably Secure Disambiguating Neural Linguistic Steganography
Authors: Yuang Qi, Kejiang Chen, Kai Zeng, Weiming Zhang, and Nenghai Yu
Categories: cs.CR cs.CL
\\
  Recent research in provably secure neural linguistic steganography has
overlooked a crucial aspect: the sender must detokenize stegotexts to avoid
raising suspicion from the eavesdropper. The segmentation ambiguity problem,
which arises when using language models based on subwords, leads to occasional
decoding failures in all neural language steganography implementations based on
these models. Current solutions to this issue involve altering the probability
distribution of candidate words, rendering them incompatible with provably
secure steganography. We propose a novel secure disambiguation method named
SyncPool, which effectively addresses the segmentation ambiguity problem. We
group all tokens with prefix relationships in the candidate pool before the
steganographic embedding algorithm runs to eliminate uncertainty among
ambiguous tokens. To enable the receiver to synchronize the sampling process of
the sender, a shared cryptographically-secure pseudorandom number generator
(CSPRNG) is deployed to select a token from the ambiguity pool. SyncPool does
not change the size of the candidate pool or the distribution of tokens and
thus is applicable to provably secure language steganography methods. We
provide theoretical proofs and experimentally demonstrate the applicability of
our solution to various languages and models, showing its potential to
significantly improve the reliability and security of neural linguistic
steganography systems.
\\ ( https://arxiv.org/abs/2403.17524 ,  2804kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17599 (*cross-listing*)
Date: Tue, 26 Mar 2024 11:09:58 GMT   (1125kb)

Title: Coimagining the Future of Voice Assistants with Cultural Sensitivity
Authors: Katie Seaborn, Yuto Sawa, Mizuki Watanabe
Categories: cs.HC cs.CL cs.CY
Comments: 21 pages
Journal-ref: Human Behavior and Emerging Technologies, vol. 2024, Article ID
  3238737, 21 pages, 2024
DOI: 10.1155/2024/3238737
\\
  Voice assistants (VAs) are becoming a feature of our everyday life. Yet, the
user experience (UX) is often limited, leading to underuse, disengagement, and
abandonment. Co-designing interactions for VAs with potential end-users can be
useful. Crowdsourcing this process online and anonymously may add value.
However, most work has been done in the English-speaking West on dialogue data
sets. We must be sensitive to cultural differences in language, social
interactions, and attitudes towards technology. Our aims were to explore the
value of co-designing VAs in the non-Western context of Japan and demonstrate
the necessity of cultural sensitivity. We conducted an online elicitation study
(N = 135) where Americans (n = 64) and Japanese people (n = 71) imagined
dialogues (N = 282) and activities (N = 73) with future VAs. We discuss the
implications for coimagining interactions with future VAs, offer design
guidelines for the Japanese and English-speaking US contexts, and suggest
opportunities for cultural plurality in VA design and scholarship.
\\ ( https://arxiv.org/abs/2403.17599 ,  1125kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17691 (*cross-listing*)
Date: Tue, 26 Mar 2024 13:32:32 GMT   (10819kb,D)

Title: Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to
  Inform GenAI Copyright Disputes
Authors: Uri Hacohen, Adi Haviv, Shahar Sarfaty, Bruria Friedman, Niva
  Elkin-Koren, Roi Livni and Amit H Bermano
Categories: cs.CV cs.CL
Comments: Presented at ACM CSLAW 2024
\\
  The advent of Generative Artificial Intelligence (GenAI) models, including
GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content
creation, enabling non-professionals to produce high-quality content across
various domains. This transformative technology has led to a surge of synthetic
content and sparked legal disputes over copyright infringement. To address
these challenges, this paper introduces a novel approach that leverages the
learning capacity of GenAI models for copyright legal analysis, demonstrated
with GPT2 and Stable Diffusion models. Copyright law distinguishes between
original expressions and generic ones (Sc\`enes \`a faire), protecting the
former and permitting reproduction of the latter. However, this distinction has
historically been challenging to make consistently, leading to over-protection
of copyrighted works. GenAI offers an unprecedented opportunity to enhance this
legal analysis by revealing shared patterns in preexisting works. We propose a
data-driven approach to identify the genericity of works created by GenAI,
employing "data-driven bias" to assess the genericity of expressive
compositions. This approach aids in copyright scope determination by utilizing
the capabilities of GenAI to identify and prioritize expressive elements and
rank them according to their frequency in the model's dataset. The potential
implications of measuring expressive genericity for copyright law are profound.
Such scoring could assist courts in determining copyright scope during
litigation, inform the registration practices of Copyright Offices, allowing
registration of only highly original synthetic works, and help copyright owners
signal the value of their works and facilitate fairer licensing deals. More
generally, this approach offers valuable insights to policymakers grappling
with adapting copyright law to the challenges posed by the era of GenAI.
\\ ( https://arxiv.org/abs/2403.17691 ,  10819kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17727 (*cross-listing*)
Date: Tue, 26 Mar 2024 14:16:56 GMT   (2724kb,D)

Title: FastPerson: Enhancing Video Learning through Effective Video
  Summarization that Preserves Linguistic and Visual Contexts
Authors: Kazuki Kawamura and Jun Rekimoto
Categories: cs.CV cs.CL cs.HC cs.MM
Journal-ref: AHs '24: Proceedings of the Augmented Humans International
  Conference 2024
DOI: 10.1145/3652920.3652922
\\
  Quickly understanding lengthy lecture videos is essential for learners with
limited time and interest in various topics to improve their learning
efficiency. To this end, video summarization has been actively researched to
enable users to view only important scenes from a video. However, these studies
focus on either the visual or audio information of a video and extract
important segments in the video. Therefore, there is a risk of missing
important information when both the teacher's speech and visual information on
the blackboard or slides are important, such as in a lecture video. To tackle
this issue, we propose FastPerson, a video summarization approach that
considers both the visual and auditory information in lecture videos.
FastPerson creates summary videos by utilizing audio transcriptions along with
on-screen images and text, minimizing the risk of overlooking crucial
information for learners. Further, it provides a feature that allows learners
to switch between the summary and original videos for each chapter of the
video, enabling them to adjust the pace of learning based on their interests
and level of understanding. We conducted an evaluation with 40 participants to
assess the effectiveness of our method and confirmed that it reduced viewing
time by 53\% at the same level of comprehension as that when using traditional
video playback methods.
\\ ( https://arxiv.org/abs/2403.17727 ,  2724kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17804 (*cross-listing*)
Date: Tue, 26 Mar 2024 15:42:01 GMT   (29784kb,D)

Title: Improving Text-to-Image Consistency via Automatic Prompt Optimization
Authors: Oscar Ma\~nas, Pietro Astolfi, Melissa Hall, Candace Ross, Jack
  Urbanek, Adina Williams, Aishwarya Agrawal, Adriana Romero-Soriano, Michal
  Drozdzal
Categories: cs.CV cs.CL
\\
  Impressive advances in text-to-image (T2I) generative models have yielded a
plethora of high performing models which are able to generate aesthetically
appealing, photorealistic images. Despite the progress, these models still
struggle to produce images that are consistent with the input prompt,
oftentimes failing to capture object quantities, relations and attributes
properly. Existing solutions to improve prompt-image consistency suffer from
the following challenges: (1) they oftentimes require model fine-tuning, (2)
they only focus on nearby prompt samples, and (3) they are affected by
unfavorable trade-offs among image quality, representation diversity, and
prompt-image consistency. In this paper, we address these challenges and
introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a
large language model (LLM) to improve prompt-image consistency in T2I models.
Our framework starts from a user prompt and iteratively generates revised
prompts with the goal of maximizing a consistency score. Our extensive
validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost
the initial consistency score by up to 24.9% in terms of DSG score while
preserving the FID and increasing the recall between generated and real data.
Our work paves the way toward building more reliable and robust T2I systems by
harnessing the power of LLMs.
\\ ( https://arxiv.org/abs/2403.17804 ,  29784kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13358 (*cross-listing*)
Date: Wed, 20 Mar 2024 07:36:43 GMT   (2181kb,D)

Title: GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped
  Robot
Authors: Wenxuan Song, Han Zhao, Pengxiang Ding, Can Cui, Shangke Lyu, Yaning
  Fan, Donglin Wang
Categories: cs.RO cs.CV cs.LG
\\
  Multi-task robot learning holds significant importance in tackling diverse
and complex scenarios. However, current approaches are hindered by performance
issues and difficulties in collecting training datasets. In this paper, we
propose GeRM (Generalist Robotic Model). We utilize offline reinforcement
learning to optimize data utilization strategies to learn from both
demonstrations and sub-optimal data, thus surpassing the limitations of human
demonstrations. Thereafter, we employ a transformer-based VLA network to
process multi-modal inputs and output actions. By introducing the
Mixture-of-Experts structure, GeRM allows faster inference speed with higher
whole model capacity, and thus resolves the issue of limited RL parameters,
enhancing model performance in multi-task learning while controlling
computational costs. Through a series of experiments, we demonstrate that GeRM
outperforms other methods across all tasks, while also validating its
efficiency in both training and inference processes. Additionally, we uncover
its potential to acquire emergent skills. Additionally, we contribute the
QUARD-Auto dataset, collected automatically to support our training approach
and foster advancements in multi-task quadruped robot learning. This work
presents a new paradigm for reducing the cost of collecting robot data and
driving progress in the multi-task learning community.
\\ ( https://arxiv.org/abs/2403.13358 ,  2181kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17013 (*cross-listing*)
Date: Mon, 12 Feb 2024 16:24:13 GMT   (2590kb,D)

Title: Temporal-Spatial Processing of Event Camera Data via Delay-Loop
  Reservoir Neural Network
Authors: Richard Lau, Anthony Tylan-Tyler, Lihan Yao, Rey de Castro Roberto,
  Robert Taylor, Isaiah Jones
Categories: cs.CV cs.LG
Comments: 10 pages, 12 figures, Darpa Distribution Statement A. Approved for
  public release. Distribution Unlimited
\\
  This paper describes a temporal-spatial model for video processing with
special applications to processing event camera videos. We propose to study a
conjecture motivated by our previous study of video processing with delay loop
reservoir (DLR) neural network, which we call Temporal-Spatial Conjecture
(TSC). The TSC postulates that there is significant information content carried
in the temporal representation of a video signal and that machine learning
algorithms would benefit from separate optimization of the spatial and temporal
components for intelligent processing. To verify or refute the TSC, we propose
a Visual Markov Model (VMM) which decompose the video into spatial and temporal
components and estimate the mutual information (MI) of these components. Since
computation of video mutual information is complex and time consuming, we use a
Mutual Information Neural Network to estimate the bounds of the mutual
information. Our result shows that the temporal component carries significant
MI compared to that of the spatial component. This finding has often been
overlooked in neural network literature. In this paper, we will exploit this
new finding to guide our design of a delay-loop reservoir neural network for
event camera classification, which results in a 18% improvement on
classification accuracy.
\\ ( https://arxiv.org/abs/2403.17013 ,  2590kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17014 (*cross-listing*)
Date: Mon, 12 Feb 2024 21:33:46 GMT   (1434kb,D)

Title: Contrastive Learning for Regression on Hyperspectral Data
Authors: Mohamad Dhaini, Maxime Berar, Paul Honeine, Antonin Van Exem
Categories: cs.CV cs.LG
Comments: Accepted in IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP) 2024
\\
  Contrastive learning has demonstrated great effectiveness in representation
learning especially for image classification tasks. However, there is still a
shortage in the studies targeting regression tasks, and more specifically
applications on hyperspectral data. In this paper, we propose a contrastive
learning framework for the regression tasks for hyperspectral data. To this
end, we provide a collection of transformations relevant for augmenting
hyperspectral data, and investigate contrastive learning for regression.
Experiments on synthetic and real hyperspectral datasets show that the proposed
framework and transformations significantly improve the performance of
regression models, achieving better scores than other state-of-the-art
transformations.
\\ ( https://arxiv.org/abs/2403.17014 ,  1434kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17016 (*cross-listing*)
Date: Wed, 14 Feb 2024 22:10:52 GMT   (2077kb,D)

Title: HEAL-ViT: Vision Transformers on a spherical mesh for medium-range
  weather forecasting
Authors: Vivek Ramavajjala
Categories: cs.CV cs.LG physics.ao-ph
Comments: 18 pages, 14 figures, preprint
\\
  In recent years, a variety of ML architectures and techniques have seen
success in producing skillful medium range weather forecasts. In particular,
Vision Transformer (ViT)-based models (e.g. Pangu-Weather, FuXi) have shown
strong performance, working nearly "out-of-the-box" by treating weather data as
a multi-channel image on a rectilinear grid. While a rectilinear grid is
appropriate for 2D images, weather data is inherently spherical and thus
heavily distorted at the poles on a rectilinear grid, leading to
disproportionate compute being used to model data near the poles. Graph-based
methods (e.g. GraphCast) do not suffer from this problem, as they map the
longitude-latitude grid to a spherical mesh, but are generally more memory
intensive and tend to need more compute resources for training and inference.
While spatially homogeneous, the spherical mesh does not lend itself readily to
be modeled by ViT-based models that implicitly rely on the rectilinear grid
structure. We present HEAL-ViT, a novel architecture that uses ViT models on a
spherical mesh, thus benefiting from both the spatial homogeneity enjoyed by
graph-based models and efficient attention-based mechanisms exploited by
transformers. HEAL-ViT produces weather forecasts that outperform the ECMWF IFS
on key metrics, and demonstrate better bias accumulation and blurring than
other ML weather prediction models. Further, the lowered compute footprint of
HEAL-ViT makes it attractive for operational use as well, where other models in
addition to a 6-hourly prediction model may be needed to produce the full set
of operational forecasts required.
\\ ( https://arxiv.org/abs/2403.17016 ,  2077kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17042 (*cross-listing*)
Date: Mon, 25 Mar 2024 15:58:26 GMT   (7282kb,D)

Title: Provably Robust Score-Based Diffusion Posterior Sampling for
  Plug-and-Play Image Reconstruction
Authors: Xingyu Xu, Yuejie Chi
Categories: eess.IV cs.CV cs.LG eess.SP math.OC stat.ML
\\
  In a great number of tasks in science and engineering, the goal is to infer
an unknown image from a small number of measurements collected from a known
forward model describing certain sensing or imaging modality. Due to resource
constraints, this task is often extremely ill-posed, which necessitates the
adoption of expressive prior information to regularize the solution space.
Score-based diffusion models, due to its impressive empirical success, have
emerged as an appealing candidate of an expressive prior in image
reconstruction. In order to accommodate diverse tasks at once, it is of great
interest to develop efficient, consistent and robust algorithms that
incorporate {\em unconditional} score functions of an image prior distribution
in conjunction with flexible choices of forward models.
  This work develops an algorithmic framework for employing score-based
diffusion models as an expressive data prior in general nonlinear inverse
problems. Motivated by the plug-and-play framework in the imaging community, we
introduce a diffusion plug-and-play method (\textsf{DPnP}) that alternatively
calls two samplers, a proximal consistency sampler based solely on the
likelihood function of the forward model, and a denoising diffusion sampler
based solely on the score functions of the image prior. The key insight is that
denoising under white Gaussian noise can be solved {\em rigorously} via both
stochastic (i.e., DDPM-type) and deterministic (i.e., DDIM-type) samplers using
the unconditional score functions. We establish both asymptotic and
non-asymptotic performance guarantees of \textsf{DPnP}, and provide numerical
experiments to illustrate its promise in solving both linear and nonlinear
image reconstruction tasks. To the best of our knowledge, \textsf{DPnP} is the
first provably-robust posterior sampling method for nonlinear inverse problems
using unconditional diffusion priors.
\\ ( https://arxiv.org/abs/2403.17042 ,  7282kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17081 (*cross-listing*)
Date: Mon, 25 Mar 2024 18:12:16 GMT   (380kb,D)

Title: Machine Learning on Blockchain Data: A Systematic Mapping Study
Authors: Georgios Palaiokrassas, Sarah Bouraga, Leandros Tassiulas
Categories: cs.CR cs.LG
\\
  Context: Blockchain technology has drawn growing attention in the literature
and in practice. Blockchain technology generates considerable amounts of data
and has thus been a topic of interest for Machine Learning (ML).
  Objective: The objective of this paper is to provide a comprehensive review
of the state of the art on machine learning applied to blockchain data. This
work aims to systematically identify, analyze, and classify the literature on
ML applied to blockchain data. This will allow us to discover the fields where
more effort should be placed in future research.
  Method: A systematic mapping study has been conducted to identify the
relevant literature. Ultimately, 159 articles were selected and classified
according to various dimensions, specifically, the domain use case, the
blockchain, the data, and the machine learning models.
  Results: The majority of the papers (49.7%) fall within the Anomaly use case.
Bitcoin (47.2%) was the blockchain that drew the most attention. A dataset
consisting of more than 1.000.000 data points was used by 31.4% of the papers.
And Classification (46.5%) was the ML task most applied to blockchain data.
  Conclusion: The results confirm that ML applied to blockchain data is a
relevant and a growing topic of interest both in the literature and in
practice. Nevertheless, some open challenges and gaps remain, which can lead to
future research directions. Specifically, we identify novel machine learning
algorithms, the lack of a standardization framework, blockchain scalability
issues and cross-chain interactions as areas worth exploring in the future.
\\ ( https://arxiv.org/abs/2403.17081 ,  380kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17094 (*cross-listing*)
Date: Mon, 25 Mar 2024 18:32:41 GMT   (35163kb,D)

Title: SynFog: A Photo-realistic Synthetic Fog Dataset based on End-to-end
  Imaging Simulation for Advancing Real-World Defogging in Autonomous Driving
Authors: Yiming Xie, Henglu Wei, Zhenyi Liu, Xiaoyu Wang, Xiangyang Ji
Categories: cs.CV cs.LG
\\
  To advance research in learning-based defogging algorithms, various synthetic
fog datasets have been developed. However, existing datasets created using the
Atmospheric Scattering Model (ASM) or real-time rendering engines often
struggle to produce photo-realistic foggy images that accurately mimic the
actual imaging process. This limitation hinders the effective generalization of
models from synthetic to real data. In this paper, we introduce an end-to-end
simulation pipeline designed to generate photo-realistic foggy images. This
pipeline comprehensively considers the entire physically-based foggy scene
imaging process, closely aligning with real-world image capture methods. Based
on this pipeline, we present a new synthetic fog dataset named SynFog, which
features both sky light and active lighting conditions, as well as three levels
of fog density. Experimental results demonstrate that models trained on SynFog
exhibit superior performance in visual perception and detection accuracy
compared to others when applied to real-world foggy images.
\\ ( https://arxiv.org/abs/2403.17094 ,  35163kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17142 (*cross-listing*)
Date: Mon, 25 Mar 2024 19:39:17 GMT   (113kb,D)

Title: Approximation with Random Shallow ReLU Networks with Applications to
  Model Reference Adaptive Control
Authors: Andrew Lamperski and Tyler Lekang
Categories: math.OC cs.LG cs.SY eess.SY
Comments: Under Review for Conference on Decision and Control
\\
  Neural networks are regularly employed in adaptive control of nonlinear
systems and related methods o reinforcement learning. A common architecture
uses a neural network with a single hidden layer (i.e. a shallow network), in
which the weights and biases are fixed in advance and only the output layer is
trained. While classical results show that there exist neural networks of this
type that can approximate arbitrary continuous functions over bounded regions,
they are non-constructive, and the networks used in practice have no
approximation guarantees. Thus, the approximation properties required for
control with neural networks are assumed, rather than proved. In this paper, we
aim to fill this gap by showing that for sufficiently smooth functions, ReLU
networks with randomly generated weights and biases achieve $L_{\infty}$ error
of $O(m^{-1/2})$ with high probability, where $m$ is the number of neurons. It
suffices to generate the weights uniformly over a sphere and the biases
uniformly over an interval. We show how the result can be used to get
approximations of required accuracy in a model reference adaptive control
application.
\\ ( https://arxiv.org/abs/2403.17142 ,  113kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17177 (*cross-listing*)
Date: Mon, 25 Mar 2024 20:44:01 GMT   (3313kb,D)

Title: Brain Stroke Segmentation Using Deep Learning Models: A Comparative
  Study
Authors: Ahmed Soliman, Yousif Yousif, Ahmed Ibrahim, Yalda Zafari-Ghadim,
  Essam A. Rashed and Mohamed Mabrok
Categories: eess.IV cs.CV cs.LG
\\
  Stroke segmentation plays a crucial role in the diagnosis and treatment of
stroke patients by providing spatial information about affected brain regions
and the extent of damage. Segmenting stroke lesions accurately is a challenging
task, given that conventional manual techniques are time consuming and prone to
errors. Recently, advanced deep models have been introduced for general medical
image segmentation, demonstrating promising results that surpass many state of
the art networks when evaluated on specific datasets. With the advent of the
vision Transformers, several models have been introduced based on them, while
others have aimed to design better modules based on traditional convolutional
layers to extract long-range dependencies like Transformers. The question of
whether such high-level designs are necessary for all segmentation cases to
achieve the best results remains unanswered. In this study, we selected four
types of deep models that were recently proposed and evaluated their
performance for stroke segmentation: a pure Transformer-based architecture
(DAE-Former), two advanced CNN-based models (LKA and DLKA) with attention
mechanisms in their design, an advanced hybrid model that incorporates CNNs
with Transformers (FCT), and the well- known self-adaptive nnUNet framework
with its configuration based on given data. We examined their performance on
two publicly available datasets, and found that the nnUNet achieved the best
results with the simplest design among all. Revealing the robustness issue of
Transformers to such variabilities serves as a potential reason for their
weaker performance. Furthermore, nnUNet's success underscores the significant
impact of preprocessing and postprocessing techniques in enhancing segmentation
results, surpassing the focus solely on architectural designs
\\ ( https://arxiv.org/abs/2403.17177 ,  3313kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17181 (*cross-listing*)
Date: Mon, 25 Mar 2024 20:47:10 GMT   (7483kb,D)

Title: On the Intersection of Signal Processing and Machine Learning: A Use
  Case-Driven Analysis Approach
Authors: Sulaiman Aburakhia, Abdallah Shami, George K. Karagiannidis
Categories: eess.SP cs.LG
\\
  Recent advancements in sensing, measurement, and computing technologies have
significantly expanded the potential for signal-based applications, leveraging
the synergy between signal processing and Machine Learning (ML) to improve both
performance and reliability. This fusion represents a critical point in the
evolution of signal-based systems, highlighting the need to bridge the existing
knowledge gap between these two interdisciplinary fields. Despite many attempts
in the existing literature to bridge this gap, most are limited to specific
applications and focus mainly on feature extraction, often assuming extensive
prior knowledge in signal processing. This assumption creates a significant
obstacle for a wide range of readers. To address these challenges, this paper
takes an integrated article approach. It begins with a detailed tutorial on the
fundamentals of signal processing, providing the reader with the necessary
background knowledge. Following this, it explores the key stages of a standard
signal processing-based ML pipeline, offering an in-depth review of feature
extraction techniques, their inherent challenges, and solutions. Differing from
existing literature, this work offers an application-independent review and
introduces a novel classification taxonomy for feature extraction techniques.
Furthermore, it aims at linking theoretical concepts with practical
applications, and demonstrates this through two specific use cases: a
spectral-based method for condition monitoring of rolling bearings and a
wavelet energy analysis for epilepsy detection using EEG signals. In addition
to theoretical contributions, this work promotes a collaborative research
culture by providing a public repository of relevant Python and MATLAB signal
processing codes. This effort is intended to support collaborative research
efforts and ensure the reproducibility of the results presented.
\\ ( https://arxiv.org/abs/2403.17181 ,  7483kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17218 (*cross-listing*)
Date: Mon, 25 Mar 2024 21:47:36 GMT   (5724kb,D)

Title: A Comprehensive Study of the Capabilities of Large Language Models for
  Vulnerability Detection
Authors: Benjamin Steenhoek, Md Mahbubur Rahman, Monoshi Kumar Roy, Mirza
  Sanjida Alam, Earl T. Barr, and Wei Le
Categories: cs.SE cs.CR cs.LG
\\
  Large Language Models (LLMs) have demonstrated great potential for code
generation and other software engineering tasks. Vulnerability detection is of
crucial importance to maintaining the security, integrity, and trustworthiness
of software systems. Precise vulnerability detection requires reasoning about
the code, making it a good case study for exploring the limits of LLMs'
reasoning capabilities. Although recent work has applied LLMs to vulnerability
detection using generic prompting techniques, their full capabilities for this
task and the types of errors they make when explaining identified
vulnerabilities remain unclear.
  In this paper, we surveyed eleven LLMs that are state-of-the-art in code
generation and commonly used as coding assistants, and evaluated their
capabilities for vulnerability detection. We systematically searched for the
best-performing prompts, incorporating techniques such as in-context learning
and chain-of-thought, and proposed three of our own prompting methods. Our
results show that while our prompting methods improved the models' performance,
LLMs generally struggled with vulnerability detection. They reported 0.5-0.63
Balanced Accuracy and failed to distinguish between buggy and fixed versions of
programs in 76% of cases on average. By comprehensively analyzing and
categorizing 287 instances of model reasoning, we found that 57% of LLM
responses contained errors, and the models frequently predicted incorrect
locations of buggy code and misidentified bug types. LLMs only correctly
localized 6 out of 27 bugs in DbgBench, and these 6 bugs were predicted
correctly by 70-100% of human participants. These findings suggest that despite
their potential for other tasks, LLMs may fail to properly comprehend critical
code structures and security-related concepts. Our data and code are available
at https://figshare.com/s/78fe02e56e09ec49300b.
\\ ( https://arxiv.org/abs/2403.17218 ,  5724kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17231 (*cross-listing*)
Date: Mon, 25 Mar 2024 22:17:51 GMT   (1978kb,D)

Title: Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from
  Learned Hallucination
Authors: Saad Abdul Ghani, Zizhao Wang, Peter Stone, Xuesu Xiao
Categories: cs.RO cs.LG
Comments: Submitted to International Conference on Intelligent Robots and
  Systems (IROS) 2024
\\
  This paper presents a self-supervised learning method to safely learn a
motion planner for ground robots to navigate environments with dense and
dynamic obstacles. When facing highly-cluttered, fast-moving, hard-to-predict
obstacles, classical motion planners may not be able to keep up with limited
onboard computation. For learning-based planners, high-quality demonstrations
are difficult to acquire for imitation learning while reinforcement learning
becomes inefficient due to the high probability of collision during
exploration. To safely and efficiently provide training data, the Learning from
Hallucination (LfH) approaches synthesize difficult navigation environments
based on past successful navigation experiences in relatively easy or
completely open ones, but unfortunately cannot address dynamic obstacles. In
our new Dynamic Learning from Learned Hallucination (Dyna-LfLH), we design and
learn a novel latent distribution and sample dynamic obstacles from it, so the
generated training data can be used to learn a motion planner to navigate in
dynamic environments. Dyna-LfLH is evaluated on a ground robot in both
simulated and physical environments and achieves up to 25% better success rate
compared to baselines.
\\ ( https://arxiv.org/abs/2403.17231 ,  1978kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17233 (*cross-listing*)
Date: Mon, 25 Mar 2024 22:20:45 GMT   (862kb,D)

Title: Active Learning of Dynamics Using Prior Domain Knowledge in the Sampling
  Process
Authors: Kevin S. Miller, Adam J. Thorpe, Ufuk Topcu
Categories: eess.SY cs.LG cs.SY
\\
  We present an active learning algorithm for learning dynamics that leverages
side information by explicitly incorporating prior domain knowledge into the
sampling process. Our proposed algorithm guides the exploration toward regions
that demonstrate high empirical discrepancy between the observed data and an
imperfect prior model of the dynamics derived from side information. Through
numerical experiments, we demonstrate that this strategy explores regions of
high discrepancy and accelerates learning while simultaneously reducing model
uncertainty. We rigorously prove that our active learning algorithm yields a
consistent estimate of the underlying dynamics by providing an explicit rate of
convergence for the maximum predictive variance. We demonstrate the efficacy of
our approach on an under-actuated pendulum system and on the half-cheetah
MuJoCo environment.
\\ ( https://arxiv.org/abs/2403.17233 ,  862kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17238 (*cross-listing*)
Date: Mon, 25 Mar 2024 22:39:20 GMT   (2725kb,D)

Title: Temporal and Semantic Evaluation Metrics for Foundation Models in
  Post-Hoc Analysis of Robotic Sub-tasks
Authors: Jonathan Salfity, Selma Wanna, Minkyu Choi, and Mitch Pryor
Categories: cs.RO cs.LG
Comments: 8 pages, 3 figures. IROS 2024 Submission
\\
  Recent works in Task and Motion Planning (TAMP) show that training control
policies on language-supervised robot trajectories with quality labeled data
markedly improves agent task success rates. However, the scarcity of such data
presents a significant hurdle to extending these methods to general use cases.
To address this concern, we present an automated framework to decompose
trajectory data into temporally bounded and natural language-based descriptive
sub-tasks by leveraging recent prompting strategies for Foundation Models (FMs)
including both Large Language Models (LLMs) and Vision Language Models (VLMs).
Our framework provides both time-based and language-based descriptions for
lower-level sub-tasks that comprise full trajectories. To rigorously evaluate
the quality of our automatic labeling framework, we contribute an algorithm
SIMILARITY to produce two novel metrics, temporal similarity and semantic
similarity. The metrics measure the temporal alignment and semantic fidelity of
language descriptions between two sub-task decompositions, namely an FM
sub-task decomposition prediction and a ground-truth sub-task decomposition. We
present scores for temporal similarity and semantic similarity above 90%,
compared to 30% of a randomized baseline, for multiple robotic environments,
demonstrating the effectiveness of our proposed framework. Our results enable
building diverse, large-scale, language-supervised datasets for improved
robotic TAMP.
\\ ( https://arxiv.org/abs/2403.17238 ,  2725kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17285 (*cross-listing*)
Date: Tue, 26 Mar 2024 00:25:32 GMT   (8253kb,D)

Title: An Analysis of Switchback Designs in Reinforcement Learning
Authors: Qianglin Wen, Chengchun Shi, Ying Yang, Niansheng Tang and Hongtu Zhu
Categories: stat.ML cs.LG
\\
  This paper offers a detailed investigation of switchback designs in A/B
testing, which alternate between baseline and new policies over time. Our aim
is to thoroughly evaluate the effects of these designs on the accuracy of their
resulting average treatment effect (ATE) estimators. We propose a novel "weak
signal analysis" framework, which substantially simplifies the calculations of
the mean squared errors (MSEs) of these ATEs in Markov decision process
environments. Our findings suggest that (i) when the majority of reward errors
are positively correlated, the switchback design is more efficient than the
alternating-day design which switches policies in a daily basis. Additionally,
increasing the frequency of policy switches tends to reduce the MSE of the ATE
estimator. (ii) When the errors are uncorrelated, however, all these designs
become asymptotically equivalent. (iii) In cases where the majority of errors
are negative correlated, the alternating-day design becomes the optimal choice.
These insights are crucial, offering guidelines for practitioners on designing
experiments in A/B testing. Our analysis accommodates a variety of policy value
estimators, including model-based estimators, least squares temporal difference
learning estimators, and double reinforcement learning estimators, thereby
offering a comprehensive understanding of optimal design strategies for policy
evaluation in reinforcement learning.
\\ ( https://arxiv.org/abs/2403.17285 ,  8253kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17296 (*cross-listing*)
Date: Tue, 26 Mar 2024 00:51:12 GMT   (473kb,D)

Title: Hawk: Accurate and Fast Privacy-Preserving Machine Learning Using Secure
  Lookup Table Computation
Authors: Hamza Saleem, Amir Ziashahabi, Muhammad Naveed, Salman Avestimehr
Categories: cs.CR cs.LG
Comments: Accepted at Privacy Enhancing Technologies Symposium (PETS) 2024
\\
  Training machine learning models on data from multiple entities without
direct data sharing can unlock applications otherwise hindered by business,
legal, or ethical constraints. In this work, we design and implement new
privacy-preserving machine learning protocols for logistic regression and
neural network models. We adopt a two-server model where data owners
secret-share their data between two servers that train and evaluate the model
on the joint data. A significant source of inefficiency and inaccuracy in
existing methods arises from using Yao's garbled circuits to compute non-linear
activation functions. We propose new methods for computing non-linear functions
based on secret-shared lookup tables, offering both computational efficiency
and improved accuracy.
  Beyond introducing leakage-free techniques, we initiate the exploration of
relaxed security measures for privacy-preserving machine learning. Instead of
claiming that the servers gain no knowledge during the computation, we contend
that while some information is revealed about access patterns to lookup tables,
it maintains epsilon-dX-privacy. Leveraging this relaxation significantly
reduces the computational resources needed for training. We present new
cryptographic protocols tailored to this relaxed security paradigm and define
and analyze the leakage. Our evaluations show that our logistic regression
protocol is up to 9x faster, and the neural network training is up to 688x
faster than SecureML. Notably, our neural network achieves an accuracy of 96.6%
on MNIST in 15 epochs, outperforming prior benchmarks that capped at 93.4%
using the same architecture.
\\ ( https://arxiv.org/abs/2403.17296 ,  473kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17353 (*cross-listing*)
Date: Tue, 26 Mar 2024 03:32:45 GMT   (1928kb,D)

Title: Multi-Objective Trajectory Planning with Dual-Encoder
Authors: Beibei Zhang, Tian Xiang, Chentao Mao, Yuhua Zheng, Shuai Li, Haoyi
  Niu, Xiangming Xi, Wenyuan Bai, Feng Gao
Categories: cs.RO cs.LG
Comments: 6 pages, 7 figures, conference
\\
  Time-jerk optimal trajectory planning is crucial in advancing robotic arms'
performance in dynamic tasks. Traditional methods rely on solving complex
nonlinear programming problems, bringing significant delays in generating
optimized trajectories. In this paper, we propose a two-stage approach to
accelerate time-jerk optimal trajectory planning. Firstly, we introduce a
dual-encoder based transformer model to establish a good preliminary
trajectory. This trajectory is subsequently refined through sequential
quadratic programming to improve its optimality and robustness. Our approach
outperforms the state-of-the-art by up to 79.72\% in reducing trajectory
planning time. Compared with existing methods, our method shrinks the
optimality gap with the objective function value decreasing by up to 29.9\%.
\\ ( https://arxiv.org/abs/2403.17353 ,  1928kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17364 (*cross-listing*)
Date: Tue, 26 Mar 2024 04:02:09 GMT   (310kb)

Title: A Moreau Envelope Approach for LQR Meta-Policy Estimation
Authors: Ashwin Aravind, Mohammad Taha Toghani, and C\'esar A. Uribe
Categories: math.OC cs.LG cs.SY eess.SY
Comments: 8 pages
MSC-class: 49M99, 93E35, 93C05
ACM-class: I.2.8
\\
  We study the problem of policy estimation for the Linear Quadratic Regulator
(LQR) in discrete-time linear time-invariant uncertain dynamical systems. We
propose a Moreau Envelope-based surrogate LQR cost, built from a finite set of
realizations of the uncertain system, to define a meta-policy efficiently
adjustable to new realizations. Moreover, we design an algorithm to find an
approximate first-order stationary point of the meta-LQR cost function.
Numerical results show that the proposed approach outperforms naive averaging
of controllers on new realizations of the linear system. We also provide
empirical evidence that our method has better sample complexity than
Model-Agnostic Meta-Learning (MAML) approaches.
\\ ( https://arxiv.org/abs/2403.17364 ,  310kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17425 (*cross-listing*)
Date: Tue, 26 Mar 2024 06:42:23 GMT   (1922kb,D)

Title: Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion
  Rate Prediction with a Single Model
Authors: Wentao Ouyang, Xiuwu Zhang, Chaofeng Guo, Shukui Ren, Yupei Sui, Kun
  Zhang, Jinmei Luo, Yunfeng Chen, Dongbo Xu, Xiangzheng Liu, Yanlong Du
Categories: cs.IR cs.LG
Comments: CIKM 2023 (larger figures)
DOI: 10.1145/3583780.3614697
\\
  In real-world advertising systems, conversions have different types in nature
and ads can be shown in different display scenarios, both of which highly
impact the actual conversion rate (CVR). This results in the multi-type and
multi-scenario CVR prediction problem. A desired model for this problem should
satisfy the following requirements: 1) Accuracy: the model should achieve
fine-grained accuracy with respect to any conversion type in any display
scenario. 2) Scalability: the model parameter size should be affordable. 3)
Convenience: the model should not require a large amount of effort in data
partitioning, subset processing and separate storage. Existing approaches
cannot simultaneously satisfy these requirements. For example, building a
separate model for each (conversion type, display scenario) pair is neither
scalable nor convenient. Building a unified model trained on all the data with
conversion type and display scenario included as two features is not accurate
enough. In this paper, we propose the Masked Multi-domain Network (MMN) to
solve this problem. To achieve the accuracy requirement, we model
domain-specific parameters and propose a dynamically weighted loss to account
for the loss scale imbalance issue within each mini-batch. To achieve the
scalability requirement, we propose a parameter sharing and composition
strategy to reduce model parameters from a product space to a sum space. To
achieve the convenience requirement, we propose an auto-masking strategy which
can take mixed data from all the domains as input. It avoids the overhead
caused by data partitioning, individual processing and separate storage. Both
offline and online experimental results validate the superiority of MMN for
multi-type and multi-scenario CVR prediction. MMN is now the serving model for
real-time CVR prediction in UC Toutiao.
\\ ( https://arxiv.org/abs/2403.17425 ,  1922kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17436 (*cross-listing*)
Date: Tue, 26 Mar 2024 07:05:06 GMT   (1608kb,D)

Title: Particle identification with machine learning from incomplete data in
  the ALICE experiment
Authors: Maja Karwowska, {\L}ukasz Graczykowski, Kamil Deja, Mi{\l}osz Kasak
  and Ma{\l}gorzata Janik (for the ALICE collaboration)
Categories: hep-ex cs.LG
Comments: Proceedings of 3rd Artificial Intelligence for the Electron Ion
  Collider workshop -- AI4EIC2023, 28.11-1.12.2023. Prepared for submission to
  JINST
\\
  The ALICE experiment at the LHC measures properties of the strongly
interacting matter formed in ultrarelativistic heavy-ion collisions. Such
studies require accurate particle identification (PID). ALICE provides PID
information via several detectors for particles with momentum from about 100
MeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular
cuts. Acmuch better performance can be achieved with machine learning (ML)
methods. Our solution uses multiple neural networks (NN) serving as binary
classifiers. Moreover, we extended our particle classifier with Feature Set
Embedding and attention in order to train on data with incomplete samples. We
also present the integration of the ML project with the ALICE analysis
software, and we discuss domain adaptation, the ML technique needed to transfer
the knowledge between simulated and real experimental data.
\\ ( https://arxiv.org/abs/2403.17436 ,  1608kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17458 (*cross-listing*)
Date: Tue, 26 Mar 2024 07:46:27 GMT   (258kb)

Title: Expectations Versus Reality: Evaluating Intrusion Detection Systems in
  Practice
Authors: Jake Hesford, Daniel Cheng, Alan Wan, Larry Huynh, Seungho Kim,
  Hyoungshick Kim, Jin B. Hong
Categories: cs.CR cs.LG
Comments: 10 pages
MSC-class: 68M25, 68M20
ACM-class: C.4; D.m
\\
  Our paper provides empirical comparisons between recent IDSs to provide an
objective comparison between them to help users choose the most appropriate
solution based on their requirements. Our results show that no one solution is
the best, but is dependent on external variables such as the types of attacks,
complexity, and network environment in the dataset. For example, BoT_IoT and
Stratosphere IoT datasets both capture IoT-related attacks, but the deep neural
network performed the best when tested using the BoT_IoT dataset while HELAD
performed the best when tested using the Stratosphere IoT dataset. So although
we found that a deep neural network solution had the highest average F1 scores
on tested datasets, it is not always the best-performing one. We further
discuss difficulties in using IDS from literature and project repositories,
which complicated drawing definitive conclusions regarding IDS selection.
\\ ( https://arxiv.org/abs/2403.17458 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17480 (*cross-listing*)
Date: Tue, 26 Mar 2024 08:22:09 GMT   (72kb,D)

Title: Capacity Provisioning Motivated Online Non-Convex Optimization Problem
  with Memory and Switching Cost
Authors: Rahul Vaze, Jayakrishnan Nair
Categories: cs.DS cs.LG
\\
  An online non-convex optimization problem is considered where the goal is to
minimize the flow time (total delay) of a set of jobs by modulating the number
of active servers, but with a switching cost associated with changing the
number of active servers over time. Each job can be processed by at most one
fixed speed server at any time. Compared to the usual online convex
optimization (OCO) problem with switching cost, the objective function
considered is non-convex and more importantly, at each time, it depends on all
past decisions and not just the present one. Both worst-case and stochastic
inputs are considered; for both cases, competitive algorithms are derived.
\\ ( https://arxiv.org/abs/2403.17480 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17550 (*cross-listing*)
Date: Tue, 26 Mar 2024 09:58:06 GMT   (9359kb,D)

Title: DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping
Authors: Kutay Y{\i}lmaz, Matthias Nie{\ss}ner, Anastasiia Kornilova and Alexey
  Artemov
Categories: cs.CV cs.LG cs.RO
Comments: 8 pages, 6 figures
\\
  Recently, significant progress has been achieved in sensing real large-scale
outdoor 3D environments, particularly by using modern acquisition equipment
such as LiDAR sensors. Unfortunately, they are fundamentally limited in their
ability to produce dense, complete 3D scenes. To address this issue, recent
learning-based methods integrate neural implicit representations and
optimizable feature grids to approximate surfaces of 3D scenes. However,
naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results
due to the nature of sparse, conflicting LiDAR measurements. Instead, in this
work we depart from fitting LiDAR data exactly, instead letting the network
optimize a non-metric monotonic implicit field defined in 3D space. To fit our
field, we design a learning system integrating a monotonicity loss that enables
optimizing neural monotonic fields and leverages recent progress in large-scale
3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as
captured by multiple quantitative and perceptual measures and visual results
obtained for Mai City, Newer College, and KITTI benchmarks. The code of our
approach will be made publicly available.
\\ ( https://arxiv.org/abs/2403.17550 ,  9359kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17608 (*cross-listing*)
Date: Tue, 26 Mar 2024 11:39:00 GMT   (28601kb,D)

Title: Fake or JPEG? Revealing Common Biases in Generated Image Detection
  Datasets
Authors: Patrick Grommelt, Louis Weiss, Franz-Josef Pfreundt, Janis Keuper
Categories: cs.CV cs.LG
\\
  The widespread adoption of generative image models has highlighted the urgent
need to detect artificial content, which is a crucial step in combating
widespread manipulation and misinformation. Consequently, numerous detectors
and associated datasets have emerged. However, many of these datasets
inadvertently introduce undesirable biases, thereby impacting the effectiveness
and evaluation of detectors. In this paper, we emphasize that many datasets for
AI-generated image detection contain biases related to JPEG compression and
image size. Using the GenImage dataset, we demonstrate that detectors indeed
learn from these undesired factors. Furthermore, we show that removing the
named biases substantially increases robustness to JPEG compression and
significantly alters the cross-generator performance of evaluated detectors.
Specifically, it leads to more than 11 percentage points increase in
cross-generator performance for ResNet50 and Swin-T detectors on the GenImage
dataset, achieving state-of-the-art results.
  We provide the dataset and source codes of this paper on the anonymous
website: https://www.unbiased-genimage.org
\\ ( https://arxiv.org/abs/2403.17608 ,  28601kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17634 (*cross-listing*)
Date: Tue, 26 Mar 2024 12:08:58 GMT   (4795kb,D)

Title: Retentive Decision Transformer with Adaptive Masking for Reinforcement
  Learning based Recommendation Systems
Authors: Siyu Wang and Xiaocong Chen and Lina Yao
Categories: cs.IR cs.LG
\\
  Reinforcement Learning-based Recommender Systems (RLRS) have shown promise
across a spectrum of applications, from e-commerce platforms to streaming
services. Yet, they grapple with challenges, notably in crafting reward
functions and harnessing large pre-existing datasets within the RL framework.
Recent advancements in offline RLRS provide a solution for how to address these
two challenges. However, existing methods mainly rely on the transformer
architecture, which, as sequence lengths increase, can introduce challenges
associated with computational resources and training costs. Additionally, the
prevalent methods employ fixed-length input trajectories, restricting their
capacity to capture evolving user preferences. In this study, we introduce a
new offline RLRS method to deal with the above problems. We reinterpret the
RLRS challenge by modeling sequential decision-making as an inference task,
leveraging adaptive masking configurations. This adaptive approach selectively
masks input tokens, transforming the recommendation task into an inference
challenge based on varying token subsets, thereby enhancing the agent's ability
to infer across diverse trajectory lengths. Furthermore, we incorporate a
multi-scale segmented retention mechanism that facilitates efficient modeling
of long sequences, significantly enhancing computational efficiency. Our
experimental analysis, conducted on both online simulator and offline datasets,
clearly demonstrates the advantages of our proposed method.
\\ ( https://arxiv.org/abs/2403.17634 ,  4795kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17692 (*cross-listing*)
Date: Tue, 26 Mar 2024 13:33:16 GMT   (2920kb,D)

Title: Manifold-Guided Lyapunov Control with Diffusion Models
Authors: Amartya Mukherjee, Thanin Quartz, Jun Liu
Categories: cs.CV cs.LG math.DG math.OC stat.CO
Comments: 14 pages
\\
  This paper presents a novel approach to generating stabilizing controllers
for a large class of dynamical systems using diffusion models. The core
objective is to develop stabilizing control functions by identifying the
closest asymptotically stable vector field relative to a predetermined manifold
and adjusting the control function based on this finding. To achieve this, we
employ a diffusion model trained on pairs consisting of asymptotically stable
vector fields and their corresponding Lyapunov functions. Our numerical results
demonstrate that this pre-trained model can achieve stabilization over
previously unseen systems efficiently and rapidly, showcasing the potential of
our approach in fast zero-shot control and generalizability.
\\ ( https://arxiv.org/abs/2403.17692 ,  2920kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17695 (*cross-listing*)
Date: Tue, 26 Mar 2024 13:35:10 GMT   (1625kb,D)

Title: PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition
Authors: Chenhongyi Yang, Zehui Chen, Miguel Espinosa, Linus Ericsson, Zhenyu
  Wang, Jiaming Liu, and Elliot J. Crowley
Categories: cs.CV cs.LG
\\
  We present PlainMamba: a simple non-hierarchical state space model (SSM)
designed for general visual recognition. The recent Mamba model has shown how
SSMs can be highly competitive with other architectures on sequential data and
initial attempts have been made to apply it to images. In this paper, we
further adapt the selective scanning process of Mamba to the visual domain,
enhancing its ability to learn features from two-dimensional images by (i) a
continuous 2D scanning process that improves spatial continuity by ensuring
adjacency of tokens in the scanning sequence, and (ii) direction-aware updating
which enables the model to discern the spatial relations of tokens by encoding
directional information. Our architecture is designed to be easy to use and
easy to scale, formed by stacking identical PlainMamba blocks, resulting in a
model with constant width throughout all layers. The architecture is further
simplified by removing the need for special tokens. We evaluate PlainMamba on a
variety of visual recognition tasks including image classification, semantic
segmentation, object detection, and instance segmentation. Our method achieves
performance gains over previous non-hierarchical models and is competitive with
hierarchical alternatives. For tasks requiring high-resolution inputs, in
particular, PlainMamba requires much less computing while maintaining high
performance. Code and models are available at
https://github.com/ChenhongyiYang/PlainMamba
\\ ( https://arxiv.org/abs/2403.17695 ,  1625kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17701 (*cross-listing*)
Date: Tue, 26 Mar 2024 13:40:18 GMT   (980kb,D)

Title: Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical
  Image Segmentation
Authors: Hao Tang, Lianglun Cheng, Guoheng Huang, Zhengguang Tan, Junhao Lu and
  Kaihong Wu
Categories: eess.IV cs.CV cs.LG
\\
  Image segmentation holds a vital position in the realms of diagnosis and
treatment within the medical domain. Traditional convolutional neural networks
(CNNs) and Transformer models have made significant advancements in this realm,
but they still encounter challenges because of limited receptive field or high
computing complexity. Recently, State Space Models (SSMs), particularly Mamba
and its variants, have demonstrated notable performance in the field of vision.
However, their feature extraction methods may not be sufficiently effective and
retain some redundant structures, leaving room for parameter reduction.
Motivated by previous spatial and channel attention methods, we propose Triplet
Mamba-UNet. The method leverages residual VSS Blocks to extract intensive
contextual features, while Triplet SSM is employed to fuse features across
spatial and channel dimensions. We conducted experiments on ISIC17, ISIC18,
CVC-300, CVC-ClinicDB, Kvasir-SEG, CVC-ColonDB, and Kvasir-Instrument datasets,
demonstrating the superior segmentation performance of our proposed TM-UNet.
Additionally, compared to the previous VM-UNet, our model achieves a one-third
reduction in parameters.
\\ ( https://arxiv.org/abs/2403.17701 ,  980kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17729 (*cross-listing*)
Date: Tue, 26 Mar 2024 14:18:43 GMT   (6018kb,D)

Title: EulerFormer: Sequential User Behavior Modeling with Complex Vector
  Attention
Authors: Zhen Tian, Wayne Xin Zhao, Changwang Zhang, Xin Zhao, Zhongrui Ma and
  Ji-Rong Wen
Categories: cs.IR cs.LG
Comments: Accepted for publication in SIGIR'24
\\
  To capture user preference, transformer models have been widely applied to
model sequential user behavior data. The core of transformer architecture lies
in the self-attention mechanism, which computes the pairwise attention scores
in a sequence. Due to the permutation-equivariant nature, positional encoding
is used to enhance the attention between token representations. In this
setting, the pairwise attention scores can be derived by both semantic
difference and positional difference. However, prior studies often model the
two kinds of difference measurements in different ways, which potentially
limits the expressive capacity of sequence modeling. To address this issue,
this paper proposes a novel transformer variant with complex vector attention,
named EulerFormer, which provides a unified theoretical framework to formulate
both semantic difference and positional difference. The EulerFormer involves
two key technical improvements. First, it employs a new transformation function
for efficiently transforming the sequence tokens into polar-form complex
vectors using Euler's formula, enabling the unified modeling of both semantic
and positional information in a complex rotation form.Secondly, it develops a
differential rotation mechanism, where the semantic rotation angles can be
controlled by an adaptation function, enabling the adaptive integration of the
semantic and positional information according to the semantic
contexts.Furthermore, a phase contrastive learning task is proposed to improve
the anisotropy of contextual representations in EulerFormer. Our theoretical
framework possesses a high degree of completeness and generality. It is more
robust to semantic variations and possesses moresuperior theoretical properties
in principle. Extensive experiments conducted on four public datasets
demonstrate the effectiveness and efficiency of our approach.
\\ ( https://arxiv.org/abs/2403.17729 ,  6018kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17757 (*cross-listing*)
Date: Tue, 26 Mar 2024 14:49:22 GMT   (3683kb,D)

Title: Noise2Noise Denoising of CRISM Hyperspectral Data
Authors: Robert Platt, Rossella Arcucci, C\'edric John
Categories: cs.CV cs.LG
Comments: 5 pages, 3 figures. Accepted as a conference paper at the ICLR 2024
  ML4RS Workshop
\\
  Hyperspectral data acquired by the Compact Reconnaissance Imaging
Spectrometer for Mars (CRISM) have allowed for unparalleled mapping of the
surface mineralogy of Mars. Due to sensor degradation over time, a significant
portion of the recently acquired data is considered unusable. Here a new
data-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to
remove noise from CRISM images. Our model is self-supervised and does not
require zero-noise target data, making it well suited for use in Planetary
Science applications where high quality labelled data is scarce. We demonstrate
its strong performance on synthetic-noise data and CRISM images, and its impact
on downstream classification performance, outperforming benchmark methods on
most metrics. This allows for detailed analysis for critical sites of interest
on the Martian surface, including proposed lander sites.
\\ ( https://arxiv.org/abs/2403.17757 ,  3683kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17767 (*cross-listing*)
Date: Tue, 26 Mar 2024 14:54:35 GMT   (108kb)

Title: Asymptotic Bayes risk of semi-supervised learning with uncertain
  labeling
Authors: Victor Leger and Romain Couillet
Categories: stat.ML cs.LG
\\
  This article considers a semi-supervised classification setting on a Gaussian
mixture model, where the data is not labeled strictly as usual, but instead
with uncertain labels. Our main aim is to compute the Bayes risk for this
model. We compare the behavior of the Bayes risk and the best known algorithm
for this model. This comparison eventually gives new insights over the
algorithm.
\\ ( https://arxiv.org/abs/2403.17767 ,  108kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17805 (*cross-listing*)
Date: Tue, 26 Mar 2024 15:42:04 GMT   (17781kb,D)

Title: Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving
Authors: Axel Brunnbauer, Luigi Berducci, Peter Priller, Dejan Nickovic, Radu
  Grosu
Categories: cs.RO cs.LG cs.MA
Comments: 7 Pages, Under Review
\\
  The automated generation of diverse and complex training scenarios has been
an important ingredient in many complex learning tasks. Especially in
real-world application domains, such as autonomous driving, auto-curriculum
generation is considered vital for obtaining robust and general policies.
However, crafting traffic scenarios with multiple, heterogeneous agents is
typically considered as a tedious and time-consuming task, especially in more
complex simulation environments. In our work, we introduce MATS-Gym, a
Multi-Agent Traffic Scenario framework to train agents in CARLA, a
high-fidelity driving simulator. MATS-Gym is a multi-agent training framework
for autonomous driving that uses partial scenario specifications to generate
traffic scenarios with variable numbers of agents. This paper unifies various
existing approaches to traffic scenario description into a single training
framework and demonstrates how it can be integrated with techniques from
unsupervised environment design to automate the generation of adaptive
auto-curricula. The code is available at
https://github.com/AutonomousDrivingExaminer/mats-gym.
\\ ( https://arxiv.org/abs/2403.17805 ,  17781kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17808 (*cross-listing*)
Date: Tue, 26 Mar 2024 15:45:29 GMT   (2200kb)

Title: Annotated Biomedical Video Generation using Denoising Diffusion
  Probabilistic Models and Flow Fields
Authors: R\"uveyda Yilmaz, Dennis Eschweiler, Johannes Stegmaier
Categories: eess.IV cs.CV cs.LG
\\
  The segmentation and tracking of living cells play a vital role within the
biomedical domain, particularly in cancer research, drug development, and
developmental biology. These are usually tedious and time-consuming tasks that
are traditionally done by biomedical experts. Recently, to automatize these
processes, deep learning based segmentation and tracking methods have been
proposed. These methods require large-scale datasets and their full potential
is constrained by the scarcity of annotated data in the biomedical imaging
domain. To address this limitation, we propose Biomedical Video Diffusion Model
(BVDM), capable of generating realistic-looking synthetic microscopy videos.
Trained only on a single real video, BVDM can generate videos of arbitrary
length with pixel-level annotations that can be used for training data-hungry
models. It is composed of a denoising diffusion probabilistic model (DDPM)
generating high-fidelity synthetic cell microscopy images and a flow prediction
model (FPM) predicting the non-rigid transformation between consecutive video
frames. During inference, initially, the DDPM imposes realistic cell textures
on synthetic cell masks which are generated based on real data statistics. The
flow prediction model predicts the flow field between consecutive masks and
applies that to the DDPM output from the previous time frame to create the next
one while keeping temporal consistency. BVDM outperforms state-of-the-art
synthetic live cell microscopy video generation models. Furthermore, we
demonstrate that a sufficiently large synthetic dataset enhances the
performance of cell segmentation and tracking models compared to using a
limited amount of available real data.
\\ ( https://arxiv.org/abs/2403.17808 ,  2200kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17837 (*cross-listing*)
Date: Tue, 26 Mar 2024 16:24:42 GMT   (28728kb,D)

Title: GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction
Authors: Hrishav Bakul Barua, Kalin Stefanov, KokSheik Wong, Abhinav Dhall,
  Ganesh Krishnasamy
Categories: cs.CV cs.GR cs.LG cs.MM eess.IV
Comments: Submitted to IEEE
MSC-class: Artificial intelligence, Computer vision, Machine learning, Deep
  learning
ACM-class: I.3.3; I.4.5
\\
  High Dynamic Range (HDR) content (i.e., images and videos) has a broad range
of applications. However, capturing HDR content from real-world scenes is
expensive and time- consuming. Therefore, the challenging task of
reconstructing visually accurate HDR images from their Low Dynamic Range (LDR)
counterparts is gaining attention in the vision research community. A major
challenge in this research problem is the lack of datasets, which capture
diverse scene conditions (e.g., lighting, shadows, weather, locations,
landscapes, objects, humans, buildings) and various image features (e.g.,
color, contrast, saturation, hue, luminance, brightness, radiance). To address
this gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset
of photo-realistic HDR images sampled from the GTA-V video game. We perform
thorough evaluation of the proposed dataset, which demonstrates significant
qualitative and quantitative improvements of the state-of-the-art HDR image
reconstruction methods. Furthermore, we demonstrate the effectiveness of the
proposed dataset and its impact on additional computer vision tasks including
3D human pose estimation, human body part segmentation, and holistic scene
segmentation. The dataset, data collection pipeline, and evaluation code are
available at: https://github.com/HrishavBakulBarua/GTA-HDR.
\\ ( https://arxiv.org/abs/2403.17837 ,  28728kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17868 (*cross-listing*)
Date: Tue, 26 Mar 2024 16:57:01 GMT   (164kb)

Title: Sample complexity of quantum hypothesis testing
Authors: Hao-Chung Cheng, Nilanjana Datta, Nana Liu, Theshani Nuradha, Robert
  Salzmann, and Mark M. Wilde
Categories: quant-ph cs.IT cs.LG math.IT math.ST stat.TH
Comments: 38 pages, 1 figure, preliminary version; see independent and
  concurrent work of Pensia, Jog, Loh at arXiv:2403.16981
\\
  Quantum hypothesis testing has been traditionally studied from the
information-theoretic perspective, wherein one is interested in the optimal
decay rate of error probabilities as a function of the number of samples of an
unknown state. In this paper, we study the sample complexity of quantum
hypothesis testing, wherein the goal is to determine the minimum number of
samples needed to reach a desired error probability. By making use of the
wealth of knowledge that already exists in the literature on quantum hypothesis
testing, we characterize the sample complexity of binary quantum hypothesis
testing in the symmetric and asymmetric settings, and we provide bounds on the
sample complexity of multiple quantum hypothesis testing. In more detail, we
prove that the sample complexity of symmetric binary quantum hypothesis testing
depends logarithmically on the inverse error probability and inversely on the
negative logarithm of the fidelity. As a counterpart of the quantum Stein's
lemma, we also find that the sample complexity of asymmetric binary quantum
hypothesis testing depends logarithmically on the inverse type~II error
probability and inversely on the quantum relative entropy. Finally, we provide
lower and upper bounds on the sample complexity of multiple quantum hypothesis
testing, with it remaining an intriguing open question to improve these bounds.
\\ ( https://arxiv.org/abs/2403.17868 ,  164kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17889 (*cross-listing*)
Date: Tue, 26 Mar 2024 17:21:54 GMT   (698kb,D)

Title: Large scale paired antibody language models
Authors: Henry Kenlay, Fr\'ed\'eric A. Dreyer, Aleksandr Kovaltsuk, Dom Miketa,
  Douglas Pires, Charlotte M. Deane
Categories: q-bio.BM cs.LG
Comments: 14 pages, 2 figures, 6 tables, model weights available at
  https://zenodo.org/doi/10.5281/zenodo.10876908
\\
  Antibodies are proteins produced by the immune system that can identify and
neutralise a wide variety of antigens with high specificity and affinity, and
constitute the most successful class of biotherapeutics. With the advent of
next-generation sequencing, billions of antibody sequences have been collected
in recent years, though their application in the design of better therapeutics
has been constrained by the sheer volume and complexity of the data. To address
this challenge, we present IgBert and IgT5, the best performing
antibody-specific language models developed to date which can consistently
handle both paired and unpaired variable region sequences as input. These
models are trained comprehensively using the more than two billion unpaired
sequences and two million paired sequences of light and heavy chains present in
the Observed Antibody Space dataset. We show that our models outperform
existing antibody and protein language models on a diverse range of design and
regression tasks relevant to antibody engineering. This advancement marks a
significant leap forward in leveraging machine learning, large scale data sets
and high-performance computing for enhancing antibody design for therapeutic
development.
\\ ( https://arxiv.org/abs/2403.17889 ,  698kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17902 (*cross-listing*)
Date: Tue, 26 Mar 2024 17:43:15 GMT   (4914kb,D)

Title: Serpent: Scalable and Efficient Image Restoration via Multi-scale
  Structured State Space Models
Authors: Mohammad Shahab Sepehri, Zalan Fabian, Mahdi Soltanolkotabi
Categories: eess.IV cs.CV cs.LG
Comments: 7 pages, 5 figures, preliminary workshop submission of a
  comprehensive work to be released soon
ACM-class: I.4.4; I.4.5
\\
  The landscape of computational building blocks of efficient image restoration
architectures is dominated by a combination of convolutional processing and
various attention mechanisms. However, convolutional filters are inherently
local and therefore struggle at modeling long-range dependencies in images. On
the other hand, attention excels at capturing global interactions between
arbitrary image regions, however at a quadratic cost in image dimension. In
this work, we propose Serpent, an architecture that leverages recent advances
in state space models (SSMs) in its core computational block. SSMs, originally
introduced for sequence modeling, can maintain a global receptive field with a
favorable linear scaling in input size. Our preliminary results demonstrate
that Serpent can achieve reconstruction quality on par with state-of-the-art
techniques, while requiring orders of magnitude less compute (up to $150$ fold
reduction in FLOPS) and a factor of up to $5\times$ less GPU memory while
maintaining a compact model size.
\\ ( https://arxiv.org/abs/2403.17902 ,  4914kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17905 (*cross-listing*)
Date: Tue, 26 Mar 2024 17:45:06 GMT   (5114kb,D)

Title: Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2
Authors: Chen Yiwei, Tang Chao, Aghabiglou Amir, Chu Chung San and Wiaux Yves
Categories: eess.IV cs.CV cs.LG eess.SP
Comments: submitted to IEEE EUSIPCO 2024
\\
  We propose a new approach for non-Cartesian magnetic resonance image
reconstruction. While unrolled architectures provide robustness via
data-consistency layers, embedding measurement operators in Deep Neural Network
(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)
approaches, where the denoising DNNs are blind to the measurement setting, are
not affected by this limitation and have also proven effective, but their
highly iterative nature also affects scalability. To address this scalability
challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic
range imaging (R2D2)" approach recently introduced in astronomical imaging.
R2D2's reconstruction is formed as a series of residual images, iteratively
estimated as outputs of DNNs taking the previous iteration's image estimate and
associated data residual as inputs. The method can be interpreted as a learned
version of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,
considering radial k-space sampling acquisition sequences. Our preliminary
results suggest that R2D2 achieves: (i) suboptimal performance compared to its
unrolled incarnation R2D2-Net, which is however non-scalable due to the
necessary embedding of NUFFT-based data-consistency layers; (ii) superior
reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based
approximation for data consistency; (iii) superior reconstruction quality to
PnP, while only requiring few iterations.
\\ ( https://arxiv.org/abs/2403.17905 ,  5114kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2307.05300
replaced with revised version Tue, 26 Mar 2024 14:32:33 GMT   (7039kb,D)

Title: Unleashing the Emergent Cognitive Synergy in Large Language Models: A
  Task-Solving Agent through Multi-Persona Self-Collaboration
Authors: Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji
Categories: cs.AI cs.CL
Comments: Accepted as a main conference paper at NAACL 2024
\\ ( https://arxiv.org/abs/2307.05300 ,  7039kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02099
replaced with revised version Tue, 26 Mar 2024 14:25:52 GMT   (2126kb,D)

Title: A Safe Preference Learning Approach for Personalization with
  Applications to Autonomous Vehicles
Authors: Ruya Karagulle and Nikos Arechiga and Andrew Best and Jonathan
  DeCastro and Necmiye Ozay
Categories: cs.AI cs.SY eess.SY
Comments: 9 pages, 3 figures, 2 tables. This work has been published at IEEE
  Robotics and Automation Letters. Copyright may be transferred without notice,
  after which this version may no longer be accessible
\\ ( https://arxiv.org/abs/2311.02099 ,  2126kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08644
replaced with revised version Tue, 26 Mar 2024 11:13:56 GMT   (647kb,D)

Title: Tandem Transformers for Inference Efficient LLMs
Authors: Aishwarya P S and Pranav Ajit Nair and Yashas Samaga and Toby Boyd and
  Sanjiv Kumar and Prateek Jain and Praneeth Netrapalli
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.08644 ,  647kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05000
replaced with revised version Tue, 26 Mar 2024 01:51:37 GMT   (1402kb,D)

Title: Medical Speech Symptoms Classification via Disentangled Representation
Authors: Jianzong Wang, Pengcheng Li, Xulong Zhang, Ning Cheng, Jing Xiao
Categories: cs.AI
Comments: Accepted by the 27th International Conference on Computer Supported
  Cooperative Work in Design (CSCWD 2024)
\\ ( https://arxiv.org/abs/2403.05000 ,  1402kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12151
replaced with revised version Mon, 25 Mar 2024 18:50:06 GMT   (2805kb,D)

Title: Fusing Domain-Specific Content from Large Language Models into Knowledge
  Graphs for Enhanced Zero Shot Object State Classification
Authors: Filippos Gouidis, Katerina Papantoniou, Konstantinos Papoutsakis
  Theodore Patkos, Antonis Argyros and Dimitris Plexousakis
Categories: cs.AI cs.CL cs.CV cs.LG
Comments: Accepted at the AAAI-MAKE 24
\\ ( https://arxiv.org/abs/2403.12151 ,  2805kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13518
replaced with revised version Tue, 26 Mar 2024 11:16:47 GMT   (3151kb,D)

Title: Motion Generation from Fine-grained Textual Descriptions
Authors: Kunhang Li and Yansong Feng
Categories: cs.AI cs.CL cs.CV cs.RO
\\ ( https://arxiv.org/abs/2403.13518 ,  3151kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14077
replaced with revised version Tue, 26 Mar 2024 16:02:36 GMT   (34762kb,D)

Title: Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language
  Models for Media Forensics
Authors: Shan Jia, Reilin Lyu, Kangran Zhao, Yize Chen, Zhiyuan Yan, Yan Ju,
  Chuanbo Hu, Xin Li, Baoyuan Wu, Siwei Lyu
Categories: cs.AI cs.CR
\\ ( https://arxiv.org/abs/2403.14077 ,  34762kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16206
replaced with revised version Tue, 26 Mar 2024 04:23:23 GMT   (836kb)

Title: Rumor Detection with a novel graph neural network approach
Authors: Tianrui Liu, Qi Cai, Changxin Xu, Bo Hong, Fanghao Ni, Yuxin Qiao, and
  Tsungwei Yang
Categories: cs.AI
Comments: 10 pages, 5 figures
\\ ( https://arxiv.org/abs/2403.16206 ,  836kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16222
replaced with revised version Tue, 26 Mar 2024 15:28:27 GMT   (31223kb,D)

Title: Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative
  Matrix Factorization
Authors: Ryan Barron, Maksim E. Eren, Manish Bhattarai, Selma Wanna, Nicholas
  Solovyev, Kim Rasmussen, Boian S. Alexandrov, Charles Nicholas, Cynthia
  Matuszek
Categories: cs.AI
Comments: Accepted at IEEE ISDFS
\\ ( https://arxiv.org/abs/2403.16222 ,  31223kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16427
replaced with revised version Tue, 26 Mar 2024 07:21:01 GMT   (8007kb,D)

Title: Re2LLM: Reflective Reinforcement Large Language Model for Session-based
  Recommendation
Authors: Ziyan Wang, Yingpeng Du, Zhu Sun, Haoyan Chua, Kaidong Feng, Wenya
  Wang, Jie Zhang
Categories: cs.AI
Comments: 11 pages, 4 figures
\\ ( https://arxiv.org/abs/2403.16427 ,  8007kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16649
replaced with revised version Tue, 26 Mar 2024 06:08:20 GMT   (2393kb,D)

Title: CLHA: A Simple yet Effective Contrastive Learning Framework for Human
  Alignment
Authors: Feiteng Fang, Liang Zhu, Min Yang, Xi Feng, Jinchang Hou, Qixuan Zhao,
  Chengming Li, Xiping Hu and Ruifeng Xu
Categories: cs.AI
\\ ( https://arxiv.org/abs/2403.16649 ,  2393kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16808
replaced with revised version Tue, 26 Mar 2024 08:59:17 GMT   (216kb,D)

Title: Navigating the EU AI Act: A Methodological Approach to Compliance for
  Safety-critical Products
Authors: J. Kelly, S. Zafar, L. Heidemann, J. Zacchi, D. Espinoza, N. Mata
Categories: cs.AI
Comments: To be published in: 2024 IEEE Conference on Artificial Intelligence
  (CAI 2024)
\\ ( https://arxiv.org/abs/2403.16808 ,  216kb)
------------------------------------------------------------------------------
\\
arXiv:2112.06166
replaced with revised version Tue, 26 Mar 2024 04:26:18 GMT   (1829kb,D)

Title: Topic Detection and Tracking with Time-Aware Document Embeddings
Authors: Hang Jiang, Doug Beeferman, Weiquan Mao, Deb Roy
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\ ( https://arxiv.org/abs/2112.06166 ,  1829kb)
------------------------------------------------------------------------------
\\
arXiv:2303.08014
replaced with revised version Tue, 26 Mar 2024 01:46:50 GMT   (1059kb)

Title: Do large language models resemble humans in language use?
Authors: Zhenguang G. Cai, Xufeng Duan, David A. Haslett, Shuqi Wang, Martin J.
  Pickering
Categories: cs.CL
\\ ( https://arxiv.org/abs/2303.08014 ,  1059kb)
------------------------------------------------------------------------------
\\
arXiv:2304.02541
replaced with revised version Tue, 26 Mar 2024 10:13:06 GMT   (672kb,D)

Title: PWESuite: Phonetic Word Embeddings and Tasks They Facilitate
Authors: Vil\'em Zouhar, Kalvin Chang, Chenxuan Cui, Nathaniel Carlson,
  Nathaniel Robinson, Mrinmaya Sachan, David Mortensen
Categories: cs.CL
Comments: LREC-COLING 2024
\\ ( https://arxiv.org/abs/2304.02541 ,  672kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14230
replaced with revised version Tue, 26 Mar 2024 13:16:37 GMT   (203kb,D)

Title: Exploring Representational Disparities Between Multilingual and
  Bilingual Translation Models
Authors: Neha Verma, Kenton Murray, Kevin Duh
Categories: cs.CL
Comments: LREC-COLING 2024
\\ ( https://arxiv.org/abs/2305.14230 ,  203kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14790
replaced with revised version Tue, 26 Mar 2024 11:29:21 GMT   (7499kb,D)

Title: Advancing Topic Segmentation and Outline Generation in Chinese Texts:
  The Paragraph-level Topic Representation, Corpus, and Benchmark
Authors: Feng Jiang, Weihao Liu, Xiaomin Chu, Peifeng Li, Qiaoming Zhu, Haizhou
  Li
Categories: cs.CL cs.AI
Comments: Accepted by LREC-COLING 2024
\\ ( https://arxiv.org/abs/2305.14790 ,  7499kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16031
replaced with revised version Tue, 26 Mar 2024 09:16:36 GMT   (7182kb,D)

Title: Efficient Document Embeddings via Self-Contrastive Bregman Divergence
  Learning
Authors: Daniel Saggau, Mina Rezaei, Bernd Bischl, Ilias Chalkidis
Categories: cs.CL
Comments: 5 pages, short paper at Findings of ACL 2023
\\ ( https://arxiv.org/abs/2305.16031 ,  7182kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07645
replaced with revised version Mon, 25 Mar 2024 18:52:34 GMT   (743kb,D)

Title: Othering and low status framing of immigrant cuisines in US restaurant
  reviews and large language models
Authors: Yiwei Luo, Kristina Gligori\'c, Dan Jurafsky
Categories: cs.CL cs.AI
Comments: ICWSM '24
\\ ( https://arxiv.org/abs/2307.07645 ,  743kb)
------------------------------------------------------------------------------
\\
arXiv:2307.11278
replaced with revised version Tue, 26 Mar 2024 16:40:50 GMT   (680kb,D)

Title: Generator-Retriever-Generator Approach for Open-Domain Question
  Answering
Authors: Abdelrahman Abdallah, Adam Jatowt
Categories: cs.CL
\\ ( https://arxiv.org/abs/2307.11278 ,  680kb)
------------------------------------------------------------------------------
\\
arXiv:2308.03742
replaced with revised version Tue, 26 Mar 2024 16:03:57 GMT   (1792kb,D)

Title: Training BERT Models to Carry Over a Coding System Developed on One
  Corpus to Another
Authors: Dalma Galambos and P\'al Zs\'amboki
Categories: cs.CL
Comments: Camera-ready version, to be presented at the 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
  (LREC-COLING 2024)
\\ ( https://arxiv.org/abs/2308.03742 ,  1792kb)
------------------------------------------------------------------------------
\\
arXiv:2308.06463
replaced with revised version Tue, 26 Mar 2024 04:23:12 GMT   (5962kb,D)

Title: GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher
Authors: Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He,
  Shuming Shi and Zhaopeng Tu
Categories: cs.CL
Comments: Accepted by ICLR 2024. 21 pages, 3 figures, 13 tables
\\ ( https://arxiv.org/abs/2308.06463 ,  5962kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10592
replaced with revised version Tue, 26 Mar 2024 12:31:35 GMT   (4324kb,D)

Title: BAN-PL: a Novel Polish Dataset of Banned Harmful and Offensive Content
  from Wykop.pl web service
Authors: Anna Ko{\l}os and Inez Okulska, Kinga G{\l}\k{a}bi\'nska, Agnieszka
  Karli\'nska, Emilia Wi\'snios, Pawe{\l} Ellerik, Andrzej Pra{\l}at
Categories: cs.CL
Comments: Accepted for LREC-COLING 2024 Conference
\\ ( https://arxiv.org/abs/2308.10592 ,  4324kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06578
replaced with revised version Tue, 26 Mar 2024 03:33:45 GMT   (2011kb,D)

Title: Can Large Language Models Discern Evidence for Scientific Hypotheses?
  Case Studies in the Social Sciences
Authors: Sai Koneru, Jian Wu, Sarah Rajtmajer
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2309.06578 ,  2011kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09800
replaced with revised version Tue, 26 Mar 2024 16:05:51 GMT   (1904kb,D)

Title: AMuRD: Annotated Arabic-English Receipt Dataset for Key Information
  Extraction and Classification
Authors: Abdelrahman Abdallah, Mahmoud Abdalla, Mohamed Elkasaby, Yasser
  Elbendary, Adam Jatowt
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.09800 ,  1904kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11888
replaced with revised version Tue, 26 Mar 2024 08:04:36 GMT   (80kb,D)

Title: High-order Joint Constituency and Dependency Parsing
Authors: Yanggan Gu, Yang Hou, Zhefeng Wang, Xinyu Duan, Zhenghua Li
Categories: cs.CL
Comments: LREC-COLING 2024
\\ ( https://arxiv.org/abs/2309.11888 ,  80kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13318
replaced with revised version Tue, 26 Mar 2024 11:26:04 GMT   (184kb,D)

Title: Spanish Resource Grammar version 2023
Authors: Olga Zamaraeva, Lorena S. Allegue, Carlos G\'omez-Rodr\'iguez
Categories: cs.CL
Comments: 12 pages, 5 figures
MSC-class: 03B65
\\ ( https://arxiv.org/abs/2309.13318 ,  184kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13339
replaced with revised version Tue, 26 Mar 2024 01:53:30 GMT   (312kb,D)

Title: Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models
  through Logic
Authors: Xufeng Zhao, Mengdi Li, Wenhao Lu, Cornelius Weber, Jae Hee Lee, Kun
  Chu, Stefan Wermter
Categories: cs.CL cs.AI cs.LG cs.SC
Comments: Accepted in COLING 2024. Code see https://github.com/xf-zhao/LoT
\\ ( https://arxiv.org/abs/2309.13339 ,  312kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14974
replaced with revised version Tue, 26 Mar 2024 08:46:07 GMT   (319kb,D)

Title: Detecting Sexual Content at the Sentence Level in First Millennium Latin
  Texts
Authors: Thibault Cl\'erice (ALMAnaCH, CJM)
Categories: cs.CL cs.AI
Journal-ref: Joint International Conference on Computational Linguistics,
  Language Resources and Evaluation (LREC-COLING 2024), ELRA Language Resources
  Association (ELRA); International Committee on Computational Linguistics
  (ICCL), May 2024, Torino, Italy
\\ ( https://arxiv.org/abs/2309.14974 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02129
replaced with revised version Tue, 26 Mar 2024 14:38:23 GMT   (447kb,D)

Title: Unveiling the Pitfalls of Knowledge Editing for Large Language Models
Authors: Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen
Categories: cs.CL cs.AI cs.CV cs.DB cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.02129 ,  447kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13257
replaced with revised version Mon, 25 Mar 2024 18:48:40 GMT   (17112kb,D)

Title: Visual Grounding Helps Learn Word Meanings in Low-Data Regimes
Authors: Chengxu Zhuang, Evelina Fedorenko, Jacob Andreas
Categories: cs.CL cs.AI
Comments: Accepted by NAACL 2024
\\ ( https://arxiv.org/abs/2310.13257 ,  17112kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16937
replaced with revised version Mon, 25 Mar 2024 20:14:07 GMT   (16799kb,D)

Title: Learning Transfers over Several Programming Languages
Authors: Razan Baltaji, Saurabh Pujar, Louis Mandel, Martin Hirzel, Luca
  Buratti, Lav Varshney
Categories: cs.CL
Comments: 15 pages, 9 figures, 8 tables
ACM-class: I.2.7; I.2.5
\\ ( https://arxiv.org/abs/2310.16937 ,  16799kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05020
replaced with revised version Tue, 26 Mar 2024 02:23:27 GMT   (821kb,D)

Title: First Tragedy, then Parse: History Repeats Itself in the New Era of
  Large Language Models
Authors: Naomi Saphra, Eve Fleisig, Kyunghyun Cho, Adam Lopez
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.05020 ,  821kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07703
replaced with revised version Tue, 26 Mar 2024 15:31:34 GMT   (246kb,D)

Title: Measuring Entrainment in Spontaneous Code-switched Speech
Authors: Debasmita Bhattacharya and Siying Ding and Alayna Nguyen and Julia
  Hirschberg
Categories: cs.CL cs.SD eess.AS
Comments: Edits: camera-ready manuscript for NAACL 2024
\\ ( https://arxiv.org/abs/2311.07703 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09602
replaced with revised version Mon, 25 Mar 2024 20:42:57 GMT   (7228kb,D)

Title: Language Models (Mostly) Do Not Consider Emotion Triggers When
  Predicting Emotion
Authors: Smriti Singh, Cornelia Caragea, Junyi Jessy Li
Categories: cs.CL
Comments: NAACL 2024 Camera Ready
\\ ( https://arxiv.org/abs/2311.09602 ,  7228kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08274
replaced with revised version Tue, 26 Mar 2024 10:36:31 GMT   (1105kb)

Title: High-throughput Biomedical Relation Extraction for Semi-Structured Web
  Articles Empowered by Large Language Models
Authors: Songchi Zhou, Sheng Yu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2312.08274 ,  1105kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06795
replaced with revised version Tue, 26 Mar 2024 16:44:34 GMT   (2532kb,D)

Title: AI and Generative AI for Research Discovery and Summarization
Authors: Mark Glickman and Yi Zhang
Categories: cs.CL cs.AI cs.LG
Comments: 29 pages, 9 figures
\\ ( https://arxiv.org/abs/2401.06795 ,  2532kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11911
replaced with revised version Tue, 26 Mar 2024 15:47:14 GMT   (3185kb,D)

Title: Blinded by Generated Contexts: How Language Models Merge Generated and
  Retrieved Contexts for Open-Domain QA?
Authors: Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, Xueqi Cheng
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.11911 ,  3185kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18040
replaced with revised version Mon, 25 Mar 2024 23:03:58 GMT   (336kb)

Title: Enhancing End-to-End Multi-Task Dialogue Systems: A Study on Intrinsic
  Motivation Reinforcement Learning Algorithms for Improved Training and
  Adaptability
Authors: Navin Kamuni, Hardik Shah, Sathishkumar Chintala, Naveen Kunchakuri,
  Sujatha Alla Old Dominion
Categories: cs.CL cs.AI
Comments: 6 pages, 1 figure, 18th IEEE International Conference on Semantic
  Computing
\\ ( https://arxiv.org/abs/2401.18040 ,  336kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11537
replaced with revised version Tue, 26 Mar 2024 10:45:40 GMT   (1905kb,D)

Title: Deciphering the Impact of Pretraining Data on Large Language Models
  through Machine Unlearning
Authors: Yang Zhao, Li Du, Xiao Ding, Kai Xiong, Zhouhao Sun, Jun Shi, Ting Liu
  and Bing Qin
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.11537 ,  1905kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12243
replaced with revised version Mon, 25 Mar 2024 19:48:16 GMT   (705kb,D)

Title: Understanding the Effects of Noise in Text-to-SQL: An Examination of the
  BIRD-Bench Benchmark
Authors: Niklas Wretblad, Fredrik Gordh Riseby, Rahul Biswas, Amin Ahmadi,
  Oskar Holmstr\"om
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.12243 ,  705kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01748
replaced with revised version Tue, 26 Mar 2024 15:26:21 GMT   (329kb,D)

Title: Decode Neural signal as Speech
Authors: Yiqian Yang, Yiqun Duan, Qiang Zhang, Renjing Xu, Hui Xiong
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.01748 ,  329kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02270
replaced with revised version Tue, 26 Mar 2024 13:14:52 GMT   (8257kb,D)

Title: FENICE: Factuality Evaluation of summarization based on Natural language
  Inference and Claim Extraction
Authors: Alessandro Scir\`e and Karim Ghonim and Roberto Navigli
Categories: cs.CL
Comments: 9 pages, long paper
\\ ( https://arxiv.org/abs/2403.02270 ,  8257kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02472
replaced with revised version Mon, 25 Mar 2024 23:09:58 GMT   (300kb,D)

Title: OffLanDat: A Community Based Implicit Offensive Language Dataset
  Generated by Large Language Model Through Prompt Engineering
Authors: Amit Das, Mostafa Rahgouy, Dongji Feng, Zheng Zhang, Tathagata
  Bhattacharya, Nilanjana Raychawdhary, Mary Sandage, Lauramarie Pope, Gerry
  Dozier and Cheryl Seals
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.02472 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08281
replaced with revised version Tue, 26 Mar 2024 09:29:51 GMT   (7386kb,D)

Title: Mastering Text, Code and Math Simultaneously via Fusing Highly
  Specialized Language Models
Authors: Ning Ding, Yulin Chen, Ganqu Cui, Xingtai Lv, Weilin Zhao, Ruobing
  Xie, Bowen Zhou, Zhiyuan Liu, Maosong Sun
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.08281 ,  7386kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09738
replaced with revised version Mon, 25 Mar 2024 23:53:01 GMT   (1098kb,D)

Title: Evaluating Large Language Models as Generative User Simulators for
  Conversational Recommendation
Authors: Se-eun Yoon, Zhankui He, Jessica Maria Echterhoff, Julian McAuley
Categories: cs.CL cs.AI cs.IR
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2403.09738 ,  1098kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09963
replaced with revised version Tue, 26 Mar 2024 04:08:47 GMT   (292kb,D)

Title: Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias
  in Factual Knowledge Extraction
Authors: Ziyang Xu, Keqin Peng, Liang Ding, Dacheng Tao, Xiliang Lu
Categories: cs.CL cs.AI cs.IR
Comments: Accepted by COLING 2024
\\ ( https://arxiv.org/abs/2403.09963 ,  292kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10949
replaced with revised version Tue, 26 Mar 2024 01:15:09 GMT   (5469kb,D)

Title: SelfIE: Self-Interpretation of Large Language Model Embeddings
Authors: Haozhe Chen, Carl Vondrick, Chengzhi Mao
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.10949 ,  5469kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12721
replaced with revised version Tue, 26 Mar 2024 14:32:34 GMT   (64kb,D)

Title: CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched
  with Linguistic and Genre Annotation
Authors: Nikola Ljube\v{s}i\'c, Taja Kuzman
Categories: cs.CL
Comments: Accepted to the LREC-COLING 2024 conference
\\ ( https://arxiv.org/abs/2403.12721 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13334
replaced with revised version Tue, 26 Mar 2024 12:24:46 GMT   (347kb)

Title: Hyacinth6B: A large language model for Traditional Chinese
Authors: Chih-Wei Song, Yin-Te Tsai
Categories: cs.CL cs.AI
Comments: 14pages
\\ ( https://arxiv.org/abs/2403.13334 ,  347kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13737
replaced with revised version Tue, 26 Mar 2024 13:01:38 GMT   (65kb)

Title: EthioLLM: Multilingual Large Language Models for Ethiopian Languages
  with Task Evaluation
Authors: Atnafu Lambebo Tonja, Israel Abebe Azime, Tadesse Destaw Belay, Mesay
  Gemeda Yigezu, Moges Ahmed Mehamed, Abinew Ali Ayele, Ebrahim Chekol Jibril,
  Michael Melese Woldeyohannis, Olga Kolesnikova, Philipp Slusallek, Dietrich
  Klakow, Shengwu Xiong, Seid Muhie Yimam
Categories: cs.CL
Comments: Accepted at LREC-Coling 2024
\\ ( https://arxiv.org/abs/2403.13737 ,  65kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14438
replaced with revised version Tue, 26 Mar 2024 11:02:32 GMT   (1819kb,D)

Title: A Multimodal Approach to Device-Directed Speech Detection with Large
  Language Models
Authors: Dominik Wagner, Alexander Churchill, Siddharth Sigtia, Panayiotis
  Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi
Categories: cs.CL cs.LG eess.AS
Comments: arXiv admin note: text overlap with arXiv:2312.03632
DOI: 10.1109/ICASSP48485.2024.10446224
\\ ( https://arxiv.org/abs/2403.14438 ,  1819kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15729
replaced with revised version Tue, 26 Mar 2024 02:42:08 GMT   (4267kb,D)

Title: Towards a RAG-based Summarization Agent for the Electron-Ion Collider
Authors: Karthik Suresh, Neeltje Kackar, Luke Schleck, Cristiano Fanelli
Categories: cs.CL cs.AI hep-ex physics.ins-det
Comments: updated title to have no latex formatting
\\ ( https://arxiv.org/abs/2403.15729 ,  4267kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15885
replaced with revised version Tue, 26 Mar 2024 10:26:04 GMT   (800kb,D)

Title: STEntConv: Predicting Disagreement with Stance Detection and a Signed
  Graph Convolutional Network
Authors: Isabelle Lorge, Li Zhang, Xiaowen Dong, Janet B. Pierrehumbert
Categories: cs.CL
Comments: Accepted for the 2024 Joint International Conference on Computational
  Linguistics, Language Resources and Evaluation (LREC-COLING 2024)
\\ ( https://arxiv.org/abs/2403.15885 ,  800kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16248
replaced with revised version Tue, 26 Mar 2024 17:46:26 GMT   (6374kb,D)

Title: Large Language Models Offer an Alternative to the Traditional Approach
  of Topic Modelling
Authors: Yida Mu, Chun Dong, Kalina Bontcheva, Xingyi Song
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2403.16248 ,  6374kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16662
replaced with revised version Tue, 26 Mar 2024 07:13:15 GMT   (4089kb,D)

Title: RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking
  on Russia-Ukraine Conflict
Authors: Yirong Zeng, Xiao Ding, Yi Zhao, Xiangyu Li, Jie Zhang, Chao Yao, Ting
  Liu and Bing Qin
Categories: cs.CL
Comments: 12 pages, 3 figures, accepted by lrec-coling2024
\\ ( https://arxiv.org/abs/2403.16662 ,  4089kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16950
replaced with revised version Tue, 26 Mar 2024 02:28:42 GMT   (3373kb,D)

Title: Aligning with Human Judgement: The Role of Pairwise Preference in Large
  Language Model Evaluators
Authors: Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vuli\'c,
  Anna Korhonen and Nigel Collier
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.16950 ,  3373kb)
------------------------------------------------------------------------------
\\
arXiv:2106.04088
replaced with revised version Tue, 26 Mar 2024 06:47:13 GMT   (2326kb,D)

Title: A Lightweight and Gradient-Stable Neural Layer
Authors: Yueyao Yu and Yin Zhang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2106.04088 ,  2326kb)
------------------------------------------------------------------------------
\\
arXiv:2106.13329
replaced with revised version Mon, 25 Mar 2024 21:11:44 GMT   (63kb)

Title: Covariance-Aware Private Mean Estimation Without Private Covariance
  Estimation
Authors: Gavin Brown, Marco Gaboardi, Adam Smith, Jonathan Ullman, Lydia
  Zakynthinou
Categories: cs.LG
Comments: 49 pages. Appeared in NeurIPS 2021. Updated version contains improved
  analysis of Tukey depth mechanism: robustness guarantees, tighter error
  analysis, and techniques for faster implementation
\\ ( https://arxiv.org/abs/2106.13329 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2204.07773
replaced with revised version Tue, 26 Mar 2024 10:34:14 GMT   (1126kb)

Title: FedCau: A Proactive Stop Policy for Communication and Computation
  Efficient Federated Learning
Authors: Afsaneh Mahmoudi, Hossein S. Ghadikolaei, Jos\'e Mairton Barros Da
  Silva J\'unior and Carlo Fischione
Categories: cs.LG
DOI: 10.1109/TWC.2024.3378351
\\ ( https://arxiv.org/abs/2204.07773 ,  1126kb)
------------------------------------------------------------------------------
\\
arXiv:2211.01364
replaced with revised version Tue, 26 Mar 2024 17:45:01 GMT   (3778kb,D)

Title: An optimal control perspective on diffusion-based generative modeling
Authors: Julius Berner, Lorenz Richter, Karen Ullrich
Categories: cs.LG math.OC stat.ML
Comments: Accepted for oral presentation at NeurIPS 2022 Workshop on
  Score-Based Methods
Journal-ref: Transactions on Machine Learning Research, 2024
\\ ( https://arxiv.org/abs/2211.01364 ,  3778kb)
------------------------------------------------------------------------------
\\
arXiv:2211.04152
replaced with revised version Mon, 25 Mar 2024 20:50:12 GMT   (1657kb,D)

Title: Federated Learning Using Three-Operator ADMM
Authors: Shashi Kant, Jos\'e Mairton B. da Silva Jr., Gabor Fodor, Bo
  G\"oransson, Mats Bengtsson, and Carlo Fischione
Categories: cs.LG eess.SP math.OC
Comments: accepted to IEEE Journal of Selected Topics in Signal Processing,
  2022
DOI: 10.1109/JSTSP.2022.3221681
\\ ( https://arxiv.org/abs/2211.04152 ,  1657kb)
------------------------------------------------------------------------------
\\
arXiv:2301.12778
replaced with revised version Tue, 26 Mar 2024 13:29:16 GMT   (2323kb,D)

Title: Investigating Feature and Model Importance in Android Malware Detection:
  An Implemented Survey and Experimental Comparison of ML-Based Methods
Authors: Ali Muzaffar, Hani Ragab Hassen, Hind Zantout, Michael A Lones
Categories: cs.LG cs.CR
\\ ( https://arxiv.org/abs/2301.12778 ,  2323kb)
------------------------------------------------------------------------------
\\
arXiv:2302.10891
replaced with revised version Tue, 26 Mar 2024 08:50:19 GMT   (6399kb,D)

Title: An Implicit GNN Solver for Poisson-like problems
Authors: Matthieu Nastorg (TAU, IFPEN), Michele Alessandro Bucci (TAU),
  Thibault Faney (IFPEN), Jean-Marc Gratien (IFPEN), Guillaume Charpiat (TAU),
  Marc Schoenauer (TAU)
Categories: cs.LG cs.AI math.AP
\\ ( https://arxiv.org/abs/2302.10891 ,  6399kb)
------------------------------------------------------------------------------
\\
arXiv:2302.13143
replaced with revised version Tue, 26 Mar 2024 08:36:47 GMT   (752kb,D)

Title: Ensemble learning for Physics Informed Neural Networks: a Gradient
  Boosting approach
Authors: Zhiwei Fang, Sifan Wang, and Paris Perdikaris
Categories: cs.LG cs.NA math.NA
Journal-ref: ICLR 2024
\\ ( https://arxiv.org/abs/2302.13143 ,  752kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19125
replaced with revised version Tue, 26 Mar 2024 05:18:13 GMT   (38548kb,D)

Title: Graph Generation with $K^2$-trees
Authors: Yunhui Jang, Dongwoo Kim, Sungsoo Ahn
Categories: cs.LG cs.AI cs.SI
Comments: International Conference on Learning Representations (ICLR) 2024
\\ ( https://arxiv.org/abs/2305.19125 ,  38548kb)
------------------------------------------------------------------------------
\\
arXiv:2306.07905
replaced with revised version Tue, 26 Mar 2024 00:21:41 GMT   (136kb,D)

Title: Omega: Optimistic EMA Gradients
Authors: Juan Ramirez, Rohan Sukumaran, Quentin Bertrand, Gauthier Gidel
Categories: cs.LG math.OC stat.ML
Comments: Oral at the LatinX in AI workshop @ ICML 2023
\\ ( https://arxiv.org/abs/2306.07905 ,  136kb)
------------------------------------------------------------------------------
\\
arXiv:2306.15909
replaced with revised version Tue, 26 Mar 2024 15:13:20 GMT   (8623kb,D)

Title: RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$
Authors: Abhinav Bhatia, Samer B. Nashed, Shlomo Zilberstein
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2306.15909 ,  8623kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07728
replaced with revised version Tue, 26 Mar 2024 07:43:08 GMT   (18174kb,D)

Title: Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability
Authors: Seokhyeon Ha, Sunbeom Jung, Jungwoo Lee
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2308.07728 ,  18174kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12243
replaced with revised version Tue, 26 Mar 2024 15:12:19 GMT   (8057kb,D)

Title: Multi-Objective Optimization for Sparse Deep Multi-Task Learning
Authors: S. S. Hotegni, M. Berkemeier, S. Peitz
Categories: cs.LG cs.AI math.OC
Comments: 12 pages, 7 figures
\\ ( https://arxiv.org/abs/2308.12243 ,  8057kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15798
replaced with revised version Mon, 25 Mar 2024 20:09:26 GMT   (10572kb,D)

Title: Node-Aligned Graph-to-Graph (NAG2G): Elevating Template-Free Deep
  Learning Approaches in Single-Step Retrosynthesis
Authors: Lin Yao, Wentao Guo, Zhen Wang, Shang Xiang, Wentan Liu, Guolin Ke
Categories: cs.LG physics.chem-ph q-bio.QM
Journal-ref: JACS Au 4 (2024) 992-1003
DOI: 10.1021/jacsau.3c00737
\\ ( https://arxiv.org/abs/2309.15798 ,  10572kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02156
replaced with revised version Tue, 26 Mar 2024 17:36:54 GMT   (358kb,D)

Title: Probabilistically Rewired Message-Passing Neural Networks
Authors: Chendi Qian, Andrei Manolache, Kareem Ahmed, Zhe Zeng, Guy Van den
  Broeck, Mathias Niepert, Christopher Morris
Categories: cs.LG cs.NE
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.02156 ,  358kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02969
replaced with revised version Tue, 26 Mar 2024 14:00:59 GMT   (157kb,D)

Title: Dual Conic Proxies for AC Optimal Power Flow
Authors: Guancheng Qiu, Mathieu Tanneau, Pascal Van Hentenryck
Categories: cs.LG math.OC
Comments: accepted to PSCC 2024
\\ ( https://arxiv.org/abs/2310.02969 ,  157kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05401
replaced with revised version Mon, 25 Mar 2024 18:07:22 GMT   (1451kb,D)

Title: Entropy-MCMC: Sampling from Flat Basins with Ease
Authors: Bolian Li, Ruqi Zhang
Categories: cs.LG stat.ML
Journal-ref: ICLR 2024
\\ ( https://arxiv.org/abs/2310.05401 ,  1451kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10971
replaced with revised version Mon, 25 Mar 2024 23:14:28 GMT   (6609kb,D)

Title: Context-Aware Meta-Learning
Authors: Christopher Fifty, Dennis Duan, Ronald G. Junkins, Ehsan Amid, Jure
  Leskovec, Christopher Re, Sebastian Thrun
Categories: cs.LG cs.CV
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.10971 ,  6609kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15694
replaced with revised version Tue, 26 Mar 2024 11:52:59 GMT   (727kb,D)

Title: COPR: Continual Learning Human Preference through Optimal Policy
  Regularization
Authors: Han Zhang, Lin Gui, Yuanzhao Zhai, Hui Wang, Yu Lei, Ruifeng Xu
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2310.15694 ,  727kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01885
replaced with revised version Tue, 26 Mar 2024 12:59:44 GMT   (4063kb,D)

Title: Domain Randomization via Entropy Maximization
Authors: Gabriele Tiboni, Pascal Klink, Jan Peters, Tatiana Tommasi, Carlo
  D'Eramo, Georgia Chalvatzaki
Categories: cs.LG cs.RO
Comments: Published as a conference paper at ICLR 2024. Project website at
  https://gabrieletiboni.github.io/doraemon/
\\ ( https://arxiv.org/abs/2311.01885 ,  4063kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02766
replaced with revised version Tue, 26 Mar 2024 08:05:11 GMT   (9060kb,D)

Title: Riemannian Laplace Approximation with the Fisher Metric
Authors: Hanlin Yu, Marcelo Hartmann, Bernardo Williams, Mark Girolami, Arto
  Klami
Categories: cs.LG stat.ME stat.ML
Comments: AISTATS 2024, with additional fixes
\\ ( https://arxiv.org/abs/2311.02766 ,  9060kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03520
replaced with revised version Tue, 26 Mar 2024 06:05:13 GMT   (509kb,D)

Title: Brain Networks and Intelligence: A Graph Neural Network Based Approach
  to Resting State fMRI Data
Authors: Bishal Thapaliya, Esra Akbas, Jiayu Chen, Raam Sapkota, Bhaskar Ray,
  Pranav Suresh, Vince Calhoun, Jingyu Liu
Categories: cs.LG cs.AI q-bio.NC
\\ ( https://arxiv.org/abs/2311.03520 ,  509kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12086
replaced with revised version Tue, 26 Mar 2024 07:54:02 GMT   (1019kb,D)

Title: Masked Autoencoders Are Robust Neural Architecture Search Learners
Authors: Yiming Hu and Xiangxiang Chu and Bo Zhang
Categories: cs.LG cs.NE
\\ ( https://arxiv.org/abs/2311.12086 ,  1019kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15404
replaced with revised version Mon, 25 Mar 2024 22:55:43 GMT   (4618kb,D)

Title: Applying statistical learning theory to deep learning
Authors: C\'edric Gerbelot, Avetik Karagulyan, Stefani Karp, Kavya
  Ravichandran, Menachem Stern, Nathan Srebro
Categories: cs.LG cond-mat.dis-nn stat.ML
Comments: 66 pages, 20 figures
\\ ( https://arxiv.org/abs/2311.15404 ,  4618kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17094
replaced with revised version Tue, 26 Mar 2024 13:21:43 GMT   (18460kb,D)

Title: In Search of a Data Transformation That Accelerates Neural Field
  Training
Authors: Junwon Seo, Sangyoon Lee, Kwang In Kim, Jaeho Lee
Categories: cs.LG cs.CV
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2311.17094 ,  18460kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01201
replaced with revised version Mon, 25 Mar 2024 19:56:06 GMT   (321kb,D)

Title: PAC Privacy Preserving Diffusion Models
Authors: Qipan Xu, Youlong Ding, Xinxi Zhang, Jie Gao, Hao Wang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2312.01201 ,  321kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02230
replaced with revised version Tue, 26 Mar 2024 05:10:53 GMT   (27611kb,D)

Title: A Simple and Scalable Representation for Graph Generation
Authors: Yunhui Jang, Seul Lee, Sungsoo Ahn
Categories: cs.LG cs.AI
Comments: International Conference on Learning Representations (ICLR) 2024
\\ ( https://arxiv.org/abs/2312.02230 ,  27611kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05337
replaced with revised version Tue, 26 Mar 2024 14:09:56 GMT   (1801kb,D)

Title: Artificial Neural Nets and the Representation of Human Concepts
Authors: Timo Freiesleben
Categories: cs.LG cs.AI
Comments: For: Philosophy of Science for Machine Learning: Core Issues and New
  Perspectives, edited by Juan Duran and Giorgia Pozzi
\\ ( https://arxiv.org/abs/2312.05337 ,  1801kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08533
replaced with revised version Mon, 25 Mar 2024 21:09:21 GMT   (36647kb,D)

Title: World Models via Policy-Guided Trajectory Diffusion
Authors: Marc Rigter, Jun Yamada, Ingmar Posner
Categories: cs.LG cs.AI
Comments: Published in TMLR, March 2024
\\ ( https://arxiv.org/abs/2312.08533 ,  36647kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12467
replaced with revised version Tue, 26 Mar 2024 01:50:54 GMT   (29853kb,D)

Title: Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh
  Transformer
Authors: Youn-Yeol Yu, Jeongwhan Choi, Woojin Cho, Kookjin Lee, Nayong Kim,
  Kiseok Chang, Chang-Seung Woo, Ilho Kim, Seok-Woo Lee, Joon-Young Yang,
  Sooyoung Yoon, Noseong Park
Categories: cs.LG cs.AI cs.CE
Comments: Accepted at ICLR 2024
\\ ( https://arxiv.org/abs/2312.12467 ,  29853kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17329
replaced with revised version Tue, 26 Mar 2024 16:22:36 GMT   (503kb,D)

Title: PINN surrogate of Li-ion battery models for parameter inference. Part I:
  Implementation and multi-fidelity hierarchies for the single-particle model
Authors: Malik Hassanaly, Peter J. Weddle, Ryan N. King, Subhayan De, Alireza
  Doostan, Corey R. Randall, Eric J. Dufek, Andrew M. Colclasure, Kandler Smith
Categories: cs.LG physics.app-ph
\\ ( https://arxiv.org/abs/2312.17329 ,  503kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17336
replaced with revised version Tue, 26 Mar 2024 16:35:15 GMT   (733kb,D)

Title: PINN surrogate of Li-ion battery models for parameter inference. Part
  II: Regularization and application of the pseudo-2D model
Authors: Malik Hassanaly, Peter J. Weddle, Ryan N. King, Subhayan De, Alireza
  Doostan, Corey R. Randall, Eric J. Dufek, Andrew M. Colclasure, Kandler Smith
Categories: cs.LG physics.app-ph
\\ ( https://arxiv.org/abs/2312.17336 ,  733kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07788
replaced with revised version Tue, 26 Mar 2024 16:49:44 GMT   (3859kb,D)

Title: Activations and Gradients Compression for Model-Parallel Training
Authors: Mikhail Rudakov, Aleksandr Beznosikov, Yaroslav Kholodov, Alexander
  Gasnikov
Categories: cs.LG cs.DC math.OC
Comments: 17 pages, 6 figures, 5 tables
DOI: 10.1134/S1064562423701314
\\ ( https://arxiv.org/abs/2401.07788 ,  3859kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12533
replaced with revised version Tue, 26 Mar 2024 00:22:59 GMT   (8193kb,D)

Title: Near-Optimal Algorithms for Constrained k-Center Clustering with
  Instance-level Background Knowledge
Authors: Longkun Guo, Chaoqi Jia, Kewen Liao, Zhigang Lu and Minhui Xue
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.12533 ,  8193kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15059
replaced with revised version Tue, 26 Mar 2024 17:44:45 GMT   (27661kb,D)

Title: Fully Independent Communication in Multi-Agent Reinforcement Learning
Authors: Rafael Pina, Varuna De Silva, Corentin Artaud and Xiaolan Liu
Categories: cs.LG cs.MA
Comments: Extended version of the paper appearing on AAMAS 2024 with the same
  title. 11 pages, 8 figures
\\ ( https://arxiv.org/abs/2401.15059 ,  27661kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11800
replaced with revised version Mon, 25 Mar 2024 22:48:22 GMT   (99kb,D)

Title: Stochastic Approximation with Delayed Updates: Finite-Time Rates under
  Markovian Sampling
Authors: Arman Adibi, Nicolo Dal Fabbro, Luca Schenato, Sanjeev Kulkarni, H.
  Vincent Poor, George J. Pappas, Hamed Hassani, Aritra Mitra
Categories: cs.LG cs.AI cs.MA cs.SY eess.SY math.OC
Comments: Accepted to the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024!
\\ ( https://arxiv.org/abs/2402.11800 ,  99kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08763
replaced with revised version Tue, 26 Mar 2024 17:58:48 GMT   (10732kb,D)

Title: Simple and Scalable Strategies to Continually Pre-train Large Language
  Models
Authors: Adam Ibrahim, Benjamin Th\'erien, Kshitij Gupta, Mats L. Richter,
  Quentin Anthony, Timoth\'ee Lesort, Eugene Belilovsky, and Irina Rish
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2403.08763 ,  10732kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09428
replaced with revised version Tue, 26 Mar 2024 17:38:38 GMT   (8012kb,D)

Title: Borrowing Treasures from Neighbors: In-Context Learning for Multimodal
  Learning with Missing Modalities and Data Scarcity
Authors: Zhuo Zhi, Ziquan Liu, Moe Elbadawi, Adam Daneshmend, Mine Orlu, Abdul
  Basit, Andreas Demosthenous, Miguel Rodrigues
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.09428 ,  8012kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09857
replaced with revised version Mon, 25 Mar 2024 20:08:07 GMT   (1803kb,D)

Title: Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive
  Prompt
Authors: Chenxi Liu, Zhenyi Wang, Tianyi Xiong, Ruibo Chen, Yihan Wu, Junfeng
  Guo, Heng Huang
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2403.09857 ,  1803kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11996
replaced with revised version Tue, 26 Mar 2024 14:46:04 GMT   (37986kb,D)

Title: Accelerating Scientific Discovery with Generative Knowledge Extraction,
  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning
Authors: Markus J. Buehler
Categories: cs.LG cond-mat.mes-hall cond-mat.mtrl-sci cond-mat.soft cs.AI cs.CL
\\ ( https://arxiv.org/abs/2403.11996 ,  37986kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13374
replaced with revised version Tue, 26 Mar 2024 12:33:16 GMT   (1126kb,D)

Title: Byzantine-resilient Federated Learning With Adaptivity to Data
  Heterogeneity
Authors: Shiyuan Zuo, Xingrun Yan, Rongfei Fan, Han Hu, Hangguan Shan, Tony Q.
  S. Quek
Categories: cs.LG cs.AI cs.CR
\\ ( https://arxiv.org/abs/2403.13374 ,  1126kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15905
replaced with revised version Tue, 26 Mar 2024 11:11:49 GMT   (5298kb,D)

Title: Towards Low-Energy Adaptive Personalization for Resource-Constrained
  Devices
Authors: Yushan Huang, Josh Millar, Yuxuan Long, Yuchen Zhao, Hamed Hadaddi
Categories: cs.LG cs.CV
Comments: Accepetd to The 4th Workshop on Machine Learning and Systems
  (EuroMLSys '24)
\\ ( https://arxiv.org/abs/2403.15905 ,  5298kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16451
replaced with revised version Tue, 26 Mar 2024 11:35:08 GMT   (28061kb,D)

Title: DeepMachining: Online Prediction of Machining Errors of Lathe Machines
Authors: Xiang-Li Lu, Hwai-Jung Hsu, Che-Wei Chou, H. T. Kung, and Chen-Hsin
  Lee
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.16451 ,  28061kb)
------------------------------------------------------------------------------
\\
arXiv:2110.07953
replaced with revised version Tue, 26 Mar 2024 09:52:36 GMT   (44035kb,D)

Title: Attention-based Estimation and Prediction of Human Intent to augment
  Haptic Glove aided Control of Robotic Hand
Authors: Muneeb Ahmed, Rajesh Kumar, Qaim Abbas, Brejesh Lall, Arzad A.
  Kherani, Sudipto Mukherjee
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2110.07953 ,  44035kb)
------------------------------------------------------------------------------
\\
arXiv:2302.03788
replaced with revised version Tue, 26 Mar 2024 15:41:28 GMT   (7232kb,D)

Title: Toward a Theory of Causation for Interpreting Neural Code Models
Authors: David N. Palacio and Alejandro Velasco and Nathan Cooper and Alvaro
  Rodriguez and Kevin Moran and Denys Poshyvanyk
Categories: cs.SE cs.AI cs.LG stat.ME
Comments: Accepted to appear in IEEE Transactions on Software Engineering
\\ ( https://arxiv.org/abs/2302.03788 ,  7232kb)
------------------------------------------------------------------------------
\\
arXiv:2303.02490
replaced with revised version Tue, 26 Mar 2024 03:41:26 GMT   (28620kb,D)

Title: Diffusion Models Generate Images Like Painters: an Analytical Theory of
  Outline First, Details Later
Authors: Binxu Wang, John J. Vastola
Categories: cs.CV cs.AI cs.GR cs.NE
Comments: 44 pages, 28 figures. A briefer version was presented at NeurIPS23
  Workshop on Diffusion Models [arXiv:2311.10892]
ACM-class: F.2.2; I.3.3; I.2.10; I.2.6
\\ ( https://arxiv.org/abs/2303.02490 ,  28620kb)
------------------------------------------------------------------------------
\\
arXiv:2304.03773
replaced with revised version Tue, 26 Mar 2024 17:17:39 GMT   (14596kb,D)

Title: Safe Explicable Planning
Authors: Akkamahadevi Hanni, Andrew Boateng and Yu Zhang
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2304.03773 ,  14596kb)
------------------------------------------------------------------------------
\\
arXiv:2305.03123
replaced with revised version Tue, 26 Mar 2024 16:22:54 GMT   (1860kb)

Title: ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A Review
Authors: Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, and Lewis
  Nkenyereye
Categories: cs.CY cs.AI cs.CL cs.LG
Comments: 29 pages, 8 figures, 4 tables
\\ ( https://arxiv.org/abs/2305.03123 ,  1860kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18983
replaced with revised version Mon, 25 Mar 2024 20:21:25 GMT   (5481kb,D)

Title: SO(2)-Equivariant Downwash Models for Close Proximity Flight
Authors: H. Smith, A. Shankar, J. Gielis, J. Blumenkamp, A. Prorok
Categories: cs.RO cs.AI cs.LG
Journal-ref: Smith, H., Shankar, A., Gielis, J., Blumenkamp, J., & Prorok, A.
  IEEE Robotics and Automation Letters 9(2) (2024) 1174-1181
DOI: 10.1109/LRA.2023.3337701
\\ ( https://arxiv.org/abs/2305.18983 ,  5481kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00038
replaced with revised version Tue, 26 Mar 2024 11:07:30 GMT   (2673kb,D)

Title: FedCSD: A Federated Learning Based Approach for Code-Smell Detection
Authors: Sadi Alawadi, Khalid Alkharabsheh, Fahed Alkhabbas, Victor Kebande,
  Feras M. Awaysheh, Fabio Palomba, Mohammed Awad
Categories: cs.SE cs.AI cs.LG
Comments: 17 pages, 7 figures, Journal paper
ACM-class: D.2.4
DOI: 10.1109/JSYST.2023.3241415
\\ ( https://arxiv.org/abs/2306.00038 ,  2673kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16897
replaced with revised version Tue, 26 Mar 2024 17:40:47 GMT   (25122kb,D)

Title: DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields
Authors: Cheng-You Lu, Peisen Zhou, Angela Xing, Chandradeep Pokhariya, Arnab
  Dey, Ishaan Shah, Rugved Mavidipalli, Dylan Hu, Andrew Comport, Kefan Chen,
  Srinath Sridhar
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2307.16897 ,  25122kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01557
replaced with revised version Tue, 26 Mar 2024 06:50:43 GMT   (2524kb,D)

Title: Motion Planning Diffusion: Learning and Planning of Robot Motions with
  Diffusion Models
Authors: Joao Carvalho, An T. Le, Mark Baierl, Dorothea Koert, Jan Peters
Categories: cs.RO cs.AI cs.LG
\\ ( https://arxiv.org/abs/2308.01557 ,  2524kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10275
replaced with revised version Mon, 25 Mar 2024 20:28:22 GMT   (1284kb,D)

Title: Optimizing Crowd-Aware Multi-Agent Path Finding through Local
  Broadcasting with Graph Neural Networks
Authors: Phu Pham, Aniket Bera
Categories: cs.RO cs.AI cs.LG cs.MA
Comments: 8 pages, 5 figures, 2 tables
\\ ( https://arxiv.org/abs/2309.10275 ,  1284kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10863
replaced with revised version Mon, 25 Mar 2024 20:46:28 GMT   (2313kb,D)

Title: Greedy Perspectives: Multi-Drone View Planning for Collaborative
  Perception in Cluttered Environments
Authors: Krishna Suresh, Aditya Rauniyar, Micah Corah, Sebastian Scherer
Categories: cs.RO cs.AI
Comments: Submitted to IROS'24; 8 pages, 8 figures, 2 tables
\\ ( https://arxiv.org/abs/2310.10863 ,  2313kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12541
replaced with revised version Tue, 26 Mar 2024 12:04:44 GMT   (907kb,D)

Title: Large Language Model for Multi-objective Evolutionary Optimization
Authors: Fei Liu, Xi Lin, Zhenkun Wang, Shunyu Yao, Xialiang Tong, Mingxuan
  Yuan, Qingfu Zhang
Categories: cs.NE cs.AI cs.CL cs.ET
\\ ( https://arxiv.org/abs/2310.12541 ,  907kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09994
replaced with revised version Tue, 26 Mar 2024 13:06:28 GMT   (277kb,D)

Title: Towards more Practical Threat Models in Artificial Intelligence Security
Authors: Kathrin Grosse, Lukas Bieringer, Tarek Richard Besold, Alexandre Alahi
Categories: cs.CR cs.AI
Comments: 18 pages, 4 figures, 8 tables, accepted to Usenix Security,
  incorporated external feedback
\\ ( https://arxiv.org/abs/2311.09994 ,  277kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15964
replaced with revised version Tue, 26 Mar 2024 15:58:26 GMT   (0kb,I)

Title: Efficient Pre-training for Localized Instruction Generation of Videos
Authors: Anil Batra, Davide Moltisanti, Laura Sevilla-Lara, Marcus Rohrbach,
  Frank Keller
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: This version has some missing experiments and elaborative technical
  details
\\ ( https://arxiv.org/abs/2311.15964 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16081
replaced with revised version Tue, 26 Mar 2024 13:32:06 GMT   (18583kb,D)

Title: ViT-Lens: Towards Omni-modal Representations
Authors: Weixian Lei, Yixiao Ge, Kun Yi, Jianfeng Zhang, Difei Gao, Dylan Sun,
  Yuying Ge, Ying Shan, Mike Zheng Shou
Categories: cs.CV cs.AI
Comments: This work is a follow-up of arXiv:2308.10185. Accepted to CVPR2024
\\ ( https://arxiv.org/abs/2311.16081 ,  18583kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02512
replaced with revised version Tue, 26 Mar 2024 13:21:28 GMT   (1319kb,D)

Title: AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation
  with Unified Audio-Visual Speech Representation
Authors: Jeongsoo Choi, Se Jin Park, Minsu Kim, Yong Man Ro
Categories: cs.CV cs.AI cs.MM eess.AS
Comments: CVPR 2024. Code & Demo: https://choijeongsoo.github.io/av2av
\\ ( https://arxiv.org/abs/2312.02512 ,  1319kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03611
replaced with revised version Tue, 26 Mar 2024 10:13:11 GMT   (5561kb,D)

Title: DreamComposer: Controllable 3D Object Generation via Multi-View
  Conditions
Authors: Yunhan Yang, Yukun Huang, Xiaoyang Wu, Yuan-Chen Guo, Song-Hai Zhang,
  Hengshuang Zhao, Tong He, Xihui Liu
Categories: cs.CV cs.AI cs.LG
Comments: Project Page: https://yhyang-myron.github.io/DreamComposer/
\\ ( https://arxiv.org/abs/2312.03611 ,  5561kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14149
replaced with revised version Tue, 26 Mar 2024 12:47:12 GMT   (6524kb,D)

Title: TagAlign: Improving Vision-Language Alignment with Multi-Tag
  Classification
Authors: Qinying Liu, Wei Wu, Kecheng Zheng, Zhan Tong, Jiawei Liu, Yu Liu, Wei
  Chen, Zilei Wang, Yujun Shen
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2312.14149 ,  6524kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15101
replaced with revised version Mon, 25 Mar 2024 22:13:44 GMT   (1549kb,D)

Title: Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model
  Conversions between Frameworks
Authors: Nikolaos Louloudakis, Perry Gibson, Jos\'e Cano, and Ajitha Rajan
Categories: cs.SE cs.AI cs.CV cs.LG
Comments: 12 pages, 4 figures, 3 tables, 1 algorithm
\\ ( https://arxiv.org/abs/2312.15101 ,  1549kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12261
replaced with revised version Tue, 26 Mar 2024 15:52:06 GMT   (24520kb,D)

Title: Analyzing the Quality Attributes of AI Vision Models in Open
  Repositories Under Adversarial Attacks
Authors: Zerui Wang, Yan Liu
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2401.12261 ,  24520kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03246
replaced with revised version Tue, 26 Mar 2024 12:35:03 GMT   (29643kb,D)

Title: SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM
Authors: Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Tianchen
  Deng, Hongyu Wang
Categories: cs.CV cs.AI cs.RO
\\ ( https://arxiv.org/abs/2402.03246 ,  29643kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02232
replaced with revised version Mon, 25 Mar 2024 21:33:18 GMT   (246kb)

Title: Comprehensive evaluation of Mal-API-2019 dataset by machine learning in
  malware detection
Authors: Zhenglin Li, Haibei Zhu, Houze Liu, Jintong Song, Qishuo Cheng
Categories: cs.CR cs.AI cs.LG
Journal-ref: International Journal of Computer Science and Information
  Technology, 2024, 2(1), 1-9
DOI: 10.62051/ijcsit.v2n1.01
\\ ( https://arxiv.org/abs/2403.02232 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04202
replaced with revised version Tue, 26 Mar 2024 17:18:33 GMT   (28522kb,D)

Title: Dynamics of Moral Behavior in Heterogeneous Populations of Learning
  Agents
Authors: Elizaveta Tennant, Stephen Hailes, Mirco Musolesi
Categories: cs.MA cs.AI cs.CY cs.LG
\\ ( https://arxiv.org/abs/2403.04202 ,  28522kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04701
replaced with revised version Tue, 26 Mar 2024 11:26:17 GMT   (45787kb,D)

Title: ObjectCompose: Evaluating Resilience of Vision-Based Models on
  Object-to-Background Compositional Changes
Authors: Hashmat Shadab Malik, Muhammad Huzaifa, Muzammal Naseer, Salman Khan,
  Fahad Shahbaz Khan
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.04701 ,  45787kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06247
replaced with revised version Tue, 26 Mar 2024 14:42:21 GMT   (11425kb,D)

Title: Text-Guided Variational Image Generation for Industrial Anomaly
  Detection and Segmentation
Authors: Mingyu Lee, Jongwon Choi
Categories: cs.CV cs.AI
Comments: 18 pages, Accepted to CVPR 2024
\\ ( https://arxiv.org/abs/2403.06247 ,  11425kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13653
replaced with revised version Tue, 26 Mar 2024 08:45:09 GMT   (3279kb,D)

Title: Learning User Embeddings from Human Gaze for Personalised Saliency
  Prediction
Authors: Florian Strohm, Mihai B\^ace and Andreas Bulling
Categories: cs.CV cs.AI cs.HC
\\ ( https://arxiv.org/abs/2403.13653 ,  3279kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13682
replaced with revised version Tue, 26 Mar 2024 00:59:12 GMT   (2959kb,D)

Title: Threats, Attacks, and Defenses in Machine Unlearning: A Survey
Authors: Ziyao Liu, Huanyi Ye, Chen Chen, and Kwok-Yan Lam
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2403.13682 ,  2959kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14633
replaced with revised version Tue, 26 Mar 2024 07:12:40 GMT   (11357kb,D)

Title: Born With a Silver Spoon? Investigating Socioeconomic Bias in Large
  Language Models
Authors: Smriti Singh, Shuvam Keshari, Vinija Jain, Aman Chadha
Categories: cs.CY cs.AI cs.CL
\\ ( https://arxiv.org/abs/2403.14633 ,  11357kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14736 (*cross-listing*)
replaced with revised version Tue, 26 Mar 2024 05:25:04 GMT   (2920kb,D)

Title: NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein
  Classification in Graph Neural Networks
Authors: Yi-Shan Lan, Pin-Yu Chen, Tsung-Yi Ho
Categories: q-bio.QM cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.14736 ,  2920kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15585
replaced with revised version Tue, 26 Mar 2024 14:51:57 GMT   (8600kb,D)

Title: MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis
Authors: Mai A. Shaaban, Adnan Khan, Mohammad Yaqub
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.15585 ,  8600kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15931
replaced with revised version Tue, 26 Mar 2024 04:15:02 GMT   (14200kb,D)

Title: X-Portrait: Expressive Portrait Animation with Hierarchical Motion
  Attention
Authors: You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, Linjie Luo
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.15931 ,  14200kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16209
replaced with revised version Tue, 26 Mar 2024 04:22:02 GMT   (1578kb)

Title: Image Captioning in news report scenario
Authors: Tianrui Liu, Qi Cai, Changxin Xu, Bo Hong, Jize Xiong, Yuxin Qiao, and
  Tsungwei Yang
Categories: cs.CV cs.AI
Comments: 10 pages, 4 figures
\\ ( https://arxiv.org/abs/2403.16209 ,  1578kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16291
replaced with revised version Tue, 26 Mar 2024 09:23:07 GMT   (12055kb,D)

Title: Guessing human intentions to avoid dangerous situations in caregiving
  robots
Authors: No\'e Zapata, Gerardo P\'erez, Lucas Bonilla, Pedro N\'u\~nez, Pilar
  Bachiller and Pablo Bustos
Categories: cs.RO cs.AI
Comments: 8 pages, 6 figures. Submitted to IROS2024. For associated mpeg file
  see https://youtu.be/87UEB8P97KY
\\ ( https://arxiv.org/abs/2403.16291 ,  12055kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16303
replaced with revised version Tue, 26 Mar 2024 02:24:36 GMT   (4211kb)

Title: Large Language Models in Biomedical and Health Informatics: A
  Bibliometric Review
Authors: Huizi Yu, Lizhou Fan, Lingyao Li, Jiayan Zhou, Zihui Ma, Lu Xian,
  Wenyue Hua, Sijia He, Mingyu Jin, Yongfeng Zhang, Ashvin Gandhi, Xin Ma
Categories: cs.DL cs.AI cs.CL cs.SI
Comments: 50 pages, 7 figures, 4 tables
\\ ( https://arxiv.org/abs/2403.16303 ,  4211kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16760
replaced with revised version Tue, 26 Mar 2024 15:17:51 GMT   (359kb)

Title: As Good As A Coin Toss: Human detection of AI-generated images, videos,
  audio, and audiovisual stimuli
Authors: Di Cooke, Abigail Edwards, Sophia Barkoff, and Kathryn Kelly
Categories: cs.HC cs.AI cs.SD eess.AS
Comments: For study pre-registration, see https://osf.io/fnhr3
MSC-class: 68T01
ACM-class: I.2
\\ ( https://arxiv.org/abs/2403.16760 ,  359kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16915
replaced with revised version Tue, 26 Mar 2024 13:11:44 GMT   (84kb,D)

Title: Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language
  Models
Authors: Atsushi Keyaki and Ribeka Keyaki
Categories: cs.IR cs.AI cs.CL cs.LG
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2403.16915 ,  84kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16971
replaced with revised version Tue, 26 Mar 2024 02:35:07 GMT   (394kb,D)

Title: AIOS: LLM Agent Operating System
Authors: Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, Yongfeng
  Zhang
Categories: cs.OS cs.AI cs.CL
Comments: 14 pages, 5 figures, 5 tables; comments and suggestions are
  appreciated
\\ ( https://arxiv.org/abs/2403.16971 ,  394kb)
------------------------------------------------------------------------------
\\
arXiv:2207.01262
replaced with revised version Tue, 26 Mar 2024 06:54:43 GMT   (244kb,D)

Title: Understanding Performance of Long-Document Ranking Models through
  Comprehensive Evaluation and Leaderboarding
Authors: Leonid Boytsov, David Akinpelu, Tianyi Lin, Fangwei Gao, Yutian Zhao,
  Jeffrey Huang, Eric Nyberg
Categories: cs.IR cs.CL
\\ ( https://arxiv.org/abs/2207.01262 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2303.15230
replaced with revised version Tue, 26 Mar 2024 03:07:56 GMT   (4459kb,D)

Title: Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot
  Learning
Authors: Siteng Huang, Biao Gong, Yutong Feng, Min Zhang, Yiliang Lv, Donglin
  Wang
Categories: cs.CV cs.CL cs.LG
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2303.15230 ,  4459kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16338 (*cross-listing*)
replaced with revised version Mon, 25 Mar 2024 18:18:40 GMT   (1169kb,D)

Title: Generative Pre-training for Speech with Flow Matching
Authors: Alexander H. Liu, Matt Le, Apoorv Vyas, Bowen Shi, Andros Tjandra,
  Wei-Ning Hsu
Categories: eess.AS cs.CL cs.LG cs.SD
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.16338 ,  1169kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13452
replaced with revised version Tue, 26 Mar 2024 17:59:14 GMT   (1086kb,D)

Title: LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based
  on Twitter Data
Authors: Vijeta Deshpande, Minhwa Lee, Zonghai Yao, Zihao Zhang, Jason Brian
  Gibbons, Hong Yu
Categories: cs.SI cs.CL cs.LG
Journal-ref: LREC-COLING 2024
\\ ( https://arxiv.org/abs/2402.13452 ,  1086kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14117
replaced with revised version Tue, 26 Mar 2024 12:53:14 GMT   (1069kb,D)

Title: A Design Space for Intelligent and Interactive Writing Assistants
Authors: Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham
  Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss,
  David Zhou, Emad A. Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi,
  Senjuti Dutta, Jin L.C. Guo, Md Naimul Hoque, Yewon Kim, Simon Knight, Seyed
  Parsa Neshaei, Agnia Sergeyuk, Antonette Shibani, Disha Shrivastava, Lila
  Shroff, Jessi Stark, Sarah Sterman, Sitong Wang, Antoine Bosselut, Daniel
  Buschek, Joseph Chee Chang, Sherol Chen, Max Kreminski, Joonsuk Park, Roy
  Pea, Eugenia H. Rho, Shannon Zejiang Shen, Pao Siangliulue
Categories: cs.HC cs.CL
Comments: Published as a conference paper at CHI 2024
DOI: 10.1145/3613904.3642697
\\ ( https://arxiv.org/abs/2403.14117 ,  1069kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16167
replaced with revised version Tue, 26 Mar 2024 15:14:25 GMT   (3512kb,D)

Title: Exploiting Semantic Reconstruction to Mitigate Hallucinations in
  Vision-Language Models
Authors: Minchan Kim, Minyeong Kim, Junik Bae, Suhwan Choi, Sungkyung Kim, Buru
  Chang
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2403.16167 ,  3512kb)
------------------------------------------------------------------------------
\\
arXiv:2102.01432 (*cross-listing*)
replaced with revised version Tue, 26 Mar 2024 08:21:24 GMT   (9221kb,D)

Title: Bayesian data-driven discovery of partial differential equations with
  variable coefficients
Authors: Aoxue Chen, Yifan Du, Liyao Mars Gao, Guang Lin
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2102.01432 ,  9221kb)
------------------------------------------------------------------------------
\\
arXiv:2108.13969
replaced with revised version Tue, 26 Mar 2024 16:13:26 GMT   (1190kb,D)

Title: Semi-Supervised Crowd Counting from Unlabeled Data
Authors: Haoran Duan, Fan Wan, Rui Sun, Zeyu Wang, Varun Ojha, Yu Guan, Hubert
  P. H. Shum, Bingzhang Hu, Yang Long
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2108.13969 ,  1190kb)
------------------------------------------------------------------------------
\\
arXiv:2207.12730
replaced with revised version Tue, 26 Mar 2024 09:35:03 GMT   (2996kb,D)

Title: P2ANet: A Dataset and Benchmark for Dense Action Detection from Table
  Tennis Match Broadcasting Videos
Authors: Jiang Bian, Xuhong Li, Tao Wang, Qingzhong Wang, Jun Huang, Chen Liu,
  Jun Zhao, Feixiang Lu, Dejing Dou, Haoyi Xiong
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2207.12730 ,  2996kb)
------------------------------------------------------------------------------
\\
arXiv:2208.06620
replaced with revised version Tue, 26 Mar 2024 01:22:44 GMT   (1661kb,D)

Title: Opinion Market Model: Stemming Far-Right Opinion Spread using Positive
  Interventions
Authors: Pio Calderon, Rohit Ram, Marian-Andrei Rizoiu
Categories: cs.SI cs.LG
Comments: accepted in the 18th AAAI International Conference on Web and Social
  Media (ICWSM'24)
\\ ( https://arxiv.org/abs/2208.06620 ,  1661kb)
------------------------------------------------------------------------------
\\
arXiv:2210.06459 (*cross-listing*)
replaced with revised version Tue, 26 Mar 2024 16:49:11 GMT   (808kb,D)

Title: Differentially private multivariate medians
Authors: Kelly Ramsay, Aukosh Jagannath, Shoja'eddin Chenouri
Categories: math.ST cs.CR cs.LG stat.ML stat.TH
Comments: 42 pages, 3 figures, 2 tables
MSC-class: 62G35, 62G05, 62H12
\\ ( https://arxiv.org/abs/2210.06459 ,  808kb)
------------------------------------------------------------------------------
\\
arXiv:2305.10474
replaced with revised version Tue, 26 Mar 2024 01:11:52 GMT   (37272kb,D)

Title: Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models
Authors: Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan
  Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, Yogesh Balaji
Categories: cs.CV cs.GR cs.LG
Comments: ICCV 2023. Project webpage:
  https://research.nvidia.com/labs/dir/pyoco
\\ ( https://arxiv.org/abs/2305.10474 ,  37272kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06157
replaced with revised version Mon, 25 Mar 2024 21:23:11 GMT   (1151kb,D)

Title: Fault Localization for Buggy Deep Learning Framework Conversions in
  Image Recognition
Authors: Nikolaos Louloudakis, Perry Gibson, Jos\'e Cano, and Ajitha Rajan
Categories: cs.CV cs.LG cs.SE cs.SY eess.SY
Comments: 5 pages, 3 figures, 1 table
DOI: 10.1109/ASE56229.2023.00147
\\ ( https://arxiv.org/abs/2306.06157 ,  1151kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06208
replaced with revised version Mon, 25 Mar 2024 21:08:25 GMT   (2974kb,D)

Title: DeltaNN: Assessing the Impact of Computational Environment Parameters on
  the Performance of Image Recognition Models
Authors: Nikolaos Louloudakis, Perry Gibson, Jos\'e Cano, and Ajitha Rajan
Categories: cs.CV cs.LG cs.SE cs.SY eess.SY
Comments: 11 pages, 10 figures, 2 tables
DOI: 10.1109/ICSME58846.2023.00054
\\ ( https://arxiv.org/abs/2306.06208 ,  2974kb)
------------------------------------------------------------------------------
\\
arXiv:2308.16212 (*cross-listing*)
replaced with revised version Tue, 26 Mar 2024 11:32:36 GMT   (9418kb,D)

Title: RetroBridge: Modeling Retrosynthesis with Markov Bridges
Authors: Ilia Igashov, Arne Schneuing, Marwin Segler, Michael Bronstein and
  Bruno Correia
Categories: q-bio.QM cs.LG q-bio.BM
\\ ( https://arxiv.org/abs/2308.16212 ,  9418kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00736 (*cross-listing*)
replaced with revised version Tue, 26 Mar 2024 02:33:12 GMT   (1680kb)

Title: Prediction Error Estimation in Random Forests
Authors: Ian Krupkin and Johanna Hardin
Categories: stat.ML cs.LG
Comments: arXiv admin note: text overlap with arXiv:2104.00673 by other authors
\\ ( https://arxiv.org/abs/2309.00736 ,  1680kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10837 (*cross-listing*)
replaced with revised version Tue, 26 Mar 2024 01:23:52 GMT   (2520kb,D)

Title: Improving Opioid Use Disorder Risk Modelling through Behavioral and
  Genetic Feature Integration
Authors: Sybille L\'egitime, Kaustubh Prabhu, Devin McConnell, Bing Wang, Dipak
  K. Dey, and Derek Aguiar
Categories: q-bio.QM cs.CY cs.LG
Comments: 32 pages (including References section), 8 figures. Under review by
  PLOS One
\\ ( https://arxiv.org/abs/2309.10837 ,  2520kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02869 (*cross-listing*)
replaced with revised version Tue, 26 Mar 2024 13:43:55 GMT   (1229kb,D)

Title: Harmonic Control Lyapunov Barrier Functions for Constrained Optimal
  Control with Reach-Avoid Specifications
Authors: Amartya Mukherjee, Ruikun Zhou, Haocheng Chang and Jun Liu
Categories: math.OC cs.LG math.AP
\\ ( https://arxiv.org/abs/2310.02869 ,  1229kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09362
replaced with revised version Mon, 25 Mar 2024 19:08:53 GMT   (1982kb,D)

Title: From Words and Exercises to Wellness: Farsi Chatbot for Self-Attachment
  Technique
Authors: Sina Elahimanesh, Shayan Salehi, Sara Zahedi Movahed, Lisa Alazraki,
  Ruoyu Hu, Abbas Edalat
Categories: cs.HC cs.LG
\\ ( https://arxiv.org/abs/2310.09362 ,  1982kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17569
replaced with revised version Tue, 26 Mar 2024 11:52:23 GMT   (14342kb,D)

Title: SD4Match: Learning to Prompt Stable Diffusion Model for Semantic
  Matching
Authors: Xinghui Li, Jingyi Lu, Kai Han, Victor Prisacariu
Categories: cs.CV cs.LG
Comments: Accepted to CVPR 2024. Project website:
  https://sd4match.active.vision/
\\ ( https://arxiv.org/abs/2310.17569 ,  14342kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18841 (*cross-listing*)
replaced with revised version Tue, 26 Mar 2024 17:39:30 GMT   (59kb)

Title: A randomized algorithm for nonconvex minimization with inexact
  evaluations and complexity guarantees
Authors: Shuyao Li, Stephen J. Wright
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2310.18841 ,  59kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01453 (*cross-listing*)
replaced with revised version Tue, 26 Mar 2024 01:44:52 GMT   (485kb,D)

Title: PPI++: Efficient Prediction-Powered Inference
Authors: Anastasios N. Angelopoulos and John C. Duchi and Tijana Zrnic
Categories: stat.ML cs.LG stat.ME
Comments: Code available at https://github.com/aangelopoulos/ppi_py
\\ ( https://arxiv.org/abs/2311.01453 ,  485kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03197
replaced with revised version Tue, 26 Mar 2024 11:37:38 GMT   (765kb,D)

Title: Stable Linear Subspace Identification: A Machine Learning Approach
Authors: Loris Di Natale, Muhammad Zakwan, Bratislav Svetozarevic, Philipp
  Heer, Giancarlo Ferrari-Trecate, Colin N. Jones
Categories: eess.SY cs.LG cs.SY
Comments: Accepted at ECC 2024
\\ ( https://arxiv.org/abs/2311.03197 ,  765kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07939 (*cross-listing*)
replaced with revised version Tue, 26 Mar 2024 11:54:27 GMT   (5374kb,D)

Title: Discretized Distributed Optimization over Dynamic Digraphs
Authors: Mohammadreza Doostmohammadian, Wei Jiang, Muwahida Liaquat, Alireza
  Aghasi, Houman Zarrabi
Categories: math.OC cs.LG cs.SY eess.SP eess.SY
Journal-ref: IEEE Transactions on Automation science and Engineering 2024
\\ ( https://arxiv.org/abs/2311.07939 ,  5374kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08744
replaced with revised version Tue, 26 Mar 2024 08:14:22 GMT   (249kb,D)

Title: Graph Signal Diffusion Model for Collaborative Filtering
Authors: Yunqin Zhu, Chao Wang, Qi Zhang and Hui Xiong
Categories: cs.IR cs.LG
Comments: 11 pages, 8 figures, Accepted by SIGIR 2024
\\ ( https://arxiv.org/abs/2311.08744 ,  249kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14427 (*cross-listing*)
replaced with revised version Tue, 26 Mar 2024 16:15:40 GMT   (8063kb,D)

Title: Disentangling the Spectral Properties of the Hodge Laplacian: Not All
  Small Eigenvalues Are Equal
Authors: Vincent P. Grande, Michael T. Schaub
Categories: math.AT cs.LG
Comments: 5 pages, 4 figures, comments welcome
Journal-ref: ICASSP 2024 - 2024 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)
DOI: 10.1109/ICASSP48485.2024.10446051
\\ ( https://arxiv.org/abs/2311.14427 ,  8063kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05264
replaced with revised version Tue, 26 Mar 2024 05:35:38 GMT   (617kb,D)

Title: All Rivers Run to the Sea: Private Learning with Asymmetric Flows
Authors: Yue Niu, Ramy E. Ali, Saurav Prakash, Salman Avestimehr
Categories: cs.CR cs.LG
Comments: Camera-ready for CVPR 2024
\\ ( https://arxiv.org/abs/2312.05264 ,  617kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07809 (*cross-listing*)
replaced with revised version Tue, 26 Mar 2024 17:29:07 GMT   (436kb,D)

Title: Optimal Data Splitting in Distributed Optimization for Machine Learning
Authors: Daniil Medyakov, Gleb Molodtsov, Aleksandr Beznosikov, Alexander
  Gasnikov
Categories: math.OC cs.LG
Comments: 17 pages, 2 figures
DOI: 10.1134/S1064562423701600
\\ ( https://arxiv.org/abs/2401.07809 ,  436kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00093
replaced with revised version Tue, 26 Mar 2024 11:20:02 GMT   (2745kb,D)

Title: ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation
Authors: Bhabesh Mali, Karthik Maddala, Sweeya Reddy, Vatsal Gupta, Chandan
  Karfa, Ramesh Karri
Categories: cs.SE cs.LG
Comments: 6 pages, 5 figures and 2 table
\\ ( https://arxiv.org/abs/2402.00093 ,  2745kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04866 (*cross-listing*)
replaced with revised version Tue, 26 Mar 2024 16:57:46 GMT   (2938kb,D)

Title: Room Transfer Function Reconstruction Using Complex-valued Neural
  Networks and Irregularly Distributed Microphones
Authors: Francesca Ronchini, Luca Comanducci, Mirco Pezzoli, Fabio Antonacci,
  Augusto Sarti
Categories: eess.AS cs.LG cs.SD eess.SP
Comments: Submitted to EUSIPCO 2024
\\ ( https://arxiv.org/abs/2402.04866 ,  2938kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16731
replaced with revised version Mon, 25 Mar 2024 18:51:02 GMT   (773kb,D)

Title: Accelerating Graph Neural Networks on Real Processing-In-Memory Systems
Authors: Christina Giannoula, Peiming Yang, Ivan Fernandez Vega, Jiacheng Yang,
  Yu Xin Li, Juan Gomez Luna, Mohammad Sadrosadati, Onur Mutlu, Gennady
  Pekhimenko
Categories: cs.AR cs.DC cs.LG cs.PF
\\ ( https://arxiv.org/abs/2402.16731 ,  773kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00033 (*cross-listing*)
replaced with revised version Tue, 26 Mar 2024 04:58:01 GMT   (6319kb,D)

Title: Identification of Craving Maps among Marijuana Users via the Analysis of
  Functional Brain Networks with High-Order Attention Graph Neural Networks
Authors: Jun-En Ding, Shihao Yang, Anna Zilverstand, and Feng Liu
Categories: q-bio.NC cs.LG eess.SP
\\ ( https://arxiv.org/abs/2403.00033 ,  6319kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16861
replaced with revised version Tue, 26 Mar 2024 07:56:21 GMT   (40kb)

Title: DISL: Fueling Research with A Large Dataset of Solidity Smart Contracts
Authors: Gabriele Morello, Mojtaba Eshghie, Sofia Bobadilla, Martin Monperrus
Categories: cs.SE cs.DC cs.LG
\\ ( https://arxiv.org/abs/2403.16861 ,  40kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
