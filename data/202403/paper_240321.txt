paper_240321.txt

Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 100021 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月21日 11:55
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Tue 19 Mar 24 18:00:00 GMT  to  Wed 20 Mar 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.13311
Date: Wed, 20 Mar 2024 05:23:24 GMT   (7245kb,D)

Title: Multi-Robot Connected Fermat Spiral Coverage
Authors: Jingtao Tang, Hang Ma
Categories: cs.AI cs.MA cs.RO
Comments: accepted to ICAPS24
\\
  We introduce the Multi-Robot Connected Fermat Spiral (MCFS), a novel
algorithmic framework for Multi-Robot Coverage Path Planning (MCPP) that adapts
Connected Fermat Spiral (CFS) from the computer graphics community to
multi-robot coordination for the first time. MCFS uniquely enables the
orchestration of multiple robots to generate coverage paths that contour around
arbitrarily shaped obstacles, a feature that is notably lacking in traditional
methods. Our framework not only enhances area coverage and optimizes task
performance, particularly in terms of makespan, for workspaces rich in
irregular obstacles but also addresses the challenges of path continuity and
curvature critical for non-holonomic robots by generating smooth paths without
decomposing the workspace. MCFS solves MCPP by constructing a graph of isolines
and transforming MCPP into a combinatorial optimization problem, aiming to
minimize the makespan while covering all vertices. Our contributions include
developing a unified CFS version for scalable and adaptable MCPP, extending it
to MCPP with novel optimization techniques for cost reduction and path
continuity and smoothness, and demonstrating through extensive experiments that
MCFS outperforms existing MCPP methods in makespan, path curvature, coverage
ratio, and overlapping ratio. Our research marks a significant step in MCPP,
showcasing the fusion of computer graphics and automated planning principles to
advance the capabilities of multi-robot systems in complex environments. Our
code is available at https://github.com/reso1/MCFS.
\\ ( https://arxiv.org/abs/2403.13311 ,  7245kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13313
Date: Wed, 20 Mar 2024 05:34:03 GMT   (2570kb,D)

Title: Polaris: A Safety-focused LLM Constellation Architecture for Healthcare
Authors: Subhabrata Mukherjee, Paul Gamble, Markel Sanz Ausin, Neel Kant, Kriti
  Aggarwal, Neha Manjunath, Debajyoti Datta, Zhengliang Liu, Jiayuan Ding,
  Sophia Busacca, Cezanne Bianco, Swapnil Sharma, Rae Lasko, Michelle Voisard,
  Sanchay Harneja, Darya Filippova, Gerry Meixiong, Kevin Cha, Amir Youssefi,
  Meyhaa Buvanesh, Howard Weingram, Sebastian Bierman-Lytle, Harpreet Singh
  Mangat, Kim Parikh, Saad Godil, Alex Miller
Categories: cs.AI cs.CL
\\
  We develop Polaris, the first safety-focused LLM constellation for real-time
patient-AI healthcare conversations. Unlike prior LLM works in healthcare
focusing on tasks like question answering, our work specifically focuses on
long multi-turn voice conversations. Our one-trillion parameter constellation
system is composed of several multibillion parameter LLMs as co-operative
agents: a stateful primary agent that focuses on driving an engaging
conversation and several specialist support agents focused on healthcare tasks
performed by nurses to increase safety and reduce hallucinations. We develop a
sophisticated training protocol for iterative co-training of the agents that
optimize for diverse objectives. We train our models on proprietary data,
clinical care plans, healthcare regulatory documents, medical manuals, and
other medical reasoning documents. We align our models to speak like medical
professionals, using organic healthcare conversations and simulated ones
between patient actors and experienced nurses. This allows our system to
express unique capabilities such as rapport building, trust building, empathy
and bedside manner. Finally, we present the first comprehensive clinician
evaluation of an LLM system for healthcare. We recruited over 1100 U.S.
licensed nurses and over 130 U.S. licensed physicians to perform end-to-end
conversational evaluations of our system by posing as patients and rating the
system on several measures. We demonstrate Polaris performs on par with human
nurses on aggregate across dimensions such as medical safety, clinical
readiness, conversational quality, and bedside manner. Additionally, we conduct
a challenging task-based evaluation of the individual specialist support
agents, where we demonstrate our LLM agents significantly outperform a much
larger general-purpose LLM (GPT-4) as well as from its own medium-size class
(LLaMA-2 70B).
\\ ( https://arxiv.org/abs/2403.13313 ,  2570kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13433
Date: Wed, 20 Mar 2024 09:21:32 GMT   (3402kb,D)

Title: Agent Group Chat: An Interactive Group Chat Simulacra For Better
  Eliciting Collective Emergent Behavior
Authors: Zhouhong Gu, Xiaoxuan Zhu, Haoran Guo, Lin Zhang, Yin Cai, Hao Shen,
  Jiangjie Chen, Zheyu Ye, Yifei Dai, Yan Gao, Yao Hu, Hongwei Feng, Yanghua
  Xiao
Categories: cs.AI cs.CL cs.CY
\\
  To investigate the role of language in human collective behaviors, we
developed the Agent Group Chat simulation to simulate linguistic interactions
among multi-agent in different settings. Agents are asked to free chat in this
simulation for their own purposes based on their character setting, aiming to
see agents exhibit emergent behaviours that are both unforeseen and
significant. Four narrative scenarios, Inheritance Disputes, Law Court Debates,
Philosophical Discourses, Movie Casting Contention, are integrated into Agent
Group Chat to evaluate its support for diverse storylines. By configuring
specific environmental settings within Agent Group Chat, we are able to assess
whether agents exhibit behaviors that align with human expectations. We
evaluate the disorder within the environment by computing the n-gram Shannon
entropy of all the content speak by characters. Our findings reveal that under
the premise of agents possessing substantial alignment with human expectations,
facilitating more extensive information exchange within the simulation ensures
greater orderliness amidst diversity, which leads to the emergence of more
unexpected and meaningful emergent behaviors. The code is open source in
https://github.com/MikeGu721/AgentGroup, and online platform will be open soon.
\\ ( https://arxiv.org/abs/2403.13433 ,  3402kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13441
Date: Wed, 20 Mar 2024 09:34:38 GMT   (74kb,D)

Title: Robustness Verifcation in Neural Networks
Authors: Adrian Wurm
Categories: cs.AI cs.LG
Comments: 16 pages, 1 figure
\\
  In this paper we investigate formal verification problems for Neural Network
computations. Of central importance will be various robustness and minimization
problems such as: Given symbolic specifications of allowed inputs and outputs
in form of Linear Programming instances, one question is whether there do exist
valid inputs such that the network computes a valid output? And does this
property hold for all valid inputs? Do two given networks compute the same
function? Is there a smaller network computing the same function?
  The complexity of these questions have been investigated recently from a
practical point of view and approximated by heuristic algorithms. We complement
these achievements by giving a theoretical framework that enables us to
interchange security and efficiency questions in neural networks and analyze
their computational complexities. We show that the problems are conquerable in
a semi-linear setting, meaning that for piecewise linear activation functions
and when the sum- or maximum metric is used, most of them are in P or in NP at
most.
\\ ( https://arxiv.org/abs/2403.13441 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13447
Date: Wed, 20 Mar 2024 09:42:43 GMT   (8932kb,D)

Title: HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal
  Large Language Models
Authors: Wenqiao Zhang, Tianwei Lin, Jiang Liu, Fangxun Shu, Haoyuan Li, Lei
  Zhang, He Wanggui, Hao Zhou, Zheqi Lv, Hao Jiang, Juncheng Li, Siliang Tang,
  Yueting Zhuang
Categories: cs.AI cs.CL cs.CV
\\
  Recent advancements indicate that scaling up Multimodal Large Language Models
(MLLMs) effectively enhances performance on downstream multimodal tasks. The
prevailing MLLM paradigm, \emph{e.g.}, LLaVA, transforms visual features into
text-like tokens using a \emph{static} vision-language mapper, thereby enabling
\emph{static} LLMs to develop the capability to comprehend visual information
through visual instruction tuning. Although promising, the \emph{static} tuning
strategy~\footnote{The static tuning refers to the trained model with static
parameters.} that shares the same parameters may constrain performance across
different downstream multimodal tasks. In light of this, we introduce
HyperLLaVA, which involves adaptive tuning of the projector and LLM parameters,
in conjunction with a dynamic visual expert and language expert, respectively.
These experts are derived from HyperNetworks, which generates adaptive
parameter shifts through visual and language guidance, enabling dynamic
projector and LLM modeling in two-stage training.
  Our experiments demonstrate that our solution significantly surpasses LLaVA
on existing MLLM benchmarks, including MME, MMBench, SEED-Bench, and
LLaVA-Bench. ~\footnote{Our project is available on the link
https://github.com/DCDmllm/HyperLLaVA}.
\\ ( https://arxiv.org/abs/2403.13447 ,  8932kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13518
Date: Wed, 20 Mar 2024 11:38:30 GMT   (3166kb,D)

Title: Motion Generation from Fine-grained Textual Descriptions
Authors: Kunhang Li and Yansong Feng
Categories: cs.AI cs.CL cs.CV cs.RO
\\
  The task of text2motion is to generate motion sequences from given textual
descriptions, where a model should explore the interactions between natural
language instructions and human body movements. While most existing works are
confined to coarse-grained motion descriptions (e.g., "A man squats."),
fine-grained ones specifying movements of relevant body parts are barely
explored. Models trained with coarse texts may not be able to learn mappings
from fine-grained motion-related words to motion primitives, resulting in the
failure in generating motions from unseen descriptions. In this paper, we build
a large-scale language-motion dataset with fine-grained textual descriptions,
FineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts. Accordingly, we
design a new text2motion model, FineMotionDiffuse, which makes full use of
fine-grained textual information. Our experiments show that FineMotionDiffuse
trained on FineHumanML3D acquires good results in quantitative evaluation. We
also find this model can better generate spatially/chronologically composite
motions by learning the implicit mappings from simple descriptions to the
corresponding basic motions.
\\ ( https://arxiv.org/abs/2403.13518 ,  3166kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13705
Date: Wed, 20 Mar 2024 16:08:57 GMT   (694kb)

Title: Research Re: search & Re-search
Authors: Aske Plaat
Categories: cs.AI
Comments: PhD thesis Aske Plaat 20 June 1996. AlphaBeta, SSS*, MTD(f)
\\
  Search algorithms are often categorized by their node expansion strategy. One
option is the depth-first strategy, a simple backtracking strategy that
traverses the search space in the order in which successor nodes are generated.
An alternative is the best-first strategy, which was designed to make it
possible to use domain-specific heuristic information. By exploring promising
parts of the search space first, best-first algorithms are usually more
efficient than depth-first algorithms.
  In programs that play minimax games such as chess and checkers, the
efficiency of the search is of crucial importance. Given the success of
best-first algorithms in other domains, one would expect them to be used for
minimax games too. However, all high-performance game-playing programs are
based on a depth-first algorithm.
  This study takes a closer look at a depth-first algorithm, AB, and a
best-first algorithm, SSS. The prevailing opinion on these algorithms is that
SSS offers the potential for a more efficient search, but that its complicated
formulation and exponential memory requirements render it impractical. The
theoretical part of this work shows that there is a surprisingly
straightforward link between the two algorithms -- for all practical purposes,
SSS is a special case of AB. Subsequent empirical evidence proves the
prevailing opinion on SSS to be wrong: it is not a complicated algorithm, it
does not need too much memory, and it is also not more efficient than
depth-first search.
\\ ( https://arxiv.org/abs/2403.13705 ,  694kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13089
Date: Tue, 19 Mar 2024 18:37:05 GMT   (293kb)

Title: Automatic Summarization of Doctor-Patient Encounter Dialogues Using
  Large Language Model through Prompt Tuning
Authors: Mengxian Lyu, Cheng Peng, Xiaohan Li, Patrick Balian, Jiang Bian,
  Yonghui Wu
Categories: cs.CL
\\
  Automatic text summarization (ATS) is an emerging technology to assist
clinicians in providing continuous and coordinated care. This study presents an
approach to summarize doctor-patient dialogues using generative large language
models (LLMs). We developed prompt-tuning algorithms to instruct generative
LLMs to summarize clinical text. We examined the prompt-tuning strategies, the
size of soft prompts, and the few-short learning ability of GatorTronGPT, a
generative clinical LLM developed using 277 billion clinical and general
English words with up to 20 billion parameters. We compared GatorTronGPT with a
previous solution based on fine-tuning of a widely used T5 model, using a
clinical benchmark dataset MTS-DIALOG. The experimental results show that the
GatorTronGPT- 20B model achieved the best performance on all evaluation
metrics. The proposed solution has a low computing cost as the LLM parameters
are not updated during prompt-tuning. This study demonstrates the efficiency of
generative clinical LLMs for clinical ATS through prompt tuning.
\\ ( https://arxiv.org/abs/2403.13089 ,  293kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13107
Date: Tue, 19 Mar 2024 19:15:13 GMT   (117kb,D)

Title: Towards Unsupervised Question Answering System with Multi-level
  Summarization for Legal Text
Authors: M Manvith Prabhu, Haricharana Srinivasa, Anand Kumar M
Categories: cs.CL cs.CY cs.LG
Comments: 6 pages, 2 figures
\\
  This paper summarizes Team SCaLAR's work on SemEval-2024 Task 5: Legal
Argument Reasoning in Civil Procedure. To address this Binary Classification
task, which was daunting due to the complexity of the Legal Texts involved, we
propose a simple yet novel similarity and distance-based unsupervised approach
to generate labels. Further, we explore the Multi-level fusion of Legal-Bert
embeddings using ensemble features, including CNN, GRU, and LSTM. To address
the lengthy nature of Legal explanation in the dataset, we introduce T5-based
segment-wise summarization, which successfully retained crucial information,
enhancing the model's performance. Our unsupervised system witnessed a 20-point
increase in macro F1-score on the development set and a 10-point increase on
the test set, which is promising given its uncomplicated architecture.
\\ ( https://arxiv.org/abs/2403.13107 ,  117kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13112
Date: Tue, 19 Mar 2024 19:27:23 GMT   (142kb,D)

Title: Encode Once and Decode in Parallel: Efficient Transformer Decoding
Authors: Bo-Ru Lu, Nikita Haduong, Chien-Yu Lin, Hao Cheng, Noah A. Smith, Mari
  Ostendorf
Categories: cs.CL
Comments: 14 pages, 4 figures.
  https://github.com/boru-roylu/encode-once-and-decode-in-parallel
\\
  Transformer-based NLP models are powerful but have high computational costs
that limit deployment scenarios. Finetuned encoder-decoder models are popular
in specialized domains and can outperform larger more generalized decoder-only
models, such as GPT-4. We introduce a new configuration for encoder-decoder
models that improves efficiency on structured output and question-answering
tasks where multiple outputs are required of a single input. Our method,
prompt-in-decoder (PiD), encodes the input once and decodes output in parallel,
boosting both training and inference efficiency by avoiding duplicate input
encoding, thereby reducing the decoder's memory footprint. We achieve
computation reduction that roughly scales with the number of subtasks, gaining
up to 4.6x speed-up over state-of-the-art models for dialogue state tracking,
summarization, and question-answering tasks with comparable or better
performance. We release our training/inference code and checkpoints.
\\ ( https://arxiv.org/abs/2403.13112 ,  142kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13130
Date: Tue, 19 Mar 2024 19:59:54 GMT   (7123kb,D)

Title: Self-generated Replay Memories for Continual Neural Machine Translation
Authors: Michele Resta and Davide Bacciu
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at NAACL 2024
\\
  Modern Neural Machine Translation systems exhibit strong performance in
several different languages and are constantly improving. Their ability to
learn continuously is, however, still severely limited by the catastrophic
forgetting issue. In this work, we leverage a key property of encoder-decoder
Transformers, i.e. their generative ability, to propose a novel approach to
continually learning Neural Machine Translation systems. We show how this can
effectively learn on a stream of experiences comprising different languages, by
leveraging a replay memory populated by using the model itself as a generator
of parallel sentences. We empirically demonstrate that our approach can
counteract catastrophic forgetting without requiring explicit memorization of
training data. Code will be publicly available upon publication. Code:
https://github.com/m-resta/sg-rep
\\ ( https://arxiv.org/abs/2403.13130 ,  7123kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13169
Date: Tue, 19 Mar 2024 21:45:29 GMT   (7817kb,D)

Title: Wav2Gloss: Generating Interlinear Glossed Text from Speech
Authors: Taiqi He, Kwanghee Choi, Lindia Tjuatja, Nathaniel R. Robinson,
  Jiatong Shi, Shinji Watanabe, Graham Neubig, David R. Mortensen, Lori Levin
Categories: cs.CL
\\
  Thousands of the world's languages are in danger of extinction--a tremendous
threat to cultural identities and human language diversity. Interlinear Glossed
Text (IGT) is a form of linguistic annotation that can support documentation
and resource creation for these languages' communities. IGT typically consists
of (1) transcriptions, (2) morphological segmentation, (3) glosses, and (4)
free translations to a majority language. We propose Wav2Gloss: a task to
extract these four annotation components automatically from speech, and
introduce the first dataset to this end, Fieldwork: a corpus of speech with all
these annotations covering 37 languages with standard formatting and
train/dev/test splits. We compare end-to-end and cascaded Wav2Gloss methods,
with analysis suggesting that pre-trained decoders assist with translation and
glossing, that multi-task and multilingual approaches are underperformant, and
that end-to-end systems perform better than cascaded systems, despite the
text-only systems' advantages. We provide benchmarks to lay the ground work for
future research on IGT generation from speech.
\\ ( https://arxiv.org/abs/2403.13169 ,  7817kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13233
Date: Wed, 20 Mar 2024 01:46:06 GMT   (1388kb,D)

Title: Technical Report: Competition Solution For BetterMixture
Authors: Shuaijiang Zhao and Xiaoquan Fang
Categories: cs.CL
Comments: 6 pages
\\
  In the era of flourishing large-scale models, the challenge of selecting and
optimizing datasets from the vast and complex sea of data, to enhance the
performance of large language models within the constraints of limited
computational resources, has become paramount. This paper details our solution
for the BetterMixture challenge, which focuses on the fine-tuning data mixing
for large language models. Our approach, which secured third place,
incorporates data deduplication, low-level and high-level quality filtering,
and diversity selection. The foundation of our solution is Ke-Data-Juicer, an
extension of Data-Juicer, demonstrating its robust capabilities in handling and
optimizing data for large language models.
\\ ( https://arxiv.org/abs/2403.13233 ,  1388kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13240
Date: Wed, 20 Mar 2024 02:04:42 GMT   (9447kb,D)

Title: SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual
  Summarization
Authors: Jacob Parnell, Inigo Jauregi Unanue, Massimo Piccardi
Categories: cs.CL
Comments: Accepted to NAACL 2024
\\
  Cross-lingual summarization (XLS) generates summaries in a language different
from that of the input documents (e.g., English to Spanish), allowing speakers
of the target language to gain a concise view of their content. In the present
day, the predominant approach to this task is to take a performing, pretrained
multilingual language model (LM) and fine-tune it for XLS on the language pairs
of interest. However, the scarcity of fine-tuning samples makes this approach
challenging in some cases. For this reason, in this paper we propose revisiting
the summarize-and-translate pipeline, where the summarization and translation
tasks are performed in a sequence. This approach allows reusing the many,
publicly-available resources for monolingual summarization and translation,
obtaining a very competitive zero-shot performance. In addition, the proposed
pipeline is completely differentiable end-to-end, allowing it to take advantage
of few-shot fine-tuning, where available. Experiments over two contemporary and
widely adopted XLS datasets (CrossSum and WikiLingua) have shown the remarkable
zero-shot performance of the proposed approach, and also its strong few-shot
performance compared to an equivalent multilingual LM baseline, that the
proposed approach has been able to outperform in many languages with only 10%
of the fine-tuning samples.
\\ ( https://arxiv.org/abs/2403.13240 ,  9447kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13244
Date: Wed, 20 Mar 2024 02:15:55 GMT   (2170kb)

Title: Instruction Multi-Constraint Molecular Generation Using a
  Teacher-Student Large Language Model
Authors: Peng Zhou, Jianmin Wang, Chunyan Li, Zixu Wang, Yiping Liu, Siqi Sun,
  Jianxin Lin, Longyue Wang, Xiangxiang Zeng
Categories: cs.CL cs.AI
Comments: 25 pages, 4 figures
\\
  While various models and computational tools have been proposed for structure
and property analysis of molecules, generating molecules that conform to all
desired structures and properties remains a challenge. Here, we introduce a
multi-constraint molecular generation large language model, TSMMG, which, akin
to a student, incorporates knowledge from various small models and tools,
namely, the 'teachers'. To train TSMMG, we construct a large set of
text-molecule pairs by extracting molecular knowledge from these 'teachers',
enabling it to generate novel molecules that conform to the descriptions
through various text prompts. We experimentally show that TSMMG remarkably
performs in generating molecules meeting complex, natural language-described
property requirements across two-, three-, and four-constraint tasks, with an
average molecular validity of over 99% and success ratio of 88.08%, 65.27%, and
61.44%, respectively. The model also exhibits adaptability through zero-shot
testing, creating molecules that satisfy combinations of properties that have
not been encountered. It can comprehend text inputs with various language
styles, extending beyond the confines of outlined prompts, as confirmed through
empirical validation. Additionally, the knowledge distillation feature of TSMMG
contributes to the continuous enhancement of small models, while the innovative
approach to dataset construction effectively addresses the issues of data
scarcity and quality, which positions TSMMG as a promising tool in the domains
of drug discovery and materials science. Code is available at
https://github.com/HHW-zhou/TSMMG.
\\ ( https://arxiv.org/abs/2403.13244 ,  2170kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13250
Date: Wed, 20 Mar 2024 02:29:09 GMT   (927kb,D)

Title: Facilitating Pornographic Text Detection for Open-Domain Dialogue
  Systems via Knowledge Distillation of Large Language Models
Authors: Huachuan Qiu, Shuai Zhang, Hongliang He, Anqi Li, Zhenzhong Lan
Categories: cs.CL
Comments: Accepted to CSCWD 2024 (27th International Conference on Computer
  Supported Cooperative Work in Design). arXiv admin note: text overlap with
  arXiv:2309.09749
\\
  Pornographic content occurring in human-machine interaction dialogues can
cause severe side effects for users in open-domain dialogue systems. However,
research on detecting pornographic language within human-machine interaction
dialogues is an important subject that is rarely studied. To advance in this
direction, we introduce CensorChat, a dialogue monitoring dataset aimed at
detecting whether the dialogue session contains pornographic content. To this
end, we collect real-life human-machine interaction dialogues in the wild and
break them down into single utterances and single-turn dialogues, with the last
utterance spoken by the chatbot. We propose utilizing knowledge distillation of
large language models to annotate the dataset. Specifically, first, the raw
dataset is annotated by four open-source large language models, with the
majority vote determining the label. Second, we use ChatGPT to update the empty
label from the first step. Third, to ensure the quality of the validation and
test sets, we utilize GPT-4 for label calibration. If the current label does
not match the one generated by GPT-4, we employ a self-criticism strategy to
verify its correctness. Finally, to facilitate the detection of pornographic
text, we develop a series of text classifiers using a pseudo-labeled dataset.
Detailed data analysis demonstrates that leveraging knowledge distillation
techniques with large language models provides a practical and cost-efficient
method for developing pornographic text detectors.
\\ ( https://arxiv.org/abs/2403.13250 ,  927kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13253
Date: Wed, 20 Mar 2024 02:32:24 GMT   (570kb,D)

Title: Document Author Classification Using Parsed Language Structure
Authors: Todd K Moon, Jacob H. Gunther
Categories: cs.CL eess.AS
Journal-ref: International Journal on Natural Language Computing (IJNLC), Feb.
  24, 2024
\\
  Over the years there has been ongoing interest in detecting authorship of a
text based on statistical properties of the text, such as by using occurrence
rates of noncontextual words. In previous work, these techniques have been
used, for example, to determine authorship of all of \emph{The Federalist
Papers}. Such methods may be useful in more modern times to detect fake or AI
authorship. Progress in statistical natural language parsers introduces the
possibility of using grammatical structure to detect authorship. In this paper
we explore a new possibility for detecting authorship using grammatical
structural information extracted using a statistical natural language parser.
This paper provides a proof of concept, testing author classification based on
grammatical structure on a set of "proof texts," The Federalist Papers and
Sanditon which have been as test cases in previous authorship detection
studies. Several features extracted from the statistical natural language
parser were explored: all subtrees of some depth from any level; rooted
subtrees of some depth, part of speech, and part of speech by level in the
parse tree. It was found to be helpful to project the features into a lower
dimensional space. Statistical experiments on these documents demonstrate that
information from a statistical parser can, in fact, assist in distinguishing
authors.
\\ ( https://arxiv.org/abs/2403.13253 ,  570kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13257
Date: Wed, 20 Mar 2024 02:38:01 GMT   (2704kb,D)

Title: Arcee's MergeKit: A Toolkit for Merging Large Language Models
Authors: Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers,
  Vlad Karpukhin, Brian Benedict, Mark McQuade, Jacob Solawetz
Categories: cs.CL cs.AI cs.LG
Comments: 11 pages, 4 figures
\\
  The rapid expansion of the open-source language model landscape presents an
opportunity to merge the competencies of these model checkpoints by combining
their parameters. Advances in transfer learning, the process of fine-tuning
pre-trained models for specific tasks, has resulted in the development of vast
amounts of task-specific models, typically specialized in individual tasks and
unable to utilize each other's strengths. Model merging facilitates the
creation of multitask models without the need for additional training, offering
a promising avenue for enhancing model performance and versatility. By
preserving the intrinsic capabilities of the original models, model merging
addresses complex challenges in AI - including the difficulties of catastrophic
forgetting and multi-task learning. To support this expanding area of research,
we introduce MergeKit, a comprehensive, open-source library designed to
facilitate the application of model merging strategies. MergeKit offers an
extensible framework to efficiently merge models on any hardware, providing
utility to researchers and practitioners. To date, thousands of models have
been merged by the open-source community, leading to the creation of some of
the worlds most powerful open-source model checkpoints, as assessed by the Open
LLM Leaderboard. The library is accessible at
https://github.com/arcee-ai/MergeKit.
\\ ( https://arxiv.org/abs/2403.13257 ,  2704kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13269
Date: Wed, 20 Mar 2024 03:07:50 GMT   (7708kb,D)

Title: AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient
  Fine-Tuning of Large Models
Authors: Zeyu Liu, Souvik Kundu, Anni Li, Junrui Wan, Lianghao Jiang, Peter
  Anthony Beerel
Categories: cs.CL cs.AI cs.LG
Comments: 5 pages, 5 figures
\\
  We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as
Adaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each
pre-trained frozen weight tensor, we add a parallel path of trainable low-rank
matrices, namely a down-projection and an up-projection matrix, each of which
is followed by a feature transformation vector. Based on a novel freezing
score, we the incrementally freeze these projection matrices during fine-tuning
to reduce the computation and alleviate over-fitting. Our experimental results
demonstrate that we can achieve state-of-the-art performance with an average
improvement of up to $0.85\%$ as evaluated on GLUE benchmark while yeilding up
to $9.5\times$ fewer average trainable parameters. While compared in terms of
runtime, AFLoRA can yield up to $1.86\times$ improvement as opposed to similar
PEFT alternatives. Besides the practical utility of our approach, we provide
insights on the trainability requirements of LoRA paths at different modules
and the freezing schedule for the different projection matrices. Code will be
released.
\\ ( https://arxiv.org/abs/2403.13269 ,  7708kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13312
Date: Wed, 20 Mar 2024 05:29:06 GMT   (2544kb,D)

Title: LeanReasoner: Boosting Complex Logical Reasoning with Lean
Authors: Dongwei Jiang, Marcio Fonseca, Shay B. Cohen
Categories: cs.CL
Comments: Accepted to NAACL 2024 main conference
\\
  Large language models (LLMs) often struggle with complex logical reasoning
due to logical inconsistencies and the inherent difficulty of such reasoning.
We use Lean, a theorem proving framework, to address these challenges. By
formalizing logical reasoning problems into theorems within Lean, we can solve
them by proving or disproving the corresponding theorems. This method reduces
the risk of logical inconsistencies with the help of Lean's symbolic solver. It
also enhances our ability to treat complex reasoning tasks by using Lean's
extensive library of theorem proofs. Our method achieves state-of-the-art
performance on the FOLIO dataset and achieves performance near this level on
ProofWriter. Notably, these results were accomplished by fine-tuning on fewer
than 100 in-domain samples for each dataset.
\\ ( https://arxiv.org/abs/2403.13312 ,  2544kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13334
Date: Wed, 20 Mar 2024 06:37:59 GMT   (364kb)

Title: Hyacinth6B: A large language model for Traditional Chinese
Authors: Chih-Wei Song, Yin-Te Tsai
Categories: cs.CL cs.AI
Comments: 14pages
\\
  This research's primary motivation of this study is to address the high
hardware and computational demands typically associated with LLMs.Therefore,our
goal is to find a balance between model lightness and performance,striving to
maximize performance while using a comparatively lightweight model. Hyacinth6B
was developed with this objective in mind,aiming to fully leverage the core
capabilities of LLMs without incurring substantial resource costs, effectively
pushing the boundaries of smaller model's performance. The training approach
involves parameter efficient finetuning using the LoRA method.
\\ ( https://arxiv.org/abs/2403.13334 ,  364kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13368
Date: Wed, 20 Mar 2024 08:01:22 GMT   (921kb,D)

Title: Computational Models to Study Language Processing in the Human Brain: A
  Survey
Authors: Shaonan Wang, Jingyuan Sun, Yunhao Zhang, Nan Lin, Marie-Francine
  Moens, Chengqing Zong
Categories: cs.CL cs.AI
\\
  Despite differing from the human language processing mechanism in
implementation and algorithms, current language models demonstrate remarkable
human-like or surpassing language capabilities. Should computational language
models be employed in studying the brain, and if so, when and how? To delve
into this topic, this paper reviews efforts in using computational models for
brain research, highlighting emerging trends. To ensure a fair comparison, the
paper evaluates various computational models using consistent metrics on the
same dataset. Our analysis reveals that no single model outperforms others on
all datasets, underscoring the need for rich testing datasets and rigid
experimental control to draw robust conclusions in studies involving
computational models.
\\ ( https://arxiv.org/abs/2403.13368 ,  921kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13369
Date: Wed, 20 Mar 2024 08:01:33 GMT   (2933kb,D)

Title: Clinical information extraction for Low-resource languages with Few-shot
  learning using Pre-trained language models and Prompting
Authors: Phillip Richter-Pechanski, Philipp Wiesenbach, Dominic M. Schwab,
  Christina Kiriakou, Nicolas Geis, Christoph Dieterich, Anette Frank
Categories: cs.CL cs.AI cs.LG
\\
  Automatic extraction of medical information from clinical documents poses
several challenges: high costs of required clinical expertise, limited
interpretability of model predictions, restricted computational resources and
privacy regulations. Recent advances in domain-adaptation and prompting methods
showed promising results with minimal training data using lightweight masked
language models, which are suited for well-established interpretability
methods. We are first to present a systematic evaluation of these methods in a
low-resource setting, by performing multi-class section classification on
German doctor's letters. We conduct extensive class-wise evaluations supported
by Shapley values, to validate the quality of our small training data set and
to ensure the interpretability of model predictions. We demonstrate that a
lightweight, domain-adapted pretrained model, prompted with just 20 shots,
outperforms a traditional classification model by 30.5% accuracy. Our results
serve as a process-oriented guideline for clinical information extraction
projects working with low-resource.
\\ ( https://arxiv.org/abs/2403.13369 ,  2933kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13372
Date: Wed, 20 Mar 2024 08:08:54 GMT   (51kb,D)

Title: LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models
Authors: Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo
Categories: cs.CL cs.AI
Comments: 12 pages, preprint
\\
  Efficient fine-tuning is vital for adapting large language models (LLMs) to
downstream tasks. However, it requires non-trivial efforts to implement these
methods on different models. We present LlamaFactory, a unified framework that
integrates a suite of cutting-edge efficient training methods. It allows users
to flexibly customize the fine-tuning of 100+ LLMs without the need for coding
through the built-in web UI LlamaBoard. We empirically validate the efficiency
and effectiveness of our framework on language modeling and text generation
tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and
already received over 13,000 stars and 1,600 forks.
\\ ( https://arxiv.org/abs/2403.13372 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13485
Date: Wed, 20 Mar 2024 10:40:01 GMT   (529kb,D)

Title: An Entropy-based Text Watermarking Detection Method
Authors: Yijian Lu, Aiwei Liu, Dianzhi Yu, Jingjing Li, Irwin King
Categories: cs.CL
Comments: 8 pages, 5 figures, submitted to ARR Feb 2024
\\
  Currently, text watermarking algorithms for large language models (LLMs) can
embed hidden features to texts generated by LLMs to facilitate subsequent
detection, thus alleviating the problem of misuse of LLMs. Although the current
text watermarking algorithms perform well in most high-entropy scenarios, its
performance in low-entropy scenarios still needs to be improved. In this work,
we proposed that the influence of token entropy should be fully considered in
the watermark detection process, that is, the weight of each token should be
adjusted according to its entropy during watermark detection, rather than
setting the weight of all tokens to the same value as in previous methods.
Specifically, we proposed an Entropy-based Watermark Detection (EWD) that gives
higher-entropy tokens higher weights during watermark detection, so as to
better reflect the degree of watermarking. Furthermore, the proposed detection
process is training-free and fully automated. %In actual detection, we use a
proxy-LLM to calculate the entropy of each token, without the need to use the
original LLM. In the experiment, we found that our method can achieve better
detection performance in low-entropy scenarios, and our method is also general
and can be applied to texts with different entropy distributions. Our code and
data will be available online.
\\ ( https://arxiv.org/abs/2403.13485 ,  529kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13514
Date: Wed, 20 Mar 2024 11:30:45 GMT   (221kb,D)

Title: How Gender Interacts with Political Values: A Case Study on Czech BERT
  Models
Authors: Adnan Al Ali, Jind\v{r}ich Libovick\'y
Categories: cs.CL cs.CY
Comments: 11 pages, 2 figures; LREC-COLING 2024
\\
  Neural language models, which reach state-of-the-art results on most natural
language processing tasks, are trained on large text corpora that inevitably
contain value-burdened content and often capture undesirable biases, which the
models reflect. This case study focuses on the political biases of pre-trained
encoders in Czech and compares them with a representative value survey. Because
Czech is a gendered language, we also measure how the grammatical gender
coincides with responses to men and women in the survey. We introduce a novel
method for measuring the model's perceived political values. We find that the
models do not assign statement probability following value-driven reasoning,
and there is no systematic difference between feminine and masculine sentences.
We conclude that BERT-sized models do not manifest systematic alignment with
political values and that the biases observed in the models are rather due to
superficial imitation of training data patterns than systematic value beliefs
encoded in the models.
\\ ( https://arxiv.org/abs/2403.13514 ,  221kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13537
Date: Wed, 20 Mar 2024 12:14:54 GMT   (3172kb,D)

Title: What explains the success of cross-modal fine-tuning with ORCA?
Authors: Paloma Garc\'ia-de-Herreros, Vagrant Gautam, Philipp Slusallek,
  Dietrich Klakow, Marius Mosbach
Categories: cs.CL cs.AI cs.CV cs.LG
\\
  ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning,
i.e., applying pre-trained transformer models to modalities beyond their
training data. The technique consists primarily of training an embedder and
fine-tuning the embedder and model. Despite its high performance on a variety
of downstream tasks, we do not understand precisely how each of these
components contribute to ORCA's success. Therefore, we run a series of
ablations and find that embedder training does not help 2D tasks at all,
contrary to what the original paper posits. In 1D tasks, some amount of
embedder training is necessary but more is not better. In 4 out of 6 datasets
we experiment with, it is model fine-tuning that makes the biggest difference.
Through our ablations and baselines, we contribute a better understanding of
the individual components of ORCA.
\\ ( https://arxiv.org/abs/2403.13537 ,  3172kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13560
Date: Wed, 20 Mar 2024 12:52:38 GMT   (1859kb,D)

Title: eRST: A Signaled Graph Theory of Discourse Relations and Organization
Authors: Amir Zeldes, Tatsuya Aoyama, Yang Janet Liu, Siyao Peng, Debopam Das,
  Luke Gessler
Categories: cs.CL
\\
  In this article we present Enhanced Rhetorical Structure Theory (eRST), a new
theoretical framework for computational discourse analysis, based on an
expansion of Rhetorical Structure Theory (RST). The framework encompasses
discourse relation graphs with tree-breaking, nonprojective and concurrent
relations, as well as implicit and explicit signals which give explainable
rationales to our analyses. We survey shortcomings of RST and other existing
frameworks, such as Segmented Discourse Representation Theory (SDRT), the Penn
Discourse Treebank (PDTB) and Discourse Dependencies, and address these using
constructs in the proposed theory. We provide annotation, search and
visualization tools for data, and present and evaluate a freely available
corpus of English annotated according to our framework, encompassing 12 spoken
and written genres with over 200K tokens. Finally, we discuss automatic
parsing, evaluation metrics and applications for data in our framework.
\\ ( https://arxiv.org/abs/2403.13560 ,  1859kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13578
Date: Wed, 20 Mar 2024 13:24:41 GMT   (639kb,D)

Title: Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for
  Counselor Reflection Generation
Authors: Do June Min, Veronica Perez-Rosas, Kenneth Resnicow and Rada Mihalcea
Categories: cs.CL cs.LG
\\
  In this paper, we study the problem of multi-reward reinforcement learning to
jointly optimize for multiple text qualities for natural language generation.
We focus on the task of counselor reflection generation, where we optimize the
generators to simultaneously improve the fluency, coherence, and reflection
quality of generated counselor responses. We introduce two novel bandit
methods, DynaOpt and C-DynaOpt, which rely on the broad strategy of combining
rewards into a single value and optimizing them simultaneously. Specifically,
we employ non-contextual and contextual multi-arm bandits to dynamically adjust
multiple reward weights during training. Through automatic and manual
evaluations, we show that our proposed techniques, DynaOpt and C-DynaOpt,
outperform existing naive and bandit baselines, showcasing their potential for
enhancing language models.
\\ ( https://arxiv.org/abs/2403.13578 ,  639kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13590
Date: Wed, 20 Mar 2024 13:38:07 GMT   (248kb,D)

Title: Teacher-Student Training for Debiasing: General Permutation Debiasing
  for Large Language Models
Authors: Adian Liusie, Yassir Fathullah, Mark J. F. Gales
Categories: cs.CL
\\
  Large Language Models (LLMs) have demonstrated impressive zero-shot
capabilities and versatility in NLP tasks, however they sometimes fail to
maintain crucial invariances for specific tasks. One example is permutation
sensitivity, where LLMs' outputs may significantly vary depending on the order
of the input options. While debiasing techniques can mitigate these issues, and
yield better performance and reliability, they often come with a high
computational cost at inference. This paper addresses this inefficiency at
inference time. The aim is to distill the capabilities of a computationally
intensive, debiased, teacher model into a more compact student model. We
explore two variants of student models: one based on pure distillation, and the
other on an error-correction approach for more complex tasks, where the student
corrects a single biased decision from the teacher to achieve a debiased
output. Our approach is general and can be applied to both black-box and
white-box LLMs. Furthermore, we demonstrate that our compact, encoder-only
student models can outperform their larger, biased teacher counterparts,
achieving better results with significantly fewer parameters.
\\ ( https://arxiv.org/abs/2403.13590 ,  248kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13592
Date: Wed, 20 Mar 2024 13:42:57 GMT   (4766kb,D)

Title: Llama meets EU: Investigating the European Political Spectrum through
  the Lens of LLMs
Authors: Ilias Chalkidis and Stephanie Brandl
Categories: cs.CL
Comments: accepted to NAACL 2024 as a short paper
\\
  Instruction-finetuned Large Language Models inherit clear political leanings
that have been shown to influence downstream task performance. We expand this
line of research beyond the two-party system in the US and audit Llama Chat in
the context of EU politics in various settings to analyze the model's political
knowledge and its ability to reason in context. We adapt, i.e., further
fine-tune, Llama Chat on speeches of individual euro-parties from debates in
the European Parliament to reevaluate its political leaning based on the EUandI
questionnaire. Llama Chat shows considerable knowledge of national parties'
positions and is capable of reasoning in context. The adapted, party-specific,
models are substantially re-aligned towards respective positions which we see
as a starting point for using chat-based LLMs as data-driven conversational
engines to assist research in political science.
\\ ( https://arxiv.org/abs/2403.13592 ,  4766kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13638
Date: Wed, 20 Mar 2024 14:41:01 GMT   (8205kb,D)

Title: Do Not Worry if You Do Not Have Data: Building Pretrained Language
  Models Using Translationese
Authors: Meet Doshi, Raj Dabre, Pushpak Bhattacharyya
Categories: cs.CL
\\
  In this paper, we explore the utility of \textit{Translationese} as synthetic
data created using machine translation for pre-training language models (LMs).
Pre-training requires vast amounts of monolingual data, which is mostly
unavailable for languages other than English. Recently, there has been a
growing interest in using synthetic data to address this data scarcity. We take
the case of English and Indic languages and translate web-crawled monolingual
documents (clean) into the target language. Then, we train language models
containing 28M and 85M parameters on this translationese data (synthetic). We
show that their performance on downstream natural language understanding and
generative tasks is only 3.56\% poorer on NLU tasks and 1.51\% on NLG tasks
than LMs pre-trained on clean data. Further, we propose the use of lightweight
\textit{TinyLMs} pre-trained on clean data to filter synthetic data efficiently
which significantly improves the performance of our models. We also find that
LMs trained on synthetic data strongly benefit from extended pretraining on a
tiny fraction (10\%) of clean data. We release the data we collected and
created as a part of this work, \textit{IndicMonoDoc}, the largest collection
of monolingual document-level corpora, which we hope will help bridge the gap
between English and non-English performance for large language models.
\\ ( https://arxiv.org/abs/2403.13638 ,  8205kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13666
Date: Wed, 20 Mar 2024 15:20:30 GMT   (9115kb,D)

Title: Grounding Spatial Relations in Text-Only Language Models
Authors: Gorka Azkune, Ander Salaberria, Eneko Agirre
Categories: cs.CL
Comments: Accepted in Neural Networks
DOI: 10.1016/j.neunet.2023.11.031
\\
  This paper shows that text-only Language Models (LM) can learn to ground
spatial relations like "left of" or "below" if they are provided with explicit
location information of objects and they are properly trained to leverage those
locations. We perform experiments on a verbalized version of the Visual Spatial
Reasoning (VSR) dataset, where images are coupled with textual statements which
contain real or fake spatial relations between two objects of the image. We
verbalize the images using an off-the-shelf object detector, adding location
tokens to every object label to represent their bounding boxes in textual form.
Given the small size of VSR, we do not observe any improvement when using
locations, but pretraining the LM over a synthetic dataset automatically
derived by us improves results significantly when using location tokens. We
thus show that locations allow LMs to ground spatial relations, with our
text-only LMs outperforming Vision-and-Language Models and setting the new
state-of-the-art for the VSR dataset. Our analysis show that our text-only LMs
can generalize beyond the relations seen in the synthetic dataset to some
extent, learning also more useful information than that encoded in the spatial
rules we used to create the synthetic dataset itself.
\\ ( https://arxiv.org/abs/2403.13666 ,  9115kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13679
Date: Wed, 20 Mar 2024 15:38:36 GMT   (8386kb,D)

Title: RoleInteract: Evaluating the Social Interaction of Role-Playing Agents
Authors: Hongzhan Chen,Hehong Chen,Ming Yan,Wenshen Xu,Xing Gao,Weizhou
  Shen,Xiaojun Quan,Chenliang Li,Ji Zhang,Fei Huang,Jingren Zhou
Categories: cs.CL
\\
  Large language models (LLMs) have advanced the development of various AI
conversational agents, including role-playing conversational agents that mimic
diverse characters and human behaviors. While prior research has predominantly
focused on enhancing the conversational capability, role-specific knowledge,
and stylistic attributes of these agents, there has been a noticeable gap in
assessing their social intelligence. In this paper, we introduce RoleInteract,
the first benchmark designed to systematically evaluate the sociality of
role-playing conversational agents at both individual and group levels of
social interactions. The benchmark is constructed from a variety of sources and
covers a wide range of 500 characters and over 6,000 question prompts and
30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations
on this benchmark using mainstream open-source and closed-source LLMs. We find
that agents excelling in individual level does not imply their proficiency in
group level. Moreover, the behavior of individuals may drift as a result of the
influence exerted by other agents within the group. Experimental results on
RoleInteract confirm its significance as a testbed for assessing the social
interaction of role-playing conversational agents. The benchmark is publicly
accessible at https://github.com/X-PLUG/RoleInteract.
\\ ( https://arxiv.org/abs/2403.13679 ,  8386kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13681
Date: Wed, 20 Mar 2024 15:39:54 GMT   (6898kb)

Title: PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned
  Language Model for Indian Legal Case Documents
Authors: Mitodru Niyogi, Arnab Bhattacharya
Categories: cs.CL cs.AI cs.LG
\\
  In this paper, we present PARAMANU-AYN, a language model based exclusively on
case documents of the Supreme Court of India, the Constitution of India, and
the Indian Penal Code. The novel Auto Regressive (AR) decoder based model is
pretrained from scratch at a context size of 8192. We evaluated our pretrained
legal model on perplexity metrics. We also instruction-tuned our pretrained
model on a set of 10,763 instructions covering various legal tasks such as
legal reasoning, judgement explanation, legal clause generation, legal
drafting, legal contract drafting, case summarization, constitutional
question-answering, etc. We also evaluated the responses of prompts for
instruction-tuned models by GPT-3.5-Turbo on clarity, relevance, completeness,
and legal reasoning metrics in a scale of 10. Our model can be run on CPU and
achieved 42.46 tokens/sec CPU inference speed. We found that our models,
despite not being pretrained on legal books, various legal contracts, and legal
documents, were able to learn the domain knowledge required for drafting
various legal contracts and legal clauses, and generalize to draft legal
contracts and legal clauses with limited instruction tuning. Hence, we conclude
that for a strong domain-specialized generative language model (such as legal),
very large amounts of data are not required to develop models from scratch. We
believe that this work is the first attempt to make a dedicated generative
legal language model from scratch for Indian Supreme Court jurisdiction or in
legal NLP overall. We plan to release our Paramanu-Ayn model at
https://www.bharatgpts.com.
\\ ( https://arxiv.org/abs/2403.13681 ,  6898kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13737
Date: Wed, 20 Mar 2024 16:43:42 GMT   (846kb)

Title: EthioLLM: Multilingual Large Language Models for Ethiopian Languages
  with Task Evaluation
Authors: Atnafu Lambebo Tonja, Israel Abebe Azime, Tadesse Destaw Belay, Mesay
  Gemeda Yigezu, Moges Ahmed Mehamed, Abinew Ali Ayele, Ebrahim Chekol Jibril,
  Michael Melese Woldeyohannis, Olga Kolesnikova, Philipp Slusallek, Dietrich
  Klakow, Shengwu Xiong, Seid Muhie Yimam
Categories: cs.CL
Comments: Accepted at LREC-Coling 2024
\\
  Large language models (LLMs) have gained popularity recently due to their
outstanding performance in various downstream Natural Language Processing (NLP)
tasks. However, low-resource languages are still lagging behind current
state-of-the-art (SOTA) developments in the field of NLP due to insufficient
resources to train LLMs. Ethiopian languages exhibit remarkable linguistic
diversity, encompassing a wide array of scripts, and are imbued with profound
religious and cultural significance. This paper introduces EthioLLM --
multilingual large language models for five Ethiopian languages (Amharic,
Ge'ez, Afan Oromo, Somali, and Tigrinya) and English, and Ethiobenchmark -- a
new benchmark dataset for various downstream NLP tasks. We evaluate the
performance of these models across five downstream NLP tasks. We open-source
our multilingual language models, new benchmark datasets for various downstream
tasks, and task-specific fine-tuned language models and discuss the performance
of the models. Our dataset and models are available at the
https://huggingface.co/EthioNLP repository.
\\ ( https://arxiv.org/abs/2403.13737 ,  846kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13754
Date: Wed, 20 Mar 2024 17:01:56 GMT   (8504kb,D)

Title: Different Tokenization Schemes Lead to Comparable Performance in Spanish
  Number Agreement
Authors: Catherine Arnett, Pamela D. Rivi\`ere, Tyler A. Chang, Sean Trott
Categories: cs.CL
\\
  The relationship between language model tokenization and performance is an
open area of research. Here, we investigate how different tokenization schemes
impact number agreement in Spanish plurals. We find that
morphologically-aligned tokenization performs similarly to other tokenization
schemes, even when induced artificially for words that would not be tokenized
that way during training. We then present exploratory analyses demonstrating
that language model embeddings for different plural tokenizations have similar
distributions along the embedding space axis that maximally distinguishes
singular and plural nouns. Our results suggest that morphologically-aligned
tokenization is a viable tokenization approach, and existing models already
generalize some morphological patterns to new items. However, our results
indicate that morphological tokenization is not strictly required for
performance.
\\ ( https://arxiv.org/abs/2403.13754 ,  8504kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13780
Date: Wed, 20 Mar 2024 17:42:08 GMT   (1606kb,D)

Title: Information-Theoretic Distillation for Reference-less Summarization
Authors: Jaehun Jung, Ximing Lu, Liwei Jiang, Faeze Brahman, Peter West, Pang
  Wei Koh, Yejin Choi
Categories: cs.CL cs.AI
\\
  The current winning recipe for automatic summarization is using proprietary
large-scale language models (LLMs) such as ChatGPT as is, or imitation learning
from them as teacher models. While increasingly ubiquitous dependence on such
large-scale language models is convenient, there remains an important question
of whether small-scale models could have achieved competitive results, if we
were to seek an alternative learning method -- that allows for a more
cost-efficient, controllable, yet powerful summarizer. We present InfoSumm, a
novel framework to distill a powerful summarizer based on the
information-theoretic objective for summarization, without relying on either
the LLM's capability or human-written references. To achieve this, we first
propose a novel formulation of the desiderata of summarization (saliency,
faithfulness and brevity) through the lens of mutual information between the
original document and the summary. Based on this formulation, we start off from
Pythia-2.8B as the teacher model, which is not yet capable of summarization,
then self-train the model to optimize for the information-centric measures of
ideal summaries. Distilling from the improved teacher, we arrive at a compact
but powerful summarizer with only 568M parameters that performs competitively
against ChatGPT, without ever relying on ChatGPT's capabilities. Extensive
analysis demonstrates that our approach outperforms in-domain supervised models
in human evaluation, let alone state-of-the-art unsupervised methods, and wins
over ChatGPT in controllable summarization.
\\ ( https://arxiv.org/abs/2403.13780 ,  1606kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13786
Date: Wed, 20 Mar 2024 17:47:49 GMT   (904kb,D)

Title: Chain-of-Interaction: Enhancing Large Language Models for Psychiatric
  Behavior Understanding by Dyadic Contexts
Authors: Guangzeng Han and Weisi Liu and Xiaolei Huang and Brian Borsari
Categories: cs.CL
Comments: Accepted to IEEE ICHI 2024
\\
  Automatic coding patient behaviors is essential to support decision making
for psychotherapists during the motivational interviewing (MI), a collaborative
communication intervention approach to address psychiatric issues, such as
alcohol and drug addiction. While the behavior coding task has rapidly adapted
machine learning to predict patient states during the MI sessions, lacking of
domain-specific knowledge and overlooking patient-therapist interactions are
major challenges in developing and deploying those models in real practice. To
encounter those challenges, we introduce the Chain-of-Interaction (CoI)
prompting method aiming to contextualize large language models (LLMs) for
psychiatric decision support by the dyadic interactions. The CoI prompting
approach systematically breaks down the coding task into three key reasoning
steps, extract patient engagement, learn therapist question strategies, and
integrates dyadic interactions between patients and therapists. This approach
enables large language models to leverage the coding scheme, patient state, and
domain knowledge for patient behavioral coding. Experiments on real-world
datasets can prove the effectiveness and flexibility of our prompting method
with multiple state-of-the-art LLMs over existing prompting baselines. We have
conducted extensive ablation analysis and demonstrate the critical role of
dyadic interactions in applying LLMs for psychotherapy behavior understanding.
\\ ( https://arxiv.org/abs/2403.13786 ,  904kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13799
Date: Wed, 20 Mar 2024 17:55:35 GMT   (283kb,D)

Title: Reverse Training to Nurse the Reversal Curse
Authors: Olga Golovneva, Zeyuan Allen-Zhu, Jason Weston, Sainbayar Sukhbaatar
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) have a surprising failure: when trained on "A
has a feature B", they do not generalize to "B is a feature of A", which is
termed the Reversal Curse. Even when training with trillions of tokens this
issue still appears due to Zipf's law - hence even if we train on the entire
internet. This work proposes an alternative training scheme, called reverse
training, whereby all words are used twice, doubling the amount of available
tokens. The LLM is trained in both forward and reverse directions by reversing
the training strings while preserving (i.e., not reversing) chosen substrings,
such as entities. We show that data-matched reverse-trained models provide
superior performance to standard models on standard tasks, and compute-matched
reverse-trained models provide far superior performance on reversal tasks,
helping resolve the reversal curse issue.
\\ ( https://arxiv.org/abs/2403.13799 ,  283kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12969
Date: Tue, 9 Jan 2024 00:07:36 GMT   (1257kb,D)

Title: Entangling Machine Learning with Quantum Tensor Networks
Authors: Constantijn van der Poel, Dan Zhao
Categories: cs.LG quant-ph
Comments: See source code at https://github.com/ConstantijnvdP/eidolon
\\
  This paper examines the use of tensor networks, which can efficiently
represent high-dimensional quantum states, in language modeling. It is a
distillation and continuation of the work done in (van der Poel, 2023). To do
so, we will abstract the problem down to modeling Motzkin spin chains, which
exhibit long-range correlations reminiscent of those found in language. The
Matrix Product State (MPS), also known as the tensor train, has a bond
dimension which scales as the length of the sequence it models. To combat this,
we use the factored core MPS, whose bond dimension scales sub-linearly. We find
that the tensor models reach near perfect classifying ability, and maintain a
stable level of performance as the number of valid training examples is
decreased.
\\ ( https://arxiv.org/abs/2403.12969 ,  1257kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12993
Date: Tue, 5 Mar 2024 08:04:01 GMT   (2073kb)

Title: Simple Full-Spectrum Correlated k-Distribution Model based on Multilayer
  Perceptron
Authors: Xin Wang, Yucheng Kuang, Chaojun Wang, Hongyuan Di, Boshu He
Categories: cs.LG
\\
  While neural networks have been successfully applied to the full-spectrum
k-distribution (FSCK) method at a large range of thermodynamics with k-values
predicted by a trained multilayer perceptron (MLP) model, the required a-values
still need to be calculated on-the-fly, which theoretically degrades the FSCK
method and may lead to errors. On the other hand, too complicated structure of
the current MLP model inevitably slows down the calculation efficiency.
Therefore, to compensate among accuracy, efficiency and storage, the simple MLP
designed based on the nature of FSCK method are developed, i.e., the simple
FSCK MLP (SFM) model, from which those correlated k-values and corresponding
ka-values can be efficiently obtained. Several test cases have been carried out
to compare the developed SFM model and other FSCK tools including look-up
tables and traditional FSCK MLP (TFM) model. Results show that the SFM model
can achieve excellent accuracy that is even better than look-up tables at a
tiny computational cost that is far less than that of TFM model. Considering
accuracy, efficiency and portability, the SFM model is not only an excellent
tool for the prediction of spectral properties, but also provides a method to
reduce the errors due to nonlinear effects.
\\ ( https://arxiv.org/abs/2403.12993 ,  2073kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13000
Date: Tue, 12 Mar 2024 16:25:38 GMT   (3034kb,D)

Title: Duwak: Dual Watermarks in Large Language Models
Authors: Chaoyi Zhu, Jeroen Galjaard, Pin-Yu Chen, Lydia Y. Chen
Categories: cs.LG cs.AI cs.CL cs.CR
\\
  As large language models (LLM) are increasingly used for text generation
tasks, it is critical to audit their usages, govern their applications, and
mitigate their potential harms. Existing watermark techniques are shown
effective in embedding single human-imperceptible and machine-detectable
patterns without significantly affecting generated text quality and semantics.
However, the efficiency in detecting watermarks, i.e., the minimum number of
tokens required to assert detection with significance and robustness against
post-editing, is still debatable. In this paper, we propose, Duwak, to
fundamentally enhance the efficiency and quality of watermarking by embedding
dual secret patterns in both token probability distribution and sampling
schemes. To mitigate expression degradation caused by biasing toward certain
tokens, we design a contrastive search to watermark the sampling scheme, which
minimizes the token repetition and enhances the diversity. We theoretically
explain the interdependency of the two watermarks within Duwak. We evaluate
Duwak extensively on Llama2 under various post-editing attacks, against four
state-of-the-art watermarking techniques and combinations of them. Our results
show that Duwak marked text achieves the highest watermarked text quality at
the lowest required token count for detection, up to 70% tokens less than
existing approaches, especially under post paraphrasing.
\\ ( https://arxiv.org/abs/2403.13000 ,  3034kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13001
Date: Wed, 13 Mar 2024 01:29:40 GMT   (8622kb,D)

Title: Fundamental Components of Deep Learning: A category-theoretic approach
Authors: Bruno Gavranovi\'c
Categories: cs.LG cs.AI math.CT
Comments: PhD Thesis defended at University of Strathclyde
\\
  Deep learning, despite its remarkable achievements, is still a young field.
Like the early stages of many scientific disciplines, it is marked by the
discovery of new phenomena, ad-hoc design decisions, and the lack of a uniform
and compositional mathematical foundation. From the intricacies of the
implementation of backpropagation, through a growing zoo of neural network
architectures, to the new and poorly understood phenomena such as double
descent, scaling laws or in-context learning, there are few unifying principles
in deep learning. This thesis develops a novel mathematical foundation for deep
learning based on the language of category theory. We develop a new framework
that is a) end-to-end, b) unform, and c) not merely descriptive, but
prescriptive, meaning it is amenable to direct implementation in programming
languages with sufficient features. We also systematise many existing
approaches, placing many existing constructions and concepts from the
literature under the same umbrella. In Part I we identify and model two main
properties of deep learning systems parametricity and bidirectionality by we
expand on the previously defined construction of actegories and Para to study
the former, and define weighted optics to study the latter. Combining them
yields parametric weighted optics, a categorical model of artificial neural
networks, and more. Part II justifies the abstractions from Part I, applying
them to model backpropagation, architectures, and supervised learning. We
provide a lens-theoretic axiomatisation of differentiation, covering not just
smooth spaces, but discrete settings of boolean circuits as well. We survey
existing, and develop new categorical models of neural network architectures.
We formalise the notion of optimisers and lastly, combine all the existing
concepts together, providing a uniform and compositional framework for
supervised learning.
\\ ( https://arxiv.org/abs/2403.13001 ,  8622kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13032
Date: Tue, 19 Mar 2024 09:33:07 GMT   (1422kb,D)

Title: Hybrid Unsupervised Learning Strategy for Monitoring Industrial Batch
  Processes
Authors: Christian W. Frey
Categories: cs.LG cs.SY eess.SP eess.SY
Comments: 6 pages, 4 figures, \c{opyright} 2024 Christian W. frey. This work
  has been accepted to IFAC for publication under a Creative Commons Licence
  CC-BY-NC-ND
\\
  Industrial production processes, especially in the pharmaceutical industry,
are complex systems that require continuous monitoring to ensure efficiency,
product quality, and safety. This paper presents a hybrid unsupervised learning
strategy (HULS) for monitoring complex industrial processes. Addressing the
limitations of traditional Self-Organizing Maps (SOMs), especially in scenarios
with unbalanced data sets and highly correlated process variables, HULS
combines existing unsupervised learning techniques to address these challenges.
To evaluate the performance of the HULS concept, comparative experiments are
performed based on a laboratory batch
\\ ( https://arxiv.org/abs/2403.13032 ,  1422kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13037
Date: Tue, 19 Mar 2024 14:11:20 GMT   (3915kb,D)

Title: BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient
  Low-Rank Adaptation of Large Pre-trained Models
Authors: Rushi Qiang, Ruiyi Zhang, Pengtao Xie
Categories: cs.LG cs.CL
\\
  Low-rank adaptation (LoRA) is a popular method for fine-tuning large-scale
pre-trained models in downstream tasks by learning low-rank incremental
matrices. Though LoRA and its variants effectively reduce the number of
trainable parameters compared to full fine-tuning methods, they often overfit
training data, resulting in sub-optimal generalization on test data. To address
this problem, we introduce BiLoRA, an overfitting-alleviating fine-tuning
approach based on bi-level optimization (BLO). BiLoRA employs pseudo singular
value decomposition to parameterize low-rank incremental matrices and splits
the training of pseudo singular vectors and values across two different subsets
of training data. This division, embedded within separate levels of the BLO
framework, mitigates the risk of overfitting to a single dataset. Tested on ten
datasets covering natural language understanding and generation tasks and
applied to various well-known large pre-trained models, BiLoRA significantly
outperforms LoRA methods and other fine-tuning approaches, with similar amounts
of trainable parameters.
\\ ( https://arxiv.org/abs/2403.13037 ,  3915kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13091
Date: Tue, 19 Mar 2024 18:40:50 GMT   (86kb,D)

Title: JaxUED: A simple and useable UED library in Jax
Authors: Samuel Coward, Michael Beukman, Jakob Foerster
Categories: cs.LG cs.AI
Comments: 11 pages, 5 figures
\\
  We present JaxUED, an open-source library providing minimal dependency
implementations of modern Unsupervised Environment Design (UED) algorithms in
Jax. JaxUED leverages hardware acceleration to obtain on the order of 100x
speedups compared to prior, CPU-based implementations. Inspired by CleanRL, we
provide fast, clear, understandable, and easily modifiable implementations,
with the aim of accelerating research into UED. This paper describes our
library and contains baseline results. Code can be found at
https://github.com/DramaCow/jaxued.
\\ ( https://arxiv.org/abs/2403.13091 ,  86kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13097
Date: Tue, 19 Mar 2024 18:57:53 GMT   (8574kb,D)

Title: Simple Ingredients for Offline Reinforcement Learning
Authors: Edoardo Cetin, Andrea Tirinzoni, Matteo Pirotta, Alessandro Lazaric,
  Yann Ollivier and Ahmed Touati
Categories: cs.LG cs.AI
\\
  Offline reinforcement learning algorithms have proven effective on datasets
highly connected to the target downstream task. Yet, leveraging a novel testbed
(MOOD) in which trajectories come from heterogeneous sources, we show that
existing methods struggle with diverse data: their performance considerably
deteriorates as data collected for related but different tasks is simply added
to the offline buffer. In light of this finding, we conduct a large empirical
study where we formulate and test several hypotheses to explain this failure.
Surprisingly, we find that scale, more than algorithmic considerations, is the
key factor influencing performance. We show that simple methods like AWAC and
IQL with increased network size overcome the paradoxical failure modes from the
inclusion of additional data in MOOD, and notably outperform prior
state-of-the-art algorithms on the canonical D4RL benchmark.
\\ ( https://arxiv.org/abs/2403.13097 ,  8574kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13101
Date: Tue, 19 Mar 2024 19:05:24 GMT   (1499kb,D)

Title: AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge
  Networks
Authors: Zheng Lin, Guanqiao Qu, Wei Wei, Xianhao Chen, Kin K. Leung
Categories: cs.LG cs.AI cs.DC
Comments: 15 pages, 10 figures
\\
  The increasing complexity of deep neural networks poses significant barriers
to democratizing them to resource-limited edge devices. To address this
challenge, split federated learning (SFL) has emerged as a promising solution
by of floading the primary training workload to a server via model partitioning
while enabling parallel training among edge devices. However, although system
optimization substantially influences the performance of SFL under
resource-constrained systems, the problem remains largely uncharted. In this
paper, we provide a convergence analysis of SFL which quantifies the impact of
model splitting (MS) and client-side model aggregation (MA) on the learning
performance, serving as a theoretical foundation. Then, we propose AdaptSFL, a
novel resource-adaptive SFL framework, to expedite SFL under
resource-constrained edge computing systems. Specifically, AdaptSFL adaptively
controls client-side MA and MS to balance communication-computing latency and
training convergence. Extensive simulations across various datasets validate
that our proposed AdaptSFL framework takes considerably less time to achieve a
target accuracy than benchmarks, demonstrating the effectiveness of the
proposed strategies.
\\ ( https://arxiv.org/abs/2403.13101 ,  1499kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13106
Date: Tue, 19 Mar 2024 19:13:22 GMT   (668kb,D)

Title: Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying
  Structure of Data
Authors: Divyansh Singhvi, Andrej Erkelens, Raghav Jain, Diganta Misra, Naomi
  Saphra
Categories: cs.LG cs.AI cs.CL cs.CV
\\
  Measuring nonlinear feature interaction is an established approach to
understanding complex patterns of attribution in many models. In this paper, we
use Shapley Taylor interaction indices (STII) to analyze the impact of
underlying data structure on model representations in a variety of modalities,
tasks, and architectures. Considering linguistic structure in masked and
auto-regressive language models (MLMs and ALMs), we find that STII increases
within idiomatic expressions and that MLMs scale STII with syntactic distance,
relying more on syntax in their nonlinear structure than ALMs do. Our speech
model findings reflect the phonetic principal that the openness of the oral
cavity determines how much a phoneme varies based on its context. Finally, we
study image classifiers and illustrate that feature interactions intuitively
reflect object boundaries. Our wide range of results illustrates the benefits
of interdisciplinary work and domain expertise in interpretability research.
\\ ( https://arxiv.org/abs/2403.13106 ,  668kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13108
Date: Tue, 19 Mar 2024 19:15:38 GMT   (414kb)

Title: Analyzing the Impact of Partial Sharing on the Resilience of Online
  Federated Learning Against Model Poisoning Attacks
Authors: Ehsan Lari, Vinay Chakravarthi Gogineni, Reza Arablouei, Stefan Werner
Categories: cs.LG cs.CR cs.DC eess.SP
\\
  We scrutinize the resilience of the partial-sharing online federated learning
(PSO-Fed) algorithm against model-poisoning attacks. PSO-Fed reduces the
communication load by enabling clients to exchange only a fraction of their
model estimates with the server at each update round. Partial sharing of model
estimates also enhances the robustness of the algorithm against model-poisoning
attacks. To gain better insights into this phenomenon, we analyze the
performance of the PSO-Fed algorithm in the presence of Byzantine clients,
malicious actors who may subtly tamper with their local models by adding noise
before sharing them with the server. Through our analysis, we demonstrate that
PSO-Fed maintains convergence in both mean and mean-square senses, even under
the strain of model-poisoning attacks. We further derive the theoretical mean
square error (MSE) of PSO-Fed, linking it to various parameters such as
stepsize, attack probability, number of Byzantine clients, client participation
rate, partial-sharing ratio, and noise variance. We also show that there is a
non-trivial optimal stepsize for PSO-Fed when faced with model-poisoning
attacks. The results of our extensive numerical experiments affirm our
theoretical assertions and highlight the superior ability of PSO-Fed to
counteract Byzantine attacks, outperforming other related leading algorithms.
\\ ( https://arxiv.org/abs/2403.13108 ,  414kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13111
Date: Tue, 19 Mar 2024 19:24:00 GMT   (477kb,D)

Title: Deep learning with noisy labels in medical prediction problems: a
  scoping review
Authors: Yishu Wei, Yu Deng, Cong Sun, Mingquan Lin, Hongmei Jiang, Yifan Peng
Categories: cs.LG cs.AI
\\
  Objectives: Medical research faces substantial challenges from noisy labels
attributed to factors like inter-expert variability and machine-extracted
labels. Despite this, the adoption of label noise management remains limited,
and label noise is largely ignored. To this end, there is a critical need to
conduct a scoping review focusing on the problem space. This scoping review
aims to comprehensively review label noise management in deep learning-based
medical prediction problems, which includes label noise detection, label noise
handling, and evaluation. Research involving label uncertainty is also
included.
  Methods: Our scoping review follows the Preferred Reporting Items for
Systematic Reviews and Meta-Analyses (PRISMA) guidelines. We searched 4
databases, including PubMed, IEEE Xplore, Google Scholar, and Semantic Scholar.
Our search terms include "noisy label AND medical / healthcare / clinical",
"un-certainty AND medical / healthcare / clinical", and "noise AND medical /
healthcare / clinical".
  Results: A total of 60 papers met inclusion criteria between 2016 and 2023. A
series of practical questions in medical research are investigated. These
include the sources of label noise, the impact of label noise, the detection of
label noise, label noise handling techniques, and their evaluation.
Categorization of both label noise detection methods and handling techniques
are provided.
  Discussion: From a methodological perspective, we observe that the medical
community has been up to date with the broader deep-learning community, given
that most techniques have been evaluated on medical data. We recommend
considering label noise as a standard element in medical research, even if it
is not dedicated to handling noisy labels. Initial experiments can start with
easy-to-implement methods, such as noise-robust loss functions, weighting, and
curriculum learning.
\\ ( https://arxiv.org/abs/2403.13111 ,  477kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13125
Date: Tue, 19 Mar 2024 19:55:38 GMT   (4118kb,D)

Title: Probabilistic Circuits with Constraints via Convex Optimization
Authors: Soroush Ghandi, Benjamin Quost, Cassio de Campos
Categories: cs.LG cs.AI
\\
  This work addresses integrating probabilistic propositional logic constraints
into the distribution encoded by a probabilistic circuit (PC). PCs are a class
of tractable models that allow efficient computations (such as conditional and
marginal probabilities) while achieving state-of-the-art performance in some
domains. The proposed approach takes both a PC and constraints as inputs, and
outputs a new PC that satisfies the constraints. This is done efficiently via
convex optimization without the need to retrain the entire model. Empirical
evaluations indicate that the combination of constraints and PCs can have
multiple use cases, including the improvement of model performance under scarce
or incomplete data, as well as the enforcement of machine learning fairness
measures into the model without compromising model fitness. We believe that
these ideas will open possibilities for multiple other applications involving
the combination of logics and deep probabilistic models.
\\ ( https://arxiv.org/abs/2403.13125 ,  4118kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13128
Date: Tue, 19 Mar 2024 19:57:37 GMT   (132kb)

Title: AdaFish: Fast low-rank parameter-efficient fine-tuning by using
  second-order information
Authors: Jiang Hu, Quanzheng Li
Categories: cs.LG
\\
  Recent advancements in large-scale pretrained models have significantly
improved performance across a variety of tasks in natural language processing
and computer vision. However, the extensive number of parameters in these
models necessitates substantial memory and computational resources for full
training. To adapt these models for downstream tasks or specific
application-oriented datasets, parameter-efficient fine-tuning methods
leveraging pretrained parameters have gained considerable attention. However,
it can still be time-consuming due to lots of parameters and epochs. In this
work, we introduce AdaFish, an efficient algorithm of the second-order type
designed to expedite the training process within low-rank decomposition-based
fine-tuning frameworks. Our key observation is that the associated generalized
Fisher information matrix is either low-rank or extremely small-scaled. Such a
generalized Fisher information matrix is shown to be equivalent to the Hessian
matrix. Moreover, we prove the global convergence of AdaFish, along with its
iteration/oracle complexity. Numerical experiments show that our algorithm is
quite competitive with the state-of-the-art AdamW method.
\\ ( https://arxiv.org/abs/2403.13128 ,  132kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13134
Date: Tue, 19 Mar 2024 20:10:23 GMT   (412kb,D)

Title: Robust NAS under adversarial training: benchmark, theory, and beyond
Authors: Yongtao Wu, Fanghui Liu, Carl-Johann Simon-Gabriel, Grigorios G
  Chrysos, Volkan Cevher
Categories: cs.LG cs.AI stat.ML
\\
  Recent developments in neural architecture search (NAS) emphasize the
significance of considering robust architectures against malicious data.
However, there is a notable absence of benchmark evaluations and theoretical
guarantees for searching these robust architectures, especially when
adversarial training is considered. In this work, we aim to address these two
challenges, making twofold contributions. First, we release a comprehensive
data set that encompasses both clean accuracy and robust accuracy for a vast
array of adversarially trained networks from the NAS-Bench-201 search space on
image datasets. Then, leveraging the neural tangent kernel (NTK) tool from deep
learning theory, we establish a generalization theory for searching
architecture in terms of clean accuracy and robust accuracy under
multi-objective adversarial training. We firmly believe that our benchmark and
theoretical insights will significantly benefit the NAS community through
reliable reproducibility, efficient assessment, and theoretical foundation,
particularly in the pursuit of robust architectures.
\\ ( https://arxiv.org/abs/2403.13134 ,  412kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13136
Date: Tue, 19 Mar 2024 20:12:46 GMT   (1518kb)

Title: Multi-fidelity surrogate with heterogeneous input spaces for modeling
  melt pools in laser-directed energy deposition
Authors: Nandana Menon and Amrita Basak
Categories: cs.LG cs.NA math.NA
\\
  Multi-fidelity (MF) modeling is a powerful statistical approach that can
intelligently blend data from varied fidelity sources. This approach finds a
compelling application in predicting melt pool geometry for laser-directed
energy deposition (L-DED). One major challenge in using MF surrogates to merge
a hierarchy of melt pool models is the variability in input spaces. To address
this challenge, this paper introduces a novel approach for constructing an MF
surrogate for predicting melt pool geometry by integrating models of varying
complexity, that operate on heterogeneous input spaces. The first thermal model
incorporates five input parameters i.e., laser power, scan velocity, powder
flow rate, carrier gas flow rate, and nozzle height. In contrast, the second
thermal model can only handle laser power and scan velocity. A mapping is
established between the heterogeneous input spaces so that the five-dimensional
space can be morphed into a pseudo two-dimensional space. Predictions are then
blended using a Gaussian process-based co-kriging method. The resulting
heterogeneous multi-fidelity Gaussian process (Het-MFGP) surrogate not only
improves predictive accuracy but also offers computational efficiency by
reducing evaluations required from the high-dimensional, high-fidelity thermal
model. The results underscore the benefits of employing Het-MFGP for modeling
melt pool behavior in L-DED. The framework successfully demonstrates how to
leverage multimodal data and handle scenarios where certain input parameters
may be difficult to model or measure.
\\ ( https://arxiv.org/abs/2403.13136 ,  1518kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13150
Date: Tue, 19 Mar 2024 20:58:38 GMT   (593kb,D)

Title: Training Survival Models using Scoring Rules
Authors: Philipp Kopper, David R\"ugamer, Raphael Sonabend, Bernd Bischl,
  Andreas Bender
Categories: cs.LG cs.AI stat.CO stat.ML
\\
  Survival Analysis provides critical insights for partially incomplete
time-to-event data in various domains. It is also an important example of
probabilistic machine learning. The probabilistic nature of the predictions can
be exploited by using (proper) scoring rules in the model fitting process
instead of likelihood-based optimization. Our proposal does so in a generic
manner and can be used for a variety of model classes. We establish different
parametric and non-parametric sub-frameworks that allow different degrees of
flexibility. Incorporated into neural networks, it leads to a computationally
efficient and scalable optimization routine, yielding state-of-the-art
predictive performance. Finally, we show that using our framework, we can
recover various parametric models and demonstrate that optimization works
equally well when compared to likelihood-based methods.
\\ ( https://arxiv.org/abs/2403.13150 ,  593kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13164
Date: Tue, 19 Mar 2024 21:31:56 GMT   (562kb,D)

Title: VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal
  In-Context Learning
Authors: Yongshuo Zong, Ondrej Bohdal, Timothy Hospedales
Categories: cs.LG
\\
  Large language models (LLMs) famously exhibit emergent in-context learning
(ICL) -- the ability to rapidly adapt to new tasks using few-shot examples
provided as a prompt, without updating the model's weights. Built on top of
LLMs, vision large language models (VLLMs) have advanced significantly in areas
such as recognition, reasoning, and grounding. However, investigations into
\emph{multimodal ICL} have predominantly focused on few-shot visual question
answering (VQA), and image captioning, which we will show neither exploit the
strengths of ICL, nor test its limitations. The broader capabilities and
limitations of multimodal ICL remain under-explored. In this study, we
introduce a comprehensive benchmark VL-ICL Bench for multimodal in-context
learning, encompassing a broad spectrum of tasks that involve both images and
text as inputs and outputs, and different types of challenges, from {perception
to reasoning and long context length}. We evaluate the abilities of
state-of-the-art VLLMs against this benchmark suite, revealing their diverse
strengths and weaknesses, and showing that even the most advanced models, such
as GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks,
and the associated strengths and limitations of existing models, we hope that
our dataset will inspire future work on enhancing the in-context learning
capabilities of VLLMs, as well as inspire new applications that leverage VLLM
ICL. The code and dataset are available at https://github.com/ys-zong/VL-ICL.
\\ ( https://arxiv.org/abs/2403.13164 ,  562kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13176
Date: Tue, 19 Mar 2024 22:05:32 GMT   (3288kb,D)

Title: Castor: Competing shapelets for fast and accurate time series
  classification
Authors: Isak Samsten and Zed Lee
Categories: cs.LG cs.CV
Comments: Submitted to Data Mining and Knowledge Discovery Journal
\\
  Shapelets are discriminative subsequences, originally embedded in
shapelet-based decision trees but have since been extended to shapelet-based
transformations. We propose Castor, a simple, efficient, and accurate time
series classification algorithm that utilizes shapelets to transform time
series. The transformation organizes shapelets into groups with varying
dilation and allows the shapelets to compete over the time context to construct
a diverse feature representation. By organizing the shapelets into groups, we
enable the transformation to transition between levels of competition,
resulting in methods that more closely resemble distance-based transformations
or dictionary-based transformations. We demonstrate, through an extensive
empirical investigation, that Castor yields transformations that result in
classifiers that are significantly more accurate than several state-of-the-art
classifiers. In an extensive ablation study, we examine the effect of choosing
hyperparameters and suggest accurate and efficient default values.
\\ ( https://arxiv.org/abs/2403.13176 ,  3288kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13179
Date: Tue, 19 Mar 2024 22:19:29 GMT   (2204kb,D)

Title: Predictive, scalable and interpretable knowledge tracing on structured
  domains
Authors: Hanqi Zhou, Robert Bamler, Charley M. Wu and \'Alvaro Tejero-Cantero
Categories: cs.LG cs.CY stat.ML
\\
  Intelligent tutoring systems optimize the selection and timing of learning
materials to enhance understanding and long-term retention. This requires
estimates of both the learner's progress (''knowledge tracing''; KT), and the
prerequisite structure of the learning domain (''knowledge mapping''). While
recent deep learning models achieve high KT accuracy, they do so at the expense
of the interpretability of psychologically-inspired models. In this work, we
present a solution to this trade-off. PSI-KT is a hierarchical generative
approach that explicitly models how both individual cognitive traits and the
prerequisite structure of knowledge influence learning dynamics, thus achieving
interpretability by design. Moreover, by using scalable Bayesian inference,
PSI-KT targets the real-world need for efficient personalization even with a
growing body of learners and learning histories. Evaluated on three datasets
from online learning platforms, PSI-KT achieves superior multi-step predictive
accuracy and scalable inference in continual-learning settings, all while
providing interpretable representations of learner-specific traits and the
prerequisite structure of knowledge that causally supports learning. In sum,
predictive, scalable and interpretable knowledge tracing with solid knowledge
mapping lays a key foundation for effective personalized learning to make
education accessible to a broad, global audience.
\\ ( https://arxiv.org/abs/2403.13179 ,  2204kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13196
Date: Tue, 19 Mar 2024 23:13:40 GMT   (275kb,D)

Title: ADAPT to Robustify Prompt Tuning Vision Transformers
Authors: Masih Eskandar, Tooba Imtiaz, Zifeng Wang, Jennifer Dy
Categories: cs.LG cs.AI cs.CV stat.ML
\\
  The performance of deep models, including Vision Transformers, is known to be
vulnerable to adversarial attacks. Many existing defenses against these
attacks, such as adversarial training, rely on full-model fine-tuning to induce
robustness in the models. These defenses require storing a copy of the entire
model, that can have billions of parameters, for each task. At the same time,
parameter-efficient prompt tuning is used to adapt large transformer-based
models to downstream tasks without the need to save large copies. In this
paper, we examine parameter-efficient prompt tuning of Vision Transformers for
downstream tasks under the lens of robustness. We show that previous
adversarial defense methods, when applied to the prompt tuning paradigm, suffer
from gradient obfuscation and are vulnerable to adaptive attacks. We introduce
ADAPT, a novel framework for performing adaptive adversarial training in the
prompt tuning paradigm. Our method achieves competitive robust accuracy of ~40%
w.r.t. SOTA robustness methods using full-model fine-tuning, by tuning only ~1%
of the number of parameters.
\\ ( https://arxiv.org/abs/2403.13196 ,  275kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13204
Date: Tue, 19 Mar 2024 23:50:11 GMT   (2862kb,D)

Title: Diversity-Aware Agnostic Ensemble of Sharpness Minimizers
Authors: Anh Bui, Vy Vo, Tung Pham, Dinh Phung, Trung Le
Categories: cs.LG cs.CV stat.ML
\\
  There has long been plenty of theoretical and empirical evidence supporting
the success of ensemble learning. Deep ensembles in particular take advantage
of training randomness and expressivity of individual neural networks to gain
prediction diversity, ultimately leading to better generalization, robustness
and uncertainty estimation. In respect of generalization, it is found that
pursuing wider local minima result in models being more robust to shifts
between training and testing sets. A natural research question arises out of
these two approaches as to whether a boost in generalization ability can be
achieved if ensemble learning and loss sharpness minimization are integrated.
Our work investigates this connection and proposes DASH - a learning algorithm
that promotes diversity and flatness within deep ensembles. More concretely,
DASH encourages base learners to move divergently towards low-loss regions of
minimal sharpness. We provide a theoretical backbone for our method along with
extensive empirical evidence demonstrating an improvement in ensemble
generalizability.
\\ ( https://arxiv.org/abs/2403.13204 ,  2862kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13213
Date: Wed, 20 Mar 2024 00:22:38 GMT   (1069kb,D)

Title: From Representational Harms to Quality-of-Service Harms: A Case Study on
  Llama 2 Safety Safeguards
Authors: Khaoula Chehbouni (1 and 3), Megha Roshan (2 and 3), Emmanuel Ma (1),
  Futian Andrew Wei (1), Afaf Ta\"ik (2 and 3), Jackie CK Cheung (1 and 3),
  Golnoosh Farnadi (1 and 3) ((1) McGill University, (2) University of
  Montreal, (3) Mila - Quebec AI Institute)
Categories: cs.LG cs.CL cs.CY
Comments: 9 pages, 4 figures, submitted to the 62nd Annual Meeting of the
  Association for Computational Linguistics (ACL 2024)
\\
  Recent progress in large language models (LLMs) has led to their widespread
adoption in various domains. However, these advancements have also introduced
additional safety risks and raised concerns regarding their detrimental impact
on already marginalized populations. Despite growing mitigation efforts to
develop safety safeguards, such as supervised safety-oriented fine-tuning and
leveraging safe reinforcement learning from human feedback, multiple concerns
regarding the safety and ingrained biases in these models remain. Furthermore,
previous work has demonstrated that models optimized for safety often display
exaggerated safety behaviors, such as a tendency to refrain from responding to
certain requests as a precautionary measure. As such, a clear trade-off between
the helpfulness and safety of these models has been documented in the
literature. In this paper, we further investigate the effectiveness of safety
measures by evaluating models on already mitigated biases. Using the case of
Llama 2 as an example, we illustrate how LLMs' safety responses can still
encode harmful assumptions. To do so, we create a set of non-toxic prompts,
which we then use to evaluate Llama models. Through our new taxonomy of LLMs
responses to users, we observe that the safety/helpfulness trade-offs are more
pronounced for certain demographic groups which can lead to quality-of-service
harms for marginalized populations.
\\ ( https://arxiv.org/abs/2403.13213 ,  1069kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13219
Date: Wed, 20 Mar 2024 00:41:12 GMT   (27831kb,D)

Title: Diffusion Model for Data-Driven Black-Box Optimization
Authors: Zihao Li, Hui Yuan, Kaixuan Huang, Chengzhuo Ni, Yinyu Ye, Minshuo
  Chen, Mengdi Wang
Categories: cs.LG math.OC
Comments: arXiv admin note: substantial text overlap with arXiv:2307.07055
\\
  Generative AI has redefined artificial intelligence, enabling the creation of
innovative content and customized solutions that drive business practices into
a new era of efficiency and creativity. In this paper, we focus on diffusion
models, a powerful generative AI technology, and investigate their potential
for black-box optimization over complex structured variables. Consider the
practical scenario where one wants to optimize some structured design in a
high-dimensional space, based on massive unlabeled data (representing design
variables) and a small labeled dataset. We study two practical types of labels:
1) noisy measurements of a real-valued reward function and 2) human preference
based on pairwise comparisons. The goal is to generate new designs that are
near-optimal and preserve the designed latent structures. Our proposed method
reformulates the design optimization problem into a conditional sampling
problem, which allows us to leverage the power of diffusion models for modeling
complex distributions. In particular, we propose a reward-directed conditional
diffusion model, to be trained on the mixed data, for sampling a near-optimal
solution conditioned on high predicted rewards. Theoretically, we establish
sub-optimality error bounds for the generated designs. The sub-optimality gap
nearly matches the optimal guarantee in off-policy bandits, demonstrating the
efficiency of reward-directed diffusion models for black-box optimization.
Moreover, when the data admits a low-dimensional latent subspace structure, our
model efficiently generates high-fidelity designs that closely respect the
latent structure. We provide empirical experiments validating our model in
decision-making and content-creation tasks.
\\ ( https://arxiv.org/abs/2403.13219 ,  27831kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13241
Date: Wed, 20 Mar 2024 02:11:28 GMT   (7292kb,D)

Title: Tackling Noisy Labels with Network Parameter Additive Decomposition
Authors: Jingyi Wang, Xiaobo Xia, Long Lan, Xinghao Wu, Jun Yu, Wenjing Yang,
  Bo Han and Tongliang Liu
Categories: cs.LG
Comments: Accepted by IEEE T-PAMI
\\
  Given data with noisy labels, over-parameterized deep networks suffer
overfitting mislabeled data, resulting in poor generalization. The memorization
effect of deep networks shows that although the networks have the ability to
memorize all noisy data, they would first memorize clean training data, and
then gradually memorize mislabeled training data. A simple and effective method
that exploits the memorization effect to combat noisy labels is early stopping.
However, early stopping cannot distinguish the memorization of clean data and
mislabeled data, resulting in the network still inevitably overfitting
mislabeled data in the early training stage.In this paper, to decouple the
memorization of clean data and mislabeled data, and further reduce the side
effect of mislabeled data, we perform additive decomposition on network
parameters. Namely, all parameters are additively decomposed into two groups,
i.e., parameters $\mathbf{w}$ are decomposed as
$\mathbf{w}=\bm{\sigma}+\bm{\gamma}$. Afterward, the parameters $\bm{\sigma}$
are considered to memorize clean data, while the parameters $\bm{\gamma}$ are
considered to memorize mislabeled data. Benefiting from the memorization
effect, the updates of the parameters $\bm{\sigma}$ are encouraged to fully
memorize clean data in early training, and then discouraged with the increase
of training epochs to reduce interference of mislabeled data. The updates of
the parameters $\bm{\gamma}$ are the opposite. In testing, only the parameters
$\bm{\sigma}$ are employed to enhance generalization. Extensive experiments on
both simulated and real-world benchmarks confirm the superior performance of
our method.
\\ ( https://arxiv.org/abs/2403.13241 ,  7292kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13246
Date: Wed, 20 Mar 2024 02:17:16 GMT   (936kb,D)

Title: Divide-Conquer Transformer Learning for Predicting Electric Vehicle
  Charging Events Using Smart Meter Data
Authors: Fucai Ke, Hao Wang
Categories: cs.LG cs.CY
Comments: 2024 IEEE Power & Energy Society General Meeting (PESGM)
\\
  Predicting electric vehicle (EV) charging events is crucial for load
scheduling and energy management, promoting seamless transportation
electrification and decarbonization. While prior studies have focused on EV
charging demand prediction, primarily for public charging stations using
historical charging data, home charging prediction is equally essential.
However, existing prediction methods may not be suitable due to the
unavailability of or limited access to home charging data. To address this
research gap, inspired by the concept of non-intrusive load monitoring (NILM),
we develop a home charging prediction method using historical smart meter data.
Different from NILM detecting EV charging that has already occurred, our method
provides predictive information of future EV charging occurrences, thus
enhancing its utility for charging management. Specifically, our method,
leverages a self-attention mechanism-based transformer model, employing a
``divide-conquer'' strategy, to process historical meter data to effectively
and learn EV charging representation for charging occurrence prediction. Our
method enables prediction at one-minute interval hour-ahead. Experimental
results demonstrate the effectiveness of our method, achieving consistently
high accuracy of over 96.81\% across different prediction time spans. Notably,
our method achieves high prediction performance solely using smart meter data,
making it a practical and suitable solution for grid operators.
\\ ( https://arxiv.org/abs/2403.13246 ,  936kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13247
Date: Wed, 20 Mar 2024 02:17:47 GMT   (508kb,D)

Title: Decentralized Federated Learning: Model Update Tracking Under Imperfect
  Information Sharing
Authors: Vishnu Pandi Chellapandi, Antesh Upadhyay, Abolfazl Hashemi, and
  Stanislaw H. \.Zak
Categories: cs.LG cs.DC
Comments: arXiv admin note: substantial text overlap with arXiv:2303.10695
\\
  A novel Decentralized Noisy Model Update Tracking Federated Learning
algorithm (FedNMUT) is proposed, which is tailored to function efficiently in
the presence of noisy communication channels that reflect imperfect information
exchange. This algorithm uses gradient tracking to minimize the impact of data
heterogeneity while minimizing communication overhead. The proposed algorithm
incorporates noise into its parameters to mimic the conditions of noisy
communication channels, thereby enabling consensus among clients through a
communication graph topology in such challenging environments. FedNMUT
prioritizes parameter sharing and noise incorporation to increase the
resilience of decentralized learning systems against noisy communications.
Through theoretical and empirical validation, it is demonstrated that the
performance of FedNMUT is superior compared to the existing state-of-the-art
methods and conventional parameter-mixing approaches in dealing with imperfect
information sharing. This proves the capability of the proposed algorithm to
counteract the negative effects of communication noise in a decentralized
learning framework.
\\ ( https://arxiv.org/abs/2403.13247 ,  508kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13249
Date: Wed, 20 Mar 2024 02:21:44 GMT   (59kb)

Title: A Unified and General Framework for Continual Learning
Authors: Zhenyi Wang, Yan Li, Li Shen, Heng Huang
Categories: cs.LG cs.AI cs.CV
Comments: ICLR 2024
\\
  Continual Learning (CL) focuses on learning from dynamic and changing data
distributions while retaining previously acquired knowledge. Various methods
have been developed to address the challenge of catastrophic forgetting,
including regularization-based, Bayesian-based, and memory-replay-based
techniques. However, these methods lack a unified framework and common
terminology for describing their approaches. This research aims to bridge this
gap by introducing a comprehensive and overarching framework that encompasses
and reconciles these existing methodologies. Notably, this new framework is
capable of encompassing established CL approaches as special instances within a
unified and general optimization objective. An intriguing finding is that
despite their diverse origins, these methods share common mathematical
structures. This observation highlights the compatibility of these seemingly
distinct techniques, revealing their interconnectedness through a shared
underlying optimization objective. Moreover, the proposed general framework
introduces an innovative concept called refresh learning, specifically designed
to enhance the CL performance. This novel approach draws inspiration from
neuroscience, where the human brain often sheds outdated information to improve
the retention of crucial knowledge and facilitate the acquisition of new
information. In essence, refresh learning operates by initially unlearning
current data and subsequently relearning it. It serves as a versatile plug-in
that seamlessly integrates with existing CL methods, offering an adaptable and
effective enhancement to the learning process. Extensive experiments on CL
benchmarks and theoretical analysis demonstrate the effectiveness of the
proposed refresh learning. Code is available at
\url{https://github.com/joey-wang123/CL-refresh-learning}.
\\ ( https://arxiv.org/abs/2403.13249 ,  59kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13268
Date: Wed, 20 Mar 2024 03:07:30 GMT   (3966kb,D)

Title: Unifews: Unified Entry-Wise Sparsification for Efficient Graph Neural
  Network
Authors: Ningyi Liao and Zihao Yu and Siqiang Luo
Categories: cs.LG cs.DB
\\
  Graph Neural Networks (GNNs) have shown promising performance in various
graph learning tasks, but at the cost of resource-intensive computations. The
primary overhead of GNN update stems from graph propagation and weight
transformation, both involving operations on graph-scale matrices. Previous
studies attempt to reduce the computational budget by leveraging graph-level or
network-level sparsification techniques, resulting in downsized graph or
weights. In this work, we propose Unifews, which unifies the two operations in
an entry-wise manner considering individual matrix elements, and conducts joint
edge-weight sparsification to enhance learning efficiency. The entry-wise
design of Unifews enables adaptive compression across GNN layers with
progressively increased sparsity, and is applicable to a variety of
architectural designs with on-the-fly operation simplification. Theoretically,
we establish a novel framework to characterize sparsified GNN learning in view
of a graph optimization process, and prove that Unifews effectively
approximates the learning objective with bounded error and reduced
computational load. We conduct extensive experiments to evaluate the
performance of our method in diverse settings. Unifews is advantageous in
jointly removing more than 90% of edges and weight entries with comparable or
better accuracy than baseline models. The sparsification offers remarkable
efficiency improvements including 10-20x matrix operation reduction and up to
100x acceleration in graph propagation time for the largest graph at the
billion-edge scale.
\\ ( https://arxiv.org/abs/2403.13268 ,  3966kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13335
Date: Wed, 20 Mar 2024 06:38:13 GMT   (693kb,D)

Title: Adaptive Ensembles of Fine-Tuned Transformers for LLM-Generated Text
  Detection
Authors: Zhixin Lai, Xuesheng Zhang, Suiyao Chen
Categories: cs.LG cs.AI
\\
  Large language models (LLMs) have reached human-like proficiency in
generating diverse textual content, underscoring the necessity for effective
fake text detection to avoid potential risks such as fake news in social media.
Previous research has mostly tested single models on in-distribution datasets,
limiting our understanding of how these models perform on different types of
data for LLM-generated text detection task. We researched this by testing five
specialized transformer-based models on both in-distribution and
out-of-distribution datasets to better assess their performance and
generalizability. Our results revealed that single transformer-based
classifiers achieved decent performance on in-distribution dataset but limited
generalization ability on out-of-distribution dataset. To improve it, we
combined the individual classifiers models using adaptive ensemble algorithms,
which improved the average accuracy significantly from 91.8% to 99.2% on an
in-distribution test set and from 62.9% to 72.5% on an out-of-distribution test
set. The results indicate the effectiveness, good generalization ability, and
great potential of adaptive ensemble algorithms in LLM-generated text
detection.
\\ ( https://arxiv.org/abs/2403.13335 ,  693kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13349
Date: Wed, 20 Mar 2024 07:21:37 GMT   (2099kb,D)

Title: Hierarchical Gaussian Mixture Normalizing Flow Modeling for Unified
  Anomaly Detection
Authors: Xincheng Yao and Ruoqi Li and Zefeng Qian and Lu Wang and Chongyang
  Zhang
Categories: cs.LG cs.CV
Comments: 15 pages
\\
  Unified anomaly detection (AD) is one of the most challenges for anomaly
detection, where one unified model is trained with normal samples from multiple
classes with the objective to detect anomalies in these classes. For such a
challenging task, popular normalizing flow (NF) based AD methods may fall into
a "homogeneous mapping" issue,where the NF-based AD models are biased to
generate similar latent representations for both normal and abnormal features,
and thereby lead to a high missing rate of anomalies. In this paper, we propose
a novel Hierarchical Gaussian mixture normalizing flow modeling method for
accomplishing unified Anomaly Detection, which we call HGAD. Our HGAD consists
of two key components: inter-class Gaussian mixture modeling and intra-class
mixed class centers learning. Compared to the previous NF-based AD methods, the
hierarchical Gaussian mixture modeling approach can bring stronger
representation capability to the latent space of normalizing flows, so that
even complex multi-class distribution can be well represented and learned in
the latent space. In this way, we can avoid mapping different class
distributions into the same single Gaussian prior, thus effectively avoiding or
mitigating the "homogeneous mapping" issue. We further indicate that the more
distinguishable different class centers, the more conducive to avoiding the
bias issue. Thus, we further propose a mutual information maximization loss for
better structuring the latent feature space. We evaluate our method on four
real-world AD benchmarks, where we can significantly improve the previous
NF-based AD methods and also outperform the SOTA unified AD methods.
\\ ( https://arxiv.org/abs/2403.13349 ,  2099kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13374
Date: Wed, 20 Mar 2024 08:15:08 GMT   (1126kb,D)

Title: Byzantine-resilient Federated Learning With Adaptivity to Data
  Heterogeneity
Authors: Shiyuan Zuo, Xingrun Yan, Rongfei Fan, Han Hu, Hangguan Shan, Tony Q.
  S. Quek
Categories: cs.LG cs.AI cs.CR
\\
  This paper deals with federated learning (FL) in the presence of malicious
Byzantine attacks and data heterogeneity. A novel Robust Average Gradient
Algorithm (RAGA) is proposed, which leverages the geometric median for
aggregation and can freely select the round number for local updating.
Different from most existing resilient approaches, which perform convergence
analysis based on strongly-convex loss function or homogeneously distributed
dataset, we conduct convergence analysis for not only strongly-convex but also
non-convex loss function over heterogeneous dataset. According to our
theoretical analysis, as long as the fraction of dataset from malicious users
is less than half, RAGA can achieve convergence at rate
$\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and
$\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for
strongly-convex loss function. Moreover, stationary point or global optimal
solution is proved to obtainable as data heterogeneity vanishes. Experimental
results corroborate the robustness of RAGA to Byzantine attacks and verifies
the advantage of RAGA over baselines on convergence performance under various
intensity of Byzantine attacks, for heterogeneous dataset.
\\ ( https://arxiv.org/abs/2403.13374 ,  1126kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13502
Date: Wed, 20 Mar 2024 10:59:06 GMT   (14090kb,D)

Title: Adversarial Attacks and Defenses in Automated Control Systems: A
  Comprehensive Benchmark
Authors: Vitaliy Pozdnyakov, Aleksandr Kovalenko, Ilya Makarov, Mikhail
  Drobyshevskiy, Kirill Lukyanov
Categories: cs.LG cs.CR cs.SY eess.SY
ACM-class: I.2.6; I.2.1
\\
  Integrating machine learning into Automated Control Systems (ACS) enhances
decision-making in industrial process management. One of the limitations to the
widespread adoption of these technologies in industry is the vulnerability of
neural networks to adversarial attacks. This study explores the threats in
deploying deep learning models for fault diagnosis in ACS using the Tennessee
Eastman Process dataset. By evaluating three neural networks with different
architectures, we subject them to six types of adversarial attacks and explore
five different defense methods. Our results highlight the strong vulnerability
of models to adversarial samples and the varying effectiveness of defense
strategies. We also propose a novel protection approach by combining multiple
defense methods and demonstrate it's efficacy. This research contributes
several insights into securing machine learning within ACS, ensuring robust
fault diagnosis in industrial processes.
\\ ( https://arxiv.org/abs/2403.13502 ,  14090kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13522
Date: Wed, 20 Mar 2024 11:48:10 GMT   (913kb,D)

Title: REAL: Representation Enhanced Analytic Learning for Exemplar-free
  Class-incremental Learning
Authors: Run He, Huiping Zhuang, Di Fang, Yizhu Chen, Kai Tong and Cen Chen
Categories: cs.LG cs.CV
\\
  Exemplar-free class-incremental learning (EFCIL) aims to mitigate
catastrophic forgetting in class-incremental learning without available
historical data. Compared with its counterpart (replay-based CIL) that stores
historical samples, the EFCIL suffers more from forgetting issues under the
exemplar-free constraint. In this paper, inspired by the recently developed
analytic learning (AL) based CIL, we propose a representation enhanced analytic
learning (REAL) for EFCIL. The REAL constructs a dual-stream base pretraining
(DS-BPT) and a representation enhancing distillation (RED) process to enhance
the representation of the extractor. The DS-BPT pretrains model in streams of
both supervised learning and self-supervised contrastive learning (SSCL) for
base knowledge extraction. The RED process distills the supervised knowledge to
the SSCL pretrained backbone and facilitates a subsequent AL-basd CIL that
converts the CIL to a recursive least-square problem. Our method addresses the
issue of insufficient discriminability in representations of unseen data caused
by a frozen backbone in the existing AL-based CIL. Empirical results on various
datasets including CIFAR-100, ImageNet-100 and ImageNet-1k, demonstrate that
our REAL outperforms the state-of-the-arts in EFCIL, and achieves comparable or
even more superior performance compared with the replay-based methods.
\\ ( https://arxiv.org/abs/2403.13522 ,  913kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13523
Date: Wed, 20 Mar 2024 11:50:16 GMT   (1985kb,D)

Title: Have You Poisoned My Data? Defending Neural Networks against Data
  Poisoning
Authors: Fabio De Gaspari, Dorjan Hitaj, Luigi V. Mancini
Categories: cs.LG cs.AI cs.CR
Comments: Paper accepted for publication at European Symposium on Research in
  Computer Security (ESORICS) 2024
\\
  The unprecedented availability of training data fueled the rapid development
of powerful neural networks in recent years. However, the need for such large
amounts of data leads to potential threats such as poisoning attacks:
adversarial manipulations of the training data aimed at compromising the
learned model to achieve a given adversarial goal.
  This paper investigates defenses against clean-label poisoning attacks and
proposes a novel approach to detect and filter poisoned datapoints in the
transfer learning setting. We define a new characteristic vector representation
of datapoints and show that it effectively captures the intrinsic properties of
the data distribution. Through experimental analysis, we demonstrate that
effective poisons can be successfully differentiated from clean points in the
characteristic vector space. We thoroughly evaluate our proposed approach and
compare it to existing state-of-the-art defenses using multiple architectures,
datasets, and poison budgets. Our evaluation shows that our proposal
outperforms existing approaches in defense rate and final trained model
performance across all experimental settings.
\\ ( https://arxiv.org/abs/2403.13523 ,  1985kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13547
Date: Wed, 20 Mar 2024 12:33:51 GMT   (8956kb,D)

Title: Integrating Large Language Models for Severity Classification in Traffic
  Incident Management: A Machine Learning Approach
Authors: Artur Grigorev, Khaled Saleh, Yuming Ou, Adriana-Simona Mihaita
Categories: cs.LG cs.SY eess.SY
\\
  This study evaluates the impact of large language models on enhancing machine
learning processes for managing traffic incidents. It examines the extent to
which features generated by modern language models improve or match the
accuracy of predictions when classifying the severity of incidents using
accident reports. Multiple comparisons performed between combinations of
language models and machine learning algorithms, including Gradient Boosted
Decision Trees, Random Forests, and Extreme Gradient Boosting. Our research
uses both conventional and language model-derived features from texts and
incident reports, and their combinations to perform severity classification.
Incorporating features from language models with those directly obtained from
incident reports has shown to improve, or at least match, the performance of
machine learning techniques in assigning severity levels to incidents,
particularly when employing Random Forests and Extreme Gradient Boosting
methods. This comparison was quantified using the F1-score over uniformly
sampled data sets to obtain balanced severity classes. The primary contribution
of this research is in the demonstration of how Large Language Models can be
integrated into machine learning workflows for incident management, thereby
simplifying feature extraction from unstructured text and enhancing or matching
the precision of severity predictions using conventional machine learning
pipeline. The engineering application of this research is illustrated through
the effective use of these language processing models to refine the modelling
process for incident severity classification. This work provides significant
insights into the application of language processing capabilities in
combination with traditional data for improving machine learning pipelines in
the context of classifying incident severity.
\\ ( https://arxiv.org/abs/2403.13547 ,  8956kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13612
Date: Wed, 20 Mar 2024 14:03:57 GMT   (139kb,D)

Title: Does Differentially Private Synthetic Data Lead to Synthetic
  Discoveries?
Authors: Ileana Montoya Perez, Parisa Movahedi, Valtteri Nieminen, Antti
  Airola, Tapio Pahikkala
Categories: cs.LG stat.ML
\\
  Background: Synthetic data has been proposed as a solution for sharing
anonymized versions of sensitive biomedical datasets. Ideally, synthetic data
should preserve the structure and statistical properties of the original data,
while protecting the privacy of the individual subjects. Differential privacy
(DP) is currently considered the gold standard approach for balancing this
trade-off.
  Objectives: The aim of this study is to evaluate the Mann-Whitney U test on
DP-synthetic biomedical data in terms of Type I and Type II errors, in order to
establish whether statistical hypothesis testing performed on privacy
preserving synthetic data is likely to lead to loss of test's validity or
decreased power.
  Methods: We evaluate the Mann-Whitney U test on DP-synthetic data generated
from real-world data, including a prostate cancer dataset (n=500) and a
cardiovascular dataset (n=70 000), as well as on data drawn from two Gaussian
distributions. Five different DP-synthetic data generation methods are
evaluated, including two basic DP histogram release methods and MWEM,
Private-PGM, and DP GAN algorithms.
  Conclusion: Most of the tested DP-synthetic data generation methods showed
inflated Type I error, especially at privacy budget levels of $\epsilon\leq 1$.
This result calls for caution when releasing and analyzing DP-synthetic data:
low p-values may be obtained in statistical tests simply as a byproduct of the
noise added to protect privacy. A DP smoothed histogram-based synthetic data
generation method was shown to produce valid Type I error for all privacy
levels tested but required a large original dataset size and a modest privacy
budget ($\epsilon\geq 5$) in order to have reasonable Type II error levels.
\\ ( https://arxiv.org/abs/2403.13612 ,  139kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13625
Date: Wed, 20 Mar 2024 14:22:19 GMT   (972kb)

Title: Enhancing Law Enforcement Training: A Gamified Approach to Detecting
  Terrorism Financing
Authors: Francesco Zola, Lander Segurola, Erin King, Martin Mullins, Raul
  Orduna
Categories: cs.LG cs.CY cs.SI q-fin.CP
Journal-ref: International Journal of Police Science & Management, Sage, 0(0),
  [2024]
DOI: 10.1177/14613557241237174
\\
  Tools for fighting cyber-criminal activities using new technologies are
promoted and deployed every day. However, too often, they are unnecessarily
complex and hard to use, requiring deep domain and technical knowledge. These
characteristics often limit the engagement of law enforcement and end-users in
these technologies that, despite their potential, remain misunderstood. For
this reason, in this study, we describe our experience in combining learning
and training methods and the potential benefits of gamification to enhance
technology transfer and increase adult learning. In fact, in this case,
participants are experienced practitioners in professions/industries that are
exposed to terrorism financing (such as Law Enforcement Officers, Financial
Investigation Officers, private investigators, etc.) We define training
activities on different levels for increasing the exchange of information about
new trends and criminal modus operandi among and within law enforcement
agencies, intensifying cross-border cooperation and supporting efforts to
combat and prevent terrorism funding activities. On the other hand, a game
(hackathon) is designed to address realistic challenges related to the dark
net, crypto assets, new payment systems and dark web marketplaces that could be
used for terrorist activities. The entire methodology was evaluated using
quizzes, contest results, and engagement metrics. In particular, training
events show about 60% of participants complete the 11-week training course,
while the Hackathon results, gathered in two pilot studies (Madrid and The
Hague), show increasing expertise among the participants (progression in the
achieved points on average). At the same time, more than 70% of participants
positively evaluate the use of the gamification approach, and more than 85% of
them consider the implemented Use Cases suitable for their investigations.
\\ ( https://arxiv.org/abs/2403.13625 ,  972kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13658
Date: Wed, 20 Mar 2024 15:06:49 GMT   (16532kb,D)

Title: Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics
  Instability Detection
Authors: Mohammod N. I. Suvon, Prasun C. Tripathi, Wenrui Fan, Shuo Zhou,
  Xianyuan Liu, Samer Alabed, Venet Osmani, Andrew J. Swift, Chen Chen, Haiping
  Lu
Categories: cs.LG cs.CV
\\
  Recent advancements in non-invasive detection of cardiac hemodynamic
instability (CHDI) primarily focus on applying machine learning techniques to a
single data modality, e.g. cardiac magnetic resonance imaging (MRI). Despite
their potential, these approaches often fall short especially when the size of
labeled patient data is limited, a common challenge in the medical domain.
Furthermore, only a few studies have explored multimodal methods to study CHDI,
which mostly rely on costly modalities such as cardiac MRI and echocardiogram.
In response to these limitations, we propose a novel multimodal variational
autoencoder ($\text{CardioVAE}_\text{X,G}$) to integrate low-cost chest X-ray
(CXR) and electrocardiogram (ECG) modalities with pre-training on a large
unlabeled dataset. Specifically, $\text{CardioVAE}_\text{X,G}$ introduces a
novel tri-stream pre-training strategy to learn both shared and
modality-specific features, thus enabling fine-tuning with both unimodal and
multimodal datasets. We pre-train $\text{CardioVAE}_\text{X,G}$ on a large,
unlabeled dataset of $50,982$ subjects from a subset of MIMIC database and then
fine-tune the pre-trained model on a labeled dataset of $795$ subjects from the
ASPIRE registry. Comprehensive evaluations against existing methods show that
$\text{CardioVAE}_\text{X,G}$ offers promising performance (AUROC $=0.79$ and
Accuracy $=0.77$), representing a significant step forward in non-invasive
prediction of CHDI. Our model also excels in producing fine interpretations of
predictions directly associated with clinical features, thereby supporting
clinical decision-making.
\\ ( https://arxiv.org/abs/2403.13658 ,  16532kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13672
Date: Wed, 20 Mar 2024 15:29:59 GMT   (3542kb,D)

Title: Machine Learning Optimized Approach for Parameter Selection in MESHFREE
  Simulations
Authors: Paulami Banerjee, Mohan Padmanabha, Chaitanya Sanghavi, Isabel Michel,
  Simone Gramsch
Categories: cs.LG physics.flu-dyn
\\
  Meshfree simulation methods are emerging as compelling alternatives to
conventional mesh-based approaches, particularly in the fields of Computational
Fluid Dynamics (CFD) and continuum mechanics. In this publication, we provide a
comprehensive overview of our research combining Machine Learning (ML) and
Fraunhofer's MESHFREE software (www.meshfree.eu), a powerful tool utilizing a
numerical point cloud in a Generalized Finite Difference Method (GFDM). This
tool enables the effective handling of complex flow domains, moving geometries,
and free surfaces, while allowing users to finely tune local refinement and
quality parameters for an optimal balance between computation time and results
accuracy. However, manually determining the optimal parameter combination poses
challenges, especially for less experienced users. We introduce a novel
ML-optimized approach, using active learning, regression trees, and
visualization on MESHFREE simulation data, demonstrating the impact of input
combinations on results quality and computation time. This research contributes
valuable insights into parameter optimization in meshfree simulations,
enhancing accessibility and usability for a broader user base in scientific and
engineering applications.
\\ ( https://arxiv.org/abs/2403.13672 ,  3542kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13724
Date: Wed, 20 Mar 2024 16:33:06 GMT   (5485kb,D)

Title: Probabilistic Forecasting with Stochastic Interpolants and F\"ollmer
  Processes
Authors: Yifan Chen, Mark Goldstein, Mengjian Hua, Michael S. Albergo, Nicholas
  M. Boffi, Eric Vanden-Eijnden
Categories: cs.LG stat.ML
\\
  We propose a framework for probabilistic forecasting of dynamical systems
based on generative modeling. Given observations of the system state over time,
we formulate the forecasting problem as sampling from the conditional
distribution of the future system state given its current state. To this end,
we leverage the framework of stochastic interpolants, which facilitates the
construction of a generative model between an arbitrary base distribution and
the target. We design a fictitious, non-physical stochastic dynamics that takes
as initial condition the current system state and produces as output a sample
from the target conditional distribution in finite time and without bias. This
process therefore maps a point mass centered at the current state onto a
probabilistic ensemble of forecasts. We prove that the drift coefficient
entering the stochastic differential equation (SDE) achieving this task is
non-singular, and that it can be learned efficiently by square loss regression
over the time-series data. We show that the drift and the diffusion
coefficients of this SDE can be adjusted after training, and that a specific
choice that minimizes the impact of the estimation error gives a F\"ollmer
process. We highlight the utility of our approach on several complex,
high-dimensional forecasting problems, including stochastically forced
Navier-Stokes and video prediction on the KTH and CLEVRER datasets.
\\ ( https://arxiv.org/abs/2403.13724 ,  5485kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13728
Date: Wed, 20 Mar 2024 16:38:26 GMT   (12000kb)

Title: M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via
  Multiplier Induced Loss Landscape Scheduling
Authors: Xudong Sun, Nutan Chen, Alexej Gossmann, Yu Xing, Carla Feistner,
  Emilio Dorigatt, Felix Drost, Daniele Scarcella, Lisa Beer, Carsten Marr
Categories: cs.LG cs.AI
\\
  When a neural network parameterized loss function consists of many terms, the
combinatorial choice of weight multipliers during the optimization process
forms a challenging problem. To address this, we proposed a probabilistic
graphical model (PGM) for the joint model parameter and multiplier evolution
process, with a hypervolume based likelihood that promotes multi-objective
descent of each loss term. The corresponding parameter and multiplier
estimation as a sequential decision process is then cast into an optimal
control problem, where the multi-objective descent goal is dispatched
hierarchically into a series of constraint optimization sub-problems. The
sub-problem constraint automatically adapts itself according to Pareto
dominance and serves as the setpoint for the low level multiplier controller to
schedule loss landscapes via output feedback of each loss term. Our method is
multiplier-free and operates at the timescale of epochs, thus saves tremendous
computational resources compared to full training cycle multiplier tuning. We
applied it to domain invariant variational auto-encoding with 6 loss terms on
the PACS domain generalization task, and observed robust performance across a
range of controller hyperparameters, as well as different multiplier initial
conditions, outperforming other multiplier scheduling methods. We offered
modular implementation of our method, admitting custom definition of many loss
terms for applying our multi-objective hierarchical output feedback training
scheme to other deep learning fields.
\\ ( https://arxiv.org/abs/2403.13728 ,  12000kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13740
Date: Wed, 20 Mar 2024 16:47:28 GMT   (8744kb,D)

Title: Uncertainty-Aware Explanations Through Probabilistic Self-Explainable
  Neural Networks
Authors: Jon Vadillo, Roberto Santana, Jose A. Lozano, Marta Kwiatkowska
Categories: cs.LG
\\
  The lack of transparency of Deep Neural Networks continues to be a limitation
that severely undermines their reliability and usage in high-stakes
applications. Promising approaches to overcome such limitations are
Prototype-Based Self-Explainable Neural Networks (PSENNs), whose predictions
rely on the similarity between the input at hand and a set of prototypical
representations of the output classes, offering therefore a deep, yet
transparent-by-design, architecture. So far, such models have been designed by
considering pointwise estimates for the prototypes, which remain fixed after
the learning phase of the model. In this paper, we introduce a probabilistic
reformulation of PSENNs, called Prob-PSENN, which replaces point estimates for
the prototypes with probability distributions over their values. This provides
not only a more flexible framework for an end-to-end learning of prototypes,
but can also capture the explanatory uncertainty of the model, which is a
missing feature in previous approaches. In addition, since the prototypes
determine both the explanation and the prediction, Prob-PSENNs allow us to
detect when the model is making uninformed or uncertain predictions, and to
obtain valid explanations for them. Our experiments demonstrate that
Prob-PSENNs provide more meaningful and robust explanations than their
non-probabilistic counterparts, thus enhancing the explainability and
reliability of the models.
\\ ( https://arxiv.org/abs/2403.13740 ,  8744kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13749
Date: Wed, 20 Mar 2024 16:58:28 GMT   (280kb,D)

Title: Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph
  Representational Learning
Authors: Raffaele Paolino, Sohir Maskey, Pascal Welke, Gitta Kutyniok
Categories: cs.LG
Comments: Accepted at ICLR 2024 Workshop on Bridging the Gap Between Practice
  and Theory in Deep Learning
\\
  We introduce $r$-loopy Weisfeiler-Leman ($r$-$\ell{}$WL), a novel hierarchy
of graph isomorphism tests and a corresponding GNN framework, $r$-$\ell{}$MPNN,
that can count cycles up to length $r + 2$. Most notably, we show that
$r$-$\ell{}$WL can count homomorphisms of cactus graphs. This strictly extends
classical 1-WL, which can only count homomorphisms of trees and, in fact, is
incomparable to $k$-WL for any fixed $k$. We empirically validate the
expressive and counting power of the proposed $r$-$\ell{}$MPNN on several
synthetic datasets and present state-of-the-art predictive performance on
various real-world datasets. The code is available at
https://github.com/RPaolino/loopy
\\ ( https://arxiv.org/abs/2403.13749 ,  280kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13765
Date: Wed, 20 Mar 2024 17:28:17 GMT   (13220kb,D)

Title: Towards Principled Representation Learning from Videos for Reinforcement
  Learning
Authors: Dipendra Misra, Akanksha Saran, Tengyang Xie, Alex Lamb, John Langford
Categories: cs.LG cs.AI cs.CV
Comments: ICLR 2024 Spotlight Conference Paper
\\
  We study pre-training representations for decision-making using video data,
which is abundantly available for tasks such as game agents and software
testing. Even though significant empirical advances have been made on this
problem, a theoretical understanding remains absent. We initiate the
theoretical investigation into principled approaches for representation
learning and focus on learning the latent state representations of the
underlying MDP using video data. We study two types of settings: one where
there is iid noise in the observation, and a more challenging setting where
there is also the presence of exogenous noise, which is non-iid noise that is
temporally correlated, such as the motion of people or cars in the background.
We study three commonly used approaches: autoencoding, temporal contrastive
learning, and forward modeling. We prove upper bounds for temporal contrastive
learning and forward modeling in the presence of only iid noise. We show that
these approaches can learn the latent state and use it to do efficient
downstream RL with polynomial sample complexity. When exogenous noise is also
present, we establish a lower bound result showing that the sample complexity
of learning from video data can be exponentially worse than learning from
action-labeled trajectory data. This partially explains why reinforcement
learning with video pre-training is hard. We evaluate these representational
learning methods in two visual domains, yielding results that are consistent
with our theoretical findings.
\\ ( https://arxiv.org/abs/2403.13765 ,  13220kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13781
Date: Wed, 20 Mar 2024 17:43:58 GMT   (40kb)

Title: Sparse Implementation of Versatile Graph-Informed Layers
Authors: Francesco Della Santa
Categories: cs.LG cs.NA math.NA
MSC-class: 68T07, 03D32
\\
  Graph Neural Networks (GNNs) have emerged as effective tools for learning
tasks on graph-structured data. Recently, Graph-Informed (GI) layers were
introduced to address regression tasks on graph nodes, extending their
applicability beyond classic GNNs. However, existing implementations of GI
layers lack efficiency due to dense memory allocation. This paper presents a
sparse implementation of GI layers, leveraging the sparsity of adjacency
matrices to reduce memory usage significantly. Additionally, a versatile
general form of GI layers is introduced, enabling their application to subsets
of graph nodes. The proposed sparse implementation improves the concrete
computational efficiency and scalability of the GI layers, permitting to build
deeper Graph-Informed Neural Networks (GINNs) and facilitating their
scalability to larger graphs.
\\ ( https://arxiv.org/abs/2403.13781 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13784
Date: Wed, 20 Mar 2024 17:47:08 GMT   (342kb)

Title: The Model Openness Framework: Promoting Completeness and Openness for
  Reproducibility, Transparency and Usability in AI
Authors: Matt White, Ibrahim Haddad, Cailean Osborne, Xiao-Yang (Yanglet) Liu,
  Ahmed Abdelmonsef, Sachin Varghese
Categories: cs.LG cs.AI cs.CY cs.SE
Comments: 45 pages
\\
  Generative AI (GAI) offers unprecedented possibilities but its
commercialization has raised concerns about transparency, reproducibility,
bias, and safety. Many "open-source" GAI models lack the necessary components
for full understanding and reproduction, and some use restrictive licenses, a
practice known as "openwashing." We propose the Model Openness Framework (MOF),
a ranked classification system that rates machine learning models based on
their completeness and openness, following principles of open science, open
source, open data, and open access. The MOF requires specific components of the
model development lifecycle to be included and released under appropriate open
licenses. This framework aims to prevent misrepresentation of models claiming
to be open, guide researchers and developers in providing all model components
under permissive licenses, and help companies, academia, and hobbyists identify
models that can be safely adopted without restrictions. Wide adoption of the
MOF will foster a more open AI ecosystem, accelerating research, innovation,
and adoption.
\\ ( https://arxiv.org/abs/2403.13784 ,  342kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13785
Date: Wed, 20 Mar 2024 17:47:25 GMT   (734kb,D)

Title: Towards an extension of Fault Trees in the Predictive Maintenance
  Scenario
Authors: Roberta De Fazio, Stefano Marrone, Laura Verde, Vincenzo Reccia, Paolo
  Valletta
Categories: cs.LG
Comments: S. Bernardi, T. Zoppi (Editors), Fast Abstracts and Student Forum
  Proceedings - EDCC 2024 - 19th European Dependable Computing Conference,
  Leuven, Belgium, 8-11 April 2024
\\
  One of the most appreciated features of Fault Trees (FTs) is their
simplicity, making them fit into industrial processes. As such processes evolve
in time, considering new aspects of large modern systems, modelling techniques
based on FTs have adapted to these needs. This paper proposes an extension of
FTs to take into account the problem of Predictive Maintenance, one of the
challenges of the modern dependability field of study. The paper sketches the
Predictive Fault Tree language and proposes some use cases to support their
modelling and analysis in concrete industrial settings.
\\ ( https://arxiv.org/abs/2403.13785 ,  734kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13787
Date: Wed, 20 Mar 2024 17:49:54 GMT   (493kb,D)

Title: RewardBench: Evaluating Reward Models for Language Modeling
Authors: Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill
  Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi,
  Noah A. Smith, Hannaneh Hajishirzi
Categories: cs.LG
Comments: 40 pages, 19 figures, 12 tables
\\
  Reward models (RMs) are at the crux of successful RLHF to align pretrained
models to human preferences, yet there has been relatively little study that
focuses on evaluation of those reward models. Evaluating reward models presents
an opportunity to understand the opaque technologies used for alignment of
language models and which values are embedded in them. To date, very few
descriptors of capabilities, training methods, or open-source reward models
exist. In this paper, we present RewardBench, a benchmark dataset and code-base
for evaluation, to enhance scientific understanding of reward models. The
RewardBench dataset is a collection of prompt-win-lose trios spanning chat,
reasoning, and safety, to benchmark how reward models perform on challenging,
structured and out-of-distribution queries. We created specific comparison
datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect
facts) why one answer should be preferred to another. On the RewardBench
leaderboard, we evaluate reward models trained with a variety of methods, such
as the direct MLE training of classifiers and the implicit reward modeling of
Direct Preference Optimization (DPO), and on a spectrum of datasets. We present
many findings on propensity for refusals, reasoning limitations, and
instruction following shortcomings of various reward models towards a better
understanding of the RLHF process.
\\ ( https://arxiv.org/abs/2403.13787 ,  493kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13793
Date: Wed, 20 Mar 2024 17:54:26 GMT   (838kb,D)

Title: Evaluating Frontier Models for Dangerous Capabilities
Authors: Mary Phuong, Matthew Aitchison, Elliot Catt, Sarah Cogan, Alexandre
  Kaskasoli, Victoria Krakovna, David Lindner, Matthew Rahtz, Yannis Assael,
  Sarah Hodkinson, Heidi Howard, Tom Lieberum, Ramana Kumar, Maria Abi Raad,
  Albert Webson, Lewis Ho, Sharon Lin, Sebastian Farquhar, Marcus Hutter,
  Gregoire Deletang, Anian Ruoss, Seliem El-Sayed, Sasha Brown, Anca Dragan,
  Rohin Shah, Allan Dafoe, Toby Shevlane
Categories: cs.LG
\\
  To understand the risks posed by a new AI system, we must understand what it
can and cannot do. Building on prior work, we introduce a programme of new
"dangerous capability" evaluations and pilot them on Gemini 1.0 models. Our
evaluations cover four areas: (1) persuasion and deception; (2) cyber-security;
(3) self-proliferation; and (4) self-reasoning. We do not find evidence of
strong dangerous capabilities in the models we evaluated, but we flag early
warning signs. Our goal is to help advance a rigorous science of dangerous
capability evaluation, in preparation for future models.
\\ ( https://arxiv.org/abs/2403.13793 ,  838kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13797
Date: Wed, 20 Mar 2024 17:54:58 GMT   (4789kb,D)

Title: Bridge the Modality and Capacity Gaps in Vision-Language Model Selection
Authors: Chao Yi,De-Chuan Zhan,Han-Jia Ye
Categories: cs.LG cs.CV
\\
  Vision Language Models (VLMs) excel in zero-shot image classification by
pairing images with textual category names. The expanding variety of
Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for
specific tasks. Thus, a promising zero-shot image classification strategy is
selecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely
on the text data of the target dataset without access to the dataset's images.
In this paper, we analyze two inherent challenges in assessing the ability of a
VLM in this Language-Only VLM selection: the "Modality Gap" -- the disparity in
VLM's embeddings across two different modalities, making text a less reliable
substitute for images; and the "Capability Gap" -- the discrepancy between the
VLM's overall ranking and its ranking for target dataset, hindering direct
prediction of a model's dataset-specific performance from its general
performance. We propose VLM Selection With gAp Bridging (SWAB) to mitigate the
negative impact of these two gaps. SWAB first adopts optimal transport to
capture the relevance between open-source datasets and target dataset with a
transportation matrix. It then uses this matrix to transfer useful statistics
of VLMs from open-source datasets to the target dataset for bridging those two
gaps and enhancing the VLM's capacity estimation for VLM selection. Experiments
across various VLMs and image classification datasets validate SWAB's
effectiveness.
\\ ( https://arxiv.org/abs/2403.13797 ,  4789kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2403.12981 (*cross-listing*)
Date: Sat, 2 Mar 2024 02:35:08 GMT   (606kb,D)

Title: Beyond Inference: Performance Analysis of DNN Server Overheads for
  Computer Vision
Authors: Ahmed F. AbouElhamayed, Susanne Balle, Deshanand Singh, Mohamed S.
  Abdelfattah
Categories: cs.DC cs.AI cs.CV cs.LG
Comments: 6 pages, 11 figures, DAC 2024: 61st IEEE/ACM Design Automation
  Conference. (DAC'24)
\\
  Deep neural network (DNN) inference has become an important part of many
data-center workloads. This has prompted focused efforts to design ever-faster
deep learning accelerators such as GPUs and TPUs. However, an end-to-end
DNN-based vision application contains more than just DNN inference, including
input decompression, resizing, sampling, normalization, and data transfer. In
this paper, we perform a thorough evaluation of computer vision inference
requests performed on a throughput-optimized serving system. We quantify the
performance impact of server overheads such as data movement, preprocessing,
and message brokers between two DNNs producing outputs at different rates. Our
empirical analysis encompasses many computer vision tasks including image
classification, segmentation, detection, depth-estimation, and more complex
processing pipelines with multiple DNNs. Our results consistently demonstrate
that end-to-end application performance can easily be dominated by data
processing and data movement functions (up to 56% of end-to-end latency in a
medium-sized image, and $\sim$ 80% impact on system throughput in a large
image), even though these functions have been conventionally overlooked in deep
learning system design. Our work identifies important performance bottlenecks
in different application scenarios, achieves 2.25$\times$ better throughput
compared to prior work, and paves the way for more holistic deep learning
system design.
\\ ( https://arxiv.org/abs/2403.12981 ,  606kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12988 (*cross-listing*)
Date: Mon, 4 Mar 2024 13:32:48 GMT   (1311kb)

Title: Improving the Robustness of Object Detection and Classification AI
  models against Adversarial Patch Attacks
Authors: Roie Kazoom, Raz Birman and Ofer Hadar
Categories: cs.CV cs.AI
\\
  Adversarial patch attacks, crafted to compromise the integrity of Deep Neural
Networks (DNNs), significantly impact Artificial Intelligence (AI) systems
designed for object detection and classification tasks. The primary purpose of
this work is to defend models against real-world physical attacks that target
object detection and classification. We analyze attack techniques and propose a
robust defense approach. We successfully reduce model confidence by over 20%
using adversarial patch attacks that exploit object shape, texture and
position. Leveraging the inpainting pre-processing technique, we effectively
restore the original confidence levels, demonstrating the importance of robust
defenses in mitigating these threats. Following fine-tuning of an AI model for
traffic sign classification, we subjected it to a simulated pixelized
patch-based physical adversarial attack, resulting in misclassifications. Our
inpainting defense approach significantly enhances model resilience, achieving
high accuracy and reliable localization despite the adversarial attacks. This
contribution advances the resilience and reliability of object detection and
classification networks against adversarial challenges, providing a robust
foundation for critical applications.
\\ ( https://arxiv.org/abs/2403.12988 ,  1311kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12997 (*cross-listing*)
Date: Wed, 6 Mar 2024 12:04:24 GMT   (1958kb,D)

Title: A Multi-Task Oriented Semantic Communication Framework for Autonomous
  Vehicles
Authors: Eslam Eldeeb, Mohammad Shehab and Hirley Alves
Categories: cs.NI cs.AI cs.IT cs.LG math.IT
\\
  Task-oriented semantic communication is an emerging technology that transmits
only the relevant semantics of a message instead of the whole message to
achieve a specific task. It reduces latency, compresses the data, and is more
robust in low SNR scenarios. This work presents a multi-task-oriented semantic
communication framework for connected and autonomous vehicles (CAVs). We
propose a convolutional autoencoder (CAE) that performs the semantic encoding
of the road traffic signs. These encoded images are then transmitted from one
CAV to another CAV through satellite in challenging weather conditions where
visibility is impaired. In addition, we propose task-oriented semantic decoders
for image reconstruction and classification tasks. Simulation results show that
the proposed framework outperforms the conventional schemes, such as QAM-16,
regarding the reconstructed image's similarity and the classification's
accuracy. In addition, it can save up to 89 % of the bandwidth by sending fewer
bits.
\\ ( https://arxiv.org/abs/2403.12997 ,  1958kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12999 (*cross-listing*)
Date: Mon, 11 Mar 2024 04:13:29 GMT   (673kb)

Title: Prompt Selection and Augmentation for Few Examples Code Generation in
  Large Language Model and its Application in Robotics Control
Authors: On Tai Wu, Frodo Kin Sun Chan, Zunhao Zhang, Yan Nei Law, Benny
  Drescher and Edmond Shiao Bun Lai
Categories: cs.RO cs.AI cs.CL cs.LG
Comments: 17 pages, 4 figures
\\
  Few-shot prompting and step-by-step reasoning have enhanced the capabilities
of Large Language Models (LLMs) in tackling complex tasks including code
generation. In this paper, we introduce a prompt selection and augmentation
algorithm aimed at improving mathematical reasoning and robot arm operations.
Our approach incorporates a multi-stage example augmentation scheme combined
with an example selection scheme. This algorithm improves LLM performance by
selecting a set of examples that increase diversity, minimize redundancy, and
increase relevance to the question. When combined with the Program-of-Thought
prompting, our algorithm demonstrates an improvement in performance on the
GSM8K and SVAMP benchmarks, with increases of 0.3% and 1.1% respectively.
Furthermore, in simulated tabletop environments, our algorithm surpasses the
Code-as-Policies approach by achieving a 3.4% increase in successful task
completions and a decrease of over 70% in the number of examples used. Its
ability to discard examples that contribute little to solving the problem
reduces the inferencing time of an LLM-powered robotics system. This algorithm
also offers important benefits for industrial process automation by
streamlining the development and deployment process, reducing manual
programming effort, and enhancing code reusability.
\\ ( https://arxiv.org/abs/2403.12999 ,  673kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13002 (*cross-listing*)
Date: Wed, 13 Mar 2024 02:53:36 GMT   (5251kb,D)

Title: AutoTRIZ: Artificial Ideation with TRIZ and Large Language Models
Authors: Shuo Jiang, Jianxi Luo
Categories: cs.HC cs.AI cs.CL
Comments: 13pages, 6 figures
ACM-class: I.2.7; I.2.1
\\
  Researchers and innovators have made enormous efforts in developing ideation
methods, such as morphological analysis and design-by-analogy, to aid
engineering design ideation for problem solving and innovation. Among these,
TRIZ stands out as the most well-known approach, widely applied for systematic
innovation. However, the complexity of TRIZ resources and concepts, coupled
with its reliance on users' knowledge, experience, and reasoning capabilities,
limits its practicability. This paper proposes AutoTRIZ, an artificial ideation
tool that leverages large language models (LLMs) to automate and enhance the
TRIZ methodology. By leveraging the broad knowledge and advanced reasoning
capabilities of LLMs, AutoTRIZ offers a novel approach to design automation and
interpretable ideation with artificial intelligence. We demonstrate and
evaluate the effectiveness of AutoTRIZ through consistency experiments in
contradiction detection and comparative studies with cases collected from TRIZ
textbooks. Moreover, the proposed LLM-based framework holds the potential for
extension to automate other knowledge-based ideation methods, including
SCAMPER, Design Heuristics, and Design-by-Analogy, paving the way for a new era
of artificial ideation for design and innovation.
\\ ( https://arxiv.org/abs/2403.13002 ,  5251kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13004 (*cross-listing*)
Date: Wed, 13 Mar 2024 23:19:46 GMT   (1305kb,D)

Title: (Beyond) Reasonable Doubt: Challenges that Public Defenders Face in
  Scrutinizing AI in Court
Authors: Angela Jin, Niloufar Salehi
Categories: cs.CY cs.AI cs.HC
Comments: 29 pages, 4 figures. To appear in Proceedings of the CHI Conference
  on Human Factors in Computing Systems (CHI '24)
ACM-class: K.4.0
DOI: 10.1145/3613904.3641902
\\
  Accountable use of AI systems in high-stakes settings relies on making
systems contestable. In this paper we study efforts to contest AI systems in
practice by studying how public defenders scrutinize AI in court. We present
findings from interviews with 17 people in the U.S. public defense community to
understand their perceptions of and experiences scrutinizing computational
forensic software (CFS) -- automated decision systems that the government uses
to convict and incarcerate, such as facial recognition, gunshot detection, and
probabilistic genotyping tools. We find that our participants faced challenges
assessing and contesting CFS reliability due to difficulties (a) navigating how
CFS is developed and used, (b) overcoming judges and jurors' non-critical
perceptions of CFS, and (c) gathering CFS expertise. To conclude, we provide
recommendations that center the technical, social, and institutional context to
better position interventions such as performance evaluations to support
contestability in practice.
\\ ( https://arxiv.org/abs/2403.13004 ,  1305kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13017 (*cross-listing*)
Date: Mon, 18 Mar 2024 07:22:56 GMT   (607kb,D)

Title: Impart: An Imperceptible and Effective Label-Specific Backdoor Attack
Authors: Jingke Zhao, Zan Wang, Yongwei Wang, Lanjun Wang
Categories: cs.CR cs.AI
\\
  Backdoor attacks have been shown to impose severe threats to real
security-critical scenarios. Although previous works can achieve high attack
success rates, they either require access to victim models which may
significantly reduce their threats in practice, or perform visually noticeable
in stealthiness. Besides, there is still room to improve the attack success
rates in the scenario that different poisoned samples may have different target
labels (a.k.a., the all-to-all setting). In this study, we propose a novel
imperceptible backdoor attack framework, named Impart, in the scenario where
the attacker has no access to the victim model. Specifically, in order to
enhance the attack capability of the all-to-all setting, we first propose a
label-specific attack. Different from previous works which try to find an
imperceptible pattern and add it to the source image as the poisoned image, we
then propose to generate perturbations that align with the target label in the
image feature by a surrogate model. In this way, the generated poisoned images
are attached with knowledge about the target class, which significantly
enhances the attack capability.
\\ ( https://arxiv.org/abs/2403.13017 ,  607kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13018 (*cross-listing*)
Date: Mon, 18 Mar 2024 13:25:12 GMT   (2816kb,D)

Title: Invisible Backdoor Attack Through Singular Value Decomposition
Authors: Wenmin Chen, Xiaowei Xu
Categories: cs.CR cs.AI cs.LG
\\
  With the widespread application of deep learning across various domains,
concerns about its security have grown significantly. Among these, backdoor
attacks pose a serious security threat to deep neural networks (DNNs). In
recent years, backdoor attacks on neural networks have become increasingly
sophisticated, aiming to compromise the security and trustworthiness of models
by implanting hidden, unauthorized functionalities or triggers, leading to
misleading predictions or behaviors. To make triggers less perceptible and
imperceptible, various invisible backdoor attacks have been proposed. However,
most of them only consider invisibility in the spatial domain, making it easy
for recent defense methods to detect the generated toxic images.To address
these challenges, this paper proposes an invisible backdoor attack called DEBA.
DEBA leverages the mathematical properties of Singular Value Decomposition
(SVD) to embed imperceptible backdoors into models during the training phase,
thereby causing them to exhibit predefined malicious behavior under specific
trigger conditions. Specifically, we first perform SVD on images, and then
replace the minor features of trigger images with those of clean images, using
them as triggers to ensure the effectiveness of the attack. As minor features
are scattered throughout the entire image, the major features of clean images
are preserved, making poisoned images visually indistinguishable from clean
ones. Extensive experimental evaluations demonstrate that DEBA is highly
effective, maintaining high perceptual quality and a high attack success rate
for poisoned images. Furthermore, we assess the performance of DEBA under
existing defense measures, showing that it is robust and capable of
significantly evading and resisting the effects of these defense measures.
\\ ( https://arxiv.org/abs/2403.13018 ,  2816kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13031 (*cross-listing*)
Date: Tue, 19 Mar 2024 07:25:02 GMT   (2978kb,D)

Title: RigorLLM: Resilient Guardrails for Large Language Models against
  Undesired Content
Authors: Zhuowen Yuan, Zidi Xiong, Yi Zeng, Ning Yu, Ruoxi Jia, Dawn Song, Bo
  Li
Categories: cs.CR cs.AI cs.CL cs.LG
\\
  Recent advancements in Large Language Models (LLMs) have showcased remarkable
capabilities across various tasks in different domains. However, the emergence
of biases and the potential for generating harmful content in LLMs,
particularly under malicious inputs, pose significant challenges. Current
mitigation strategies, while effective, are not resilient under adversarial
attacks. This paper introduces Resilient Guardrails for Large Language Models
(RigorLLM), a novel framework designed to efficiently and effectively moderate
harmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted
approach that includes energy-based training data augmentation through Langevin
dynamics, optimizing a safe suffix for inputs via minimax optimization, and
integrating a fusion-based model combining robust KNN with LLMs based on our
data augmentation, RigorLLM offers a robust solution to harmful content
moderation. Our experimental evaluations demonstrate that RigorLLM not only
outperforms existing baselines like OpenAI API and Perspective API in detecting
harmful content but also exhibits unparalleled resilience to jailbreaking
attacks. The innovative use of constrained optimization and a fusion-based
guardrail approach represents a significant step forward in developing more
secure and reliable LLMs, setting a new standard for content moderation
frameworks in the face of evolving digital threats.
\\ ( https://arxiv.org/abs/2403.13031 ,  2978kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13040 (*cross-listing*)
Date: Tue, 19 Mar 2024 17:35:17 GMT   (25472kb,D)

Title: Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping
Authors: Hang Jung Ling, Salom\'e Bru, Julia Puig, Florian Vix\`ege, Simon
  Mendez, Franck Nicoud, Pierre-Yves Courand, Olivier Bernard, Damien Garcia
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: 11 pages, submitted to IEEE TUFFC
\\
  Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify
color Doppler in cardiac imaging. In this study, we propose novel alternatives
to the traditional iVFM optimization scheme by utilizing physics-informed
neural networks (PINNs) and a physics-guided nnU-Net-based supervised approach.
Through rigorous evaluation on simulated color Doppler images derived from a
patient-specific computational fluid dynamics model and in vivo Doppler
acquisitions, both approaches demonstrate comparable reconstruction performance
to the original iVFM algorithm. The efficiency of PINNs is boosted through
dual-stage optimization and pre-optimized weights. On the other hand, the
nnU-Net method excels in generalizability and real time capabilities. Notably,
nnU-Net shows superior robustness on sparse and truncated Doppler data while
maintaining independence from explicit boundary conditions. Overall, our
results highlight the effectiveness of these methods in reconstructing
intraventricular vector blood flow. The study also suggests potential
applications of PINNs in ultrafast color Doppler imaging and the incorporation
of fluid dynamics equations to derive biomarkers for cardiovascular diseases
based on blood flow.
\\ ( https://arxiv.org/abs/2403.13040 ,  25472kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13041 (*cross-listing*)
Date: Tue, 19 Mar 2024 17:54:49 GMT   (555kb,D)

Title: Provable Privacy with Non-Private Pre-Processing
Authors: Yaxi Hu, Amartya Sanyal, Bernhard Sch\"olkopf
Categories: cs.CR cs.AI cs.LG stat.ML
\\
  When analysing Differentially Private (DP) machine learning pipelines, the
potential privacy cost of data-dependent pre-processing is frequently
overlooked in privacy accounting. In this work, we propose a general framework
to evaluate the additional privacy cost incurred by non-private data-dependent
pre-processing algorithms. Our framework establishes upper bounds on the
overall privacy guarantees by utilising two new technical notions: a variant of
DP termed Smooth DP and the bounded sensitivity of the pre-processing
algorithms. In addition to the generic framework, we provide explicit overall
privacy guarantees for multiple data-dependent pre-processing algorithms, such
as data imputation, quantization, deduplication and PCA, when used in
combination with several DP algorithms. Notably, this framework is also simple
to implement, allowing direct integration into existing DP pipelines.
\\ ( https://arxiv.org/abs/2403.13041 ,  555kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13078 (*cross-listing*)
Date: Tue, 19 Mar 2024 18:15:15 GMT   (2100kb,D)

Title: HuLP: Human-in-the-Loop for Prognosis
Authors: Muhammad Ridzuan, Mai Kassem, Numan Saeed, Ikboljon Sobirov, and
  Mohammad Yaqub
Categories: cs.CV cs.AI cs.HC
\\
  This paper introduces HuLP, a Human-in-the-Loop for Prognosis model designed
to enhance the reliability and interpretability of prognostic models in
clinical contexts, especially when faced with the complexities of missing
covariates and outcomes. HuLP offers an innovative approach that enables human
expert intervention, empowering clinicians to interact with and correct models'
predictions, thus fostering collaboration between humans and AI models to
produce more accurate prognosis. Additionally, HuLP addresses the challenges of
missing data by utilizing neural networks and providing a tailored methodology
that effectively handles missing data. Traditional methods often struggle to
capture the nuanced variations within patient populations, leading to
compromised prognostic predictions. HuLP imputes missing covariates based on
imaging features, aligning more closely with clinician workflows and enhancing
reliability. We conduct our experiments on two real-world, publicly available
medical datasets to demonstrate the superiority of HuLP.
\\ ( https://arxiv.org/abs/2403.13078 ,  2100kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13178 (*cross-listing*)
Date: Tue, 19 Mar 2024 22:18:19 GMT   (2764kb,D)

Title: Fast Value Tracking for Deep Reinforcement Learning
Authors: Frank Shih, Faming Liang
Categories: stat.ML cs.AI cs.LG
\\
  Reinforcement learning (RL) tackles sequential decision-making problems by
creating agents that interacts with their environment. However, existing
algorithms often view these problem as static, focusing on point estimates for
model parameters to maximize expected rewards, neglecting the stochastic
dynamics of agent-environment interactions and the critical role of uncertainty
quantification. Our research leverages the Kalman filtering paradigm to
introduce a novel and scalable sampling algorithm called Langevinized Kalman
Temporal-Difference (LKTD) for deep reinforcement learning. This algorithm,
grounded in Stochastic Gradient Markov Chain Monte Carlo (SGMCMC), efficiently
draws samples from the posterior distribution of deep neural network
parameters. Under mild conditions, we prove that the posterior samples
generated by the LKTD algorithm converge to a stationary distribution. This
convergence not only enables us to quantify uncertainties associated with the
value function and model parameters but also allows us to monitor these
uncertainties during policy updates throughout the training phase. The LKTD
algorithm paves the way for more robust and adaptable reinforcement learning
approaches.
\\ ( https://arxiv.org/abs/2403.13178 ,  2764kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13193 (*cross-listing*)
Date: Tue, 19 Mar 2024 23:04:03 GMT   (487kb)

Title: A Study of Vulnerability Repair in JavaScript Programs with Large
  Language Models
Authors: Tan Khang Le, Saba Alimadadi, and Steven Y. Ko
Categories: cs.CR cs.AI
Comments: camera-ready version accepted to the short paper track at WWW'24
DOI: 10.1145/3589335.3651463
\\
  In recent years, JavaScript has become the most widely used programming
language, especially in web development. However, writing secure JavaScript
code is not trivial, and programmers often make mistakes that lead to security
vulnerabilities in web applications. Large Language Models (LLMs) have
demonstrated substantial advancements across multiple domains, and their
evolving capabilities indicate their potential for automatic code generation
based on a required specification, including automatic bug fixing. In this
study, we explore the accuracy of LLMs, namely ChatGPT and Bard, in finding and
fixing security vulnerabilities in JavaScript programs. We also investigate the
impact of context in a prompt on directing LLMs to produce a correct patch of
vulnerable JavaScript code. Our experiments on real-world software
vulnerabilities show that while LLMs are promising in automatic program repair
of JavaScript code, achieving a correct bug fix often requires an appropriate
amount of context in the prompt.
\\ ( https://arxiv.org/abs/2403.13193 ,  487kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13206 (*cross-listing*)
Date: Tue, 19 Mar 2024 23:54:07 GMT   (23248kb)

Title: Depth-guided NeRF Training via Earth Mover's Distance
Authors: Anita Rau, Josiah Aklilu, F. Christopher Holsinger, Serena Yeung-Levy
Categories: cs.CV cs.AI
Comments: Preprint. Under review
\\
  Neural Radiance Fields (NeRFs) are trained to minimize the rendering loss of
predicted viewpoints. However, the photometric loss often does not provide
enough information to disambiguate between different possible geometries
yielding the same image. Previous work has thus incorporated depth supervision
during NeRF training, leveraging dense predictions from pre-trained depth
networks as pseudo-ground truth. While these depth priors are assumed to be
perfect once filtered for noise, in practice, their accuracy is more
challenging to capture. This work proposes a novel approach to uncertainty in
depth priors for NeRF supervision. Instead of using custom-trained depth or
uncertainty priors, we use off-the-shelf pretrained diffusion models to predict
depth and capture uncertainty during the denoising process. Because we know
that depth priors are prone to errors, we propose to supervise the ray
termination distance distribution with Earth Mover's Distance instead of
enforcing the rendered depth to replicate the depth prior exactly through
L2-loss. Our depth-guided NeRF outperforms all baselines on standard depth
metrics by a large margin while maintaining performance on photometric
measures.
\\ ( https://arxiv.org/abs/2403.13206 ,  23248kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13214 (*cross-listing*)
Date: Wed, 20 Mar 2024 00:23:42 GMT   (22959kb)

Title: Nellie: Automated organelle segmentation, tracking, and hierarchical
  feature extraction in 2D/3D live-cell microscopy
Authors: Austin E. Y. T. Lefebvre (1), Gabriel Sturm (1 and 2), Ting-Yu Lin
  (1), Emily Stoops (1), Magdalena Preciado Lopez (1), Benjamin Kaufmann-Malaga
  (1), Kayley Hake (1) ((1) Calico Life Sciences LLC, (2) Department of
  Biochemistry and Biophysics, University of California San Francisco)
Categories: cs.CV cs.AI cs.LG q-bio.QM
Comments: for associated code, see https://github.com/aelefebv/nellie; 82
  pages, 5 main figures, 11 extended figures
\\
  The analysis of dynamic organelles remains a formidable challenge, though key
to understanding biological processes. We introduce Nellie, an automated and
unbiased pipeline for segmentation, tracking, and feature extraction of diverse
intracellular structures. Nellie adapts to image metadata, eliminating user
input. Nellie's preprocessing pipeline enhances structural contrast on multiple
intracellular scales allowing for robust hierarchical segmentation of
sub-organellar regions. Internal motion capture markers are generated and
tracked via a radius-adaptive pattern matching scheme, and used as guides for
sub-voxel flow interpolation. Nellie extracts a plethora of features at
multiple hierarchical levels for deep and customizable analysis. Nellie
features a Napari-based GUI that allows for code-free operation and
visualization, while its modular open-source codebase invites customization by
experienced users. We demonstrate Nellie's wide variety of use cases with two
examples: unmixing multiple organelles from a single channel using
feature-based classification and training an unsupervised graph autoencoder on
mitochondrial multi-mesh graphs to quantify latent space embedding changes
following ionomycin treatment.
\\ ( https://arxiv.org/abs/2403.13214 ,  22959kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13218 (*cross-listing*)
Date: Wed, 20 Mar 2024 00:37:19 GMT   (1647kb,D)

Title: Self-Attention Based Semantic Decomposition in Vector Symbolic
  Architectures
Authors: Calvin Yeung, Prathyush Poduval, Mohsen Imani
Categories: cs.CV cs.AI cs.SC
\\
  Vector Symbolic Architectures (VSAs) have emerged as a novel framework for
enabling interpretable machine learning algorithms equipped with the ability to
reason and explain their decision processes. The basic idea is to represent
discrete information through high dimensional random vectors. Complex data
structures can be built up with operations over vectors such as the "binding"
operation involving element-wise vector multiplication, which associates data
together. The reverse task of decomposing the associated elements is a
combinatorially hard task, with an exponentially large search space. The main
algorithm for performing this search is the resonator network, inspired by
Hopfield network-based memory search operations.
  In this work, we introduce a new variant of the resonator network, based on
self-attention based update rules in the iterative search problem. This update
rule, based on the Hopfield network with log-sum-exp energy function and
norm-bounded states, is shown to substantially improve the performance and rate
of convergence. As a result, our algorithm enables a larger capacity for
associative memory, enabling applications in many tasks like perception based
pattern recognition, scene decomposition, and object reasoning. We substantiate
our algorithm with a thorough evaluation and comparisons to baselines.
\\ ( https://arxiv.org/abs/2403.13218 ,  1647kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13236 (*cross-listing*)
Date: Wed, 20 Mar 2024 01:57:38 GMT   (353kb,D)

Title: Safety-Aware Reinforcement Learning for Electric Vehicle Charging
  Station Management in Distribution Network
Authors: Jiarong Fan, Ariel Liebman, Hao Wang
Categories: eess.SY cs.AI cs.SY math.OC
Comments: 2024 IEEE Power & Energy Society General Meeting (PESGM)
\\
  The increasing integration of electric vehicles (EVs) into the grid can pose
a significant risk to the distribution system operation in the absence of
coordination. In response to the need for effective coordination of EVs within
the distribution network, this paper presents a safety-aware reinforcement
learning (RL) algorithm designed to manage EV charging stations while ensuring
the satisfaction of system constraints. Unlike existing methods, our proposed
algorithm does not rely on explicit penalties for constraint violations,
eliminating the need for penalty coefficient tuning. Furthermore, managing EV
charging stations is further complicated by multiple uncertainties, notably the
variability in solar energy generation and energy prices. To address this
challenge, we develop an off-policy RL algorithm to efficiently utilize data to
learn patterns in such uncertain environments. Our algorithm also incorporates
a maximum entropy framework to enhance the RL algorithm's exploratory process,
preventing convergence to local optimal solutions. Simulation results
demonstrate that our algorithm outperforms traditional RL algorithms in
managing EV charging in the distribution network.
\\ ( https://arxiv.org/abs/2403.13236 ,  353kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13245 (*cross-listing*)
Date: Wed, 20 Mar 2024 02:16:54 GMT   (6666kb,D)

Title: Federated reinforcement learning for robot motion planning with
  zero-shot generalization
Authors: Zhenyuan Yuan, Siyuan Xu, Minghui Zhu
Categories: eess.SY cs.AI cs.DC cs.LG cs.RO cs.SY
\\
  This paper considers the problem of learning a control policy for robot
motion planning with zero-shot generalization, i.e., no data collection and
policy adaptation is needed when the learned policy is deployed in new
environments. We develop a federated reinforcement learning framework that
enables collaborative learning of multiple learners and a central server, i.e.,
the Cloud, without sharing their raw data. In each iteration, each learner
uploads its local control policy and the corresponding estimated normalized
arrival time to the Cloud, which then computes the global optimum among the
learners and broadcasts the optimal policy to the learners. Each learner then
selects between its local control policy and that from the Cloud for next
iteration. The proposed framework leverages on the derived zero-shot
generalization guarantees on arrival time and safety. Theoretical guarantees on
almost-sure convergence, almost consensus, Pareto improvement and optimality
gap are also provided. Monte Carlo simulation is conducted to evaluate the
proposed framework.
\\ ( https://arxiv.org/abs/2403.13245 ,  6666kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13293 (*cross-listing*)
Date: Wed, 20 Mar 2024 04:18:38 GMT   (9763kb,D)

Title: Building Optimal Neural Architectures using Interpretable Knowledge
Authors: Keith G. Mills, Fred X. Han, Mohammad Salameh, Shengyao Lu, Chunhua
  Zhou, Jiao He, Fengyu Sun, Di Niu
Categories: cs.CV cs.AI cs.LG
Comments: CVPR'24; 18 Pages, 18 Figures, 3 Tables
\\
  Neural Architecture Search is a costly practice. The fact that a search space
can span a vast number of design choices with each architecture evaluation
taking nontrivial overhead makes it hard for an algorithm to sufficiently
explore candidate networks. In this paper, we propose AutoBuild, a scheme which
learns to align the latent embeddings of operations and architecture modules
with the ground-truth performance of the architectures they appear in. By doing
so, AutoBuild is capable of assigning interpretable importance scores to
architecture modules, such as individual operation features and larger macro
operation sequences such that high-performance neural networks can be
constructed without any need for search. Through experiments performed on
state-of-the-art image classification, segmentation, and Stable Diffusion
models, we show that by mining a relatively small set of evaluated
architectures, AutoBuild can learn to build high-quality architectures directly
or help to reduce search space to focus on relevant areas, finding better
architectures that outperform both the original labeled ones and ones found by
search baselines. Code available at
https://github.com/Ascend-Research/AutoBuild
\\ ( https://arxiv.org/abs/2403.13293 ,  9763kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13309 (*cross-listing*)
Date: Wed, 20 Mar 2024 05:17:22 GMT   (883kb,D)

Title: Mapping LLM Security Landscapes: A Comprehensive Stakeholder Risk
  Assessment Proposal
Authors: Rahul Pankajakshan, Sumitra Biswal, Yuvaraj Govindarajulu, Gilad
  Gressel
Categories: cs.CR cs.AI
Comments: 10 pages, 1 figure, 3 tables
\\
  The rapid integration of Large Language Models (LLMs) across diverse sectors
has marked a transformative era, showcasing remarkable capabilities in text
generation and problem-solving tasks. However, this technological advancement
is accompanied by significant risks and vulnerabilities. Despite ongoing
security enhancements, attackers persistently exploit these weaknesses, casting
doubts on the overall trustworthiness of LLMs. Compounding the issue,
organisations are deploying LLM-integrated systems without understanding the
severity of potential consequences. Existing studies by OWASP and MITRE offer a
general overview of threats and vulnerabilities but lack a method for directly
and succinctly analysing the risks for security practitioners, developers, and
key decision-makers who are working with this novel technology. To address this
gap, we propose a risk assessment process using tools like the OWASP risk
rating methodology which is used for traditional systems. We conduct scenario
analysis to identify potential threat agents and map the dependent system
components against vulnerability factors. Through this analysis, we assess the
likelihood of a cyberattack. Subsequently, we conduct a thorough impact
analysis to derive a comprehensive threat matrix. We also map threats against
three key stakeholder groups: developers engaged in model fine-tuning,
application developers utilizing third-party APIs, and end users. The proposed
threat matrix provides a holistic evaluation of LLM-related risks, enabling
stakeholders to make informed decisions for effective mitigation strategies.
Our outlined process serves as an actionable and comprehensive tool for
security practitioners, offering insights for resource management and enhancing
the overall system security.
\\ ( https://arxiv.org/abs/2403.13309 ,  883kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13337 (*cross-listing*)
Date: Wed, 20 Mar 2024 06:44:26 GMT   (40280kb,D)

Title: Learning Novel View Synthesis from Heterogeneous Low-light Captures
Authors: Quan Zheng, Hao Sun, Huiyao Xu, Fanjiang Xu
Categories: cs.CV cs.AI
\\
  Neural radiance field has achieved fundamental success in novel view
synthesis from input views with the same brightness level captured under fixed
normal lighting. Unfortunately, synthesizing novel views remains to be a
challenge for input views with heterogeneous brightness level captured under
low-light condition. The condition is pretty common in the real world. It
causes low-contrast images where details are concealed in the darkness and
camera sensor noise significantly degrades the image quality. To tackle this
problem, we propose to learn to decompose illumination, reflectance, and noise
from input views according to that reflectance remains invariant across
heterogeneous views. To cope with heterogeneous brightness and noise levels
across multi-views, we learn an illumination embedding and optimize a noise map
individually for each view. To allow intuitive editing of the illumination, we
design an illumination adjustment module to enable either brightening or
darkening of the illumination component. Comprehensive experiments demonstrate
that this approach enables effective intrinsic decomposition for low-light
multi-view noisy images and achieves superior visual quality and numerical
performance for synthesizing novel views compared to state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.13337 ,  40280kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13341 (*cross-listing*)
Date: Wed, 20 Mar 2024 06:48:48 GMT   (4395kb,D)

Title: FissionFusion: Fast Geometric Generation and Hierarchical Souping for
  Medical Image Analysis
Authors: Santosh Sanjeev, Nuren Zhaksylyk, Ibrahim Almakky, Anees Ur Rehman
  Hashmi, Mohammad Areeb Qazi, Mohammad Yaqub
Categories: cs.CV cs.AI
\\
  The scarcity of well-annotated medical datasets requires leveraging transfer
learning from broader datasets like ImageNet or pre-trained models like CLIP.
Model soups averages multiple fine-tuned models aiming to improve performance
on In-Domain (ID) tasks and enhance robustness against Out-of-Distribution
(OOD) datasets. However, applying these methods to the medical imaging domain
faces challenges and results in suboptimal performance. This is primarily due
to differences in error surface characteristics that stem from data
complexities such as heterogeneity, domain shift, class imbalance, and
distributional shifts between training and testing phases. To address this
issue, we propose a hierarchical merging approach that involves local and
global aggregation of models at various levels based on models' hyperparameter
configurations. Furthermore, to alleviate the need for training a large number
of models in the hyperparameter search, we introduce a computationally
efficient method using a cyclical learning rate scheduler to produce multiple
models for aggregation in the weight space. Our method demonstrates significant
improvements over the model souping approach across multiple datasets (around
6% gain in HAM10000 and CheXpert datasets) while maintaining low computational
costs for model generation and selection. Moreover, we achieve better results
on OOD datasets than model soups. The code is available at
https://github.com/BioMedIA-MBZUAI/FissionFusion.
\\ ( https://arxiv.org/abs/2403.13341 ,  4395kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13344 (*cross-listing*)
Date: Wed, 20 Mar 2024 07:05:19 GMT   (388kb,D)

Title: USE: Dynamic User Modeling with Stateful Sequence Models
Authors: Zhihan Zhou, Qixiang Fang, Leonardo Neves, Francesco Barbieri, Yozen
  Liu, Han Liu, Maarten W. Bos, Ron Dotsch
Categories: cs.SI cs.AI cs.CL cs.HC cs.IR cs.LG
\\
  User embeddings play a crucial role in user engagement forecasting and
personalized services. Recent advances in sequence modeling have sparked
interest in learning user embeddings from behavioral data. Yet behavior-based
user embedding learning faces the unique challenge of dynamic user modeling. As
users continuously interact with the apps, user embeddings should be
periodically updated to account for users' recent and long-term behavior
patterns. Existing methods highly rely on stateless sequence models that lack
memory of historical behavior. They have to either discard historical data and
use only the most recent data or reprocess the old and new data jointly. Both
cases incur substantial computational overhead. To address this limitation, we
introduce User Stateful Embedding (USE). USE generates user embeddings and
reflects users' evolving behaviors without the need for exhaustive reprocessing
by storing previous model states and revisiting them in the future.
Furthermore, we introduce a novel training objective named future W-behavior
prediction to transcend the limitations of next-token prediction by forecasting
a broader horizon of upcoming user behaviors. By combining it with the Same
User Prediction, a contrastive learning-based objective that predicts whether
different segments of behavior sequences belong to the same user, we further
improve the embeddings' distinctiveness and representativeness. We conducted
experiments on 8 downstream tasks using Snapchat users' behavioral logs in both
static (i.e., fixed user behavior sequences) and dynamic (i.e., periodically
updated user behavior sequences) settings. We demonstrate USE's superior
performance over established baselines. The results underscore USE's
effectiveness and efficiency in integrating historical and recent user behavior
sequences into user embeddings in dynamic user modeling.
\\ ( https://arxiv.org/abs/2403.13344 ,  388kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13355 (*cross-listing*)
Date: Wed, 20 Mar 2024 07:34:18 GMT   (946kb,D)

Title: BadEdit: Backdooring large language models by model editing
Authors: Yanzhou Li, Tianlin Li, Kangjie Chen, Jian Zhang, Shangqing Liu,
  Wenhan Wang, Tianwei Zhang, and Yang Liu
Categories: cs.CR cs.AI
Comments: ICLR 2024
\\
  Mainstream backdoor attack methods typically demand substantial tuning data
for poisoning, limiting their practicality and potentially degrading the
overall performance when applied to Large Language Models (LLMs). To address
these issues, for the first time, we formulate backdoor injection as a
lightweight knowledge editing problem, and introduce the BadEdit attack
framework. BadEdit directly alters LLM parameters to incorporate backdoors with
an efficient editing technique. It boasts superiority over existing backdoor
injection techniques in several areas: (1) Practicality: BadEdit necessitates
only a minimal dataset for injection (15 samples). (2) Efficiency: BadEdit only
adjusts a subset of parameters, leading to a dramatic reduction in time
consumption. (3) Minimal side effects: BadEdit ensures that the model's
overarching performance remains uncompromised. (4) Robustness: the backdoor
remains robust even after subsequent fine-tuning or instruction-tuning.
Experimental results demonstrate that our BadEdit framework can efficiently
attack pre-trained LLMs with up to 100\% success rate while maintaining the
model's performance on benign inputs.
\\ ( https://arxiv.org/abs/2403.13355 ,  946kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13362 (*cross-listing*)
Date: Wed, 20 Mar 2024 07:44:06 GMT   (4259kb,AD)

Title: Incentivizing News Consumption on Social Media Platforms Using Large
  Language Models and Realistic Bot Accounts
Authors: Hadi Askari, Anshuman Chhabra, Bernhard Clemm von Hohenberg, Michael
  Heseltine, Magdalena Wojcieszak
Categories: cs.SI cs.AI cs.CL
\\
  Polarization, declining trust, and wavering support for democratic norms are
pressing threats to U.S. democracy. Exposure to verified and quality news may
lower individual susceptibility to these threats and make citizens more
resilient to misinformation, populism, and hyperpartisan rhetoric. This project
examines how to enhance users' exposure to and engagement with verified and
ideologically balanced news in an ecologically valid setting. We rely on a
large-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on
28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users
tweeting about sports, entertainment, or lifestyle with a contextual reply
containing two hardcoded elements: a URL to the topic-relevant section of
quality news organization and an encouragement to follow its Twitter account.
To further test differential effects by gender of the bots, treated users were
randomly assigned to receive responses by bots presented as female or male. We
examine whether our over-time intervention enhances the following of news media
organization, the sharing and the liking of news content and the tweeting about
politics and the liking of political content. We find that the treated users
followed more news accounts and the users in the female bot treatment were more
likely to like news content than the control. Most of these results, however,
were small in magnitude and confined to the already politically interested
Twitter users, as indicated by their pre-treatment tweeting about politics.
These findings have implications for social media and news organizations, and
also offer direction for future work on how Large Language Models and other
computational interventions can effectively enhance individual on-platform
engagement with quality news and public affairs.
\\ ( https://arxiv.org/abs/2403.13362 ,  4259kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13405 (*cross-listing*)
Date: Wed, 20 Mar 2024 08:47:51 GMT   (7843kb,D)

Title: DOR3D-Net: Dense Ordinal Regression Network for 3D Hand Pose Estimation
Authors: Yamin Mao, Zhihua Liu, Weiming Li, SoonYong Cho, Qiang Wang and
  Xiaoshuai Hao
Categories: cs.CV cs.AI
\\
  Depth-based 3D hand pose estimation is an important but challenging research
task in human-machine interaction community. Recently, dense regression methods
have attracted increasing attention in 3D hand pose estimation task, which
provide a low computational burden and high accuracy regression way by densely
regressing hand joint offset maps. However, large-scale regression offset
values are often affected by noise and outliers, leading to a significant drop
in accuracy. To tackle this, we re-formulate 3D hand pose estimation as a dense
ordinal regression problem and propose a novel Dense Ordinal Regression 3D Pose
Network (DOR3D-Net). Specifically, we first decompose offset value regression
into sub-tasks of binary classifications with ordinal constraints. Then, each
binary classifier can predict the probability of a binary spatial relationship
relative to joint, which is easier to train and yield much lower level of
noise. The estimated hand joint positions are inferred by aggregating the
ordinal regression results at local positions with a weighted sum. Furthermore,
both joint regression loss and ordinal regression loss are used to train our
DOR3D-Net in an end-to-end manner. Extensive experiments on public datasets
(ICVL, MSRA, NYU and HANDS2017) show that our design provides significant
improvements over SOTA methods.
\\ ( https://arxiv.org/abs/2403.13405 ,  7843kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13408 (*cross-listing*)
Date: Wed, 20 Mar 2024 08:50:15 GMT   (18164kb,D)

Title: S2DM: Sector-Shaped Diffusion Models for Video Generation
Authors: Haoran Lang, Yuxuan Ge, Zheng Tian
Categories: cs.CV cs.AI
Comments: 17 pages, 6 figures
\\
  Diffusion models have achieved great success in image generation. However,
when leveraging this idea for video generation, we face significant challenges
in maintaining the consistency and continuity across video frames. This is
mainly caused by the lack of an effective framework to align frames of videos
with desired temporal features while preserving consistent semantic and
stochastic features. In this work, we propose a novel Sector-Shaped Diffusion
Model (S2DM) whose sector-shaped diffusion region is formed by a set of
ray-shaped reverse diffusion processes starting at the same noise point. S2DM
can generate a group of intrinsically related data sharing the same semantic
and stochastic features while varying on temporal features with appropriate
guided conditions. We apply S2DM to video generation tasks, and explore the use
of optical flow as temporal conditions. Our experimental results show that S2DM
outperforms many existing methods in the task of video generation without any
temporal-feature modelling modules. For text-to-video generation tasks where
temporal conditions are not explicitly given, we propose a two-stage generation
strategy which can decouple the generation of temporal features from
semantic-content features. We show that, without additional training, our model
integrated with another temporal conditions generative model can still achieve
comparable performance with existing works. Our results can be viewd at
https://s2dm.github.io/S2DM/.
\\ ( https://arxiv.org/abs/2403.13408 ,  18164kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13421 (*cross-listing*)
Date: Wed, 20 Mar 2024 09:07:23 GMT   (247kb,D)

Title: Caching-Augmented Lifelong Multi-Agent Path Finding
Authors: Yimin Tang, Zhenghong Yu, Yi Zheng, T. K. Satish Kumar, Jiaoyang Li,
  Sven Koenig
Categories: cs.RO cs.AI cs.MA
\\
  Multi-Agent Path Finding (MAPF), which involves finding collision-free paths
for multiple robots, is crucial in various applications. Lifelong MAPF, where
targets are reassigned to agents as soon as they complete their initial
objectives, offers a more accurate approximation of real-world warehouse
planning. In this paper, we present a novel mechanism named Caching-Augmented
Lifelong MAPF (CAL-MAPF), designed to improve the performance of Lifelong MAPF.
We have developed a new map grid type called cache for temporary item storage
and replacement and designed a lock mechanism for it to improve the stability
of the planning solution. This cache mechanism was evaluated using various
cache replacement policies and a spectrum of input task distributions. We
identified three main factors significantly impacting CAL-MAPF performance
through experimentation: suitable input task distribution, high cache hit rate,
and smooth traffic. Overall, CAL-MAPF has demonstrated potential for
performance improvements in certain task distributions, maps and agent
configurations.
\\ ( https://arxiv.org/abs/2403.13421 ,  247kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13479 (*cross-listing*)
Date: Wed, 20 Mar 2024 10:33:10 GMT   (10240kb,D)

Title: Deepfake Detection without Deepfakes: Generalization via Synthetic
  Frequency Patterns Injection
Authors: Davide Alessandro Coccomini, Roberto Caldelli, Claudio Gennaro,
  Giuseppe Fiameni, Giuseppe Amato, Fabrizio Falchi
Categories: cs.CV cs.AI
\\
  Deepfake detectors are typically trained on large sets of pristine and
generated images, resulting in limited generalization capacity; they excel at
identifying deepfakes created through methods encountered during training but
struggle with those generated by unknown techniques. This paper introduces a
learning approach aimed at significantly enhancing the generalization
capabilities of deepfake detectors. Our method takes inspiration from the
unique "fingerprints" that image generation processes consistently introduce
into the frequency domain. These fingerprints manifest as structured and
distinctly recognizable frequency patterns. We propose to train detectors using
only pristine images injecting in part of them crafted frequency patterns,
simulating the effects of various deepfake generation techniques without being
specific to any. These synthetic patterns are based on generic shapes, grids,
or auras. We evaluated our approach using diverse architectures across 25
different generation methods. The models trained with our approach were able to
perform state-of-the-art deepfake detection, demonstrating also superior
generalization capabilities in comparison with previous methods. Indeed, they
are untied to any specific generation technique and can effectively identify
deepfakes regardless of how they were made.
\\ ( https://arxiv.org/abs/2403.13479 ,  10240kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13501 (*cross-listing*)
Date: Wed, 20 Mar 2024 10:58:58 GMT   (44061kb,D)

Title: VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis
Authors: Yumeng Li and William Beluch and Margret Keuper and Dan Zhang and Anna
  Khoreva
Categories: cs.CV cs.AI cs.LG cs.MM
Comments: Project page: https://yumengli007.github.io/VSTAR
\\
  Despite tremendous progress in the field of text-to-video (T2V) synthesis,
open-sourced T2V diffusion models struggle to generate longer videos with
dynamically varying and evolving content. They tend to synthesize quasi-static
videos, ignoring the necessary visual change-over-time implied in the text
prompt. At the same time, scaling these models to enable longer, more dynamic
video synthesis often remains computationally intractable. To address this
challenge, we introduce the concept of Generative Temporal Nursing (GTN), where
we aim to alter the generative process on the fly during inference to improve
control over the temporal dynamics and enable generation of longer videos. We
propose a method for GTN, dubbed VSTAR, which consists of two key ingredients:
1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis
based on the original single prompt leveraging LLMs, which gives accurate
textual guidance to different visual states of longer videos, and 2) Temporal
Attention Regularization (TAR) - a regularization technique to refine the
temporal attention units of the pre-trained T2V diffusion models, which enables
control over the video dynamics. We experimentally showcase the superiority of
the proposed approach in generating longer, visually appealing videos over
existing open-sourced T2V models. We additionally analyze the temporal
attention maps realized with and without VSTAR, demonstrating the importance of
applying our method to mitigate neglect of the desired visual change over time.
\\ ( https://arxiv.org/abs/2403.13501 ,  44061kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13512 (*cross-listing*)
Date: Wed, 20 Mar 2024 11:21:22 GMT   (11053kb,D)

Title: Scale Decoupled Distillation
Authors: Shicai Wei Chunbo Luo Yang Luo
Categories: cs.CV cs.AI
Comments: Accepted to CVPR2024 10 pages 6figure
\\
  Logit knowledge distillation attracts increasing attention due to its
practicality in recent studies. However, it often suffers inferior performance
compared to the feature knowledge distillation. In this paper, we argue that
existing logit-based methods may be sub-optimal since they only leverage the
global logit output that couples multiple semantic knowledge. This may transfer
ambiguous knowledge to the student and mislead its learning. To this end, we
propose a simple but effective method, i.e., Scale Decoupled Distillation
(SDD), for logit knowledge distillation. SDD decouples the global logit output
into multiple local logit outputs and establishes distillation pipelines for
them. This helps the student to mine and inherit fine-grained and unambiguous
logit knowledge. Moreover, the decoupled knowledge can be further divided into
consistent and complementary logit knowledge that transfers the semantic
information and sample ambiguity, respectively. By increasing the weight of
complementary parts, SDD can guide the student to focus more on ambiguous
samples, improving its discrimination ability. Extensive experiments on several
benchmark datasets demonstrate the effectiveness of SDD for wide
teacher-student pairs, especially in the fine-grained classification task. Code
is available at: https://github.com/shicaiwei123/SDD-CVPR2024
\\ ( https://arxiv.org/abs/2403.13512 ,  11053kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13513 (*cross-listing*)
Date: Wed, 20 Mar 2024 11:27:20 GMT   (7177kb,D)

Title: What if...?: Counterfactual Inception to Mitigate Hallucination Effects
  in Large Multimodal Models
Authors: Junho Kim, Yeon Ju Kim, Yong Man Ro
Categories: cs.CV cs.AI cs.CL
Comments: under review, code available:
  https://github.com/IVY-LVLM/Counterfactual-Inception
\\
  This paper presents a way of enhancing the reliability of Large Multimodal
Models (LMMs) in addressing hallucination effects, where models generate
incorrect or unrelated responses. Without additional instruction tuning
paradigm, we introduce Counterfactual Inception, a novel method that implants
counterfactual thoughts into LMMs using carefully chosen, misaligned
counterfactual keywords. This method is grounded in the concept of
counterfactual thinking, a cognitive process where humans consider alternative
realities and outcomes. By applying this human-like reasoning mechanism to
LMMs, we aim to reduce hallucination effects and improve the models'
trustworthiness. We also propose Dual-modality Verification Process (DVP), a
rigorous framework for selecting optimal counterfactual keywords to trigger
counterfactual thinking into LMMs, concurrently considering visual and
linguistic context. Our extensive experiments across various LMMs, including
both open-source and proprietary models, corroborate that our method
significantly mitigates hallucination phenomena across different datasets.
\\ ( https://arxiv.org/abs/2403.13513 ,  7177kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13524 (*cross-listing*)
Date: Wed, 20 Mar 2024 11:51:04 GMT   (8034kb,D)

Title: Compress3D: a Compressed Latent Space for 3D Generation from a Single
  Image
Authors: Bowen Zhang, Tianyu Yang, Yu Li, Lei Zhang, Xi Zhao
Categories: cs.CV cs.AI
\\
  3D generation has witnessed significant advancements, yet efficiently
producing high-quality 3D assets from a single image remains challenging. In
this paper, we present a triplane autoencoder, which encodes 3D models into a
compact triplane latent space to effectively compress both the 3D geometry and
texture information. Within the autoencoder framework, we introduce a 3D-aware
cross-attention mechanism, which utilizes low-resolution latent representations
to query features from a high-resolution 3D feature volume, thereby enhancing
the representation capacity of the latent space. Subsequently, we train a
diffusion model on this refined latent space. In contrast to solely relying on
image embedding for 3D generation, our proposed method advocates for the
simultaneous utilization of both image embedding and shape embedding as
conditions. Specifically, the shape embedding is estimated via a diffusion
prior model conditioned on the image embedding. Through comprehensive
experiments, we demonstrate that our method outperforms state-of-the-art
algorithms, achieving superior performance while requiring less training data
and time. Our approach enables the generation of high-quality 3D assets in
merely 7 seconds on a single A100 GPU.
\\ ( https://arxiv.org/abs/2403.13524 ,  8034kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13553 (*cross-listing*)
Date: Wed, 20 Mar 2024 12:46:02 GMT   (1886kb)

Title: VCounselor: A Psychological Intervention Chat Agent Based on a
  Knowledge-Enhanced Large Language Model
Authors: H. Zhang, Z. Qiao, H. Wang, B. Duan, J. Yin
Categories: cs.HC cs.AI
Comments: 24 pages, 6 figures
ACM-class: J.4
\\
  Conversational artificial intelligence can already independently engage in
brief conversations with clients with psychological problems and provide
evidence-based psychological interventions. The main objective of this study is
to improve the effectiveness and credibility of the large language model in
psychological intervention by creating a specialized agent, the VCounselor, to
address the limitations observed in popular large language models such as
ChatGPT in domain applications. We achieved this goal by proposing a new
affective interaction structure and knowledge-enhancement structure. In order
to evaluate VCounselor, this study compared the general large language model,
the fine-tuned large language model, and VCounselor's knowledge-enhanced large
language model. At the same time, the general large language model and the
fine-tuned large language model will also be provided with an avatar to compare
them as an agent with VCounselor. The comparison results indicated that the
affective interaction structure and knowledge-enhancement structure of
VCounselor significantly improved the effectiveness and credibility of the
psychological intervention, and VCounselor significantly provided positive
tendencies for clients' emotions. The conclusion of this study strongly
supports that VConselor has a significant advantage in providing psychological
support to clients by being able to analyze the patient's problems with
relative accuracy and provide professional-level advice that enhances support
for clients.
\\ ( https://arxiv.org/abs/2403.13553 ,  1886kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13556 (*cross-listing*)
Date: Wed, 20 Mar 2024 12:51:30 GMT   (48321kb,D)

Title: Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban
  Environments
Authors: Djamahl Etchegaray and Zi Huang and Tatsuya Harada and Yadan Luo
Categories: cs.CV cs.AI
\\
  In this work, we tackle the limitations of current LiDAR-based 3D object
detection systems, which are hindered by a restricted class vocabulary and the
high costs associated with annotating new object classes. Our exploration of
open-vocabulary (OV) learning in urban environments aims to capture novel
instances using pre-trained vision-language models (VLMs) with multi-sensor
data. We design and benchmark a set of four potential solutions as baselines,
categorizing them into either top-down or bottom-up approaches based on their
input data strategies. While effective, these methods exhibit certain
limitations, such as missing novel objects in 3D box estimation or applying
rigorous priors, leading to biases towards objects near the camera or of
rectangular geometries. To overcome these limitations, we introduce a universal
\textsc{Find n' Propagate} approach for 3D OV tasks, aimed at maximizing the
recall of novel objects and propagating this detection capability to more
distant areas thereby progressively capturing more. In particular, we utilize a
greedy box seeker to search against 3D novel boxes of varying orientations and
depth in each generated frustum and ensure the reliability of newly identified
boxes by cross alignment and density ranker. Additionally, the inherent bias
towards camera-proximal objects is alleviated by the proposed remote simulator,
which randomly diversifies pseudo-labeled novel instances in the self-training
process, combined with the fusion of base samples in the memory bank. Extensive
experiments demonstrate a 53% improvement in novel recall across diverse OV
settings, VLMs, and 3D detectors. Notably, we achieve up to a 3.97-fold
increase in Average Precision (AP) for novel object classes. The source code is
made available in the supplementary material.
\\ ( https://arxiv.org/abs/2403.13556 ,  48321kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13574 (*cross-listing*)
Date: Wed, 20 Mar 2024 13:14:29 GMT   (504kb,D)

Title: A Large Language Model Enhanced Sequential Recommender for Joint Video
  and Comment Recommendation
Authors: Bowen Zheng, Zihan Lin, Enze Liu, Chen Yang, Enyang Bai, Cheng Ling,
  Wayne Xin Zhao, Ji-Rong Wen
Categories: cs.IR cs.AI
\\
  In online video platforms, reading or writing comments on interesting videos
has become an essential part of the video watching experience. However,
existing video recommender systems mainly model users' interaction behaviors
with videos, lacking consideration of comments in user behavior modeling. In
this paper, we propose a novel recommendation approach called LSVCR by
leveraging user interaction histories with both videos and comments, so as to
jointly conduct personalized video and comment recommendation. Specifically,
our approach consists of two key components, namely sequential recommendation
(SR) model and supplemental large language model (LLM) recommender. The SR
model serves as the primary recommendation backbone (retained in deployment) of
our approach, allowing for efficient user preference modeling. Meanwhile, we
leverage the LLM recommender as a supplemental component (discarded in
deployment) to better capture underlying user preferences from heterogeneous
interaction behaviors. In order to integrate the merits of the SR model and the
supplemental LLM recommender, we design a twostage training paradigm. The first
stage is personalized preference alignment, which aims to align the preference
representations from both components, thereby enhancing the semantics of the SR
model. The second stage is recommendation-oriented fine-tuning, in which the
alignment-enhanced SR model is fine-tuned according to specific objectives.
Extensive experiments in both video and comment recommendation tasks
demonstrate the effectiveness of LSVCR. Additionally, online A/B testing on the
KuaiShou platform verifies the actual benefits brought by our approach. In
particular, we achieve a significant overall gain of 4.13% in comment watch
time.
\\ ( https://arxiv.org/abs/2403.13574 ,  504kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13597 (*cross-listing*)
Date: Wed, 20 Mar 2024 13:44:30 GMT   (368kb,D)

Title: No more optimization rules: LLM-enabled policy-based multi-modal query
  optimizer (version 1)
Authors: Yifan Wang, Haodi Ma, Daisy Zhe Wang
Categories: cs.DB cs.AI cs.IR
Comments: Yifan and Haodi contribute equally to the work
\\
  Large language model (LLM) has marked a pivotal moment in the field of
machine learning and deep learning. Recently its capability for query planning
has been investigated, including both single-modal and multi-modal queries.
However, there is no work on the query optimization capability of LLM. As a
critical (or could even be the most important) step that significantly impacts
the execution performance of the query plan, such analysis and attempts should
not be missed. From another aspect, existing query optimizers are usually
rule-based or rule-based + cost-based, i.e., they are dependent on manually
created rules to complete the query plan rewrite/transformation. Given the fact
that modern optimizers include hundreds to thousands of rules, designing a
multi-modal query optimizer following a similar way is significantly
time-consuming since we will have to enumerate as many multi-modal optimization
rules as possible, which has not been well addressed today. In this paper, we
investigate the query optimization ability of LLM and use LLM to design LaPuda,
a novel LLM and Policy based multi-modal query optimizer. Instead of
enumerating specific and detailed rules, LaPuda only needs a few abstract
policies to guide LLM in the optimization, by which much time and human effort
are saved. Furthermore, to prevent LLM from making mistakes or negative
optimization, we borrow the idea of gradient descent and propose a guided cost
descent (GCD) algorithm to perform the optimization, such that the optimization
can be kept in the correct direction. In our evaluation, our methods
consistently outperform the baselines in most cases. For example, the optimized
plans generated by our methods result in 1~3x higher execution speed than those
by the baselines.
\\ ( https://arxiv.org/abs/2403.13597 ,  368kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13619 (*cross-listing*)
Date: Wed, 20 Mar 2024 14:13:44 GMT   (307kb)

Title: Dynamic Resource Allocation for Virtual Machine Migration Optimization
  using Machine Learning
Authors: Yulu Gong, Jiaxin Huang, Bo Liu, Jingyu Xu, Binbin Wu, Yifan Zhang
Categories: cs.DC cs.AI
\\
  The paragraph is grammatically correct and logically coherent. It discusses
the importance of mobile terminal cloud computing migration technology in
meeting the demands of evolving computer and cloud computing technologies. It
emphasizes the need for efficient data access and storage, as well as the
utilization of cloud computing migration technology to prevent additional time
delays. The paragraph also highlights the contributions of cloud computing
migration technology to expanding cloud computing services. Additionally, it
acknowledges the role of virtualization as a fundamental capability of cloud
computing while emphasizing that cloud computing and virtualization are not
inherently interconnected. Finally, it introduces machine learning-based
virtual machine migration optimization and dynamic resource allocation as a
critical research direction in cloud computing, citing the limitations of
static rules or manual settings in traditional cloud computing environments.
Overall, the paragraph effectively communicates the importance of machine
learning technology in addressing resource allocation and virtual machine
migration challenges in cloud computing.
\\ ( https://arxiv.org/abs/2403.13619 ,  307kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13653 (*cross-listing*)
Date: Wed, 20 Mar 2024 14:58:40 GMT   (3279kb,D)

Title: Learning User Embeddings from Human Gaze for Personalised Saliency
  Prediction
Authors: Florian Strohm, Mihai B\^ace and Andreas Bulling
Categories: cs.CV cs.AI cs.HC
\\
  Reusable embeddings of user behaviour have shown significant performance
improvements for the personalised saliency prediction task. However, prior
works require explicit user characteristics and preferences as input, which are
often difficult to obtain. We present a novel method to extract user embeddings
from pairs of natural images and corresponding saliency maps generated from a
small amount of user-specific eye tracking data. At the core of our method is a
Siamese convolutional neural encoder that learns the user embeddings by
contrasting the image and personal saliency map pairs of different users.
Evaluations on two public saliency datasets show that the generated embeddings
have high discriminative power, are effective at refining universal saliency
maps to the individual users, and generalise well across users and images.
Finally, based on our model's ability to encode individual user
characteristics, our work points towards other applications that can benefit
from reusable embeddings of gaze behaviour.
\\ ( https://arxiv.org/abs/2403.13653 ,  3279kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13682 (*cross-listing*)
Date: Wed, 20 Mar 2024 15:40:18 GMT   (2957kb,D)

Title: Threats, Attacks, and Defenses in Machine Unlearning: A Survey
Authors: Ziyao Liu, Huanyi Ye, Chen Chen, and Kwok-Yan Lam
Categories: cs.CR cs.AI
\\
  Recently, Machine Unlearning (MU) has gained considerable attention for its
potential to improve AI safety by removing the influence of specific data from
trained Machine Learning (ML) models. This process, known as knowledge removal,
addresses concerns about data such as sensitivity, copyright restrictions,
obsolescence, or low quality. This capability is also crucial for ensuring
compliance with privacy regulations such as the Right To Be Forgotten (RTBF).
Therefore, strategic knowledge removal mitigates the risk of harmful outcomes,
safeguarding against biases, misinformation, and unauthorized data
exploitation, thereby enhancing the ethical use and reliability of AI systems.
Efforts have been made to design efficient unlearning approaches, with MU
services being examined for integration with existing machine learning as a
service (MLaaS), allowing users to submit requests to erase data. However,
recent research highlights vulnerabilities in machine unlearning systems, such
as information leakage and malicious unlearning requests, that can lead to
significant security and privacy concerns. Moreover, extensive research
indicates that unlearning methods and prevalent attacks fulfill diverse roles
within MU systems. For instance, unlearning can act as a mechanism to recover
models from backdoor attacks, while backdoor attacks themselves can serve as an
evaluation metric for unlearning effectiveness. This underscores the intricate
relationship and complex interplay between these elements in maintaining system
functionality and safety. Therefore, this survey seeks to bridge the gap
between the extensive number of studies on threats, attacks, and defenses in
machine unlearning and the absence of a comprehensive review that categorizes
their taxonomy, methods, and solutions, thus offering valuable insights for
future research directions and practical implementations.
\\ ( https://arxiv.org/abs/2403.13682 ,  2957kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13684 (*cross-listing*)
Date: Wed, 20 Mar 2024 15:41:39 GMT   (37374kb,D)

Title: SPTNet: An Efficient Alternative Framework for Generalized Category
  Discovery with Spatial Prompt Tuning
Authors: Hongjun Wang, Sagar Vaze, Kai Han
Categories: cs.CV cs.AI
Comments: Accepted as a conference paper at ICLR 2024; Project page:
  https://visual-ai.github.io/sptnet
\\
  Generalized Category Discovery (GCD) aims to classify unlabelled images from
both `seen' and `unseen' classes by transferring knowledge from a set of
labelled `seen' class images. A key theme in existing GCD approaches is
adapting large-scale pre-trained models for the GCD task. An alternate
perspective, however, is to adapt the data representation itself for better
alignment with the pre-trained model. As such, in this paper, we introduce a
two-stage adaptation approach termed SPTNet, which iteratively optimizes model
parameters (i.e., model-finetuning) and data parameters (i.e., prompt
learning). Furthermore, we propose a novel spatial prompt tuning method (SPT)
which considers the spatial property of image data, enabling the method to
better focus on object parts, which can transfer between seen and unseen
classes. We thoroughly evaluate our SPTNet on standard benchmarks and
demonstrate that our method outperforms existing GCD methods. Notably, we find
our method achieves an average accuracy of 61.4% on the SSB, surpassing prior
state-of-the-art methods by approximately 10%. The improvement is particularly
remarkable as our method yields extra parameters amounting to only 0.117% of
those in the backbone architecture. Project page:
https://visual-ai.github.io/sptnet.
\\ ( https://arxiv.org/abs/2403.13684 ,  37374kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13703 (*cross-listing*)
Date: Wed, 20 Mar 2024 16:07:04 GMT   (632kb)

Title: Fostc3net:A Lightweight YOLOv5 Based On the Network Structure
  Optimization
Authors: Danqing Ma, Shaojie Li, Bo Dang, Hengyi Zang, Xinqi Dong
Categories: cs.CV cs.AI
\\
  Transmission line detection technology is crucial for automatic monitoring
and ensuring the safety of electrical facilities. The YOLOv5 series is
currently one of the most advanced and widely used methods for object
detection. However, it faces inherent challenges, such as high computational
load on devices and insufficient detection accuracy. To address these concerns,
this paper presents an enhanced lightweight YOLOv5 technique customized for
mobile devices, specifically intended for identifying objects associated with
transmission lines. The C3Ghost module is integrated into the convolutional
network of YOLOv5 to reduce floating point operations per second (FLOPs) in the
feature channel fusion process and improve feature expression performance. In
addition, a FasterNet module is introduced to replace the c3 module in the
YOLOv5 Backbone. The FasterNet module uses Partial Convolutions to process only
a portion of the input channels, improving feature extraction efficiency and
reducing computational overhead. To address the imbalance between simple and
challenging samples in the dataset and the diversity of aspect ratios of
bounding boxes, the wIoU v3 LOSS is adopted as the loss function. To validate
the performance of the proposed approach, Experiments are conducted on a custom
dataset of transmission line poles. The results show that the proposed model
achieves a 1% increase in detection accuracy, a 13% reduction in FLOPs, and a
26% decrease in model parameters compared to the existing YOLOv5.In the
ablation experiment, it was also discovered that while the Fastnet module and
the CSghost module improved the precision of the original YOLOv5 baseline
model, they caused a decrease in the mAP@.5-.95 metric. However, the
improvement of the wIoUv3 loss function significantly mitigated the decline of
the mAP@.5-.95 metric.
\\ ( https://arxiv.org/abs/2403.13703 ,  632kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13721 (*cross-listing*)
Date: Wed, 20 Mar 2024 16:29:52 GMT   (4543kb,D)

Title: Large Language Models meet Network Slicing Management and Orchestration
Authors: Abdulhalim Dandoush (1 and 2), Viswanath Kumarskandpriya (1), Mueen
  Uddin (2), Usman Khalil (3) ((1) Esme Research Lab, SA ESME, Ivry-Sur-Seine,
  France, (2) University of Doha for Science and Technology (UDST), Doha,
  Qatar, (3) University Brunei Darussalam, Brunei Darrussalam)
Categories: cs.NI cs.AI
\\
  Network slicing, a cornerstone technology for future networks, enables the
creation of customized virtual networks on a shared physical infrastructure.
This fosters innovation and agility by providing dedicated resources tailored
to specific applications. However, current orchestration and management
approaches face limitations in handling the complexity of new service demands
within multi-administrative domain environments. This paper proposes a future
vision for network slicing powered by Large Language Models (LLMs) and
multi-agent systems, offering a framework that can be integrated with existing
Management and Orchestration (MANO) frameworks. This framework leverages LLMs
to translate user intent into technical requirements, map network functions to
infrastructure, and manage the entire slice lifecycle, while multi-agent
systems facilitate collaboration across different administrative domains. We
also discuss the challenges associated with implementing this framework and
potential solutions to mitigate them.
\\ ( https://arxiv.org/abs/2403.13721 ,  4543kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13729 (*cross-listing*)
Date: Wed, 20 Mar 2024 16:39:17 GMT   (9959kb,D)

Title: Reinforcement Learning for Online Testing of Autonomous Driving Systems:
  a Replication and Extension Study
Authors: Luca Giamattei, Matteo Biagiola, Roberto Pietrantuono, Stefano Russo,
  Paolo Tonella
Categories: cs.SE cs.AI cs.LG cs.RO
\\
  In a recent study, Reinforcement Learning (RL) used in combination with
many-objective search, has been shown to outperform alternative techniques
(random search and many-objective search) for online testing of Deep Neural
Network-enabled systems. The empirical evaluation of these techniques was
conducted on a state-of-the-art Autonomous Driving System (ADS). This work is a
replication and extension of that empirical study. Our replication shows that
RL does not outperform pure random test generation in a comparison conducted
under the same settings of the original study, but with no confounding factor
coming from the way collisions are measured. Our extension aims at eliminating
some of the possible reasons for the poor performance of RL observed in our
replication: (1) the presence of reward components providing contrasting or
useless feedback to the RL agent; (2) the usage of an RL algorithm (Q-learning)
which requires discretization of an intrinsically continuous state space.
Results show that our new RL agent is able to converge to an effective policy
that outperforms random testing. Results also highlight other possible
improvements, which open to further investigations on how to best leverage RL
for online ADS testing.
\\ ( https://arxiv.org/abs/2403.13729 ,  9959kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13731 (*cross-listing*)
Date: Tue, 19 Mar 2024 12:26:53 GMT   (390kb,D)

Title: Emotion Recognition Using Transformers with Masked Learning
Authors: Seongjae Min, Junseok Yang, Sangjun Lim, Junyong Lee, Sangwon Lee and
  Sejoon Lim
Categories: cs.CV cs.AI
\\
  In recent years, deep learning has achieved innovative advancements in
various fields, including the analysis of human emotions and behaviors.
Initiatives such as the Affective Behavior Analysis in-the-wild (ABAW)
competition have been particularly instrumental in driving research in this
area by providing diverse and challenging datasets that enable precise
evaluation of complex emotional states. This study leverages the Vision
Transformer (ViT) and Transformer models to focus on the estimation of
Valence-Arousal (VA), which signifies the positivity and intensity of emotions,
recognition of various facial expressions, and detection of Action Units (AU)
representing fundamental muscle movements. This approach transcends traditional
Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) based
methods, proposing a new Transformer-based framework that maximizes the
understanding of temporal and spatial features. The core contributions of this
research include the introduction of a learning technique through random frame
masking and the application of Focal loss adapted for imbalanced data,
enhancing the accuracy and applicability of emotion and behavior analysis in
real-world settings. This approach is expected to contribute to the advancement
of emotional computing and deep learning methodologies.
\\ ( https://arxiv.org/abs/2403.13731 ,  390kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13741 (*cross-listing*)
Date: Wed, 20 Mar 2024 16:47:53 GMT   (103kb,D)

Title: Hyper Strategy Logic
Authors: Raven Beutner, Bernd Finkbeiner
Categories: cs.MA cs.AI cs.LO
Comments: AAMAS 2024
\\
  Strategy logic (SL) is a powerful temporal logic that enables strategic
reasoning in multi-agent systems. SL supports explicit (first-order)
quantification over strategies and provides a logical framework to express many
important properties such as Nash equilibria, dominant strategies, etc. While
in SL the same strategy can be used in multiple strategy profiles, each such
profile is evaluated w.r.t. a path-property, i.e., a property that considers
the single path resulting from a particular strategic interaction. In this
paper, we present Hyper Strategy Logic (HyperSL), a strategy logic where the
outcome of multiple strategy profiles can be compared w.r.t. a hyperproperty,
i.e., a property that relates multiple paths. We show that HyperSL can capture
important properties that cannot be expressed in SL, including
non-interference, quantitative Nash equilibria, optimal adversarial planning,
and reasoning under imperfect information. On the algorithmic side, we identify
an expressive fragment of HyperSL with decidable model checking and present a
model-checking algorithm. We contribute a prototype implementation of our
algorithm and report on encouraging experimental results.
\\ ( https://arxiv.org/abs/2403.13741 ,  103kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13798 (*cross-listing*)
Date: Wed, 20 Mar 2024 17:55:21 GMT   (20602kb,D)

Title: Hierarchical NeuroSymbolic Approach for Action Quality Assessment
Authors: Lauren Okamoto, Paritosh Parmar
Categories: cs.CV cs.AI cs.LG cs.SC
\\
  Action quality assessment (AQA) applies computer vision to quantitatively
assess the performance or execution of a human action. Current AQA approaches
are end-to-end neural models, which lack transparency and tend to be biased
because they are trained on subjective human judgements as ground-truth. To
address these issues, we introduce a neuro-symbolic paradigm for AQA, which
uses neural networks to abstract interpretable symbols from video data and
makes quality assessments by applying rules to those symbols. We take diving as
the case study. We found that domain experts prefer our system and find it more
informative than purely neural approaches to AQA in diving. Our system also
achieves state-of-the-art action recognition and temporal segmentation, and
automatically generates a detailed report that breaks the dive down into its
elements and provides objective scoring with visual evidence. As verified by a
group of domain experts, this report may be used to assist judges in scoring,
help train judges, and provide feedback to divers. We will open-source all of
our annotated training data and code for ease of reproducibility.
\\ ( https://arxiv.org/abs/2403.13798 ,  20602kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13801 (*cross-listing*)
Date: Wed, 20 Mar 2024 17:58:12 GMT   (585kb,D)

Title: Natural Language as Polices: Reasoning for Coordinate-Level Embodied
  Control with LLMs
Authors: Yusuke Mikami, Andrew Melnik, Jun Miura, Ville Hautam\"aki
Categories: cs.RO cs.AI cs.CL
Comments: 8 pages, 2 figures
ACM-class: I.2.9; I.2.7
\\
  We demonstrate experimental results with LLMs that address robotics action
planning problems. Recently, LLMs have been applied in robotics action
planning, particularly using a code generation approach that converts complex
high-level instructions into mid-level policy codes. In contrast, our approach
acquires text descriptions of the task and scene objects, then formulates
action planning through natural language reasoning, and outputs coordinate
level control commands, thus reducing the necessity for intermediate
representation code as policies. Our approach is evaluated on a multi-modal
prompt simulation benchmark, demonstrating that our prompt engineering
experiments with natural language reasoning significantly enhance success rates
compared to its absence. Furthermore, our approach illustrates the potential
for natural language descriptions to transfer robotics skills from known tasks
to previously unseen tasks.
\\ ( https://arxiv.org/abs/2403.13801 ,  585kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13802 (*cross-listing*)
Date: Wed, 20 Mar 2024 17:59:14 GMT   (18702kb,D)

Title: ZigMa: Zigzag Mamba Diffusion Model
Authors: Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova,
  Pingchuan Ma, Johannes Fischer, Bjorn Ommer
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: Project Page: https://taohu.me/zigma/
\\
  The diffusion model has long been plagued by scalability and quadratic
complexity issues, especially within transformer-based structures. In this
study, we aim to leverage the long sequence modeling capability of a
State-Space Model called Mamba to extend its applicability to visual data
generation. Firstly, we identify a critical oversight in most current
Mamba-based vision methods, namely the lack of consideration for spatial
continuity in the scan scheme of Mamba. Secondly, building upon this insight,
we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,
which outperforms Mamba-based baselines and demonstrates improved speed and
memory utilization compared to transformer-based baselines. Lastly, we
integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate
the scalability of the model on large-resolution visual datasets, such as
FacesHQ $1024\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO
$256\times 256$. Code will be released at https://taohu.me/zigma/
\\ ( https://arxiv.org/abs/2403.13802 ,  18702kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13805 (*cross-listing*)
Date: Wed, 20 Mar 2024 17:59:55 GMT   (5655kb,D)

Title: RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition
Authors: Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xiaoyi Dong,
  Yuanjun Xiong, Dahua Lin, Jiaqi Wang
Categories: cs.CV cs.AI cs.LG
Comments: Project: https://github.com/Liuziyu77/RAR
\\
  CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from
noise image-text pairs to excel at recognizing a wide array of candidates, yet
its focus on broad associations hinders the precision in distinguishing subtle
differences among fine-grained items. Conversely, Multimodal Large Language
Models (MLLMs) excel at classifying fine-grained categories, thanks to their
substantial knowledge from pre-training on web-level corpora. However, the
performance of MLLMs declines with an increase in category numbers, primarily
due to growing complexity and constraints of limited context window size. To
synergize the strengths of both approaches and enhance the few-shot/zero-shot
recognition abilities for datasets characterized by extensive and fine-grained
vocabularies, this paper introduces RAR, a Retrieving And Ranking augmented
method for MLLMs. We initially establish a multi-modal retriever based on CLIP
to create and store explicit memory for different categories beyond the
immediate context window. During inference, RAR retrieves the top-k similar
results from the memory and uses MLLMs to rank and make the final predictions.
Our proposed approach not only addresses the inherent limitations in
fine-grained recognition but also preserves the model's comprehensive knowledge
base, significantly boosting accuracy across a range of vision-language
recognition tasks. Notably, our approach demonstrates a significant improvement
in performance on 5 fine-grained visual recognition benchmarks, 11 few-shot
image recognition datasets, and the 2 object detection datasets under the
zero-shot recognition setting.
\\ ( https://arxiv.org/abs/2403.13805 ,  5655kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13808 (*cross-listing*)
Date: Wed, 20 Mar 2024 17:59:58 GMT   (2094kb,D)

Title: On Pretraining Data Diversity for Self-Supervised Learning
Authors: Hasan Abed Al Kader Hammoud, Tuhin Das, Fabio Pizzati, Philip Torr,
  Adel Bibi, Bernard Ghanem
Categories: cs.CV cs.AI cs.LG
Comments: Under review
\\
  We explore the impact of training with more diverse datasets, characterized
by the number of unique samples, on the performance of self-supervised learning
(SSL) under a fixed computational budget. Our findings consistently demonstrate
that increasing pretraining data diversity enhances SSL performance, albeit
only when the distribution distance to the downstream data is minimal. Notably,
even with an exceptionally large pretraining data diversity achieved through
methods like web crawling or diffusion-generated data, among other ways, the
distribution shift remains a challenge. Our experiments are comprehensive with
seven SSL methods using large-scale datasets such as ImageNet and YFCC100M
amounting to over 200 GPU days. Code and trained models will be available at
https://github.com/hammoudhasan/DiversitySSL .
\\ ( https://arxiv.org/abs/2403.13808 ,  2094kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12973 (*cross-listing*)
Date: Sun, 28 Jan 2024 11:43:16 GMT   (2384kb)

Title: C Analyzer : A Static Program Analysis Tool for C Programs
Authors: Rajendra Kumar Solanki
Categories: cs.PL cs.CL
\\
  In our times, when the world is increasingly becoming more dependent on
software programs, writing bug-free, correct programs is crucial. Program
verification based on formal methods can guarantee this by detecting run-time
errors in safety-critical systems to avoid possible adverse impacts on human
life and save time and money.
  This project work tries to leverage Abstract Interpretation techniques for
static analysis of C programs. C Analyzer is a tool developed for static
analysis of C programs. This implementation of C Analyzer provides a
plug-and-play domain architecture for multiple abstract domains to be used. C
Analyzer supports four abstract domains - Interval, Octagon, Polyhedra, and Bit
Vector. We use these different domains for required precision in program
verification. C Analyzer tool uses LLVM C/C++ compiler frontend Clang API to
generate and traverse the Control Flow Graph (CFG) of a given C program. This
tool generates invariants in different abstract domains for statements in basic
blocks of CFG during CFG traversal. Using these invariants, some properties of
a program, such as dividing by zero, modulus zero, arithmetic overflow, etc.,
can be analyzed. We also use a source-to-source transformation tool, CIL
(Common Intermediate language), to transform some C constructs into simpler
constructs, such as transforming logical operators, switch statements, and
conditional operators into if-else ladders and transforming do-while and for
loops into while loops.
  Using C Analyzer, C program constructs such as declarations, assignments,
binary operations (arithmetic, relational, bitwise shift, etc.), conditions
(if-else), loops (while, do while, for loop), nested conditions, and nested
loops can be analyzed. Currently, this tool does not support arrays,
structures, unions, pointers, or function calls.
\\ ( https://arxiv.org/abs/2403.12973 ,  2384kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12984 (*cross-listing*)
Date: Sun, 3 Mar 2024 11:09:32 GMT   (241kb,D)

Title: When SMILES have Language: Drug Classification using Text Classification
  Methods on Drug SMILES Strings
Authors: Azmine Toushik Wasi and \v{S}erbetar Karlo and Raima Islam and Taki
  Hasan Rafi and Dong-Kyu Chae
Categories: q-bio.BM cs.CL cs.IR cs.LG stat.ML
Comments: 7 pages, 2 figures, 5 tables, Accepted (invited to present) to the
  The Second Tiny Papers Track at ICLR 2024
  (https://openreview.net/forum?id=VUYCyH8fCw)
Journal-ref: The Second Tiny Papers Track at {ICLR} 2024, Tiny Papers @ {ICLR}
  2024, Vienna Austria, May 11, 2024
\\
  Complex chemical structures, like drugs, are usually defined by SMILES
strings as a sequence of molecules and bonds. These SMILES strings are used in
different complex machine learning-based drug-related research and
representation works. Escaping from complex representation, in this work, we
pose a single question: What if we treat drug SMILES as conventional sentences
and engage in text classification for drug classification? Our experiments
affirm the possibility with very competitive scores. The study explores the
notion of viewing each atom and bond as sentence components, employing basic
NLP methods to categorize drug types, proving that complex problems can also be
solved with simpler perspectives. The data and code are available here:
https://github.com/azminewasi/Drug-Classification-NLP.
\\ ( https://arxiv.org/abs/2403.12984 ,  241kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13272 (*cross-listing*)
Date: Wed, 20 Mar 2024 03:14:54 GMT   (3308kb,D)

Title: Community Needs and Assets: A Computational Analysis of Community
  Conversations
Authors: Md Towhidul Absar Chowdhury, Naveen Sharma, Ashiqur R. KhudaBukhsh
Categories: cs.CY cs.CL cs.SI
\\
  A community needs assessment is a tool used by non-profits and government
agencies to quantify the strengths and issues of a community, allowing them to
allocate their resources better. Such approaches are transitioning towards
leveraging social media conversations to analyze the needs of communities and
the assets already present within them. However, manual analysis of
exponentially increasing social media conversations is challenging. There is a
gap in the present literature in computationally analyzing how community
members discuss the strengths and needs of the community. To address this gap,
we introduce the task of identifying, extracting, and categorizing community
needs and assets from conversational data using sophisticated natural language
processing methods. To facilitate this task, we introduce the first dataset
about community needs and assets consisting of 3,511 conversations from Reddit,
annotated using crowdsourced workers. Using this dataset, we evaluate an
utterance-level classification model compared to sentiment classification and a
popular large language model (in a zero-shot setting), where we find that our
model outperforms both baselines at an F1 score of 94% compared to 49% and 61%
respectively. Furthermore, we observe through our study that conversations
about needs have negative sentiments and emotions, while conversations about
assets focus on location and entities. The dataset is available at
https://github.com/towhidabsar/CommunityNeeds.
\\ ( https://arxiv.org/abs/2403.13272 ,  3308kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13301 (*cross-listing*)
Date: Wed, 20 Mar 2024 04:57:32 GMT   (665kb)

Title: Reading Users' Minds from What They Say: An Investigation into LLM-based
  Empathic Mental Inference
Authors: Qihao Zhu, Leah Chong, Maria Yang, Jianxi Luo
Categories: cs.HC cs.CL
Comments: Submitted to IDETC-CIE2024
\\
  In human-centered design, developing a comprehensive and in-depth
understanding of user experiences, i.e., empathic understanding, is paramount
for designing products that truly meet human needs. Nevertheless, accurately
comprehending the real underlying mental states of a large human population
remains a significant challenge today. This difficulty mainly arises from the
trade-off between depth and scale of user experience research: gaining in-depth
insights from a small group of users does not easily scale to a larger
population, and vice versa. This paper investigates the use of Large Language
Models (LLMs) for performing mental inference tasks, specifically inferring
users' underlying goals and fundamental psychological needs (FPNs). Baseline
and benchmark datasets were collected from human users and designers to develop
an empathic accuracy metric for measuring the mental inference performance of
LLMs. The empathic accuracy of inferring goals and FPNs of different LLMs with
varied zero-shot prompt engineering techniques are experimented against that of
human designers. Experimental results suggest that LLMs can infer and
understand the underlying goals and FPNs of users with performance comparable
to that of human designers, suggesting a promising avenue for enhancing the
scalability of empathic design approaches through the integration of advanced
artificial intelligence technologies. This work has the potential to
significantly augment the toolkit available to designers during human-centered
design, enabling the development of both large-scale and in-depth understanding
of users' experiences.
\\ ( https://arxiv.org/abs/2403.13301 ,  665kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13583 (*cross-listing*)
Date: Wed, 20 Mar 2024 13:33:55 GMT   (699kb,D)

Title: CONLINE: Complex Code Generation and Refinement with Online Searching
  and Correctness Testing
Authors: Xinyi He, Jiaru Zou, Yun Lin, Mengyu Zhou, Shi Han, Zejian Yuan,
  Dongmei Zhang
Categories: cs.SE cs.CL cs.LG
\\
  Large Language Models (LLMs) have revolutionized code generation ability by
converting natural language descriptions into executable code. However,
generating complex code within real-world scenarios remains challenging due to
intricate structures, subtle bugs, understanding of advanced data types, and
lack of supplementary contents. To address these challenges, we introduce the
CONLINE framework, which enhances code generation by incorporating planned
online searches for information retrieval and automated correctness testing for
iterative refinement. CONLINE also serializes the complex inputs and outputs to
improve comprehension and generate test case to ensure the framework's
adaptability for real-world applications. CONLINE is validated through rigorous
experiments on the DS-1000 and ClassEval datasets. It shows that CONLINE
substantially improves the quality of complex code generation, highlighting its
potential to enhance the practicality and reliability of LLMs in generating
intricate code.
\\ ( https://arxiv.org/abs/2403.13583 ,  699kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13588 (*cross-listing*)
Date: Wed, 20 Mar 2024 13:37:00 GMT   (408kb,D)

Title: Genetic Auto-prompt Learning for Pre-trained Code Intelligence Language
  Models
Authors: Chengzhe Feng, Yanan Sun, Ke Li, Pan Zhou, Jiancheng Lv, Aojun Lu
Categories: cs.SE cs.CL
\\
  As Pre-trained Language Models (PLMs), a popular approach for code
intelligence, continue to grow in size, the computational cost of their usage
has become prohibitively expensive. Prompt learning, a recent development in
the field of natural language processing, emerges as a potential solution to
address this challenge. In this paper, we investigate the effectiveness of
prompt learning in code intelligence tasks. We unveil its reliance on manually
designed prompts, which often require significant human effort and expertise.
Moreover, we discover existing automatic prompt design methods are very limited
to code intelligence tasks due to factors including gradient dependence, high
computational demands, and limited applicability. To effectively address both
issues, we propose Genetic Auto Prompt (GenAP), which utilizes an elaborate
genetic algorithm to automatically design prompts. With GenAP, non-experts can
effortlessly generate superior prompts compared to meticulously manual-designed
ones. GenAP operates without the need for gradients or additional computational
costs, rendering it gradient-free and cost-effective. Moreover, GenAP supports
both understanding and generation types of code intelligence tasks, exhibiting
great applicability. We conduct GenAP on three popular code intelligence PLMs
with three canonical code intelligence tasks including defect prediction, code
summarization, and code translation. The results suggest that GenAP can
effectively automate the process of designing prompts. Specifically, GenAP
outperforms all other methods across all three tasks (e.g., improving accuracy
by an average of 2.13% for defect prediction). To the best of our knowledge,
GenAP is the first work to automatically design prompts for code intelligence
PLMs.
\\ ( https://arxiv.org/abs/2403.13588 ,  408kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13804 (*cross-listing*)
Date: Wed, 20 Mar 2024 17:59:43 GMT   (4728kb,D)

Title: Learning from Models and Data for Visual Grounding
Authors: Ruozhen He, Paola Cascante-Bonilla, Ziyan Yang, Alexander C. Berg,
  Vicente Ordonez
Categories: cs.CV cs.CL cs.LG
Comments: Project Page: https://catherine-r-he.github.io/SynGround/
\\
  We introduce SynGround, a novel framework that combines data-driven learning
and knowledge transfer from various large-scale pretrained models to enhance
the visual grounding capabilities of a pretrained vision-and-language model.
The knowledge transfer from the models initiates the generation of image
descriptions through an image description generator. These descriptions serve
dual purposes: they act as prompts for synthesizing images through a
text-to-image generator, and as queries for synthesizing text, from which
phrases are extracted using a large language model. Finally, we leverage an
open-vocabulary object detector to generate synthetic bounding boxes for the
synthetic images and texts. We finetune a pretrained vision-and-language model
on this dataset by optimizing a mask-attention consistency objective that
aligns region annotations with gradient-based model explanations. The resulting
model improves the grounding capabilities of an off-the-shelf
vision-and-language model. Particularly, SynGround improves the pointing game
accuracy of ALBEF on the Flickr30k dataset from 79.38% to 87.26%, and on
RefCOCO+ Test A from 69.35% to 79.06% and on RefCOCO+ Test B from 53.77% to
63.67%.
\\ ( https://arxiv.org/abs/2403.13804 ,  4728kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10404 (*cross-listing*)
Date: Mon, 16 Oct 2023 13:49:46 GMT   (4667kb,D)
Date (revised v2): Wed, 18 Oct 2023 04:05:40 GMT   (4667kb,D)
Date (revised v3): Thu, 19 Oct 2023 04:01:41 GMT   (4667kb,D)
Date (revised v4): Fri, 20 Oct 2023 01:12:52 GMT   (4667kb,D)
Date (revised v5): Mon, 27 Nov 2023 11:41:32 GMT   (6418kb,D)

Title: LLM4SGG: Large Language Model for Weakly Supervised Scene Graph
  Generation
Authors: Kibum Kim, Kanghoon Yoon, Jaehyeong Jeon, Yeonjun In, Jinyoung Moon,
  Donghyun Kim, Chanyoung Park
Categories: cs.CV cs.AI cs.LG
Comments: 21 pages, Preprint
\\
  Weakly-Supervised Scene Graph Generation (WSSGG) research has recently
emerged as an alternative to the fully-supervised approach that heavily relies
on costly annotations. In this regard, studies on WSSGG have utilized image
captions to obtain unlocalized triplets while primarily focusing on grounding
the unlocalized triplets over image regions. However, they have overlooked the
two issues involved in the triplet formation process from the captions: 1)
Semantic over-simplification issue arises when extracting triplets from
captions, where fine-grained predicates in captions are undesirably converted
into coarse-grained predicates, resulting in a long-tailed predicate
distribution, and 2) Low-density scene graph issue arises when aligning the
triplets in the caption with entity/predicate classes of interest, where many
triplets are discarded and not used in training, leading to insufficient
supervision. To tackle the two issues, we propose a new approach, i.e., Large
Language Model for weakly-supervised SGG (LLM4SGG), where we mitigate the two
issues by leveraging the LLM's in-depth understanding of language and reasoning
ability during the extraction of triplets from captions and alignment of
entity/predicate classes with target data. To further engage the LLM in these
processes, we adopt the idea of Chain-of-Thought and the in-context few-shot
learning strategy. To validate the effectiveness of LLM4SGG, we conduct
extensive experiments on Visual Genome and GQA datasets, showing significant
improvements in both Recall@K and mean Recall@K compared to the
state-of-the-art WSSGG methods. A further appeal is that LLM4SGG is
data-efficient, enabling effective model training with a small amount of
training images.
\\ ( https://arxiv.org/abs/2310.10404 ,  6418kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12975 (*cross-listing*)
Date: Mon, 5 Feb 2024 12:11:15 GMT   (282kb,D)

Title: Training morphological neural networks with gradient descent: some
  theoretical insights
Authors: Samy Blusseau (CMM)
Categories: cs.CV cs.LG stat.ML
Journal-ref: IAPR Third International Conference on Discrete Geometry and
  Mathematical Morphology, Andrea Frosini; Elena Barcucci; Elisa Pergola;
  Michela Ascolese; Niccol{\'o} Di Marco; Simone Rinaldi; Sara Brunetti; Giulia
  Palma; Veronica Gierrini; Leonardo Bindi, Apr 2024, Firenze, Italy
\\
  Morphological neural networks, or layers, can be a powerful tool to boost the
progress in mathematical morphology, either on theoretical aspects such as the
representation of complete lattice operators, or in the development of image
processing pipelines. However, these architectures turn out to be difficult to
train when they count more than a few morphological layers, at least within
popular machine learning frameworks which use gradient descent based
optimization algorithms. In this paper we investigate the potential and
limitations of differentiation based approaches and back-propagation applied to
morphological networks, in light of the non-smooth optimization concept of
Bouligand derivative. We provide insights and first theoretical guidelines, in
particular regarding initialization and learning rates.
\\ ( https://arxiv.org/abs/2403.12975 ,  282kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12977 (*cross-listing*)
Date: Sat, 10 Feb 2024 01:16:21 GMT   (4811kb,D)

Title: SportsNGEN: Sustained Generation of Multi-player Sports Gameplay
Authors: Lachlan Thorpe, Lewis Bawden, Karanjot Vendal, John Bronskill, Richard
  E. Turner
Categories: cs.CV cs.LG eess.IV stat.AP
\\
  We present a transformer decoder based model, SportsNGEN, that is trained on
sports player and ball tracking sequences that is capable of generating
realistic and sustained gameplay. We train and evaluate SportsNGEN on a large
database of professional tennis tracking data and demonstrate that by combining
the generated simulations with a shot classifier and logic to start and end
rallies, the system is capable of simulating an entire tennis match. In
addition, a generic version of SportsNGEN can be customized to a specific
player by fine-tuning on match data that includes that player. We show that our
model is well calibrated and can be used to derive insights for coaches and
broadcasters by evaluating counterfactual or what if options. Finally, we show
qualitative results indicating the same approach works for football.
\\ ( https://arxiv.org/abs/2403.12977 ,  4811kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12979 (*cross-listing*)
Date: Fri, 23 Feb 2024 19:01:47 GMT   (1882kb,D)

Title: AltGraph: Redesigning Quantum Circuits Using Generative Graph Models for
  Efficient Optimization
Authors: Collin Beaudoin, Koustubh Phalak, Swaroop Ghosh
Categories: quant-ph cs.ET cs.LG
\\
  Quantum circuit transformation aims to produce equivalent circuits while
optimizing for various aspects such as circuit depth, gate count, and
compatibility with modern Noisy Intermediate Scale Quantum (NISQ) devices.
There are two techniques for circuit transformation. The first is a rule-based
approach that greedily cancels out pairs of gates that equate to the identity
unitary operation. Rule-based approaches are used in quantum compilers such as
Qiskit, tket, and Quilc. The second is a search-based approach that tries to
find an equivalent quantum circuit by exploring the quantum circuits search
space. Search-based approaches typically rely on machine learning techniques
such as generative models and Reinforcement Learning (RL). In this work, we
propose AltGraph, a novel search-based circuit transformation approach that
generates equivalent quantum circuits using existing generative graph models.
We use three main graph models: DAG Variational Autoencoder (D-VAE) with two
variants: Gated Recurrent Unit (GRU) and Graph Convolutional Network (GCN), and
Deep Generative Model for Graphs (DeepGMG) that take a Direct Acyclic Graph
(DAG) of the quantum circuit as input and output a new DAG from which we
reconstruct the equivalent quantum circuit. Next, we perturb the latent space
to generate equivalent quantum circuits some of which may be more compatible
with the hardware coupling map and/or enable better optimization leading to
reduced gate count and circuit depth. AltGraph achieves on average a 37.55%
reduction in the number of gates and a 37.75% reduction in the circuit depth
post-transpiling compared to the original transpiled circuit with only 0.0074
Mean Squared Error (MSE) in the density matrix.
\\ ( https://arxiv.org/abs/2403.12979 ,  1882kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12982 (*cross-listing*)
Date: Sat, 2 Mar 2024 12:41:25 GMT   (2444kb)

Title: Knowledge-Reuse Transfer Learning Methods in Molecular and Material
  Science
Authors: An Chen, Zhilong Wang, Karl Luigi Loza Vidaurre, Yanqiang Han, Simin
  Ye, Kehao Tao, Shiwei Wang, Jing Gao, and Jinjin Li
Categories: cond-mat.mtrl-sci cs.LG physics.chem-ph
Comments: 42 pages, 10 figures
\\
  Molecules and materials are the foundation for the development of modern
advanced industries such as energy storage systems and semiconductor devices.
However, traditional trial-and-error methods or theoretical calculations are
highly resource-intensive, and extremely long R&D (Research and Development)
periods cannot meet the urgent need for molecules/materials in industrial
development. Machine learning (ML) methods based on big data are expected to
break this dilemma. However, the difficulty in constructing large-scale
datasets of new molecules/materials due to the high cost of data acquisition
and annotation limits the development of machine learning. The application of
transfer learning lowers the data requirements for model training, which makes
transfer learning stand out in researches addressing data quality issues. In
this review, we summarize recent advances in transfer learning related to
molecular and materials science. We focus on the application of transfer
learning methods for the discovery of advanced molecules/materials,
particularly, the construction of transfer learning frameworks for different
systems, and how transfer learning can enhance the performance of models. In
addition, the challenges of transfer learning are also discussed.
\\ ( https://arxiv.org/abs/2403.12982 ,  2444kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12983 (*cross-listing*)
Date: Sat, 2 Mar 2024 19:38:10 GMT   (98kb,D)

Title: OSSCAR: One-Shot Structured Pruning in Vision and Language Models with
  Combinatorial Optimization
Authors: Xiang Meng, Shibal Ibrahim, Kayhan Behdin, Hussein Hazimeh, Natalia
  Ponomareva, Rahul Mazumder
Categories: cs.CV cs.LG
\\
  Structured pruning is a promising approach for reducing the inference costs
of large vision and language models. By removing carefully chosen structures,
e.g., neurons or attention heads, the improvements from this approach can be
realized on standard deep learning hardware. In this work, we focus on
structured pruning in the one-shot (post-training) setting, which does not
require model retraining after pruning. We propose a novel combinatorial
optimization framework for this problem, based on a layer-wise reconstruction
objective and a careful reformulation that allows for scalable optimization.
Moreover, we design a new local combinatorial optimization algorithm, which
exploits low-rank updates for efficient local search. Our framework is time and
memory-efficient and considerably improves upon state-of-the-art one-shot
methods on vision models (e.g., ResNet50, MobileNet) and language models (e.g.,
OPT-1.3B -- OPT-30B). For language models, e.g., OPT-2.7B, OSSCAR can lead to
$125\times$ lower test perplexity on WikiText with $2\times$ inference time
speedup in comparison to the state-of-the-art ZipLM approach. Our framework is
also $6\times$ -- $8\times$ faster. Notably, our work considers models with
tens of billions of parameters, which is up to $100\times$ larger than what has
been previously considered in the structured pruning literature.
\\ ( https://arxiv.org/abs/2403.12983 ,  98kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12986 (*cross-listing*)
Date: Mon, 4 Mar 2024 06:43:16 GMT   (4846kb,D)

Title: BaCon: Boosting Imbalanced Semi-supervised Learning via Balanced
  Feature-Level Contrastive Learning
Authors: Qianhan Feng, Lujing Xie, Shijie Fang, Tong Lin
Categories: cs.CV cs.LG
Comments: Accpeted paper of AAAI2024
\\
  Semi-supervised Learning (SSL) reduces the need for extensive annotations in
deep learning, but the more realistic challenge of imbalanced data distribution
in SSL remains largely unexplored. In Class Imbalanced Semi-supervised Learning
(CISSL), the bias introduced by unreliable pseudo-labels can be exacerbated by
imbalanced data distributions. Most existing methods address this issue at
instance-level through reweighting or resampling, but the performance is
heavily limited by their reliance on biased backbone representation. Some other
methods do perform feature-level adjustments like feature blending but might
introduce unfavorable noise. In this paper, we discuss the bonus of a more
balanced feature distribution for the CISSL problem, and further propose a
Balanced Feature-Level Contrastive Learning method (BaCon). Our method directly
regularizes the distribution of instances' representations in a well-designed
contrastive manner. Specifically, class-wise feature centers are computed as
the positive anchors, while negative anchors are selected by a straightforward
yet effective mechanism. A distribution-related temperature adjustment is
leveraged to control the class-wise contrastive degrees dynamically. Our method
demonstrates its effectiveness through comprehensive experiments on the
CIFAR10-LT, CIFAR100-LT, STL10-LT, and SVHN-LT datasets across various
settings. For example, BaCon surpasses instance-level method FixMatch-based ABC
on CIFAR10-LT with a 1.21% accuracy improvement, and outperforms
state-of-the-art feature-level method CoSSL on CIFAR100-LT with a 0.63%
accuracy improvement. When encountering more extreme imbalance degree, BaCon
also shows better robustness than other methods.
\\ ( https://arxiv.org/abs/2403.12986 ,  4846kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12987 (*cross-listing*)
Date: Mon, 4 Mar 2024 07:40:25 GMT   (11139kb,D)

Title: Rethinking Specificity in SBDD: Leveraging Delta Score and Energy-Guided
  Diffusion
Authors: Bowen Gao, Minsi Ren, Yuyan Ni, Yanwen Huang, Bo Qiang, Zhi-Ming Ma,
  Wei-Ying Ma, Yanyan Lan
Categories: q-bio.BM cs.LG
\\
  In the field of Structure-based Drug Design (SBDD), deep learning-based
generative models have achieved outstanding performance in terms of docking
score. However, further study shows that the existing molecular generative
methods and docking scores both have lacked consideration in terms of
specificity, which means that generated molecules bind to almost every protein
pocket with high affinity. To address this, we introduce the Delta Score, a new
metric for evaluating the specificity of molecular binding. To further
incorporate this insight for generation, we develop an innovative energy-guided
approach using contrastive learning, with active compounds as decoys, to direct
generative models toward creating molecules with high specificity. Our
empirical results show that this method not only enhances the delta score but
also maintains or improves traditional docking scores, successfully bridging
the gap between SBDD and real-world needs.
\\ ( https://arxiv.org/abs/2403.12987 ,  11139kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12991 (*cross-listing*)
Date: Tue, 5 Mar 2024 06:37:14 GMT   (30966kb,D)

Title: Tel2Veh: Fusion of Telecom Data and Vehicle Flow to Predict Camera-Free
  Traffic via a Spatio-Temporal Framework
Authors: ChungYi Lin, Shen-Lung Tung, Hung-Ting Su, Winston H. Hsu
Categories: cs.CV cs.LG
Comments: 4 pages, 5 figures, 4 tables. Accepted by WWW'24, to appear
\\
  Vehicle flow, a crucial indicator for transportation, is often limited by
detector coverage. With the advent of extensive mobile network coverage, we can
leverage mobile user activities, or cellular traffic, on roadways as a proxy
for vehicle flow. However, as counts of cellular traffic may not directly align
with vehicle flow due to data from various user types, we present a new task:
predicting vehicle flow in camera-free areas using cellular traffic. To uncover
correlations within multi-source data, we deployed cameras on selected roadways
to establish the Tel2Veh dataset, consisting of extensive cellular traffic and
sparse vehicle flows. Addressing this challenge, we propose a framework that
independently extracts features and integrates them with a graph neural network
(GNN)-based fusion to discern disparities, thereby enabling the prediction of
unseen vehicle flows using cellular traffic. This work advances the use of
telecom data in transportation and pioneers the fusion of telecom and
vision-based data, offering solutions for traffic management.
\\ ( https://arxiv.org/abs/2403.12991 ,  30966kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12995 (*cross-listing*)
Date: Tue, 5 Mar 2024 13:35:41 GMT   (578kb,D)

Title: Multi-Scale Protein Language Model for Unified Molecular Modeling
Authors: Kangjie Zheng, Siyu Long, Tianyu Lu, Junwei Yang, Xinyu Dai, Ming
  Zhang, Zaiqing Nie, Wei-Ying Ma, Hao Zhou
Categories: q-bio.BM cs.CE cs.LG
\\
  Protein language models have demonstrated significant potential in the field
of protein engineering. However, current protein language models primarily
operate at the residue scale, which limits their ability to provide information
at the atom level. This limitation prevents us from fully exploiting the
capabilities of protein language models for applications involving both
proteins and small molecules. In this paper, we propose ms-ESM (multi-scale
ESM), a novel approach that enables multi-scale unified molecular modeling.
ms-ESM achieves this by pre-training on multi-scale code-switch protein
sequences and utilizing a multi-scale position encoding to capture
relationships among residues and atoms. Experimental results indicate that
ms-ESM surpasses previous methods in protein-molecule tasks, demonstrating the
full utilization of protein language models. Further investigations reveal that
through unified molecular modeling, ms-ESM not only gains molecular knowledge
but also retains its understanding of proteins.
\\ ( https://arxiv.org/abs/2403.12995 ,  578kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13005 (*cross-listing*)
Date: Thu, 14 Mar 2024 11:53:35 GMT   (4058kb,D)

Title: Leap: molecular synthesisability scoring with intermediates
Authors: Antonia Calvi, Th\'eophile Gaudin, Dominik Miketa, Dominique Sydow,
  Liam Wilbraham
Categories: q-bio.BM cs.LG physics.chem-ph
Comments: New Frontiers of AI for Drug Discovery and Development workshop paper
\\
  Assessing whether a molecule can be synthesised is a primary task in drug
discovery. It enables computational chemists to filter for viable compounds or
bias molecular generative models. The notion of synthesisability is dynamic as
it evolves depending on the availability of key compounds. A common approach in
drug discovery involves exploring the chemical space surrounding
synthetically-accessible intermediates. This strategy improves the
synthesisability of the derived molecules due to the availability of key
intermediates. Existing synthesisability scoring methods such as SAScore,
SCScore and RAScore, cannot condition on intermediates dynamically. Our
approach, Leap, is a GPT-2 model trained on the depth, or longest linear path,
of predicted synthesis routes that allows information on the availability of
key intermediates to be included at inference time. We show that Leap surpasses
all other scoring methods by at least 5% on AUC score when identifying
synthesisable molecules, and can successfully adapt predicted scores when
presented with a relevant intermediate compound.
\\ ( https://arxiv.org/abs/2403.13005 ,  4058kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13008 (*cross-listing*)
Date: Sat, 16 Mar 2024 19:04:11 GMT   (837kb,D)

Title: Speedrunning and path integrals
Authors: Gabriele Lami
Categories: cs.HC cs.GT cs.LG
\\
  In this article we will explore the concept of speedrunning as a
representation of a simplified version of quantum mechanics within a classical
simulation. This analogy can be seen as a simplified approach to understanding
the broader idea that quantum mechanics may emerge from classical mechanics
simulations due to the limitations of the simulation. The concept of
speedrunning will be explored from the perspective inside the simulation, where
the player is seen as a "force of nature" that can be interpreted through
Newton's first law. Starting from this general assumption, the aim is to build
a bridge between these two fields by using the mathematical representation of
path integrals. The use of such an approach as an intermediate layer between
machine learning techniques aimed at finding an optimal strategy and a game
simulation is also analysed. This article will focus primarily on the
relationship between classical and quantum physics within the simulation,
leaving aside more technical issues in field theory such as invariance with
respect to Lorentz transformations and virtual particles.
\\ ( https://arxiv.org/abs/2403.13008 ,  837kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13010 (*cross-listing*)
Date: Sun, 17 Mar 2024 12:26:30 GMT   (4669kb,D)

Title: A Dual-Tier Adaptive One-Class Classification IDS for Emerging
  Cyberthreats
Authors: Md. Ashraf Uddin, Sunil Aryal, Mohamed Reda Bouadjenek, Muna
  Al-Hawawreh, Md. Alamin Talukder
Categories: cs.CR cs.LG
Comments: Deakin University, Australia | This material is based upon work
  supported by the Air Force Office of Scientific Research under award number
  FA2386-23-1-4003
\\
  In today's digital age, our dependence on IoT (Internet of Things) and IIoT
(Industrial IoT) systems has grown immensely, which facilitates sensitive
activities such as banking transactions and personal, enterprise data, and
legal document exchanges. Cyberattackers consistently exploit weak security
measures and tools. The Network Intrusion Detection System (IDS) acts as a
primary tool against such cyber threats. However, machine learning-based IDSs,
when trained on specific attack patterns, often misclassify new emerging
cyberattacks. Further, the limited availability of attack instances for
training a supervised learner and the ever-evolving nature of cyber threats
further complicate the matter. This emphasizes the need for an adaptable IDS
framework capable of recognizing and learning from unfamiliar/unseen attacks
over time. In this research, we propose a one-class classification-driven IDS
system structured on two tiers. The first tier distinguishes between normal
activities and attacks/threats, while the second tier determines if the
detected attack is known or unknown. Within this second tier, we also embed a
multi-classification mechanism coupled with a clustering algorithm. This model
not only identifies unseen attacks but also uses them for retraining them by
clustering unseen attacks. This enables our model to be future-proofed, capable
of evolving with emerging threat patterns. Leveraging one-class classifiers
(OCC) at the first level, our approach bypasses the need for attack samples,
addressing data imbalance and zero-day attack concerns and OCC at the second
level can effectively separate unknown attacks from the known attacks. Our
methodology and evaluations indicate that the presented framework exhibits
promising potential for real-world deployments.
\\ ( https://arxiv.org/abs/2403.13010 ,  4669kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13013 (*cross-listing*)
Date: Sun, 17 Mar 2024 17:16:55 GMT   (10071kb,D)

Title: Hierarchical Classification for Intrusion Detection System: Effective
  Design and Empirical Analysis
Authors: Md. Ashraf Uddin, Sunil Aryal, Mohamed Reda Bouadjenek, Muna
  Al-Hawawreh, Md. Alamin Talukder
Categories: cs.CR cs.LG
Comments: Deakin University, Australia | This material is based upon work
  supported by the Air Force Office of Scientific Research under award number
  FA2386-23-1-4003
\\
  With the increased use of network technologies like Internet of Things (IoT)
in many real-world applications, new types of cyberattacks have been emerging.
To safeguard critical infrastructures from these emerging threats, it is
crucial to deploy an Intrusion Detection System (IDS) that can detect different
types of attacks accurately while minimizing false alarms. Machine learning
approaches have been used extensively in IDS and they are mainly using flat
multi-class classification to differentiate normal traffic and different types
of attacks. Though cyberattack types exhibit a hierarchical structure where
similar granular attack subtypes can be grouped into more high-level attack
types, hierarchical classification approach has not been explored well. In this
paper, we investigate the effectiveness of hierarchical classification approach
in IDS. We use a three-level hierarchical classification model to classify
various network attacks, where the first level classifies benign or attack, the
second level classifies coarse high-level attack types, and the third level
classifies a granular level attack types. Our empirical results of using 10
different classification algorithms in 10 different datasets show that there is
no significant difference in terms of overall classification performance (i.e.,
detecting normal and different types of attack correctly) of hierarchical and
flat classification approaches. However, flat classification approach
misclassify attacks as normal whereas hierarchical approach misclassify one
type of attack as another attack type. In other words, the hierarchical
classification approach significantly minimises attacks from misclassified as
normal traffic, which is more important in critical systems.
\\ ( https://arxiv.org/abs/2403.13013 ,  10071kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13014 (*cross-listing*)
Date: Sun, 17 Mar 2024 17:42:20 GMT   (2304kb)

Title: General Line Coordinates in 3D
Authors: Joshua Martinez, Boris Kovalerchuk
Categories: cs.HC cs.GR cs.LG
Comments: 8 pages, 25 figures
\\
  Interpretable interactive visual pattern discovery in lossless 3D
visualization is a promising way to advance machine learning. It enables end
users who are not data scientists to take control of the model development
process as a self-service. It is conducted in 3D General Line Coordinates (GLC)
visualization space, which preserves all n-D information in 3D. This paper
presents a system which combines three types of GLC: Shifted Paired Coordinates
(SPC), Shifted Tripled Coordinates (STC), and General Line Coordinates-Linear
(GLC-L) for interactive visual pattern discovery. A transition from 2-D
visualization to 3-D visualization allows for a more distinct visual pattern
than in 2-D and it also allows for finding the best data viewing positions,
which are not available in 2-D. It enables in-depth visual analysis of various
class-specific data subsets comprehensible for end users in the original
interpretable attributes. Controlling model overgeneralization by end users is
an additional benefit of this approach.
\\ ( https://arxiv.org/abs/2403.13014 ,  2304kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13015 (*cross-listing*)
Date: Mon, 18 Mar 2024 03:17:08 GMT   (6560kb,D)

Title: HyperVQ: MLR-based Vector Quantization in Hyperbolic Space
Authors: Nabarun Goswami, Yusuke Mukuta, Tatsuya Harada
Categories: eess.IV cs.LG
\\
  The success of models operating on tokenized data has led to an increased
demand for effective tokenization methods, particularly when applied to vision
or auditory tasks, which inherently involve non-discrete data. One of the most
popular tokenization methods is Vector Quantization (VQ), a key component of
several recent state-of-the-art methods across various domains. Typically, a VQ
Variational Autoencoder (VQVAE) is trained to transform data to and from its
tokenized representation. However, since the VQVAE is trained with a
reconstruction objective, there is no constraint for the embeddings to be well
disentangled, a crucial aspect for using them in discriminative tasks.
Recently, several works have demonstrated the benefits of utilizing hyperbolic
spaces for representation learning. Hyperbolic spaces induce compact latent
representations due to their exponential volume growth and inherent ability to
model hierarchical and structured data. In this work, we explore the use of
hyperbolic spaces for vector quantization (HyperVQ), formulating the VQ
operation as a hyperbolic Multinomial Logistic Regression (MLR) problem, in
contrast to the Euclidean K-Means clustering used in VQVAE. Through extensive
experiments, we demonstrate that hyperVQ performs comparably in reconstruction
and generative tasks while outperforming VQ in discriminative tasks and
learning a highly disentangled latent space.
\\ ( https://arxiv.org/abs/2403.13015 ,  6560kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13023 (*cross-listing*)
Date: Mon, 18 Mar 2024 20:20:00 GMT   (438kb,D)

Title: Thwarting Cybersecurity Attacks with Explainable Concept Drift
Authors: Ibrahim Shaer and Abdallah Shami
Categories: cs.CR cs.LG
Comments: 6 pages: Submitted to 2024 IWCMC Smart Energy Workshop
\\
  Cyber-security attacks pose a significant threat to the operation of
autonomous systems. Particularly impacted are the Heating, Ventilation, and Air
Conditioning (HVAC) systems in smart buildings, which depend on data gathered
by sensors and Machine Learning (ML) models using the captured data. As such,
attacks that alter the readings of these sensors can severely affect the HVAC
system operations impacting residents' comfort and energy reduction goals. Such
attacks may induce changes in the online data distribution being fed to the ML
models, violating the fundamental assumption of similarity in training and
testing data distribution. This leads to a degradation in model prediction
accuracy due to a phenomenon known as Concept Drift (CD) - the alteration in
the relationship between input features and the target variable. Addressing CD
requires identifying the source of drift to apply targeted mitigation
strategies, a process termed drift explanation. This paper proposes a Feature
Drift Explanation (FDE) module to identify the drifting features. FDE utilizes
an Auto-encoder (AE) that reconstructs the activation of the first layer of the
regression Deep Learning (DL) model and finds their latent representations.
When a drift is detected, each feature of the drifting data is replaced by its
representative counterpart from the training data. The Minkowski distance is
then used to measure the divergence between the altered drifting data and the
original training data. The results show that FDE successfully identifies 85.77
% of drifting features and showcases its utility in the DL adaptation method
under the CD phenomenon. As a result, the FDE method is an effective strategy
for identifying drifting features towards thwarting cyber-security attacks.
\\ ( https://arxiv.org/abs/2403.13023 ,  438kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13027 (*cross-listing*)
Date: Tue, 19 Mar 2024 01:57:09 GMT   (1511kb,D)

Title: Towards Better Statistical Understanding of Watermarking LLMs
Authors: Zhongze Cai, Shang Liu, Hanzhao Wang, Huaiyang Zhong, Xiaocheng Li
Categories: cs.CR cs.IT cs.LG math.IT stat.ML
\\
  In this paper, we study the problem of watermarking large language models
(LLMs). We consider the trade-off between model distortion and detection
ability and formulate it as a constrained optimization problem based on the
green-red algorithm of Kirchenbauer et al. (2023a). We show that the optimal
solution to the optimization problem enjoys a nice analytical property which
provides a better understanding and inspires the algorithm design for the
watermarking process. We develop an online dual gradient ascent watermarking
algorithm in light of this optimization formulation and prove its asymptotic
Pareto optimality between model distortion and detection ability. Such a result
guarantees an averaged increased green list probability and henceforth
detection ability explicitly (in contrast to previous results). Moreover, we
provide a systematic discussion on the choice of the model distortion metrics
for the watermarking problem. We justify our choice of KL divergence and
present issues with the existing criteria of ``distortion-free'' and
perplexity. Finally, we empirically evaluate our algorithms on extensive
datasets against benchmark algorithms.
\\ ( https://arxiv.org/abs/2403.13027 ,  1511kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13082 (*cross-listing*)
Date: Tue, 19 Mar 2024 18:26:45 GMT   (6177kb,D)

Title: Pruning for Improved ADC Efficiency in Crossbar-based Analog In-memory
  Accelerators
Authors: Timur Ibrayev, Isha Garg, Indranil Chakraborty, and Kaushik Roy
Categories: cs.ET cs.LG
Comments: 11 pages, 5 figures
\\
  Deep learning has proved successful in many applications but suffers from
high computational demands and requires custom accelerators for deployment.
Crossbar-based analog in-memory architectures are attractive for acceleration
of deep neural networks (DNN), due to their high data reuse and high efficiency
enabled by combining storage and computation in memory. However, they require
analog-to-digital converters (ADCs) to communicate crossbar outputs. ADCs
consume a significant portion of energy and area of every crossbar processing
unit, thus diminishing the potential efficiency benefits. Pruning is a
well-studied technique to improve the efficiency of DNNs but requires
modifications to be effective for crossbars. In this paper, we motivate
crossbar-attuned pruning to target ADC-specific inefficiencies. This is
achieved by identifying three key properties (dubbed D.U.B.) that induce
sparsity that can be utilized to reduce ADC energy without sacrificing
accuracy. The first property ensures that sparsity translates effectively to
hardware efficiency by restricting sparsity levels to Discrete powers of 2. The
other 2 properties encourage columns in the same crossbar to achieve both
Unstructured and Balanced sparsity in order to amortize the accuracy drop. The
desired D.U.B. sparsity is then achieved by regularizing the variance of
$L_{0}$ norms of neighboring columns within the same crossbar. Our proposed
implementation allows it to be directly used in end-to-end gradient-based
training. We apply the proposed algorithm to convolutional layers of VGG11 and
ResNet18 models, trained on CIFAR-10 and ImageNet datasets, and achieve up to
7.13x and 1.27x improvement, respectively, in ADC energy with less than 1% drop
in accuracy.
\\ ( https://arxiv.org/abs/2403.13082 ,  6177kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13086 (*cross-listing*)
Date: Tue, 19 Mar 2024 18:32:48 GMT   (7537kb,D)

Title: Listenable Maps for Audio Classifiers
Authors: Francesco Paissan, Mirco Ravanelli, Cem Subakan
Categories: cs.SD cs.LG eess.AS eess.SP
\\
  Despite the impressive performance of deep learning models across diverse
tasks, their complexity poses challenges for interpretation. This challenge is
particularly evident for audio signals, where conveying interpretations becomes
inherently difficult. To address this issue, we introduce Listenable Maps for
Audio Classifiers (L-MAC), a posthoc interpretation method that generates
faithful and listenable interpretations. L-MAC utilizes a decoder on top of a
pretrained classifier to generate binary masks that highlight relevant portions
of the input audio. We train the decoder with a special loss that maximizes the
confidence of the classifier decision on the masked-in portion of the audio
while minimizing the probability of model output for the masked-out portion.
Quantitative evaluations on both in-domain and out-of-domain data demonstrate
that L-MAC consistently produces more faithful interpretations than several
gradient and masking-based methodologies. Furthermore, a user study confirms
that, on average, users prefer the interpretations generated by the proposed
technique.
\\ ( https://arxiv.org/abs/2403.13086 ,  7537kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13117 (*cross-listing*)
Date: Tue, 19 Mar 2024 19:44:54 GMT   (1885kb,D)

Title: Optimal Flow Matching: Learning Straight Trajectories in Just One Step
Authors: Nikita Kornilov, Alexander Gasnikov, Alexander Korotin
Categories: stat.ML cs.LG
\\
  Over the several recent years, there has been a boom in development of flow
matching methods for generative modeling. One intriguing property pursued by
the community is the ability to learn flows with straight trajectories which
realize the optimal transport (OT) displacements. Straightness is crucial for
fast integration of the learned flow's paths. Unfortunately, most existing flow
straightening methods are based on non-trivial iterative procedures which
accumulate the error during training or exploit heuristic minibatch OT
approximations. To address this issue, we develop a novel optimal flow matching
approach which recovers the straight OT displacement for the quadratic cost in
just one flow matching step.
\\ ( https://arxiv.org/abs/2403.13117 ,  1885kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13118 (*cross-listing*)
Date: Tue, 19 Mar 2024 19:47:02 GMT   (33941kb,D)

Title: Modal Analysis of Spatiotemporal Data via Multivariate Gaussian Process
  Regression
Authors: Jiwoo Song, Daning Huang
Categories: stat.ME cs.LG math.DS math.SP stat.ML
Comments: 43 pages, 35 figures
\\
  Modal analysis has become an essential tool to understand the coherent
structure of complex flows. The classical modal analysis methods, such as
dynamic mode decomposition (DMD) and spectral proper orthogonal decomposition
(SPOD), rely on a sufficient amount of data that is regularly sampled in time.
However, often one needs to deal with sparse temporally irregular data, e.g.,
due to experimental measurements and simulation algorithm. To overcome the
limitations of data scarcity and irregular sampling, we propose a novel modal
analysis technique using multi-variate Gaussian process regression (MVGPR). We
first establish the connection between MVGPR and the existing modal analysis
techniques, DMD and SPOD, from a linear system identification perspective.
Next, leveraging this connection, we develop a MVGPR-based modal analysis
technique that addresses the aforementioned limitations. The capability of
MVGPR is endowed by its judiciously designed kernel structure for correlation
function, that is derived from the assumed linear dynamics. Subsequently, the
proposed MVGPR method is benchmarked against DMD and SPOD on a range of
examples, from academic and synthesized data to unsteady airfoil aerodynamics.
The results demonstrate MVGPR as a promising alternative to classical modal
analysis methods, especially in the scenario of scarce and temporally irregular
data.
\\ ( https://arxiv.org/abs/2403.13118 ,  33941kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13135 (*cross-listing*)
Date: Tue, 19 Mar 2024 20:10:50 GMT   (7796kb,D)

Title: A Parallel Workflow for Polar Sea-Ice Classification using Auto-labeling
  of Sentinel-2 Imagery
Authors: Jurdana Masuma Iqrah, Wei Wang, Hongjie Xie, Sushil Prasad
Categories: cs.CV cs.DC cs.LG
Comments: Accepted in the 25th IEEE International Workshop on Parallel and
  Distributed Scientific and Engineering Computing (PDSEC 2024), May 2024.
  arXiv admin note: substantial text overlap with arXiv:2303.12719
\\
  The observation of the advancing and retreating pattern of polar sea ice
cover stands as a vital indicator of global warming. This research aims to
develop a robust, effective, and scalable system for classifying polar sea ice
as thick/snow-covered, young/thin, or open water using Sentinel-2 (S2) images.
Since the S2 satellite is actively capturing high-resolution imagery over the
earth's surface, there are lots of images that need to be classified. One major
obstacle is the absence of labeled S2 training data (images) to act as the
ground truth. We demonstrate a scalable and accurate method for segmenting and
automatically labeling S2 images using carefully determined color thresholds.
We employ a parallel workflow using PySpark to scale and achieve 9-fold data
loading and 16-fold map-reduce speedup on auto-labeling S2 images based on thin
cloud and shadow-filtered color-based segmentation to generate label data. The
auto-labeled data generated from this process are then employed to train a
U-Net machine learning model, resulting in good classification accuracy. As
training the U-Net classification model is computationally heavy and
time-consuming, we distribute the U-Net model training to scale it over 8 GPUs
using the Horovod framework over a DGX cluster with a 7.21x speedup without
affecting the accuracy of the model. Using the Antarctic's Ross Sea region as
an example, the U-Net model trained on auto-labeled data achieves a
classification accuracy of 98.97% for auto-labeled training datasets when the
thin clouds and shadows from the S2 images are filtered out.
\\ ( https://arxiv.org/abs/2403.13135 ,  7796kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13141 (*cross-listing*)
Date: Tue, 19 Mar 2024 20:23:31 GMT   (255kb,D)

Title: Function Trees: Transparent Machine Learning
Authors: Jerome H. Friedman
Categories: stat.ML cs.LG
\\
  The output of a machine learning algorithm can usually be represented by one
or more multivariate functions of its input variables. Knowing the global
properties of such functions can help in understanding the system that produced
the data as well as interpreting and explaining corresponding model
predictions. A method is presented for representing a general multivariate
function as a tree of simpler functions. This tree exposes the global internal
structure of the function by uncovering and describing the combined joint
influences of subsets of its input variables. Given the inputs and
corresponding function values, a function tree is constructed that can be used
to rapidly identify and compute all of the function's main and interaction
effects up to high order. Interaction effects involving up to four variables
are graphically visualized.
\\ ( https://arxiv.org/abs/2403.13141 ,  255kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13148 (*cross-listing*)
Date: Tue, 19 Mar 2024 20:52:31 GMT   (1138kb,D)

Title: SIFT-DBT: Self-supervised Initialization and Fine-Tuning for Imbalanced
  Digital Breast Tomosynthesis Image Classification
Authors: Yuexi Du, Regina J. Hooley, John Lewin, Nicha C. Dvornek
Categories: eess.IV cs.CV cs.LG
Comments: Accepted by IEEE ISBI 2024
\\
  Digital Breast Tomosynthesis (DBT) is a widely used medical imaging modality
for breast cancer screening and diagnosis, offering higher spatial resolution
and greater detail through its 3D-like breast volume imaging capability.
However, the increased data volume also introduces pronounced data imbalance
challenges, where only a small fraction of the volume contains suspicious
tissue. This further exacerbates the data imbalance due to the case-level
distribution in real-world data and leads to learning a trivial classification
model that only predicts the majority class. To address this, we propose a
novel method using view-level contrastive Self-supervised Initialization and
Fine-Tuning for identifying abnormal DBT images, namely SIFT-DBT. We further
introduce a patch-level multi-instance learning method to preserve spatial
resolution. The proposed method achieves 92.69% volume-wise AUC on an
evaluation of 970 unique studies.
\\ ( https://arxiv.org/abs/2403.13148 ,  1138kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13243 (*cross-listing*)
Date: Wed, 20 Mar 2024 02:15:48 GMT   (646kb,D)

Title: A Comparative Study of Machine Learning Models Predicting Energetics of
  Interacting Defects
Authors: Hao Yu
Categories: cond-mat.mtrl-sci cs.LG physics.comp-ph
\\
  Interacting defect systems are ubiquitous in materials under realistic
scenarios, yet gaining an atomic-level understanding of these systems from a
computational perspective is challenging - it often demands substantial
resources due to the necessity of employing supercell calculations. While
machine learning techniques have shown potential in accelerating materials
simulations, their application to systems involving interacting defects remains
relatively rare. In this work, we present a comparative study of three
different methods to predict the free energy change of systems with interacting
defects. We leveraging a limited dataset from Density Functional Theory(DFT)
calculations to assess the performance models using materials descriptors,
graph neural networks and cluster expansion. Our findings indicate that the
cluster expansion model can achieve precise energetics predictions even with
this limited dataset. Furthermore, with synthetic data generate from cluster
expansion model at near-DFT levels, we obtained enlarged dataset to assess the
demands on data for training accurate prediction models using graph neural
networks for systems featuring interacting defects. A brief discussion of the
computational cost for each method is provided at the end. This research
provide a preliminary evaluation of applying machine learning techniques in
imperfect surface systems.
\\ ( https://arxiv.org/abs/2403.13243 ,  646kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13286 (*cross-listing*)
Date: Wed, 20 Mar 2024 03:56:22 GMT   (1899kb,D)

Title: A Sampling-based Framework for Hypothesis Testing on Large Attributed
  Graphs
Authors: Yun Wang, Chrysanthi Kosyfaki, Sihem Amer-Yahia, Reynold Cheng
Categories: stat.ML cs.DB cs.LG
\\
  Hypothesis testing is a statistical method used to draw conclusions about
populations from sample data, typically represented in tables. With the
prevalence of graph representations in real-life applications, hypothesis
testing in graphs is gaining importance. In this work, we formalize node, edge,
and path hypotheses in attributed graphs. We develop a sampling-based
hypothesis testing framework, which can accommodate existing
hypothesis-agnostic graph sampling methods. To achieve accurate and efficient
sampling, we then propose a Path-Hypothesis-Aware SamplEr, PHASE, an m-
dimensional random walk that accounts for the paths specified in a hypothesis.
We further optimize its time efficiency and propose PHASEopt. Experiments on
real datasets demonstrate the ability of our framework to leverage common graph
sampling methods for hypothesis testing, and the superiority of
hypothesis-aware sampling in terms of accuracy and time efficiency.
\\ ( https://arxiv.org/abs/2403.13286 ,  1899kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13298 (*cross-listing*)
Date: Wed, 20 Mar 2024 04:47:13 GMT   (2647kb,D)

Title: Rotary Position Embedding for Vision Transformer
Authors: Byeongho Heo, Song Park, Dongyoon Han, Sangdoo Yun
Categories: cs.CV cs.LG
Comments: 20 pages, 5 figures
\\
  Rotary Position Embedding (RoPE) performs remarkably on language models,
especially for length extrapolation of Transformers. However, the impacts of
RoPE on computer vision domains have been underexplored, even though RoPE
appears capable of enhancing Vision Transformer (ViT) performance in a way
similar to the language domain. This study provides a comprehensive analysis of
RoPE when applied to ViTs, utilizing practical implementations of RoPE for 2D
vision data. The analysis reveals that RoPE demonstrates impressive
extrapolation performance, i.e., maintaining precision while increasing image
resolution at inference. It eventually leads to performance improvement for
ImageNet-1k, COCO detection, and ADE-20k segmentation. We believe this study
provides thorough guidelines to apply RoPE into ViT, promising improved
backbone performance with minimal extra computational overhead. Our code and
pre-trained models are available at https://github.com/naver-ai/rope-vit
\\ ( https://arxiv.org/abs/2403.13298 ,  2647kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13299 (*cross-listing*)
Date: Wed, 20 Mar 2024 04:56:02 GMT   (1597kb,D)

Title: Bridging scales in multiscale bubble growth dynamics with correlated
  fluctuations using neural operator learning
Authors: Minglei Lu, Chensen Lin, Martian Maxey, George Karniadakis, Zhen Li
Categories: physics.flu-dyn cs.LG physics.comp-ph
Comments: 19 pages, 9 figures
\\
  The intricate process of bubble growth dynamics involves a broad spectrum of
physical phenomena from microscale mechanics of bubble formation to macroscale
interplay between bubbles and surrounding thermo-hydrodynamics. Traditional
bubble dynamics models including atomistic approaches and continuum-based
methods segment the bubble dynamics into distinct scale-specific models. In
order to bridge the gap between microscale stochastic fluid models and
continuum-based fluid models for bubble dynamics, we develop a composite neural
operator model to unify the analysis of nonlinear bubble dynamics across
microscale and macroscale regimes by integrating a many-body dissipative
particle dynamics (mDPD) model with a continuum-based Rayleigh-Plesset (RP)
model through a novel neural network architecture, which consists of a deep
operator network for learning the mean behavior of bubble growth subject to
pressure variations and a long short-term memory network for learning the
statistical features of correlated fluctuations in microscale bubble dynamics.
Training and testing data are generated by conducting mDPD and RP simulations
for nonlinear bubble dynamics with initial bubble radii ranging from 0.1 to 1.5
micrometers. Results show that the trained composite neural operator model can
accurately predict bubble dynamics across scales, with a 99% accuracy for the
time evaluation of the bubble radius under varying external pressure while
containing correct size-dependent stochastic fluctuations in microscale bubble
growth dynamics. The composite neural operator is the first deep learning
surrogate for multiscale bubble growth dynamics that can capture correct
stochastic fluctuations in microscopic fluid phenomena, which sets a new
direction for future research in multiscale fluid dynamics modeling.
\\ ( https://arxiv.org/abs/2403.13299 ,  1597kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13300 (*cross-listing*)
Date: Wed, 20 Mar 2024 04:57:27 GMT   (975kb,D)

Title: Kernel Multigrid: Accelerate Back-fitting via Sparse Gaussian Process
  Regression
Authors: Lu Zou and Liang Ding
Categories: stat.ML cs.LG
\\
  Additive Gaussian Processes (GPs) are popular approaches for nonparametric
feature selection. The common training method for these models is Bayesian
Back-fitting. However, the convergence rate of Back-fitting in training
additive GPs is still an open problem. By utilizing a technique called Kernel
Packets (KP), we prove that the convergence rate of Back-fitting is no faster
than $(1-\mathcal{O}(\frac{1}{n}))^t$, where $n$ and $t$ denote the data size
and the iteration number, respectively. Consequently, Back-fitting requires a
minimum of $\mathcal{O}(n\log n)$ iterations to achieve convergence. Based on
KPs, we further propose an algorithm called Kernel Multigrid (KMG). This
algorithm enhances Back-fitting by incorporating a sparse Gaussian Process
Regression (GPR) to process the residuals subsequent to each Back-fitting
iteration. It is applicable to additive GPs with both structured and scattered
data. Theoretically, we prove that KMG reduces the required iterations to
$\mathcal{O}(\log n)$ while preserving the time and space complexities at
$\mathcal{O}(n\log n)$ and $\mathcal{O}(n)$ per iteration, respectively.
Numerically, by employing a sparse GPR with merely 10 inducing points, KMG can
produce accurate approximations of high-dimensional targets within 5
iterations.
\\ ( https://arxiv.org/abs/2403.13300 ,  975kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13310 (*cross-listing*)
Date: Wed, 20 Mar 2024 05:23:09 GMT   (2103kb,D)

Title: A Semantic Search Engine for Mathlib4
Authors: Guoxiong Gao, Haocheng Ju, Jiedong Jiang, Zihan Qin, Bin Dong
Categories: cs.IR cs.LG cs.LO
\\
  The interactive theorem prover, Lean, enables the verification of formal
mathematical proofs and is backed by an expanding community. Central to this
ecosystem is its mathematical library, mathlib4, which lays the groundwork for
the formalization of an expanding range of mathematical theories. However,
searching for theorems in mathlib4 can be challenging. To successfully search
in mathlib4, users often need to be familiar with its naming conventions or
documentation strings. Therefore, creating a semantic search engine that can be
used easily by individuals with varying familiarity with mathlib4 is very
important. In this paper, we present a semantic search engine for mathlib4 that
accepts informal queries and finds the relevant theorems. We also establish a
benchmark for assessing the performance of various search engines for mathlib4.
\\ ( https://arxiv.org/abs/2403.13310 ,  2103kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13319 (*cross-listing*)
Date: Wed, 20 Mar 2024 05:50:04 GMT   (861kb,D)

Title: HyperFusion: A Hypernetwork Approach to Multimodal Integration of
  Tabular and Medical Imaging Data for Predictive Modeling
Authors: Daniel Duenias, Brennan Nichyporuk, Tal Arbel, Tammy Riklin Raviv
Categories: cs.CV cs.LG eess.IV
Comments: 17 pages, 8 figures
\\
  The integration of diverse clinical modalities such as medical imaging and
the tabular data obtained by the patients' Electronic Health Records (EHRs) is
a crucial aspect of modern healthcare. The integrative analysis of multiple
sources can provide a comprehensive understanding of a patient's condition and
can enhance diagnoses and treatment decisions. Deep Neural Networks (DNNs)
consistently showcase outstanding performance in a wide range of multimodal
tasks in the medical domain. However, the complex endeavor of effectively
merging medical imaging with clinical, demographic and genetic information
represented as numerical tabular data remains a highly active and ongoing
research pursuit.
  We present a novel framework based on hypernetworks to fuse clinical imaging
and tabular data by conditioning the image processing on the EHR's values and
measurements. This approach aims to leverage the complementary information
present in these modalities to enhance the accuracy of various medical
applications. We demonstrate the strength and the generality of our method on
two different brain Magnetic Resonance Imaging (MRI) analysis tasks, namely,
brain age prediction conditioned by subject's sex, and multiclass Alzheimer's
Disease (AD) classification conditioned by tabular data. We show that our
framework outperforms both single-modality models and state-of-the-art
MRI-tabular data fusion methods. The code, enclosed to this manuscript will be
made publicly available.
\\ ( https://arxiv.org/abs/2403.13319 ,  861kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13370 (*cross-listing*)
Date: Wed, 20 Mar 2024 08:04:00 GMT   (1073kb,D)

Title: Counting Network for Learning from Majority Label
Authors: Kaito Shiku, Shinnosuke Matsuo, Daiki Suehiro, Ryoma Bise
Categories: cs.CV cs.LG
Comments: 5 pages, 4 figures, Accepted in ICASSP 2024
\\
  The paper proposes a novel problem in multi-class Multiple-Instance Learning
(MIL) called Learning from the Majority Label (LML). In LML, the majority class
of instances in a bag is assigned as the bag's label. LML aims to classify
instances using bag-level majority classes. This problem is valuable in various
applications. Existing MIL methods are unsuitable for LML due to aggregating
confidences, which may lead to inconsistency between the bag-level label and
the label obtained by counting the number of instances for each class. This may
lead to incorrect instance-level classification. We propose a counting network
trained to produce the bag-level majority labels estimated by counting the
number of instances for each class. This led to the consistency of the majority
class between the network outputs and one obtained by counting the number of
instances. Experimental results show that our counting network outperforms
conventional MIL methods on four datasets The code is publicly available at
https://github.com/Shiku-Kaito/Counting-Network-for-Learning-from-Majority-Label.
\\ ( https://arxiv.org/abs/2403.13370 ,  1073kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13429 (*cross-listing*)
Date: Wed, 20 Mar 2024 09:17:12 GMT   (1651kb,D)

Title: Detecting and Triaging Spoofing using Temporal Convolutional Networks
Authors: Kaushalya Kularatnam, Tania Stathaki
Categories: q-fin.TR cs.CE cs.LG q-fin.CP q-fin.GN
Journal-ref: AAAI 2024 Workshop on AI in Finance for Social Impact
\\
  As algorithmic trading and electronic markets continue to transform the
landscape of financial markets, detecting and deterring rogue agents to
maintain a fair and efficient marketplace is crucial. The explosion of large
datasets and the continually changing tricks of the trade make it difficult to
adapt to new market conditions and detect bad actors. To that end, we propose a
framework that can be adapted easily to various problems in the space of
detecting market manipulation. Our approach entails initially employing a
labelling algorithm which we use to create a training set to learn a weakly
supervised model to identify potentially suspicious sequences of order book
states. The main goal here is to learn a representation of the order book that
can be used to easily compare future events. Subsequently, we posit the
incorporation of expert assessment to scrutinize specific flagged order book
states. In the event of an expert's unavailability, recourse is taken to the
application of a more complex algorithm on the identified suspicious order book
states. We then conduct a similarity search between any new representation of
the order book against the expert labelled representations to rank the results
of the weak learner. We show some preliminary results that are promising to
explore further in this direction
\\ ( https://arxiv.org/abs/2403.13429 ,  1651kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13545 (*cross-listing*)
Date: Wed, 20 Mar 2024 12:31:13 GMT   (244kb,D)

Title: Next day fire prediction via semantic segmentation
Authors: Konstantinos Alexis, Stella Girtsou, Alexis Apostolakis, Giorgos
  Giannopoulos and Charalampos Kontoes
Categories: cs.CV cs.LG
Comments: Accepted in MACLEAN@ECML/PKDD 2023
\\
  In this paper we present a deep learning pipeline for next day fire
prediction. The next day fire prediction task consists in learning models that
receive as input the available information for an area up until a certain day,
in order to predict the occurrence of fire for the next day. Starting from our
previous problem formulation as a binary classification task on instances
(daily snapshots of each area) represented by tabular feature vectors, we
reformulate the problem as a semantic segmentation task on images; there, each
pixel corresponds to a daily snapshot of an area, while its channels represent
the formerly tabular training features. We demonstrate that this problem
formulation, built within a thorough pipeline achieves state of the art
results.
\\ ( https://arxiv.org/abs/2403.13545 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13551 (*cross-listing*)
Date: Wed, 20 Mar 2024 12:40:32 GMT   (38592kb,D)

Title: Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute
  Editing
Authors: Hangeol Chang, Jinho Chang, and Jong Chul Ye
Categories: cs.CV cs.LG
\\
  Despite recent advancements in text-to-image diffusion models facilitating
various image editing techniques, complex text prompts often lead to an
oversight of some requests due to a bottleneck in processing text information.
To tackle this challenge, we present Ground-A-Score, a simple yet powerful
model-agnostic image editing method by incorporating grounding during score
distillation. This approach ensures a precise reflection of intricate prompt
requirements in the editing outcomes, taking into account the prior knowledge
of the object locations within the image. Moreover, the selective application
with a new penalty coefficient and contrastive loss helps to precisely target
editing areas while preserving the integrity of the objects in the source
image. Both qualitative assessments and quantitative analyses confirm that
Ground-A-Score successfully adheres to the intricate details of extended and
multifaceted prompts, ensuring high-quality outcomes that respect the original
image attributes.
\\ ( https://arxiv.org/abs/2403.13551 ,  38592kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13563 (*cross-listing*)
Date: Wed, 20 Mar 2024 12:56:40 GMT   (1413kb)

Title: DL2Fence: Integrating Deep Learning and Frame Fusion for Enhanced
  Detection and Localization of Refined Denial-of-Service in Large-Scale NoCs
Authors: Haoyu Wang, Basel Halak, Jianjie Ren, Ahmad Atamli
Categories: cs.CR cs.AR cs.LG
\\
  This study introduces a refined Flooding Injection Rate-adjustable
Denial-of-Service (DoS) model for Network-on-Chips (NoCs) and more importantly
presents DL2Fence, a novel framework utilizing Deep Learning (DL) and Frame
Fusion (2F) for DoS detection and localization. Two Convolutional Neural
Networks models for classification and segmentation were developed to detect
and localize DoS respectively. It achieves detection and localization
accuracies of 95.8\% and 91.7\%, and precision rates of 98.5\% and 99.3\% in a
16x16 mesh NoC. The framework's hardware overhead notably decreases by 76.3\%
when scaling from 8x8 to 16x16 NoCs, and it requires 42.4\% less hardware
compared to state-of-the-arts. This advancement demonstrates DL2Fence's
effectiveness in balancing outstanding detection performance in large-scale
NoCs with extremely low hardware overhead.
\\ ( https://arxiv.org/abs/2403.13563 ,  1413kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13565 (*cross-listing*)
Date: Wed, 20 Mar 2024 12:58:46 GMT   (2717kb,D)

Title: AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for
  High-dimensional Regression
Authors: Zelin He, Ying Sun, Jingyuan Liu, Runze Li
Categories: stat.ML cs.LG math.ST stat.ME stat.TH
Comments: Technical Report
\\
  We consider the transfer learning problem in the high dimensional setting,
where the feature dimension is larger than the sample size. To learn
transferable information, which may vary across features or the source samples,
we propose an adaptive transfer learning method that can detect and aggregate
the feature-wise (F-AdaTrans) or sample-wise (S-AdaTrans) transferable
structures. We achieve this by employing a novel fused-penalty, coupled with
weights that can adapt according to the transferable structure. To choose the
weight, we propose a theoretically informed, data-driven procedure, enabling
F-AdaTrans to selectively fuse the transferable signals with the target while
filtering out non-transferable signals, and S-AdaTrans to obtain the optimal
combination of information transferred from each source sample. The
non-asymptotic rates are established, which recover existing near-minimax
optimal rates in special cases. The effectiveness of the proposed method is
validated using both synthetic and real data.
\\ ( https://arxiv.org/abs/2403.13565 ,  2717kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13627 (*cross-listing*)
Date: Wed, 20 Mar 2024 14:23:17 GMT   (5534kb,D)

Title: Efficient exploration of high-Tc superconductors by a gradient-based
  composition design
Authors: Akihiro Fujii, Koji Shimizu and Satoshi Watanabe
Categories: cond-mat.supr-con cond-mat.mtrl-sci cs.LG
\\
  We propose a material design method via gradient-based optimization on
compositions, overcoming the limitations of traditional methods: exhaustive
database searches and conditional generation models. It optimizes inputs via
backpropagation, aligning the model's output closely with the target property
and facilitating the discovery of unlisted materials and precise property
determination. Our method is also capable of adaptive optimization under new
conditions without retraining. Applying to exploring high-Tc superconductors,
we identified potential compositions beyond existing databases and discovered
new hydrogen superconductors via conditional optimization. This method is
versatile and significantly advances material design by enabling efficient,
extensive searches and adaptability to new constraints.
\\ ( https://arxiv.org/abs/2403.13627 ,  5534kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13695 (*cross-listing*)
Date: Wed, 20 Mar 2024 15:57:44 GMT   (132kb,D)

Title: Loss Regularizing Robotic Terrain Classification
Authors: Shakti Deo Kumar, Sudhanshu Tripathi, Krishna Ujjwal, Sarvada Sakshi
  Jha, Suddhasil De
Categories: cs.RO cs.LG
Comments: Preliminary draft of the work published in IEEE conference 2023
DOI: 10.1109/ICEFEET59656.2023.10452217
\\
  Locomotion mechanics of legged robots are suitable when pacing through
difficult terrains. Recognising terrains for such robots are important to fully
yoke the versatility of their movements. Consequently, robotic terrain
classification becomes significant to classify terrains in real time with high
accuracy. The conventional classifiers suffer from overfitting problem, low
accuracy problem, high variance problem, and not suitable for live dataset. On
the other hand, classifying a growing dataset is difficult for convolution
based terrain classification. Supervised recurrent models are also not
practical for this classification. Further, the existing recurrent
architectures are still evolving to improve accuracy of terrain classification
based on live variable-length sensory data collected from legged robots. This
paper proposes a new semi-supervised method for terrain classification of
legged robots, avoiding preprocessing of long variable-length dataset. The
proposed method has a stacked Long Short-Term Memory architecture, including a
new loss regularization. The proposed method solves the existing problems and
improves accuracy. Comparison with the existing architectures show the
improvements.
\\ ( https://arxiv.org/abs/2403.13695 ,  132kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13701 (*cross-listing*)
Date: Wed, 20 Mar 2024 16:06:01 GMT   (2464kb,D)

Title: What Matters for Active Texture Recognition With Vision-Based Tactile
  Sensors
Authors: Alina B\"ohm, Tim Schneider, Boris Belousov, Alap Kshirsagar, Lisa
  Lin, Katja Doerschner, Knut Drewing, Constantin A. Rothkopf, Jan Peters
Categories: cs.RO cs.LG
Comments: 7 pages, 9 figures, accepted at 2024 IEEE International Conference on
  Robotics and Automation (ICRA)
\\
  This paper explores active sensing strategies that employ vision-based
tactile sensors for robotic perception and classification of fabric textures.
We formalize the active sampling problem in the context of tactile fabric
recognition and provide an implementation of information-theoretic exploration
strategies based on minimizing predictive entropy and variance of probabilistic
models. Through ablation studies and human experiments, we investigate which
components are crucial for quick and reliable texture recognition. Along with
the active sampling strategies, we evaluate neural network architectures,
representations of uncertainty, influence of data augmentation, and dataset
variability. By evaluating our method on a previously published Active Clothing
Perception Dataset and on a real robotic system, we establish that the choice
of the active exploration strategy has only a minor influence on the
recognition accuracy, whereas data augmentation and dropout rate play a
significantly larger role. In a comparison study, while humans achieve 66.9%
recognition accuracy, our best approach reaches 90.0% in under 5 touches,
highlighting that vision-based tactile sensors are highly effective for fabric
texture recognition.
\\ ( https://arxiv.org/abs/2403.13701 ,  2464kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13704 (*cross-listing*)
Date: Wed, 20 Mar 2024 16:08:27 GMT   (5512kb,D)

Title: Improving the Adaptive Moment Estimation (ADAM) stochastic optimizer
  through an Implicit-Explicit (IMEX) time-stepping approach
Authors: Abhinab Bhattacharjee, Andrey A. Popov, Arash Sarshar and Adrian Sandu
Categories: cs.CE cs.LG cs.NA math.NA
Report-no: CSL-TR-2024-2
\\
  The Adam optimizer, often used in Machine Learning for neural network
training, corresponds to an underlying ordinary differential equation (ODE) in
the limit of very small learning rates. This work shows that the classical Adam
algorithm is a first order implicit-explicit (IMEX) Euler discretization of the
underlying ODE. Employing the time discretization point of view, we propose new
extensions of the Adam scheme obtained by using higher order IMEX methods to
solve the ODE. Based on this approach, we derive a new optimization algorithm
for neural network training that performs better than classical Adam on several
regression and classification problems.
\\ ( https://arxiv.org/abs/2403.13704 ,  5512kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13748 (*cross-listing*)
Date: Wed, 20 Mar 2024 16:56:08 GMT   (1285kb,D)

Title: An Ordering of Divergences for Variational Inference with Factorized
  Gaussian Approximations
Authors: Charles C. Margossian, Loucas Pillaud-Vivien and Lawrence K. Saul
Categories: stat.ML cs.LG stat.CO
\\
  Given an intractable distribution $p$, the problem of variational inference
(VI) is to compute the best approximation $q$ from some more tractable family
$\mathcal{Q}$. Most commonly the approximation is found by minimizing a
Kullback-Leibler (KL) divergence. However, there exist other valid choices of
divergences, and when $\mathcal{Q}$ does not contain~$p$, each divergence
champions a different solution. We analyze how the choice of divergence affects
the outcome of VI when a Gaussian with a dense covariance matrix is
approximated by a Gaussian with a diagonal covariance matrix. In this setting
we show that different divergences can be \textit{ordered} by the amount that
their variational approximations misestimate various measures of uncertainty,
such as the variance, precision, and entropy. We also derive an impossibility
theorem showing that no two of these measures can be simultaneously matched by
a factorized approximation; hence, the choice of divergence informs which
measure, if any, is correctly estimated. Our analysis covers the KL divergence,
the R\'enyi divergences, and a score-based divergence that compares $\nabla\log
p$ and $\nabla\log q$. We empirically evaluate whether these orderings hold
when VI is used to approximate non-Gaussian distributions.
\\ ( https://arxiv.org/abs/2403.13748 ,  1285kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13771 (*cross-listing*)
Date: Wed, 20 Mar 2024 17:33:02 GMT   (33144kb,D)

Title: Describe-and-Dissect: Interpreting Neurons in Vision Networks with
  Language Models
Authors: Nicholas Bai, Rahul A. Iyer, Tuomas Oikarinen, Tsui-Wei Weng
Categories: cs.CV cs.LG
\\
  In this paper, we propose Describe-and-Dissect (DnD), a novel method to
describe the roles of hidden neurons in vision networks. DnD utilizes recent
advancements in multimodal deep learning to produce complex natural language
descriptions, without the need for labeled training data or a predefined set of
concepts to choose from. Additionally, DnD is training-free, meaning we don't
train any new models and can easily leverage more capable general purpose
models in the future. We have conducted extensive qualitative and quantitative
analysis to show that DnD outperforms prior work by providing higher quality
neuron descriptions. Specifically, our method on average provides the highest
quality labels and is more than 2 times as likely to be selected as the best
explanation for a neuron than the best baseline.
\\ ( https://arxiv.org/abs/2403.13771 ,  33144kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13795 (*cross-listing*)
Date: Wed, 22 Nov 2023 14:18:30 GMT   (615kb,D)

Title: PyVRP: a high-performance VRP solver package
Authors: Niels A. Wouda and Leon Lan and Wouter Kool
Categories: cs.NE cs.LG
Comments: Pre-print of accepted paper in INFORMS Journal on Computing. 21
  pages, 1 figure
DOI: 10.1287/ijoc.2023.0055
\\
  We introduce PyVRP, a Python package that implements Hybrid Genetic Search as
a state-of-the-art Vehicle Routing Problem (VRP) solver. The package is
designed for the VRP with Time Windows (VRPTW), but can be easily extended to
support other VRP variants. PyVRP combines the flexibility of Python with the
performance of C++, by implementing (only) performance critical parts of the
algorithm in C++, while being fully customisable at the Python level. PyVRP is
a polished implementation of the algorithm that ranked 1st in the 2021 DIMACS
VRPTW Challenge and, after improvements, ranked 1st on the static variant of
the EURO Meets NeurIPS 2022 Vehicle Routing Competition. The code follows good
software engineering practices, and is well-documented and unit tested. PyVRP
is freely available under the liberal MIT license. Through numerical
experiments we show that PyVRP achieves state-of-the-art results on the VRPTW
and Capacitated VRP. We hope that PyVRP offers a useful contribution to
improving the state-of-the-art in VRP solving, by enabling researchers to
easily and quickly build on a state-of-the-art solver.
\\ ( https://arxiv.org/abs/2403.13795 ,  615kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13807 (*cross-listing*)
Date: Wed, 20 Mar 2024 17:59:57 GMT   (32228kb,D)

Title: Editing Massive Concepts in Text-to-Image Diffusion Models
Authors: Tianwei Xiong,Yue Wu,Enze Xie,Yue Wu,Zhenguo Li,Xihui Liu
Categories: cs.CV cs.LG
Comments: Project page: https://silentview.github.io/EMCID/ . Code:
  https://github.com/SilentView/EMCID
\\
  Text-to-image diffusion models suffer from the risk of generating outdated,
copyrighted, incorrect, and biased content. While previous methods have
mitigated the issues on a small scale, it is essential to handle them
simultaneously in larger-scale real-world scenarios. We propose a two-stage
method, Editing Massive Concepts In Diffusion Models (EMCID). The first stage
performs memory optimization for each individual concept with dual
self-distillation from text alignment loss and diffusion noise prediction loss.
The second stage conducts massive concept editing with multi-layer, closed form
model editing. We further propose a comprehensive benchmark, named ImageNet
Concept Editing Benchmark (ICEB), for evaluating massive concept editing for
T2I models with two subtasks, free-form prompts, massive concept categories,
and extensive evaluation metrics. Extensive experiments conducted on our
proposed benchmark and previous benchmarks demonstrate the superior scalability
of EMCID for editing up to 1,000 concepts, providing a practical approach for
fast adjustment and re-deployment of T2I diffusion models in real-world
applications.
\\ ( https://arxiv.org/abs/2403.13807 ,  32228kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2402.15506
replaced with revised version Wed, 20 Mar 2024 06:00:14 GMT   (3807kb,D)

Title: AgentOhana: Design Unified Data and Training Pipeline for Effective
  Agent Learning
Authors: Jianguo Zhang, Tian Lan, Rithesh Murthy, Zhiwei Liu, Weiran Yao,
  Juntao Tan, Thai Hoang, Liangwei Yang, Yihao Feng, Zuxin Liu, Tulika
  Awalgaonkar, Juan Carlos Niebles, Silvio Savarese, Shelby Heinecke, Huan
  Wang, Caiming Xiong
Categories: cs.AI cs.CL cs.LG
Comments: Add GitHub repo link at
  \url{https://github.com/SalesforceAIResearch/xLAM} and HuggingFace model link
  at \url{https://huggingface.co/Salesforce/xLAM-v0.1-r}
\\ ( https://arxiv.org/abs/2402.15506 ,  3807kb)
------------------------------------------------------------------------------
\\
arXiv:2205.00415
replaced with revised version Wed, 20 Mar 2024 03:23:11 GMT   (6756kb,D)

Title: Don't Blame the Annotator: Bias Already Starts in the Annotation
  Instructions
Authors: Mihir Parmar, Swaroop Mishra, Mor Geva, Chitta Baral
Categories: cs.CL cs.AI cs.CV cs.LG
Comments: EACL 2023 (Outstanding Paper Award)
\\ ( https://arxiv.org/abs/2205.00415 ,  6756kb)
------------------------------------------------------------------------------
\\
arXiv:2210.03963
replaced with revised version Wed, 20 Mar 2024 03:06:16 GMT   (4279kb,D)

Title: SDA: Simple Discrete Augmentation for Contrastive Sentence
  Representation Learning
Authors: Dongsheng Zhu, Zhenyu Mao, Jinghui Lu, Rui Zhao, Fei Tan
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\ ( https://arxiv.org/abs/2210.03963 ,  4279kb)
------------------------------------------------------------------------------
\\
arXiv:2303.11117
replaced with revised version Wed, 20 Mar 2024 11:35:04 GMT   (1168kb,D)

Title: EmotionIC: emotional inertia and contagion-driven dependency modeling
  for emotion recognition in conversation
Authors: Yingjian Liu, Jiang Li, Xiaoping Wang, Zhigang Zeng
Categories: cs.CL
Comments: Accepted by SCIENCE CHINA Information Sciences (SCIS)
DOI: 10.1007/s11432-023-3908-6
\\ ( https://arxiv.org/abs/2303.11117 ,  1168kb)
------------------------------------------------------------------------------
\\
arXiv:2305.09098
replaced with revised version Wed, 20 Mar 2024 15:41:07 GMT   (2354kb,D)

Title: Weight-Inherited Distillation for Task-Agnostic BERT Compression
Authors: Taiqiang Wu, Cheng Hou, Shanshan Lao, Jiayi Li, Ngai Wong, Zhe Zhao,
  Yujiu Yang
Categories: cs.CL cs.LG
Comments: 9 pages, 4 figures, NAACL2024 findings
\\ ( https://arxiv.org/abs/2305.09098 ,  2354kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13888
replaced with revised version Wed, 20 Mar 2024 08:37:42 GMT   (6021kb,D)

Title: PaD: Program-aided Distillation Can Teach Small Models Reasoning Better
  than Chain-of-thought Fine-tuning
Authors: Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xinwei Long, Zhouhan Lin, Bowen
  Zhou
Categories: cs.CL
Comments: NAACL 2024 Long Paper; Code and data are available at
  https://github.com/Xuekai-Zhu/pad
\\ ( https://arxiv.org/abs/2305.13888 ,  6021kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14456
replaced with revised version Wed, 20 Mar 2024 17:16:37 GMT   (9724kb,D)

Title: Having Beer after Prayer? Measuring Cultural Bias in Large Language
  Models
Authors: Tarek Naous, Michael J. Ryan, Alan Ritter, Wei Xu
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2305.14456 ,  9724kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18248
replaced with revised version Wed, 20 Mar 2024 13:12:48 GMT   (804kb,D)

Title: Do Language Models Know When They're Hallucinating References?
Authors: Ayush Agrawal, Mirac Suzgun, Lester Mackey, and Adam Tauman Kalai
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2305.18248 ,  804kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01931
replaced with revised version Wed, 20 Mar 2024 03:33:32 GMT   (2336kb,D)

Title: Exploring semantic information in disease: Simple Data Augmentation
  Techniques for Chinese Disease Normalization
Authors: Wenqian Cui and Xiangling Fu and Shaohui Liu and Mingjun Gu and Xien
  Liu and Ji Wu and Irwin King
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2306.01931 ,  2336kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12245
replaced with revised version Wed, 20 Mar 2024 03:51:23 GMT   (3669kb,D)

Title: Bidirectional End-to-End Learning of Retriever-Reader Paradigm for
  Entity Linking
Authors: Yinghui Li, Yong Jiang, Yangning Li, Xingyu Lu, Pengjun Xie, Ying
  Shen, Hai-Tao Zheng
Categories: cs.CL
\\ ( https://arxiv.org/abs/2306.12245 ,  3669kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12725
replaced with revised version Wed, 20 Mar 2024 01:30:41 GMT   (3280kb,D)

Title: Generative Multimodal Entity Linking
Authors: Senbao Shi, Zhenran Xu, Baotian Hu, Min Zhang
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\ ( https://arxiv.org/abs/2306.12725 ,  3280kb)
------------------------------------------------------------------------------
\\
arXiv:2306.17447
replaced with revised version Wed, 20 Mar 2024 15:53:37 GMT   (1189kb,D)

Title: Correct Like Humans: Progressive Learning Framework for Chinese Text
  Error Correction
Authors: Yinghui Li, Shirong Ma, Shaoshen Chen, Haojing Huang, Shulin Huang,
  Yangning Li, Hai-Tao Zheng, Ying Shen
Categories: cs.CL
\\ ( https://arxiv.org/abs/2306.17447 ,  1189kb)
------------------------------------------------------------------------------
\\
arXiv:2307.05113
replaced with revised version Wed, 20 Mar 2024 11:56:52 GMT   (7766kb,D)

Title: Piecing Together Clues: A Benchmark for Evaluating the Detective Skills
  of Large Language Models
Authors: Zhouhong Gu, Lin Zhang, Jiangjie Chen, Haoning Ye, Xiaoxuan Zhu, Zihan
  Li, Zheyu Ye, Yan Gao, Yao Hu, Yanghua Xiao, Hongwei Feng
Categories: cs.CL
\\ ( https://arxiv.org/abs/2307.05113 ,  7766kb)
------------------------------------------------------------------------------
\\
arXiv:2308.00221
replaced with revised version Wed, 20 Mar 2024 01:04:11 GMT   (516kb,D)

Title: Advancing Beyond Identification: Multi-bit Watermark for Large Language
  Models
Authors: KiYoon Yoo, Wonhyuk Ahn, Nojun Kwak
Categories: cs.CL cs.AI cs.CR
Comments: NAACL 2024 main. 9 pages and appendix
\\ ( https://arxiv.org/abs/2308.00221 ,  516kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08739
replaced with revised version Wed, 20 Mar 2024 16:41:11 GMT   (673kb,D)

Title: Enhancing Phrase Representation by Information Bottleneck Guided Text
  Diffusion Process for Keyphrase Extraction
Authors: Yuanzhen Luo, Qingyu Zhou and Feng Zhou
Categories: cs.CL
Comments: 10 pages, 2 figures, accepted to LREC-COLING 2024
\\ ( https://arxiv.org/abs/2308.08739 ,  673kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02706
replaced with revised version Wed, 20 Mar 2024 16:56:48 GMT   (756kb,D)

Title: HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models
Authors: Guijin Son, Hanwool Lee, Suwan Kim, Huiseo Kim, Jaecheol Lee, Je Won
  Yeom, Jihyu Jung, Jung Woo Kim, Songseong Kim
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2309.02706 ,  756kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04461
replaced with revised version Tue, 19 Mar 2024 21:48:59 GMT   (29521kb,D)

Title: Measuring and Improving Chain-of-Thought Reasoning in Vision-Language
  Models
Authors: Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, Ajay Divakaran
Categories: cs.CL cs.CV cs.LG
Comments: NAACL 2024 Main Conference. The data is released at
  https://github.com/Yangyi-Chen/CoTConsistency
\\ ( https://arxiv.org/abs/2309.04461 ,  29521kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07915
replaced with revised version Wed, 20 Mar 2024 16:17:02 GMT   (43479kb,D)

Title: MMICL: Empowering Vision-language Model with Multi-Modal In-Context
  Learning
Authors: Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang
  Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, Baobao Chang
Categories: cs.CL cs.AI cs.CV
Comments: Accepted by ICLR2024
\\ ( https://arxiv.org/abs/2309.07915 ,  43479kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08150
replaced with revised version Wed, 20 Mar 2024 02:17:16 GMT   (1140kb,D)

Title: Unimodal Aggregation for CTC-based Speech Recognition
Authors: Ying Fang, Xiaofei Li
Categories: cs.CL cs.SD eess.AS
Comments: Accepted by ICASSP 2024
\\ ( https://arxiv.org/abs/2309.08150 ,  1140kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09783
replaced with revised version Wed, 20 Mar 2024 10:33:24 GMT   (37kb,D)

Title: The ParlaSent Multilingual Training Dataset for Sentiment Identification
  in Parliamentary Proceedings
Authors: Michal Mochtak, Peter Rupnik, Nikola Ljube\v{s}i\'c
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.09783 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13243
replaced with revised version Wed, 20 Mar 2024 08:16:14 GMT   (0kb,I)

Title: ChEDDAR: Student-ChatGPT Dialogue in EFL Writing Education
Authors: Jieun Han, Haneul Yoo, Junho Myung, Minsun Kim, Tak Yeon Lee, So-Yeon
  Ahn, Alice Oh
Categories: cs.CL
Comments: The new version of this paper is on arXiv as arXiv:2403.08272
\\ ( https://arxiv.org/abs/2309.13243 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00785
replaced with revised version Tue, 19 Mar 2024 20:13:59 GMT   (1436kb,D)

Title: BooookScore: A systematic exploration of book-length summarization in
  the era of LLMs
Authors: Yapei Chang, Kyle Lo, Tanya Goyal, Mohit Iyyer
Categories: cs.CL cs.AI cs.LG
Comments: ICLR 2024 camera-ready
\\ ( https://arxiv.org/abs/2310.00785 ,  1436kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12963
replaced with revised version Wed, 20 Mar 2024 16:36:06 GMT   (469kb,D)

Title: AutoMix: Automatically Mixing Language Models
Authors: Aman Madaan, Pranjal Aggarwal, Ankit Anand, Srividya Pranavi
  Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik
  Kappaganthu, Yiming Yang, Shyam Upadhyay, Mausam, Manaal Faruqui
Categories: cs.CL cs.AI
Comments: The first two authors contributed equally. Work started and partly
  done during Aman's internship at Google. This version adds results on
  additional models and datasets
\\ ( https://arxiv.org/abs/2310.12963 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07838
replaced with revised version Wed, 20 Mar 2024 05:04:06 GMT   (321kb,D)

Title: LLatrieval: LLM-Verified Retrieval for Verifiable Generation
Authors: Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin, Tianxiang Sun,
  Xipeng Qiu
Categories: cs.CL cs.AI cs.IR
Comments: Accepted by NAACL 2024 (Main Conference)
\\ ( https://arxiv.org/abs/2311.07838 ,  321kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14648
replaced with revised version Wed, 20 Mar 2024 02:21:20 GMT   (168kb,D)

Title: Calibrated Language Models Must Hallucinate
Authors: Adam Tauman Kalai, Santosh S. Vempala
Categories: cs.CL cs.AI
Comments: In Proceedings of the 56th Annual ACM Symposium on Theory of
  Computing (STOC) 2024
\\ ( https://arxiv.org/abs/2311.14648 ,  168kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04262
replaced with revised version Wed, 20 Mar 2024 01:59:39 GMT   (997kb,D)

Title: PsyChat: A Client-Centric Dialogue System for Mental Health Support
Authors: Huachuan Qiu, Anqi Li, Lizhi Ma, Zhenzhong Lan
Categories: cs.CL cs.HC
Comments: Accepted to CSCWD 2024 (27th International Conference on Computer
  Supported Cooperative Work in Design)
\\ ( https://arxiv.org/abs/2312.04262 ,  997kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03512
replaced with revised version Wed, 20 Mar 2024 07:39:48 GMT   (2698kb,D)

Title: CharPoet: A Chinese Classical Poetry Generation System Based on
  Token-free LLM
Authors: Chengyue Yu, Lei Zang, Jiaotuan Wang, Chenyi Zhuang, Jinjie Gu
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.03512 ,  2698kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09002
replaced with revised version Wed, 20 Mar 2024 14:08:39 GMT   (7823kb,D)

Title: AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on
  Large Language Models
Authors: Dong shu, Mingyu Jin, Suiyuan Zhu, Beichen Wang, Zihao Zhou, Chong
  Zhang, Yongfeng Zhang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.09002 ,  7823kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11467
replaced with revised version Wed, 20 Mar 2024 07:08:22 GMT   (7869kb,D)

Title: Over-Reasoning and Redundant Calculation of Large Language Models
Authors: Cheng-Han Chiang, Hung-yi Lee
Categories: cs.CL
Comments: EACL 2024 main conference paper. Camera-ready version
\\ ( https://arxiv.org/abs/2401.11467 ,  7869kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14166
replaced with revised version Wed, 20 Mar 2024 08:52:42 GMT   (580kb,D)

Title: BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on
  Few-shot Inference via Debiased Domain Abstraction
Authors: Jiangmeng Li, Fei Song, Yifan Jin, Wenwen Qiang, Changwen Zheng,
  Fuchun Sun, Hui Xiong
Categories: cs.CL cs.AI
Comments: Accepted by ICLR2024
\\ ( https://arxiv.org/abs/2401.14166 ,  580kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08015
replaced with revised version Wed, 20 Mar 2024 13:33:19 GMT   (1166kb,D)

Title: Walia-LLM: Enhancing Amharic-LLaMA by Integrating Task-Specific and
  Generative Datasets
Authors: Israel Abebe Azime, Atnafu Lambebo Tonja, Tadesse Destaw Belay, Mitiku
  Yohannes Fuge, Aman Kassahun Wassie, Eyasu Shiferaw Jada, Yonas Chanie,
  Walelign Tewabe Sewunetie, Seid Muhie Yimam
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.08015 ,  1166kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00758
replaced with revised version Wed, 20 Mar 2024 07:37:24 GMT   (73kb,D)

Title: Mitigating Reversal Curse in Large Language Models via Semantic-aware
  Permutation Training
Authors: Qingyan Guo, Rui Wang, Junliang Guo, Xu Tan, Jiang Bian, Yujiu Yang
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.00758 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02889
replaced with revised version Wed, 20 Mar 2024 09:53:17 GMT   (7038kb,D)

Title: In Search of Truth: An Interrogation Approach to Hallucination Detection
Authors: Yakir Yehuda, Itzik Malkiel, Oren Barkan, Jonathan Weill, Royi Ronen
  and Noam Koenigstein
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2403.02889 ,  7038kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03861
replaced with revised version Tue, 19 Mar 2024 19:51:09 GMT   (8010kb,D)

Title: Designing Informative Metrics for Few-Shot Example Selection
Authors: Rishabh Adiga, Lakshminarayanan Subramanian, Varun Chandrasekaran
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2403.03861 ,  8010kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06832
replaced with revised version Wed, 20 Mar 2024 10:02:54 GMT   (470kb,D)

Title: The Power of Noise: Toward a Unified Multi-modal Knowledge Graph
  Representation Framework
Authors: Zhuo Chen, Yin Fang, Yichi Zhang, Lingbing Guo, Jiaoyan Chen, Huajun
  Chen, Wen Zhang
Categories: cs.CL cs.AI
Comments: Ongoing work; 10 pages, 6 Tables, 2 Figures; Repo is available at
  https://github.com/zjukg/SNAG
\\ ( https://arxiv.org/abs/2403.06832 ,  470kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07693
replaced with revised version Tue, 19 Mar 2024 19:20:05 GMT   (1507kb,D)

Title: Large, Small or Both: A Novel Data Augmentation Framework Based on
  Language Models for Debiasing Opinion Summarization
Authors: Yanyue Zhang, Pengfei Li, Yilong Lai, Deyu Zhou, Yulan He
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.07693 ,  1507kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07726
replaced with revised version Wed, 20 Mar 2024 09:36:13 GMT   (8830kb,D)

Title: SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and
  Related Observable Overgeneration Mistakes
Authors: Timothee Mickus, Elaine Zosa, Ra\'ul V\'azquez, Teemu Vahtola, J\"org
  Tiedemann, Vincent Segonne, Alessandro Raganato, Marianna Apidianaki
Categories: cs.CL
Comments: SemEval 2024 shared task. Pre-review version
\\ ( https://arxiv.org/abs/2403.07726 ,  8830kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09738
replaced with revised version Tue, 19 Mar 2024 18:35:40 GMT   (1105kb,D)

Title: Evaluating Large Language Models as Generative User Simulators for
  Conversational Recommendation
Authors: Se-eun Yoon, Zhankui He, Jessica Maria Echterhoff, Julian McAuley
Categories: cs.CL cs.AI cs.IR
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2403.09738 ,  1105kb)
------------------------------------------------------------------------------
\\
arXiv:2002.10121
replaced with revised version Wed, 20 Mar 2024 17:15:32 GMT   (2121kb,D)

Title: The Unreasonable Effectiveness of Greedy Algorithms in Multi-Armed
  Bandit with Many Arms
Authors: Mohsen Bayati, Nima Hamidi, Ramesh Johari, Khashayar Khosravi
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2002.10121 ,  2121kb)
------------------------------------------------------------------------------
\\
arXiv:2104.02726
replaced with revised version Tue, 19 Mar 2024 18:00:02 GMT   (533kb,D)

Title: Creativity and Machine Learning: A Survey
Authors: Giorgio Franceschelli and Mirco Musolesi
Categories: cs.LG cs.AI cs.CY
Comments: 39 pages, 1 figure, 2 tables
\\ ( https://arxiv.org/abs/2104.02726 ,  533kb)
------------------------------------------------------------------------------
\\
arXiv:2105.06251
replaced with revised version Tue, 19 Mar 2024 18:02:11 GMT   (5680kb,D)

Title: Learning Weakly Convex Sets in Metric Spaces
Authors: Eike Stadtl\"ander, Tam\'as Horv\'ath, Stefan Wrobel
Categories: cs.LG cs.AI stat.ML
Comments: completely revised version, currently under review
\\ ( https://arxiv.org/abs/2105.06251 ,  5680kb)
------------------------------------------------------------------------------
\\
arXiv:2110.14961
replaced with revised version Wed, 20 Mar 2024 16:45:00 GMT   (3368kb,D)

Title: Roto-translated Local Coordinate Frames For Interacting Dynamical
  Systems
Authors: Miltiadis Kofinas, Naveen Shankar Nagaraja, Efstratios Gavves
Categories: cs.LG stat.ML
Comments: In NeurIPS 2021. Source code: https://github.com/mkofinas/locs
\\ ( https://arxiv.org/abs/2110.14961 ,  3368kb)
------------------------------------------------------------------------------
\\
arXiv:2208.08270
replaced with revised version Wed, 20 Mar 2024 14:13:44 GMT   (423kb,D)

Title: On the Privacy Effect of Data Enhancement via the Lens of Memorization
Authors: Xiao Li and Qiongxiu Li and Zhanhao Hu and Xiaolin Hu
Categories: cs.LG cs.CR cs.CV
Comments: Accepted by IEEE TIFS, 17 pages
\\ ( https://arxiv.org/abs/2208.08270 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2208.12932
replaced with revised version Wed, 20 Mar 2024 02:11:56 GMT   (454kb,D)

Title: BOBA: Byzantine-Robust Federated Learning with Label Skewness
Authors: Wenxuan Bao, Jun Wu, Jingrui He
Categories: cs.LG
Comments: Accepted by AISTATS 2024
\\ ( https://arxiv.org/abs/2208.12932 ,  454kb)
------------------------------------------------------------------------------
\\
arXiv:2210.04688
replaced with revised version Wed, 20 Mar 2024 01:20:29 GMT   (3450kb,D)

Title: BAFFLE: Hiding Backdoors in Offline Reinforcement Learning Datasets
Authors: Chen Gong, Zhou Yang, Yunpeng Bai, Junda He, Jieke Shi, Kecen Li,
  Arunesh Sinha, Bowen Xu, Xinwen Hou, David Lo, Tianhao Wang
Categories: cs.LG cs.AI cs.CR
Comments: Accepted at IEEE S&P (Oakland) 2024
\\ ( https://arxiv.org/abs/2210.04688 ,  3450kb)
------------------------------------------------------------------------------
\\
arXiv:2210.17159
replaced with revised version Wed, 20 Mar 2024 02:21:23 GMT   (2717kb,D)

Title: PAGE: Prototype-Based Model-Level Explanations for Graph Neural Networks
Authors: Yong-Min Shin, Sun-Woo Kim, Won-Yong Shin
Categories: cs.LG cs.AI cs.IT cs.NE cs.SI math.IT
Comments: 18 pages, 12 figures, 5 tables; to appear in the IEEE Transactions on
  Pattern Analysis and Machine Intelligence (Please cite our journal version
  that will appear in an upcoming issue. Its two-page extended summary was
  presented in the AAAI-22 Student Abstract and Poster Program.)
\\ ( https://arxiv.org/abs/2210.17159 ,  2717kb)
------------------------------------------------------------------------------
\\
arXiv:2301.12951
replaced with revised version Wed, 20 Mar 2024 03:26:28 GMT   (3860kb,D)

Title: Unraveling Privacy Risks of Individual Fairness in Graph Neural Networks
Authors: He Zhang, Xingliang Yuan, Shirui Pan
Categories: cs.LG cs.CY
Comments: Accepted by IEEE International Conference on Data Engineering (ICDE)
  2024
\\ ( https://arxiv.org/abs/2301.12951 ,  3860kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11288
replaced with revised version Wed, 20 Mar 2024 15:10:09 GMT   (477kb,D)

Title: Riemannian Multinomial Logistics Regression for SPD Neural Networks
Authors: Ziheng Chen, Yue Song, Gaowen Liu, Ramana Rao Kompella, Xiaojun Wu,
  Nicu Sebe
Categories: cs.LG
Comments: Accepted to CVPR 2024
\\ ( https://arxiv.org/abs/2305.11288 ,  477kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11414
replaced with revised version Tue, 19 Mar 2024 20:04:43 GMT   (7010kb,D)

Title: Federated Foundation Models: Privacy-Preserving and Collaborative
  Learning for Large Models
Authors: Sixing Yu, J. Pablo Mu\~noz, Ali Jannesari
Categories: cs.LG cs.AI cs.CR
Comments: Accepted at the 2024 Joint International Conference on Computational
  Linguistics, Language Resources and Evaluation (LREC-COLING 2024)
\\ ( https://arxiv.org/abs/2305.11414 ,  7010kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17282
replaced with revised version Wed, 20 Mar 2024 17:25:52 GMT   (39kb,D)

Title: Universal consistency of the $k$-NN rule in metric spaces and Nagata
  dimension. II
Authors: Sushma Kumari and Vladimir G. Pestov
Categories: cs.LG
Comments: Latex 2e, 27 pages, 1 figure. Minor revisions to conform with the
  last set of journal page proofs: two typos corrected, the bibliography
  rearranged in the order of citations (the ESAIM:PS home style), and two
  articles that were no longer cited removed
MSC-class: 62H30, 54F45
\\ ( https://arxiv.org/abs/2305.17282 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06101
replaced with revised version Tue, 19 Mar 2024 23:01:06 GMT   (4028kb,D)

Title: Prodigy: An Expeditiously Adaptive Parameter-Free Learner
Authors: Konstantin Mishchenko, Aaron Defazio
Categories: cs.LG cs.AI math.OC stat.ML
\\ ( https://arxiv.org/abs/2306.06101 ,  4028kb)
------------------------------------------------------------------------------
\\
arXiv:2306.07292
replaced with revised version Tue, 19 Mar 2024 20:03:20 GMT   (2965kb,D)

Title: $GRU^{spa}$: Gated Recurrent Unit with Spatial Attention for
  Spatio-Temporal Disaggregation
Authors: Bin Han, Bill Howe
Categories: cs.LG cs.AI cs.CR
\\ ( https://arxiv.org/abs/2306.07292 ,  2965kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00618
replaced with revised version Wed, 20 Mar 2024 15:17:43 GMT   (3203kb,D)

Title: Bounce: Reliable High-Dimensional Bayesian Optimization for
  Combinatorial and Mixed Spaces
Authors: Leonard Papenmeier, Luigi Nardi, Matthias Poloczek
Categories: cs.LG
Comments: 30 pages, 22 figures
Journal-ref: Advances in Neural Information Processing Systems 36 (NeurIPS
  2023)
\\ ( https://arxiv.org/abs/2307.00618 ,  3203kb)
------------------------------------------------------------------------------
\\
arXiv:2307.14531
replaced with revised version Wed, 20 Mar 2024 07:49:41 GMT   (200kb,D)

Title: Controlling the Inductive Bias of Wide Neural Networks by Modifying the
  Kernel's Spectrum
Authors: Amnon Geifman, Daniel Barzilai, Ronen Basri and Meirav Galun
Categories: cs.LG
\\ ( https://arxiv.org/abs/2307.14531 ,  200kb)
------------------------------------------------------------------------------
\\
arXiv:2308.00721
replaced with revised version Wed, 20 Mar 2024 07:29:15 GMT   (2333kb,D)

Title: A Pre-trained Data Deduplication Model based on Active Learning
Authors: Xinyao Liu, Shengdong Du, Fengmao Lv, Hongtao Xue, Jie Hu, and Tianrui
  Li
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2308.00721 ,  2333kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09384
replaced with revised version Tue, 19 Mar 2024 20:11:25 GMT   (1137kb,D)

Title: Mitigating Over-Smoothing and Over-Squashing using Augmentations of
  Forman-Ricci Curvature
Authors: Lukas Fesser and Melanie Weber
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2309.09384 ,  1137kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01211
replaced with revised version Wed, 20 Mar 2024 11:33:50 GMT   (12958kb,D)

Title: From Bricks to Bridges: Product of Invariances to Enhance Latent Space
  Communication
Authors: Irene Cannistraci, Luca Moschella, Marco Fumero, Valentino Maiorca,
  Emanuele Rodol\`a
Categories: cs.LG
Comments: 41 pages, 14 figures and 31 tables
\\ ( https://arxiv.org/abs/2310.01211 ,  12958kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02903
replaced with revised version Tue, 19 Mar 2024 18:20:51 GMT   (5688kb,D)

Title: FroSSL: Frobenius Norm Minimization for Efficient Multiview
  Self-Supervised Learning
Authors: Oscar Skean, Aayush Dhakal, Nathan Jacobs, Luis Gonzalo Sanchez
  Giraldo
Categories: cs.LG
Comments: Updated to reflect ECCV submission
\\ ( https://arxiv.org/abs/2310.02903 ,  5688kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07183
replaced with revised version Wed, 20 Mar 2024 06:42:18 GMT   (22750kb,D)

Title: SAM-OCTA: Prompting Segment-Anything for OCTA Image Segmentation
Authors: Xinrun Chen, Chengliang Wang, Haojian Ning, Shiying Li, Mei Shen
Categories: cs.LG
Comments: arXiv admin note: text overlap with arXiv:2309.11758
\\ ( https://arxiv.org/abs/2310.07183 ,  22750kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07923
replaced with revised version Wed, 20 Mar 2024 17:55:48 GMT   (100kb)

Title: The Expressive Power of Transformers with Chain of Thought
Authors: William Merrill and Ashish Sabharwal
Categories: cs.LG cs.CC cs.CL cs.LO
Comments: 9-page preprint. Updated March 20 after ICLR acceptance
\\ ( https://arxiv.org/abs/2310.07923 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08056
replaced with revised version Wed, 20 Mar 2024 07:23:32 GMT   (928kb,D)

Title: Learning from Label Proportions: Bootstrapping Supervised Learners via
  Belief Propagation
Authors: Shreyas Havaldar, Navodita Sharma, Shubhi Sareen, Karthikeyan
  Shanmugam, Aravindan Raghuveer
Categories: cs.LG cs.AI
Comments: Published as a conference paper at The Twelfth International
  Conference on Learning Representations (ICLR 2024) & Oral Presentation at
  Regulatable ML @ NeurIPS 2023
\\ ( https://arxiv.org/abs/2310.08056 ,  928kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10060
replaced with revised version Wed, 20 Mar 2024 12:25:51 GMT   (2241kb,D)

Title: Data Augmentation for Time-Series Classification: An Extensive Empirical
  Study and Comprehensive Survey
Authors: Zijun Gao and Lingbo Li
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.10060 ,  2241kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10402
replaced with revised version Wed, 20 Mar 2024 12:52:10 GMT   (5684kb,D)

Title: Real-Fake: Effective Training Data Synthesis Through Distribution
  Matching
Authors: Jianhao Yuan and Jie Zhang and Shuyang Sun and Philip Torr and Bo Zhao
Categories: cs.LG cs.AI
Comments: Code released at
  (https://github.com/BAAI-DCAI/Training-Data-Synthesis)
Journal-ref: ICLR 2024
\\ ( https://arxiv.org/abs/2310.10402 ,  5684kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10705
replaced with revised version Wed, 20 Mar 2024 15:26:55 GMT   (22185kb)

Title: Observational and Experimental Insights into Machine Learning-Based
  Defect Classification in Wafers
Authors: Kamal Taha
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.10705 ,  22185kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13805
replaced with revised version Wed, 20 Mar 2024 16:23:20 GMT   (0kb,I)

Title: Normalizing flow-based deep variational Bayesian network for seismic
  multi-hazards and impacts estimation from InSAR imagery
Authors: Xuechun Li, Paula M. Burgi, Wei Ma, Hae Young Noh, David J. Wald, Susu
  Xu
Categories: cs.LG cs.CV
Comments: This paper needs to be reviewed by the USGS
\\ ( https://arxiv.org/abs/2310.13805 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20204
replaced with revised version Wed, 20 Mar 2024 10:52:03 GMT   (1221kb,D)

Title: General-Purpose Retrieval-Enhanced Medical Prediction Model Using
  Near-Infinite History
Authors: Junu Kim and Chaeeun Shim and Bosco Seong Kyu Yang and Chami Im and
  Sung Yoon Lim and Han-Gil Jeong and Edward Choi
Categories: cs.LG cs.CL
Comments: The source codes corresponding to this paper are available at:
  https://github.com/starmpcc/REMed
\\ ( https://arxiv.org/abs/2310.20204 ,  1221kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20679
replaced with revised version Wed, 20 Mar 2024 16:05:03 GMT   (33680kb,D)

Title: Latent Field Discovery In Interacting Dynamical Systems With Neural
  Fields
Authors: Miltiadis Kofinas, Erik J. Bekkers, Naveen Shankar Nagaraja,
  Efstratios Gavves
Categories: cs.LG stat.ML
Comments: In NeurIPS 2023. Source code: https://github.com/mkofinas/aether
\\ ( https://arxiv.org/abs/2310.20679 ,  33680kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00118
replaced with revised version Tue, 19 Mar 2024 18:53:38 GMT   (1163kb,D)

Title: Extracting the Multiscale Causal Backbone of Brain Dynamics
Authors: Gabriele D'Acunto, Francesco Bonchi, Gianmarco De Francisci Morales,
  Giovanni Petri
Categories: cs.LG q-bio.NC stat.AP stat.ME stat.ML
Comments: Accepted at the 3rd conference on Causal Learning and Reasoning
  (CLeaR 2024)
\\ ( https://arxiv.org/abs/2311.00118 ,  1163kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04896
replaced with revised version Wed, 20 Mar 2024 01:42:25 GMT   (7090kb,D)

Title: Machine-learning optimized measurements of chaotic dynamical systems via
  the information bottleneck
Authors: Kieran A. Murphy and Dani S. Bassett
Categories: cs.LG cs.IT math.IT nlin.CD
Comments: Project page: https://distributed-information-bottleneck.github.io
\\ ( https://arxiv.org/abs/2311.04896 ,  7090kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09441
replaced with revised version Wed, 20 Mar 2024 04:19:25 GMT   (5344kb,D)

Title: Exploring the Privacy-Energy Consumption Tradeoff for Split Federated
  Learning
Authors: Joohyung Lee, Mohamed Seif, Jungchan Cho, H. Vincent Poor
Categories: cs.LG cs.AI cs.CR
Comments: 10 pages, 5 figures
\\ ( https://arxiv.org/abs/2311.09441 ,  5344kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13279
replaced with revised version Wed, 20 Mar 2024 02:25:36 GMT   (845kb,D)

Title: Comprehensive Evaluation of GNN Training Systems: A Data Management
  Perspective
Authors: Hao Yuan, Yajiong Liu, Yanfeng Zhang, Xin Ai, Qiange Wang, Chaoyi
  Chen, Yu Gu, Ge Yu
Categories: cs.LG cs.DC
Comments: 12 pages, 18 figures. (Accepted by VLDB 2024)
\\ ( https://arxiv.org/abs/2311.13279 ,  845kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13349
replaced with revised version Wed, 20 Mar 2024 10:21:34 GMT   (644kb,D)

Title: REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource
  Constraints
Authors: Francesco Corti, Balz Maag, Joachim Schauer, Ulrich Pferschy, Olga
  Saukh
Categories: cs.LG
\\ ( https://arxiv.org/abs/2311.13349 ,  644kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13469
replaced with revised version Tue, 19 Mar 2024 18:24:43 GMT   (31kb)

Title: Span-Based Optimal Sample Complexity for Average Reward MDPs
Authors: Matthew Zurek, Yudong Chen
Categories: cs.LG cs.IT math.IT math.OC stat.ML
Comments: 19 pages; this article superseded by arXiv:2403.11477
\\ ( https://arxiv.org/abs/2311.13469 ,  31kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00477
replaced with revised version Wed, 20 Mar 2024 15:25:02 GMT   (5771kb,D)

Title: Interpretable Meta-Learning of Physical Systems
Authors: Matthieu Blanke and Marc Lelarge
Categories: cs.LG stat.ML
Journal-ref: The Twelfth International Conference on Learning Representations,
  ICLR 2024
\\ ( https://arxiv.org/abs/2312.00477 ,  5771kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16560
replaced with revised version Wed, 20 Mar 2024 10:10:33 GMT   (1688kb,D)

Title: Adaptive Message Passing: A General Framework to Mitigate Oversmoothing,
  Oversquashing, and Underreaching
Authors: Federico Errica, Henrik Christiansen, Viktor Zaverkin, Takashi
  Maruyama, Mathias Niepert, Francesco Alesiani
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.16560 ,  1688kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03597
replaced with revised version Tue, 19 Mar 2024 23:01:23 GMT   (1357kb,D)

Title: Few-Shot Causal Representation Learning for Out-of-Distribution
  Generalization on Heterogeneous Graphs
Authors: Pengfei Ding and Yan Wang and Guanfeng Liu and Nan Wang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.03597 ,  1357kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12517
replaced with revised version Wed, 20 Mar 2024 11:24:06 GMT   (22453kb,D)

Title: DDMI: Domain-Agnostic Latent Diffusion Models for Synthesizing
  High-Quality Implicit Neural Representations
Authors: Dogyun Park, Sihyeon Kim, Sojin Lee, Hyunwoo J. Kim
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2401.12517 ,  22453kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16914
replaced with revised version Wed, 20 Mar 2024 10:37:10 GMT   (15795kb,D)

Title: Energy-conserving equivariant GNN for elasticity of lattice architected
  metamaterials
Authors: Ivan Grega, Ilyes Batatia, G\'abor Cs\'anyi, Sri Karlapati, Vikram S.
  Deshpande
Categories: cs.LG cond-mat.mtrl-sci
Comments: International Conference on Learning Representations 2024
\\ ( https://arxiv.org/abs/2401.16914 ,  15795kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03818
replaced with revised version Wed, 20 Mar 2024 15:08:27 GMT   (246kb,D)

Title: Asymptotic generalization error of a single-layer graph convolutional
  network
Authors: O. Duranthon, L. Zdeborov\'a
Categories: cs.LG cond-mat.dis-nn
\\ ( https://arxiv.org/abs/2402.03818 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05667
replaced with revised version Wed, 20 Mar 2024 15:33:23 GMT   (6438kb,D)

Title: S$\Omega$I: Score-based O-INFORMATION Estimation
Authors: Mustapha Bounoua, Giulio Franzese, Pietro Michiardi
Categories: cs.LG cs.IT math.IT
\\ ( https://arxiv.org/abs/2402.05667 ,  6438kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07785
replaced with revised version Tue, 19 Mar 2024 20:17:10 GMT   (2829kb,D)

Title: HYPO: Hyperspherical Out-of-Distribution Generalization
Authors: Yifei Ming, Haoyue Bai, Julian Katz-Samuels, and Yixuan Li
Categories: cs.LG
Comments: The conference version of this paper is published at ICLR 2024; First
  two authors contributed equally
\\ ( https://arxiv.org/abs/2402.07785 ,  2829kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07999
replaced with revised version Wed, 20 Mar 2024 14:30:41 GMT   (451kb,D)

Title: NetInfoF Framework: Measuring and Exploiting Network Usable Information
Authors: Meng-Chieh Lee, Haiyang Yu, Jian Zhang, Vassilis N. Ioannidis, Xiang
  Song, Soji Adeshina, Da Zheng, Christos Faloutsos
Categories: cs.LG cs.SI
Comments: Accepted to ICLR 2024 (Spotlight)
\\ ( https://arxiv.org/abs/2402.07999 ,  451kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09782
replaced with revised version Wed, 20 Mar 2024 08:50:46 GMT   (4811kb,D)

Title: MC-DBN: A Deep Belief Network-Based Model for Modality Completion
Authors: Zihong Luo, Zheng Tao, Yuxuan Huang, Kexin He, Chengzhi Liu
Categories: cs.LG cs.AI
Journal-ref: International Conference on Computer Supported Cooperative Work in
  Design 2024
\\ ( https://arxiv.org/abs/2402.09782 ,  4811kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15266
replaced with revised version Wed, 20 Mar 2024 10:12:49 GMT   (423kb,D)

Title: Calibration of Deep Learning Classification Models in fNIRS
Authors: Zhihao Cao, Zizhou Luo
Categories: cs.LG cs.AI eess.SP
DOI: 10.36227/techrxiv.170861971.16301364/v1
\\ ( https://arxiv.org/abs/2402.15266 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18546
replaced with revised version Tue, 19 Mar 2024 21:54:05 GMT   (7500kb,D)

Title: Generalizability Under Sensor Failure: Tokenization + Transformers
  Enable More Robust Latent Spaces
Authors: Geeling Chau, Yujin An, Ahamed Raffey Iqbal, Soon-Jo Chung, Yisong
  Yue, Sabera Talukder
Categories: cs.LG
Comments: 9 pages, 8 figures
\\ ( https://arxiv.org/abs/2402.18546 ,  7500kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03672
replaced with revised version Wed, 20 Mar 2024 08:50:24 GMT   (55kb)

Title: Learning Adversarial MDPs with Stochastic Hard Constraints
Authors: Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi,
  Nicola Gatti
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.03672 ,  55kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11062
replaced with revised version Wed, 20 Mar 2024 00:38:58 GMT   (8603kb,D)

Title: A Simple Mixture Policy Parameterization for Improving Sample Efficiency
  of CVaR Optimization
Authors: Yudong Luo, Yangchen Pan, Han Wang, Philip Torr, Pascal Poupart
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2403.11062 ,  8603kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11544
replaced with revised version Wed, 20 Mar 2024 01:22:11 GMT   (63kb)

Title: RL in Markov Games with Independent Function Approximation: Improved
  Sample Complexity Bound under the Local Access Model
Authors: Junyi Fan, Yuxuan Han, Jialin Zeng, Jian-Feng Cai, Yang Wang, Yang
  Xiang, Jiheng Zhang
Categories: cs.LG
Comments: Accepted at the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS 2024)
\\ ( https://arxiv.org/abs/2403.11544 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12143
replaced with revised version Wed, 20 Mar 2024 16:12:12 GMT   (1072kb,D)

Title: Graph Neural Networks for Learning Equivariant Representations of Neural
  Networks
Authors: Miltiadis Kofinas, Boris Knyazev, Yan Zhang, Yunlu Chen, Gertjan J.
  Burghouts, Efstratios Gavves, Cees G. M. Snoek, David W. Zhang
Categories: cs.LG cs.AI stat.ML
Comments: In ICLR 2024. Source code: https://github.com/mkofinas/neural-graphs
\\ ( https://arxiv.org/abs/2403.12143 ,  1072kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12166
replaced with revised version Wed, 20 Mar 2024 15:27:44 GMT   (3790kb,D)

Title: The Power of Few: Accelerating and Enhancing Data Reweighting with
  Coreset Selection
Authors: Mohammad Jafari, Yimeng Zhang, Yihua Zhang, Sijia Liu
Categories: cs.LG stat.ML
Comments: Accepted to ICASSP 2024
\\ ( https://arxiv.org/abs/2403.12166 ,  3790kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12690
replaced with revised version Wed, 20 Mar 2024 11:06:34 GMT   (765kb,D)

Title: LNPT: Label-free Network Pruning and Training
Authors: Jinying Xiao, Ping Li, Zhe Tang, Jie Nie
Categories: cs.LG
Comments: 8 pages,7 figures
\\ ( https://arxiv.org/abs/2403.12690 ,  765kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12844
replaced with revised version Wed, 20 Mar 2024 09:06:08 GMT   (4534kb,D)

Title: MELTing point: Mobile Evaluation of Language Transformers
Authors: Stefanos Laskaridis, Kleomenis Katevas, Lorenzo Minto, Hamed Haddadi
Categories: cs.LG
Comments: Under review
\\ ( https://arxiv.org/abs/2403.12844 ,  4534kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12847
replaced with revised version Wed, 20 Mar 2024 03:13:47 GMT   (2590kb,D)

Title: Policy Bifurcation in Safe Reinforcement Learning
Authors: Wenjun Zou, Yao Lv, Jie Li, Yujie Yang, Shengbo Eben Li, Jingliang
  Duan, Xianyuan Zhan, Jingjing Liu, Yaqin Zhang, and Keqiang Li
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.12847 ,  2590kb)
------------------------------------------------------------------------------
\\
arXiv:2302.05666
replaced with revised version Wed, 20 Mar 2024 16:50:25 GMT   (4566kb,D)

Title: Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels
Authors: Zifu Wang and Xuefei Ning and Matthew B. Blaschko
Categories: cs.CV cs.AI cs.LG
Comments: NeurIPS 2023
\\ ( https://arxiv.org/abs/2302.05666 ,  4566kb)
------------------------------------------------------------------------------
\\
arXiv:2302.09389
replaced with revised version Wed, 20 Mar 2024 13:11:19 GMT   (616kb)

Title: Vulnerability analysis of captcha using Deep learning
Authors: Jaskaran Singh Walia and Aryan Odugoudar
Categories: cs.CR cs.AI cs.CV cs.LG
DOI: 10.1109/ICTBIG59752.2023.10456218
\\ ( https://arxiv.org/abs/2302.09389 ,  616kb)
------------------------------------------------------------------------------
\\
arXiv:2303.16296
replaced with revised version Wed, 20 Mar 2024 15:52:49 GMT   (702kb,D)

Title: Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels
Authors: Zifu Wang, Teodora Popordanoska, Jeroen Bertels, Robin Lemmens,
  Matthew B. Blaschko
Categories: cs.CV cs.AI cs.LG
Comments: MICCAI 2023
\\ ( https://arxiv.org/abs/2303.16296 ,  702kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06192
replaced with revised version Wed, 20 Mar 2024 17:36:07 GMT   (8078kb,D)

Title: Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy
  Learning for Robotic Navigation
Authors: Bhrij Patel, Kasun Weerakoon, Wesley A. Suttle, Alec Koppel, Brian M.
  Sadler, Tianyi Zhou, Amrit Singh Bedi and Dinesh Manocha
Categories: cs.RO cs.AI cs.LG
Comments: 11 pages, 9 figures, 2 tables
\\ ( https://arxiv.org/abs/2306.06192 ,  8078kb)
------------------------------------------------------------------------------
\\
arXiv:2306.11335
replaced with revised version Wed, 20 Mar 2024 13:18:18 GMT   (5674kb,D)

Title: Surfer: Progressive Reasoning with World Models for Robotic Manipulation
Authors: Pengzhen Ren, Kaidong Zhang, Hetao Zheng, Zixuan Li, Yuhang Wen,
  Fengda Zhu, Mas Ma, Xiaodan Liang
Categories: cs.RO cs.AI cs.CV cs.LG
\\ ( https://arxiv.org/abs/2306.11335 ,  5674kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14565
replaced with revised version Tue, 19 Mar 2024 22:53:25 GMT   (6948kb,D)

Title: Mitigating Hallucination in Large Multi-Modal Models via Robust
  Instruction Tuning
Authors: Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, Lijuan
  Wang
Categories: cs.CV cs.AI cs.CE cs.CL cs.MM
Comments: 40 pages, 32 figures, ICLR 2024
\\ ( https://arxiv.org/abs/2306.14565 ,  6948kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06742
replaced with revised version Wed, 20 Mar 2024 05:43:00 GMT   (4313kb,D)

Title: Vehicle Dispatching and Routing of On-Demand Intercity Ride-Pooling
  Services: A Multi-Agent Hierarchical Reinforcement Learning Approach
Authors: Jinhua Si, Fang He, Xi Lin, Xindi Tang
Categories: eess.SY cs.AI cs.LG cs.SY
\\ ( https://arxiv.org/abs/2307.06742 ,  4313kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08309
replaced with revised version Wed, 20 Mar 2024 12:12:59 GMT   (9555kb,D)

Title: LogPr\'ecis: Unleashing Language Models for Automated Shell Log Analysis
Authors: Matteo Boffa, Rodolfo Vieira Valentim, Luca Vassio, Danilo Giordano,
  Idilio Drago, Marco Mellia, Zied Ben Houidi
Categories: cs.CR cs.AI cs.NI
Comments: 18 pages, Computer&Security
  (https://www.sciencedirect.com/science/article/pii/S0167404824001068), code
  available at https://github.com/SmartData-Polito/logprecis, models available
  at https://huggingface.co/SmartDataPolito
Journal-ref: Computers & Security, 2024, 103805, ISSN 0167-4048
DOI: 10.1016/j.cose.2024.103805
\\ ( https://arxiv.org/abs/2307.08309 ,  9555kb)
------------------------------------------------------------------------------
\\
arXiv:2307.10711
replaced with revised version Wed, 20 Mar 2024 07:17:19 GMT   (47980kb,D)

Title: AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of
  Diffusion Probabilistic Models
Authors: Jiachun Pan, Jun Hao Liew, Vincent Y. F. Tan, Jiashi Feng, Hanshu Yan
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2307.10711 ,  47980kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00117
replaced with revised version Wed, 20 Mar 2024 14:26:12 GMT   (5170kb,D)

Title: ABScribe: Rapid Exploration & Organization of Multiple Writing
  Variations in Human-AI Co-Writing Tasks using Large Language Models
Authors: Mohi Reza, Nathan Laundry, Ilya Musabirov, Peter Dushniku, Zhi Yuan
  "Michael" Yu, Kashish Mittal, Tovi Grossman, Michael Liut, Anastasia
  Kuzminykh, Joseph Jay Williams
Categories: cs.HC cs.AI cs.LG
Comments: CHI 2024
DOI: 10.1145/3613904.3641899
\\ ( https://arxiv.org/abs/2310.00117 ,  5170kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07946
replaced with revised version Tue, 19 Mar 2024 19:25:21 GMT   (511kb,D)

Title: The Impact of Adversarial Node Placement in Decentralized Federated
  Learning Networks
Authors: Adam Piaseczny, Eric Ruzomberka, Rohit Parasnis, Christopher G.
  Brinton
Categories: cs.CR cs.AI cs.MA
Comments: Accepted to ICC 2024 conference
\\ ( https://arxiv.org/abs/2311.07946 ,  511kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00210
replaced with revised version Tue, 19 Mar 2024 22:19:18 GMT   (10630kb,D)

Title: DREAM: Diffusion Rectification and Estimation-Adaptive Models
Authors: Jinxin Zhou, Tianyu Ding, Tianyi Chen, Jiachen Jiang, Ilya Zharkov,
  Zhihui Zhu and Luming Liang
Categories: cs.CV cs.AI
Comments: 16 pages, 22 figures, 5 tables; the first two authors contributed to
  this work equally
\\ ( https://arxiv.org/abs/2312.00210 ,  10630kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00651
replaced with revised version Wed, 20 Mar 2024 17:28:02 GMT   (8051kb,D)

Title: TrackDiffusion: Tracklet-Conditioned Video Generation via Diffusion
  Models
Authors: Pengxiang Li, Kai Chen, Zhili Liu, Ruiyuan Gao, Lanqing Hong, Guo
  Zhou, Hua Yao, Dit-Yan Yeung, Huchuan Lu, Xu Jia
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2312.00651 ,  8051kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02696
replaced with revised version Wed, 20 Mar 2024 12:58:14 GMT   (22740kb,D)

Title: Analyzing and Improving the Training Dynamics of Diffusion Models
Authors: Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo
  Aila, Samuli Laine
Categories: cs.CV cs.AI cs.LG cs.NE stat.ML
\\ ( https://arxiv.org/abs/2312.02696 ,  22740kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06644
replaced with revised version Wed, 20 Mar 2024 17:58:05 GMT   (10749kb,D)

Title: AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes
Authors: Rao Fu,Zehao Wen,Zichen Liu,Srinath Sridhar
Categories: cs.CV cs.AI cs.GR
\\ ( https://arxiv.org/abs/2312.06644 ,  10749kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06731
replaced with revised version Wed, 20 Mar 2024 07:00:39 GMT   (8335kb,D)

Title: Genixer: Empowering Multimodal Large Language Models as a Powerful Data
  Generator
Authors: Henry Hengyuan Zhao, Pan Zhou, Mike Zheng Shou
Categories: cs.CV cs.AI
Comments: Technical report
\\ ( https://arxiv.org/abs/2312.06731 ,  8335kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12450
replaced with revised version Wed, 20 Mar 2024 03:59:00 GMT   (651kb,D)

Title: Can It Edit? Evaluating the Ability of Large Language Models to Follow
  Code Editing Instructions
Authors: Federico Cassano, Luisa Li, Akul Sethi, Noah Shinn, Abby
  Brennan-Jones, Jacob Ginesin, Edward Berman, George Chakhnashvili, Anton
  Lozhkov, Carolyn Jane Anderson, Arjun Guha
Categories: cs.SE cs.AI cs.LG cs.PL
\\ ( https://arxiv.org/abs/2312.12450 ,  651kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14115
replaced with revised version Wed, 20 Mar 2024 00:23:39 GMT   (37644kb,D)

Title: LingoQA: Video Question Answering for Autonomous Driving
Authors: Ana-Maria Marcu, Long Chen, Jan H\"unermann, Alice Karnsund, Benoit
  Hanotte, Prajwal Chidananda, Saurabh Nair, Vijay Badrinarayanan, Alex
  Kendall, Jamie Shotton, Elahe Arani, Oleg Sinavski
Categories: cs.RO cs.AI cs.CV
Comments: Benchmark and dataset are available at
  https://github.com/wayveai/LingoQA/
\\ ( https://arxiv.org/abs/2312.14115 ,  37644kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09368
replaced with revised version Wed, 20 Mar 2024 17:36:35 GMT   (31203kb,D)

Title: Magic-Me: Identity-Specific Video Customized Diffusion
Authors: Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui
  Yang, Zhen Dong, Kurt Keutzer, Jiashi Feng
Categories: cs.CV cs.AI
Comments: Project Page at https://magic-me-webpage.github.io
\\ ( https://arxiv.org/abs/2402.09368 ,  31203kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14304
replaced with revised version Fri, 15 Mar 2024 12:31:35 GMT   (0kb,I)

Title: Vision-Language Navigation with Embodied Intelligence: A Survey
Authors: Peng Gao, Peng Wang, Feng Gao, Fei Wang, Ruyue Yuan
Categories: cs.RO cs.AI cs.CV
Comments: The pictures in Figures 2, 4, and 5 are used without authorization,
  and the literatures in Table 1 have been cited improperly
\\ ( https://arxiv.org/abs/2402.14304 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06225
replaced with revised version Wed, 20 Mar 2024 10:05:02 GMT   (48672kb,D)

Title: MoST: Motion Style Transformer between Diverse Action Contents
Authors: Boeun Kim, Jungho Kim, Hyung Jin Chang, Jin Young Choi
Categories: cs.CV cs.AI
Comments: Accepted by CVPR 2024
\\ ( https://arxiv.org/abs/2403.06225 ,  48672kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08153
replaced with revised version Wed, 20 Mar 2024 00:18:40 GMT   (15kb)

Title: The Runtime of Random Local Search on the Generalized Needle Problem
Authors: Benjamin Doerr, Andrew James Kelley
Categories: cs.NE cs.AI cs.DS
Comments: 18 pages
\\ ( https://arxiv.org/abs/2403.08153 ,  15kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08505 (*cross-listing*)
replaced with revised version Wed, 20 Mar 2024 02:35:57 GMT   (2616kb,D)

Title: Content-aware Masked Image Modeling Transformer for Stereo Image
  Compression
Authors: Xinjie Zhang, Shenyuan Gao, Zhening Liu, Jiawei Shao, Xingtong Ge,
  Dailan He, Tongda Xu, Yan Wang, Jun Zhang
Categories: eess.IV cs.AI cs.CV cs.MM
\\ ( https://arxiv.org/abs/2403.08505 ,  2616kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09184
replaced with revised version Wed, 20 Mar 2024 16:34:37 GMT   (120kb)

Title: Learning Algorithms for Verification of Markov Decision Processes
Authors: Tom\'a\v{s} Br\'azdil, Krishnendu Chatterjee, Martin Chmelik,
  Vojt\v{e}ch Forejt, Jan K\v{r}et\'insk\'y, Marta Kwiatkowska, Tobias
  Meggendorfer, David Parker and Mateusz Ujma
Categories: eess.SY cs.AI cs.SY
\\ ( https://arxiv.org/abs/2403.09184 ,  120kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11552
replaced with revised version Wed, 20 Mar 2024 13:15:39 GMT   (15710kb,D)

Title: LLM3:Large Language Model-based Task and Motion Planning with Motion
  Failure Reasoning
Authors: Shu Wang, Muzhi Han, Ziyuan Jiao, Zeyu Zhang, Ying Nian Wu, Song-Chun
  Zhu, Hangxin Liu
Categories: cs.RO cs.AI
Comments: Submitted to IROS 2024. Codes available:
  https://github.com/AssassinWS/LLM-TAMP
\\ ( https://arxiv.org/abs/2403.11552 ,  15710kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11959
replaced with revised version Wed, 20 Mar 2024 11:58:23 GMT   (661kb,D)

Title: IVAC-P2L: Leveraging Irregular Repetition Priors for Improving Video
  Action Counting
Authors: Hang Wang, Zhi-Qi Cheng, Youtian Du, and Lei Zhang
Categories: cs.CV cs.AI cs.MM
Comments: Source code: https://github.com/hwang-cs-ime/IVAC-P2L
\\ ( https://arxiv.org/abs/2403.11959 ,  661kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12660
replaced with revised version Wed, 20 Mar 2024 05:10:22 GMT   (210kb,D)

Title: ERASE: Benchmarking Feature Selection Methods for Deep Recommender
  Systems
Authors: Pengyue Jia, Yejing Wang, Zhaocheng Du, Xiangyu Zhao, Yichao Wang, Bo
  Chen, Wanyu Wang, Huifeng Guo, Ruiming Tang
Categories: cs.IR cs.AI
\\ ( https://arxiv.org/abs/2403.12660 ,  210kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07081 (*cross-listing*)
replaced with revised version Wed, 20 Mar 2024 03:04:47 GMT   (354kb,D)

Title: Can Whisper perform speech-based in-context learning?
Authors: Siyin Wang, Chao-Han Huck Yang, Ji Wu, Chao Zhang
Categories: eess.AS cs.CL cs.SD
Comments: Accepted by ICASSP 2024
\\ ( https://arxiv.org/abs/2309.07081 ,  354kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10943
replaced with revised version Wed, 20 Mar 2024 02:52:42 GMT   (3509kb,D)

Title: MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent
  Recognition and Out-of-scope Detection in Conversations
Authors: Hanlei Zhang, Xin Wang, Hua Xu, Qianrui Zhou, Kai Gao, Jianhua Su,
  jinyue Zhao, Wenrui Li, Yanting Chen
Categories: cs.MM cs.CL
Comments: Published in ICLR 2024; The abstract is slightly modified due to the
  length limitation
\\ ( https://arxiv.org/abs/2403.10943 ,  3509kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11085
replaced with revised version Wed, 20 Mar 2024 17:35:15 GMT   (10921kb,D)

Title: m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks
Authors: Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, Ranjay Krishna
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2403.11085 ,  10921kb)
------------------------------------------------------------------------------
\\
arXiv:2208.14797
replaced with revised version Wed, 20 Mar 2024 09:44:53 GMT   (1808kb,D)

Title: Sparsification of the regularized magnetic Laplacian with multi-type
  spanning forests
Authors: Micha\"el Fanuel and R\'emi Bardenet
Categories: cs.SI cs.LG stat.ML
Comments: 51 pages, 15 figures. Improved presentation of the theoretical
  results and simulations of larger scale
\\ ( https://arxiv.org/abs/2208.14797 ,  1808kb)
------------------------------------------------------------------------------
\\
arXiv:2209.09371 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 20:41:32 GMT   (2122kb,D)

Title: Topological data analysis on noisy quantum computers
Authors: Ismail Yunus Akhalwaya, Shashanka Ubaru, Kenneth L. Clarkson, Mark S.
  Squillante, Vishnu Jejjala, Yang-Hui He, Kugendran Naidoo, Vasileios
  Kalantzis, Lior Horesh
Categories: quant-ph cs.LG cs.NA math.NA
Comments: This paper is a follow up to arXiv:2108.02811 with improved
  theoretical results and other additional results. This new version presents
  an improved runtime for the algorithm, and fixes an issue present in the
  previous version
Journal-ref: In the Proceedings of The Twelfth International Conference on
  Learning Representations (ICLR 2024)
\\ ( https://arxiv.org/abs/2209.09371 ,  2122kb)
------------------------------------------------------------------------------
\\
arXiv:2212.02477 (*cross-listing*)
replaced with revised version Wed, 20 Mar 2024 00:20:34 GMT   (1457kb)

Title: Malaria Parasitic Detection using a New Deep Boosted and Ensemble
  Learning Framework
Authors: Saddam Hussain Khan (1), Tahani Jaser Alahmadi (2) ((1) Department of
  Computer Systems Engineering, University of Engineering and Applied Science,
  Swat, Pakistan, (2) Department of Information Systems, Princess Nourah bint
  Abdulrahman University, Riyadh, Saudi Arabia)
Categories: eess.IV cs.CV cs.LG
Comments: 26 pages, 10 figures, 9 Tables
DOI: 10.1088/1674-1137/acb7ce
\\ ( https://arxiv.org/abs/2212.02477 ,  1457kb)
------------------------------------------------------------------------------
\\
arXiv:2303.06519 (*cross-listing*)
replaced with revised version Wed, 20 Mar 2024 10:00:15 GMT   (5472kb,D)

Title: Lossless Point Cloud Geometry and Attribute Compression Using a Learned
  Conditional Probability Model
Authors: Dat Thanh Nguyen, Andre Kaup
Categories: eess.IV cs.LG
Comments: 12 pages, accepted to IEEE Transactions on Circuits and Systems for
  Video Technology
Journal-ref: EEE Transactions on Circuits and Systems for Video Technology,
  vol. 33, no. 8, pp. 4337-4348, Aug. 2023
DOI: 10.1109/TCSVT.2023.3239321
\\ ( https://arxiv.org/abs/2303.06519 ,  5472kb)
------------------------------------------------------------------------------
\\
arXiv:2303.17992 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 22:10:18 GMT   (881kb,D)

Title: A fast Multiplicative Updates algorithm for Non-negative Matrix
  Factorization
Authors: Mai-Quyen Pham, J\'er\'emy Cohen, and Thierry Chonavel
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2303.17992 ,  881kb)
------------------------------------------------------------------------------
\\
arXiv:2304.02472 (*cross-listing*)
replaced with revised version Wed, 20 Mar 2024 11:33:35 GMT   (2461kb,D)

Title: Learning to Predict Short-Term Volatility with Order Flow Image
  Representation
Authors: Artem Lensky, Mingyu Hao
Categories: q-fin.RM cs.LG q-fin.TR
\\ ( https://arxiv.org/abs/2304.02472 ,  2461kb)
------------------------------------------------------------------------------
\\
arXiv:2305.09820
replaced with revised version Wed, 20 Mar 2024 03:58:34 GMT   (8871kb,D)

Title: Machine-Made Media: Monitoring the Mobilization of Machine-Generated
  Articles on Misinformation and Mainstream News Websites
Authors: Hans W. A. Hanley, Zakir Durumeric
Categories: cs.CY cs.LG cs.SI
Comments: Accepted to ICWSM 2024
\\ ( https://arxiv.org/abs/2305.09820 ,  8871kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12621 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 19:42:14 GMT   (34251kb,D)

Title: DermSynth3D: Synthesis of in-the-wild Annotated Dermatology Images
Authors: Ashish Sinha, Jeremy Kawahara, Arezou Pakzad, Kumar Abhishek, Matthieu
  Ruthven, Enjie Ghorbel, Anis Kacem, Djamila Aouada, Ghassan Hamarneh
Categories: eess.IV cs.CV cs.LG
Comments: Accepted to Medical Image Analysis (MedIA) 2024
\\ ( https://arxiv.org/abs/2305.12621 ,  34251kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07975
replaced with revised version Wed, 20 Mar 2024 12:30:01 GMT   (9581kb,D)

Title: Pseudo-rigid body networks: learning interpretable deformable object
  dynamics from partial observations
Authors: Shamil Mamedov, A. Ren\'e Geist, Jan Swevers, Sebastian Trimpe
Categories: cs.RO cs.LG
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\ ( https://arxiv.org/abs/2307.07975 ,  9581kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00736 (*cross-listing*)
replaced with revised version Wed, 20 Mar 2024 03:48:36 GMT   (2329kb)

Title: Prediction Error Estimation in Random Forests
Authors: Ian Krupkin and Johanna Hardin
Categories: stat.ML cs.LG
Comments: arXiv admin note: text overlap with arXiv:2104.00673 by other authors
\\ ( https://arxiv.org/abs/2309.00736 ,  2329kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18430 (*cross-listing*)
replaced with revised version Wed, 20 Mar 2024 16:32:42 GMT   (1151kb,D)

Title: MCRAGE: Synthetic Healthcare Data for Fairness
Authors: Keira Behal, Jiayi Chen, Caleb Fikes, and Sophia Xiao
Categories: stat.ML cs.LG
Comments: Keywords: synthetic electronic health records, conditional denoising
  diffusion probabilistic model, healthcare AI, tabular data, fairness,
  synthetic data. This paper is the result of work completed at the 2023 Emory
  University Department of Mathematics REU/RET program under the direction of
  Project Advisor Dr. Xi Yuanzhe. This work is sponsored by NSF DMS 2051019
\\ ( https://arxiv.org/abs/2310.18430 ,  1151kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10026
replaced with revised version Wed, 20 Mar 2024 15:30:31 GMT   (6180kb,D)

Title: Guaranteeing Control Requirements via Reward Shaping in Reinforcement
  Learning
Authors: Francesco De Lellis, Marco Coraggio, Giovanni Russo, Mirco Musolesi,
  Mario di Bernardo
Categories: eess.SY cs.LG cs.SY
\\ ( https://arxiv.org/abs/2311.10026 ,  6180kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13261 (*cross-listing*)
replaced with revised version Wed, 20 Mar 2024 10:06:09 GMT   (1521kb)

Title: Immunohistochemistry guided segmentation of benign epithelial cells, in
  situ lesions, and invasive epithelial cells in breast cancer slides
Authors: Maren H{\o}ib{\o}, Andr\'e Pedersen, Vibeke Grotnes Dale, Sissel Marie
  Berget, Borgny Ytterhus, Cecilia Lindskog, Elisabeth Wik, Lars A. Akslen,
  Ingerid Reinertsen, Erik Smistad, Marit Valla
Categories: eess.IV cs.CV cs.LG
Comments: 19 pages, 6 figures. Submitted to a scientific journal
MSC-class: I.4.6
ACM-class: I.4.6; I.4.9; I.5.4; J.3
\\ ( https://arxiv.org/abs/2311.13261 ,  1521kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17885 (*cross-listing*)
replaced with revised version Wed, 20 Mar 2024 13:36:23 GMT   (1395kb,D)

Title: Are Ensembles Getting Better all the Time?
Authors: Pierre-Alexandre Mattei, Damien Garreau
Categories: stat.ML cs.LG math.ST stat.ME stat.TH
MSC-class: 62-08 (Primary) 60F10 (Secondary)
ACM-class: G.3
\\ ( https://arxiv.org/abs/2311.17885 ,  1395kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06071
replaced with revised version Wed, 20 Mar 2024 00:12:22 GMT   (9417kb,D)

Title: Precipitation Downscaling with Spatiotemporal Video Diffusion
Authors: Prakhar Srivastava, Ruihan Yang, Gavin Kerrigan, Gideon Dresdner,
  Jeremy McGibbon, Christopher Bretherton, Stephan Mandt
Categories: cs.CV cs.LG physics.ao-ph stat.ML
\\ ( https://arxiv.org/abs/2312.06071 ,  9417kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09121 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 21:12:37 GMT   (581kb,D)

Title: Does provable absence of barren plateaus imply classical simulability?
  Or, why we need to rethink variational quantum computing
Authors: M. Cerezo, Martin Larocca, Diego Garc\'ia-Mart\'in, N. L. Diaz, Paolo
  Braccia, Enrico Fontana, Manuel S. Rudolph, Pablo Bermejo, Aroosa Ijaz,
  Supanut Thanasilp, Eric R. Anschuetz, Zo\"e Holmes
Categories: quant-ph cs.LG stat.ML
Comments: 14+15 pages, 5 figures, 2 tables, minor corrections added
Report-no: LA-UR-23-33705
\\ ( https://arxiv.org/abs/2312.09121 ,  581kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03653
replaced with revised version Wed, 20 Mar 2024 08:06:33 GMT   (3883kb,D)

Title: An Exploratory Study on Automatic Identification of Assumptions in the
  Development of Deep Learning Frameworks
Authors: Chen Yang, Peng Liang, Zinan Ma
Categories: cs.SE cs.LG
Comments: 28 pages, 15 images, 10 tables, Manuscript submitted to a Journal
  (2024)
\\ ( https://arxiv.org/abs/2401.03653 ,  3883kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05394 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 21:04:28 GMT   (2848kb,D)

Title: Iterative Regularization with k-support Norm: An Important Complement to
  Sparse Recovery
Authors: William de Vazelhes, Bhaskar Mukhoty, Xiao-Tong Yuan, Bin Gu
Categories: eess.SP cs.LG math.OC stat.ML
Comments: Accepted at AAAI 2024. Code at
  https://github.com/wdevazelhes/IRKSN_AAAI2024
\\ ( https://arxiv.org/abs/2401.05394 ,  2848kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10298 (*cross-listing*)
replaced with revised version Wed, 20 Mar 2024 09:11:26 GMT   (2429kb,D)

Title: Machine learning approach to detect dynamical states from recurrence
  measures
Authors: Dheeraja Thakur, Athul Mohan, G. Ambika, Chandrakala Meena
Categories: physics.data-an cs.LG
\\ ( https://arxiv.org/abs/2401.10298 ,  2429kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00776 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 21:32:26 GMT   (2402kb,D)

Title: Hybrid Quantum Vision Transformers for Event Classification in High
  Energy Physics
Authors: Eyup B. Unlu, Mar\c{c}al Comajoan Cara, Gopal Ramesh Dahale, Zhongtian
  Dong, Roy T. Forestano, Sergei Gleyzer, Daniel Justice, Kyoungchul Kong, Tom
  Magorsch, Konstantin T. Matchev, Katia Matcheva
Categories: quant-ph cs.LG hep-ph stat.ML
Comments: 13 pages, 9 figures. Published version in a special issue
  "Computational Aspects of Machine Learning and Quantum Computing"
MSC-class: 68Q12, 81P68
Journal-ref: Axioms v. 13, no 3, (2024) 187
DOI: 10.3390/axioms13030187
\\ ( https://arxiv.org/abs/2402.00776 ,  2402kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03008 (*cross-listing*)
replaced with revised version Wed, 20 Mar 2024 12:05:28 GMT   (2706kb,D)

Title: Diffusive Gibbs Sampling
Authors: Wenlin Chen, Mingtian Zhang, Brooks Paige, Jos\'e Miguel
  Hern\'andez-Lobato, David Barber
Categories: stat.ML cs.LG stat.CO
Comments: 15 pages, 11 figures, 4 tables, 1 algorithm. Code available:
  https://github.com/Wenlin-Chen/DiGS
\\ ( https://arxiv.org/abs/2402.03008 ,  2706kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16908
replaced with revised version Wed, 20 Mar 2024 07:05:55 GMT   (3057kb)

Title: Lightweight, error-tolerant edge detection using memristor-enabled
  stochastic logics
Authors: Lekai Song, Pengyu Liu, Jingfang Pei, Yang Liu, Songwei Liu, Shengbo
  Wang, Leonard W. T. Ng, Tawfique Hasan, Kong-Pang Pun, Shuo Gao, Guohua Hu
Categories: cs.ET cond-mat.mtrl-sci cs.LG eess.IV
\\ ( https://arxiv.org/abs/2402.16908 ,  3057kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18112 (*cross-listing*)
replaced with revised version Wed, 20 Mar 2024 10:08:28 GMT   (1207kb,D)

Title: Simple But Effective: Rethinking the Ability of Deep Learning in fNIRS
  to Exclude Abnormal Input
Authors: Zhihao Cao
Categories: eess.SP cs.LG
DOI: 10.36227/techrxiv.170906859.92444731/v1
\\ ( https://arxiv.org/abs/2402.18112 ,  1207kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19212 (*cross-listing*)
replaced with revised version Wed, 20 Mar 2024 16:34:27 GMT   (36kb)

Title: Deep Reinforcement Learning: A Convex Optimization Approach
Authors: Ather Gattami
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2402.19212 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02626
replaced with revised version Wed, 20 Mar 2024 03:56:57 GMT   (1423kb,D)

Title: Modeling Collaborator: Enabling Subjective Vision Classification With
  Minimal Human Effort via LLM Tool-Use
Authors: Imad Eddine Toubal, Aditya Avinash, Neil Gordon Alldrin, Jan Dlabal,
  Wenlei Zhou, Enming Luo, Otilia Stretcu, Hao Xiong, Chun-Ta Lu, Howard Zhou,
  Ranjay Krishna, Ariel Fuxman, Tom Duerig
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2403.02626 ,  1423kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11757
replaced with revised version Tue, 19 Mar 2024 18:14:35 GMT   (190kb,D)

Title: Efficient Feature Extraction and Late Fusion Strategy for Audiovisual
  Emotional Mimicry Intensity Estimation
Authors: Jun Yu, Wangyuan Zhu, Jichao Zhu
Categories: cs.MM cs.LG cs.SD eess.AS
\\ ( https://arxiv.org/abs/2403.11757 ,  190kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12267
replaced with revised version Wed, 20 Mar 2024 01:46:13 GMT   (391kb,D)

Title: Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data
  Quality over Quantity
Authors: Siddharth Joshi, Arnav Jain, Ali Payani and Baharan Mirzasoleiman
Categories: cs.CV cs.LG
Comments: AISTATS 2024, Code:
  https://github.com/BigML-CS-UCLA/clipcov-data-efficient-clip
\\ ( https://arxiv.org/abs/2403.12267 ,  391kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12384
replaced with revised version Wed, 20 Mar 2024 02:50:36 GMT   (4174kb,D)

Title: An Aligning and Training Framework for Multimodal Recommendations
Authors: Yifan Liu, Kangning Zhang, Xiangyuan Ren, Yanhua Huang, Jiarui Jin,
  Yingjie Qin, Ruilong Su, Ruiwen Xu, Weinan Zhang
Categories: cs.IR cs.LG
Comments: 11 pages, add some necessary explanations, revise typos
\\ ( https://arxiv.org/abs/2403.12384 ,  4174kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
