Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月29日 12:49
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Wed 27 Mar 24 18:00:00 GMT  to  Thu 28 Mar 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.18827
Date: Thu, 25 Jan 2024 05:48:50 GMT   (2718kb,D)

Title: Bridging Generative Networks with the Common Model of Cognition
Authors: Robert L. West, Spencer Eckler, Brendan Conway-Smith, Nico Turcas,
  Eilene Tomkins-Flanagan, Mary Alexandria Kelly
Categories: cs.AI cs.LG cs.NE q-bio.NC
\\
  This article presents a theoretical framework for adapting the Common Model
of Cognition to large generative network models within the field of artificial
intelligence. This can be accomplished by restructuring modules within the
Common Model into shadow production systems that are peripheral to a central
production system, which handles higher-level reasoning based on the shadow
productions' output. Implementing this novel structure within the Common Model
allows for a seamless connection between cognitive architectures and generative
neural networks.
\\ ( https://arxiv.org/abs/2403.18827 ,  2718kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18932
Date: Wed, 27 Mar 2024 18:22:48 GMT   (6999kb,D)

Title: Measuring Political Bias in Large Language Models: What Is Said and How
  It Is Said
Authors: Yejin Bang, Delong Chen, Nayeon Lee, Pascale Fung
Categories: cs.CL cs.AI
Comments: 16 pages
\\
  We propose to measure political bias in LLMs by analyzing both the content
and style of their generated content regarding political issues. Existing
benchmarks and measures focus on gender and racial biases. However, political
bias exists in LLMs and can lead to polarization and other harms in downstream
applications. In order to provide transparency to users, we advocate that there
should be fine-grained and explainable measures of political biases generated
by LLMs. Our proposed measure looks at different political issues such as
reproductive rights and climate change, at both the content (the substance of
the generation) and the style (the lexical polarity) of such bias. We measured
the political bias in eleven open-sourced LLMs and showed that our proposed
framework is easily scalable to other topics and is explainable.
\\ ( https://arxiv.org/abs/2403.18932 ,  6999kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18933
Date: Wed, 27 Mar 2024 18:30:26 GMT   (5257kb,D)

Title: SemEval Task 1: Semantic Textual Relatedness for African and Asian
  Languages
Authors: Nedjma Ousidhoum, Shamsuddeen Hassan Muhammad, Mohamed Abdalla, Idris
  Abdulmumin, Ibrahim Said Ahmad, Sanchit Ahuja, Alham Fikri Aji, Vladimir
  Araujo, Meriem Beloucif, Christine De Kock, Oumaima Hourrane, Manish
  Shrivastava, Thamar Solorio, Nirmal Surange, Krishnapriya Vishnubhotla, Seid
  Muhie Yimam, Saif M. Mohammad
Categories: cs.CL
Comments: SemEval 2024 Task Description Paper. arXiv admin note: text overlap
  with arXiv:2402.08638
\\
  We present the first shared task on Semantic Textual Relatedness (STR). While
earlier shared tasks primarily focused on semantic similarity, we instead
investigate the broader phenomenon of semantic relatedness across 14 languages:
Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian,
Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi,
Spanish, and Telugu. These languages originate from five distinct language
families and are predominantly spoken in Africa and Asia -- regions
characterised by the relatively limited availability of NLP resources. Each
instance in the datasets is a sentence pair associated with a score that
represents the degree of semantic textual relatedness between the two
sentences. Participating systems were asked to rank sentence pairs by their
closeness in meaning (i.e., their degree of semantic relatedness) in the 14
languages in three main tracks: (a) supervised, (b) unsupervised, and (c)
crosslingual. The task attracted 163 participants. We received 70 submissions
in total (across all tasks) from 51 different teams, and 38 system description
papers. We report on the best-performing systems as well as the most common and
the most effective approaches for the three different tracks.
\\ ( https://arxiv.org/abs/2403.18933 ,  5257kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18938
Date: Wed, 27 Mar 2024 18:38:39 GMT   (1242kb)

Title: Reshaping Free-Text Radiology Notes Into Structured Reports With
  Generative Transformers
Authors: Laura Bergomi, Tommaso M. Buonocore, Paolo Antonazzo, Lorenzo
  Alberghi, Riccardo Bellazzi, Lorenzo Preda, Chandra Bortolotto, Enea
  Parimbelli
Categories: cs.CL cs.AI
ACM-class: I.2.7; J.3
\\
  BACKGROUND: Radiology reports are typically written in a free-text format,
making clinical information difficult to extract and use. Recently the adoption
of structured reporting (SR) has been recommended by various medical societies
thanks to the advantages it offers, e.g. standardization, completeness and
information retrieval. We propose a pipeline to extract information from
free-text radiology reports, that fits with the items of the reference SR
registry proposed by a national society of interventional and medical
radiology, focusing on CT staging of patients with lymphoma. METHODS: Our work
aims to leverage the potential of Natural Language Processing (NLP) and
Transformer-based models to deal with automatic SR registry filling. With the
availability of 174 radiology reports, we investigate a rule-free generative
Question Answering approach based on a domain-specific version of T5 (IT5). Two
strategies (batch-truncation and ex-post combination) are implemented to comply
with the model's context length limitations. Performance is evaluated in terms
of strict accuracy, F1, and format accuracy, and compared with the widely used
GPT-3.5 Large Language Model. A 5-point Likert scale questionnaire is used to
collect human-expert feedback on the similarity between medical annotations and
generated answers. RESULTS: The combination of fine-tuning and batch splitting
allows IT5 to achieve notable results; it performs on par with GPT-3.5 albeit
its size being a thousand times smaller in terms of parameters. Human-based
assessment scores show a high correlation (Spearman's correlation
coefficients>0.88, p-values<0.001) with AI performance metrics (F1) and confirm
the superior ability of LLMs (i.e., GPT-3.5, 175B of parameters) in generating
plausible human-like statements.
\\ ( https://arxiv.org/abs/2403.18938 ,  1242kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18969
Date: Wed, 27 Mar 2024 19:35:41 GMT   (1420kb,D)

Title: A Survey on Large Language Models from Concept to Implementation
Authors: Chen Wang, Jin Zhao, Jiaqi Gong
Categories: cs.CL cs.AI cs.IT cs.LG math.IT
Comments: Preprint in ArXiv template, total 24 pages, 5 figures, 5 tables
\\
  Recent advancements in Large Language Models (LLMs), particularly those built
on Transformer architectures, have significantly broadened the scope of natural
language processing (NLP) applications, transcending their initial use in
chatbot technology. This paper investigates the multifaceted applications of
these models, with an emphasis on the GPT series. This exploration focuses on
the transformative impact of artificial intelligence (AI) driven tools in
revolutionizing traditional tasks like coding and problem-solving, while also
paving new paths in research and development across diverse industries. From
code interpretation and image captioning to facilitating the construction of
interactive systems and advancing computational domains, Transformer models
exemplify a synergy of deep learning, data analysis, and neural network design.
This survey provides an in-depth look at the latest research in Transformer
models, highlighting their versatility and the potential they hold for
transforming diverse application sectors, thereby offering readers a
comprehensive understanding of the current and future landscape of
Transformer-based LLMs in practical applications.
\\ ( https://arxiv.org/abs/2403.18969 ,  1420kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18973
Date: Wed, 27 Mar 2024 19:42:01 GMT   (9932kb,D)

Title: Conformal Intent Classification and Clarification for Fast and Accurate
  Intent Recognition
Authors: Floris den Hengst, Ralf Wolter, Patrick Altmeyer and Arda Kaygan
Categories: cs.CL
Comments: 9 pages,2 figures,3 tables,6 appendices,to be published in ACL's
  NAACL Findings 2024
\\
  We present Conformal Intent Classification and Clarification (CICC), a
framework for fast and accurate intent classification for task-oriented
dialogue systems. The framework turns heuristic uncertainty scores of any
intent classifier into a clarification question that is guaranteed to contain
the true intent at a pre-defined confidence level. By disambiguating between a
small number of likely intents, the user query can be resolved quickly and
accurately. Additionally, we propose to augment the framework for out-of-scope
detection. In a comparative evaluation using seven intent recognition datasets
we find that CICC generates small clarification questions and is capable of
out-of-scope detection. CICC can help practitioners and researchers
substantially in improving the user experience of dialogue agents with specific
clarification questions.
\\ ( https://arxiv.org/abs/2403.18973 ,  9932kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18975
Date: Wed, 27 Mar 2024 19:43:45 GMT   (2754kb,D)

Title: A Novel Corpus of Annotated Medical Imaging Reports and Information
  Extraction Results Using BERT-based Language Models
Authors: Namu Park, Kevin Lybarger, Giridhar Kaushik Ramachandran, Spencer
  Lewis, Aashka Damani, Ozlem Uzuner, Martin Gunn, Meliha Yetisgen
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  Medical imaging is critical to the diagnosis, surveillance, and treatment of
many health conditions, including oncological, neurological, cardiovascular,
and musculoskeletal disorders, among others. Radiologists interpret these
complex, unstructured images and articulate their assessments through narrative
reports that remain largely unstructured. This unstructured narrative must be
converted into a structured semantic representation to facilitate secondary
applications such as retrospective analyses or clinical decision support. Here,
we introduce the Corpus of Annotated Medical Imaging Reports (CAMIR), which
includes 609 annotated radiology reports from three imaging modality types:
Computed Tomography, Magnetic Resonance Imaging, and Positron Emission
Tomography-Computed Tomography. Reports were annotated using an event-based
schema that captures clinical indications, lesions, and medical problems. Each
event consists of a trigger and multiple arguments, and a majority of the
argument types, including anatomy, normalize the spans to pre-defined concepts
to facilitate secondary use. CAMIR uniquely combines a granular event structure
and concept normalization. To extract CAMIR events, we explored two BERT
(Bi-directional Encoder Representation from Transformers)-based architectures,
including an existing architecture (mSpERT) that jointly extracts all event
information and a multi-step approach (PL-Marker++) that we augmented for the
CAMIR schema.
\\ ( https://arxiv.org/abs/2403.18975 ,  2754kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18976
Date: Wed, 27 Mar 2024 19:45:09 GMT   (4163kb,D)

Title: "Sorry, Come Again?" Prompting -- Enhancing Comprehension and
  Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing
Authors: Vipula Rawte, S.M Towhidul Islam Tonmoy, S M Mehedi Zaman, Prachi
  Priya, Aman Chadha, Amit P. Sheth, Amitava Das
Categories: cs.CL cs.AI
\\
  Hallucination has emerged as the most vulnerable aspect of contemporary Large
Language Models (LLMs). In this paper, we introduce the Sorry, Come Again (SCA)
prompting, aimed to avoid LLM hallucinations by enhancing comprehension
through: (i) optimal paraphrasing and (ii) injecting [PAUSE] tokens to delay
LLM generation. First, we provide an in-depth analysis of linguistic nuances:
formality, readability, and concreteness of prompts for 21 LLMs, and elucidate
how these nuances contribute to hallucinated generation. Prompts with lower
readability, formality, or concreteness pose comprehension challenges for LLMs,
similar to those faced by humans. In such scenarios, an LLM tends to speculate
and generate content based on its imagination (associative memory) to fill
these information gaps. Although these speculations may occasionally align with
factual information, their accuracy is not assured, often resulting in
hallucination. Recent studies reveal that an LLM often neglects the middle
sections of extended prompts, a phenomenon termed as lost in the middle. While
a specific paraphrase may suit one LLM, the same paraphrased version may elicit
a different response from another LLM. Therefore, we propose an optimal
paraphrasing technique to identify the most comprehensible paraphrase of a
given prompt, evaluated using Integrated Gradient (and its variations) to
guarantee that the LLM accurately processes all words. While reading lengthy
sentences, humans often pause at various points to better comprehend the
meaning read thus far. We have fine-tuned an LLM with injected [PAUSE] tokens,
allowing the LLM to pause while reading lengthier prompts. This has brought
several key contributions: (i) determining the optimal position to inject
[PAUSE], (ii) determining the number of [PAUSE] tokens to be inserted, and
(iii) introducing reverse proxy tuning to fine-tune the LLM for [PAUSE]
insertion.
\\ ( https://arxiv.org/abs/2403.18976 ,  4163kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19012
Date: Wed, 27 Mar 2024 21:10:07 GMT   (1909kb,D)

Title: ReflectSumm: A Benchmark for Course Reflection Summarization
Authors: Yang Zhong, Mohamed Elaraby, Diane Litman, Ahmed Ashraf Butt, Muhsin
  Menekse
Categories: cs.CL cs.AI
Comments: LREC-COLING 2024 camera ready; code and dataset are available at
  https://github.com/EngSalem/ReflectSUMM
\\
  This paper introduces ReflectSumm, a novel summarization dataset specifically
designed for summarizing students' reflective writing. The goal of ReflectSumm
is to facilitate developing and evaluating novel summarization techniques
tailored to real-world scenarios with little training data, %practical tasks
with potential implications in the opinion summarization domain in general and
the educational domain in particular. The dataset encompasses a diverse range
of summarization tasks and includes comprehensive metadata, enabling the
exploration of various research questions and supporting different
applications. To showcase its utility, we conducted extensive evaluations using
multiple state-of-the-art baselines. The results provide benchmarks for
facilitating further research in this area.
\\ ( https://arxiv.org/abs/2403.19012 ,  1909kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19031
Date: Wed, 27 Mar 2024 22:05:10 GMT   (1233kb)

Title: Evaluating Large Language Models for Health-Related Text Classification
  Tasks with Public Social Media Data
Authors: Yuting Guo, Anthony Ovadje, Mohammed Ali Al-Garadi, Abeed Sarker
Categories: cs.CL cs.AI cs.LG
\\
  Large language models (LLMs) have demonstrated remarkable success in NLP
tasks. However, there is a paucity of studies that attempt to evaluate their
performances on social media-based health-related natural language processing
tasks, which have traditionally been difficult to achieve high scores in. We
benchmarked one supervised classic machine learning model based on Support
Vector Machines (SVMs), three supervised pretrained language models (PLMs)
based on RoBERTa, BERTweet, and SocBERT, and two LLM based classifiers (GPT3.5
and GPT4), across 6 text classification tasks. We developed three approaches
for leveraging LLMs for text classification: employing LLMs as zero-shot
classifiers, us-ing LLMs as annotators to annotate training data for supervised
classifiers, and utilizing LLMs with few-shot examples for augmentation of
manually annotated data. Our comprehensive experiments demonstrate that
employ-ing data augmentation using LLMs (GPT-4) with relatively small
human-annotated data to train lightweight supervised classification models
achieves superior results compared to training with human-annotated data alone.
Supervised learners also outperform GPT-4 and GPT-3.5 in zero-shot settings. By
leveraging this data augmentation strategy, we can harness the power of LLMs to
develop smaller, more effective domain-specific NLP models. LLM-annotated data
without human guidance for training light-weight supervised classification
models is an ineffective strategy. However, LLM, as a zero-shot classifier,
shows promise in excluding false negatives and potentially reducing the human
effort required for data annotation. Future investigations are imperative to
explore optimal training data sizes and the optimal amounts of augmented data.
\\ ( https://arxiv.org/abs/2403.19031 ,  1233kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19056
Date: Wed, 27 Mar 2024 23:45:31 GMT   (235kb,D)

Title: CAUSE: Counterfactual Assessment of User Satisfaction Estimation in
  Task-Oriented Dialogue Systems
Authors: Amin Abolghasemi, Zhaochun Ren, Arian Askari, Mohammad Aliannejadi,
  Maarten de Rijke, Suzan Verberne
Categories: cs.CL
\\
  An important unexplored aspect in previous work on user satisfaction
estimation for Task-Oriented Dialogue (TOD) systems is their evaluation in
terms of robustness for the identification of user dissatisfaction: current
benchmarks for user satisfaction estimation in TOD systems are highly skewed
towards dialogues for which the user is satisfied. The effect of having a more
balanced set of satisfaction labels on performance is unknown. However,
balancing the data with more dissatisfactory dialogue samples requires further
data collection and human annotation, which is costly and time-consuming. In
this work, we leverage large language models (LLMs) and unlock their ability to
generate satisfaction-aware counterfactual dialogues to augment the set of
original dialogues of a test collection. We gather human annotations to ensure
the reliability of the generated samples. We evaluate two open-source LLMs as
user satisfaction estimators on our augmented collection against
state-of-the-art fine-tuned models. Our experiments show that when used as
few-shot user satisfaction estimators, open-source LLMs show higher robustness
to the increase in the number of dissatisfaction labels in the test collection
than the fine-tuned state-of-the-art models. Our results shed light on the need
for data augmentation approaches for user satisfaction estimation in TOD
systems. We release our aligned counterfactual dialogues, which are curated by
human annotation, to facilitate further research on this topic.
\\ ( https://arxiv.org/abs/2403.19056 ,  235kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19094
Date: Thu, 28 Mar 2024 02:12:49 GMT   (2557kb,D)

Title: Learning From Correctness Without Prompting Makes LLM Efficient Reasoner
Authors: Yuxuan Yao, Han Wu, Zhijiang Guo, Biyan Zhou, Jiahui Gao, Sichun Luo,
  Hanxu Hou, Xiaojin Fu, Linqi Song
Categories: cs.CL
\\
  Large language models (LLMs) have demonstrated outstanding performance across
various tasks, yet they still exhibit limitations such as hallucination,
unfaithful reasoning, and toxic content. One potential approach to mitigate
these issues is learning from human or external feedback (e.g. tools). In this
paper, we introduce an intrinsic self-correct reasoning framework for LLMs that
eliminates the need for human feedback, external tools, and handcraft prompts.
The proposed framework, based on a multi-step reasoning paradigm
\textbf{Le}arning from \textbf{Co}rrectness (\textsc{LeCo}), improves reasoning
performance without needing to learn from errors. This paradigm prioritizes
learning from correct reasoning steps, and a unique method to measure
confidence for each reasoning step based on generation logits. Experimental
results across various multi-step reasoning tasks demonstrate the effectiveness
of the framework in improving reasoning performance with reduced token
consumption.
\\ ( https://arxiv.org/abs/2403.19094 ,  2557kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19113
Date: Thu, 28 Mar 2024 03:09:42 GMT   (3303kb,D)

Title: FACTOID: FACtual enTailment fOr hallucInation Detection
Authors: Vipula Rawte, S.M Towhidul Islam Tonmoy, Krishnav Rajbangshi, Shravani
  Nag, Aman Chadha, Amit P. Sheth, Amitava Das
Categories: cs.CL cs.AI
\\
  The widespread adoption of Large Language Models (LLMs) has facilitated
numerous benefits. However, hallucination is a significant concern. In
response, Retrieval Augmented Generation (RAG) has emerged as a highly
promising paradigm to improve LLM outputs by grounding them in factual
information. RAG relies on textual entailment (TE) or similar methods to check
if the text produced by LLMs is supported or contradicted, compared to
retrieved documents. This paper argues that conventional TE methods are
inadequate for spotting hallucinations in content generated by LLMs. For
instance, consider a prompt about the 'USA's stance on the Ukraine war''. The
AI-generated text states, ...U.S. President Barack Obama says the U.S. will not
put troops in Ukraine...'' However, during the war the U.S. president is Joe
Biden which contradicts factual reality. Moreover, current TE systems are
unable to accurately annotate the given text and identify the exact portion
that is contradicted. To address this, we introduces a new type of TE called
``Factual Entailment (FE).'', aims to detect factual inaccuracies in content
generated by LLMs while also highlighting the specific text segment that
contradicts reality. We present FACTOID (FACTual enTAILment for hallucInation
Detection), a benchmark dataset for FE. We propose a multi-task learning (MTL)
framework for FE, incorporating state-of-the-art (SoTA) long text embeddings
such as e5-mistral-7b-instruct, along with GPT-3, SpanBERT, and RoFormer. The
proposed MTL architecture for FE achieves an avg. 40\% improvement in accuracy
on the FACTOID benchmark compared to SoTA TE methods. As FE automatically
detects hallucinations, we assessed 15 modern LLMs and ranked them using our
proposed Auto Hallucination Vulnerability Index (HVI_auto). This index
quantifies and offers a comparative scale to evaluate and rank LLMs according
to their hallucinations.
\\ ( https://arxiv.org/abs/2403.19113 ,  3303kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19116
Date: Thu, 28 Mar 2024 03:14:18 GMT   (913kb)

Title: MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering
Authors: Che Guan, Mengyu Huang and Peng Zhang
Categories: cs.CL cs.AI
Comments: 8 pages
\\
  In today's fast-paced industry, professionals face the challenge of
summarizing a large number of documents and extracting vital information from
them on a daily basis. These metrics are frequently hidden away in tables
and/or their nested hyperlinks. To address this challenge, the approach of
Table Question Answering (QA) has been developed to extract the relevant
information. However, traditional Table QA training tasks that provide a table
and an answer(s) from a gold cell coordinate(s) for a question may not always
ensure extracting the accurate answer(s). Recent advancements in Large Language
Models (LLMs) have opened up new possibilities for extracting information from
tabular data using prompts. In this paper, we introduce the Multi-hop Few-shot
Open Rich Table QA (MFORT-QA) approach, which consists of two major steps. The
first step involves Few-Shot Learning (FSL), where relevant tables and
associated contexts of hyperlinks are retrieved based on a given question. The
retrieved content is then used to construct few-shot prompts as inputs to an
LLM, such as ChatGPT. To tackle the challenge of answering complex questions,
the second step leverages Chain-of-thought (CoT) prompting to decompose the
complex question into a sequential chain of questions and reasoning thoughts in
a multi-hop manner. Retrieval-Augmented Generation (RAG) enhances this process
by retrieving relevant tables and contexts of hyperlinks that are relevant to
the resulting reasoning thoughts and questions. These additional contexts are
then used to supplement the prompt used in the first step, resulting in more
accurate answers from an LLM. Empirical results from OTT-QA demonstrate that
our abstractive QA approach significantly improves the accuracy of extractive
Table QA methods.
\\ ( https://arxiv.org/abs/2403.19116 ,  913kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19121
Date: Thu, 28 Mar 2024 03:25:23 GMT   (9495kb,D)

Title: Code Comparison Tuning for Code Large Language Models
Authors: Yufan Jiang, Qiaozhi He, Xiaomin Zhuang, Zhihua Wu
Categories: cs.CL
Comments: Preprint
\\
  We present Code Comparison Tuning (CCT), a simple and effective tuning method
for code large language models (Code LLMs) to better handle subtle code errors.
Specifically, we integrate the concept of comparison into instruction tuning,
both at the token and sequence levels, enabling the model to discern even the
slightest deviations in code. To compare the original code with an erroneous
version containing manually added code errors, we use token-level preference
loss for detailed token-level comparisons. Additionally, we combine code
segments to create a new instruction tuning sample for sequence-level
comparisons, enhancing the model's bug-fixing capability. Experimental results
on the HumanEvalFix benchmark show that CCT surpasses instruction tuning in
pass@1 scores by up to 4 points across diverse code LLMs, and extensive
analysis demonstrates the effectiveness of our method.
\\ ( https://arxiv.org/abs/2403.19121 ,  9495kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19135
Date: Thu, 28 Mar 2024 04:12:13 GMT   (217kb,D)

Title: Compressing Large Language Models by Streamlining the Unimportant Layer
Authors: Xiaodong Chen, Yuxuan Hu, Jing Zhang
Categories: cs.CL cs.AI
ACM-class: I.2.7
\\
  Large language models (LLM) have been extensively applied in various natural
language tasks and domains, but their applicability is constrained by the large
number of parameters of the models. Consequently, there is an increasing
emphasis on compact models that exhibit high performance. In this study, we
observe that different layers in LLM have varying degrees of perturbation on
the hidden states, which allows us to identify less important layers. Based on
this phenomenon, we propose LLM-Streamline, which consists of two parts: layer
pruning, where we remove a set of consecutive layers with the lowest importance
in the model according to the target sparsity; and layer replacement, where we
train a lightweight model to substitute the pruned layers, thereby mitigating
the performance degradation caused by pruning. In our experiments, we utilize
structures such as a multi-layer perceptron (MLP) and a transformer layer as
lightweight models and ultimately demonstrate that a single MLP can effectively
fit the pruned layers. Comprehensive experiments show that our proposed method,
LLM-Streamline, outperforms previous state-of-the-art (SOTA) model pruning
methods.
\\ ( https://arxiv.org/abs/2403.19135 ,  217kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19142
Date: Thu, 28 Mar 2024 04:30:07 GMT   (4149kb,D)

Title: A Tulu Resource for Machine Translation
Authors: Manu Narayanan, No\"emi Aepli
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  We present the first parallel dataset for English-Tulu translation. Tulu,
classified within the South Dravidian linguistic family branch, is
predominantly spoken by approximately 2.5 million individuals in southwestern
India. Our dataset is constructed by integrating human translations into the
multilingual machine translation resource FLORES-200. Furthermore, we use this
dataset for evaluation purposes in developing our English-Tulu machine
translation model. For the model's training, we leverage resources available
for related South Dravidian languages. We adopt a transfer learning approach
that exploits similarities between high-resource and low-resource languages.
This method enables the training of a machine translation system even in the
absence of parallel data between the source and target language, thereby
overcoming a significant obstacle in machine translation development for
low-resource languages. Our English-Tulu system, trained without using parallel
English-Tulu data, outperforms Google Translate by 19 BLEU points (in September
2023). The dataset and code are available here:
https://github.com/manunarayanan/Tulu-NMT.
\\ ( https://arxiv.org/abs/2403.19142 ,  4149kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19154
Date: Thu, 28 Mar 2024 05:35:22 GMT   (717kb,D)

Title: STaR-GATE: Teaching Language Models to Ask Clarifying Questions
Authors: Chinmaya Andukuri, Jan-Philipp Fr\"anken, Tobias Gerstenberg, Noah D.
  Goodman
Categories: cs.CL cs.AI
\\
  When prompting language models to complete a task, users often leave
important aspects unsaid. While asking questions could resolve this ambiguity
\citep[GATE;][]{li2023eliciting}, models often struggle to ask good questions.
We explore a language model's ability to self-improve
\citep[STaR;][]{zelikman2022star} by rewarding the model for generating useful
questions -- a simple method we dub STaR-GATE. We generate a synthetic dataset
of 25,500 unique persona-task prompts to simulate conversations between a
pretrained language model -- the \texttt{Questioner} -- and a
\texttt{Roleplayer} whose preferences are unknown to the \texttt{Questioner}.
By asking questions, the \texttt{Questioner} elicits preferences from the
\texttt{Roleplayer}. The \texttt{Questioner} is iteratively finetuned on
questions that increase the probability of high-quality responses to the task,
which are generated by an \texttt{Oracle} with access to the
\texttt{Roleplayer}'s latent preferences. After two iterations of
self-improvement, the \texttt{Questioner} asks better questions, allowing it to
generate responses that are preferred over responses from the initial model on
\highlightpink{\textbf{72\%}} of tasks. Our results indicate that teaching a
language model to ask better questions leads to better personalized responses.
\\ ( https://arxiv.org/abs/2403.19154 ,  717kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19159
Date: Thu, 28 Mar 2024 06:03:47 GMT   (13277kb,D)

Title: Disentangling Length from Quality in Direct Preference Optimization
Authors: Ryan Park and Rafael Rafailov and Stefano Ermon and Chelsea Finn
Categories: cs.CL cs.LG
\\
  Reinforcement Learning from Human Feedback (RLHF) has been a crucial
component in the recent success of Large Language Models. However, RLHF is know
to exploit biases in human preferences, such as verbosity. A well-formatted and
eloquent answer is often more highly rated by users, even when it is less
helpful and objective. A number of approaches have been developed to control
those biases in the classical RLHF literature, but the problem remains
relatively under-explored for Direct Alignment Algorithms such as Direct
Preference Optimization (DPO). Unlike classical RLHF, DPO does not train a
separate reward model or use reinforcement learning directly, so previous
approaches developed to control verbosity cannot be directly applied to this
setting. Our work makes several contributions. For the first time, we study the
length problem in the DPO setting, showing significant exploitation in DPO and
linking it to out-of-distribution bootstrapping. We then develop a principled
but simple regularization strategy that prevents length exploitation, while
still maintaining improvements in model quality. We demonstrate these effects
across datasets on summarization and dialogue, where we achieve up to 20\%
improvement in win rates when controlling for length, despite the GPT4 judge's
well-known verbosity bias.
\\ ( https://arxiv.org/abs/2403.19159 ,  13277kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19161
Date: Thu, 28 Mar 2024 06:07:15 GMT   (422kb,D)

Title: Improving Vietnamese-English Medical Machine Translation
Authors: Nhu Vo, Dat Quoc Nguyen, Dung D. Le, Massimo Piccardi, Wray Buntine
Categories: cs.CL
Comments: To appear in Proceedings of LREC-COLING 2024
\\
  Machine translation for Vietnamese-English in the medical domain is still an
under-explored research area. In this paper, we introduce MedEV -- a
high-quality Vietnamese-English parallel dataset constructed specifically for
the medical domain, comprising approximately 360K sentence pairs. We conduct
extensive experiments comparing Google Translate, ChatGPT (gpt-3.5-turbo),
state-of-the-art Vietnamese-English neural machine translation models and
pre-trained bilingual/multilingual sequence-to-sequence models on our new MedEV
dataset. Experimental results show that the best performance is achieved by
fine-tuning "vinai-translate" for each translation direction. We publicly
release our dataset to promote further research.
\\ ( https://arxiv.org/abs/2403.19161 ,  422kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19167
Date: Thu, 28 Mar 2024 06:28:35 GMT   (7888kb,D)

Title: Mitigating Misleading Chain-of-Thought Reasoning with Selective
  Filtering
Authors: Yexin Wu, Zhuosheng Zhang, Hai Zhao
Categories: cs.CL cs.AI
\\
  Large language models have manifested remarkable capabilities by leveraging
chain-of-thought (CoT) reasoning techniques to solve intricate questions
through step-by-step reasoning chains. Despite its success, the efficacy of
such reasoning is inherently contingent upon the quality of CoT. However,
flawless CoT reasoning cannot be guaranteed due to the presence of
indecomposable questions and the potential for erroneous reasoning chains,
particularly in the case of small-scale language models. To tackle this
challenge, we propose a novel approach called the selective filtering reasoner
(SelF-Reasoner) that assesses the entailment relationship between the question
and the candidate reasoning chain. Then, we proceed with CoT reasoning when the
reasoning chain demonstrates confidence; otherwise, we opt to predict the
answer directly. SelF-Reasoner improves the fine-tuned T5 baseline consistently
over the ScienceQA, ECQA, and LastLetter tasks. Code is available at
\texttt{https://github.com/LibroWu/SelF-Reasoner}.
\\ ( https://arxiv.org/abs/2403.19167 ,  7888kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19183
Date: Thu, 28 Mar 2024 07:27:10 GMT   (6658kb,D)

Title: Empirical Analysis for Unsupervised Universal Dependency Parse Tree
  Aggregation
Authors: Adithya Kulkarni, Oliver Eulenstein, Qi Li
Categories: cs.CL
\\
  Dependency parsing is an essential task in NLP, and the quality of dependency
parsers is crucial for many downstream tasks. Parsers' quality often varies
depending on the domain and the language involved. Therefore, it is essential
to combat the issue of varying quality to achieve stable performance. In
various NLP tasks, aggregation methods are used for post-processing aggregation
and have been shown to combat the issue of varying quality. However,
aggregation methods for post-processing aggregation have not been sufficiently
studied in dependency parsing tasks. In an extensive empirical study, we
compare different unsupervised post-processing aggregation methods to identify
the most suitable dependency tree structure aggregation method.
\\ ( https://arxiv.org/abs/2403.19183 ,  6658kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19219
Date: Thu, 28 Mar 2024 08:32:14 GMT   (797kb,D)

Title: Collaborative Knowledge Infusion for Low-resource Stance Detection
Authors: Ming Yan, Joey Tianyi Zhou, Ivor W. Tsang
Categories: cs.CL
Comments: 13 pages, 3 figures, Big Data Mining and Analysis
DOI: 10.26599/BDMA.2024.9020021
\\
  Stance detection is the view towards a specific target by a given context
(\textit{e.g.} tweets, commercial reviews). Target-related knowledge is often
needed to assist stance detection models in understanding the target well and
making detection correctly. However, prevailing works for knowledge-infused
stance detection predominantly incorporate target knowledge from a singular
source that lacks knowledge verification in limited domain knowledge. The
low-resource training data further increases the challenge for the data-driven
large models in this task. To address those challenges, we propose a
collaborative knowledge infusion approach for low-resource stance detection
tasks, employing a combination of aligned knowledge enhancement and efficient
parameter learning techniques. Specifically, our stance detection approach
leverages target background knowledge collaboratively from different knowledge
sources with the help of knowledge alignment. Additionally, we also introduce
the parameter-efficient collaborative adaptor with a staged optimization
algorithm, which collaboratively addresses the challenges associated with
low-resource stance detection tasks from both network structure and learning
perspectives. To assess the effectiveness of our method, we conduct extensive
experiments on three public stance detection datasets, including low-resource
and cross-target settings. The results demonstrate significant performance
improvements compared to the existing stance detection approaches.
\\ ( https://arxiv.org/abs/2403.19219 ,  797kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19259
Date: Thu, 28 Mar 2024 09:32:43 GMT   (3730kb,D)

Title: J-CRe3: A Japanese Conversation Dataset for Real-world Reference
  Resolution
Authors: Nobuhiro Ueda, Hideko Habe, Yoko Matsui, Akishige Yuguchi, Seiya
  Kawano, Yasutomo Kawanishi, Sadao Kurohashi, Koichiro Yoshino
Categories: cs.CL
Comments: LREC-COLING 2024
\\
  Understanding expressions that refer to the physical world is crucial for
such human-assisting systems in the real world, as robots that must perform
actions that are expected by users. In real-world reference resolution, a
system must ground the verbal information that appears in user interactions to
the visual information observed in egocentric views. To this end, we propose a
multimodal reference resolution task and construct a Japanese Conversation
dataset for Real-world Reference Resolution (J-CRe3). Our dataset contains
egocentric video and dialogue audio of real-world conversations between two
people acting as a master and an assistant robot at home. The dataset is
annotated with crossmodal tags between phrases in the utterances and the object
bounding boxes in the video frames. These tags include indirect reference
relations, such as predicate-argument structures and bridging references as
well as direct reference relations. We also constructed an experimental model
and clarified the challenges in multimodal reference resolution tasks.
\\ ( https://arxiv.org/abs/2403.19259 ,  3730kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19260
Date: Thu, 28 Mar 2024 09:34:31 GMT   (8181kb,D)

Title: NaijaHate: Evaluating Hate Speech Detection on Nigerian Twitter Using
  Representative Data
Authors: Manuel Tonneau, Pedro Vitor Quinta de Castro, Karim Lasri, Ibrahim
  Farouq, Lakshminarayanan Subramanian, Victor Orozco-Olvera, Samuel Fraiberger
Categories: cs.CL
\\
  To address the global issue of hateful content proliferating in online
platforms, hate speech detection (HSD) models are typically developed on
datasets collected in the United States, thereby failing to generalize to
English dialects from the Majority World. Furthermore, HSD models are often
evaluated on curated samples, raising concerns about overestimating model
performance in real-world settings. In this work, we introduce NaijaHate, the
first dataset annotated for HSD which contains a representative sample of
Nigerian tweets. We demonstrate that HSD evaluated on biased datasets
traditionally used in the literature largely overestimates real-world
performance on representative data. We also propose NaijaXLM-T, a pretrained
model tailored to the Nigerian Twitter context, and establish the key role
played by domain-adaptive pretraining and finetuning in maximizing HSD
performance. Finally, we show that in this context, a human-in-the-loop
approach to content moderation where humans review 1% of Nigerian tweets
flagged as hateful would enable to moderate 60% of all hateful content. Taken
together, these results pave the way towards robust HSD systems and a better
protection of social media users from hateful content in low-resource settings.
\\ ( https://arxiv.org/abs/2403.19260 ,  8181kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19267
Date: Thu, 28 Mar 2024 09:53:41 GMT   (28439kb,D)

Title: MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited
  Multimodal Senses and Physical Needs
Authors: Xianhao Yu, Jiaqi Fu, Renjia Deng, Wenjuan Han
Categories: cs.CL cs.AI
Comments: Project website: https://github.com/cocacola-lab/MineLand
\\
  Conventional multi-agent simulators often assume perfect information and
limitless capabilities, hindering the ecological validity of social
interactions. We propose a multi-agent Minecraft simulator, MineLand, that
bridges this gap by introducing limited multimodal senses and physical needs.
Our simulator supports up to 48 agents with limited visual, auditory, and
environmental awareness, forcing them to actively communicate and collaborate
to fulfill physical needs like food and resources. This fosters dynamic and
valid multi-agent interactions. We further introduce an AI agent framework,
Alex, inspired by multitasking theory, enabling agents to handle intricate
coordination and scheduling. Our experiments demonstrate that the simulator,
the corresponding benchmark, and the AI agent framework contribute to more
ecological and nuanced collective behavior. The source code of MineLand and
Alex is openly available at https://github.com/cocacola-lab/MineLand.
\\ ( https://arxiv.org/abs/2403.19267 ,  28439kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19270
Date: Thu, 28 Mar 2024 09:56:04 GMT   (1513kb,D)

Title: sDPO: Don't Use Your Data All at Once
Authors: Dahyun Kim, Yungi Kim, Wonho Song, Hyeonwoo Kim, Yunsu Kim, Sanghoon
  Kim, Chanjun Park
Categories: cs.CL cs.AI
\\
  As development of large language models (LLM) progresses, aligning them with
human preferences has become increasingly important. We propose stepwise DPO
(sDPO), an extension of the recently popularized direct preference optimization
(DPO) for alignment tuning. This approach involves dividing the available
preference datasets and utilizing them in a stepwise manner, rather than
employing it all at once. We demonstrate that this method facilitates the use
of more precisely aligned reference models within the DPO training framework.
Furthermore, sDPO trains the final model to be more performant, even
outperforming other popular LLMs with more parameters.
\\ ( https://arxiv.org/abs/2403.19270 ,  1513kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19275
Date: Thu, 28 Mar 2024 10:01:23 GMT   (5153kb,D)

Title: Knowledge Boundary and Persona Dynamic Shape A Better Social Media Agent
Authors: Junkai Zhou, Liang Pang, Ya Jing, Jia Gu, Huawei Shen, Xueqi Cheng
Categories: cs.CL cs.AI
\\
  Constructing personalized and anthropomorphic agents holds significant
importance in the simulation of social networks. However, there are still two
key problems in existing works: the agent possesses world knowledge that does
not belong to its personas, and it cannot eliminate the interference of diverse
persona information on current actions, which reduces the personalization and
anthropomorphism of the agent. To solve the above problems, we construct the
social media agent based on personalized knowledge and dynamic persona
information. For personalized knowledge, we add external knowledge sources and
match them with the persona information of agents, thereby giving the agent
personalized world knowledge. For dynamic persona information, we use current
action information to internally retrieve the persona information of the agent,
thereby reducing the interference of diverse persona information on the current
action. To make the agent suitable for social media, we design five basic
modules for it: persona, planning, action, memory and reflection. To provide an
interaction and verification environment for the agent, we build a social media
simulation sandbox. In the experimental verification, automatic and human
evaluations demonstrated the effectiveness of the agent we constructed.
\\ ( https://arxiv.org/abs/2403.19275 ,  5153kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19279
Date: Thu, 28 Mar 2024 10:02:10 GMT   (96kb,D)

Title: Fine-Tuning Language Models with Reward Learning on Policy
Authors: Hao Lang, Fei Huang, Yongbin Li
Categories: cs.CL cs.AI cs.LG
Comments: NAACL2024 Main Track Long Paper
\\
  Reinforcement learning from human feedback (RLHF) has emerged as an effective
approach to aligning large language models (LLMs) to human preferences. RLHF
contains three steps, i.e., human preference collecting, reward learning, and
policy optimization, which are usually performed serially. Despite its
popularity, however, (fixed) reward models may suffer from inaccurate
off-distribution, since policy optimization continuously shifts LLMs' data
distribution. Repeatedly collecting new preference data from the latest LLMs
may alleviate this issue, which unfortunately makes the resulting system more
complicated and difficult to optimize. In this paper, we propose reward
learning on policy (RLP), an unsupervised framework that refines a reward model
using policy samples to keep it on-distribution. Specifically, an unsupervised
multi-view learning method is introduced to learn robust representations of
policy samples. Meanwhile, a synthetic preference generation approach is
developed to simulate high-quality preference data with policy outputs.
Extensive experiments on three benchmark datasets show that RLP consistently
outperforms the state-of-the-art. Our code is available at
\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp}.
\\ ( https://arxiv.org/abs/2403.19279 ,  96kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19283
Date: Thu, 28 Mar 2024 10:05:57 GMT   (8082kb,D)

Title: Ungrammatical-syntax-based In-context Example Selection for Grammatical
  Error Correction
Authors: Chenming Tang, Fanyi Qu and Yunfang Wu
Categories: cs.CL
Comments: Accepted to NAACL 2024 Main Conference
\\
  In the era of large language models (LLMs), in-context learning (ICL) stands
out as an effective prompting strategy that explores LLMs' potency across
various tasks. However, applying LLMs to grammatical error correction (GEC) is
still a challenging task. In this paper, we propose a novel
ungrammatical-syntax-based in-context example selection strategy for GEC.
Specifically, we measure similarity of sentences based on their syntactic
structures with diverse algorithms, and identify optimal ICL examples sharing
the most similar ill-formed syntax to the test input. Additionally, we carry
out a two-stage process to further improve the quality of selection results. On
benchmark English GEC datasets, empirical results show that our proposed
ungrammatical-syntax-based strategies outperform commonly-used word-matching or
semantics-based methods with multiple LLMs. This indicates that for a
syntax-oriented task like GEC, paying more attention to syntactic information
can effectively boost LLMs' performance. Our code will be publicly available
after the publication of this paper.
\\ ( https://arxiv.org/abs/2403.19283 ,  8082kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19285
Date: Thu, 28 Mar 2024 10:13:34 GMT   (249kb,D)

Title: Going Beyond Word Matching: Syntax Improves In-context Example Selection
  for Machine Translation
Authors: Chenming Tang, Zhixiang Wang and Yunfang Wu
Categories: cs.CL
\\
  In-context learning (ICL) is the trending prompting strategy in the era of
large language models (LLMs), where a few examples are demonstrated to evoke
LLMs' power for a given task. How to select informative examples remains an
open issue. Previous works on in-context example selection for machine
translation (MT) focus on superficial word-level features while ignoring deep
syntax-level knowledge. In this paper, we propose a syntax-based in-context
example selection method for MT, by computing the syntactic similarity between
dependency trees using Polynomial Distance. In addition, we propose an ensemble
strategy combining examples selected by both word-level and syntax-level
criteria. Experimental results between English and 6 common languages indicate
that syntax can effectively enhancing ICL for MT, obtaining the highest COMET
scores on 11 out of 12 translation directions.
\\ ( https://arxiv.org/abs/2403.19285 ,  249kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19305
Date: Thu, 28 Mar 2024 10:41:47 GMT   (3640kb,D)

Title: MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended
  Text Evaluation
Authors: Yu Li, Shenyu Zhang, Rui Wu, Xiutian Huang, Yongrui Chen, Wenhao Xu,
  Guilin Qi and Dehai Min
Categories: cs.CL cs.AI
Comments: This paper has been ACCEPTED as a LONG PAPER presentation by DASFAA
  2024 Industrial Track
\\
  Recent advancements in generative Large Language Models(LLMs) have been
remarkable, however, the quality of the text generated by these models often
reveals persistent issues. Evaluating the quality of text generated by these
models, especially in open-ended text, has consistently presented a significant
challenge. Addressing this, recent work has explored the possibility of using
LLMs as evaluators. While using a single LLM as an evaluation agent shows
potential, it is filled with significant uncertainty and instability. To
address these issues, we propose the MATEval: A "Multi-Agent Text Evaluation
framework" where all agents are played by LLMs like GPT-4. The MATEval
framework emulates human collaborative discussion methods, integrating multiple
agents' interactions to evaluate open-ended text. Our framework incorporates
self-reflection and Chain-of-Thought (CoT) strategies, along with feedback
mechanisms, enhancing the depth and breadth of the evaluation process and
guiding discussions towards consensus, while the framework generates
comprehensive evaluation reports, including error localization, error types and
scoring. Experimental results show that our framework outperforms existing
open-ended text evaluation methods and achieves the highest correlation with
human evaluation, which confirms the effectiveness and advancement of our
framework in addressing the uncertainties and instabilities in evaluating
LLMs-generated text. Furthermore, our framework significantly improves the
efficiency of text evaluation and model iteration in industrial scenarios.
\\ ( https://arxiv.org/abs/2403.19305 ,  3640kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19317
Date: Thu, 28 Mar 2024 11:18:31 GMT   (59kb,D)

Title: Beyond Borders: Investigating Cross-Jurisdiction Transfer in Legal Case
  Summarization
Authors: T.Y.S.S Santosh, Vatsal Venkatkrishna, Saptarshi Ghosh, Matthias
  Grabmair
Categories: cs.CL
Comments: Accepted to NAACL 2024
\\
  Legal professionals face the challenge of managing an overwhelming volume of
lengthy judgments, making automated legal case summarization crucial. However,
prior approaches mainly focused on training and evaluating these models within
the same jurisdiction. In this study, we explore the cross-jurisdictional
generalizability of legal case summarization models.Specifically, we explore
how to effectively summarize legal cases of a target jurisdiction where
reference summaries are not available. In particular, we investigate whether
supplementing models with unlabeled target jurisdiction corpus and extractive
silver summaries obtained from unsupervised algorithms on target data enhances
transfer performance. Our comprehensive study on three datasets from different
jurisdictions highlights the role of pre-training in improving transfer
performance. We shed light on the pivotal influence of jurisdictional
similarity in selecting optimal source datasets for effective transfer.
Furthermore, our findings underscore that incorporating unlabeled target data
yields improvements in general pre-trained models, with additional gains when
silver summaries are introduced. This augmentation is especially valuable when
dealing with extractive datasets and scenarios featuring limited alignment
between source and target jurisdictions. Our study provides key insights for
developing adaptable legal case summarization systems, transcending
jurisdictional boundaries.
\\ ( https://arxiv.org/abs/2403.19317 ,  59kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19318
Date: Thu, 28 Mar 2024 11:21:12 GMT   (3244kb,D)

Title: TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office
  Usage Scenarios
Authors: Xiaokang Zhang, Jing Zhang, Zeyao Ma, Yang Li, Bohan Zhang, Guanlin
  Li, Zijun Yao, Kangli Xu, Jinchang Zhou, Daniel Zhang-Li, Jifan Yu, Shu Zhao,
  Juanzi Li, Jie Tang
Categories: cs.CL
Comments: 30 pages
\\
  We introduce TableLLM, a robust large language model (LLM) with 13 billion
parameters, purpose-built for proficiently handling tabular data manipulation
tasks, whether they are embedded within documents or spreadsheets, catering to
real-world office scenarios. We propose a distant supervision method for
training, which comprises a reasoning process extension strategy, aiding in
training LLMs to understand reasoning patterns more effectively as well as a
cross-way validation strategy, ensuring the quality of the automatically
generated data. To evaluate the performance of TableLLM, we have crafted a
benchmark tailored to address both document and spreadsheet formats as well as
constructed a well-organized evaluation pipeline capable of handling both
scenarios. Thorough evaluations underscore the advantages of TableLLM when
compared to various existing general-purpose and tabular data-focused LLMs. We
have publicly released the model checkpoint, source code, benchmarks, and a web
application for user interaction.
\\ ( https://arxiv.org/abs/2403.19318 ,  3244kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19335
Date: Thu, 28 Mar 2024 11:51:11 GMT   (795kb,D)

Title: KazSAnDRA: Kazakh Sentiment Analysis Dataset of Reviews and Attitudes
Authors: Rustem Yeshpanov, Huseyin Atakan Varol
Categories: cs.CL
\\
  This paper presents KazSAnDRA, a dataset developed for Kazakh sentiment
analysis that is the first and largest publicly available dataset of its kind.
KazSAnDRA comprises an extensive collection of 180,064 reviews obtained from
various sources and includes numerical ratings ranging from 1 to 5, providing a
quantitative representation of customer attitudes. The study also pursued the
automation of Kazakh sentiment classification through the development and
evaluation of four machine learning models trained for both polarity
classification and score classification. Experimental analysis included
evaluation of the results considering both balanced and imbalanced scenarios.
The most successful model attained an F1-score of 0.81 for polarity
classification and 0.39 for score classification on the test sets. The dataset
and fine-tuned models are open access and available for download under the
Creative Commons Attribution 4.0 International License (CC BY 4.0) through our
GitHub repository.
\\ ( https://arxiv.org/abs/2403.19335 ,  795kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19340
Date: Thu, 28 Mar 2024 11:57:08 GMT   (618kb,D)

Title: Dataverse: Open-Source ETL (Extract, Transform, Load) Pipeline for Large
  Language Models
Authors: Hyunbyung Park, Sukyung Lee, Gyoungjin Gim, Yungi Kim, Dahyun Kim,
  Chanjun Park
Categories: cs.CL cs.AI
\\
  To address the challenges associated with data processing at scale, we
propose Dataverse, a unified open-source Extract-Transform-Load (ETL) pipeline
for large language models (LLMs) with a user-friendly design at its core. Easy
addition of custom processors with block-based interface in Dataverse allows
users to readily and efficiently use Dataverse to build their own ETL pipeline.
We hope that Dataverse will serve as a vital tool for LLM development and open
source the entire library to welcome community contribution. Additionally, we
provide a concise, two-minute video demonstration of our system, illustrating
its capabilities and implementation.
\\ ( https://arxiv.org/abs/2403.19340 ,  618kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19346
Date: Thu, 28 Mar 2024 12:04:28 GMT   (3000kb,D)

Title: Large Language Models Are Unconscious of Unreasonability in Math
  Problems
Authors: Jingyuan Ma, Damai Dai, Zhifang Sui
Categories: cs.CL
Comments: 12 pages, 4 figures
\\
  Large language models (LLMs) demonstrate substantial capabilities in solving
math problems. However, they tend to produce hallucinations when given
questions containing unreasonable errors. In this paper, we study the behavior
of LLMs when faced with unreasonable math problems and further explore their
potential to address these problems. First, we construct the Unreasonable Math
Problem (UMP) benchmark to examine the error detection ability of LLMs.
Experiments show that LLMs are able to detect unreasonable errors, but still
fail in generating non-hallucinatory content. In order to improve their ability
of error detection and correction, we further design a strategic prompt
template called Critical Calculation and Conclusion(CCC). With CCC, LLMs can
better self-evaluate and detect unreasonable errors in math questions, making
them more reliable and safe in practical application scenarios.
\\ ( https://arxiv.org/abs/2403.19346 ,  3000kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19352
Date: Thu, 28 Mar 2024 12:08:39 GMT   (2795kb,D)

Title: A diverse Multilingual News Headlines Dataset from around the World
Authors: Felix Leeb and Bernhard Sch\"olkopf
Categories: cs.CL
Comments: Published in NAACL 2024 Proceedings (Short Paper track)
\\
  Babel Briefings is a novel dataset featuring 4.7 million news headlines from
August 2020 to November 2021, across 30 languages and 54 locations worldwide
with English translations of all articles included. Designed for natural
language processing and media studies, it serves as a high-quality dataset for
training or evaluating language models as well as offering a simple, accessible
collection of articles, for example, to analyze global news coverage and
cultural narratives. As a simple demonstration of the analyses facilitated by
this dataset, we use a basic procedure using a TF-IDF weighted similarity
metric to group articles into clusters about the same event. We then visualize
the \emph{event signatures} of the event showing articles of which languages
appear over time, revealing intuitive features based on the proximity of the
event and unexpectedness of the event. The dataset is available on
\href{https://www.kaggle.com/datasets/felixludos/babel-briefings}{Kaggle} and
\href{https://huggingface.co/datasets/felixludos/babel-briefings}{HuggingFace}
with accompanying \href{https://github.com/felixludos/babel-briefings}{GitHub}
code.
\\ ( https://arxiv.org/abs/2403.19352 ,  2795kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19354
Date: Thu, 28 Mar 2024 12:10:30 GMT   (8342kb,D)

Title: AIpom at SemEval-2024 Task 8: Detecting AI-produced Outputs in M4
Authors: Alexander Shirnin, Nikita Andreev, Vladislav Mikhailov, Ekaterina
  Artemova
Categories: cs.CL
Comments: 2nd place at SemEval-2024 Task 8, Subtask C, to appear in
  SemEval-2024 proceedings
\\
  This paper describes AIpom, a system designed to detect a boundary between
human-written and machine-generated text (SemEval-2024 Task 8, Subtask C:
Human-Machine Mixed Text Detection). We propose a two-stage pipeline combining
predictions from an instruction-tuned decoder-only model and encoder-only
sequence taggers. AIpom is ranked second on the leaderboard while achieving a
Mean Absolute Error of 15.94. Ablation studies confirm the benefits of
pipelining encoder and decoder models, particularly in terms of improved
performance.
\\ ( https://arxiv.org/abs/2403.19354 ,  8342kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19358
Date: Thu, 28 Mar 2024 12:17:36 GMT   (2086kb,D)

Title: Risk prediction of pathological gambling on social media
Authors: Angelina Parfenova, Marianne Clausel
Categories: cs.CL
\\
  This paper addresses the problem of risk prediction on social media data,
specifically focusing on the classification of Reddit users as having a
pathological gambling disorder. To tackle this problem, this paper focuses on
incorporating temporal and emotional features into the model. The preprocessing
phase involves dealing with the time irregularity of posts by padding
sequences. Two baseline architectures are used for preliminary evaluation: BERT
classifier on concatenated posts per user and GRU with LSTM on sequential data.
Experimental results demonstrate that the sequential models outperform the
concatenation-based model. The results of the experiments conclude that the
incorporation of a time decay layer (TD) and passing the emotion classification
layer (EmoBERTa) through LSTM improves the performance significantly.
Experiments concluded that the addition of a self-attention layer didn't
significantly improve the performance of the model, however provided easily
interpretable attention scores. The developed architecture with the inclusion
of EmoBERTa and TD layers achieved a high F1 score, beating existing benchmarks
on pathological gambling dataset. Future work may involve the early prediction
of risk factors associated with pathological gambling disorder and testing
models on other datasets. Overall, this research highlights the significance of
the sequential processing of posts including temporal and emotional features to
boost the predictive power, as well as adding an attention layer for
interpretability.
\\ ( https://arxiv.org/abs/2403.19358 ,  2086kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19365
Date: Thu, 28 Mar 2024 12:26:45 GMT   (872kb,D)

Title: EthioMT: Parallel Corpus for Low-resource Ethiopian Languages
Authors: Atnafu Lambebo Tonja, Olga Kolesnikova, Alexander Gelbukh, Jugal
  Kalita
Categories: cs.CL
Comments: Accepted at The Fifth workshop on Resources for African Indigenous
  Languages (RAIL) 2024 ( LREC-COLING 2024)
\\
  Recent research in natural language processing (NLP) has achieved impressive
performance in tasks such as machine translation (MT), news classification, and
question-answering in high-resource languages. However, the performance of MT
leaves much to be desired for low-resource languages. This is due to the
smaller size of available parallel corpora in these languages, if such corpora
are available at all. NLP in Ethiopian languages suffers from the same issues
due to the unavailability of publicly accessible datasets for NLP tasks,
including MT. To help the research community and foster research for Ethiopian
languages, we introduce EthioMT -- a new parallel corpus for 15 languages. We
also create a new benchmark by collecting a dataset for better-researched
languages in Ethiopia. We evaluate the newly collected corpus and the benchmark
dataset for 23 Ethiopian languages using transformer and fine-tuning
approaches.
\\ ( https://arxiv.org/abs/2403.19365 ,  872kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19390
Date: Thu, 28 Mar 2024 13:01:18 GMT   (1835kb,D)

Title: Checkpoint Merging via Bayesian Optimization in LLM Pretraining
Authors: Deyuan Liu, Zecheng Wang, Bingning Wang, Weipeng Chen, Chunshan Li,
  Zhiying Tu, Dianhui Chu, Bo Li, Dianbo Sui
Categories: cs.CL
\\
  The rapid proliferation of large language models (LLMs) such as GPT-4 and
Gemini underscores the intense demand for resources during their training
processes, posing significant challenges due to substantial computational and
environmental costs. To alleviate this issue, we propose checkpoint merging in
pretraining LLM. This method utilizes LLM checkpoints with shared training
trajectories, and is rooted in an extensive search space exploration for the
best merging weight via Bayesian optimization. Through various experiments, we
demonstrate that: (1) Our proposed methodology exhibits the capacity to augment
pretraining, presenting an opportunity akin to obtaining substantial benefits
at minimal cost; (2) Our proposed methodology, despite requiring a given
held-out dataset, still demonstrates robust generalization capabilities across
diverse domains, a pivotal aspect in pretraining.
\\ ( https://arxiv.org/abs/2403.19390 ,  1835kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19399
Date: Thu, 28 Mar 2024 13:19:16 GMT   (54kb)

Title: KazParC: Kazakh Parallel Corpus for Machine Translation
Authors: Rustem Yeshpanov, Alina Polonskaya, Huseyin Atakan Varol
Categories: cs.CL
\\
  We introduce KazParC, a parallel corpus designed for machine translation
across Kazakh, English, Russian, and Turkish. The first and largest publicly
available corpus of its kind, KazParC contains a collection of 371,902 parallel
sentences covering different domains and developed with the assistance of human
translators. Our research efforts also extend to the development of a neural
machine translation model nicknamed Tilmash. Remarkably, the performance of
Tilmash is on par with, and in certain instances, surpasses that of industry
giants, such as Google Translate and Yandex Translate, as measured by standard
evaluation metrics, such as BLEU and chrF. Both KazParC and Tilmash are openly
available for download under the Creative Commons Attribution 4.0 International
License (CC BY 4.0) through our GitHub repository.
\\ ( https://arxiv.org/abs/2403.19399 ,  54kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19414
Date: Thu, 28 Mar 2024 13:38:13 GMT   (1550kb,D)

Title: BP4ER: Bootstrap Prompting for Explicit Reasoning in Medical Dialogue
  Generation
Authors: Yuhong He and Yongqi Zhang and Shizhu He and Jun Wan
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  Medical dialogue generation (MDG) has gained increasing attention due to its
substantial practical value. Previous works typically employ a
sequence-to-sequence framework to generate medical responses by modeling
dialogue context as sequential text with annotated medical entities. While
these methods have been successful in generating fluent responses, they fail to
provide process explanations of reasoning and require extensive entity
annotation. To address these limitations, we propose the method Bootstrap
Prompting for Explicit Reasoning in MDG (BP4ER), which explicitly model MDG's
multi-step reasoning process and iteratively enhance this reasoning process. We
employ a least-to-most prompting strategy to guide a large language model (LLM)
in explicit reasoning, breaking down MDG into simpler sub-questions. These
sub-questions build on answers from previous ones. Additionally, we also
introduce two distinct bootstrapping techniques for prompting, which
autonomously correct errors and facilitate the LLM's explicit reasoning. This
approach eliminates the need for entity annotation and increases the
transparency of the MDG process by explicitly generating the intermediate
reasoning chain. The experimental findings on the two public datasets indicate
that BP4ER outperforms state-of-the-art methods in terms of both objective and
subjective evaluation metrics.
\\ ( https://arxiv.org/abs/2403.19414 ,  1550kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19424
Date: Thu, 28 Mar 2024 13:56:23 GMT   (470kb,D)

Title: The Role of Syntactic Span Preferences in Post-Hoc Explanation
  Disagreement
Authors: Jonathan Kamp, Lisa Beinborn, Antske Fokkens
Categories: cs.CL cs.AI
Comments: Long paper accepted to LREC-Coling 2024 main conference. Please cite
  the conference proceedings version when available
\\
  Post-hoc explanation methods are an important tool for increasing model
transparency for users. Unfortunately, the currently used methods for
attributing token importance often yield diverging patterns. In this work, we
study potential sources of disagreement across methods from a linguistic
perspective. We find that different methods systematically select different
classes of words and that methods that agree most with other methods and with
humans display similar linguistic preferences. Token-level differences between
methods are smoothed out if we compare them on the syntactic span level. We
also find higher agreement across methods by estimating the most important
spans dynamically instead of relying on a fixed subset of size $k$. We
systematically investigate the interaction between $k$ and spans and propose an
improved configuration for selecting important tokens.
\\ ( https://arxiv.org/abs/2403.19424 ,  470kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19432
Date: Thu, 28 Mar 2024 14:03:12 GMT   (554kb,D)

Title: Uncovering Misattributed Suicide Causes through Annotation Inconsistency
  Detection in Death Investigation Notes
Authors: Song Wang, Yiliang Zhou, Ziqiang Han, Cui Tao, Yunyu Xiao, Ying Ding,
  Joydeep Ghosh, Yifan Peng
Categories: cs.CL cs.AI
Comments: 19 pages, 6 figures
\\
  Data accuracy is essential for scientific research and policy development.
The National Violent Death Reporting System (NVDRS) data is widely used for
discovering the patterns and causes of death. Recent studies suggested the
annotation inconsistencies within the NVDRS and the potential impact on
erroneous suicide-cause attributions. We present an empirical Natural Language
Processing (NLP) approach to detect annotation inconsistencies and adopt a
cross-validation-like paradigm to identify problematic instances. We analyzed
267,804 suicide death incidents between 2003 and 2020 from the NVDRS. Our
results showed that incorporating the target state's data into training the
suicide-crisis classifier brought an increase of 5.4% to the F-1 score on the
target state's test set and a decrease of 1.1% on other states' test set. To
conclude, we demonstrated the annotation inconsistencies in NVDRS's death
investigation notes, identified problematic instances, evaluated the
effectiveness of correcting problematic instances, and eventually proposed an
NLP improvement solution.
\\ ( https://arxiv.org/abs/2403.19432 ,  554kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19443
Date: Thu, 28 Mar 2024 14:15:10 GMT   (1131kb,D)

Title: Mixed Preference Optimization: Reinforcement Learning with Data
  Selection and Better Reference Model
Authors: Qi Gou and Cam-Tu Nguyen
Categories: cs.CL
\\
  Large Language Models (LLMs) have become increasingly popular due to their
ability to process and generate natural language. However, as they are trained
on massive datasets of text, LLMs can inherit harmful biases and produce
outputs that are not aligned with human values. This paper studies two main
approaches to LLM alignment: Reinforcement Learning with Human Feedback (RLHF)
and contrastive learning-based methods like Direct Preference Optimization
(DPO). By analyzing the stability and robustness of RLHF and DPO, we propose
MPO (Mixed Preference Optimization), a novel method that mitigates the
weaknesses of both approaches. Specifically, we propose a two-stage training
procedure: first train DPO on an easy dataset, and then perform RLHF on a
difficult set with DPO model being the reference model. Here, the easy and
difficult sets are constructed by a well-trained reward model that splits
response pairs into those with large gaps of reward (easy), and those with
small gaps (difficult). The first stage allows us to obtain a relatively
optimal policy (LLM) model quickly, whereas the second stage refines LLM with
online RLHF, thus mitigating the distribution shift issue associated with DPO.
Experiments are conducted on two public alignment datasets, namely HH-RLHF and
TLDR, demonstrating the effectiveness of MPO, both in terms of GPT4 and human
evaluation.
\\ ( https://arxiv.org/abs/2403.19443 ,  1131kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19454
Date: Thu, 28 Mar 2024 14:22:54 GMT   (3954kb,D)

Title: JDocQA: Japanese Document Question Answering Dataset for Generative
  Language Models
Authors: Eri Onami, Shuhei Kurita, Taiki Miyanishi, Taro Watanabe
Categories: cs.CL
Comments: LREC-COLING2024
\\
  Document question answering is a task of question answering on given
documents such as reports, slides, pamphlets, and websites, and it is a truly
demanding task as paper and electronic forms of documents are so common in our
society. This is known as a quite challenging task because it requires not only
text understanding but also understanding of figures and tables, and hence
visual question answering (VQA) methods are often examined in addition to
textual approaches. We introduce Japanese Document Question Answering (JDocQA),
a large-scale document-based QA dataset, essentially requiring both visual and
textual information to answer questions, which comprises 5,504 documents in PDF
format and annotated 11,600 question-and-answer instances in Japanese. Each QA
instance includes references to the document pages and bounding boxes for the
answer clues. We incorporate multiple categories of questions and unanswerable
questions from the document for realistic question-answering applications. We
empirically evaluate the effectiveness of our dataset with text-based large
language models (LLMs) and multimodal models. Incorporating unanswerable
questions in finetuning may contribute to harnessing the so-called
hallucination generation.
\\ ( https://arxiv.org/abs/2403.19454 ,  3954kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19509
Date: Thu, 28 Mar 2024 15:42:07 GMT   (1346kb)

Title: Phonetic Segmentation of the UCLA Phonetics Lab Archive
Authors: Eleanor Chodroff, Bla\v{z} Pa\v{z}on, Annie Baker, Steven Moran
Categories: cs.CL cs.SD eess.AS
Comments: Accepted at LREC-COLING 2024
\\
  Research in speech technologies and comparative linguistics depends on access
to diverse and accessible speech data. The UCLA Phonetics Lab Archive is one of
the earliest multilingual speech corpora, with long-form audio recordings and
phonetic transcriptions for 314 languages (Ladefoged et al., 2009). Recently,
95 of these languages were time-aligned with word-level phonetic transcriptions
(Li et al., 2021). Here we present VoxAngeles, a corpus of audited phonetic
transcriptions and phone-level alignments of the UCLA Phonetics Lab Archive,
which uses the 95-language CMU re-release as our starting point. VoxAngeles
also includes word- and phone-level segmentations from the original UCLA
corpus, as well as phonetic measurements of word and phone durations, vowel
formants, and vowel f0. This corpus enhances the usability of the original
data, particularly for quantitative phonetic typology, as demonstrated through
a case study of vowel intrinsic f0. We also discuss the utility of the
VoxAngeles corpus for general research and pedagogy in crosslinguistic
phonetics, as well as for low-resource and multilingual speech technologies.
VoxAngeles is free to download and use under a CC-BY-NC 4.0 license.
\\ ( https://arxiv.org/abs/2403.19509 ,  1346kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19511
Date: Thu, 28 Mar 2024 15:44:18 GMT   (1769kb)

Title: Improving Clinical NLP Performance through Language Model-Generated
  Synthetic Clinical Data
Authors: Shan Chen, Jack Gallifant, Marco Guevara, Yanjun Gao, Majid Afshar,
  Timothy Miller, Dmitriy Dligach, Danielle S. Bitterman
Categories: cs.CL
Comments: submitted to review
\\
  Generative models have been showing potential for producing data in mass.
This study explores the enhancement of clinical natural language processing
performance by utilizing synthetic data generated from advanced language
models. Promising results show feasible applications in such a high-stakes
domain.
\\ ( https://arxiv.org/abs/2403.19511 ,  1769kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19521
Date: Thu, 28 Mar 2024 15:54:59 GMT   (6157kb,D)

Title: Interpreting Key Mechanisms of Factual Recall in Transformer-Based
  Language Models
Authors: Ang Lv, Kaiyi Zhang, Yuhan Chen, Yulong Wang, Lifeng Liu, Ji-Rong Wen,
  Jian Xie, Rui Yan
Categories: cs.CL cs.AI cs.LG
\\
  In this paper, we deeply explore the mechanisms employed by Transformer-based
language models in factual recall tasks. In zero-shot scenarios, given a prompt
like "The capital of France is," task-specific attention heads extract the
topic entity, such as "France," from the context and pass it to subsequent MLPs
to recall the required answer such as "Paris." We introduce a novel analysis
method aimed at decomposing the outputs of the MLP into components
understandable by humans. Through this method, we quantify the function of the
MLP layer following these task-specific heads. In the residual stream, it
either erases or amplifies the information originating from individual heads.
Moreover, it generates a component that redirects the residual stream towards
the direction of its expected answer. These zero-shot mechanisms are also
employed in few-shot scenarios. Additionally, we observed a widely existent
anti-overconfidence mechanism in the final layer of models, which suppresses
correct predictions. We mitigate this suppression by leveraging our
interpretation to improve factual recall performance. Our interpretations have
been evaluated across various language models, from the GPT-2 families to 1.3B
OPT, and across tasks covering different domains of factual knowledge.
\\ ( https://arxiv.org/abs/2403.19521 ,  6157kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19548
Date: Thu, 28 Mar 2024 16:28:38 GMT   (3419kb,D)

Title: WaterJudge: Quality-Detection Trade-off when Watermarking Large Language
  Models
Authors: Piotr Molenda, Adian Liusie, Mark J. F. Gales
Categories: cs.CL
Comments: NAACL 2024 (Findings)
\\
  Watermarking generative-AI systems, such as LLMs, has gained considerable
interest, driven by their enhanced capabilities across a wide range of tasks.
Although current approaches have demonstrated that small, context-dependent
shifts in the word distributions can be used to apply and detect watermarks,
there has been little work in analyzing the impact that these perturbations
have on the quality of generated texts. Balancing high detectability with
minimal performance degradation is crucial in terms of selecting the
appropriate watermarking setting; therefore this paper proposes a simple
analysis framework where comparative assessment, a flexible NLG evaluation
framework, is used to assess the quality degradation caused by a particular
watermark setting. We demonstrate that our framework provides easy
visualization of the quality-detection trade-off of watermark settings,
enabling a simple solution to find an LLM watermark operating point that
provides a well-balanced performance. This approach is applied to two different
summarization systems and a translation system, enabling cross-model analysis
for a task, and cross-task analysis.
\\ ( https://arxiv.org/abs/2403.19548 ,  3419kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19559
Date: Thu, 28 Mar 2024 16:44:14 GMT   (8153kb,D)

Title: Improving Adversarial Data Collection by Supporting Annotators: Lessons
  from GAHD, a German Hate Speech Dataset
Authors: Janis Goldzycher, Paul R\"ottger, Gerold Schneider
Categories: cs.CL
Comments: Accepted at NAACL 2024 (main conference)
\\
  Hate speech detection models are only as good as the data they are trained
on. Datasets sourced from social media suffer from systematic gaps and biases,
leading to unreliable models with simplistic decision boundaries. Adversarial
datasets, collected by exploiting model weaknesses, promise to fix this
problem. However, adversarial data collection can be slow and costly, and
individual annotators have limited creativity. In this paper, we introduce
GAHD, a new German Adversarial Hate speech Dataset comprising ca.\ 11k
examples. During data collection, we explore new strategies for supporting
annotators, to create more diverse adversarial examples more efficiently and
provide a manual analysis of annotator disagreements for each strategy. Our
experiments show that the resulting dataset is challenging even for
state-of-the-art hate speech detection models, and that training on GAHD
clearly improves model robustness. Further, we find that mixing multiple
support strategies is most advantageous. We make GAHD publicly available at
https://github.com/jagol/gahd.
\\ ( https://arxiv.org/abs/2403.19559 ,  8153kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19603
Date: Thu, 28 Mar 2024 17:27:44 GMT   (2533kb,D)

Title: Semantic Map-based Generation of Navigation Instructions
Authors: Chengzu Li, Chao Zhang, Simone Teufel, Rama Sanand Doddipatla,
  Svetlana Stoyanchev
Categories: cs.CL cs.AI cs.CV
Comments: 5 pages, 2 figures, 3 tables (13 pages, 3 figures, 5 tables including
  references and appendices), accepted at LREC-COLING 2024
\\
  We are interested in the generation of navigation instructions, either in
their own right or as training material for robotic navigation task. In this
paper, we propose a new approach to navigation instruction generation by
framing the problem as an image captioning task using semantic maps as visual
input. Conventional approaches employ a sequence of panorama images to generate
navigation instructions. Semantic maps abstract away from visual details and
fuse the information in multiple panorama images into a single top-down
representation, thereby reducing computational complexity to process the input.
We present a benchmark dataset for instruction generation using semantic maps,
propose an initial model and ask human subjects to manually assess the quality
of generated instructions. Our initial investigations show promise in using
semantic maps for instruction generation instead of a sequence of panorama
images, but there is vast scope for improvement. We release the code for data
preparation and model training at https://github.com/chengzu-li/VLGen.
\\ ( https://arxiv.org/abs/2403.19603 ,  2533kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19631
Date: Thu, 28 Mar 2024 17:47:19 GMT   (2140kb,D)

Title: Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in
  Language Models
Authors: Yucheng Shi, Qiaoyu Tan, Xuansheng Wu, Shaochen Zhong, Kaixiong Zhou,
  Ninghao Liu
Categories: cs.CL cs.AI cs.LG
Comments: Work in progress
\\
  Large Language Models (LLMs) have shown proficiency in question-answering
tasks but often struggle to integrate real-time knowledge updates, leading to
potentially outdated or inaccurate responses. This problem becomes even more
challenging when dealing with multi-hop questions since they require LLMs to
update and integrate multiple knowledge pieces relevant to the questions. To
tackle the problem, we propose the Retrieval-Augmented model Editing (RAE)
framework tailored for multi-hop question answering. RAE first retrieves edited
facts and then refines the language model through in-context learning.
Specifically, our retrieval approach, based on mutual information maximization,
leverages the reasoning abilities of LLMs to identify chain facts that na\"ive
similarity-based searches might miss. Additionally, our framework incorporates
a pruning strategy to eliminate redundant information from the retrieved facts,
which enhances the editing accuracy and mitigates the hallucination problem.
Our framework is supported by theoretical justification for its fact retrieval
efficacy. Finally, comprehensive evaluation across various LLMs validates RAE's
ability in providing accurate answers with updated knowledge.
\\ ( https://arxiv.org/abs/2403.19631 ,  2140kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18872
Date: Tue, 26 Mar 2024 12:51:02 GMT   (1934kb,D)

Title: Targeted Visualization of the Backbone of Encoder LLMs
Authors: Isaac Roberts, Alexander Schulz, Luca Hermes, Barbara Hammer
Categories: cs.LG cs.AI cs.CL
\\
  Attention based Large Language Models (LLMs) are the state-of-the-art in
natural language processing (NLP). The two most common architectures are
encoders such as BERT, and decoders like the GPT models. Despite the success of
encoder models, on which we focus in this work, they also bear several risks,
including issues with bias or their susceptibility for adversarial attacks,
signifying the necessity for explainable AI to detect such issues. While there
does exist various local explainability methods focusing on the prediction of
single inputs, global methods based on dimensionality reduction for
classification inspection, which have emerged in other domains and that go
further than just using t-SNE in the embedding space, are not widely spread in
NLP.
  To reduce this gap, we investigate the application of DeepView, a method for
visualizing a part of the decision function together with a data set in two
dimensions, to the NLP domain. While in previous work, DeepView has been used
to inspect deep image classification models, we demonstrate how to apply it to
BERT-based NLP classifiers and investigate its usability in this domain,
including settings with adversarially perturbed input samples and pre-trained,
fine-tuned, and multi-task models.
\\ ( https://arxiv.org/abs/2403.18872 ,  1934kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18886
Date: Wed, 27 Mar 2024 17:59:21 GMT   (1849kb,D)

Title: Self-Expansion of Pre-trained Models with Mixture of Adapters for
  Continual Learning
Authors: Huiyi Wang, Haodong Lu, Lina Yao, Dong Gong
Categories: cs.LG cs.CV
\\
  Continual learning aims to learn from a stream of continuously arriving data
with minimum forgetting of previously learned knowledge. While previous works
have explored the effectiveness of leveraging the generalizable knowledge from
pre-trained models in continual learning, existing parameter-efficient
fine-tuning approaches focus on the use of a predetermined or task-wise set of
adapters or prompts. However, these approaches still suffer from forgetting due
to task interference on jointly used parameters or restricted flexibility. The
reliance on a static model architecture may lead to the allocation of excessive
parameters that are not essential or, conversely, inadequate adaptation for
downstream tasks, given that the scale and distribution of incoming data are
unpredictable in continual learning. We propose Self-Expansion of pre-trained
models with Modularized Adaptation (SEMA), a novel fine-tuning approach which
automatically decides to reuse or add adapter modules on demand in continual
learning, depending on whether drastic distribution shift that could not be
handled by existing modules is detected at different representation levels. We
design each adapter module to consist of an adapter and a representation
descriptor, specifically, implemented as an autoencoder. The representation
descriptor functions as a distributional shift indicator during training and
triggers adapter expansion. For better usage of the adapters, an expandable
weighting router is learned jointly for mixture of adapter outputs. By
comparing with vision-transformer-based continual learning adaptation methods,
we demonstrate that the proposed framework outperforms the state-of-the-art
without memory rehearsal.
\\ ( https://arxiv.org/abs/2403.18886 ,  1849kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18910
Date: Wed, 27 Mar 2024 18:02:49 GMT   (7311kb,D)

Title: A Geometric Explanation of the Likelihood OOD Detection Paradox
Authors: Hamidreza Kamkari, Brendan Leigh Ross, Jesse C. Cresswell, Anthony L.
  Caterini, Rahul G. Krishnan, Gabriel Loaiza-Ganem
Categories: cs.LG cs.AI cs.CV stat.ML
\\
  Likelihood-based deep generative models (DGMs) commonly exhibit a puzzling
behaviour: when trained on a relatively complex dataset, they assign higher
likelihood values to out-of-distribution (OOD) data from simpler sources.
Adding to the mystery, OOD samples are never generated by these DGMs despite
having higher likelihoods. This two-pronged paradox has yet to be conclusively
explained, making likelihood-based OOD detection unreliable. Our primary
observation is that high-likelihood regions will not be generated if they
contain minimal probability mass. We demonstrate how this seeming contradiction
of large densities yet low probability mass can occur around data confined to
low-dimensional manifolds. We also show that this scenario can be identified
through local intrinsic dimension (LID) estimation, and propose a method for
OOD detection which pairs the likelihoods and LID estimates obtained from a
pre-trained DGM. Our method can be applied to normalizing flows and score-based
diffusion models, and obtains results which match or surpass state-of-the-art
OOD detection benchmarks using the same DGM backbones. Our code is available at
https://github.com/layer6ai-labs/dgm_ood_detection.
\\ ( https://arxiv.org/abs/2403.18910 ,  7311kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18926
Date: Tue, 27 Feb 2024 08:18:02 GMT   (8466kb,D)

Title: Enhancing Efficiency in Sparse Models with Sparser Selection
Authors: Yuanhang Yang, Shiyi Qi, Wenchao Gu, Chaozheng Wang, Cuiyun Gao,
  Zenglin Xu
Categories: cs.LG cs.CL
\\
  Sparse models, including sparse Mixture-of-Experts (MoE) models, have emerged
as an effective approach for scaling Transformer models. However, they often
suffer from computational inefficiency since a significant number of parameters
are unnecessarily involved in computations via multiplying values by zero or
low activation values. To address this issue, we present \tool, a novel MoE
designed to enhance both the efficacy and efficiency of sparse MoE models.
\tool leverages small experts and a threshold-based router to enable tokens to
selectively engage only essential parameters. Our extensive experiments on
language modeling and machine translation tasks demonstrate that \tool can
enhance model performance while decreasing the computation load at MoE layers
by over 50\% without sacrificing performance. Furthermore, we present the
versatility of \tool by applying it to dense models, enabling sparse
computation during inference. We provide a comprehensive analysis and make our
code available at https://anonymous.4open.science/r/XMoE.
\\ ( https://arxiv.org/abs/2403.18926 ,  8466kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18947
Date: Wed, 21 Feb 2024 15:17:20 GMT   (3050kb,D)

Title: Self-Supervised Interpretable Sensorimotor Learning via Latent
  Functional Modularity
Authors: Hyunki Seong, David Hyunchul Shim
Categories: cs.LG cs.RO
Comments: 10 pages, 6 figures. Accepted for an oral presentation at the AAAI
  2024 Workshop on Explainable AI Approaches for Deep Reinforcement Learning
\\
  We introduce MoNet, a novel method that combines end-to-end learning with
modular network architectures for self-supervised and interpretable
sensorimotor learning. MoNet is composed of three functionally distinct neural
modules: Perception, Planning, and Control. Leveraging its inherent modularity
through a cognition-guided contrastive loss function, MoNet efficiently learns
task-specific decision-making processes in latent space, without requiring
task-level supervision. Moreover, our method incorporates an online post-hoc
explainability approach, which enhances the interpretability of the end-to-end
inferences without a trade-off in sensorimotor performance. In real-world
indoor environments, MoNet demonstrates effective visual autonomous navigation,
surpassing baseline models by 11% to 47% in task specificity analysis. We
further delve into the interpretability of our network through the post-hoc
analysis of perceptual saliency maps and latent decision vectors. This offers
insights into the incorporation of explainable artificial intelligence within
the realm of robotic learning, encompassing both perceptual and behavioral
perspectives.
\\ ( https://arxiv.org/abs/2403.18947 ,  3050kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18953
Date: Mon, 4 Mar 2024 17:35:17 GMT   (6342kb)

Title: Hybridizing Traditional and Next-Generation Reservoir Computing to
  Accurately and Efficiently Forecast Dynamical Systems
Authors: Ravi Chepuri, Dael Amzalag, Thomas Antonsen Jr., Michelle Girvan
Categories: cs.LG
Comments: 10 pages, 7 figures
\\
  Reservoir computers (RCs) are powerful machine learning architectures for
time series prediction. Recently, next generation reservoir computers (NGRCs)
have been introduced, offering distinct advantages over RCs, such as reduced
computational expense and lower data requirements. However, NGRCs have their
own practical difficulties distinct from those of RCs, including sensitivity to
sampling time and type of nonlinearities in the data. Here, we introduce a
hybrid RC-NGRC approach for time series forecasting of complex and chaotic
dynamical systems. We show that our hybrid approach can produce accurate short
term predictions and capture the long term statistics of dynamical systems in
situations where the RC and NGRC components alone are insufficient. The
advantage of the hybrid RC-NGRC approach is most pronounced when both
components are limited in their prediction capabilities, e.g. for a small RC
and a large sampling time in the training data. Under these conditions, we show
for several chaotic systems that the hybrid RC-NGRC method with a small
reservoir ($N \approx 100$) can achieve prediction performance rivaling that of
a pure RC with a much larger reservoir ($N \approx 1000$), illustrating that
the hybrid approach offers significant gains in computational efficiency over
traditional RCs while simultaneously addressing some of the limitations of
NGRCs.
\\ ( https://arxiv.org/abs/2403.18953 ,  6342kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18955
Date: Sun, 3 Mar 2024 13:49:49 GMT   (4119kb,D)

Title: Structurally Prune Anything: Any Architecture, Any Framework, Any Time
Authors: Xun Wang, John Rachwan, Stephan G\"unnemann, Bertrand Charpentier
Categories: cs.LG cs.CV
\\
  Neural network pruning serves as a critical technique for enhancing the
efficiency of deep learning models. Unlike unstructured pruning, which only
sets specific parameters to zero, structured pruning eliminates entire
channels, thus yielding direct computational and storage benefits. However, the
diverse patterns for coupling parameters, such as residual connections and
group convolutions, the diverse deep learning frameworks, and the various time
stages at which pruning can be performed make existing pruning methods less
adaptable to different architectures, frameworks, and pruning criteria. To
address this, we introduce Structurally Prune Anything (SPA), a versatile
structured pruning framework that can prune neural networks with any
architecture, from any framework, and at any stage of training. SPA leverages a
standardized computational graph and ONNX representation to prune diverse
neural network architectures without the need for manual intervention. SPA
employs a group-level importance estimation method, which groups dependent
computational operators, estimates their importance, and prunes unimportant
coupled channels. This enables the transfer of various existing pruning
criteria into a structured group style. As a result, SPA supports pruning at
any time, either before training, after training with fine-tuning, or after
training without fine-tuning. In the context of the latter, we introduce
Optimal Brain SPA (OBSPA), an algorithm that achieves state-of-the-art pruning
results needing neither fine-tuning nor calibration data. In extensive
experiments, SPA shows competitive to state-of-the-art pruning performance
across various architectures, from popular frameworks, at different pruning
times.
\\ ( https://arxiv.org/abs/2403.18955 ,  4119kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18985
Date: Wed, 27 Mar 2024 20:07:39 GMT   (488kb)

Title: Robustness and Visual Explanation for Black Box Image, Video, and ECG
  Signal Classification with Reinforcement Learning
Authors: Soumyendu Sarkar, Ashwin Ramesh Babu, Sajad Mousavi, Vineet Gundecha,
  Avisek Naug, Sahand Ghorbanpour
Categories: cs.LG cs.AI cs.CR cs.CV cs.MA
Comments: AAAI Proceedings reference:
  https://ojs.aaai.org/index.php/AAAI/article/view/30579
Journal-ref: 2024 Proceedings of the AAAI Conference on Artificial Intelligence
DOI: 10.1609/aaai.v38i21.30579
\\
  We present a generic Reinforcement Learning (RL) framework optimized for
crafting adversarial attacks on different model types spanning from ECG signal
analysis (1D), image classification (2D), and video classification (3D). The
framework focuses on identifying sensitive regions and inducing
misclassifications with minimal distortions and various distortion types. The
novel RL method outperforms state-of-the-art methods for all three
applications, proving its efficiency. Our RL approach produces superior
localization masks, enhancing interpretability for image classification and ECG
analysis models. For applications such as ECG analysis, our platform highlights
critical ECG segments for clinicians while ensuring resilience against
prevalent distortions. This comprehensive tool aims to bolster both resilience
with adversarial training and transparency across varied applications and data
types.
\\ ( https://arxiv.org/abs/2403.18985 ,  488kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19009
Date: Wed, 27 Mar 2024 21:02:15 GMT   (204kb,D)

Title: Towards Sustainable SecureML: Quantifying Carbon Footprint of
  Adversarial Machine Learning
Authors: Syed Mhamudul Hasan, Abdur R. Shahid, Ahmed Imteaj
Categories: cs.LG cs.CR
Comments: Accepted at GreenNet Workshop @ IEEE International Conference on
  Communications (IEEE ICC 2024)
\\
  The widespread adoption of machine learning (ML) across various industries
has raised sustainability concerns due to its substantial energy usage and
carbon emissions. This issue becomes more pressing in adversarial ML, which
focuses on enhancing model security against different network-based attacks.
Implementing defenses in ML systems often necessitates additional computational
resources and network security measures, exacerbating their environmental
impacts. In this paper, we pioneer the first investigation into adversarial
ML's carbon footprint, providing empirical evidence connecting greater model
robustness to higher emissions. Addressing the critical need to quantify this
trade-off, we introduce the Robustness Carbon Trade-off Index (RCTI). This
novel metric, inspired by economic elasticity principles, captures the
sensitivity of carbon emissions to changes in adversarial robustness. We
demonstrate the RCTI through an experiment involving evasion attacks, analyzing
the interplay between robustness against attacks, performance, and carbon
emissions.
\\ ( https://arxiv.org/abs/2403.19009 ,  204kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19014
Date: Wed, 27 Mar 2024 21:14:17 GMT   (608kb)

Title: Thelxino\"e: Recognizing Human Emotions Using Pupillometry and Machine
  Learning
Authors: Darlene Barker, Haim Levkowitz
Categories: cs.LG cs.HC
Comments: 14 pages, 9 figures, 1 table, journal
Journal-ref: Machine Learning and Applications: An International Journal
  (MLAIJ), vol. 11, no. 1, pp. 1-14, Mar. 2024
DOI: 10.5121/mlaij.2024.11101
\\
  In this study, we present a method for emotion recognition in Virtual Reality
(VR) using pupillometry. We analyze pupil diameter responses to both visual and
auditory stimuli via a VR headset and focus on extracting key features in the
time-domain, frequency-domain, and time-frequency domain from VR generated
data. Our approach utilizes feature selection to identify the most impactful
features using Maximum Relevance Minimum Redundancy (mRMR). By applying a
Gradient Boosting model, an ensemble learning technique using stacked decision
trees, we achieve an accuracy of 98.8% with feature engineering, compared to
84.9% without it. This research contributes significantly to the Thelxino\"e
framework, aiming to enhance VR experiences by integrating multiple sensor data
for realistic and emotionally resonant touch interactions. Our findings open
new avenues for developing more immersive and interactive VR environments,
paving the way for future advancements in virtual touch technology.
\\ ( https://arxiv.org/abs/2403.19014 ,  608kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19024
Date: Wed, 27 Mar 2024 21:31:46 GMT   (2912kb,D)

Title: Exploiting Symmetry in Dynamics for Model-Based Reinforcement Learning
  with Asymmetric Rewards
Authors: Yasin Sonmez, Neelay Junnarkar, Murat Arcak
Categories: cs.LG cs.AI cs.RO cs.SY eess.SY
\\
  Recent work in reinforcement learning has leveraged symmetries in the model
to improve sample efficiency in training a policy. A commonly used simplifying
assumption is that the dynamics and reward both exhibit the same symmetry.
However, in many real-world environments, the dynamical model exhibits symmetry
independent of the reward model: the reward may not satisfy the same symmetries
as the dynamics. In this paper, we investigate scenarios where only the
dynamics are assumed to exhibit symmetry, extending the scope of problems in
reinforcement learning and learning in control theory where symmetry techniques
can be applied. We use Cartan's moving frame method to introduce a technique
for learning dynamics which, by construction, exhibit specified symmetries. We
demonstrate through numerical experiments that the proposed method learns a
more accurate dynamical model.
\\ ( https://arxiv.org/abs/2403.19024 ,  2912kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19040
Date: Wed, 27 Mar 2024 22:26:50 GMT   (7028kb,D)

Title: Visualizing High-Dimensional Temporal Data Using Direction-Aware t-SNE
Authors: Pavlin G. Poli\v{c}ar and Bla\v{z} Zupan
Categories: cs.LG cs.HC
\\
  Many real-world data sets contain a temporal component or involve transitions
from state to state. For exploratory data analysis, we can represent these
high-dimensional data sets in two-dimensional maps, using embeddings of the
data objects under exploration and representing their temporal relationships
with directed edges. Most existing dimensionality reduction techniques, such as
t-SNE and UMAP, do not take into account the temporal or relational nature of
the data when constructing the embeddings, resulting in temporally cluttered
visualizations that obscure potentially interesting patterns. To address this
problem, we propose two complementary, direction-aware loss terms in the
optimization function of t-SNE that emphasize the temporal aspects of the data,
guiding the optimization and the resulting embedding to reveal temporal
patterns that might otherwise go unnoticed. The Directional Coherence Loss
(DCL) encourages nearby arrows connecting two adjacent time series points to
point in the same direction, while the Edge Length Loss (ELL) penalizes arrows
- which effectively represent time gaps in the visualized embedding - based on
their length. Both loss terms are differentiable and can be easily incorporated
into existing dimensionality reduction techniques. By promoting local
directionality of the directed edges, our procedure produces more temporally
meaningful and less cluttered visualizations. We demonstrate the effectiveness
of our approach on a toy dataset and two real-world datasets.
\\ ( https://arxiv.org/abs/2403.19040 ,  7028kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19050
Date: Wed, 27 Mar 2024 23:10:33 GMT   (219kb,D)

Title: Detecting Generative Parroting through Overfitting Masked Autoencoders
Authors: Saeid Asgari Taghanaki, Joseph Lambourne
Categories: cs.LG cs.AI
\\
  The advent of generative AI models has revolutionized digital content
creation, yet it introduces challenges in maintaining copyright integrity due
to generative parroting, where models mimic their training data too closely.
Our research presents a novel approach to tackle this issue by employing an
overfitted Masked Autoencoder (MAE) to detect such parroted samples
effectively. We establish a detection threshold based on the mean loss across
the training dataset, allowing for the precise identification of parroted
content in modified datasets. Preliminary evaluations demonstrate promising
results, suggesting our method's potential to ensure ethical use and enhance
the legal compliance of generative models.
\\ ( https://arxiv.org/abs/2403.19050 ,  219kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19057
Date: Wed, 27 Mar 2024 23:49:22 GMT   (106kb)

Title: Equity in Healthcare: Analyzing Disparities in Machine Learning
  Predictions of Diabetic Patient Readmissions
Authors: Zainab Al-Zanbouri, Gauri Sharma, Shaina Raza
Categories: cs.LG
\\
  This study investigates how machine learning (ML) models can predict hospital
readmissions for diabetic patients fairly and accurately across different
demographics (age, gender, race). We compared models like Deep Learning,
Generalized Linear Models, Gradient Boosting Machines (GBM), and Naive Bayes.
GBM stood out with an F1-score of 84.3% and accuracy of 82.2%, accurately
predicting readmissions across demographics. A fairness analysis was conducted
across all the models. GBM minimized disparities in predictions, achieving
balanced results across genders and races. It showed low False Discovery Rates
(FDR) (6-7%) and False Positive Rates (FPR) (5%) for both genders.
Additionally, FDRs remained low for racial groups, such as African Americans
(8%) and Asians (7%). Similarly, FPRs were consistent across age groups (4%)
for both patients under 40 and those above 40, indicating its precision and
ability to reduce bias. These findings emphasize the importance of choosing ML
models carefully to ensure both accuracy and fairness for all patients. By
showcasing effectiveness of various models with fairness metrics, this study
promotes personalized medicine and the need for fair ML algorithms in
healthcare. This can ultimately reduce disparities and improve outcomes for
diabetic patients of all backgrounds.
\\ ( https://arxiv.org/abs/2403.19057 ,  106kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19076
Date: Thu, 28 Mar 2024 00:34:56 GMT   (36538kb,D)

Title: Tiny Machine Learning: Progress and Futures
Authors: Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Song Han
Categories: cs.LG cs.AI cs.CV
Comments: IEEE Circuits and Systems Magazine (2023). arXiv admin note: text
  overlap with arXiv:2206.15472
Journal-ref: IEEE Circuits and Systems Magazine, 23(3), pp. 8-34, October 2023
DOI: 10.1109/MCAS.2023.3302182
\\
  Tiny Machine Learning (TinyML) is a new frontier of machine learning. By
squeezing deep learning models into billions of IoT devices and
microcontrollers (MCUs), we expand the scope of AI applications and enable
ubiquitous intelligence. However, TinyML is challenging due to hardware
constraints: the tiny memory resource makes it difficult to hold deep learning
models designed for cloud and mobile platforms. There is also limited compiler
and inference engine support for bare-metal devices. Therefore, we need to
co-design the algorithm and system stack to enable TinyML. In this review, we
will first discuss the definition, challenges, and applications of TinyML. We
then survey the recent progress in TinyML and deep learning on MCUs. Next, we
will introduce MCUNet, showing how we can achieve ImageNet-scale AI
applications on IoT devices with system-algorithm co-design. We will further
extend the solution from inference to training and introduce tiny on-device
training techniques. Finally, we present future directions in this area.
Today's large model might be tomorrow's tiny model. The scope of TinyML should
evolve and adapt over time.
\\ ( https://arxiv.org/abs/2403.19076 ,  36538kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19082
Date: Thu, 28 Mar 2024 01:14:25 GMT   (206kb,D)

Title: Enhancing Conformal Prediction Using E-Test Statistics
Authors: A.A.Balinsky and A.D.Balinsky
Categories: cs.LG cs.AI math.ST stat.TH
\\
  Conformal Prediction (CP) serves as a robust framework that quantifies
uncertainty in predictions made by Machine Learning (ML) models. Unlike
traditional point predictors, CP generates statistically valid prediction
regions, also known as prediction intervals, based on the assumption of data
exchangeability. Typically, the construction of conformal predictions hinges on
p-values. This paper, however, ventures down an alternative path, harnessing
the power of e-test statistics to augment the efficacy of conformal predictions
by introducing a BB-predictor (bounded from the below predictor).
\\ ( https://arxiv.org/abs/2403.19082 ,  206kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19083
Date: Thu, 28 Mar 2024 01:27:10 GMT   (295kb,D)

Title: Improving Cancer Imaging Diagnosis with Bayesian Networks and Deep
  Learning: A Bayesian Deep Learning Approach
Authors: Pei Xi (Alex) Lin
Categories: cs.LG cs.AI eess.IV
\\
  With recent advancements in the development of artificial intelligence
applications using theories and algorithms in machine learning, many accurate
models can be created to train and predict on given datasets. With the
realization of the importance of imaging interpretation in cancer diagnosis,
this article aims to investigate the theory behind Deep Learning and Bayesian
Network prediction models. Based on the advantages and drawbacks of each model,
different approaches will be used to construct a Bayesian Deep Learning Model,
combining the strengths while minimizing the weaknesses. Finally, the
applications and accuracy of the resulting Bayesian Deep Learning approach in
the health industry in classifying images will be analyzed.
\\ ( https://arxiv.org/abs/2403.19083 ,  295kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19143
Date: Thu, 28 Mar 2024 04:35:27 GMT   (1535kb)

Title: Tiny Graph Neural Networks for Radio Resource Management
Authors: Ahmad Ghasemi, Hossein Pishro-Nik
Categories: cs.LG cs.NI eess.SP
Comments: Accepted as a full paper by the tinyML Research Symposium 2024
\\
  The surge in demand for efficient radio resource management has necessitated
the development of sophisticated yet compact neural network architectures. In
this paper, we introduce a novel approach to Graph Neural Networks (GNNs)
tailored for radio resource management by presenting a new architecture: the
Low Rank Message Passing Graph Neural Network (LR-MPGNN). The cornerstone of
LR-MPGNN is the implementation of a low-rank approximation technique that
substitutes the conventional linear layers with their low-rank counterparts.
This innovative design significantly reduces the model size and the number of
parameters. We evaluate the performance of the proposed LR-MPGNN model based on
several key metrics: model size, number of parameters, weighted sum rate of the
communication system, and the distribution of eigenvalues of weight matrices.
Our extensive evaluations demonstrate that the LR-MPGNN model achieves a
sixtyfold decrease in model size, and the number of model parameters can be
reduced by up to 98%. Performance-wise, the LR-MPGNN demonstrates robustness
with a marginal 2% reduction in the best-case scenario in the normalized
weighted sum rate compared to the original MPGNN model. Additionally, the
distribution of eigenvalues of the weight matrices in the LR-MPGNN model is
more uniform and spans a wider range, suggesting a strategic redistribution of
weights.
\\ ( https://arxiv.org/abs/2403.19143 ,  1535kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19149
Date: Thu, 28 Mar 2024 05:07:41 GMT   (423kb,D)

Title: Topological Cycle Graph Attention Network for Brain Functional
  Connectivity
Authors: Jinghan Huang, Nanguang Chen, Anqi Qiu
Categories: cs.LG q-bio.NC
\\
  This study, we introduce a novel Topological Cycle Graph Attention Network
(CycGAT), designed to delineate a functional backbone within brain functional
graph--key pathways essential for signal transmissio--from non-essential,
redundant connections that form cycles around this core structure. We first
introduce a cycle incidence matrix that establishes an independent cycle basis
within a graph, mapping its relationship with edges. We propose a cycle graph
convolution that leverages a cycle adjacency matrix, derived from the cycle
incidence matrix, to specifically filter edge signals in a domain of cycles.
Additionally, we strengthen the representation power of the cycle graph
convolution by adding an attention mechanism, which is further augmented by the
introduction of edge positional encodings in cycles, to enhance the topological
awareness of CycGAT. We demonstrate CycGAT's localization through simulation
and its efficacy on an ABCD study's fMRI data (n=8765), comparing it with
baseline models. CycGAT outperforms these models, identifying a functional
backbone with significantly fewer cycles, crucial for understanding neural
circuits related to general intelligence. Our code will be released once
accepted.
\\ ( https://arxiv.org/abs/2403.19149 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19150
Date: Thu, 28 Mar 2024 05:08:25 GMT   (419kb,D)

Title: Towards Understanding Dual BN In Hybrid Adversarial Training
Authors: Chenshuang Zhang, Chaoning Zhang, Kang Zhang, Axi Niu, Junmo Kim, In
  So Kweon
Categories: cs.LG cs.AI cs.CR cs.CV
Comments: Accepted at TMLR
\\
  There is a growing concern about applying batch normalization (BN) in
adversarial training (AT), especially when the model is trained on both
adversarial samples and clean samples (termed Hybrid-AT). With the assumption
that adversarial and clean samples are from two different domains, a common
practice in prior works is to adopt Dual BN, where BN and BN are used for
adversarial and clean branches, respectively. A popular belief for motivating
Dual BN is that estimating normalization statistics of this mixture
distribution is challenging and thus disentangling it for normalization
achieves stronger robustness. In contrast to this belief, we reveal that
disentangling statistics plays a less role than disentangling affine parameters
in model training. This finding aligns with prior work (Rebuffi et al., 2023),
and we build upon their research for further investigations. We demonstrate
that the domain gap between adversarial and clean samples is not very large,
which is counter-intuitive considering the significant influence of adversarial
perturbation on the model accuracy. We further propose a two-task hypothesis
which serves as the empirical foundation and a unified framework for Hybrid-AT
improvement. We also investigate Dual BN in test-time and reveal that affine
parameters characterize the robustness during inference. Overall, our work
sheds new light on understanding the mechanism of Dual BN in Hybrid-AT and its
underlying justification.
\\ ( https://arxiv.org/abs/2403.19150 ,  419kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19163
Date: Thu, 28 Mar 2024 06:18:12 GMT   (3862kb,D)

Title: D'OH: Decoder-Only random Hypernetworks for Implicit Neural
  Representations
Authors: Cameron Gordon, Lachlan Ewen MacDonald, Hemanth Saratchandran, and
  Simon Lucey
Categories: cs.LG cs.CV
Comments: 29 pages, 17 figures
\\
  Deep implicit functions have been found to be an effective tool for
efficiently encoding all manner of natural signals. Their attractiveness stems
from their ability to compactly represent signals with little to no off-line
training data. Instead, they leverage the implicit bias of deep networks to
decouple hidden redundancies within the signal. In this paper, we explore the
hypothesis that additional compression can be achieved by leveraging the
redundancies that exist between layers. We propose to use a novel run-time
decoder-only hypernetwork - that uses no offline training data - to better
model this cross-layer parameter redundancy. Previous applications of
hyper-networks with deep implicit functions have applied feed-forward
encoder/decoder frameworks that rely on large offline datasets that do not
generalize beyond the signals they were trained on. We instead present a
strategy for the initialization of run-time deep implicit functions for
single-instance signals through a Decoder-Only randomly projected Hypernetwork
(D'OH). By directly changing the dimension of a latent code to approximate a
target implicit neural architecture, we provide a natural way to vary the
memory footprint of neural representations without the costly need for neural
architecture search on a space of alternative low-rate structures.
\\ ( https://arxiv.org/abs/2403.19163 ,  3862kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19165
Date: Thu, 28 Mar 2024 06:24:04 GMT   (688kb)

Title: Evaluating Fair Feature Selection in Machine Learning for Healthcare
Authors: Md Rahat Shahriar Zawad, Peter Washington
Categories: cs.LG cs.CY
Comments: 10 pages, 7 figures
\\
  With the universal adoption of machine learning in healthcare, the potential
for the automation of societal biases to further exacerbate health disparities
poses a significant risk. We explore algorithmic fairness from the perspective
of feature selection. Traditional feature selection methods identify features
for better decision making by removing resource-intensive, correlated, or
non-relevant features but overlook how these factors may differ across
subgroups. To counter these issues, we evaluate a fair feature selection method
that considers equal importance to all demographic groups. We jointly
considered a fairness metric and an error metric within the feature selection
process to ensure a balance between minimizing both bias and global
classification error. We tested our approach on three publicly available
healthcare datasets. On all three datasets, we observed improvements in
fairness metrics coupled with a minimal degradation of balanced accuracy. Our
approach addresses both distributive and procedural fairness within the fair
machine learning context.
\\ ( https://arxiv.org/abs/2403.19165 ,  688kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19211
Date: Thu, 28 Mar 2024 08:19:33 GMT   (3442kb,D)

Title: Dual-Personalizing Adapter for Federated Foundation Models
Authors: Yiyuan Yang, Guodong Long, Tao Shen, Jing Jiang, Michael Blumenstein
Categories: cs.LG cs.AI cs.CL
\\
  Recently, foundation models, particularly large language models (LLMs), have
demonstrated an impressive ability to adapt to various tasks by fine-tuning
large amounts of instruction data. Notably, federated foundation models emerge
as a privacy preservation method to fine-tune models collaboratively under
federated learning (FL) settings by leveraging many distributed datasets with
non-IID data. To alleviate communication and computation overhead,
parameter-efficient methods are introduced for efficiency, and some research
adapted personalization methods to federated foundation models for better user
preferences alignment. However, a critical gap in existing research is the
neglect of test-time distribution shifts in real-world applications. Therefore,
to bridge this gap, we propose a new setting, termed test-time personalization,
which not only concentrates on the targeted local task but also extends to
other tasks that exhibit test-time distribution shifts. To address challenges
in this new setting, we explore a simple yet effective solution to learn a
comprehensive foundation model. Specifically, a dual-personalizing adapter
architecture (FedDPA) is proposed, comprising a global adapter and a local
adapter for addressing test-time distribution shifts and personalization,
respectively. Additionally, we introduce an instance-wise dynamic weighting
mechanism to optimize the balance between the global and local adapters,
enhancing overall performance. The effectiveness of the proposed method has
been evaluated on benchmark datasets across different NLP tasks.
\\ ( https://arxiv.org/abs/2403.19211 ,  3442kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19243
Date: Thu, 28 Mar 2024 08:58:20 GMT   (10614kb,D)

Title: Sine Activated Low-Rank Matrices for Parameter Efficient Learning
Authors: Yiping Ji, Hemanth Saratchandran, Cameron Gordon, Zeyu Zhang, Simon
  Lucey
Categories: cs.LG cs.CV cs.NE
Comments: The first two authors contributed equally
\\
  Low-rank decomposition has emerged as a vital tool for enhancing parameter
efficiency in neural network architectures, gaining traction across diverse
applications in machine learning. These techniques significantly lower the
number of parameters, striking a balance between compactness and performance.
However, a common challenge has been the compromise between parameter
efficiency and the accuracy of the model, where reduced parameters often lead
to diminished accuracy compared to their full-rank counterparts. In this work,
we propose a novel theoretical framework that integrates a sinusoidal function
within the low-rank decomposition process. This approach not only preserves the
benefits of the parameter efficiency characteristic of low-rank methods but
also increases the decomposition's rank, thereby enhancing model accuracy. Our
method proves to be an adaptable enhancement for existing low-rank models, as
evidenced by its successful application in Vision Transformers (ViT), Large
Language Models (LLMs), Neural Radiance Fields (NeRF), and 3D shape modeling.
This demonstrates the wide-ranging potential and efficiency of our proposed
technique.
\\ ( https://arxiv.org/abs/2403.19243 ,  10614kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19246
Date: Thu, 28 Mar 2024 09:06:23 GMT   (590kb,D)

Title: MPXGAT: An Attention based Deep Learning Model for Multiplex Graphs
  Embedding
Authors: Marco Bongiovanni, Luca Gallo, Roberto Grasso, Alfredo Pulvirenti
Categories: cs.LG cs.DM cs.SI
\\
  Graph representation learning has rapidly emerged as a pivotal field of
study. Despite its growing popularity, the majority of research has been
confined to embedding single-layer graphs, which fall short in representing
complex systems with multifaceted relationships. To bridge this gap, we
introduce MPXGAT, an innovative attention-based deep learning model tailored to
multiplex graph embedding. Leveraging the robustness of Graph Attention
Networks (GATs), MPXGAT captures the structure of multiplex networks by
harnessing both intra-layer and inter-layer connections. This exploitation
facilitates accurate link prediction within and across the network's multiple
layers. Our comprehensive experimental evaluation, conducted on various
benchmark datasets, confirms that MPXGAT consistently outperforms
state-of-the-art competing algorithms.
\\ ( https://arxiv.org/abs/2403.19246 ,  590kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19253
Date: Thu, 28 Mar 2024 09:20:15 GMT   (979kb,D)

Title: Inferring Latent Temporal Sparse Coordination Graph for Multi-Agent
  Reinforcement Learning
Authors: Wei Duan, Jie Lu, Junyu Xuan
Categories: cs.LG cs.MA
\\
  Effective agent coordination is crucial in cooperative Multi-Agent
Reinforcement Learning (MARL). While agent cooperation can be represented by
graph structures, prevailing graph learning methods in MARL are limited. They
rely solely on one-step observations, neglecting crucial historical
experiences, leading to deficient graphs that foster redundant or detrimental
information exchanges. Additionally, high computational demands for action-pair
calculations in dense graphs impede scalability. To address these challenges,
we propose inferring a Latent Temporal Sparse Coordination Graph (LTS-CG) for
MARL. The LTS-CG leverages agents' historical observations to calculate an
agent-pair probability matrix, where a sparse graph is sampled from and used
for knowledge exchange between agents, thereby simultaneously capturing agent
dependencies and relation uncertainty. The computational complexity of this
procedure is only related to the number of agents. This graph learning process
is further augmented by two innovative characteristics: Predict-Future, which
enables agents to foresee upcoming observations, and Infer-Present, ensuring a
thorough grasp of the environmental context from limited data. These features
allow LTS-CG to construct temporal graphs from historical and real-time
information, promoting knowledge exchange during policy learning and effective
collaboration. Graph learning and agent training occur simultaneously in an
end-to-end manner. Our demonstrated results on the StarCraft II benchmark
underscore LTS-CG's superior performance.
\\ ( https://arxiv.org/abs/2403.19253 ,  979kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19273
Date: Thu, 28 Mar 2024 09:57:50 GMT   (676kb,D)

Title: A Machine Learning Approach for Crop Yield and Disease Prediction
  Integrating Soil Nutrition and Weather Factors
Authors: Forkan Uddin Ahmed (1), Annesha Das (1) and Md Zubair (1) ((1)
  Department of Computer Science and Engineering, Chittagong University of
  Engineering & Technology, Chattogram, Bangladesh)
Categories: cs.LG cs.AI
Comments: This paper was presented to the IEEE conference, "2024 International
  Conference on Advances in Computing, Communication, Electrical, and Smart
  Systems (iCACCESS), 8-9 March, Dhaka, Bangladesh"
\\
  The development of an intelligent agricultural decision-supporting system for
crop selection and disease forecasting in Bangladesh is the main objective of
this work. The economy of the nation depends heavily on agriculture. However,
choosing crops with better production rates and efficiently controlling crop
disease are obstacles that farmers have to face. These issues are addressed in
this research by utilizing machine learning methods and real-world datasets.
The recommended approach uses a variety of datasets on the production of crops,
soil conditions, agro-meteorological regions, crop disease, and meteorological
factors. These datasets offer insightful information on disease trends, soil
nutrition demand of crops, and agricultural production history. By
incorporating this knowledge, the model first recommends the list of primarily
selected crops based on the soil nutrition of a particular user location. Then
the predictions of meteorological variables like temperature, rainfall, and
humidity are made using SARIMAX models. These weather predictions are then used
to forecast the possibilities of diseases for the primary crops list by
utilizing the support vector classifier. Finally, the developed model makes use
of the decision tree regression model to forecast crop yield and provides a
final crop list along with associated possible disease forecast. Utilizing the
outcome of the model, farmers may choose the best productive crops as well as
prevent crop diseases and reduce output losses by taking preventive actions.
Consequently, planning and decision-making processes are supported and farmers
can predict possible crop yields. Overall, by offering a detailed decision
support system for crop selection and disease prediction, this work can play a
vital role in advancing agricultural practices in Bangladesh.
\\ ( https://arxiv.org/abs/2403.19273 ,  676kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19289
Date: Thu, 28 Mar 2024 10:19:36 GMT   (276kb,D)

Title: Graph Neural Networks for Treatment Effect Prediction
Authors: George Panagopoulos, Daniele Malitesta, Fragkiskos D. Malliaros, Jun
  Pang
Categories: cs.LG cs.AI stat.ME
\\
  Estimating causal effects in e-commerce tends to involve costly treatment
assignments which can be impractical in large-scale settings. Leveraging
machine learning to predict such treatment effects without actual intervention
is a standard practice to diminish the risk. However, existing methods for
treatment effect prediction tend to rely on training sets of substantial size,
which are built from real experiments and are thus inherently risky to create.
In this work we propose a graph neural network to diminish the required
training set size, relying on graphs that are common in e-commerce data.
Specifically, we view the problem as node regression with a restricted number
of labeled instances, develop a two-model neural architecture akin to previous
causal effect estimators, and test varying message-passing layers for encoding.
Furthermore, as an extra step, we combine the model with an acquisition
function to guide the creation of the training set in settings with extremely
low experimental budget. The framework is flexible since each step can be used
separately with other models or policies. The experiments on real large-scale
networks indicate a clear advantage of our methodology over the state of the
art, which in many cases performs close to random underlining the need for
models that can generalize with limited labeled samples to reduce experimental
risks.
\\ ( https://arxiv.org/abs/2403.19289 ,  276kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19326
Date: Thu, 28 Mar 2024 11:33:02 GMT   (3105kb,D)

Title: MedBN: Robust Test-Time Adaptation against Malicious Test Samples
Authors: Hyejin Park, Jeongyeon Hwang, Sunung Mun, Sangdon Park, Jungseul Ok
Categories: cs.LG cs.CR cs.CV
Comments: Accepted to CVPR 2024
\\
  Test-time adaptation (TTA) has emerged as a promising solution to address
performance decay due to unforeseen distribution shifts between training and
test data. While recent TTA methods excel in adapting to test data variations,
such adaptability exposes a model to vulnerability against malicious examples,
an aspect that has received limited attention. Previous studies have uncovered
security vulnerabilities within TTA even when a small proportion of the test
batch is maliciously manipulated. In response to the emerging threat, we
propose median batch normalization (MedBN), leveraging the robustness of the
median for statistics estimation within the batch normalization layer during
test-time inference. Our method is algorithm-agnostic, thus allowing seamless
integration with existing TTA frameworks. Our experimental results on benchmark
datasets, including CIFAR10-C, CIFAR100-C and ImageNet-C, consistently
demonstrate that MedBN outperforms existing approaches in maintaining robust
performance across different attack scenarios, encompassing both instant and
cumulative attacks. Through extensive experiments, we show that our approach
sustains the performance even in the absence of attacks, achieving a practical
balance between robustness and performance.
\\ ( https://arxiv.org/abs/2403.19326 ,  3105kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19339
Date: Thu, 28 Mar 2024 11:57:06 GMT   (3839kb,D)

Title: An Interactive Human-Machine Learning Interface for Collecting and
  Learning from Complex Annotations
Authors: Jonathan Erskine, Matt Clifford, Alexander Hepburn, Ra\'ul
  Santos-Rodr\'iguez
Categories: cs.LG cs.HC
Comments: 4 pages, 2 figures, Submitted to IJCAI 2024 Demonstration Track
\\
  Human-Computer Interaction has been shown to lead to improvements in machine
learning systems by boosting model performance, accelerating learning and
building user confidence. In this work, we aim to alleviate the expectation
that human annotators adapt to the constraints imposed by traditional labels by
allowing for extra flexibility in the form that supervision information is
collected. For this, we propose a human-machine learning interface for binary
classification tasks which enables human annotators to utilise counterfactual
examples to complement standard binary labels as annotations for a dataset.
Finally we discuss the challenges in future extensions of this work.
\\ ( https://arxiv.org/abs/2403.19339 ,  3839kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19355
Date: Thu, 28 Mar 2024 12:11:29 GMT   (1648kb)

Title: Artificial Intelligence (AI) Based Prediction of Mortality, for COVID-19
  Patients
Authors: Mahbubunnabi Tamala, Mohammad Marufur Rahmanb, Maryam Alhasimc,
  Mobarak Al Mulhimd, Mohamed Derichee
Categories: cs.LG
Comments: Submitted to Biocybernetics and Biomedical Engineering, 22 March,
  2024
\\
  For severely affected COVID-19 patients, it is crucial to identify high-risk
patients and predict survival and need for intensive care (ICU). Most of the
proposed models are not well reported making them less reproducible and prone
to high risk of bias particularly in presence of imbalance data/class. In this
study, the performances of nine machine and deep learning algorithms in
combination with two widely used feature selection methods were investigated to
predict last status representing mortality, ICU requirement, and ventilation
days. Fivefold cross-validation was used for training and validation purposes.
To minimize bias, the training and testing sets were split maintaining similar
distributions. Only 10 out of 122 features were found to be useful in
prediction modelling with Acute kidney injury during hospitalization feature
being the most important one. The algorithms performances depend on feature
numbers and data pre-processing techniques. LSTM performs the best in
predicting last status and ICU requirement with 90%, 92%, 86% and 95% accuracy,
sensitivity, specificity, and AUC respectively. DNN performs the best in
predicting Ventilation days with 88% accuracy. Considering all the factors and
limitations including absence of exact time point of clinical onset, LSTM with
carefully selected features can accurately predict last status and ICU
requirement. DNN performs the best in predicting Ventilation days. Appropriate
machine learning algorithm with carefully selected features and balance data
can accurately predict mortality, ICU requirement and ventilation support. Such
model can be very useful in emergency and pandemic where prompt and precise
\\ ( https://arxiv.org/abs/2403.19355 ,  1648kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19405
Date: Thu, 28 Mar 2024 13:29:29 GMT   (615kb,D)

Title: Tabular Learning: Encoding for Entity and Context Embeddings
Authors: Fredy Reusser
Categories: cs.LG cs.AI cs.CE
\\
  Examining the effect of different encoding techniques on entity and context
embeddings, the goal of this work is to challenge commonly used Ordinal
encoding for tabular learning. Applying different preprocessing methods and
network architectures over several datasets resulted in a benchmark on how the
encoders influence the learning outcome of the networks. By keeping the test,
validation and training data consistent, results have shown that ordinal
encoding is not the most suited encoder for categorical data in terms of
preprocessing the data and thereafter, classifying the target variable
correctly. A better outcome was achieved, encoding the features based on string
similarities by computing a similarity matrix as input for the network. This is
the case for both, entity and context embeddings, where the transformer
architecture showed improved performance for Ordinal and Similarity encoding
with regard to multi-label classification tasks.
\\ ( https://arxiv.org/abs/2403.19405 ,  615kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19418
Date: Thu, 28 Mar 2024 13:49:43 GMT   (340kb,D)

Title: Constants of Motion for Conserved and Non-conserved Dynamics
Authors: Michael F. Zimmer
Categories: cs.LG nlin.CD
Comments: 14 pages, 5 figures
\\
  This paper begins with a dynamical model that was obtained by applying a
machine learning technique (FJet) to time-series data; this dynamical model is
then analyzed with Lie symmetry techniques to obtain constants of motion. This
analysis is performed on both the conserved and non-conserved cases of the 1D
and 2D harmonic oscillators. For the 1D oscillator, constants are found in the
cases where the system is underdamped, overdamped, and critically damped. The
novel existence of such a constant for a non-conserved model is interpreted as
a manifestation of the conservation of energy of the {\em total} system (i.e.,
oscillator plus dissipative environment). For the 2D oscillator, constants are
found for the isotropic and anisotropic cases, including when the frequencies
are incommensurate; it is also generalized to arbitrary dimensions. In
addition, a constant is identified which generalizes angular momentum for all
ratios of the frequencies. The approach presented here can produce {\em
multiple} constants of motion from a {\em single}, generic data set.
\\ ( https://arxiv.org/abs/2403.19418 ,  340kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19419
Date: Thu, 28 Mar 2024 13:50:24 GMT   (1053kb,D)

Title: Fairness in Ranking: Robustness through Randomization without the
  Protected Attribute
Authors: Andrii Kliachkin and Eleni Psaroudaki and Jakub Marecek and Dimitris
  Fotakis
Categories: cs.LG cs.AI cs.CY
\\
  There has been great interest in fairness in machine learning, especially in
relation to classification problems. In ranking-related problems, such as in
online advertising, recommender systems, and HR automation, much work on
fairness remains to be done. Two complications arise: first, the protected
attribute may not be available in many applications. Second, there are multiple
measures of fairness of rankings, and optimization-based methods utilizing a
single measure of fairness of rankings may produce rankings that are unfair
with respect to other measures. In this work, we propose a randomized method
for post-processing rankings, which do not require the availability of the
protected attribute. In an extensive numerical study, we show the robustness of
our methods with respect to P-Fairness and effectiveness with respect to
Normalized Discounted Cumulative Gain (NDCG) from the baseline ranking,
improving on previously proposed methods.
\\ ( https://arxiv.org/abs/2403.19419 ,  1053kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19421
Date: Thu, 28 Mar 2024 13:52:12 GMT   (4123kb,D)

Title: Scaling up ridge regression for brain encoding in a massive individual
  fMRI dataset
Authors: Sana Ahmadi and Pierre Bellec and Tristan Glatard
Categories: cs.LG cs.AI q-bio.NC q-bio.QM
\\
  Brain encoding with neuroimaging data is an established analysis aimed at
predicting human brain activity directly from complex stimuli features such as
movie frames. Typically, these features are the latent space representation
from an artificial neural network, and the stimuli are image, audio, or text
inputs. Ridge regression is a popular prediction model for brain encoding due
to its good out-of-sample generalization performance. However, training a ridge
regression model can be highly time-consuming when dealing with large-scale
deep functional magnetic resonance imaging (fMRI) datasets that include many
space-time samples of brain activity. This paper evaluates different
parallelization techniques to reduce the training time of brain encoding with
ridge regression on the CNeuroMod Friends dataset, one of the largest deep fMRI
resource currently available. With multi-threading, our results show that the
Intel Math Kernel Library (MKL) significantly outperforms the OpenBLAS library,
being 1.9 times faster using 32 threads on a single machine. We then evaluated
the Dask multi-CPU implementation of ridge regression readily available in
scikit-learn (MultiOutput), and we proposed a new "batch" version of Dask
parallelization, motivated by a time complexity analysis. In line with our
theoretical analysis, MultiOutput parallelization was found to be impractical,
i.e., slower than multi-threading on a single machine. In contrast, the
Batch-MultiOutput regression scaled well across compute nodes and threads,
providing speed-ups of up to 33 times with 8 compute nodes and 32 threads
compared to a single-threaded scikit-learn execution. Batch parallelization
using Dask thus emerges as a scalable approach for brain encoding with ridge
regression on high-performance computing systems using scikit-learn and large
fMRI datasets.
\\ ( https://arxiv.org/abs/2403.19421 ,  4123kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19442
Date: Thu, 28 Mar 2024 14:11:40 GMT   (822kb,D)

Title: Exploiting Individual Graph Structures to Enhance Ecological Momentary
  Assessment (EMA) Forecasting
Authors: Mandani Ntekouli, Gerasimos Spanakis, Lourens Waldorp, Anne Roefs
Categories: cs.LG
Comments: 9 pages, 3 figures, 2024 IEEE 40th International Conference on Data
  Engineering Workshops
\\
  In the evolving field of psychopathology, the accurate assessment and
forecasting of data derived from Ecological Momentary Assessment (EMA) is
crucial. EMA offers contextually-rich psychopathological measurements over
time, that practically lead to Multivariate Time Series (MTS) data. Thus, many
challenges arise in analysis from the temporal complexities inherent in
emotional, behavioral, and contextual EMA data as well as their
inter-dependencies. To address both of these aspects, this research
investigates the performance of Recurrent and Temporal Graph Neural Networks
(GNNs). Overall, GNNs, by incorporating additional information from graphs
reflecting the inner relationships between the variables, notably enhance the
results by decreasing the Mean Squared Error (MSE) to 0.84 compared to the
baseline LSTM model at 1.02. Therefore, the effect of constructing graphs with
different characteristics on GNN performance is also explored. Additionally,
GNN-learned graphs, which are dynamically refined during the training process,
were evaluated. Using such graphs showed a similarly good performance. Thus,
graph learning proved also promising for other GNN methods, potentially
refining the pre-defined graphs.
\\ ( https://arxiv.org/abs/2403.19442 ,  822kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19444
Date: Thu, 28 Mar 2024 14:15:13 GMT   (12576kb,D)

Title: Transparent and Clinically Interpretable AI for Lung Cancer Detection in
  Chest X-Rays
Authors: Amy Rafferty and Rishi Ramaesh and Ajitha Rajan
Categories: cs.LG cs.CV
Comments: 12 pages, 10 figures
\\
  The rapidly advancing field of Explainable Artificial Intelligence (XAI) aims
to tackle the issue of trust regarding the use of complex black-box deep
learning models in real-world applications. Existing post-hoc XAI techniques
have recently been shown to have poor performance on medical data, producing
unreliable explanations which are infeasible for clinical use. To address this,
we propose an ante-hoc approach based on concept bottleneck models which
introduces for the first time clinical concepts into the classification
pipeline, allowing the user valuable insight into the decision-making process.
On a large public dataset of chest X-rays and associated medical reports, we
focus on the binary classification task of lung cancer detection. Our approach
yields improved classification performance in lung cancer detection when
compared to baseline deep learning models (F1 > 0.9), while also generating
clinically relevant and more reliable explanations than existing techniques. We
evaluate our approach against post-hoc image XAI techniques LIME and SHAP, as
well as CXR-LLaVA, a recent textual XAI tool which operates in the context of
question answering on chest X-rays.
\\ ( https://arxiv.org/abs/2403.19444 ,  12576kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19462
Date: Thu, 28 Mar 2024 14:34:02 GMT   (225kb,D)

Title: Offline Imitation Learning from Multiple Baselines with Applications to
  Compiler Optimization
Authors: Teodor V. Marinov, Alekh Agarwal, Mircea Trofin
Categories: cs.LG cs.PL
\\
  This work studies a Reinforcement Learning (RL) problem in which we are given
a set of trajectories collected with K baseline policies. Each of these
policies can be quite suboptimal in isolation, and have strong performance in
complementary parts of the state space. The goal is to learn a policy which
performs as well as the best combination of baselines on the entire state
space. We propose a simple imitation learning based algorithm, show a sample
complexity bound on its accuracy and prove that the the algorithm is minimax
optimal by showing a matching lower bound. Further, we apply the algorithm in
the setting of machine learning guided compiler optimization to learn policies
for inlining programs with the objective of creating a small binary. We
demonstrate that we can learn a policy that outperforms an initial policy
learned via standard RL through a few iterations of our approach.
\\ ( https://arxiv.org/abs/2403.19462 ,  225kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19480
Date: Thu, 28 Mar 2024 15:08:51 GMT   (39kb)

Title: $H$-Consistency Guarantees for Regression
Authors: Anqi Mao, Mehryar Mohri, Yutao Zhong
Categories: cs.LG stat.ML
\\
  We present a detailed study of $H$-consistency bounds for regression. We
first present new theorems that generalize the tools previously given to
establish $H$-consistency bounds. This generalization proves essential for
analyzing $H$-consistency bounds specific to regression. Next, we prove a
series of novel $H$-consistency bounds for surrogate loss functions of the
squared loss, under the assumption of a symmetric distribution and a bounded
hypothesis set. This includes positive results for the Huber loss, all $\ell_p$
losses, $p \geq 1$, the squared $\epsilon$-insensitive loss, as well as a
negative result for the $\epsilon$-insensitive loss used in squared Support
Vector Regression (SVR). We further leverage our analysis of $H$-consistency
for regression and derive principled surrogate losses for adversarial
regression (Section 5). This readily establishes novel algorithms for
adversarial regression, for which we report favorable experimental results in
Section 6.
\\ ( https://arxiv.org/abs/2403.19480 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19494
Date: Thu, 28 Mar 2024 15:26:38 GMT   (36kb)

Title: Regression with Multi-Expert Deferral
Authors: Anqi Mao, Mehryar Mohri, Yutao Zhong
Categories: cs.LG stat.ML
\\
  Learning to defer with multiple experts is a framework where the learner can
choose to defer the prediction to several experts. While this problem has
received significant attention in classification contexts, it presents unique
challenges in regression due to the infinite and continuous nature of the label
space. In this work, we introduce a novel framework of regression with
deferral, which involves deferring the prediction to multiple experts. We
present a comprehensive analysis for both the single-stage scenario, where
there is simultaneous learning of predictor and deferral functions, and the
two-stage scenario, which involves a pre-trained predictor with a learned
deferral function. We introduce new surrogate loss functions for both scenarios
and prove that they are supported by $H$-consistency bounds. These bounds
provide consistency guarantees that are stronger than Bayes consistency, as
they are non-asymptotic and hypothesis set-specific. Our framework is
versatile, applying to multiple experts, accommodating any bounded regression
losses, addressing both instance-dependent and label-dependent costs, and
supporting both single-stage and two-stage methods. A by-product is that our
single-stage formulation includes the recent regression with abstention
framework (Cheng et al., 2023) as a special case, where only a single expert,
the squared loss and a label-independent cost are considered. Minimizing our
proposed loss functions directly leads to novel algorithms for regression with
deferral. We report the results of extensive experiments showing the
effectiveness of our proposed algorithms.
\\ ( https://arxiv.org/abs/2403.19494 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19499
Date: Thu, 28 Mar 2024 15:29:19 GMT   (20047kb,D)

Title: Client-supervised Federated Learning: Towards One-model-for-all
  Personalization
Authors: Peng Yan, Guodong Long
Categories: cs.LG
\\
  Personalized Federated Learning (PerFL) is a new machine learning paradigm
that delivers personalized models for diverse clients under federated learning
settings. Most PerFL methods require extra learning processes on a client to
adapt a globally shared model to the client-specific personalized model using
its own local data. However, the model adaptation process in PerFL is still an
open challenge in the stage of model deployment and test time. This work
tackles the challenge by proposing a novel federated learning framework to
learn only one robust global model to achieve competitive performance to those
personalized models on unseen/test clients in the FL system. Specifically, we
design a new Client-Supervised Federated Learning (FedCS) to unravel clients'
bias on instances' latent representations so that the global model can learn
both client-specific and client-agnostic knowledge. Experimental study shows
that the FedCS can learn a robust FL global model for the changing data
distributions of unseen/test clients. The FedCS's global model can be directly
deployed to the test clients while achieving comparable performance to other
personalized FL methods that require model adaptation.
\\ ( https://arxiv.org/abs/2403.19499 ,  20047kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19500
Date: Thu, 28 Mar 2024 15:29:30 GMT   (307kb,D)

Title: Tensor Network-Constrained Kernel Machines as Gaussian Processes
Authors: Frederiek Wesel, Kim Batselier
Categories: cs.LG stat.ML
ACM-class: I.5.0
\\
  Tensor Networks (TNs) have recently been used to speed up kernel machines by
constraining the model weights, yielding exponential computational and storage
savings. In this paper we prove that the outputs of Canonical Polyadic
Decomposition (CPD) and Tensor Train (TT)-constrained kernel machines recover a
Gaussian Process (GP), which we fully characterize, when placing i.i.d. priors
over their parameters. We analyze the convergence of both CPD and
TT-constrained models, and show how TT yields models exhibiting more GP
behavior compared to CPD, for the same number of model parameters. We
empirically observe this behavior in two numerical experiments where we
respectively analyze the convergence to the GP and the performance at
prediction. We thereby establish a connection between TN-constrained kernel
machines and GPs.
\\ ( https://arxiv.org/abs/2403.19500 ,  307kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19507
Date: Thu, 28 Mar 2024 15:41:41 GMT   (23086kb,D)

Title: SineNet: Learning Temporal Dynamics in Time-Dependent Partial
  Differential Equations
Authors: Xuan Zhang, Jacob Helwig, Yuchao Lin, Yaochen Xie, Cong Fu, Stephan
  Wojtowytsch, Shuiwang Ji
Categories: cs.LG
Comments: The Twelfth International Conference on Learning Representations
\\
  We consider using deep neural networks to solve time-dependent partial
differential equations (PDEs), where multi-scale processing is crucial for
modeling complex, time-evolving dynamics. While the U-Net architecture with
skip connections is commonly used by prior studies to enable multi-scale
processing, our analysis shows that the need for features to evolve across
layers results in temporally misaligned features in skip connections, which
limits the model's performance. To address this limitation, we propose SineNet,
consisting of multiple sequentially connected U-shaped network blocks, referred
to as waves. In SineNet, high-resolution features are evolved progressively
through multiple stages, thereby reducing the amount of misalignment within
each stage. We furthermore analyze the role of skip connections in enabling
both parallel and sequential processing of multi-scale information. Our method
is rigorously tested on multiple PDE datasets, including the Navier-Stokes
equations and shallow water equations, showcasing the advantages of our
proposed approach over conventional U-Nets with a comparable parameter budget.
We further demonstrate that increasing the number of waves in SineNet while
maintaining the same number of parameters leads to a monotonically improved
performance. The results highlight the effectiveness of SineNet and the
potential of our approach in advancing the state-of-the-art in neural PDE
solver design. Our code is available as part of AIRS
(https://github.com/divelab/AIRS).
\\ ( https://arxiv.org/abs/2403.19507 ,  23086kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19522
Date: Thu, 28 Mar 2024 15:57:20 GMT   (2971kb,D)

Title: Model Stock: All we need is just a few fine-tuned models
Authors: Dong-Hwan Jang, Sangdoo Yun, Dongyoon Han
Categories: cs.LG cs.CV
Comments: Code at https://github.com/naver-ai/model-stock
\\
  This paper introduces an efficient fine-tuning method for large pre-trained
models, offering strong in-distribution (ID) and out-of-distribution (OOD)
performance. Breaking away from traditional practices that need a multitude of
fine-tuned models for averaging, our approach employs significantly fewer
models to achieve final weights yet yield superior accuracy. Drawing from key
insights in the weight space of fine-tuned weights, we uncover a strong link
between the performance and proximity to the center of weight space. Based on
this, we introduce a method that approximates a center-close weight using only
two fine-tuned models, applicable during or after training. Our innovative
layer-wise weight averaging technique surpasses state-of-the-art model methods
such as Model Soup, utilizing only two fine-tuned models. This strategy can be
aptly coined Model Stock, highlighting its reliance on selecting a minimal
number of models to draw a more optimized-averaged model. We demonstrate the
efficacy of Model Stock with fine-tuned models based upon pre-trained CLIP
architectures, achieving remarkable performance on both ID and OOD tasks on the
standard benchmarks, all while barely bringing extra computational demands. Our
code and pre-trained models are available at
https://github.com/naver-ai/model-stock.
\\ ( https://arxiv.org/abs/2403.19522 ,  2971kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19546
Date: Thu, 28 Mar 2024 16:27:26 GMT   (1268kb,D)

Title: Croissant: A Metadata Format for ML-Ready Datasets
Authors: Mubashara Akhtar, Omar Benjelloun, Costanza Conforti, Joan
  Giner-Miguelez, Nitisha Jain, Michael Kuchnik, Quentin Lhoest, Pierre
  Marcenac, Manil Maskey, Peter Mattson, Luis Oala, Pierre Ruyssen, Rajat
  Shinde, Elena Simperl, Goeffry Thomas, Slava Tykhonov, Joaquin Vanschoren,
  Steffen Vogler, Carole-Jean Wu
Categories: cs.LG cs.AI cs.DB cs.IR
Comments: Preprint. Contributors listed in alphabetical order
\\
  Data is a critical resource for Machine Learning (ML), yet working with data
remains a key friction point. This paper introduces Croissant, a metadata
format for datasets that simplifies how data is used by ML tools and
frameworks. Croissant makes datasets more discoverable, portable and
interoperable, thereby addressing significant challenges in ML data management
and responsible AI. Croissant is already supported by several popular dataset
repositories, spanning hundreds of thousands of datasets, ready to be loaded
into the most popular ML frameworks.
\\ ( https://arxiv.org/abs/2403.19546 ,  1268kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19561
Date: Thu, 28 Mar 2024 16:46:53 GMT   (617kb,D)

Title: Self-Improved Learning for Scalable Neural Combinatorial Optimization
Authors: Fu Luo, Xi Lin, Zhenkun Wang, Tong Xialiang, Mingxuan Yuan, Qingfu
  Zhang
Categories: cs.LG cs.AI
\\
  The end-to-end neural combinatorial optimization (NCO) method shows promising
performance in solving complex combinatorial optimization problems without the
need for expert design. However, existing methods struggle with large-scale
problems, hindering their practical applicability. To overcome this limitation,
this work proposes a novel Self-Improved Learning (SIL) method for better
scalability of neural combinatorial optimization. Specifically, we develop an
efficient self-improved mechanism that enables direct model training on
large-scale problem instances without any labeled data. Powered by an
innovative local reconstruction approach, this method can iteratively generate
better solutions by itself as pseudo-labels to guide efficient model training.
In addition, we design a linear complexity attention mechanism for the model to
efficiently handle large-scale combinatorial problem instances with low
computation overhead. Comprehensive experiments on the Travelling Salesman
Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to
100K nodes in both uniform and real-world distributions demonstrate the
superior scalability of our method.
\\ ( https://arxiv.org/abs/2403.19561 ,  617kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19570
Date: Thu, 28 Mar 2024 16:52:47 GMT   (1413kb,D)

Title: GrINd: Grid Interpolation Network for Scattered Observations
Authors: Andrzej Dulny, Paul Heinisch, Andreas Hotho, Anna Krause
Categories: cs.LG
\\
  Predicting the evolution of spatiotemporal physical systems from sparse and
scattered observational data poses a significant challenge in various
scientific domains. Traditional methods rely on dense grid-structured data,
limiting their applicability in scenarios with sparse observations. To address
this challenge, we introduce GrINd (Grid Interpolation Network for Scattered
Observations), a novel network architecture that leverages the high-performance
of grid-based models by mapping scattered observations onto a high-resolution
grid using a Fourier Interpolation Layer. In the high-resolution space, a
NeuralPDE-class model predicts the system's state at future timepoints using
differentiable ODE solvers and fully convolutional neural networks
parametrizing the system's dynamics. We empirically evaluate GrINd on the
DynaBench benchmark dataset, comprising six different physical systems observed
at scattered locations, demonstrating its state-of-the-art performance compared
to existing models. GrINd offers a promising approach for forecasting physical
systems from sparse, scattered observational data, extending the applicability
of deep learning methods to real-world scenarios with limited data
availability.
\\ ( https://arxiv.org/abs/2403.19570 ,  1413kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19572
Date: Thu, 28 Mar 2024 16:56:39 GMT   (2657kb,D)

Title: Swarm Characteristics Classification Using Neural Networks
Authors: Donald W. Peltier III, Isaac Kaminer, Abram Clark, Marko Orescanin
Categories: cs.LG
\\
  Understanding the characteristics of swarming autonomous agents is critical
for defense and security applications. This article presents a study on using
supervised neural network time series classification (NN TSC) to predict key
attributes and tactics of swarming autonomous agents for military contexts.
Specifically, NN TSC is applied to infer two binary attributes - communication
and proportional navigation - which combine to define four mutually exclusive
swarm tactics. We identify a gap in literature on using NNs for swarm
classification and demonstrate the effectiveness of NN TSC in rapidly deducing
intelligence about attacking swarms to inform counter-maneuvers. Through
simulated swarm-vs-swarm engagements, we evaluate NN TSC performance in terms
of observation window requirements, noise robustness, and scalability to swarm
size. Key findings show NNs can predict swarm behaviors with 97% accuracy using
short observation windows of 20 time steps, while also demonstrating graceful
degradation down to 80% accuracy under 50% noise, as well as excellent
scalability to swarm sizes from 10 to 100 agents. These capabilities are
promising for real-time decision-making support in defense scenarios by rapidly
inferring insights about swarm behavior.
\\ ( https://arxiv.org/abs/2403.19572 ,  2657kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19591
Date: Thu, 28 Mar 2024 17:13:47 GMT   (858kb,D)

Title: Genetic Quantization-Aware Approximation for Non-Linear Operations in
  Transformers
Authors: Pingcheng Dong, Yonghao Tan, Dong Zhang, Tianwei Ni, Xuejiao Liu, Yu
  Liu, Peng Luo, Luhong Liang, Shih-Yang Liu, Xijie Huang, Huaiyu Zhu, Yun Pan,
  Fengwei An, Kwang-Ting Cheng
Categories: cs.LG cs.AR cs.NE
Comments: 61st ACM/IEEE Design Automation Conference (DAC) 2024
\\
  Non-linear functions are prevalent in Transformers and their lightweight
variants, incurring substantial and frequently underestimated hardware costs.
Previous state-of-the-art works optimize these operations by piece-wise linear
approximation and store the parameters in look-up tables (LUT), but most of
them require unfriendly high-precision arithmetics such as FP/INT 32 and lack
consideration of integer-only INT quantization. This paper proposed a genetic
LUT-Approximation algorithm namely GQA-LUT that can automatically determine the
parameters with quantization awareness. The results demonstrate that GQA-LUT
achieves negligible degradation on the challenging semantic segmentation task
for both vanilla and linear Transformer models. Besides, proposed GQA-LUT
enables the employment of INT8-based LUT-Approximation that achieves an area
savings of 81.3~81.7% and a power reduction of 79.3~80.2% compared to the
high-precision FP/INT 32 alternatives. Code is available at https://
github.com/PingchengDong/GQA-LUT.
\\ ( https://arxiv.org/abs/2403.19591 ,  858kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19625
Date: Thu, 28 Mar 2024 17:45:03 GMT   (56kb,D)

Title: Top-$k$ Classification and Cardinality-Aware Prediction
Authors: Anqi Mao, Mehryar Mohri, Yutao Zhong
Categories: cs.LG stat.ML
\\
  We present a detailed study of top-$k$ classification, the task of predicting
the $k$ most probable classes for an input, extending beyond single-class
prediction. We demonstrate that several prevalent surrogate loss functions in
multi-class classification, such as comp-sum and constrained losses, are
supported by $H$-consistency bounds with respect to the top-$k$ loss. These
bounds guarantee consistency in relation to the hypothesis set $H$, providing
stronger guarantees than Bayes-consistency due to their non-asymptotic and
hypothesis-set specific nature. To address the trade-off between accuracy and
cardinality $k$, we further introduce cardinality-aware loss functions through
instance-dependent cost-sensitive learning. For these functions, we derive
cost-sensitive comp-sum and constrained surrogate losses, establishing their
$H$-consistency bounds and Bayes-consistency. Minimizing these losses leads to
new cardinality-aware algorithms for top-$k$ classification. We report the
results of extensive experiments on CIFAR-100, ImageNet, CIFAR-10, and SVHN
datasets demonstrating the effectiveness and benefit of these algorithms.
\\ ( https://arxiv.org/abs/2403.19625 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19629
Date: Thu, 28 Mar 2024 17:46:25 GMT   (2347kb,D)

Title: Metric Learning from Limited Pairwise Preference Comparisons
Authors: Zhi Wang, Geelon So, Ramya Korlakai Vinayak
Categories: cs.LG stat.ML
\\
  We study metric learning from preference comparisons under the ideal point
model, in which a user prefers an item over another if it is closer to their
latent ideal item. These items are embedded into $\mathbb{R}^d$ equipped with
an unknown Mahalanobis distance shared across users. While recent work shows
that it is possible to simultaneously recover the metric and ideal items given
$\mathcal{O}(d)$ pairwise comparisons per user, in practice we often have a
limited budget of $o(d)$ comparisons. We study whether the metric can still be
recovered, even though it is known that learning individual ideal items is now
no longer possible. We show that in general, $o(d)$ comparisons reveals no
information about the metric, even with infinitely many users. However, when
comparisons are made over items that exhibit low-dimensional structure, each
user can contribute to learning the metric restricted to a low-dimensional
subspace so that the metric can be jointly identified. We present a
divide-and-conquer approach that achieves this, and provide theoretical
recovery guarantees and empirical validation.
\\ ( https://arxiv.org/abs/2403.19629 ,  2347kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19647
Date: Thu, 28 Mar 2024 17:56:07 GMT   (7098kb,D)

Title: Sparse Feature Circuits: Discovering and Editing Interpretable Causal
  Graphs in Language Models
Authors: Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau,
  Aaron Mueller
Categories: cs.LG cs.AI cs.CL
Comments: Code and data at https://github.com/saprmarks/feature-circuits.
  Demonstration at https://feature-circuits.xyz
\\
  We introduce methods for discovering and applying sparse feature circuits.
These are causally implicated subnetworks of human-interpretable features for
explaining language model behaviors. Circuits identified in prior work consist
of polysemantic and difficult-to-interpret units like attention heads or
neurons, rendering them unsuitable for many downstream applications. In
contrast, sparse feature circuits enable detailed understanding of
unanticipated mechanisms. Because they are based on fine-grained units, sparse
feature circuits are useful for downstream tasks: We introduce SHIFT, where we
improve the generalization of a classifier by ablating features that a human
judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised
and scalable interpretability pipeline by discovering thousands of sparse
feature circuits for automatically discovered model behaviors.
\\ ( https://arxiv.org/abs/2403.19647 ,  7098kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2403.18831 (*cross-listing*)
Date: Tue, 6 Feb 2024 14:20:51 GMT   (2051kb,D)

Title: DeepTraderX: Challenging Conventional Trading Strategies with Deep
  Learning in Multi-Threaded Market Simulations
Authors: Armand Mihai Cismaru
Categories: q-fin.TR cs.AI
Comments: 11 pages, 9 png figures, uses apalike.sty and SCITEPRESS.sty, to be
  published in the proceedings of ICAART 2024
ACM-class: I.2.6; J.1
Journal-ref: In Proceedings of the 16th International Conference on Agents and
  Artificial Intelligence - Volume 3, ISBN 978-989-758-680-4, ISSN 2184-433X,
  pages 412-421 (2024)
DOI: 10.5220/0000183700003636
\\
  In this paper, we introduce DeepTraderX (DTX), a simple Deep Learning-based
trader, and present results that demonstrate its performance in a
multi-threaded market simulation. In a total of about 500 simulated market
days, DTX has learned solely by watching the prices that other strategies
produce. By doing this, it has successfully created a mapping from market data
to quotes, either bid or ask orders, to place for an asset. Trained on
historical Level-2 market data, i.e., the Limit Order Book (LOB) for specific
tradable assets, DTX processes the market state $S$ at each timestep $T$ to
determine a price $P$ for market orders. The market data used in both training
and testing was generated from unique market schedules based on real historic
stock market data. DTX was tested extensively against the best strategies in
the literature, with its results validated by statistical analysis. Our
findings underscore DTX's capability to rival, and in many instances, surpass,
the performance of public-domain traders, including those that outclass human
traders, emphasising the efficiency of simple models, as this is required to
succeed in intricate multi-threaded simulations. This highlights the potential
of leveraging "black-box" Deep Learning systems to create more efficient
financial markets.
\\ ( https://arxiv.org/abs/2403.18831 ,  2051kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18838 (*cross-listing*)
Date: Thu, 22 Feb 2024 15:10:02 GMT   (727kb)

Title: Unleashing the Power of AI. A Systematic Review of Cutting-Edge
  Techniques in AI-Enhanced Scientometrics, Webometrics, and Bibliometrics
Authors: Hamid Reza Saeidnia, Elaheh Hosseini, Shadi Abdoli, Marcel Ausloos
Categories: cs.DL cs.AI physics.soc-ph
Comments: to be published in Library High Tech; 30 pages; 80 references; 4
  figures; 3 tables
DOI: 10.1108/LHT-10-2023-0514
\\
  Purpose: The study aims to analyze the synergy of Artificial Intelligence
(AI), with scientometrics, webometrics, and bibliometrics to unlock and to
emphasize the potential of the applications and benefits of AI algorithms in
these fields.
  Design/methodology/approach: By conducting a systematic literature review,
our aim is to explore the potential of AI in revolutionizing the methods used
to measure and analyze scholarly communication, identify emerging research
trends, and evaluate the impact of scientific publications. To achieve this, we
implemented a comprehensive search strategy across reputable databases such as
ProQuest, IEEE Explore, EBSCO, Web of Science, and Scopus. Our search
encompassed articles published from January 1, 2000, to September 2022,
resulting in a thorough review of 61 relevant articles.
  Findings: (i) Regarding scientometrics, the application of AI yields various
distinct advantages, such as conducting analyses of publications, citations,
research impact prediction, collaboration, research trend analysis, and
knowledge mapping, in a more objective and reliable framework. (ii) In terms of
webometrics, AI algorithms are able to enhance web crawling and data
collection, web link analysis, web content analysis, social media analysis, web
impact analysis, and recommender systems. (iii) Moreover, automation of data
collection, analysis of citations, disambiguation of authors, analysis of
co-authorship networks, assessment of research impact, text mining, and
recommender systems are considered as the potential of AI integration in the
field of bibliometrics.
  Originality/value: This study covers the particularly new benefits and
potential of AI-enhanced scientometrics, webometrics, and bibliometrics to
highlight the significant prospects of the synergy of this integration through
AI.
\\ ( https://arxiv.org/abs/2403.18838 ,  727kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18846 (*cross-listing*)
Date: Fri, 8 Mar 2024 04:08:40 GMT   (1742kb,D)

Title: The Blind Normalized Stein Variational Gradient Descent-Based Detection
  for Intelligent Massive Random Access
Authors: Xin Zhu, Ahmet Enis Cetin
Categories: cs.IT cs.AI cs.LG eess.SP math.IT
\\
  The lack of an efficient preamble detection algorithm remains a challenge for
solving preamble collision problems in intelligent massive random access (RA)
in practical communication scenarios. To solve this problem, we present a novel
early preamble detection scheme based on a maximum likelihood estimation (MLE)
model at the first step of the grant-based RA procedure. A novel blind
normalized Stein variational gradient descent (SVGD)-based detector is proposed
to obtain an approximate solution to the MLE model. First, by exploring the
relationship between the Hadamard transform and wavelet transform, a new
modified Hadamard transform (MHT) is developed to separate high-frequencies
from important components using the second-order derivative filter. Next, to
eliminate noise and mitigate the vanishing gradients problem in the SVGD-based
detectors, the block MHT layer is designed based on the MHT, scaling layer,
soft-thresholding layer, inverse MHT and sparsity penalty. Then, the blind
normalized SVGD algorithm is derived to perform preamble detection without
prior knowledge of noise power and the number of active devices. The
experimental results show the proposed block MHT layer outperforms other
transform-based methods in terms of computation costs and denoising
performance. Furthermore, with the assistance of the block MHT layer, the
proposed blind normalized SVGD algorithm achieves a higher preamble detection
accuracy and throughput than other state-of-the-art detection methods.
\\ ( https://arxiv.org/abs/2403.18846 ,  1742kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18850 (*cross-listing*)
Date: Thu, 14 Mar 2024 21:10:07 GMT   (1033kb,D)

Title: Are Colors Quanta of Light for Human Vision? A Quantum Cognition Study
  of Visual Perception
Authors: Jonito Aerts Argu\"elles
Categories: q-bio.NC cs.AI quant-ph
Comments: 28 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:2208.03726
\\
  We study the phenomenon of categorical perception within the quantum
measurement process. The mechanism underlying this phenomenon consists in
dilating stimuli being perceived to belong to different categories and
contracting stimuli being perceived to belong to the same category. We show
that, due to the naturally different way in determining the distance between
pure states compared to the distance between density states, the phenomenon of
categorical perception is rooted in the structure of the quantum measurement
process itself. We apply our findings to the situation of visual perception of
colors and argue that it is possible to consider colors as light quanta for
human visual perception in a similar way as photons are light quanta for
physical measurements of light frequencies. In our approach we see perception
as a complex encounter between the existing physical reality, the stimuli, and
the reality expected by the perciever, resulting in the experience of the
percepts. We investigate what that means for the situation of two colors, which
we call Light and Dark, given our findings on categorical perception within the
quantum measurement process.
\\ ( https://arxiv.org/abs/2403.18850 ,  1033kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18864 (*cross-listing*)
Date: Sun, 24 Mar 2024 14:23:35 GMT   (12770kb,D)

Title: Interpretable Machine Learning for Weather and Climate Prediction: A
  Survey
Authors: Ruyi Yang, Jingyu Hu, Zihao Li, Jianli Mu, Tingzhao Yu, Jiangjiang
  Xia, Xuhong Li, Aritra Dasgupta, Haoyi Xiong
Categories: physics.ao-ph cs.AI cs.LG
Comments: 26 pages, 5 figures
\\
  Advanced machine learning models have recently achieved high predictive
accuracy for weather and climate prediction. However, these complex models
often lack inherent transparency and interpretability, acting as "black boxes"
that impede user trust and hinder further model improvements. As such,
interpretable machine learning techniques have become crucial in enhancing the
credibility and utility of weather and climate modeling. In this survey, we
review current interpretable machine learning approaches applied to
meteorological predictions. We categorize methods into two major paradigms: 1)
Post-hoc interpretability techniques that explain pre-trained models, such as
perturbation-based, game theory based, and gradient-based attribution methods.
2) Designing inherently interpretable models from scratch using architectures
like tree ensembles and explainable neural networks. We summarize how each
technique provides insights into the predictions, uncovering novel
meteorological relationships captured by machine learning. Lastly, we discuss
research challenges around achieving deeper mechanistic interpretations aligned
with physical principles, developing standardized evaluation benchmarks,
integrating interpretability into iterative model development workflows, and
providing explainability for large foundation models.
\\ ( https://arxiv.org/abs/2403.18864 ,  12770kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18870 (*cross-listing*)
Date: Tue, 26 Mar 2024 11:23:08 GMT   (1557kb)

Title: SugarcaneNet2024: An Optimized Weighted Average Ensemble Approach of
  LASSO Regularized Pre-trained Models for Sugarcane Disease Classification
Authors: Md. Simul Hasan Talukder, Sharmin Akter, Abdullah Hafez Nur
Categories: cs.CV cs.AI cs.LG
Comments: 32 pages, 11 Figures, 13 Tables
\\
  Sugarcane, a key crop for the world's sugar industry, is prone to several
diseases that have a substantial negative influence on both its yield and
quality. To effectively manage and implement preventative initiatives, diseases
must be detected promptly and accurately. In this study, we present a unique
model called sugarcaneNet2024 that outperforms previous methods for
automatically and quickly detecting sugarcane disease through leaf image
processing. Our proposed model consolidates an optimized weighted average
ensemble of seven customized and LASSO-regularized pre-trained models,
particularly InceptionV3, InceptionResNetV2, DenseNet201, DenseNet169,
Xception, and ResNet152V2. Initially, we added three more dense layers with
0.0001 LASSO regularization, three 30% dropout layers, and three batch
normalizations with renorm enabled at the bottom of these pre-trained models to
improve the performance. The accuracy of sugarcane leaf disease classification
was greatly increased by this addition. Following this, several comparative
studies between the average ensemble and individual models were carried out,
indicating that the ensemble technique performed better. The average ensemble
of all modified pre-trained models produced outstanding outcomes: 100%, 99%,
99%, and 99.45% for f1 score, precision, recall, and accuracy, respectively.
Performance was further enhanced by the implementation of an optimized weighted
average ensemble technique incorporated with grid search. This optimized
sugarcaneNet2024 model performed the best for detecting sugarcane diseases,
having achieved accuracy, precision, recall, and F1 score of 99.67%, 100%,
100%, and 100% , respectively.
\\ ( https://arxiv.org/abs/2403.18870 ,  1557kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18871 (*cross-listing*)
Date: Tue, 26 Mar 2024 11:40:06 GMT   (3987kb)

Title: Clinical Domain Knowledge-Derived Template Improves Post Hoc AI
  Explanations in Pneumothorax Classification
Authors: Han Yuan, Chuan Hong, Pengtao Jiang, Gangming Zhao, Nguyen Tuan Anh
  Tran, Xinxing Xu, Yet Yen Yan, Nan Liu
Categories: cs.CV cs.AI cs.LG
\\
  Background: Pneumothorax is an acute thoracic disease caused by abnormal air
collection between the lungs and chest wall. To address the opaqueness often
associated with deep learning (DL) models, explainable artificial intelligence
(XAI) methods have been introduced to outline regions related to pneumothorax
diagnoses made by DL models. However, these explanations sometimes diverge from
actual lesion areas, highlighting the need for further improvement. Method: We
propose a template-guided approach to incorporate the clinical knowledge of
pneumothorax into model explanations generated by XAI methods, thereby
enhancing the quality of these explanations. Utilizing one lesion delineation
created by radiologists, our approach first generates a template that
represents potential areas of pneumothorax occurrence. This template is then
superimposed on model explanations to filter out extraneous explanations that
fall outside the template's boundaries. To validate its efficacy, we carried
out a comparative analysis of three XAI methods with and without our template
guidance when explaining two DL models in two real-world datasets. Results: The
proposed approach consistently improved baseline XAI methods across twelve
benchmark scenarios built on three XAI methods, two DL models, and two
datasets. The average incremental percentages, calculated by the performance
improvements over the baseline performance, were 97.8% in Intersection over
Union (IoU) and 94.1% in Dice Similarity Coefficient (DSC) when comparing model
explanations and ground-truth lesion areas. Conclusions: In the context of
pneumothorax diagnoses, we proposed a template-guided approach for improving AI
explanations. We anticipate that our template guidance will forge a fresh
approach to elucidating AI models by integrating clinical domain expertise.
\\ ( https://arxiv.org/abs/2403.18871 ,  3987kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18920 (*cross-listing*)
Date: Wed, 27 Mar 2024 18:09:55 GMT   (34469kb,D)

Title: CPR: Retrieval Augmented Generation for Copyright Protection
Authors: Aditya Golatkar, Alessandro Achille, Luca Zancato, Yu-Xiang Wang,
  Ashwin Swaminathan, Stefano Soatto
Categories: cs.CR cs.AI cs.CV
Comments: CVPR 2024
\\
  Retrieval Augmented Generation (RAG) is emerging as a flexible and robust
technique to adapt models to private users data without training, to handle
credit attribution, and to allow efficient machine unlearning at scale.
However, RAG techniques for image generation may lead to parts of the retrieved
samples being copied in the model's output. To reduce risks of leaking private
information contained in the retrieved set, we introduce Copy-Protected
generation with Retrieval (CPR), a new method for RAG with strong copyright
protection guarantees in a mixed-private setting for diffusion models.CPR
allows to condition the output of diffusion models on a set of retrieved
images, while also guaranteeing that unique identifiable information about
those example is not exposed in the generated outputs. In particular, it does
so by sampling from a mixture of public (safe) distribution and private (user)
distribution by merging their diffusion scores at inference. We prove that CPR
satisfies Near Access Freeness (NAF) which bounds the amount of information an
attacker may be able to extract from the generated images. We provide two
algorithms for copyright protection, CPR-KL and CPR-Choose. Unlike previously
proposed rejection-sampling-based NAF methods, our methods enable efficient
copyright-protected sampling with a single run of backward diffusion. We show
that our method can be applied to any pre-trained conditional diffusion model,
such as Stable Diffusion or unCLIP. In particular, we empirically show that
applying CPR on top of unCLIP improves quality and text-to-image alignment of
the generated results (81.4 to 83.17 on TIFA benchmark), while enabling credit
attribution, copy-right protection, and deterministic, constant time,
unlearning.
\\ ( https://arxiv.org/abs/2403.18920 ,  34469kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18923 (*cross-listing*)
Date: Thu, 15 Feb 2024 20:27:33 GMT   (17375kb,D)

Title: Nature-Guided Cognitive Evolution for Predicting Dissolved Oxygen
  Concentrations in North Temperate Lakes
Authors: Runlong Yu, Robert Ladwig, Xiang Xu, Peijun Zhu, Paul C. Hanson, Yiqun
  Xie, Xiaowei Jia
Categories: cs.NE cs.AI cs.LG
\\
  Predicting dissolved oxygen (DO) concentrations in north temperate lakes
requires a comprehensive study of phenological patterns across various
ecosystems, which highlights the significance of selecting phenological
features and feature interactions. Process-based models are limited by partial
process knowledge or oversimplified feature representations, while machine
learning models face challenges in efficiently selecting relevant feature
interactions for different lake types and tasks, especially under the
infrequent nature of DO data collection. In this paper, we propose a
Nature-Guided Cognitive Evolution (NGCE) strategy, which represents a
multi-level fusion of adaptive learning with natural processes. Specifically,
we utilize metabolic process-based models to generate simulated DO labels.
Using these simulated labels, we implement a multi-population cognitive
evolutionary search, where models, mirroring natural organisms, adaptively
evolve to select relevant feature interactions within populations for different
lake types and tasks. These models are not only capable of undergoing crossover
and mutation mechanisms within intra-populations but also, albeit infrequently,
engage in inter-population crossover. The second stage involves refining these
models by retraining them with real observed labels. We have tested the
performance of our NGCE strategy in predicting daily DO concentrations across a
wide range of lakes in the Midwest, USA. These lakes, varying in size, depth,
and trophic status, represent a broad spectrum of north temperate lakes. Our
findings demonstrate that NGCE not only produces accurate predictions with few
observed labels but also, through gene maps of models, reveals sophisticated
phenological patterns of different lakes.
\\ ( https://arxiv.org/abs/2403.18923 ,  17375kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18946 (*cross-listing*)
Date: Tue, 20 Feb 2024 23:59:45 GMT   (862kb,D)

Title: Random Aggregate Beamforming for Over-the-Air Federated Learning in
  Large-Scale Networks
Authors: Chunmei Xu, Shengheng Liu, Yongming Huang, Bjorn Ottersten, Dusit
  Niyato
Categories: cs.IT cs.AI cs.LG math.IT
Comments: 30 pages, 11 figures
\\
  At present, there is a trend to deploy ubiquitous artificial intelligence
(AI) applications at the edge of the network. As a promising framework that
enables secure edge intelligence, federated learning (FL) has received
widespread attention, and over-the-air computing (AirComp) has been integrated
to further improve the communication efficiency. In this paper, we consider a
joint device selection and aggregate beamforming design with the objectives of
minimizing the aggregate error and maximizing the number of selected devices.
This yields a combinatorial problem, which is difficult to solve especially in
large-scale networks. To tackle the problems in a cost-effective manner, we
propose a random aggregate beamforming-based scheme, which generates the
aggregator beamforming vector via random sampling rather than optimization. The
implementation of the proposed scheme does not require the channel estimation.
We additionally use asymptotic analysis to study the obtained aggregate error
and the number of the selected devices when the number of devices becomes
large. Furthermore, a refined method that runs with multiple randomizations is
also proposed for performance improvement. Extensive simulation results are
presented to demonstrate the effectiveness of the proposed random aggregate
beamforming-based scheme as well as the refined method.
\\ ( https://arxiv.org/abs/2403.18946 ,  862kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18958 (*cross-listing*)
Date: Wed, 27 Mar 2024 19:02:56 GMT   (1216kb,D)

Title: A State-of-the-practice Release-readiness Checklist for Generative
  AI-based Software Products
Authors: Harsh Patel, Dominique Boucher, Emad Fallahzadeh, Ahmed E. Hassan and
  Bram Adams
Categories: cs.SE cs.AI
\\
  This paper investigates the complexities of integrating Large Language Models
(LLMs) into software products, with a focus on the challenges encountered for
determining their readiness for release. Our systematic review of grey
literature identifies common challenges in deploying LLMs, ranging from
pre-training and fine-tuning to user experience considerations. The study
introduces a comprehensive checklist designed to guide practitioners in
evaluating key release readiness aspects such as performance, monitoring, and
deployment strategies, aiming to enhance the reliability and effectiveness of
LLM-based applications in real-world settings.
\\ ( https://arxiv.org/abs/2403.18958 ,  1216kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18963 (*cross-listing*)
Date: Wed, 27 Mar 2024 19:16:56 GMT   (718kb,D)

Title: Using Quantum Computing to Infer Dynamic Behaviors of Biological and
  Artificial Neural Networks
Authors: Gabriel A. Silva
Categories: quant-ph cs.AI q-bio.NC
\\
  The exploration of new problem classes for quantum computation is an active
area of research. An essentially completely unexplored topic is the use of
quantum algorithms and computing to explore and ask questions \textit{about}
the functional dynamics of neural networks. This is a component of the
still-nascent topic of applying quantum computing to the modeling and
simulations of biological and artificial neural networks. In this work, we show
how a carefully constructed set of conditions can use two foundational quantum
algorithms, Grover and Deutsch-Josza, in such a way that the output
measurements admit an interpretation that guarantees we can infer if a simple
representation of a neural network (which applies to both biological and
artificial networks) after some period of time has the potential to continue
sustaining dynamic activity. Or whether the dynamics are guaranteed to stop
either through 'epileptic' dynamics or quiescence.
\\ ( https://arxiv.org/abs/2403.18963 ,  718kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18965 (*cross-listing*)
Date: Wed, 27 Mar 2024 19:30:06 GMT   (5495kb,D)

Title: LORD: Large Models based Opposite Reward Design for Autonomous Driving
Authors: Xin Ye, Feng Tao, Abhirup Mallik, Burhaneddin Yaman, Liu Ren
Categories: cs.RO cs.AI cs.LG
\\
  Reinforcement learning (RL) based autonomous driving has emerged as a
promising alternative to data-driven imitation learning approaches. However,
crafting effective reward functions for RL poses challenges due to the
complexity of defining and quantifying good driving behaviors across diverse
scenarios. Recently, large pretrained models have gained significant attention
as zero-shot reward models for tasks specified with desired linguistic goals.
However, the desired linguistic goals for autonomous driving such as "drive
safely" are ambiguous and incomprehensible by pretrained models. On the other
hand, undesired linguistic goals like "collision" are more concrete and
tractable. In this work, we introduce LORD, a novel large models based opposite
reward design through undesired linguistic goals to enable the efficient use of
large pretrained models as zero-shot reward models. Through extensive
experiments, our proposed framework shows its efficiency in leveraging the
power of large pretrained models for achieving safe and enhanced autonomous
driving. Moreover, the proposed approach shows improved generalization
capabilities as it outperforms counterpart methods across diverse and
challenging driving scenarios.
\\ ( https://arxiv.org/abs/2403.18965 ,  5495kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18978 (*cross-listing*)
Date: Wed, 27 Mar 2024 19:52:55 GMT   (47167kb,D)

Title: TextCraftor: Your Text Encoder Can be Image Quality Controller
Authors: Yanyu Li, Xian Liu, Anil Kag, Ju Hu, Yerlan Idelbayev, Dhritiman
  Sagar, Yanzhi Wang, Sergey Tulyakov, Jian Ren
Categories: cs.CV cs.AI cs.LG
\\
  Diffusion-based text-to-image generative models, e.g., Stable Diffusion, have
revolutionized the field of content generation, enabling significant
advancements in areas like image editing and video synthesis. Despite their
formidable capabilities, these models are not without their limitations. It is
still challenging to synthesize an image that aligns well with the input text,
and multiple runs with carefully crafted prompts are required to achieve
satisfactory results. To mitigate these limitations, numerous studies have
endeavored to fine-tune the pre-trained diffusion models, i.e., UNet, utilizing
various technologies. Yet, amidst these efforts, a pivotal question of
text-to-image diffusion model training has remained largely unexplored: Is it
possible and feasible to fine-tune the text encoder to improve the performance
of text-to-image diffusion models? Our findings reveal that, instead of
replacing the CLIP text encoder used in Stable Diffusion with other large
language models, we can enhance it through our proposed fine-tuning approach,
TextCraftor, leading to substantial improvements in quantitative benchmarks and
human assessments. Interestingly, our technique also empowers controllable
image generation through the interpolation of different text encoders
fine-tuned with various rewards. We also demonstrate that TextCraftor is
orthogonal to UNet finetuning, and can be combined to further improve
generative quality.
\\ ( https://arxiv.org/abs/2403.18978 ,  47167kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18989 (*cross-listing*)
Date: Wed, 27 Mar 2024 20:09:59 GMT   (6582kb,D)

Title: Dealing with Imbalanced Classes in Bot-IoT Dataset
Authors: Jesse Atuhurra, Takanori Hara, Yuanyu Zhang, Masahiro Sasabe, Shoji
  Kasahara
Categories: cs.CR cs.AI
\\
  With the rapidly spreading usage of Internet of Things (IoT) devices, a
network intrusion detection system (NIDS) plays an important role in detecting
and protecting various types of attacks in the IoT network. To evaluate the
robustness of the NIDS in the IoT network, the existing work proposed a
realistic botnet dataset in the IoT network (Bot-IoT dataset) and applied it to
machine learning-based anomaly detection. This dataset contains imbalanced
normal and attack packets because the number of normal packets is much smaller
than that of attack ones. The nature of imbalanced data may make it difficult
to identify the minority class correctly. In this thesis, to address the class
imbalance problem in the Bot-IoT dataset, we propose a binary classification
method with synthetic minority over-sampling techniques (SMOTE). The proposed
classifier aims to detect attack packets and overcome the class imbalance
problem using the SMOTE algorithm. Through numerical results, we demonstrate
the proposed classifier's fundamental characteristics and the impact of
imbalanced data on its performance.
\\ ( https://arxiv.org/abs/2403.18989 ,  6582kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18998 (*cross-listing*)
Date: Wed, 27 Mar 2024 20:38:04 GMT   (2583kb,D)

Title: Few-Shot Cross-System Anomaly Trace Classification for
  Microservice-based systems
Authors: Yuqing Wang and Mika V. Mantyl\"a and Serge Demeyer and Mutlu Beyazit
  and Joanna Kisaakye and Jesse Nyyss\"ol\"a
Categories: cs.SE cs.AI cs.LG
Comments: 12 pages
\\
  Microservice-based systems (MSS) may experience failures in various fault
categories due to their complex and dynamic nature. To effectively handle
failures, AIOps tools utilize trace-based anomaly detection and root cause
analysis. In this paper, we propose a novel framework for few-shot abnormal
trace classification for MSS. Our framework comprises two main components: (1)
Multi-Head Attention Autoencoder for constructing system-specific trace
representations, which enables (2) Transformer Encoder-based Model-Agnostic
Meta-Learning to perform effective and efficient few-shot learning for abnormal
trace classification. The proposed framework is evaluated on two representative
MSS, Trainticket and OnlineBoutique, with open datasets. The results show that
our framework can adapt the learned knowledge to classify new, unseen abnormal
traces of novel fault categories both within the same system it was initially
trained on and even in the different MSS. Within the same MSS, our framework
achieves an average accuracy of 93.26\% and 85.2\% across 50 meta-testing tasks
for Trainticket and OnlineBoutique, respectively, when provided with 10
instances for each task. In a cross-system context, our framework gets an
average accuracy of 92.19\% and 84.77\% for the same meta-testing tasks of the
respective system, also with 10 instances provided for each task. Our work
demonstrates the applicability of achieving few-shot abnormal trace
classification for MSS and shows how it can enable cross-system adaptability.
This opens an avenue for building more generalized AIOps tools that require
less system-specific data labeling for anomaly detection and root cause
analysis.
\\ ( https://arxiv.org/abs/2403.18998 ,  2583kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19001 (*cross-listing*)
Date: Wed, 27 Mar 2024 20:51:02 GMT   (445kb,D)

Title: Cross--domain Fiber Cluster Shape Analysis for Language Performance
  Cognitive Score Prediction
Authors: Yui Lo, Yuqian Chen, Dongnan Liu, Wan Liu, Leo Zekelman, Fan Zhang,
  Yogesh Rathi, Nikos Makris, Alexandra J. Golby, Weidong Cai, Lauren J.
  O'Donnell
Categories: cs.CV cs.AI eess.IV q-bio.NC
Comments: 2 figures, 11 pages
\\
  Shape plays an important role in computer graphics, offering informative
features to convey an object's morphology and functionality. Shape analysis in
brain imaging can help interpret structural and functionality correlations of
the human brain. In this work, we investigate the shape of the brain's 3D white
matter connections and its potential predictive relationship to human cognitive
function. We reconstruct brain connections as sequences of 3D points using
diffusion magnetic resonance imaging (dMRI) tractography. To describe each
connection, we extract 12 shape descriptors in addition to traditional dMRI
connectivity and tissue microstructure features. We introduce a novel
framework, Shape--fused Fiber Cluster Transformer (SFFormer), that leverages a
multi-head cross-attention feature fusion module to predict subject-specific
language performance based on dMRI tractography. We assess the performance of
the method on a large dataset including 1065 healthy young adults. The results
demonstrate that both the transformer-based SFFormer model and its inter/intra
feature fusion with shape, microstructure, and connectivity are informative,
and together, they improve the prediction of subject-specific language
performance scores. Overall, our results indicate that the shape of the brain's
connections is predictive of human language function.
\\ ( https://arxiv.org/abs/2403.19001 ,  445kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19021 (*cross-listing*)
Date: Wed, 27 Mar 2024 21:22:37 GMT   (8759kb,D)

Title: Towards LLM-RecSys Alignment with Textual ID Learning
Authors: Juntao Tan, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Zelong Li and
  Yongfeng Zhang
Categories: cs.IR cs.AI cs.CL cs.LG
Comments: Accepted in SIGIR 2024
\\
  Generative recommendation based on Large Language Models (LLMs) have
transformed the traditional ranking-based recommendation style into a
text-to-text generation paradigm. However, in contrast to standard NLP tasks
that inherently operate on human vocabulary, current research in generative
recommendations struggles to effectively encode recommendation items within the
text-to-text framework using concise yet meaningful ID representations. To
better align LLMs with recommendation needs, we propose IDGen, representing
each item as a unique, concise, semantically rich, platform-agnostic textual ID
using human language tokens. This is achieved by training a textual ID
generator alongside the LLM-based recommender, enabling seamless integration of
personalized recommendations into natural language generation. Notably, as user
history is expressed in natural language and decoupled from the original
dataset, our approach suggests the potential for a foundational generative
recommendation model. Experiments show that our framework consistently
surpasses existing models in sequential recommendation under standard
experimental setting. Then, we explore the possibility of training a foundation
recommendation model with the proposed method on data collected from 19
different datasets and tested its recommendation performance on 6 unseen
datasets across different platforms under a completely zero-shot setting. The
results show that the zero-shot performance of the pre-trained foundation model
is comparable to or even better than some traditional recommendation models
based on supervised training, showing the potential of the IDGen paradigm
serving as the foundation model for generative recommendation. Code and data
are open-sourced at https://github.com/agiresearch/IDGenRec.
\\ ( https://arxiv.org/abs/2403.19021 ,  8759kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19046 (*cross-listing*)
Date: Wed, 27 Mar 2024 22:50:48 GMT   (3272kb,D)

Title: LITA: Language Instructed Temporal-Localization Assistant
Authors: De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo
  Molchanov, Zhiding Yu, Jan Kautz
Categories: cs.CV cs.AI
\\
  There has been tremendous progress in multimodal Large Language Models
(LLMs). Recent works have extended these models to video input with promising
instruction following capabilities. However, an important missing piece is
temporal localization. These models cannot accurately answer the "When?"
questions. We identify three key aspects that limit their temporal localization
capabilities: (i) time representation, (ii) architecture, and (iii) data. We
address these shortcomings by proposing Language Instructed
Temporal-Localization Assistant (LITA) with the following features: (1) We
introduce time tokens that encode timestamps relative to the video length to
better represent time in videos. (2) We introduce SlowFast tokens in the
architecture to capture temporal information at fine temporal resolution. (3)
We emphasize temporal localization data for LITA. In addition to leveraging
existing video datasets with timestamps, we propose a new task, Reasoning
Temporal Localization (RTL), along with the dataset, ActivityNet-RTL, for
learning and evaluating this task. Reasoning temporal localization requires
both the reasoning and temporal localization of Video LLMs. LITA demonstrates
strong performance on this challenging task, nearly doubling the temporal mean
intersection-over-union (mIoU) of baselines. In addition, we show that our
emphasis on temporal localization also substantially improves video-based text
generation compared to existing Video LLMs, including a 36% relative
improvement of Temporal Understanding. Code is available at:
https://github.com/NVlabs/LITA
\\ ( https://arxiv.org/abs/2403.19046 ,  3272kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19060 (*cross-listing*)
Date: Wed, 27 Mar 2024 23:55:02 GMT   (42777kb,D)

Title: Towards Human-Centered Construction Robotics: An RL-Driven Companion
  Robot For Contextually Assisting Carpentry Workers
Authors: Yuning Wu, Jiaying Wei, Jean Oh, Daniel Cardoso Llach
Categories: cs.RO cs.AI cs.HC cs.LG
Comments: 8 pages, 9 figures. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
\\
  In the dynamic construction industry, traditional robotic integration has
primarily focused on automating specific tasks, often overlooking the
complexity and variability of human aspects in construction workflows. This
paper introduces a human-centered approach with a ``work companion rover"
designed to assist construction workers within their existing practices, aiming
to enhance safety and workflow fluency while respecting construction labor's
skilled nature. We conduct an in-depth study on deploying a robotic system in
carpentry formwork, showcasing a prototype that emphasizes mobility, safety,
and comfortable worker-robot collaboration in dynamic environments through a
contextual Reinforcement Learning (RL)-driven modular framework. Our research
advances robotic applications in construction, advocating for collaborative
models where adaptive robots support rather than replace humans, underscoring
the potential for an interactive and collaborative human-robot workforce.
\\ ( https://arxiv.org/abs/2403.19060 ,  42777kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19066 (*cross-listing*)
Date: Thu, 28 Mar 2024 00:11:12 GMT   (39615kb,D)

Title: Generative Quanta Color Imaging
Authors: Vishal Purohit, Junjie Luo, Yiheng Chi, Qi Guo, Stanley H. Chan, Qiang
  Qiu
Categories: cs.CV cs.AI
Comments: Accepted at IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR), 2024
\\
  The astonishing development of single-photon cameras has created an
unprecedented opportunity for scientific and industrial imaging. However, the
high data throughput generated by these 1-bit sensors creates a significant
bottleneck for low-power applications. In this paper, we explore the
possibility of generating a color image from a single binary frame of a
single-photon camera. We evidently find this problem being particularly
difficult to standard colorization approaches due to the substantial degree of
exposure variation. The core innovation of our paper is an exposure synthesis
model framed under a neural ordinary differential equation (Neural ODE) that
allows us to generate a continuum of exposures from a single observation. This
innovation ensures consistent exposure in binary images that colorizers take
on, resulting in notably enhanced colorization. We demonstrate applications of
the method in single-image and burst colorization and show superior generative
performance over baselines. Project website can be found at
https://vishal-s-p.github.io/projects/2023/generative_quanta_color.html.
\\ ( https://arxiv.org/abs/2403.19066 ,  39615kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19073 (*cross-listing*)
Date: Thu, 28 Mar 2024 00:29:15 GMT   (3735kb)

Title: Dataflow-Aware PIM-Enabled Manycore Architecture for Deep Learning
  Workloads
Authors: Harsh Sharma, Gaurav Narang, Janardhan Rao Doppa, Umit Ogras, and
  Partha Pratim Pande
Categories: cs.AR cs.AI cs.ET
Comments: Presented at DATE Conference, Valencia, Spain 2024
\\
  Processing-in-memory (PIM) has emerged as an enabler for the energy-efficient
and high-performance acceleration of deep learning (DL) workloads. Resistive
random-access memory (ReRAM) is one of the most promising technologies to
implement PIM. However, as the complexity of Deep convolutional neural networks
(DNNs) grows, we need to design a manycore architecture with multiple
ReRAM-based processing elements (PEs) on a single chip. Existing PIM-based
architectures mostly focus on computation while ignoring the role of
communication. ReRAM-based tiled manycore architectures often involve many
Processing Elements (PEs), which need to be interconnected via an efficient
on-chip communication infrastructure. Simply allocating more resources (ReRAMs)
to speed up only computation is ineffective if the communication infrastructure
cannot keep up with it. In this paper, we highlight the design principles of a
dataflow-aware PIM-enabled manycore platform tailor-made for various types of
DL workloads. We consider the design challenges with both 2.5D interposer- and
3D integration-enabled architectures.
\\ ( https://arxiv.org/abs/2403.19073 ,  3735kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19078 (*cross-listing*)
Date: Thu, 28 Mar 2024 00:50:02 GMT   (5494kb,D)

Title: MVEB: Self-Supervised Learning with Multi-View Entropy Bottleneck
Authors: Liangjian Wen, Xiasi Wang, Jianzhuang Liu, Zenglin Xu
Categories: cs.CV cs.AI
Comments: Accepted by TPAMI
\\
  Self-supervised learning aims to learn representation that can be effectively
generalized to downstream tasks. Many self-supervised approaches regard two
views of an image as both the input and the self-supervised signals, assuming
that either view contains the same task-relevant information and the shared
information is (approximately) sufficient for predicting downstream tasks.
Recent studies show that discarding superfluous information not shared between
the views can improve generalization. Hence, the ideal representation is
sufficient for downstream tasks and contains minimal superfluous information,
termed minimal sufficient representation. One can learn this representation by
maximizing the mutual information between the representation and the supervised
view while eliminating superfluous information. Nevertheless, the computation
of mutual information is notoriously intractable. In this work, we propose an
objective termed multi-view entropy bottleneck (MVEB) to learn minimal
sufficient representation effectively. MVEB simplifies the minimal sufficient
learning to maximizing both the agreement between the embeddings of two views
and the differential entropy of the embedding distribution. Our experiments
confirm that MVEB significantly improves performance. For example, it achieves
top-1 accuracy of 76.9\% on ImageNet with a vanilla ResNet-50 backbone on
linear evaluation. To the best of our knowledge, this is the new
state-of-the-art result with ResNet-50.
\\ ( https://arxiv.org/abs/2403.19078 ,  5494kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19093 (*cross-listing*)
Date: Thu, 28 Mar 2024 02:02:00 GMT   (5836kb,D)

Title: Task2Morph: Differentiable Task-inspired Framework for Contact-Aware
  Robot Design
Authors: Yishuai Cai, Shaowu Yang, Minglong Li, Xinglin Chen, Yunxin Mao,
  Xiaodong Yi and Wenjing Yang
Categories: cs.RO cs.AI
Comments: 9 pages, 10 figures, published to IROS
Journal-ref: 2023 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS). IEEE, 2023: 452-459
DOI: 10.1109/IROS55552.2023.10341360
\\
  Optimizing the morphologies and the controllers that adapt to various tasks
is a critical issue in the field of robot design, aka. embodied intelligence.
Previous works typically model it as a joint optimization problem and use
search-based methods to find the optimal solution in the morphology space.
However, they ignore the implicit knowledge of task-to-morphology mapping which
can directly inspire robot design. For example, flipping heavier boxes tends to
require more muscular robot arms. This paper proposes a novel and general
differentiable task-inspired framework for contact-aware robot design called
Task2Morph. We abstract task features highly related to task performance and
use them to build a task-to-morphology mapping. Further, we embed the mapping
into a differentiable robot design process, where the gradient information is
leveraged for both the mapping learning and the whole optimization. The
experiments are conducted on three scenarios, and the results validate that
Task2Morph outperforms DiffHand, which lacks a task-inspired morphology module,
in terms of efficiency and effectiveness.
\\ ( https://arxiv.org/abs/2403.19093 ,  5836kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19101 (*cross-listing*)
Date: Thu, 28 Mar 2024 02:31:06 GMT   (1546kb,D)

Title: AAPMT: AGI Assessment Through Prompt and Metric Transformer
Authors: Benhao Huang
Categories: cs.CV cs.AI
\\
  The emergence of text-to-image models marks a significant milestone in the
evolution of AI-generated images (AGIs), expanding their use in diverse domains
like design, entertainment, and more. Despite these breakthroughs, the quality
of AGIs often remains suboptimal, highlighting the need for effective
evaluation methods. These methods are crucial for assessing the quality of
images relative to their textual descriptions, and they must accurately mirror
human perception. Substantial progress has been achieved in this domain, with
innovative techniques such as BLIP and DBCNN contributing significantly.
However, recent studies, including AGIQA-3K, reveal a notable discrepancy
between current methods and state-of-the-art (SOTA) standards. This gap
emphasizes the necessity for a more sophisticated and precise evaluation
metric. In response, our objective is to develop a model that could give
ratings for metrics, which focuses on parameters like perceptual quality,
authenticity, and the correspondence between text and image, that more closely
aligns with human perception. In our paper, we introduce a range of effective
methods, including prompt designs and the Metric Transformer. The Metric
Transformer is a novel structure inspired by the complex interrelationships
among various AGI quality metrics. The code is available at
https://github.com/huskydoge/CS3324-Digital-Image-Processing/tree/main/Assignment1
\\ ( https://arxiv.org/abs/2403.19101 ,  1546kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19103 (*cross-listing*)
Date: Thu, 28 Mar 2024 02:35:53 GMT   (38572kb,D)

Title: Automated Black-box Prompt Engineering for Personalized Text-to-Image
  Generation
Authors: Yutong He, Alexander Robey, Naoki Murata, Yiding Jiang, Joshua
  Williams, George J. Pappas, Hamed Hassani, Yuki Mitsufuji, Ruslan
  Salakhutdinov, J. Zico Kolter
Categories: cs.CV cs.AI cs.CL cs.LG
\\
  Prompt engineering is effective for controlling the output of text-to-image
(T2I) generative models, but it is also laborious due to the need for manually
crafted prompts. This challenge has spurred the development of algorithms for
automated prompt generation. However, these methods often struggle with
transferability across T2I models, require white-box access to the underlying
model, and produce non-intuitive prompts. In this work, we introduce PRISM, an
algorithm that automatically identifies human-interpretable and transferable
prompts that can effectively generate desired concepts given only black-box
access to T2I models. Inspired by large language model (LLM) jailbreaking,
PRISM leverages the in-context learning ability of LLMs to iteratively refine
the candidate prompts distribution for given reference images. Our experiments
demonstrate the versatility and effectiveness of PRISM in generating accurate
prompts for objects, styles and images across multiple T2I models, including
Stable Diffusion, DALL-E, and Midjourney.
\\ ( https://arxiv.org/abs/2403.19103 ,  38572kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19140 (*cross-listing*)
Date: Thu, 28 Mar 2024 04:24:56 GMT   (43605kb,D)

Title: QNCD: Quantization Noise Correction for Diffusion Models
Authors: Huanpeng Chu, Wei Wu, Chengjie Zang, Kun Yuan
Categories: cs.CV cs.AI
\\
  Diffusion models have revolutionized image synthesis, setting new benchmarks
in quality and creativity. However, their widespread adoption is hindered by
the intensive computation required during the iterative denoising process.
Post-training quantization (PTQ) presents a solution to accelerate sampling,
aibeit at the expense of sample quality, extremely in low-bit settings.
Addressing this, our study introduces a unified Quantization Noise Correction
Scheme (QNCD), aimed at minishing quantization noise throughout the sampling
process. We identify two primary quantization challenges: intra and inter
quantization noise. Intra quantization noise, mainly exacerbated by embeddings
in the resblock module, extends activation quantization ranges, increasing
disturbances in each single denosing step. Besides, inter quantization noise
stems from cumulative quantization deviations across the entire denoising
process, altering data distributions step-by-step. QNCD combats these through
embedding-derived feature smoothing for eliminating intra quantization noise
and an effective runtime noise estimatiation module for dynamicly filtering
inter quantization noise. Extensive experiments demonstrate that our method
outperforms previous quantization methods for diffusion models, achieving
lossless results in W4A8 and W8A8 quantization settings on ImageNet (LDM-4).
Code is available at: https://github.com/huanpengchu/QNCD
\\ ( https://arxiv.org/abs/2403.19140 ,  43605kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19148 (*cross-listing*)
Date: Thu, 28 Mar 2024 04:57:13 GMT   (525kb)

Title: GenAI Detection Tools, Adversarial Techniques and Implications for
  Inclusivity in Higher Education
Authors: Mike Perkins (1), Jasper Roe (2), Binh H. Vu (1), Darius Postma (1),
  Don Hickerson (1), James McGaughran (1), Huy Q. Khuat (1) ((1) British
  University Vietnam, (2) James Cook University Singapore)
Categories: cs.CY cs.AI
\\
  This study investigates the efficacy of six major Generative AI (GenAI) text
detectors when confronted with machine-generated content that has been modified
using techniques designed to evade detection by these tools (n=805). The
results demonstrate that the detectors' already low accuracy rates (39.5%) show
major reductions in accuracy (17.4%) when faced with manipulated content, with
some techniques proving more effective than others in evading detection.
  The accuracy limitations and the potential for false accusations demonstrate
that these tools cannot currently be recommended for determining whether
violations of academic integrity have occurred, underscoring the challenges
educators face in maintaining inclusive and fair assessment practices. However,
they may have a role in supporting student learning and maintaining academic
integrity when used in a non-punitive manner.
  These results underscore the need for a combined approach to addressing the
challenges posed by GenAI in academia to promote the responsible and equitable
use of these emerging technologies. The study concludes that the current
limitations of AI text detectors require a critical approach for any possible
implementation in HE and highlight possible alternatives to AI assessment
strategies.
\\ ( https://arxiv.org/abs/2403.19148 ,  525kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19177 (*cross-listing*)
Date: Thu, 28 Mar 2024 07:01:11 GMT   (2186kb,D)

Title: Rethinking Information Loss in Medical Image Segmentation with
  Various-sized Targets
Authors: Tianyi Liu, Zhaorui Tan, Kaizhu Huang, Haochuan Jiang
Categories: cs.CV cs.AI
\\
  Medical image segmentation presents the challenge of segmenting various-size
targets, demanding the model to effectively capture both local and global
information. Despite recent efforts using CNNs and ViTs to predict annotations
of different scales, these approaches often struggle to effectively balance the
detection of targets across varying sizes. Simply utilizing local information
from CNNs and global relationships from ViTs without considering potential
significant divergence in latent feature distributions may result in
substantial information loss. To address this issue, in this paper, we will
introduce a novel Stagger Network (SNet) and argues that a well-designed fusion
structure can mitigate the divergence in latent feature distributions between
CNNs and ViTs, thereby reducing information loss. Specifically, to emphasize
both global dependencies and local focus, we design a Parallel Module to bridge
the semantic gap. Meanwhile, we propose the Stagger Module, trying to fuse the
selected features that are more semantically similar. An Information Recovery
Module is further adopted to recover complementary information back to the
network. As a key contribution, we theoretically analyze that the proposed
parallel and stagger strategies would lead to less information loss, thus
certifying the SNet's rationale. Experimental results clearly proved that the
proposed SNet excels comparisons with recent SOTAs in segmenting on the Synapse
dataset where targets are in various sizes. Besides, it also demonstrates
superiority on the ACDC and the MoNuSeg datasets where targets are with more
consistent dimensions.
\\ ( https://arxiv.org/abs/2403.19177 ,  2186kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19178 (*cross-listing*)
Date: Thu, 28 Mar 2024 07:08:26 GMT   (1679kb,D)

Title: Enhancing Trust and Privacy in Distributed Networks: A Comprehensive
  Survey on Blockchain-based Federated Learning
Authors: Ji Liu, Chunlu Chen, Yu Li, Lin Sun, Yulun Song, Jingbo Zhou, Bo Jing,
  Dejing Dou
Categories: cs.CR cs.AI cs.DC cs.LG
Comments: 25 pages, accepted by KAIS 2024
\\
  While centralized servers pose a risk of being a single point of failure,
decentralized approaches like blockchain offer a compelling solution by
implementing a consensus mechanism among multiple entities. Merging distributed
computing with cryptographic techniques, decentralized technologies introduce a
novel computing paradigm. Blockchain ensures secure, transparent, and
tamper-proof data management by validating and recording transactions via
consensus across network nodes. Federated Learning (FL), as a distributed
machine learning framework, enables participants to collaboratively train
models while safeguarding data privacy by avoiding direct raw data exchange.
Despite the growing interest in decentralized methods, their application in FL
remains underexplored. This paper presents a thorough investigation into
Blockchain-based FL (BCFL), spotlighting the synergy between blockchain's
security features and FL's privacy-preserving model training capabilities.
First, we present the taxonomy of BCFL from three aspects, including
decentralized, separate networks, and reputation-based architectures. Then, we
summarize the general architecture of BCFL systems, providing a comprehensive
perspective on FL architectures informed by blockchain. Afterward, we analyze
the application of BCFL in healthcare, IoT, and other privacy-sensitive areas.
Finally, we identify future research directions of BCFL.
\\ ( https://arxiv.org/abs/2403.19178 ,  1679kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19221 (*cross-listing*)
Date: Thu, 28 Mar 2024 08:35:46 GMT   (3920kb,D)

Title: Towards Multimodal Video Paragraph Captioning Models Robust to Missing
  Modality
Authors: Sishuo Chen, Lei Li, Shuhuai Ren, Rundong Gao, Yuanxin Liu, Xiaohan
  Bi, Xu Sun, Lu Hou
Categories: cs.CV cs.AI
Comments: Code available at https://github.com/lancopku/MR-VPC
\\
  Video paragraph captioning (VPC) involves generating detailed narratives for
long videos, utilizing supportive modalities such as speech and event
boundaries. However, the existing models are constrained by the assumption of
constant availability of a single auxiliary modality, which is impractical
given the diversity and unpredictable nature of real-world scenarios. To this
end, we propose a Missing-Resistant framework MR-VPC that effectively harnesses
all available auxiliary inputs and maintains resilience even in the absence of
certain modalities. Under this framework, we propose the Multimodal VPC (MVPC)
architecture integrating video, speech, and event boundary inputs in a unified
manner to process various auxiliary inputs. Moreover, to fortify the model
against incomplete data, we introduce DropAM, a data augmentation strategy that
randomly omits auxiliary inputs, paired with DistillAM, a regularization target
that distills knowledge from teacher models trained on modality-complete data,
enabling efficient learning in modality-deficient environments. Through
exhaustive experimentation on YouCook2 and ActivityNet Captions, MR-VPC has
proven to deliver superior performance on modality-complete and
modality-missing test data. This work highlights the significance of developing
resilient VPC models and paves the way for more adaptive, robust multimodal
video understanding.
\\ ( https://arxiv.org/abs/2403.19221 ,  3920kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19238 (*cross-listing*)
Date: Thu, 28 Mar 2024 08:49:35 GMT   (16210kb,D)

Title: Taming Lookup Tables for Efficient Image Retouching
Authors: Sidi Yang, Binxiao Huang, Mingdeng Cao, Yatai Ji, Hanzhong Guo, Ngai
  Wong, Yujiu Yang
Categories: cs.CV cs.AI eess.IV
\\
  The widespread use of high-definition screens in edge devices, such as
end-user cameras, smartphones, and televisions, is spurring a significant
demand for image enhancement. Existing enhancement models often optimize for
high performance while falling short of reducing hardware inference time and
power consumption, especially on edge devices with constrained computing and
storage resources. To this end, we propose Image Color Enhancement Lookup Table
(ICELUT) that adopts LUTs for extremely efficient edge inference, without any
convolutional neural network (CNN). During training, we leverage pointwise
(1x1) convolution to extract color information, alongside a split fully
connected layer to incorporate global information. Both components are then
seamlessly converted into LUTs for hardware-agnostic deployment. ICELUT
achieves near-state-of-the-art performance and remarkably low power
consumption. We observe that the pointwise network structure exhibits robust
scalability, upkeeping the performance even with a heavily downsampled 32x32
input image. These enable ICELUT, the first-ever purely LUT-based image
enhancer, to reach an unprecedented speed of 0.4ms on GPU and 7ms on CPU, at
least one order faster than any CNN solution. Codes are available at
https://github.com/Stephen0808/ICELUT.
\\ ( https://arxiv.org/abs/2403.19238 ,  16210kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19271 (*cross-listing*)
Date: Thu, 28 Mar 2024 09:56:26 GMT   (197kb,D)

Title: DeepSample: DNN sampling-based testing for operational accuracy
  assessment
Authors: Antonio Guerriero, Roberto Pietrantuono, Stefano Russo
Categories: cs.SE cs.AI
Comments: Accepted for publication at ICSE 2024, Lisbon, Portugal
DOI: 10.1145/3597503.3639584
\\
  Deep Neural Networks (DNN) are core components for classification and
regression tasks of many software systems. Companies incur in high costs for
testing DNN with datasets representative of the inputs expected in operation,
as these need to be manually labelled. The challenge is to select a
representative set of test inputs as small as possible to reduce the labelling
cost, while sufficing to yield unbiased high-confidence estimates of the
expected DNN accuracy. At the same time, testers are interested in exposing as
many DNN mispredictions as possible to improve the DNN, ending up in the need
for techniques pursuing a threefold aim: small dataset size, trustworthy
estimates, mispredictions exposure. This study presents DeepSample, a family of
DNN testing techniques for cost-effective accuracy assessment based on
probabilistic sampling. We investigate whether, to what extent, and under which
conditions probabilistic sampling can help to tackle the outlined challenge. We
implement five new sampling-based testing techniques, and perform a
comprehensive comparison of such techniques and of three further
state-of-the-art techniques for both DNN classification and regression tasks.
Results serve as guidance for best use of sampling-based testing for faithful
and high-confidence estimates of DNN accuracy in operation at low cost.
\\ ( https://arxiv.org/abs/2403.19271 ,  197kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19316 (*cross-listing*)
Date: Thu, 28 Mar 2024 11:17:00 GMT   (3586kb,D)

Title: Hypergraph-based Multi-View Action Recognition using Event Cameras
Authors: Yue Gao, Jiaxuan Lu, Siqi Li, Yipeng Li, Shaoyi Du
Categories: cs.CV cs.AI cs.LG
Comments: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI 2024)
DOI: 10.1109/TPAMI.2024.3382117
\\
  Action recognition from video data forms a cornerstone with wide-ranging
applications. Single-view action recognition faces limitations due to its
reliance on a single viewpoint. In contrast, multi-view approaches capture
complementary information from various viewpoints for improved accuracy.
Recently, event cameras have emerged as innovative bio-inspired sensors,
leading to advancements in event-based action recognition. However, existing
works predominantly focus on single-view scenarios, leaving a gap in multi-view
event data exploitation, particularly in challenges like information deficit
and semantic misalignment. To bridge this gap, we introduce HyperMV, a
multi-view event-based action recognition framework. HyperMV converts discrete
event data into frame-like representations and extracts view-related features
using a shared convolutional network. By treating segments as vertices and
constructing hyperedges using rule-based and KNN-based strategies, a multi-view
hypergraph neural network that captures relationships across viewpoint and
temporal features is established. The vertex attention hypergraph propagation
is also introduced for enhanced feature fusion. To prompt research in this
area, we present the largest multi-view event-based action dataset
$\text{THU}^{\text{MV-EACT}}\text{-50}$, comprising 50 actions from 6
viewpoints, which surpasses existing datasets by over tenfold. Experimental
results show that HyperMV significantly outperforms baselines in both
cross-subject and cross-view scenarios, and also exceeds the state-of-the-arts
in frame-based multi-view action recognition.
\\ ( https://arxiv.org/abs/2403.19316 ,  3586kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19336 (*cross-listing*)
Date: Thu, 28 Mar 2024 11:52:42 GMT   (10912kb,D)

Title: IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot
  Navigation
Authors: Jiacui Huang, Hongtao Zhang, Mingbo Zhao, Zhou Wu
Categories: cs.CV cs.AI
\\
  Vision-and-Language Navigation (VLN) is a challenging task that requires a
robot to navigate in photo-realistic environments with human natural language
promptings. Recent studies aim to handle this task by constructing the semantic
spatial map representation of the environment, and then leveraging the strong
ability of reasoning in large language models for generalizing code for guiding
the robot navigation. However, these methods face limitations in instance-level
and attribute-level navigation tasks as they cannot distinguish different
instances of the same object. To address this challenge, we propose a new
method, namely, Instance-aware Visual Language Map (IVLMap), to empower the
robot with instance-level and attribute-level semantic mapping, where it is
autonomously constructed by fusing the RGBD video data collected from the robot
agent with special-designed natural language map indexing in the bird's-in-eye
view. Such indexing is instance-level and attribute-level. In particular, when
integrated with a large language model, IVLMap demonstrates the capability to
i) transform natural language into navigation targets with instance and
attribute information, enabling precise localization, and ii) accomplish
zero-shot end-to-end navigation tasks based on natural language commands.
Extensive navigation experiments are conducted. Simulation results illustrate
that our method can achieve an average improvement of 14.4\% in navigation
accuracy. Code and demo are released at https://ivlmap.github.io/.
\\ ( https://arxiv.org/abs/2403.19336 ,  10912kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19345 (*cross-listing*)
Date: Thu, 28 Mar 2024 12:02:45 GMT   (600kb)

Title: Intelligent Classification and Personalized Recommendation of E-commerce
  Products Based on Machine Learning
Authors: Kangming Xu, Huiming Zhou, Haotian Zheng, Mingwei Zhu, Qi Xin
Categories: cs.IR cs.AI
\\
  With the rapid evolution of the Internet and the exponential proliferation of
information, users encounter information overload and the conundrum of choice.
Personalized recommendation systems play a pivotal role in alleviating this
burden by aiding users in filtering and selecting information tailored to their
preferences and requirements. Such systems not only enhance user experience and
satisfaction but also furnish opportunities for businesses and platforms to
augment user engagement, sales, and advertising efficacy.This paper undertakes
a comparative analysis between the operational mechanisms of traditional
e-commerce commodity classification systems and personalized recommendation
systems. It delineates the significance and application of personalized
recommendation systems across e-commerce, content information, and media
domains. Furthermore, it delves into the challenges confronting personalized
recommendation systems in e-commerce, including data privacy, algorithmic bias,
scalability, and the cold start problem. Strategies to address these challenges
are elucidated.Subsequently, the paper outlines a personalized recommendation
system leveraging the BERT model and nearest neighbor algorithm, specifically
tailored to address the exigencies of the eBay e-commerce platform. The
efficacy of this recommendation system is substantiated through manual
evaluation, and a practical application operational guide and structured output
recommendation results are furnished to ensure the system's operability and
scalability.
\\ ( https://arxiv.org/abs/2403.19345 ,  600kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19347 (*cross-listing*)
Date: Thu, 28 Mar 2024 12:05:15 GMT   (3496kb,D)

Title: Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual
  User Behaviors
Authors: Binzong Geng, Zhaoxin Huan, Xiaolu Zhang, Yong He, Liang Zhang, Fajie
  Yuan, Jun Zhou, Linjian Mo
Categories: cs.IR cs.AI
Comments: Accepted by the 47th International ACM SIGIR Conference on Research
  and Development in Information Retrieval (SIGIR), 2024
\\
  With the rise of large language models (LLMs), recent works have leveraged
LLMs to improve the performance of click-through rate (CTR) prediction.
However, we argue that a critical obstacle remains in deploying LLMs for
practical use: the efficiency of LLMs when processing long textual user
behaviors. As user sequences grow longer, the current efficiency of LLMs is
inadequate for training on billions of users and items. To break through the
efficiency barrier of LLMs, we propose Behavior Aggregated Hierarchical
Encoding (BAHE) to enhance the efficiency of LLM-based CTR modeling.
Specifically, BAHE proposes a novel hierarchical architecture that decouples
the encoding of user behaviors from inter-behavior interactions. Firstly, to
prevent computational redundancy from repeated encoding of identical user
behaviors, BAHE employs the LLM's pre-trained shallow layers to extract
embeddings of the most granular, atomic user behaviors from extensive user
sequences and stores them in the offline database. Subsequently, the deeper,
trainable layers of the LLM facilitate intricate inter-behavior interactions,
thereby generating comprehensive user embeddings. This separation allows the
learning of high-level user representations to be independent of low-level
behavior encoding, significantly reducing computational complexity. Finally,
these refined user embeddings, in conjunction with correspondingly processed
item embeddings, are incorporated into the CTR model to compute the CTR scores.
Extensive experimental results show that BAHE reduces training time and memory
by five times for CTR models using LLMs, especially with longer user sequences.
BAHE has been deployed in a real-world system, allowing for daily updates of 50
million CTR data on 8 A100 GPUs, making LLMs practical for industrial CTR
prediction.
\\ ( https://arxiv.org/abs/2403.19347 ,  3496kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19376 (*cross-listing*)
Date: Thu, 28 Mar 2024 12:38:21 GMT   (15837kb,D)

Title: NIGHT -- Non-Line-of-Sight Imaging from Indirect Time of Flight Data
Authors: Matteo Caligiuri, Adriano Simonetto, Gianluca Agresti and Pietro
  Zanuttigh
Categories: cs.CV cs.AI eess.IV
Comments: Submitted to ECCV 24, 17 pages, 6 figures, 2 tables
\\
  The acquisition of objects outside the Line-of-Sight of cameras is a very
intriguing but also extremely challenging research topic. Recent works showed
the feasibility of this idea exploiting transient imaging data produced by
custom direct Time of Flight sensors. In this paper, for the first time, we
tackle this problem using only data from an off-the-shelf indirect Time of
Flight sensor without any further hardware requirement. We introduced a Deep
Learning model able to re-frame the surfaces where light bounces happen as a
virtual mirror. This modeling makes the task easier to handle and also
facilitates the construction of annotated training data. From the obtained data
it is possible to retrieve the depth information of the hidden scene. We also
provide a first-in-its-kind synthetic dataset for the task and demonstrate the
feasibility of the proposed idea over it.
\\ ( https://arxiv.org/abs/2403.19376 ,  15837kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19386 (*cross-listing*)
Date: Thu, 28 Mar 2024 12:51:15 GMT   (1680kb,D)

Title: PointCloud-Text Matching: Benchmark Datasets and a Baseline
Authors: Yanglin Feng, Yang Qin, Dezhong Peng, Hongyuan Zhu, Xi Peng, Peng Hu
Categories: cs.CV cs.AI
\\
  In this paper, we present and study a new instance-level retrieval task:
PointCloud-Text Matching~(PTM), which aims to find the exact cross-modal
instance that matches a given point-cloud query or text query. PTM could be
applied to various scenarios, such as indoor/urban-canyon localization and
scene retrieval. However, there exists no suitable and targeted dataset for PTM
in practice. Therefore, we construct three new PTM benchmark datasets, namely
3D2T-SR, 3D2T-NR, and 3D2T-QA. We observe that the data is challenging and with
noisy correspondence due to the sparsity, noise, or disorder of point clouds
and the ambiguity, vagueness, or incompleteness of texts, which make existing
cross-modal matching methods ineffective for PTM. To tackle these challenges,
we propose a PTM baseline, named Robust PointCloud-Text Matching method (RoMa).
RoMa consists of two modules: a Dual Attention Perception module (DAP) and a
Robust Negative Contrastive Learning module (RNCL). Specifically, DAP leverages
token-level and feature-level attention to adaptively focus on useful local and
global features, and aggregate them into common representations, thereby
reducing the adverse impact of noise and ambiguity. To handle noisy
correspondence, RNCL divides negative pairs, which are much less error-prone
than positive pairs, into clean and noisy subsets, and assigns them forward and
reverse optimization directions respectively, thus enhancing robustness against
noisy correspondence. We conduct extensive experiments on our benchmarks and
demonstrate the superiority of our RoMa.
\\ ( https://arxiv.org/abs/2403.19386 ,  1680kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19459 (*cross-listing*)
Date: Thu, 28 Mar 2024 14:31:01 GMT   (4682kb,D)

Title: NeuroLGP-SM: A Surrogate-assisted Neuroevolution Approach using Linear
  Genetic Programming
Authors: Fergal Stapleton, Brendan Cody-Kenny and Edgar Galv\'an
Categories: cs.NE cs.AI
Comments: Accepted in "International Conference on Optimization and Learning
  (OLA), Dubrovnik, Croatia, 2024", 13 pages, 4 figures, 1 table
\\
  Evolutionary algorithms are increasingly recognised as a viable computational
approach for the automated optimisation of deep neural networks (DNNs) within
artificial intelligence. This method extends to the training of DNNs, an
approach known as neuroevolution. However, neuroevolution is an inherently
resource-intensive process, with certain studies reporting the consumption of
thousands of GPU days for refining and training a single DNN network. To
address the computational challenges associated with neuroevolution while still
attaining good DNN accuracy, surrogate models emerge as a pragmatic solution.
Despite their potential, the integration of surrogate models into
neuroevolution is still in its early stages, hindered by factors such as the
effective use of high-dimensional data and the representation employed in
neuroevolution. In this context, we address these challenges by employing a
suitable representation based on Linear Genetic Programming, denoted as
NeuroLGP, and leveraging Kriging Partial Least Squares. The amalgamation of
these two techniques culminates in our proposed methodology known as the
NeuroLGP-Surrogate Model (NeuroLGP-SM). For comparison purposes, we also code
and use a baseline approach incorporating a repair mechanism, a common practice
in neuroevolution. Notably, the baseline approach surpasses the renowned VGG-16
model in accuracy. Given the computational intensity inherent in DNN
operations, a singular run is typically the norm. To evaluate the efficacy of
our proposed approach, we conducted 96 independent runs. Significantly, our
methodologies consistently outperform the baseline, with the SM model
demonstrating superior accuracy or comparable results to the NeuroLGP approach.
Noteworthy is the additional advantage that the SM approach exhibits a 25%
reduction in computational requirements, further emphasising its efficiency for
neuroevolution.
\\ ( https://arxiv.org/abs/2403.19459 ,  4682kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19460 (*cross-listing*)
Date: Thu, 28 Mar 2024 14:31:10 GMT   (9505kb,D)

Title: RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without
  Point Cloud Segmentation
Authors: Chongkai Gao, Zhengrong Xue, Shuying Deng, Tianhai Liang, Siqi Yang,
  Lin Shao, Huazhe Xu
Categories: cs.RO cs.AI
\\
  We present RiEMann, an end-to-end near Real-time SE(3)-Equivariant Robot
Manipulation imitation learning framework from scene point cloud input.
Compared to previous methods that rely on descriptor field matching, RiEMann
directly predicts the target poses of objects for manipulation without any
object segmentation. RiEMann learns a manipulation task from scratch with 5 to
10 demonstrations, generalizes to unseen SE(3) transformations and instances of
target objects, resists visual interference of distracting objects, and follows
the near real-time pose change of the target object. The scalable action space
of RiEMann facilitates the addition of custom equivariant actions such as the
direction of turning the faucet, which makes articulated object manipulation
possible for RiEMann. In simulation and real-world 6-DOF robot manipulation
experiments, we test RiEMann on 5 categories of manipulation tasks with a total
of 25 variants and show that RiEMann outperforms baselines in both task success
rates and SE(3) geodesic distance errors on predicted poses (reduced by 68.6%),
and achieves a 5.4 frames per second (FPS) network inference speed. Code and
video results are available at https://riemann-web.github.io/.
\\ ( https://arxiv.org/abs/2403.19460 ,  9505kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19545 (*cross-listing*)
Date: Thu, 28 Mar 2024 16:27:20 GMT   (43025kb,D)

Title: Lamarckian Inheritance Improves Robot Evolution in Dynamic Environments
Authors: Jie Luo, Karine Miras, Carlo Longhi, Oliver Weissl, Agoston E. Eiben
Categories: cs.RO cs.AI
Comments: Nature. arXiv admin note: substantial text overlap with
  arXiv:2309.13099; text overlap with arXiv:2303.12594, arXiv:2309.14387
\\
  This study explores the integration of Lamarckian system into evolutionary
robotics (ER), comparing it with the traditional Darwinian model across various
environments. By adopting Lamarckian principles, where robots inherit learned
traits, alongside Darwinian learning without inheritance, we investigate
adaptation in dynamic settings. Our research, conducted in six distinct
environmental setups, demonstrates that Lamarckian systems outperform Darwinian
ones in adaptability and efficiency, particularly in challenging conditions.
Our analysis highlights the critical role of the interplay between controller
\& morphological evolution and environment adaptation, with parent-offspring
similarities and newborn \&survivors before and after learning providing
insights into the effectiveness of trait inheritance. Our findings suggest
Lamarckian principles could significantly advance autonomous system design,
highlighting the potential for more adaptable and robust robotic solutions in
complex, real-world applications. These theoretical insights were validated
using real physical robots, bridging the gap between simulation and practical
application.
\\ ( https://arxiv.org/abs/2403.19545 ,  43025kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19584 (*cross-listing*)
Date: Thu, 28 Mar 2024 17:07:02 GMT   (4192kb,D)

Title: Img2Loc: Revisiting Image Geolocalization using Multi-modality
  Foundation Models and Image-based Retrieval-Augmented Generation
Authors: Zhongliang Zhou, Jielu Zhang, Zihan Guan, Mengxuan Hu, Ni Lao, Lan Mu,
  Sheng Li, Gengchen Mai
Categories: cs.CV cs.AI
\\
  Geolocating precise locations from images presents a challenging problem in
computer vision and information retrieval.Traditional methods typically employ
either classification, which dividing the Earth surface into grid cells and
classifying images accordingly, or retrieval, which identifying locations by
matching images with a database of image-location pairs. However,
classification-based approaches are limited by the cell size and cannot yield
precise predictions, while retrieval-based systems usually suffer from poor
search quality and inadequate coverage of the global landscape at varied scale
and aggregation levels. To overcome these drawbacks, we present Img2Loc, a
novel system that redefines image geolocalization as a text generation task.
This is achieved using cutting-edge large multi-modality models like GPT4V or
LLaVA with retrieval augmented generation. Img2Loc first employs CLIP-based
representations to generate an image-based coordinate query database. It then
uniquely combines query results with images itself, forming elaborate prompts
customized for LMMs. When tested on benchmark datasets such as Im2GPS3k and
YFCC4k, Img2Loc not only surpasses the performance of previous state-of-the-art
models but does so without any model training.
\\ ( https://arxiv.org/abs/2403.19584 ,  4192kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19595 (*cross-listing*)
Date: Thu, 28 Mar 2024 17:19:16 GMT   (4634kb,D)

Title: Situation Awareness for Driver-Centric Driving Style Adaptation
Authors: Johann Haselberger and Bonifaz Stuhr and Bernhard Schick and Steffen
  M\"uller
Categories: cs.CV cs.AI cs.SY eess.SY
Comments: 14 pages, 6 figures. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
\\
  There is evidence that the driving style of an autonomous vehicle is
important to increase the acceptance and trust of the passengers. The driving
situation has been found to have a significant influence on human driving
behavior. However, current driving style models only partially incorporate
driving environment information, limiting the alignment between an agent and
the given situation. Therefore, we propose a situation-aware driving style
model based on different visual feature encoders pretrained on fleet data, as
well as driving behavior predictors, which are adapted to the driving style of
a specific driver. Our experiments show that the proposed method outperforms
static driving styles significantly and forms plausible situation clusters.
Furthermore, we found that feature encoders pretrained on our dataset lead to
more precise driving behavior modeling. In contrast, feature encoders
pretrained supervised and unsupervised on different data sources lead to more
specific situation clusters, which can be utilized to constrain and control the
driving style adaptation for specific situations. Moreover, in a real-world
setting, where driving style adaptation is happening iteratively, we found the
MLP-based behavior predictors achieve good performance initially but suffer
from catastrophic forgetting. In contrast, behavior predictors based on
situationdependent statistics can learn iteratively from continuous data
streams by design. Overall, our experiments show that important information for
driving behavior prediction is contained within the visual feature encoder. The
dataset is publicly available at
huggingface.co/datasets/jHaselberger/SADC-Situation-Awareness-for-Driver-Centric-Driving-Style-Adaptation.
\\ ( https://arxiv.org/abs/2403.19595 ,  4634kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19620 (*cross-listing*)
Date: Thu, 28 Mar 2024 17:40:15 GMT   (19703kb,D)

Title: Collaborative Interactive Evolution of Art in the Latent Space of Deep
  Generative Models
Authors: Ole Hall and Anil Yaman
Categories: cs.NE cs.AI cs.CV cs.HC cs.LG
Comments: Preprint. The Version of Record of this contribution is to be
  published in the proceedings of the 13th International Conference on
  Artificial Intelligence in Music, Sound, Art and Design (EvoMUSART) 2024
\\
  Generative Adversarial Networks (GANs) have shown great success in generating
high quality images and are thus used as one of the main approaches to generate
art images. However, usually the image generation process involves sampling
from the latent space of the learned art representations, allowing little
control over the output. In this work, we first employ GANs that are trained to
produce creative images using an architecture known as Creative Adversarial
Networks (CANs), then, we employ an evolutionary approach to navigate within
the latent space of the models to discover images. We use automatic aesthetic
and collaborative interactive human evaluation metrics to assess the generated
images. In the human interactive evaluation case, we propose a collaborative
evaluation based on the assessments of several participants. Furthermore, we
also experiment with an intelligent mutation operator that aims to improve the
quality of the images through local search based on an aesthetic measure. We
evaluate the effectiveness of this approach by comparing the results produced
by the automatic and collaborative interactive evolution. The results show that
the proposed approach can generate highly attractive art images when the
evolution is guided by collaborative human feedback.
\\ ( https://arxiv.org/abs/2403.19620 ,  19703kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19648 (*cross-listing*)
Date: Thu, 28 Mar 2024 17:56:56 GMT   (6680kb,D)

Title: Human-compatible driving partners through data-regularized self-play
  reinforcement learning
Authors: Daphne Cornelisse, Eugene Vinitsky
Categories: cs.RO cs.AI cs.LG cs.MA
\\
  A central challenge for autonomous vehicles is coordinating with humans.
Therefore, incorporating realistic human agents is essential for scalable
training and evaluation of autonomous driving systems in simulation. Simulation
agents are typically developed by imitating large-scale, high-quality datasets
of human driving. However, pure imitation learning agents empirically have high
collision rates when executed in a multi-agent closed-loop setting. To build
agents that are realistic and effective in closed-loop settings, we propose
Human-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are
trained through self-play with a small penalty for deviating from a human
reference policy. In contrast to prior work, our approach is RL-first and only
uses 30 minutes of imperfect human demonstrations. We evaluate agents in a
large set of multi-agent traffic scenes. Results show our HR-PPO agents are
highly effective in achieving goals, with a success rate of 93%, an off-road
rate of 3.5%, and a collision rate of 3%. At the same time, the agents drive in
a human-like manner, as measured by their similarity to existing human driving
logs. We also find that HR-PPO agents show considerable improvements on proxy
measures for coordination with human driving, particularly in highly
interactive scenarios. We open-source our code and trained agents at
https://github.com/Emerge-Lab/nocturne_lab and provide demonstrations of agent
behaviors at https://sites.google.com/view/driving-partners.
\\ ( https://arxiv.org/abs/2403.19648 ,  6680kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19651 (*cross-listing*)
Date: Thu, 28 Mar 2024 17:59:20 GMT   (9612kb,D)

Title: MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions
Authors: Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen,
  Yu Su, Ming-Wei Chang
Categories: cs.CV cs.AI cs.CL cs.IR cs.MM
Comments: Work in progress
\\
  Image retrieval, i.e., finding desired images given a reference image,
inherently encompasses rich, multi-faceted search intents that are difficult to
capture solely using image-based measures. Recent work leverages text
instructions to allow users to more freely express their search intents.
However, existing work primarily focuses on image pairs that are visually
similar and/or can be characterized by a small set of pre-defined relations.
The core thesis of this paper is that text instructions can enable retrieving
images with richer relations beyond visual similarity. To show this, we
introduce MagicLens, a series of self-supervised image retrieval models that
support open-ended instructions. MagicLens is built on a key novel insight:
image pairs that naturally occur on the same web pages contain a wide range of
implicit relations (e.g., inside view of), and we can bring those implicit
relations explicit by synthesizing instructions via large multimodal models
(LMMs) and large language models (LLMs). Trained on 36.7M (query image,
instruction, target image) triplets with rich semantic relations mined from the
web, MagicLens achieves comparable or better results on eight benchmarks of
various image retrieval tasks than prior state-of-the-art (SOTA) methods.
Remarkably, it outperforms previous SOTA but with a 50X smaller model size on
multiple benchmarks. Additional human analyses on a 1.4M-image unseen corpus
further demonstrate the diversity of search intents supported by MagicLens.
\\ ( https://arxiv.org/abs/2403.19651 ,  9612kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19652 (*cross-listing*)
Date: Thu, 28 Mar 2024 17:59:30 GMT   (20738kb,D)

Title: InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction
Authors: Sirui Xu, Ziyin Wang, Yu-Xiong Wang, Liang-Yan Gui
Categories: cs.CV cs.AI
Comments: Project Page: https://sirui-xu.github.io/InterDreamer/
\\
  Text-conditioned human motion generation has experienced significant
advancements with diffusion models trained on extensive motion capture data and
corresponding textual annotations. However, extending such success to 3D
dynamic human-object interaction (HOI) generation faces notable challenges,
primarily due to the lack of large-scale interaction data and comprehensive
descriptions that align with these interactions. This paper takes the
initiative and showcases the potential of generating human-object interactions
without direct training on text-interaction pair data. Our key insight in
achieving this is that interaction semantics and dynamics can be decoupled.
Being unable to learn interaction semantics through supervised training, we
instead leverage pre-trained large models, synergizing knowledge from a large
language model and a text-to-motion model. While such knowledge offers
high-level control over interaction semantics, it cannot grasp the intricacies
of low-level interaction dynamics. To overcome this issue, we further introduce
a world model designed to comprehend simple physics, modeling how human actions
influence object motion. By integrating these components, our novel framework,
InterDreamer, is able to generate text-aligned 3D HOI sequences in a zero-shot
manner. We apply InterDreamer to the BEHAVE and CHAIRS datasets, and our
comprehensive experimental analysis demonstrates its capability to generate
realistic and coherent interaction sequences that seamlessly align with the
text directives.
\\ ( https://arxiv.org/abs/2403.19652 ,  20738kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18843 (*cross-listing*)
Date: Mon, 4 Mar 2024 00:30:24 GMT   (96kb,D)

Title: JEP-KD: Joint-Embedding Predictive Architecture Based Knowledge
  Distillation for Visual Speech Recognition
Authors: Chang Sun, Hong Yang, Bo Qin
Categories: cs.CV cs.CL cs.LG cs.SD eess.AS
\\
  Visual Speech Recognition (VSR) tasks are generally recognized to have a
lower theoretical performance ceiling than Automatic Speech Recognition (ASR),
owing to the inherent limitations of conveying semantic information visually.
To mitigate this challenge, this paper introduces an advanced knowledge
distillation approach using a Joint-Embedding Predictive Architecture (JEPA),
named JEP-KD, designed to more effectively utilize audio features during model
training. Central to JEP-KD is the inclusion of a generative network within the
embedding layer, which enhances the video encoder's capacity for semantic
feature extraction and brings it into closer alignment with the audio features
from a pre-trained ASR model's encoder. This approach aims to progressively
reduce the performance gap between VSR and ASR. Moreover, a comprehensive
multimodal, multistage training regimen for the JEP-KD framework is
established, bolstering the robustness and efficacy of the training process.
Experiment results demonstrate that JEP-KD significantly improves the
performance of VSR models and demonstrates versatility across different VSR
platforms, indicating its potential for broader application within other
multimodal tasks.
\\ ( https://arxiv.org/abs/2403.18843 ,  96kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18957 (*cross-listing*)
Date: Wed, 27 Mar 2024 19:02:13 GMT   (6845kb,D)

Title: Moderating Illicit Online Image Promotion for Unsafe User-Generated
  Content Games Using Large Vision-Language Models
Authors: Keyan Guo, Ayush Utkarsh, Wenbo Ding, Isabelle Ondracek, Ziming Zhao,
  Guo Freeman, Nishant Vishwamitra, Hongxin Hu
Categories: cs.CY cs.CL cs.LG cs.SI
Comments: To Appear in the 33rd USENIX Security Symposium, August 14-16, 2024
\\
  Online user-generated content games (UGCGs) are increasingly popular among
children and adolescents for social interaction and more creative online
entertainment. However, they pose a heightened risk of exposure to explicit
content, raising growing concerns for the online safety of children and
adolescents. Despite these concerns, few studies have addressed the issue of
illicit image-based promotions of unsafe UGCGs on social media, which can
inadvertently attract young users. This challenge arises from the difficulty of
obtaining comprehensive training data for UGCG images and the unique nature of
these images, which differ from traditional unsafe content. In this work, we
take the first step towards studying the threat of illicit promotions of unsafe
UGCGs. We collect a real-world dataset comprising 2,924 images that display
diverse sexually explicit and violent content used to promote UGCGs by their
game creators. Our in-depth studies reveal a new understanding of this problem
and the urgent need for automatically flagging illicit UGCG promotions. We
additionally create a cutting-edge system, UGCG-Guard, designed to aid social
media platforms in effectively identifying images used for illicit UGCG
promotions. This system leverages recently introduced large vision-language
models (VLMs) and employs a novel conditional prompting strategy for zero-shot
domain adaptation, along with chain-of-thought (CoT) reasoning for contextual
identification. UGCG-Guard achieves outstanding results, with an accuracy rate
of 94% in detecting these images used for the illicit promotion of such games
in real-world scenarios.
\\ ( https://arxiv.org/abs/2403.18957 ,  6845kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19114 (*cross-listing*)
Date: Thu, 28 Mar 2024 03:10:39 GMT   (1510kb,D)

Title: Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval:
  Evolving Coding Benchmarks via LLM
Authors: Chunqiu Steven Xia, Yinlin Deng, Lingming Zhang
Categories: cs.SE cs.CL cs.LG cs.PL
\\
  LLMs have become the go-to choice for code generation tasks, with an
exponential increase in the training, development, and usage of LLMs
specifically for code generation. To evaluate the ability of LLMs on code, both
academic and industry practitioners rely on popular handcrafted benchmarks.
However, prior benchmarks contain only a very limited set of problems, both in
quantity and variety. Further, due to popularity and age, many benchmarks are
prone to data leakage where example solutions can be readily found on the web
and thus potentially in training data. Such limitations inevitably lead us to
inquire: Is the leaderboard performance on existing benchmarks reliable and
comprehensive enough to measure the program synthesis ability of LLMs? To
address this, we introduce EvoEval -- a program synthesis benchmark suite
created by evolving existing benchmarks into different targeted domains for a
comprehensive evaluation of LLM coding abilities. Our study on 51 LLMs shows
that compared to the high performance obtained on standard benchmarks like
HumanEval, there is a significant drop in performance (on average 39.4%) when
using EvoEval. Additionally, the decrease in performance can range from 19.6%
to 47.7%, leading to drastic ranking changes amongst LLMs and showing potential
overfitting of existing benchmarks. Furthermore, we showcase various insights,
including the brittleness of instruction-following models when encountering
rewording or subtle changes as well as the importance of learning problem
composition and decomposition. EvoEval not only provides comprehensive
benchmarks, but can be used to further evolve arbitrary problems to keep up
with advances and the ever-changing landscape of LLMs for code. We have
open-sourced our benchmarks, tools, and complete LLM generations at
https://github.com/evo-eval/evoeval
\\ ( https://arxiv.org/abs/2403.19114 ,  1510kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19181 (*cross-listing*)
Date: Thu, 28 Mar 2024 07:22:16 GMT   (384kb,D)

Title: Make Large Language Model a Better Ranker
Authors: Wenshuo Chao, Zhi Zheng, Hengshu Zhu, Hao Liu
Categories: cs.IR cs.CL cs.LG
Comments: 10 pages, 5 figures
\\
  The evolution of Large Language Models (LLMs) has significantly enhanced
capabilities across various fields, leading to a paradigm shift in how
Recommender Systems (RSs) are conceptualized and developed. However, existing
research primarily focuses on point-wise and pair-wise recommendation
paradigms. These approaches prove inefficient in LLM-based recommenders due to
the high computational cost of utilizing Large Language Models. While some
studies have delved into list-wise approaches, they fall short in ranking
tasks. This shortfall is attributed to the misalignment between the objectives
of ranking and language generation. To this end, this paper introduces the
Language Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO
is designed to bridge the gap between the capabilities of LLMs and the nuanced
requirements of ranking tasks within recommender systems. A key feature of ALRO
is the introduction of soft lambda loss, an adaptation of lambda loss tailored
to suit language generation tasks. Additionally, ALRO incorporates a
permutation-sensitive learning mechanism that addresses position bias, a
prevalent issue in generative models, without imposing additional computational
burdens during inference. Our evaluative studies reveal that ALRO outperforms
existing embedding-based recommendation methods and the existing LLM-based
recommendation baselines, highlighting its efficacy.
\\ ( https://arxiv.org/abs/2403.19181 ,  384kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19201 (*cross-listing*)
Date: Thu, 28 Mar 2024 07:55:29 GMT   (3588kb)

Title: Understanding Archives: Towards New Research Interfaces Relying on the
  Semantic Annotation of Documents
Authors: Nicolas Gutehrl\'e (CRIT), Iana Atanassova (CRIT, STIH, TESNIERE,
  LaLIC)
Categories: cs.DL cs.CL
Comments: in French language. CiDE.23: Document et archivage: pratiques
  formelles et informelles, Oct 2023, Grenoble, France
\\
  The digitisation campaigns carried out by libraries and archives in recent
years have facilitated access to documents in their collections. However,
exploring and exploiting these documents remain difficult tasks due to the
sheer quantity of documents available for consultation. In this article, we
show how the semantic annotation of the textual content of study corpora of
archival documents allow to facilitate their exploitation and valorisation.
First, we present a methodological framework for the construction of new
interfaces based on textual semantics, then address the current technological
obstacles and their potential solutions. We conclude by presenting a practical
case of the application of this framework.
\\ ( https://arxiv.org/abs/2403.19201 ,  3588kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19322 (*cross-listing*)
Date: Thu, 28 Mar 2024 11:26:30 GMT   (2468kb,D)

Title: Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models
Authors: Jiaxing Chen, Yuxuan Liu, Dehu Li, Xiang An, Ziyong Feng, Yongle Zhao,
  Yin Xie
Categories: cs.CV cs.CL
Comments: 14 pages, 3 figures
\\
  The surge of Multimodal Large Language Models (MLLMs), given their prominent
emergent capabilities in instruction following and reasoning, has greatly
advanced the field of visual reasoning. However, constrained by their
non-lossless image tokenization, most MLLMs fall short of comprehensively
capturing details of text and objects, especially in high-resolution images. To
address this, we propose P2G, a novel framework for plug-and-play grounding of
reasoning in MLLMs. Specifically, P2G exploits the tool-usage potential of
MLLMs to employ expert agents to achieve on-the-fly grounding to critical
visual and textual objects of image, thus achieving deliberate reasoning via
multimodal prompting. We further create P2GB, a benchmark aimed at assessing
MLLMs' ability to understand inter-object relationships and text in challenging
high-resolution images. Comprehensive experiments on visual reasoning tasks
demonstrate the superiority of P2G. Noteworthy, P2G achieved comparable
performance with GPT-4V on P2GB, with a 7B backbone. Our work highlights the
potential of plug-and-play grounding of reasoning and opens up a promising
alternative beyond model scaling.
\\ ( https://arxiv.org/abs/2403.19322 ,  2468kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19423 (*cross-listing*)
Date: Thu, 28 Mar 2024 13:55:51 GMT   (3235kb,D)

Title: Echo-chambers and Idea Labs: Communication Styles on Twitter
Authors: Aleksandra Sorokovikova, Michael Becker, Ivan P. Yamshchikov
Categories: cs.SI cs.CL
ACM-class: J.4; K.4.1; K.4.2
\\
  This paper investigates the communication styles and structures of Twitter
(X) communities within the vaccination context. While mainstream research
primarily focuses on the echo-chamber phenomenon, wherein certain ideas are
reinforced and participants are isolated from opposing opinions, this study
reveals the presence of diverse communication styles across various
communities. In addition to the communities exhibiting echo-chamber behavior,
this research uncovers communities with distinct communication patterns. By
shedding light on the nuanced nature of communication within social networks,
this study emphasizes the significance of understanding the diversity of
perspectives within online communities.
\\ ( https://arxiv.org/abs/2403.19423 ,  3235kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19634 (*cross-listing*)
Date: Thu, 28 Mar 2024 17:49:31 GMT   (37kb)

Title: Asymmetric and trial-dependent modeling: the contribution of LIA to SdSV
  Challenge Task 2
Authors: Pierre-Michel Bousquet, Mickael Rouvier
Categories: cs.SD cs.CL eess.AS
Comments: LIA system description for the Short Duration Speaker Verification
  (SdSv) challenge 2020 Task 2
\\
  The SdSv challenge Task 2 provided an opportunity to assess efficiency and
robustness of modern text-independent speaker verification systems. But it also
made it possible to test new approaches, capable of taking into account the
main issues of this challenge (duration, language, ...). This paper describes
the contributions of our laboratory to the speaker recognition field. These
contributions highlight two other challenges in addition to short-duration and
language: the mismatch between enrollment and test data and the one between
subsets of the evaluation trial dataset. The proposed approaches experimentally
show their relevance and efficiency on the SdSv evaluation, and could be of
interest in many real-life applications.
\\ ( https://arxiv.org/abs/2403.19634 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18438 (*cross-listing*)
Date: Thu, 30 Nov 2023 10:39:47 GMT   (1706kb)
Date (revised v2): Fri, 22 Mar 2024 13:26:52 GMT   (952kb,D)

Title: Solution-Set Geometry and Regularization Path of a Nonconvexly
  Regularized Convex Sparse Model
Authors: Yi Zhang and Isao Yamada
Categories: math.OC cs.LG eess.SP math.ST stat.TH
Comments: 53 pages, 10 figures. Submitted to journal
\\
  The generalized minimax concave (GMC) penalty is a nonconvex sparse
regularizer which can preserve the overall-convexity of the regularized
least-squares problem. In this paper, we focus on a significant instance of the
GMC model termed scaled GMC (sGMC), and present various notable findings on its
solution-set geometry and regularization path. Our investigation indicates that
while the sGMC penalty is a nonconvex extension of the LASSO penalty (i.e., the
$\ell_1$-norm), the sGMC model preserves many celebrated properties of the
LASSO model, hence can serve as a less biased surrogate of LASSO without losing
its advantages. Specifically, for a fixed regularization parameter $\lambda$,
we show that the solution-set geometry, solution uniqueness and sparseness of
the sGMC model can be characterized in a similar elegant way to the LASSO model
(see, e.g., Osborne et al. 2000, R. J. Tibshirani 2013). For a varying
$\lambda$, we prove that the sGMC solution set is a continuous polytope-valued
mapping of $\lambda$. Most noticeably, our study indicates that similar to
LASSO, the minimum $\ell_2$-norm regularization path of the sGMC model is
continuous and piecewise linear in $\lambda$. Based on these theoretical
results, an efficient regularization path algorithm is proposed for the sGMC
model, extending the well-known least angle regression (LARS) algorithm for
LASSO. We prove the correctness and finite termination of the proposed
algorithm under a mild assumption, and confirm its
correctness-in-general-situation, efficiency, and practical utility through
numerical experiments. Many results in this study also contribute to the
theoretical research of LASSO.
\\ ( https://arxiv.org/abs/2311.18438 ,  952kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18822 (*cross-listing*)
Date: Sat, 9 Dec 2023 07:53:25 GMT   (415kb)

Title: Enhancing Financial Data Visualization for Investment Decision-Making
Authors: Nisarg Patel, Harmit Shah, Kishan Mewada
Categories: q-fin.TR cs.LG
Comments: 5 pages, 10 figures
\\
  Navigating the intricate landscape of financial markets requires adept
forecasting of stock price movements. This paper delves into the potential of
Long Short-Term Memory (LSTM) networks for predicting stock dynamics, with a
focus on discerning nuanced rise and fall patterns. Leveraging a dataset from
the New York Stock Exchange (NYSE), the study incorporates multiple features to
enhance LSTM's capacity in capturing complex patterns. Visualization of key
attributes, such as opening, closing, low, and high prices, aids in unraveling
subtle distinctions crucial for comprehensive market understanding. The
meticulously crafted LSTM input structure, inspired by established guidelines,
incorporates both price and volume attributes over a 25-day time step, enabling
the model to capture temporal intricacies. A comprehensive methodology,
including hyperparameter tuning with Grid Search, Early Stopping, and Callback
mechanisms, leads to a remarkable 53% improvement in predictive accuracy. The
study concludes with insights into model robustness, contributions to financial
forecasting literature, and a roadmap for real-time stock market prediction.
The amalgamation of LSTM networks, strategic hyperparameter tuning, and
informed feature selection presents a potent framework for advancing the
accuracy of stock price predictions, contributing substantively to financial
time series forecasting discourse.
\\ ( https://arxiv.org/abs/2403.18822 ,  415kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18833 (*cross-listing*)
Date: Wed, 7 Feb 2024 21:15:53 GMT   (5147kb)

Title: A New Method for Sensorless Estimation of the Speed and Position in
  Brushed DC Motors Using Support Vector Machines
Authors: Ernesto Vazquez-Sanchez, Jaime Gomez-Gil, Jose-Carlos Gamazo-Real,
  Jose Fernando Diez-Higuera
Categories: eess.SP cs.LG cs.SY eess.SY
Journal-ref: IEEE Transactions on Industrial Electronics, 2012, vol. 59, no. 3,
  pp. 1397-1408, ISSN 0278-0046
DOI: 10.1109/TIE.2011.2161651
\\
  Currently, for many applications, it is necessary to know the speed and
position of motors. This can be achieved using mechanical sensors coupled to
the motor shaft or using sensorless techniques. The sensorless techniques in
brushed dc motors can be classified into two types: 1) techniques based on the
dynamic brushed dc motor model and 2) techniques based on the ripple component
of the current. This paper presents a new method, based on the ripple
component, for speed and position estimation in brushed dc motors, using
support vector machines. The proposed method only measures the current and
detects the pulses in this signal. The motor speed is estimated by using the
inverse distance between the detected pulses, and the position is estimated by
counting all detected pulses. The ability to detect ghost pulses and to discard
false pulses is the main advantage of this method over other sensorless
methods. The performed tests on two fractional horsepower brushed dc motors
indicate that the method works correctly in a wide range of speeds and
situations, in which the speed is constant or varies dynamically.
\\ ( https://arxiv.org/abs/2403.18833 ,  5147kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18839 (*cross-listing*)
Date: Fri, 23 Feb 2024 12:59:49 GMT   (777kb,D)

Title: Long Short-Term Memory Pattern Recognition in Currency Trading
Authors: Jai Pal
Categories: q-fin.TR cs.LG
Comments: 10 Pages, 8 Figures, 4 Listings
\\
  This study delves into the analysis of financial markets through the lens of
Wyckoff Phases, a framework devised by Richard D. Wyckoff in the early 20th
century. Focusing on the accumulation pattern within the Wyckoff framework, the
research explores the phases of trading range and secondary test, elucidating
their significance in understanding market dynamics and identifying potential
trading opportunities. By dissecting the intricacies of these phases, the study
sheds light on the creation of liquidity through market structure, offering
insights into how traders can leverage this knowledge to anticipate price
movements and make informed decisions. The effective detection and analysis of
Wyckoff patterns necessitate robust computational models capable of processing
complex market data, with spatial data best analyzed using Convolutional Neural
Networks (CNNs) and temporal data through Long Short-Term Memory (LSTM) models.
The creation of training data involves the generation of swing points,
representing significant market movements, and filler points, introducing noise
and enhancing model generalization. Activation functions, such as the sigmoid
function, play a crucial role in determining the output behavior of neural
network models. The results of the study demonstrate the remarkable efficacy of
deep learning models in detecting Wyckoff patterns within financial data,
underscoring their potential for enhancing pattern recognition and analysis in
financial markets. In conclusion, the study highlights the transformative
potential of AI-driven approaches in financial analysis and trading strategies,
with the integration of AI technologies shaping the future of trading and
investment practices.
\\ ( https://arxiv.org/abs/2403.18839 ,  777kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18840 (*cross-listing*)
Date: Wed, 28 Feb 2024 03:45:55 GMT   (2551kb,D)

Title: Feynman Diagrams as Computational Graphs
Authors: Pengcheng Hou, Tao Wang, Daniel Cerkoney, Xiansheng Cai, Zhiyi Li,
  Youjin Deng, Lei Wang, and Kun Chen
Categories: hep-th cond-mat.str-el cs.LG hep-ph physics.comp-ph
\\
  We propose a computational graph representation of high-order Feynman
diagrams in Quantum Field Theory (QFT), applicable to any combination of
spatial, temporal, momentum, and frequency domains. Utilizing the
Dyson-Schwinger and parquet equations, our approach effectively organizes these
diagrams into a fractal structure of tensor operations, significantly reducing
computational redundancy. This approach not only streamlines the evaluation of
complex diagrams but also facilitates an efficient implementation of the
field-theoretic renormalization scheme, crucial for enhancing perturbative QFT
calculations. Key to this advancement is the integration of Taylor-mode
automatic differentiation, a key technique employed in machine learning
packages to compute higher-order derivatives efficiently on computational
graphs. To operationalize these concepts, we develop a Feynman diagram compiler
that optimizes diagrams for various computational platforms, utilizing machine
learning frameworks. Demonstrating this methodology's effectiveness, we apply
it to the three-dimensional uniform electron gas problem, achieving
unprecedented accuracy in calculating the quasiparticle effective mass at metal
density. Our work demonstrates the synergy between QFT and machine learning,
establishing a new avenue for applying AI techniques to complex quantum
many-body problems.
\\ ( https://arxiv.org/abs/2403.18840 ,  2551kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18853 (*cross-listing*)
Date: Mon, 18 Mar 2024 10:13:24 GMT   (8988kb,D)

Title: Spatio-seasonal risk assessment of upward lightning at tall objects
  using meteorological reanalysis data
Authors: Isabell Stucke, Deborah Morgenstern, Georg J. Mayr, Thorsten Simon,
  Achim Zeileis, Gerhard Diendorfer, Wolfgang Schulz, Hannes Pichler
Categories: physics.soc-ph cs.LG stat.AP
\\
  This study investigates lightning at tall objects and evaluates the risk of
upward lightning (UL) over the eastern Alps and its surrounding areas. While
uncommon, UL poses a threat, especially to wind turbines, as the long-duration
current of UL can cause significant damage. Current risk assessment methods
overlook the impact of meteorological conditions, potentially underestimating
UL risks. Therefore, this study employs random forests, a machine learning
technique, to analyze the relationship between UL measured at Gaisberg Tower
(Austria) and $35$ larger-scale meteorological variables. Of these, the
larger-scale upward velocity, wind speed and direction at 10 meters and cloud
physics variables contribute most information. The random forests predict the
risk of UL across the study area at a 1 km$^2$ resolution. Strong near-surface
winds combined with upward deflection by elevated terrain increase UL risk. The
diurnal cycle of the UL risk as well as high-risk areas shift seasonally. They
are concentrated north/northeast of the Alps in winter due to prevailing
northerly winds, and expanding southward, impacting northern Italy in the
transitional and summer months. The model performs best in winter, with the
highest predicted UL risk coinciding with observed peaks in measured lightning
at tall objects. The highest concentration is north of the Alps, where most
wind turbines are located, leading to an increase in overall lightning
activity. Comprehensive meteorological information is essential for UL risk
assessment, as lightning densities are a poor indicator of lightning at tall
objects.
\\ ( https://arxiv.org/abs/2403.18853 ,  8988kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18855 (*cross-listing*)
Date: Mon, 18 Mar 2024 20:47:38 GMT   (2298kb,D)

Title: Directed Criteria Citation Recommendation and Ranking Through Link
  Prediction
Authors: William Watson and Lawrence Yong
Categories: cs.SI cs.IR cs.LG
Comments: Extended Abstract at the International Conference of AI in Finance
  (ICAIF '20)
\\
  We explore link prediction as a proxy for automatically surfacing documents
from existing literature that might be topically or contextually relevant to a
new document. Our model uses transformer-based graph embeddings to encode the
meaning of each document, presented as a node within a citation network. We
show that the semantic representations that our model generates can outperform
other content-based methods in recommendation and ranking tasks. This provides
a holistic approach to exploring citation graphs in domains where it is
critical that these documents properly cite each other, so as to minimize the
possibility of any inconsistencies
\\ ( https://arxiv.org/abs/2403.18855 ,  2298kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18866 (*cross-listing*)
Date: Mon, 25 Mar 2024 14:50:01 GMT   (827kb,D)

Title: Graph Bayesian Optimization for Multiplex Influence Maximization
Authors: Zirui Yuan, Minglai Shao, Zhiqian Chen
Categories: cs.SI cs.LG
Comments: Proceedings of the AAAI Conference on Artificial Intelligence, 2024
\\
  Influence maximization (IM) is the problem of identifying a limited number of
initial influential users within a social network to maximize the number of
influenced users. However, previous research has mostly focused on individual
information propagation, neglecting the simultaneous and interactive
dissemination of multiple information items. In reality, when users encounter a
piece of information, such as a smartphone product, they often associate it
with related products in their minds, such as earphones or computers from the
same brand. Additionally, information platforms frequently recommend related
content to users, amplifying this cascading effect and leading to multiplex
influence diffusion.
  This paper first formulates the Multiplex Influence Maximization (Multi-IM)
problem using multiplex diffusion models with an information association
mechanism. In this problem, the seed set is a combination of influential users
and information. To effectively manage the combinatorial complexity, we propose
Graph Bayesian Optimization for Multi-IM (GBIM). The multiplex diffusion
process is thoroughly investigated using a highly effective global kernelized
attention message-passing module. This module, in conjunction with Bayesian
linear regression (BLR), produces a scalable surrogate model. A data
acquisition module incorporating the exploration-exploitation trade-off is
developed to optimize the seed set further. Extensive experiments on synthetic
and real-world datasets have proven our proposed framework effective. The code
is available at https://github.com/zirui-yuan/GBIM.
\\ ( https://arxiv.org/abs/2403.18866 ,  827kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18873 (*cross-listing*)
Date: Tue, 26 Mar 2024 14:42:46 GMT   (1348kb)

Title: Predicting risk of cardiovascular disease using retinal OCT imaging
Authors: Cynthia Maldonado-Garcia, Rodrigo Bonazzola, Enzo Ferrante, Thomas H
  Julian, Panagiotis I Sergouniotis, Nishant Ravikumara, Alejandro F Frangi
Categories: eess.IV cs.CV cs.LG
Comments: 18 pages for main manuscript, 7 figures, 2 pages for appendix and
  preprint for a journal
\\
  We investigated the potential of optical coherence tomography (OCT) as an
additional imaging technique to predict future cardiovascular disease (CVD). We
utilised a self-supervised deep learning approach based on Variational
Autoencoders (VAE) to learn low-dimensional representations of high-dimensional
3D OCT images and to capture distinct characteristics of different retinal
layers within the OCT image. A Random Forest (RF) classifier was subsequently
trained using the learned latent features and participant demographic and
clinical data, to differentiate between patients at risk of CVD events (MI or
stroke) and non-CVD cases. Our predictive model, trained on multimodal data,
was assessed based on its ability to correctly identify individuals likely to
suffer from a CVD event(MI or stroke), within a 5-year interval after image
acquisition. Our self-supervised VAE feature selection and multimodal Random
Forest classifier differentiate between patients at risk of future CVD events
and the control group with an AUC of 0.75, outperforming the clinically
established QRISK3 score (AUC= 0.597). The choroidal layer visible in OCT
images was identified as an important predictor of future CVD events using a
novel approach to model explanability. Retinal OCT imaging provides a
cost-effective and non-invasive alternative to predict the risk of
cardiovascular disease and is readily accessible in optometry practices and
hospitals.
\\ ( https://arxiv.org/abs/2403.18873 ,  1348kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18878 (*cross-listing*)
Date: Wed, 27 Mar 2024 10:46:24 GMT   (16789kb,D)

Title: AIC-UNet: Anatomy-informed Cascaded UNet for Robust Multi-Organ
  Segmentation
Authors: Young Seok Jeon, Hongfei Yang, Huazhu Fu, Mengling Feng
Categories: cs.CV cs.LG eess.IV
\\
  Imposing key anatomical features, such as the number of organs, their shapes,
sizes, and relative positions, is crucial for building a robust multi-organ
segmentation model. Current attempts to incorporate anatomical features include
broadening effective receptive fields (ERF) size with resource- and
data-intensive modules such as self-attention or introducing organ-specific
topology regularizers, which may not scale to multi-organ segmentation problems
where inter-organ relation also plays a huge role. We introduce a new approach
to impose anatomical constraints on any existing encoder-decoder segmentation
model by conditioning model prediction with learnable anatomy prior. More
specifically, given an abdominal scan, a part of the encoder spatially warps a
learnable prior to align with the given input scan using thin plate spline
(TPS) grid interpolation. The warped prior is then integrated during the
decoding phase to guide the model for more anatomy-informed predictions. Code
is available at
\hyperlink{https://anonymous.4open.science/r/AIC-UNet-7048}{https://anonymous.4open.science/r/AIC-UNet-7048}.
\\ ( https://arxiv.org/abs/2403.18878 ,  16789kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18915 (*cross-listing*)
Date: Wed, 27 Mar 2024 18:08:14 GMT   (2348kb,D)

Title: PLOT-TAL -- Prompt Learning with Optimal Transport for Few-Shot Temporal
  Action Localization
Authors: Edward Fish, Jon Weinbren, and Andrew Gilbert
Categories: cs.CV cs.LG
Comments: Under Review
\\
  This paper introduces a novel approach to temporal action localization (TAL)
in few-shot learning. Our work addresses the inherent limitations of
conventional single-prompt learning methods that often lead to overfitting due
to the inability to generalize across varying contexts in real-world videos.
Recognizing the diversity of camera views, backgrounds, and objects in videos,
we propose a multi-prompt learning framework enhanced with optimal transport.
This design allows the model to learn a set of diverse prompts for each action,
capturing general characteristics more effectively and distributing the
representation to mitigate the risk of overfitting. Furthermore, by employing
optimal transport theory, we efficiently align these prompts with action
features, optimizing for a comprehensive representation that adapts to the
multifaceted nature of video data. Our experiments demonstrate significant
improvements in action localization accuracy and robustness in few-shot
settings on the standard challenging datasets of THUMOS-14 and EpicKitchens100,
highlighting the efficacy of our multi-prompt optimal transport approach in
overcoming the challenges of conventional few-shot TAL methods.
\\ ( https://arxiv.org/abs/2403.18915 ,  2348kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18921 (*cross-listing*)
Date: Wed, 27 Mar 2024 18:12:24 GMT   (1337kb,D)

Title: SMOF: Streaming Modern CNNs on FPGAs with Smart Off-Chip Eviction
Authors: Petros Toupas, Zhewen Yu, Christos-Savvas Bouganis, Dimitrios Tzovaras
Categories: cs.AR cs.CV cs.LG
Comments: 12 pages, 8 figures, 5 tables
\\
  Convolutional Neural Networks (CNNs) have demonstrated their effectiveness in
numerous vision tasks. However, their high processing requirements necessitate
efficient hardware acceleration to meet the application's performance targets.
In the space of FPGAs, streaming-based dataflow architectures are often adopted
by users, as significant performance gains can be achieved through layer-wise
pipelining and reduced off-chip memory access by retaining data on-chip.
However, modern topologies, such as the UNet, YOLO, and X3D models, utilise
long skip connections, requiring significant on-chip storage and thus limiting
the performance achieved by such system architectures. The paper addresses the
above limitation by introducing weight and activation eviction mechanisms to
off-chip memory along the computational pipeline, taking into account the
available compute and memory resources. The proposed mechanism is incorporated
into an existing toolflow, expanding the design space by utilising off-chip
memory as a buffer. This enables the mapping of such modern CNNs to devices
with limited on-chip memory, under the streaming architecture design approach.
SMOF has demonstrated the capacity to deliver competitive and, in some cases,
state-of-the-art performance across a spectrum of computer vision tasks,
achieving up to 10.65 X throughput improvement compared to previous works.
\\ ( https://arxiv.org/abs/2403.18921 ,  1337kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18929 (*cross-listing*)
Date: Fri, 16 Feb 2024 18:05:09 GMT   (226kb,D)

Title: A Review of Neuroscience-Inspired Machine Learning
Authors: Alexander Ororbia, Ankur Mali, Adam Kohan, Beren Millidge, Tommaso
  Salvatori
Categories: cs.NE cs.LG
Comments: 13 Pages, 1 figure
\\
  One major criticism of deep learning centers around the biological
implausibility of the credit assignment schema used for learning --
backpropagation of errors. This implausibility translates into practical
limitations, spanning scientific fields, including incompatibility with
hardware and non-differentiable implementations, thus leading to expensive
energy requirements. In contrast, biologically plausible credit assignment is
compatible with practically any learning condition and is energy-efficient. As
a result, it accommodates hardware and scientific modeling, e.g. learning with
physical systems and non-differentiable behavior. Furthermore, it can lead to
the development of real-time, adaptive neuromorphic processing systems. In
addressing this problem, an interdisciplinary branch of artificial intelligence
research that lies at the intersection of neuroscience, cognitive science, and
machine learning has emerged. In this paper, we survey several vital algorithms
that model bio-plausible rules of credit assignment in artificial neural
networks, discussing the solutions they provide for different scientific fields
as well as their advantages on CPUs, GPUs, and novel implementations of
neuromorphic hardware. We conclude by discussing the future challenges that
will need to be addressed in order to make such algorithms more useful in
practical applications.
\\ ( https://arxiv.org/abs/2403.18929 ,  226kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18930 (*cross-listing*)
Date: Sat, 17 Feb 2024 16:36:01 GMT   (2156kb,D)

Title: Optimizing Wireless Networks with Deep Unfolding: Comparative Study on
  Two Deep Unfolding Mechanisms
Authors: Abuzar B. M. Adam, Mohammed A. M. Elhassan, Elhadj Moustapha Diallo
Categories: cs.NI cs.LG
Comments: 11 pages, 13 figures
\\
  In this work, we conduct a comparative study on two deep unfolding mechanisms
to efficiently perform power control in the next generation wireless networks.
The power control problem is formulated as energy efficiency over multiple
interference links. The problem is nonconvex. We employ fractional programming
transformation to design two solutions for the problem. The first solution is a
numerical solution while the second solution is a closed-form solution. Based
on the first solution, we design a semi-unfolding deep learning model where we
combine the domain knowledge of the wireless communications and the recent
advances in the data-driven deep learning. Moreover, on the highlights of the
closed-form solution, fully deep unfolded deep learning model is designed in
which we fully leveraged the expressive closed-form power control solution and
deep learning advances. In the simulation results, we compare the performance
of the proposed deep learning models and the iterative solutions in terms of
accuracy and inference speed to show their suitability for the real-time
application in next generation networks.
\\ ( https://arxiv.org/abs/2403.18930 ,  2156kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18994 (*cross-listing*)
Date: Wed, 27 Mar 2024 20:27:31 GMT   (238kb,D)

Title: Causal-StoNet: Causal Inference for High-Dimensional Complex Data
Authors: Yaxin Fang, Faming Liang
Categories: stat.ML cs.LG
\\
  With the advancement of data science, the collection of increasingly complex
datasets has become commonplace. In such datasets, the data dimension can be
extremely high, and the underlying data generation process can be unknown and
highly nonlinear. As a result, the task of making causal inference with
high-dimensional complex data has become a fundamental problem in many
disciplines, such as medicine, econometrics, and social science. However, the
existing methods for causal inference are frequently developed under the
assumption that the data dimension is low or that the underlying data
generation process is linear or approximately linear. To address these
challenges, this paper proposes a novel causal inference approach for dealing
with high-dimensional complex data. The proposed approach is based on deep
learning techniques, including sparse deep learning theory and stochastic
neural networks, that have been developed in recent literature. By using these
techniques, the proposed approach can address both the high dimensionality and
unknown data generation process in a coherent way. Furthermore, the proposed
approach can also be used when missing values are present in the datasets.
Extensive numerical studies indicate that the proposed approach outperforms
existing ones.
\\ ( https://arxiv.org/abs/2403.18994 ,  238kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19011 (*cross-listing*)
Date: Wed, 27 Mar 2024 21:06:26 GMT   (457kb,D)

Title: Sequential Inference of Hospitalization ElectronicHealth Records Using
  Probabilistic Models
Authors: Alan D. Kaplan, Priyadip Ray, John D. Greene, Vincent X. Liu
Categories: q-bio.QM cs.LG
\\
  In the dynamic hospital setting, decision support can be a valuable tool for
improving patient outcomes. Data-driven inference of future outcomes is
challenging in this dynamic setting, where long sequences such as laboratory
tests and medications are updated frequently. This is due in part to
heterogeneity of data types and mixed-sequence types contained in variable
length sequences. In this work we design a probabilistic unsupervised model for
multiple arbitrary-length sequences contained in hospitalization Electronic
Health Record (EHR) data. The model uses a latent variable structure and
captures complex relationships between medications, diagnoses, laboratory
tests, neurological assessments, and medications. It can be trained on original
data, without requiring any lossy transformations or time binning. Inference
algorithms are derived that use partial data to infer properties of the
complete sequences, including their length and presence of specific values. We
train this model on data from subjects receiving medical care in the Kaiser
Permanente Northern California integrated healthcare delivery system. The
results are evaluated against held-out data for predicting the length of
sequences and presence of Intensive Care Unit (ICU) in hospitalization bed
sequences. Our method outperforms a baseline approach, showing that in these
experiments the trained model captures information in the sequences that is
informative of their future values.
\\ ( https://arxiv.org/abs/2403.19011 ,  457kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19099 (*cross-listing*)
Date: Thu, 28 Mar 2024 02:25:12 GMT   (647kb,D)

Title: Optimizing Quantum Convolutional Neural Network Architectures for
  Arbitrary Data Dimension
Authors: Changwon Lee, Israel F. Araujo, Dongha Kim, Junghan Lee, Siheon Park,
  Ju-Young Ryu, Daniel K. Park
Categories: quant-ph cs.LG
Comments: 17 pages, 7 figures
\\
  Quantum convolutional neural networks (QCNNs) represent a promising approach
in quantum machine learning, paving new directions for both quantum and
classical data analysis. This approach is particularly attractive due to the
absence of the barren plateau problem, a fundamental challenge in training
quantum neural networks (QNNs), and its feasibility. However, a limitation
arises when applying QCNNs to classical data. The network architecture is most
natural when the number of input qubits is a power of two, as this number is
reduced by a factor of two in each pooling layer. The number of input qubits
determines the dimensions (i.e. the number of features) of the input data that
can be processed, restricting the applicability of QCNN algorithms to
real-world data. To address this issue, we propose a QCNN architecture capable
of handling arbitrary input data dimensions while optimizing the allocation of
quantum resources such as ancillary qubits and quantum gates. This optimization
is not only important for minimizing computational resources, but also
essential in noisy intermediate-scale quantum (NISQ) computing, as the size of
the quantum circuits that can be executed reliably is limited. Through
numerical simulations, we benchmarked the classification performance of various
QCNN architectures when handling arbitrary input data dimensions on the MNIST
and Breast Cancer datasets. The results validate that the proposed QCNN
architecture achieves excellent classification performance while utilizing a
minimal resource overhead, providing an optimal solution when reliable quantum
computation is constrained by noise and imperfections.
\\ ( https://arxiv.org/abs/2403.19099 ,  647kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19107 (*cross-listing*)
Date: Thu, 28 Mar 2024 02:51:33 GMT   (693kb)

Title: Synthetic Medical Imaging Generation with Generative Adversarial
  Networks For Plain Radiographs
Authors: John R. McNulty, Lee Kho, Alexandria L. Case, Charlie Fornaca, Drew
  Johnston, David Slater, Joshua M. Abzug, Sybil A. Russell
Categories: cs.CV cs.LG
Report-no: Public Release Case Number 22-3965
\\
  In medical imaging, access to data is commonly limited due to patient privacy
restrictions and the issue that it can be difficult to acquire enough data in
the case of rare diseases.[1] The purpose of this investigation was to develop
a reusable open-source synthetic image generation pipeline, the GAN Image
Synthesis Tool (GIST), that is easy to use as well as easy to deploy. The
pipeline helps to improve and standardize AI algorithms in the digital health
space by generating high quality synthetic image data that is not linked to
specific patients. Its image generation capabilities include the ability to
generate imaging of pathologies or injuries with low incidence rates. This
improvement of digital health AI algorithms could improve diagnostic accuracy,
aid in patient care, decrease medicolegal claims, and ultimately decrease the
overall cost of healthcare. The pipeline builds on existing Generative
Adversarial Networks (GANs) algorithms, and preprocessing and evaluation steps
were included for completeness. For this work, we focused on ensuring the
pipeline supports radiography, with a focus on synthetic knee and elbow x-ray
images. In designing the pipeline, we evaluated the performance of current GAN
architectures, studying the performance on available x-ray data. We show that
the pipeline is capable of generating high quality and clinically relevant
images based on a lay person's evaluation and the Fr\'echet Inception Distance
(FID) metric.
\\ ( https://arxiv.org/abs/2403.19107 ,  693kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19205 (*cross-listing*)
Date: Thu, 28 Mar 2024 08:06:48 GMT   (21740kb,D)

Title: From Activation to Initialization: Scaling Insights for Optimizing
  Neural Fields
Authors: Hemanth Saratchandran, Sameera Ramasinghe, Simon Lucey
Categories: cs.CV cs.LG
Comments: CVPR 2024
\\
  In the realm of computer vision, Neural Fields have gained prominence as a
contemporary tool harnessing neural networks for signal representation. Despite
the remarkable progress in adapting these networks to solve a variety of
problems, the field still lacks a comprehensive theoretical framework. This
article aims to address this gap by delving into the intricate interplay
between initialization and activation, providing a foundational basis for the
robust optimization of Neural Fields. Our theoretical insights reveal a
deep-seated connection among network initialization, architectural choices, and
the optimization process, emphasizing the need for a holistic approach when
designing cutting-edge Neural Fields.
\\ ( https://arxiv.org/abs/2403.19205 ,  21740kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19232 (*cross-listing*)
Date: Thu, 28 Mar 2024 08:44:36 GMT   (675kb,D)

Title: AZ-NAS: Assembling Zero-Cost Proxies for Network Architecture Search
Authors: Junghyup Lee, Bumsub Ham
Categories: cs.CV cs.LG
Comments: CVPR 2024
\\
  Training-free network architecture search (NAS) aims to discover
high-performing networks with zero-cost proxies, capturing network
characteristics related to the final performance. However, network rankings
estimated by previous training-free NAS methods have shown weak correlations
with the performance. To address this issue, we propose AZ-NAS, a novel
approach that leverages the ensemble of various zero-cost proxies to enhance
the correlation between a predicted ranking of networks and the ground truth
substantially in terms of the performance. To achieve this, we introduce four
novel zero-cost proxies that are complementary to each other, analyzing
distinct traits of architectures in the views of expressivity, progressivity,
trainability, and complexity. The proxy scores can be obtained simultaneously
within a single forward and backward pass, making an overall NAS process highly
efficient. In order to integrate the rankings predicted by our proxies
effectively, we introduce a non-linear ranking aggregation method that
highlights the networks highly-ranked consistently across all the proxies.
Experimental results conclusively demonstrate the efficacy and efficiency of
AZ-NAS, outperforming state-of-the-art methods on standard benchmarks, all
while maintaining a reasonable runtime cost.
\\ ( https://arxiv.org/abs/2403.19232 ,  675kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19262 (*cross-listing*)
Date: Thu, 28 Mar 2024 09:36:55 GMT   (1697kb,D)

Title: Removing the need for ground truth UWB data collection: self-supervised
  ranging error correction using deep reinforcement learning
Authors: Dieter Coppens, Ben Van Herbruggen, Adnan Shahid and Eli De Poorter
Categories: eess.SP cs.LG
Comments: 11 pages, 8 figures and 4 tables
\\
  Indoor positioning using UWB technology has gained interest due to its
centimeter-level accuracy potential. However, multipath effects and
non-line-of-sight conditions cause ranging errors between anchors and tags.
Existing approaches for mitigating these ranging errors rely on collecting
large labeled datasets, making them impractical for real-world deployments.
This paper proposes a novel self-supervised deep reinforcement learning
approach that does not require labeled ground truth data. A reinforcement
learning agent uses the channel impulse response as a state and predicts
corrections to minimize the error between corrected and estimated ranges. The
agent learns, self-supervised, by iteratively improving corrections that are
generated by combining the predictability of trajectories with filtering and
smoothening. Experiments on real-world UWB measurements demonstrate comparable
performance to state-of-the-art supervised methods, overcoming data dependency
and lack of generalizability limitations. This makes self-supervised deep
reinforcement learning a promising solution for practical and scalable
UWB-ranging error correction.
\\ ( https://arxiv.org/abs/2403.19262 ,  1697kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19294 (*cross-listing*)
Date: Thu, 28 Mar 2024 10:31:23 GMT   (6160kb,D)

Title: FlowDepth: Decoupling Optical Flow for Self-Supervised Monocular Depth
  Estimation
Authors: Yiyang Sun, Zhiyuan Xu, Xiaonian Wang, Jing Yao
Categories: cs.CV cs.LG
\\
  Self-supervised multi-frame methods have currently achieved promising results
in depth estimation. However, these methods often suffer from mismatch problems
due to the moving objects, which break the static assumption. Additionally,
unfairness can occur when calculating photometric errors in high-freq or
low-texture regions of the images. To address these issues, existing approaches
use additional semantic priori black-box networks to separate moving objects
and improve the model only at the loss level. Therefore, we propose FlowDepth,
where a Dynamic Motion Flow Module (DMFM) decouples the optical flow by a
mechanism-based approach and warps the dynamic regions thus solving the
mismatch problem. For the unfairness of photometric errors caused by high-freq
and low-texture regions, we use Depth-Cue-Aware Blur (DCABlur) and Cost-Volume
sparsity loss respectively at the input and the loss level to solve the
problem. Experimental results on the KITTI and Cityscapes datasets show that
our method outperforms the state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.19294 ,  6160kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19381 (*cross-listing*)
Date: Thu, 28 Mar 2024 12:42:25 GMT   (697kb,D)

Title: On Uncertainty Quantification for Near-Bayes Optimal Algorithms
Authors: Ziyu Wang, Chris Holmes
Categories: stat.ML cs.LG
\\
  Bayesian modelling allows for the quantification of predictive uncertainty
which is crucial in safety-critical applications. Yet for many machine learning
(ML) algorithms, it is difficult to construct or implement their Bayesian
counterpart. In this work we present a promising approach to address this
challenge, based on the hypothesis that commonly used ML algorithms are
efficient across a wide variety of tasks and may thus be near Bayes-optimal
w.r.t. an unknown task distribution. We prove that it is possible to recover
the Bayesian posterior defined by the task distribution, which is unknown but
optimal in this setting, by building a martingale posterior using the
algorithm. We further propose a practical uncertainty quantification method
that apply to general ML algorithms. Experiments based on a variety of non-NN
and NN algorithms demonstrate the efficacy of our method.
\\ ( https://arxiv.org/abs/2403.19381 ,  697kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19401 (*cross-listing*)
Date: Thu, 28 Mar 2024 13:24:18 GMT   (20kb)

Title: Hardness of Learning Boolean Functions from Label Proportions
Authors: Venkatesan Guruswami and Rishi Saket
Categories: cs.CC cs.DS cs.LG
Comments: 17 pages. Conference version of this paper appeared in FSTTCS 2023
DOI: 10.4230/LIPIcs.FSTTCS.2023.37
\\
  In recent years the framework of learning from label proportions (LLP) has
been gaining importance in machine learning. In this setting, the training
examples are aggregated into subsets or bags and only the average label per bag
is available for learning an example-level predictor. This generalizes
traditional PAC learning which is the special case of unit-sized bags. The
computational learning aspects of LLP were studied in recent works (Saket,
NeurIPS'21; Saket, NeurIPS'22) which showed algorithms and hardness for
learning halfspaces in the LLP setting. In this work we focus on the
intractability of LLP learning Boolean functions. Our first result shows that
given a collection of bags of size at most $2$ which are consistent with an OR
function, it is NP-hard to find a CNF of constantly many clauses which
satisfies any constant-fraction of the bags. This is in contrast with the work
of (Saket, NeurIPS'21) which gave a $(2/5)$-approximation for learning ORs
using a halfspace. Thus, our result provides a separation between constant
clause CNFs and halfspaces as hypotheses for LLP learning ORs.
  Next, we prove the hardness of satisfying more than $1/2 + o(1)$ fraction of
such bags using a $t$-DNF (i.e. DNF where each term has $\leq t$ literals) for
any constant $t$. In usual PAC learning such a hardness was known (Khot-Saket,
FOCS'08) only for learning noisy ORs. We also study the learnability of
parities and show that it is NP-hard to satisfy more than $(q/2^{q-1} +
o(1))$-fraction of $q$-sized bags which are consistent with a parity using a
parity, while a random parity based algorithm achieves a
$(1/2^{q-2})$-approximation.
\\ ( https://arxiv.org/abs/2403.19401 ,  20kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19441 (*cross-listing*)
Date: Thu, 28 Mar 2024 14:11:40 GMT   (1253kb,D)

Title: A Novel Stochastic Transformer-based Approach for Post-Traumatic Stress
  Disorder Detection using Audio Recording of Clinical Interviews
Authors: Mamadou Dia, Ghazaleh Khodabandelou, Alice Othmani
Categories: cs.SD cs.LG eess.AS
Journal-ref: 2023 IEEE 36th International Symposium on Computer-Based Medical
  Systems (2023) 700-705
DOI: 10.1109/CBMS58004.2023.00303
\\
  Post-traumatic stress disorder (PTSD) is a mental disorder that can be
developed after witnessing or experiencing extremely traumatic events. PTSD can
affect anyone, regardless of ethnicity, or culture. An estimated one in every
eleven people will experience PTSD during their lifetime. The
Clinician-Administered PTSD Scale (CAPS) and the PTSD Check List for Civilians
(PCL-C) interviews are gold standards in the diagnosis of PTSD. These
questionnaires can be fooled by the subject's responses. This work proposes a
deep learning-based approach that achieves state-of-the-art performances for
PTSD detection using audio recordings during clinical interviews. Our approach
is based on MFCC low-level features extracted from audio recordings of clinical
interviews, followed by deep high-level learning using a Stochastic
Transformer. Our proposed approach achieves state-of-the-art performances with
an RMSE of 2.92 on the eDAIC dataset thanks to the stochastic depth, stochastic
deep learning layers, and stochastic activation function.
\\ ( https://arxiv.org/abs/2403.19441 ,  1253kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19448 (*cross-listing*)
Date: Thu, 28 Mar 2024 14:16:23 GMT   (2410kb,D)

Title: Fisher-Rao Gradient Flows of Linear Programs and State-Action Natural
  Policy Gradients
Authors: Johannes M\"uller, Semih \c{C}ayc{\i}, Guido Mont\'ufar
Categories: math.OC cs.LG cs.NA cs.SY eess.SY math.NA stat.ML
Comments: 27 pages, 4 figures, under review
MSC-class: 65K05, 90C05, 90C08, 90C40, 90C53
\\
  Kakade's natural policy gradient method has been studied extensively in the
last years showing linear convergence with and without regularization. We study
another natural gradient method which is based on the Fisher information matrix
of the state-action distributions and has received little attention from the
theoretical side. Here, the state-action distributions follow the Fisher-Rao
gradient flow inside the state-action polytope with respect to a linear
potential. Therefore, we study Fisher-Rao gradient flows of linear programs
more generally and show linear convergence with a rate that depends on the
geometry of the linear program. Equivalently, this yields an estimate on the
error induced by entropic regularization of the linear program which improves
existing results. We extend these results and show sublinear convergence for
perturbed Fisher-Rao gradient flows and natural gradient flows up to an
approximation error. In particular, these general results cover the case of
state-action natural policy gradients.
\\ ( https://arxiv.org/abs/2403.19448 ,  2410kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19470 (*cross-listing*)
Date: Thu, 28 Mar 2024 14:54:31 GMT   (18919kb,D)

Title: Deep decomposition method for the limited aperture inverse obstacle
  scattering problem
Authors: Yunwen Yin, Liang Yan
Categories: math.NA cs.LG cs.NA eess.SP
\\
  In this paper, we consider a deep learning approach to the limited aperture
inverse obstacle scattering problem. It is well known that traditional deep
learning relies solely on data, which may limit its performance for the inverse
problem when only indirect observation data and a physical model are available.
A fundamental question arises in light of these limitations: is it possible to
enable deep learning to work on inverse problems without labeled data and to be
aware of what it is learning? This work proposes a deep decomposition method
(DDM) for such purposes, which does not require ground truth labels. It
accomplishes this by providing physical operators associated with the
scattering model to the neural network architecture. Additionally, a deep
learning based data completion scheme is implemented in DDM to prevent
distorting the solution of the inverse problem for limited aperture data.
Furthermore, apart from addressing the ill-posedness imposed by the inverse
problem itself, DDM is a physics-aware machine learning technique that can have
interpretability property. The convergence result of DDM is theoretically
proven. Numerical experiments are presented to demonstrate the validity of the
proposed DDM even when the incident and observation apertures are extremely
limited.
\\ ( https://arxiv.org/abs/2403.19470 ,  18919kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19508 (*cross-listing*)
Date: Thu, 28 Mar 2024 15:41:43 GMT   (2948kb,D)

Title: Debiasing Cardiac Imaging with Controlled Latent Diffusion Models
Authors: Grzegorz Skorupko, Richard Osuala, Zuzanna Szafranowska, Kaisar
  Kushibar, Nay Aung, Steffen E Petersen, Karim Lekadir, Polyxeni Gkontra
Categories: eess.IV cs.CV cs.LG
\\
  The progress in deep learning solutions for disease diagnosis and prognosis
based on cardiac magnetic resonance imaging is hindered by highly imbalanced
and biased training data. To address this issue, we propose a method to
alleviate imbalances inherent in datasets through the generation of synthetic
data based on sensitive attributes such as sex, age, body mass index, and
health condition. We adopt ControlNet based on a denoising diffusion
probabilistic model to condition on text assembled from patient metadata and
cardiac geometry derived from segmentation masks using a large-cohort study,
specifically, the UK Biobank. We assess our method by evaluating the realism of
the generated images using established quantitative metrics. Furthermore, we
conduct a downstream classification task aimed at debiasing a classifier by
rectifying imbalances within underrepresented groups through synthetically
generated samples. Our experiments demonstrate the effectiveness of the
proposed approach in mitigating dataset imbalances, such as the scarcity of
younger patients or individuals with normal BMI level suffering from heart
failure. This work represents a major step towards the adoption of synthetic
data for the development of fair and generalizable models for medical
classification tasks. Notably, we conduct all our experiments using a single,
consumer-level GPU to highlight the feasibility of our approach within
resource-constrained environments. Our code is available at
https://github.com/faildeny/debiasing-cardiac-mri.
\\ ( https://arxiv.org/abs/2403.19508 ,  2948kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19514 (*cross-listing*)
Date: Thu, 28 Mar 2024 15:45:03 GMT   (526kb,D)

Title: CDIMC-net: Cognitive Deep Incomplete Multi-view Clustering Network
Authors: Jie Wen, Zheng Zhang, Yong Xu, Bob Zhang, Lunke Fei, Guo-Sen Xie
Categories: cs.CV cs.LG
Comments: Accepted by IJCAI 2020
\\
  In recent years, incomplete multi-view clustering, which studies the
challenging multi-view clustering problem on missing views, has received
growing research interests. Although a series of methods have been proposed to
address this issue, the following problems still exist: 1) Almost all of the
existing methods are based on shallow models, which is difficult to obtain
discriminative common representations. 2) These methods are generally sensitive
to noise or outliers since the negative samples are treated equally as the
important samples. In this paper, we propose a novel incomplete multi-view
clustering network, called Cognitive Deep Incomplete Multi-view Clustering
Network (CDIMC-net), to address these issues. Specifically, it captures the
high-level features and local structure of each view by incorporating the
view-specific deep encoders and graph embedding strategy into a framework.
Moreover, based on the human cognition, i.e., learning from easy to hard, it
introduces a self-paced strategy to select the most confident samples for model
training, which can reduce the negative influence of outliers. Experimental
results on several incomplete datasets show that CDIMC-net outperforms the
state-of-the-art incomplete multi-view clustering methods.
\\ ( https://arxiv.org/abs/2403.19514 ,  526kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19516 (*cross-listing*)
Date: Thu, 28 Mar 2024 15:47:13 GMT   (2173kb)

Title: Maximum Likelihood Estimation on Stochastic Blockmodels for Directed
  Graph Clustering
Authors: Mihai Cucuringu, Xiaowen Dong, Ning Zhang
Categories: stat.ML cs.LG cs.SI math.ST stat.TH
\\
  This paper studies the directed graph clustering problem through the lens of
statistics, where we formulate clustering as estimating underlying communities
in the directed stochastic block model (DSBM). We conduct the maximum
likelihood estimation (MLE) on the DSBM and thereby ascertain the most probable
community assignment given the observed graph structure. In addition to the
statistical point of view, we further establish the equivalence between this
MLE formulation and a novel flow optimization heuristic, which jointly
considers two important directed graph statistics: edge density and edge
orientation. Building on this new formulation of directed clustering, we
introduce two efficient and interpretable directed clustering algorithms, a
spectral clustering algorithm and a semidefinite programming based clustering
algorithm. We provide a theoretical upper bound on the number of misclustered
vertices of the spectral clustering algorithm using tools from matrix
perturbation theory. We compare, both quantitatively and qualitatively, our
proposed algorithms with existing directed clustering methods on both synthetic
and real-world data, thus providing further ground to our theoretical
contributions.
\\ ( https://arxiv.org/abs/2403.19516 ,  2173kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19530 (*cross-listing*)
Date: Thu, 28 Mar 2024 16:06:06 GMT   (632kb,D)

Title: Detecting Financial Bots on the Ethereum Blockchain
Authors: Thomas Niedermayer, Pietro Saggese, Bernhard Haslhofer
Categories: cs.CR cs.LG
DOI: 10.1145/3589335.3651959
\\
  The integration of bots in Distributed Ledger Technologies (DLTs) fosters
efficiency and automation. However, their use is also associated with predatory
trading and market manipulation, and can pose threats to system integrity. It
is therefore essential to understand the extent of bot deployment in DLTs;
despite this, current detection systems are predominantly rule-based and lack
flexibility. In this study, we present a novel approach that utilizes machine
learning for the detection of financial bots on the Ethereum platform. First,
we systematize existing scientific literature and collect anecdotal evidence to
establish a taxonomy for financial bots, comprising 7 categories and 24
subcategories. Next, we create a ground-truth dataset consisting of 133 human
and 137 bot addresses. Third, we employ both unsupervised and supervised
machine learning algorithms to detect bots deployed on Ethereum. The
highest-performing clustering algorithm is a Gaussian Mixture Model with an
average cluster purity of 82.6%, while the highest-performing model for binary
classification is a Random Forest with an accuracy of 83%. Our machine
learning-based detection mechanism contributes to understanding the Ethereum
ecosystem dynamics by providing additional insights into the current bot
landscape.
\\ ( https://arxiv.org/abs/2403.19530 ,  632kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19578 (*cross-listing*)
Date: Thu, 28 Mar 2024 17:04:00 GMT   (8425kb,D)

Title: Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics
Authors: Norman Di Palo and Edward Johns
Categories: cs.RO cs.LG cs.NE
\\
  We show that off-the-shelf text-based Transformers, with no additional
training, can perform few-shot in-context visual imitation learning, mapping
visual observations to action sequences that emulate the demonstrator's
behaviour. We achieve this by transforming visual observations (inputs) and
trajectories of actions (outputs) into sequences of tokens that a
text-pretrained Transformer (GPT-4 Turbo) can ingest and generate, via a
framework we call Keypoint Action Tokens (KAT). Despite being trained only on
language, we show that these Transformers excel at translating tokenised visual
keypoint observations into action trajectories, performing on par or better
than state-of-the-art imitation learning (diffusion policies) in the low-data
regime on a suite of real-world, everyday tasks. Rather than operating in the
language domain as is typical, KAT leverages text-based Transformers to operate
in the vision and action domains to learn general patterns in demonstration
data for highly efficient imitation learning, indicating promising new avenues
for repurposing natural language models for embodied tasks. Videos are
available at https://www.robot-learning.uk/keypoint-action-tokens.
\\ ( https://arxiv.org/abs/2403.19578 ,  8425kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19588 (*cross-listing*)
Date: Thu, 28 Mar 2024 17:12:39 GMT   (3365kb,D)

Title: DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs
Authors: Donghyun Kim, Byeongho Heo, Dongyoon Han
Categories: cs.CV cs.LG cs.NE
Comments: Code at https://github.com/naver-ai/rdnet
\\
  This paper revives Densely Connected Convolutional Networks (DenseNets) and
reveals the underrated effectiveness over predominant ResNet-style
architectures. We believe DenseNets' potential was overlooked due to untouched
training methods and traditional design elements not fully revealing their
capabilities. Our pilot study shows dense connections through concatenation are
strong, demonstrating that DenseNets can be revitalized to compete with modern
architectures. We methodically refine suboptimal components - architectural
adjustments, block redesign, and improved training recipes towards widening
DenseNets and boosting memory efficiency while keeping concatenation shortcuts.
Our models, employing simple architectural elements, ultimately surpass Swin
Transformer, ConvNeXt, and DeiT-III - key architectures in the residual
learning lineage. Furthermore, our models exhibit near state-of-the-art
performance on ImageNet-1K, competing with the very recent models and
downstream tasks, ADE20k semantic segmentation, and COCO object
detection/instance segmentation. Finally, we provide empirical analyses that
uncover the merits of the concatenation over additive shortcuts, steering a
renewed preference towards DenseNet-style designs. Our code is available at
https://github.com/naver-ai/rdnet.
\\ ( https://arxiv.org/abs/2403.19588 ,  3365kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19605 (*cross-listing*)
Date: Thu, 28 Mar 2024 17:28:06 GMT   (268kb,D)

Title: Data-Adaptive Tradeoffs among Multiple Risks in Distribution-Free
  Prediction
Authors: Drew T. Nguyen, Reese Pathak, Anastasios N. Angelopoulos, Stephen
  Bates, Michael I. Jordan
Categories: stat.ME cs.LG
Comments: 27 pages, 10 figures
\\
  Decision-making pipelines are generally characterized by tradeoffs among
various risk functions. It is often desirable to manage such tradeoffs in a
data-adaptive manner. As we demonstrate, if this is done naively, state-of-the
art uncertainty quantification methods can lead to significant violations of
putative risk guarantees.
  To address this issue, we develop methods that permit valid control of risk
when threshold and tradeoff parameters are chosen adaptively. Our methodology
supports monotone and nearly-monotone risks, but otherwise makes no
distributional assumptions.
  To illustrate the benefits of our approach, we carry out numerical
experiments on synthetic data and the large-scale vision dataset MS-COCO.
\\ ( https://arxiv.org/abs/2403.19605 ,  268kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19612 (*cross-listing*)
Date: Thu, 28 Mar 2024 17:32:01 GMT   (11952kb,D)

Title: ILPO-NET: Network for the invariant recognition of arbitrary volumetric
  patterns in 3D
Authors: Dmitrii Zhemchuzhnikov and Sergei Grudinin
Categories: cs.CV cs.LG
\\
  Effective recognition of spatial patterns and learning their hierarchy is
crucial in modern spatial data analysis. Volumetric data applications seek
techniques ensuring invariance not only to shifts but also to pattern
rotations. While traditional methods can readily achieve translational
invariance, rotational invariance possesses multiple challenges and remains an
active area of research. Here, we present ILPO-Net (Invariant to Local Patterns
Orientation Network), a novel approach that handles arbitrarily shaped patterns
with the convolutional operation inherently invariant to local spatial pattern
orientations using the Wigner matrix expansions. Our architecture seamlessly
integrates the new convolution operator and, when benchmarked on diverse
volumetric datasets such as MedMNIST and CATH, demonstrates superior
performance over the baselines with significantly reduced parameter counts - up
to 1000 times fewer in the case of MedMNIST. Beyond these demonstrations,
ILPO-Net's rotational invariance paves the way for other applications across
multiple disciplines. Our code is publicly available at
https://gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPONet.
\\ ( https://arxiv.org/abs/2403.19612 ,  11952kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2305.09535
replaced with revised version Thu, 28 Mar 2024 14:10:37 GMT   (0kb,I)

Title: What's the Problem, Linda? The Conjunction Fallacy as a Fairness Problem
Authors: Jose Alvarez Colmenares
Categories: cs.AI cs.CY
Comments: Large portions of this paper were used for another paper with a
  different direction. Therefore, this paper became an early draft of the other
  paper, which is why I am removing it until it can stand by itself
\\ ( https://arxiv.org/abs/2305.09535 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10370
replaced with revised version Thu, 28 Mar 2024 09:12:21 GMT   (233kb,D)

Title: Do Similar Entities have Similar Embeddings?
Authors: Nicolas Hubert, Heiko Paulheim, Armelle Brun, Davy Monticolo
Categories: cs.AI cs.IR cs.LG
Comments: Accepted at ESWC 2024
\\ ( https://arxiv.org/abs/2312.10370 ,  233kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01786
replaced with revised version Thu, 28 Mar 2024 15:22:42 GMT   (12825kb,D)

Title: COA-GPT: Generative Pre-trained Transformers for Accelerated Course of
  Action Development in Military Operations
Authors: Vinicius G. Goecks, Nicholas Waytowich
Categories: cs.AI cs.CL cs.HC cs.LG
Comments: Accepted at the NATO Science and Technology Organization Symposium
  (ICMCIS) organized by the Information Systems Technology (IST) Panel,
  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024
ACM-class: I.2.6; I.2.7; J.7
\\ ( https://arxiv.org/abs/2402.01786 ,  12825kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15456
replaced with revised version Thu, 28 Mar 2024 16:40:05 GMT   (1310kb,D)

Title: WoLF: Large Language Model Framework for CXR Understanding
Authors: Seil Kang, Donghyun Kim, Junhyeok Kim, Hyo Kyung Lee, Seong Jae Hwang
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2403.15456 ,  1310kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16097
replaced with revised version Thu, 28 Mar 2024 06:56:47 GMT   (435kb,D)

Title: Can Language Models Pretend Solvers? Logic Code Simulation with LLMs
Authors: Minyu Chen, Guoqiang Li, Ling-I Wu, Ruibang Liu, Yuxin Su, Xi Chang,
  Jianxin Xue
Categories: cs.AI cs.LO cs.SE
Comments: 12 pages, 8 figures
\\ ( https://arxiv.org/abs/2403.16097 ,  435kb)
------------------------------------------------------------------------------
\\
arXiv:2010.05330
replaced with revised version Thu, 28 Mar 2024 11:26:58 GMT   (3805kb,D)

Title: Incremental Processing in the Age of Non-Incremental Encoders: An
  Empirical Assessment of Bidirectional Models for Incremental NLU
Authors: Brielen Madureira and David Schlangen
Categories: cs.CL
Comments: Accepted to the EMNLP 2020 conference (long paper). V2 has minor
  updates, see note in last page
\\ ( https://arxiv.org/abs/2010.05330 ,  3805kb)
------------------------------------------------------------------------------
\\
arXiv:2212.00851
replaced with revised version Thu, 28 Mar 2024 09:25:21 GMT   (630kb)

Title: SOLD: Sinhala Offensive Language Dataset
Authors: Tharindu Ranasinghe, Isuri Anuradha, Damith Premasiri, Kanishka Silva,
  Hansi Hettiarachchi, Lasitha Uyangodage, Marcos Zampieri
Categories: cs.CL cs.AI cs.LG cs.SI
Comments: Accepted to Language Resources and Evaluation, Springer
DOI: 10.1007/s10579-024-09723-1
\\ ( https://arxiv.org/abs/2212.00851 ,  630kb)
------------------------------------------------------------------------------
\\
arXiv:2212.08635
replaced with revised version Thu, 28 Mar 2024 06:06:59 GMT   (7930kb,D)

Title: Self-Prompting Large Language Models for Zero-Shot Open-Domain QA
Authors: Junlong Li, Jinyuan Wang, Zhuosheng Zhang, Hai Zhao
Categories: cs.CL cs.AI
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2212.08635 ,  7930kb)
------------------------------------------------------------------------------
\\
arXiv:2212.08686
replaced with revised version Thu, 28 Mar 2024 08:20:12 GMT   (255kb,D)

Title: Evaluating Step-by-Step Reasoning through Symbolic Verification
Authors: Yi-Fan Zhang, Hanlin Zhang, Li Erran Li, Eric Xing
Categories: cs.CL
Comments: NAACL-Findings, 2024
\\ ( https://arxiv.org/abs/2212.08686 ,  255kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12612
replaced with revised version Thu, 28 Mar 2024 14:23:08 GMT   (1736kb,D)

Title: PrOnto: Language Model Evaluations for 859 Languages
Authors: Luke Gessler
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2305.12612 ,  1736kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18034
replaced with revised version Thu, 28 Mar 2024 16:27:01 GMT   (6925kb,D)

Title: A Corpus for Sentence-level Subjectivity Detection on English News
  Articles
Authors: Francesco Antici, Andrea Galassi, Federico Ruggeri, Katerina Korre,
  Arianna Muti, Alessandra Bardi, Alice Fedotova, Alberto Barr\'on-Cede\~no
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2305.18034 ,  6925kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03799
replaced with revised version Thu, 28 Mar 2024 03:23:59 GMT   (2204kb,D)

Title: Prompt Space Optimizing Few-shot Reasoning Success with Large Language
  Models
Authors: Fobo Shi, Peijun Qing, Dong Yang, Nan Wang, Youbo Lei, Haonan Lu,
  Xiaodong Lin, Duantengchuan Li
Categories: cs.CL
Comments: Natural language processing (NLP)
\\ ( https://arxiv.org/abs/2306.03799 ,  2204kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09336
replaced with revised version Thu, 28 Mar 2024 00:50:55 GMT   (1802kb,D)

Title: LLMRefine: Pinpointing and Refining Large Language Models via
  Fine-Grained Actionable Feedback
Authors: Wenda Xu, Daniel Deutsch, Mara Finkelstein, Juraj Juraska, Biao Zhang,
  Zhongtao Liu, William Yang Wang, Lei Li, and Markus Freitag
Categories: cs.CL
Comments: Accepted to NAACL 2024
\\ ( https://arxiv.org/abs/2311.09336 ,  1802kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09363
replaced with revised version Thu, 28 Mar 2024 16:31:26 GMT   (9335kb,D)

Title: Investigating the Emergent Audio Classification Ability of ASR
  Foundation Models
Authors: Rao Ma, Adian Liusie, Mark J. F. Gales, Kate M. Knill
Categories: cs.CL
Comments: NAACL 2024 (main conference)
\\ ( https://arxiv.org/abs/2311.09363 ,  9335kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09519
replaced with revised version Wed, 27 Mar 2024 21:52:11 GMT   (7886kb,D)

Title: Leveraging Code to Improve In-context Learning for Semantic Parsing
Authors: Ben Bogin, Shivanshu Gupta, Peter Clark, Ashish Sabharwal
Categories: cs.CL
Comments: Accepted to NAACL 2024
\\ ( https://arxiv.org/abs/2311.09519 ,  7886kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09682
replaced with revised version Wed, 27 Mar 2024 23:43:54 GMT   (8340kb,D)

Title: MacGyver: Are Large Language Models Creative Problem Solvers?
Authors: Yufei Tian, Abhilasha Ravichander, Lianhui Qin, Ronan Le Bras, Raja
  Marjieh, Nanyun Peng, Yejin Choi, Thomas L. Griffiths, Faeze Brahman
Categories: cs.CL cs.AI
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2311.09682 ,  8340kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04021
replaced with revised version Thu, 28 Mar 2024 03:01:45 GMT   (223kb,D)

Title: A Study on the Calibration of In-context Learning
Authors: Hanlin Zhang, Yi-Fan Zhang, Yaodong Yu, Dhruv Madeka, Dean Foster,
  Eric Xing, Himabindu Lakkaraju, Sham Kakade
Categories: cs.CL cs.AI cs.LG
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2312.04021 ,  223kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01286
replaced with revised version Thu, 28 Mar 2024 15:56:55 GMT   (4063kb,D)

Title: A Comprehensive Study of Knowledge Editing for Large Language Models
Authors: Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru
  Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen
  Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang,
  Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen
Categories: cs.CL cs.AI cs.CV cs.HC cs.LG
Comments: Ongoing work; 52 pages, 282 citations; benchmark is available at
  https://huggingface.co/datasets/zjunlp/KnowEdit code is available at
  https://github.com/zjunlp/EasyEdit paper list is available at
  https://github.com/zjunlp/KnowledgeEditingPapers
\\ ( https://arxiv.org/abs/2401.01286 ,  4063kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06877
replaced with revised version Thu, 28 Mar 2024 17:17:17 GMT   (426kb,D)

Title: Promptly Predicting Structures: The Return of Inference
Authors: Maitrey Mehta, Valentina Pyatkin, Vivek Srikumar
Categories: cs.CL
Comments: 18 pages, 13 figures Accepted to NAACL'2024 (Main)
\\ ( https://arxiv.org/abs/2401.06877 ,  426kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08403
replaced with revised version Thu, 28 Mar 2024 12:51:44 GMT   (316kb,D)

Title: LLMs and the Human Condition
Authors: Peter Wallis
Categories: cs.CL
Comments: 3rd draft includes Roger's comments. Added images of Sagrada Familia
  and termite mounds. target is IVA in 2024
\\ ( https://arxiv.org/abs/2402.08403 ,  316kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11549
replaced with revised version Thu, 28 Mar 2024 11:16:28 GMT   (5978kb,D)

Title: Syntactic Language Change in English and German: Metrics, Parsers, and
  Convergences
Authors: Yanran Chen, Wei Zhao, Anne Breitbarth, Manuel Stoeckel, Alexander
  Mehler, Steffen Eger
Categories: cs.CL cs.AI
Comments: Updated to the current version
\\ ( https://arxiv.org/abs/2402.11549 ,  5978kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11815
replaced with revised version Wed, 27 Mar 2024 20:30:08 GMT   (8028kb,D)

Title: HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to
  Detect Machine-Generated Text?
Authors: Shubhashis Roy Dipta and Sadat Shahriar
Categories: cs.CL cs.AI cs.LG
Comments: Camera Ready Version - Accepted in SemEval 2024 (Colocated with NAACL
  2024)
\\ ( https://arxiv.org/abs/2402.11815 ,  8028kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13492
replaced with revised version Wed, 27 Mar 2024 18:48:34 GMT   (304kb,D)

Title: Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval
  Augmentation to Language Models
Authors: Seiji Maekawa, Hayate Iso, Sairam Gurajada, Nikita Bhutani
Categories: cs.CL
Comments: NAACL2024 (main)
\\ ( https://arxiv.org/abs/2402.13492 ,  304kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02472
replaced with revised version Thu, 28 Mar 2024 14:44:48 GMT   (300kb,D)

Title: OffLanDat: A Community Based Implicit Offensive Language Dataset
  Generated by Large Language Model Through Prompt Engineering
Authors: Amit Das, Mostafa Rahgouy, Dongji Feng, Zheng Zhang, Tathagata
  Bhattacharya, Nilanjana Raychawdhary, Mary Sandage, Lauramarie Pope, Gerry
  Dozier and Cheryl Seals
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.02472 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14403
replaced with revised version Thu, 28 Mar 2024 06:45:11 GMT   (8186kb,D)

Title: Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language
  Models through Question Complexity
Authors: Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park
Categories: cs.CL cs.AI
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2403.14403 ,  8186kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14472
replaced with revised version Thu, 28 Mar 2024 15:24:17 GMT   (9786kb,D)

Title: Detoxifying Large Language Models via Knowledge Editing
Authors: Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi
  Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen
Categories: cs.CL cs.AI cs.CV cs.HC cs.LG
Comments: Ongoing work. Project website:
  https://zjunlp.github.io/project/SafeEdit Due to the specificity of the
  knowledge editing setting, we revise Tables 1 and 3 to present a fair
  comparison of experimental results. More experimental results will be updated
  soon
\\ ( https://arxiv.org/abs/2403.14472 ,  9786kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15268
replaced with revised version Thu, 28 Mar 2024 16:28:24 GMT   (764kb,D)

Title: Imagination Augmented Generation: Learning to Imagine Richer Context for
  Question Answering over Large Language Models
Authors: Huanxuan Liao, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, Shengping
  Liu, Jun Zhao
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.15268 ,  764kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17645
replaced with revised version Thu, 28 Mar 2024 15:59:09 GMT   (663kb)

Title: DANCER: Entity Description Augmented Named Entity Corrector for
  Automatic Speech Recognition
Authors: Yi-Cheng Wang, Hsin-Wei Wang, Bi-Cheng Yan, Chi-Han Lin and Berlin
  Chen
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.17645 ,  663kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17752
replaced with revised version Thu, 28 Mar 2024 09:57:05 GMT   (12811kb,D)

Title: Can multiple-choice questions really be useful in detecting the
  abilities of LLMs?
Authors: Wangyue Li, Liangzhi Li, Tong Xiang, Xiao Liu, Wei Deng, Noa Garcia
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.17752 ,  12811kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18018
replaced with revised version Thu, 28 Mar 2024 10:19:46 GMT   (834kb,D)

Title: DORE: A Dataset For Portuguese Definition Generation
Authors: Anna Beatriz Dimas Furtado, Tharindu Ranasinghe, Fr\'ed\'eric Blain,
  Ruslan Mitkov
Categories: cs.CL cs.LG
Comments: Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)
\\ ( https://arxiv.org/abs/2403.18018 ,  834kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18025
replaced with revised version Thu, 28 Mar 2024 11:01:21 GMT   (8270kb,D)

Title: Improving Pre-trained Language Model Sensitivity via Mask Specific
  losses: A case study on Biomedical NER
Authors: Micheal Abaho, Danushka Bollegala, Gary Leeming, Dan Joyce, Iain E
  Buchan
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: Paper alrerady accepted for publishing by the NAACL 2024 conference
  (main conference paper)
\\ ( https://arxiv.org/abs/2403.18025 ,  8270kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18314
replaced with revised version Thu, 28 Mar 2024 05:27:43 GMT   (8834kb,D)

Title: Chinese Offensive Language Detection:Current Status and Future
  Directions
Authors: Yunze Xiao, Houda Bouamor and Wajdi Zaghouani
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.18314 ,  8834kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18346
replaced with revised version Thu, 28 Mar 2024 17:09:36 GMT   (5557kb,D)

Title: Quantifying and Mitigating Unimodal Biases in Multimodal Large Language
  Models: A Causal Perspective
Authors: Meiqi Chen, Yixin Cao, Yan Zhang, and Chaochao Lu
Categories: cs.CL cs.CV
\\ ( https://arxiv.org/abs/2403.18346 ,  5557kb)
------------------------------------------------------------------------------
\\
arXiv:2211.01579
replaced with revised version Thu, 28 Mar 2024 10:53:54 GMT   (524kb,D)

Title: Data-free Defense of Black Box Models Against Adversarial Attacks
Authors: Gaurav Kumar Nayak, Inder Khatri, Ruchit Rawal, Anirban Chakraborty
Categories: cs.LG cs.CR cs.CV
Comments: CVPR Workshop (Under Review)
\\ ( https://arxiv.org/abs/2211.01579 ,  524kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13375
replaced with revised version Thu, 28 Mar 2024 16:08:43 GMT   (457kb,D)

Title: Optimal Transport Perturbations for Safe Reinforcement Learning with
  Robustness Guarantees
Authors: James Queeney, Erhan Can Ozcan, Ioannis Ch. Paschalidis, Christos G.
  Cassandras
Categories: cs.LG cs.AI stat.ML
Comments: Transactions on Machine Learning Research (TMLR), 2024
\\ ( https://arxiv.org/abs/2301.13375 ,  457kb)
------------------------------------------------------------------------------
\\
arXiv:2302.03357
replaced with revised version Thu, 28 Mar 2024 11:54:32 GMT   (1365kb,D)

Title: Towards Enhancing Time Series Contrastive Learning: A Dynamic Bad Pair
  Mining Approach
Authors: Xiang Lan, Hanshu Yan, Shenda Hong, Mengling Feng
Categories: cs.LG
Comments: ICLR 2024 Camera Ready (https://openreview.net/pdf?id=K2c04ulKXn)
\\ ( https://arxiv.org/abs/2302.03357 ,  1365kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12627
replaced with revised version Wed, 27 Mar 2024 23:54:26 GMT   (1792kb,D)

Title: Targeted collapse regularized autoencoder for anomaly detection: black
  hole at the center
Authors: Amin Ghafourian, Huanyi Shui, Devesh Upadhyay, Rajesh Gupta, Dimitar
  Filev, Iman Soltani Bozchalooi
Categories: cs.LG cs.AI cs.CV q-bio.NC stat.ML
Comments: 18 pages, 4 figures, 8 tables
\\ ( https://arxiv.org/abs/2306.12627 ,  1792kb)
------------------------------------------------------------------------------
\\
arXiv:2306.15865
replaced with revised version Thu, 28 Mar 2024 16:56:06 GMT   (5339kb,D)

Title: Differentially Private Distributed Estimation and Learning
Authors: Marios Papachristou, M. Amin Rahimian
Categories: cs.LG cs.SI cs.SY eess.SY math.ST stat.AP stat.ML stat.TH
Comments: Accepted for publication at IISE Transactions (Special issue on
  Federated, Distributed Learning and Analytics)
\\ ( https://arxiv.org/abs/2306.15865 ,  5339kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10757
replaced with revised version Thu, 28 Mar 2024 08:26:50 GMT   (15556kb,D)

Title: To Whom are You Talking? A Deep Learning Model to Endow Social Robots
  with Addressee Estimation Skills
Authors: Carlo Mazzola, Marta Romeo, Francesco Rea, Alessandra Sciutti, Angelo
  Cangelosi
Categories: cs.LG cs.AI cs.RO
Comments: Accepted v. of IJCNN 2023 publication. Funded by the Horizon Europe
  project TERAIS (G.A. 101079338), the UKRI Node on Trust (EP/V026682/1), the
  EU projects TRAINCREASE and MUSAE, and the US project THRIVE++. Cite:
  https://doi.org/10.1109/IJCNN54540.2023.10191452 Code:
  https://zenodo.org/doi/10.5281/zenodo.10709857 Data:
  https://zenodo.org/doi/10.5281/zenodo.10711587 10 pages, 8 Figures, 3 Tables
MSC-class: 68T07, 68T40
ACM-class: I.2.6; I.2.9; I.2.10; J.7
Journal-ref: 2023 International Joint Conference on Neural Networks (IJCNN),
  pp. 1-10
DOI: 10.1109/IJCNN54540.2023.10191452
\\ ( https://arxiv.org/abs/2308.10757 ,  15556kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11076
replaced with revised version Thu, 28 Mar 2024 01:00:05 GMT   (2492kb,D)

Title: Symbolic Regression on Sparse and Noisy Data with Gaussian Processes
Authors: Junette Hsin, Shubhankar Agarwal, Adam Thorpe, Luis Sentis, David
  Fridovich-Keil
Categories: cs.LG cs.SY eess.SY
Comments: Submitted to CDC 2024
\\ ( https://arxiv.org/abs/2309.11076 ,  2492kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17053
replaced with revised version Thu, 28 Mar 2024 11:00:52 GMT   (44kb)

Title: On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters
Authors: Matthias Lanzinger, Pablo Barcel\'o
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.17053 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00926
replaced with revised version Wed, 27 Mar 2024 19:34:21 GMT   (3958kb,D)

Title: Integration of Graph Neural Network and Neural-ODEs for Tumor Dynamic
  Prediction
Authors: Omid Bazgir, Zichen Wang, Ji Won Park, Marc Hafner, James Lu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.00926 ,  3958kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01201
replaced with revised version Thu, 28 Mar 2024 15:09:13 GMT   (330kb,D)

Title: SWoTTeD: An Extension of Tensor Decomposition to Temporal Phenotyping
Authors: Hana Sebia, Thomas Guyet, Etienne Audureau
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.01201 ,  330kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02861
replaced with revised version Thu, 28 Mar 2024 03:29:34 GMT   (266kb)

Title: Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly
  Detection
Authors: Xiangyu Dong, Xingyi Zhang, Sibo Wang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.02861 ,  266kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04190
replaced with revised version Thu, 28 Mar 2024 12:56:25 GMT   (112kb,D)

Title: On the Two Sides of Redundancy in Graph Neural Networks
Authors: Franka Bause, Samir Moustafa, Johannes Langguth, Wilfried N.
  Gansterer, Nils M. Kriege
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.04190 ,  112kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12387
replaced with revised version Thu, 28 Mar 2024 00:52:59 GMT   (9867kb,D)

Title: Learning to Optimise Climate Sensor Placement using a Transformer
Authors: Chen Wang, Victoria Huang, Gang Chen, Hui Ma, Bryce Chen, and Jochen
  Schmidt
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.12387 ,  9867kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15301
replaced with revised version Wed, 27 Mar 2024 21:56:59 GMT   (6544kb,D)

Title: ADMarker: A Multi-Modal Federated Learning System for Monitoring Digital
  Biomarkers of Alzheimer's Disease
Authors: Xiaomin Ouyang, Xian Shuai, Yang Li, Li Pan, Xifan Zhang, Heming Fu,
  Xinyan Wang, Shihua Cao, Jiang Xin, Hazel Mok, Zhenyu Yan, Doris Sau Fung Yu,
  Timothy Kwok, Guoliang Xing
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.15301 ,  6544kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01990
replaced with revised version Wed, 27 Mar 2024 22:03:46 GMT   (55kb,D)

Title: Conditions on Preference Relations that Guarantee the Existence of
  Optimal Policies
Authors: Jonathan Cola\c{c}o Carr, Prakash Panangaden, Doina Precup
Categories: cs.LG
Comments: v2: replaced with accepted AISTATS 2024 version, containing a new
  summary figure and one extra example. Results and conclusions are unchanged
\\ ( https://arxiv.org/abs/2311.01990 ,  55kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04830
replaced with revised version Thu, 28 Mar 2024 10:30:57 GMT   (4065kb,D)

Title: Real-Time Recurrent Reinforcement Learning
Authors: Julian Lemmel, Radu Grosu
Categories: cs.LG cs.NE cs.SY eess.SY
Comments: 14 pages, 9 figures, includes Appendix
\\ ( https://arxiv.org/abs/2311.04830 ,  4065kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06958
replaced with revised version Thu, 28 Mar 2024 11:13:33 GMT   (0kb,I)

Title: Towards probabilistic Weather Forecasting with Conditioned
  Spatio-Temporal Normalizing Flows
Authors: Christina Winkler
Categories: cs.LG cs.AI
Comments: Wrong version, will upload a new one
\\ ( https://arxiv.org/abs/2311.06958 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11908
replaced with revised version Thu, 28 Mar 2024 13:16:50 GMT   (1949kb,D)

Title: Continual Learning: Applications and the Road Forward
Authors: Eli Verwimp, Rahaf Aljundi, Shai Ben-David, Matthias Bethge, Andrea
  Cossu, Alexander Gepperth, Tyler L. Hayes, Eyke H\"ullermeier, Christopher
  Kanan, Dhireesha Kudithipudi, Christoph H. Lampert, Martin Mundt, Razvan
  Pascanu, Adrian Popescu, Andreas S. Tolias, Joost van de Weijer, Bing Liu,
  Vincenzo Lomonaco, Tinne Tuytelaars, Gido M. van de Ven
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2311.11908 ,  1949kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13628
replaced with revised version Wed, 27 Mar 2024 20:20:22 GMT   (1320kb,D)

Title: Prompt Risk Control: A Rigorous Framework for Responsible Deployment of
  Large Language Models
Authors: Thomas P. Zollo, Todd Morrill, Zhun Deng, Jake C. Snell, Toniann
  Pitassi, Richard Zemel
Categories: cs.LG cs.AI cs.CL
Comments: 34 pages, 10 figures, published as conference paper at ICLR 2024, and
  accepted to the Socially Responsible Language Modelling Research (SoLaR)
  workshop at NeurIPS 2023
\\ ( https://arxiv.org/abs/2311.13628 ,  1320kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06153
replaced with revised version Thu, 28 Mar 2024 02:20:36 GMT   (329kb,D)

Title: Open Datasheets: Machine-readable Documentation for Open Datasets and
  Responsible AI Assessments
Authors: Anthony Cintron Roman, Jennifer Wortman Vaughan, Valerie See, Steph
  Ballard, Jehu Torres, Caleb Robinson, Juan M. Lavista Ferres
Categories: cs.LG cs.AI cs.HC
\\ ( https://arxiv.org/abs/2312.06153 ,  329kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11034
replaced with revised version Thu, 28 Mar 2024 04:46:19 GMT   (5212kb,D)

Title: Appeal: Allow Mislabeled Samples the Chance to be Rectified in Partial
  Label Learning
Authors: Chongjie Si, Xuehui Wang, Yan Wang, Xiaokang Yang, Wei Shen
Categories: cs.LG
Comments: Under review. An extended version of 2024 AAAI oral paper "Partial
  Label Learning with a Partner"
\\ ( https://arxiv.org/abs/2312.11034 ,  5212kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00365
replaced with revised version Thu, 28 Mar 2024 06:38:55 GMT   (44862kb,D)

Title: HQ-VAE: Hierarchical Discrete Representation Learning with Variational
  Bayes
Authors: Yuhta Takida, Yukara Ikemiya, Takashi Shibuya, Kazuki Shimada, Woosung
  Choi, Chieh-Hsin Lai, Naoki Murata, Toshimitsu Uesaka, Kengo Uchida,
  Wei-Hsiang Liao, Yuki Mitsufuji
Categories: cs.LG cs.AI cs.CV
Comments: 34 pages with 17 figures, accepted for TMLR
\\ ( https://arxiv.org/abs/2401.00365 ,  44862kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06501
replaced with revised version Thu, 28 Mar 2024 15:17:01 GMT   (1266kb,D)

Title: Scalable Interactive Machine Learning for Future Command and Control
Authors: Anna Madison, Ellen Novoseller, Vinicius G. Goecks, Benjamin T. Files,
  Nicholas Waytowich, Alfred Yu, Vernon J. Lawhern, Steven Thurman, Christopher
  Kelshaw, Kaleb McDowell
Categories: cs.LG cs.AI cs.CL cs.HC
Comments: Accepted at the NATO Science and Technology Organization Symposium
  (ICMCIS) organized by the Information Systems Technology (IST) Panel,
  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024
ACM-class: I.2.6; I.2.7; J.7
\\ ( https://arxiv.org/abs/2402.06501 ,  1266kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08714
replaced with revised version Wed, 27 Mar 2024 21:37:39 GMT   (19881kb,D)

Title: PRDP: Proximal Reward Difference Prediction for Large-Scale Reward
  Finetuning of Diffusion Models
Authors: Fei Deng, Qifei Wang, Wei Wei, Matthias Grundmann, Tingbo Hou
Categories: cs.LG cs.AI
Comments: CVPR 2024. Project page: https://fdeng18.github.io/prdp
\\ ( https://arxiv.org/abs/2402.08714 ,  19881kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14490
replaced with revised version Thu, 28 Mar 2024 08:36:27 GMT   (3964kb,D)

Title: Imbalanced Data Clustering using Equilibrium K-Means
Authors: Yudong He
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.14490 ,  3964kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16105
replaced with revised version Thu, 28 Mar 2024 09:16:03 GMT   (15671kb,D)

Title: Informed Meta-Learning
Authors: Katarzyna Kobalczyk, Mihaela van der Schaar
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.16105 ,  15671kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01121
replaced with revised version Thu, 28 Mar 2024 09:11:27 GMT   (267kb,D)

Title: OpenGraph: Towards Open Graph Foundation Models
Authors: Lianghao Xia, Ben Kao and Chao Huang
Categories: cs.LG cs.AI cs.SI
\\ ( https://arxiv.org/abs/2403.01121 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12031
replaced with revised version Thu, 28 Mar 2024 17:56:28 GMT   (1356kb,D)

Title: RouterBench: A Benchmark for Multi-LLM Routing System
Authors: Qitian Jason Hu, Jacob Bieker, Xiuyu Li, Nan Jiang, Benjamin Keigwin,
  Gaurav Ranganath, Kurt Keutzer, Shriyash Kaustubh Upadhyay
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.12031 ,  1356kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12847
replaced with revised version Thu, 28 Mar 2024 11:46:02 GMT   (2496kb,D)

Title: Policy Bifurcation in Safe Reinforcement Learning
Authors: Wenjun Zou, Yao Lyu, Jie Li, Yujie Yang, Shengbo Eben Li, Jingliang
  Duan, Xianyuan Zhan, Jingjing Liu, Yaqin Zhang, and Keqiang Li
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.12847 ,  2496kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15022
replaced with revised version Wed, 27 Mar 2024 10:47:24 GMT   (25273kb,D)

Title: Insights into the Lottery Ticket Hypothesis and Iterative Magnitude
  Pruning
Authors: Tausifa Jan Saleem, Ramanjit Ahuja, Surendra Prasad, Brejesh Lall
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.15022 ,  25273kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15905
replaced with revised version Thu, 28 Mar 2024 15:00:04 GMT   (5298kb,D)

Title: Towards Low-Energy Adaptive Personalization for Resource-Constrained
  Devices
Authors: Yushan Huang, Josh Millar, Yuxuan Long, Yuchen Zhao, Hamed Hadaddi
Categories: cs.LG cs.CV
Comments: Accepetd to The 4th Workshop on Machine Learning and Systems
  (EuroMLSys '24)
\\ ( https://arxiv.org/abs/2403.15905 ,  5298kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16451
replaced with revised version Thu, 28 Mar 2024 11:36:06 GMT   (31854kb,D)

Title: DeepMachining: Online Prediction of Machining Errors of Lathe Machines
Authors: Xiang-Li Lu, Hwai-Jung Hsu, Che-Wei Chou, H. T. Kung, Chen-Hsin Lee,
  and Sheng-Mao Cheng
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.16451 ,  31854kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16591
replaced with revised version Thu, 28 Mar 2024 15:27:38 GMT   (1243kb,D)

Title: Deciphering the Interplay between Local Differential Privacy, Average
  Bayesian Privacy, and Maximum Bayesian Privacy
Authors: Xiaojin Zhang, Yulin Fei, Wei Chen, Hai Jin
Categories: cs.LG cs.AI cs.CR
\\ ( https://arxiv.org/abs/2403.16591 ,  1243kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17210
replaced with revised version Wed, 27 Mar 2024 21:47:49 GMT   (382kb,D)

Title: CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug
  Interactions
Authors: Azmine Toushik Wasi, Taki Hasan Rafi, Raima Islam, Serbetar Karlo,
  Dong-Kyu Chae
Categories: cs.LG cs.AI cs.IR q-bio.BM q-bio.MN
Comments: 8 Pages, 4 Figures; In review
\\ ( https://arxiv.org/abs/2403.17210 ,  382kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17919
replaced with revised version Thu, 28 Mar 2024 15:44:39 GMT   (1661kb,D)

Title: LISA: Layerwise Importance Sampling for Memory-Efficient Large Language
  Model Fine-Tuning
Authors: Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han,
  Tong Zhang
Categories: cs.LG cs.AI cs.CL math.OC
\\ ( https://arxiv.org/abs/2403.17919 ,  1661kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18028
replaced with revised version Thu, 28 Mar 2024 17:06:15 GMT   (3687kb,D)

Title: Predicting Species Occurrence Patterns from Partial Observations
Authors: Hager Radi Abdelwahed, M\'elisande Teng, David Rolnick
Categories: cs.LG cs.AI cs.CV q-bio.PE
Comments: Tackling Climate Change with Machine Learning workshop at ICLR 2024
\\ ( https://arxiv.org/abs/2403.18028 ,  3687kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18159
replaced with revised version Thu, 28 Mar 2024 08:22:31 GMT   (236kb,D)

Title: Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal
  Propagation Analysis for Large Language Models
Authors: Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Kyunggeun
  Lee, Jun Ma, Harris Teague
Categories: cs.LG cs.AI cs.CL
Comments: Accepted at Practical ML for Low Resource Settings Workshop at ICLR
  2024
\\ ( https://arxiv.org/abs/2403.18159 ,  236kb)
------------------------------------------------------------------------------
\\
arXiv:2204.08989 (*cross-listing*)
replaced with revised version Thu, 28 Mar 2024 16:17:43 GMT   (4055kb,D)

Title: Efficient Deep Learning-based Estimation of the Vital Signs on
  Smartphones
Authors: Taha Samavati, Mahdi Farvardin, Aboozar Ghaffari
Categories: eess.SP cs.AI cs.CV cs.LG eess.IV
Comments: 10 pages, 8 figures, 11 tables
ACM-class: I.5.4; I.2.0
\\ ( https://arxiv.org/abs/2204.08989 ,  4055kb)
------------------------------------------------------------------------------
\\
arXiv:2302.03788
replaced with revised version Thu, 28 Mar 2024 01:36:14 GMT   (7232kb,D)

Title: Toward a Theory of Causation for Interpreting Neural Code Models
Authors: David N. Palacio and Alejandro Velasco and Nathan Cooper and Alvaro
  Rodriguez and Kevin Moran and Denys Poshyvanyk
Categories: cs.SE cs.AI cs.LG stat.ME
Comments: Accepted to appear in IEEE Transactions on Software Engineering
DOI: 10.1109/TSE.2024.3379943
\\ ( https://arxiv.org/abs/2302.03788 ,  7232kb)
------------------------------------------------------------------------------
\\
arXiv:2306.17077
replaced with revised version Thu, 28 Mar 2024 00:09:13 GMT   (3186kb,D)

Title: RAPGen: An Approach for Fixing Code Inefficiencies in Zero-Shot
Authors: Spandan Garg, Roshanak Zilouchian Moghaddam, Neel Sundaresan
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2306.17077 ,  3186kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02192
replaced with revised version Thu, 28 Mar 2024 07:52:02 GMT   (2465kb,D)

Title: The FormAI Dataset: Generative AI in Software Security Through the Lens
  of Formal Verification
Authors: Norbert Tihanyi, Tamas Bisztray, Ridhi Jain, Mohamed Amine Ferrag,
  Lucas C. Cordeiro, Vasileios Mavroeidis
Categories: cs.DB cs.AI
Comments: https://github.com/FormAI-Dataset PLEASE USE PUBLISHED VERSION FOR
  CITATION: https://doi.org/10.1145/3617555.3617874
Journal-ref: PROMISE 2023: Proceedings of the 19th International Conference on
  Predictive Models and Data Analytics in Software Engineering December 2023
  Pages 33 to 43
DOI: 10.1145/3617555.3617874
\\ ( https://arxiv.org/abs/2307.02192 ,  2465kb)
------------------------------------------------------------------------------
\\
arXiv:2307.04132
replaced with revised version Wed, 27 Mar 2024 18:17:46 GMT   (2399kb,D)

Title: Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type
  Recognition
Authors: Amrit Diggavi Seshadri, Alessandra Russo
Categories: cs.CV cs.AI cs.SC
\\ ( https://arxiv.org/abs/2307.04132 ,  2399kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08453
replaced with revised version Wed, 27 Mar 2024 21:46:41 GMT   (175kb,D)

Title: Tightest Admissible Shortest Path
Authors: Eyal Weiss, Ariel Felner, Gal A. Kaminka
Categories: cs.DS cs.AI cs.DM
Comments: arXiv admin note: text overlap with arXiv:2208.11489
\\ ( https://arxiv.org/abs/2308.08453 ,  175kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10226
replaced with revised version Thu, 28 Mar 2024 15:53:48 GMT   (2679kb,D)

Title: Machine Learning-Powered Combinatorial Clock Auction
Authors: Ermis Soumalias, Jakob Weissteiner, Jakob Heiss, Sven Seuken
Categories: cs.GT cs.AI cs.LG
Comments: AAAI 2024 (8 pages + appendix)
MSC-class: 91A06
ACM-class: I.2
Journal-ref: Proceedings of the AAAI Conference on Artificial Intelligence,
  38(9) (2024) 9891-9900
DOI: 10.1609/aaai.v38i9.28850
\\ ( https://arxiv.org/abs/2308.10226 ,  2679kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19056
replaced with revised version Thu, 28 Mar 2024 06:49:56 GMT   (7228kb,D)

Title: MILL: Mutual Verification with Large Language Models for Zero-Shot Query
  Expansion
Authors: Pengyue Jia, Yiding Liu, Xiangyu Zhao, Xiaopeng Li, Changying Hao,
  Shuaiqiang Wang, Dawei Yin
Categories: cs.IR cs.AI cs.CL
Comments: Accepted to NAACL 2024
\\ ( https://arxiv.org/abs/2310.19056 ,  7228kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19654
replaced with revised version Thu, 28 Mar 2024 08:47:14 GMT   (9499kb,D)

Title: MCAD: Multi-teacher Cross-modal Alignment Distillation for efficient
  image-text retrieval
Authors: Youbo Lei, Feifei He, Chen Chen, Yingbin Mo, Si Jia Li, Defeng Xie,
  Haonan Lu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2310.19654 ,  9499kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10522
replaced with revised version Thu, 28 Mar 2024 06:20:10 GMT   (17843kb,D)

Title: Enhancing Object Coherence in Layout-to-Image Synthesis
Authors: Yibin Wang and Weizhong Zhang and Jianwei Zheng and Cheng Jin
Categories: cs.CV cs.AI
Comments: GitHub: https://github.com/CodeGoat24/EOCNet
\\ ( https://arxiv.org/abs/2311.10522 ,  17843kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13099
replaced with revised version Wed, 27 Mar 2024 23:49:07 GMT   (16200kb,D)

Title: PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF
Authors: Yutao Feng, Yintong Shang, Xuan Li, Tianjia Shao, Chenfanfu Jiang, Yin
  Yang
Categories: cs.CV cs.AI cs.GR cs.LG
\\ ( https://arxiv.org/abs/2311.13099 ,  16200kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18331
replaced with revised version Thu, 28 Mar 2024 13:27:33 GMT   (2974kb,D)

Title: MRFP: Learning Generalizable Semantic Segmentation from Sim-2-Real with
  Multi-Resolution Feature Perturbation
Authors: Sumanth Udupa, Prajwal Gurunath, Aniruddh Sikdar, Suresh Sundaram
Categories: cs.CV cs.AI
Comments: Accepted to CVPR 2024
\\ ( https://arxiv.org/abs/2311.18331 ,  2974kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02051
replaced with revised version Thu, 28 Mar 2024 12:41:14 GMT   (2377kb,D)

Title: TimeChat: A Time-sensitive Multimodal Large Language Model for Long
  Video Understanding
Authors: Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, Lu Hou
Categories: cs.CV cs.AI cs.CL
Comments: CVPR 2024 camera-ready version, code is available at
  https://github.com/RenShuhuai-Andy/TimeChat
\\ ( https://arxiv.org/abs/2312.02051 ,  2377kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03596
replaced with revised version Thu, 28 Mar 2024 03:26:51 GMT   (7126kb,D)

Title: MMM: Generative Masked Motion Model
Authors: Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, Chen Chen
Categories: cs.CV cs.AI cs.LG
Comments: accepted to CVPR
\\ ( https://arxiv.org/abs/2312.03596 ,  7126kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10746 (*cross-listing*)
replaced with revised version Wed, 27 Mar 2024 19:47:06 GMT   (435kb,D)

Title: A Systematic Evaluation of Euclidean Alignment with Deep Learning for
  EEG Decoding
Authors: Bruna Junqueira, Bruno Aristimunha, Sylvain Chevallier, Raphael Y. de
  Camargo
Categories: eess.SP cs.AI cs.LG
Comments: 14 pages and 10 figures
ACM-class: I.5.1; I.6.3; I.2.6
\\ ( https://arxiv.org/abs/2401.10746 ,  435kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07946
replaced with revised version Thu, 28 Mar 2024 15:17:30 GMT   (371kb)

Title: Re-Envisioning Command and Control
Authors: Kaleb McDowell, Ellen Novoseller, Anna Madison, Vinicius G. Goecks,
  Christopher Kelshaw
Categories: cs.HC cs.AI cs.CL cs.LG
Comments: Accepted at the NATO Science and Technology Organization Symposium
  (ICMCIS) organized by the Information Systems Technology (IST) Panel,
  IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024
ACM-class: I.2.6; I.2.7; J.7
\\ ( https://arxiv.org/abs/2402.07946 ,  371kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10251 (*cross-listing*)
replaced with revised version Thu, 28 Mar 2024 13:55:31 GMT   (5572kb,D)

Title: Brant-2: Foundation Model for Brain Signals
Authors: Zhizhang Yuan, Daoze Zhang, Junru Chen, Gefei Gu, Yang Yang
Categories: q-bio.NC cs.AI cs.LG eess.SP
Comments: 14 pages, 7 figures
\\ ( https://arxiv.org/abs/2402.10251 ,  5572kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12373
replaced with revised version Wed, 27 Mar 2024 20:00:00 GMT   (7478kb,D)

Title: LTL learning on GPUs
Authors: Mojtaba Valizadeh, Nathana\"el Fijalkow, Martin Berger
Categories: cs.PL cs.AI
Comments: 27 pages
MSC-class: 68
ACM-class: D.3
\\ ( https://arxiv.org/abs/2402.12373 ,  7478kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19161
replaced with revised version Thu, 28 Mar 2024 04:07:57 GMT   (17514kb,D)

Title: MemoNav: Working Memory Model for Visual Navigation
Authors: Hongxin Li, Zeyu Wang, Xu Yang, Yuran Yang, Shuqi Mei, Zhaoxiang Zhang
Categories: cs.CV cs.AI cs.RO
Comments: Accepted to CVPR 2024. Code: https://github.com/ZJULiHongxin/MemoNav
\\ ( https://arxiv.org/abs/2402.19161 ,  17514kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05950
replaced with revised version Thu, 28 Mar 2024 17:14:53 GMT   (746kb)

Title: Classifying Objects in 3D Point Clouds Using Recurrent Neural Network: A
  GRU LSTM Hybrid Approach
Authors: Ramin Mousa, Mitra Khezli, Mohamadreza Azadi, Vahid Nikoofard, Saba
  Hesaraki
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.05950 ,  746kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08059
replaced with revised version Thu, 28 Mar 2024 00:59:37 GMT   (6284kb,D)

Title: FluoroSAM: A Language-aligned Foundation Model for X-ray Image
  Segmentation
Authors: Benjamin D. Killeen, Liam J. Wang, Han Zhang, Mehran Armand, Russell
  H. Taylor, Dave Dreizin, Greg Osgood, Mathias Unberath
Categories: cs.CV cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2403.08059 ,  6284kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09412
replaced with revised version Thu, 28 Mar 2024 14:10:08 GMT   (7694kb,D)

Title: OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in
  Large-Scale Outdoor Environments
Authors: Yinan Deng, Jiahui Wang, Jingyu Zhao, Xinyu Tian, Guangyan Chen, Yi
  Yang, Yufeng Yue
Categories: cs.CV cs.AI cs.RO
\\ ( https://arxiv.org/abs/2403.09412 ,  7694kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10667
replaced with revised version Wed, 27 Mar 2024 21:11:19 GMT   (1840kb,D)

Title: Towards Unified Multi-Modal Personalization: Large Vision-Language
  Models for Generative Recommendation and Beyond
Authors: Tianxin Wei, Bowen Jin, Ruirui Li, Hansi Zeng, Zhengyang Wang, Jianhui
  Sun, Qingyu Yin, Hanqing Lu, Suhang Wang, Jingrui He, Xianfeng Tang
Categories: cs.IR cs.AI cs.CL cs.MM
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2403.10667 ,  1840kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14680
replaced with revised version Thu, 28 Mar 2024 14:13:35 GMT   (1036kb)

Title: Trust in AI: Progress, Challenges, and Future Directions
Authors: Saleh Afroogh, Ali Akbari, Evan Malone, Mohammadali Kargar, Hananeh
  Alambeigi
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2403.14680 ,  1036kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15931
replaced with revised version Wed, 27 Mar 2024 23:57:47 GMT   (14200kb,D)

Title: X-Portrait: Expressive Portrait Animation with Hierarchical Motion
  Attention
Authors: You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, Linjie Luo
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.15931 ,  14200kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15955
replaced with revised version Thu, 28 Mar 2024 07:30:25 GMT   (2074kb,D)

Title: Finding needles in a haystack: A Black-Box Approach to Invisible
  Watermark Detection
Authors: Minzhou Pan, Zhengting Wang, Xin Dong, Vikash Sehwag, Lingjuan Lyu,
  Xue Lin
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.15955 ,  2074kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17608
replaced with revised version Thu, 28 Mar 2024 15:24:16 GMT   (14301kb,D)

Title: Fake or JPEG? Revealing Common Biases in Generated Image Detection
  Datasets
Authors: Patrick Grommelt, Louis Weiss, Franz-Josef Pfreundt, Janis Keuper
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.17608 ,  14301kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17633
replaced with revised version Thu, 28 Mar 2024 09:47:45 GMT   (4622kb,D)

Title: UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object
  Detection with Sparse LiDAR and Large Domain Gaps
Authors: Maciej K Wozniak, Mattias Hansson, Marko Thiel, Patric Jensfelt
Categories: cs.CV cs.AI cs.RO
\\ ( https://arxiv.org/abs/2403.17633 ,  4622kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17740
replaced with revised version Thu, 28 Mar 2024 04:40:59 GMT   (4884kb,D)

Title: All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating
  Prediction
Authors: Shuheng Fang, Kangfei Zhao, Yu Rong, Zhixun Li and Jeffrey Xu Yu
Categories: cs.IR cs.AI
Comments: 14 pages, 9 figures
\\ ( https://arxiv.org/abs/2403.17740 ,  4884kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18755
replaced with revised version Thu, 28 Mar 2024 14:05:56 GMT   (1737kb,D)

Title: Many-Objective Evolutionary Influence Maximization: Balancing Spread,
  Budget, Fairness, and Time
Authors: Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca
Categories: cs.NE cs.AI cs.SI
Comments: To appear in Genetic and Evolutionary Computation Conference (GECCO
  24 Companion), July 14 18, 2024, Melbourne, VIC, Australia. ACM, New York,
  NY, USA
DOI: 10.1145/3638530.3654161
\\ ( https://arxiv.org/abs/2403.18755 ,  1737kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18807
replaced with revised version Thu, 28 Mar 2024 08:01:34 GMT   (5594kb,D)

Title: ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth
  Estimation
Authors: Suraj Patni, Aradhye Agarwal, Chetan Arora
Categories: cs.CV cs.AI cs.LG
Comments: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
  2024
\\ ( https://arxiv.org/abs/2403.18807 ,  5594kb)
------------------------------------------------------------------------------
\\
arXiv:2306.17563
replaced with revised version Thu, 28 Mar 2024 13:59:09 GMT   (56kb,D)

Title: Large Language Models are Effective Text Rankers with Pairwise Ranking
  Prompting
Authors: Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan,
  Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, Michael
  Bendersky
Categories: cs.IR cs.CL cs.LG
Comments: Accepted to NAACL 2024. Corrected results of RankT5 on TREC-DL19
\\ ( https://arxiv.org/abs/2306.17563 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15596
replaced with revised version Thu, 28 Mar 2024 11:35:55 GMT   (4985kb,D)

Title: EgoThink: Evaluating First-Person Perspective Thinking Capability of
  Vision-Language Models
Authors: Sijie Cheng, Zhicheng Guo, Jingwen Wu, Kechen Fang, Peng Li, Huaping
  Liu, Yang Liu
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2311.15596 ,  4985kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04260
replaced with revised version Thu, 28 Mar 2024 11:55:32 GMT   (4068kb,D)

Title: Can Small Language Models be Good Reasoners for Sequential
  Recommendation?
Authors: Yuling Wang, Changxin Tian, Binbin Hu, Yanhua Yu, Ziqi Liu, Zhiqiang
  Zhang, Jun Zhou, Liang Pang, Xiao Wang
Categories: cs.IR cs.CL cs.LG
Comments: Accepted by TheWebConf (WWW) 2024
\\ ( https://arxiv.org/abs/2403.04260 ,  4068kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12984 (*cross-listing*)
replaced with revised version Wed, 27 Mar 2024 21:51:03 GMT   (241kb,D)

Title: When SMILES have Language: Drug Classification using Text Classification
  Methods on Drug SMILES Strings
Authors: Azmine Toushik Wasi and \v{S}erbetar Karlo and Raima Islam and Taki
  Hasan Rafi and Dong-Kyu Chae
Categories: q-bio.BM cs.CL cs.IR cs.LG stat.ML
Comments: 7 pages, 2 figures, 5 tables, Accepted (invited to present) to the
  The Second Tiny Papers Track at ICLR 2024
  (https://openreview.net/forum?id=VUYCyH8fCw)
Journal-ref: The Second Tiny Papers Track at {ICLR} 2024, Tiny Papers @ {ICLR}
  2024, Vienna Austria, May 11, 2024
\\ ( https://arxiv.org/abs/2403.12984 ,  241kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16385
replaced with revised version Thu, 28 Mar 2024 16:45:44 GMT   (4089kb,D)

Title: Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators
  for Reasoning-Based Chart VQA
Authors: Zhuowan Li, Bhavan Jasani, Peng Tang, Shabnam Ghadar
Categories: cs.CV cs.CL
Comments: Accepted to CVPR 2024
\\ ( https://arxiv.org/abs/2403.16385 ,  4089kb)
------------------------------------------------------------------------------
\\
arXiv:2007.15776 (*cross-listing*)
replaced with revised version Thu, 28 Mar 2024 15:51:49 GMT   (2890kb,D)

Title: Random Vector Functional Link Networks for Function Approximation on
  Manifolds
Authors: Deanna Needell, Aaron A. Nelson, Rayan Saab, Palina Salanevich, Olov
  Schavemaker
Categories: stat.ML cs.IT cs.LG math.IT math.PR
Comments: 37 pages, 1 figure
MSC-class: 62M45
\\ ( https://arxiv.org/abs/2007.15776 ,  2890kb)
------------------------------------------------------------------------------
\\
arXiv:2101.10300 (*cross-listing*)
replaced with revised version Thu, 28 Mar 2024 03:47:39 GMT   (115kb,D)

Title: Channel Estimation via Successive Denoising in MIMO OFDM Systems: A
  Reinforcement Learning Approach
Authors: Myeung Suk Oh, Seyyedali Hosseinalipour, Taejoon Kim, Christopher G.
  Brinton, David J. Love
Categories: eess.SP cs.LG
Comments: This paper has been published in the proceedings of 2021 IEEE
  International Conference on Communications (ICC)
DOI: 10.1109/ICC42927.2021.9500671
\\ ( https://arxiv.org/abs/2101.10300 ,  115kb)
------------------------------------------------------------------------------
\\
arXiv:2204.11970 (*cross-listing*)
replaced with revised version Wed, 27 Mar 2024 22:02:30 GMT   (329kb,D)

Title: Visual Acuity Prediction on Real-Life Patient Data Using a Machine
  Learning Based Multistage System
Authors: Tobias Schlosser, Frederik Beuth, Trixy Meyer, Arunodhayan Sampath
  Kumar, Gabriel Stolze, Olga Furashova, Katrin Engelmann, Danny Kowerko
Categories: eess.IV cs.CV cs.IR cs.LG
Comments: Preprint for journal Scientific Reports (Springer)
\\ ( https://arxiv.org/abs/2204.11970 ,  329kb)
------------------------------------------------------------------------------
\\
arXiv:2209.03919 (*cross-listing*)
replaced with revised version Thu, 28 Mar 2024 14:52:27 GMT   (3740kb,D)

Title: Bi-objective Ranking and Selection Using Stochastic Kriging
Authors: Sebastian Rojas Gonzalez, Juergen Branke and Inneke van Nieuwenhuyse
Categories: stat.ML cs.LG
Comments: 33 pages, 14 figures
\\ ( https://arxiv.org/abs/2209.03919 ,  3740kb)
------------------------------------------------------------------------------
\\
arXiv:2303.05699
replaced with revised version Thu, 28 Mar 2024 03:48:40 GMT   (24328kb,D)

Title: Feature Unlearning for Pre-trained GANs and VAEs
Authors: Saemi Moon, Seunghyuk Cho, Dongwoo Kim
Categories: cs.CV cs.LG
DOI: 10.1609/aaai.v38i19.30138
\\ ( https://arxiv.org/abs/2303.05699 ,  24328kb)
------------------------------------------------------------------------------
\\
arXiv:2303.08737
replaced with revised version Thu, 28 Mar 2024 16:59:24 GMT   (3944kb,D)

Title: Evaluating gesture generation in a large-scale open challenge: The GENEA
  Challenge 2022
Authors: Taras Kucherenko, Pieter Wolfert, Youngwoo Yoon, Carla Viegas, Teodor
  Nikolov, Mihail Tsakov, Gustav Eje Henter
Categories: cs.HC cs.LG cs.MM
Comments: The first three authors made equal contributions and share joint
  first authorship. Accepted for publication in the ACM Transactions on
  Graphics (TOG).Please see https://youngwoo-yoon.github.io/GENEAchallenge2022/
  for all challenge materials. arXiv admin note: text overlap with
  arXiv:2208.10441
ACM-class: I.3; I.2
\\ ( https://arxiv.org/abs/2303.08737 ,  3944kb)
------------------------------------------------------------------------------
\\
arXiv:2303.14694 (*cross-listing*)
replaced with revised version Thu, 28 Mar 2024 16:37:06 GMT   (58kb,D)

Title: A stability theorem for bigraded persistence barcodes
Authors: Anthony Bahri, Ivan Limonchenko, Taras Panov, Jongbaek Song and Donald
  Stanley
Categories: math.AT cs.CG cs.LG math.CO math.MG
Comments: 20 pages
MSC-class: Primary 57S12, 55N31, 57Z25, Secondary 13F55, 55U10
\\ ( https://arxiv.org/abs/2303.14694 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2304.09224 (*cross-listing*)
replaced with revised version Thu, 28 Mar 2024 17:36:50 GMT   (4201kb,D)

Title: Quantum machine learning for image classification
Authors: Arsenii Senokosov, Alexandr Sedykh, Asel Sagingalieva, Basil Kyriacou,
  Alexey Melnikov
Categories: quant-ph cs.CV cs.LG
Comments: 13 pages, 10 figures, 1 table
Journal-ref: Mach. Learn.: Sci. Technol. 5(1), 015040 (2024)
DOI: 10.1088/2632-2153/ad2aef
\\ ( https://arxiv.org/abs/2304.09224 ,  4201kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02496 (*cross-listing*)
replaced with revised version Thu, 28 Mar 2024 15:33:42 GMT   (680kb)

Title: Learning to reconstruct the bubble distribution with conductivity maps
  using Invertible Neural Networks and Error Diffusion
Authors: Nishant Kumar, Lukas Krause, Thomas Wondrak, Sven Eckert, Kerstin
  Eckert, Stefan Gumhold
Categories: eess.IV cs.CV cs.LG
Comments: Accepted for Oral presentation at WCIPT11 (11th World Congress on
  Industrial Process Tomography)
\\ ( https://arxiv.org/abs/2307.02496 ,  680kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03683 (*cross-listing*)
replaced with revised version Wed, 27 Mar 2024 23:15:33 GMT   (2202kb,D)

Title: Differentiable Turbulence: Closure as a partial differential equation
  constrained optimization
Authors: Varun Shankar, Dibyajyoti Chakraborty, Venkatasubramanian Viswanathan,
  Romit Maulik
Categories: physics.flu-dyn cs.LG
\\ ( https://arxiv.org/abs/2307.03683 ,  2202kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10500 (*cross-listing*)
replaced with revised version Thu, 28 Mar 2024 16:30:07 GMT   (2261kb,D)

Title: Few-Shot Learning Patterns in Financial Time-Series for Trend-Following
  Strategies
Authors: Kieran Wood, Samuel Kessler, Stephen J. Roberts, Stefan Zohren
Categories: q-fin.TR cs.LG q-fin.PM
Comments: minor edits
DOI: 10.3905/jfds.2024.1.157
\\ ( https://arxiv.org/abs/2310.10500 ,  2261kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14344
replaced with revised version Wed, 27 Mar 2024 20:48:37 GMT   (4861kb,D)

Title: What's in a Prior? Learned Proximal Networks for Inverse Problems
Authors: Zhenghan Fang, Sam Buchanan, Jeremias Sulam
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2310.14344 ,  4861kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03160
replaced with revised version Wed, 27 Mar 2024 22:58:34 GMT   (11321kb,D)

Title: HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces
Authors: Haithem Turki, Vasu Agrawal, Samuel Rota Bul\`o, Lorenzo Porzi, Peter
  Kontschieder, Deva Ramanan, Michael Zollh\"ofer, Christian Richardt
Categories: cs.CV cs.GR cs.LG
Comments: CVPR 2024 Project page: https://haithemturki.com/hybrid-nerf/
\\ ( https://arxiv.org/abs/2312.03160 ,  11321kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11598
replaced with revised version Thu, 28 Mar 2024 16:49:40 GMT   (3724kb,D)

Title: SkillDiffuser: Interpretable Hierarchical Planning via Skill
  Abstractions in Diffusion-Based Task Execution
Authors: Zhixuan Liang, Yao Mu, Hengbo Ma, Masayoshi Tomizuka, Mingyu Ding,
  Ping Luo
Categories: cs.RO cs.CV cs.LG
Comments: Accepted by CVPR 2024. Camera ready version. Project page:
  https://skilldiffuser.github.io/
\\ ( https://arxiv.org/abs/2312.11598 ,  3724kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04061
replaced with revised version Wed, 27 Mar 2024 21:01:24 GMT   (43609kb,D)

Title: TopoNav: Topological Navigation for Efficient Exploration in Sparse
  Reward Environments
Authors: Jumman Hossain, Abu-Zaher Faridee, Nirmalya Roy, Jade Freeman, Timothy
  Gregory, Theron T. Trout
Categories: cs.RO cs.LG
Comments: Paper under review for IROS 2024
\\ ( https://arxiv.org/abs/2402.04061 ,  43609kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19212 (*cross-listing*)
replaced with revised version Thu, 28 Mar 2024 16:59:03 GMT   (37kb)

Title: Deep Reinforcement Learning: A Convex Optimization Approach
Authors: Ather Gattami
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2402.19212 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07728 (*cross-listing*)
replaced with revised version Thu, 28 Mar 2024 14:20:13 GMT   (769kb,D)

Title: CAP: A General Algorithm for Online Selective Conformal Prediction with
  FCR Control
Authors: Yajie Bao, Yuyang Huo, Haojie Ren, Changliang Zou
Categories: stat.ML cs.LG stat.ME
\\ ( https://arxiv.org/abs/2403.07728 ,  769kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11505 (*cross-listing*)
replaced with revised version Wed, 27 Mar 2024 20:10:05 GMT   (6433kb,D)

Title: COVID-19 detection from pulmonary CT scans using a novel EfficientNet
  with attention mechanism
Authors: Ramy Farag, Parth Upadhyay, Yixiang Gao, Jacket Demby, Katherin Garces
  Montoya, Seyed Mohamad Ali Tousi, Gbenga Omotara, Guilherme DeSouza
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2403.11505 ,  6433kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11624
replaced with revised version Thu, 28 Mar 2024 04:11:28 GMT   (1945kb,D)

Title: Dual-Channel Multiplex Graph Neural Networks for Recommendation
Authors: Xiang Li, Chaofan Fu, Zhongying Zhao, Guanjie Zheng, Chao Huang, Junyu
  Dong, Yanwei Yu
Categories: cs.IR cs.LG
\\ ( https://arxiv.org/abs/2403.11624 ,  1945kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11687 (*cross-listing*)
replaced with revised version Thu, 28 Mar 2024 17:56:05 GMT   (73kb,D)

Title: Nonsmooth Implicit Differentiation: Deterministic and Stochastic
  Convergence Rates
Authors: Riccardo Grazzi, Massimiliano Pontil, Saverio Salzo
Categories: stat.ML cs.LG math.OC
Comments: Removed the assumption on the conservative derivative of the fixed
  point map having a product structure: the product of partial conservative
  derivatives is not conservative in general
\\ ( https://arxiv.org/abs/2403.11687 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12659 (*cross-listing*)
replaced with revised version Thu, 28 Mar 2024 14:19:21 GMT   (14936kb,D)

Title: Graph Neural Networks for Carbon Dioxide Adsorption Prediction in
  Aluminium-Exchanged Zeolites
Authors: Marko Petkovi\'c, Jos\'e Manuel Vicent-Luna, Vlado Menkovski, Sof\'ia
  Calero
Categories: cond-mat.mtrl-sci cs.LG
\\ ( https://arxiv.org/abs/2403.12659 ,  14936kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14302
replaced with revised version Thu, 28 Mar 2024 05:13:43 GMT   (530kb,D)

Title: SpikingResformer: Bridging ResNet and Vision Transformer in Spiking
  Neural Networks
Authors: Xinyu Shi, Zecheng Hao, Zhaofei Yu
Categories: cs.NE cs.CV cs.LG
Comments: To be published in the 2024 IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)
\\ ( https://arxiv.org/abs/2403.14302 ,  530kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17458
replaced with revised version Thu, 28 Mar 2024 09:02:35 GMT   (258kb)

Title: Expectations Versus Reality: Evaluating Intrusion Detection Systems in
  Practice
Authors: Jake Hesford, Daniel Cheng, Alan Wan, Larry Huynh, Seungho Kim,
  Hyoungshick Kim, Jin B. Hong
Categories: cs.CR cs.LG
Comments: 10 pages
MSC-class: 68M25, 68M20
ACM-class: C.4; D.m
\\ ( https://arxiv.org/abs/2403.17458 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18703
replaced with revised version Thu, 28 Mar 2024 09:44:06 GMT   (1774kb,D)

Title: FPGA-Based Neural Thrust Controller for UAVs
Authors: Sharif Azem, David Scheunert, Mengguang Li, Jonas Gehrunger, Kai Cui,
  Christian Hochberger and Heinz Koeppl
Categories: eess.SY cs.LG cs.SY
\\ ( https://arxiv.org/abs/2403.18703 ,  1774kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
