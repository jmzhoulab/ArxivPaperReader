paper_240301.txt

Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 80021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月1日 19:54
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Wed 28 Feb 24 19:00:00 GMT  to  Thu 29 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.18679
Date: Wed, 28 Feb 2024 19:49:55 GMT   (34982kb,D)

Title: Data Interpreter: An LLM Agent For Data Science
Authors: Sirui Hong, Yizhang Lin, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi
  Chen, Jiayi Zhang, Jinlin Wang, Lingyao Zhang, Mingchen Zhuge, Taicheng Guo,
  Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang, Xiangtao Lu, Xinbing Liang,
  Yaying Fei, Yuheng Cheng, Zongze Xu, Chenglin Wu, Li Zhang, Min Yang, Xiawu
  Zheng
Categories: cs.AI cs.LG
\\
  Large Language Model (LLM)-based agents have demonstrated remarkable
effectiveness. However, their performance can be compromised in data science
scenarios that require real-time data adjustment, expertise in optimization due
to complex dependencies among various tasks, and the ability to identify
logical errors for precise reasoning. In this study, we introduce the Data
Interpreter, a solution designed to solve with code that emphasizes three
pivotal techniques to augment problem-solving in data science: 1) dynamic
planning with hierarchical graph structures for real-time data adaptability;2)
tool integration dynamically to enhance code proficiency during execution,
enriching the requisite expertise;3) logical inconsistency identification in
feedback, and efficiency enhancement through experience recording. We evaluate
the Data Interpreter on various data science and real-world tasks. Compared to
open-source baselines, it demonstrated superior performance, exhibiting
significant improvements in machine learning tasks, increasing from 0.86 to
0.95. Additionally, it showed a 26% increase in the MATH dataset and a
remarkable 112% improvement in open-ended tasks. The solution will be released
at https://github.com/geekan/MetaGPT.
\\ ( https://arxiv.org/abs/2402.18679 ,  34982kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18715
Date: Wed, 28 Feb 2024 21:23:54 GMT   (372kb,D)

Title: Commonsense Ontology Micropatterns
Authors: Andrew Eells, Brandon Dave, Pascal Hitzler, Cogan Shimizu
Categories: cs.AI cs.LO
\\
  The previously introduced Modular Ontology Modeling methodology (MOMo)
attempts to mimic the human analogical process by using modular patterns to
assemble more complex concepts. To support this, MOMo organizes organizes
ontology design patterns into design libraries, which are programmatically
queryable, to support accelerated ontology development, for both human and
automated processes. However, a major bottleneck to large-scale deployment of
MOMo is the (to-date) limited availability of ready-to-use ontology design
patterns. At the same time, Large Language Models have quickly become a source
of common knowledge and, in some cases, replacing search engines for questions.
In this paper, we thus present a collection of 104 ontology design patterns
representing often occurring nouns, curated from the common-sense knowledge
available in LLMs, organized into a fully-annotated modular ontology design
library ready for use with MOMo.
\\ ( https://arxiv.org/abs/2402.18715 ,  372kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18732
Date: Wed, 28 Feb 2024 22:25:02 GMT   (4869kb,D)

Title: GAIA: Categorical Foundations of Generative AI
Authors: Sridhar Mahadevan
Categories: cs.AI cs.LG
Comments: 65 pages. arXiv admin note: text overlap with arXiv:2212.08981
\\
  In this paper, we propose GAIA, a generative AI architecture based on
category theory. GAIA is based on a hierarchical model where modules are
organized as a simplicial complex. Each simplicial complex updates its internal
parameters biased on information it receives from its superior simplices and in
turn relays updates to its subordinate sub-simplices. Parameter updates are
formulated in terms of lifting diagrams over simplicial sets, where inner and
outer horn extensions correspond to different types of learning problems.
Backpropagation is modeled as an endofunctor over the category of parameters,
leading to a coalgebraic formulation of deep learning.
\\ ( https://arxiv.org/abs/2402.18732 ,  4869kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18743
Date: Wed, 28 Feb 2024 22:54:08 GMT   (9365kb,D)

Title: A revision on Multi-Criteria Decision Making methods for Multi-UAV
  Mission Planning Support
Authors: Cristian Ramirez-Atencia and Victor Rodriguez-Fernandez and David
  Camacho
Categories: cs.AI cs.RO
Comments: Preprint submitted and acepted in Expert Systems with Applications
Journal-ref: Expert Systems with Applications, Volume 160, 2020, 113708
DOI: 10.1016/j.eswa.2020.113708
\\
  Over the last decade, Unmanned Aerial Vehicles (UAVs) have been extensively
used in many commercial applications due to their manageability and risk
avoidance. One of the main problems considered is the Mission Planning for
multiple UAVs, where a solution plan must be found satisfying the different
constraints of the problem. This problem has multiple variables that must be
optimized simultaneously, such as the makespan, the cost of the mission or the
risk. Therefore, the problem has a lot of possible optimal solutions, and the
operator must select the final solution to be executed among them. In order to
reduce the workload of the operator in this decision process, a Decision
Support System (DSS) becomes necessary. In this work, a DSS consisting of
ranking and filtering systems, which order and reduce the optimal solutions,
has been designed. With regard to the ranking system, a wide range of
Multi-Criteria Decision Making (MCDM) methods, including some fuzzy MCDM, are
compared on a multi-UAV mission planning scenario, in order to study which
method could fit better in a multi-UAV decision support system. Expert
operators have evaluated the solutions returned, and the results show, on the
one hand, that fuzzy methods generally achieve better average scores, and on
the other, that all of the tested methods perform better when the preferences
of the operators are biased towards a specific variable, and worse when their
preferences are balanced. For the filtering system, a similarity function based
on the proximity of the solutions has been designed, and on top of that, a
threshold is tuned empirically to decide how to filter solutions without losing
much of the hypervolume of the space of solutions.
\\ ( https://arxiv.org/abs/2402.18743 ,  9365kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18784
Date: Thu, 29 Feb 2024 01:15:17 GMT   (4239kb,D)

Title: Brain-inspired and Self-based Artificial Intelligence
Authors: Yi Zeng, Feifei Zhao, Yuxuan Zhao, Dongcheng Zhao, Enmeng Lu, Qian
  Zhang, Yuwei Wang, Hui Feng, Zhuoya Zhao, Jihang Wang, Qingqun Kong, Yinqian
  Sun, Yang Li, Guobin Shen, Bing Han, Yiting Dong, Wenxuan Pan, Xiang He,
  Aorigele Bao, Jin Wang
Categories: cs.AI q-bio.NC
\\
  The question "Can machines think?" and the Turing Test to assess whether
machines could achieve human-level intelligence is one of the roots of AI. With
the philosophical argument "I think, therefore I am", this paper challenge the
idea of a "thinking machine" supported by current AIs since there is no sense
of self in them. Current artificial intelligence is only seemingly intelligent
information processing and does not truly understand or be subjectively aware
of oneself and perceive the world with the self as human intelligence does. In
this paper, we introduce a Brain-inspired and Self-based Artificial
Intelligence (BriSe AI) paradigm. This BriSe AI paradigm is dedicated to
coordinating various cognitive functions and learning strategies in a
self-organized manner to build human-level AI models and robotic applications.
Specifically, BriSe AI emphasizes the crucial role of the Self in shaping the
future AI, rooted with a practical hierarchical Self framework, including
Perception and Learning, Bodily Self, Autonomous Self, Social Self, and
Conceptual Self. The hierarchical framework of the Self highlights self-based
environment perception, self-bodily modeling, autonomous interaction with the
environment, social interaction and collaboration with others, and even more
abstract understanding of the Self. Furthermore, the positive mutual promotion
and support among multiple levels of Self, as well as between Self and
learning, enhance the BriSe AI's conscious understanding of information and
flexible adaptation to complex environments, serving as a driving force
propelling BriSe AI towards real Artificial General Intelligence.
\\ ( https://arxiv.org/abs/2402.18784 ,  4239kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19085
Date: Thu, 29 Feb 2024 12:12:30 GMT   (8447kb,D)

Title: Controllable Preference Optimization: Toward Controllable
  Multi-Objective Alignment
Authors: Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Jiexin Wang, Huimin Chen,
  Bowen Sun, Ruobing Xie, Jie Zhou, Yankai Lin, Zhiyuan Liu, Maosong Sun
Categories: cs.AI cs.CL cs.SY eess.SY
\\
  Alignment in artificial intelligence pursues the consistency between model
responses and human preferences as well as values. In practice, the
multifaceted nature of human preferences inadvertently introduces what is known
as the "alignment tax" -a compromise where enhancements in alignment within one
objective (e.g.,harmlessness) can diminish performance in others
(e.g.,helpfulness). However, existing alignment techniques are mostly
unidirectional, leading to suboptimal trade-offs and poor flexibility over
various objectives. To navigate this challenge, we argue the prominence of
grounding LLMs with evident preferences. We introduce controllable preference
optimization (CPO), which explicitly specifies preference scores for different
objectives, thereby guiding the model to generate responses that meet the
requirements. Our experimental analysis reveals that the aligned models can
provide responses that match various preferences among the "3H" (helpfulness,
honesty, harmlessness) desiderata. Furthermore, by introducing diverse data and
alignment goals, we surpass baseline methods in aligning with single
objectives, hence mitigating the impact of the alignment tax and achieving
Pareto improvements in multi-objective alignment.
\\ ( https://arxiv.org/abs/2402.19085 ,  8447kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19195
Date: Thu, 29 Feb 2024 14:26:20 GMT   (3451kb,D)

Title: Negative Sampling in Knowledge Graph Representation Learning: A Review
Authors: Tiroshan Madushanka, Ryutaro Ichise
Categories: cs.AI
\\
  Knowledge graph representation learning (KGRL) or knowledge graph embedding
(KGE) plays a crucial role in AI applications for knowledge construction and
information exploration. These models aim to encode entities and relations
present in a knowledge graph into a lower-dimensional vector space. During the
training process of KGE models, using positive and negative samples becomes
essential for discrimination purposes. However, obtaining negative samples
directly from existing knowledge graphs poses a challenge, emphasizing the need
for effective generation techniques. The quality of these negative samples
greatly impacts the accuracy of the learned embeddings, making their generation
a critical aspect of KGRL. This comprehensive survey paper systematically
reviews various negative sampling (NS) methods and their contributions to the
success of KGRL. Their respective advantages and disadvantages are outlined by
categorizing existing NS methods into five distinct categories. Moreover, this
survey identifies open research questions that serve as potential directions
for future investigations. By offering a generalization and alignment of
fundamental NS concepts, this survey provides valuable insights for designing
effective NS methods in the context of KGRL and serves as a motivating force
for further advancements in the field.
\\ ( https://arxiv.org/abs/2402.19195 ,  3451kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19251
Date: Thu, 29 Feb 2024 15:22:26 GMT   (11400kb,D)

Title: A Cognitive-Based Trajectory Prediction Approach for Autonomous Driving
Authors: Haicheng Liao, Yongkang Li, Zhenning Li, Chengyue Wang, Zhiyong Cui,
  Shengbo Eben Li, Chengzhong Xu
Categories: cs.AI cs.RO
\\
  In autonomous vehicle (AV) technology, the ability to accurately predict the
movements of surrounding vehicles is paramount for ensuring safety and
operational efficiency. Incorporating human decision-making insights enables
AVs to more effectively anticipate the potential actions of other vehicles,
significantly improving prediction accuracy and responsiveness in dynamic
environments. This paper introduces the Human-Like Trajectory Prediction (HLTP)
model, which adopts a teacher-student knowledge distillation framework inspired
by human cognitive processes. The HLTP model incorporates a sophisticated
teacher-student knowledge distillation framework. The "teacher" model, equipped
with an adaptive visual sector, mimics the visual processing of the human
brain, particularly the functions of the occipital and temporal lobes. The
"student" model focuses on real-time interaction and decision-making, drawing
parallels to prefrontal and parietal cortex functions. This approach allows for
dynamic adaptation to changing driving scenarios, capturing essential
perceptual cues for accurate prediction. Evaluated using the Macao Connected
and Autonomous Driving (MoCAD) dataset, along with the NGSIM and HighD
benchmarks, HLTP demonstrates superior performance compared to existing models,
particularly in challenging environments with incomplete data. The project page
is available at Github.
\\ ( https://arxiv.org/abs/2402.19251 ,  11400kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19265
Date: Thu, 29 Feb 2024 15:36:01 GMT   (6437kb,D)

Title: Learning Logic Specifications for Policy Guidance in POMDPs: an
  Inductive Logic Programming Approach
Authors: Daniele Meli, Alberto Castellini, Alessandro Farinelli
Categories: cs.AI cs.LG cs.LO
Journal-ref: Journal of Artificial Intelligence Research, volume 79 (2024), pp.
  725-776
DOI: 10.1613/jair.1.15826
\\
  Partially Observable Markov Decision Processes (POMDPs) are a powerful
framework for planning under uncertainty. They allow to model state uncertainty
as a belief probability distribution. Approximate solvers based on Monte Carlo
sampling show great success to relax the computational demand and perform
online planning. However, scaling to complex realistic domains with many
actions and long planning horizons is still a major challenge, and a key point
to achieve good performance is guiding the action-selection process with
domain-dependent policy heuristics which are tailored for the specific
application domain. We propose to learn high-quality heuristics from POMDP
traces of executions generated by any solver. We convert the belief-action
pairs to a logical semantics, and exploit data- and time-efficient Inductive
Logic Programming (ILP) to generate interpretable belief-based policy
specifications, which are then used as online heuristics. We evaluate
thoroughly our methodology on two notoriously challenging POMDP problems,
involving large action spaces and long planning horizons, namely, rocksample
and pocman. Considering different state-of-the-art online POMDP solvers,
including POMCP, DESPOT and AdaOPS, we show that learned heuristics expressed
in Answer Set Programming (ASP) yield performance superior to neural networks
and similar to optimal handcrafted task-specific heuristics within lower
computational time. Moreover, they well generalize to more challenging
scenarios not experienced in the training phase (e.g., increasing rocks and
grid size in rocksample, incrementing the size of the map and the aggressivity
of ghosts in pocman).
\\ ( https://arxiv.org/abs/2402.19265 ,  6437kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19299
Date: Thu, 29 Feb 2024 16:07:22 GMT   (6013kb,D)

Title: RL-GPT: Integrating Reinforcement Learning and Code-as-policy
Authors: Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu,
  Zongqing Lu, Jiaya Jia
Categories: cs.AI cs.LG
\\
  Large Language Models (LLMs) have demonstrated proficiency in utilizing
various tools by coding, yet they face limitations in handling intricate logic
and precise control. In embodied tasks, high-level planning is amenable to
direct coding, while low-level actions often necessitate task-specific
refinement, such as Reinforcement Learning (RL). To seamlessly integrate both
modalities, we introduce a two-level hierarchical framework, RL-GPT, comprising
a slow agent and a fast agent. The slow agent analyzes actions suitable for
coding, while the fast agent executes coding tasks. This decomposition
effectively focuses each agent on specific tasks, proving highly efficient
within our pipeline. Our approach outperforms traditional RL methods and
existing GPT agents, demonstrating superior efficiency. In the Minecraft game,
it rapidly obtains diamonds within a single day on an RTX3090. Additionally, it
achieves SOTA performance across all designated MineDojo tasks.
\\ ( https://arxiv.org/abs/2402.19299 ,  6013kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19450
Date: Thu, 29 Feb 2024 18:48:18 GMT   (184kb,D)

Title: Functional Benchmarks for Robust Evaluation of Reasoning Performance,
  and the Reasoning Gap
Authors: Saurabh Srivastava, Annarose M B, Anto P V, Shashank Menon, Ajay
  Sukumar, Adwaith Samod T, Alan Philipose, Stevin Prince and Sooraj Thomas
Categories: cs.AI cs.CL
Comments: 37 pages, 10 figures
\\
  We propose a framework for robust evaluation of reasoning capabilities of
language models, using functional variants of benchmarks. Models that solve a
reasoning test should exhibit no difference in performance over the static
version of a problem compared to a snapshot of the functional variant. We have
rewritten the relevant fragment of the MATH benchmark into its functional
variant MATH(), with functionalization of other benchmarks to follow. When
evaluating current state-of-the-art models over snapshots of MATH(), we find a
reasoning gap -- the percentage difference between the static and functional
accuracies. We find reasoning gaps from 58.35% to 80.31% among the
state-of-the-art closed and open weights models that perform well on static
benchmarks, with the caveat that the gaps are likely to be smaller with more
sophisticated prompting strategies. Here we show that models which anecdotally
have good reasoning performance over real-world tasks, have quantifiable lower
gaps, motivating the open problem of building "gap 0" models. Code for
evaluation and new evaluation datasets, three MATH() snapshots, are publicly
available at https://github.com/consequentai/fneval/.
\\ ( https://arxiv.org/abs/2402.19450 ,  184kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18659
Date: Wed, 28 Feb 2024 19:09:08 GMT   (8163kb,D)

Title: Large Language Models and Games: A Survey and Roadmap
Authors: Roberto Gallotta, Graham Todd, Marvin Zammit, Sam Earle, Antonios
  Liapis, Julian Togelius and Georgios N. Yannakakis
Categories: cs.CL cs.AI cs.HC
Comments: 13 pages, 4 figures
\\
  Recent years have seen an explosive increase in research on large language
models (LLMs), and accompanying public engagement on the topic. While starting
as a niche area within natural language processing, LLMs have shown remarkable
potential across a broad range of applications and domains, including games.
This paper surveys the current state of the art across the various applications
of LLMs in and for games, and identifies the different roles LLMs can take
within a game. Importantly, we discuss underexplored areas and promising
directions for future uses of LLMs in games and we reconcile the potential and
limitations of LLMs within the games domain. As the first comprehensive survey
and roadmap at the intersection of LLMs and games, we are hopeful that this
paper will serve as the basis for groundbreaking research and innovation in
this exciting new field.
\\ ( https://arxiv.org/abs/2402.18659 ,  8163kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18667
Date: Wed, 28 Feb 2024 19:23:27 GMT   (8066kb,D)

Title: FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability
Authors: Congying Xia, Chen Xing, Jiangshu Du, Xinyi Yang, Yihao Feng, Ran Xu,
  Wenpeng Yin, Caiming Xiong
Categories: cs.CL
Comments: The first two authors contributed equally
\\
  This paper presents FoFo, a pioneering benchmark for evaluating large
language models' (LLMs) ability to follow complex, domain-specific formats, a
crucial yet underexamined capability for their application as AI agents.
Despite LLMs' advancements, existing benchmarks fail to assess their
format-following proficiency adequately. FoFo fills this gap with a diverse
range of real-world formats and instructions, developed through an AI-Human
collaborative method. Our evaluation across both open-source (e.g., Llama 2,
WizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three
key findings: open-source models significantly lag behind closed-source ones in
format adherence; LLMs' format-following performance is independent of their
content generation quality; and LLMs' format proficiency varies across
different domains. These insights suggest the need for specialized tuning for
format-following skills and highlight FoFo's role in guiding the selection of
domain-specific AI agents. FoFo is released here at
https://github.com/SalesforceAIResearch/FoFo.
\\ ( https://arxiv.org/abs/2402.18667 ,  8066kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18668
Date: Wed, 28 Feb 2024 19:28:27 GMT   (2245kb,D)

Title: Simple linear attention language models balance the recall-throughput
  tradeoff
Authors: Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas
  Alberti, Dylan Zinsley, James Zou, Atri Rudra, Christopher R\'e
Categories: cs.CL cs.LG
\\
  Recent work has shown that attention-based language models excel at recall,
the ability to ground generations in tokens previously seen in context.
However, the efficiency of attention-based models is bottle-necked during
inference by the KV-cache's aggressive memory consumption. In this work, we
explore whether we can improve language model efficiency (e.g. by reducing
memory consumption) without compromising on recall. By applying experiments and
theory to a broad set of architectures, we identify a key tradeoff between a
model's state size and recall ability. We show that efficient alternatives to
attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but
struggle at recall. We propose BASED a simple architecture combining linear and
sliding window attention. By varying BASED window size and linear attention
feature dimension, we can dial the state size and traverse the pareto frontier
of the recall-memory tradeoff curve, recovering the full quality of attention
on one end and the small state size of attention-alternatives on the other. We
train language models up to 1.3b parameters and show that BASED matches the
strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them
on real-world recall-intensive tasks by 6.22 accuracy points. Implementations
of linear attention are often less efficient than optimized standard attention
implementations. To make BASED competitive, we develop IO-aware algorithms that
enable 24x higher throughput on language generation than FlashAttention-2, when
generating 1024 tokens using 1.3b parameter models. Code for this work is
provided at: https://github.com/HazyResearch/based.
\\ ( https://arxiv.org/abs/2402.18668 ,  2245kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18678
Date: Wed, 28 Feb 2024 19:46:21 GMT   (911kb,D)

Title: RORA: Robust Free-Text Rationale Evaluation
Authors: Zhengping Jiang, Yining Lu, Hanjie Chen, Daniel Khashabi, Benjamin Van
  Durme, Anqi Liu
Categories: cs.CL
\\
  Free-text rationales play a pivotal role in explainable NLP, bridging the
knowledge and reasoning gaps behind a model's decision-making. However, due to
the diversity of potential reasoning paths and a corresponding lack of
definitive ground truth, their evaluation remains a challenge. Existing
evaluation metrics rely on the degree to which a rationale supports a target
label, but we find these fall short in evaluating rationales that inadvertently
leak the labels. To address this problem, we propose RORA, a Robust free-text
Rationale evaluation against label leakage. RORA quantifies the new information
supplied by a rationale to justify the label. This is achieved by assessing the
conditional V-information \citep{hewitt-etal-2021-conditional} with a
predictive family robust against leaky features that can be exploited by a
small model. RORA consistently outperforms existing approaches in evaluating
human-written, synthetic, or model-generated rationales, particularly
demonstrating robustness against label leakage. We also show that RORA aligns
well with human judgment, providing a more reliable and accurate measurement
across diverse free-text rationales.
\\ ( https://arxiv.org/abs/2402.18678 ,  911kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18700
Date: Wed, 28 Feb 2024 20:41:21 GMT   (1190kb,D)

Title: Learning to Compress Prompt in Natural Language Formats
Authors: Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen,
  Xia Hu
Categories: cs.CL cs.AI cs.LG
\\
  Large language models (LLMs) are great at processing multiple natural
language processing tasks, but their abilities are constrained by inferior
performance with long context, slow inference speed, and the high cost of
computing the results. Deploying LLMs with precise and informative context
helps users process large-scale datasets more effectively and cost-efficiently.
Existing works rely on compressing long prompt contexts into soft prompts.
However, soft prompt compression encounters limitations in transferability
across different LLMs, especially API-based LLMs. To this end, this work aims
to compress lengthy prompts in the form of natural language with LLM
transferability. This poses two challenges: (i) Natural Language (NL) prompts
are incompatible with back-propagation, and (ii) NL prompts lack flexibility in
imposing length constraints. In this work, we propose a Natural Language Prompt
Encapsulation (Nano-Capsulator) framework compressing original prompts into NL
formatted Capsule Prompt while maintaining the prompt utility and
transferability. Specifically, to tackle the first challenge, the
Nano-Capsulator is optimized by a reward function that interacts with the
proposed semantics preserving loss. To address the second question, the
Nano-Capsulator is optimized by a reward function featuring length constraints.
Experimental results demonstrate that the Capsule Prompt can reduce 81.4% of
the original length, decrease inference latency up to 4.5x, and save 80.1% of
budget overheads while providing transferability across diverse LLMs and
different datasets.
\\ ( https://arxiv.org/abs/2402.18700 ,  1190kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18747
Date: Wed, 28 Feb 2024 23:01:24 GMT   (8232kb,D)

Title: Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains
Authors: Vil\'em Zouhar, Shuoyang Ding, Anna Currey, Tatyana Badeka, Jenyuan
  Wang, Brian Thompson
Categories: cs.CL cs.AI
\\
  We introduce a new, extensive multidimensional quality metrics (MQM)
annotated dataset covering 11 language pairs in the biomedical domain. We use
this dataset to investigate whether machine translation (MT) metrics which are
fine-tuned on human-generated MT quality judgements are robust to domain shifts
between training and inference. We find that fine-tuned metrics exhibit a
substantial performance drop in the unseen domain scenario relative to metrics
that rely on the surface form, as well as pre-trained metrics which are not
fine-tuned on MT quality judgments.
\\ ( https://arxiv.org/abs/2402.18747 ,  8232kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18756
Date: Wed, 28 Feb 2024 23:34:51 GMT   (2032kb,D)

Title: How Much Annotation is Needed to Compare Summarization Models?
Authors: Chantal Shaib, Joe Barrow, Alexa F. Siu, Byron C. Wallace, Ani Nenkova
Categories: cs.CL
Comments: Preprint
\\
  Modern instruction-tuned models have become highly capable in text generation
tasks such as summarization, and are expected to be released at a steady pace.
In practice one may now wish to choose confidently, but with minimal effort,
the best performing summarization model when applied to a new domain or
purpose. In this work, we empirically investigate the test sample size
necessary to select a preferred model in the context of news summarization.
Empirical results reveal that comparative evaluation converges quickly for both
automatic and human evaluation, with clear preferences for a system emerging
from under 100 examples. The human preference data allows us to quantify how
well automatic scores can reproduce preference rankings across a variety of
downstream summarization tasks. We find that, while automatic metrics are
stable at smaller sample sizes, only some automatic metrics are able to
moderately predict model win rates according to human preference.
\\ ( https://arxiv.org/abs/2402.18756 ,  2032kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18766
Date: Thu, 29 Feb 2024 00:19:13 GMT   (65kb)

Title: Advancing Generative AI for Portuguese with Open Decoder Gerv\'asio PT*
Authors: Rodrigo Santos, Jo\~ao Silva, Lu\'is Gomes, Jo\~ao Rodrigues,
  Ant\'onio Branco
Categories: cs.CL
\\
  To advance the neural decoding of Portuguese, in this paper we present a
fully open Transformer-based, instruction-tuned decoder model that sets a new
state of the art in this respect. To develop this decoder, which we named
Gerv\'asio PT*, a strong LLaMA~2 7B model was used as a starting point, and its
further improvement through additional training was done over language
resources that include new instruction data sets of Portuguese prepared for
this purpose, which are also contributed in this paper. All versions of
Gerv\'asio are open source and distributed for free under an open license,
including for either research or commercial usage, and can be run on
consumer-grade hardware, thus seeking to contribute to the advancement of
research and innovation in language technology for Portuguese.
\\ ( https://arxiv.org/abs/2402.18766 ,  65kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18807
Date: Thu, 29 Feb 2024 02:22:23 GMT   (738kb,D)

Title: On the Decision-Making Abilities in Role-Playing using Large Language
  Models
Authors: Chenglei Shen and Guofu Xie and Xiao Zhang and Jun Xu
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) are now increasingly utilized for role-playing
tasks, especially in impersonating domain-specific experts, primarily through
role-playing prompts. When interacting in real-world scenarios, the
decision-making abilities of a role significantly shape its behavioral
patterns. In this paper, we concentrate on evaluating the decision-making
abilities of LLMs post role-playing thereby validating the efficacy of
role-playing. Our goal is to provide metrics and guidance for enhancing the
decision-making abilities of LLMs in role-playing tasks. Specifically, we first
use LLMs to generate virtual role descriptions corresponding to the 16
personality types of Myers-Briggs Type Indicator (abbreviated as MBTI)
representing a segmentation of the population. Then we design specific
quantitative operations to evaluate the decision-making abilities of LLMs post
role-playing from four aspects: adaptability, exploration$\&$exploitation
trade-off ability, reasoning ability, and safety. Finally, we analyze the
association between the performance of decision-making and the corresponding
MBTI types through GPT-4. Extensive experiments demonstrate stable differences
in the four aspects of decision-making abilities across distinct roles,
signifying a robust correlation between decision-making abilities and the roles
emulated by LLMs. These results underscore that LLMs can effectively
impersonate varied roles while embodying their genuine sociological
characteristics.
\\ ( https://arxiv.org/abs/2402.18807 ,  738kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18815
Date: Thu, 29 Feb 2024 02:55:26 GMT   (1531kb,D)

Title: How do Large Language Models Handle Multilingualism?
Authors: Yiran Zhao, Wenxuan Zhang, Guizhen Chen, Kenji Kawaguchi, Lidong Bing
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) demonstrate remarkable performance across a
spectrum of languages. In this work, we delve into the question: How do LLMs
handle multilingualism? We introduce a framework that depicts LLMs' processing
of multilingual inputs: In the first several layers, LLMs understand the
question, converting multilingual inputs into English to facilitate the
task-solving phase. In the intermediate layers, LLMs engage in problem-solving
by thinking in English and incorporating multilingual knowledge to obtain
factual content, leveraging the self-attention and feed-forward structures,
respectively. In the last several layers, LLMs generate responses that align
with the original language of the query. In addition, we investigate the
existence of language-specific neurons when processing a certain language. To
detect neurons activated by the input language, even without labels, we
innovatively design a Parallel Language specific Neuron Detection
($\texttt{PLND}$) method that effectively measures the significance of neurons
when handling multilingual inputs. By comprehensive ablation analysis through
deactivating neurons of different layers and structures, we verify the
framework that we propose. Additionally, we demonstrate that we can utilize
such a framework to effectively enhance the multilingual ability with much less
training effort.
\\ ( https://arxiv.org/abs/2402.18815 ,  1531kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18825
Date: Thu, 29 Feb 2024 03:20:45 GMT   (163kb,D)

Title: Utilizing Local Hierarchy with Adversarial Training for Hierarchical
  Text Classification
Authors: Zihan Wang, Peiyi Wang, Houfeng Wang
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\
  Hierarchical text classification (HTC) is a challenging subtask of
multi-label classification due to its complex taxonomic structure. Nearly all
recent HTC works focus on how the labels are structured but ignore the
sub-structure of ground-truth labels according to each input text which
contains fruitful label co-occurrence information. In this work, we introduce
this local hierarchy with an adversarial framework. We propose a HiAdv
framework that can fit in nearly all HTC models and optimize them with the
local hierarchy as auxiliary information. We test on two typical HTC models and
find that HiAdv is effective in all scenarios and is adept at dealing with
complex taxonomic hierarchies. Further experiments demonstrate that the
promotion of our framework indeed comes from the local hierarchy and the local
hierarchy is beneficial for rare classes which have insufficient training data.
\\ ( https://arxiv.org/abs/2402.18825 ,  163kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18838
Date: Thu, 29 Feb 2024 04:11:10 GMT   (8384kb,D)

Title: When does word order matter and when doesn't it?
Authors: Xuanda Chen and Timothy O'Donnell and Siva Reddy
Categories: cs.CL
Comments: 5 pages
\\
  Language models (LMs) may appear insensitive to word order changes in natural
language understanding (NLU) tasks. In this paper, we propose that linguistic
redundancy can explain this phenomenon, whereby word order and other linguistic
cues such as case markers provide overlapping and thus redundant information.
Our hypothesis is that models exhibit insensitivity to word order when the
order provides redundant information, and the degree of insensitivity varies
across tasks. We quantify how informative word order is using mutual
information (MI) between unscrambled and scrambled sentences. Our results show
the effect that the less informative word order is, the more consistent the
model's predictions are between unscrambled and scrambled sentences. We also
find that the effect varies across tasks: for some tasks, like SST-2, LMs'
prediction is almost always consistent with the original one even if the
Pointwise-MI (PMI) changes, while for others, like RTE, the consistency is near
random when the PMI gets lower, i.e., word order is really important.
\\ ( https://arxiv.org/abs/2402.18838 ,  8384kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18873
Date: Thu, 29 Feb 2024 05:43:43 GMT   (231kb,D)

Title: Reducing Hallucinations in Entity Abstract Summarization with
  Facts-Template Decomposition
Authors: Fangwei Zhu, Peiyi Wang, Zhifang Sui
Categories: cs.CL
\\
  Entity abstract summarization aims to generate a coherent description of a
given entity based on a set of relevant Internet documents. Pretrained language
models (PLMs) have achieved significant success in this task, but they may
suffer from hallucinations, i.e. generating non-factual information about the
entity. To address this issue, we decompose the summary into two components:
Facts that represent the factual information about the given entity, which PLMs
are prone to fabricate; and Template that comprises generic content with
designated slots for facts, which PLMs can generate competently. Based on the
facts-template decomposition, we propose SlotSum, an explainable framework for
entity abstract summarization. SlotSum first creates the template and then
predicts the fact for each template slot based on the input documents.
Benefiting from our facts-template decomposition, SlotSum can easily locate
errors and further rectify hallucinated predictions with external knowledge. We
construct a new dataset WikiFactSum to evaluate the performance of SlotSum.
Experimental results demonstrate that SlotSum could generate summaries that are
significantly more factual with credible external knowledge.
\\ ( https://arxiv.org/abs/2402.18873 ,  231kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18877
Date: Thu, 29 Feb 2024 05:47:34 GMT   (888kb,D)

Title: Principal Component Analysis as a Sanity Check for Bayesian
  Phylolinguistic Reconstruction
Authors: Yugo Murawaki
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  Bayesian approaches to reconstructing the evolutionary history of languages
rely on the tree model, which assumes that these languages descended from a
common ancestor and underwent modifications over time. However, this assumption
can be violated to different extents due to contact and other factors.
Understanding the degree to which this assumption is violated is crucial for
validating the accuracy of phylolinguistic inference. In this paper, we propose
a simple sanity check: projecting a reconstructed tree onto a space generated
by principal component analysis. By using both synthetic and real data, we
demonstrate that our method effectively visualizes anomalies, particularly in
the form of jogging.
\\ ( https://arxiv.org/abs/2402.18877 ,  888kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18909
Date: Thu, 29 Feb 2024 07:08:34 GMT   (1089kb,D)

Title: Updating Language Models with Unstructured Facts: Towards Practical
  Knowledge Editing
Authors: Xiaobao Wu, Liangming Pan, William Yang Wang, Anh Tuan Luu
Categories: cs.CL cs.AI
\\
  Knowledge editing aims to inject knowledge updates into language models to
keep them correct and up-to-date. However, its current evaluation strategies
are notably impractical: they solely update with well-curated structured facts
(triplets with subjects, relations, and objects), whereas real-world knowledge
updates commonly emerge in unstructured texts like news articles. In this
paper, we propose a new benchmark, Unstructured Knowledge Editing (UKE). It
evaluates editing performance directly using unstructured texts as knowledge
updates, termed unstructured facts. Hence UKE avoids the laborious construction
of structured facts and enables efficient and responsive knowledge editing,
becoming a more practical benchmark. We conduct extensive experiments on newly
built datasets and demonstrate that UKE poses a significant challenge to
state-of-the-art knowledge editing methods, resulting in their critical
performance declines. We further show that this challenge persists even if we
extract triplets as structured facts. Our analysis discloses key insights to
motivate future research in UKE for more practical knowledge editing.
\\ ( https://arxiv.org/abs/2402.18909 ,  1089kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18913
Date: Thu, 29 Feb 2024 07:11:24 GMT   (7677kb,D)

Title: AdaMergeX: Cross-Lingual Transfer with Large Language Models via
  Adaptive Adapter Merging
Authors: Yiran Zhao, Wenxuan Zhang, Huiming Wang, Kenji Kawaguchi, Lidong Bing
Categories: cs.CL cs.AI
\\
  As an effective alternative to the direct fine-tuning on target tasks in
specific languages, cross-lingual transfer addresses the challenges of limited
training data by decoupling ''task ability'' and ''language ability'' by
fine-tuning on the target task in the source language and another selected task
in the target language, respectively. However, they fail to fully separate the
task ability from the source language or the language ability from the chosen
task. In this paper, we acknowledge the mutual reliance between task ability
and language ability and direct our attention toward the gap between the target
language and the source language on tasks. As the gap removes the impact of
tasks, we assume that it remains consistent across tasks. Based on this
assumption, we propose a new cross-lingual transfer method called
$\texttt{AdaMergeX}$ that utilizes adaptive adapter merging. By introducing a
reference task, we can determine that the divergence of adapters fine-tuned on
the reference task in both languages follows the same distribution as the
divergence of adapters fine-tuned on the target task in both languages. Hence,
we can obtain target adapters by combining the other three adapters.
Furthermore, we propose a structure-adaptive adapter merging method. Our
empirical results demonstrate that our approach yields new and effective
cross-lingual transfer, outperforming existing methods across all settings.
\\ ( https://arxiv.org/abs/2402.18913 ,  7677kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18923
Date: Thu, 29 Feb 2024 07:29:42 GMT   (672kb,D)

Title: Inappropriate Pause Detection In Dysarthric Speech Using Large-Scale
  Speech Recognition
Authors: Jeehyun Lee, Yerin Choi, Tae-Jin Song, Myoung-Wan Koo
Categories: cs.CL cs.SD eess.AS
Comments: Accepted to ICASSP 2024
\\
  Dysarthria, a common issue among stroke patients, severely impacts speech
intelligibility. Inappropriate pauses are crucial indicators in severity
assessment and speech-language therapy. We propose to extend a large-scale
speech recognition model for inappropriate pause detection in dysarthric
speech. To this end, we propose task design, labeling strategy, and a speech
recognition model with an inappropriate pause prediction layer. First, we treat
pause detection as speech recognition, using an automatic speech recognition
(ASR) model to convert speech into text with pause tags. According to the newly
designed task, we label pause locations at the text level and their
appropriateness. We collaborate with speech-language pathologists to establish
labeling criteria, ensuring high-quality annotated data. Finally, we extend the
ASR model with an inappropriate pause prediction layer for end-to-end
inappropriate pause detection. Moreover, we propose a task-tailored metric for
evaluating inappropriate pause detection independent of ASR performance. Our
experiments show that the proposed method better detects inappropriate pauses
in dysarthric speech than baselines. (Inappropriate Pause Error Rate: 14.47%)
\\ ( https://arxiv.org/abs/2402.18923 ,  672kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18944
Date: Thu, 29 Feb 2024 08:20:06 GMT   (174kb,D)

Title: SemEval 2024 -- Task 10: Emotion Discovery and Reasoning its Flip in
  Conversation (EDiReF)
Authors: Shivani Kumar, Md Shad Akhtar, Erik Cambria, Tanmoy Chakraborty
Categories: cs.CL cs.AI
Comments: 11 pages, 3 figures, 7 tables
\\
  We present SemEval-2024 Task 10, a shared task centred on identifying
emotions and finding the rationale behind their flips within monolingual
English and Hindi-English code-mixed dialogues. This task comprises three
distinct subtasks - emotion recognition in conversation for code-mixed
dialogues, emotion flip reasoning for code-mixed dialogues, and emotion flip
reasoning for English dialogues. Participating systems were tasked to
automatically execute one or more of these subtasks. The datasets for these
tasks comprise manually annotated conversations focusing on emotions and
triggers for emotion shifts (The task data is available at
https://github.com/LCS2-IIITD/EDiReF-SemEval2024.git). A total of 84
participants engaged in this task, with the most adept systems attaining
F1-scores of 0.70, 0.79, and 0.76 for the respective subtasks. This paper
summarises the results and findings from 24 teams alongside their system
descriptions.
\\ ( https://arxiv.org/abs/2402.18944 ,  174kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18950
Date: Thu, 29 Feb 2024 08:28:04 GMT   (7639kb,D)

Title: PopALM: Popularity-Aligned Language Models for Social Media Trendy
  Response Prediction
Authors: Erxin Yu, Jing Li, Chunpu Xu
Categories: cs.CL
Comments: Accepted by COLING 2024
\\
  Social media platforms are daily exhibiting millions of events. To
preliminarily predict the mainstream public reaction to these events, we study
trendy response prediction to automatically generate top-liked user replies to
social media events. While previous works focus on generating responses without
factoring in popularity, we propose Popularity-Aligned Language Models (PopALM)
to distinguish responses liked by a larger audience through reinforcement
learning. Recognizing the noisy labels from user "likes", we tailor-make
curriculum learning in proximal policy optimization (PPO) to help models
capture the essential samples for easy-to-hard training. In experiments, we
build a large-scale Weibo dataset for trendy response prediction, and its
results show that PopALM can help boost the performance of advanced language
models.
\\ ( https://arxiv.org/abs/2402.18950 ,  7639kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19052
Date: Thu, 29 Feb 2024 11:29:47 GMT   (1034kb)

Title: Exploring the Efficacy of Large Language Models in Summarizing Mental
  Health Counseling Sessions: A Benchmark Study
Authors: Prottay Kumar Adhikary, Aseem Srivastava, Shivani Kumar, Salam Michael
  Singh, Puneet Manuja, Jini K Gopinath, Vijay Krishnan, Swati Kedia, Koushik
  Sinha Deb, Tanmoy Chakraborty
Categories: cs.CL cs.HC
\\
  Comprehensive summaries of sessions enable an effective continuity in mental
health counseling, facilitating informed therapy planning. Yet, manual
summarization presents a significant challenge, diverting experts' attention
from the core counseling process. This study evaluates the effectiveness of
state-of-the-art Large Language Models (LLMs) in selectively summarizing
various components of therapy sessions through aspect-based summarization,
aiming to benchmark their performance. We introduce MentalCLOUDS, a
counseling-component guided summarization dataset consisting of 191 counseling
sessions with summaries focused on three distinct counseling components (aka
counseling aspects). Additionally, we assess the capabilities of 11
state-of-the-art LLMs in addressing the task of component-guided summarization
in counseling. The generated summaries are evaluated quantitatively using
standard summarization metrics and verified qualitatively by mental health
professionals. Our findings demonstrate the superior performance of
task-specific LLMs such as MentalLlama, Mistral, and MentalBART in terms of
standard quantitative metrics such as Rouge-1, Rouge-2, Rouge-L, and BERTScore
across all aspects of counseling components. Further, expert evaluation reveals
that Mistral supersedes both MentalLlama and MentalBART based on six parameters
-- affective attitude, burden, ethicality, coherence, opportunity costs, and
perceived effectiveness. However, these models share the same weakness by
demonstrating a potential for improvement in the opportunity costs and
perceived effectiveness metrics.
\\ ( https://arxiv.org/abs/2402.19052 ,  1034kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19076
Date: Thu, 29 Feb 2024 12:01:46 GMT   (1148kb,D)

Title: Pointing out the Shortcomings of Relation Extraction Models with
  Semantically Motivated Adversarials
Authors: Gennaro Nolano, Moritz Blum, Basil Ell, Philipp Cimiano
Categories: cs.CL
\\
  In recent years, large language models have achieved state-of-the-art
performance across various NLP tasks. However, investigations have shown that
these models tend to rely on shortcut features, leading to inaccurate
predictions and causing the models to be unreliable at generalization to
out-of-distribution (OOD) samples. For instance, in the context of relation
extraction (RE), we would expect a model to identify the same relation
independently of the entities involved in it. For example, consider the
sentence "Leonardo da Vinci painted the Mona Lisa" expressing the
created(Leonardo_da_Vinci, Mona_Lisa) relation. If we substiute "Leonardo da
Vinci" with "Barack Obama", then the sentence still expresses the created
relation. A robust model is supposed to detect the same relation in both cases.
In this work, we describe several semantically-motivated strategies to generate
adversarial examples by replacing entity mentions and investigate how
state-of-the-art RE models perform under pressure. Our analyses show that the
performance of these models significantly deteriorates on the modified datasets
(avg. of -48.5% in F1), which indicates that these models rely to a great
extent on shortcuts, such as surface forms (or patterns therein) of entities,
without making full use of the information present in the sentences.
\\ ( https://arxiv.org/abs/2402.19076 ,  1148kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19088
Date: Thu, 29 Feb 2024 12:13:50 GMT   (398kb,D)

Title: Survey in Characterization of Semantic Change
Authors: Jader Martins Camboim de S\'a, Marcos Da Silveira, C\'edric Pruski
Categories: cs.CL cs.AI
\\
  Live languages continuously evolve to integrate the cultural change of human
societies. This evolution manifests through neologisms (new words) or
\textbf{semantic changes} of words (new meaning to existing words).
Understanding the meaning of words is vital for interpreting texts coming from
different cultures (regionalism or slang), domains (e.g., technical terms), or
periods. In computer science, these words are relevant to computational
linguistics algorithms such as translation, information retrieval, question
answering, etc. Semantic changes can potentially impact the quality of the
outcomes of these algorithms. Therefore, it is important to understand and
characterize these changes formally. The study of this impact is a recent
problem that has attracted the attention of the computational linguistics
community. Several approaches propose methods to detect semantic changes with
good precision, but more effort is needed to characterize how the meaning of
words changes and to reason about how to reduce the impact of semantic change.
This survey provides an understandable overview of existing approaches to the
\textit{characterization of semantic changes} and also formally defines three
classes of characterizations: if the meaning of a word becomes more general or
narrow (change in dimension) if the word is used in a more pejorative or
positive/ameliorated sense (change in orientation), and if there is a trend to
use the word in a, for instance, metaphoric or metonymic context (change in
relation). We summarized the main aspects of the selected publications in a
table and discussed the needs and trends in the research activities on semantic
change characterization.
\\ ( https://arxiv.org/abs/2402.19088 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19097
Date: Thu, 29 Feb 2024 12:25:45 GMT   (2378kb,D)

Title: TEncDM: Understanding the Properties of Diffusion Model in the Space of
  Language Model Encodings
Authors: Alexander Shabalin, Viacheslav Meshchaninov, Tingir Badmaev, Dmitry
  Molchanov, Grigory Bartosh, Sergey Markov, Dmitry Vetrov
Categories: cs.CL
Comments: 14 pages, 8 figures, submitted to ACL 2024
ACM-class: I.2; I.7
\\
  Drawing inspiration from the success of diffusion models in various domains,
numerous research papers proposed methods for adapting them to text data.
Despite these efforts, none of them has managed to achieve the quality of the
large language models. In this paper, we conduct a comprehensive analysis of
key components of the text diffusion models and introduce a novel approach
named Text Encoding Diffusion Model (TEncDM). Instead of the commonly used
token embedding space, we train our model in the space of the language model
encodings. Additionally, we propose to use a Transformer-based decoder that
utilizes contextual information for text reconstruction. We also analyse
self-conditioning and find that it increases the magnitude of the model
outputs, allowing the reduction of the number of denoising steps at the
inference stage. Evaluation of TEncDM on two downstream text generation tasks,
QQP and XSum, demonstrates its superiority over existing non-autoregressive
models.
\\ ( https://arxiv.org/abs/2402.19097 ,  2378kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19103
Date: Thu, 29 Feb 2024 12:35:45 GMT   (508kb,D)

Title: Whispers that Shake Foundations: Analyzing and Mitigating False Premise
  Hallucinations in Large Language Models
Authors: Hongbang Yuan, Pengfei Cao, Zhuoran Jin, Yubo Chen, Daojian Zeng, Kang
  Liu, Jun Zhao
Categories: cs.CL cs.AI
Comments: 12 pages, 5 figures, 5 tables
\\
  Large Language Models (LLMs) have shown impressive capabilities but still
suffer from the issue of hallucinations. A significant type of this issue is
the false premise hallucination, which we define as the phenomenon when LLMs
generate hallucinated text when confronted with false premise questions. In
this paper, we perform a comprehensive analysis of the false premise
hallucination and elucidate its internal working mechanism: a small subset of
attention heads (which we designate as false premise heads) disturb the
knowledge extraction process, leading to the occurrence of false premise
hallucination. Based on our analysis, we propose \textbf{FAITH} (\textbf{F}alse
premise \textbf{A}ttention head constra\textbf{I}ining for mi\textbf{T}igating
\textbf{H}allucinations), a novel and effective method to mitigate false
premise hallucinations. It constrains the false premise attention heads during
the model inference process. Impressively, extensive experiments demonstrate
that constraining only approximately $1\%$ of the attention heads in the model
yields a notable increase of nearly $20\%$ of model performance.
\\ ( https://arxiv.org/abs/2402.19103 ,  508kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19116
Date: Thu, 29 Feb 2024 12:49:48 GMT   (1115kb,D)

Title: How to Understand "Support"? An Implicit-enhanced Causal Inference
  Approach for Weakly-supervised Phrase Grounding
Authors: Jiamin Luo, Jianing Zhao, Jingjing Wang, Guodong Zhou
Categories: cs.CL cs.AI
\\
  Weakly-supervised Phrase Grounding (WPG) is an emerging task of inferring the
fine-grained phrase-region matching, while merely leveraging the coarse-grained
sentence-image pairs for training. However, existing studies on WPG largely
ignore the implicit phrase-region matching relations, which are crucial for
evaluating the capability of models in understanding the deep multimodal
semantics. To this end, this paper proposes an Implicit-Enhanced Causal
Inference (IECI) approach to address the challenges of modeling the implicit
relations and highlighting them beyond the explicit. Specifically, this
approach leverages both the intervention and counterfactual techniques to
tackle the above two challenges respectively. Furthermore, a high-quality
implicit-enhanced dataset is annotated to evaluate IECI and detailed
evaluations show the great advantages of IECI over the state-of-the-art
baselines. Particularly, we observe an interesting finding that IECI
outperforms the advanced multimodal LLMs by a large margin on this
implicit-enhanced dataset, which may facilitate more research to evaluate the
multimodal LLMs in this direction.
\\ ( https://arxiv.org/abs/2402.19116 ,  1115kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19133
Date: Thu, 29 Feb 2024 13:09:26 GMT   (8251kb,D)

Title: Evaluating Webcam-based Gaze Data as an Alternative for Human Rationale
  Annotations
Authors: Stephanie Brandl, Oliver Eberle, Tiago Ribeiro, Anders S{\o}gaard,
  Nora Hollenstein
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  Rationales in the form of manually annotated input spans usually serve as
ground truth when evaluating explainability methods in NLP. They are, however,
time-consuming and often biased by the annotation process. In this paper, we
debate whether human gaze, in the form of webcam-based eye-tracking recordings,
poses a valid alternative when evaluating importance scores. We evaluate the
additional information provided by gaze data, such as total reading times, gaze
entropy, and decoding accuracy with respect to human rationale annotations. We
compare WebQAmGaze, a multilingual dataset for information-seeking QA, with
attention and explainability-based importance scores for 4 different
multilingual Transformer-based language models (mBERT, distil-mBERT, XLMR, and
XLMR-L) and 3 languages (English, Spanish, and German). Our pipeline can easily
be applied to other tasks and languages. Our findings suggest that gaze data
offers valuable linguistic insights that could be leveraged to infer task
difficulty and further show a comparable ranking of explainability methods to
that of human rationales.
\\ ( https://arxiv.org/abs/2402.19133 ,  8251kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19167
Date: Thu, 29 Feb 2024 13:50:47 GMT   (8513kb,D)

Title: Teaching Large Language Models an Unseen Language on the Fly
Authors: Chen Zhang, Xiao Liu, Jiuheng Lin, Yansong Feng
Categories: cs.CL
\\
  Existing large language models struggle to support numerous low-resource
languages, particularly the extremely low-resource ones where there is minimal
training data available for effective parameter updating. We thus investigate
whether LLMs can learn a new language on the fly solely through prompting. To
study this question, we collect a research suite for Zhuang, a language
supported by no LLMs currently. We introduce \textsc{DiPMT++}, a framework for
adapting LLMs to unseen languages by in-context learning. Using a dictionary
and only 5K parallel sentences, \textsc{DiPMT++} significantly enhances the
performance of GPT-4 from 0 to 16 BLEU for Chinese-to-Zhuang translation and
achieves 32 BLEU for Zhuang-to-Chinese translation. Furthermore, we demonstrate
the practical utility of this framework in aiding humans to translate
completely unseen languages, which could contribute to the preservation of
linguistic diversity.
\\ ( https://arxiv.org/abs/2402.19167 ,  8513kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19170
Date: Thu, 29 Feb 2024 13:52:33 GMT   (834kb)

Title: Improving Legal Judgement Prediction in Romanian with Long Text Encoders
Authors: Mihai Masala, Traian Rebedea and Horia Velicu
Categories: cs.CL cs.AI
\\
  In recent years,the entire field of Natural Language Processing (NLP) has
enjoyed amazing novel results achieving almost human-like performance on a
variety of tasks. Legal NLP domain has also been part of this process, as it
has seen an impressive growth. However, general-purpose models are not readily
applicable for legal domain. Due to the nature of the domain (e.g. specialized
vocabulary, long documents) specific models and methods are often needed for
Legal NLP. In this work we investigate both specialized and general models for
predicting the final ruling of a legal case, task known as Legal Judgment
Prediction (LJP). We particularly focus on methods to extend to sequence length
of Transformer-based models to better understand the long documents present in
legal corpora. Extensive experiments on 4 LJP datasets in Romanian, originating
from 2 sources with significantly different sizes and document lengths, show
that specialized models and handling long texts are critical for a good
performance.
\\ ( https://arxiv.org/abs/2402.19170 ,  834kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19204
Date: Thu, 29 Feb 2024 14:34:03 GMT   (20kb,D)

Title: PeLLE: Encoder-based language models for Brazilian Portuguese based on
  open data
Authors: Guilherme Lamartine de Mello and Marcelo Finger and and Felipe Serras
  and Miguel de Mello Carpi and Marcos Menon Jose and Pedro Henrique Domingues
  and Paulo Cavalim
Categories: cs.CL
Comments: 15 pages
ACM-class: I.2.7
\\
  In this paper we present PeLLE, a family of large language models based on
the RoBERTa architecture, for Brazilian Portuguese, trained on curated, open
data from the Carolina corpus. Aiming at reproducible results, we describe
details of the pretraining of the models. We also evaluate PeLLE models against
a set of existing multilingual and PT-BR refined pretrained Transformer-based
LLM encoders, contrasting performance of large versus smaller-but-curated
pretrained models in several downstream tasks. We conclude that several tasks
perform better with larger models, but some tasks benefit from
smaller-but-curated data in its pretraining.
\\ ( https://arxiv.org/abs/2402.19204 ,  20kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19218
Date: Thu, 29 Feb 2024 14:47:24 GMT   (544kb,D)

Title: Memory-Augmented Generative Adversarial Transformers
Authors: Stephan Raaijmakers, Roos Bakker, Anita Cremers, Roy de Kleijn, Tom
  Kouwenhoven, Tessa Verhoef
Categories: cs.CL
\\
  Conversational AI systems that rely on Large Language Models, like
Transformers, have difficulty interweaving external data (like facts) with the
language they generate. Vanilla Transformer architectures are not designed for
answering factual questions with high accuracy. This paper investigates a
possible route for addressing this problem. We propose to extend the standard
Transformer architecture with an additional memory bank holding extra
information (such as facts drawn from a knowledge base), and an extra attention
layer for addressing this memory. We add this augmented memory to a Generative
Adversarial Network-inspired Transformer architecture. This setup allows for
implementing arbitrary felicity conditions on the generated language of the
Transformer. We first demonstrate how this machinery can be deployed for
handling factual questions in goal-oriented dialogues. Secondly, we demonstrate
that our approach can be useful for applications like {\it style adaptation} as
well: the adaptation of utterances according to certain stylistic (external)
constraints, like social properties of human interlocutors in dialogues.
\\ ( https://arxiv.org/abs/2402.19218 ,  544kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19248
Date: Thu, 29 Feb 2024 15:22:13 GMT   (8140kb,D)

Title: Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question
  Answering Benchmark
Authors: Zhikun Xu, Yinghui Li, Ruixue Ding, Xinyu Wang, Boli Chen, Yong Jiang,
  Xiaodong Deng, Jianxin Ma, Hai-Tao Zheng, Wenlian Lu, Pengjun Xie, Chang
  Zhou, Fei Huang
Categories: cs.CL
Comments: Work in progress!
\\
  How to better evaluate the capabilities of Large Language Models (LLMs) is
the focal point and hot topic in current LLMs research. Previous work has noted
that due to the extremely high cost of iterative updates of LLMs, they are
often unable to answer the latest dynamic questions well. To promote the
improvement of Chinese LLMs' ability to answer dynamic questions, in this
paper, we introduce CDQA, a Chinese Dynamic QA benchmark containing
question-answer pairs related to the latest news on the Chinese Internet. We
obtain high-quality data through a pipeline that combines humans and models,
and carefully classify the samples according to the frequency of answer changes
to facilitate a more fine-grained observation of LLMs' capabilities. We have
also evaluated and analyzed mainstream and advanced Chinese LLMs on CDQA.
Extensive experiments and valuable insights suggest that our proposed CDQA is
challenging and worthy of more further study. We believe that the benchmark we
provide will become the key data resource for improving LLMs' Chinese
question-answering ability in the future.
\\ ( https://arxiv.org/abs/2402.19248 ,  8140kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19255
Date: Thu, 29 Feb 2024 15:26:14 GMT   (7926kb,D)

Title: GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of
  LLMs as Mathematical Problem Solvers
Authors: Qintong Li and Leyang Cui and Xueliang Zhao and Lingpeng Kong and Wei
  Bi
Categories: cs.CL
\\
  Large language models (LLMs) have achieved impressive performance across
various mathematical reasoning benchmarks. However, there are increasing
debates regarding whether these models truly understand and apply mathematical
knowledge or merely rely on shortcuts for mathematical reasoning. One essential
and frequently occurring evidence is that when the math questions are slightly
changed, LLMs can behave incorrectly. This motivates us to evaluate the
robustness of LLMs' math reasoning capability by testing a wide range of
question variations. We introduce the adversarial grade school math
(\datasetname) dataset, an extension of GSM8K augmented with various
mathematical perturbations. Our experiments on 25 LLMs and 4 prompting
techniques show that while LLMs exhibit different levels of math reasoning
abilities, their performances are far from robust. In particular, even for
problems that have been solved in GSM8K, LLMs can make mistakes when new
statements are added or the question targets are altered. We also explore
whether more robust performance can be achieved by composing existing prompting
methods, in which we try an iterative method that generates and verifies each
intermediate thought based on its reasoning goal and calculation result. Code
and data are available at \url{https://github.com/qtli/GSM-Plus}.
\\ ( https://arxiv.org/abs/2402.19255 ,  7926kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19267
Date: Thu, 29 Feb 2024 15:38:28 GMT   (319kb,D)

Title: Robust Guidance for Unsupervised Data Selection: Capturing Perplexing
  Named Entities for Domain-Specific Machine Translation
Authors: Seunghyun Ji, Hagai Raja Sinulingga, Darongsae Kwon
Categories: cs.CL cs.AI
Comments: Submitted to SIGUL 2024, a satellite workshop of LREC-COLING 2024
\\
  Employing extensive datasets enables the training of multilingual machine
translation models; however, these models often fail to accurately translate
sentences within specialized domains. Although obtaining and translating
domain-specific data incurs high costs, it is inevitable for high-quality
translations. Hence, finding the most 'effective' data with an unsupervised
setting becomes a practical strategy for reducing labeling costs. Recent
research indicates that this effective data could be found by selecting
'properly difficult data' based on its volume. This means the data should not
be excessively challenging or overly simplistic, especially if the amount of
data is limited. However, we found that establishing a criterion for
unsupervised data selection remains challenging, as the 'proper difficulty'
might vary based on the data domain being trained on. We introduce a novel
unsupervised data selection method, 'Capturing Perplexing Named Entities',
which adopts the maximum inference entropy in translated named entities as a
selection measure. The motivation was that named entities in domain-specific
data are considered the most complex portion of the data and should be
predicted with high confidence. When verified with the 'Korean-English Parallel
Corpus of Specialized Domains,' our method served as a robust guidance for
unsupervised data selection, in contrast to existing methods.
\\ ( https://arxiv.org/abs/2402.19267 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19273
Date: Thu, 29 Feb 2024 15:41:20 GMT   (4538kb,D)

Title: PlanGPT: Enhancing Urban Planning with Tailored Language Model and
  Efficient Retrieval
Authors: He Zhu, Wenjia Zhang, Nuoxian Huang, Boyang Li, Luyao Niu, Zipei Fan,
  Tianle Lun, Yicheng Tao, Junyou Su, Zhaoya Gong, Chenyu Fang and Xing Liu
Categories: cs.CL
\\
  In the field of urban planning, general-purpose large language models often
struggle to meet the specific needs of planners. Tasks like generating urban
planning texts, retrieving related information, and evaluating planning
documents pose unique challenges. To enhance the efficiency of urban
professionals and overcome these obstacles, we introduce PlanGPT, the first
specialized Large Language Model tailored for urban and spatial planning.
Developed through collaborative efforts with institutions like the Chinese
Academy of Urban Planning, PlanGPT leverages a customized local database
retrieval framework, domain-specific fine-tuning of base models, and advanced
tooling capabilities. Empirical tests demonstrate that PlanGPT has achieved
advanced performance, delivering responses of superior quality precisely
tailored to the intricacies of urban planning.
\\ ( https://arxiv.org/abs/2402.19273 ,  4538kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19282
Date: Thu, 29 Feb 2024 15:49:15 GMT   (123kb,D)

Title: WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset
Authors: Jiantao Qiu, Haijun Lv, Zhenjiang Jin, Rui Wang, Wenchang Ning, Jia
  Yu, ChaoBin Zhang, Pei Chu, Yuan Qu, Runyu Peng, Zhiyuan Zeng, Huanze Tang,
  Ruiliang Xu, Wei Li, Hang Yan, and Conghui He
Categories: cs.CL
\\
  This paper presents WanJuan-CC, a safe and high-quality open-sourced English
webtext dataset derived from Common Crawl data. The study addresses the
challenges of constructing large-scale pre-training datasets for language
models, which require vast amounts of high-quality data. A comprehensive
process was designed to handle Common Crawl data, including extraction,
heuristic rule filtering, fuzzy deduplication, content safety filtering, and
data quality filtering. From approximately 68 billion original English
documents, we obtained 2.22T Tokens of safe data and selected 1.0T Tokens of
high-quality data as part of WanJuan-CC. We have open-sourced 300B Tokens from
this dataset. The paper also provides statistical information related to data
quality, enabling users to select appropriate data according to their needs. To
evaluate the quality and utility of the dataset, we trained 1B-parameter and
3B-parameter models using WanJuan-CC and another dataset, RefinedWeb. Results
show that WanJuan-CC performs better on validation datasets and downstream
tasks.
\\ ( https://arxiv.org/abs/2402.19282 ,  123kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19333
Date: Thu, 29 Feb 2024 16:36:51 GMT   (7700kb,D)

Title: Compact Speech Translation Models via Discrete Speech Units Pretraining
Authors: Tsz Kin Lam and Alexandra Birch and Barry Haddow
Categories: cs.CL cs.SD eess.AS
\\
  Using Self-Supervised Learning (SSL) as model initialization is now common to
obtain strong results in Speech Translation (ST). However, they also impose a
large memory footprint, hindering on-device deployment. In this paper, we
leverage the SSL models by pretraining smaller models on their Discrete Speech
Units (DSU). We pretrain encoder-decoder models on 1) Filterbank-to-DSU and 2)
DSU-to-Translation data, and take the encoder from 1) and the decoder from 2)
to initialise a new model, finetuning this on limited speech-translation data.
The final model becomes compact by using the DSU pretraining to distil the
knowledge of the SSL model. Our method has several benefits over using DSU as
model inputs, such as shorter inference pipeline and robustness over (DSU)
tokenization. In contrast to ASR pretraining, it does not require transcripts,
making it applicable to low-resource settings. Evaluation on CoVoST-2 X-En
shows that our method is >$0.5$ BLEU better than a ST model that directly
finetune the SSL model, given only half the model size, and on a par with ASR
pretraining.
\\ ( https://arxiv.org/abs/2402.19333 ,  7700kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19334
Date: Thu, 29 Feb 2024 16:37:08 GMT   (1114kb,D)

Title: Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge
Authors: Ansh Arora, Xuanli He, Maximilian Mozes, Srinibas Swain, Mark Dras,
  and Qiongkai Xu
Categories: cs.CL
Comments: work in progress
\\
  The democratization of pre-trained language models through open-source
initiatives has rapidly advanced innovation and expanded access to cutting-edge
technologies. However, this openness also brings significant security risks,
including backdoor attacks, where hidden malicious behaviors are triggered by
specific inputs, compromising natural language processing (NLP) system
integrity and reliability. This paper suggests that merging a backdoored model
with other homogeneous models can remediate backdoor vulnerabilities even if
such models are not entirely secure. In our experiments, we explore various
models (BERT-Base, RoBERTa-Large, Llama2-7B, and Mistral-7B) and datasets
(SST-2, OLID, AG News, and QNLI). Compared to multiple advanced defensive
approaches, our method offers an effective and efficient inference-stage
defense against backdoor attacks without additional resources or specific
knowledge. Our approach consistently outperforms the other advanced baselines,
leading to an average of 75% reduction in the attack success rate. Since model
merging has been an established approach for improving model performance, the
extra advantage it provides regarding defense can be seen as a cost-free bonus.
\\ ( https://arxiv.org/abs/2402.19334 ,  1114kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19350
Date: Thu, 29 Feb 2024 16:56:36 GMT   (229kb,D)

Title: Prompting Explicit and Implicit Knowledge for Multi-hop Question
  Answering Based on Human Reading Process
Authors: Guangming Huang, Yunfei Long, Cunjin Luo, Jiaxing Shen, Xia Sun
Categories: cs.CL
Comments: This paper has been accepted at LREC-COLING 2024
\\
  Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to
simulate human reasoning and inference processes, achieving proficient
performance in multi-hop QA. However, a gap persists between PLMs' reasoning
abilities and those of humans when tackling complex problems. Psychological
studies suggest a vital connection between explicit information in passages and
human prior knowledge during reading. Nevertheless, current research has given
insufficient attention to linking input passages and PLMs' pre-training-based
knowledge from the perspective of human cognition studies. In this study, we
introduce a \textbf{P}rompting \textbf{E}xplicit and \textbf{I}mplicit
knowledge (PEI) framework, which uses prompts to connect explicit and implicit
knowledge, aligning with human reading process for multi-hop QA. We consider
the input passages as explicit knowledge, employing them to elicit implicit
knowledge through unified prompt reasoning. Furthermore, our model incorporates
type-specific reasoning via prompts, a form of implicit knowledge. Experimental
results show that PEI performs comparably to the state-of-the-art on HotpotQA.
Ablation studies confirm the efficacy of our model in bridging and integrating
explicit and implicit knowledge.
\\ ( https://arxiv.org/abs/2402.19350 ,  229kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19371
Date: Thu, 29 Feb 2024 17:19:39 GMT   (712kb)

Title: OpenMedLM: Prompt engineering can out-perform fine-tuning in medical
  question-answering with open-source large language models
Authors: Jenish Maharjan, Anurag Garikipati, Navan Preet Singh, Leo Cyrus,
  Mayank Sharma, Madalina Ciobanu, Gina Barnes, Rahul Thapa, Qingqing Mao,
  Ritankar Das
Categories: cs.CL cs.AI cs.IR
\\
  LLMs have become increasingly capable at accomplishing a range of
specialized-tasks and can be utilized to expand equitable access to medical
knowledge. Most medical LLMs have involved extensive fine-tuning, leveraging
specialized medical data and significant, thus costly, amounts of computational
power. Many of the top performing LLMs are proprietary and their access is
limited to very few research groups. However, open-source (OS) models represent
a key area of growth for medical LLMs due to significant improvements in
performance and an inherent ability to provide the transparency and compliance
required in healthcare. We present OpenMedLM, a prompting platform which
delivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks.
We evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks
(MedQA, MedMCQA, PubMedQA, MMLU medical-subset). We employed a series of
prompting strategies, including zero-shot, few-shot, chain-of-thought (random
selection and kNN selection), and ensemble/self-consistency voting. We found
that OpenMedLM delivers OS SOTA results on three common medical LLM benchmarks,
surpassing the previous best performing OS models that leveraged
computationally costly extensive fine-tuning. The model delivers a 72.6%
accuracy on the MedQA benchmark, outperforming the previous SOTA by 2.4%, and
achieves 81.7% accuracy on the MMLU medical-subset, establishing itself as the
first OS LLM to surpass 80% accuracy on this benchmark. Our results highlight
medical-specific emergent properties in OS LLMs which have not yet been
documented to date elsewhere, and showcase the benefits of further leveraging
prompt engineering to improve the performance of accessible LLMs for medical
applications.
\\ ( https://arxiv.org/abs/2402.19371 ,  712kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19406
Date: Thu, 29 Feb 2024 18:04:11 GMT   (10140kb,D)

Title: On the Scaling Laws of Geographical Representation in Language Models
Authors: Nathan Godey, \'Eric de la Clergerie, Beno\^it Sagot
Categories: cs.CL cs.AI
Comments: Accepted at LREC-COLING 2024
\\
  Language models have long been shown to embed geographical information in
their hidden representations. This line of work has recently been revisited by
extending this result to Large Language Models (LLMs). In this paper, we
propose to fill the gap between well-established and recent literature by
observing how geographical knowledge evolves when scaling language models. We
show that geographical knowledge is observable even for tiny models, and that
it scales consistently as we increase the model size. Notably, we observe that
larger language models cannot mitigate the geographical bias that is inherent
to the training data.
\\ ( https://arxiv.org/abs/2402.19406 ,  10140kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19457
Date: Thu, 29 Feb 2024 18:51:23 GMT   (17792kb,D)

Title: $\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization
  Evaluation
Authors: Maxime Darrin, Philippe Formont, Jackie Chi Kit Cheung, Pablo
  Piantanida
Categories: cs.CL cs.AI
Comments: AAAI 2024
\\
  Assessing the quality of summarizers poses significant challenges. In
response, we propose a novel task-oriented evaluation approach that assesses
summarizers based on their capacity to produce summaries that are useful for
downstream tasks, while preserving task outcomes. We theoretically establish a
direct relationship between the resulting error probability of these tasks and
the mutual information between source texts and generated summaries. We
introduce $\texttt{COSMIC}$ as a practical implementation of this metric,
demonstrating its strong correlation with human judgment-based metrics and its
effectiveness in predicting downstream task performance. Comparative analyses
against established metrics like $\texttt{BERTScore}$ and $\texttt{ROUGE}$
highlight the competitive performance of $\texttt{COSMIC}$.
\\ ( https://arxiv.org/abs/2402.19457 ,  17792kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19465
Date: Thu, 29 Feb 2024 18:55:06 GMT   (1500kb,D)

Title: Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period
  of Large Language Models
Authors: Chen Qian, Jie Zhang, Wei Yao, Dongrui Liu, Zhenfei Yin, Yu Qiao, Yong
  Liu, Jing Shao
Categories: cs.CL cs.AI
\\
  Ensuring the trustworthiness of large language models (LLMs) is crucial. Most
studies concentrate on fully pre-trained LLMs to better understand and improve
LLMs' trustworthiness. In this paper, to reveal the untapped potential of
pre-training, we pioneer the exploration of LLMs' trustworthiness during this
period, focusing on five key dimensions: reliability, privacy, toxicity,
fairness, and robustness. To begin with, we apply linear probing to LLMs. The
high probing accuracy suggests that \textit{LLMs in early pre-training can
already distinguish concepts in each trustworthiness dimension}. Therefore, to
further uncover the hidden possibilities of pre-training, we extract steering
vectors from a LLM's pre-training checkpoints to enhance the LLM's
trustworthiness. Finally, inspired by~\citet{choi2023understanding} that mutual
information estimation is bounded by linear probing accuracy, we also probe
LLMs with mutual information to investigate the dynamics of trustworthiness
during pre-training. We are the first to observe a similar two-phase
phenomenon: fitting and compression~\citep{shwartz2017opening}. This research
provides an initial exploration of trustworthiness modeling during LLM
pre-training, seeking to unveil new insights and spur further developments in
the field. We will make our code publicly accessible at
\url{https://github.com/ChnQ/TracingLLM}.
\\ ( https://arxiv.org/abs/2402.19465 ,  1500kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19467
Date: Thu, 29 Feb 2024 18:57:01 GMT   (10967kb,D)

Title: TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning
Authors: Kate Sanders, Nathaniel Weir, Benjamin Van Durme
Categories: cs.CL cs.AI cs.CV
Comments: 9 pages, preprint
ACM-class: I.2.7; I.2.10
\\
  It is challenging to perform question-answering over complex, multimodal
content such as television clips. This is in part because current
video-language models rely on single-modality reasoning, have lowered
performance on long inputs, and lack interpetability. We propose TV-TREES, the
first multimodal entailment tree generator. TV-TREES serves as an approach to
video understanding that promotes interpretable joint-modality reasoning by
producing trees of entailment relationships between simple premises directly
entailed by the videos and higher-level conclusions. We then introduce the task
of multimodal entailment tree generation to evaluate the reasoning quality of
such methods. Our method's experimental results on the challenging TVQA dataset
demonstrate intepretable, state-of-the-art zero-shot performance on full video
clips, illustrating a best of both worlds contrast to black-box methods.
\\ ( https://arxiv.org/abs/2402.19467 ,  10967kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19471
Date: Thu, 29 Feb 2024 18:58:15 GMT   (728kb,D)

Title: Loose LIPS Sink Ships: Asking Questions in Battleship with
  Language-Informed Program Sampling
Authors: Gabriel Grand, Valerio Pepe, Jacob Andreas, Joshua B. Tenenbaum
Categories: cs.CL cs.AI
\\
  Questions combine our mastery of language with our remarkable facility for
reasoning about uncertainty. How do people navigate vast hypothesis spaces to
pose informative questions given limited cognitive resources? We study these
tradeoffs in a classic grounded question-asking task based on the board game
Battleship. Our language-informed program sampling (LIPS) model uses large
language models (LLMs) to generate natural language questions, translate them
into symbolic programs, and evaluate their expected information gain. We find
that with a surprisingly modest resource budget, this simple Monte Carlo
optimization strategy yields informative questions that mirror human
performance across varied Battleship board scenarios. In contrast, LLM-only
baselines struggle to ground questions in the board state; notably, GPT-4V
provides no improvement over non-visual baselines. Our results illustrate how
Bayesian models of question-asking can leverage the statistics of language to
capture human priors, while highlighting some shortcomings of pure LLMs as
grounded reasoners.
\\ ( https://arxiv.org/abs/2402.19471 ,  728kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18591
Date: Mon, 12 Feb 2024 06:56:13 GMT   (28kb)

Title: Stochastic contextual bandits with graph feedback: from independence
  number to MAS number
Authors: Yuxiao Wen, Yanjun Han, Zhengyuan Zhou
Categories: cs.LG cs.GT math.ST stat.TH
\\
  We consider contextual bandits with graph feedback, a class of interactive
learning problems with richer structures than vanilla contextual bandits, where
taking an action reveals the rewards for all neighboring actions in the
feedback graph under all contexts. Unlike the multi-armed bandits setting where
a growing literature has painted a near-complete understanding of graph
feedback, much remains unexplored in the contextual bandits counterpart. In
this paper, we make inroads into this inquiry by establishing a regret lower
bound $\Omega(\sqrt{\beta_M(G) T})$, where $M$ is the number of contexts, $G$
is the feedback graph, and $\beta_M(G)$ is our proposed graph-theoretical
quantity that characterizes the fundamental learning limit for this class of
problems. Interestingly, $\beta_M(G)$ interpolates between $\alpha(G)$ (the
independence number of the graph) and $\mathsf{m}(G)$ (the maximum acyclic
subgraph (MAS) number of the graph) as the number of contexts $M$ varies. We
also provide algorithms that achieve near-optimal regrets for important classes
of context sequences and/or feedback graphs, such as transitively closed graphs
that find applications in auctions and inventory control. In particular, with
many contexts, our results show that the MAS number completely characterizes
the statistical complexity for contextual bandits, as opposed to the
independence number in multi-armed bandits.
\\ ( https://arxiv.org/abs/2402.18591 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18599
Date: Tue, 27 Feb 2024 21:15:40 GMT   (3823kb,D)

Title: Meta-Tasks: An alternative view on Meta-Learning Regularization
Authors: Mohammad Rostami, Atik Faysal, Huaxia Wang, Avimanyu Sahoo and Ryan
  Antle
Categories: cs.LG cs.AI
\\
  Few-shot learning (FSL) is a challenging machine learning problem due to a
scarcity of labeled data. The ability to generalize effectively on both novel
and training tasks is a significant barrier to FSL. This paper proposes a novel
solution that can generalize to both training and novel tasks while also
utilizing unlabeled samples. The method refines the embedding model before
updating the outer loop using unsupervised techniques as ``meta-tasks''. The
experimental results show that our proposed method performs well on novel and
training tasks, with faster and better convergence, lower generalization, and
standard deviation error, indicating its potential for practical applications
in FSL. The experimental results show that the proposed method outperforms
prototypical networks by 3.9%.
\\ ( https://arxiv.org/abs/2402.18599 ,  3823kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18603
Date: Wed, 28 Feb 2024 08:29:42 GMT   (693kb,D)

Title: MMSR: Symbolic Regression is a Multimodal Task
Authors: Yanjie Li, Jingyi Liu, Weijun Li, Lina Yu, Min Wu, Wenqiang Li, Meilan
  Hao, Su Wei, Yusong Deng
Categories: cs.LG cs.AI cs.CL
Comments: 12 page
\\
  Mathematical formulas are the crystallization of human wisdom in exploring
the laws of nature for thousands of years. Describing the complex laws of
nature with a concise mathematical formula is a constant pursuit of scientists
and a great challenge for artificial intelligence. This field is called
symbolic regression. Symbolic regression was originally formulated as a
combinatorial optimization problem, and GP and reinforcement learning
algorithms were used to solve it. However, GP is sensitive to hyperparameters,
and these two types of algorithms are inefficient. To solve this problem,
researchers treat the mapping from data to expressions as a translation
problem. And the corresponding large-scale pre-trained model is introduced.
However, the data and expression skeletons do not have very clear word
correspondences as the two languages do. Instead, they are more like two
modalities (e.g., image and text). Therefore, in this paper, we proposed MMSR.
The SR problem is solved as a pure multimodal problem, and contrastive learning
is also introduced in the training process for modal alignment to facilitate
later modal feature fusion. It is worth noting that in order to better promote
the modal feature fusion, we adopt the strategy of training contrastive
learning loss and other losses at the same time, which only needs one-step
training, instead of training contrastive learning loss first and then training
other losses. Because our experiments prove training together can make the
feature extraction module and feature fusion module running-in better.
Experimental results show that compared with multiple large-scale pre-training
baselines, MMSR achieves the most advanced results on multiple mainstream
datasets including SRBench.
\\ ( https://arxiv.org/abs/2402.18603 ,  693kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18605
Date: Wed, 28 Feb 2024 10:57:30 GMT   (1533kb,D)

Title: FORML: A Riemannian Hessian-free Method for Meta-learning with
  Orthogonality Constraint
Authors: Hadi Tabealhojeh, Soumava Kumar Roy, Peyman Adibi and Hossein
  Karshenas
Categories: cs.LG
\\
  Meta-learning problem is usually formulated as a bi-level optimization in
which the task-specific and the meta-parameters are updated in the inner and
outer loops of optimization, respectively. However, performing the optimization
in the Riemannian space, where the parameters and meta-parameters are located
on Riemannian manifolds is computationally intensive. Unlike the Euclidean
methods, the Riemannian backpropagation needs computing the second-order
derivatives that include backward computations through the Riemannian operators
such as retraction and orthogonal projection. This paper introduces a
Hessian-free approach that uses a first-order approximation of derivatives on
the Stiefel manifold. Our method significantly reduces the computational load
and memory footprint. We show how using a Stiefel fully-connected layer that
enforces orthogonality constraint on the parameters of the last classification
layer as the head of the backbone network, strengthens the representation reuse
of the gradient-based meta-learning methods. Our experimental results across
various few-shot learning datasets, demonstrate the superiority of our proposed
method compared to the state-of-the-art methods, especially MAML, its Euclidean
counterpart.
\\ ( https://arxiv.org/abs/2402.18605 ,  1533kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18606
Date: Wed, 28 Feb 2024 11:13:53 GMT   (22379kb,D)

Title: Impact of network topology on the performance of Decentralized Federated
  Learning
Authors: Luigi Palmieri and Chiara Boldrini and Lorenzo Valerio and Andrea
  Passarella and Marco Conti
Categories: cs.LG cs.AI cs.DC
Comments: Funding: H2020 HumaneAI Net (Grant N. 952026), CHIST-ERA SAI
  (CHIST-ERA-19-XAI010), PNRR FAIR (PE00000013), PNRR RESTART (PE00000001).
  arXiv admin note: text overlap with arXiv:2307.15947
\\
  Fully decentralized learning is gaining momentum for training AI models at
the Internet's edge, addressing infrastructure challenges and privacy concerns.
In a decentralized machine learning system, data is distributed across multiple
nodes, with each node training a local model based on its respective dataset.
The local models are then shared and combined to form a global model capable of
making accurate predictions on new data. Our exploration focuses on how
different types of network structures influence the spreading of knowledge -
the process by which nodes incorporate insights gained from learning patterns
in data available on other nodes across the network. Specifically, this study
investigates the intricate interplay between network structure and learning
performance using three network topologies and six data distribution methods.
These methods consider different vertex properties, including degree
centrality, betweenness centrality, and clustering coefficient, along with
whether nodes exhibit high or low values of these metrics. Our findings
underscore the significance of global centrality metrics (degree, betweenness)
in correlating with learning performance, while local clustering proves less
predictive. We highlight the challenges in transferring knowledge from
peripheral to central nodes, attributed to a dilution effect during model
aggregation. Additionally, we observe that central nodes exert a pull effect,
facilitating the spread of knowledge. In examining degree distribution, hubs in
Barabasi-Albert networks positively impact learning for central nodes but
exacerbate dilution when knowledge originates from peripheral nodes. Finally,
we demonstrate the formidable challenge of knowledge circulation outside of
segregated communities.
\\ ( https://arxiv.org/abs/2402.18606 ,  22379kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18607
Date: Wed, 28 Feb 2024 12:21:12 GMT   (5992kb,D)

Title: Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An
  Adversarial Perspective
Authors: Xinjian Luo, Yangfan Jiang, Fei Wei, Yuncheng Wu, Xiaokui Xiao, Beng
  Chin Ooi
Categories: cs.LG cs.AI cs.CR
\\
  Diffusion models have recently gained significant attention in both academia
and industry due to their impressive generative performance in terms of both
sampling quality and distribution coverage. Accordingly, proposals are made for
sharing pre-trained diffusion models across different organizations, as a way
of improving data utilization while enhancing privacy protection by avoiding
sharing private data directly. However, the potential risks associated with
such an approach have not been comprehensively examined.
  In this paper, we take an adversarial perspective to investigate the
potential privacy and fairness risks associated with the sharing of diffusion
models. Specifically, we investigate the circumstances in which one party (the
sharer) trains a diffusion model using private data and provides another party
(the receiver) black-box access to the pre-trained model for downstream tasks.
We demonstrate that the sharer can execute fairness poisoning attacks to
undermine the receiver's downstream models by manipulating the training data
distribution of the diffusion model. Meanwhile, the receiver can perform
property inference attacks to reveal the distribution of sensitive features in
the sharer's dataset. Our experiments conducted on real-world datasets
demonstrate remarkable attack performance on different types of diffusion
models, which highlights the critical importance of robust data auditing and
privacy protection protocols in pertinent applications.
\\ ( https://arxiv.org/abs/2402.18607 ,  5992kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18609
Date: Wed, 28 Feb 2024 15:06:25 GMT   (3903kb,D)

Title: ICE-SEARCH: A Language Model-Driven Feature Selection Approach
Authors: Tianze (Tom) Yang, Tianyi (Tim) Yang, Shaoshan Liu, Fuyuan Lvu, Xue
  Liu
Categories: cs.LG cs.AI
\\
  This study unveils the In-Context Evolutionary Search (ICE-SEARCH) method,
the first work that melds language models (LMs) with evolutionary algorithms
for feature selection (FS) tasks and demonstrates its effectiveness in Medical
Predictive Analytics (MPA) applications. ICE-SEARCH harnesses the crossover and
mutation capabilities inherent in LMs within an evolutionary framework,
significantly improving FS through the model's comprehensive world knowledge
and its adaptability to a variety of roles. Our evaluation of this methodology
spans three crucial MPA tasks: stroke, cardiovascular disease, and diabetes,
where ICE-SEARCH outperforms traditional FS methods in pinpointing essential
features for medical applications. ICE-SEARCH achieves State-of-the-Art (SOTA)
performance in stroke prediction and diabetes prediction; the
Decision-Randomized ICE-SEARCH ranks as SOTA in cardiovascular disease
prediction. Our results not only demonstrate the efficacy of ICE-SEARCH in
medical FS but also underscore the versatility, efficiency, and scalability of
integrating LMs in FS tasks. The study emphasizes the critical role of
incorporating domain-specific insights, illustrating ICE-SEARCH's robustness,
generalizability, and swift convergence. This opens avenues for further
research into comprehensive and intricate FS landscapes, marking a significant
stride in the application of artificial intelligence in medical predictive
analytics.
\\ ( https://arxiv.org/abs/2402.18609 ,  3903kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18610
Date: Wed, 28 Feb 2024 15:10:25 GMT   (2067kb,D)

Title: Why Attention Graphs Are All We Need: Pioneering Hierarchical
  Classification of Hematologic Cell Populations with LeukoGraph
Authors: Fatemeh Nassajian Mojarrad, Lorenzo Bini, Thomas Matthes, St\'ephane
  Marchand-Maillet
Categories: cs.LG q-bio.CB
\\
  In the complex landscape of hematologic samples such as peripheral blood or
bone marrow, cell classification, delineating diverse populations into a
hierarchical structure, presents profound challenges. This study presents
LeukoGraph, a recently developed framework designed explicitly for this purpose
employing graph attention networks (GATs) to navigate hierarchical
classification (HC) complexities. Notably, LeukoGraph stands as a pioneering
effort, marking the application of graph neural networks (GNNs) for
hierarchical inference on graphs, accommodating up to one million nodes and
millions of edges, all derived from flow cytometry data. LeukoGraph intricately
addresses a classification paradigm where for example four different cell
populations undergo flat categorization, while a fifth diverges into two
distinct child branches, exemplifying the nuanced hierarchical structure
inherent in complex datasets. The technique is more general than this example.
A hallmark achievement of LeukoGraph is its F-score of 98%, significantly
outclassing prevailing state-of-the-art methodologies. Crucially, LeukoGraph's
prowess extends beyond theoretical innovation, showcasing remarkable precision
in predicting both flat and hierarchical cell types across flow cytometry
datasets from 30 distinct patients. This precision is further underscored by
LeukoGraph's ability to maintain a correct label ratio, despite the inherent
challenges posed by hierarchical classifications.
\\ ( https://arxiv.org/abs/2402.18610 ,  2067kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18614
Date: Wed, 28 Feb 2024 15:52:30 GMT   (1229kb,D)

Title: Deep Neural Network Models Trained With A Fixed Random Classifier
  Transfer Better Across Domains
Authors: Hafiz Tiomoko Ali, Umberto Michieli, Ji Joong Moon, Daehyun Kim, Mete
  Ozay
Categories: cs.LG cs.CV cs.NE
Comments: ICASSP 2024. Copyright 2024 IEEE. Personal use of this material is
  permitted. Permission from IEEE must be obtained for all other uses, in any
  current or future media, including reprinting/republishing this material for
  advertising or promotional purposes, creating new collective works, for
  resale or redistribution to servers or lists, or reuse of any copyrighted
  component of this work in other
\\
  The recently discovered Neural collapse (NC) phenomenon states that the
last-layer weights of Deep Neural Networks (DNN), converge to the so-called
Equiangular Tight Frame (ETF) simplex, at the terminal phase of their training.
This ETF geometry is equivalent to vanishing within-class variability of the
last layer activations. Inspired by NC properties, we explore in this paper the
transferability of DNN models trained with their last layer weight fixed
according to ETF. This enforces class separation by eliminating class
covariance information, effectively providing implicit regularization. We show
that DNN models trained with such a fixed classifier significantly improve
transfer performance, particularly on out-of-domain datasets. On a broad range
of fine-grained image classification datasets, our approach outperforms i)
baseline methods that do not perform any covariance regularization (up to 22%),
as well as ii) methods that explicitly whiten covariance of activations
throughout training (up to 19%). Our findings suggest that DNNs trained with
fixed ETF classifiers offer a powerful mechanism for improving transfer
learning across domains.
\\ ( https://arxiv.org/abs/2402.18614 ,  1229kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18630
Date: Wed, 28 Feb 2024 19:00:01 GMT   (2869kb,D)

Title: GNSS Positioning using Cost Function Regulated Multilateration and Graph
  Neural Networks
Authors: Amir Jalalirad, Davide Belli, Bence Major, Songwon Jee, Himanshu Shah,
  Will Morrison
Categories: cs.LG eess.SP
Comments: Published in The Proceedings of the Institute of Navigation GNSS+
  2023
DOI: 10.33012/2023.19364
\\
  In urban environments, where line-of-sight signals from GNSS satellites are
frequently blocked by high-rise objects, GNSS receivers are subject to large
errors in measuring satellite ranges. Heuristic methods are commonly used to
estimate these errors and reduce the impact of noisy measurements on
localization accuracy. In our work, we replace these error estimation
heuristics with a deep learning model based on Graph Neural Networks.
Additionally, by analyzing the cost function of the multilateration process, we
derive an optimal method to utilize the estimated errors. Our approach
guarantees that the multilateration converges to the receiver's location as the
error estimation accuracy increases. We evaluate our solution on a real-world
dataset containing more than 100k GNSS epochs, collected from multiple cities
with diverse characteristics. The empirical results show improvements from 40%
to 80% in the horizontal localization error against recent deep learning
baselines as well as classical localization approaches.
\\ ( https://arxiv.org/abs/2402.18630 ,  2869kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18651
Date: Wed, 28 Feb 2024 19:00:36 GMT   (2142kb,D)

Title: Quantifying Human Priors over Social and Navigation Networks
Authors: Gecia Bravo-Hermsdorff
Categories: cs.LG cs.AI cs.SI physics.soc-ph q-bio.NC stat.ME
Comments: Published on Proceedings of the 40th International Conference on
  Machine Learning (ICML), PMLR 202:3063-3105, 2023
\\
  Human knowledge is largely implicit and relational -- do we have a friend in
common? can I walk from here to there? In this work, we leverage the
combinatorial structure of graphs to quantify human priors over such relational
data. Our experiments focus on two domains that have been continuously relevant
over evolutionary timescales: social interaction and spatial navigation. We
find that some features of the inferred priors are remarkably consistent, such
as the tendency for sparsity as a function of graph size. Other features are
domain-specific, such as the propensity for triadic closure in social
interactions. More broadly, our work demonstrates how nonclassical statistical
analysis of indirect behavioral experiments can be used to efficiently model
latent biases in the data.
\\ ( https://arxiv.org/abs/2402.18651 ,  2142kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18689
Date: Wed, 28 Feb 2024 20:09:14 GMT   (191kb,D)

Title: The VOROS: Lifting ROC curves to 3D
Authors: Christopher Ratigan and Lenore Cowen
Categories: cs.LG math.MG math.ST stat.ME stat.TH
Comments: 38 pages, 19 figures
MSC-class: 62H30 (Primary) 68Q32, 68U05, 68P01 (Secondary)
ACM-class: I.2.6
\\
  The area under the ROC curve is a common measure that is often used to rank
the relative performance of different binary classifiers. However, as has been
also previously noted, it can be a measure that ill-captures the benefits of
different classifiers when either the true class values or misclassification
costs are highly unbalanced between the two classes. We introduce a third
dimension to capture these costs, and lift the ROC curve to a ROC surface in a
natural way. We study both this surface and introduce the VOROS, the volume
over this ROC surface, as a 3D generalization of the 2D area under the ROC
curve. For problems where there are only bounds on the expected costs or class
imbalances, we restrict consideration to the volume of the appropriate
subregion of the ROC surface. We show how the VOROS can better capture the
costs of different classifiers on both a classical and a modern example
dataset.
\\ ( https://arxiv.org/abs/2402.18689 ,  191kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18724
Date: Wed, 28 Feb 2024 21:47:30 GMT   (8224kb,D)

Title: Learning Associative Memories with Gradient Descent
Authors: Vivien Cabannes, Berfin Simsek, Alberto Bietti
Categories: cs.LG cs.AI stat.ML
\\
  This work focuses on the training dynamics of one associative memory module
storing outer products of token embeddings. We reduce this problem to the study
of a system of particles, which interact according to properties of the data
distribution and correlations between embeddings. Through theory and
experiments, we provide several insights. In overparameterized regimes, we
obtain logarithmic growth of the ``classification margins.'' Yet, we show that
imbalance in token frequencies and memory interferences due to correlated
embeddings lead to oscillatory transitory regimes. The oscillations are more
pronounced with large step sizes, which can create benign loss spikes, although
these learning rates speed up the dynamics and accelerate the asymptotic
convergence. In underparameterized regimes, we illustrate how the cross-entropy
loss can lead to suboptimal memorization schemes. Finally, we assess the
validity of our findings on small Transformer models.
\\ ( https://arxiv.org/abs/2402.18724 ,  8224kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18726
Date: Wed, 28 Feb 2024 22:02:10 GMT   (2516kb,D)

Title: Unveiling Privacy, Memorization, and Input Curvature Links
Authors: Deepak Ravikumar, Efstathia Soufleri, Abolfazl Hashemi, Kaushik Roy
Categories: cs.LG cs.AI cs.CR
\\
  Deep Neural Nets (DNNs) have become a pervasive tool for solving many
emerging problems. However, they tend to overfit to and memorize the training
set. Memorization is of keen interest since it is closely related to several
concepts such as generalization, noisy learning, and privacy. To study
memorization, Feldman (2019) proposed a formal score, however its computational
requirements limit its practical use. Recent research has shown empirical
evidence linking input loss curvature (measured by the trace of the loss
Hessian w.r.t inputs) and memorization. It was shown to be ~3 orders of
magnitude more efficient than calculating the memorization score. However,
there is a lack of theoretical understanding linking memorization with input
loss curvature. In this paper, we not only investigate this connection but also
extend our analysis to establish theoretical links between differential
privacy, memorization, and input loss curvature. First, we derive an upper
bound on memorization characterized by both differential privacy and input loss
curvature. Second, we present a novel insight showing that input loss curvature
is upper-bounded by the differential privacy parameter. Our theoretical
findings are further empirically validated using deep models on CIFAR and
ImageNet datasets, showing a strong correlation between our theoretical
predictions and results observed in practice.
\\ ( https://arxiv.org/abs/2402.18726 ,  2516kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18734
Date: Wed, 28 Feb 2024 22:27:49 GMT   (1116kb,D)

Title: Priority Sampling of Large Language Models for Compilers
Authors: Dejan Grubisic, Chris Cummins, Volker Seeker, Hugh Leather
Categories: cs.LG cs.CL cs.PF
\\
  Large language models show great potential in generating and optimizing code.
Widely used sampling methods such as Nucleus Sampling increase the diversity of
generation but often produce repeated samples for low temperatures and
incoherent samples for high temperatures. Furthermore, the temperature
coefficient has to be tuned for each task, limiting its usability. We present
Priority Sampling, a simple and deterministic sampling technique that produces
unique samples ordered by the model's confidence. Each new sample expands the
unexpanded token with the highest probability in the augmented search tree.
Additionally, Priority Sampling supports generation based on regular expression
that provides a controllable and structured exploration process. Priority
Sampling outperforms Nucleus Sampling for any number of samples, boosting the
performance of the original model from 2.87% to 5% improvement over -Oz.
Moreover, it outperforms the autotuner used for the generation of labels for
the training of the original model in just 30 samples.
\\ ( https://arxiv.org/abs/2402.18734 ,  1116kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18751
Date: Wed, 28 Feb 2024 23:18:15 GMT   (1906kb,D)

Title: Multi-Sensor and Multi-temporal High-Throughput Phenotyping for
  Monitoring and Early Detection of Water-Limiting Stress in Soybean
Authors: Sarah E. Jones, Timilehin Ayanlade, Benjamin Fallen, Talukder Z.
  Jubery, Arti Singh, Baskar Ganapathysubramanian, Soumik Sarkar, and Asheesh
  K. Singh
Categories: cs.LG cs.CV
Comments: 25 pages, 5 figures
\\
  Soybean production is susceptible to biotic and abiotic stresses, exacerbated
by extreme weather events. Water limiting stress, i.e. drought, emerges as a
significant risk for soybean production, underscoring the need for advancements
in stress monitoring for crop breeding and production. This project combines
multi-modal information to identify the most effective and efficient automated
methods to investigate drought response. We investigated a set of diverse
soybean accessions using multiple sensors in a time series high-throughput
phenotyping manner to: (1) develop a pipeline for rapid classification of
soybean drought stress symptoms, and (2) investigate methods for early
detection of drought stress. We utilized high-throughput time-series
phenotyping using UAVs and sensors in conjunction with machine learning (ML)
analytics, which offered a swift and efficient means of phenotyping. The
red-edge and green bands were most effective to classify canopy wilting stress.
The Red-Edge Chlorophyll Vegetation Index (RECI) successfully differentiated
susceptible and tolerant soybean accessions prior to visual symptom
development. We report pre-visual detection of soybean wilting using a
combination of different vegetation indices. These results can contribute to
early stress detection methodologies and rapid classification of drought
responses in screening nurseries for breeding and production applications.
\\ ( https://arxiv.org/abs/2402.18751 ,  1906kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18752
Date: Wed, 28 Feb 2024 23:26:27 GMT   (8869kb,D)

Title: Pre-training Differentially Private Models with Limited Public Data
Authors: Zhiqi Bu, Xinwei Zhang, Mingyi Hong, Sheng Zha, George Karypis
Categories: cs.LG cs.CR
\\
  The superior performance of large foundation models relies on the use of
massive amounts of high-quality data, which often contain sensitive, private
and copyrighted material that requires formal protection. While differential
privacy (DP) is a prominent method to gauge the degree of security provided to
the models, its application is commonly limited to the model fine-tuning stage,
due to the performance degradation when applying DP during the pre-training
stage. Consequently, DP is yet not capable of protecting a substantial portion
of the data used during the initial pre-training process.
  In this work, we first provide a theoretical understanding of the efficacy of
DP training by analyzing the per-iteration loss improvement. We make a key
observation that DP optimizers' performance degradation can be significantly
mitigated by the use of limited public data, which leads to a novel DP
continual pre-training strategy. Empirically, using only 10\% of public data,
our strategy can achieve DP accuracy of 41.5\% on ImageNet-21k (with
$\epsilon=8$), as well as non-DP accuracy of 55.7\% and and 60.0\% on
downstream tasks Places365 and iNaturalist-2021, respectively, on par with
state-of-the-art standard pre-training and substantially outperforming existing
DP pre-trained models.
\\ ( https://arxiv.org/abs/2402.18752 ,  8869kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18762
Date: Thu, 29 Feb 2024 00:02:33 GMT   (5351kb,D)

Title: Disentangling the Causes of Plasticity Loss in Neural Networks
Authors: Clare Lyle, Zeyu Zheng, Khimya Khetarpal, Hado van Hasselt, Razvan
  Pascanu, James Martens, Will Dabney
Categories: cs.LG
\\
  Underpinning the past decades of work on the design, initialization, and
optimization of neural networks is a seemingly innocuous assumption: that the
network is trained on a \textit{stationary} data distribution. In settings
where this assumption is violated, e.g.\ deep reinforcement learning, learning
algorithms become unstable and brittle with respect to hyperparameters and even
random seeds. One factor driving this instability is the loss of plasticity,
meaning that updating the network's predictions in response to new information
becomes more difficult as training progresses. While many recent works provide
analyses and partial solutions to this phenomenon, a fundamental question
remains unanswered: to what extent do known mechanisms of plasticity loss
overlap, and how can mitigation strategies be combined to best maintain the
trainability of a network? This paper addresses these questions, showing that
loss of plasticity can be decomposed into multiple independent mechanisms and
that, while intervening on any single mechanism is insufficient to avoid the
loss of plasticity in all cases, intervening on multiple mechanisms in
conjunction results in highly robust learning algorithms. We show that a
combination of layer normalization and weight decay is highly effective at
maintaining plasticity in a variety of synthetic nonstationary learning tasks,
and further demonstrate its effectiveness on naturally arising
nonstationarities, including reinforcement learning in the Arcade Learning
Environment.
\\ ( https://arxiv.org/abs/2402.18762 ,  5351kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18787
Date: Thu, 29 Feb 2024 01:27:38 GMT   (1015kb,D)

Title: Enhancing the "Immunity" of Mixture-of-Experts Networks for Adversarial
  Defense
Authors: Qiao Han, yong huang, xinling Guo, Yiteng Zhai, Yu Qin and Yao Yang
Categories: cs.LG cs.CR
\\
  Recent studies have revealed the vulnerability of Deep Neural Networks (DNNs)
to adversarial examples, which can easily fool DNNs into making incorrect
predictions. To mitigate this deficiency, we propose a novel adversarial
defense method called "Immunity" (Innovative MoE with MUtual information \&
positioN stabilITY) based on a modified Mixture-of-Experts (MoE) architecture
in this work. The key enhancements to the standard MoE are two-fold: 1)
integrating of Random Switch Gates (RSGs) to obtain diverse network structures
via random permutation of RSG parameters at evaluation time, despite of RSGs
being determined after one-time training; 2) devising innovative Mutual
Information (MI)-based and Position Stability-based loss functions by
capitalizing on Grad-CAM's explanatory power to increase the diversity and the
causality of expert networks. Notably, our MI-based loss operates directly on
the heatmaps, thereby inducing subtler negative impacts on the classification
performance when compared to other losses of the same type, theoretically.
Extensive evaluation validates the efficacy of the proposed approach in
improving adversarial robustness against a wide range of attacks.
\\ ( https://arxiv.org/abs/2402.18787 ,  1015kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18792
Date: Thu, 29 Feb 2024 01:49:18 GMT   (4880kb,D)

Title: MPAT: Building Robust Deep Neural Networks against Textual Adversarial
  Attacks
Authors: Fangyuan Zhang, Huichi Zhou, Shuangjiao Li, Hongtao Wang
Categories: cs.LG cs.CL cs.CR
\\
  Deep neural networks have been proven to be vulnerable to adversarial
examples and various methods have been proposed to defend against adversarial
attacks for natural language processing tasks. However, previous defense
methods have limitations in maintaining effective defense while ensuring the
performance of the original task. In this paper, we propose a malicious
perturbation based adversarial training method (MPAT) for building robust deep
neural networks against textual adversarial attacks. Specifically, we construct
a multi-level malicious example generation strategy to generate adversarial
examples with malicious perturbations, which are used instead of original
inputs for model training. Additionally, we employ a novel training objective
function to ensure achieving the defense goal without compromising the
performance on the original task. We conduct comprehensive experiments to
evaluate our defense method by attacking five victim models on three benchmark
datasets. The result demonstrates that our method is more effective against
malicious adversarial attacks compared with previous defense methods while
maintaining or further improving the performance on the original task.
\\ ( https://arxiv.org/abs/2402.18792 ,  4880kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18800
Date: Thu, 29 Feb 2024 02:13:10 GMT   (1405kb,D)

Title: BlockEcho: Retaining Long-Range Dependencies for Imputing Block-Wise
  Missing Data
Authors: Qiao Han, Mingqian Li, Yao Yang and Yiteng Zhai
Categories: cs.LG stat.ML
\\
  Block-wise missing data poses significant challenges in real-world data
imputation tasks. Compared to scattered missing data, block-wise gaps
exacerbate adverse effects on subsequent analytic and machine learning tasks,
as the lack of local neighboring elements significantly reduces the
interpolation capability and predictive power. However, this issue has not
received adequate attention. Most SOTA matrix completion methods appeared less
effective, primarily due to overreliance on neighboring elements for
predictions. We systematically analyze the issue and propose a novel matrix
completion method ``BlockEcho" for a more comprehensive solution. This method
creatively integrates Matrix Factorization (MF) within Generative Adversarial
Networks (GAN) to explicitly retain long-distance inter-element relationships
in the original matrix. Besides, we incorporate an additional discriminator for
GAN, comparing the generator's intermediate progress with pre-trained MF
results to constrain high-order feature distributions. Subsequently, we
evaluate BlockEcho on public datasets across three domains. Results demonstrate
superior performance over both traditional and SOTA methods when imputing
block-wise missing data, especially at higher missing rates. The advantage also
holds for scattered missing data at high missing rates. We also contribute on
the analyses in providing theoretical justification on the optimality and
convergence of fusing MF and GAN for missing block data.
\\ ( https://arxiv.org/abs/2402.18800 ,  1405kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18803
Date: Thu, 29 Feb 2024 02:16:57 GMT   (4368kb,D)

Title: To Pool or Not To Pool: Analyzing the Regularizing Effects of Group-Fair
  Training on Shared Models
Authors: Cyrus Cousins, I. Elizabeth Kumar, Suresh Venkatasubramanian
Categories: cs.LG cs.CY
\\
  In fair machine learning, one source of performance disparities between
groups is over-fitting to groups with relatively few training samples. We
derive group-specific bounds on the generalization error of welfare-centric
fair machine learning that benefit from the larger sample size of the majority
group. We do this by considering group-specific Rademacher averages over a
restricted hypothesis class, which contains the family of models likely to
perform well with respect to a fair learning objective (e.g., a power-mean).
Our simulations demonstrate these bounds improve over a naive method, as
expected by theory, with particularly significant improvement for smaller group
sizes.
\\ ( https://arxiv.org/abs/2402.18803 ,  4368kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18819
Date: Thu, 29 Feb 2024 03:06:10 GMT   (4527kb,D)

Title: Dual Operating Modes of In-Context Learning
Authors: Ziqian Lin, Kangwook Lee
Categories: cs.LG
Comments: 53 pages, 20 figures
\\
  In-context learning (ICL) exhibits dual operating modes: task learning, i.e.,
acquiring a new skill from in-context samples, and task retrieval, i.e.,
locating and activating a relevant pretrained skill. Recent theoretical work
investigates various mathematical models to analyze ICL, but existing models
explain only one operating mode at a time. We introduce a probabilistic model,
with which one can explain the dual operating modes of ICL simultaneously.
Focusing on in-context learning of linear functions, we extend existing models
for pretraining data by introducing multiple task groups and task-dependent
input distributions. We then analyze the behavior of the optimally pretrained
model under the squared loss, i.e., the MMSE estimator of the label given
in-context examples. Regarding pretraining task distribution as prior and
in-context examples as the observation, we derive the closed-form expression of
the task posterior distribution. With the closed-form expression, we obtain a
quantitative understanding of the two operating modes of ICL. Furthermore, we
shed light on an unexplained phenomenon observed in practice: under certain
settings, the ICL risk initially increases and then decreases with more
in-context examples. Our model offers a plausible explanation for this "early
ascent" phenomenon: a limited number of in-context samples may lead to the
retrieval of an incorrect skill, thereby increasing the risk, which will
eventually diminish as task learning takes effect with more in-context samples.
We also theoretically analyze ICL with biased labels, e.g., zero-shot ICL,
where in-context examples are assigned random labels. Lastly, we validate our
findings and predictions via experiments involving Transformers and large
language models.
\\ ( https://arxiv.org/abs/2402.18819 ,  4527kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18824
Date: Thu, 29 Feb 2024 03:16:47 GMT   (2253kb,D)

Title: Batch size invariant Adam
Authors: Xi Wang, Laurence Aitchison
Categories: cs.LG
\\
  We propose a batch size invariant version of Adam, for use in large-scale,
distributed settings, in which the mini-batch is divided into micro-batches
which are distributed among worker nodes. For the v term, standard Adam first
computes the average over micro-batch gradients, then squares, while in the
batch size invariant Adam proposed here, we first square the micro-batch
gradients, then average. Previous work (e.g. Malladi et al. 2022) used an
alternative approach that involved a square-root scaling of the learning rate,
but this approach requires strong assumptions to work; in particular that the
gradient variance dominates the square of the expected gradient. In contrast,
the approach proposed here gives batch size invariance without this assumption.
We confirm that in practice our scheme gives batch size invariance in a much
larger range of scenarios than the previous approach.
\\ ( https://arxiv.org/abs/2402.18824 ,  2253kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18836
Date: Thu, 29 Feb 2024 03:53:02 GMT   (782kb)

Title: A Model-Based Approach for Improving Reinforcement Learning Efficiency
  Leveraging Expert Observations
Authors: Erhan Can Ozcan, Vittorio Giammarino, James Queeney, Ioannis Ch.
  Paschalidis
Categories: cs.LG
\\
  This paper investigates how to incorporate expert observations (without
explicit information on expert actions) into a deep reinforcement learning
setting to improve sample efficiency. First, we formulate an augmented policy
loss combining a maximum entropy reinforcement learning objective with a
behavioral cloning loss that leverages a forward dynamics model. Then, we
propose an algorithm that automatically adjusts the weights of each component
in the augmented loss function. Experiments on a variety of continuous control
tasks demonstrate that the proposed algorithm outperforms various benchmarks by
effectively utilizing available expert observations.
\\ ( https://arxiv.org/abs/2402.18836 ,  782kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18839
Date: Thu, 29 Feb 2024 04:12:32 GMT   (13468kb,D)

Title: Extended Flow Matching: a Method of Conditional Generation with
  Generalized Continuity Equation
Authors: Noboru Isobe, Masanori Koyama, Kohei Hayashi and Kenji Fukumizu
Categories: cs.LG math.AP math.FA math.OC math.PR
Comments: 15 pages, 4 figures
MSC-class: 68T07 (Primary), 49Q22 (Secondary)
\\
  The task of conditional generation is one of the most important applications
of generative models, and numerous methods have been developed to date based on
the celebrated diffusion models, with the guidance-based classifier-free method
taking the lead. However, the theory of the guidance-based method not only
requires the user to fine-tune the "guidance strength," but its target vector
field does not necessarily correspond to the conditional distribution used in
training. In this paper, we develop the theory of conditional generation based
on Flow Matching, a current strong contender of diffusion methods. Motivated by
the interpretation of a probability path as a distribution on path space, we
establish a novel theory of flow-based generation of conditional distribution
by employing the mathematical framework of generalized continuity equation
instead of the continuity equation in flow matching. This theory naturally
derives a method that aims to match the matrix field as opposed to the vector
field. Our framework ensures the continuity of the generated conditional
distribution through the existence of flow between conditional distributions.
We will present our theory through experiments and mathematical results.
\\ ( https://arxiv.org/abs/2402.18839 ,  13468kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18846
Date: Thu, 29 Feb 2024 04:40:25 GMT   (401kb,D)

Title: Multi-Fidelity Residual Neural Processes for Scalable Surrogate Modeling
Authors: Ruijia Niu, Dongxia Wu, Kai Kim, Yi-An Ma, Duncan Watson-Parris, Rose
  Yu
Categories: cs.LG
Comments: A novel probabilistic inference approach for scalable multi-fidelity
  surrogate modeling
\\
  Multi-fidelity surrogate modeling aims to learn an accurate surrogate at the
highest fidelity level by combining data from multiple sources. Traditional
methods relying on Gaussian processes can hardly scale to high-dimensional
data. Deep learning approaches utilize neural network based encoders and
decoders to improve scalability. These approaches share encoded representations
across fidelities without including corresponding decoder parameters. At the
highest fidelity, the representations are decoded with different parameters,
making the shared information inherently inaccurate. This hinders inference
performance, especially in out-of-distribution scenarios when the highest
fidelity data has limited domain coverage. To address these limitations, we
propose Multi-fidelity Residual Neural Processes (MFRNP), a novel
multi-fidelity surrogate modeling framework. MFRNP optimizes lower fidelity
decoders for accurate information sharing by aggregating lower fidelity
surrogate outputs and models residual between the aggregation and ground truth
on the highest fidelity. We show that MFRNP significantly outperforms current
state-of-the-art in learning partial differential equations and a real-world
climate modeling task.
\\ ( https://arxiv.org/abs/2402.18846 ,  401kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18851
Date: Thu, 29 Feb 2024 05:00:01 GMT   (7228kb,D)

Title: Applications of 0-1 Neural Networks in Prescription and Prediction
Authors: Vrishabh Patil, Kara Hoppe, Yonatan Mintz
Categories: cs.LG cs.AI math.OC stat.ML
\\
  A key challenge in medical decision making is learning treatment policies for
patients with limited observational data. This challenge is particularly
evident in personalized healthcare decision-making, where models need to take
into account the intricate relationships between patient characteristics,
treatment options, and health outcomes. To address this, we introduce
prescriptive networks (PNNs), shallow 0-1 neural networks trained with mixed
integer programming that can be used with counterfactual estimation to optimize
policies in medium data settings. These models offer greater interpretability
than deep neural networks and can encode more complex policies than common
models such as decision trees. We show that PNNs can outperform existing
methods in both synthetic data experiments and in a case study of assigning
treatments for postpartum hypertension. In particular, PNNs are shown to
produce policies that could reduce peak blood pressure by 5.47 mm Hg (p=0.02)
over existing clinical practice, and by 2 mm Hg (p=0.01) over the next best
prescriptive modeling technique. Moreover PNNs were more likely than all other
models to correctly identify clinically significant features while existing
models relied on potentially dangerous features such as patient insurance
information and race that could lead to bias in treatment.
\\ ( https://arxiv.org/abs/2402.18851 ,  7228kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18853
Date: Thu, 29 Feb 2024 05:00:30 GMT   (14476kb,D)

Title: Rethinking Multi-domain Generalization with A General Learning Objective
Authors: Zhaorui Tan, Xi Yang, Kaizhu Huang
Categories: cs.LG cs.AI cs.CV
Comments: Accepted by CVPR24
\\
  Multi-domain generalization (mDG) is universally aimed to minimize the
discrepancy between training and testing distributions to enhance
marginal-to-label distribution mapping. However, existing mDG literature lacks
a general learning objective paradigm and often imposes constraints on static
target marginal distributions. In this paper, we propose to leverage a
$Y$-mapping to relax the constraint. We rethink the learning objective for mDG
and design a new \textbf{general learning objective} to interpret and analyze
most existing mDG wisdom. This general objective is bifurcated into two
synergistic amis: learning domain-independent conditional features and
maximizing a posterior. Explorations also extend to two effective
regularization terms that incorporate prior information and suppress invalid
causality, alleviating the issues that come with relaxed constraints. We
theoretically contribute an upper bound for the domain alignment of
domain-independent conditional features, disclosing that many previous mDG
endeavors actually \textbf{optimize partially the objective} and thus lead to
limited performance. As such, our study distills a general learning objective
into four practical components, providing a general, robust, and flexible
mechanism to handle complex domain shifts. Extensive empirical results indicate
that the proposed objective with $Y$-mapping leads to substantially better mDG
performance in various downstream tasks, including regression, segmentation,
and classification.
\\ ( https://arxiv.org/abs/2402.18853 ,  14476kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18859
Date: Thu, 29 Feb 2024 05:17:36 GMT   (12530kb,D)

Title: Taking Second-life Batteries from Exhausted to Empowered using
  Experiments, Data Analysis, and Health Estimation
Authors: Xiaofan Cui, Muhammad Aadil Khan, Gabriele Pozzato, Surinder Singh,
  Ratnesh Sharma, Simona Onori
Categories: cs.LG cs.SY eess.SY
Comments: 31 pages, 18 figures
\\
  The reuse of retired electric vehicle (EV) batteries in electric grid energy
storage emerges as a promising strategy to address environmental concerns and
boost economic value. This study concentrates on devising health monitoring
algorithms for retired batteries (BMS$_2$) deployed in grid storage
applications. Over 15 months of testing, we compile, analyze, and publicly
share a dataset of second-life (SL) batteries, implementing a cycling protocol
simulating grid energy storage load profiles within a 3 V-4 V voltage window.
Four machine learning-based health estimation models, relying on BMS$_2$
features and initial capacity, are developed and compared, with the selected
model achieving a Mean Absolute Percentage Error (MAPE) below 2.3% on test
data. Additionally, an adaptive online health estimation algorithm is proposed
by integrating a clustering-based method, limiting estimation errors during
online deployment. These results constitute an initial proof of concept,
showcasing the feasibility of repurposing retired batteries for second-life
applications. Based on obtained data and representative power demand, these SL
batteries exhibit the potential, under specific conditions, for over a decade
of grid energy storage use.
\\ ( https://arxiv.org/abs/2402.18859 ,  12530kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18863
Date: Thu, 29 Feb 2024 05:25:23 GMT   (237kb,D)

Title: Probabilistic Lipschitzness and the Stable Rank for Comparing
  Explanation Models
Authors: Lachlan Simpson, Kyle Millar, Adriel Cheng, Cheng-Chew Lim, Hong Gunn
  Chew
Categories: cs.LG
\\
  Explainability models are now prevalent within machine learning to address
the black-box nature of neural networks. The question now is which
explainability model is most effective. Probabilistic Lipschitzness has
demonstrated that the smoothness of a neural network is fundamentally linked to
the quality of post hoc explanations. In this work, we prove theoretical lower
bounds on the probabilistic Lipschitzness of Integrated Gradients, LIME and
SmoothGrad. We propose a novel metric using probabilistic Lipschitzness,
normalised astuteness, to compare the robustness of explainability models.
Further, we prove a link between the local Lipschitz constant of a neural
network and its stable rank. We then demonstrate that the stable rank of a
neural network provides a heuristic for the robustness of explainability
models.
\\ ( https://arxiv.org/abs/2402.18863 ,  237kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18865
Date: Thu, 29 Feb 2024 05:27:45 GMT   (7984kb,D)

Title: Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient
  Tuning
Authors: Weijieying Ren, Xinlong Li, Lei Wang, Tianxiang Zhao, Wei Qin
Categories: cs.LG cs.AI cs.CL
\\
  Existing research has shown that large language models (LLMs) exhibit
remarkable performance in language understanding and generation. However, when
LLMs are continuously fine-tuned on complex and diverse domain-specific
downstream tasks, the inference performance on historical tasks decreases
dramatically, which is known as a catastrophic forgetting problem. A trade-off
needs to be kept between learning plasticity and memory stability. Plenty of
existing works have explored strategies like memory replay, regularization and
parameter isolation, but little is known about the geometric connection of
various adjacent minima in the continual LLMs fine-tuning scenarios. In this
work, we investigate the geometric connections of different minima through the
lens of mode connectivity, which means different minima can be connected by a
low-loss valley. Through extensive experiments, we uncover the mode
connectivity phenomenon in the LLMs continual learning scenario and find that
it can strike a balance between plasticity and stability. Building upon these
findings, we propose a simple yet effective method called Interpolation-based
LoRA (I-LoRA), which constructs a dual-memory experience replay framework based
on LoRA parameter interpolations. Extensive experiments and analysis on eight
domain-specific CL benchmarks demonstrate that I-LoRA consistently show
significant improvement over the previous state-of-the-art approaches with up
to $11\%$ performance gains, providing a strong baseline and insights for
future research on the large language model continual learning problem. Our
code is available at \url{https://github.com/which47/LLMCL}.
\\ ( https://arxiv.org/abs/2402.18865 ,  7984kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18866
Date: Thu, 29 Feb 2024 05:34:05 GMT   (12191kb,D)

Title: Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming
Authors: Hany Hamed, Subin Kim, Dongyeong Kim, Jaesik Yoon, Sungjin Ahn
Categories: cs.LG
Comments: First two authors contributed equally
\\
  Model-based reinforcement learning (MBRL) has been a primary approach to
ameliorating the sample efficiency issue as well as to make a generalist agent.
However, there has not been much effort toward enhancing the strategy of
dreaming itself. Therefore, it is a question whether and how an agent can
"dream better" in a more structured and strategic way. In this paper, inspired
by the observation from cognitive science suggesting that humans use a spatial
divide-and-conquer strategy in planning, we propose a new MBRL agent, called
Dr. Strategy, which is equipped with a novel Dreaming Strategy. The proposed
agent realizes a version of divide-and-conquer-like strategy in dreaming. This
is achieved by learning a set of latent landmarks and then utilizing these to
learn a landmark-conditioned highway policy. With the highway policy, the agent
can first learn in the dream to move to a landmark, and from there it tackles
the exploration and achievement task in a more focused way. In experiments, we
show that the proposed model outperforms prior pixel-based MBRL methods in
various visually complex and partially observable navigation tasks. The source
code will be available at https://github.com/ahn-ml/drstrategy
\\ ( https://arxiv.org/abs/2402.18866 ,  12191kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18875
Date: Thu, 29 Feb 2024 05:44:41 GMT   (25kb,D)

Title: Loss-aware Curriculum Learning for Heterogeneous Graph Neural Networks
Authors: Zhen Hao Wong, Hansi Yang, Xiaoyi Fu, Quanming Yao
Categories: cs.LG
\\
  Heterogeneous Graph Neural Networks (HGNNs) are a class of deep learning
models designed specifically for heterogeneous graphs, which are graphs that
contain different types of nodes and edges. This paper investigates the
application of curriculum learning techniques to improve the performance and
robustness of Heterogeneous Graph Neural Networks (GNNs). To better classify
the quality of the data, we design a loss-aware training schedule, named LTS
that measures the quality of every nodes of the data and incorporate the
training dataset into the model in a progressive manner that increases
difficulty step by step. LTS can be seamlessly integrated into various
frameworks, effectively reducing bias and variance, mitigating the impact of
noisy data, and enhancing overall accuracy. Our findings demonstrate the
efficacy of curriculum learning in enhancing HGNNs capabilities for analyzing
complex graph-structured data. The code is public at https:
//github.com/LARS-research/CLGNN/.
\\ ( https://arxiv.org/abs/2402.18875 ,  25kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18884
Date: Thu, 29 Feb 2024 06:02:45 GMT   (94kb)

Title: Supervised Contrastive Representation Learning: Landscape Analysis with
  Unconstrained Features
Authors: Tina Behnia, Christos Thrampoulidis
Categories: cs.LG stat.ML
Comments: 10 pages
\\
  Recent findings reveal that over-parameterized deep neural networks, trained
beyond zero training-error, exhibit a distinctive structural pattern at the
final layer, termed as Neural-collapse (NC). These results indicate that the
final hidden-layer outputs in such networks display minimal within-class
variations over the training set. While existing research extensively
investigates this phenomenon under cross-entropy loss, there are fewer studies
focusing on its contrastive counterpart, supervised contrastive (SC) loss.
Through the lens of NC, this paper employs an analytical approach to study the
solutions derived from optimizing the SC loss. We adopt the unconstrained
features model (UFM) as a representative proxy for unveiling NC-related
phenomena in sufficiently over-parameterized deep networks. We show that,
despite the non-convexity of SC loss minimization, all local minima are global
minima. Furthermore, the minimizer is unique (up to a rotation). We prove our
results by formalizing a tight convex relaxation of the UFM. Finally, through
this convex formulation, we delve deeper into characterizing the properties of
global solutions under label-imbalanced training data.
\\ ( https://arxiv.org/abs/2402.18884 ,  94kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18886
Date: Thu, 29 Feb 2024 06:11:21 GMT   (2391kb,D)

Title: BP-DeepONet: A new method for cuffless blood pressure estimation using
  the physcis-informed DeepONet
Authors: Lingfeng Li and Xue-Cheng Tai and Raymond Chan
Categories: cs.LG physics.med-ph
\\
  Cardiovascular diseases (CVDs) are the leading cause of death worldwide, with
blood pressure serving as a crucial indicator. Arterial blood pressure (ABP)
waveforms provide continuous pressure measurements throughout the cardiac cycle
and offer valuable diagnostic insights. Consequently, there is a significant
demand for non-invasive and cuff-less methods to measure ABP waveforms
continuously. Accurate prediction of ABP waveforms can also improve the
estimation of mean blood pressure, an essential cardiovascular health
characteristic.
  This study proposes a novel framework based on the physics-informed DeepONet
approach to predict ABP waveforms. Unlike previous methods, our approach
requires the predicted ABP waveforms to satisfy the Navier-Stokes equation with
a time-periodic condition and a Windkessel boundary condition. Notably, our
framework is the first to predict ABP waveforms continuously, both with
location and time, within the part of the artery that is being simulated.
Furthermore, our method only requires ground truth data at the outlet boundary
and can handle periodic conditions with varying periods. Incorporating the
Windkessel boundary condition in our solution allows for generating natural
physical reflection waves, which closely resemble measurements observed in
real-world cases. Moreover, accurately estimating the hyper-parameters in the
Navier-Stokes equation for our simulations poses a significant challenge. To
overcome this obstacle, we introduce the concept of meta-learning, enabling the
neural networks to learn these parameters during the training process.
\\ ( https://arxiv.org/abs/2402.18886 ,  2391kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18888
Date: Thu, 29 Feb 2024 06:13:10 GMT   (2473kb,D)

Title: Uncertainty-Based Extensible Codebook for Discrete Federated Learning in
  Heterogeneous Data Silos
Authors: Tianyi Zhang, Yu Cao, Dianbo Liu
Categories: cs.LG
\\
  Federated learning (FL), aimed at leveraging vast distributed datasets,
confronts a crucial challenge: the heterogeneity of data across different
silos. While previous studies have explored discrete representations to enhance
model generalization across minor distributional shifts, these approaches often
struggle to adapt to new data silos with significantly divergent distributions.
In response, we have identified that models derived from FL exhibit markedly
increased uncertainty when applied to data silos with unfamiliar distributions.
Consequently, we propose an innovative yet straightforward iterative framework,
termed Uncertainty-Based Extensible-Codebook Federated Learning (UEFL). This
framework dynamically maps latent features to trainable discrete vectors,
assesses the uncertainty, and specifically extends the discretization
dictionary or codebook for silos exhibiting high uncertainty. Our approach aims
to simultaneously enhance accuracy and reduce uncertainty by explicitly
addressing the diversity of data distributions, all while maintaining minimal
computational overhead in environments characterized by heterogeneous data
silos. Through experiments conducted on five datasets, our method has
demonstrated its superiority, achieving significant improvements in accuracy
(by 3%--22.1%) and uncertainty reduction (by 38.83%--96.24%), thereby
outperforming contemporary state-of-the-art methods. The source code is
available at https://github.com/destiny301/uefl.
\\ ( https://arxiv.org/abs/2402.18888 ,  2473kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18905
Date: Thu, 29 Feb 2024 07:01:48 GMT   (134kb,D)

Title: On the Convergence of Differentially-Private Fine-tuning: To Linearly
  Probe or to Fully Fine-tune?
Authors: Shuqi Ke, Charlie Hou, Giulia Fanti, Sewoong Oh
Categories: cs.LG cs.AI cs.CR math.OC
\\
  Differentially private (DP) machine learning pipelines typically involve a
two-phase process: non-private pre-training on a public dataset, followed by
fine-tuning on private data using DP optimization techniques. In the DP
setting, it has been observed that full fine-tuning may not always yield the
best test accuracy, even for in-distribution data. This paper (1) analyzes the
training dynamics of DP linear probing (LP) and full fine-tuning (FT), and (2)
explores the phenomenon of sequential fine-tuning, starting with linear probing
and transitioning to full fine-tuning (LP-FT), and its impact on test loss. We
provide theoretical insights into the convergence of DP fine-tuning within an
overparameterized neural network and establish a utility curve that determines
the allocation of privacy budget between linear probing and full fine-tuning.
The theoretical results are supported by empirical evaluations on various
benchmarks and models. The findings reveal the complex nature of DP fine-tuning
methods. These results contribute to a deeper understanding of DP machine
learning and highlight the importance of considering the allocation of privacy
budget in the fine-tuning process.
\\ ( https://arxiv.org/abs/2402.18905 ,  134kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18910
Date: Thu, 29 Feb 2024 07:09:01 GMT   (2114kb,D)

Title: DIGIC: Domain Generalizable Imitation Learning by Causal Discovery
Authors: Yang Chen, Yitao Liang, Zhouchen Lin
Categories: cs.LG cs.AI stat.ME
\\
  Causality has been combined with machine learning to produce robust
representations for domain generalization. Most existing methods of this type
require massive data from multiple domains to identify causal features by
cross-domain variations, which can be expensive or even infeasible and may lead
to misidentification in some cases. In this work, we make a different attempt
by leveraging the demonstration data distribution to discover the causal
features for a domain generalizable policy. We design a novel framework, called
DIGIC, to identify the causal features by finding the direct cause of the
expert action from the demonstration data distribution via causal discovery.
Our framework can achieve domain generalizable imitation learning with only
single-domain data and serve as a complement for cross-domain variation-based
methods under non-structural assumptions on the underlying causal models. Our
empirical study in various control tasks shows that the proposed framework
evidently improves the domain generalization performance and has comparable
performance to the expert in the original domain simultaneously.
\\ ( https://arxiv.org/abs/2402.18910 ,  2114kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18917
Date: Thu, 29 Feb 2024 07:17:04 GMT   (1365kb,D)

Title: Stop Relying on No-Choice and Do not Repeat the Moves: Optimal,
  Efficient and Practical Algorithms for Assortment Optimization
Authors: Aadirupa Saha, Pierre Gaillard
Categories: cs.LG cs.IR
\\
  We address the problem of active online assortment optimization problem with
preference feedback, which is a framework for modeling user choices and
subsetwise utility maximization. The framework is useful in various real-world
applications including ad placement, online retail, recommender systems,
fine-tuning language models, amongst many. The problem, although has been
studied in the past, lacks an intuitive and practical solution approach with
simultaneously efficient algorithm and optimal regret guarantee. E.g.,
popularly used assortment selection algorithms often require the presence of a
`strong reference' which is always included in the choice sets, further they
are also designed to offer the same assortments repeatedly until the reference
item gets selected -- all such requirements are quite unrealistic for practical
applications. In this paper, we designed efficient algorithms for the problem
of regret minimization in assortment selection with \emph{Plackett Luce} (PL)
based user choices. We designed a novel concentration guarantee for estimating
the score parameters of the PL model using `\emph{Pairwise Rank-Breaking}',
which builds the foundation of our proposed algorithms. Moreover, our methods
are practical, provably optimal, and devoid of the aforementioned limitations
of the existing methods. Empirical evaluations corroborate our findings and
outperform the existing baselines.
\\ ( https://arxiv.org/abs/2402.18917 ,  1365kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18946
Date: Thu, 29 Feb 2024 08:25:32 GMT   (2735kb,D)

Title: Real-Time Adaptive Safety-Critical Control with Gaussian Processes in
  High-Order Uncertain Models
Authors: Yu Zhang, Long Wen, Xiangtong Yao, Zhenshan Bing, Linghuan Kong, Wei
  He, and Alois Knoll
Categories: cs.LG cs.SY eess.SY
\\
  This paper presents an adaptive online learning framework for systems with
uncertain parameters to ensure safety-critical control in non-stationary
environments. Our approach consists of two phases. The initial phase is
centered on a novel sparse Gaussian process (GP) framework. We first integrate
a forgetting factor to refine a variational sparse GP algorithm, thus enhancing
its adaptability. Subsequently, the hyperparameters of the Gaussian model are
trained with a specially compound kernel, and the Gaussian model's online
inferential capability and computational efficiency are strengthened by
updating a solitary inducing point derived from new samples, in conjunction
with the learned hyperparameters. In the second phase, we propose a safety
filter based on high-order control barrier functions (HOCBFs), synergized with
the previously trained learning model. By leveraging the compound kernel from
the first phase, we effectively address the inherent limitations of GPs in
handling high-dimensional problems for real-time applications. The derived
controller ensures a rigorous lower bound on the probability of satisfying the
safety specification. Finally, the efficacy of our proposed algorithm is
demonstrated through real-time obstacle avoidance experiments executed using
both a simulation platform and a real-world 7-DOF robot.
\\ ( https://arxiv.org/abs/2402.18946 ,  2735kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18949
Date: Thu, 29 Feb 2024 08:27:01 GMT   (495kb,D)

Title: Improving Group Connectivity for Generalization of Federated Deep
  Learning
Authors: Zexi Li, Jie Lin, Zhiqi Li, Didi Zhu, Chao Wu
Categories: cs.LG
Comments: Preprint
\\
  Federated learning (FL) involves multiple heterogeneous clients
collaboratively training a global model via iterative local updates and model
fusion. The generalization of FL's global model has a large gap compared with
centralized training, which is its bottleneck for broader applications. In this
paper, we study and improve FL's generalization through a fundamental
``connectivity'' perspective, which means how the local models are connected in
the parameter region and fused into a generalized global model. The term
``connectivity'' is derived from linear mode connectivity (LMC), studying the
interpolated loss landscape of two different solutions (e.g., modes) of neural
networks. Bridging the gap between LMC and FL, in this paper, we leverage fixed
anchor models to empirically and theoretically study the transitivity property
of connectivity from two models (LMC) to a group of models (model fusion in
FL). Based on the findings, we propose FedGuCci and FedGuCci+, improving group
connectivity for better generalization. It is shown that our methods can boost
the generalization of FL under client heterogeneity across various tasks (4 CV
datasets and 6 NLP datasets), models (both convolutional and
transformer-based), and training paradigms (both from-scratch and
pretrain-finetune).
\\ ( https://arxiv.org/abs/2402.18949 ,  495kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18974
Date: Thu, 29 Feb 2024 09:26:46 GMT   (273kb,D)

Title: Graph Generation via Spectral Diffusion
Authors: Giorgia Minello, Alessandro Bicciato, Luca Rossi, Andrea Torsello,
  Luca Cosmo
Categories: cs.LG
\\
  In this paper, we present GRASP, a novel graph generative model based on 1)
the spectral decomposition of the graph Laplacian matrix and 2) a diffusion
process. Specifically, we propose to use a denoising model to sample
eigenvectors and eigenvalues from which we can reconstruct the graph Laplacian
and adjacency matrix. Our permutation invariant model can also handle node
features by concatenating them to the eigenvectors of each node. Using the
Laplacian spectrum allows us to naturally capture the structural
characteristics of the graph and work directly in the node space while avoiding
the quadratic complexity bottleneck that limits the applicability of other
methods. This is achieved by truncating the spectrum, which as we show in our
experiments results in a faster yet accurate generative process. An extensive
set of experiments on both synthetic and real world graphs demonstrates the
strengths of our model against state-of-the-art alternatives.
\\ ( https://arxiv.org/abs/2402.18974 ,  273kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18995
Date: Thu, 29 Feb 2024 09:46:47 GMT   (7642kb,D)

Title: Negative-Binomial Randomized Gamma Markov Processes for Heterogeneous
  Overdispersed Count Time Series
Authors: Rui Huang, Sikun Yang, Heinz Koeppl
Categories: cs.LG cs.AI stat.ML
\\
  Modeling count-valued time series has been receiving increasing attention
since count time series naturally arise in physical and social domains. Poisson
gamma dynamical systems (PGDSs) are newly-developed methods, which can well
capture the expressive latent transition structure and bursty dynamics behind
count sequences. In particular, PGDSs demonstrate superior performance in terms
of data imputation and prediction, compared with canonical linear dynamical
system (LDS) based methods. Despite these advantages, PGDS cannot capture the
heterogeneous overdispersed behaviours of the underlying dynamic processes. To
mitigate this defect, we propose a negative-binomial-randomized gamma Markov
process, which not only significantly improves the predictive performance of
the proposed dynamical system, but also facilitates the fast convergence of the
inference algorithm. Moreover, we develop methods to estimate both
factor-structured and graph-structured transition dynamics, which enable us to
infer more explainable latent structure, compared with PGDSs. Finally, we
demonstrate the explainable latent structure learned by the proposed method,
and show its superior performance in imputing missing data and forecasting
future observations, compared with the related models.
\\ ( https://arxiv.org/abs/2402.18995 ,  7642kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19009
Date: Thu, 29 Feb 2024 10:08:57 GMT   (29727kb,D)

Title: Generating, Reconstructing, and Representing Discrete and Continuous
  Data: Generalized Diffusion with Learnable Encoding-Decoding
Authors: Guangyi Liu, Yu Wang, Zeyu Feng, Qiyu Wu, Liping Tang, Yuan Gao, Zhen
  Li, Shuguang Cui, Julian McAuley, Eric P. Xing, Zichao Yang, Zhiting Hu
Categories: cs.LG cs.AI
\\
  The vast applications of deep generative models are anchored in three core
capabilities -- generating new instances, reconstructing inputs, and learning
compact representations -- across various data types, such as discrete
text/protein sequences and continuous images. Existing model families, like
Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs),
autoregressive models, and diffusion models, generally excel in specific
capabilities and data types but fall short in others. We introduce generalized
diffusion with learnable encoder-decoder (DiLED), that seamlessly integrates
the core capabilities for broad applicability and enhanced performance. DiLED
generalizes the Gaussian noising-denoising in standard diffusion by introducing
parameterized encoding-decoding. Crucially, DiLED is compatible with the
well-established diffusion model objective and training recipes, allowing
effective learning of the encoder-decoder parameters jointly with diffusion. By
choosing appropriate encoder/decoder (e.g., large language models), DiLED
naturally applies to different data types. Extensive experiments on text,
proteins, and images demonstrate DiLED's flexibility to handle diverse data and
tasks and its strong improvement over various existing models.
\\ ( https://arxiv.org/abs/2402.19009 ,  29727kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19016
Date: Thu, 29 Feb 2024 10:23:23 GMT   (2918kb,D)

Title: SPriFed-OMP: A Differentially Private Federated Learning Algorithm for
  Sparse Basis Recovery
Authors: Ajinkya Kiran Mulay, Xiaojun Lin
Categories: cs.LG cs.CR
Comments: Paper under review
\\
  Sparse basis recovery is a classical and important statistical learning
problem when the number of model dimensions $p$ is much larger than the number
of samples $n$. However, there has been little work that studies sparse basis
recovery in the Federated Learning (FL) setting, where the client data's
differential privacy (DP) must also be simultaneously protected. In particular,
the performance guarantees of existing DP-FL algorithms (such as DP-SGD) will
degrade significantly when $p \gg n$, and thus, they will fail to learn the
true underlying sparse model accurately. In this work, we develop a new
differentially private sparse basis recovery algorithm for the FL setting,
called SPriFed-OMP. SPriFed-OMP converts OMP (Orthogonal Matching Pursuit) to
the FL setting. Further, it combines SMPC (secure multi-party computation) and
DP to ensure that only a small amount of noise needs to be added in order to
achieve differential privacy. As a result, SPriFed-OMP can efficiently recover
the true sparse basis for a linear model with only $n = O(\sqrt{p})$ samples.
We further present an enhanced version of our approach, SPriFed-OMP-GRAD based
on gradient privatization, that improves the performance of SPriFed-OMP. Our
theoretical analysis and empirical results demonstrate that both SPriFed-OMP
and SPriFed-OMP-GRAD terminate in a small number of steps, and they
significantly outperform the previous state-of-the-art DP-FL solutions in terms
of the accuracy-privacy trade-off.
\\ ( https://arxiv.org/abs/2402.19016 ,  2918kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19025
Date: Thu, 29 Feb 2024 10:37:40 GMT   (5324kb,D)

Title: Combination of Weak Learners eXplanations to Improve Random Forest
  eXplicability Robustness
Authors: Riccardo Pala and Esteban Garc\'ia-Cuesta
Categories: cs.LG cs.AI
Comments: 8 pages, 10 figures
MSC-class: 68T01
ACM-class: I.2.0; I.2.1
\\
  The notion of robustness in XAI refers to the observed variations in the
explanation of the prediction of a learned model with respect to changes in the
input leading to that prediction. Intuitively, if the input being explained is
modified slightly subtly enough so as to not change the prediction of the model
too much, then we would expect that the explanation provided for that new input
does not change much either. We argue that a combination through discriminative
averaging of ensembles weak learners explanations can improve the robustness of
explanations in ensemble methods.This approach has been implemented and tested
with post-hoc SHAP method and Random Forest ensemble with successful results.
The improvements obtained have been measured quantitatively and some insights
into the explicability robustness in ensemble methods are presented.
\\ ( https://arxiv.org/abs/2402.19025 ,  5324kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19047
Date: Thu, 29 Feb 2024 11:20:16 GMT   (397kb,D)

Title: Theoretical Foundations of Deep Selective State-Space Models
Authors: Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, Cristopher Salvi
  and Terry Lyons
Categories: cs.LG math.DS
\\
  Structured state-space models (SSMs) such as S4, stemming from the seminal
work of Gu et al., are gaining popularity as effective approaches for modeling
sequential data. Deep SSMs demonstrate outstanding performance across a diverse
set of domains, at a reduced training and inference cost compared to
attention-based transformers. Recent developments show that if the linear
recurrence powering SSMs allows for multiplicative interactions between inputs
and hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture
can surpass in both in accuracy and efficiency attention-powered foundation
models trained on text, at scales of billion parameters. In this paper, we give
theoretical grounding to this recent finding using tools from Rough Path
Theory: we show that when random linear recurrences are equipped with simple
input-controlled transitions (selectivity mechanism), then the hidden state is
provably a low-dimensional projection of a powerful mathematical object called
the signature of the input -- capturing non-linear interactions between tokens
at distinct timescales. Our theory not only motivates the success of modern
selective state-space models such as Mamba but also provides a solid framework
to understand the expressive power of future SSM variants.
\\ ( https://arxiv.org/abs/2402.19047 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19072
Date: Thu, 29 Feb 2024 11:54:35 GMT   (1942kb,D)

Title: TimeXer: Empowering Transformers for Time Series Forecasting with
  Exogenous Variables
Authors: Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Yong Liu, Yunzhong Qiu, Haoran
  Zhang, Jianmin Wang, Mingsheng Long
Categories: cs.LG cs.AI
\\
  Recent studies have demonstrated remarkable performance in time series
forecasting. However, due to the partially-observed nature of real-world
applications, solely focusing on the target of interest, so-called endogenous
variables, is usually insufficient to guarantee accurate forecasting. Notably,
a system is often recorded into multiple variables, where the exogenous series
can provide valuable external information for endogenous variables. Thus,
unlike prior well-established multivariate or univariate forecasting that
either treats all the variables equally or overlooks exogenous information,
this paper focuses on a practical setting, which is time series forecasting
with exogenous variables. We propose a novel framework, TimeXer, to utilize
external information to enhance the forecasting of endogenous variables. With a
deftly designed embedding layer, TimeXer empowers the canonical Transformer
architecture with the ability to reconcile endogenous and exogenous
information, where patch-wise self-attention and variate-wise cross-attention
are employed. Moreover, a global endogenous variate token is adopted to
effectively bridge the exogenous series into endogenous temporal patches.
Experimentally, TimeXer significantly improves time series forecasting with
exogenous variables and achieves consistent state-of-the-art performance in
twelve real-world forecasting benchmarks.
\\ ( https://arxiv.org/abs/2402.19072 ,  1942kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19078
Date: Thu, 29 Feb 2024 12:03:05 GMT   (4008kb,D)

Title: Smooth Tchebycheff Scalarization for Multi-Objective Optimization
Authors: Xi Lin, Xiaoyuan Zhang, Zhiyuan Yang, Fei Liu, Zhenkun Wang, Qingfu
  Zhang
Categories: cs.LG cs.AI cs.NE math.OC
\\
  Multi-objective optimization problems can be found in many real-world
applications, where the objectives often conflict each other and cannot be
optimized by a single solution. In the past few decades, numerous methods have
been proposed to find Pareto solutions that represent different optimal
trade-offs among the objectives for a given problem. However, these existing
methods could have high computational complexity or may not have good
theoretical properties for solving a general differentiable multi-objective
optimization problem. In this work, by leveraging the smooth optimization
technique, we propose a novel and lightweight smooth Tchebycheff scalarization
approach for gradient-based multi-objective optimization. It has good
theoretical properties for finding all Pareto solutions with valid trade-off
preferences, while enjoying significantly lower computational complexity
compared to other methods. Experimental results on various real-world
application problems fully demonstrate the effectiveness of our proposed
method.
\\ ( https://arxiv.org/abs/2402.19078 ,  4008kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19090
Date: Thu, 29 Feb 2024 12:17:54 GMT   (202kb)

Title: Best Arm Identification with Resource Constraints
Authors: Zitian Li, Wang Chi Cheung
Categories: cs.LG
\\
  Motivated by the cost heterogeneity in experimentation across different
alternatives, we study the Best Arm Identification with Resource Constraints
(BAIwRC) problem. The agent aims to identify the best arm under resource
constraints, where resources are consumed for each arm pull. We make two novel
contributions. We design and analyze the Successive Halving with Resource
Rationing algorithm (SH-RR). The SH-RR achieves a near-optimal non-asymptotic
rate of convergence in terms of the probability of successively identifying an
optimal arm. Interestingly, we identify a difference in convergence rates
between the cases of deterministic and stochastic resource consumption.
\\ ( https://arxiv.org/abs/2402.19090 ,  202kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19102
Date: Thu, 29 Feb 2024 12:33:14 GMT   (2072kb,D)

Title: FlatNAS: optimizing Flatness in Neural Architecture Search for
  Out-of-Distribution Robustness
Authors: Matteo Gambella, Fabrizio Pittorino, and Manuel Roveri
Categories: cs.LG cs.AI cs.CV
\\
  Neural Architecture Search (NAS) paves the way for the automatic definition
of Neural Network (NN) architectures, attracting increasing research attention
and offering solutions in various scenarios. This study introduces a novel NAS
solution, called Flat Neural Architecture Search (FlatNAS), which explores the
interplay between a novel figure of merit based on robustness to weight
perturbations and single NN optimization with Sharpness-Aware Minimization
(SAM). FlatNAS is the first work in the literature to systematically explore
flat regions in the loss landscape of NNs in a NAS procedure, while jointly
optimizing their performance on in-distribution data, their out-of-distribution
(OOD) robustness, and constraining the number of parameters in their
architecture. Differently from current studies primarily concentrating on OOD
algorithms, FlatNAS successfully evaluates the impact of NN architectures on
OOD robustness, a crucial aspect in real-world applications of machine and deep
learning. FlatNAS achieves a good trade-off between performance, OOD
generalization, and the number of parameters, by using only in-distribution
data in the NAS exploration. The OOD robustness of the NAS-designed models is
evaluated by focusing on robustness to input data corruptions, using popular
benchmark datasets in the literature.
\\ ( https://arxiv.org/abs/2402.19102 ,  2072kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19105
Date: Thu, 29 Feb 2024 12:36:10 GMT   (5328kb,D)

Title: CollaFuse: Navigating Limited Resources and Privacy in Collaborative
  Generative AI
Authors: Domenique Zipperling, Simeon Allmendinger, Lukas Struppek, Niklas
  K\"uhl
Categories: cs.LG cs.AI
Comments: Thirty-Second European Conference on Information Systems (ECIS 2024)
\\
  In the landscape of generative artificial intelligence, diffusion-based
models present challenges for socio-technical systems in data requirements and
privacy. Traditional approaches like federated learning distribute the learning
process but strain individual clients, especially with constrained resources
(e.g., edge devices). In response to these challenges, we introduce CollaFuse,
a novel framework inspired by split learning. Tailored for efficient and
collaborative use of denoising diffusion probabilistic models, CollaFuse
enables shared server training and inference, alleviating client computational
burdens. This is achieved by retaining data and computationally inexpensive GPU
processes locally at each client while outsourcing the computationally
expensive processes to the shared server. Demonstrated in a healthcare context,
CollaFuse enhances privacy by highly reducing the need for sensitive
information sharing. These capabilities hold the potential to impact various
application areas, such as the design of edge computing solutions, healthcare
research, or autonomous driving. In essence, our work advances distributed
machine learning, shaping the future of collaborative GenAI networks.
\\ ( https://arxiv.org/abs/2402.19105 ,  5328kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19155
Date: Thu, 29 Feb 2024 13:38:07 GMT   (377kb,D)

Title: Beyond Language Models: Byte Models are Digital World Simulators
Authors: Shangda Wu, Xu Tan, Zili Wang, Rui Wang, Xiaobing Li, Maosong Sun
Categories: cs.LG
Comments: 19 pages, 5 figures, 5 tables
\\
  Traditional deep learning often overlooks bytes, the basic units of the
digital world, where all forms of information and operations are encoded and
manipulated in binary format. Inspired by the success of next token prediction
in natural language processing, we introduce bGPT, a model with next byte
prediction to simulate the digital world. bGPT matches specialized models in
performance across various modalities, including text, audio, and images, and
offers new possibilities for predicting, simulating, and diagnosing algorithm
or hardware behaviour. It has almost flawlessly replicated the process of
converting symbolic music data, achieving a low error rate of 0.0011 bits per
byte in converting ABC notation to MIDI format. In addition, bGPT demonstrates
exceptional capabilities in simulating CPU behaviour, with an accuracy
exceeding 99.99% in executing various operations. Leveraging next byte
prediction, models like bGPT can directly learn from vast binary data,
effectively simulating the intricate patterns of the digital world.
\\ ( https://arxiv.org/abs/2402.19155 ,  377kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19163
Date: Thu, 29 Feb 2024 13:47:23 GMT   (1305kb,D)

Title: FedStruct: Federated Decoupled Learning over Interconnected Graphs
Authors: Javad Aliakbari and Johan \"Ostman and Alexandre Graell i Amat
Categories: cs.LG cs.IT math.IT
Comments: 10 pages plus 13 pages of appendices
\\
  We address the challenge of federated learning on graph-structured data
distributed across multiple clients. Specifically, we focus on the prevalent
scenario of interconnected subgraphs, where inter-connections between different
clients play a critical role. We present a novel framework for this scenario,
named FedStruct, that harnesses deep structural dependencies. To uphold
privacy, unlike existing methods, FedStruct eliminates the necessity of sharing
or generating sensitive node features or embeddings among clients. Instead, it
leverages explicit global graph structure information to capture inter-node
dependencies. We validate the effectiveness of FedStruct through experimental
results conducted on six datasets for semi-supervised node classification,
showcasing performance close to the centralized approach across various
scenarios, including different data partitioning methods, varying levels of
label availability, and number of clients.
\\ ( https://arxiv.org/abs/2402.19163 ,  1305kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19226
Date: Thu, 29 Feb 2024 14:58:15 GMT   (535kb,D)

Title: Investigating Gender Fairness in Machine Learning-driven Personalized
  Care for Chronic Pain
Authors: Pratik Gajane and Sean Newman and John D. Piette
Categories: cs.LG cs.CY
\\
  This study investigates gender fairness in personalized pain care
recommendations using machine learning algorithms. Leveraging a contextual
bandits framework, personalized recommendations are formulated and evaluated
using LinUCB algorithm on a dataset comprising interactions with $164$ patients
across $10$ sessions each. Results indicate that while adjustments to algorithm
parameters influence the quality of pain care recommendations, this impact
remains consistent across genders. However, when certain patient information,
such as self-reported pain measurements, is absent, the quality of pain care
recommendations for women is notably inferior to that for men.
\\ ( https://arxiv.org/abs/2402.19226 ,  535kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19232
Date: Thu, 29 Feb 2024 15:05:59 GMT   (115kb,D)

Title: Trained Random Forests Completely Reveal your Dataset
Authors: Julien Ferry, Ricardo Fukasawa, Timoth\'ee Pascal and Thibaut Vidal
Categories: cs.LG cs.CR
\\
  We introduce an optimization-based reconstruction attack capable of
completely or near-completely reconstructing a dataset utilized for training a
random forest. Notably, our approach relies solely on information readily
available in commonly used libraries such as scikit-learn. To achieve this, we
formulate the reconstruction problem as a combinatorial problem under a maximum
likelihood objective. We demonstrate that this problem is NP-hard, though
solvable at scale using constraint programming -- an approach rooted in
constraint propagation and solution-domain reduction. Through an extensive
computational investigation, we demonstrate that random forests trained without
bootstrap aggregation but with feature randomization are susceptible to a
complete reconstruction. This holds true even with a small number of trees.
Even with bootstrap aggregation, the majority of the data can also be
reconstructed. These findings underscore a critical vulnerability inherent in
widely adopted ensemble methods, warranting attention and mitigation. Although
the potential for such reconstruction attacks has been discussed in privacy
research, our study provides clear empirical evidence of their practicability.
\\ ( https://arxiv.org/abs/2402.19232 ,  115kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19242
Date: Thu, 29 Feb 2024 15:18:37 GMT   (32923kb,D)

Title: Derivative-enhanced Deep Operator Network
Authors: Yuan Qiu, Nolan Bridges, Peng Chen
Categories: cs.LG cs.CE cs.NA math.NA
\\
  Deep operator networks (DeepONets), a class of neural operators that learn
mappings between function spaces, have recently been developed as surrogate
models for parametric partial differential equations (PDEs). In this work we
propose a derivative-enhanced deep operator network (DE-DeepONet), which
leverages the derivative information to enhance the prediction accuracy, and
provide a more accurate approximation of the derivatives, especially when the
training data are limited. DE-DeepONet incorporates dimension reduction of
input into DeepONet and includes two types of derivative labels in the loss
function for training, that is, the directional derivatives of the output
function with respect to the input function and the gradient of the output
function with respect to the physical domain variables. We test DE-DeepONet on
three different equations with increasing complexity to demonstrate its
effectiveness compared to the vanilla DeepONet.
\\ ( https://arxiv.org/abs/2402.19242 ,  32923kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19254
Date: Thu, 29 Feb 2024 15:26:03 GMT   (840kb,D)

Title: Machine learning for modular multiplication
Authors: Kristin Lauter, Cathy Yuanchen Li, Krystal Maughan, Rachel Newton and
  Megha Srivastava
Categories: cs.LG cs.CR
Comments: 14 pages, 12 figures. Comments welcome!
\\
  Motivated by cryptographic applications, we investigate two machine learning
approaches to modular multiplication: namely circular regression and a
sequence-to-sequence transformer model. The limited success of both methods
demonstrated in our results gives evidence for the hardness of tasks involving
modular multiplication upon which cryptosystems are based.
\\ ( https://arxiv.org/abs/2402.19254 ,  840kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19262
Date: Thu, 29 Feb 2024 15:32:02 GMT   (1417kb,D)

Title: Masks, Signs, And Learning Rate Rewinding
Authors: Advait Gadhikar and Rebekka Burkholz
Categories: cs.LG
Comments: Accepted for publishing at ICLR 2024
\\
  Learning Rate Rewinding (LRR) has been established as a strong variant of
Iterative Magnitude Pruning (IMP) to find lottery tickets in deep
overparameterized neural networks. While both iterative pruning schemes couple
structure and parameter learning, understanding how LRR excels in both aspects
can bring us closer to the design of more flexible deep learning algorithms
that can optimize diverse sets of sparse architectures. To this end, we conduct
experiments that disentangle the effect of mask learning and parameter
optimization and how both benefit from overparameterization. The ability of LRR
to flip parameter signs early and stay robust to sign perturbations seems to
make it not only more effective in mask identification but also in optimizing
diverse sets of masks, including random ones. In support of this hypothesis, we
prove in a simplified single hidden neuron setting that LRR succeeds in more
cases than IMP, as it can escape initially problematic sign configurations.
\\ ( https://arxiv.org/abs/2402.19262 ,  1417kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19287
Date: Thu, 29 Feb 2024 15:52:21 GMT   (43943kb,D)

Title: StiefelGen: A Simple, Model Agnostic Approach for Time Series Data
  Augmentation over Riemannian Manifolds
Authors: Prasad Cheema, Mahito Sugiyama
Categories: cs.LG
Comments: 61 pages, 41 figures
\\
  Data augmentation is an area of research which has seen active development in
many machine learning fields, such as in image-based learning models,
reinforcement learning for self driving vehicles, and general noise injection
for point cloud data. However, convincing methods for general time series data
augmentation still leaves much to be desired, especially since the methods
developed for these models do not readily cross-over. Three common approaches
for time series data augmentation include: (i) Constructing a physics-based
model and then imbuing uncertainty over the coefficient space (for example),
(ii) Adding noise to the observed data set(s), and, (iii) Having access to
ample amounts of time series data sets from which a robust generative neural
network model can be trained. However, for many practical problems that work
with time series data in the industry: (i) One usually does not have access to
a robust physical model, (ii) The addition of noise can in of itself require
large or difficult assumptions (for example, what probability distribution
should be used? Or, how large should the noise variance be?), and, (iii) In
practice, it can be difficult to source a large representative time series data
base with which to train the neural network model for the underlying problem.
In this paper, we propose a methodology which attempts to simultaneously tackle
all three of these previous limitations to a large extent. The method relies
upon the well-studied matrix differential geometry of the Stiefel manifold, as
it proposes a simple way in which time series signals can placed on, and then
smoothly perturbed over the manifold. We attempt to clarify how this method
works by showcasing several potential use cases which in particular work to
take advantage of the unique properties of this underlying manifold.
\\ ( https://arxiv.org/abs/2402.19287 ,  43943kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19290
Date: Thu, 29 Feb 2024 15:53:47 GMT   (498kb)

Title: Estimation and Deconvolution of Second Order Cyclostationary Signals
Authors: Igor Makienko, Michael Grebshtein, Eli Gildish
Categories: cs.LG eess.SP
Comments: 11 pages, 4 figures
\\
  This method solves the dual problem of blind deconvolution and estimation of
the time waveform of noisy second-order cyclo-stationary (CS2) signals that
traverse a Transfer Function (TF) en route to a sensor. We have proven that the
deconvolution filter exists and eliminates the TF effect from signals whose
statistics vary over time. This method is blind, meaning it does not require
prior knowledge about the signals or TF. Simulations demonstrate the algorithm
high precision across various signal types, TFs, and Signal-to-Noise Ratios
(SNRs). In this study, the CS2 signals family is restricted to the product of a
deterministic periodic function and white noise. Furthermore, this method has
the potential to improve the training of Machine Learning models where the
aggregation of signals from identical systems but with different TFs is
required.
\\ ( https://arxiv.org/abs/2402.19290 ,  498kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19294
Date: Thu, 29 Feb 2024 15:57:09 GMT   (17340kb,D)

Title: Degradation Modeling and Prognostic Analysis Under Unknown Failure Modes
Authors: Ying Fu, Ye Kwon Huh and Kaibo Liu
Categories: cs.LG cs.AI
\\
  Operating units often experience various failure modes in complex systems,
leading to distinct degradation paths. Relying on a prognostic model trained on
a single failure mode may lead to poor generalization performance across
multiple failure modes. Therefore, accurately identifying the failure mode is
of critical importance. Current prognostic approaches either ignore failure
modes during degradation or assume known failure mode labels, which can be
challenging to acquire in practice. Moreover, the high dimensionality and
complex relations of sensor signals make it challenging to identify the failure
modes accurately. To address these issues, we propose a novel failure mode
diagnosis method that leverages a dimension reduction technique called UMAP
(Uniform Manifold Approximation and Projection) to project and visualize each
unit's degradation trajectory into a lower dimension. Then, using these
degradation trajectories, we develop a time series-based clustering method to
identify the training units' failure modes. Finally, we introduce a
monotonically constrained prognostic model to predict the failure mode labels
and RUL of the test units simultaneously using the obtained failure modes of
the training units. The proposed prognostic model provides failure
mode-specific RUL predictions while preserving the monotonic property of the
RUL predictions across consecutive time steps. We evaluate the proposed model
using a case study with the aircraft gas turbine engine dataset.
\\ ( https://arxiv.org/abs/2402.19294 ,  17340kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19295
Date: Thu, 29 Feb 2024 15:58:16 GMT   (255kb,D)

Title: Anomaly Detection in Offshore Wind Turbine Structures using Hierarchical
  Bayesian Modelling
Authors: S. M. Smith, A. J. Hughes, T. A. Dardeno, L. A. Bull, N. Dervilis, K.
  Worden
Categories: cs.LG
Comments: Submitted to International Workshop on Structural Health Monitoring
  2023, Stanford University, California, USA
\\
  Population-based structural health monitoring (PBSHM), aims to share
information between members of a population. An offshore wind (OW) farm could
be considered as a population of nominally-identical wind-turbine structures.
However, benign variations exist among members, such as geometry, sea-bed
conditions and temperature differences. These factors could influence
structural properties and therefore the dynamic response, making it more
difficult to detect structural problems via traditional SHM techniques. This
paper explores the use of a hierarchical Bayesian model to infer expected soil
stiffness distributions at both population and local levels, as a basis to
perform anomaly detection, in the form of scour, for new and existing turbines.
To do this, observations of natural frequency will be generated as though they
are from a small population of wind turbines. Differences between individual
observations will be introduced by postulating distributions over the soil
stiffness and measurement noise, as well as reducing soil depth (to represent
scour), in the case of anomaly detection.
\\ ( https://arxiv.org/abs/2402.19295 ,  255kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19303
Date: Thu, 29 Feb 2024 16:09:19 GMT   (57kb)

Title: Learnability Gaps of Strategic Classification
Authors: Lee Cohen, Yishay Mansour, Shay Moran, Han Shao
Categories: cs.LG cs.GT
\\
  In contrast with standard classification tasks, strategic classification
involves agents strategically modifying their features in an effort to receive
favorable predictions. For instance, given a classifier determining loan
approval based on credit scores, applicants may open or close their credit
cards to fool the classifier. The learning goal is to find a classifier robust
against strategic manipulations. Various settings, based on what and when
information is known, have been explored in strategic classification. In this
work, we focus on addressing a fundamental question: the learnability gaps
between strategic classification and standard learning.
  We essentially show that any learnable class is also strategically learnable:
we first consider a fully informative setting, where the manipulation structure
(which is modeled by a manipulation graph $G^\star$) is known and during
training time the learner has access to both the pre-manipulation data and
post-manipulation data. We provide nearly tight sample complexity and regret
bounds, offering significant improvements over prior results. Then, we relax
the fully informative setting by introducing two natural types of uncertainty.
First, following Ahmadi et al. (2023), we consider the setting in which the
learner only has access to the post-manipulation data. We improve the results
of Ahmadi et al. (2023) and close the gap between mistake upper bound and lower
bound raised by them. Our second relaxation of the fully informative setting
introduces uncertainty to the manipulation structure. That is, we assume that
the manipulation graph is unknown but belongs to a known class of graphs. We
provide nearly tight bounds on the learning complexity in various unknown
manipulation graph settings. Notably, our algorithm in this setting is of
independent interest and can be applied to other problems such as multi-label
learning.
\\ ( https://arxiv.org/abs/2402.19303 ,  57kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19308
Date: Thu, 29 Feb 2024 16:15:34 GMT   (503kb,D)

Title: Loss-Free Machine Unlearning
Authors: Jack Foster, Stefan Schoepf, Alexandra Brintrup
Categories: cs.LG cs.CV
Comments: Accepted as a Tiny Paper at ICLR 2024
\\
  We present a machine unlearning approach that is both retraining- and
label-free. Most existing machine unlearning approaches require a model to be
fine-tuned to remove information while preserving performance. This is
computationally expensive and necessitates the storage of the whole dataset for
the lifetime of the model. Retraining-free approaches often utilise Fisher
information, which is derived from the loss and requires labelled data which
may not be available. Thus, we present an extension to the Selective Synaptic
Dampening algorithm, substituting the diagonal of the Fisher information matrix
for the gradient of the l2 norm of the model output to approximate sensitivity.
We evaluate our method in a range of experiments using ResNet18 and Vision
Transformer. Results show our label-free method is competitive with existing
state-of-the-art approaches.
\\ ( https://arxiv.org/abs/2402.19308 ,  503kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19322
Date: Thu, 29 Feb 2024 16:27:59 GMT   (3299kb,D)

Title: Verification of Neural Networks' Global Robustness
Authors: Anan Kabaha, Dana Drachsler-Cohen
Categories: cs.LG cs.CR cs.PL
\\
  Neural networks are successful in various applications but are also
susceptible to adversarial attacks. To show the safety of network classifiers,
many verifiers have been introduced to reason about the local robustness of a
given input to a given perturbation. While successful, local robustness cannot
generalize to unseen inputs. Several works analyze global robustness
properties, however, neither can provide a precise guarantee about the cases
where a network classifier does not change its classification. In this work, we
propose a new global robustness property for classifiers aiming at finding the
minimal globally robust bound, which naturally extends the popular local
robustness property for classifiers. We introduce VHAGaR, an anytime verifier
for computing this bound. VHAGaR relies on three main ideas: encoding the
problem as a mixed-integer programming and pruning the search space by
identifying dependencies stemming from the perturbation or network computation
and generalizing adversarial attacks to unknown inputs. We evaluate VHAGaR on
several datasets and classifiers and show that, given a three hour timeout, the
average gap between the lower and upper bound on the minimal globally robust
bound computed by VHAGaR is 1.9, while the gap of an existing global robustness
verifier is 154.7. Moreover, VHAGaR is 130.6x faster than this verifier. Our
results further indicate that leveraging dependencies and adversarial attacks
makes VHAGaR 78.6x faster.
\\ ( https://arxiv.org/abs/2402.19322 ,  3299kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19348
Date: Thu, 29 Feb 2024 16:56:23 GMT   (23621kb,D)

Title: Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy,
  Advances, and Outlook
Authors: Xingchen Zou, Yibo Yan, Xixuan Hao, Yuehong Hu, Haomin Wen, Erdong
  Liu, Junbo Zhang, Yong Li, Tianrui Li, Yu Zheng, Yuxuan Liang
Categories: cs.LG cs.AI
\\
  As cities continue to burgeon, Urban Computing emerges as a pivotal
discipline for sustainable development by harnessing the power of cross-domain
data fusion from diverse sources (e.g., geographical, traffic, social media,
and environmental data) and modalities (e.g., spatio-temporal, visual, and
textual modalities). Recently, we are witnessing a rising trend that utilizes
various deep-learning methods to facilitate cross-domain data fusion in smart
cities. To this end, we propose the first survey that systematically reviews
the latest advancements in deep learning-based data fusion methods tailored for
urban computing. Specifically, we first delve into data perspective to
comprehend the role of each modality and data source. Secondly, we classify the
methodology into four primary categories: feature-based, alignment-based,
contrast-based, and generation-based fusion methods. Thirdly, we further
categorize multi-modal urban applications into seven types: urban planning,
transportation, economy, public safety, society, environment, and energy.
Compared with previous surveys, we focus more on the synergy of deep learning
methods with urban computing applications. Furthermore, we shed light on the
interplay between Large Language Models (LLMs) and urban computing, postulating
future research directions that could revolutionize the field. We firmly
believe that the taxonomy, progress, and prospects delineated in our survey
stand poised to significantly enrich the research community. The summary of the
comprehensive and up-to-date paper list can be found at
https://github.com/yoshall/Awesome-Multimodal-Urban-Computing.
\\ ( https://arxiv.org/abs/2402.19348 ,  23621kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19369
Date: Thu, 29 Feb 2024 17:16:20 GMT   (7975kb,D)

Title: Structure Preserving Diffusion Models
Authors: Haoye Lu, Spencer Szabados, Yaoliang Yu
Categories: cs.LG cs.CV
\\
  Diffusion models have become the leading distribution-learning method in
recent years. Herein, we introduce structure-preserving diffusion processes, a
family of diffusion processes for learning distributions that possess
additional structure, such as group symmetries, by developing theoretical
conditions under which the diffusion transition steps preserve said symmetry.
While also enabling equivariant data sampling trajectories, we exemplify these
results by developing a collection of different symmetry equivariant diffusion
models capable of learning distributions that are inherently symmetric.
Empirical studies, over both synthetic and real-world datasets, are used to
validate the developed models adhere to the proposed theory and are capable of
achieving improved performance over existing methods in terms of sample
equality. We also show how the proposed models can be used to achieve
theoretically guaranteed equivariant image noise reduction without prior
knowledge of the image orientation.
\\ ( https://arxiv.org/abs/2402.19369 ,  7975kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19402
Date: Thu, 29 Feb 2024 18:01:07 GMT   (1005kb,D)

Title: A Scalable and Transferable Time Series Prediction Framework for Demand
  Forecasting
Authors: Young-Jin Park, Donghyun Kim, Fr\'ed\'eric Odermatt, Juho Lee,
  Kyung-Min Kim
Categories: cs.LG cs.AI
Comments: Published as a full paper at ICDM 2022
DOI: 10.1109/ICDM54844.2022.00048
\\
  Time series forecasting is one of the most essential and ubiquitous tasks in
many business problems, including demand forecasting and logistics
optimization. Traditional time series forecasting methods, however, have
resulted in small models with limited expressive power because they have
difficulty in scaling their model size up while maintaining high accuracy. In
this paper, we propose Forecasting orchestra (Forchestra), a simple but
powerful framework capable of accurately predicting future demand for a diverse
range of items. We empirically demonstrate that the model size is scalable to
up to 0.8 billion parameters. The proposed method not only outperforms existing
forecasting models with a significant margin, but it could generalize well to
unseen data points when evaluated in a zero-shot fashion on downstream
datasets. Last but not least, we present extensive qualitative and quantitative
studies to analyze how the proposed model outperforms baseline models and
differs from conventional approaches. The original paper was presented as a
full paper at ICDM 2022 and is available at:
https://ieeexplore.ieee.org/document/10027662.
\\ ( https://arxiv.org/abs/2402.19402 ,  1005kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19427
Date: Thu, 29 Feb 2024 18:24:46 GMT   (260kb,D)

Title: Griffin: Mixing Gated Linear Recurrences with Local Attention for
  Efficient Language Models
Authors: Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George
  Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen,
  Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee
  Whye Teh, Razvan Pascanu, Nando De Freitas and Caglar Gulcehre
Categories: cs.LG cs.CL
Comments: 25 pages, 11 figures
\\
  Recurrent neural networks (RNNs) have fast inference and scale efficiently on
long sequences, but they are difficult to train and hard to scale. We propose
Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that
mixes gated linear recurrences with local attention. Hawk exceeds the reported
performance of Mamba on downstream tasks, while Griffin matches the performance
of Llama-2 despite being trained on over 6 times fewer tokens. We also show
that Griffin can extrapolate on sequences significantly longer than those seen
during training. Our models match the hardware efficiency of Transformers
during training, and during inference they have lower latency and significantly
higher throughput. We scale Griffin up to 14B parameters, and explain how to
shard our models for efficient distributed training.
\\ ( https://arxiv.org/abs/2402.19427 ,  260kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19437
Date: Thu, 29 Feb 2024 18:38:20 GMT   (48kb)

Title: Differentially Private Worst-group Risk Minimization
Authors: Xinyu Zhou, Raef Bassily
Categories: cs.LG cs.AI cs.CR
\\
  We initiate a systematic study of worst-group risk minimization under
$(\epsilon, \delta)$-differential privacy (DP). The goal is to privately find a
model that approximately minimizes the maximal risk across $p$ sub-populations
(groups) with different distributions, where each group distribution is
accessed via a sample oracle. We first present a new algorithm that achieves
excess worst-group population risk of $\tilde{O}(\frac{p\sqrt{d}}{K\epsilon} +
\sqrt{\frac{p}{K}})$, where $K$ is the total number of samples drawn from all
groups and $d$ is the problem dimension. Our rate is nearly optimal when each
distribution is observed via a fixed-size dataset of size $K/p$. Our result is
based on a new stability-based analysis for the generalization error. In
particular, we show that $\Delta$-uniform argument stability implies
$\tilde{O}(\Delta + \frac{1}{\sqrt{n}})$ generalization error w.r.t. the
worst-group risk, where $n$ is the number of samples drawn from each sample
oracle. Next, we propose an algorithmic framework for worst-group population
risk minimization using any DP online convex optimization algorithm as a
subroutine. Hence, we give another excess risk bound of $\tilde{O}\left(
\sqrt{\frac{d^{1/2}}{\epsilon K}} +\sqrt{\frac{p}{K\epsilon^2}} \right)$.
Assuming the typical setting of $\epsilon=\Theta(1)$, this bound is more
favorable than our first bound in a certain range of $p$ as a function of $K$
and $d$. Finally, we study differentially private worst-group empirical risk
minimization in the offline setting, where each group distribution is observed
by a fixed-size dataset. We present a new algorithm with nearly optimal excess
risk of $\tilde{O}(\frac{p\sqrt{d}}{K\epsilon})$.
\\ ( https://arxiv.org/abs/2402.19437 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19442
Date: Thu, 29 Feb 2024 18:43:52 GMT   (2653kb,D)

Title: Training Dynamics of Multi-Head Softmax Attention for In-Context
  Learning: Emergence, Convergence, and Optimality
Authors: Siyu Chen, Heejune Sheen, Tianhao Wang, Zhuoran Yang
Categories: cs.LG cs.AI math.OC math.ST stat.ML stat.TH
Comments: 141 pages, 7 figures
\\
  We study the dynamics of gradient flow for training a multi-head softmax
attention model for in-context learning of multi-task linear regression. We
establish the global convergence of gradient flow under suitable choices of
initialization. In addition, we prove that an interesting "task allocation"
phenomenon emerges during the gradient flow dynamics, where each attention head
focuses on solving a single task of the multi-task model. Specifically, we
prove that the gradient flow dynamics can be split into three phases -- a
warm-up phase where the loss decreases rather slowly and the attention heads
gradually build up their inclination towards individual tasks, an emergence
phase where each head selects a single task and the loss rapidly decreases, and
a convergence phase where the attention parameters converge to a limit.
Furthermore, we prove the optimality of gradient flow in the sense that the
limiting model learned by gradient flow is on par with the best possible
multi-head softmax attention model up to a constant factor. Our analysis also
delineates a strict separation in terms of the prediction accuracy of ICL
between single-head and multi-head attention models. The key technique for our
convergence analysis is to map the gradient flow dynamics in the parameter
space to a set of ordinary differential equations in the spectral domain, where
the relative magnitudes of the semi-singular values of the attention weights
determines task allocation. To our best knowledge, our work provides the first
convergence result for the multi-head softmax attention model.
\\ ( https://arxiv.org/abs/2402.19442 ,  2653kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19446
Date: Thu, 29 Feb 2024 18:45:56 GMT   (5511kb,D)

Title: ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL
Authors: Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, Aviral Kumar
Categories: cs.LG cs.AI cs.CL
\\
  A broad use case of large language models (LLMs) is in goal-directed
decision-making tasks (or "agent" tasks), where an LLM needs to not just
generate completions for a given prompt, but rather make intelligent decisions
over a multi-turn interaction to accomplish a task (e.g., when interacting with
the web, using tools, or providing customer support). Reinforcement learning
(RL) provides a general paradigm to address such agent tasks, but current RL
methods for LLMs largely focus on optimizing single-turn rewards. By
construction, most single-turn RL methods cannot endow LLMs with the ability to
intelligently seek information over multiple turns, perform credit assignment,
or reason about their past actions -- all of which are critical in agent tasks.
This raises the question: how can we design effective and efficient multi-turn
RL algorithms for LLMs? In this paper, we develop a framework for building
multi-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility
of existing single-turn RL methods for LLMs (e.g., proximal policy
optimization), while accommodating multiple turns, long horizons, and delayed
rewards effectively. To do this, our framework adopts a hierarchical RL
approach and runs two RL algorithms in parallel: a high-level off-policy
value-based RL algorithm to aggregate reward over utterances, and a low-level
RL algorithm that utilizes this high-level value function to train a token
policy within each utterance or turn. Our hierarchical framework, Actor-Critic
Framework with a Hierarchical Structure (ArCHer), can also give rise to other
RL methods. Empirically, we find that ArCHer significantly improves efficiency
and performance on agent tasks, attaining a sample efficiency of about 100x
over existing methods, while also improving with larger model capacity (upto
the 7 billion scale that we tested on).
\\ ( https://arxiv.org/abs/2402.19446 ,  5511kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19449
Date: Thu, 29 Feb 2024 18:47:52 GMT   (1330kb,D)

Title: Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent
  on Language Models
Authors: Frederik Kunstner, Robin Yadav, Alan Milligan, Mark Schmidt, Alberto
  Bietti
Categories: cs.LG cs.CL math.OC stat.ML
\\
  Adam has been shown to outperform gradient descent in optimizing large
language transformers empirically, and by a larger margin than on other tasks,
but it is unclear why this happens. We show that the heavy-tailed class
imbalance found in language modeling tasks leads to difficulties in the
optimization dynamics. When training with gradient descent, the loss associated
with infrequent words decreases slower than the loss associated with frequent
ones. As most samples come from relatively infrequent words, the average loss
decreases slowly with gradient descent. On the other hand, Adam and sign-based
methods do not suffer from this problem and improve predictions on all classes.
To establish that this behavior is indeed caused by class imbalance, we show
empirically that it persist through different architectures and data types, on
language transformers, vision CNNs, and linear models. We further study this
phenomenon on a linear classification with cross-entropy loss, showing that
heavy-tailed class imbalance leads to ill-conditioning, and that the
normalization used by Adam can counteract it.
\\ ( https://arxiv.org/abs/2402.19449 ,  1330kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19460
Date: Thu, 29 Feb 2024 18:52:56 GMT   (17626kb,D)

Title: Benchmarking Uncertainty Disentanglement: Specialized Uncertainties for
  Specialized Tasks
Authors: B\'alint Mucs\'anyi and Michael Kirchhof and Seong Joon Oh
Categories: cs.LG stat.ML
Comments: 43 pages
\\
  Uncertainty quantification, once a singular task, has evolved into a spectrum
of tasks, including abstained prediction, out-of-distribution detection, and
aleatoric uncertainty quantification. The latest goal is disentanglement: the
construction of multiple estimators that are each tailored to one and only one
task. Hence, there is a plethora of recent advances with different intentions -
that often entirely deviate from practical behavior. This paper conducts a
comprehensive evaluation of numerous uncertainty estimators across diverse
tasks on ImageNet. We find that, despite promising theoretical endeavors,
disentanglement is not yet achieved in practice. Additionally, we reveal which
uncertainty estimators excel at which specific tasks, providing insights for
practitioners and guiding future research toward task-centric and disentangled
uncertainty estimation methods. Our code is available at
https://github.com/bmucsanyi/bud.
\\ ( https://arxiv.org/abs/2402.19460 ,  17626kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19464
Date: Thu, 29 Feb 2024 18:55:03 GMT   (554kb,D)

Title: Curiosity-driven Red-teaming for Large Language Models
Authors: Zhang-Wei Hong, Idan Shenfeld, Tsun-Hsuan Wang, Yung-Sung Chuang, Aldo
  Pareja, James Glass, Akash Srivastava, Pulkit Agrawal
Categories: cs.LG cs.AI cs.CL
Comments: Published at ICLR 2024
\\
  Large language models (LLMs) hold great potential for many natural language
applications but risk generating incorrect or toxic content. To probe when an
LLM generates unwanted content, the current paradigm is to recruit a
\textit{red team} of human testers to design input prompts (i.e., test cases)
that elicit undesirable responses from LLMs. However, relying solely on human
testers is expensive and time-consuming. Recent works automate red teaming by
training a separate red team LLM with reinforcement learning (RL) to generate
test cases that maximize the chance of eliciting undesirable responses from the
target LLM. However, current RL methods are only able to generate a small
number of effective test cases resulting in a low coverage of the span of
prompts that elicit undesirable responses from the target LLM. To overcome this
limitation, we draw a connection between the problem of increasing the coverage
of generated test cases and the well-studied approach of curiosity-driven
exploration that optimizes for novelty. Our method of curiosity-driven red
teaming (CRT) achieves greater coverage of test cases while mantaining or
increasing their effectiveness compared to existing methods. Our method, CRT
successfully provokes toxic responses from LLaMA2 model that has been heavily
fine-tuned using human preferences to avoid toxic outputs. Code is available at
\url{https://github.com/Improbable-AI/curiosity_redteam}
\\ ( https://arxiv.org/abs/2402.19464 ,  554kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19472
Date: Thu, 29 Feb 2024 18:58:26 GMT   (1895kb,D)

Title: Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid
  Progress
Authors: Ameya Prabhu, Vishaal Udandarao, Philip Torr, Matthias Bethge, Adel
  Bibi, Samuel Albanie
Categories: cs.LG cs.CV
\\
  Standardized benchmarks drive progress in machine learning. However, with
repeated testing, the risk of overfitting grows as algorithms over-exploit
benchmark idiosyncrasies. In our work, we seek to mitigate this challenge by
compiling ever-expanding large-scale benchmarks called Lifelong Benchmarks. As
exemplars of our approach, we create Lifelong-CIFAR10 and Lifelong-ImageNet,
containing (for now) 1.69M and 1.98M test samples, respectively. While reducing
overfitting, lifelong benchmarks introduce a key challenge: the high cost of
evaluating a growing number of models across an ever-expanding sample set. To
address this challenge, we also introduce an efficient evaluation framework:
Sort \& Search (S&S), which reuses previously evaluated models by leveraging
dynamic programming algorithms to selectively rank and sub-select test samples,
enabling cost-effective lifelong benchmarking. Extensive empirical evaluations
across 31,000 models demonstrate that S&S achieves highly-efficient approximate
accuracy measurement, reducing compute cost from 180 GPU days to 5 GPU hours
(1000x reduction) on a single A100 GPU, with low approximation error. As such,
lifelong benchmarks offer a robust, practical solution to the "benchmark
exhaustion" problem.
\\ ( https://arxiv.org/abs/2402.19472 ,  1895kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2401.15324 (*cross-listing*)
Date: Sat, 27 Jan 2024 06:57:24 GMT   (283kb,D)

Title: Neutrino Reconstruction in TRIDENT Based on Graph Neural Network
Authors: Cen Mo, Fuyudi Zhang, Liang Li
Categories: hep-ex cs.AI
\\
  TRopIcal DEep-sea Neutrino Telescope (TRIDENT) is a next-generation neutrino
telescope to be located in the South China Sea. With a large detector volume
and the use of advanced hybrid digital optical modules (hDOMs), TRIDENT aims to
discover multiple astrophysical neutrino sources and probe all-flavor neutrino
physics. The reconstruction resolution of primary neutrinos is on the critical
path to these scientific goals. We have developed a novel reconstruction method
based on graph neural network (GNN) for TRIDENT. In this paper, we present the
reconstruction performance of the GNN-based approach on both track- and
shower-like neutrino events in TRIDENT.
\\ ( https://arxiv.org/abs/2401.15324 ,  283kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18575 (*cross-listing*)
Date: Wed, 13 Dec 2023 03:39:05 GMT   (40889kb,D)

Title: DiffuseRAW: End-to-End Generative RAW Image Processing for Low-Light
  Images
Authors: Rishit Dagli
Categories: eess.IV cs.AI cs.CV cs.LG
\\
  Imaging under extremely low-light conditions presents a significant challenge
and is an ill-posed problem due to the low signal-to-noise ratio (SNR) caused
by minimal photon capture. Previously, diffusion models have been used for
multiple kinds of generative tasks and image-to-image tasks, however, these
models work as a post-processing step. These diffusion models are trained on
processed images and learn on processed images. However, such approaches are
often not well-suited for extremely low-light tasks. Unlike the task of
low-light image enhancement or image-to-image enhancement, we tackle the task
of learning the entire image-processing pipeline, from the RAW image to a
processed image. For this task, a traditional image processing pipeline often
consists of multiple specialized parts that are overly reliant on the
downstream tasks. Unlike these, we develop a new generative ISP that relies on
fine-tuning latent diffusion models on RAW images and generating processed
long-exposure images which allows for the apt use of the priors from large
text-to-image generation models. We evaluate our approach on popular end-to-end
low-light datasets for which we see promising results and set a new SoTA on the
See-in-Dark (SID) dataset. Furthermore, with this work, we hope to pave the way
for more generative and diffusion-based image processing and other problems on
RAW data.
\\ ( https://arxiv.org/abs/2402.18575 ,  40889kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18576 (*cross-listing*)
Date: Wed, 10 Jan 2024 01:15:33 GMT   (524kb)

Title: Improved Forecasting Using a PSO-RDV Framework to Enhance Artificial
  Neural Network
Authors: Sales Aribe Jr
Categories: cs.NE cs.AI cs.LG
Comments: 9 pages, 4 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT)
Journal-ref: International Journal of Engineering Trends and Technology, vol.
  72, no. 1, pp. 11-19, 2024
DOI: 10.14445/22315381/IJETT-V72I1P102
\\
  Decision making and planning have long relied heavily on AI-driven forecasts.
The government and the general public are working to minimize the risks while
maximizing benefits in the face of potential future public health
uncertainties. This study used an improved method of forecasting utilizing the
Random Descending Velocity Inertia Weight (RDV IW) technique to improve the
convergence of Particle Swarm Optimization (PSO) and the accuracy of Artificial
Neural Network (ANN). The IW technique, inspired by the motions of a golf ball,
modified the particles' velocities as they approached the solution point to a
parabolically descending structure. Simulation results revealed that the
proposed forecasting model with [0.4, 0.9] combination of alpha and alpha_dump
exhibits a 6.36% improvement in position error and 11.75% improvement in
computational time compared to the old model, thus, improving its convergence.
It reached the optimum level at minimal steps with 12.50% improvement as
against the old model since it provides better velocity averages when speed
stabilization occurs at the 24th iteration. Meanwhile, the computed p-values
for NRMSE (0.04889174), MAE (0.02829063), MAPE (0.02226053), WAPE (0.01701545),
and R2 (0.00000021) of the proposed algorithm are less than the set 0.05 level
of significance, thus the values indicated a significant result in terms of
accuracy performance. Applying the modified ANN-PSO using RDV IW technique
greatly improved the new HIV/AIDS forecasting model compared with the two
models.
\\ ( https://arxiv.org/abs/2402.18576 ,  524kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18577 (*cross-listing*)
Date: Wed, 10 Jan 2024 07:49:23 GMT   (7935kb,D)

Title: Motion Guided Token Compression for Efficient Masked Video Modeling
Authors: Yukun Feng, Yangming Shi, Fengze Liu, Tan Yan
Categories: cs.CV cs.AI
\\
  Recent developments in Transformers have achieved notable strides in
enhancing video comprehension. Nonetheless, the O($N^2$) computation complexity
associated with attention mechanisms presents substantial computational hurdles
when dealing with the high dimensionality of videos. This challenge becomes
particularly pronounced when striving to increase the frames per second (FPS)
to enhance the motion capturing capabilities. Such a pursuit is likely to
introduce redundancy and exacerbate the existing computational limitations. In
this paper, we initiate by showcasing the enhanced performance achieved through
an escalation in the FPS rate. Additionally, we present a novel approach,
Motion Guided Token Compression (MGTC), to empower Transformer models to
utilize a smaller yet more representative set of tokens for comprehensive video
representation. Consequently, this yields substantial reductions in
computational burden and remains seamlessly adaptable to increased FPS rates.
Specifically, we draw inspiration from video compression algorithms and
scrutinize the variance between patches in consecutive video frames across the
temporal dimension. The tokens exhibiting a disparity below a predetermined
threshold are then masked. Notably, this masking strategy effectively addresses
video redundancy while conserving essential information. Our experiments,
conducted on widely examined video recognition datasets, Kinetics-400, UCF101
and HMDB51, demonstrate that elevating the FPS rate results in a significant
top-1 accuracy score improvement of over 1.6, 1.6 and 4.0. By implementing MGTC
with the masking ratio of 25\%, we further augment accuracy by 0.1 and
simultaneously reduce computational costs by over 31\% on Kinetics-400. Even
within a fixed computational budget, higher FPS rates paired with MGTC sustain
performance gains when compared to lower FPS settings.
\\ ( https://arxiv.org/abs/2402.18577 ,  7935kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18579 (*cross-listing*)
Date: Thu, 11 Jan 2024 20:46:39 GMT   (7751kb)

Title: Wilcoxon Nonparametric CFAR Scheme for Ship Detection in SAR Image
Authors: Xiangwei Meng
Categories: cs.CV cs.AI eess.SP stat.AP
\\
  The parametric constant false alarm rate (CFAR) detection algorithms which
are based on various statistical distributions, such as Gaussian, Gamma,
Weibull, log-normal, G0 distribution, alpha-stable distribution, etc, are most
widely used to detect the ship targets in SAR image at present. However, the
clutter background in SAR images is complicated and variable. When the actual
clutter background deviates from the assumed statistical distribution, the
performance of the parametric CFAR detector will deteriorate. In addition to
the parametric CFAR schemes, there is another class of nonparametric CFAR
detectors which can maintain a constant false alarm rate for the target
detection without the assumption of a known clutter distribution. In this work,
the Wilcoxon nonparametric CFAR scheme for ship detection in SAR image is
proposed and analyzed, and a closed form of the false alarm rate for the
Wilcoxon nonparametric detector to determine the decision threshold is
presented. By comparison with several typical parametric CFAR schemes on
Radarsat-2, ICEYE-X6 and Gaofen-3 SAR images, the robustness of the Wilcoxon
nonparametric detector to maintain a good false alarm performance in different
detection backgrounds is revealed, and its detection performance for the weak
ship in rough sea surface is improved to some extent. Moreover, the Wilcoxon
nonparametric detector can suppress the false alarms resulting from the
sidelobes at some degree and its detection speed is fast.
\\ ( https://arxiv.org/abs/2402.18579 ,  7751kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18581 (*cross-listing*)
Date: Sun, 14 Jan 2024 05:02:12 GMT   (43854kb,D)

Title: Multi-objective Optimal Roadside Units Deployment in Urban Vehicular
  Networks
Authors: Weian Guo, Zecheng Kang, Dongyang Li, Lun Zhang, Li Li
Categories: cs.NE cs.AI
Comments: This manuscript has been submitted to the journal of IEEE
  Transactions on Vehicular Technology
\\
  The significance of transportation efficiency, safety, and related services
is increasing in urban vehicular networks. Within such networks, roadside units
(RSUs) serve as intermediates in facilitating communication. Therefore, the
deployment of RSUs is of utmost importance in ensuring the quality of
communication services. However, the optimization objectives, such as time
delay and deployment cost, are commonly developed from diverse perspectives. As
a result, it is possible that conflicts may arise among the objectives.
Furthermore, in urban environments, the presence of various obstacles, such as
buildings, gardens, lakes, and other infrastructure, poses challenges for the
deployment of RSUs. Hence, the deployment encounters significant difficulties
due to the existence of multiple objectives, constraints imposed by obstacles,
and the necessity to explore a large-scale optimization space. To address this
issue, two versions of multi-objective optimization algorithms are proposed in
this paper. By utilizing a multi-population strategy and an adaptive
exploration technique, the methods efficiently explore a large-scale
decision-variable space. In order to mitigate the issue of an overcrowded
deployment of RSUs, a calibrating mechanism is adopted to adjust RSU density
during the optimization procedures. The proposed methods also take care of data
offloading between vehicles and RSUs by setting up an iterative best response
sequence game (IBRSG). By comparing the proposed algorithms with several
state-of-the-art algorithms, the results demonstrate that our strategies
perform better in both high-density and low-density urban scenarios. The
results also indicate that the proposed solutions substantially improve the
efficiency of vehicular networks.
\\ ( https://arxiv.org/abs/2402.18581 ,  43854kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18582 (*cross-listing*)
Date: Sun, 14 Jan 2024 11:16:16 GMT   (1029kb)

Title: Streamlining the Selection Phase of Systematic Literature Reviews (SLRs)
  Using AI-Enabled GPT-4 Assistant API
Authors: Seyed Mohammad Ali Jafari
Categories: cs.DL cs.AI
Comments: 11 pages, 5 figures
\\
  The escalating volume of academic literature presents a formidable challenge
in staying updated with the newest research developments. Addressing this, this
study introduces a pioneering AI-based tool, configured specifically to
streamline the efficiency of the article selection phase in Systematic
Literature Reviews (SLRs). Utilizing the robust capabilities of OpenAI's GPT-4
Assistant API, the tool successfully homogenizes the article selection process
across a broad array of academic disciplines. Implemented through a tripartite
approach consisting of data preparation, AI-mediated article assessment, and
structured result presentation, this tool significantly accelerates the
time-consuming task of literature reviews. Importantly, this tool could be
highly beneficial in fields such as management and economics, where the SLR
process involves substantial human judgment. The adoption of a standard GPT
model can substantially reduce potential biases and enhance the speed and
precision of the SLR selection phase. This not only amplifies researcher
productivity and accuracy but also denotes a considerable stride forward in the
way academic research is conducted amidst the surging body of scholarly
publications.
\\ ( https://arxiv.org/abs/2402.18582 ,  1029kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18587 (*cross-listing*)
Date: Fri, 2 Feb 2024 06:23:25 GMT   (4303kb,D)

Title: At the Dawn of Generative AI Era: A Tutorial-cum-Survey on New Frontiers
  in 6G Wireless Intelligence
Authors: Abdulkadir Celik, Ahmed M. Eltawil
Categories: cs.NI cs.AI cs.LG
DOI: 10.1109/OJCOMS.2024.3362271
\\
  The majority of data-driven wireless research leans heavily on discriminative
AI (DAI) that requires vast real-world datasets. Unlike the DAI, Generative AI
(GenAI) pertains to generative models (GMs) capable of discerning the
underlying data distribution, patterns, and features of the input data. This
makes GenAI a crucial asset in wireless domain wherein real-world data is often
scarce, incomplete, costly to acquire, and hard to model or comprehend. With
these appealing attributes, GenAI can replace or supplement DAI methods in
various capacities. Accordingly, this combined tutorial-survey paper commences
with preliminaries of 6G and wireless intelligence by outlining candidate 6G
applications and services, presenting a taxonomy of state-of-the-art DAI
models, exemplifying prominent DAI use cases, and elucidating the multifaceted
ways through which GenAI enhances DAI. Subsequently, we present a tutorial on
GMs by spotlighting seminal examples such as generative adversarial networks,
variational autoencoders, flow-based GMs, diffusion-based GMs, generative
transformers, large language models, to name a few. Contrary to the prevailing
belief that GenAI is a nascent trend, our exhaustive review of approximately
120 technical papers demonstrates the scope of research across core wireless
research areas, including physical layer design; network optimization,
organization, and management; network traffic analytics; cross-layer network
security; and localization & positioning. Furthermore, we outline the central
role of GMs in pioneering areas of 6G network research, including
semantic/THz/near-field communications, ISAC, extremely large antenna arrays,
digital twins, AI-generated content services, mobile edge computing and edge
AI, adversarial ML, and trustworthy AI. Lastly, we shed light on the
multifarious challenges ahead, suggesting potential strategies and promising
remedies.
\\ ( https://arxiv.org/abs/2402.18587 ,  4303kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18589 (*cross-listing*)
Date: Fri, 9 Feb 2024 10:25:01 GMT   (896kb,D)

Title: Verif.ai: Towards an Open-Source Scientific Generative
  Question-Answering System with Referenced and Verifiable Answers
Authors: Milo\v{s} Ko\v{s}prdi\'c, Adela Ljaji\'c, Bojana Ba\v{s}aragin, Darija
  Medvecki, Nikola Milo\v{s}evi\'c
Categories: cs.IR cs.AI cs.CL cs.LG
Comments: Accepted as a short paper at The Sixteenth International Conference
  on Evolving Internet (INTERNET 2024)
Journal-ref: The Sixteenth International Conference on Evolving Internet
  (INTERNET 2024)
\\
  In this paper, we present the current progress of the project Verif.ai, an
open-source scientific generative question-answering system with referenced and
verified answers. The components of the system are (1) an information retrieval
system combining semantic and lexical search techniques over scientific papers
(PubMed), (2) a fine-tuned generative model (Mistral 7B) taking top answers and
generating answers with references to the papers from which the claim was
derived, and (3) a verification engine that cross-checks the generated claim
and the abstract or paper from which the claim was derived, verifying whether
there may have been any hallucinations in generating the claim. We are
reinforcing the generative model by providing the abstract in context, but in
addition, an independent set of methods and models are verifying the answer and
checking for hallucinations. Therefore, we believe that by using our method, we
can make scientists more productive, while building trust in the use of
generative language models in scientific environments, where hallucinations and
misinformation cannot be tolerated.
\\ ( https://arxiv.org/abs/2402.18589 ,  896kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18590 (*cross-listing*)
Date: Sun, 11 Feb 2024 00:24:17 GMT   (701kb)

Title: Exploring the Impact of Large Language Models on Recommender Systems: An
  Extensive Review
Authors: Arpita Vats, Vinija Jain, Rahul Raja, Aman Chadha
Categories: cs.IR cs.AI
\\
  The paper underscores the significance of Large Language Models (LLMs) in
reshaping recommender systems, attributing their value to unique reasoning
abilities absent in traditional recommenders. Unlike conventional systems
lacking direct user interaction data, LLMs exhibit exceptional proficiency in
recommending items, showcasing their adeptness in comprehending intricacies of
language. This marks a fundamental paradigm shift in the realm of
recommendations. Amidst the dynamic research landscape, researchers actively
harness the language comprehension and generation capabilities of LLMs to
redefine the foundations of recommendation tasks. The investigation thoroughly
explores the inherent strengths of LLMs within recommendation frameworks,
encompassing nuanced contextual comprehension, seamless transitions across
diverse domains, adoption of unified approaches, holistic learning strategies
leveraging shared data reservoirs, transparent decision-making, and iterative
improvements. Despite their transformative potential, challenges persist,
including sensitivity to input prompts, occasional misinterpretations, and
unforeseen recommendations, necessitating continuous refinement and evolution
in LLM-driven recommender systems.
\\ ( https://arxiv.org/abs/2402.18590 ,  701kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18593 (*cross-listing*)
Date: Sun, 25 Feb 2024 02:22:34 GMT   (329kb,D)

Title: Sustainable Supercomputing for AI: GPU Power Capping at HPC Scale
Authors: Dan Zhao, Siddharth Samsi, Joseph McDonald, Baolin Li, David Bestor,
  Michael Jones, Devesh Tiwari, Vijay Gadepally
Categories: cs.AR cs.AI cs.DC
DOI: 10.1145/3620678.3624793
\\
  As research and deployment of AI grows, the computational burden to support
and sustain its progress inevitably does too. To train or fine-tune
state-of-the-art models in NLP, computer vision, etc., some form of AI hardware
acceleration is virtually a requirement. Recent large language models require
considerable resources to train and deploy, resulting in significant energy
usage, potential carbon emissions, and massive demand for GPUs and other
hardware accelerators. However, this surge carries large implications for
energy sustainability at the HPC/datacenter level. In this paper, we study the
aggregate effect of power-capping GPUs on GPU temperature and power draw at a
research supercomputing center. With the right amount of power-capping, we show
significant decreases in both temperature and power draw, reducing power
consumption and potentially improving hardware life-span with minimal impact on
job performance. While power-capping reduces power draw by design, the
aggregate system-wide effect on overall energy consumption is less clear; for
instance, if users notice job performance degradation from GPU power-caps, they
may request additional GPU-jobs to compensate, negating any energy savings or
even worsening energy consumption. To our knowledge, our work is the first to
conduct and make available a detailed analysis of the effects of GPU
power-capping at the supercomputing scale. We hope our work will inspire
HPCs/datacenters to further explore, evaluate, and communicate the impact of
power-capping AI hardware accelerators for more sustainable AI.
\\ ( https://arxiv.org/abs/2402.18593 ,  329kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18598 (*cross-listing*)
Date: Tue, 27 Feb 2024 14:10:34 GMT   (3851kb)

Title: Note: Evolutionary Game Theory Focus Informational Health: The Cocktail
  Party Effect Through Werewolfgame under Incomplete Information and ESS Search
  Method Using Expected Gains of Repeated Dilemmas
Authors: Yasuko Kawahata
Categories: physics.soc-ph cs.AI
Comments: Werewolf Games, Evolutionary Game Theory, Non-Complete Information
  Games, Expanding Form Games, Cocktail Party Effect, Fake News, Evolutionary
  Stability Strategy (ESS), Information Pollution Risk, Numerical Simulation,
  Strategic Interaction, Replicator Equation
\\
  We explore the state of information disruption caused by the cocktail party
effect within the framework of non-perfect information games and evolutive
games with multiple werewolves. In particular, we mathematically model and
analyze the effects on the gain of each strategy choice and the formation
process of evolutionary stable strategies (ESS) under the assumption that the
pollution risk of fake news is randomly assigned in the context of repeated
dilemmas. We will develop the computational process in detail, starting with
the construction of the gain matrix, modeling the evolutionary dynamics using
the replicator equation, and identifying the ESS. In addition, numerical
simulations will be performed to observe system behavior under different
initial conditions and parameter settings to better understand the impact of
the spread of fake news on strategy evolution. This research will provide
theoretical insights into the complex issues of contemporary society regarding
the authenticity of information and expand the range of applications of
evolutionary game theory.
\\ ( https://arxiv.org/abs/2402.18598 ,  3851kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18600 (*cross-listing*)
Date: Wed, 28 Feb 2024 00:31:17 GMT   (1373kb)

Title: Artificial Intelligence and Diabetes Mellitus: An Inside Look Through
  the Retina
Authors: Yasin Sadeghi Bazargani, Majid Mirzaei, Navid Sobhi, Mirsaeed
  Abdollahi, Ali Jafarizadeh, Siamak Pedrammehr, Roohallah Alizadehsani, Ru San
  Tan, Sheikh Mohammed Shariful Islam, U. Rajendra Acharya
Categories: eess.IV cs.AI q-bio.TO
Comments: 44 Pages, 6 figures, 1 table, 166 references
ACM-class: J.3.2; J.3.3
\\
  Diabetes mellitus (DM) predisposes patients to vascular complications.
Retinal images and vasculature reflect the body's micro- and macrovascular
health. They can be used to diagnose DM complications, including diabetic
retinopathy (DR), neuropathy, nephropathy, and atherosclerotic cardiovascular
disease, as well as forecast the risk of cardiovascular events. Artificial
intelligence (AI)-enabled systems developed for high-throughput detection of DR
using digitized retinal images have become clinically adopted. Beyond DR
screening, AI integration also holds immense potential to address challenges
associated with the holistic care of the patient with DM. In this work, we aim
to comprehensively review the literature for studies on AI applications based
on retinal images related to DM diagnosis, prognostication, and management. We
will describe the findings of holistic AI-assisted diabetes care, including but
not limited to DR screening, and discuss barriers to implementing such systems,
including issues concerning ethics, data privacy, equitable access, and
explainability. With the ability to evaluate the patient's health status vis a
vis DM complication as well as risk prognostication of future cardiovascular
complications, AI-assisted retinal image analysis has the potential to become a
central tool for modern personalized medicine in patients with DM.
\\ ( https://arxiv.org/abs/2402.18600 ,  1373kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18616 (*cross-listing*)
Date: Wed, 28 Feb 2024 17:38:01 GMT   (1256kb)

Title: JCLEC-MO: a Java suite for solving many-objective optimization
  engineering problems
Authors: Aurora Ram\'irez and Jos\'e Ra\'ul Romero and Carlos
  Garc\'ia-Mart\'inez and Sebasti\'an Ventura
Categories: cs.NE cs.AI
Comments: 41 pages, 5 figures, journal paper
MSC-class: 68T20
ACM-class: D.0; I.2.8
Journal-ref: Engineering Applications of Artificial Intelligence, Volume 81,
  May 2019, Pages 14-28
DOI: 10.1016/j.engappai.2019.02.003
\\
  Although metaheuristics have been widely recognized as efficient techniques
to solve real-world optimization problems, implementing them from scratch
remains difficult for domain-specific experts without programming skills. In
this scenario, metaheuristic optimization frameworks are a practical
alternative as they provide a variety of algorithms composed of customized
elements, as well as experimental support. Recently, many engineering problems
require to optimize multiple or even many objectives, increasing the interest
in appropriate metaheuristic algorithms and frameworks that might integrate new
specific requirements while maintaining the generality and reusability
principles they were conceived for. Based on this idea, this paper introduces
JCLEC-MO, a Java framework for both multi- and many-objective optimization that
enables engineers to apply, or adapt, a great number of multi-objective
algorithms with little coding effort. A case study is developed and explained
to show how JCLEC-MO can be used to address many-objective engineering
problems, often requiring the inclusion of domain-specific elements, and to
analyze experimental outcomes by means of conveniently connected R utilities.
\\ ( https://arxiv.org/abs/2402.18616 ,  1256kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18617 (*cross-listing*)
Date: Wed, 28 Feb 2024 17:44:02 GMT   (5088kb,D)

Title: ELA: Exploited Level Augmentation for Offline Learning in Zero-Sum Games
Authors: Shiqi Lei, Kanghoon Lee, Linjing Li, Jinkyoo Park, and Jiachen Li
Categories: cs.GT cs.AI cs.LG cs.MA
Comments: 12 pages, 8 figures
\\
  Offline learning has become widely used due to its ability to derive
effective policies from offline datasets gathered by expert demonstrators
without interacting with the environment directly. Recent research has explored
various ways to enhance offline learning efficiency by considering the
characteristics (e.g., expertise level or multiple demonstrators) of the
dataset. However, a different approach is necessary in the context of zero-sum
games, where outcomes vary significantly based on the strategy of the opponent.
In this study, we introduce a novel approach that uses unsupervised learning
techniques to estimate the exploited level of each trajectory from the offline
dataset of zero-sum games made by diverse demonstrators. Subsequently, we
incorporate the estimated exploited level into the offline learning to maximize
the influence of the dominant strategy. Our method enables interpretable
exploited level estimation in multiple zero-sum games and effectively
identifies dominant strategy data. Also, our exploited level augmented offline
learning significantly enhances the original offline learning algorithms
including imitation learning and offline reinforcement learning for zero-sum
games.
\\ ( https://arxiv.org/abs/2402.18617 ,  5088kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18649 (*cross-listing*)
Date: Wed, 28 Feb 2024 19:00:12 GMT   (20709kb,D)

Title: A New Era in LLM Security: Exploring Security Concerns in Real-World
  LLM-based Systems
Authors: Fangzhou Wu, Ning Zhang, Somesh Jha, Patrick McDaniel, Chaowei Xiao
Categories: cs.CR cs.AI
\\
  Large Language Model (LLM) systems are inherently compositional, with
individual LLM serving as the core foundation with additional layers of objects
such as plugins, sandbox, and so on. Along with the great potential, there are
also increasing concerns over the security of such probabilistic intelligent
systems. However, existing studies on LLM security often focus on individual
LLM, but without examining the ecosystem through the lens of LLM systems with
other objects (e.g., Frontend, Webtool, Sandbox, and so on). In this paper, we
systematically analyze the security of LLM systems, instead of focusing on the
individual LLMs. To do so, we build on top of the information flow and
formulate the security of LLM systems as constraints on the alignment of the
information flow within LLM and between LLM and other objects. Based on this
construction and the unique probabilistic nature of LLM, the attack surface of
the LLM system can be decomposed into three key components: (1) multi-layer
security analysis, (2) analysis of the existence of constraints, and (3)
analysis of the robustness of these constraints. To ground this new attack
surface, we propose a multi-layer and multi-step approach and apply it to the
state-of-art LLM system, OpenAI GPT4. Our investigation exposes several
security issues, not just within the LLM model itself but also in its
integration with other components. We found that although the OpenAI GPT4 has
designed numerous safety constraints to improve its safety features, these
safety constraints are still vulnerable to attackers. To further demonstrate
the real-world threats of our discovered vulnerabilities, we construct an
end-to-end attack where an adversary can illicitly acquire the user's chat
history, all without the need to manipulate the user's input or gain direct
access to OpenAI GPT4. Our demo is in the link:
https://fzwark.github.io/LLM-System-Attack-Demo/
\\ ( https://arxiv.org/abs/2402.18649 ,  20709kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18673 (*cross-listing*)
Date: Wed, 28 Feb 2024 19:35:30 GMT   (398kb,D)

Title: Trends, Applications, and Challenges in Human Attention Modelling
Authors: Giuseppe Cartella, Marcella Cornia, Vittorio Cuculo, Alessandro
  D'Amelio, Dario Zanca, Giuseppe Boccignone, Rita Cucchiara
Categories: cs.CV cs.AI
Comments: GitHub repository:
  https://github.com/aimagelab/awesome-human-visual-attention
\\
  Human attention modelling has proven, in recent years, to be particularly
useful not only for understanding the cognitive processes underlying visual
exploration, but also for providing support to artificial intelligence models
that aim to solve problems in various domains, including image and video
processing, vision-and-language applications, and language modelling. This
survey offers a reasoned overview of recent efforts to integrate human
attention mechanisms into contemporary deep learning models and discusses
future research directions and challenges. For a comprehensive overview on the
ongoing research refer to our dedicated repository available at
https://github.com/aimagelab/awesome-human-visual-attention.
\\ ( https://arxiv.org/abs/2402.18673 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18677 (*cross-listing*)
Date: Wed, 28 Feb 2024 19:44:19 GMT   (9143kb,D)

Title: Fault Tolerant Neural Control Barrier Functions for Robotic Systems
  under Sensor Faults and Attacks
Authors: Hongchao Zhang, Luyao Niu, Andrew Clark, Radha Poovendran
Categories: cs.RO cs.AI cs.SY eess.SY
\\
  Safety is a fundamental requirement of many robotic systems. Control barrier
function (CBF)-based approaches have been proposed to guarantee the safety of
robotic systems. However, the effectiveness of these approaches highly relies
on the choice of CBFs. Inspired by the universal approximation power of neural
networks, there is a growing trend toward representing CBFs using neural
networks, leading to the notion of neural CBFs (NCBFs). Current NCBFs, however,
are trained and deployed in benign environments, making them ineffective for
scenarios where robotic systems experience sensor faults and attacks. In this
paper, we study safety-critical control synthesis for robotic systems under
sensor faults and attacks. Our main contribution is the development and
synthesis of a new class of CBFs that we term fault tolerant neural control
barrier function (FT-NCBF). We derive the necessary and sufficient conditions
for FT-NCBFs to guarantee safety, and develop a data-driven method to learn
FT-NCBFs by minimizing a loss function constructed using the derived
conditions. Using the learned FT-NCBF, we synthesize a control input and
formally prove the safety guarantee provided by our approach. We demonstrate
our proposed approach using two case studies: obstacle avoidance problem for an
autonomous mobile robot and spacecraft rendezvous problem, with code available
via https://github.com/HongchaoZhang-HZ/FTNCBF.
\\ ( https://arxiv.org/abs/2402.18677 ,  9143kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18759 (*cross-listing*)
Date: Wed, 28 Feb 2024 23:57:04 GMT   (24208kb,D)

Title: Learning with Language-Guided State Abstractions
Authors: Andi Peng, Ilia Sucholutsky, Belinda Z. Li, Theodore R. Sumers, Thomas
  L. Griffiths, Jacob Andreas, Julie A. Shah
Categories: cs.RO cs.AI cs.LG
Comments: ICLR 2024
\\
  We describe a framework for using natural language to design state
abstractions for imitation learning. Generalizable policy learning in
high-dimensional observation spaces is facilitated by well-designed state
representations, which can surface important features of an environment and
hide irrelevant ones. These state representations are typically manually
specified, or derived from other labor-intensive labeling procedures. Our
method, LGA (language-guided abstraction), uses a combination of natural
language supervision and background knowledge from language models (LMs) to
automatically build state representations tailored to unseen tasks. In LGA, a
user first provides a (possibly incomplete) description of a target task in
natural language; next, a pre-trained LM translates this task description into
a state abstraction function that masks out irrelevant features; finally, an
imitation policy is trained using a small number of demonstrations and
LGA-generated abstract states. Experiments on simulated robotic tasks show that
LGA yields state abstractions similar to those designed by humans, but in a
fraction of the time, and that these abstractions improve generalization and
robustness in the presence of spurious correlations and ambiguous
specifications. We illustrate the utility of the learned abstractions on mobile
manipulation tasks with a Spot robot.
\\ ( https://arxiv.org/abs/2402.18759 ,  24208kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18826 (*cross-listing*)
Date: Thu, 29 Feb 2024 03:20:53 GMT   (498kb)

Title: The Machine Can't Replace the Human Heart
Authors: Baihan Lin
Categories: cs.CY cs.AI cs.HC q-bio.NC
\\
  What is the true heart of mental healthcare -- innovation or humanity? Can
virtual therapy ever replicate the profound human bonds where healing arises?
As artificial intelligence and immersive technologies promise expanded access,
safeguards must ensure technologies remain supplementary tools guided by
providers' wisdom. Implementation requires nuance balancing efficiency and
empathy. If conscious of ethical risks, perhaps AI could restore humanity by
automating tasks, giving providers more time to listen. Yet no algorithm can
replicate the seat of dignity within. We must ask ourselves: What future has
people at its core? One where AI thoughtfully plays a collaborative role? Or
where pursuit of progress leaves vulnerability behind? This commentary argues
for a balanced approach thoughtfully integrating technology while retaining
care's irreplaceable human essence, at the heart of this profoundly human
profession. Ultimately, by nurturing innovation and humanity together, perhaps
we reach new heights of empathy previously unimaginable.
\\ ( https://arxiv.org/abs/2402.18826 ,  498kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18849 (*cross-listing*)
Date: Thu, 29 Feb 2024 04:53:06 GMT   (1390kb)

Title: Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP
  Models on Accuracy and Semantic Coherence
Authors: Mingyang Li, Maoqin Yuan, Luyao Li, Han Pengsihua
Categories: cs.CV cs.AI cs.CL
\\
  This study discusses a new method combining image steganography technology
with Natural Language Processing (NLP) large models, aimed at improving the
accuracy and robustness of extracting steganographic text. Traditional Least
Significant Bit (LSB) steganography techniques face challenges in accuracy and
robustness of information extraction when dealing with complex character
encoding, such as Chinese characters. To address this issue, this study
proposes an innovative LSB-NLP hybrid framework. This framework integrates the
advanced capabilities of NLP large models, such as error detection, correction,
and semantic consistency analysis, as well as information reconstruction
techniques, thereby significantly enhancing the robustness of steganographic
text extraction. Experimental results show that the LSB-NLP hybrid framework
excels in improving the extraction accuracy of steganographic text, especially
in handling Chinese characters. The findings of this study not only confirm the
effectiveness of combining image steganography technology and NLP large models
but also propose new ideas for research and application in the field of
information hiding. The successful implementation of this interdisciplinary
approach demonstrates the great potential of integrating image steganography
technology with natural language processing technology in solving complex
information processing problems.
\\ ( https://arxiv.org/abs/2402.18849 ,  1390kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18908 (*cross-listing*)
Date: Thu, 29 Feb 2024 07:08:18 GMT   (23kb)

Title: Facility Location Games with Scaling Effects
Authors: Yu He, Alexander Lam and Minming Li
Categories: cs.GT cs.AI cs.MA econ.TH
\\
  We take the classic facility location problem and consider a variation, in
which each agent's individual cost function is equal to their distance from the
facility multiplied by a scaling factor which is determined by the facility
placement. In addition to the general class of continuous scaling functions, we
also provide results for piecewise linear scaling functions which can
effectively approximate or model the scaling of many real world scenarios. We
focus on the objectives of total and maximum cost, describing the computation
of the optimal solution. We then move to the approximate mechanism design
setting, observing that the agents' preferences may no longer be single-peaked.
Consequently, we characterize the conditions on scaling functions which ensure
that agents have single-peaked preferences. Under these conditions, we find
results on the total and maximum cost approximation ratios achievable by
strategyproof and anonymous mechanisms.
\\ ( https://arxiv.org/abs/2402.18908 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18920 (*cross-listing*)
Date: Thu, 29 Feb 2024 07:26:23 GMT   (43301kb,D)

Title: Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation
Authors: Dongliang Cao, Marvin Eisenberger, Nafie El Amrani, Daniel Cremers,
  Florian Bernard
Categories: cs.CV cs.AI cs.CG
Comments: accepted by CVPR2024
\\
  Although 3D shape matching and interpolation are highly interrelated, they
are often studied separately and applied sequentially to relate different 3D
shapes, thus resulting in sub-optimal performance. In this work we present a
unified framework to predict both point-wise correspondences and shape
interpolation between 3D shapes. To this end, we combine the deep functional
map framework with classical surface deformation models to map shapes in both
spectral and spatial domains. On the one hand, by incorporating spatial maps,
our method obtains more accurate and smooth point-wise correspondences compared
to previous functional map methods for shape matching. On the other hand, by
introducing spectral maps, our method gets rid of commonly used but
computationally expensive geodesic distance constraints that are only valid for
near-isometric shape deformations. Furthermore, we propose a novel test-time
adaptation scheme to capture both pose-dominant and shape-dominant
deformations. Using different challenging datasets, we demonstrate that our
method outperforms previous state-of-the-art methods for both shape matching
and interpolation, even compared to supervised approaches.
\\ ( https://arxiv.org/abs/2402.18920 ,  43301kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18929 (*cross-listing*)
Date: Thu, 29 Feb 2024 07:44:31 GMT   (24543kb,D)

Title: Navigating Beyond Dropout: An Intriguing Solution Towards Generalizable
  Image Super Resolution
Authors: Hongjun Wang, Jiyuan Chen, Yinqiang Zheng, Tieyong Zeng
Categories: cs.CV cs.AI
\\
  Deep learning has led to a dramatic leap on Single Image Super-Resolution
(SISR) performances in recent years. %Despite the substantial advancement%
While most existing work assumes a simple and fixed degradation model (e.g.,
bicubic downsampling), the research of Blind SR seeks to improve model
generalization ability with unknown degradation. Recently, Kong et al pioneer
the investigation of a more suitable training strategy for Blind SR using
Dropout. Although such method indeed brings substantial generalization
improvements via mitigating overfitting, we argue that Dropout simultaneously
introduces undesirable side-effect that compromises model's capacity to
faithfully reconstruct fine details. We show both the theoretical and
experimental analyses in our paper, and furthermore, we present another easy
yet effective training strategy that enhances the generalization ability of the
model by simply modulating its first and second-order features statistics.
Experimental results have shown that our method could serve as a model-agnostic
regularization and outperforms Dropout on seven benchmark datasets including
both synthetic and real-world scenarios.
\\ ( https://arxiv.org/abs/2402.18929 ,  24543kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18945 (*cross-listing*)
Date: Thu, 29 Feb 2024 08:20:49 GMT   (5298kb,D)

Title: Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on
  Pre-trained Language Models
Authors: Pengzhou Cheng, Wei Du, Zongru Wu, Fengwei Zhang, Libo Chen and
  Gongshen Liu
Categories: cs.CR cs.AI cs.CL
Comments: 16 pages, 16 figures, 13 tables
\\
  Pre-trained language models (PLMs) have been found susceptible to backdoor
attacks, which can transfer vulnerabilities to various downstream tasks.
However, existing PLM backdoors are conducted with explicit triggers under the
manually aligned, thus failing to satisfy expectation goals simultaneously in
terms of effectiveness, stealthiness, and universality. In this paper, we
propose a novel approach to achieve invisible and general backdoor
implantation, called \textbf{Syntactic Ghost} (synGhost for short).
Specifically, the method hostilely manipulates poisoned samples with different
predefined syntactic structures as stealth triggers and then implants the
backdoor to pre-trained representation space without disturbing the primitive
knowledge. The output representations of poisoned samples are distributed as
uniformly as possible in the feature space via contrastive learning, forming a
wide range of backdoors. Additionally, in light of the unique properties of
syntactic triggers, we introduce an auxiliary module to drive the PLMs to learn
this knowledge in priority, which can alleviate the interference between
different syntactic structures. Experiments show that our method outperforms
the previous methods and achieves the predefined objectives. Not only do severe
threats to various natural language understanding (NLU) tasks on two tuning
paradigms but also to multiple PLMs. Meanwhile, the synGhost is imperceptible
against three countermeasures based on perplexity, fine-pruning, and the
proposed maxEntropy.
\\ ( https://arxiv.org/abs/2402.18945 ,  5298kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18960 (*cross-listing*)
Date: Thu, 29 Feb 2024 08:59:51 GMT   (2061kb,D)

Title: Towards Out-of-Distribution Detection for breast cancer classification
  in Point-of-Care Ultrasound Imaging
Authors: Jennie Karlsson, Marisa Wodrich, Niels Christian Overgaard, Freja
  Sahlin, Kristina L{\aa}ng, Anders Heyden, Ida Arvidsson
Categories: cs.CV cs.AI
\\
  Deep learning has shown to have great potential in medical applications. In
critical domains as such, it is of high interest to have trustworthy algorithms
which are able to tell when reliable assessments cannot be guaranteed.
Detecting out-of-distribution (OOD) samples is a crucial step towards building
a safe classifier. Following a previous study, showing that it is possible to
classify breast cancer in point-of-care ultrasound images, this study
investigates OOD detection using three different methods: softmax, energy score
and deep ensembles. All methods are tested on three different OOD data sets.
The results show that the energy score method outperforms the softmax method,
performing well on two of the data sets. The ensemble method is the most
robust, performing the best at detecting OOD samples for all three OOD data
sets.
\\ ( https://arxiv.org/abs/2402.18960 ,  2061kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18975 (*cross-listing*)
Date: Thu, 29 Feb 2024 09:27:40 GMT   (4341kb,D)

Title: Theoretically Achieving Continuous Representation of Oriented Bounding
  Boxes
Authors: Zikai Xiao, Guo-Ye Yang, Xue Yang, Tai-Jiang Mu, Junchi Yan, Shi-min
  Hu
Categories: cs.CV cs.AI
Comments: 15 pages, 10 tables, 8 figures. Accepted by CVPR'24. Code:
  https://github.com/Jittor/JDet
\\
  Considerable efforts have been devoted to Oriented Object Detection (OOD).
However, one lasting issue regarding the discontinuity in Oriented Bounding Box
(OBB) representation remains unresolved, which is an inherent bottleneck for
extant OOD methods. This paper endeavors to completely solve this issue in a
theoretically guaranteed manner and puts an end to the ad-hoc efforts in this
direction. Prior studies typically can only address one of the two cases of
discontinuity: rotation and aspect ratio, and often inadvertently introduce
decoding discontinuity, e.g. Decoding Incompleteness (DI) and Decoding
Ambiguity (DA) as discussed in literature. Specifically, we propose a novel
representation method called Continuous OBB (COBB), which can be readily
integrated into existing detectors e.g. Faster-RCNN as a plugin. It can
theoretically ensure continuity in bounding box regression which to our best
knowledge, has not been achieved in literature for rectangle-based object
representation. For fairness and transparency of experiments, we have developed
a modularized benchmark based on the open-source deep learning framework
Jittor's detection toolbox JDet for OOD evaluation. On the popular DOTA
dataset, by integrating Faster-RCNN as the same baseline model, our new method
outperforms the peer method Gliding Vertex by 1.13% mAP50 (relative improvement
1.54%), and 2.46% mAP75 (relative improvement 5.91%), without any tricks.
\\ ( https://arxiv.org/abs/2402.18975 ,  4341kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19002 (*cross-listing*)
Date: Thu, 29 Feb 2024 09:53:19 GMT   (4015kb,D)

Title: GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction
Authors: Ching-Lin Lee, Zhi-Xuan Wang, Kuan-Ting Lai, Amar Fadillah
Categories: cs.CV cs.AI
\\
  Predicting the future trajectories of pedestrians on the road is an important
task for autonomous driving. The pedestrian trajectory prediction is affected
by scene paths, pedestrian's intentions and decision-making, which is a
multi-modal problem. Most recent studies use past trajectories to predict a
variety of potential future trajectory distributions, which do not account for
the scene context and pedestrian targets. Instead of predicting the future
trajectory directly, we propose to use scene context and observed trajectory to
predict the goal points first, and then reuse the goal points to predict the
future trajectories. By leveraging the information from scene context and
observed trajectory, the uncertainty can be limited to a few target areas,
which represent the "goals" of the pedestrians. In this paper, we propose
GoalNet, a new trajectory prediction neural network based on the goal areas of
a pedestrian. Our network can predict both pedestrian's trajectories and
bounding boxes. The overall model is efficient and modular, and its outputs can
be changed according to the usage scenario. Experimental results show that
GoalNet significantly improves the previous state-of-the-art performance by
48.7% on the JAAD and 40.8% on the PIE dataset.
\\ ( https://arxiv.org/abs/2402.19002 ,  4015kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19027 (*cross-listing*)
Date: Thu, 29 Feb 2024 10:38:56 GMT   (1023kb,D)

Title: How to Train your Antivirus: RL-based Hardening through the
  Problem-Space
Authors: Jacopo Cortellazzi and Ilias Tsingenopoulos and Branislav
  Bo\v{s}ansk\'y and Simone Aonzo and Davy Preuveneers and Wouter Joosen and
  Fabio Pierazzi and Lorenzo Cavallaro
Categories: cs.CR cs.AI
Comments: 20 pages,4 figures
\\
  ML-based malware detection on dynamic analysis reports is vulnerable to both
evasion and spurious correlations. In this work, we investigate a specific ML
architecture employed in the pipeline of a widely-known commercial antivirus
company, with the goal to harden it against adversarial malware. Adversarial
training, the sole defensive technique that can confer empirical robustness, is
not applicable out of the box in this domain, for the principal reason that
gradient-based perturbations rarely map back to feasible problem-space
programs. We introduce a novel Reinforcement Learning approach for constructing
adversarial examples, a constituent part of adversarially training a model
against evasion. Our approach comes with multiple advantages. It performs
modifications that are feasible in the problem-space, and only those; thus it
circumvents the inverse mapping problem. It also makes possible to provide
theoretical guarantees on the robustness of the model against a particular set
of adversarial capabilities. Our empirical exploration validates our
theoretical insights, where we can consistently reach 0\% Attack Success Rate
after a few adversarial retraining iterations.
\\ ( https://arxiv.org/abs/2402.19027 ,  1023kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19041 (*cross-listing*)
Date: Thu, 29 Feb 2024 11:06:14 GMT   (5502kb,D)

Title: Atmospheric Turbulence Removal with Video Sequence Deep Visual Priors
Authors: P. Hill, N. Anantrasirichai, A. Achim, and D.R. Bull
Categories: cs.CV cs.AI eess.IV
\\
  Atmospheric turbulence poses a challenge for the interpretation and visual
perception of visual imagery due to its distortion effects. Model-based
approaches have been used to address this, but such methods often suffer from
artefacts associated with moving content. Conversely, deep learning based
methods are dependent on large and diverse datasets that may not effectively
represent any specific content. In this paper, we address these problems with a
self-supervised learning method that does not require ground truth. The
proposed method is not dependent on any dataset outside of the single data
sequence being processed but is also able to improve the quality of any input
raw sequences or pre-processed sequences. Specifically, our method is based on
an accelerated Deep Image Prior (DIP), but integrates temporal information
using pixel shuffling and a temporal sliding window. This efficiently learns
spatio-temporal priors leading to a system that effectively mitigates
atmospheric turbulence distortions. The experiments show that our method
improves visual quality results qualitatively and quantitatively.
\\ ( https://arxiv.org/abs/2402.19041 ,  5502kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19054 (*cross-listing*)
Date: Thu, 29 Feb 2024 11:31:50 GMT   (481kb,D)

Title: RobWE: Robust Watermark Embedding for Personalized Federated Learning
  Model Ownership Protection
Authors: Yang Xu, Yunlin Tan, Cheng Zhang, Kai Chi, Peng Sun, Wenyuan Yang, Ju
  Ren, Hongbo Jiang, Yaoxue Zhang
Categories: cs.CR cs.AI
\\
  Embedding watermarks into models has been widely used to protect model
ownership in federated learning (FL). However, existing methods are inadequate
for protecting the ownership of personalized models acquired by clients in
personalized FL (PFL). This is due to the aggregation of the global model in
PFL, resulting in conflicts over clients' private watermarks. Moreover,
malicious clients may tamper with embedded watermarks to facilitate model
leakage and evade accountability. This paper presents a robust watermark
embedding scheme, named RobWE, to protect the ownership of personalized models
in PFL. We first decouple the watermark embedding of personalized models into
two parts: head layer embedding and representation layer embedding. The head
layer belongs to clients' private part without participating in model
aggregation, while the representation layer is the shared part for aggregation.
For representation layer embedding, we employ a watermark slice embedding
operation, which avoids watermark embedding conflicts. Furthermore, we design a
malicious watermark detection scheme enabling the server to verify the
correctness of watermarks before aggregating local models. We conduct an
exhaustive experimental evaluation of RobWE. The results demonstrate that RobWE
significantly outperforms the state-of-the-art watermark embedding schemes in
FL in terms of fidelity, reliability, and robustness.
\\ ( https://arxiv.org/abs/2402.19054 ,  481kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19135 (*cross-listing*)
Date: Thu, 29 Feb 2024 13:12:31 GMT   (1975kb)

Title: Think Fast, Think Slow, Think Critical: Designing an Automated
  Propaganda Detection Tool
Authors: Liudmila Zavolokina, Kilian Sprenkamp, Zoya Katashinskaya, Daniel
  Gordon Jones, Gerhard Schwabe
Categories: cs.HC cs.AI
Comments: The paper is accepted for publication in proceedings of the CHI
  Conference on Human Factors in Computing Systems (2024)
DOI: 10.1145/3613904.3642805
\\
  In today's digital age, characterized by rapid news consumption and
increasing vulnerability to propaganda, fostering citizens' critical thinking
is crucial for stable democracies. This paper introduces the design of
ClarifAI, a novel automated propaganda detection tool designed to nudge readers
towards more critical news consumption by activating the analytical mode of
thinking, following Kahneman's dual-system theory of cognition. Using Large
Language Models, ClarifAI detects propaganda in news articles and provides
context-rich explanations, enhancing users' understanding and critical
thinking. Our contribution is threefold: first, we propose the design of
ClarifAI; second, in an online experiment, we demonstrate that this design
effectively encourages news readers to engage in more critical reading; and
third, we emphasize the value of explanations for fostering critical thinking.
The study thus offers both a practical tool and useful design knowledge for
mitigating propaganda in digital news.
\\ ( https://arxiv.org/abs/2402.19135 ,  1975kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19161 (*cross-listing*)
Date: Thu, 29 Feb 2024 13:45:13 GMT   (18923kb,D)

Title: MemoNav: Working Memory Model for Visual Navigation
Authors: Hongxin Li, Zeyu Wang, Xu Yang, Yuran Yang, Shuqi Mei, Zhaoxiang Zhang
Categories: cs.CV cs.AI cs.RO
Comments: Accepted to CVPR 2024
\\
  Image-goal navigation is a challenging task that requires an agent to
navigate to a goal indicated by an image in unfamiliar environments. Existing
methods utilizing diverse scene memories suffer from inefficient exploration
since they use all historical observations for decision-making without
considering the goal-relevant fraction. To address this limitation, we present
MemoNav, a novel memory model for image-goal navigation, which utilizes a
working memory-inspired pipeline to improve navigation performance.
Specifically, we employ three types of navigation memory. The node features on
a map are stored in the short-term memory (STM), as these features are
dynamically updated. A forgetting module then retains the informative STM
fraction to increase efficiency. We also introduce long-term memory (LTM) to
learn global scene representations by progressively aggregating STM features.
Subsequently, a graph attention module encodes the retained STM and the LTM to
generate working memory (WM) which contains the scene features essential for
efficient navigation. The synergy among these three memory types boosts
navigation performance by enabling the agent to learn and leverage
goal-relevant scene features within a topological map. Our evaluation on
multi-goal tasks demonstrates that MemoNav significantly outperforms previous
methods across all difficulty levels in both Gibson and Matterport3D scenes.
Qualitative results further illustrate that MemoNav plans more efficient
routes.
\\ ( https://arxiv.org/abs/2402.19161 ,  18923kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19173 (*cross-listing*)
Date: Thu, 29 Feb 2024 13:53:35 GMT   (275kb,D)

Title: StarCoder 2 and The Stack v2: The Next Generation
Authors: Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel
  Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang
  Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada,
  Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding
  Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii,
  Nii Osae Osae Dade, Wenhao Yu, Lucas Krau{\ss}, Naman Jain, Yixuan Su, Xuanli
  He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang,
  Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank
  Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas
  Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet,
  Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary,
  Nima Tajbakhsh, Yacine Jernite, Carlos Mu\~noz Ferrandis, Lingming Zhang,
  Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries
Categories: cs.SE cs.AI
\\
  The BigCode project, an open-scientific collaboration focused on the
responsible development of Large Language Models for Code (Code LLMs),
introduces StarCoder2. In partnership with Software Heritage (SWH), we build
The Stack v2 on top of the digital commons of their source code archive.
Alongside the SWH repositories spanning 619 programming languages, we carefully
select other high-quality data sources, such as GitHub pull requests, Kaggle
notebooks, and code documentation. This results in a training set that is 4x
larger than the first StarCoder dataset. We train StarCoder2 models with 3B,
7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate
them on a comprehensive set of Code LLM benchmarks. We find that our small
model, StarCoder2-3B, outperforms other Code LLMs of similar size on most
benchmarks, and also outperforms StarCoderBase-15B. Our large model,
StarCoder2- 15B, significantly outperforms other models of comparable size. In
addition, it matches or outperforms CodeLlama-34B, a model more than twice its
size. Although DeepSeekCoder- 33B is the best-performing model at code
completion for high-resource languages, we find that StarCoder2-15B outperforms
it on math and code reasoning benchmarks, as well as several low-resource
languages. We make the model weights available under an OpenRAIL license and
ensure full transparency regarding the training data by releasing the SoftWare
Heritage persistent IDentifiers (SWHIDs) of the source code data.
\\ ( https://arxiv.org/abs/2402.19173 ,  275kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19197 (*cross-listing*)
Date: Thu, 29 Feb 2024 14:26:46 GMT   (14231kb,D)

Title: Fine Structure-Aware Sampling: A New Sampling Training Scheme for
  Pixel-Aligned Implicit Models in Single-View Human Reconstruction
Authors: Kennard Yanting Chan, Fayao Liu, Guosheng Lin, Chuan Sheng Foo, Weisi
  Lin
Categories: cs.CV cs.AI cs.LG
Comments: Accepted in Proceedings of the AAAI Conference on Artificial
  Intelligence, 2024 (AAAI 2024)
\\
  Pixel-aligned implicit models, such as PIFu, PIFuHD, and ICON, are used for
single-view clothed human reconstruction. These models need to be trained using
a sampling training scheme. Existing sampling training schemes either fail to
capture thin surfaces (e.g. ears, fingers) or cause noisy artefacts in
reconstructed meshes. To address these problems, we introduce Fine
Structured-Aware Sampling (FSS), a new sampling training scheme to train
pixel-aligned implicit models for single-view human reconstruction. FSS
resolves the aforementioned problems by proactively adapting to the thickness
and complexity of surfaces. In addition, unlike existing sampling training
schemes, FSS shows how normals of sample points can be capitalized in the
training process to improve results. Lastly, to further improve the training
process, FSS proposes a mesh thickness loss signal for pixel-aligned implicit
models. It becomes computationally feasible to introduce this loss once a
slight reworking of the pixel-aligned implicit function framework is carried
out. Our results show that our methods significantly outperform SOTA methods
qualitatively and quantitatively. Our code is publicly available at
https://github.com/kcyt/FSS.
\\ ( https://arxiv.org/abs/2402.19197 ,  14231kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19237 (*cross-listing*)
Date: Wed, 21 Feb 2024 17:51:30 GMT   (2875kb)

Title: Context-based Interpretable Spatio-Temporal Graph Convolutional Network
  for Human Motion Forecasting
Authors: Edgar Medina, Leyong Loh, Namrata Gurung, Kyung Hun Oh, Niels Heller
Categories: cs.CV cs.AI
Comments: 10 pages, 6 figures
\\
  Human motion prediction is still an open problem extremely important for
autonomous driving and safety applications. Due to the complex spatiotemporal
relation of motion sequences, this remains a challenging problem not only for
movement prediction but also to perform a preliminary interpretation of the
joint connections. In this work, we present a Context-based Interpretable
Spatio-Temporal Graph Convolutional Network (CIST-GCN), as an efficient 3D
human pose forecasting model based on GCNs that encompasses specific layers,
aiding model interpretability and providing information that might be useful
when analyzing motion distribution and body behavior. Our architecture extracts
meaningful information from pose sequences, aggregates displacements and
accelerations into the input model, and finally predicts the output
displacements. Extensive experiments on Human 3.6M, AMASS, 3DPW, and ExPI
datasets demonstrate that CIST-GCN outperforms previous methods in human motion
prediction and robustness. Since the idea of enhancing interpretability for
motion prediction has its merits, we showcase experiments towards it and
provide preliminary evaluations of such insights here. available code:
https://github.com/QualityMinds/cistgcn
\\ ( https://arxiv.org/abs/2402.19237 ,  2875kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19263 (*cross-listing*)
Date: Thu, 29 Feb 2024 15:32:25 GMT   (17084kb,D)

Title: Spinal Osteophyte Detection via Robust Patch Extraction on minimally
  annotated X-rays
Authors: Soumya Snigdha Kundu, Yuanhan Mo, Nicharee Srikijkasemwat,
  Bart{\l}omiej W. Papiez
Categories: cs.CV cs.AI
Comments: ISBI'24 Full Paper
\\
  The development and progression of arthritis is strongly associated with
osteophytes, which are small and elusive bone growths. This paper presents one
of the first efforts towards automated spinal osteophyte detection in spinal
X-rays. A novel automated patch extraction process, called SegPatch, has been
proposed based on deep learning-driven vertebrae segmentation and the
enlargement of mask contours. A final patch classification accuracy of 84.5\%
is secured, surpassing a baseline tiling-based patch generation technique by
9.5%. This demonstrates that even with limited annotations, SegPatch can
deliver superior performance for detection of tiny structures such as
osteophytes. The proposed approach has potential to assist clinicians in
expediting the process of manually identifying osteophytes in spinal X-ray.
\\ ( https://arxiv.org/abs/2402.19263 ,  17084kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19339 (*cross-listing*)
Date: Thu, 29 Feb 2024 16:46:48 GMT   (31570kb,D)

Title: Stitching Gaps: Fusing Situated Perceptual Knowledge with Vision
  Transformers for High-Level Image Classification
Authors: Delfina Sol Martinez Pandiani, Nicolas Lazzari, Valentina Presutti
Categories: cs.CV cs.AI
Comments: Preprint
\\
  The increasing demand for automatic high-level image understanding,
particularly in detecting abstract concepts (AC) within images, underscores the
necessity for innovative and more interpretable approaches. These approaches
need to harmonize traditional deep vision methods with the nuanced,
context-dependent knowledge humans employ to interpret images at intricate
semantic levels. In this work, we leverage situated perceptual knowledge of
cultural images to enhance performance and interpretability in AC image
classification. We automatically extract perceptual semantic units from images,
which we then model and integrate into the ARTstract Knowledge Graph (AKG).
This resource captures situated perceptual semantics gleaned from over 14,000
cultural images labeled with ACs. Additionally, we enhance the AKG with
high-level linguistic frames. We compute KG embeddings and experiment with
relative representations and hybrid approaches that fuse these embeddings with
visual transformer embeddings. Finally, for interpretability, we conduct
posthoc qualitative analyses by examining model similarities with training
instances. Our results show that our hybrid KGE-ViT methods outperform existing
techniques in AC image classification. The posthoc interpretability analyses
reveal the visual transformer's proficiency in capturing pixel-level visual
attributes, contrasting with our method's efficacy in representing more
abstract and semantic scene elements. We demonstrate the synergy and
complementarity between KGE embeddings' situated perceptual knowledge and deep
visual model's sensory-perceptual understanding for AC image classification.
This work suggests a strong potential of neuro-symbolic methods for knowledge
integration and robust image representation for use in downstream intricate
visual comprehension tasks. All the materials and code are available online.
\\ ( https://arxiv.org/abs/2402.19339 ,  31570kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19361 (*cross-listing*)
Date: Thu, 29 Feb 2024 17:12:39 GMT   (569kb,D)

Title: Watermark Stealing in Large Language Models
Authors: Nikola Jovanovi\'c, Robin Staab, Martin Vechev
Categories: cs.CR cs.AI cs.LG
\\
  LLM watermarking has attracted attention as a promising way to detect
AI-generated content, with some works suggesting that current schemes may
already be fit for deployment. In this work we dispute this claim, identifying
watermark stealing (WS) as a fundamental vulnerability of these schemes. We
show that querying the API of the watermarked LLM to approximately
reverse-engineer a watermark enables practical spoofing attacks, as suggested
in prior work, but also greatly boosts scrubbing attacks, which was previously
unnoticed. We are the first to propose an automated WS algorithm and use it in
the first comprehensive study of spoofing and scrubbing in realistic settings.
We show that for under $50 an attacker can both spoof and scrub
state-of-the-art schemes previously considered safe, with average success rate
of over 80%. Our findings challenge common beliefs about LLM watermarking,
stressing the need for more robust schemes. We make all our code and additional
examples available at https://watermark-stealing.org.
\\ ( https://arxiv.org/abs/2402.19361 ,  569kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19366 (*cross-listing*)
Date: Thu, 29 Feb 2024 17:13:44 GMT   (547kb,D)

Title: SoK: Exploring the Potential of Large Language Models for Improving
  Digital Forensic Investigation Efficiency
Authors: Akila Wickramasekara and Frank Breitinger and Mark Scanlon
Categories: cs.CR cs.AI
\\
  The growing number of cases requiring digital forensic analysis raises
concerns about law enforcement's ability to conduct investigations promptly.
Consequently, this systemisation of knowledge paper delves into the potential
and effectiveness of integrating Large Language Models (LLMs) into digital
forensic investigation to address these challenges. A thorough literature
review is undertaken, encompassing existing digital forensic models, tools,
LLMs, deep learning techniques, and the utilisation of LLMs in investigations.
The review identifies current challenges within existing digital forensic
processes and explores both the obstacles and possibilities of incorporating
LLMs. In conclusion, the study asserts that the adoption of LLMs in digital
forensics, with appropriate constraints, holds the potential to enhance
investigation efficiency, improve traceability, and alleviate technical and
judicial barriers faced by law enforcement entities.
\\ ( https://arxiv.org/abs/2402.19366 ,  547kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19379 (*cross-listing*)
Date: Thu, 29 Feb 2024 17:27:59 GMT   (248kb,D)

Title: Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match
  Human Crowd Accuracy
Authors: Philipp Schoenegger, Indre Tuminauskaite, Peter S. Park, Philip E.
  Tetlock
Categories: cs.CY cs.AI cs.CL cs.LG
Comments: 20 pages; 13 visualizations (nine figures, four tables)
\\
  Human forecasting accuracy in practice relies on the 'wisdom of the crowd'
effect, in which predictions about future events are significantly improved by
aggregating across a crowd of individual forecasters. Past work on the
forecasting ability of large language models (LLMs) suggests that frontier
LLMs, as individual forecasters, underperform compared to the gold standard of
a human crowd forecasting tournament aggregate. In Study 1, we expand this
research by using an LLM ensemble approach consisting of a crowd of twelve
LLMs. We compare the aggregated LLM predictions on 31 binary questions to that
of a crowd of 925 human forecasters from a three-month forecasting tournament.
Our main analysis shows that the LLM crowd outperforms a simple no-information
benchmark and is statistically equivalent to the human crowd. We also observe
an acquiescence effect, with mean model predictions being significantly above
50%, despite an almost even split of positive and negative resolutions.
Moreover, in Study 2, we test whether LLM predictions (of GPT-4 and Claude 2)
can be improved by drawing on human cognitive output. We find that both models'
forecasting accuracy benefits from exposure to the median human prediction as
information, improving accuracy by between 17% and 28%: though this leads to
less accurate predictions than simply averaging human and machine forecasts.
Our results suggest that LLMs can achieve forecasting accuracy rivaling that of
human crowd forecasting tournaments: via the simple, practically applicable
method of forecast aggregation. This replicates the 'wisdom of the crowd'
effect for LLMs, and opens up their use for a variety applications throughout
society.
\\ ( https://arxiv.org/abs/2402.19379 ,  248kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19420 (*cross-listing*)
Date: Thu, 29 Feb 2024 18:16:13 GMT   (1679kb,D)

Title: Understanding Iterative Combinatorial Auction Designs via Multi-Agent
  Reinforcement Learning
Authors: Greg d'Eon, Neil Newman, Kevin Leyton-Brown
Categories: cs.GT cs.AI cs.MA
Comments: 18 pages (body) + 10 pages (acknowledgements, references, appendices)
\\
  Iterative combinatorial auctions are widely used in high stakes settings such
as spectrum auctions. Such auctions can be hard to understand analytically,
making it difficult for bidders to determine how to behave and for designers to
optimize auction rules to ensure desirable outcomes such as high revenue or
welfare. In this paper, we investigate whether multi-agent reinforcement
learning (MARL) algorithms can be used to understand iterative combinatorial
auctions, given that these algorithms have recently shown empirical success in
several other domains. We find that MARL can indeed benefit auction analysis,
but that deploying it effectively is nontrivial. We begin by describing
modelling decisions that keep the resulting game tractable without sacrificing
important features such as imperfect information or asymmetry between bidders.
We also discuss how to navigate pitfalls of various MARL algorithms, how to
overcome challenges in verifying convergence, and how to generate and interpret
multiple equilibria. We illustrate the promise of our resulting approach by
using it to evaluate a specific rule change to a clock auction, finding
substantially different auction outcomes due to complex changes in bidders'
behavior.
\\ ( https://arxiv.org/abs/2402.19420 ,  1679kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19421 (*cross-listing*)
Date: Thu, 29 Feb 2024 18:20:37 GMT   (1554kb,D)

Title: Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based
  Search Engines
Authors: Lijia Ma, Xingchen Xu, Yong Tan
Categories: cs.IR cs.AI econ.GN q-fin.EC
Comments: 38 pages, 2 figures, 7 tables
ACM-class: J.4
\\
  In the domain of digital information dissemination, search engines act as
pivotal conduits linking information seekers with providers. The advent of
chat-based search engines utilizing Large Language Models (LLMs) and Retrieval
Augmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary
leap in the search ecosystem. They demonstrate metacognitive abilities in
interpreting web information and crafting responses with human-like
understanding and creativity. Nonetheless, the intricate nature of LLMs renders
their "cognitive" processes opaque, challenging even their designers'
understanding. This research aims to dissect the mechanisms through which an
LLM-powered chat-based search engine, specifically Bing Chat, selects
information sources for its responses. To this end, an extensive dataset has
been compiled through engagements with New Bing, documenting the websites it
cites alongside those listed by the conventional search engine. Employing
natural language processing (NLP) techniques, the research reveals that Bing
Chat exhibits a preference for content that is not only readable and formally
structured, but also demonstrates lower perplexity levels, indicating a unique
inclination towards text that is predictable by the underlying LLM. Further
enriching our analysis, we procure an additional dataset through interactions
with the GPT-4 based knowledge retrieval API, unveiling a congruent text
preference between the RAG API and Bing Chat. This consensus suggests that
these text preferences intrinsically emerge from the underlying language
models, rather than being explicitly crafted by Bing Chat's developers.
Moreover, our investigation documents a greater similarity among websites cited
by RAG technologies compared to those ranked highest by conventional search
engines.
\\ ( https://arxiv.org/abs/2402.19421 ,  1554kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19422 (*cross-listing*)
Date: Thu, 29 Feb 2024 18:21:54 GMT   (7221kb,D)

Title: PEM: Prototype-based Efficient MaskFormer for Image Segmentation
Authors: Niccol\`o Cavagnero, Gabriele Rosi, Claudia Ruttano, Francesca
  Pistilli, Marco Ciccone, Giuseppe Averta, Fabio Cermelli
Categories: cs.CV cs.AI
Comments: 8 pages, 3 figures, CVPR 2024
\\
  Recent transformer-based architectures have shown impressive results in the
field of image segmentation. Thanks to their flexibility, they obtain
outstanding performance in multiple segmentation tasks, such as semantic and
panoptic, under a single unified framework. To achieve such impressive
performance, these architectures employ intensive operations and require
substantial computational resources, which are often not available, especially
on edge devices. To fill this gap, we propose Prototype-based Efficient
MaskFormer (PEM), an efficient transformer-based architecture that can operate
in multiple segmentation tasks. PEM proposes a novel prototype-based
cross-attention which leverages the redundancy of visual features to restrict
the computation and improve the efficiency without harming the performance. In
addition, PEM introduces an efficient multi-scale feature pyramid network,
capable of extracting features that have high semantic content in an efficient
way, thanks to the combination of deformable convolutions and context-based
self-modulation. We benchmark the proposed PEM architecture on two tasks,
semantic and panoptic segmentation, evaluated on two different datasets,
Cityscapes and ADE20K. PEM demonstrates outstanding performance on every task
and dataset, outperforming task-specific architectures while being comparable
and even better than computationally-expensive baselines.
\\ ( https://arxiv.org/abs/2402.19422 ,  7221kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19423 (*cross-listing*)
Date: Thu, 29 Feb 2024 18:22:12 GMT   (530kb,D)

Title: Leveraging AI Predicted and Expert Revised Annotations in Interactive
  Segmentation: Continual Tuning or Full Training?
Authors: Tiezheng Zhang, Xiaoxi Chen, Chongyu Qu, Alan Yuille, Zongwei Zhou
Categories: cs.CV cs.AI
Comments: IEEE International Symposium on Biomedical Imaging (ISBI)
\\
  Interactive segmentation, an integration of AI algorithms and human
expertise, premises to improve the accuracy and efficiency of curating
large-scale, detailed-annotated datasets in healthcare. Human experts revise
the annotations predicted by AI, and in turn, AI improves its predictions by
learning from these revised annotations. This interactive process continues to
enhance the quality of annotations until no major revision is needed from
experts. The key challenge is how to leverage AI predicted and expert revised
annotations to iteratively improve the AI. Two problems arise: (1) The risk of
catastrophic forgetting--the AI tends to forget the previously learned classes
if it is only retrained using the expert revised classes. (2) Computational
inefficiency when retraining the AI using both AI predicted and expert revised
annotations; moreover, given the dominant AI predicted annotations in the
dataset, the contribution of newly revised annotations--often account for a
very small fraction--to the AI training remains marginal. This paper proposes
Continual Tuning to address the problems from two perspectives: network design
and data reuse. Firstly, we design a shared network for all classes followed by
class-specific networks dedicated to individual classes. To mitigate
forgetting, we freeze the shared network for previously learned classes and
only update the class-specific network for revised classes. Secondly, we reuse
a small fraction of data with previous annotations to avoid over-computing. The
selection of such data relies on the importance estimate of each data. The
importance score is computed by combining the uncertainty and consistency of AI
predictions. Our experiments demonstrate that Continual Tuning achieves a speed
16x greater than repeatedly training AI from scratch without compromising the
performance.
\\ ( https://arxiv.org/abs/2402.19423 ,  530kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19431 (*cross-listing*)
Date: Thu, 29 Feb 2024 18:27:27 GMT   (2336kb,D)

Title: Compositional API Recommendation for Library-Oriented Code Generation
Authors: Zexiong Ma, Shengnan An, Bing Xie, Zeqi Lin
Categories: cs.SE cs.AI cs.CL
Journal-ref: 32nd IEEE/ACM International Conference on Program Comprehension
  (ICPC 2024), Apr 2024, Lisboa, Portugal
DOI: 10.1145/3643916.3644403
\\
  Large language models (LLMs) have achieved exceptional performance in code
generation. However, the performance remains unsatisfactory in generating
library-oriented code, especially for the libraries not present in the training
data of LLMs. Previous work utilizes API recommendation technology to help LLMs
use libraries: it retrieves APIs related to the user requirements, then
leverages them as context to prompt LLMs. However, developmental requirements
can be coarse-grained, requiring a combination of multiple fine-grained APIs.
This granularity inconsistency makes API recommendation a challenging task. To
address this, we propose CAPIR (Compositional API Recommendation), which adopts
a "divide-and-conquer" strategy to recommend APIs for coarse-grained
requirements. Specifically, CAPIR employs an LLM-based Decomposer to break down
a coarse-grained task description into several detailed subtasks. Then, CAPIR
applies an embedding-based Retriever to identify relevant APIs corresponding to
each subtask. Moreover, CAPIR leverages an LLM-based Reranker to filter out
redundant APIs and provides the final recommendation. To facilitate the
evaluation of API recommendation methods on coarse-grained requirements, we
present two challenging benchmarks, RAPID (Recommend APIs based on
Documentation) and LOCG (Library-Oriented Code Generation). Experimental
results on these benchmarks, demonstrate the effectiveness of CAPIR in
comparison to existing baselines. Specifically, on RAPID's Torchdata-AR
dataset, compared to the state-of-the-art API recommendation approach, CAPIR
improves recall@5 from 18.7% to 43.2% and precision@5 from 15.5% to 37.1%. On
LOCG's Torchdata-Code dataset, compared to code generation without API
recommendation, CAPIR improves pass@100 from 16.0% to 28.0%.
\\ ( https://arxiv.org/abs/2402.19431 ,  2336kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19443 (*cross-listing*)
Date: Thu, 29 Feb 2024 18:43:53 GMT   (151kb,D)

Title: Probing the Information Encoded in Neural-based Acoustic Models of
  Automatic Speech Recognition Systems
Authors: Quentin Raymondaud, Mickael Rouvier, Richard Dufour
Categories: cs.SD cs.AI eess.AS
\\
  Deep learning architectures have made significant progress in terms of
performance in many research areas. The automatic speech recognition (ASR)
field has thus benefited from these scientific and technological advances,
particularly for acoustic modeling, now integrating deep neural network
architectures. However, these performance gains have translated into increased
complexity regarding the information learned and conveyed through these
black-box architectures. Following many researches in neural networks
interpretability, we propose in this article a protocol that aims to determine
which and where information is located in an ASR acoustic model (AM). To do so,
we propose to evaluate AM performance on a determined set of tasks using
intermediate representations (here, at different layer levels). Regarding the
performance variation and targeted tasks, we can emit hypothesis about which
information is enhanced or perturbed at different architecture steps.
Experiments are performed on both speaker verification, acoustic environment
classification, gender classification, tempo-distortion detection systems and
speech sentiment/emotion identification. Analysis showed that neural-based AMs
hold heterogeneous information that seems surprisingly uncorrelated with
phoneme recognition, such as emotion, sentiment or speaker identity. The
low-level hidden layers globally appears useful for the structuring of
information while the upper ones would tend to delete useless information for
phoneme recognition.
\\ ( https://arxiv.org/abs/2402.19443 ,  151kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19475 (*cross-listing*)
Date: Thu, 29 Feb 2024 18:59:25 GMT   (638kb,D)

Title: The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of
  Their Incorrect Generations?
Authors: Alex Gu, Wen-Ding Li, Naman Jain, Theo X. Olausson, Celine Lee,
  Koushik Sen, Armando Solar-Lezama
Categories: cs.SE cs.AI cs.LG
Comments: 54 pages, 25 figures
\\
  While language models are increasingly more proficient at code generation,
they still frequently generate incorrect programs. Many of these programs are
obviously wrong, but others are more subtle and pass weaker correctness checks
such as being able to compile. In this work, we focus on these counterfeit
samples: programs sampled from a language model that 1) have a high enough
log-probability to be generated at a moderate temperature and 2) pass weak
correctness checks. Overall, we discover that most models have a very shallow
understanding of counterfeits through three clear failure modes. First, models
mistakenly classify them as correct. Second, models are worse at reasoning
about the execution behaviour of counterfeits and often predict their execution
results as if they were correct. Third, when asking models to fix counterfeits,
the likelihood of a model successfully repairing a counterfeit is often even
lower than that of sampling a correct program from scratch. Counterfeits also
have very unexpected properties: first, counterfeit programs for problems that
are easier for a model to solve are not necessarily easier to detect and only
slightly easier to execute and repair. Second, counterfeits from a given model
are just as confusing to the model itself as they are to other models. Finally,
both strong and weak models are able to generate counterfeit samples that
equally challenge all models. In light of our findings, we recommend that care
and caution be taken when relying on models to understand their own samples,
especially when no external feedback is incorporated.
\\ ( https://arxiv.org/abs/2402.19475 ,  638kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18695 (*cross-listing*)
Date: Wed, 28 Feb 2024 20:22:17 GMT   (6576kb,D)

Title: Grounding Language Models for Visual Entity Recognition
Authors: Zilin Xiao, Ming Gong, Paola Cascante-Bonilla, Xingyao Zhang, Jie Wu,
  Vicente Ordonez
Categories: cs.CV cs.CL
\\
  We introduce AutoVER, an Autoregressive model for Visual Entity Recognition.
Our model extends an autoregressive Multi-modal Large Language Model by
employing retrieval augmented constrained generation. It mitigates low
performance on out-of-domain entities while excelling in queries that require
visually-situated reasoning. Our method learns to distinguish similar entities
within a vast label space by contrastively training on hard negative pairs in
parallel with a sequence-to-sequence objective without an external retriever.
During inference, a list of retrieved candidate answers explicitly guides
language generation by removing invalid decoding paths. The proposed method
achieves significant improvements across different dataset splits in the
recently proposed Oven-Wiki benchmark. Accuracy on the Entity seen split rises
from 32.7% to 61.5%. It also demonstrates superior performance on the unseen
and query splits by a substantial double-digit margin.
\\ ( https://arxiv.org/abs/2402.18695 ,  6576kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18789 (*cross-listing*)
Date: Thu, 29 Feb 2024 01:33:08 GMT   (2823kb,D)

Title: FlexLLM: A System for Co-Serving Large Language Model Inference and
  Parameter-Efficient Finetuning
Authors: Xupeng Miao, Gabriele Oliaro, Xinhao Cheng, Mengdi Wu, Colin Unger,
  Zhihao Jia
Categories: cs.DC cs.CL cs.LG
\\
  Parameter-efficient finetuning (PEFT) is a widely used technique to adapt
large language models for different tasks. Service providers typically create
separate systems for users to perform PEFT model finetuning and inference
tasks. This is because existing systems cannot handle workloads that include a
mix of inference and PEFT finetuning requests. As a result, shared GPU
resources are underutilized, leading to inefficiencies. To address this
problem, we present FlexLLM, the first system that can serve inference and
parameter-efficient finetuning requests in the same iteration. Our system
leverages the complementary nature of these two tasks and utilizes shared GPU
resources to run them jointly, using a method called co-serving. To achieve
this, FlexLLM introduces a novel token-level finetuning mechanism, which breaks
down the finetuning computation of a sequence into smaller token-level
computations and uses dependent parallelization and graph pruning, two static
compilation optimizations, to minimize the memory overhead and latency for
co-serving. Compared to existing systems, FlexLLM's co-serving approach reduces
the activation GPU memory overhead by up to 8x, and the end-to-end GPU memory
requirement of finetuning by up to 36% while maintaining a low inference
latency and improving finetuning throughput. For example, under a heavy
inference workload, FlexLLM can still preserve more than 80% of the peak
finetuning throughput, whereas existing systems cannot make any progress with
finetuning. The source code of FlexLLM is publicly available at
https://github.com/flexflow/FlexFlow.
\\ ( https://arxiv.org/abs/2402.18789 ,  2823kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18797 (*cross-listing*)
Date: Thu, 29 Feb 2024 01:58:49 GMT   (2316kb,D)

Title: ARTiST: Automated Text Simplification for Task Guidance in Augmented
  Reality
Authors: Guande Wu, Jing Qian, Sonia Castelo, Shaoyu Chen, Joao Rulff, Claudio
  Silva
Categories: cs.HC cs.CL
Comments: Conditionally accepted by CHI '24
ACM-class: H.1.2; I.2.7
DOI: 10.1145/3613904.3642669
\\
  Text presented in augmented reality provides in-situ, real-time information
for users. However, this content can be challenging to apprehend quickly when
engaging in cognitively demanding AR tasks, especially when it is presented on
a head-mounted display. We propose ARTiST, an automatic text simplification
system that uses a few-shot prompt and GPT-3 models to specifically optimize
the text length and semantic content for augmented reality. Developed out of a
formative study that included seven users and three experts, our system
combines a customized error calibration model with a few-shot prompt to
integrate the syntactic, lexical, elaborative, and content simplification
techniques, and generate simplified AR text for head-worn displays. Results
from a 16-user empirical study showed that ARTiST lightens the cognitive load
and improves performance significantly over both unmodified text and text
modified via traditional methods. Our work constitutes a step towards
automating the optimization of batch text data for readability and performance
in augmented reality.
\\ ( https://arxiv.org/abs/2402.18797 ,  2316kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19119 (*cross-listing*)
Date: Thu, 29 Feb 2024 12:56:18 GMT   (45287kb,D)

Title: VIXEN: Visual Text Comparison Network for Image Difference Captioning
Authors: Alexander Black and Jing Shi and Yifei Fai and Tu Bui and John
  Collomosse
Categories: cs.CV cs.CL
Comments: AAAI 2024
\\
  We present VIXEN - a technique that succinctly summarizes in text the visual
differences between a pair of images in order to highlight any content
manipulation present. Our proposed network linearly maps image features in a
pairwise manner, constructing a soft prompt for a pretrained large language
model. We address the challenge of low volume of training data and lack of
manipulation variety in existing image difference captioning (IDC) datasets by
training on synthetically manipulated images from the recent InstructPix2Pix
dataset generated via prompt-to-prompt editing framework. We augment this
dataset with change summaries produced via GPT-3. We show that VIXEN produces
state-of-the-art, comprehensible difference captions for diverse image contents
and edit types, offering a potential mitigation against misinformation
disseminated via manipulated image content. Code and data are available at
http://github.com/alexblck/vixen
\\ ( https://arxiv.org/abs/2402.19119 ,  45287kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19200 (*cross-listing*)
Date: Thu, 29 Feb 2024 14:30:28 GMT   (8404kb,D)

Title: PRSA: Prompt Reverse Stealing Attacks against Large Language Models
Authors: Yong Yang, Xuhong Zhang, Yi Jiang, Xi Chen, Haoyu Wang, Shouling Ji,
  Zonghui Wang
Categories: cs.CR cs.CL
\\
  Prompt, recognized as crucial intellectual property, enables large language
models (LLMs) to perform specific tasks without the need of fine-tuning,
underscoring their escalating importance. With the rise of prompt-based
services, such as prompt marketplaces and LLM applications, providers often
display prompts' capabilities through input-output examples to attract users.
However, this paradigm raises a pivotal security concern: does the exposure of
input-output pairs pose the risk of potential prompt leakage, infringing on the
intellectual property rights of the developers? To our knowledge, this problem
still has not been comprehensively explored yet. To remedy this gap, in this
paper, we perform the first in depth exploration and propose a novel attack
framework for reverse-stealing prompts against commercial LLMs, namely PRSA.
The main idea of PRSA is that by analyzing the critical features of the
input-output pairs, we mimic and gradually infer (steal) the target prompts. In
detail, PRSA mainly consists of two key phases: prompt mutation and prompt
pruning. In the mutation phase, we propose a prompt attention algorithm based
on differential feedback to capture these critical features for effectively
inferring the target prompts. In the prompt pruning phase, we identify and mask
the words dependent on specific inputs, enabling the prompts to accommodate
diverse inputs for generalization. Through extensive evaluation, we verify that
PRSA poses a severe threat in real world scenarios. We have reported these
findings to prompt service providers and actively collaborate with them to take
protective measures for prompt copyright.
\\ ( https://arxiv.org/abs/2402.19200 ,  8404kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19404 (*cross-listing*)
Date: Thu, 29 Feb 2024 18:03:00 GMT   (4463kb,D)

Title: Entity-Aware Multimodal Alignment Framework for News Image Captioning
Authors: Junzhe Zhang and Huixuan Zhang and Xiaojun Wan
Categories: cs.CV cs.CL
\\
  News image captioning task is a variant of image captioning task which
requires model to generate a more informative caption with news image and the
associated news article. Multimodal Large Language models have developed
rapidly in recent years and is promising in news image captioning task.
However, according to our experiments, common MLLMs are not good at generating
the entities in zero-shot setting. Their abilities to deal with the entities
information are still limited after simply fine-tuned on news image captioning
dataset. To obtain a more powerful model to handle the multimodal entity
information, we design two multimodal entity-aware alignment tasks and an
alignment framework to align the model and generate the news image captions.
Our method achieves better results than previous state-of-the-art models in
CIDEr score (72.33 -> 86.29) on GoodNews dataset and (70.83 -> 85.61) on
NYTimes800k dataset.
\\ ( https://arxiv.org/abs/2402.19404 ,  4463kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19411 (*cross-listing*)
Date: Thu, 29 Feb 2024 18:09:03 GMT   (611kb)

Title: PaECTER: Patent-level Representation Learning using Citation-informed
  Transformers
Authors: Mainak Ghosh, Sebastian Erhardt, Michael E. Rose, Erik Buunk, Dietmar
  Harhoff
Categories: cs.IR cs.CL cs.LG
Comments: 7 pages, 3 figures
\\
  PaECTER is a publicly available, open-source document-level encoder specific
for patents. We fine-tune BERT for Patents with examiner-added citation
information to generate numerical representations for patent documents. PaECTER
performs better in similarity tasks than current state-of-the-art models used
in the patent domain. More specifically, our model outperforms the next-best
patent specific pre-trained language model (BERT for Patents) on our patent
citation prediction test dataset on two different rank evaluation metrics.
PaECTER predicts at least one most similar patent at a rank of 1.32 on average
when compared against 25 irrelevant patents. Numerical representations
generated by PaECTER from patent text can be used for downstream tasks such as
classification, tracing knowledge flows, or semantic similarity search.
Semantic similarity search is especially relevant in the context of prior art
search for both inventors and patent examiners. PaECTER is available on Hugging
Face.
\\ ( https://arxiv.org/abs/2402.19411 ,  611kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19462 (*cross-listing*)
Date: Thu, 29 Feb 2024 18:54:46 GMT   (5391kb,D)

Title: Accelerating materials discovery for polymer solar cells: Data-driven
  insights enabled by natural language processing
Authors: Pranav Shetty, Aishat Adeboye, Sonakshi Gupta, Chao Zhang, Rampi
  Ramprasad
Categories: cond-mat.mtrl-sci cs.CL physics.app-ph
\\
  We present a natural language processing pipeline that was used to extract
polymer solar cell property data from the literature and simulate various
active learning strategies. While data-driven methods have been well
established to discover novel materials faster than Edisonian trial-and-error
approaches, their benefits have not been quantified. Our approach demonstrates
a potential reduction in discovery time by approximately 75 %, equivalent to a
15 year acceleration in material innovation. Our pipeline enables us to extract
data from more than 3300 papers which is ~5 times larger than similar data sets
reported by others. We also trained machine learning models to predict the
power conversion efficiency and used our model to identify promising
donor-acceptor combinations that are as yet unreported. We thus demonstrate a
workflow that goes from published literature to extracted material property
data which in turn is used to obtain data-driven insights. Our insights include
active learning strategies that can simultaneously optimize the material system
and train strong predictive models of material properties. This work provides a
valuable framework for research in material science.
\\ ( https://arxiv.org/abs/2402.19462 ,  5391kb)
------------------------------------------------------------------------------
\\
arXiv:2112.01799 (*cross-listing*)
Date: Fri, 3 Dec 2021 09:09:34 GMT   (29839kb,D)

Title: Global Context with Discrete Diffusion in Vector Quantised Modelling for
  Image Generation
Authors: Minghui Hu, Yujie Wang, Tat-Jen Cham, Jianfei Yang, P.N.Suganthan
Categories: cs.CV cs.LG
\\
  The integration of Vector Quantised Variational AutoEncoder (VQ-VAE) with
autoregressive models as generation part has yielded high-quality results on
image generation. However, the autoregressive models will strictly follow the
progressive scanning order during the sampling phase. This leads the existing
VQ series models to hardly escape the trap of lacking global information.
Denoising Diffusion Probabilistic Models (DDPM) in the continuous domain have
shown a capability to capture the global context, while generating high-quality
images. In the discrete state space, some works have demonstrated the potential
to perform text generation and low resolution image generation. We show that
with the help of a content-rich discrete visual codebook from VQ-VAE, the
discrete diffusion model can also generate high fidelity images with global
context, which compensates for the deficiency of the classical autoregressive
model along pixel space. Meanwhile, the integration of the discrete VAE with
the diffusion model resolves the drawback of conventional autoregressive models
being oversized, and the diffusion model which demands excessive time in the
sampling process when generating images. It is found that the quality of the
generated images is heavily dependent on the discrete visual codebook.
Extensive experiments demonstrate that the proposed Vector Quantised Discrete
Diffusion Model (VQ-DDM) is able to achieve comparable performance to top-tier
methods with low complexity. It also demonstrates outstanding advantages over
other vectors quantised with autoregressive models in terms of image inpainting
tasks without additional training.
\\ ( https://arxiv.org/abs/2112.01799 ,  29839kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00964 (*cross-listing*)
Date: Thu, 1 Jun 2023 17:55:32 GMT   (26239kb,D)

Title: Cocktail: Mixing Multi-Modality Controls for Text-Conditional Image
  Generation
Authors: Minghui Hu, Jianbin Zheng, Daqing Liu, Chuanxia Zheng, Chaoyue Wang,
  Dacheng Tao, Tat-Jen Cham
Categories: cs.CV cs.LG
Comments: Project Page: https://mhh0318.github.io/cocktail/
\\
  Text-conditional diffusion models are able to generate high-fidelity images
with diverse contents. However, linguistic representations frequently exhibit
ambiguous descriptions of the envisioned objective imagery, requiring the
incorporation of additional control signals to bolster the efficacy of
text-guided diffusion models. In this work, we propose Cocktail, a pipeline to
mix various modalities into one embedding, amalgamated with a generalized
ControlNet (gControlNet), a controllable normalisation (ControlNorm), and a
spatial guidance sampling method, to actualize multi-modal and
spatially-refined control for text-conditional diffusion models. Specifically,
we introduce a hyper-network gControlNet, dedicated to the alignment and
infusion of the control signals from disparate modalities into the pre-trained
diffusion model. gControlNet is capable of accepting flexible modality signals,
encompassing the simultaneous reception of any combination of modality signals,
or the supplementary fusion of multiple modality signals. The control signals
are then fused and injected into the backbone model according to our proposed
ControlNorm. Furthermore, our advanced spatial guidance sampling methodology
proficiently incorporates the control signal into the designated region,
thereby circumventing the manifestation of undesired objects within the
generated image. We demonstrate the results of our method in controlling
various modalities, proving high-quality synthesis and fidelity to multiple
external signals.
\\ ( https://arxiv.org/abs/2306.00964 ,  26239kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13429 (*cross-listing*)
Date: Tue, 20 Feb 2024 23:45:37 GMT   (869kb)

Title: Everything You Always Wanted to Know About Storage Compressibility of
  Pre-Trained ML Models but Were Afraid to Ask
Authors: Zhaoyuan Su, Ammar Ahmed, Zirui Wang, Ali Anwar, Yue Cheng
Categories: cs.DB cs.LG
Comments: This paper presents the first, exhaustive analysis to date of PTM
  datasets on storage compressibility. Motivated by our findings, we design
  ELF, a simple yet effective, error-bounded, lossy floating-point compression
  method
ACM-class: H.2.7
\\
  As the number of pre-trained machine learning (ML) models is growing
exponentially, data reduction tools are not catching up. Existing data
reduction techniques are not specifically designed for pre-trained model (PTM)
dataset files. This is largely due to a lack of understanding of the patterns
and characteristics of these datasets, especially those relevant to data
reduction and compressibility.
  This paper presents the first, exhaustive analysis to date of PTM datasets on
storage compressibility. Our analysis spans different types of data reduction
and compression techniques, from hash-based data deduplication, data similarity
detection, to dictionary-coding compression. Our analysis explores these
techniques at three data granularity levels, from model layers, model chunks,
to model parameters. We draw new observations that indicate that modern data
reduction tools are not effective when handling PTM datasets. There is a
pressing need for new compression methods that take into account PTMs' data
characteristics for effective storage reduction.
  Motivated by our findings, we design ELF, a simple yet effective,
error-bounded, lossy floating-point compression method. ELF transforms
floating-point parameters in such a way that the common exponent field of the
transformed parameters can be completely eliminated to save storage space. We
develop Elves, a compression framework that integrates ELF along with several
other data reduction methods. Elves uses the most effective method to compress
PTMs that exhibit different patterns. Evaluation shows that Elves achieves an
overall compression ratio of $1.52\times$, which is $1.31\times$, $1.32\times$
and $1.29\times$ higher than a general-purpose compressor (zstd), an
error-bounded lossy compressor (SZ3), and the uniform model quantization,
respectively, with negligible model accuracy loss.
\\ ( https://arxiv.org/abs/2402.13429 ,  869kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18583 (*cross-listing*)
Date: Mon, 15 Jan 2024 00:34:00 GMT   (3172kb,D)

Title: Binding-Adaptive Diffusion Models for Structure-Based Drug Design
Authors: Zhilin Huang, Ling Yang, Zaixi Zhang, Xiangxin Zhou, Yu Bao, Xiawu
  Zheng, Yuwei Yang, Yu Wang, Wenming Yang
Categories: q-bio.BM cs.LG
Comments: Accepted by AAAI 2024. Project:
  https://github.com/YangLing0818/BindDM
\\
  Structure-based drug design (SBDD) aims to generate 3D ligand molecules that
bind to specific protein targets. Existing 3D deep generative models including
diffusion models have shown great promise for SBDD. However, it is complex to
capture the essential protein-ligand interactions exactly in 3D space for
molecular generation. To address this problem, we propose a novel framework,
namely Binding-Adaptive Diffusion Models (BindDM). In BindDM, we adaptively
extract subcomplex, the essential part of binding sites responsible for
protein-ligand interactions. Then the selected protein-ligand subcomplex is
processed with SE(3)-equivariant neural networks, and transmitted back to each
atom of the complex for augmenting the target-aware 3D molecule diffusion
generation with binding interaction information. We iterate this hierarchical
complex-subcomplex process with cross-hierarchy interaction node for adequately
fusing global binding context between the complex and its corresponding
subcomplex. Empirical studies on the CrossDocked2020 dataset show BindDM can
generate molecules with more realistic 3D structures and higher binding
affinities towards the protein targets, with up to -5.92 Avg. Vina Score, while
maintaining proper molecular properties. Our code is available at
https://github.com/YangLing0818/BindDM
\\ ( https://arxiv.org/abs/2402.18583 ,  3172kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18595 (*cross-listing*)
Date: Sun, 25 Feb 2024 09:35:30 GMT   (4283kb,D)

Title: EncodingNet: A Novel Encoding-based MAC Design for Efficient Neural
  Network Acceleration
Authors: Bo Liu, Grace Li Zhang, Xunzhao Yin, Ulf Schlichtmann, Bing Li
Categories: cs.AR cs.CE cs.LG
\\
  Deep neural networks (DNNs) have achieved great breakthroughs in many fields
such as image classification and natural language processing. However, the
execution of DNNs needs to conduct massive numbers of multiply-accumulate (MAC)
operations on hardware and thus incurs a large power consumption. To address
this challenge, we propose a novel digital MAC design based on encoding. In
this new design, the multipliers are replaced by simple logic gates to project
the results onto a wide bit representation. These bits carry individual
position weights, which can be trained for specific neural networks to enhance
inference accuracy. The outputs of the new multipliers are added by bit-wise
weighted accumulation and the accumulation results are compatible with existing
computing platforms accelerating neural networks with either uniform or
non-uniform quantization. Since the multiplication function is replaced by
simple logic projection, the critical paths in the resulting circuits become
much shorter. Correspondingly, pipelining stages in the MAC array can be
reduced, leading to a significantly smaller area as well as a better power
efficiency. The proposed design has been synthesized and verified by
ResNet18-Cifar10, ResNet20-Cifar100 and ResNet50-ImageNet. The experimental
results confirmed the reduction of circuit area by up to 79.63% and the
reduction of power consumption of executing DNNs by up to 70.18%, while the
accuracy of the neural networks can still be well maintained.
\\ ( https://arxiv.org/abs/2402.18595 ,  4283kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18611 (*cross-listing*)
Date: Wed, 28 Feb 2024 15:15:38 GMT   (1432kb,D)

Title: HemaGraph: Breaking Barriers in Hematologic Single Cell Classification
  with Graph Attention
Authors: Lorenzo Bini, Fatemeh Nassajian Mojarrad, Thomas Matthes, St\'ephane
  Marchand-Maillet
Categories: q-bio.QM cs.LG q-bio.CB
\\
  In the realm of hematologic cell populations classification, the intricate
patterns within flow cytometry data necessitate advanced analytical tools. This
paper presents 'HemaGraph', a novel framework based on Graph Attention Networks
(GATs) for single-cell multi-class classification of hematological cells from
flow cytometry data. Harnessing the power of GATs, our method captures subtle
cell relationships, offering highly accurate patient profiling. Based on
evaluation of data from 30 patients, HemaGraph demonstrates classification
performance across five different cell classes, outperforming traditional
methodologies and state-of-the-art methods. Moreover, the uniqueness of this
framework lies in the training and testing phase of HemaGraph, where it has
been applied for extremely large graphs, containing up to hundreds of thousands
of nodes and two million edges, to detect low frequency cell populations (e.g.
0.01% for one population), with accuracies reaching 98%. Our findings
underscore the potential of HemaGraph in improving hematoligic multi-class
classification, paving the way for patient-personalized interventions. To the
best of our knowledge, this is the first effort to use GATs, and Graph Neural
Networks (GNNs) in general, to classify cell populations from single-cell flow
cytometry data. We envision applying this method to single-cell data from
larger cohort of patients and on other hematologic diseases.
\\ ( https://arxiv.org/abs/2402.18611 ,  1432kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18612 (*cross-listing*)
Date: Wed, 28 Feb 2024 15:29:02 GMT   (5956kb)

Title: Understanding random forests and overfitting: a visualization and
  simulation study
Authors: Lasai Barre\~nada, Paula Dhiman, Dirk Timmerman, Anne-Laure
  Boulesteix, Ben Van Calster
Categories: stat.ME cs.CY cs.LG
Comments: 20 pages, 8 figures
\\
  Random forests have become popular for clinical risk prediction modelling. In
a case study on predicting ovarian malignancy, we observed training
c-statistics close to 1. Although this suggests overfitting, performance was
competitive on test data. We aimed to understand the behaviour of random
forests by (1) visualizing data space in three real world case studies and (2)
a simulation study. For the case studies, risk estimates were visualised using
heatmaps in a 2-dimensional subspace. The simulation study included 48 logistic
data generating mechanisms (DGM), varying the predictor distribution, the
number of predictors, the correlation between predictors, the true c-statistic
and the strength of true predictors. For each DGM, 1000 training datasets of
size 200 or 4000 were simulated and RF models trained with minimum node size 2
or 20 using ranger package, resulting in 192 scenarios in total. The
visualizations suggested that the model learned spikes of probability around
events in the training set. A cluster of events created a bigger peak, isolated
events local peaks. In the simulation study, median training c-statistics were
between 0.97 and 1 unless there were 4 or 16 binary predictors with minimum
node size 20. Median test c-statistics were higher with higher events per
variable, higher minimum node size, and binary predictors. Median training
slopes were always above 1, and were not correlated with median test slopes
across scenarios (correlation -0.11). Median test slopes were higher with
higher true c-statistic, higher minimum node size, and higher sample size.
Random forests learn local probability peaks that often yield near perfect
training c-statistics without strongly affecting c-statistics on test data.
When the aim is probability estimation, the simulation results go against the
common recommendation to use fully grown trees in random forest models.
\\ ( https://arxiv.org/abs/2402.18612 ,  5956kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18697 (*cross-listing*)
Date: Wed, 28 Feb 2024 20:24:56 GMT   (3773kb,D)

Title: Inferring Dynamic Networks from Marginals with Iterative Proportional
  Fitting
Authors: Serina Chang, Frederic Koehler, Zhaonan Qu, Jure Leskovec, Johan
  Ugander
Categories: stat.ML cs.LG cs.SI math.OC math.ST stat.TH
\\
  A common network inference problem, arising from real-world data constraints,
is how to infer a dynamic network from its time-aggregated adjacency matrix and
time-varying marginals (i.e., row and column sums). Prior approaches to this
problem have repurposed the classic iterative proportional fitting (IPF)
procedure, also known as Sinkhorn's algorithm, with promising empirical
results. However, the statistical foundation for using IPF has not been well
understood: under what settings does IPF provide principled estimation of a
dynamic network from its marginals, and how well does it estimate the network?
In this work, we establish such a setting, by identifying a generative network
model whose maximum likelihood estimates are recovered by IPF. Our model both
reveals implicit assumptions on the use of IPF in such settings and enables new
analyses, such as structure-dependent error bounds on IPF's parameter
estimates. When IPF fails to converge on sparse network data, we introduce a
principled algorithm that guarantees IPF converges under minimal changes to the
network structure. Finally, we conduct experiments with synthetic and
real-world data, which demonstrate the practical value of our theoretical and
algorithmic contributions.
\\ ( https://arxiv.org/abs/2402.18697 ,  3773kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18729 (*cross-listing*)
Date: Wed, 28 Feb 2024 22:19:55 GMT   (1366kb,D)

Title: A Priori Uncertainty Quantification of Reacting Turbulence Closure
  Models using Bayesian Neural Networks
Authors: Graham Pash, Malik Hassanaly, Shashank Yellapantula
Categories: physics.flu-dyn cs.LG physics.data-an
\\
  While many physics-based closure model forms have been posited for the
sub-filter scale (SFS) in large eddy simulation (LES), vast amounts of data
available from direct numerical simulation (DNS) create opportunities to
leverage data-driven modeling techniques. Albeit flexible, data-driven models
still depend on the dataset and the functional form of the model chosen.
Increased adoption of such models requires reliable uncertainty estimates both
in the data-informed and out-of-distribution regimes. In this work, we employ
Bayesian neural networks (BNNs) to capture both epistemic and aleatoric
uncertainties in a reacting flow model. In particular, we model the filtered
progress variable scalar dissipation rate which plays a key role in the
dynamics of turbulent premixed flames. We demonstrate that BNN models can
provide unique insights about the structure of uncertainty of the data-driven
closure models. We also propose a method for the incorporation of
out-of-distribution information in a BNN. The efficacy of the model is
demonstrated by a priori evaluation on a dataset consisting of a variety of
flame conditions and fuels.
\\ ( https://arxiv.org/abs/2402.18729 ,  1366kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18746 (*cross-listing*)
Date: Wed, 28 Feb 2024 23:00:57 GMT   (1270kb,D)

Title: Accelerating Computer Architecture Simulation through Machine Learning
Authors: Wajid Ali and Ayaz Akram
Categories: cs.AR cs.LG
\\
  This paper presents our approach to accelerate computer architecture
simulation by leveraging machine learning techniques. Traditional computer
architecture simulations are time-consuming, making it challenging to explore
different design choices efficiently. Our proposed model utilizes a combination
of application features and micro-architectural features to predict the
performance of an application. These features are derived from simulations of a
small portion of the application. We demonstrate the effectiveness of our
approach by building and evaluating a machine learning model that offers
significant speedup in architectural exploration. This model demonstrates the
ability to predict IPC values for the testing data with a root mean square
error of less than 0.1.
\\ ( https://arxiv.org/abs/2402.18746 ,  1270kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18781 (*cross-listing*)
Date: Thu, 29 Feb 2024 01:07:29 GMT   (253kb)

Title: Conjectural Online Learning with First-order Beliefs in Asymmetric
  Information Stochastic Games
Authors: Tao Li, Kim Hammar, Rolf Stadler, and Quanyan Zhu
Categories: cs.GT cs.LG cs.SY eess.SY
\\
  Stochastic games arise in many complex socio-technical systems, such as
cyber-physical systems and IT infrastructures, where information asymmetry
presents challenges for decision-making entities (players). Existing
computational methods for asymmetric information stochastic games (AISG) are
primarily offline, targeting special classes of AISGs to avoid belief
hierarchies, and lack online adaptability to deviations from equilibrium. To
address this limitation, we propose a conjectural online learning (COL), a
learning scheme for generic AISGs. COL, structured as a forecaster-actor-critic
(FAC) architecture, utilizes first-order beliefs over the hidden states and
subjective forecasts of the opponent's strategies. Against the conjectured
opponent, COL updates strategies in an actor-critic approach using online
rollout and calibrates conjectures through Bayesian learning. We prove that
conjecture in COL is asymptotically consistent with the information feedback in
the sense of a relaxed Bayesian consistency. The resulting empirical strategy
profile converges to the Berk-Nash equilibrium, a solution concept
characterizing rationality under subjectivity. Experimental results from an
intrusion response use case demonstrate COL's superiority over state-of-the-art
reinforcement learning methods against nonstationary attacks.
\\ ( https://arxiv.org/abs/2402.18781 ,  253kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18830 (*cross-listing*)
Date: Thu, 29 Feb 2024 03:31:41 GMT   (10868kb,D)

Title: Training-set-free two-stage deep learning for Spectroscopic data
  de-noising
Authors: Dongchen Huang. Junde Liu, Tian Qian, and Hongming Weng
Categories: cond-mat.mtrl-sci cs.LG physics.data-an
\\
  De-noising is a prominent step in the spectra post-processing procedure.
Previous machine learning-based methods are fast but mostly based on supervised
learning and require a training set that may be typically expensive in real
experimental measurements. Unsupervised learning-based algorithms are slow and
require many iterations to achieve convergence. Here, we bridge this gap by
proposing a training-set-free two-stage deep learning method. We show that the
fuzzy fixed input in previous methods can be improved by introducing an
adaptive prior. Combined with more advanced optimization techniques, our
approach can achieve five times acceleration compared to previous work.
Theoretically, we study the landscape of a corresponding non-convex linear
problem, and our results indicates that this problem has benign geometry for
first-order algorithms to converge.
\\ ( https://arxiv.org/abs/2402.18830 ,  10868kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18919 (*cross-listing*)
Date: Thu, 29 Feb 2024 07:24:24 GMT   (42058kb,D)

Title: Decompose-and-Compose: A Compositional Approach to Mitigating Spurious
  Correlation
Authors: Fahimeh Hosseini Noohdani, Parsa Hosseini, Arian Yazdan Parast,
  Hamidreza Yaghoubi Araghi, Mahdieh Soleymani Baghshah
Categories: cs.CV cs.LG
\\
  While standard Empirical Risk Minimization (ERM) training is proven effective
for image classification on in-distribution data, it fails to perform well on
out-of-distribution samples. One of the main sources of distribution shift for
image classification is the compositional nature of images. Specifically, in
addition to the main object or component(s) determining the label, some other
image components usually exist, which may lead to the shift of input
distribution between train and test environments. More importantly, these
components may have spurious correlations with the label. To address this
issue, we propose Decompose-and-Compose (DaC), which improves robustness to
correlation shift by a compositional approach based on combining elements of
images. Based on our observations, models trained with ERM usually highly
attend to either the causal components or the components having a high spurious
correlation with the label (especially in datapoints on which models have a
high confidence). In fact, according to the amount of spurious correlation and
the easiness of classification based on the causal or non-causal components,
the model usually attends to one of these more (on samples with high
confidence). Following this, we first try to identify the causal components of
images using class activation maps of models trained with ERM. Afterward, we
intervene on images by combining them and retraining the model on the augmented
data, including the counterfactual ones. Along with its high interpretability,
this work proposes a group-balancing method by intervening on images without
requiring group labels or information regarding the spurious features during
training. The method has an overall better worst group accuracy compared to
previous methods with the same amount of supervision on the group labels in
correlation shift.
\\ ( https://arxiv.org/abs/2402.18919 ,  42058kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18994 (*cross-listing*)
Date: Thu, 29 Feb 2024 09:46:44 GMT   (21kb)

Title: Spyx: A Library for Just-In-Time Compiled Optimization of Spiking Neural
  Networks
Authors: Kade M. Heckel and Thomas Nowotny
Categories: cs.NE cs.LG
ACM-class: I.2.5
\\
  As the role of artificial intelligence becomes increasingly pivotal in modern
society, the efficient training and deployment of deep neural networks have
emerged as critical areas of focus. Recent advancements in attention-based
large neural architectures have spurred the development of AI accelerators,
facilitating the training of extensive, multi-billion parameter models. Despite
their effectiveness, these powerful networks often incur high execution costs
in production environments. Neuromorphic computing, inspired by biological
neural processes, offers a promising alternative. By utilizing
temporally-sparse computations, Spiking Neural Networks (SNNs) offer to enhance
energy efficiency through a reduced and low-power hardware footprint. However,
the training of SNNs can be challenging due to their recurrent nature which
cannot as easily leverage the massive parallelism of modern AI accelerators. To
facilitate the investigation of SNN architectures and dynamics researchers have
sought to bridge Python-based deep learning frameworks such as PyTorch or
TensorFlow with custom-implemented compute kernels. This paper introduces Spyx,
a new and lightweight SNN simulation and optimization library designed in JAX.
By pre-staging data in the expansive vRAM of contemporary accelerators and
employing extensive JIT compilation, Spyx allows for SNN optimization to be
executed as a unified, low-level program on NVIDIA GPUs or Google TPUs. This
approach achieves optimal hardware utilization, surpassing the performance of
many existing SNN training frameworks while maintaining considerable
flexibility.
\\ ( https://arxiv.org/abs/2402.18994 ,  21kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19037 (*cross-listing*)
Date: Thu, 29 Feb 2024 11:02:47 GMT   (744kb,D)

Title: A Deep-Learning Technique to Locate Cryptographic Operations in
  Side-Channel Traces
Authors: Giuseppe Chiari, Davide Galli, Francesco Lattari, Matteo Matteucci,
  Davide Zoni
Categories: cs.CR cs.LG
Comments: Accepted for presentation by DATE'24
\\
  Side-channel attacks allow extracting secret information from the execution
of cryptographic primitives by correlating the partially known computed data
and the measured side-channel signal. However, to set up a successful
side-channel attack, the attacker has to perform i) the challenging task of
locating the time instant in which the target cryptographic primitive is
executed inside a side-channel trace and then ii)the time-alignment of the
measured data on that time instant. This paper presents a novel deep-learning
technique to locate the time instant in which the target computed cryptographic
operations are executed in the side-channel trace. In contrast to
state-of-the-art solutions, the proposed methodology works even in the presence
of trace deformations obtained through random delay insertion techniques. We
validated our proposal through a successful attack against a variety of
unprotected and protected cryptographic primitives that have been executed on
an FPGA-implemented system-on-chip featuring a RISC-V CPU.
\\ ( https://arxiv.org/abs/2402.19037 ,  744kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19062 (*cross-listing*)
Date: Thu, 29 Feb 2024 11:45:24 GMT   (25844kb,D)

Title: Graph Convolutional Neural Networks for Automated Echocardiography View
  Recognition: A Holistic Approach
Authors: Sarina Thomas, Cristiana Tiago, B{\o}rge Solli Andreassen, Svein-Arne
  Aase, Jurica Sprem, Erik Steen, Anne Solberg, Guy Ben-Yosef
Categories: eess.IV cs.CV cs.LG
Comments: Presented at ASMUS - MICCAI conference 2023, Vancouver
DOI: 10.1007/978-3-031-44521-7_5
\\
  To facilitate diagnosis on cardiac ultrasound (US), clinical practice has
established several standard views of the heart, which serve as reference
points for diagnostic measurements and define viewports from which images are
acquired. Automatic view recognition involves grouping those images into
classes of standard views. Although deep learning techniques have been
successful in achieving this, they still struggle with fully verifying the
suitability of an image for specific measurements due to factors like the
correct location, pose, and potential occlusions of cardiac structures. Our
approach goes beyond view classification and incorporates a 3D mesh
reconstruction of the heart that enables several more downstream tasks, like
segmentation and pose estimation. In this work, we explore learning 3D heart
meshes via graph convolutions, using similar techniques to learn 3D meshes in
natural images, such as human pose estimation. As the availability of fully
annotated 3D images is limited, we generate synthetic US images from 3D meshes
by training an adversarial denoising diffusion model. Experiments were
conducted on synthetic and clinical cases for view recognition and structure
detection. The approach yielded good performance on synthetic images and,
despite being exclusively trained on synthetic data, it already showed
potential when applied to clinical images. With this proof-of-concept, we aim
to demonstrate the benefits of graphs to improve cardiac view recognition that
can ultimately lead to better efficiency in cardiac diagnosis.
\\ ( https://arxiv.org/abs/2402.19062 ,  25844kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19095 (*cross-listing*)
Date: Thu, 29 Feb 2024 12:24:20 GMT   (1360kb)

Title: A Protein Structure Prediction Approach Leveraging Transformer and CNN
  Integration
Authors: Yanlin Zhou, Kai Tan, Xinyu Shen, Zheng He
Categories: q-bio.BM cs.LG
\\
  Proteins are essential for life, and their structure determines their
function. The protein secondary structure is formed by the folding of the
protein primary structure, and the protein tertiary structure is formed by the
bending and folding of the secondary structure. Therefore, the study of protein
secondary structure is very helpful to the overall understanding of protein
structure. Although the accuracy of protein secondary structure prediction has
continuously improved with the development of machine learning and deep
learning, progress in the field of protein structure prediction, unfortunately,
remains insufficient to meet the large demand for protein information.
Therefore, based on the advantages of deep learning-based methods in feature
extraction and learning ability, this paper adopts a two-dimensional fusion
deep neural network model, DstruCCN, which uses Convolutional Neural Networks
(CCN) and a supervised Transformer protein language model for single-sequence
protein structure prediction. The training features of the two are combined to
predict the protein Transformer binding site matrix, and then the
three-dimensional structure is reconstructed using energy minimization.
\\ ( https://arxiv.org/abs/2402.19095 ,  1360kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19101 (*cross-listing*)
Date: Thu, 29 Feb 2024 12:29:58 GMT   (4688kb,D)

Title: Effective Two-Stage Knowledge Transfer for Multi-Entity Cross-Domain
  Recommendation
Authors: Jianyu Guan, Zongming Yin, Tianyi Zhang, Leihui Chen, Yin Zhang, Fei
  Huang, Jufeng Chen, Shuguang Han
Categories: cs.IR cs.LG
\\
  In recent years, the recommendation content on e-commerce platforms has
become increasingly rich -- a single user feed may contain multiple entities,
such as selling products, short videos, and content posts. To deal with the
multi-entity recommendation problem, an intuitive solution is to adopt the
shared-network-based architecture for joint training. The idea is to transfer
the extracted knowledge from one type of entity (source entity) to another
(target entity). However, different from the conventional same-entity
cross-domain recommendation, multi-entity knowledge transfer encounters several
important issues: (1) data distributions of the source entity and target entity
are naturally different, making the shared-network-based joint training
susceptible to the negative transfer issue, (2) more importantly, the
corresponding feature schema of each entity is not exactly aligned (e.g., price
is an essential feature for selling product while missing for content posts),
making the existing methods no longer appropriate. Recent researchers have also
experimented with the pre-training and fine-tuning paradigm. Again, they only
consider the scenarios with the same entity type and feature systems, which is
inappropriate in our case. To this end, we design a pre-training & fine-tuning
based Multi-entity Knowledge Transfer framework called MKT. MKT utilizes a
multi-entity pre-training module to extract transferable knowledge across
different entities. In particular, a feature alignment module is first applied
to scale and align different feature schemas. Afterward, a couple of knowledge
extractors are employed to extract the common and entity-specific knowledge. In
the end, the extracted common knowledge is adopted for target entity model
training. Through extensive offline and online experiments, we demonstrated the
superiority of MKT over multiple State-Of-The-Art methods.
\\ ( https://arxiv.org/abs/2402.19101 ,  4688kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19110 (*cross-listing*)
Date: Thu, 29 Feb 2024 12:41:54 GMT   (12543kb,D)

Title: Temporal-Aware Deep Reinforcement Learning for Energy Storage Bidding in
  Energy and Contingency Reserve Markets
Authors: Jinhao Li, Changlong Wang, Yanru Zhang, Hao Wang
Categories: eess.SY cs.LG cs.SY math.OC
Comments: 15 pages
Journal-ref: IEEE Transactions on Energy Markets, Policy and Regulation, 2024
\\
  The battery energy storage system (BESS) has immense potential for enhancing
grid reliability and security through its participation in the electricity
market. BESS often seeks various revenue streams by taking part in multiple
markets to unlock its full potential, but effective algorithms for joint-market
participation under price uncertainties are insufficiently explored in the
existing research. To bridge this gap, we develop a novel BESS joint bidding
strategy that utilizes deep reinforcement learning (DRL) to bid in the spot and
contingency frequency control ancillary services (FCAS) markets. Our approach
leverages a transformer-based temporal feature extractor to effectively respond
to price fluctuations in seven markets simultaneously and helps DRL learn the
best BESS bidding strategy in joint-market participation. Additionally, unlike
conventional "black-box" DRL model, our approach is more interpretable and
provides valuable insights into the temporal bidding behavior of BESS in the
dynamic electricity market. We validate our method using realistic market
prices from the Australian National Electricity Market. The results show that
our strategy outperforms benchmarks, including both optimization-based and
other DRL-based strategies, by substantial margins. Our findings further
suggest that effective temporal-aware bidding can significantly increase
profits in the spot and contingency FCAS markets compared to individual market
participation.
\\ ( https://arxiv.org/abs/2402.19110 ,  12543kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19142 (*cross-listing*)
Date: Thu, 29 Feb 2024 13:25:15 GMT   (9388kb,D)

Title: ProtoP-OD: Explainable Object Detection with Prototypical Parts
Authors: Pavlos Rath-Manakidis, Frederik Strothmann, Tobias Glasmachers,
  Laurenz Wiskott
Categories: cs.CV cs.LG
Comments: 9 pages, 11 figures
\\
  Interpretation and visualization of the behavior of detection transformers
tends to highlight the locations in the image that the model attends to, but it
provides limited insight into the \emph{semantics} that the model is focusing
on. This paper introduces an extension to detection transformers that
constructs prototypical local features and uses them in object detection. These
custom features, which we call prototypical parts, are designed to be mutually
exclusive and align with the classifications of the model. The proposed
extension consists of a bottleneck module, the prototype neck, that computes a
discretized representation of prototype activations and a new loss term that
matches prototypes to object classes. This setup leads to interpretable
representations in the prototype neck, allowing visual inspection of the image
content perceived by the model and a better understanding of the model's
reliability. We show experimentally that our method incurs only a limited
performance penalty, and we provide examples that demonstrate the quality of
the explanations provided by our method, which we argue outweighs the
performance penalty.
\\ ( https://arxiv.org/abs/2402.19142 ,  9388kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19186 (*cross-listing*)
Date: Thu, 29 Feb 2024 14:11:08 GMT   (5993kb,D)

Title: Disentangling representations of retinal images with generative models
Authors: Sarah M\"uller, Lisa M. Koch, Hendrik P. A. Lensch, Philipp Berens
Categories: cs.CV cs.LG
\\
  Retinal fundus images play a crucial role in the early detection of eye
diseases and, using deep learning approaches, recent studies have even
demonstrated their potential for detecting cardiovascular risk factors and
neurological disorders. However, the impact of technical factors on these
images can pose challenges for reliable AI applications in ophthalmology. For
example, large fundus cohorts are often confounded by factors like camera type,
image quality or illumination level, bearing the risk of learning shortcuts
rather than the causal relationships behind the image generation process. Here,
we introduce a novel population model for retinal fundus images that
effectively disentangles patient attributes from camera effects, thus enabling
controllable and highly realistic image generation. To achieve this, we propose
a novel disentanglement loss based on distance correlation. Through qualitative
and quantitative analyses, we demonstrate the effectiveness of this novel loss
function in disentangling the learned subspaces. Our results show that our
model provides a new perspective on the complex relationship between patient
attributes and technical confounders in retinal fundus image generation.
\\ ( https://arxiv.org/abs/2402.19186 ,  5993kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19212 (*cross-listing*)
Date: Thu, 29 Feb 2024 14:41:31 GMT   (38kb)

Title: Deep Reinforcement Learning: A Convex Optimization Approach
Authors: Ather Gattami
Categories: math.OC cs.LG
\\
  In this paper, we consider reinforcement learning of nonlinear systems with
continuous state and action spaces. We present an episodic learning algorithm,
where we for each episode use convex optimization to find a two-layer neural
network approximation of the optimal $Q$-function. The convex optimization
approach guarantees that the weights calculated at each episode are optimal,
with respect to the given sampled states and actions of the current episode.
For stable nonlinear systems, we show that the algorithm converges and that the
converging parameters of the trained neural network can be made arbitrarily
close to the optimal neural network parameters. In particular, if the
regularization parameter is $\rho$ and the time horizon is $T$, then the
parameters of the trained neural network converge to $w$, where the distance
between $w$ from the optimal parameters $w^\star$ is bounded by
$\mathcal{O}(\rho T^{-1})$. That is, when the number of episodes goes to
infinity, there exists a constant $C$ such that \[\|w-w^\star\| \le
C\cdot\frac{\rho}{T}.\] In particular, our algorithm converges arbitrarily
close to the optimal neural network parameters as the time horizon increases or
as the regularization parameter decreases.
\\ ( https://arxiv.org/abs/2402.19212 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19275 (*cross-listing*)
Date: Thu, 29 Feb 2024 15:42:33 GMT   (4145kb,D)

Title: Adaptive Testing Environment Generation for Connected and Automated
  Vehicles with Dense Reinforcement Learning
Authors: Jingxuan Yang, Ruoxuan Bai, Haoyuan Ji, Yi Zhang, Jianming Hu, Shuo
  Feng
Categories: eess.SY cs.LG cs.SY
\\
  The assessment of safety performance plays a pivotal role in the development
and deployment of connected and automated vehicles (CAVs). A common approach
involves designing testing scenarios based on prior knowledge of CAVs (e.g.,
surrogate models), conducting tests in these scenarios, and subsequently
evaluating CAVs' safety performances. However, substantial differences between
CAVs and the prior knowledge can significantly diminish the evaluation
efficiency. In response to this issue, existing studies predominantly
concentrate on the adaptive design of testing scenarios during the CAV testing
process. Yet, these methods have limitations in their applicability to
high-dimensional scenarios. To overcome this challenge, we develop an adaptive
testing environment that bolsters evaluation robustness by incorporating
multiple surrogate models and optimizing the combination coefficients of these
surrogate models to enhance evaluation efficiency. We formulate the
optimization problem as a regression task utilizing quadratic programming. To
efficiently obtain the regression target via reinforcement learning, we propose
the dense reinforcement learning method and devise a new adaptive policy with
high sample efficiency. Essentially, our approach centers on learning the
values of critical scenes displaying substantial surrogate-to-real gaps. The
effectiveness of our method is validated in high-dimensional overtaking
scenarios, demonstrating that our approach achieves notable evaluation
efficiency.
\\ ( https://arxiv.org/abs/2402.19275 ,  4145kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19296 (*cross-listing*)
Date: Thu, 29 Feb 2024 15:59:42 GMT   (29386kb)

Title: An AI based Digital Score of Tumour-Immune Microenvironment Predicts
  Benefit to Maintenance Immunotherapy in Advanced Oesophagogastric
  Adenocarcinoma
Authors: Quoc Dang Vu, Caroline Fong, Anderley Gordon, Tom Lund, Tatiany L
  Silveira, Daniel Rodrigues, Katharina von Loga, Shan E Ahmed Raza, David
  Cunningham, Nasir Rajpoot
Categories: cs.CV cs.LG
\\
  Gastric and oesophageal (OG) cancers are the leading causes of cancer
mortality worldwide. In OG cancers, recent studies have showed that PDL1 immune
checkpoint inhibitors (ICI) in combination with chemotherapy improves patient
survival. However, our understanding of the tumour immune microenvironment in
OG cancers remains limited. In this study, we interrogate multiplex
immunofluorescence (mIF) images taken from patients with advanced
Oesophagogastric Adenocarcinoma (OGA) who received first-line fluoropyrimidine
and platinum-based chemotherapy in the PLATFORM trial (NCT02678182) to predict
the efficacy of the treatment and to explore the biological basis of patients
responding to maintenance durvalumab (PDL1 inhibitor). Our proposed Artificial
Intelligence (AI) based marker successfully identified responder from
non-responder (p < 0.05) as well as those who could potentially benefit from
ICI with statistical significance (p < 0.05) for both progression free and
overall survival. Our findings suggest that T cells that express FOXP3 seem to
heavily influence the patient treatment response and survival outcome. We also
observed that higher levels of CD8+PD1+ cells are consistently linked to poor
prognosis for both OS and PFS, regardless of ICI.
\\ ( https://arxiv.org/abs/2402.19296 ,  29386kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19319 (*cross-listing*)
Date: Thu, 29 Feb 2024 16:24:19 GMT   (475kb,D)

Title: Attacks Against Mobility Prediction in 5G Networks
Authors: Syafiq Al Atiiq, Yachao Yuan, Christian Gehrmann, Jakob Sternby, Luis
  Barriga
Categories: cs.CR cs.LG cs.NI
Comments: This is the preprint version of a paper which appears in 22th IEEE
  International Conference on Trust, Security and Privacy in Computing and
  Communications (TrustCom 2023)
\\
  The $5^{th}$ generation of mobile networks introduces a new Network Function
(NF) that was not present in previous generations, namely the Network Data
Analytics Function (NWDAF). Its primary objective is to provide advanced
analytics services to various entities within the network and also towards
external application services in the 5G ecosystem. One of the key use cases of
NWDAF is mobility trajectory prediction, which aims to accurately support
efficient mobility management of User Equipment (UE) in the network by
allocating ``just in time'' necessary network resources. In this paper, we show
that there are potential mobility attacks that can compromise the accuracy of
these predictions. In a semi-realistic scenario with 10,000 subscribers, we
demonstrate that an adversary equipped with the ability to hijack cellular
mobile devices and clone them can significantly reduce the prediction accuracy
from 75\% to 40\% using just 100 adversarial UEs. While a defense mechanism
largely depends on the attack and the mobility types in a particular area, we
prove that a basic KMeans clustering is effective in distinguishing legitimate
and adversarial UEs.
\\ ( https://arxiv.org/abs/2402.19319 ,  475kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19355 (*cross-listing*)
Date: Thu, 29 Feb 2024 17:06:52 GMT   (5548kb,D)

Title: Unraveling Adversarial Examples against Speaker Identification --
  Techniques for Attack Detection and Victim Model Classification
Authors: Sonal Joshi, Thomas Thebaud, Jes\'us Villalba, Najim Dehak
Categories: cs.SD cs.CR cs.LG eess.AS
\\
  Adversarial examples have proven to threaten speaker identification systems,
and several countermeasures against them have been proposed. In this paper, we
propose a method to detect the presence of adversarial examples, i.e., a binary
classifier distinguishing between benign and adversarial examples. We build
upon and extend previous work on attack type classification by exploring new
architectures. Additionally, we introduce a method for identifying the victim
model on which the adversarial attack is carried out. To achieve this, we
generate a new dataset containing multiple attacks performed against various
victim models. We achieve an AUC of 0.982 for attack detection, with no more
than a 0.03 drop in performance for unknown attacks. Our attack classification
accuracy (excluding benign) reaches 86.48% across eight attack types using our
LightResNet34 architecture, while our victim model classification accuracy
reaches 72.28% across four victim models.
\\ ( https://arxiv.org/abs/2402.19355 ,  5548kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19455 (*cross-listing*)
Date: Thu, 29 Feb 2024 18:50:11 GMT   (8187kb,D)

Title: Listening to the Noise: Blind Denoising with Gibbs Diffusion
Authors: David Heurtel-Depeiges, Charles C. Margossian, Ruben Ohana, Bruno
  R\'egaldo-Saint Blancard
Categories: stat.ML astro-ph.CO cs.CV cs.LG eess.SP
Comments: 12+8 pages, 7+3 figures, 1+1 tables, code:
  https://github.com/rubenohana/Gibbs-Diffusion
\\
  In recent years, denoising problems have become intertwined with the
development of deep generative models. In particular, diffusion models are
trained like denoisers, and the distribution they model coincide with denoising
priors in the Bayesian picture. However, denoising through diffusion-based
posterior sampling requires the noise level and covariance to be known,
preventing blind denoising. We overcome this limitation by introducing Gibbs
Diffusion (GDiff), a general methodology addressing posterior sampling of both
the signal and the noise parameters. Assuming arbitrary parametric Gaussian
noise, we develop a Gibbs algorithm that alternates sampling steps from a
conditional diffusion model trained to map the signal prior to the family of
noise distributions, and a Monte Carlo sampler to infer the noise parameters.
Our theoretical analysis highlights potential pitfalls, guides diagnostic
usage, and quantifies errors in the Gibbs stationary distribution caused by the
diffusion model. We showcase our method for 1) blind denoising of natural
images involving colored noises with unknown amplitude and spectral index, and
2) a cosmology problem, namely the analysis of cosmic microwave background
data, where Bayesian inference of "noise" parameters means constraining models
of the evolution of the Universe.
\\ ( https://arxiv.org/abs/2402.19455 ,  8187kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19469 (*cross-listing*)
Date: Thu, 29 Feb 2024 18:57:37 GMT   (12946kb,D)

Title: Humanoid Locomotion as Next Token Prediction
Authors: Ilija Radosavovic, Bike Zhang, Baifeng Shi, Jathushan Rajasegaran,
  Sarthak Kamat, Trevor Darrell, Koushil Sreenath, Jitendra Malik
Categories: cs.RO cs.CV cs.LG
\\
  We cast real-world humanoid control as a next token prediction problem, akin
to predicting the next word in language. Our model is a causal transformer
trained via autoregressive prediction of sensorimotor trajectories. To account
for the multi-modal nature of the data, we perform prediction in a
modality-aligned way, and for each input token predict the next token from the
same modality. This general formulation enables us to leverage data with
missing modalities, like video trajectories without actions. We train our model
on a collection of simulated trajectories coming from prior neural network
policies, model-based controllers, motion capture data, and YouTube videos of
humans. We show that our model enables a full-sized humanoid to walk in San
Francisco zero-shot. Our model can transfer to the real world even when trained
on only 27 hours of walking data, and can generalize to commands not seen
during training like walking backward. These findings suggest a promising path
toward learning challenging real-world control tasks by generative modeling of
sensorimotor trajectories.
\\ ( https://arxiv.org/abs/2402.19469 ,  12946kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2110.11385
replaced with revised version Thu, 29 Feb 2024 04:50:25 GMT   (365kb)

Title: Self-Initiated Open World Learning for Autonomous AI Agents
Authors: Bing Liu, Eric Robertson, Scott Grigsby, Sahisnu Mazumder
Categories: cs.AI cs.HC cs.LG
Comments: Published in AAAI 2022 Spring Symposium Series
\\ ( https://arxiv.org/abs/2110.11385 ,  365kb)
------------------------------------------------------------------------------
\\
arXiv:2302.04831
replaced with revised version Wed, 28 Feb 2024 23:58:20 GMT   (11192kb,D)

Title: Cooperative Open-ended Learning Framework for Zero-shot Coordination
Authors: Yang Li, Shao Zhang, Jichen Sun, Yali Du, Ying Wen, Xinbing Wang, Wei
  Pan
Categories: cs.AI cs.LG
Comments: 15 pages with 9 pages main body
\\ ( https://arxiv.org/abs/2302.04831 ,  11192kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11461
replaced with revised version Thu, 29 Feb 2024 13:47:27 GMT   (7942kb,D)

Title: Hint of Thought prompting: an explainable and zero-shot approach to
  reasoning tasks with LLMs
Authors: Ioktong Lei and Zhidong Deng
Categories: cs.AI
Comments: preprint, under review
\\ ( https://arxiv.org/abs/2305.11461 ,  7942kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04689
replaced with revised version Wed, 28 Feb 2024 21:29:43 GMT   (283kb)

Title: Web crawler strategies for web pages under robot.txt restriction
Authors: Piyush Vyas, Akhilesh Chauhan, Tushar Mandge, Surbhi Hardikar
Categories: cs.AI cs.IR
\\ ( https://arxiv.org/abs/2308.04689 ,  283kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00746
replaced with revised version Thu, 29 Feb 2024 06:24:28 GMT   (25522kb,D)

Title: Deciphering Digital Detectives: Understanding LLM Behaviors and
  Capabilities in Multi-Agent Mystery Games
Authors: Dekun Wu, Haochen Shi, Zhiyuan Sun, Bang Liu
Categories: cs.AI
ACM-class: I.2.0; I.2.1; I.2.7
\\ ( https://arxiv.org/abs/2312.00746 ,  25522kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04140
replaced with revised version Thu, 29 Feb 2024 17:23:01 GMT   (907kb)

Title: Advancing Legal Reasoning: The Integration of AI to Navigate
  Complexities and Biases in Global Jurisprudence with Semi-Automated
  Arbitration Processes (SAAPs)
Authors: Michael De'Shazer
Categories: cs.AI cs.CY cs.HC
\\ ( https://arxiv.org/abs/2402.04140 ,  907kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08208
replaced with revised version Thu, 29 Feb 2024 18:18:04 GMT   (1206kb,D)

Title: Inherent Diverse Redundant Safety Mechanisms for AI-based Software
  Elements in Automotive Applications
Authors: Mandar Pitale, Alireza Abbaspour, Devesh Upadhyay
Categories: cs.AI
Comments: This article is accepted for the SAE WCX 2024 conference proceedings
\\ ( https://arxiv.org/abs/2402.08208 ,  1206kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16631
replaced with revised version Wed, 28 Feb 2024 19:43:51 GMT   (2593kb,D)

Title: GenAINet: Enabling Wireless Collective Intelligence via Knowledge
  Transfer and Reasoning
Authors: Hang Zou, Qiyang Zhao, Lina Bariah, Yu Tian, Mehdi Bennis, Samson
  Lasaulce, Merouane Debbah, Faouzi Bader
Categories: cs.AI cs.NI eess.SP
\\ ( https://arxiv.org/abs/2402.16631 ,  2593kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18409
replaced with revised version Thu, 29 Feb 2024 13:12:34 GMT   (1156kb,D)

Title: A Cognitive Evaluation Benchmark of Image Reasoning and Description for
  Large Vision Language Models
Authors: Xiujie Song, Mengyue Wu, Kenny Q. Zhu, Chunhao Zhang, Yanyi Chen
Categories: cs.AI cs.CL cs.CV
\\ ( https://arxiv.org/abs/2402.18409 ,  1156kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18496
replaced with revised version Thu, 29 Feb 2024 13:22:17 GMT   (2665kb,D)

Title: Language Models Represent Beliefs of Self and Others
Authors: Wentao Zhu, Zhining Zhang, Yizhou Wang
Categories: cs.AI cs.CL
Comments: project page: https://walter0807.github.io/RepBelief/
\\ ( https://arxiv.org/abs/2402.18496 ,  2665kb)
------------------------------------------------------------------------------
\\
arXiv:2212.10529
replaced with revised version Thu, 29 Feb 2024 13:14:37 GMT   (6922kb,D)

Title: Evaluating Psychological Safety of Large Language Models
Authors: Xingxuan Li, Yutong Li, Lin Qiu, Shafiq Joty, Lidong Bing
Categories: cs.CL cs.AI cs.CY
Comments: Preprint. Under review
\\ ( https://arxiv.org/abs/2212.10529 ,  6922kb)
------------------------------------------------------------------------------
\\
arXiv:2305.02215
replaced with revised version Thu, 29 Feb 2024 08:35:05 GMT   (10826kb,D)

Title: Exploring Linguistic Properties of Monolingual BERTs with Typological
  Classification among Languages
Authors: Elena Sofia Ruzzetti, Federico Ranaldi, Felicia Logozzo, Michele
  Mastromattei, Leonardo Ranaldi, Fabio Massimo Zanzotto
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2305.02215 ,  10826kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16230
replaced with revised version Thu, 29 Feb 2024 14:01:28 GMT   (251kb,D)

Title: An Unforgeable Publicly Verifiable Watermark for Large Language Models
Authors: Aiwei Liu, Leyi Pan, Xuming Hu, Shu'ang Li, Lijie Wen, Irwin King and
  Philip S. Yu
Categories: cs.CL
Comments: ICLR2024, 17 pages, 5 figures, 8 tables
MSC-class: 68T50
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2307.16230 ,  251kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05696
replaced with revised version Thu, 29 Feb 2024 03:04:22 GMT   (5473kb,D)

Title: A Preliminary Study of the Intrinsic Relationship between Complexity and
  Alignment
Authors: Yingxiu Zhao, Bowen Yu, Binyuan Hui, Haiyang Yu, Fei Huang, Yongbin
  Li, Nevin L. Zhang
Categories: cs.CL
Comments: LREC-Coling 2024
\\ ( https://arxiv.org/abs/2308.05696 ,  5473kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05605
replaced with revised version Wed, 28 Feb 2024 21:00:13 GMT   (5249kb,D)

Title: Memory Injections: Correcting Multi-Hop Reasoning Failures during
  Inference in Transformer-Based Language Models
Authors: Mansi Sakarvadia, Aswathy Ajith, Arham Khan, Daniel Grzenda, Nathaniel
  Hudson, Andr\'e Bauer, Kyle Chard, Ian Foster
Categories: cs.CL cs.AI cs.LG
Comments: Oral Presentation at BlackboxNLP Workshop at EMNLP 2023
\\ ( https://arxiv.org/abs/2309.05605 ,  5249kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06275
replaced with revised version Thu, 29 Feb 2024 06:28:55 GMT   (1997kb,D)

Title: Re-Reading Improves Reasoning in Large Language Models
Authors: Xiaohan Xu, Chongyang Tao, Tao Shen, Can Xu, Hongbo Xu, Guodong Long,
  Jian-guang Lou
Categories: cs.CL
Comments: 25 pages
\\ ( https://arxiv.org/abs/2309.06275 ,  1997kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11143
replaced with revised version Thu, 29 Feb 2024 13:38:47 GMT   (500kb,D)

Title: CoT-BERT: Enhancing Unsupervised Sentence Representation through
  Chain-of-Thought
Authors: Bowen Zhang, Kehua Chang, Chunping Li
Categories: cs.CL cs.AI
Comments: Work in Progress
\\ ( https://arxiv.org/abs/2309.11143 ,  500kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12444
replaced with revised version Wed, 28 Feb 2024 20:15:54 GMT   (1025kb,D)

Title: Foundation Metrics for Evaluating Effectiveness of Healthcare
  Conversations Powered by Generative AI
Authors: Mahyar Abbasian, Elahe Khatibi, Iman Azimi, David Oniani, Zahra
  Shakeri Hossein Abad, Alexander Thieme, Ram Sriram, Zhongqi Yang, Yanshan
  Wang, Bryant Lin, Olivier Gevaert, Li-Jia Li, Ramesh Jain, Amir M. Rahmani
Categories: cs.CL
Comments: 14 pages, 4 figures, 2 tables, journal paper
\\ ( https://arxiv.org/abs/2309.12444 ,  1025kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13339
replaced with revised version Thu, 29 Feb 2024 07:26:00 GMT   (312kb,D)

Title: Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models
  through Logic
Authors: Xufeng Zhao, Mengdi Li, Wenhao Lu, Cornelius Weber, Jae Hee Lee, Kun
  Chu, Stefan Wermter
Categories: cs.CL cs.AI cs.LG cs.SC
Comments: Accepted in COLING 2024. Rename LogiCoT (previous version) to LoT
  (Logical Thoughts, current)
\\ ( https://arxiv.org/abs/2309.13339 ,  312kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03560
replaced with revised version Thu, 29 Feb 2024 18:37:40 GMT   (2495kb,D)

Title: Redefining Digital Health Interfaces with Large Language Models
Authors: Fergus Imrie, Paulius Rauba, Mihaela van der Schaar
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.03560 ,  2495kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08540
replaced with revised version Thu, 29 Feb 2024 18:47:18 GMT   (3213kb,D)

Title: Revisiting the Hypothesis: Do pretrained Transformers Learn In-Context
  by Gradient Descent?
Authors: Lingfeng Shen, Aayush Mishra, Daniel Khashabi
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2310.08540 ,  3213kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09758
replaced with revised version Wed, 28 Feb 2024 20:41:25 GMT   (8406kb,D)

Title: OrchestraLLM: Efficient Orchestration of Language Models for Dialogue
  State Tracking
Authors: Chia-Hsuan Lee, Hao Cheng, Mari Ostendorf
Categories: cs.CL
Comments: updated version
\\ ( https://arxiv.org/abs/2311.09758 ,  8406kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09827
replaced with revised version Thu, 29 Feb 2024 08:20:07 GMT   (10449kb,D)

Title: Cognitive Overload: Jailbreaking Large Language Models with Overloaded
  Logical Thinking
Authors: Nan Xu, Fei Wang, Ben Zhou, Bang Zheng Li, Chaowei Xiao, Muhao Chen
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.09827 ,  10449kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02982
replaced with revised version Thu, 29 Feb 2024 02:27:23 GMT   (7123kb,D)

Title: BIBench: Benchmarking Data Analysis Knowledge of Large Language Models
Authors: Shu Liu, Shangqing Zhao, Chenghao Jia, Xinlin Zhuang, Zhaoguang Long,
  Qingquan Wu, Chong Yang, Aimin Zhou, Man Lan
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.02982 ,  7123kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13919
replaced with revised version Thu, 29 Feb 2024 12:07:42 GMT   (19153kb,D)

Title: WebVoyager: Building an End-to-End Web Agent with Large Multimodal
  Models
Authors: Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming
  Zhang, Zhenzhong Lan, Dong Yu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.13919 ,  19153kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15861
replaced with revised version Thu, 29 Feb 2024 01:02:56 GMT   (63kb,D)

Title: BPDec: Unveiling the Potential of Masked Language Modeling Decoder in
  BERT pretraining
Authors: Wen Liang, Youzhi Liang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.15861 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05699
replaced with revised version Thu, 29 Feb 2024 08:46:47 GMT   (1167kb,D)

Title: Self-Alignment of Large Language Models via Monopolylogue-based Social
  Scene Simulation
Authors: Xianghe Pang, Shuo Tang, Rui Ye, Yuxin Xiong, Bolun Zhang, Yanfeng
  Wang, Siheng Chen
Categories: cs.CL cs.AI cs.CY
Comments: 36 pages, 9 figures
\\ ( https://arxiv.org/abs/2402.05699 ,  1167kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10153
replaced with revised version Wed, 28 Feb 2024 19:40:13 GMT   (253kb,D)

Title: Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study
  for Diabetes Patients
Authors: Mahyar Abbasian, Zhongqi Yang, Elahe Khatibi, Pengfei Zhang, Nitish
  Nagesh, Iman Azimi, Ramesh Jain, Amir M. Rahmani
Categories: cs.CL
Comments: 4 pages, 3 figures, and 2 tables, conference paper
\\ ( https://arxiv.org/abs/2402.10153 ,  253kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10311
replaced with revised version Thu, 29 Feb 2024 16:13:01 GMT   (52kb,D)

Title: The optimal placement of the head in the noun phrase. The case of
  demonstrative, numeral, adjective and noun
Authors: Ramon Ferrer-i-Cancho
Categories: cs.CL physics.soc-ph
Comments: Many typos corrected
\\ ( https://arxiv.org/abs/2402.10311 ,  52kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11194
replaced with revised version Thu, 29 Feb 2024 09:13:58 GMT   (14926kb,D)

Title: Evaluating LLMs' Mathematical Reasoning in Financial Document Question
  Answering
Authors: Pragya Srivastava, Manuj Malik, Vivek Gupta, Tanuja Ganu, Dan Roth
Categories: cs.CL
Comments: 25 pages, 17 figures
\\ ( https://arxiv.org/abs/2402.11194 ,  14926kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12374
replaced with revised version Thu, 29 Feb 2024 18:48:38 GMT   (1062kb,D)

Title: Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding
Authors: Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang, Max
  Ryabinin, Zhihao Jia, Beidi Chen
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.12374 ,  1062kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14614
replaced with revised version Thu, 29 Feb 2024 09:20:37 GMT   (7974kb,D)

Title: Two Counterexamples to Tokenization and the Noiseless Channel
Authors: Marco Cognetta and Vil\'em Zouhar and Sangwhan Moon and Naoaki Okazaki
Categories: cs.CL
Comments: 9 pages, 2 figures, to appear in LREC-COLING 2024, de-texified
  metadata
\\ ( https://arxiv.org/abs/2402.14614 ,  7974kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14808
replaced with revised version Thu, 29 Feb 2024 16:09:58 GMT   (21563kb,D)

Title: RelayAttention for Efficient Large Language Model Serving with Long
  System Prompts
Authors: Lei Zhu, Xinjiang Wang, Wayne Zhang, Rynson W.H. Lau
Categories: cs.CL
Comments: fix typos; add code link
\\ ( https://arxiv.org/abs/2402.14808 ,  21563kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15813
replaced with revised version Thu, 29 Feb 2024 13:04:11 GMT   (468kb,D)

Title: Measuring Bargaining Abilities of LLMs: A Benchmark and A
  Buyer-Enhancement Method
Authors: Tian Xia, Zhiwei He, Tong Ren, Yibo Miao, Zhuosheng Zhang, Yang Yang,
  Rui Wang
Categories: cs.CL cs.GT
Comments: The dataset AmazonHistoryPrice and our code are available at
  https://github.com/TianXiaSJTU/AmazonPriceHistory
\\ ( https://arxiv.org/abs/2402.15813 ,  468kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16041
replaced with revised version Thu, 29 Feb 2024 14:46:44 GMT   (3008kb,D)

Title: Detecting Machine-Generated Texts by Multi-Population Aware Optimization
  for Maximum Mean Discrepancy
Authors: Shuhai Zhang, Yiliao Song, Jiahao Yang, Yuanqing Li, Bo Han, Mingkui
  Tan
Categories: cs.CL cs.LG
Comments: Accepted at ICLR 2024
\\ ( https://arxiv.org/abs/2402.16041 ,  3008kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16192
replaced with revised version Wed, 28 Feb 2024 23:11:33 GMT   (464kb,D)

Title: Defending Large Language Models against Jailbreak Attacks via Semantic
  Smoothing
Authors: Jiabao Ji, Bairu Hou, Alexander Robey, George J. Pappas, Hamed
  Hassani, Yang Zhang, Eric Wong, Shiyu Chang
Categories: cs.CL
Comments: 37 pages
\\ ( https://arxiv.org/abs/2402.16192 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16363
replaced with revised version Thu, 29 Feb 2024 17:08:51 GMT   (1458kb,D)

Title: LLM Inference Unveiled: Survey and Roofline Model Insights
Authors: Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Chenhao Xue,
  Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, Beidi Chen, Guangyu
  Sun, Kurt Keutzer
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.16363 ,  1458kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16459
replaced with revised version Wed, 28 Feb 2024 22:21:05 GMT   (6924kb,D)

Title: Defending LLMs against Jailbreaking Attacks via Backtranslation
Authors: Yihan Wang, Zhouxing Shi, Andrew Bai, Cho-Jui Hsieh
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.16459 ,  6924kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17532
replaced with revised version Thu, 29 Feb 2024 07:56:14 GMT   (6916kb,D)

Title: Retrieval is Accurate Generation
Authors: Bowen Cao, Deng Cai, Leyang Cui, Xuxin Cheng, Wei Bi, Yuexian Zou,
  Shuming Shi
Categories: cs.CL
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2402.17532 ,  6916kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18060
replaced with revised version Thu, 29 Feb 2024 16:31:57 GMT   (494kb,D)

Title: Benchmarking Large Language Models on Answering and Explaining
  Challenging Medical Questions
Authors: Hanjie Chen, Zhouxiang Fang, Yash Singla, Mark Dredze
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.18060 ,  494kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18101
replaced with revised version Thu, 29 Feb 2024 10:53:40 GMT   (822kb)

Title: Assessing the Efficacy of Grammar Error Correction: A Human Evaluation
  Approach in the Japanese Context
Authors: Qiao Wang and Zheng Yuan
Categories: cs.CL
Comments: 2024 Joint International Conference on Computational Linguistics,
  Language Resources and Evaluation (LREC-COLING 2024)
\\ ( https://arxiv.org/abs/2402.18101 ,  822kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18169
replaced with revised version Thu, 29 Feb 2024 06:45:56 GMT   (5142kb)

Title: MIKO: Multimodal Intention Knowledge Distillation from Large Language
  Models for Social-Media Commonsense Discovery
Authors: Feihong Lu, Weiqi Wang, Yangyifei Luo, Ziqin Zhu, Qingyun Sun, Baixuan
  Xu, Haochen Shi, Shiqi Gao, Qian Li, Yangqiu Song, Jianxin Li
Categories: cs.CL
Comments: 11 pages, 5 figures
\\ ( https://arxiv.org/abs/2402.18169 ,  5142kb)
------------------------------------------------------------------------------
\\
arXiv:1806.05451
replaced with revised version Thu, 29 Feb 2024 11:10:45 GMT   (142kb,D)

Title: The committee machine: Computational to statistical gaps in learning a
  two-layers neural network
Authors: Benjamin Aubin, Antoine Maillard, Jean Barbier, Florent Krzakala,
  Nicolas Macris and Lenka Zdeborov\'a
Categories: cs.LG cond-mat.dis-nn cond-mat.stat-mech physics.comp-ph stat.ML
Comments: 18 pages + supplementary material, 3 figures. (v2: update to match
  the published version ; v3: clarification of the caption of Fig. 3)
Journal-ref: J. Stat. Mech. (2019) 124023. & NeurIPS 2018
DOI: 10.1088/1742-5468/ab43d2
\\ ( https://arxiv.org/abs/1806.05451 ,  142kb)
------------------------------------------------------------------------------
\\
arXiv:2006.10628
replaced with revised version Thu, 29 Feb 2024 15:30:48 GMT   (1861kb,D)

Title: Offline detection of change-points in the mean for stationary graph
  signals
Authors: Alejandro de la Concha, Nicolas Vayatis, Argyris Kalogeratos
Categories: cs.LG stat.AP stat.ML
Comments: 16 pages, 2 figures, 1 table, 1 annex. 9 pages of main text
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2006.10628 ,  1861kb)
------------------------------------------------------------------------------
\\
arXiv:2202.08370
replaced with revised version Thu, 29 Feb 2024 16:58:25 GMT   (4570kb,D)

Title: CAREER: A Foundation Model for Labor Sequence Data
Authors: Keyon Vafa, Emil Palikot, Tianyu Du, Ayush Kanodia, Susan Athey, David
  M. Blei
Categories: cs.LG econ.EM
\\ ( https://arxiv.org/abs/2202.08370 ,  4570kb)
------------------------------------------------------------------------------
\\
arXiv:2209.00945
replaced with revised version Thu, 29 Feb 2024 12:20:59 GMT   (7384kb,D)

Title: IMG2IMU: Translating Knowledge from Large-Scale Images to IMU Sensing
  Applications
Authors: Hyungjun Yoon, Hyeongheon Cha, Hoang C. Nguyen, Taesik Gong, Sung-Ju
  Lee
Categories: cs.LG
Comments: 12 pages
MSC-class: 68T20
\\ ( https://arxiv.org/abs/2209.00945 ,  7384kb)
------------------------------------------------------------------------------
\\
arXiv:2210.01834
replaced with revised version Thu, 29 Feb 2024 02:29:20 GMT   (296kb,D)

Title: Invariant Aggregator for Defending against Federated Backdoor Attacks
Authors: Xiaoyang Wang, Dimitrios Dimitriadis, Sanmi Koyejo, Shruti Tople
Categories: cs.LG cs.CR
\\ ( https://arxiv.org/abs/2210.01834 ,  296kb)
------------------------------------------------------------------------------
\\
arXiv:2211.07303
replaced with revised version Thu, 29 Feb 2024 05:00:32 GMT   (1441kb,D)

Title: Adaptive Federated Minimax Optimization with Lower Complexities
Authors: Feihu Huang, Xinrui Wang, Junyi Li, Songcan Chen
Categories: cs.LG math.OC
Comments: To appear in AISTATS 2024
\\ ( https://arxiv.org/abs/2211.07303 ,  1441kb)
------------------------------------------------------------------------------
\\
arXiv:2302.02237
replaced with revised version Thu, 29 Feb 2024 11:49:45 GMT   (1062kb,D)

Title: Conformalized Semi-supervised Random Forest for Classification and
  Abnormality Detection
Authors: Yujin Han, Mingwenchan Xu, Leying Guan
Categories: cs.LG
\\ ( https://arxiv.org/abs/2302.02237 ,  1062kb)
------------------------------------------------------------------------------
\\
arXiv:2302.07457
replaced with revised version Wed, 28 Feb 2024 22:58:23 GMT   (17114kb,D)

Title: When Demonstrations Meet Generative World Models: A Maximum Likelihood
  Framework for Offline Inverse Reinforcement Learning
Authors: Siliang Zeng, Chenliang Li, Alfredo Garcia, Mingyi Hong
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2302.07457 ,  17114kb)
------------------------------------------------------------------------------
\\
arXiv:2304.00933
replaced with revised version Thu, 29 Feb 2024 15:04:41 GMT   (2059kb,D)

Title: Knowledge Accumulation in Continually Learned Representations and the
  Issue of Feature Forgetting
Authors: Timm Hess, Eli Verwimp, Gido M. van de Ven, Tinne Tuytelaars
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2304.00933 ,  2059kb)
------------------------------------------------------------------------------
\\
arXiv:2304.10398
replaced with revised version Thu, 29 Feb 2024 10:23:45 GMT   (3162kb,D)

Title: Multi-label Node Classification On Graph-Structured Data
Authors: Tianqi Zhao, Ngan Thi Dong, Alan Hanjalic, Megha Khosla
Categories: cs.LG
Comments: Published in TMLR 2023. Link:
  https://openreview.net/forum?id=EZhkV2BjDP
Journal-ref: Transaction Of Machine Learning Research, 2835-8856, 2023
\\ ( https://arxiv.org/abs/2304.10398 ,  3162kb)
------------------------------------------------------------------------------
\\
arXiv:2305.10361
replaced with revised version Wed, 28 Feb 2024 21:36:54 GMT   (2813kb,D)

Title: Human Choice Prediction in Language-based Persuasion Games:
  Simulation-based Off-Policy Evaluation
Authors: Eilam Shapira, Reut Apel, Moshe Tennenholtz, Roi Reichart
Categories: cs.LG cs.AI cs.GT
\\ ( https://arxiv.org/abs/2305.10361 ,  2813kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03997
replaced with revised version Thu, 29 Feb 2024 15:40:41 GMT   (59kb,D)

Title: Efficient Model-Free Exploration in Low-Rank MDPs
Authors: Zakaria Mhammedi, Adam Block, Dylan J. Foster, Alexander Rakhlin
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2307.03997 ,  59kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08924
replaced with revised version Thu, 29 Feb 2024 02:53:32 GMT   (13381kb,D)

Title: Towards Task Sampler Learning for Meta-Learning
Authors: Jingyao Wang, Wenwen Qiang, Xingzhe Su, Changwen Zheng, Fuchun Sun,
  Hui Xiong
Categories: cs.LG cs.CV
Comments: 28 pages, 11 tables, 12 figures
\\ ( https://arxiv.org/abs/2307.08924 ,  13381kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08705
replaced with revised version Thu, 29 Feb 2024 04:25:14 GMT   (2047kb,D)

Title: Partially Observable Multi-agent RL with (Quasi-)Efficiency: The
  Blessing of Information Sharing
Authors: Xiangyu Liu, Kaiqing Zhang
Categories: cs.LG cs.GT cs.MA
Comments: International Conference on Machine Learning (ICML) 2023
\\ ( https://arxiv.org/abs/2308.08705 ,  2047kb)
------------------------------------------------------------------------------
\\
arXiv:2308.16089
replaced with revised version Wed, 28 Feb 2024 19:26:34 GMT   (2530kb,D)

Title: Application of Zone Method based Physics-Informed Neural Networks in
  Reheating Furnaces
Authors: Ujjal Kr Dutta, Aldo Lipani, Chuan Wang, Yukun Hu
Categories: cs.LG cs.AI cs.NE cs.SY eess.SY
Comments: Accepted in: NeurIPS 2023 - Machine Learning and the Physical
  Sciences Workshop
\\ ( https://arxiv.org/abs/2308.16089 ,  2530kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10834
replaced with revised version Wed, 28 Feb 2024 21:22:15 GMT   (2459kb,D)

Title: Communication-Efficient Federated Learning via Regularized Sparse Random
  Networks
Authors: Mohamad Mestoukirdi, Omid Esrafilian, David Gesbert, Qianrui Li,
  Nicolas Gresset
Categories: cs.LG cs.CV cs.DC cs.DS
\\ ( https://arxiv.org/abs/2309.10834 ,  2459kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13192
replaced with revised version Thu, 29 Feb 2024 18:27:47 GMT   (566kb,D)

Title: Towards Green AI in Fine-tuning Large Language Models via Adaptive
  Backpropagation
Authors: Kai Huang, Hanyun Yin, Heng Huang, Wei Gao
Categories: cs.LG cs.AI
Comments: 16 pages
\\ ( https://arxiv.org/abs/2309.13192 ,  566kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14720
replaced with revised version Thu, 29 Feb 2024 08:30:03 GMT   (2102kb,D)

Title: Extended Deep Adaptive Input Normalization for Preprocessing Time Series
  Data for Neural Networks
Authors: Marcus A. K. September, Francesco Sanna Passino, Leonie Goldmann,
  Anton Hinel
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.14720 ,  2102kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17273
replaced with revised version Thu, 29 Feb 2024 14:39:52 GMT   (12375kb,D)

Title: Looping in the Human Collaborative and Explainable Bayesian Optimization
Authors: Masaki Adachi, Brady Planden, David A. Howey, Michael A. Osborne,
  Sebastian Orbell, Natalia Ares, Krikamol Muandet, Siu Lun Chau
Categories: cs.LG cs.HC stat.ML
Comments: Accepted at AISTATS 2024, 24 pages, 11 figures
MSC-class: 62C10, 62F15
\\ ( https://arxiv.org/abs/2310.17273 ,  12375kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18127
replaced with revised version Thu, 29 Feb 2024 03:41:23 GMT   (4611kb,D)

Title: Ask more, know better: Reinforce-Learned Prompt Questions for Decision
  Making with Large Language Models
Authors: Xue Yan, Yan Song, Xinyu Cui, Filippos Christianos, Haifeng Zhang,
  David Henry Mguni, Jun Wang
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2310.18127 ,  4611kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02013
replaced with revised version Thu, 29 Feb 2024 03:47:12 GMT   (3410kb,D)

Title: SMORE: Score Models for Offline Goal-Conditioned Reinforcement Learning
Authors: Harshit Sikchi, Rohan Chitnis, Ahmed Touati, Alborz Geramifard, Amy
  Zhang, Scott Niekum
Categories: cs.LG cs.AI cs.RO
Comments: Published at International Conference of Learning Representations
  (ICLR) 2024. 26 pages
\\ ( https://arxiv.org/abs/2311.02013 ,  3410kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08675
replaced with revised version Thu, 29 Feb 2024 14:31:40 GMT   (2739kb,D)

Title: Refined Coreset Selection: Towards Minimal Coreset Size under Model
  Performance Constraints
Authors: Xiaobo Xia, Jiale Liu, Shaokun Zhang, Qingyun Wu, Hongxin Wei,
  Tongliang Liu
Categories: cs.LG
Comments: 22 pages, 10 tables, 4 figures
\\ ( https://arxiv.org/abs/2311.08675 ,  2739kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13817
replaced with revised version Thu, 29 Feb 2024 02:16:02 GMT   (16843kb,D)

Title: Molecular Identification and Peak Assignment: Leveraging Multi-Level
  Multimodal Alignment on NMR
Authors: Hao Xu, Zhengyang Zhou, Pengyu Hong
Categories: cs.LG physics.chem-ph q-bio.QM
\\ ( https://arxiv.org/abs/2311.13817 ,  16843kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14658
replaced with revised version Thu, 29 Feb 2024 05:14:28 GMT   (102kb,D)

Title: Convergence Analysis for Learning Orthonormal Deep Linear Neural
  Networks
Authors: Zhen Qin, Xuwei Tan, Zhihui Zhu
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2311.14658 ,  102kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03151
replaced with revised version Wed, 28 Feb 2024 22:27:31 GMT   (881kb,D)

Title: Multitask Learning Can Improve Worst-Group Outcomes
Authors: Atharva Kulkarni, Lucio Dery, Amrith Setlur, Aditi Raghunathan, Ameet
  Talwalkar and Graham Neubig
Categories: cs.LG
Comments: 20 pages, 7 tables, 6 Figures
\\ ( https://arxiv.org/abs/2312.03151 ,  881kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12183
replaced with revised version Thu, 29 Feb 2024 11:41:45 GMT   (12832kb,D)

Title: Poincar\'e Differential Privacy for Hierarchy-Aware Graph Embedding
Authors: Yuecen Wei, Haonan Yuan, Xingcheng Fu, Qingyun Sun, Hao Peng, Xianxian
  Li, Chunming Hu
Categories: cs.LG cs.CR
Comments: Accepted by the Thirty-Eighth AAAI Conference on Artificial
  Intelligence (AAAI-24)
\\ ( https://arxiv.org/abs/2312.12183 ,  12832kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16731
replaced with revised version Thu, 29 Feb 2024 12:10:56 GMT   (1571kb,D)

Title: Infinite dSprites for Disentangled Continual Learning: Separating Memory
  Edits from Generalization
Authors: Sebastian Dziadzio, \c{C}a\u{g}atay Y{\i}ld{\i}z, Gido M. van de Ven,
  Tomasz Trzci\'nski, Tinne Tuytelaars, Matthias Bethge
Categories: cs.LG cs.CV
Comments: 10 pages, 10 figures
\\ ( https://arxiv.org/abs/2312.16731 ,  1571kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09181
replaced with revised version Thu, 29 Feb 2024 03:06:42 GMT   (4298kb,D)

Title: Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with
  Positive Forward Transfer
Authors: Junhao Zheng, Qianli Ma, Zhen Liu, Binquan Wu, Huawen Feng
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.09181 ,  4298kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11627
replaced with revised version Wed, 28 Feb 2024 19:04:12 GMT   (240kb,D)

Title: Tight Verification of Probabilistic Robustness in Bayesian Neural
  Networks
Authors: Ben Batten, Mehran Hosseini, Alessio Lomuscio
Categories: cs.LG cs.AI cs.FL cs.LO
Comments: Accepted at AISTATS 2024
MSC-class: 68T27 (Primary) 68T45, 68T07, 68T01 (Secondary)
ACM-class: I.2.0; I.2.4; F.3.1; D.2.4
\\ ( https://arxiv.org/abs/2401.11627 ,  240kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01109
replaced with revised version Thu, 29 Feb 2024 07:15:13 GMT   (1005kb,D)

Title: Vaccine: Perturbation-aware Alignment for Large Language Model
Authors: Tiansheng Huang, Sihao Hu, Ling Liu
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2402.01109 ,  1005kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01431
replaced with revised version Thu, 29 Feb 2024 09:36:17 GMT   (2330kb,D)

Title: Approximate Control for Continuous-Time POMDPs
Authors: Yannick Eich, Bastian Alt, Heinz Koeppl
Categories: cs.LG cs.SY eess.SY q-bio.QM
Comments: To be published in AISTATS 2024
\\ ( https://arxiv.org/abs/2402.01431 ,  2330kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03659
replaced with revised version Thu, 29 Feb 2024 12:10:37 GMT   (3324kb,D)

Title: Learning to Generate Explainable Stock Predictions using Self-Reflective
  Large Language Models
Authors: Kelvin J.L. Koa, Yunshan Ma, Ritchie Ng, Tat-Seng Chua
Categories: cs.LG cs.CL q-fin.ST
Comments: WWW 2024
DOI: 10.1145/3589334.3645611
\\ ( https://arxiv.org/abs/2402.03659 ,  3324kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03726
replaced with revised version Thu, 29 Feb 2024 10:14:00 GMT   (1253kb,D)

Title: Learning Granger Causality from Instance-wise Self-attentive Hawkes
  Processes
Authors: Dongxia Wu, Tsuyoshi Id\'e, Aur\'elie Lozano, Georgios Kollias,
  Ji\v{r}\'i Navr\'atil, Naoki Abe, Yi-An Ma, Rose Yu
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.03726 ,  1253kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10192
replaced with revised version Thu, 29 Feb 2024 11:22:28 GMT   (2156kb,D)

Title: Multi-Excitation Projective Simulation with a Many-Body Physics Inspired
  Inductive Bias
Authors: Philip A. LeMaitre, Marius Krumm, and Hans J. Briegel
Categories: cs.LG cs.AI cs.DM quant-ph
Comments: 24 pages, 8 figures; Code repository at
  https://github.com/MariusKrumm/ManyBodyMEPS. Added figures and shortened
  computer maintenance section text for better readability
\\ ( https://arxiv.org/abs/2402.10192 ,  2156kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10401
replaced with revised version Thu, 29 Feb 2024 08:02:27 GMT   (40387kb,D)

Title: ManiFPT: Defining and Analyzing Fingerprints of Generative Models
Authors: Hae Jin Song, Mahyar Khayatkhoei, Wael AbdAlmageed
Categories: cs.LG cs.CV
Comments: Accepted to CVPR 2024
\\ ( https://arxiv.org/abs/2402.10401 ,  40387kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10487
replaced with revised version Thu, 29 Feb 2024 01:04:15 GMT   (4528kb,D)

Title: Random Projection Layers for Multidimensional Time Series Forecasting
Authors: Chin-Chia Michael Yeh, Yujie Fan, Xin Dai, Vivian Lai, Prince Osei
  Aboagye, Junpeng Wang, Huiyuan Chen, Yan Zheng, Zhongfang Zhuang, Liang Wang,
  Wei Zhang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.10487 ,  4528kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14041
replaced with revised version Thu, 29 Feb 2024 06:37:12 GMT   (4317kb)

Title: E2USD: Efficient-yet-effective Unsupervised State Detection for
  Multivariate Time Series
Authors: Zhichen Lai, Huan Li, Dalin Zhang, Yan Zhao, Weizhu Qian, Christian S.
  Jensen
Categories: cs.LG cs.AI cs.DB
Comments: accepted by The Web Conference 2024 (WWW 2024)
\\ ( https://arxiv.org/abs/2402.14041 ,  4317kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15183
replaced with revised version Thu, 29 Feb 2024 04:15:44 GMT   (960kb,D)

Title: GraphEdit: Large Language Models for Graph Structure Learning
Authors: Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei,
  Liang Pang, Tat-Seng Chua, Chao Huang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.15183 ,  960kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17002
replaced with revised version Thu, 29 Feb 2024 06:23:22 GMT   (3988kb,D)

Title: Discovering Symmetry Group Structures via Implicit Orthogonality Bias
Authors: Dongsung Huh
Categories: cs.LG math.GR math.RT
Comments: 19 pages, 14 figures
\\ ( https://arxiv.org/abs/2402.17002 ,  3988kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18495
replaced with revised version Thu, 29 Feb 2024 13:02:50 GMT   (469kb,D)

Title: ROG$_{PL}$: Robust Open-Set Graph Learning via Region-Based Prototype
  Learning
Authors: Qin Zhang, Xiaowei Li, Jiexin Lu, Liping Qiu, Shirui Pan, Xiaojun
  Chen, Junyang Chen
Categories: cs.LG
Comments: 9 pages, 5 figures
\\ ( https://arxiv.org/abs/2402.18495 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18510
replaced with revised version Thu, 29 Feb 2024 07:06:10 GMT   (333kb,D)

Title: RNNs are not Transformers (Yet): The Key Bottleneck on In-context
  Retrieval
Authors: Kaiyue Wen, Xingyu Dang, Kaifeng Lyu
Categories: cs.LG cs.CL stat.ML
Comments: 42 pages, 5 figures
\\ ( https://arxiv.org/abs/2402.18510 ,  333kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18546
replaced with revised version Thu, 29 Feb 2024 18:35:58 GMT   (7503kb,D)

Title: Generalizability Under Sensor Failure: Tokenization + Transformers
  Enable More Robust Latent Spaces
Authors: Geeling Chau, Yujin An, Ahamed Raffey Iqbal, Soon-Jo Chung, Yisong
  Yue, Sabera Talukder
Categories: cs.LG
Comments: 8 pages, 8 figures
\\ ( https://arxiv.org/abs/2402.18546 ,  7503kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18571
replaced with revised version Thu, 29 Feb 2024 04:33:29 GMT   (320kb,D)

Title: Arithmetic Control of LLMs for Diverse User Preferences: Directional
  Preference Alignment with Multi-Objective Rewards
Authors: Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu,
  Han Zhao, Tong Zhang
Categories: cs.LG cs.AI cs.CL stat.ML
Comments: The code and model are released at
  https://github.com/Haoxiang-Wang/directional-preference-alignment
\\ ( https://arxiv.org/abs/2402.18571 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2106.10479
replaced with revised version Thu, 29 Feb 2024 06:29:09 GMT   (1148kb,D)

Title: Practical Transferability Estimation for Image Classification Tasks
Authors: Yang Tan, Yang Li, Shao-Lun Huang
Categories: cs.CV cs.AI cs.LG
Comments: This paper is not the latest version. Please refer to
  Transferability-Guided Cross-Domain Cross-Task Transfer Learning (IEEE
  TNNLS'24) for more
  details.https://ieeexplore.ieee.org/abstract/document/10420486
\\ ( https://arxiv.org/abs/2106.10479 ,  1148kb)
------------------------------------------------------------------------------
\\
arXiv:2207.05510
replaced with revised version Thu, 29 Feb 2024 06:53:00 GMT   (6790kb,D)

Title: Transferability-Guided Cross-Domain Cross-Task Transfer Learning
Authors: Yang Tan, Enming Zhang, Yang Li, Shao-Lun Huang, Xiao-Ping Zhang
Categories: cs.CV cs.AI cs.LG
Comments: This work is accepted by IEEE TNNLS. Please see the official version
  https://ieeexplore.ieee.org/abstract/document/10420486.Copyright may be
  transferred without notice, after which this version may no longer be
  accessible
\\ ( https://arxiv.org/abs/2207.05510 ,  6790kb)
------------------------------------------------------------------------------
\\
arXiv:2302.13991
replaced with revised version Thu, 29 Feb 2024 18:18:00 GMT   (892kb,D)

Title: Learning to Generalize towards Unseen Domains via a Content-Aware Style
  Invariant Model for Disease Detection from Chest X-rays
Authors: Mohammad Zunaed, Md. Aynal Haque, Taufiq Hasan
Categories: cs.CV cs.AI cs.LG eess.IV
Comments: Accepted to IEEE Journal of Biomedical and Health Informatics
\\ ( https://arxiv.org/abs/2302.13991 ,  892kb)
------------------------------------------------------------------------------
\\
arXiv:2304.05805 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 15:32:15 GMT   (1137kb,D)

Title: GDP nowcasting with artificial neural networks: How much does long-term
  memory matter?
Authors: Krist\'of N\'emeth, D\'aniel Hadh\'azi
Categories: econ.EM cs.AI cs.LG
Comments: arXiv admin note: text overlap with arXiv:2106.08901 by other authors
\\ ( https://arxiv.org/abs/2304.05805 ,  1137kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00950
replaced with revised version Wed, 28 Feb 2024 21:10:08 GMT   (33512kb,D)

Title: Differential Diffusion: Giving Each Pixel Its Strength
Authors: Eran Levin, Ohad Fried
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: Project Page: https://differential-diffusion.github.io/
ACM-class: I.3.3
\\ ( https://arxiv.org/abs/2306.00950 ,  33512kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02140
replaced with revised version Thu, 29 Feb 2024 14:42:23 GMT   (1784kb,D)

Title: Towards Open Federated Learning Platforms: Survey and Vision from
  Technical and Legal Perspectives
Authors: Moming Duan, Qinbin Li, Linshan Jiang, Bingsheng He
Categories: cs.SE cs.AI cs.LG
Comments: Download Appendix from
  https://github.com/morningD/Towards-Open-Federated-Learning-Platforms-Survey/blob/main/TKDE-Tex/APPENDIX.pdf
\\ ( https://arxiv.org/abs/2307.02140 ,  1784kb)
------------------------------------------------------------------------------
\\
arXiv:2307.10811
replaced with revised version Thu, 29 Feb 2024 15:53:12 GMT   (3218kb,D)

Title: "It Felt Like Having a Second Mind": Investigating Human-AI
  Co-creativity in Prewriting with Large Language Models
Authors: Qian Wan, Siying Hu, Yu Zhang, Piaohong Wang, Bo Wen, Zhicong Lu
Categories: cs.HC cs.AI cs.CL
Comments: To appear at ACM CSCW 2024; Accepted to PACM HCI (CSCW); 25 pages, 2
  figures
ACM-class: H.5.m; K.4.0
Journal-ref: Proc. ACM Hum.-Comput. Interact. 8, CSCW1, Article 84 (2024)
DOI: 10.1145/3637361
\\ ( https://arxiv.org/abs/2307.10811 ,  3218kb)
------------------------------------------------------------------------------
\\
arXiv:2307.15852 (*cross-listing*)
replaced with revised version Wed, 28 Feb 2024 21:52:19 GMT   (18870kb,D)

Title: Dimensionless Policies based on the Buckingham $\pi$ Theorem: Is This a
  Good Way to Generalize Numerical Results?
Authors: Alexandre Girard
Categories: math.OC cs.AI cs.RO cs.SY eess.SY
MSC-class: 70Q05 (Primary), 00A73, 93C85, 68T40 (Secondary)
Journal-ref: Mathematics 2024, 12(5), 709
DOI: 10.3390/math12050709
\\ ( https://arxiv.org/abs/2307.15852 ,  18870kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10686
replaced with revised version Thu, 29 Feb 2024 13:10:16 GMT   (10537kb,D)

Title: Normative Conditional Reasoning as a Fragment of HOL
Authors: Xavier Parent and Christoph Benzm\"uller
Categories: cs.LO cs.AI cs.SC
Comments: 30 pages, 34 figures, 3 tables
MSC-class: 03B60, 03B15, 68T27, 68T30, 68T15
ACM-class: I.2.3; I.2.4; I.2.0; F.4
\\ ( https://arxiv.org/abs/2308.10686 ,  10537kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11131
replaced with revised version Thu, 29 Feb 2024 05:55:30 GMT   (850kb,D)

Title: ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential
  Behavior Comprehension in Recommendation
Authors: Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang
  Quan, Ruiming Tang, Yong Yu, Weinan Zhang
Categories: cs.IR cs.AI
Comments: Accepted by WWW 2024. Full and More Readable Version
\\ ( https://arxiv.org/abs/2308.11131 ,  850kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02561
replaced with revised version Thu, 29 Feb 2024 08:44:12 GMT   (12155kb,D)

Title: Physically Grounded Vision-Language Models for Robotic Manipulation
Authors: Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian
  Ichter, Anirudha Majumdar, Dorsa Sadigh
Categories: cs.RO cs.AI cs.CV
Comments: Updated version for ICRA 2024
\\ ( https://arxiv.org/abs/2309.02561 ,  12155kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12309
replaced with revised version Thu, 29 Feb 2024 06:38:27 GMT   (2459kb,D)

Title: Rehearsal: Simulating Conflict to Teach Conflict Resolution
Authors: Omar Shaikh, Valentino Chai, Michele J. Gelfand, Diyi Yang, Michael S.
  Bernstein
Categories: cs.HC cs.AI cs.CL
Comments: CHI 2024
\\ ( https://arxiv.org/abs/2309.12309 ,  2459kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16909
replaced with revised version Thu, 29 Feb 2024 07:05:52 GMT   (5008kb,D)

Title: ASAP: Automated Sequence Planning for Complex Robotic Assembly with
  Physical Feasibility
Authors: Yunsheng Tian, Karl D.D. Willis, Bassel Al Omari, Jieliang Luo,
  Pingchuan Ma, Yichen Li, Farhad Javid, Edward Gu, Joshua Jacob, Shinjiro
  Sueda, Hui Li, Sachin Chitta and Wojciech Matusik
Categories: cs.RO cs.AI cs.GR
Comments: ICRA 2024
\\ ( https://arxiv.org/abs/2309.16909 ,  5008kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17260
replaced with revised version Thu, 29 Feb 2024 12:09:33 GMT   (4643kb,D)

Title: PlaceNav: Topological Navigation through Place Recognition
Authors: Lauri Suomela, Jussi Kalliola, Harry Edelman, Joni-Kristian
  K\"am\"ar\"ainen
Categories: cs.RO cs.AI cs.LG
Comments: ICRA2024 camera ready
\\ ( https://arxiv.org/abs/2309.17260 ,  4643kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02195
replaced with revised version Thu, 29 Feb 2024 17:56:43 GMT   (81kb)

Title: Efficient Online Scheduling and Routing for Automated Guided Vehicles In
  Loop-Based Graphs
Authors: Louis Stubbe, Jens Goemaere, Jan Goedgebeur
Categories: cs.CE cs.AI
Comments: 15 pages, 4 figures
\\ ( https://arxiv.org/abs/2310.02195 ,  81kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14175
replaced with revised version Wed, 28 Feb 2024 21:02:56 GMT   (8235kb,D)

Title: Appearance-based gaze estimation enhanced with synthetic images using
  deep neural networks
Authors: Dmytro Herashchenko and Igor Farka\v{s}
Categories: cs.CV cs.AI
Comments: 6 pages, 10 figures, accepted to 2023 IEEE Symposium Series on
  Computational Intelligence (SSCI). Published version copyrighted by IEEE.
  This work was funded by the Horizon Europe Twinning project TERAIS G.A.
  number 101079338, and in part by the national project APVV-21-0105. The link
  to the code: https://zenodo.org/doi/10.5281/zenodo.10696083
ACM-class: I.2.10
\\ ( https://arxiv.org/abs/2311.14175 ,  8235kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17950
replaced with revised version Thu, 29 Feb 2024 13:23:00 GMT   (12849kb,D)

Title: Generalized Large-Scale Data Condensation via Various Backbone and
  Statistical Matching
Authors: Shitong Shao, Zeyuan Yin, Muxin Zhou, Xindong Zhang and Zhiqiang Shen
Categories: cs.CV cs.AI
Comments: Accepted by CVPR2024
\\ ( https://arxiv.org/abs/2311.17950 ,  12849kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07337
replaced with revised version Thu, 29 Feb 2024 13:05:58 GMT   (19567kb,D)

Title: RMS: Redundancy-Minimizing Point Cloud Sampling for Real-Time Pose
  Estimation
Authors: Pavel Petracek, Kostas Alexis, Martin Saska
Categories: cs.RO cs.AI
Comments: Submitted to IEEE RA-L on December 1, 2023. Resubmitted on February
  28, 2024
\\ ( https://arxiv.org/abs/2312.07337 ,  19567kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09468
replaced with revised version Wed, 28 Feb 2024 21:04:12 GMT   (889kb,D)

Title: Safe Reinforcement Learning in a Simulated Robotic Arm
Authors: Luka Kova\v{c} and Igor Farka\v{s}
Categories: cs.RO cs.AI cs.LG
Comments: 4 pages, 2 figures. Appeared in 2023 International Conference on
  Artificial Neural Networks (ICANN) proceedings. Published version copyrighted
  by Springer. This work was funded by the Horizon Europe Twinning project
  TERAIS, G.A. number 101079338 and in part by the national project
  APVV-21-0105. Link to the code:
  https://zenodo.org/doi/10.5281/zenodo.10694747
\\ ( https://arxiv.org/abs/2312.09468 ,  889kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14949
replaced with revised version Thu, 29 Feb 2024 10:55:25 GMT   (445kb,D)

Title: LLM Interactive Optimization of Open Source Python Libraries -- Case
  Studies and Generalization
Authors: Andreas Florath
Categories: cs.SE cs.AI cs.HC cs.PF
Comments: 20 pages, 10 figures
\\ ( https://arxiv.org/abs/2312.14949 ,  445kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06788 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 18:09:40 GMT   (81kb,D)

Title: The NPU-ASLP-LiAuto System Description for Visual Speech Recognition in
  CNVSRC 2023
Authors: He Wang, Pengcheng Guo, Wei Chen, Pan Zhou, Lei Xie
Categories: eess.AS cs.AI cs.SD
Comments: Included in CNVSRC Workshop 2023, NCMMSC 2023
\\ ( https://arxiv.org/abs/2401.06788 ,  81kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12202
replaced with revised version Thu, 29 Feb 2024 17:20:08 GMT   (7579kb,D)

Title: OK-Robot: What Really Matters in Integrating Open-Knowledge Models for
  Robotics
Authors: Peiqi Liu, Yaswanth Orru, Jay Vakil, Chris Paxton, Nur Muhammad Mahi
  Shafiullah, Lerrel Pinto
Categories: cs.RO cs.AI cs.CV cs.LG
Comments: Github repo: https://github.com/ok-robot/ok-robot
\\ ( https://arxiv.org/abs/2401.12202 ,  7579kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14292
replaced with revised version Thu, 29 Feb 2024 17:23:01 GMT   (7798kb,D)

Title: Single and bi-layered 2-D acoustic soft tactile skin (AST2)
Authors: Vishnu Rajendran, Simon Parsons and Amir Ghalamzan E
Categories: cs.RO cs.AI
Comments: IEEE Robosoft conference 2024 (accepted)
\\ ( https://arxiv.org/abs/2401.14292 ,  7798kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15721
replaced with revised version Thu, 29 Feb 2024 11:02:02 GMT   (1774kb,D)

Title: A Study of Acquisition Functions for Medical Imaging Deep Active
  Learning
Authors: Bonaventure F. P. Dossou
Categories: cs.CV cs.AI cs.HC cs.LG
Comments: Best Poster Award at Deep Learning Indaba 2023 Conference
\\ ( https://arxiv.org/abs/2401.15721 ,  1774kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15741
replaced with revised version Thu, 29 Feb 2024 09:20:12 GMT   (1032kb)

Title: SERNet-Former: Semantic Segmentation by Efficient Residual Network with
  Attention-Boosting Gates and Attention-Fusion Networks
Authors: Serdar Erisen
Categories: cs.CV cs.AI
DOI: 10.48550/arXiv.2401.15741
\\ ( https://arxiv.org/abs/2401.15741 ,  1032kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17010
replaced with revised version Thu, 29 Feb 2024 07:42:40 GMT   (77kb)

Title: Finetuning Large Language Models for Vulnerability Detection
Authors: Alexey Shestov, Rodion Levichev, Ravil Mussabayev, Anton Cheshkov
Categories: cs.CR cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.17010 ,  77kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01744 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 16:05:32 GMT   (2065kb,D)

Title: Unveiling Molecular Moieties through Hierarchical Graph Explainability
Authors: Paolo Sortino, Salvatore Contino, Ugo Perricone and Roberto Pirrone
Categories: q-bio.QM cs.AI cs.LG q-bio.MN
\\ ( https://arxiv.org/abs/2402.01744 ,  2065kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12928
replaced with revised version Thu, 29 Feb 2024 15:34:40 GMT   (4218kb,D)

Title: A Literature Review of Literature Reviews in Pattern Analysis and
  Machine Intelligence
Authors: Penghai Zhao, Xin Zhang, Ming-Ming Cheng, Jian Yang, Xiang Li
Categories: cs.DL cs.AI cs.CV
Comments: IEEE version v1. [February 19, 2024] IEEE version v2 with typos
  fixed. [February 23, 2024] IEEE version v3 with errors fixed. [February 29,
  2024]
\\ ( https://arxiv.org/abs/2402.12928 ,  4218kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14096
replaced with revised version Thu, 29 Feb 2024 13:33:58 GMT   (9169kb,D)

Title: EyeTrans: Merging Human and Machine Attention for Neural Code
  Summarization
Authors: Yifan Zhang, Jiliang Li, Zachary Karas, Aakash Bansal, Toby Jia-Jun
  Li, Collin McMillan, Kevin Leach, Yu Huang
Categories: cs.SE cs.AI cs.HC
DOI: 10.1145/3643732
\\ ( https://arxiv.org/abs/2402.14096 ,  9169kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16068
replaced with revised version Thu, 29 Feb 2024 10:26:21 GMT   (847kb,D)

Title: ROS-Causal: A ROS-based Causal Analysis Framework for Human-Robot
  Interaction Applications
Authors: Luca Castri, Gloria Beraldo, Sariah Mghames, Marc Hanheide, Nicola
  Bellotto
Categories: cs.RO cs.AI
Comments: Accepted by the "Causal-HRI: Causal Learning for Human-Robot
  Interaction" workshop at the 2024 ACM/IEEE International Conference on
  Human-Robot Interaction (HRI)
\\ ( https://arxiv.org/abs/2402.16068 ,  847kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16235
replaced with revised version Thu, 29 Feb 2024 05:22:01 GMT   (1964kb,D)

Title: Human-AI Co-Creation of Worked Examples for Programming Classes
Authors: Mohammad Hassany, Peter Brusilovsky, Jiaze Ke, Kamil Akhuseyinoglu and
  Arun Balajiee Lekshmi Narayanan
Categories: cs.HC cs.AI
Comments: arXiv admin note: substantial text overlap with arXiv:2312.02105
\\ ( https://arxiv.org/abs/2402.16235 ,  1964kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16749
replaced with revised version Thu, 29 Feb 2024 16:53:20 GMT   (48616kb,D)

Title: MISC: Ultra-low Bitrate Image Semantic Compression Driven by Large
  Multimodal Model
Authors: Chunyi Li, Guo Lu, Donghui Feng, Haoning Wu, Zicheng Zhang, Xiaohong
  Liu, Guangtao Zhai, Weisi Lin, Wenjun Zhang
Categories: cs.CV cs.AI eess.IV
\\ ( https://arxiv.org/abs/2402.16749 ,  48616kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17289
replaced with revised version Thu, 29 Feb 2024 15:54:46 GMT   (4929kb,D)

Title: Active propulsion noise shaping for multi-rotor aircraft localization
Authors: Gabriele Serussi, Tamir Shor, Tom Hirshberg, Chaim Baskin, Alex
  Bronstein
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2402.17289 ,  4929kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17398 (*cross-listing*)
replaced with revised version Wed, 28 Feb 2024 22:33:55 GMT   (2301kb,D)

Title: A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE)
Authors: Nishikanta Mohanty, Bikash K. Behera, Christopher Ferrie and Pravat
  Dash
Categories: quant-ph cs.AI cs.LG
Comments: 18 Pages, 22 Figures, 2 Tables
\\ ( https://arxiv.org/abs/2402.17398 ,  2301kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18222
replaced with revised version Thu, 29 Feb 2024 05:11:05 GMT   (10560kb,D)

Title: HearHere: Mitigating Echo Chambers in News Consumption through an
  AI-based Web System
Authors: Youngseung Jeon, Jaehoon Kim, Sohyun Park, Yunyong Ko, Seongeun Ryu,
  Sang-Wook Kim, Kyungsik Han
Categories: cs.HC cs.AI
Comments: 34 pages, 6 figures, 6 tables, CSCW 2024
\\ ( https://arxiv.org/abs/2402.18222 ,  10560kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18485 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 12:49:03 GMT   (14008kb,D)

Title: A Multimodal Foundation Agent for Financial Trading: Tool-Augmented,
  Diversified, and Generalist
Authors: Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei
  Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, Longtao Zheng, Xinrun
  Wang, Bo An
Categories: q-fin.TR cs.AI
\\ ( https://arxiv.org/abs/2402.18485 ,  14008kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06356
replaced with revised version Thu, 29 Feb 2024 14:15:30 GMT   (673kb,D)

Title: A Semantic Invariant Robust Watermark for Large Language Models
Authors: Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng and Lijie Wen
Categories: cs.CR cs.CL
Comments: ICLR2024, 21 pages, 10 figures, 6 tables
MSC-class: 68T50
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2310.06356 ,  673kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14566
replaced with revised version Thu, 29 Feb 2024 09:04:49 GMT   (23012kb,D)

Title: HallusionBench: An Advanced Diagnostic Suite for Entangled Language
  Hallucination and Visual Illusion in Large Vision-Language Models
Authors: Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu
  Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha,
  Tianyi Zhou
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2310.14566 ,  23012kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18275
replaced with revised version Thu, 29 Feb 2024 05:28:18 GMT   (126kb,D)

Title: Investigation of Adapter for Automatic Speech Recognition in Noisy
  Environment
Authors: Hao Shi, Tatsuya Kawahara
Categories: cs.SD cs.CL eess.AS
\\ ( https://arxiv.org/abs/2402.18275 ,  126kb)
------------------------------------------------------------------------------
\\
arXiv:2203.01360
replaced with revised version Thu, 29 Feb 2024 16:33:52 GMT   (747kb,D)

Title: Neural Galerkin Schemes with Active Learning for High-Dimensional
  Evolution Equations
Authors: Joan Bruna and Benjamin Peherstorfer and Eric Vanden-Eijnden
Categories: math.NA cs.LG cs.NA stat.ML
Journal-ref: Journal of Computational Physics, Volume 496, 2024
DOI: 10.1016/j.jcp.2023.112588
\\ ( https://arxiv.org/abs/2203.01360 ,  747kb)
------------------------------------------------------------------------------
\\
arXiv:2210.14484 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 16:21:33 GMT   (741kb,D)

Title: Imputation of missing values in multi-view data
Authors: Wouter van Loon, Marjolein Fokkema, Frank de Vos, Marisa Koini,
  Reinhold Schmidt, Mark de Rooij
Categories: stat.ML cs.LG stat.ME
Comments: 48 pages, 15 figures. Major revisions
\\ ( https://arxiv.org/abs/2210.14484 ,  741kb)
------------------------------------------------------------------------------
\\
arXiv:2302.06089
replaced with revised version Thu, 29 Feb 2024 14:57:29 GMT   (6236kb,D)

Title: Federated attention contrastive learning models for prostate cancer
  diagnosis and Gleason grading
Authors: Fei Kong, Xiyue Wang, Jinxi Xiang, Sen Yang, Xinran Wang, Meng Yue,
  Jun Zhang, Junhan Zhao, Xiao Han, Yuhan Dong, Biyue Zhu, Fang Wang, Yueping
  Liu
Categories: cs.CV cs.LG q-bio.QM
Comments: 15 pages
\\ ( https://arxiv.org/abs/2302.06089 ,  6236kb)
------------------------------------------------------------------------------
\\
arXiv:2303.17674 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 16:14:01 GMT   (7918kb,D)

Title: Convex Hulls of Reachable Sets
Authors: Thomas Lew, Riccardo Bonalli, Marco Pavone
Categories: math.OC cs.LG cs.RO cs.SY eess.SY
Comments: 19 pages. Submitted to the IEEE Transactions on Automatic Control.
  Substantial extension of arXiv:2303.17674v2
\\ ( https://arxiv.org/abs/2303.17674 ,  7918kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15560
replaced with revised version Thu, 29 Feb 2024 08:14:16 GMT   (12144kb,D)

Title: Differentially Private Synthetic Data via Foundation Model APIs 1:
  Images
Authors: Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Harsha Nori, Sergey
  Yekhanin
Categories: cs.CV cs.CR cs.LG
Comments: 49 pages, 42 figures
\\ ( https://arxiv.org/abs/2305.15560 ,  12144kb)
------------------------------------------------------------------------------
\\
arXiv:2306.15012 (*cross-listing*)
replaced with revised version Wed, 28 Feb 2024 21:52:25 GMT   (6033kb,D)

Title: Statistical Component Separation for Targeted Signal Recovery in Noisy
  Mixtures
Authors: Bruno R\'egaldo-Saint Blancard, Michael Eickenberg
Categories: stat.ML astro-ph.IM cs.LG eess.SP
Comments: 13+17 pages, 6+8 figures, published in TMLR, code:
  https://github.com/bregaldo/stat_comp_sep
\\ ( https://arxiv.org/abs/2306.15012 ,  6033kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07357 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 09:27:16 GMT   (4095kb,D)

Title: Inverse Optimization for Routing Problems
Authors: Pedro Zattoni Scroccaro, Piet van Beek, Peyman Mohajerin Esfahani,
  Bilge Atasoy
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2307.07357 ,  4095kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07470
replaced with revised version Wed, 28 Feb 2024 21:40:00 GMT   (563kb,D)

Title: Symphony: Optimized DNN Model Serving using Deferred Batch Scheduling
Authors: Lequn Chen, Weixin Deng, Anirudh Canumalla, Yu Xin, Danyang Zhuo,
  Matthai Philipose, Arvind Krishnamurthy
Categories: cs.DC cs.LG
\\ ( https://arxiv.org/abs/2308.07470 ,  563kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08567 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 10:28:35 GMT   (1674kb)

Title: CMISR: Circular Medical Image Super-Resolution
Authors: Honggui Li, Nahid Md Lokman Hossain, Maria Trocan, Dimitri Galayko,
  Mohamad Sawan
Categories: eess.IV cs.LG
\\ ( https://arxiv.org/abs/2308.08567 ,  1674kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14947
replaced with revised version Wed, 28 Feb 2024 21:59:22 GMT   (3806kb,D)

Title: Improving Generalization in Reinforcement Learning Training Regimes for
  Social Robot Navigation
Authors: Adam Sigal, Hsiu-Chin Lin, AJung Moon
Categories: cs.RO cs.LG cs.MA
Comments: NeurIPS 2023 Workshop on Generalization in Planning, 2023
\\ ( https://arxiv.org/abs/2308.14947 ,  3806kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16598 (*cross-listing*)
replaced with revised version Wed, 28 Feb 2024 22:34:51 GMT   (13505kb,D)

Title: Cross-Prediction-Powered Inference
Authors: Tijana Zrnic, Emmanuel J. Cand\`es
Categories: stat.ML cs.LG stat.ME
\\ ( https://arxiv.org/abs/2309.16598 ,  13505kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01236 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 06:07:33 GMT   (4031kb,D)

Title: Mirror Diffusion Models for Constrained and Watermarked Generation
Authors: Guan-Horng Liu, Tianrong Chen, Evangelos A. Theodorou, Molei Tao
Categories: stat.ML cs.CV cs.LG
Comments: submitted to NeurIPS on 5/18 but did not arxiv per NeurIPS policy,
  accepted on 9/22
\\ ( https://arxiv.org/abs/2310.01236 ,  4031kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01678 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 01:57:56 GMT   (5530kb,D)

Title: Score dynamics: scaling molecular dynamics with picosecond timesteps via
  conditional diffusion model
Authors: Tim Hsu, Babak Sadigh, Vasily Bulatov, Fei Zhou
Categories: physics.comp-ph cs.LG
\\ ( https://arxiv.org/abs/2310.01678 ,  5530kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07576
replaced with revised version Thu, 29 Feb 2024 01:25:43 GMT   (223kb,D)

Title: Analyzing Trendy Twitter Hashtags in the 2022 French Election
Authors: Aamir Mandviwalla, Lake Yin, Boleslaw K. Szymanski
Categories: cs.SI cs.LG
Comments: 9 pages, 1 figure, published in Complex Networks and their
  Applications XII
DOI: 10.1007/978-3-031-53468-3_18
\\ ( https://arxiv.org/abs/2310.07576 ,  223kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11143 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 16:38:27 GMT   (2787kb)

Title: Exploring a new machine learning based probabilistic model for
  high-resolution indoor radon mapping, using the German indoor radon survey
  data
Authors: Eric Petermann, Peter Bossew, Joachim Kemski, Valeria Gruber, Nils
  Suhr and Bernd Hoffmann
Categories: stat.ML cs.LG physics.data-an
\\ ( https://arxiv.org/abs/2310.11143 ,  2787kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01410
replaced with revised version Thu, 29 Feb 2024 14:31:13 GMT   (9781kb,D)

Title: The Blessing of Randomness: SDE Beats ODE in General Diffusion-based
  Image Editing
Authors: Shen Nie, Hanzhong Allan Guo, Cheng Lu, Yuhao Zhou, Chenyu Zheng,
  Chongxuan Li
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2311.01410 ,  9781kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13250
replaced with revised version Thu, 29 Feb 2024 03:32:35 GMT   (2211kb,D)

Title: FedHCA$^2$: Towards Hetero-Client Federated Multi-Task Learning
Authors: Yuxiang Lu, Suizhi Huang, Yuwen Yang, Shalayiding Sirejiding, Yue
  Ding, Hongtao Lu
Categories: cs.CV cs.LG
Comments: Accepted by CVPR 2024
\\ ( https://arxiv.org/abs/2311.13250 ,  2211kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02959 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 13:30:59 GMT   (8843kb,D)

Title: Detecting algorithmic bias in medical AI-models
Authors: Jeffrey Smith, Andre Holder, Rishikesan Kamaleswaran, Yao Xie
Categories: stat.ML cs.CY cs.LG stat.AP
Comments: 26 pages, 9 figures
\\ ( https://arxiv.org/abs/2312.02959 ,  8843kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05248
replaced with revised version Thu, 29 Feb 2024 12:36:14 GMT   (274kb,D)

Title: Topology-Based Reconstruction Prevention for Decentralised Learning
Authors: Florine W. Dekker (1), Zekeriya Erkin (1), Mauro Conti (2 and 1) ((1)
  Delft University of Technology, the Netherlands and (2) Universit\`a di
  Padova, Italy)
Categories: cs.CR cs.DC cs.DM cs.LG
Comments: 13 pages, 8 figures, submitted to PETS 2024, for associated
  experiment source code see doi:10.4121/21572601
ACM-class: C.2.4; D.4.6; G.2.2
\\ ( https://arxiv.org/abs/2312.05248 ,  274kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16624 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 15:40:34 GMT   (16638kb,D)

Title: Dual-stage optimizer for systematic overestimation adjustment applied to
  multi-objective genetic algorithms for biomarker selection
Authors: Luca Cattelani and Vittorio Fortino
Categories: q-bio.QM cs.LG
Comments: Added link to source code repository
\\ ( https://arxiv.org/abs/2312.16624 ,  16638kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02694 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 02:35:54 GMT   (724kb,D)

Title: Description on IEEE ICME 2024 Grand Challenge: Semi-supervised Acoustic
  Scene Classification under Domain Shift
Authors: Jisheng Bai, Mou Wang, Haohe Liu, Han Yin, Yafei Jia, Siwei Huang,
  Yutong Du, Dongzhe Zhang, Dongyuan Shi, Woon-Seng Gan, Mark D. Plumbley,
  Susanto Rahardja, Bin Xiang, Jianfeng Chen
Categories: eess.AS cs.LG cs.SD
\\ ( https://arxiv.org/abs/2402.02694 ,  724kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09164
replaced with revised version Thu, 29 Feb 2024 03:29:41 GMT   (6261kb,D)

Title: Less is More: Fewer Interpretable Region via Submodular Subset Selection
Authors: Ruoyu Chen, Hua Zhang, Siyuan Liang, Jingzhi Li, Xiaochun Cao
Categories: cs.CV cs.LG
Comments: Accepted to ICLR 2024 (Oral)
\\ ( https://arxiv.org/abs/2402.09164 ,  6261kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14148 (*cross-listing*)
replaced with revised version Wed, 28 Feb 2024 21:09:17 GMT   (2320kb,D)

Title: Neural Networks and Friction: Slide, Hold, Learn
Authors: Joaquin Garcia-Suarez
Categories: physics.geo-ph cs.LG
Comments: 10 paged, 10 figures, 2 tables
\\ ( https://arxiv.org/abs/2402.14148 ,  2320kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15893
replaced with revised version Thu, 29 Feb 2024 11:50:18 GMT   (3843kb,D)

Title: Concurrent Learning of Policy and Unknown Safety Constraints in
  Reinforcement Learning
Authors: Lunet Yifru and Ali Baheri
Categories: eess.SY cs.LG cs.SY
\\ ( https://arxiv.org/abs/2402.15893 ,  3843kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16326 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 09:05:38 GMT   (10841kb,D)

Title: A Provably Accurate Randomized Sampling Algorithm for Logistic
  Regression
Authors: Agniva Chowdhury, Pradeep Ramuhalli
Categories: stat.ML cs.DS cs.LG
Comments: To appear in the proceedings of AAAI 2024
\\ ( https://arxiv.org/abs/2402.16326 ,  10841kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17886 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 15:27:06 GMT   (3337kb,D)

Title: Zeroth-Order Sampling Methods for Non-Log-Concave Distributions:
  Alleviating Metastability by Denoising Diffusion
Authors: Ye He, Kevin Rojas, Molei Tao
Categories: stat.ML cs.LG math.PR math.ST stat.ME stat.TH
Comments: Figure 4 on page 13 corrected. Comments are welcome
\\ ( https://arxiv.org/abs/2402.17886 ,  3337kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
