paper_240307.txt

Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月7日 17:17
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Tue  5 Mar 24 19:00:00 GMT  to  Wed  6 Mar 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.03288
Date: Tue, 5 Mar 2024 19:40:53 GMT   (601kb)

Title: Should We Fear Large Language Models? A Structural Analysis of the Human
  Reasoning System for Elucidating LLM Capabilities and Risks Through the Lens
  of Heidegger's Philosophy
Authors: Jianqiiu Zhang
Categories: cs.AI
Comments: 39 pages
\\
  In the rapidly evolving field of Large Language Models (LLMs), there is a
critical need to thoroughly analyze their capabilities and risks. Central to
our investigation are two novel elements. Firstly, it is the innovative
parallels between the statistical patterns of word relationships within LLMs
and Martin Heidegger's concepts of "ready-to-hand" and "present-at-hand," which
encapsulate the utilitarian and scientific altitudes humans employ in
interacting with the world. This comparison lays the groundwork for positioning
LLMs as the digital counterpart to the Faculty of Verbal Knowledge, shedding
light on their capacity to emulate certain facets of human reasoning. Secondly,
a structural analysis of human reasoning, viewed through Heidegger's notion of
truth as "unconcealment" is conducted This foundational principle enables us to
map out the inputs and outputs of the reasoning system and divide reasoning
into four distinct categories. Respective cognitive faculties are delineated,
allowing us to place LLMs within the broader schema of human reasoning, thus
clarifying their strengths and inherent limitations. Our findings reveal that
while LLMs possess the capability for Direct Explicative Reasoning and Pseudo
Rational Reasoning, they fall short in authentic rational reasoning and have no
creative reasoning capabilities, due to the current lack of many analogous AI
models such as the Faculty of Judgement. The potential and risks of LLMs when
they are augmented with other AI technologies are also evaluated. The results
indicate that although LLMs have achieved proficiency in some reasoning
abilities, the aspiration to match or exceed human intellectual capabilities is
yet unattained. This research not only enriches our comprehension of LLMs but
also propels forward the discourse on AI's potential and its bounds, paving the
way for future explorations into AI's evolving landscape.
\\ ( https://arxiv.org/abs/2403.03288 ,  601kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03293
Date: Tue, 5 Mar 2024 19:47:57 GMT   (576kb,D)

Title: AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research
  Paper Analysis
Authors: Anjalee De Silva, Janaka L. Wijekoon, Rashini Liyanarachchi, Rrubaa
  Panchendrarajan, Weranga Rajapaksha
Categories: cs.AI
\\
  This paper discusses the effectiveness of leveraging Chatbot: Generative
Pre-trained Transformer (ChatGPT) versions 3.5 and 4 for analyzing research
papers for effective writing of scientific literature surveys. The study
selected the \textit{Application of Artificial Intelligence in Breast Cancer
Treatment} as the research topic. Research papers related to this topic were
collected from three major publication databases Google Scholar, Pubmed, and
Scopus. ChatGPT models were used to identify the category, scope, and relevant
information from the research papers for automatic identification of relevant
papers related to Breast Cancer Treatment (BCT), organization of papers
according to scope, and identification of key information for survey paper
writing. Evaluations performed using ground truth data annotated using subject
experts reveal, that GPT-4 achieves 77.3\% accuracy in identifying the research
paper categories and 50\% of the papers were correctly identified by GPT-4 for
their scopes. Further, the results demonstrate that GPT-4 can generate reasons
for its decisions with an average of 27\% new words, and 67\% of the reasons
given by the model were completely agreeable to the subject experts.
\\ ( https://arxiv.org/abs/2403.03293 ,  576kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03357
Date: Tue, 5 Mar 2024 22:54:15 GMT   (3268kb,D)

Title: The Case for Globalizing Fairness: A Mixed Methods Study on Colonialism,
  AI, and Health in Africa
Authors: Mercy Asiedu, Awa Dieng, Alexander Haykel, Negar Rostamzadeh, Stephen
  Pfohl, Chirag Nagpal, Maria Nagawa, Abigail Oppong, Sanmi Koyejo, Katherine
  Heller
Categories: cs.AI cs.CY
Comments: 11 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:2304.02190
\\
  With growing application of machine learning (ML) technologies in healthcare,
there have been calls for developing techniques to understand and mitigate
biases these systems may exhibit. Fair-ness considerations in the development
of ML-based solutions for health have particular implications for Africa, which
already faces inequitable power imbalances between the Global North and
South.This paper seeks to explore fairness for global health, with Africa as a
case study. We conduct a scoping review to propose axes of disparities for
fairness consideration in the African context and delineate where they may come
into play in different ML-enabled medical modalities. We then conduct
qualitative research studies with 672 general population study participants and
28 experts inML, health, and policy focused on Africa to obtain corroborative
evidence on the proposed axes of disparities. Our analysis focuses on
colonialism as the attribute of interest and examines the interplay between
artificial intelligence (AI), health, and colonialism. Among the pre-identified
attributes, we found that colonial history, country of origin, and national
income level were specific axes of disparities that participants believed would
cause an AI system to be biased.However, there was also divergence of opinion
between experts and general population participants. Whereas experts generally
expressed a shared view about the relevance of colonial history for the
development and implementation of AI technologies in Africa, the majority of
the general population participants surveyed did not think there was a direct
link between AI and colonialism. Based on these findings, we provide practical
recommendations for developing fairness-aware ML solutions for health in
Africa.
\\ ( https://arxiv.org/abs/2403.03357 ,  3268kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03359
Date: Tue, 5 Mar 2024 23:03:56 GMT   (20779kb,D)

Title: RACE-SM: Reinforcement Learning Based Autonomous Control for Social
  On-Ramp Merging
Authors: Jordan Poots
Categories: cs.AI cs.LG cs.RO
\\
  Autonomous parallel-style on-ramp merging in human controlled traffic
continues to be an existing issue for autonomous vehicle control. Existing
non-learning based solutions for vehicle control rely on rules and optimization
primarily. These methods have been seen to present significant challenges.
Recent advancements in Deep Reinforcement Learning have shown promise and have
received significant academic interest however the available learning based
approaches show inadequate attention to other highway vehicles and often rely
on inaccurate road traffic assumptions. In addition, the parallel-style case is
rarely considered. A novel learning based model for acceleration and lane
change decision making that explicitly considers the utility to both the ego
vehicle and its surrounding vehicles which may be cooperative or uncooperative
to produce behaviour that is socially acceptable is proposed. The novel reward
function makes use of Social Value Orientation to weight the vehicle's level of
social cooperation and is divided into ego vehicle and surrounding vehicle
utility which are weighted according to the model's designated Social Value
Orientation. A two-lane highway with an on-ramp divided into a taper-style and
parallel-style section is considered. Simulation results indicated the
importance of considering surrounding vehicles in reward function design and
show that the proposed model matches or surpasses those in literature in terms
of collisions while also introducing socially courteous behaviour avoiding near
misses and anti-social behaviour through direct consideration of the effect of
merging on surrounding vehicles.
\\ ( https://arxiv.org/abs/2403.03359 ,  20779kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03382
Date: Wed, 6 Mar 2024 00:17:03 GMT   (608kb,D)

Title: Adaptive Discovering and Merging for Incremental Novel Class Discovery
Authors: Guangyao Chen, Peixi Peng, Yangru Huang, Mengyue Geng, Yonghong Tian
Categories: cs.AI
Comments: AAAI 2024. arXiv admin note: text overlap with arXiv:2207.08605 by
  other authors
\\
  One important desideratum of lifelong learning aims to discover novel classes
from unlabelled data in a continuous manner. The central challenge is twofold:
discovering and learning novel classes while mitigating the issue of
catastrophic forgetting of established knowledge. To this end, we introduce a
new paradigm called Adaptive Discovering and Merging (ADM) to discover novel
categories adaptively in the incremental stage and integrate novel knowledge
into the model without affecting the original knowledge. To discover novel
classes adaptively, we decouple representation learning and novel class
discovery, and use Triple Comparison (TC) and Probability Regularization (PR)
to constrain the probability discrepancy and diversity for adaptive category
assignment. To merge the learned novel knowledge adaptively, we propose a
hybrid structure with base and novel branches named Adaptive Model Merging
(AMM), which reduces the interference of the novel branch on the old classes to
preserve the previous knowledge, and merges the novel branch to the base model
without performance loss and parameter growth. Extensive experiments on several
datasets show that ADM significantly outperforms existing class-incremental
Novel Class Discovery (class-iNCD) approaches. Moreover, our AMM also benefits
the class-incremental Learning (class-IL) task by alleviating the catastrophic
forgetting problem.
\\ ( https://arxiv.org/abs/2403.03382 ,  608kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03401
Date: Wed, 6 Mar 2024 01:56:17 GMT   (233kb,D)

Title: BAIT: Benchmarking (Embedding) Architectures for Interactive
  Theorem-Proving
Authors: Sean Lamont, Michael Norrish, Amir Dezfouli, Christian Walder, Paul
  Montague
Categories: cs.AI cs.LG cs.LO
\\
  Artificial Intelligence for Theorem Proving has given rise to a plethora of
benchmarks and methodologies, particularly in Interactive Theorem Proving
(ITP). Research in the area is fragmented, with a diverse set of approaches
being spread across several ITP systems. This presents a significant challenge
to the comparison of methods, which are often complex and difficult to
replicate. Addressing this, we present BAIT, a framework for fair and
streamlined comparison of learning approaches in ITP. We demonstrate BAIT's
capabilities with an in-depth comparison, across several ITP benchmarks, of
state-of-the-art architectures applied to the problem of formula embedding. We
find that Structure Aware Transformers perform particularly well, improving on
techniques associated with the original problem sets. BAIT also allows us to
assess the end-to-end proving performance of systems built on interactive
environments. This unified perspective reveals a novel end-to-end system that
improves on prior work. We also provide a qualitative analysis, illustrating
that improved performance is associated with more semantically-aware
embeddings. By streamlining the implementation and comparison of Machine
Learning algorithms in the ITP context, we anticipate BAIT will be a
springboard for future research.
\\ ( https://arxiv.org/abs/2403.03401 ,  233kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03406
Date: Wed, 6 Mar 2024 02:09:50 GMT   (688kb)

Title: An EnKF-LSTM Assimilation Algorithm for Crop Growth Model
Authors: Siqi Zhou, Ling Wang, Jie Liu, Jinshan Tang
Categories: cs.AI cs.LG
\\
  Accurate and timely prediction of crop growth is of great significance to
ensure crop yields and researchers have developed several crop models for the
prediction of crop growth. However, there are large difference between the
simulation results obtained by the crop models and the actual results, thus in
this paper, we proposed to combine the simulation results with the collected
crop data for data assimilation so that the accuracy of prediction will be
improved. In this paper, an EnKF-LSTM data assimilation method for various
crops is proposed by combining ensemble Kalman filter and LSTM neural network,
which effectively avoids the overfitting problem of existing data assimilation
methods and eliminates the uncertainty of the measured data. The verification
of the proposed EnKF-LSTM method and the comparison of the proposed method with
other data assimilation methods were performed using datasets collected by
sensor equipment deployed on a farm.
\\ ( https://arxiv.org/abs/2403.03406 ,  688kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03517
Date: Wed, 6 Mar 2024 07:54:40 GMT   (158kb,D)

Title: IB-Net: Initial Branch Network for Variable Decision in Boolean
  Satisfiability
Authors: Tsz Ho Chan, Wenyi Xiao, Junhua Huang, Huiling Zhen, Guangji Tian and
  Mingxuan Yuan
Categories: cs.AI
Comments: 7 pages, 12 figures
\\
  Boolean Satisfiability problems are vital components in Electronic Design
Automation, particularly within the Logic Equivalence Checking process.
Currently, SAT solvers are employed for these problems and neural network is
tried as assistance to solvers. However, as SAT problems in the LEC context are
distinctive due to their predominantly unsatisfiability nature and a
substantial proportion of UNSAT-core variables, existing neural network
assistance has proven unsuccessful in this specialized domain. To tackle this
challenge, we propose IB-Net, an innovative framework utilizing graph neural
networks and novel graph encoding techniques to model unsatisfiable problems
and interact with state-of-the-art solvers. Extensive evaluations across
solvers and datasets demonstrate IB-Net's acceleration, achieving an average
runtime speedup of 5.0% on industrial data and 8.3% on SAT competition data
empirically. This breakthrough advances efficient solving in LEC workflows.
\\ ( https://arxiv.org/abs/2403.03517 ,  158kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03544
Date: Wed, 6 Mar 2024 08:43:30 GMT   (674kb,D)

Title: Prompt Mining for Language-based Human Mobility Forecasting
Authors: Hao Xue, Tianye Tang, Ali Payani, Flora D. Salim
Categories: cs.AI cs.CL
\\
  With the advancement of large language models, language-based forecasting has
recently emerged as an innovative approach for predicting human mobility
patterns. The core idea is to use prompts to transform the raw mobility data
given as numerical values into natural language sentences so that the language
models can be leveraged to generate the description for future observations.
However, previous studies have only employed fixed and manually designed
templates to transform numerical values into sentences. Since the forecasting
performance of language models heavily relies on prompts, using fixed templates
for prompting may limit the forecasting capability of language models. In this
paper, we propose a novel framework for prompt mining in language-based
mobility forecasting, aiming to explore diverse prompt design strategies.
Specifically, the framework includes a prompt generation stage based on the
information entropy of prompts and a prompt refinement stage to integrate
mechanisms such as the chain of thought. Experimental results on real-world
large-scale data demonstrate the superiority of generated prompts from our
prompt mining pipeline. Additionally, the comparison of different prompt
variants shows that the proposed prompt refinement process is effective. Our
study presents a promising direction for further advancing language-based
mobility forecasting.
\\ ( https://arxiv.org/abs/2403.03544 ,  674kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03550
Date: Wed, 6 Mar 2024 08:50:25 GMT   (710kb)

Title: Emotional Manipulation Through Prompt Engineering Amplifies
  Disinformation Generation in AI Large Language Models
Authors: Rasita Vinay, Giovanni Spitale, Nikola Biller-Andorno, Federico
  Germani
Categories: cs.AI cs.CY cs.HC
Comments: 14 pages, 3 figures
\\
  This study investigates the generation of synthetic disinformation by
OpenAI's Large Language Models (LLMs) through prompt engineering and explores
their responsiveness to emotional prompting. Leveraging various LLM iterations
using davinci-002, davinci-003, gpt-3.5-turbo and gpt-4, we designed
experiments to assess their success in producing disinformation. Our findings,
based on a corpus of 19,800 synthetic disinformation social media posts, reveal
that all LLMs by OpenAI can successfully produce disinformation, and that they
effectively respond to emotional prompting, indicating their nuanced
understanding of emotional cues in text generation. When prompted politely, all
examined LLMs consistently generate disinformation at a high frequency.
Conversely, when prompted impolitely, the frequency of disinformation
production diminishes, as the models often refuse to generate disinformation
and instead caution users that the tool is not intended for such purposes. This
research contributes to the ongoing discourse surrounding responsible
development and application of AI technologies, particularly in mitigating the
spread of disinformation and promoting transparency in AI-generated content.
\\ ( https://arxiv.org/abs/2403.03550 ,  710kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03594
Date: Wed, 6 Mar 2024 10:27:09 GMT   (199kb,D)

Title: Assessing the Aesthetic Evaluation Capabilities of GPT-4 with Vision:
  Insights from Group and Individual Assessments
Authors: Yoshia Abe, Tatsuya Daikoku, Yasuo Kuniyoshi
Categories: cs.AI
Comments: 8 pages, 6 figures, submitted to The 38th Annual Conference of the
  Japanese Society for Artificial Intelligence, 2024
\\
  Recently, it has been recognized that large language models demonstrate high
performance on various intellectual tasks. However, few studies have
investigated alignment with humans in behaviors that involve sensibility, such
as aesthetic evaluation. This study investigates the performance of GPT-4 with
Vision, a state-of-the-art language model that can handle image input, on the
task of aesthetic evaluation of images. We employ two tasks, prediction of the
average evaluation values of a group and an individual's evaluation values. We
investigate the performance of GPT-4 with Vision by exploring prompts and
analyzing prediction behaviors. Experimental results reveal GPT-4 with Vision's
superior performance in predicting aesthetic evaluations and the nature of
different responses to beauty and ugliness. Finally, we discuss developing an
AI system for aesthetic evaluation based on scientific knowledge of the human
perception of beauty, employing agent technologies that integrate traditional
deep learning models with large language models.
\\ ( https://arxiv.org/abs/2403.03594 ,  199kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03600
Date: Wed, 6 Mar 2024 10:40:08 GMT   (951kb,D)

Title: A Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain
  Recommendation
Authors: Li Wang, Lei Sang, Quangui Zhang, Qiang Wu, Min Xu
Categories: cs.AI
\\
  Cross-domain recommendation (CDR) aims to enhance recommendation accuracy in
a target domain with sparse data by leveraging rich information in a source
domain, thereby addressing the data-sparsity problem. Some existing CDR methods
highlight the advantages of extracting domain-common and domain-specific
features to learn comprehensive user and item representations. However, these
methods can't effectively disentangle these components as they often rely on
simple user-item historical interaction information (such as ratings, clicks,
and browsing), neglecting the rich multi-modal features. Additionally, they
don't protect user-sensitive data from potential leakage during knowledge
transfer between domains. To address these challenges, we propose a
Privacy-Preserving Framework with Multi-Modal Data for Cross-Domain
Recommendation, called P2M2-CDR. Specifically, we first design a multi-modal
disentangled encoder that utilizes multi-modal information to disentangle more
informative domain- common and domain-specific embeddings. Furthermore, we
introduce a privacy-preserving decoder to mitigate user privacy leakage during
knowledge transfer. Local differential privacy (LDP) is utilized to obfuscate
the disentangled embeddings before inter- domain exchange, thereby enhancing
privacy protection. To ensure both consistency and differentiation among these
obfuscated disentangled embeddings, we incorporate contrastive learning-based
domain-inter and domain-intra losses. Extensive Experiments conducted on four
real-world datasets demonstrate that P2M2-CDR outperforms other
state-of-the-art single-domain and cross- domain baselines.
\\ ( https://arxiv.org/abs/2403.03600 ,  951kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03607
Date: Wed, 6 Mar 2024 10:53:51 GMT   (2128kb,D)

Title: The Geometric Structure of Topic Models
Authors: Johannes Hirth, Tom Hanika
Categories: cs.AI
\\
  Topic models are a popular tool for clustering and analyzing textual data.
They allow texts to be classified on the basis of their affiliation to the
previously calculated topics. Despite their widespread use in research and
application, an in-depth analysis of topic models is still an open research
topic. State-of-the-art methods for interpreting topic models are based on
simple visualizations, such as similarity matrices, top-term lists or
embeddings, which are limited to a maximum of three dimensions. In this paper,
we propose an incidence-geometric method for deriving an ordinal structure from
flat topic models, such as non-negative matrix factorization. These enable the
analysis of the topic model in a higher (order) dimension and the possibility
of extracting conceptual relationships between several topics at once. Due to
the use of conceptual scaling, our approach does not introduce any artificial
topical relationships, such as artifacts of feature compression. Based on our
findings, we present a new visualization paradigm for concept hierarchies based
on ordinal motifs. These allow for a top-down view on topic spaces. We
introduce and demonstrate the applicability of our approach based on a topic
model derived from a corpus of scientific papers taken from 32 top machine
learning venues.
\\ ( https://arxiv.org/abs/2403.03607 ,  2128kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03636
Date: Wed, 6 Mar 2024 11:48:08 GMT   (921kb,D)

Title: SheetAgent: A Generalist Agent for Spreadsheet Reasoning and
  Manipulation via Large Language Models
Authors: Yibin Chen, Yifu Yuan, Zeyu Zhang, Yan Zheng, Jinyi Liu, Fei Ni,
  Jianye Hao
Categories: cs.AI cs.LG
Comments: 24 pages, 14 figures
\\
  Spreadsheet manipulation is widely existing in most daily works and
significantly improves working efficiency. Large language model (LLM) has been
recently attempted for automatic spreadsheet manipulation but has not yet been
investigated in complicated and realistic tasks where reasoning challenges
exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous
requirements). To bridge the gap with the real-world requirements, we introduce
$\textbf{SheetRM}$, a benchmark featuring long-horizon and multi-category tasks
with reasoning-dependent manipulation caused by real-life challenges. To
mitigate the above challenges, we further propose $\textbf{SheetAgent}$, a
novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of
three collaborative modules: $\textit{Planner}$, $\textit{Informer}$, and
$\textit{Retriever}$, achieving both advanced reasoning and accurate
manipulation over spreadsheets without human interaction through iterative task
reasoning and reflection. Extensive experiments demonstrate that SheetAgent
delivers 20-30% pass rate improvements on multiple benchmarks over baselines,
achieving enhanced precision in spreadsheet manipulation and demonstrating
superior table reasoning abilities. More details and visualizations are
available at https://sheetagent.github.io.
\\ ( https://arxiv.org/abs/2403.03636 ,  921kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03645
Date: Wed, 6 Mar 2024 12:08:14 GMT   (2143kb,D)

Title: K-Link: Knowledge-Link Graph from LLMs for Enhanced Representation
  Learning in Multivariate Time-Series Data
Authors: Yucheng Wang, Ruibing Jin, Min Wu, Xiaoli Li, Lihua Xie, Zhenghua Chen
Categories: cs.AI
Comments: 12 pages,7 figures
\\
  Sourced from various sensors and organized chronologically, Multivariate
Time-Series (MTS) data involves crucial spatial-temporal dependencies, e.g.,
correlations among sensors. To capture these dependencies, Graph Neural
Networks (GNNs) have emerged as powerful tools, yet their effectiveness is
restricted by the quality of graph construction from MTS data. Typically,
existing approaches construct graphs solely from MTS signals, which may
introduce bias due to a small training dataset and may not accurately represent
underlying dependencies. To address this challenge, we propose a novel
framework named K-Link, leveraging Large Language Models (LLMs) to encode
extensive general knowledge and thereby providing effective solutions to reduce
the bias. Leveraging the knowledge embedded in LLMs, such as physical
principles, we extract a \textit{Knowledge-Link graph}, capturing vast semantic
knowledge of sensors and the linkage of the sensor-level knowledge. To harness
the potential of the knowledge-link graph in enhancing the graph derived from
MTS data, we propose a graph alignment module, facilitating the transfer of
semantic knowledge within the knowledge-link graph into the MTS-derived graph.
By doing so, we can improve the graph quality, ensuring effective
representation learning with GNNs for MTS data. Extensive experiments
demonstrate the efficacy of our approach for superior performance across
various MTS-related downstream tasks.
\\ ( https://arxiv.org/abs/2403.03645 ,  2143kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03744
Date: Wed, 6 Mar 2024 14:34:07 GMT   (41kb,D)

Title: Towards Safe and Aligned Large Language Models for Medicine
Authors: Tessa Han, Aounon Kumar, Chirag Agarwal, Himabindu Lakkaraju
Categories: cs.AI
\\
  The capabilities of large language models (LLMs) have been progressing at a
breathtaking speed, leaving even their own developers grappling with the depth
of their potential and risks. While initial steps have been taken to evaluate
the safety and alignment of general-knowledge LLMs, exposing some weaknesses,
to our knowledge, the safety and alignment of medical LLMs has not been
evaluated despite their risks for personal health and safety, public health and
safety, and human rights. To this end, we carry out the first safety evaluation
for medical LLMs. Specifically, we set forth a definition of medical safety and
alignment for medical artificial intelligence systems, develop a dataset of
harmful medical questions to evaluate the medical safety and alignment of an
LLM, evaluate both general and medical safety and alignment of medical LLMs,
demonstrate fine-tuning as an effective mitigation strategy, and discuss
broader, large-scale approaches used by the machine learning community to
develop safe and aligned LLMs. We hope that this work casts light on the safety
and alignment of medical LLMs and motivates future work to study it and develop
additional mitigation strategies, minimizing the risks of harm of LLMs in
medicine.
\\ ( https://arxiv.org/abs/2403.03744 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03768
Date: Wed, 6 Mar 2024 15:03:09 GMT   (19588kb,D)

Title: DeepCRE: Revolutionizing Drug R&D with Cutting-Edge Computational Models
Authors: Yushuai Wu
Categories: cs.AI cs.LG q-bio.QM
\\
  The field of pharmaceutical development and therapeutic application both face
substantial challenges. Therapeutic domain calls for more treatment
alternatives while numerous promising pre-clinical drugs fail in clinical
trails. One of the reasons is the inadequacy of Cross-drug Response Evaluation
(CRE) during the late stage of drug development. Although in-silico CRE models
offer a solution to this problem, existing methodologies are either limited to
early development stages or lack the capacity for a comprehensive CRE analysis.
Herein, we introduce a novel computational model named DeepCRE and present the
potential of DeepCRE in advancing therapeutic discovery and development.
DeepCRE outperforms the existing best models by achieving an average
performance improvement of 17.7\% in patient-level CRE, and a 5-fold increase
in indication-level CRE. Furthermore, DeepCRE has identified six drug
candidates that show significantly greater effectiveness than a comparator set
of two approved drug in 5/8 colorectal cancer (CRC) organoids. This highlights
DeepCRE's ability to identify a collection of drug candidates with superior
therapeutic effects, underscoring its potential to revolutionize the field of
therapeutic development.
\\ ( https://arxiv.org/abs/2403.03768 ,  19588kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03828
Date: Wed, 6 Mar 2024 16:18:02 GMT   (472kb)

Title: From Clicks to Security: Investigating Continuous Authentication via
  Mouse Dynamics
Authors: Rushit Dave, Marcho Handoko, Ali Rashid, Cole Schoenbauer
Categories: cs.AI
\\
  In the realm of computer security, the importance of efficient and reliable
user authentication methods has become increasingly critical. This paper
examines the potential of mouse movement dynamics as a consistent metric for
continuous authentication. By analyzing user mouse movement patterns in two
contrasting gaming scenarios, "Team Fortress" and Poly Bridge we investigate
the distinctive behavioral patterns inherent in high-intensity and
low-intensity UI interactions. The study extends beyond conventional
methodologies by employing a range of machine learning models. These models are
carefully selected to assess their effectiveness in capturing and interpreting
the subtleties of user behavior as reflected in their mouse movements. This
multifaceted approach allows for a more nuanced and comprehensive understanding
of user interaction patterns. Our findings reveal that mouse movement dynamics
can serve as a reliable indicator for continuous user authentication. The
diverse machine learning models employed in this study demonstrate competent
performance in user verification, marking an improvement over previous methods
used in this field. This research contributes to the ongoing efforts to enhance
computer security and highlights the potential of leveraging user behavior,
specifically mouse dynamics, in developing robust authentication systems.
\\ ( https://arxiv.org/abs/2403.03828 ,  472kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03832
Date: Wed, 6 Mar 2024 16:22:49 GMT   (1421kb)

Title: Your device may know you better than you know yourself- continuous
  authentication on novel dataset using machine learning
Authors: Pedro Gomes do Nascimento, Pidge Witiak, Tucker MacCallum, Zachary
  Winterfeldt, Rushit Dave
Categories: cs.AI
\\
  This research aims to further understanding in the field of continuous
authentication using behavioral biometrics. We are contributing a novel dataset
that encompasses the gesture data of 15 users playing Minecraft with a Samsung
Tablet, each for a duration of 15 minutes. Utilizing this dataset, we employed
machine learning (ML) binary classifiers, being Random Forest (RF), K-Nearest
Neighbors (KNN), and Support Vector Classifier (SVC), to determine the
authenticity of specific user actions. Our most robust model was SVC, which
achieved an average accuracy of approximately 90%, demonstrating that touch
dynamics can effectively distinguish users. However, further studies are needed
to make it viable option for authentication systems
\\ ( https://arxiv.org/abs/2403.03832 ,  1421kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03894
Date: Wed, 6 Mar 2024 17:52:08 GMT   (2441kb,D)

Title: IRCoder: Intermediate Representations Make Language Models Robust
  Multilingual Code Generators
Authors: Indraneil Paul, Jun Luo, Goran Glava\v{s}, Iryna Gurevych
Categories: cs.AI cs.CL cs.PL
\\
  Code understanding and generation have fast become some of the most popular
applications of language models (LMs). Nonetheless, research on multilingual
aspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual
transfer between different programming languages, language-specific data
augmentation, and post-hoc LM adaptation, alongside exploitation of data
sources other than the original textual content, has been much sparser than for
their natural language counterparts. In particular, most mainstream Code-LMs
have been pre-trained on source code files alone. In this work, we investigate
the prospect of leveraging readily available compiler intermediate
representations - shared across programming languages - to improve the
multilingual capabilities of Code-LMs and facilitate cross-lingual transfer.
  To this end, we first compile SLTrans, a parallel dataset consisting of
nearly 4M self-contained source code files coupled with respective intermediate
representations. Next, starting from various base Code-LMs (ranging in size
from 1.1B to 7.3B parameters), we carry out continued causal language modelling
training on SLTrans, forcing the Code-LMs to (1) learn the IR language and (2)
align the IR constructs with respective constructs of various programming
languages. Our resulting models, dubbed IRCoder, display sizeable and
consistent gains across a wide variety of code generation tasks and metrics,
including prompt robustness, multilingual code completion, code understanding,
and instruction following.
\\ ( https://arxiv.org/abs/2403.03894 ,  2441kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03920
Date: Wed, 6 Mar 2024 18:29:18 GMT   (211kb,D)

Title: Enhancing Instructional Quality: Leveraging Computer-Assisted Textual
  Analysis to Generate In-Depth Insights from Educational Artifacts
Authors: Zewei Tian, Min Sun, Alex Liu, Shawon Sarkar, Jing Liu
Categories: cs.AI cs.CL cs.HC
\\
  This paper explores the transformative potential of computer-assisted textual
analysis in enhancing instructional quality through in-depth insights from
educational artifacts. We integrate Richard Elmore's Instructional Core
Framework to examine how artificial intelligence (AI) and machine learning (ML)
methods, particularly natural language processing (NLP), can analyze
educational content, teacher discourse, and student responses to foster
instructional improvement. Through a comprehensive review and case studies
within the Instructional Core Framework, we identify key areas where AI/ML
integration offers significant advantages, including teacher coaching, student
support, and content development. We unveil patterns that indicate AI/ML not
only streamlines administrative tasks but also introduces novel pathways for
personalized learning, providing actionable feedback for educators and
contributing to a richer understanding of instructional dynamics. This paper
emphasizes the importance of aligning AI/ML technologies with pedagogical goals
to realize their full potential in educational settings, advocating for a
balanced approach that considers ethical considerations, data quality, and the
integration of human expertise.
\\ ( https://arxiv.org/abs/2403.03920 ,  211kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03304
Date: Tue, 5 Mar 2024 20:07:42 GMT   (9775kb,D)

Title: Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event
  Argument Data
Authors: Joseph Gatto, Parker Seegmiller, Omar Sharif, Sarah M. Preum
Categories: cs.CL cs.LG
\\
  Document-Level Event Argument Extraction (DocEAE) is an extremely difficult
information extraction problem -- with significant limitations in low-resource
cross-domain settings. To address this problem, we introduce Mad Lib Aug (MLA),
a novel generative DocEAE data augmentation framework. Our approach leverages
the intuition that Mad Libs, which are categorically masked documents used as a
part of a popular game, can be generated and solved by LLMs to produce data for
DocEAE. Using MLA, we achieve a 2.6-point average improvement in overall F1
score. Moreover, this approach achieves a 3.9 and 5.2 point average increase in
zero and few-shot event roles compared to augmentation-free baselines across
all experiments.
  To better facilitate analysis of cross-domain DocEAE, we additionally
introduce a new metric, Role-Depth F1 (RDF1), which uses statistical depth to
identify roles in the target domain which are semantic outliers with respect to
roles observed in the source domain. Our experiments show that MLA augmentation
can boost RDF1 performance by an average of 5.85 points compared to
non-augmented datasets.
\\ ( https://arxiv.org/abs/2403.03304 ,  9775kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03305
Date: Tue, 5 Mar 2024 20:08:32 GMT   (6988kb,D)

Title: Best of Both Worlds: A Pliable and Generalizable Neuro-Symbolic Approach
  for Relation Classification
Authors: Robert Vacareanu, Fahmida Alam, Md Asiful Islam, Haris Riaz, Mihai
  Surdeanu
Categories: cs.CL cs.AI
\\
  This paper introduces a novel neuro-symbolic architecture for relation
classification (RC) that combines rule-based methods with contemporary deep
learning techniques. This approach capitalizes on the strengths of both
paradigms: the adaptability of rule-based systems and the generalization power
of neural networks. Our architecture consists of two components: a declarative
rule-based model for transparent classification and a neural component to
enhance rule generalizability through semantic text matching. Notably, our
semantic matcher is trained in an unsupervised domain-agnostic way, solely with
synthetic data. Further, these components are loosely coupled, allowing for
rule modifications without retraining the semantic matcher. In our evaluation,
we focused on two few-shot relation classification datasets: Few-Shot TACRED
and a Few-Shot version of NYT29. We show that our proposed method outperforms
previous state-of-the-art models in three out of four settings, despite not
seeing any human-annotated training data. Further, we show that our approach
remains modular and pliable, i.e., the corresponding rules can be locally
modified to improve the overall model. Human interventions to the rules for the
TACRED relation \texttt{org:parents} boost the performance on that relation by
as much as 26\% relative improvement, without negatively impacting the other
relations, and without retraining the semantic matching component.
\\ ( https://arxiv.org/abs/2403.03305 ,  6988kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03307
Date: Tue, 5 Mar 2024 20:12:05 GMT   (1231kb,D)

Title: Book2Dial: Generating Teacher-Student Interactions from Textbooks for
  Cost-Effective Development of Educational Chatbots
Authors: Junling Wang, Jakub Macina, Nico Daheim, Sankalan Pal Chowdhury,
  Mrinmaya Sachan
Categories: cs.CL
Comments: 24 pages, 19 tables, 2 figures
\\
  Educational chatbots are a promising tool for assisting student learning.
However, the development of effective chatbots in education has been
challenging, as high-quality data is seldom available in this domain. In this
paper, we propose a framework for generating synthetic teacher-student
interactions grounded in a set of textbooks. Our approaches capture one aspect
of learning interactions where curious students with partial knowledge
interactively ask a teacher questions about the material in the textbook. We
highlight various quality criteria that such dialogues should fulfill and
compare several approaches relying on either prompting or fine-tuning large
language models. We use synthetic dialogues to train educational chatbots and
show benefits of further fine-tuning in different educational domains. However,
human evaluation shows that our best data synthesis method still suffers from
hallucinations and tends to reiterate information from previous conversations.
Our findings offer insights for future efforts in synthesizing conversational
data that strikes a balance between size and quality. We will open-source our
data and code.
\\ ( https://arxiv.org/abs/2403.03307 ,  1231kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03329
Date: Tue, 5 Mar 2024 21:19:06 GMT   (69kb,D)

Title: Guardrail Baselines for Unlearning in LLMs
Authors: Pratiksha Thaker, Yash Maurya, Virginia Smith
Categories: cs.CL
Comments: Preliminary work, accepted to ICLR workshop SeT-LLM 2024
\\
  Recent work has demonstrated that fine-tuning is a promising approach to
`unlearn' concepts from large language models. However, fine-tuning can be
expensive, as it requires both generating a set of examples and running
iterations of fine-tuning to update the model. In this work, we show that
simple guardrail-based approaches such as prompting and filtering can achieve
unlearning results comparable to fine-tuning. We recommend that researchers
investigate these lightweight baselines when evaluating the performance of more
computationally intensive fine-tuning methods. While we do not claim that
methods such as prompting or filtering are universal solutions to the problem
of unlearning, our work suggests the need for evaluation metrics that can
better separate the power of guardrails vs. fine-tuning, and highlights
scenarios where guardrails themselves may be advantageous for unlearning, such
as in generating examples for fine-tuning or unlearning when only API access is
available.
\\ ( https://arxiv.org/abs/2403.03329 ,  69kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03334
Date: Tue, 5 Mar 2024 21:36:23 GMT   (405kb,D)

Title: DIVERSE: Deciphering Internet Views on the U.S. Military Through Video
  Comment Stance Analysis, A Novel Benchmark Dataset for Stance Classification
Authors: Iain J. Cruickshank, Lynnette Hui Xian Ng
Categories: cs.CL cs.AI
Comments: Paper under review for dataset track of ICWSM 2024. 11 pages, 5
  figures
\\
  Stance detection of social media text is a key component of downstream tasks
involving the identification of groups of users with opposing opinions on
contested topics such as vaccination and within arguments. In particular,
stance provides an indication of an opinion towards an entity. This paper
introduces DIVERSE, a dataset of over 173,000 YouTube video comments annotated
for their stance towards videos of the U.S. military. The stance is annotated
through a human-guided, machine-assisted labeling methodology that makes use of
weak signals of tone within the sentence as supporting indicators, as opposed
to using manual annotations by humans. These weak signals consist of the
presence of hate speech and sarcasm, the presence of specific keywords, the
sentiment of the text, and the stance inference from two Large Language Models.
The weak signals are then consolidated using a data programming model before
each comment is annotated with a final stance label. On average, the videos
have 200 comments each, and the stance of the comments skews slightly towards
the "against" characterization for both the U.S. Army and the videos posted on
the channel.
\\ ( https://arxiv.org/abs/2403.03334 ,  405kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03336
Date: Tue, 5 Mar 2024 21:38:19 GMT   (726kb,D)

Title: Scope of Large Language Models for Mining Emerging Opinions in Online
  Health Discourse
Authors: Joseph Gatto, Madhusudan Basak, Yash Srivastava, Philip Bohlman, Sarah
  M. Preum
Categories: cs.CL cs.SI
\\
  In this paper, we develop an LLM-powered framework for the curation and
evaluation of emerging opinion mining in online health communities. We
formulate emerging opinion mining as a pairwise stance detection problem
between (title, comment) pairs sourced from Reddit, where post titles contain
emerging health-related claims on a topic that is not predefined. The claims
are either explicitly or implicitly expressed by the user. We detail (i) a
method of claim identification -- the task of identifying if a post title
contains a claim and (ii) an opinion mining-driven evaluation framework for
stance detection using LLMs.
  We facilitate our exploration by releasing a novel test dataset, Long
COVID-Stance, or LC-stance, which can be used to evaluate LLMs on the tasks of
claim identification and stance detection in online health communities. Long
Covid is an emerging post-COVID disorder with uncertain and complex treatment
guidelines, thus making it a suitable use case for our task. LC-Stance contains
long COVID treatment related discourse sourced from a Reddit community. Our
evaluation shows that GPT-4 significantly outperforms prior works on zero-shot
stance detection. We then perform thorough LLM model diagnostics, identifying
the role of claim type (i.e. implicit vs explicit claims) and comment length as
sources of model error.
\\ ( https://arxiv.org/abs/2403.03336 ,  726kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03348
Date: Tue, 5 Mar 2024 22:21:45 GMT   (2519kb,D)

Title: Learning to Maximize Mutual Information for Chain-of-Thought
  Distillation
Authors: Xin Chen, Hanxian Huang, Yanjun Gao, Yi Wang, Jishen Zhao, Ke Ding
Categories: cs.CL cs.AI
\\
  Knowledge distillation, the technique of transferring knowledge from large,
complex models to smaller ones, marks a pivotal step towards efficient AI
deployment. Distilling Step-by-Step (DSS), a novel method utilizing
chain-of-thought (CoT) distillation, has demonstrated promise by imbuing
smaller models with the superior reasoning capabilities of their larger
counterparts. In DSS, the distilled model acquires the ability to generate
rationales and predict labels concurrently through a multi-task learning
framework. However, DSS overlooks the intrinsic relationship between the two
training tasks, leading to ineffective integration of CoT knowledge with the
task of label prediction. To this end, we investigate the mutual relationship
of the two tasks from Information Bottleneck perspective and formulate it as
maximizing the mutual information of the representation features of the two
tasks. We propose a variational approach to solve this optimization problem
using a learning-based method. Our experimental results across four datasets
demonstrate that our method outperforms the state-of-the-art DSS. Our findings
offer insightful guidance for future research on language model distillation as
well as applications involving CoT. Code and models will be released soon.
\\ ( https://arxiv.org/abs/2403.03348 ,  2519kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03396
Date: Wed, 6 Mar 2024 01:37:03 GMT   (201kb,D)

Title: Japanese-English Sentence Translation Exercises Dataset for Automatic
  Grading
Authors: Naoki Miura, Hiroaki Funayama, Seiya Kikuchi, Yuichiroh Matsubayashi,
  Yuya Iwase, Kentaro Inui
Categories: cs.CL
Comments: 9 pages
\\
  This paper proposes the task of automatic assessment of Sentence Translation
Exercises (STEs), that have been used in the early stage of L2 language
learning. We formalize the task as grading student responses for each rubric
criterion pre-specified by the educators. We then create a dataset for STE
between Japanese and English including 21 questions, along with a total of 3,
498 student responses (167 on average). The answer responses were collected
from students and crowd workers. Using this dataset, we demonstrate the
performance of baselines including finetuned BERT and GPT models with few-shot
in-context learning. Experimental results show that the baseline model with
finetuned BERT was able to classify correct responses with approximately 90% in
F1, but only less than 80% for incorrect responses. Furthermore, the GPT models
with few-shot learning show poorer results than finetuned BERT, indicating that
our newly proposed task presents a challenging issue, even for the
stateof-the-art large language models.
\\ ( https://arxiv.org/abs/2403.03396 ,  201kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03419
Date: Wed, 6 Mar 2024 03:02:38 GMT   (10182kb,D)

Title: Negating Negatives: Alignment without Human Positive Samples via
  Distributional Dispreference Optimization
Authors: Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, Ning Gu
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) have revolutionized the role of AI, yet also
pose potential risks of propagating unethical content. Alignment technologies
have been introduced to steer LLMs towards human preference, gaining increasing
attention. Despite notable breakthroughs in this direction, existing methods
heavily rely on high-quality positive-negative training pairs, suffering from
noisy labels and the marginal distinction between preferred and dispreferred
response data. Given recent LLMs' proficiency in generating helpful responses,
this work pivots towards a new research focus: achieving alignment using solely
human-annotated negative samples, preserving helpfulness while reducing
harmfulness. For this purpose, we propose Distributional Dispreference
Optimization (D$^2$O), which maximizes the discrepancy between the generated
responses and the dispreferred ones to effectively eschew harmful information.
We theoretically demonstrate that D$^2$O is equivalent to learning a
distributional instead of instance-level preference model reflecting human
dispreference against the distribution of negative responses. Besides, D$^2$O
integrates an implicit Jeffrey Divergence regularization to balance the
exploitation and exploration of reference policies and converges to a
non-negative one during training. Extensive experiments demonstrate that our
method achieves comparable generation quality and surpasses the latest
baselines in producing less harmful and more informative responses with better
training stability and faster convergence.
\\ ( https://arxiv.org/abs/2403.03419 ,  10182kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03432
Date: Wed, 6 Mar 2024 03:33:48 GMT   (681kb,D)

Title: Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language
  Models
Authors: Wenfeng Feng and Chuzhan Hao and Yuewei Zhang and Yu Han and Hao Wang
Categories: cs.CL cs.AI
Comments: 10 pages, COLING24 Accepted
\\
  Instruction Tuning has the potential to stimulate or enhance specific
capabilities of large language models (LLMs). However, achieving the right
balance of data is crucial to prevent catastrophic forgetting and interference
between tasks. To address these limitations and enhance training flexibility,
we propose the Mixture-of-LoRAs (MoA) architecture which is a novel and
parameter-efficient tuning method designed for multi-task learning with LLMs.
In this paper, we start by individually training multiple domain-specific LoRA
modules using corresponding supervised corpus data. These LoRA modules can be
aligned with the expert design principles observed in Mixture-of-Experts (MoE).
Subsequently, we combine the multiple LoRAs using an explicit routing strategy
and introduce domain labels to facilitate multi-task learning, which help
prevent interference between tasks and ultimately enhances the performance of
each individual task. Furthermore, each LoRA model can be iteratively adapted
to a new domain, allowing for quick domain-specific adaptation. Experiments on
diverse tasks demonstrate superior and robust performance, which can further
promote the wide application of domain-specific LLMs.
\\ ( https://arxiv.org/abs/2403.03432 ,  681kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03435
Date: Wed, 6 Mar 2024 03:42:06 GMT   (7237kb)

Title: VLSP 2023 - LTER: A Summary of the Challenge on Legal Textual Entailment
  Recognition
Authors: Vu Tran, Ha-Thanh Nguyen, Trung Vo, Son T. Luu, Hoang-Anh Dang,
  Ngoc-Cam Le, Thi-Thuy Le, Minh-Tien Nguyen, Truong-Son Nguyen, Le-Minh Nguyen
Categories: cs.CL
\\
  In this new era of rapid AI development, especially in language processing,
the demand for AI in the legal domain is increasingly critical. In the context
where research in other languages such as English, Japanese, and Chinese has
been well-established, we introduce the first fundamental research for the
Vietnamese language in the legal domain: legal textual entailment recognition
through the Vietnamese Language and Speech Processing workshop. In analyzing
participants' results, we discuss certain linguistic aspects critical in the
legal domain that pose challenges that need to be addressed.
\\ ( https://arxiv.org/abs/2403.03435 ,  7237kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03481
Date: Wed, 6 Mar 2024 05:40:31 GMT   (182kb,D)

Title: Magic Markup: Maintaining Document-External Markup with an LLM
Authors: Edward Misback, Zachary Tatlock, Steven L. Tanimoto
Categories: cs.CL
Comments: 10 pages; 2 figures; to be published in the <Programming> 2024
  Conference Companion
\\
  Text documents, including programs, typically have human-readable semantic
structure. Historically, programmatic access to these semantics has required
explicit in-document tagging. Especially in systems where the text has an
execution semantics, this means it is an opt-in feature that is hard to support
properly. Today, language models offer a new method: metadata can be bound to
entities in changing text using a model's human-like understanding of
semantics, with no requirements on the document structure. This method expands
the applications of document annotation, a fundamental operation in program
writing, debugging, maintenance, and presentation. We contribute a system that
employs an intelligent agent to re-tag modified programs, enabling rich
annotations to automatically follow code as it evolves. We also contribute a
formal problem definition, an empirical synthetic benchmark suite, and our
benchmark generator. Our system achieves an accuracy of 90% on our benchmarks
and can replace a document's tags in parallel at a rate of 5 seconds per tag.
While there remains significant room for improvement, we find performance
reliable enough to justify further exploration of applications.
\\ ( https://arxiv.org/abs/2403.03481 ,  182kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03496
Date: Wed, 6 Mar 2024 06:54:02 GMT   (50kb)

Title: A Knowledge Plug-and-Play Test Bed for Open-domain Dialogue Generation
Authors: Xiangci Li, Linfeng Song, Lifeng Jin, Haitao Mi, Jessica Ouyang, Dong
  Yu
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\
  Knowledge-based, open-domain dialogue generation aims to build chit-chat
systems that talk to humans using mined support knowledge. Many types and
sources of knowledge have previously been shown to be useful as support
knowledge. Even in the era of large language models, response generation
grounded in knowledge retrieved from additional up-to-date sources remains a
practically important approach. While prior work using single-source knowledge
has shown a clear positive correlation between the performances of knowledge
selection and response generation, there are no existing multi-source datasets
for evaluating support knowledge retrieval. Further, prior work has assumed
that the knowledge sources available at test time are the same as during
training. This unrealistic assumption unnecessarily handicaps models, as new
knowledge sources can become available after a model is trained. In this paper,
we present a high-quality benchmark named multi-source Wizard of Wikipedia
(Ms.WoW) for evaluating multi-source dialogue knowledge selection and response
generation. Unlike existing datasets, it contains clean support knowledge,
grounded at the utterance level and partitioned into multiple knowledge
sources. We further propose a new challenge, dialogue knowledge plug-and-play,
which aims to test an already trained dialogue model on using new support
knowledge from previously unseen sources in a zero-shot fashion.
\\ ( https://arxiv.org/abs/2403.03496 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03506
Date: Wed, 6 Mar 2024 07:25:46 GMT   (232kb,D)

Title: Towards Detecting AI-Generated Text within Human-AI Collaborative Hybrid
  Texts
Authors: Zijie Zeng, Shiqi Liu, Lele Sha, Zhuang Li, Kaixun Yang, Sannyuya Liu,
  Dragan Ga\v{s}evi\'c, Guanliang Chen
Categories: cs.CL cs.AI
Comments: under review; 7 pages
\\
  This study explores the challenge of sentence-level AI-generated text
detection within human-AI collaborative hybrid texts. Existing studies of
AI-generated text detection for hybrid texts often rely on synthetic datasets.
These typically involve hybrid texts with a limited number of boundaries. We
contend that studies of detecting AI-generated content within hybrid texts
should cover different types of hybrid texts generated in realistic settings to
better inform real-world applications. Therefore, our study utilizes the
CoAuthor dataset, which includes diverse, realistic hybrid texts generated
through the collaboration between human writers and an intelligent writing
system in multi-turn interactions. We adopt a two-step, segmentation-based
pipeline: (i) detect segments within a given hybrid text where each segment
contains sentences of consistent authorship, and (ii) classify the authorship
of each identified segment. Our empirical findings highlight (1) detecting
AI-generated sentences in hybrid texts is overall a challenging task because
(1.1) human writers' selecting and even editing AI-generated sentences based on
personal preferences adds difficulty in identifying the authorship of segments;
(1.2) the frequent change of authorship between neighboring sentences within
the hybrid text creates difficulties for segment detectors in identifying
authorship-consistent segments; (1.3) the short length of text segments within
hybrid texts provides limited stylistic cues for reliable authorship
determination; (2) before embarking on the detection process, it is beneficial
to assess the average length of segments within the hybrid text. This
assessment aids in deciding whether (2.1) to employ a text segmentation-based
strategy for hybrid texts with longer segments, or (2.2) to adopt a direct
sentence-by-sentence classification strategy for those with shorter segments.
\\ ( https://arxiv.org/abs/2403.03506 ,  232kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03514
Date: Wed, 6 Mar 2024 07:43:43 GMT   (183kb,D)

Title: CLongEval: A Chinese Benchmark for Evaluating Long-Context Large
  Language Models
Authors: Zexuan Qiu, Jingjing Li, Shijue Huang, Wanjun Zhong, Irwin King
Categories: cs.CL
Comments: 19 pages, 4 figures
\\
  Developing Large Language Models (LLMs) with robust long-context capabilities
has been the recent research focus, resulting in the emergence of long-context
LLMs proficient in Chinese. However, the evaluation of these models remains
underdeveloped due to a lack of benchmarks. To address this gap, we present
CLongEval, a comprehensive Chinese benchmark for evaluating long-context LLMs.
CLongEval is characterized by three key features: (1) Sufficient data volume,
comprising 7 distinct tasks and 7,267 examples; (2) Broad applicability,
accommodating to models with context windows size from 1K to 100K; (3) High
quality, with over 2,000 manually annotated question-answer pairs in addition
to the automatically constructed labels. With CLongEval, we undertake a
comprehensive assessment of 6 open-source long-context LLMs and 2 leading
commercial counterparts that feature both long-context abilities and
proficiency in Chinese. We also provide in-depth analysis based on the
empirical results, trying to shed light on the critical capabilities that
present challenges in long-context settings. The dataset, evaluation scripts,
and model outputs will be released.
\\ ( https://arxiv.org/abs/2403.03514 ,  183kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03516
Date: Wed, 6 Mar 2024 07:49:06 GMT   (8687kb,D)

Title: Unsupervised Multilingual Dense Retrieval via Generative Pseudo Labeling
Authors: Chao-Wei Huang, Chen-An Li, Tsu-Yuan Hsu, Chen-Yu Hsu, Yun-Nung Chen
Categories: cs.CL cs.IR
Comments: Accepted to Findings of EACL 2024
\\
  Dense retrieval methods have demonstrated promising performance in
multilingual information retrieval, where queries and documents can be in
different languages. However, dense retrievers typically require a substantial
amount of paired data, which poses even greater challenges in multilingual
scenarios. This paper introduces UMR, an Unsupervised Multilingual dense
Retriever trained without any paired data. Our approach leverages the sequence
likelihood estimation capabilities of multilingual language models to acquire
pseudo labels for training dense retrievers. We propose a two-stage framework
which iteratively improves the performance of multilingual dense retrievers.
Experimental results on two benchmark datasets show that UMR outperforms
supervised baselines, showcasing the potential of training multilingual
retrievers without paired data, thereby enhancing their practicality. Our
source code, data, and models are publicly available at
https://github.com/MiuLab/UMR
\\ ( https://arxiv.org/abs/2403.03516 ,  8687kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03521
Date: Wed, 6 Mar 2024 08:02:21 GMT   (1092kb,D)

Title: BiVert: Bidirectional Vocabulary Evaluation using Relations for Machine
  Translation
Authors: Carinne Cherf, Yuval Pinter
Categories: cs.CL
Comments: LREC-COLING 2024
\\
  Neural machine translation (NMT) has progressed rapidly in the past few
years, promising improvements and quality translations for different languages.
Evaluation of this task is crucial to determine the quality of the translation.
Overall, insufficient emphasis is placed on the actual sense of the translation
in traditional methods. We propose a bidirectional semantic-based evaluation
method designed to assess the sense distance of the translation from the source
text. This approach employs the comprehensive multilingual encyclopedic
dictionary BabelNet. Through the calculation of the semantic distance between
the source and its back translation of the output, our method introduces a
quantifiable approach that empowers sentence comparison on the same linguistic
level. Factual analysis shows a strong correlation between the average
evaluation scores generated by our method and the human assessments across
various machine translation systems for English-German language pair. Finally,
our method proposes a new multilingual approach to rank MT systems without the
need for parallel corpora.
\\ ( https://arxiv.org/abs/2403.03521 ,  1092kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03558
Date: Wed, 6 Mar 2024 09:06:34 GMT   (329kb,D)

Title: Benchmarking Hallucination in Large Language Models based on
  Unanswerable Math Word Problem
Authors: Yuhong Sun, Zhangyue Yin, Qipeng Guo, Jiawen Wu, Xipeng Qiu, Hui Zhao
Categories: cs.CL
Comments: 11 pages, 8 figures, accepted by LREC-Coling 2024
\\
  Large language models (LLMs) are highly effective in various natural language
processing (NLP) tasks. However, they are susceptible to producing unreliable
conjectures in ambiguous contexts called hallucination. This paper presents a
new method for evaluating LLM hallucination in Question Answering (QA) based on
the unanswerable math word problem (MWP). To support this approach, we
innovatively develop a dataset called Unanswerable Math Word Problem (UMWP)
which comprises 5200 questions across five categories. We developed an
evaluation methodology combining text similarity and mathematical expression
detection to determine whether LLM considers the question unanswerable. The
results of extensive experiments conducted on 31 LLMs, including GPT-3,
InstructGPT, LLaMA, and Claude, demonstrate that in-context learning and
reinforcement learning with human feedback (RLHF) training significantly
enhance the model's ability to avoid hallucination. We show that utilizing MWP
is a reliable and effective approach to assess hallucination. Our code and data
are available at https://github.com/Yuki-Asuuna/UMWP.
\\ ( https://arxiv.org/abs/2403.03558 ,  329kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03575
Date: Wed, 6 Mar 2024 09:36:36 GMT   (449kb,D)

Title: gaHealth: An English-Irish Bilingual Corpus of Health Data
Authors: S\'eamus Lankford, Haithem Afli, \'Orla N\'i Loinsigh, Andy Way
Categories: cs.CL cs.AI
Comments: arXiv admin note: text overlap with arXiv:2403.02367
Journal-ref: In Proceedings of the Thirteenth Language Resources and Evaluation
  Conference, pages 6753-6758, Marseille, France. European Language Resources
  Association, 2022
\\
  Machine Translation is a mature technology for many high-resource language
pairs. However in the context of low-resource languages, there is a paucity of
parallel data datasets available for developing translation models.
Furthermore, the development of datasets for low-resource languages often
focuses on simply creating the largest possible dataset for generic
translation. The benefits and development of smaller in-domain datasets can
easily be overlooked. To assess the merits of using in-domain data, a dataset
for the specific domain of health was developed for the low-resource English to
Irish language pair. Our study outlines the process used in developing the
corpus and empirically demonstrates the benefits of using an in-domain dataset
for the health domain. In the context of translating health-related data,
models developed using the gaHealth corpus demonstrated a maximum BLEU score
improvement of 22.2 points (40%) when compared with top performing models from
the LoResMT2021 Shared Task. Furthermore, we define linguistic guidelines for
developing gaHealth, the first bilingual corpus of health data for the Irish
language, which we hope will be of use to other creators of low-resource data
sets. gaHealth is now freely available online and is ready to be explored for
further research.
\\ ( https://arxiv.org/abs/2403.03575 ,  449kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03581
Date: Wed, 6 Mar 2024 09:57:42 GMT   (1695kb)

Title: Enhancing ASD detection accuracy: a combined approach of machine
  learning and deep learning models with natural language processing
Authors: Sergio Rubio-Mart\'in, Mar\'ia Teresa Garc\'ia-Ord\'as, Mart\'in
  Bay\'on-Guti\'errez, Natalia Prieto-Fern\'andez and Jos\'e Alberto
  Ben\'itez-Andrades
Categories: cs.CL cs.LG
Journal-ref: Health Inf Sci Syst 12, 20 (2024)
DOI: 10.1007/s13755-024-00281-y
\\
  Purpose: Our study explored the use of artificial intelligence (AI) to
diagnose autism spectrum disorder (ASD). It focused on machine learning (ML)
and deep learning (DL) to detect ASD from text inputs on social media,
addressing challenges in traditional ASD diagnosis.
  Methods: We used natural language processing (NLP), ML, and DL models
(including decision trees, XGB, KNN, RNN, LSTM, Bi-LSTM, BERT, and BERTweet) to
analyze 404,627 tweets, classifying them based on ASD or non-ASD authors. A
subset of 90,000 tweets was used for model training and testing.
  Results: Our AI models showed high accuracy, with an 88% success rate in
identifying texts from individuals with ASD.
  Conclusion: The study demonstrates AI's potential in improving ASD diagnosis,
especially in children, highlighting the importance of early detection.
\\ ( https://arxiv.org/abs/2403.03581 ,  1695kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03582
Date: Wed, 6 Mar 2024 09:57:52 GMT   (151kb,D)

Title: Design of an Open-Source Architecture for Neural Machine Translation
Authors: S\'eamus Lankford, Haithem Afli and Andy Way
Categories: cs.CL cs.AI
Comments: arXiv admin note: substantial text overlap with arXiv:2403.02367
Journal-ref: In Proceedings of the 1st Workshop on Open Community-Driven
  Machine Translation, pages 15-20, Tampere, Finland. European Association for
  Machine Translation, 2023
\\
  adaptNMT is an open-source application that offers a streamlined approach to
the development and deployment of Recurrent Neural Networks and Transformer
models. This application is built upon the widely-adopted OpenNMT ecosystem,
and is particularly useful for new entrants to the field, as it simplifies the
setup of the development environment and creation of train, validation, and
test splits. The application offers a graphing feature that illustrates the
progress of model training, and employs SentencePiece for creating subword
segmentation models. Furthermore, the application provides an intuitive user
interface that facilitates hyperparameter customization. Notably, a
single-click model development approach has been implemented, and models
developed by adaptNMT can be evaluated using a range of metrics. To encourage
eco-friendly research, adaptNMT incorporates a green report that flags the
power consumption and kgCO${_2}$ emissions generated during model development.
The application is freely available.
\\ ( https://arxiv.org/abs/2403.03582 ,  151kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03627
Date: Wed, 6 Mar 2024 11:32:41 GMT   (2828kb,D)

Title: Multimodal Large Language Models to Support Real-World Fact-Checking
Authors: Jiahui Geng, Yova Kementchedjhieva, Preslav Nakov, Iryna Gurevych
Categories: cs.CL cs.AI
\\
  Multimodal large language models (MLLMs) carry the potential to support
humans in processing vast amounts of information. While MLLMs are already being
used as a fact-checking tool, their abilities and limitations in this regard
are understudied. Here is aim to bridge this gap. In particular, we propose a
framework for systematically assessing the capacity of current multimodal
models to facilitate real-world fact-checking. Our methodology is
evidence-free, leveraging only these models' intrinsic knowledge and reasoning
capabilities. By designing prompts that extract models' predictions,
explanations, and confidence levels, we delve into research questions
concerning model accuracy, robustness, and reasons for failure. We empirically
find that (1) GPT-4V exhibits superior performance in identifying malicious and
misleading multimodal claims, with the ability to explain the unreasonable
aspects and underlying motives, and (2) existing open-source models exhibit
strong biases and are highly sensitive to the prompt. Our study offers insights
into combating false multimodal information and building secure, trustworthy
multimodal models. To the best of our knowledge, we are the first to evaluate
MLLMs for real-world fact-checking.
\\ ( https://arxiv.org/abs/2403.03627 ,  2828kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03628
Date: Wed, 6 Mar 2024 11:34:20 GMT   (7916kb,D)

Title: GPTopic: Dynamic and Interactive Topic Representations
Authors: Arik Reuter, Anton Thielmann, Christoph Weisser, Sebastian Fischer,
  Benjamin S\"afken
Categories: cs.CL
\\
  Topic modeling seems to be almost synony- mous with generating lists of top
words to repre- sent topics within large text corpora. However, deducing a
topic from such list of individual terms can require substantial expertise and
ex- perience, making topic modelling less accessi- ble to people unfamiliar
with the particularities and pitfalls of top-word interpretation. A topic
representation limited to top-words might fur- ther fall short of offering a
comprehensive and easily accessible characterization of the vari- ous aspects,
facets and nuances a topic might have. To address these challenges, we intro-
duce GPTopic, a software package that lever- ages Large Language Models (LLMs)
to create dynamic, interactive topic representations. GP- Topic provides an
intuitive chat interface for users to explore, analyze, and refine topics in-
teractively, making topic modeling more acces- sible and comprehensive. The
corresponding code is available here: https://github. com/05ec6602be/GPTopic.
\\ ( https://arxiv.org/abs/2403.03628 ,  7916kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03640
Date: Wed, 6 Mar 2024 11:56:02 GMT   (14155kb,D)

Title: Apollo: Lightweight Multilingual Medical LLMs towards Democratizing
  Medical AI to 6B People
Authors: Xidong Wang, Nuo Chen, Junyin Chen, Yan Hu, Yidong Wang, Xiangbo Wu,
  Anningzhe Gao, Xiang Wan, Haizhou Li, Benyou Wang
Categories: cs.CL cs.AI
Comments: Preprint
\\
  Despite the vast repository of global medical knowledge predominantly being
in English, local languages are crucial for delivering tailored healthcare
services, particularly in areas with limited medical resources. To extend the
reach of medical AI advancements to a broader population, we aim to develop
medical LLMs across the six most widely spoken languages, encompassing a global
population of 6.1 billion. This effort culminates in the creation of the
ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the
multilingual medical benchmark, the released Apollo models, at various
relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best
performance among models of equivalent size. Especially, Apollo-7B is the
state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite
models could be used to improve the multi-lingual medical capabilities of
larger models without fine-tuning in a proxy-tuning fashion. We will
open-source training corpora, code, model weights and evaluation benchmark.
\\ ( https://arxiv.org/abs/2403.03640 ,  14155kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03689
Date: Wed, 6 Mar 2024 13:15:21 GMT   (175kb,D)

Title: General2Specialized LLMs Translation for E-commerce
Authors: Kaidi Chen, Ben Chen, Dehong Gao, Huangyu Dai, Wen Jiang, Wei Ning,
  Shanqing Yu, Libin Yang, Xiaoyan Cai
Categories: cs.CL cs.AI
Comments: 4 pages, 1 figure, WWW2024 accepted
\\
  Existing Neural Machine Translation (NMT) models mainly handle translation in
the general domain, while overlooking domains with special writing formulas,
such as e-commerce and legal documents. Taking e-commerce as an example, the
texts usually include amounts of domain-related words and have more grammar
problems, which leads to inferior performances of current NMT methods. To
address these problems, we collect two domain-related resources, including a
set of term pairs (aligned Chinese-English bilingual terms) and a parallel
corpus annotated for the e-commerce domain. Furthermore, we propose a two-step
fine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to
transfer one general NMT model to the specialized NMT model for e-commerce. The
paradigm can be used for the NMT models based on Large language models (LLMs).
Extensive evaluations on real e-commerce titles demonstrate the superior
translation quality and robustness of our G2ST approach, as compared with
state-of-the-art NMT models such as LLaMA, Qwen, GPT-3.5, and even GPT-4.
\\ ( https://arxiv.org/abs/2403.03689 ,  175kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03690
Date: Wed, 6 Mar 2024 13:17:07 GMT   (453kb)

Title: Rapidly Developing High-quality Instruction Data and Evaluation
  Benchmark for Large Language Models with Minimal Human Effort: A Case Study
  on Japanese
Authors: Yikun Sun, Zhen Wan, Nobuhiro Ueda, Sakiko Yahata, Fei Cheng, Chenhui
  Chu, Sadao Kurohashi
Categories: cs.CL cs.AI
Comments: COLING 2024. Our code are available here:
  \href{https://github.com/hitoshizuku7/awesome-Ja-self-instruct}{self-instruct
  data} and \href{https://github.com/ku-nlp/ja-vicuna-qa-benchmark}{evaluation
  benchmark}
\\
  The creation of instruction data and evaluation benchmarks for serving Large
language models often involves enormous human annotation. This issue becomes
particularly pronounced when rapidly developing such resources for a
non-English language like Japanese. Instead of following the popular practice
of directly translating existing English resources into Japanese (e.g.,
Japanese-Alpaca), we propose an efficient self-instruct method based on GPT-4.
We first translate a small amount of English instructions into Japanese and
post-edit them to obtain native-level quality. GPT-4 then utilizes them as
demonstrations to automatically generate Japanese instruction data. We also
construct an evaluation benchmark containing 80 questions across 8 categories,
using GPT-4 to automatically assess the response quality of LLMs without human
references. The empirical results suggest that the models fine-tuned on our
GPT-4 self-instruct data significantly outperformed the Japanese-Alpaca across
all three base pre-trained models. Our GPT-4 self-instruct data allowed the
LLaMA 13B model to defeat GPT-3.5 (Davinci-003) with a 54.37\% win-rate. The
human evaluation exhibits the consistency between GPT-4's assessments and human
preference. Our high-quality instruction data and evaluation benchmark have
been released here.
\\ ( https://arxiv.org/abs/2403.03690 ,  453kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03750
Date: Wed, 6 Mar 2024 14:37:30 GMT   (1126kb,D)

Title: German also Hallucinates! Inconsistency Detection in News Summaries with
  the Absinth Dataset
Authors: Laura Mascarell, Ribin Chalumattu, Annette Rios
Categories: cs.CL cs.AI
Comments: 11 pages, 2 figures, 7 tables, conference: Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
  (LREC-COLING 2024), Turin, Italy, May 20-25, 2024
ACM-class: I.2.7
DOI: 10.3929/ethz-b-000661775
\\
  The advent of Large Language Models (LLMs) has led to remarkable progress on
a wide range of natural language processing tasks. Despite the advances, these
large-sized models still suffer from hallucinating information in their output,
which poses a major issue in automatic text summarization, as we must guarantee
that the generated summary is consistent with the content of the source
document. Previous research addresses the challenging task of detecting
hallucinations in the output (i.e. inconsistency detection) in order to
evaluate the faithfulness of the generated summaries. However, these works
primarily focus on English and recent multilingual approaches lack German data.
This work presents absinth, a manually annotated dataset for hallucination
detection in German news summarization and explores the capabilities of novel
open-source LLMs on this task in both fine-tuning and in-context learning set-
tings. We open-source and release the absinth dataset to foster further
research on hallucination detection in German.
\\ ( https://arxiv.org/abs/2403.03750 ,  1126kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03788
Date: Wed, 6 Mar 2024 15:33:32 GMT   (1943kb,D)

Title: PPTC-R benchmark: Towards Evaluating the Robustness of Large Language
  Models for PowerPoint Task Completion
Authors: Zekai Zhang, Yiduo Guo, Yaobo Liang, Dongyan Zhao, Nan Duan
Categories: cs.CL
Comments: LLM evaluation, Multi-turn, Multi-language, Multi-modal benchmark
\\
  The growing dependence on Large Language Models (LLMs) for finishing user
instructions necessitates a comprehensive understanding of their robustness to
complex task completion in real-world situations. To address this critical
need, we propose the PowerPoint Task Completion Robustness benchmark (PPTC-R)
to measure LLMs' robustness to the user PPT task instruction and software
version. Specifically, we construct adversarial user instructions by attacking
user instructions at sentence, semantic, and multi-language levels. To assess
the robustness of Language Models to software versions, we vary the number of
provided APIs to simulate both the newest version and earlier version settings.
Subsequently, we test 3 closed-source and 4 open-source LLMs using a benchmark
that incorporates these robustness settings, aiming to evaluate how deviations
impact LLMs' API calls for task completion. We find that GPT-4 exhibits the
highest performance and strong robustness in our benchmark, particularly in the
version update and the multilingual settings. However, we find that all LLMs
lose their robustness when confronted with multiple challenges (e.g.,
multi-turn) simultaneously, leading to significant performance drops. We
further analyze the robustness behavior and error reasons of LLMs in our
benchmark, which provide valuable insights for researchers to understand the
LLM's robustness in task completion and develop more robust LLMs and agents. We
release the code and data at \url{https://github.com/ZekaiGalaxy/PPTCR}.
\\ ( https://arxiv.org/abs/2403.03788 ,  1943kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03814
Date: Wed, 6 Mar 2024 16:01:44 GMT   (1858kb,D)

Title: Evaluating the Elementary Multilingual Capabilities of Large Language
  Models with MultiQ
Authors: Carolin Holtermann, Paul R\"ottger, Timm Dill, Anne Lauscher
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) need to serve everyone, including a global
majority of non-English speakers. However, most LLMs today, and open LLMs in
particular, are often intended for use in just English (e.g. Llama2, Mistral)
or a small handful of high-resource languages (e.g. Mixtral, Qwen). Recent
research shows that, despite limits in their intended use, people prompt LLMs
in many different languages. Therefore, in this paper, we investigate the basic
multilingual capabilities of state-of-the-art open LLMs beyond their intended
use. For this purpose, we introduce MultiQ, a new silver standard benchmark for
basic open-ended question answering with 27.4k test questions across a
typologically diverse set of 137 languages. With MultiQ, we evaluate language
fidelity, i.e.\ whether models respond in the prompted language, and question
answering accuracy. All LLMs we test respond faithfully and/or accurately for
at least some languages beyond their intended use. Most models are more
accurate when they respond faithfully. However, differences across models are
large, and there is a long tail of languages where models are neither accurate
nor faithful. We explore differences in tokenization as a potential explanation
for our findings, identifying possible correlations that warrant further
investigation.
\\ ( https://arxiv.org/abs/2403.03814 ,  1858kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03823
Date: Wed, 6 Mar 2024 16:10:01 GMT   (2359kb,D)

Title: A Modular Approach for Multimodal Summarization of TV Shows
Authors: Louis Mahon, Mirella Lapata
Categories: cs.CL
\\
  In this paper we address the task of summarizing television shows, which
touches key areas in AI research: complex reasoning, multiple modalities, and
long narratives. We present a modular approach where separate components
perform specialized sub-tasks which we argue affords greater flexibility
compared to end-to-end methods. Our modules involve detecting scene boundaries,
reordering scenes so as to minimize the number of cuts between different
events, converting visual information to text, summarizing the dialogue in each
scene, and fusing the scene summaries into a final summary for the entire
episode. We also present a new metric, PREFS (\textbf{P}recision and
\textbf{R}ecall \textbf{E}valuation of Summary \textbf{F}act\textbf{s}), to
measure both precision and recall of generated summaries, which we decompose
into atomic facts. Tested on the recently released SummScreen3D dataset
Papalampidi and Lapata (2023), our method produces higher quality summaries
than comparison models, as measured with ROUGE and our new fact-based metric.
\\ ( https://arxiv.org/abs/2403.03823 ,  2359kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03853
Date: Wed, 6 Mar 2024 17:04:18 GMT   (2509kb,D)

Title: ShortGPT: Layers in Large Language Models are More Redundant Than You
  Expect
Authors: Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie
  Lu, Xianpei Han, Weipeng Chen
Categories: cs.CL
\\
  As Large Language Models (LLMs) continue to advance in performance, their
size has escalated significantly, with current LLMs containing billions or even
trillions of parameters. However, in this study, we discovered that many layers
of LLMs exhibit high similarity, and some layers play a negligible role in
network functionality. Based on this observation, we define a metric called
Block Influence (BI) to gauge the significance of each layer in LLMs. We then
propose a straightforward pruning approach: layer removal, in which we directly
delete the redundant layers in LLMs based on their BI scores. Experiments
demonstrate that our method, which we call ShortGPT, significantly outperforms
previous state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT
is orthogonal to quantization-like methods, enabling further reduction in
parameters and computation. The ability to achieve better results through
simple layer removal, as opposed to more complex pruning techniques, suggests a
high degree of redundancy in the model architecture.
\\ ( https://arxiv.org/abs/2403.03853 ,  2509kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03857
Date: Wed, 6 Mar 2024 17:06:17 GMT   (4262kb,D)

Title: Emojinize : Enriching Any Text with Emoji Translations
Authors: Lars Henning Klein, Roland Aydin, Robert West
Categories: cs.CL cs.HC
\\
  Emoji have become ubiquitous in written communication, on the Web and beyond.
They can emphasize or clarify emotions, add details to conversations, or simply
serve decorative purposes. This casual use, however, barely scratches the
surface of the expressive power of emoji. To further unleash this power, we
present Emojinize, a method for translating arbitrary text phrases into
sequences of one or more emoji without requiring human input. By leveraging the
power of large language models, Emojinize can choose appropriate emoji by
disambiguating based on context (eg, cricket-bat vs bat) and can express
complex concepts compositionally by combining multiple emoji (eq, ''Emojinize''
is translated to input-latin-letters right-arrow grinning-face). In a cloze
test--based user study, we show that Emojinize's emoji translations increase
the human guessability of masked words by 55%, whereas human-picked emoji
translations do so by only 29%. These results suggest that emoji provide a
sufficiently rich vocabulary to accurately translate a wide variety of words.
Moreover, annotating words and phrases with Emojinize's emoji translations
opens the door to numerous downstream applications, including children learning
how to read, adults learning foreign languages, and text understanding for
people with learning disabilities.
\\ ( https://arxiv.org/abs/2403.03857 ,  4262kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03861
Date: Wed, 6 Mar 2024 17:11:38 GMT   (8010kb,D)

Title: Designing Informative Metrics for Few-Shot Example Selection
Authors: Rishabh Adiga, Lakshminarayanan Subramanian, Varun Chandrasekaran
Categories: cs.CL cs.LG
\\
  Pretrained language models (PLMs) have shown remarkable few-shot learning
capabilities when provided with properly formatted examples. However, selecting
the "best" examples remains an open challenge. We propose a complexity-based
prompt selection approach for sequence tagging tasks. This approach avoids the
training of a dedicated model for selection of examples, and instead uses
certain metrics to align the syntactico-semantic complexity of test sentences
and examples. We use both sentence- and word-level metrics to match the
complexity of examples to the (test) sentence being considered. Our results
demonstrate that our approach extracts greater performance from PLMs: it
achieves state-of-the-art performance on few-shot NER, achieving a 5% absolute
improvement in F1 score on the CoNLL2003 dataset for GPT-4. We also see large
gains of upto 28.85 points (F1/Acc.) in smaller models like GPT-j-6B.
\\ ( https://arxiv.org/abs/2403.03861 ,  8010kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03863
Date: Wed, 6 Mar 2024 17:13:24 GMT   (518kb,D)

Title: X-Shot: A Unified System to Handle Frequent, Few-shot and Zero-shot
  Learning Simultaneously in Classification
Authors: Hanzi Xu, Muhao Chen, Lifu Huang, Slobodan Vucetic, Wenpeng Yin
Categories: cs.CL
\\
  In recent years, few-shot and zero-shot learning, which learn to predict
labels with limited annotated instances, have garnered significant attention.
Traditional approaches often treat frequent-shot (freq-shot; labels with
abundant instances), few-shot, and zero-shot learning as distinct challenges,
optimizing systems for just one of these scenarios. Yet, in real-world
settings, label occurrences vary greatly. Some of them might appear thousands
of times, while others might only appear sporadically or not at all. For
practical deployment, it is crucial that a system can adapt to any label
occurrence. We introduce a novel classification challenge: X-shot, reflecting a
real-world context where freq-shot, few-shot, and zero-shot labels co-occur
without predefined limits. Here, X can span from 0 to positive infinity. The
crux of X-shot centers on open-domain generalization and devising a system
versatile enough to manage various label scenarios. To solve X-shot, we propose
BinBin (Binary INference Based on INstruction following) that leverages the
Indirect Supervision from a large collection of NLP tasks via instruction
following, bolstered by Weak Supervision provided by large language models.
BinBin surpasses previous state-of-the-art techniques on three benchmark
datasets across multiple domains. To our knowledge, this is the first work
addressing X-shot learning, where X remains variable.
\\ ( https://arxiv.org/abs/2403.03863 ,  518kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03866
Date: Wed, 6 Mar 2024 17:16:44 GMT   (8630kb,D)

Title: KIWI: A Dataset of Knowledge-Intensive Writing Instructions for
  Answering Research Questions
Authors: Fangyuan Xu, Kyle Lo, Luca Soldaini, Bailey Kuehl, Eunsol Choi, David
  Wadden
Categories: cs.CL
\\
  Large language models (LLMs) adapted to follow user instructions are now
widely deployed as conversational agents. In this work, we examine one
increasingly common instruction-following task: providing writing assistance to
compose a long-form answer. To evaluate the capabilities of current LLMs on
this task, we construct KIWI, a dataset of knowledge-intensive writing
instructions in the scientific domain. Given a research question, an initial
model-generated answer and a set of relevant papers, an expert annotator
iteratively issues instructions for the model to revise and improve its answer.
We collect 1,260 interaction turns from 234 interaction sessions with three
state-of-the-art LLMs. Each turn includes a user instruction, a model response,
and a human evaluation of the model response. Through a detailed analysis of
the collected responses, we find that all models struggle to incorporate new
information into an existing answer, and to perform precise and unambiguous
edits. Further, we find that models struggle to judge whether their outputs
successfully followed user instructions, with accuracy at least 10 points short
of human agreement. Our findings indicate that KIWI will be a valuable resource
to measure progress and improve LLMs' instruction-following capabilities for
knowledge intensive writing tasks.
\\ ( https://arxiv.org/abs/2403.03866 ,  8630kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03867
Date: Wed, 6 Mar 2024 17:17:36 GMT   (593kb,D)

Title: On the Origins of Linear Representations in Large Language Models
Authors: Yibo Jiang, Goutham Rajendran, Pradeep Ravikumar, Bryon Aragam, Victor
  Veitch
Categories: cs.CL cs.LG stat.ML
\\
  Recent works have argued that high-level semantic concepts are encoded
"linearly" in the representation space of large language models. In this work,
we study the origins of such linear representations. To that end, we introduce
a simple latent variable model to abstract and formalize the concept dynamics
of the next token prediction. We use this formalism to show that the next token
prediction objective (softmax with cross-entropy) and the implicit bias of
gradient descent together promote the linear representation of concepts.
Experiments show that linear representations emerge when learning from data
matching the latent variable model, confirming that this simple structure
already suffices to yield linear representations. We additionally confirm some
predictions of the theory using the LLaMA-2 large language model, giving
evidence that the simplified model yields generalizable insights.
\\ ( https://arxiv.org/abs/2403.03867 ,  593kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03870
Date: Wed, 6 Mar 2024 17:23:28 GMT   (9354kb,D)

Title: Learning to Decode Collaboratively with Multiple Language Models
Authors: Shannon Zejiang Shen, Hunter Lang, Bailin Wang, Yoon Kim, David Sontag
Categories: cs.CL cs.LG
Comments: 16 pages, 4 figures, 11 tables
\\
  We propose a method to teach multiple large language models (LLM) to
collaborate by interleaving their generations at the token level. We model the
decision of which LLM generates the next token as a latent variable. By
optimizing the marginal likelihood of a training set under our latent variable
model, the base LLM automatically learns when to generate itself and when to
call on one of the ``assistant'' language models to generate, all without
direct supervision. Token-level collaboration during decoding allows for a
fusion of each model's expertise in a manner tailored to the specific task at
hand. Our collaborative decoding is especially useful in cross-domain settings
where a generalist base LLM learns to invoke domain expert models. On
instruction-following, domain-specific QA, and reasoning tasks, we show that
the performance of the joint system exceeds that of the individual models.
Through qualitative analysis of the learned latent decisions, we show models
trained with our method exhibit several interesting collaboration patterns,
e.g., template-filling. Our code is available at
https://github.com/clinicalml/co-llm.
\\ ( https://arxiv.org/abs/2403.03870 ,  9354kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03874
Date: Wed, 6 Mar 2024 17:35:27 GMT   (7685kb)

Title: Impoverished Language Technology: The Lack of (Social) Class in NLP
Authors: Amanda Cercas Curry, Zeerak Talat, Dirk Hovy
Categories: cs.CL cs.AI cs.CY
Comments: Accepted to LREC-COLING 2024
\\
  Since Labov's (1964) foundational work on the social stratification of
language, linguistics has dedicated concerted efforts towards understanding the
relationships between socio-demographic factors and language production and
perception. Despite the large body of evidence identifying significant
relationships between socio-demographic factors and language production,
relatively few of these factors have been investigated in the context of NLP
technology. While age and gender are well covered, Labov's initial target,
socio-economic class, is largely absent. We survey the existing Natural
Language Processing (NLP) literature and find that only 20 papers even mention
socio-economic status. However, the majority of those papers do not engage with
class beyond collecting information of annotator-demographics. Given this
research lacuna, we provide a definition of class that can be operationalised
by NLP researchers, and argue for including socio-economic class in future
language technologies.
\\ ( https://arxiv.org/abs/2403.03874 ,  7685kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03883
Date: Wed, 6 Mar 2024 17:42:16 GMT   (8531kb,D)

Title: SaulLM-7B: A pioneering Large Language Model for Law
Authors: Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui
  Melo, Caio Corro, Andre F. T. Martins, Fabrizio Esposito, Vera L\'ucia
  Raposo, Sofia Morgado, Michael Desa
Categories: cs.CL
\\
  In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored
for the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM
designed explicitly for legal text comprehension and generation. Leveraging the
Mistral 7B architecture as its foundation, SaulLM-7B is trained on an English
legal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art
proficiency in understanding and processing legal documents. Additionally, we
present a novel instructional fine-tuning method that leverages legal datasets
to further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is
released under the CC-BY-SA-4.0 License.
\\ ( https://arxiv.org/abs/2403.03883 ,  8531kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03888
Date: Wed, 6 Mar 2024 17:48:06 GMT   (573kb,D)

Title: FaaF: Facts as a Function for the evaluation of RAG systems
Authors: Vasileios Katranidis and Gabor Barany
Categories: cs.CL
Comments: 12 pages, 3 figures
\\
  Factual recall from a reference source is crucial for evaluating the
performance of Retrieval Augmented Generation (RAG) systems, as it directly
probes into the quality of both retrieval and generation. However, it still
remains a challenge to perform this evaluation reliably and efficiently. Recent
work has focused on fact verification via prompting language model (LM)
evaluators, however we demonstrate that these methods are unreliable in the
presence of incomplete or inaccurate information. We introduce Facts as a
Function (FaaF), a new approach to fact verification that utilizes the function
calling abilities of LMs and a framework for RAG factual recall evaluation.
FaaF substantially improves the ability of LMs to identify unsupported facts in
text with incomplete information whilst improving efficiency and lowering cost
by several times, compared to prompt-based approaches.
\\ ( https://arxiv.org/abs/2403.03888 ,  573kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03893
Date: Wed, 6 Mar 2024 17:51:43 GMT   (297kb,D)

Title: From One to Many: Expanding the Scope of Toxicity Mitigation in Language
  Models
Authors: Luiza Pozzobon, Patrick Lewis, Sara Hooker and Beyza Ermis
Categories: cs.CL cs.AI
\\
  To date, toxicity mitigation in language models has almost entirely been
focused on single-language settings. As language models embrace multilingual
capabilities, it's crucial our safety measures keep pace. Recognizing this
research gap, our approach expands the scope of conventional toxicity
mitigation to address the complexities presented by multiple languages. In the
absence of sufficient annotated datasets across languages, we employ translated
data to evaluate and enhance our mitigation techniques. We also compare
finetuning mitigation approaches against retrieval-augmented techniques under
both static and continual toxicity mitigation scenarios. This allows us to
examine the effects of translation quality and the cross-lingual transfer on
toxicity mitigation. We also explore how model size and data quantity affect
the success of these mitigation efforts. Covering nine languages, our study
represents a broad array of linguistic families and levels of resource
availability, ranging from high to mid-resource languages. Through
comprehensive experiments, we provide insights into the complexities of
multilingual toxicity mitigation, offering valuable insights and paving the way
for future research in this increasingly important field. Code and data are
available at https://github.com/for-ai/goodtriever.
\\ ( https://arxiv.org/abs/2403.03893 ,  297kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03909
Date: Wed, 6 Mar 2024 18:14:22 GMT   (879kb,D)

Title: A Measure for Transparent Comparison of Linguistic Diversity in
  Multilingual NLP Data Sets
Authors: Tanja Samardzic, Ximena Gutierrez, Christian Bentz, Steven Moran, Olga
  Pelloni
Categories: cs.CL
\\
  Typologically diverse benchmarks are increasingly created to track the
progress achieved in multilingual NLP. Linguistic diversity of these data sets
is typically measured as the number of languages or language families included
in the sample, but such measures do not consider structural properties of the
included languages. In this paper, we propose assessing linguistic diversity of
a data set against a reference language sample as a means of maximising
linguistic diversity in the long run. We represent languages as sets of
features and apply a version of the Jaccard index suitable for comparing sets
of measures. In addition to the features extracted from typological data bases,
we propose an automatic text-based measure, which can be used as a means of
overcoming the well-known problem of data sparsity in manually collected
features. Our diversity score is interpretable in terms of linguistic features
and can identify the types of languages that are not represented in a data set.
Using our method, we analyse a range of popular multilingual data sets (UD,
Bible100, mBERT, XTREME, XGLUE, XNLI, XCOPA, TyDiQA, XQuAD). In addition to
ranking these data sets, we find, for example, that (poly)synthetic languages
are missing in almost all of them.
\\ ( https://arxiv.org/abs/2403.03909 ,  879kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03923
Date: Wed, 6 Mar 2024 18:33:51 GMT   (110kb,D)

Title: Did Translation Models Get More Robust Without Anyone Even Noticing?
Authors: Ben Peters and Andr\'e F.T. Martins
Categories: cs.CL
\\
  Neural machine translation (MT) models achieve strong results across a
variety of settings, but it is widely believed that they are highly sensitive
to "noisy" inputs, such as spelling errors, abbreviations, and other formatting
issues. In this paper, we revisit this insight in light of recent multilingual
MT models and large language models (LLMs) applied to machine translation.
Somewhat surprisingly, we show through controlled experiments that these models
are far more robust to many kinds of noise than previous models, even when they
perform similarly on clean data. This is notable because, even though LLMs have
more parameters and more complex training processes than past models, none of
the open ones we consider use any techniques specifically designed to encourage
robustness. Next, we show that similar trends hold for social media translation
experiments -- LLMs are more robust to social media text. We include an
analysis of the circumstances in which source correction techniques can be used
to mitigate the effects of noise. Altogether, we show that robustness to many
types of noise has increased.
\\ ( https://arxiv.org/abs/2403.03923 ,  110kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03942
Date: Wed, 6 Mar 2024 18:50:14 GMT   (1049kb,D)

Title: The Heuristic Core: Understanding Subnetwork Generalization in
  Pretrained Language Models
Authors: Adithya Bhaskar, Dan Friedman, Danqi Chen
Categories: cs.CL cs.LG
Comments: Our code is available at
  https://github.com/princeton-nlp/Heuristic-Core
\\
  Prior work has found that pretrained language models (LMs) fine-tuned with
different random seeds can achieve similar in-domain performance but generalize
differently on tests of syntactic generalization. In this work, we show that,
even within a single model, we can find multiple subnetworks that perform
similarly in-domain, but generalize vastly differently. To better understand
these phenomena, we investigate if they can be understood in terms of
"competing subnetworks": the model initially represents a variety of distinct
algorithms, corresponding to different subnetworks, and generalization occurs
when it ultimately converges to one. This explanation has been used to account
for generalization in simple algorithmic tasks. Instead of finding competing
subnetworks, we find that all subnetworks -- whether they generalize or not --
share a set of attention heads, which we refer to as the heuristic core.
Further analysis suggests that these attention heads emerge early in training
and compute shallow, non-generalizing features. The model learns to generalize
by incorporating additional attention heads, which depend on the outputs of the
"heuristic" heads to compute higher-level features. Overall, our results offer
a more detailed picture of the mechanisms for syntactic generalization in
pretrained LMs.
\\ ( https://arxiv.org/abs/2403.03942 ,  1049kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03222
Date: Thu, 15 Feb 2024 01:52:44 GMT   (745kb,D)

Title: Knowledge-guided EEG Representation Learning
Authors: Aditya Kommineni, Kleanthis Avramidis, Richard Leahy, Shrikanth
  Narayanan
Categories: cs.LG cs.AI eess.SP
Comments: 6 Pages, 5 figures, Submitted to EMBC 2024
\\
  Self-supervised learning has produced impressive results in multimedia
domains of audio, vision and speech. This paradigm is equally, if not more,
relevant for the domain of biosignals, owing to the scarcity of labelled data
in such scenarios. The ability to leverage large-scale unlabelled data to learn
robust representations could help improve the performance of numerous inference
tasks on biosignals. Given the inherent domain differences between multimedia
modalities and biosignals, the established objectives for self-supervised
learning may not translate well to this domain. Hence, there is an unmet need
to adapt these methods to biosignal analysis. In this work we propose a
self-supervised model for EEG, which provides robust performance and remarkable
parameter efficiency by using state space-based deep learning architecture. We
also propose a novel knowledge-guided pre-training objective that accounts for
the idiosyncrasies of the EEG signal. The results indicate improved embedding
representation learning and downstream performance compared to prior works on
exemplary tasks. Also, the proposed objective significantly reduces the amount
of pre-training data required to obtain performance equivalent to prior works.
\\ ( https://arxiv.org/abs/2403.03222 ,  745kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03223
Date: Thu, 15 Feb 2024 17:41:02 GMT   (36198kb,D)

Title: Exact Enforcement of Temporal Continuity in Sequential Physics-Informed
  Neural Networks
Authors: Pratanu Roy and Stephen Castonguay
Categories: cs.LG physics.comp-ph
\\
  The use of deep learning methods in scientific computing represents a
potential paradigm shift in engineering problem solving. One of the most
prominent developments is Physics-Informed Neural Networks (PINNs), in which
neural networks are trained to satisfy partial differential equations (PDEs)
and/or observed data. While this method shows promise, the standard version has
been shown to struggle in accurately predicting the dynamic behavior of
time-dependent problems. To address this challenge, methods have been proposed
that decompose the time domain into multiple segments, employing a distinct
neural network in each segment and directly incorporating continuity between
them in the loss function of the minimization problem. In this work we
introduce a method to exactly enforce continuity between successive time
segments via a solution ansatz. This hard constrained sequential PINN
(HCS-PINN) method is simple to implement and eliminates the need for any loss
terms associated with temporal continuity. The method is tested for a number of
benchmark problems involving both linear and non-linear PDEs. Examples include
various first order time dependent problems in which traditional PINNs
struggle, namely advection, Allen-Cahn, and Korteweg-de Vries equations.
Furthermore, second and third order time-dependent problems are demonstrated
via wave and Jerky dynamics examples, respectively. Notably, the Jerky dynamics
problem is chaotic, making the problem especially sensitive to temporal
accuracy. The numerical experiments conducted with the proposed method
demonstrated superior convergence and accuracy over both traditional PINNs and
the soft-constrained counterparts.
\\ ( https://arxiv.org/abs/2403.03223 ,  36198kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03281
Date: Tue, 5 Mar 2024 19:25:55 GMT   (574kb,D)

Title: Credibility-Aware Multi-Modal Fusion Using Probabilistic Circuits
Authors: Sahil Sidheekh, Pranuthi Tenali, Saurabh Mathur, Erik Blasch, Kristian
  Kersting, Sriraam Natarajan
Categories: cs.LG cs.AI
\\
  We consider the problem of late multi-modal fusion for discriminative
learning. Motivated by noisy, multi-source domains that require understanding
the reliability of each data source, we explore the notion of credibility in
the context of multi-modal fusion. We propose a combination function that uses
probabilistic circuits (PCs) to combine predictive distributions over
individual modalities. We also define a probabilistic measure to evaluate the
credibility of each modality via inference queries over the PC. Our
experimental evaluation demonstrates that our fusion method can reliably infer
credibility while maintaining competitive performance with the
state-of-the-art.
\\ ( https://arxiv.org/abs/2403.03281 ,  574kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03292
Date: Tue, 5 Mar 2024 19:47:51 GMT   (133kb,D)

Title: Averaging Rate Scheduler for Decentralized Learning on Heterogeneous
  Data
Authors: Sai Aparna Aketi, Sakshi Choudhary, Kaushik Roy
Categories: cs.LG cs.DC
Comments: 9 pages, 3 figures, 4 tables. arXiv admin note: text overlap with
  arXiv:2305.04792
\\
  State-of-the-art decentralized learning algorithms typically require the data
distribution to be Independent and Identically Distributed (IID). However, in
practical scenarios, the data distribution across the agents can have
significant heterogeneity. In this work, we propose averaging rate scheduling
as a simple yet effective way to reduce the impact of heterogeneity in
decentralized learning. Our experiments illustrate the superiority of the
proposed method (~3% improvement in test accuracy) compared to the conventional
approach of employing a constant averaging rate.
\\ ( https://arxiv.org/abs/2403.03292 ,  133kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03328
Date: Tue, 5 Mar 2024 21:12:10 GMT   (2238kb)

Title: An Ensemble Framework for Explainable Geospatial Machine Learning Models
Authors: Lingbo Liu
Categories: cs.LG cs.CY
\\
  Analyzing spatial varying effect is pivotal in geographic analysis. Yet,
accurately capturing and interpreting this variability is challenging due to
the complexity and non-linearity of geospatial data. Herein, we introduce an
integrated framework that merges local spatial weighting scheme, Explainable
Artificial Intelligence (XAI), and cutting-edge machine learning technologies
to bridge the gap between traditional geographic analysis models and general
machine learning approaches. Through tests on synthetic datasets, this
framework is verified to enhance the interpretability and accuracy of
predictions in both geographic regression and classification by elucidating
spatial variability. It significantly boosts prediction precision, offering a
novel approach to understanding spatial phenomena.
\\ ( https://arxiv.org/abs/2403.03328 ,  2238kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03333
Date: Tue, 5 Mar 2024 21:34:23 GMT   (2436kb,D)

Title: Solution Simplex Clustering for Heterogeneous Federated Learning
Authors: Dennis Grinwald, Philipp Wiesner, Shinichi Nakajima
Categories: cs.LG cs.DC
\\
  We tackle a major challenge in federated learning (FL) -- achieving good
performance under highly heterogeneous client distributions. The difficulty
partially arises from two seemingly contradictory goals: learning a common
model by aggregating the information from clients, and learning local
personalized models that should be adapted to each local distribution. In this
work, we propose Solution Simplex Clustered Federated Learning (SosicFL) for
dissolving such contradiction. Based on the recent ideas of learning solution
simplices, SosicFL assigns a subregion in a simplex to each client, and
performs FL to learn a common solution simplex. This allows the client models
to possess their characteristics within the degrees of freedom in the solution
simplex, and at the same time achieves the goal of learning a global common
model. Our experiments show that SosicFL improves the performance and
accelerates the training process for global and personalized FL with minimal
computational overhead.
\\ ( https://arxiv.org/abs/2403.03333 ,  2436kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03362
Date: Tue, 5 Mar 2024 23:16:13 GMT   (3599kb,D)

Title: Level Set Teleportation: An Optimization Perspective
Authors: Aaron Mishkin, Alberto Bietti, and Robert M. Gower
Categories: cs.LG math.OC
Comments: Thirty-five pages including appendices
\\
  We study level set teleportation, an optimization sub-routine which seeks to
accelerate gradient methods by maximizing the gradient norm on a level-set of
the objective function. Since the descent lemma implies that gradient descent
(GD) decreases the objective proportional to the squared norm of the gradient,
level-set teleportation maximizes this one-step progress guarantee. For convex
functions satisfying Hessian stability, we prove that GD with level-set
teleportation obtains a combined sub-linear/linear convergence rate which is
strictly faster than standard GD when the optimality gap is small. This is in
sharp contrast to the standard (strongly) convex setting, where we show
level-set teleportation neither improves nor worsens convergence rates. To
evaluate teleportation in practice, we develop a projected-gradient-type method
requiring only Hessian-vector products. We use this method to show that
gradient methods with access to a teleportation oracle uniformly out-perform
their standard versions on a variety of learning problems.
\\ ( https://arxiv.org/abs/2403.03362 ,  3599kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03368
Date: Tue, 5 Mar 2024 23:31:07 GMT   (775kb,D)

Title: Leveraging Federated Learning for Automatic Detection of Clopidogrel
  Treatment Failures
Authors: Samuel Kim and Min Sang Kim
Categories: cs.LG cs.CY
\\
  The effectiveness of clopidogrel, a widely used antiplatelet medication,
varies significantly among individuals, necessitating the development of
precise predictive models to optimize patient care. In this study, we leverage
federated learning strategies to address clopidogrel treatment failure
detection. Our research harnesses the collaborative power of multiple
healthcare institutions, allowing them to jointly train machine learning models
while safeguarding sensitive patient data. Utilizing the UK Biobank dataset,
which encompasses a vast and diverse population, we partitioned the data based
on geographic centers and evaluated the performance of federated learning. Our
results show that while centralized training achieves higher Area Under the
Curve (AUC) values and faster convergence, federated learning approaches can
substantially narrow this performance gap. Our findings underscore the
potential of federated learning in addressing clopidogrel treatment failure
detection, offering a promising avenue for enhancing patient care through
personalized treatment strategies while respecting data privacy. This study
contributes to the growing body of research on federated learning in healthcare
and lays the groundwork for secure and privacy-preserving predictive models for
various medical conditions.
\\ ( https://arxiv.org/abs/2403.03368 ,  775kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03372
Date: Tue, 5 Mar 2024 23:37:43 GMT   (9749kb,D)

Title: TartanAviation: Image, Speech, and ADS-B Trajectory Datasets for
  Terminal Airspace Operations
Authors: Jay Patrikar, Joao Dantas, Brady Moon, Milad Hamidi, Sourish Ghosh,
  Nikhil Keetha, Ian Higgins, Atharva Chandak, Takashi Yoneyama, and Sebastian
  Scherer
Categories: cs.LG
Comments: 8 pages, 6 figures, 2 tables
\\
  We introduce TartanAviation, an open-source multi-modal dataset focused on
terminal-area airspace operations. TartanAviation provides a holistic view of
the airport environment by concurrently collecting image, speech, and ADS-B
trajectory data using setups installed inside airport boundaries. The datasets
were collected at both towered and non-towered airfields across multiple months
to capture diversity in aircraft operations, seasons, aircraft types, and
weather conditions. In total, TartanAviation provides 3.1M images, 3374 hours
of Air Traffic Control speech data, and 661 days of ADS-B trajectory data. The
data was filtered, processed, and validated to create a curated dataset. In
addition to the dataset, we also open-source the code-base used to collect and
pre-process the dataset, further enhancing accessibility and usability. We
believe this dataset has many potential use cases and would be particularly
vital in allowing AI and machine learning technologies to be integrated into
air traffic control systems and advance the adoption of autonomous aircraft in
the airspace.
\\ ( https://arxiv.org/abs/2403.03372 ,  9749kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03375
Date: Tue, 5 Mar 2024 23:54:00 GMT   (41745kb,D)

Title: Complexity Matters: Dynamics of Feature Learning in the Presence of
  Spurious Correlations
Authors: GuanWen Qiu, Da Kuang, Surbhi Goel
Categories: cs.LG
Comments: Code for the project is available at
  https://github.com/NayutaQiu/Boolean_Spurious
\\
  Existing research often posits spurious features as "easier" to learn than
core features in neural network optimization, but the impact of their relative
simplicity remains under-explored. Moreover they mainly focus on the end
performance intead of the learning dynamics of feature learning. In this paper,
we propose a theoretical framework and associated synthetic dataset grounded in
boolean function analysis which allows for fine-grained control on the relative
complexity (compared to core features) and correlation strength (with respect
to the label) of spurious features to study the dynamics of feature learning
under spurious correlation. Our setup uncovers several interesting phenomenon:
(1) stronger spurious correlations or simpler spurious features slow down the
rate of learning for the core features, (2) learning phases of spurious
features and core features are not always separable, (3) spurious features are
not forgotten even after core features are fully learned. We show that our
findings justify the success of retraining the last layer to remove spurious
correlation and also identifies limitations of popular debiasing algorithms
that exploit early learning of spurious features. We support our empirical
findings with theoretical analyses for the case of learning XOR features with a
one-hidden-layer ReLU network.
\\ ( https://arxiv.org/abs/2403.03375 ,  41745kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03410
Date: Wed, 6 Mar 2024 02:37:26 GMT   (319kb)

Title: Prediction Of Cryptocurrency Prices Using LSTM, SVM And Polynomial
  Regression
Authors: Novan Fauzi Al Giffary, Feri Sulianta
Categories: cs.LG q-fin.ST
Comments: Asian Journal of Engineering, Social and Health Volume 3, No. 2
  February 2024 (308-319)
\\
  The rapid development of information technology, especially the Internet, has
facilitated users with a quick and easy way to seek information. With these
convenience offered by internet services, many individuals who initially
invested in gold and precious metals are now shifting into digital investments
in form of cryptocurrencies. However, investments in crypto coins are filled
with uncertainties and fluctuation in daily basis. This risk posed as
significant challenges for coin investors that could result in substantial
investment losses. The uncertainty of the value of these crypto coins is a
critical issue in the field of coin investment. Forecasting, is one of the
methods used to predict the future value of these crypto coins. By utilizing
the models of Long Short Term Memory, Support Vector Machine, and Polynomial
Regression algorithm for forecasting, a performance comparison is conducted to
determine which algorithm model is most suitable for predicting crypto currency
prices. The mean square error is employed as a benchmark for the comparison. By
applying those three constructed algorithm models, the Support Vector Machine
uses a linear kernel to produce the smallest mean square error compared to the
Long Short Term Memory and Polynomial Regression algorithm models, with a mean
square error value of 0.02. Keywords: Cryptocurrency, Forecasting, Long Short
Term Memory, Mean Square Error, Polynomial Regression, Support Vector Machine
\\ ( https://arxiv.org/abs/2403.03410 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03412
Date: Wed, 6 Mar 2024 02:39:22 GMT   (12176kb,D)

Title: Advancing Out-of-Distribution Detection through Data Purification and
  Dynamic Activation Function Design
Authors: Yingrui Ji, Yao Zhu, Zhigang Li, Jiansheng Chen, Yunlong Kong and
  Jingbo Chen
Categories: cs.LG cs.CV
\\
  In the dynamic realms of machine learning and deep learning, the robustness
and reliability of models are paramount, especially in critical real-world
applications. A fundamental challenge in this sphere is managing
Out-of-Distribution (OOD) samples, significantly increasing the risks of model
misclassification and uncertainty. Our work addresses this challenge by
enhancing the detection and management of OOD samples in neural networks. We
introduce OOD-R (Out-of-Distribution-Rectified), a meticulously curated
collection of open-source datasets with enhanced noise reduction properties.
In-Distribution (ID) noise in existing OOD datasets can lead to inaccurate
evaluation of detection algorithms. Recognizing this, OOD-R incorporates noise
filtering technologies to refine the datasets, ensuring a more accurate and
reliable evaluation of OOD detection algorithms. This approach not only
improves the overall quality of data but also aids in better distinguishing
between OOD and ID samples, resulting in up to a 2.5\% improvement in model
accuracy and a minimum 3.2\% reduction in false positives. Furthermore, we
present ActFun, an innovative method that fine-tunes the model's response to
diverse inputs, thereby improving the stability of feature extraction and
minimizing specificity issues. ActFun addresses the common problem of model
overconfidence in OOD detection by strategically reducing the influence of
hidden units, which enhances the model's capability to estimate OOD uncertainty
more accurately. Implementing ActFun in the OOD-R dataset has led to
significant performance enhancements, including an 18.42\% increase in AUROC of
the GradNorm method and a 16.93\% decrease in FPR95 of the Energy method.
Overall, our research not only advances the methodologies in OOD detection but
also emphasizes the importance of dataset integrity for accurate algorithm
evaluation.
\\ ( https://arxiv.org/abs/2403.03412 ,  12176kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03414
Date: Wed, 6 Mar 2024 02:46:17 GMT   (1066kb,D)

Title: Leveraging The Finite States of Emotion Processing to Study Late-Life
  Mental Health
Authors: Yuanzhe Huang, Saurab Faruque, Minjie Wu, Akiko Mizuno, Eduardo Diniz,
  Shaolin Yang, George Dewitt Stetten, Noah Schweitzer, Hecheng Jin, Linghai
  Wang, Howard J. Aizenstein
Categories: cs.LG q-bio.NC
\\
  Traditional approaches in mental health research apply General Linear Models
(GLM) to describe the longitudinal dynamics of observed psycho-behavioral
measurements (questionnaire summary scores). Similarly, GLMs are also applied
to characterize relationships between neurobiological measurements (regional
fMRI signals) and perceptual stimuli or other regional signals. While these
methods are useful for exploring linear correlations among the isolated signals
of those constructs (i.e., summary scores or fMRI signals), these classical
frameworks fall short in providing insights into the comprehensive system-level
dynamics underlying observable changes. Hidden Markov Models (HMM) are a
statistical model that enable us to describe the sequential relations among
multiple observable constructs, and when applied through the lens of Finite
State Automata (FSA), can provide a more integrated and intuitive framework for
modeling and understanding the underlying controller (the prescription for how
to respond to inputs) that fundamentally defines any system, as opposed to
linearly correlating output signals produced by the controller. We present a
simple and intuitive HMM processing pipeline vcHMM (See Preliminary Data) that
highlights FSA theory and is applicable for both behavioral analysis of
questionnaire data and fMRI data. HMMs offer theoretic promise as they are
computationally equivalent to the FSA, the control processor of a Turing
Machine (TM) The dynamic programming Viterbi algorithm is used to leverage the
HMM model. It efficiently identifies the most likely sequence of hidden states.
The vcHMM pipeline leverages this grammar to understand how behavior and neural
activity relate to depression.
\\ ( https://arxiv.org/abs/2403.03414 ,  1066kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03425
Date: Wed, 6 Mar 2024 03:15:25 GMT   (5177kb,D)

Title: Sculpting Molecules in 3D: A Flexible Substructure Aware Framework for
  Text-Oriented Molecular Optimization
Authors: Kaiwei Zhang, Yange Lin, Guangcheng Wu, Yuxiang Ren, Xuecang Zhang, Bo
  wang, Xiaoyu Zhang, Weitao Du
Categories: cs.LG physics.chem-ph q-bio.BM
\\
  The integration of deep learning, particularly AI-Generated Content, with
high-quality data derived from ab initio calculations has emerged as a
promising avenue for transforming the landscape of scientific research.
However, the challenge of designing molecular drugs or materials that
incorporate multi-modality prior knowledge remains a critical and complex
undertaking. Specifically, achieving a practical molecular design necessitates
not only meeting the diversity requirements but also addressing structural and
textural constraints with various symmetries outlined by domain experts. In
this article, we present an innovative approach to tackle this inverse design
problem by formulating it as a multi-modality guidance generation/optimization
task. Our proposed solution involves a textural-structure alignment symmetric
diffusion framework for the implementation of molecular generation/optimization
tasks, namely 3DToMolo. 3DToMolo aims to harmonize diverse modalities, aligning
them seamlessly to produce molecular structures adhere to specified symmetric
structural and textural constraints by experts in the field. Experimental
trials across three guidance generation settings have shown a superior hit
generation performance compared to state-of-the-art methodologies. Moreover,
3DToMolo demonstrates the capability to generate novel molecules, incorporating
specified target substructures, without the need for prior knowledge. This work
not only holds general significance for the advancement of deep learning
methodologies but also paves the way for a transformative shift in molecular
design strategies. 3DToMolo creates opportunities for a more nuanced and
effective exploration of the vast chemical space, opening new frontiers in the
development of molecular entities with tailored properties and functionalities.
\\ ( https://arxiv.org/abs/2403.03425 ,  5177kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03444
Date: Wed, 6 Mar 2024 04:02:30 GMT   (38527kb,D)

Title: Uncertainty quantification for deeponets with ensemble kalman inversion
Authors: Andrew Pensoneault, Xueyu Zhu
Categories: cs.LG cs.AI cs.NA math.NA stat.ML
Comments: 25 pages
MSC-class: 65
\\
  In recent years, operator learning, particularly the DeepONet, has received
much attention for efficiently learning complex mappings between input and
output functions across diverse fields. However, in practical scenarios with
limited and noisy data, accessing the uncertainty in DeepONet predictions
becomes essential, especially in mission-critical or safety-critical
applications. Existing methods, either computationally intensive or yielding
unsatisfactory uncertainty quantifica- tion, leave room for developing
efficient and informative uncertainty quantification (UQ) techniques tailored
for DeepONets. In this work, we proposed a novel inference approach for
efficient UQ for operator learning by harnessing the power of the Ensemble
Kalman Inversion (EKI) approach. EKI, known for its derivative-free,
noise-robust, and highly parallelizable feature, has demonstrated its
advantages for UQ for physics-informed neural networks [28]. Our innovative
application of EKI enables us to efficiently train ensembles of DeepONets while
obtaining informative uncertainty estimates for the output of interest. We
deploy a mini-batch variant of EKI to accommodate larger datasets, mitigating
the computational demand due to large datasets during the training stage.
Furthermore, we introduce a heuristic method to estimate the artificial
dynamics covariance, thereby improving our uncertainty estimates. Finally, we
demonstrate the effectiveness and versatility of our proposed methodology
across various benchmark problems, showcasing its potential to address the
pressing challenges of uncertainty quantification in DeepONets, especially for
practical applications with limited and noisy data.
\\ ( https://arxiv.org/abs/2403.03444 ,  38527kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03448
Date: Wed, 6 Mar 2024 04:24:43 GMT   (110kb)

Title: Kernel Correlation-Dissimilarity for Multiple Kernel k-Means Clustering
Authors: Rina Su, Yu Guo, Caiying Wu, Qiyu Jin, Tieyong Zeng
Categories: cs.LG cs.CV
Comments: 36 pages. This paper was accepted by Pattern Recognition on January
  31, 2024
Journal-ref: Pattern Recognition, 2024, 150:110307
DOI: 10.1016/j.patcog.2024.110307
\\
  The main objective of the Multiple Kernel k-Means (MKKM) algorithm is to
extract non-linear information and achieve optimal clustering by optimizing
base kernel matrices. Current methods enhance information diversity and reduce
redundancy by exploiting interdependencies among multiple kernels based on
correlations or dissimilarities. Nevertheless, relying solely on a single
metric, such as correlation or dissimilarity, to define kernel relationships
introduces bias and incomplete characterization. Consequently, this limitation
hinders efficient information extraction, ultimately compromising clustering
performance. To tackle this challenge, we introduce a novel method that
systematically integrates both kernel correlation and dissimilarity. Our
approach comprehensively captures kernel relationships, facilitating more
efficient classification information extraction and improving clustering
performance. By emphasizing the coherence between kernel correlation and
dissimilarity, our method offers a more objective and transparent strategy for
extracting non-linear information and significantly improving clustering
precision, supported by theoretical rationale. We assess the performance of our
algorithm on 13 challenging benchmark datasets, demonstrating its superiority
over contemporary state-of-the-art MKKM techniques.
\\ ( https://arxiv.org/abs/2403.03448 ,  110kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03454
Date: Wed, 6 Mar 2024 04:43:22 GMT   (679kb,D)

Title: Learning Constrained Optimization with Deep Augmented Lagrangian Methods
Authors: James Kotary and Ferdinando Fioretto
Categories: cs.LG math.OC
\\
  Learning to Optimize (LtO) is a problem setting in which a machine learning
(ML) model is trained to emulate a constrained optimization solver. Learning to
produce optimal and feasible solutions subject to complex constraints is a
difficult task, but is often made possible by restricting the input space to a
limited distribution of related problems. Most LtO methods focus on directly
learning solutions to the primal problem, and applying correction schemes or
loss function penalties to encourage feasibility. This paper proposes an
alternative approach, in which the ML model is trained instead to predict dual
solution estimates directly, from which primal estimates are constructed to
form dual-feasible solution pairs. This enables an end-to-end training scheme
is which the dual objective is maximized as a loss function, and solution
estimates iterate toward primal feasibility, emulating a Dual Ascent method.
First it is shown that the poor convergence properties of classical Dual Ascent
are reflected in poor convergence of the proposed training scheme. Then, by
incorporating techniques from practical Augmented Lagrangian methods, we show
how the training scheme can be improved to learn highly accurate constrained
optimization solvers, for both convex and nonconvex problems.
\\ ( https://arxiv.org/abs/2403.03454 ,  679kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03465
Date: Wed, 6 Mar 2024 05:00:31 GMT   (2099kb,D)

Title: Self-Attention Empowered Graph Convolutional Network for Structure
  Learning and Node Embedding
Authors: Mengying Jiang, Guizhong Liu, Yuanchao Su, Xinliang Wu
Categories: cs.LG cs.SI
Comments: 33 pages,6 figures,9 tables
\\
  In representation learning on graph-structured data, many popular graph
neural networks (GNNs) fail to capture long-range dependencies, leading to
performance degradation. Furthermore, this weakness is magnified when the
concerned graph is characterized by heterophily (low homophily). To solve this
issue, this paper proposes a novel graph learning framework called the graph
convolutional network with self-attention (GCN-SA). The proposed scheme
exhibits an exceptional generalization capability in node-level representation
learning. The proposed GCN-SA contains two enhancements corresponding to edges
and node features. For edges, we utilize a self-attention mechanism to design a
stable and effective graph-structure-learning module that can capture the
internal correlation between any pair of nodes. This graph-structure-learning
module can identify reliable neighbors for each node from the entire graph.
Regarding the node features, we modify the transformer block to make it more
applicable to enable GCN to fuse valuable information from the entire graph.
These two enhancements work in distinct ways to help our GCN-SA capture
long-range dependencies, enabling it to perform representation learning on
graphs with varying levels of homophily. The experimental results on benchmark
datasets demonstrate the effectiveness of the proposed GCN-SA. Compared to
other outstanding GNN counterparts, the proposed GCN-SA is competitive.
\\ ( https://arxiv.org/abs/2403.03465 ,  2099kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03472
Date: Wed, 6 Mar 2024 05:13:23 GMT   (1959kb,D)

Title: Boosting Meta-Training with Base Class Information for Few-Shot Learning
Authors: Weihao Jiang, Guodong Liu, Di He, Kun He
Categories: cs.LG cs.CV
Comments: 11 pages, 6 figures, submitted to a journal
\\
  Few-shot learning, a challenging task in machine learning, aims to learn a
classifier adaptable to recognize new, unseen classes with limited labeled
examples. Meta-learning has emerged as a prominent framework for few-shot
learning. Its training framework is originally a task-level learning method,
such as Model-Agnostic Meta-Learning (MAML) and Prototypical Networks. And a
recently proposed training paradigm called Meta-Baseline, which consists of
sequential pre-training and meta-training stages, gains state-of-the-art
performance. However, as a non-end-to-end training method, indicating the
meta-training stage can only begin after the completion of pre-training,
Meta-Baseline suffers from higher training cost and suboptimal performance due
to the inherent conflicts of the two training stages. To address these
limitations, we propose an end-to-end training paradigm consisting of two
alternative loops. In the outer loop, we calculate cross entropy loss on the
entire training set while updating only the final linear layer. In the inner
loop, we employ the original meta-learning training mode to calculate the loss
and incorporate gradients from the outer loss to guide the parameter updates.
This training paradigm not only converges quickly but also outperforms existing
baselines, indicating that information from the overall training set and the
meta-learning training paradigm could mutually reinforce one another. Moreover,
being model-agnostic, our framework achieves significant performance gains,
surpassing the baseline systems by approximate 1%.
\\ ( https://arxiv.org/abs/2403.03472 ,  1959kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03473
Date: Wed, 6 Mar 2024 05:13:28 GMT   (3872kb,D)

Title: Inverse-Free Fast Natural Gradient Descent Method for Deep Learning
Authors: Xinwei Ou, Ce Zhu, Xiaolin Huang, and Yipeng Liu
Categories: cs.LG cs.CV
\\
  Second-order methods can converge much faster than first-order methods by
incorporating second-order derivates or statistics, but they are far less
prevalent in deep learning due to their computational inefficiency. To handle
this, many of the existing solutions focus on reducing the size of the matrix
to be inverted. However, it is still needed to perform the inverse operator in
each iteration. In this paper, we present a fast natural gradient descent
(FNGD) method, which only requires computing the inverse during the first
epoch. Firstly, we reformulate the gradient preconditioning formula in the
natural gradient descent (NGD) as a weighted sum of per-sample gradients using
the Sherman-Morrison-Woodbury formula. Building upon this, to avoid the
iterative inverse operation involved in computing coefficients, the weighted
coefficients are shared across epochs without affecting the empirical
performance.
  FNGD approximates the NGD as a fixed-coefficient weighted sum, akin to the
average sum in first-order methods. Consequently, the computational complexity
of FNGD can approach that of first-order methods. To demonstrate the efficiency
of the proposed FNGD, we perform empirical evaluations on image classification
and machine translation tasks. For training ResNet-18 on the CIFAR-100 dataset,
FNGD can achieve a speedup of 2.05$\times$ compared with KFAC. For training
Transformer on Multi30K, FNGD outperforms AdamW by 24 BLEU score while
requiring almost the same training time.
\\ ( https://arxiv.org/abs/2403.03473 ,  3872kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03483
Date: Wed, 6 Mar 2024 05:52:13 GMT   (1633kb,D)

Title: A Teacher-Free Graph Knowledge Distillation Framework with Dual
  Self-Distillation
Authors: Lirong Wu, Haitao Lin, Zhangyang Gao, Guojiang Zhao, Stan Z. Li
Categories: cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2210.02097
\\
  Recent years have witnessed great success in handling graph-related tasks
with Graph Neural Networks (GNNs). Despite their great academic success,
Multi-Layer Perceptrons (MLPs) remain the primary workhorse for practical
industrial applications. One reason for such an academic-industry gap is the
neighborhood-fetching latency incurred by data dependency in GNNs. To reduce
their gaps, Graph Knowledge Distillation (GKD) is proposed, usually based on a
standard teacher-student architecture, to distill knowledge from a large
teacher GNN into a lightweight student GNN or MLP. However, we found in this
paper that neither teachers nor GNNs are necessary for graph knowledge
distillation. We propose a Teacher-Free Graph Self-Distillation (TGS) framework
that does not require any teacher model or GNNs during both training and
inference. More importantly, the proposed TGS framework is purely based on
MLPs, where structural information is only implicitly used to guide dual
knowledge self-distillation between the target node and its neighborhood. As a
result, TGS enjoys the benefits of graph topology awareness in training but is
free from data dependency in inference. Extensive experiments have shown that
the performance of vanilla MLPs can be greatly improved with dual
self-distillation, e.g., TGS improves over vanilla MLPs by 15.54% on average
and outperforms state-of-the-art GKD algorithms on six real-world datasets. In
terms of inference speed, TGS infers 75X-89X faster than existing GNNs and
16X-25X faster than classical inference acceleration methods.
\\ ( https://arxiv.org/abs/2403.03483 ,  1633kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03507
Date: Wed, 6 Mar 2024 07:29:57 GMT   (325kb,D)

Title: GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection
Authors: Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima
  Anandkumar, Yuandong Tian
Categories: cs.LG
\\
  Training Large Language Models (LLMs) presents significant memory challenges,
predominantly due to the growing size of weights and optimizer states. Common
memory-reduction approaches, such as low-rank adaptation (LoRA), add a
trainable low-rank matrix to the frozen pre-trained weight in each layer,
reducing trainable parameters and optimizer states. However, such approaches
typically underperform training with full-rank weights in both pre-training and
fine-tuning stages since they limit the parameter search to a low-rank subspace
and alter the training dynamics, and further, may require full-rank warm start.
In this work, we propose Gradient Low-Rank Projection (GaLore), a training
strategy that allows full-parameter learning but is more memory-efficient than
common low-rank adaptation methods such as LoRA. Our approach reduces memory
usage by up to 65.5% in optimizer states while maintaining both efficiency and
performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset
with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit
GaLore further reduces optimizer memory by up to 82.5% and total training
memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the
first time, the feasibility of pre-training a 7B model on consumer GPUs with
24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or
offloading strategies.
\\ ( https://arxiv.org/abs/2403.03507 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03508
Date: Wed, 6 Mar 2024 07:34:47 GMT   (1775kb,D)

Title: Probing the Robustness of Time-series Forecasting Models with
  CounterfacTS
Authors: H{\aa}kon Hanisch Kj{\ae}rnli, Lluis Mas-Ribas, Aida Ashrafi, Gleb
  Sizov, Helge Langseth and Odd Erik Gundersen
Categories: cs.LG
Comments: Submitted. Code publicly available
\\
  A common issue for machine learning models applied to time-series forecasting
is the temporal evolution of the data distributions (i.e., concept drift).
Because most of the training data does not reflect such changes, the models
present poor performance on the new out-of-distribution scenarios and,
therefore, the impact of such events cannot be reliably anticipated ahead of
time. We present and publicly release CounterfacTS, a tool to probe the
robustness of deep learning models in time-series forecasting tasks via
counterfactuals. CounterfacTS has a user-friendly interface that allows the
user to visualize, compare and quantify time series data and their forecasts,
for a number of datasets and deep learning models. Furthermore, the user can
apply various transformations to the time series and explore the resulting
changes in the forecasts in an interpretable manner. Through example cases, we
illustrate how CounterfacTS can be used to i) identify the main features
characterizing and differentiating sets of time series, ii) assess how the
model performance depends on these characateristics, and iii) guide
transformations of the original time series to create counterfactuals with
desired properties for training and increasing the forecasting performance in
new regions of the data distribution. We discuss the importance of visualizing
and considering the location of the data in a projected feature space to
transform time-series and create effective counterfactuals for training the
models. Overall, CounterfacTS aids at creating counterfactuals to efficiently
explore the impact of hypothetical scenarios not covered by the original data
in time-series forecasting tasks.
\\ ( https://arxiv.org/abs/2403.03508 ,  1775kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03542
Date: Wed, 6 Mar 2024 08:38:34 GMT   (889kb,D)

Title: DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE
  Pre-Training
Authors: Zhongkai Hao, Chang Su, Songming Liu, Julius Berner, Chengyang Ying,
  Hang Su, Anima Anandkumar, Jian Song, Jun Zhu
Categories: cs.LG cs.NA math.NA
\\
  Pre-training has been investigated to improve the efficiency and performance
of training neural operators in data-scarce settings. However, it is largely in
its infancy due to the inherent complexity and diversity, such as long
trajectories, multiple scales and varying dimensions of partial differential
equations (PDEs) data. In this paper, we present a new auto-regressive
denoising pre-training strategy, which allows for more stable and efficient
pre-training on PDE data and generalizes to various downstream tasks. Moreover,
by designing a flexible and scalable model architecture based on Fourier
attention, we can easily scale up the model for large-scale pre-training. We
train our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets
with more than 100k trajectories. Extensive experiments show that we achieve
SOTA on these benchmarks and validate the strong generalizability of our model
to significantly enhance performance on diverse downstream PDE tasks like 3D
data. Code is available at \url{https://github.com/thu-ml/DPOT}.
\\ ( https://arxiv.org/abs/2403.03542 ,  889kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03562
Date: Wed, 6 Mar 2024 09:14:24 GMT   (456kb,D)

Title: Efficient Algorithms for Empirical Group Distributional Robust
  Optimization and Beyond
Authors: Dingzhi Yu, Yunuo Cai, Wei Jiang, Lijun Zhang
Categories: cs.LG stat.ML
Comments: 30 pages, 1 figure
\\
  We investigate the empirical counterpart of group distributionally robust
optimization (GDRO), which aims to minimize the maximal empirical risk across
$m$ distinct groups. We formulate empirical GDRO as a $\textit{two-level}$
finite-sum convex-concave minimax optimization problem and develop a stochastic
variance reduced mirror prox algorithm. Unlike existing methods, we construct
the stochastic gradient by per-group sampling technique and perform variance
reduction for all groups, which fully exploits the $\textit{two-level}$
finite-sum structure of empirical GDRO. Furthermore, we compute the snapshot
and mirror snapshot point by a one-index-shifted weighted average, which
distinguishes us from the naive ergodic average. Our algorithm also supports
non-constant learning rates, which is different from existing literature. We
establish convergence guarantees both in expectation and with high probability,
demonstrating a complexity of
$\mathcal{O}\left(\frac{m\sqrt{\bar{n}\ln{m}}}{\varepsilon}\right)$, where
$\bar n$ is the average number of samples among $m$ groups. Remarkably, our
approach outperforms the state-of-the-art method by a factor of $\sqrt{m}$.
Furthermore, we extend our methodology to deal with the empirical minimax
excess risk optimization (MERO) problem and manage to give the expectation
bound and the high probability bound, accordingly. The complexity of our
empirical MERO algorithm matches that of empirical GDRO at
$\mathcal{O}\left(\frac{m\sqrt{\bar{n}\ln{m}}}{\varepsilon}\right)$,
significantly surpassing the bounds of existing methods.
\\ ( https://arxiv.org/abs/2403.03562 ,  456kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03569
Date: Wed, 6 Mar 2024 09:25:22 GMT   (1076kb,D)

Title: On Transfer in Classification: How Well do Subsets of Classes
  Generalize?
Authors: Raphael Baena, Lucas Drumetz, Vincent Gripon
Categories: cs.LG cs.CV
\\
  In classification, it is usual to observe that models trained on a given set
of classes can generalize to previously unseen ones, suggesting the ability to
learn beyond the initial task. This ability is often leveraged in the context
of transfer learning where a pretrained model can be used to process new
classes, with or without fine tuning. Surprisingly, there are a few papers
looking at the theoretical roots beyond this phenomenon. In this work, we are
interested in laying the foundations of such a theoretical framework for
transferability between sets of classes. Namely, we establish a partially
ordered set of subsets of classes. This tool allows to represent which subset
of classes can generalize to others. In a more practical setting, we explore
the ability of our framework to predict which subset of classes can lead to the
best performance when testing on all of them. We also explore few-shot
learning, where transfer is the golden standard. Our work contributes to better
understanding of transfer mechanics and model generalization.
\\ ( https://arxiv.org/abs/2403.03569 ,  1076kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03585
Date: Wed, 6 Mar 2024 10:01:35 GMT   (9937kb,D)

Title: RouteExplainer: An Explanation Framework for Vehicle Routing Problem
Authors: Daisuke Kikuta and Hiroki Ikeuchi and Kengo Tajiri and Yuusuke Nakano
Categories: cs.LG cs.AI math.OC
Comments: Accepted at PAKDD 2024. This extended version includes more
  comprehensive explanations and appendices
\\
  The Vehicle Routing Problem (VRP) is a widely studied combinatorial
optimization problem and has been applied to various practical problems. While
the explainability for VRP is significant for improving the reliability and
interactivity in practical VRP applications, it remains unexplored. In this
paper, we propose RouteExplainer, a post-hoc explanation framework that
explains the influence of each edge in a generated route. Our framework
realizes this by rethinking a route as the sequence of actions and extending
counterfactual explanations based on the action influence model to VRP. To
enhance the explanation, we additionally propose an edge classifier that infers
the intentions of each edge, a loss function to train the edge classifier, and
explanation-text generation by Large Language Models (LLMs). We quantitatively
evaluate our edge classifier on four different VRPs. The results demonstrate
its rapid computation while maintaining reasonable accuracy, thereby
highlighting its potential for deployment in practical applications. Moreover,
on the subject of a tourist route, we qualitatively evaluate explanations
generated by our framework. This evaluation not only validates our framework
but also shows the synergy between explanation frameworks and LLMs. See
https://ntt-dkiku.github.io/xai-vrp for our code, datasets, models, and demo.
\\ ( https://arxiv.org/abs/2403.03585 ,  9937kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03599
Date: Wed, 6 Mar 2024 10:36:56 GMT   (2451kb,D)

Title: Learning Invariant Representations of Graph Neural Networks via Cluster
  Generalization
Authors: Donglin Xia, Xiao Wang, Nian Liu, Chuan Shi
Categories: cs.LG
\\
  Graph neural networks (GNNs) have become increasingly popular in modeling
graph-structured data due to their ability to learn node representations by
aggregating local structure information. However, it is widely acknowledged
that the test graph structure may differ from the training graph structure,
resulting in a structure shift. In this paper, we experimentally find that the
performance of GNNs drops significantly when the structure shift happens,
suggesting that the learned models may be biased towards specific structure
patterns. To address this challenge, we propose the Cluster Information
Transfer (CIT) mechanism (Code available at
https://github.com/BUPT-GAMMA/CITGNN), which can learn invariant
representations for GNNs, thereby improving their generalization ability to
various and unknown test graphs with structure shift. The CIT mechanism
achieves this by combining different cluster information with the nodes while
preserving their cluster-independent information. By generating nodes across
different clusters, the mechanism significantly enhances the diversity of the
nodes and helps GNNs learn the invariant representations. We provide a
theoretical analysis of the CIT mechanism, showing that the impact of changing
clusters during structure shift can be mitigated after transfer. Additionally,
the proposed mechanism is a plug-in that can be easily used to improve existing
GNNs. We comprehensively evaluate our proposed method on three typical
structure shift scenarios, demonstrating its effectiveness in enhancing GNNs'
performance.
\\ ( https://arxiv.org/abs/2403.03599 ,  2451kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03631
Date: Wed, 6 Mar 2024 11:38:08 GMT   (548kb,D)

Title: Tackling Missing Values in Probabilistic Wind Power Forecasting: A
  Generative Approach
Authors: Honglin Wen, Pierre Pinson, Jie Gu, Zhijian Jin
Categories: cs.LG cs.SY eess.SY
Comments: 8 pages, to be presented at Power Systems Computation Conference
  (PSCC) 2024
\\
  Machine learning techniques have been successfully used in probabilistic wind
power forecasting. However, the issue of missing values within datasets due to
sensor failure, for instance, has been overlooked for a long time. Although it
is natural to consider addressing this issue by imputing missing values before
model estimation and forecasting, we suggest treating missing values and
forecasting targets indifferently and predicting all unknown values
simultaneously based on observations. In this paper, we offer an efficient
probabilistic forecasting approach by estimating the joint distribution of
features and targets based on a generative model. It is free of preprocessing,
and thus avoids introducing potential errors. Compared with the traditional
"impute, then predict" pipeline, the proposed approach achieves better
performance in terms of continuous ranked probability score.
\\ ( https://arxiv.org/abs/2403.03631 ,  548kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03643
Date: Wed, 6 Mar 2024 12:05:56 GMT   (834kb)

Title: A Survey on Applications of Reinforcement Learning in Spatial Resource
  Allocation
Authors: Di Zhang, Moyang Wang, Joseph Mango, Xiang Li
Categories: cs.LG cs.AI
\\
  The challenge of spatial resource allocation is pervasive across various
domains such as transportation, industry, and daily life. As the scale of
real-world issues continues to expand and demands for real-time solutions
increase, traditional algorithms face significant computational pressures,
struggling to achieve optimal efficiency and real-time capabilities. In recent
years, with the escalating computational power of computers, the remarkable
achievements of reinforcement learning in domains like Go and robotics have
demonstrated its robust learning and sequential decision-making capabilities.
Given these advancements, there has been a surge in novel methods employing
reinforcement learning to tackle spatial resource allocation problems. These
methods exhibit advantages such as rapid solution convergence and strong model
generalization abilities, offering a new perspective on resolving spatial
resource allocation problems. Therefore, this paper aims to summarize and
review recent theoretical methods and applied research utilizing reinforcement
learning to address spatial resource allocation problems. It provides a summary
and comprehensive overview of its fundamental principles, related
methodologies, and applied research. Additionally, it highlights several
unresolved issues that urgently require attention in this direction for the
future.
\\ ( https://arxiv.org/abs/2403.03643 ,  834kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03659
Date: Wed, 6 Mar 2024 12:29:13 GMT   (1514kb,D)

Title: Robust Graph Structure Learning under Heterophily
Authors: Xuanting Xie, Zhao Kang, Wenyu Chen
Categories: cs.LG
Comments: 26 pages, 5 figures
\\
  Graph is a fundamental mathematical structure in characterizing relations
between different objects and has been widely used on various learning tasks.
Most methods implicitly assume a given graph to be accurate and complete.
However, real data is inevitably noisy and sparse, which will lead to inferior
results. Despite the remarkable success of recent graph representation learning
methods, they inherently presume that the graph is homophilic, and largely
overlook heterophily, where most connected nodes are from different classes. In
this regard, we propose a novel robust graph structure learning method to
achieve a high-quality graph from heterophilic data for downstream tasks. We
first apply a high-pass filter to make each node more distinctive from its
neighbors by encoding structure information into the node features. Then, we
learn a robust graph with an adaptive norm characterizing different levels of
noise. Afterwards, we propose a novel regularizer to further refine the graph
structure. Clustering and semi-supervised classification experiments on
heterophilic graphs verify the effectiveness of our method.
\\ ( https://arxiv.org/abs/2403.03659 ,  1514kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03666
Date: Wed, 6 Mar 2024 12:37:49 GMT   (1460kb,D)

Title: Provable Filter for Real-world Graph Clustering
Authors: Xuanting Xie, Erlin Pan, Zhao Kang, Wenyu Chen and Bingheng Li
Categories: cs.LG
Comments: 12 pages, 5 figures
\\
  Graph clustering, an important unsupervised problem, has been shown to be
more resistant to advances in Graph Neural Networks (GNNs). In addition, almost
all clustering methods focus on homophilic graphs and ignore heterophily. This
significantly limits their applicability in practice, since real-world graphs
exhibit a structural disparity and cannot simply be classified as homophily and
heterophily. Thus, a principled way to handle practical graphs is urgently
needed. To fill this gap, we provide a novel solution with theoretical support.
Interestingly, we find that most homophilic and heterophilic edges can be
correctly identified on the basis of neighbor information. Motivated by this
finding, we construct two graphs that are highly homophilic and heterophilic,
respectively. They are used to build low-pass and high-pass filters to capture
holistic information. Important features are further enhanced by the
squeeze-and-excitation block. We validate our approach through extensive
experiments on both homophilic and heterophilic graphs. Empirical results
demonstrate the superiority of our method compared to state-of-the-art
clustering methods.
\\ ( https://arxiv.org/abs/2403.03666 ,  1460kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03670
Date: Wed, 6 Mar 2024 12:47:14 GMT   (1097kb,D)

Title: CDC: A Simple Framework for Complex Data Clustering
Authors: Zhao Kang, Xuanting Xie, Bingheng Li and Erlin Pan
Categories: cs.LG
Comments: 10 pages, 5 figures
\\
  In today's data-driven digital era, the amount as well as complexity, such as
multi-view, non-Euclidean, and multi-relational, of the collected data are
growing exponentially or even faster. Clustering, which unsupervisely extracts
valid knowledge from data, is extremely useful in practice. However, existing
methods are independently developed to handle one particular challenge at the
expense of the others. In this work, we propose a simple but effective
framework for complex data clustering (CDC) that can efficiently process
different types of data with linear complexity. We first utilize graph
filtering to fuse geometry structure and attribute information. We then reduce
the complexity with high-quality anchors that are adaptively learned via a
novel similarity-preserving regularizer. We illustrate the cluster-ability of
our proposed method theoretically and experimentally. In particular, we deploy
CDC to graph data of size 111M.
\\ ( https://arxiv.org/abs/2403.03670 ,  1097kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03672
Date: Wed, 6 Mar 2024 12:49:08 GMT   (55kb)

Title: Learning Adversarial MDPs with Stochastic Hard Constraints
Authors: Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi,
  Nicola Gatti
Categories: cs.LG
\\
  We study online learning problems in constrained Markov decision processes
(CMDPs) with adversarial losses and stochastic hard constraints. We consider
two different scenarios. In the first one, we address general CMDPs, where we
design an algorithm that attains sublinear regret and cumulative positive
constraints violation. In the second scenario, under the mild assumption that a
policy strictly satisfying the constraints exists and is known to the learner,
we design an algorithm that achieves sublinear regret while ensuring that the
constraints are satisfied at every episode with high probability. To the best
of our knowledge, our work is the first to study CMDPs involving both
adversarial losses and hard constraints. Indeed, previous works either focus on
much weaker soft constraints--allowing for positive violation to cancel out
negative ones--or are restricted to stochastic losses. Thus, our algorithms can
deal with general non-stationary environments subject to requirements much
stricter than those manageable with state-of-the-art algorithms. This enables
their adoption in a much wider range of real-world applications, ranging from
autonomous driving to online advertising and recommender systems.
\\ ( https://arxiv.org/abs/2403.03672 ,  55kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03676
Date: Wed, 6 Mar 2024 12:57:48 GMT   (1017kb,D)

Title: Simplified PCNet with Robustness
Authors: Bingheng Li, Xuanting Xie, Haoxiang Lei, Ruiyi Fang, and Zhao Kang
Categories: cs.LG
Comments: 10 pages, 3 figures
\\
  Graph Neural Networks (GNNs) have garnered significant attention for their
success in learning the representation of homophilic or heterophilic graphs.
However, they cannot generalize well to real-world graphs with different levels
of homophily. In response, the Possion-Charlier Network (PCNet)
\cite{li2024pc}, the previous work, allows graph representation to be learned
from heterophily to homophily. Although PCNet alleviates the heterophily issue,
there remain some challenges in further improving the efficacy and efficiency.
In this paper, we simplify PCNet and enhance its robustness. We first extend
the filter order to continuous values and reduce its parameters. Two variants
with adaptive neighborhood sizes are implemented. Theoretical analysis shows
our model's robustness to graph structure perturbations or adversarial attacks.
We validate our approach through semi-supervised learning tasks on various
datasets representing both homophilic and heterophilic graphs.
\\ ( https://arxiv.org/abs/2403.03676 ,  1017kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03698
Date: Wed, 6 Mar 2024 13:27:34 GMT   (4091kb,D)

Title: Towards Controllable Time Series Generation
Authors: Yifan Bao, Yihao Ang, Qiang Huang, Anthony K. H. Tung, Zhiyong Huang
Categories: cs.LG cs.AI cs.DB
Comments: 14 pages, 13 figures, and 5 tables
\\
  Time Series Generation (TSG) has emerged as a pivotal technique in
synthesizing data that accurately mirrors real-world time series, becoming
indispensable in numerous applications. Despite significant advancements in
TSG, its efficacy frequently hinges on having large training datasets. This
dependency presents a substantial challenge in data-scarce scenarios,
especially when dealing with rare or unique conditions. To confront these
challenges, we explore a new problem of Controllable Time Series Generation
(CTSG), aiming to produce synthetic time series that can adapt to various
external conditions, thereby tackling the data scarcity issue.
  In this paper, we propose \textbf{C}ontrollable \textbf{T}ime \textbf{S}eries
(\textsf{CTS}), an innovative VAE-agnostic framework tailored for CTSG. A key
feature of \textsf{CTS} is that it decouples the mapping process from standard
VAE training, enabling precise learning of a complex interplay between latent
features and external conditions. Moreover, we develop a comprehensive
evaluation scheme for CTSG. Extensive experiments across three real-world time
series datasets showcase \textsf{CTS}'s exceptional capabilities in generating
high-quality, controllable outputs. This underscores its adeptness in
seamlessly integrating latent features with external conditions. Extending
\textsf{CTS} to the image domain highlights its remarkable potential for
explainability and further reinforces its versatility across different
modalities.
\\ ( https://arxiv.org/abs/2403.03698 ,  4091kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03726
Date: Wed, 6 Mar 2024 14:15:20 GMT   (11332kb,D)

Title: Diffusion on language model embeddings for protein sequence generation
Authors: Viacheslav Meshchaninov, Pavel Strashnov, Andrey Shevtsov, Fedor
  Nikolaev, Nikita Ivanisenko, Olga Kardymon, Dmitry Vetrov
Categories: cs.LG cs.AI q-bio.BM
\\
  Protein design requires a deep understanding of the inherent complexities of
the protein universe. While many efforts lean towards conditional generation or
focus on specific families of proteins, the foundational task of unconditional
generation remains underexplored and undervalued. Here, we explore this pivotal
domain, introducing DiMA, a model that leverages continuous diffusion on
embeddings derived from the protein language model, ESM-2, to generate amino
acid sequences. DiMA surpasses leading solutions, including autoregressive
transformer-based and discrete diffusion models, and we quantitatively
illustrate the impact of the design choices that lead to its superior
performance. We extensively evaluate the quality, diversity, distribution
similarity, and biological relevance of the generated sequences using multiple
metrics across various modalities. Our approach consistently produces novel,
diverse protein sequences that accurately reflect the inherent structural and
functional diversity of the protein space. This work advances the field of
protein design and sets the stage for conditional models by providing a robust
framework for scalable and high-quality protein sequence generation.
\\ ( https://arxiv.org/abs/2403.03726 ,  11332kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03728
Date: Wed, 6 Mar 2024 14:18:24 GMT   (110kb,D)

Title: Bridging Diversity and Uncertainty in Active learning with
  Self-Supervised Pre-Training
Authors: Paul Doucet, Benjamin Estermann, Till Aczel, Roger Wattenhofer
Categories: cs.LG cs.AI cs.CV
Comments: Accepted at ICLR 2024 Workshop on Practical Machine Learning for Low
  Resource Settings (PML4LRS)
\\
  This study addresses the integration of diversity-based and uncertainty-based
sampling strategies in active learning, particularly within the context of
self-supervised pre-trained models. We introduce a straightforward heuristic
called TCM that mitigates the cold start problem while maintaining strong
performance across various data levels. By initially applying TypiClust for
diversity sampling and subsequently transitioning to uncertainty sampling with
Margin, our approach effectively combines the strengths of both strategies. Our
experiments demonstrate that TCM consistently outperforms existing methods
across various datasets in both low and high data regimes.
\\ ( https://arxiv.org/abs/2403.03728 ,  110kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03737
Date: Wed, 6 Mar 2024 14:27:29 GMT   (8935kb,D)

Title: Probabilistic Topic Modelling with Transformer Representations
Authors: Arik Reuter, Anton Thielmann, Christoph Weisser, Benjamin S\"afken,
  Thomas Kneib
Categories: cs.LG cs.CL
\\
  Topic modelling was mostly dominated by Bayesian graphical models during the
last decade. With the rise of transformers in Natural Language Processing,
however, sev- eral successful models that rely on straightforward cluster- ing
approaches in transformer-based embedding spaces have emerged and consolidated
the notion of topics as clusters of embedding vectors. We propose the
Transformer-Representation Neural Topic Model (TNTM), which combines the
benefits of topic representations in transformer-based embedding spaces and
probabilistic modelling. Therefore, this approach unifies the powerful and
versatile notion of topics based on transformer embeddings with fully
probabilistic modelling, as in models such as Latent Dirichlet Allocation
(LDA). We utilize the variational autoencoder (VAE) framework for improved
inference speed and modelling flexibility. Experimental results show that our
proposed model achieves results on par with various state- of-the-art
approaches in terms of embedding coherence while maintaining almost perfect
topic diversity. The corresponding source code is available at
https://github.com/ArikReuter/TNTM.
\\ ( https://arxiv.org/abs/2403.03737 ,  8935kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03739
Date: Wed, 6 Mar 2024 14:28:49 GMT   (2008kb,D)

Title: A&B BNN: Add&Bit-Operation-Only Hardware-Friendly Binary Neural Network
Authors: Ruichen Ma, Guanchao Qiao, Yian Liu, Liwei Meng, Ning Ning, Yang Liu,
  Shaogang Hu
Categories: cs.LG cs.AI
Comments: CVPR 2024 Accepted
\\
  Binary neural networks utilize 1-bit quantized weights and activations to
reduce both the model's storage demands and computational burden. However,
advanced binary architectures still incorporate millions of inefficient and
nonhardware-friendly full-precision multiplication operations. A&B BNN is
proposed to directly remove part of the multiplication operations in a
traditional BNN and replace the rest with an equal number of bit operations,
introducing the mask layer and the quantized RPReLU structure based on the
normalizer-free network architecture. The mask layer can be removed during
inference by leveraging the intrinsic characteristics of BNN with
straightforward mathematical transformations to avoid the associated
multiplication operations. The quantized RPReLU structure enables more
efficient bit operations by constraining its slope to be integer powers of 2.
Experimental results achieved 92.30%, 69.35%, and 66.89% on the CIFAR-10,
CIFAR-100, and ImageNet datasets, respectively, which are competitive with the
state-of-the-art. Ablation studies have verified the efficacy of the quantized
RPReLU structure, leading to a 1.14% enhancement on the ImageNet compared to
using a fixed slope RLeakyReLU. The proposed add&bit-operation-only BNN offers
an innovative approach for hardware-friendly network architecture.
\\ ( https://arxiv.org/abs/2403.03739 ,  2008kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03741
Date: Wed, 6 Mar 2024 14:30:09 GMT   (2460kb,D)

Title: SUPClust: Active Learning at the Boundaries
Authors: Yuta Ono, Till Aczel, Benjamin Estermann, Roger Wattenhofer
Categories: cs.LG cs.AI cs.CV
Comments: Accepted at ICLR 2024 Workshop on Practical Machine Learning for Low
  Resource Settings (PML4LRS)
\\
  Active learning is a machine learning paradigm designed to optimize model
performance in a setting where labeled data is expensive to acquire. In this
work, we propose a novel active learning method called SUPClust that seeks to
identify points at the decision boundary between classes. By targeting these
points, SUPClust aims to gather information that is most informative for
refining the model's prediction of complex decision regions. We demonstrate
experimentally that labeling these points leads to strong model performance.
This improvement is observed even in scenarios characterized by strong class
imbalance.
\\ ( https://arxiv.org/abs/2403.03741 ,  2460kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03772
Date: Wed, 6 Mar 2024 15:06:11 GMT   (229kb,D)

Title: AcceleratedLiNGAM: Learning Causal DAGs at the speed of GPUs
Authors: Victor Akinwande, J. Zico Kolter
Categories: cs.LG cs.DC stat.ML
Comments: Accepted at MLGenX @ ICLR 2024. Open source at
  https://github.com/Viktour19/culingam
\\
  Existing causal discovery methods based on combinatorial optimization or
search are slow, prohibiting their application on large-scale datasets. In
response, more recent methods attempt to address this limitation by formulating
causal discovery as structure learning with continuous optimization but such
approaches thus far provide no statistical guarantees. In this paper, we show
that by efficiently parallelizing existing causal discovery methods, we can in
fact scale them to thousands of dimensions, making them practical for
substantially larger-scale problems. In particular, we parallelize the LiNGAM
method, which is quadratic in the number of variables, obtaining up to a
32-fold speed-up on benchmark datasets when compared with existing sequential
implementations. Specifically, we focus on the causal ordering sub- procedure
in DirectLiNGAM and implement GPU kernels to accelerate it. This allows us to
apply DirectLiNGAM to causal inference on large-scale gene expression data with
genetic interventions yielding competitive results compared with specialized
continuous optimization methods, and Var-LiNGAM for causal discovery on U.S.
stock data.
\\ ( https://arxiv.org/abs/2403.03772 ,  229kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03773
Date: Wed, 6 Mar 2024 15:06:16 GMT   (2704kb,D)

Title: Verified Training for Counterfactual Explanation Robustness under Data
  Shift
Authors: Anna P. Meyer and Yuhao Zhang and Aws Albarghouthi and Loris D'Antoni
Categories: cs.LG
Comments: 16 pages, 2 figures. Accepted at DMLR workshop at ICLR 2024
\\
  Counterfactual explanations (CEs) enhance the interpretability of machine
learning models by describing what changes to an input are necessary to change
its prediction to a desired class. These explanations are commonly used to
guide users' actions, e.g., by describing how a user whose loan application was
denied can be approved for a loan in the future. Existing approaches generate
CEs by focusing on a single, fixed model, and do not provide any formal
guarantees on the CEs' future validity. When models are updated periodically to
account for data shift, if the generated CEs are not robust to the shifts,
users' actions may no longer have the desired impacts on their predictions.
This paper introduces VeriTraCER, an approach that jointly trains a classifier
and an explainer to explicitly consider the robustness of the generated CEs to
small model shifts. VeriTraCER optimizes over a carefully designed loss
function that ensures the verifiable robustness of CEs to local model updates,
thus providing deterministic guarantees to CE validity. Our empirical
evaluation demonstrates that VeriTraCER generates CEs that (1) are verifiably
robust to small model updates and (2) display competitive robustness to
state-of-the-art approaches in handling empirical model updates including
random initialization, leave-one-out, and distribution shifts.
\\ ( https://arxiv.org/abs/2403.03773 ,  2704kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03777
Date: Wed, 6 Mar 2024 15:15:42 GMT   (6077kb,D)

Title: ENOT: Expectile Regularization for Fast and Accurate Training of Neural
  Optimal Transport
Authors: Nazar Buzun, Maksim Bobrin, Dmitry V. Dylov
Categories: cs.LG cs.AI
\\
  We present a new extension for Neural Optimal Transport (NOT) training
procedure, capable of accurately and efficiently estimating optimal
transportation plan via specific regularisation on conjugate potentials. The
main bottleneck of existing NOT solvers is associated with the procedure of
finding a near-exact approximation of the conjugate operator (i.e., the
c-transform), which is done either by optimizing over maximin objectives or by
the computationally-intensive fine-tuning of the initial approximated
prediction. We resolve both issues by proposing a new, theoretically justified
loss in the form of expectile regularization that enforces binding conditions
on the learning dual potentials. Such a regularization provides the upper bound
estimation over the distribution of possible conjugate potentials and makes the
learning stable, eliminating the need for additional extensive finetuning. We
formally justify the efficiency of our method, called Expectile-Regularised
Neural Optimal Transport (ENOT). ENOT outperforms previous state-of-the-art
approaches on the Wasserstein-2 benchmark tasks by a large margin (up to a
3-fold improvement in quality and up to a 10-fold improvement in runtime).
\\ ( https://arxiv.org/abs/2403.03777 ,  6077kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03791
Date: Wed, 6 Mar 2024 15:37:22 GMT   (1306kb,D)

Title: KG-TREAT: Pre-training for Treatment Effect Estimation by Synergizing
  Patient Data with Knowledge Graphs
Authors: Ruoqi Liu, Lingfei Wu, Ping Zhang
Categories: cs.LG cs.AI
Comments: AAAI 2024 Main Track
\\
  Treatment effect estimation (TEE) is the task of determining the impact of
various treatments on patient outcomes. Current TEE methods fall short due to
reliance on limited labeled data and challenges posed by sparse and
high-dimensional observational patient data. To address the challenges, we
introduce a novel pre-training and fine-tuning framework, KG-TREAT, which
synergizes large-scale observational patient data with biomedical knowledge
graphs (KGs) to enhance TEE. Unlike previous approaches, KG-TREAT constructs
dual-focus KGs and integrates a deep bi-level attention synergy method for
in-depth information fusion, enabling distinct encoding of treatment-covariate
and outcome-covariate relationships. KG-TREAT also incorporates two
pre-training tasks to ensure a thorough grounding and contextualization of
patient data and KGs. Evaluation on four downstream TEE tasks shows KG-TREAT's
superiority over existing methods, with an average improvement of 7% in Area
under the ROC Curve (AUC) and 9% in Influence Function-based Precision of
Estimating Heterogeneous Effects (IF-PEHE). The effectiveness of our estimated
treatment effects is further affirmed by alignment with established randomized
clinical trial findings.
\\ ( https://arxiv.org/abs/2403.03791 ,  1306kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03812
Date: Wed, 6 Mar 2024 16:00:50 GMT   (1233kb,D)

Title: ProbSAINT: Probabilistic Tabular Regression for Used Car Pricing
Authors: Kiran Madhusudhanan, Gunnar Behrens, Maximilian Stubbemann, Lars
  Schmidt-Thieme
Categories: cs.LG cs.AI
Comments: 9 pages, 4 figures
\\
  Used car pricing is a critical aspect of the automotive industry, influenced
by many economic factors and market dynamics. With the recent surge in online
marketplaces and increased demand for used cars, accurate pricing would benefit
both buyers and sellers by ensuring fair transactions. However, the transition
towards automated pricing algorithms using machine learning necessitates the
comprehension of model uncertainties, specifically the ability to flag
predictions that the model is unsure about. Although recent literature proposes
the use of boosting algorithms or nearest neighbor-based approaches for swift
and precise price predictions, encapsulating model uncertainties with such
algorithms presents a complex challenge. We introduce ProbSAINT, a model that
offers a principled approach for uncertainty quantification of its price
predictions, along with accurate point predictions that are comparable to
state-of-the-art boosting techniques. Furthermore, acknowledging that the
business prefers pricing used cars based on the number of days the vehicle was
listed for sale, we show how ProbSAINT can be used as a dynamic forecasting
model for predicting price probabilities for different expected offer duration.
Our experiments further indicate that ProbSAINT is especially accurate on
instances where it is highly certain. This proves the applicability of its
probabilistic predictions in real-world scenarios where trustworthiness is
crucial.
\\ ( https://arxiv.org/abs/2403.03812 ,  1233kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03835
Date: Wed, 6 Mar 2024 16:26:40 GMT   (3063kb,D)

Title: Cobweb: An Incremental and Hierarchical Model of Human-Like Category
  Learning
Authors: Xin Lian, Sashank Varma, Christopher J. MacLellan
Categories: cs.LG cs.AI cs.IR
\\
  Cobweb, a human like category learning system, differs from other incremental
categorization models in constructing hierarchically organized cognitive
tree-like structures using the category utility measure. Prior studies have
shown that Cobweb can capture psychological effects such as the basic level,
typicality, and fan effects. However, a broader evaluation of Cobweb as a model
of human categorization remains lacking. The current study addresses this gap.
It establishes Cobweb's alignment with classical human category learning
effects. It also explores Cobweb's flexibility to exhibit both exemplar and
prototype like learning within a single model. These findings set the stage for
future research on Cobweb as a comprehensive model of human category learning.
\\ ( https://arxiv.org/abs/2403.03835 ,  3063kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03838
Date: Wed, 6 Mar 2024 16:31:56 GMT   (5005kb,D)

Title: Feature Selection as Deep Sequential Generative Learning
Authors: Wangyang Ying, Dongjie Wang, Haifeng Chen, Yanjie Fu
Categories: cs.LG
\\
  Feature selection aims to identify the most pattern-discriminative feature
subset. In prior literature, filter (e.g., backward elimination) and embedded
(e.g., Lasso) methods have hyperparameters (e.g., top-K, score thresholding)
and tie to specific models, thus, hard to generalize; wrapper methods search a
feature subset in a huge discrete space and is computationally costly. To
transform the way of feature selection, we regard a selected feature subset as
a selection decision token sequence and reformulate feature selection as a deep
sequential generative learning task that distills feature knowledge and
generates decision sequences. Our method includes three steps: (1) We develop a
deep variational transformer model over a joint of sequential reconstruction,
variational, and performance evaluator losses. Our model can distill feature
selection knowledge and learn a continuous embedding space to map feature
selection decision sequences into embedding vectors associated with utility
scores. (2) We leverage the trained feature subset utility evaluator as a
gradient provider to guide the identification of the optimal feature subset
embedding;(3) We decode the optimal feature subset embedding to
autoregressively generate the best feature selection decision sequence with
autostop. Extensive experimental results show this generative perspective is
effective and generic, without large discrete search space and expert-specific
hyperparameters.
\\ ( https://arxiv.org/abs/2403.03838 ,  5005kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03846
Date: Wed, 6 Mar 2024 16:42:10 GMT   (8054kb,D)

Title: On the Effectiveness of Distillation in Mitigating Backdoors in
  Pre-trained Encoder
Authors: Tingxu Han, Shenghan Huang, Ziqi Ding, Weisong Sun, Yebo Feng,
  Chunrong Fang, Jun Li, Hanwei Qian, Cong Wu, Quanjun Zhang, Yang Liu and
  Zhenyu Chen
Categories: cs.LG
\\
  In this paper, we study a defense against poisoned encoders in SSL called
distillation, which is a defense used in supervised learning originally.
Distillation aims to distill knowledge from a given model (a.k.a the teacher
net) and transfer it to another (a.k.a the student net). Now, we use it to
distill benign knowledge from poisoned pre-trained encoders and transfer it to
a new encoder, resulting in a clean pre-trained encoder. In particular, we
conduct an empirical study on the effectiveness and performance of distillation
against poisoned encoders. Using two state-of-the-art backdoor attacks against
pre-trained image encoders and four commonly used image classification
datasets, our experimental results show that distillation can reduce attack
success rate from 80.87% to 27.51% while suffering a 6.35% loss in accuracy.
Moreover, we investigate the impact of three core components of distillation on
performance: teacher net, student net, and distillation loss. By comparing 4
different teacher nets, 3 student nets, and 6 distillation losses, we find that
fine-tuned teacher nets, warm-up-training-based student nets, and
attention-based distillation loss perform best, respectively.
\\ ( https://arxiv.org/abs/2403.03846 ,  8054kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03852
Date: Wed, 6 Mar 2024 17:02:39 GMT   (4391kb,D)

Title: Accelerating Convergence of Score-Based Diffusion Models, Provably
Authors: Gen Li, Yu Huang, Timofey Efimov, Yuting Wei, Yuejie Chi, Yuxin Chen
Categories: cs.LG cs.AI cs.IT math.IT math.OC stat.ML
Comments: The first two authors contributed equally
\\
  Score-based diffusion models, while achieving remarkable empirical
performance, often suffer from low sampling speed, due to extensive function
evaluations needed during the sampling phase. Despite a flurry of recent
activities towards speeding up diffusion generative modeling in practice,
theoretical underpinnings for acceleration techniques remain severely limited.
In this paper, we design novel training-free algorithms to accelerate popular
deterministic (i.e., DDIM) and stochastic (i.e., DDPM) samplers. Our
accelerated deterministic sampler converges at a rate $O(1/{T}^2)$ with $T$ the
number of steps, improving upon the $O(1/T)$ rate for the DDIM sampler; and our
accelerated stochastic sampler converges at a rate $O(1/T)$, outperforming the
rate $O(1/\sqrt{T})$ for the DDPM sampler. The design of our algorithms
leverages insights from higher-order approximation, and shares similar
intuitions as popular high-order ODE solvers like the DPM-Solver-2. Our theory
accommodates $\ell_2$-accurate score estimates, and does not require
log-concavity or smoothness on the target distribution.
\\ ( https://arxiv.org/abs/2403.03852 ,  4391kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03856
Date: Wed, 6 Mar 2024 17:06:11 GMT   (48kb)

Title: Public-data Assisted Private Stochastic Optimization: Power and
  Limitations
Authors: Enayat Ullah, Michael Menart, Raef Bassily, Crist\'obal Guzm\'an,
  Raman Arora
Categories: cs.LG cs.CR math.OC stat.ML
\\
  We study the limits and capability of public-data assisted differentially
private (PA-DP) algorithms. Specifically, we focus on the problem of stochastic
convex optimization (SCO) with either labeled or unlabeled public data. For
complete/labeled public data, we show that any $(\epsilon,\delta)$-PA-DP has
excess risk
$\tilde{\Omega}\big(\min\big\{\frac{1}{\sqrt{n_{\text{pub}}}},\frac{1}{\sqrt{n}}+\frac{\sqrt{d}}{n\epsilon}
\big\} \big)$, where $d$ is the dimension, ${n_{\text{pub}}}$ is the number of
public samples, ${n_{\text{priv}}}$ is the number of private samples, and
$n={n_{\text{pub}}}+{n_{\text{priv}}}$. These lower bounds are established via
our new lower bounds for PA-DP mean estimation, which are of a similar form. Up
to constant factors, these lower bounds show that the simple strategy of either
treating all data as private or discarding the private data, is optimal. We
also study PA-DP supervised learning with \textit{unlabeled} public samples. In
contrast to our previous result, we here show novel methods for leveraging
public data in private supervised learning. For generalized linear models (GLM)
with unlabeled public data, we show an efficient algorithm which, given
$\tilde{O}({n_{\text{priv}}}\epsilon)$ unlabeled public samples, achieves the
dimension independent rate $\tilde{O}\big(\frac{1}{\sqrt{{n_{\text{priv}}}}} +
\frac{1}{\sqrt{{n_{\text{priv}}}\epsilon}}\big)$. We develop new lower bounds
for this setting which shows that this rate cannot be improved with more public
samples, and any fewer public samples leads to a worse rate. Finally, we
provide extensions of this result to general hypothesis classes with finite
fat-shattering dimension with applications to neural networks and non-Euclidean
geometries.
\\ ( https://arxiv.org/abs/2403.03856 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03871
Date: Wed, 6 Mar 2024 17:23:28 GMT   (454kb,D)

Title: Decoupled Vertical Federated Learning for Practical Training on
  Vertically Partitioned Data
Authors: Avi Amalanshu, Yash Sirvi, David I. Inouye
Categories: cs.LG cs.DC
Comments: 10 pages, 4 figures (excluding appendix)
\\
  Vertical Federated Learning (VFL) is an emergent distributed machine learning
paradigm wherein owners of disjoint features of a common set of entities
collaborate to learn a global model without sharing data. In VFL, a host client
owns data labels for each entity and learns a final representation based on
intermediate local representations from all guest clients. Therefore, the host
is a single point of failure and label feedback can be used by malicious guest
clients to infer private features. Requiring all participants to remain active
and trustworthy throughout the entire training process is generally impractical
and altogether infeasible outside of controlled environments. We propose
Decoupled VFL (DVFL), a blockwise learning approach to VFL. By training each
model on its own objective, DVFL allows for decentralized aggregation and
isolation between feature learning and label supervision. With these
properties, DVFL is fault tolerant and secure. We implement DVFL to train split
neural networks and show that model performance is comparable to VFL on a
variety of classification datasets.
\\ ( https://arxiv.org/abs/2403.03871 ,  454kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03880
Date: Wed, 6 Mar 2024 17:40:26 GMT   (1396kb,D)

Title: Graph neural network outputs are almost surely asymptotically constant
Authors: Sam Adam-Day, Michael Benedikt, \.Ismail \.Ilkan Ceylan, Ben
  Finkelshtein
Categories: cs.LG cs.LO
Comments: 10 body pages, 23 appendix pages, 2 figures
\\
  Graph neural networks (GNNs) are the predominant architectures for a variety
of learning tasks on graphs. We present a new angle on the expressive power of
GNNs by studying how the predictions of a GNN probabilistic classifier evolve
as we apply it on larger graphs drawn from some random graph model. We show
that the output converges to a constant function, which upper-bounds what these
classifiers can express uniformly. This convergence phenomenon applies to a
very wide class of GNNs, including state of the art models, with aggregates
including mean and the attention-based mechanism of graph transformers. Our
results apply to a broad class of random graph models, including the (sparse)
Erd\H{o}s-R\'enyi model and the stochastic block model. We empirically validate
these findings, observing that the convergence phenomenon already manifests
itself on graphs of relatively modest size.
\\ ( https://arxiv.org/abs/2403.03880 ,  1396kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03929
Date: Wed, 6 Mar 2024 18:39:41 GMT   (16795kb,D)

Title: Extreme Precipitation Nowcasting using Transformer-based Generative
  Models
Authors: Cristian Meo, Ankush Roy, Mircea Lic\u{a}, Junzhe Yin, Zeineb Bou Che,
  Yanbo Wang, Ruben Imhoff, Remko Uijlenhoet, Justin Dauwels
Categories: cs.LG cs.AI
\\
  This paper presents an innovative approach to extreme precipitation
nowcasting by employing Transformer-based generative models, namely
NowcastingGPT with Extreme Value Loss (EVL) regularization. Leveraging a
comprehensive dataset from the Royal Netherlands Meteorological Institute
(KNMI), our study focuses on predicting short-term precipitation with high
accuracy. We introduce a novel method for computing EVL without assuming fixed
extreme representations, addressing the limitations of current models in
capturing extreme weather events. We present both qualitative and quantitative
analyses, demonstrating the superior performance of the proposed
NowcastingGPT-EVL in generating accurate precipitation forecasts, especially
when dealing with extreme precipitation events. The code is available at
\url{https://github.com/Cmeo97/NowcastingGPT}.
\\ ( https://arxiv.org/abs/2403.03929 ,  16795kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03938
Date: Wed, 6 Mar 2024 18:47:32 GMT   (7225kb,D)

Title: GUIDE: Guidance-based Incremental Learning with Diffusion Models
Authors: Bartosz Cywi\'nski, Kamil Deja, Tomasz Trzci\'nski, Bart{\l}omiej
  Twardowski, {\L}ukasz Kuci\'nski
Categories: cs.LG
\\
  We introduce GUIDE, a novel continual learning approach that directs
diffusion models to rehearse samples at risk of being forgotten. Existing
generative strategies combat catastrophic forgetting by randomly sampling
rehearsal examples from a generative model. Such an approach contradicts
buffer-based approaches where sampling strategy plays an important role. We
propose to bridge this gap by integrating diffusion models with classifier
guidance techniques to produce rehearsal examples specifically targeting
information forgotten by a continuously trained model. This approach enables
the generation of samples from preceding task distributions, which are more
likely to be misclassified in the context of recently encountered classes. Our
experimental results show that GUIDE significantly reduces catastrophic
forgetting, outperforming conventional random sampling approaches and
surpassing recent state-of-the-art methods in continual learning with
generative replay.
\\ ( https://arxiv.org/abs/2403.03938 ,  7225kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03945
Date: Wed, 6 Mar 2024 18:52:39 GMT   (2146kb,D)

Title: SPEAR:Exact Gradient Inversion of Batches in Federated Learning
Authors: Dimitar I. Dimitrov, Maximilian Baader, Mark Niklas M\"uller, Martin
  Vechev
Categories: cs.LG cs.CR cs.DC
ACM-class: I.2.11
\\
  Federated learning is a popular framework for collaborative machine learning
where multiple clients only share gradient updates on their local data with the
server and not the actual data. Unfortunately, it was recently shown that
gradient inversion attacks can reconstruct this data from these shared
gradients. Existing attacks enable exact reconstruction only for a batch size
of $b=1$ in the important honest-but-curious setting, with larger batches
permitting only approximate reconstruction. In this work, we propose \emph{the
first algorithm reconstructing whole batches with $b >1$ exactly}. This
approach combines mathematical insights into the explicit low-rank structure of
gradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced
gradient sparsity to precisely filter out large numbers of incorrect samples,
making a final reconstruction step tractable. We provide an efficient GPU
implementation for fully connected networks and show that it recovers batches
of $b \lesssim 25$ elements exactly while being tractable for large network
widths and depths.
\\ ( https://arxiv.org/abs/2403.03945 ,  2146kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03950
Date: Wed, 6 Mar 2024 18:55:47 GMT   (2577kb,D)

Title: Stop Regressing: Training Value Functions via Classification for
  Scalable Deep RL
Authors: Jesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali Ta\"iga, Yevgen
  Chebotar, Ted Xiao, Alex Irpan, Sergey Levine, Pablo Samuel Castro,
  Aleksandra Faust, Aviral Kumar, Rishabh Agarwal
Categories: cs.LG cs.AI stat.ML
\\
  Value functions are a central component of deep reinforcement learning (RL).
These functions, parameterized by neural networks, are trained using a mean
squared error regression objective to match bootstrapped target values.
However, scaling value-based RL methods that use regression to large networks,
such as high-capacity Transformers, has proven challenging. This difficulty is
in stark contrast to supervised learning: by leveraging a cross-entropy
classification loss, supervised methods have scaled reliably to massive
networks. Observing this discrepancy, in this paper, we investigate whether the
scalability of deep RL can also be improved simply by using classification in
place of regression for training value functions. We demonstrate that value
functions trained with categorical cross-entropy significantly improves
performance and scalability in a variety of domains. These include: single-task
RL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale
ResNets, robotic manipulation with Q-transformers, playing Chess without
search, and a language-agent Wordle task with high-capacity Transformers,
achieving state-of-the-art results on these domains. Through careful analysis,
we show that the benefits of categorical cross-entropy primarily stem from its
ability to mitigate issues inherent to value-based RL, such as noisy targets
and non-stationarity. Overall, we argue that a simple shift to training value
functions with categorical cross-entropy can yield substantial improvements in
the scalability of deep RL at little-to-no cost.
\\ ( https://arxiv.org/abs/2403.03950 ,  2577kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2402.16627 (*cross-listing*)
Date: Mon, 26 Feb 2024 15:01:16 GMT   (35014kb,D)
Date (revised v2): Mon, 4 Mar 2024 14:17:04 GMT   (35014kb,D)

Title: Cross-Modal Contextualized Diffusion Models for Text-Guided Visual
  Generation and Editing
Authors: Ling Yang, Zhilong Zhang, Zhaochen Yu, Jingwei Liu, Minkai Xu, Stefano
  Ermon, Bin Cui
Categories: cs.CV cs.AI cs.LG
Comments: ICLR 2024. Project: https://github.com/YangLing0818/ContextDiff
\\
  Conditional diffusion models have exhibited superior performance in
high-fidelity text-guided visual generation and editing. Nevertheless,
prevailing text-guided visual diffusion models primarily focus on incorporating
text-visual relationships exclusively into the reverse process, often
disregarding their relevance in the forward process. This inconsistency between
forward and reverse processes may limit the precise conveyance of textual
semantics in visual synthesis results. To address this issue, we propose a
novel and general contextualized diffusion model (ContextDiff) by incorporating
the cross-modal context encompassing interactions and alignments between text
condition and visual sample into forward and reverse processes. We propagate
this context to all timesteps in the two processes to adapt their trajectories,
thereby facilitating cross-modal conditional modeling. We generalize our
contextualized diffusion to both DDPMs and DDIMs with theoretical derivations,
and demonstrate the effectiveness of our model in evaluations with two
challenging tasks: text-to-image generation, and text-to-video editing. In each
task, our ContextDiff achieves new state-of-the-art performance, significantly
enhancing the semantic alignment between text condition and generated samples,
as evidenced by quantitative and qualitative evaluations. Our code is available
at https://github.com/YangLing0818/ContextDiff
\\ ( https://arxiv.org/abs/2402.16627 ,  35014kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17563 (*cross-listing*)
Date: Tue, 27 Feb 2024 15:05:13 GMT   (7123kb,D)
Date (revised v2): Mon, 4 Mar 2024 14:51:40 GMT   (7124kb,D)

Title: Structure-Guided Adversarial Training of Diffusion Models
Authors: Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, Bin Cui
Categories: cs.CV cs.AI cs.LG
Comments: Accepted by CVPR 2024
\\
  Diffusion models have demonstrated exceptional efficacy in various generative
applications. While existing models focus on minimizing a weighted sum of
denoising score matching losses for data distribution modeling, their training
primarily emphasizes instance-level optimization, overlooking valuable
structural information within each mini-batch, indicative of pair-wise
relationships among samples. To address this limitation, we introduce
Structure-guided Adversarial training of Diffusion Models (SADM). In this
pioneering approach, we compel the model to learn manifold structures between
samples in each training batch. To ensure the model captures authentic manifold
structures in the data distribution, we advocate adversarial training of the
diffusion generator against a novel structure discriminator in a minimax game,
distinguishing real manifold structures from the generated ones. SADM
substantially improves existing diffusion transformers (DiT) and outperforms
existing methods in image generation and cross-domain fine-tuning tasks across
12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on
ImageNet for class-conditional image generation at resolutions of 256x256 and
512x512, respectively.
\\ ( https://arxiv.org/abs/2402.17563 ,  7124kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03224 (*cross-listing*)
Date: Sun, 25 Feb 2024 16:46:15 GMT   (466kb,D)

Title: Reinforcement Learning Jazz Improvisation: When Music Meets Game Theory
Authors: Vedant Tapiavala, Joshua Piesner, Sourjyamoy Barman, Feng Fu
Categories: physics.soc-ph cs.AI cs.LG cs.SD eess.AS
Comments: 16 pages, 4 figures
\\
  Live performances of music are always charming, with the unpredictability of
improvisation due to the dynamic between musicians and interactions with the
audience. Jazz improvisation is a particularly noteworthy example for further
investigation from a theoretical perspective. Here, we introduce a novel
mathematical game theory model for jazz improvisation, providing a framework
for studying music theory and improvisational methodologies. We use
computational modeling, mainly reinforcement learning, to explore diverse
stochastic improvisational strategies and their paired performance on
improvisation. We find that the most effective strategy pair is a strategy that
reacts to the most recent payoff (Stepwise Changes) with a reinforcement
learning strategy limited to notes in the given chord (Chord-Following
Reinforcement Learning). Conversely, a strategy that reacts to the partner's
last note and attempts to harmonize with it (Harmony Prediction) strategy pair
yields the lowest non-control payoff and highest standard deviation, indicating
that picking notes based on immediate reactions to the partner player can yield
inconsistent outcomes. On average, the Chord-Following Reinforcement Learning
strategy demonstrates the highest mean payoff, while Harmony Prediction
exhibits the lowest. Our work lays the foundation for promising applications
beyond jazz: including the use of artificial intelligence (AI) models to
extract data from audio clips to refine musical reward systems, and training
machine learning (ML) models on existing jazz solos to further refine
strategies within the game.
\\ ( https://arxiv.org/abs/2403.03224 ,  466kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03230 (*cross-listing*)
Date: Mon, 4 Mar 2024 15:27:59 GMT   (3592kb,D)

Title: Large language models surpass human experts in predicting neuroscience
  results
Authors: Xiaoliang Luo, Akilles Rechardt, Guangzhi Sun, Kevin K. Nejad, Felipe
  Y\'a\~nez, Bati Yilmaz, Kangjoo Lee, Alexandra O. Cohen, Valentina
  Borghesani, Anton Pashkov, Daniele Marinazzo, Jonathan Nicholas, Alessandro
  Salatiello, Ilia Sucholutsky, Pasquale Minervini, Sepehr Razavi, Roberta
  Rocca, Elkhan Yusifov, Tereza Okalova, Nianlong Gu, Martin Ferianc, Mikail
  Khona, Kaustubh R. Patil, Pui-Shee Lee, Rui Mata, Nicholas E. Myers, Jennifer
  K Bizley, Sebastian Musslick, Isil Poyraz Bilgin, Guiomar Niso, Justin M.
  Ales, Michael Gaebler, N Apurva Ratan Murty, Chloe M. Hall, Jessica Dafflon,
  Sherry Dongqi Bao, Bradley C. Love
Categories: q-bio.NC cs.AI
\\
  Scientific discoveries often hinge on synthesizing decades of research, a
task that potentially outstrips human information processing capacities. Large
language models (LLMs) offer a solution. LLMs trained on the vast scientific
literature could potentially integrate noisy yet interrelated findings to
forecast novel results better than human experts. To evaluate this possibility,
we created BrainBench, a forward-looking benchmark for predicting neuroscience
results. We find that LLMs surpass experts in predicting experimental outcomes.
BrainGPT, an LLM we tuned on the neuroscience literature, performed better yet.
Like human experts, when LLMs were confident in their predictions, they were
more likely to be correct, which presages a future where humans and LLMs team
together to make discoveries. Our approach is not neuroscience-specific and is
transferable to other knowledge-intensive endeavors.
\\ ( https://arxiv.org/abs/2403.03230 ,  3592kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03239 (*cross-listing*)
Date: Tue, 5 Mar 2024 16:07:57 GMT   (904kb)

Title: Note: Harnessing Tellurium Nanoparticles in the Digital Realm Plasmon
  Resonance, in the Context of Brewster's Angle and the Drude Model for Fake
  News Adsorption in Incomplete Information Games
Authors: Yasuko Kawahata
Categories: physics.soc-ph cs.AI
Comments: Tellurium Nanoparticles, Snell's Law, Soliton Solution, Anamorphic
  Surfaces, Nonlinear Dynamics, Fake News Adsorption, User Behavior Modeling,
  Health Improvement Strategies, Plasmonic Sensors
\\
  This note explores the innovative application of soliton theory and plasmonic
phenomena in modeling user behavior and engagement within digital health
platforms. By introducing the concept of soliton solutions, we present a novel
approach to understanding stable patterns of health improvement behaviors over
time. Additionally, we delve into the role of tellurium nanoparticles and their
plasmonic properties in adsorbing fake news, thereby influencing user
interactions and engagement levels. Through a theoretical framework that
combines nonlinear dynamics with the unique characteristics of tellurium
nanoparticles, we aim to provide new insights into the dynamics of user
engagement in digital health environments. Our analysis highlights the
potential of soliton theory in capturing the complex, nonlinear dynamics of
user behavior, while the application of plasmonic phenomena offers a promising
avenue for enhancing the sensitivity and effectiveness of digital health
platforms. This research ventures into an uncharted territory where optical
phenomena such as Brewster's Angle and Snell's Law, along with the concept of
spin solitons, are metaphorically applied to address the challenge of fake news
dissemination. By exploring the analogy between light refraction, reflection,
and the propagation of information in digital platforms, we unveil a novel
perspective on how the 'angle' at which information is presented can
significantly affect its acceptance and spread. Additionally, we propose the
use of tellurium nanoparticles to manage 'information waves' through mechanisms
akin to plasmonic resonance and soliton dynamics. This theoretical exploration
aims to bridge the gap between physical sciences and digital communication,
offering insights into the development of strategies for mitigating
misinformation.
\\ ( https://arxiv.org/abs/2403.03239 ,  904kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03274 (*cross-listing*)
Date: Tue, 5 Mar 2024 19:13:57 GMT   (1211kb,D)

Title: From Noise to Signal: Unveiling Treatment Effects from Digital Health
  Data through Pharmacology-Informed Neural-SDE
Authors: Samira Pakravan, Nikolaos Evangelou, Maxime Usdin, Logan Brooks and
  James Lu
Categories: q-bio.QM cs.AI cs.LG math.DS
Comments: 6 figures
ACM-class: I.2; G.3
\\
  Digital health technologies (DHT), such as wearable devices, provide
personalized, continuous, and real-time monitoring of patient. These
technologies are contributing to the development of novel therapies and
personalized medicine. Gaining insight from these technologies requires
appropriate modeling techniques to capture clinically-relevant changes in
disease state. The data generated from these devices is characterized by being
stochastic in nature, may have missing elements, and exhibits considerable
inter-individual variability - thereby making it difficult to analyze using
traditional longitudinal modeling techniques. We present a novel
pharmacology-informed neural stochastic differential equation (SDE) model
capable of addressing these challenges. Using synthetic data, we demonstrate
that our approach is effective in identifying treatment effects and learning
causal relationships from stochastic data, thereby enabling counterfactual
simulation.
\\ ( https://arxiv.org/abs/2403.03274 ,  1211kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03276 (*cross-listing*)
Date: Tue, 5 Mar 2024 19:15:17 GMT   (2410kb,D)

Title: ARNN: Attentive Recurrent Neural Network for Multi-channel EEG Signals
  to Identify Epileptic Seizures
Authors: Salim Rukhsar and Anil Kumar Tiwari
Categories: eess.SP cs.AI cs.LG
Comments: 9 pages, 7 figures, Journal Paper
\\
  We proposed an Attentive Recurrent Neural Network (ARNN), which recurrently
applies attention layers along a sequence and has linear complexity with
respect to the sequence length. The proposed model operates on multi-channel
EEG signals rather than single channel signals and leverages parallel
computation. In this cell, the attention layer is a computational unit that
efficiently applies self-attention and cross-attention mechanisms to compute a
recurrent function over a wide number of state vectors and input signals. Our
architecture is inspired in part by the attention layer and long short-term
memory (LSTM) cells, and it uses long-short style gates, but it scales this
typical cell up by several orders to parallelize for multi-channel EEG signals.
It inherits the advantages of attention layers and LSTM gate while avoiding
their respective drawbacks. We evaluated the model effectiveness through
extensive experiments with heterogeneous datasets, including the CHB-MIT and
UPenn and Mayos Clinic, CHB-MIT datasets. The empirical findings suggest that
the ARNN model outperforms baseline methods such as LSTM, Vision Transformer
(ViT), Compact Convolution Transformer (CCT), and R-Transformer (RT),
showcasing superior performance and faster processing capabilities across a
wide range of tasks. The code has been made publicly accessible at
\url{https://github.com/Salim-Lysiun/ARNN}.
\\ ( https://arxiv.org/abs/2403.03276 ,  2410kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03322 (*cross-listing*)
Date: Tue, 5 Mar 2024 21:05:16 GMT   (1311kb,D)

Title: Deep Configuration Performance Learning: A Systematic Survey and
  Taxonomy
Authors: Jingzhi Gong, Tao Chen
Categories: cs.SE cs.AI cs.LG
\\
  Performance is arguably the most crucial attribute that reflects the behavior
of a configurable software system. However, given the increasing scale and
complexity of modern software, modeling and predicting how various
configurations can impact performance becomes one of the major challenges in
software maintenance. As such, performance is often modeled without having a
thorough knowledge of the software system, but relying mainly on data, which
fits precisely with the purpose of deep learning.
  In this paper, we conduct a comprehensive review exclusively on the topic of
deep learning for performance learning of configurable software, covering 948
searched papers spanning six indexing services, based on which 85 primary
papers were extracted and analyzed. Our results summarize the key topics and
statistics on how the configuration data is prepared; how the deep
configuration performance learning model is built; how the model is evaluated
and how they are exploited in different tasks related to software
configuration. We also identify the good practice and the potentially
problematic phenomena from the studies surveyed, together with insights on
future opportunities for the field. To promote open science, all the raw
results of this survey can be accessed at our repository:
https://github.com/ideas-labo/DCPL-SLR.
\\ ( https://arxiv.org/abs/2403.03322 ,  1311kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03344 (*cross-listing*)
Date: Tue, 5 Mar 2024 22:12:01 GMT   (3339kb,D)

Title: Learn to Code Sustainably: An Empirical Study on LLM-based Green Code
  Generation
Authors: Tina Vartziotis, Ippolyti Dellatolas, George Dasoulas, Maximilian
  Schmidt, Florian Schneider, Tim Hoffmann, Sotirios Kotsopoulos, Michael
  Keckeisen
Categories: cs.SE cs.AI
\\
  The increasing use of information technology has led to a significant share
of energy consumption and carbon emissions from data centers. These
contributions are expected to rise with the growing demand for big data
analytics, increasing digitization, and the development of large artificial
intelligence (AI) models. The need to address the environmental impact of
software development has led to increased interest in green (sustainable)
coding and claims that the use of AI models can lead to energy efficiency
gains. Here, we provide an empirical study on green code and an overview of
green coding practices, as well as metrics used to quantify the sustainability
awareness of AI models. In this framework, we evaluate the sustainability of
auto-generated code. The auto-generate codes considered in this study are
produced by generative commercial AI language models, GitHub Copilot, OpenAI
ChatGPT-3, and Amazon CodeWhisperer. Within our methodology, in order to
quantify the sustainability awareness of these AI models, we propose a
definition of the code's "green capacity", based on certain sustainability
metrics. We compare the performance and green capacity of human-generated code
and code generated by the three AI language models in response to easy-to-hard
problem statements. Our findings shed light on the current capacity of AI
models to contribute to sustainable software development.
\\ ( https://arxiv.org/abs/2403.03344 ,  3339kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03385 (*cross-listing*)
Date: Wed, 6 Mar 2024 00:36:05 GMT   (1516kb,D)

Title: Multi-modal Deep Learning
Authors: Chen Yuhua
Categories: eess.IV cs.AI cs.CV
Comments: Master's thesis
\\
  This article investigates deep learning methodologies for single-modality
clinical data analysis, as a crucial precursor to multi-modal medical research.
Building on Guo JingYuan's work, the study refines clinical data processing
through Compact Convolutional Transformer (CCT), Patch Up, and the innovative
CamCenterLoss technique, establishing a foundation for future multimodal
investigations. The proposed methodology demonstrates improved prediction
accuracy and at tentiveness to critically ill patients compared to Guo
JingYuan's ResNet and StageNet approaches. Novelty that using image-pretrained
vision transformer backbone to perform transfer learning time-series clinical
data.The study highlights the potential of CCT, Patch Up, and novel
CamCenterLoss in processing single modality clinical data within deep learning
frameworks, paving the way for future multimodal medical research and promoting
precision and personalized healthcare
\\ ( https://arxiv.org/abs/2403.03385 ,  1516kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03395 (*cross-listing*)
Date: Wed, 6 Mar 2024 01:33:48 GMT   (736kb,D)

Title: Interactive Melody Generation System for Enhancing the Creativity of
  Musicians
Authors: So Hirawata and Noriko Otani
Categories: cs.SD cs.AI cs.HC eess.AS
\\
  This study proposes a system designed to enumerate the process of
collaborative composition among humans, using automatic music composition
technology. By integrating multiple Recurrent Neural Network (RNN) models, the
system provides an experience akin to collaborating with several composers,
thereby fostering diverse creativity. Through dynamic adaptation to the user's
creative intentions, based on feedback, the system enhances its capability to
generate melodies that align with user preferences and creative needs. The
system's effectiveness was evaluated through experiments with composers of
varying backgrounds, revealing its potential to facilitate musical creativity
and suggesting avenues for further refinement. The study underscores the
importance of interaction between the composer and AI, aiming to make music
composition more accessible and personalized. This system represents a step
towards integrating AI into the creative process, offering a new tool for
composition support and collaborative artistic exploration.
\\ ( https://arxiv.org/abs/2403.03395 ,  736kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03407 (*cross-listing*)
Date: Wed, 6 Mar 2024 02:23:32 GMT   (152kb,D)

Title: Human vs. Machine: Language Models and Wargames
Authors: Max Lamparth, Anthony Corso, Jacob Ganz, Oriana Skylar Mastro,
  Jacquelyn Schneider, Harold Trinkunas
Categories: cs.CY cs.AI cs.CL
\\
  Wargames have a long history in the development of military strategy and the
response of nations to threats or attacks. The advent of artificial
intelligence (AI) promises better decision-making and increased military
effectiveness. However, there is still debate about how AI systems, especially
large language models (LLMs), behave as compared to humans. To this end, we use
a wargame experiment with 107 national security expert human players designed
to look at crisis escalation in a fictional US-China scenario and compare human
players to LLM-simulated responses. We find considerable agreement in the LLM
and human responses but also significant quantitative and qualitative
differences between simulated and human players in the wargame, motivating
caution to policymakers before handing over autonomy or following AI-based
strategy recommendations.
\\ ( https://arxiv.org/abs/2403.03407 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03409 (*cross-listing*)
Date: Wed, 6 Mar 2024 02:36:15 GMT   (11425kb,D)

Title: Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales
  for Pruning Recurrent SNN
Authors: Biswadeep Chakraborty, Beomseok Kang, Harshit Kumar and Saibal
  Mukhopadhyay
Categories: cs.NE cs.AI
Comments: Published as a conference paper at ICLR 2024
Journal-ref: ICLR 2024
\\
  Recurrent Spiking Neural Networks (RSNNs) have emerged as a computationally
efficient and brain-inspired learning model. The design of sparse RSNNs with
fewer neurons and synapses helps reduce the computational complexity of RSNNs.
Traditionally, sparse SNNs are obtained by first training a dense and complex
SNN for a target task, and, then, pruning neurons with low activity
(activity-based pruning) while maintaining task performance. In contrast, this
paper presents a task-agnostic methodology for designing sparse RSNNs by
pruning a large randomly initialized model. We introduce a novel Lyapunov Noise
Pruning (LNP) algorithm that uses graph sparsification methods and utilizes
Lyapunov exponents to design a stable sparse RSNN from a randomly initialized
RSNN. We show that the LNP can leverage diversity in neuronal timescales to
design a sparse Heterogeneous RSNN (HRSNN). Further, we show that the same
sparse HRSNN model can be trained for different tasks, such as image
classification and temporal prediction. We experimentally show that, in spite
of being task-agnostic, LNP increases computational efficiency (fewer neurons
and synapses) and prediction performance of RSNNs compared to traditional
activity-based pruning of trained dense models.
\\ ( https://arxiv.org/abs/2403.03409 ,  11425kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03421 (*cross-listing*)
Date: Wed, 6 Mar 2024 03:08:20 GMT   (21022kb,D)

Title: LEAD: Learning Decomposition for Source-free Universal Domain Adaptation
Authors: Sanqing Qu, Tianpei Zou, Lianghua He, Florian R\"ohrbein, Alois Knoll,
  Guang Chen, Changjun Jiang
Categories: cs.CV cs.AI cs.LG
Comments: To appear in CVPR 2024
\\
  Universal Domain Adaptation (UniDA) targets knowledge transfer in the
presence of both covariate and label shifts. Recently, Source-free Universal
Domain Adaptation (SF-UniDA) has emerged to achieve UniDA without access to
source data, which tends to be more practical due to data protection policies.
The main challenge lies in determining whether covariate-shifted samples belong
to target-private unknown categories. Existing methods tackle this either
through hand-crafted thresholding or by developing time-consuming iterative
clustering strategies. In this paper, we propose a new idea of LEArning
Decomposition (LEAD), which decouples features into source-known and -unknown
components to identify target-private data. Technically, LEAD initially
leverages the orthogonal decomposition analysis for feature decomposition.
Then, LEAD builds instance-level decision boundaries to adaptively identify
target-private data. Extensive experiments across various UniDA scenarios have
demonstrated the effectiveness and superiority of LEAD. Notably, in the OPDA
scenario on VisDA dataset, LEAD outperforms GLC by 3.5% overall H-score and
reduces 75% time to derive pseudo-labeling decision boundaries. Besides, LEAD
is also appealing in that it is complementary to most existing methods. The
code is available at https://github.com/ispc-lab/LEAD.
\\ ( https://arxiv.org/abs/2403.03421 ,  21022kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03456 (*cross-listing*)
Date: Wed, 6 Mar 2024 04:46:03 GMT   (26697kb,D)

Title: DLP-GAN: Learning to Draw Modern Chinese Landscape Photos with
  Generative Adversarial Network
Authors: Xiangquan Gui, Binxuan Zhang, Li Li, Yi Yang
Categories: cs.CV cs.AI
Journal-ref: Neural Computing and Applications, 2023: 1-18
DOI: 10.1007/s00521-023-09345-8
\\
  Chinese landscape painting has a unique and artistic style, and its drawing
technique is highly abstract in both the use of color and the realistic
representation of objects. Previous methods focus on transferring from modern
photos to ancient ink paintings. However, little attention has been paid to
translating landscape paintings into modern photos. To solve such problems, in
this paper, we (1) propose DLP-GAN (\textbf{D}raw Modern Chinese
\textbf{L}andscape \textbf{P}hotos with \textbf{G}enerative
\textbf{A}dversarial \textbf{N}etwork), an unsupervised cross-domain image
translation framework with a novel asymmetric cycle mapping, and (2) introduce
a generator based on a dense-fusion module to match different translation
directions. Moreover, a dual-consistency loss is proposed to balance the
realism and abstraction of model painting. In this way, our model can draw
landscape photos and sketches in the modern sense. Finally, based on our
collection of modern landscape and sketch datasets, we compare the images
generated by our model with other benchmarks. Extensive experiments including
user studies show that our model outperforms state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.03456 ,  26697kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03536 (*cross-listing*)
Date: Wed, 6 Mar 2024 08:31:35 GMT   (74kb,D)

Title: Towards Efficient and Effective Unlearning of Large Language Models for
  Recommendation
Authors: Hangyu Wang, Jianghao Lin, Bo Chen, Yang Yang, Ruiming Tang, Weinan
  Zhang, Yong Yu
Categories: cs.IR cs.AI
Comments: 12 pages
\\
  The significant advancements in large language models (LLMs) give rise to a
promising research direction, i.e., leveraging LLMs as recommenders (LLMRec).
The efficacy of LLMRec arises from the open-world knowledge and reasoning
capabilities inherent in LLMs. LLMRec acquires the recommendation capabilities
through instruction tuning based on user interaction data. However, in order to
protect user privacy and optimize utility, it is also crucial for LLMRec to
intentionally forget specific user data, which is generally referred to as
recommendation unlearning. In the era of LLMs, recommendation unlearning poses
new challenges for LLMRec in terms of \textit{inefficiency} and
\textit{ineffectiveness}. Existing unlearning methods require updating billions
of parameters in LLMRec, which is costly and time-consuming. Besides, they
always impact the model utility during the unlearning process. To this end, we
propose \textbf{E2URec}, the first \underline{E}fficient and
\underline{E}ffective \underline{U}nlearning method for LLM\underline{Rec}. Our
proposed E2URec enhances the unlearning efficiency by updating only a few
additional LoRA parameters, and improves the unlearning effectiveness by
employing a teacher-student framework, where we maintain multiple teacher
networks to guide the unlearning process. Extensive experiments show that
E2URec outperforms state-of-the-art baselines on two real-world datasets.
Specifically, E2URec can efficiently forget specific data without affecting
recommendation performance. The source code is at
\url{https://github.com/justarter/E2URec}.
\\ ( https://arxiv.org/abs/2403.03536 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03538 (*cross-listing*)
Date: Wed, 6 Mar 2024 08:34:28 GMT   (1382kb,D)

Title: RADIA - Radio Advertisement Detection with Intelligent Analytics
Authors: Jorge \'Alvarez, Juan Carlos Armenteros, Camilo Torr\'on, Miguel
  Ortega-Mart\'in, Alfonso Ardoiz, \'Oscar Garc\'ia, Ignacio Arranz, \'I\~nigo
  Galdeano, Ignacio Garrido, Adri\'an Alonso, Fernando Bay\'on, Oleg Vorontsov
Categories: cs.SD cs.AI cs.CL eess.AS
\\
  Radio advertising remains an integral part of modern marketing strategies,
with its appeal and potential for targeted reach undeniably effective. However,
the dynamic nature of radio airtime and the rising trend of multiple radio
spots necessitates an efficient system for monitoring advertisement broadcasts.
This study investigates a novel automated radio advertisement detection
technique incorporating advanced speech recognition and text classification
algorithms. RadIA's approach surpasses traditional methods by eliminating the
need for prior knowledge of the broadcast content. This contribution allows for
detecting impromptu and newly introduced advertisements, providing a
comprehensive solution for advertisement detection in radio broadcasting.
Experimental results show that the resulting model, trained on carefully
segmented and tagged text data, achieves an F1-macro score of 87.76 against a
theoretical maximum of 89.33. This paper provides insights into the choice of
hyperparameters and their impact on the model's performance. This study
demonstrates its potential to ensure compliance with advertising broadcast
contracts and offer competitive surveillance. This groundbreaking research
could fundamentally change how radio advertising is monitored and open new
doors for marketing optimization.
\\ ( https://arxiv.org/abs/2403.03538 ,  1382kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03578 (*cross-listing*)
Date: Wed, 6 Mar 2024 09:48:48 GMT   (2291kb,D)

Title: Causal Disentanglement for Regulating Social Influence Bias in Social
  Recommendation
Authors: Li Wang, Min Xu, Quangui Zhang, Yunxiao Shi, Qiang Wu
Categories: cs.SI cs.AI
\\
  Social recommendation systems face the problem of social influence bias,
which can lead to an overemphasis on recommending items that friends have
interacted with. Addressing this problem is crucial, and existing methods often
rely on techniques such as weight adjustment or leveraging unbiased data to
eliminate this bias. However, we argue that not all biases are detrimental,
i.e., some items recommended by friends may align with the user's interests.
Blindly eliminating such biases could undermine these positive effects,
potentially diminishing recommendation accuracy. In this paper, we propose a
Causal Disentanglement-based framework for Regulating Social influence Bias in
social recommendation, named CDRSB, to improve recommendation performance. From
the perspective of causal inference, we find that the user social network could
be regarded as a confounder between the user and item embeddings (treatment)
and ratings (outcome). Due to the presence of this social network confounder,
two paths exist from user and item embeddings to ratings: a non-causal social
influence path and a causal interest path. Building upon this insight, we
propose a disentangled encoder that focuses on disentangling user and item
embeddings into interest and social influence embeddings. Mutual
information-based objectives are designed to enhance the distinctiveness of
these disentangled embeddings, eliminating redundant information. Additionally,
a regulatory decoder that employs a weight calculation module to dynamically
learn the weights of social influence embeddings for effectively regulating
social influence bias has been designed. Experimental results on four
large-scale real-world datasets Ciao, Epinions, Dianping, and Douban book
demonstrate the effectiveness of CDRSB compared to state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2403.03578 ,  2291kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03592 (*cross-listing*)
Date: Wed, 6 Mar 2024 10:25:36 GMT   (1049kb,D)

Title: Wildest Dreams: Reproducible Research in Privacy-preserving Neural
  Network Training
Authors: Tanveer Khan, Mindaugas Budzys, Khoa Nguyen, Antonis Michalas
Categories: cs.CR cs.AI
\\
  Machine Learning (ML), addresses a multitude of complex issues in multiple
disciplines, including social sciences, finance, and medical research. ML
models require substantial computing power and are only as powerful as the data
utilized. Due to high computational cost of ML methods, data scientists
frequently use Machine Learning-as-a-Service (MLaaS) to outsource computation
to external servers. However, when working with private information, like
financial data or health records, outsourcing the computation might result in
privacy issues. Recent advances in Privacy-Preserving Techniques (PPTs) have
enabled ML training and inference over protected data through the use of
Privacy-Preserving Machine Learning (PPML). However, these techniques are still
at a preliminary stage and their application in real-world situations is
demanding. In order to comprehend discrepancy between theoretical research
suggestions and actual applications, this work examines the past and present of
PPML, focusing on Homomorphic Encryption (HE) and Secure Multi-party
Computation (SMPC) applied to ML. This work primarily focuses on the ML model's
training phase, where maintaining user data privacy is of utmost importance. We
provide a solid theoretical background that eases the understanding of current
approaches and their limitations. In addition, we present a SoK of the most
recent PPML frameworks for model training and provide a comprehensive
comparison in terms of the unique properties and performances on standard
benchmarks. Also, we reproduce the results for some of the papers and examine
at what level existing works in the field provide support for open science. We
believe our work serves as a valuable contribution by raising awareness about
the current gap between theoretical advancements and real-world applications in
PPML, specifically regarding open-source availability, reproducibility, and
usability.
\\ ( https://arxiv.org/abs/2403.03592 ,  1049kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03593 (*cross-listing*)
Date: Wed, 6 Mar 2024 10:27:08 GMT   (287kb,D)

Title: Do You Trust Your Model? Emerging Malware Threats in the Deep Learning
  Ecosystem
Authors: Dorjan Hitaj, Giulio Pagnotta, Fabio De Gaspari, Sediola Ruko, Briland
  Hitaj, Luigi V. Mancini, Fernando Perez-Cruz
Categories: cs.CR cs.AI
Comments: 16 pages, 9 figures
\\
  Training high-quality deep learning models is a challenging task due to
computational and technical requirements. A growing number of individuals,
institutions, and companies increasingly rely on pre-trained, third-party
models made available in public repositories. These models are often used
directly or integrated in product pipelines with no particular precautions,
since they are effectively just data in tensor form and considered safe. In
this paper, we raise awareness of a new machine learning supply chain threat
targeting neural networks. We introduce MaleficNet 2.0, a novel technique to
embed self-extracting, self-executing malware in neural networks. MaleficNet
2.0 uses spread-spectrum channel coding combined with error correction
techniques to inject malicious payloads in the parameters of deep neural
networks. MaleficNet 2.0 injection technique is stealthy, does not degrade the
performance of the model, and is robust against removal techniques. We design
our approach to work both in traditional and distributed learning settings such
as Federated Learning, and demonstrate that it is effective even when a reduced
number of bits is used for the model parameters. Finally, we implement a
proof-of-concept self-extracting neural network malware using MaleficNet 2.0,
demonstrating the practicality of the attack against a widely adopted machine
learning framework. Our aim with this work is to raise awareness against these
new, dangerous attacks both in the research community and industry, and we hope
to encourage further research in mitigation techniques against such threats.
\\ ( https://arxiv.org/abs/2403.03593 ,  287kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03606 (*cross-listing*)
Date: Wed, 6 Mar 2024 10:53:12 GMT   (1087kb)

Title: Enhancing Price Prediction in Cryptocurrency Using Transformer Neural
  Network and Technical Indicators
Authors: Mohammad Ali Labbaf Khaniki, Mohammad Manthouri
Categories: q-fin.CP cs.AI cs.LG
\\
  This study presents an innovative approach for predicting cryptocurrency time
series, specifically focusing on Bitcoin, Ethereum, and Litecoin. The
methodology integrates the use of technical indicators, a Performer neural
network, and BiLSTM (Bidirectional Long Short-Term Memory) to capture temporal
dynamics and extract significant features from raw cryptocurrency data. The
application of technical indicators, such facilitates the extraction of
intricate patterns, momentum, volatility, and trends. The Performer neural
network, employing Fast Attention Via positive Orthogonal Random features
(FAVOR+), has demonstrated superior computational efficiency and scalability
compared to the traditional Multi-head attention mechanism in Transformer
models. Additionally, the integration of BiLSTM in the feedforward network
enhances the model's capacity to capture temporal dynamics in the data,
processing it in both forward and backward directions. This is particularly
advantageous for time series data where past and future data points can
influence the current state. The proposed method has been applied to the hourly
and daily timeframes of the major cryptocurrencies and its performance has been
benchmarked against other methods documented in the literature. The results
underscore the potential of the proposed method to outperform existing models,
marking a significant progression in the field of cryptocurrency price
prediction.
\\ ( https://arxiv.org/abs/2403.03606 ,  1087kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03608 (*cross-listing*)
Date: Wed, 6 Mar 2024 10:55:50 GMT   (1037kb,D)

Title: GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D
  Scene Understanding
Authors: Zi-Ting Chou, Sheng-Yu Huang, I-Jieh Liu, Yu-Chiang Frank Wang
Categories: cs.CV cs.AI cs.LG
Comments: Accepted by CVPR2024
\\
  Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance
Fields (NeRF) have emerged as a popular research topic in 3D vision. In this
work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF),
which uniquely takes image semantics into the synthesis process so that both
novel view images and the associated semantic maps can be produced for unseen
scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and
Depth-Guided Visual rendering. The former is able to observe multi-view image
inputs to extract semantic and geometry features from a scene. Guided by the
resulting image geometry information, the latter performs both image and
semantic rendering with improved performances. Our experiments not only confirm
that GSNeRF performs favorably against prior works on both novel-view image and
semantic segmentation synthesis but the effectiveness of our sampling strategy
for visual rendering is further verified.
\\ ( https://arxiv.org/abs/2403.03608 ,  1037kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03691 (*cross-listing*)
Date: Wed, 6 Mar 2024 13:17:41 GMT   (4009kb,D)

Title: MolNexTR: A Generalized Deep Learning Model for Molecular Image
  Recognition
Authors: Yufan Chen, Ching Ting Leung, Yong Huang, Jianwei Sun, Hao Chen, Hanyu
  Gao
Categories: cs.CV cs.AI
Comments: Submitted to the Journal of Cheminformatics
\\
  In the field of chemical structure recognition, the task of converting
molecular images into graph structures and SMILES string stands as a
significant challenge, primarily due to the varied drawing styles and
conventions prevalent in chemical literature. To bridge this gap, we proposed
MolNexTR, a novel image-to-graph deep learning model that collaborates to fuse
the strengths of ConvNext, a powerful Convolutional Neural Network variant, and
Vision-TRansformer. This integration facilitates a more nuanced extraction of
both local and global features from molecular images. MolNexTR can predict
atoms and bonds simultaneously and understand their layout rules. It also
excels at flexibly integrating symbolic chemistry principles to discern
chirality and decipher abbreviated structures. We further incorporate a series
of advanced algorithms, including improved data augmentation module, image
contamination module, and a post-processing module to get the final SMILES
output. These modules synergistically enhance the model's robustness against
the diverse styles of molecular imagery found in real literature. In our test
sets, MolNexTR has demonstrated superior performance, achieving an accuracy
rate of 81-97%, marking a significant advancement in the domain of molecular
structure recognition. Scientific contribution: MolNexTR is a novel
image-to-graph model that incorporates a unique dual-stream encoder to extract
complex molecular image features, and combines chemical rules to predict atoms
and bonds while understanding atom and bond layout rules. In addition, it
employs a series of novel augmentation algorithms to significantly enhance the
robustness and performance of the model.
\\ ( https://arxiv.org/abs/2403.03691 ,  4009kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03730 (*cross-listing*)
Date: Wed, 6 Mar 2024 14:19:11 GMT   (4977kb,D)

Title: Learning 3D object-centric representation through prediction
Authors: John Day, Tushar Arora, Jirui Liu, Li Erran Li, and Ming Bo Cai
Categories: cs.CV cs.AI cs.LG
Comments: 21 pages, 11 figures. Project webpage can be found at
  https://jday54.github.io/opple_site/
ACM-class: I.2.10; I.4.8; I.4.6; I.4.10; I.2.6
\\
  As part of human core knowledge, the representation of objects is the
building block of mental representation that supports high-level concepts and
symbolic reasoning. While humans develop the ability of perceiving objects
situated in 3D environments without supervision, models that learn the same set
of abilities with similar constraints faced by human infants are lacking.
Towards this end, we developed a novel network architecture that simultaneously
learns to 1) segment objects from discrete images, 2) infer their 3D locations,
and 3) perceive depth, all while using only information directly available to
the brain as training data, namely: sequences of images and self-motion. The
core idea is treating objects as latent causes of visual input which the brain
uses to make efficient predictions of future scenes. This results in object
representations being learned as an essential byproduct of learning to predict.
\\ ( https://arxiv.org/abs/2403.03730 ,  4977kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03781 (*cross-listing*)
Date: Wed, 6 Mar 2024 15:23:26 GMT   (1395kb,D)

Title: Neural Architecture Search using Particle Swarm and Ant Colony
  Optimization
Authors: S\'eamus Lankford and Diarmuid Grimes
Categories: cs.NE cs.AI cs.LG
Journal-ref: Proceedings of The 28th Irish Conference on Artificial
  Intelligence and Cognitive Science. 2771. CEUR-WS, 2020
\\
  Neural network models have a number of hyperparameters that must be chosen
along with their architecture. This can be a heavy burden on a novice user,
choosing which architecture and what values to assign to parameters. In most
cases, default hyperparameters and architectures are used. Significant
improvements to model accuracy can be achieved through the evaluation of
multiple architectures. A process known as Neural Architecture Search (NAS) may
be applied to automatically evaluate a large number of such architectures. A
system integrating open source tools for Neural Architecture Search (OpenNAS),
in the classification of images, has been developed as part of this research.
OpenNAS takes any dataset of grayscale, or RBG images, and generates
Convolutional Neural Network (CNN) architectures based on a range of
metaheuristics using either an AutoKeras, a transfer learning or a Swarm
Intelligence (SI) approach. Particle Swarm Optimization (PSO) and Ant Colony
Optimization (ACO) are used as the SI algorithms. Furthermore, models developed
through such metaheuristics may be combined using stacking ensembles. In the
context of this paper, we focus on training and optimizing CNNs using the Swarm
Intelligence (SI) components of OpenNAS. Two major types of SI algorithms,
namely PSO and ACO, are compared to see which is more effective in generating
higher model accuracies. It is shown, with our experimental design, that the
PSO algorithm performs better than ACO. The performance improvement of PSO is
most notable with a more complex dataset. As a baseline, the performance of
fine-tuned pre-trained models is also evaluated.
\\ ( https://arxiv.org/abs/2403.03781 ,  1395kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03808 (*cross-listing*)
Date: Wed, 6 Mar 2024 15:59:39 GMT   (634kb,D)

Title: Confidence-Aware Decision-Making and Control for Tool Selection
Authors: Ajith Anil Meera and Pablo Lanillos
Categories: cs.RO cs.AI cs.SY eess.SY
\\
  Self-reflecting about our performance (e.g., how confident we are) before
doing a task is essential for decision making, such as selecting the most
suitable tool or choosing the best route to drive. While this form of awareness
-- thinking about our performance or metacognitive performance -- is well-known
in humans, robots still lack this cognitive ability. This reflective monitoring
can enhance their embodied decision power, robustness and safety. Here, we take
a step in this direction by introducing a mathematical framework that allows
robots to use their control self-confidence to make better-informed decisions.
We derive a mathematical closed-form expression for control confidence for
dynamic systems (i.e., the posterior inverse covariance of the control action).
This control confidence seamlessly integrates within an objective function for
decision making, that balances the: i) performance for task completion, ii)
control effort, and iii) self-confidence. To evaluate our theoretical account,
we framed the decision-making within the tool selection problem, where the
agent has to select the best robot arm for a particular control task. The
statistical analysis of the numerical simulations with randomized 2DOF arms
shows that using control confidence during tool selection improves both real
task performance, and the reliability of the tool for performance under
unmodelled perturbations (e.g., external forces). Furthermore, our results
indicate that control confidence is an early indicator of performance and thus,
it can be used as a heuristic for making decisions when computation power is
restricted or decision-making is intractable. Overall, we show the advantages
of using confidence-aware decision-making and control scheme for dynamic
systems.
\\ ( https://arxiv.org/abs/2403.03808 ,  634kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03864 (*cross-listing*)
Date: Wed, 6 Mar 2024 17:15:04 GMT   (1736kb,D)

Title: Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious
  Challenges in Multimodal Reasoning
Authors: Deepanway Ghosal, Vernon Toh Yan Han, Chia Yew Ken, Soujanya Poria
Categories: cs.CV cs.AI
\\
  This paper introduces the novel task of multimodal puzzle solving, framed
within the context of visual question-answering. We present a new dataset,
AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal
language models in solving algorithmic puzzles that necessitate both visual
understanding, language understanding, and complex algorithmic reasoning. We
create the puzzles to encompass a diverse array of mathematical and algorithmic
topics such as boolean logic, combinatorics, graph theory, optimization,
search, etc., aiming to evaluate the gap between visual data interpretation and
algorithmic problem-solving skills. The dataset is generated automatically from
code authored by humans. All our puzzles have exact solutions that can be found
from the algorithm without tedious human calculations. It ensures that our
dataset can be scaled up arbitrarily in terms of reasoning complexity and
dataset size. Our investigation reveals that large language models (LLMs) such
as GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. We
find that their performance is near random in a multi-choice question-answering
setup for a significant number of puzzles. The findings emphasize the
challenges of integrating visual, language, and algorithmic knowledge for
solving complex reasoning problems.
\\ ( https://arxiv.org/abs/2403.03864 ,  1736kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03879 (*cross-listing*)
Date: Wed, 6 Mar 2024 17:38:33 GMT   (21425kb,D)

Title: Redefining cystoscopy with ai: bladder cancer diagnosis using an
  efficient hybrid cnn-transformer model
Authors: Meryem Amaouche and Ouassim Karrakchou and Mounir Ghogho and Anouar El
  Ghazzaly and Mohamed Alami and Ahmed Ameur
Categories: cs.CV cs.AI
Comments: 7 pages, 5 figures
\\
  Bladder cancer ranks within the top 10 most diagnosed cancers worldwide and
is among the most expensive cancers to treat due to the high recurrence rates
which require lifetime follow-ups. The primary tool for diagnosis is
cystoscopy, which heavily relies on doctors' expertise and interpretation.
Therefore, annually, numerous cases are either undiagnosed or misdiagnosed and
treated as urinary infections. To address this, we suggest a deep learning
approach for bladder cancer detection and segmentation which combines CNNs with
a lightweight positional-encoding-free transformer and dual attention gates
that fuse self and spatial attention for feature enhancement. The architecture
suggested in this paper is efficient making it suitable for medical scenarios
that require real time inference. Experiments have proven that this model
addresses the critical need for a balance between computational efficiency and
diagnostic accuracy in cystoscopic imaging as despite its small size it rivals
large models in performance.
\\ ( https://arxiv.org/abs/2403.03879 ,  21425kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03881 (*cross-listing*)
Date: Wed, 6 Mar 2024 17:41:41 GMT   (60221kb,D)

Title: Latent Dataset Distillation with Diffusion Models
Authors: Brian B. Moser, Federico Raue, Sebastian Palacio, Stanislav Frolov and
  Andreas Dengel
Categories: cs.CV cs.AI cs.LG
\\
  The efficacy of machine learning has traditionally relied on the availability
of increasingly larger datasets. However, large datasets pose storage
challenges and contain non-influential samples, which could be ignored during
training without impacting the final accuracy of the model. In response to
these limitations, the concept of distilling the information on a dataset into
a condensed set of (synthetic) samples, namely a distilled dataset, emerged.
One crucial aspect is the selected architecture (usually ConvNet) for linking
the original and synthetic datasets. However, the final accuracy is lower if
the employed model architecture differs from the model used during
distillation. Another challenge is the generation of high-resolution images,
e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation
with Diffusion Models (LD3M) that combine diffusion in latent space with
dataset distillation to tackle both challenges. LD3M incorporates a novel
diffusion process tailored for dataset distillation, which improves the
gradient norms for learning synthetic images. By adjusting the number of
diffusion steps, LD3M also offers a straightforward way of controlling the
trade-off between speed and accuracy. We evaluate our approach in several
ImageNet subsets and for high-resolution images (128x128 and 256x256). As a
result, LD3M consistently outperforms state-of-the-art distillation techniques
by up to 4.8 p.p. and 4.2 p.p. for 1 and 10 images per class, respectively.
\\ ( https://arxiv.org/abs/2403.03881 ,  60221kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03890 (*cross-listing*)
Date: Wed, 6 Mar 2024 17:50:26 GMT   (12505kb,D)

Title: Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic
  Manipulation
Authors: Xiao Ma, Sumit Patidar, Iain Haughton, Stephen James
Categories: cs.RO cs.AI cs.CV cs.LG
Comments: Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR 2024). Videos and code:
  https://yusufma03.github.io/projects/hdp/
\\
  This paper introduces Hierarchical Diffusion Policy (HDP), a hierarchical
agent for multi-task robotic manipulation. HDP factorises a manipulation policy
into a hierarchical structure: a high-level task-planning agent which predicts
a distant next-best end-effector pose (NBP), and a low-level goal-conditioned
diffusion policy which generates optimal motion trajectories. The factorised
policy representation allows HDP to tackle both long-horizon task planning
while generating fine-grained low-level actions. To generate context-aware
motion trajectories while satisfying robot kinematics constraints, we present a
novel kinematics-aware goal-conditioned control agent, Robot Kinematics
Diffuser (RK-Diffuser). Specifically, RK-Diffuser learns to generate both the
end-effector pose and joint position trajectories, and distill the accurate but
kinematics-unaware end-effector pose diffuser to the kinematics-aware but less
accurate joint position diffuser via differentiable kinematics. Empirically, we
show that HDP achieves a significantly higher success rate than the
state-of-the-art methods in both simulation and real-world.
\\ ( https://arxiv.org/abs/2403.03890 ,  12505kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03925 (*cross-listing*)
Date: Wed, 6 Mar 2024 18:37:06 GMT   (11kb)

Title: Consciousness qua Mortal Computation
Authors: Johannes Kleiner
Categories: q-bio.NC cs.AI
\\
  Computational functionalism posits that consciousness is a computation. Here
we show, perhaps surprisingly, that it cannot be a Turing computation. Rather,
computational functionalism implies that consciousness is a novel type of
computation that has recently been proposed by Geoffrey Hinton, called mortal
computation.
\\ ( https://arxiv.org/abs/2403.03925 ,  11kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03949 (*cross-listing*)
Date: Wed, 6 Mar 2024 18:55:36 GMT   (26069kb,D)

Title: Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach
  for Robust Manipulation
Authors: Marcel Torne, Anthony Simeonov, Zechu Li, April Chan, Tao Chen,
  Abhishek Gupta, Pulkit Agrawal
Categories: cs.RO cs.AI cs.LG
Comments: Project page: https://real-to-sim-to-real.github.io/RialTo/
\\
  Imitation learning methods need significant human supervision to learn
policies robust to changes in object poses, physical disturbances, and visual
distractors. Reinforcement learning, on the other hand, can explore the
environment autonomously to learn robust behaviors but may require impractical
amounts of unsafe real-world data collection. To learn performant, robust
policies without the burden of unsafe real-world data collection or extensive
human supervision, we propose RialTo, a system for robustifying real-world
imitation learning policies via reinforcement learning in "digital twin"
simulation environments constructed on the fly from small amounts of real-world
data. To enable this real-to-sim-to-real pipeline, RialTo proposes an
easy-to-use interface for quickly scanning and constructing digital twins of
real-world environments. We also introduce a novel "inverse distillation"
procedure for bringing real-world demonstrations into simulated environments
for efficient fine-tuning, with minimal human intervention and engineering
required. We evaluate RialTo across a variety of robotic manipulation problems
in the real world, such as robustly stacking dishes on a rack, placing books on
a shelf, and six other tasks. RialTo increases (over 67%) in policy robustness
without requiring extensive human data collection. Project website and videos
at https://real-to-sim-to-real.github.io/RialTo/
\\ ( https://arxiv.org/abs/2403.03949 ,  26069kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03522 (*cross-listing*)
Date: Wed, 6 Mar 2024 08:03:05 GMT   (626kb,D)

Title: Non-verbal information in spontaneous speech - towards a new framework
  of analysis
Authors: Tirza Biron, Moshe Barboy, Eran Ben-Artzy, Alona Golubchik, Yanir
  Marmor, Smadar Szekely, Yaron Winter, David Harel
Categories: cs.SD cs.CL cs.LG eess.AS
\\
  Non-verbal signals in speech are encoded by prosody and carry information
that ranges from conversation action to attitude and emotion. Despite its
importance, the principles that govern prosodic structure are not yet
adequately understood. This paper offers an analytical schema and a
technological proof-of-concept for the categorization of prosodic signals and
their association with meaning. The schema interprets surface-representations
of multi-layered prosodic events. As a first step towards implementation, we
present a classification process that disentangles prosodic phenomena of three
orders. It relies on fine-tuning a pre-trained speech recognition model,
enabling the simultaneous multi-class/multi-label detection. It generalizes
over a large variety of spontaneous data, performing on a par with, or superior
to, human annotation. In addition to a standardized formalization of prosody,
disentangling prosodic patterns can direct a theory of communication and speech
organization. A welcome by-product is an interpretation of prosody that will
enhance speech- and language-related technologies.
\\ ( https://arxiv.org/abs/2403.03522 ,  626kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03956 (*cross-listing*)
Date: Wed, 6 Mar 2024 18:59:02 GMT   (1169kb,D)

Title: Backtracing: Retrieving the Cause of the Query
Authors: Rose E. Wang, Pawan Wirawarn, Omar Khattab, Noah Goodman, Dorottya
  Demszky
Categories: cs.IR cs.CL
Comments: Code: https://github.com/rosewang2008/backtracing; EACL 2024
  Findings, Long Paper
\\
  Many online content portals allow users to ask questions to supplement their
understanding (e.g., of lectures). While information retrieval (IR) systems may
provide answers for such user queries, they do not directly assist content
creators -- such as lecturers who want to improve their content -- identify
segments that _caused_ a user to ask those questions. We introduce the task of
backtracing, in which systems retrieve the text segment that most likely caused
a user query. We formalize three real-world domains for which backtracing is
important in improving content delivery and communication: understanding the
cause of (a) student confusion in the Lecture domain, (b) reader curiosity in
the News Article domain, and (c) user emotion in the Conversation domain. We
evaluate the zero-shot performance of popular information retrieval methods and
language modeling methods, including bi-encoder, re-ranking and
likelihood-based methods and ChatGPT. While traditional IR systems retrieve
semantically relevant information (e.g., details on "projection matrices" for a
query "does projecting multiple times still lead to the same point?"), they
often miss the causally relevant context (e.g., the lecturer states "projecting
twice gets me the same answer as one projection"). Our results show that there
is room for improvement on backtracing and it requires new retrieval
approaches. We hope our benchmark serves to improve future retrieval systems
for backtracing, spawning systems that refine content generation and identify
linguistic triggers influencing user queries. Our code and data are
open-sourced: https://github.com/rosewang2008/backtracing.
\\ ( https://arxiv.org/abs/2403.03956 ,  1169kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03229 (*cross-listing*)
Date: Mon, 4 Mar 2024 12:36:31 GMT   (1481kb,D)

Title: Embracing Uncertainty Flexibility: Harnessing a Supervised Tree Kernel
  to Empower Ensemble Modelling for 2D Echocardiography-Based Prediction of
  Right Ventricular Volume
Authors: Tuan A. Bohoran, Polydoros N. Kampaktsis, Laura McLaughlin, Jay Leb,
  Gerry P. McCann, Archontis Giannakidis
Categories: q-bio.TO cs.LG eess.IV math.AP
Comments: 8 Pages, ICMV2023
\\
  The right ventricular (RV) function deterioration strongly predicts clinical
outcomes in numerous circumstances. To boost the clinical deployment of
ensemble regression methods that quantify RV volumes using tabular data from
the widely available two-dimensional echocardiography (2DE), we propose to
complement the volume predictions with uncertainty scores. To this end, we
employ an instance-based method which uses the learned tree structure to
identify the nearest training samples to a target instance and then uses a
number of distribution types to more flexibly model the output. The
probabilistic and point-prediction performances of the proposed framework are
evaluated on a relatively small-scale dataset, comprising 100 end-diastolic and
end-systolic RV volumes. The reference values for point performance were
obtained from MRI. The results demonstrate that our flexible approach yields
improved probabilistic and point performances over other state-of-the-art
methods. The appropriateness of the proposed framework is showcased by
providing exemplar cases. The estimated uncertainty embodies both aleatoric and
epistemic types. This work aligns with trustworthy artificial intelligence
since it can be used to enhance the decision-making process and reduce risks.
The feature importance scores of our framework can be exploited to reduce the
number of required 2DE views which could enhance the proposed pipeline's
clinical application.
\\ ( https://arxiv.org/abs/2403.03229 ,  1481kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03231 (*cross-listing*)
Date: Mon, 4 Mar 2024 19:04:41 GMT   (928kb)

Title: Machine and deep learning methods for predicting 3D genome organization
Authors: Brydon P. G. Wall, My Nguyen, J. Chuck Harrell, Mikhail G. Dozmorov
Categories: q-bio.GN cs.LG
Comments: Systematic review, one figure, three tables, 29 pages
\\
  Three-Dimensional (3D) chromatin interactions, such as enhancer-promoter
interactions (EPIs), loops, Topologically Associating Domains (TADs), and A/B
compartments play critical roles in a wide range of cellular processes by
regulating gene expression. Recent development of chromatin conformation
capture technologies has enabled genome-wide profiling of various 3D
structures, even with single cells. However, current catalogs of 3D structures
remain incomplete and unreliable due to differences in technology, tools, and
low data resolution. Machine learning methods have emerged as an alternative to
obtain missing 3D interactions and/or improve resolution. Such methods
frequently use genome annotation data (ChIP-seq, DNAse-seq, etc.), DNA
sequencing information (k-mers, Transcription Factor Binding Site (TFBS)
motifs), and other genomic properties to learn the associations between genomic
features and chromatin interactions. In this review, we discuss computational
tools for predicting three types of 3D interactions (EPIs, chromatin
interactions, TAD boundaries) and analyze their pros and cons. We also point
out obstacles of computational prediction of 3D interactions and suggest future
research directions.
\\ ( https://arxiv.org/abs/2403.03231 ,  928kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03233 (*cross-listing*)
Date: Mon, 4 Mar 2024 20:40:50 GMT   (5324kb,D)

Title: From Displacements to Distributions: A Machine-Learning Enabled
  Framework for Quantifying Uncertainties in Parameters of Computational Models
Authors: Taylor Roper and Harri Hakula and Troy Butler
Categories: stat.ML cs.LG
Comments: 35 pages
MSC-class: 28A50, 60-04, 60-08
\\
  This work presents novel extensions for combining two frameworks for
quantifying both aleatoric (i.e., irreducible) and epistemic (i.e., reducible)
sources of uncertainties in the modeling of engineered systems. The
data-consistent (DC) framework poses an inverse problem and solution for
quantifying aleatoric uncertainties in terms of pullback and push-forward
measures for a given Quantity of Interest (QoI) map. Unfortunately, a
pre-specified QoI map is not always available a priori to the collection of
data associated with system outputs. The data themselves are often polluted
with measurement errors (i.e., epistemic uncertainties), which complicates the
process of specifying a useful QoI. The Learning Uncertain Quantities (LUQ)
framework defines a formal three-step machine-learning enabled process for
transforming noisy datasets into samples of a learned QoI map to enable
DC-based inversion. We develop a robust filtering step in LUQ that can learn
the most useful quantitative information present in spatio-temporal datasets.
The learned QoI map transforms simulated and observed datasets into
distributions to perform DC-based inversion. We also develop a DC-based
inversion scheme that iterates over time as new spatial datasets are obtained
and utilizes quantitative diagnostics to identify both the quality and impact
of inversion at each iteration. Reproducing Kernel Hilbert Space theory is
leveraged to mathematically analyze the learned QoI map and develop a
quantitative sufficiency test for evaluating the filtered data. An illustrative
example is utilized throughout while the final two examples involve the
manufacturing of shells of revolution to demonstrate various aspects of the
presented frameworks.
\\ ( https://arxiv.org/abs/2403.03233 ,  5324kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03234 (*cross-listing*)
Date: Tue, 5 Mar 2024 01:42:51 GMT   (1838kb,D)

Title: Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling
Authors: Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and
  Volodymyr Kuleshov
Categories: q-bio.GN cs.LG
Comments: Code to reproduce our experiments is available at
  https://github.com/kuleshov-group/caduceus
\\
  Large-scale sequence modeling has sparked rapid advances that now extend into
biology and genomics. However, modeling genomic sequences introduces challenges
such as the need to model long-range token interactions, the effects of
upstream and downstream regions of the genome, and the reverse complementarity
(RC) of DNA. Here, we propose an architecture motivated by these challenges
that builds off the long-range Mamba block, and extends it to a BiMamba
component that supports bi-directionality, and to a MambaDNA block that
additionally supports RC equivariance. We use MambaDNA as the basis of
Caduceus, the first family of RC equivariant bi-directional long-range DNA
language models, and we introduce pre-training and fine-tuning strategies that
yield Caduceus DNA foundation models. Caduceus outperforms previous long-range
models on downstream benchmarks; on a challenging long-range variant effect
prediction task, Caduceus exceeds the performance of 10x larger models that do
not leverage bi-directionality or equivariance.
\\ ( https://arxiv.org/abs/2403.03234 ,  1838kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03240 (*cross-listing*)
Date: Tue, 5 Mar 2024 18:29:36 GMT   (38kb)

Title: Triple/Debiased Lasso for Statistical Inference of Conditional Average
  Treatment Effects
Authors: Masahiro Kato
Categories: stat.ME cs.LG econ.EM stat.ML
\\
  This study investigates the estimation and the statistical inference about
Conditional Average Treatment Effects (CATEs), which have garnered attention as
a metric representing individualized causal effects. In our data-generating
process, we assume linear models for the outcomes associated with binary
treatments and define the CATE as a difference between the expected outcomes of
these linear models. This study allows the linear models to be
high-dimensional, and our interest lies in consistent estimation and
statistical inference for the CATE. In high-dimensional linear regression, one
typical approach is to assume sparsity. However, in our study, we do not assume
sparsity directly. Instead, we consider sparsity only in the difference of the
linear models. We first use a doubly robust estimator to approximate this
difference and then regress the difference on covariates with Lasso
regularization. Although this regression estimator is consistent for the CATE,
we further reduce the bias using the techniques in double/debiased machine
learning (DML) and debiased Lasso, leading to $\sqrt{n}$-consistency and
confidence intervals. We refer to the debiased estimator as the triple/debiased
Lasso (TDL), applying both DML and debiased Lasso techniques. We confirm the
soundness of our proposed method through simulation studies.
\\ ( https://arxiv.org/abs/2403.03240 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03245 (*cross-listing*)
Date: Tue, 5 Mar 2024 19:00:01 GMT   (125kb,D)

Title: Neural Network Learning and Quantum Gravity
Authors: Stefano Lanza
Categories: hep-th cs.LG
Comments: 32 pages, 8 figures
\\
  The landscape of low-energy effective field theories stemming from string
theory is too vast for a systematic exploration. However, the meadows of the
string landscape may be fertile ground for the application of machine learning
techniques. Employing neural network learning may allow for inferring novel,
undiscovered properties that consistent theories in the landscape should
possess, or checking conjectural statements about alleged characteristics
thereof. The aim of this work is to describe to what extent the string
landscape can be explored with neural network-based learning. Our analysis is
motivated by recent studies that show that the string landscape is
characterized by finiteness properties, emerging from its underlying tame,
o-minimal structures. Indeed, employing these results, we illustrate that any
low-energy effective theory of string theory is endowed with certain
statistical learnability properties. Consequently, several learning problems
therein formulated, including interpolations and multi-class classification
problems, can be concretely addressed with machine learning, delivering results
with sufficiently high accuracy.
\\ ( https://arxiv.org/abs/2403.03245 ,  125kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03273 (*cross-listing*)
Date: Tue, 5 Mar 2024 19:13:45 GMT   (1599kb,D)

Title: DINOv2 based Self Supervised Learning For Few Shot Medical Image
  Segmentation
Authors: Lev Ayzenberg, Raja Giryes, Hayit Greenspan
Categories: cs.CV cs.LG
\\
  Deep learning models have emerged as the cornerstone of medical image
segmentation, but their efficacy hinges on the availability of extensive
manually labeled datasets and their adaptability to unforeseen categories
remains a challenge. Few-shot segmentation (FSS) offers a promising solution by
endowing models with the capacity to learn novel classes from limited labeled
examples. A leading method for FSS is ALPNet, which compares features between
the query image and the few available support segmented images. A key question
about using ALPNet is how to design its features. In this work, we delve into
the potential of using features from DINOv2, which is a foundational
self-supervised learning model in computer vision. Leveraging the strengths of
ALPNet and harnessing the feature extraction capabilities of DINOv2, we present
a novel approach to few-shot segmentation that not only enhances performance
but also paves the way for more robust and adaptable medical image analysis.
\\ ( https://arxiv.org/abs/2403.03273 ,  1599kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03295 (*cross-listing*)
Date: Tue, 5 Mar 2024 19:49:44 GMT   (25kb)

Title: Proper vs Improper Quantum PAC learning
Authors: Ashwin Nayak, Pulkit Sinha
Categories: quant-ph cs.CC cs.LG
Comments: 23 Pages
\\
  A basic question in the PAC model of learning is whether proper learning is
harder than improper learning. In the classical case, there are examples of
concept classes with VC dimension $d$ that have sample complexity
$\Omega\left(\frac d\epsilon\log\frac1\epsilon\right)$ for proper learning with
error $\epsilon$, while the complexity for improper learning is O$\!\left(\frac
d\epsilon\right)$. One such example arises from the Coupon Collector problem.
  Motivated by the efficiency of proper versus improper learning with quantum
samples, Arunachalam, Belovs, Childs, Kothari, Rosmanis, and de Wolf (TQC 2020)
studied an analogue, the Quantum Coupon Collector problem. Curiously, they
discovered that for learning size $k$ subsets of $[n]$ the problem has sample
complexity $\Theta(k\log\min\{k,n-k+1\})$, in contrast with the complexity of
$\Theta(k\log k)$ for Coupon Collector. This effectively negates the
possibility of a separation between the two modes of learning via the quantum
problem, and Arunachalam et al.\ posed the possibility of such a separation as
an open question.
  In this work, we first present an algorithm for the Quantum Coupon Collector
problem with sample complexity that matches the sharper lower bound of
$(1-o_k(1))k\ln\min\{k,n-k+1\}$ shown recently by Bab Hadiashar, Nayak, and
Sinha (IEEE TIT 2024), for the entire range of the parameter $k$. Next, we
devise a variant of the problem, the Quantum Padded Coupon Collector. We prove
that its sample complexity matches that of the classical Coupon Collector
problem for both modes of learning, thereby exhibiting the same asymptotic
separation between proper and improper quantum learning as mentioned above. The
techniques we develop in the process can be directly applied to any form of
padded quantum data. We hope that padding can more generally lift other forms
of classical learning behaviour to the quantum setting.
\\ ( https://arxiv.org/abs/2403.03295 ,  25kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03310 (*cross-listing*)
Date: Tue, 5 Mar 2024 20:23:25 GMT   (2166kb,D)

Title: Graph Learning for Parameter Prediction of Quantum Approximate
  Optimization Algorithm
Authors: Zhiding Liang, Gang Liu, Zheyuan Liu, Jinglei Cheng, Tianyi Hao,
  Kecheng Liu, Hang Ren, Zhixin Song, Ji Liu, Fanny Ye, Yiyu Shi
Categories: quant-ph cs.LG
\\
  In recent years, quantum computing has emerged as a transformative force in
the field of combinatorial optimization, offering novel approaches to tackling
complex problems that have long challenged classical computational methods.
Among these, the Quantum Approximate Optimization Algorithm (QAOA) stands out
for its potential to efficiently solve the Max-Cut problem, a quintessential
example of combinatorial optimization. However, practical application faces
challenges due to current limitations on quantum computational resource. Our
work optimizes QAOA initialization, using Graph Neural Networks (GNN) as a
warm-start technique. This sacrifices affordable computational resource on
classical computer to reduce quantum computational resource overhead, enhancing
QAOA's effectiveness. Experiments with various GNN architectures demonstrate
the adaptability and stability of our framework, highlighting the synergy
between quantum algorithms and machine learning. Our findings show GNN's
potential in improving QAOA performance, opening new avenues for hybrid
quantum-classical approaches in quantum computing and contributing to practical
applications.
\\ ( https://arxiv.org/abs/2403.03310 ,  2166kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03314 (*cross-listing*)
Date: Tue, 5 Mar 2024 20:36:26 GMT   (954kb,D)

Title: Collision Avoidance Verification of Multiagent Systems with Learned
  Policies
Authors: Zihao Dong, Shayegan Omidshafiei, Michael Everett
Categories: eess.SY cs.LG cs.MA cs.RO cs.SY
Comments: 6 pages, 6 figures
\\
  For many multiagent control problems, neural networks (NNs) have enabled
promising new capabilities. However, many of these systems lack formal
guarantees (e.g., collision avoidance, robustness), which prevents leveraging
these advances in safety-critical settings. While there is recent work on
formal verification of NN-controlled systems, most existing techniques cannot
handle scenarios with more than one agent. To address this research gap, this
paper presents a backward reachability-based approach for verifying the
collision avoidance properties of Multi-Agent Neural Feedback Loops (MA-NFLs).
Given the dynamics models and trained control policies of each agent, the
proposed algorithm computes relative backprojection sets by solving a series of
Mixed Integer Linear Programs (MILPs) offline for each pair of agents. Our
pair-wise approach is parallelizable and thus scales well with increasing
number of agents, and we account for state measurement uncertainties, making it
well aligned with real-world scenarios. Using those results, the agents can
quickly check for collision avoidance online by solving low-dimensional Linear
Programs (LPs). We demonstrate the proposed algorithm can verify collision-free
properties of a MA-NFL with agents trained to imitate a collision avoidance
algorithm (Reciprocal Velocity Obstacles). We further demonstrate the
computational scalability of the approach on systems with up to 10 agents.
\\ ( https://arxiv.org/abs/2403.03314 ,  954kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03353 (*cross-listing*)
Date: Tue, 5 Mar 2024 22:42:29 GMT   (27kb)

Title: Hypothesis Spaces for Deep Learning
Authors: Rui Wang, Yuesheng Xu, Mingsong Yan
Categories: stat.ML cs.LG math.FA
\\
  This paper introduces a hypothesis space for deep learning that employs deep
neural networks (DNNs). By treating a DNN as a function of two variables, the
physical variable and parameter variable, we consider the primitive set of the
DNNs for the parameter variable located in a set of the weight matrices and
biases determined by a prescribed depth and widths of the DNNs. We then
complete the linear span of the primitive DNN set in a weak* topology to
construct a Banach space of functions of the physical variable. We prove that
the Banach space so constructed is a reproducing kernel Banach space (RKBS) and
construct its reproducing kernel. We investigate two learning models,
regularized learning and minimum interpolation problem in the resulting RKBS,
by establishing representer theorems for solutions of the learning models. The
representer theorems unfold that solutions of these learning models can be
expressed as linear combination of a finite number of kernel sessions
determined by given data and the reproducing kernel.
\\ ( https://arxiv.org/abs/2403.03353 ,  27kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03361 (*cross-listing*)
Date: Tue, 5 Mar 2024 23:08:18 GMT   (85kb)

Title: Chained Information-Theoretic bounds and Tight Regret Rate for Linear
  Bandit Problems
Authors: Amaury Gouverneur, Borja Rodr\'iguez-G\'alvez, Tobias J. Oechtering,
  Mikael Skoglund
Categories: stat.ML cs.LG
Comments: 15 pages: 8 of main text and 7 of appendices
\\
  This paper studies the Bayesian regret of a variant of the Thompson-Sampling
algorithm for bandit problems. It builds upon the information-theoretic
framework of [Russo and Van Roy, 2015] and, more specifically, on the
rate-distortion analysis from [Dong and Van Roy, 2020], where they proved a
bound with regret rate of $O(d\sqrt{T \log(T)})$ for the $d$-dimensional linear
bandit setting. We focus on bandit problems with a metric action space and,
using a chaining argument, we establish new bounds that depend on the metric
entropy of the action space for a variant of Thompson-Sampling.
  Under suitable continuity assumption of the rewards, our bound offers a tight
rate of $O(d\sqrt{T})$ for $d$-dimensional linear bandit problems.
\\ ( https://arxiv.org/abs/2403.03361 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03390 (*cross-listing*)
Date: Wed, 6 Mar 2024 00:59:51 GMT   (20515kb,D)

Title: Performance Evaluation of Semi-supervised Learning Frameworks for
  Multi-Class Weed Detection
Authors: Jiajia Li, Dong Chen, Xunyuan Yin, and Zhaojian Li
Categories: cs.CV cs.LG eess.IV
Comments: 11 pages, 7 figures
\\
  Effective weed control plays a crucial role in optimizing crop yield and
enhancing agricultural product quality. However, the reliance on herbicide
application not only poses a critical threat to the environment but also
promotes the emergence of resistant weeds. Fortunately, recent advances in
precision weed management enabled by ML and DL provide a sustainable
alternative. Despite great progress, existing algorithms are mainly developed
based on supervised learning approaches, which typically demand large-scale
datasets with manual-labeled annotations, which is time-consuming and
labor-intensive. As such, label-efficient learning methods, especially
semi-supervised learning, have gained increased attention in the broader domain
of computer vision and have demonstrated promising performance. These methods
aim to utilize a small number of labeled data samples along with a great number
of unlabeled samples to develop high-performing models comparable to the
supervised learning counterpart trained on a large amount of labeled data
samples. In this study, we assess the effectiveness of a semi-supervised
learning framework for multi-class weed detection, employing two well-known
object detection frameworks, namely FCOS and Faster-RCNN. Specifically, we
evaluate a generalized student-teacher framework with an improved pseudo-label
generation module to produce reliable pseudo-labels for the unlabeled data. To
enhance generalization, an ensemble student network is employed to facilitate
the training process. Experimental results show that the proposed approach is
able to achieve approximately 76\% and 96\% detection accuracy as the
supervised methods with only 10\% of labeled data in CottenWeedDet3 and
CottonWeedDet12, respectively. We offer access to the source code, contributing
a valuable resource for ongoing semi-supervised learning research in weed
detection and beyond.
\\ ( https://arxiv.org/abs/2403.03390 ,  20515kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03391 (*cross-listing*)
Date: Tue, 5 Mar 2024 16:55:06 GMT   (407kb,D)

Title: CoRMF: Criticality-Ordered Recurrent Mean Field Ising Solver
Authors: Zhenyu Pan, Ammar Gilani, En-Jui Kuo, Zhuo Liu
Categories: stat.ML cond-mat.stat-mech cs.LG
\\
  We propose an RNN-based efficient Ising model solver, the Criticality-ordered
Recurrent Mean Field (CoRMF), for forward Ising problems. In its core, a
criticality-ordered spin sequence of an $N$-spin Ising model is introduced by
sorting mission-critical edges with greedy algorithm, such that an
autoregressive mean-field factorization can be utilized and optimized with
Recurrent Neural Networks (RNNs). Our method has two notable characteristics:
(i) by leveraging the approximated tree structure of the underlying Ising
graph, the newly-obtained criticality order enables the unification between
variational mean-field and RNN, allowing the generally intractable Ising model
to be efficiently probed with probabilistic inference; (ii) it is
well-modulized, model-independent while at the same time expressive enough, and
hence fully applicable to any forward Ising inference problems with minimal
effort. Computationally, by using a variance-reduced Monte Carlo gradient
estimator, CoRFM solves the Ising problems in a self-train fashion without
data/evidence, and the inference tasks can be executed by directly sampling
from RNN. Theoretically, we establish a provably tighter error bound than naive
mean-field by using the matrix cut decomposition machineries. Numerically, we
demonstrate the utility of this framework on a series of Ising datasets.
\\ ( https://arxiv.org/abs/2403.03391 ,  407kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03427 (*cross-listing*)
Date: Wed, 6 Mar 2024 03:16:47 GMT   (1474kb,D)

Title: Single Transit Detection In Kepler With Machine Learning And Onboard
  Spacecraft Diagnostics
Authors: Matthew T. Hansen and Jason A. Dittmann
Categories: astro-ph.EP astro-ph.IM cs.LG
Comments: 23 pages, 23 figures, submitted to AJ
\\
  Exoplanet discovery at long orbital periods requires reliably detecting
individual transits without additional information about the system. Techniques
like phase-folding of light curves and periodogram analysis of radial velocity
data are more sensitive to planets with shorter orbital periods, leaving a
dearth of planet discoveries at long periods. We present a novel technique
using an ensemble of Convolutional Neural Networks incorporating the onboard
spacecraft diagnostics of \emph{Kepler} to classify transits within a light
curve. We create a pipeline to recover the location of individual transits, and
the period of the orbiting planet, which maintains $>80\%$ transit recovery
sensitivity out to an 800-day orbital period. Our neural network pipeline has
the potential to discover additional planets in the \emph{Kepler} dataset, and
crucially, within the $\eta$-Earth regime. We report our first candidate from
this pipeline, KOI 1271.02. KOI 1271.01 is known to exhibit strong Transit
Timing Variations (TTVs), and so we jointly model the TTVs and transits of both
transiting planets to constrain the orbital configuration and planetary
parameters and conclude with a series of potential parameters for KOI 1271.02,
as there is not enough data currently to uniquely constrain the system. We
conclude that KOI 1271.02 has a radius of 5.32 $\pm$ 0.20 $R_{\oplus}$ and a
mass of $28.94^{0.23}_{-0.47}$ $M_{\oplus}$. Future constraints on the nature
of KOI 1271.02 require measuring additional TTVs of KOI 1271.01 or observing a
second transit of KOI 1271.02.
\\ ( https://arxiv.org/abs/2403.03427 ,  1474kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03434 (*cross-listing*)
Date: Wed, 6 Mar 2024 03:36:07 GMT   (698kb)

Title: An AI-enabled Agent-Based Model and Its Application in Measles Outbreak
  Simulation for New Zealand
Authors: Sijin Zhang, Alvaro Orsi, Richard Dean, Lei Chen, Rachel Qiu, Jiawei
  Zhao
Categories: cs.MA cs.CY cs.LG
Comments: 11 pages, 9 figures
\\
  Agent Based Models (ABMs) have emerged as a powerful tool for investigating
complex social interactions, particularly in the context of public health and
infectious disease investigation. In an effort to enhance the conventional ABM,
enabling automated model calibration and reducing the computational resources
needed for scaling up the model, we have developed a tensorized and
differentiable agent-based model by coupling Graph Neural Network (GNN) and
Long Short-Term Memory (LSTM) network. The model was employed to investigate
the 2019 measles outbreak occurred in New Zealand, demonstrating a promising
ability to accurately simulate the outbreak dynamics, particularly during the
peak period of repeated cases. This paper shows that by leveraging the latest
Artificial Intelligence (AI) technology and the capabilities of traditional
ABMs, we gain deeper insights into the dynamics of infectious disease
outbreaks. This, in turn, helps us make more informed decision when developing
effective strategies that strike a balance between managing outbreaks and
minimizing disruptions to everyday life.
\\ ( https://arxiv.org/abs/2403.03434 ,  698kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03449 (*cross-listing*)
Date: Wed, 6 Mar 2024 04:27:10 GMT   (43407kb,D)

Title: SalienTime: User-driven Selection of Salient Time Steps for Large-Scale
  Geospatial Data Visualization
Authors: Juntong Chen, Haiwen Huang, Huayuan Ye, Zhong Peng, Chenhui Li,
  Changbo Wang
Categories: cs.HC cs.LG
Comments: In Proceedings of the CHI Conference on Human Factors in Computing
  Systems (CHI'24), May 11-16, 2024, Honolulu, HI, USA
DOI: 10.1145/3613904.3642944
\\
  The voluminous nature of geospatial temporal data from physical monitors and
simulation models poses challenges to efficient data access, often resulting in
cumbersome temporal selection experiences in web-based data portals. Thus,
selecting a subset of time steps for prioritized visualization and pre-loading
is highly desirable. Addressing this issue, this paper establishes a
multifaceted definition of salient time steps via extensive need-finding
studies with domain experts to understand their workflows. Building on this, we
propose a novel approach that leverages autoencoders and dynamic programming to
facilitate user-driven temporal selections. Structural features, statistical
variations, and distance penalties are incorporated to make more flexible
selections. User-specified priorities, spatial regions, and aggregations are
used to combine different perspectives. We design and implement a web-based
interface to enable efficient and context-aware selection of time steps and
evaluate its efficacy and usability through case studies, quantitative
evaluations, and expert interviews.
\\ ( https://arxiv.org/abs/2403.03449 ,  43407kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03458 (*cross-listing*)
Date: Wed, 6 Mar 2024 04:49:02 GMT   (695kb,D)

Title: Slot Abstractors: Toward Scalable Abstract Visual Reasoning
Authors: Shanka Subhra Mondal, Jonathan D. Cohen, Taylor W. Webb
Categories: cs.CV cs.LG
Comments: 15 pages, 5 figures
\\
  Abstract visual reasoning is a characteristically human ability, allowing the
identification of relational patterns that are abstracted away from object
features, and the systematic generalization of those patterns to unseen
problems. Recent work has demonstrated strong systematic generalization in
visual reasoning tasks involving multi-object inputs, through the integration
of slot-based methods used for extracting object-centric representations
coupled with strong inductive biases for relational abstraction. However, this
approach was limited to problems containing a single rule, and was not scalable
to visual reasoning problems containing a large number of objects. Other recent
work proposed Abstractors, an extension of Transformers that incorporates
strong relational inductive biases, thereby inheriting the Transformer's
scalability and multi-head architecture, but it has yet to be demonstrated how
this approach might be applied to multi-object visual inputs. Here we combine
the strengths of the above approaches and propose Slot Abstractors, an approach
to abstract visual reasoning that can be scaled to problems involving a large
number of objects and multiple relations among them. The approach displays
state-of-the-art performance across four abstract visual reasoning tasks.
\\ ( https://arxiv.org/abs/2403.03458 ,  695kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03459 (*cross-listing*)
Date: Wed, 6 Mar 2024 04:49:18 GMT   (12970kb,D)

Title: TGPT-PINN: Nonlinear model reduction with transformed GPT-PINNs
Authors: Yanlai Chen, Yajie Ji, Akil Narayan, Zhenli Xu
Categories: math.NA cs.LG cs.NA
\\
  We introduce the Transformed Generative Pre-Trained Physics-Informed Neural
Networks (TGPT-PINN) for accomplishing nonlinear model order reduction (MOR) of
transport-dominated partial differential equations in an MOR-integrating PINNs
framework. Building on the recent development of the GPT-PINN that is a
network-of-networks design achieving snapshot-based model reduction, we design
and test a novel paradigm for nonlinear model reduction that can effectively
tackle problems with parameter-dependent discontinuities. Through incorporation
of a shock-capturing loss function component as well as a parameter-dependent
transform layer, the TGPT-PINN overcomes the limitations of linear model
reduction in the transport-dominated regime. We demonstrate this new capability
for nonlinear model reduction in the PINNs framework by several nontrivial
parametric partial differential equations.
\\ ( https://arxiv.org/abs/2403.03459 ,  12970kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03462 (*cross-listing*)
Date: Wed, 6 Mar 2024 04:55:39 GMT   (1215kb,D)

Title: Interactive Continual Learning Architecture for Long-Term
  Personalization of Home Service Robots
Authors: Ali Ayub, Chrystopher Nehaniv, Kerstin Dautenhahn
Categories: cs.RO cs.CV cs.LG
Comments: Accepted at the IEEE International Conference on Robotics and
  Automation (ICRA), 2024
\\
  For robots to perform assistive tasks in unstructured home environments, they
must learn and reason on the semantic knowledge of the environments. Despite a
resurgence in the development of semantic reasoning architectures, these
methods assume that all the training data is available a priori. However, each
user's environment is unique and can continue to change over time, which makes
these methods unsuitable for personalized home service robots. Although
research in continual learning develops methods that can learn and adapt over
time, most of these methods are tested in the narrow context of object
classification on static image datasets. In this paper, we combine ideas from
continual learning, semantic reasoning, and interactive machine learning
literature and develop a novel interactive continual learning architecture for
continual learning of semantic knowledge in a home environment through
human-robot interaction. The architecture builds on core cognitive principles
of learning and memory for efficient and real-time learning of new knowledge
from humans. We integrate our architecture with a physical mobile manipulator
robot and perform extensive system evaluations in a laboratory environment over
two months. Our results demonstrate the effectiveness of our architecture to
allow a physical robot to continually adapt to the changes in the environment
from limited data provided by the users (experimenters), and use the learned
knowledge to perform object fetching tasks.
\\ ( https://arxiv.org/abs/2403.03462 ,  1215kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03526 (*cross-listing*)
Date: Wed, 6 Mar 2024 08:05:53 GMT   (3961kb,D)

Title: FingerNet: EEG Decoding of A Fine Motor Imagery with Finger-tapping Task
  Based on A Deep Neural Network
Authors: Young-Min Go, Seong-Hyun Yu, Hyeong-Yeong Park, Minji Lee, and Ji-Hoon
  Jeong
Categories: eess.SP cs.LG q-bio.NC
Comments: 12 pages,5 figures, and 2 tables
\\
  Brain-computer interface (BCI) technology facilitates communication between
the human brain and computers, primarily utilizing electroencephalography (EEG)
signals to discern human intentions. Although EEG-based BCI systems have been
developed for paralysis individuals, ongoing studies explore systems for speech
imagery and motor imagery (MI). This study introduces FingerNet, a specialized
network for fine MI classification, departing from conventional gross MI
studies. The proposed FingerNet could extract spatial and temporal features
from EEG signals, improving classification accuracy within the same hand. The
experimental results demonstrated that performance showed significantly higher
accuracy in classifying five finger-tapping tasks, encompassing thumb, index,
middle, ring, and little finger movements. FingerNet demonstrated dominant
performance compared to the conventional baseline models, EEGNet and
DeepConvNet. The average accuracy for FingerNet was 0.3049, whereas EEGNet and
DeepConvNet exhibited lower accuracies of 0.2196 and 0.2533, respectively.
Statistical validation also demonstrates the predominance of FingerNet over
baseline networks. For biased predictions, particularly for thumb and index
classes, we led to the implementation of weighted cross-entropy and also
adapted the weighted cross-entropy, a method conventionally employed to
mitigate class imbalance. The proposed FingerNet involves optimizing network
structure, improving performance, and exploring applications beyond fine MI.
Moreover, the weighted Cross Entropy approach employed to address such biased
predictions appears to have broader applicability and relevance across various
domains involving multi-class classification tasks. We believe that effective
execution of motor imagery can be achieved not only for fine MI, but also for
local muscle MI
\\ ( https://arxiv.org/abs/2403.03526 ,  3961kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03535 (*cross-listing*)
Date: Wed, 6 Mar 2024 08:29:45 GMT   (702kb,D)

Title: Task Attribute Distance for Few-Shot Learning: Theoretical Analysis and
  Applications
Authors: Minyang Hu, Hong Chang, Zong Guo, Bingpeng Ma, Shiguan Shan and Xilin
  Chen
Categories: cs.CV cs.LG
\\
  Few-shot learning (FSL) aims to learn novel tasks with very few labeled
samples by leveraging experience from \emph{related} training tasks. In this
paper, we try to understand FSL by delving into two key questions: (1) How to
quantify the relationship between \emph{training} and \emph{novel} tasks? (2)
How does the relationship affect the \emph{adaptation difficulty} on novel
tasks for different models? To answer the two questions, we introduce Task
Attribute Distance (TAD) built upon attributes as a metric to quantify the task
relatedness. Unlike many existing metrics, TAD is model-agnostic, making it
applicable to different FSL models. Then, we utilize TAD metric to establish a
theoretical connection between task relatedness and task adaptation difficulty.
By deriving the generalization error bound on a novel task, we discover how TAD
measures the adaptation difficulty on novel tasks for FSL models. To validate
our TAD metric and theoretical findings, we conduct experiments on three
benchmarks. Our experimental results confirm that TAD metric effectively
quantifies the task relatedness and reflects the adaptation difficulty on novel
tasks for various FSL methods, even if some of them do not learn attributes
explicitly or human-annotated attributes are not available. Finally, we present
two applications of the proposed TAD metric: data augmentation and test-time
intervention, which further verify its effectiveness and general applicability.
The source code is available at https://github.com/hu-my/TaskAttributeDistance.
\\ ( https://arxiv.org/abs/2403.03535 ,  702kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03539 (*cross-listing*)
Date: Wed, 6 Mar 2024 08:35:29 GMT   (4500kb,D)

Title: Gadolinium dose reduction for brain MRI using conditional deep learning
Authors: Thomas Pinetz, Erich Kobler, Robert Haase, Julian A. Luetkens, Mathias
  Meetschen, Johannes Haubold, Cornelius Deuschl, Alexander Radbruch, Katerina
  Deike, Alexander Effland
Categories: eess.IV cs.CV cs.LG
\\
  Recently, deep learning (DL)-based methods have been proposed for the
computational reduction of gadolinium-based contrast agents (GBCAs) to mitigate
adverse side effects while preserving diagnostic value. Currently, the two main
challenges for these approaches are the accurate prediction of contrast
enhancement and the synthesis of realistic images. In this work, we address
both challenges by utilizing the contrast signal encoded in the subtraction
images of pre-contrast and post-contrast image pairs. To avoid the synthesis of
any noise or artifacts and solely focus on contrast signal extraction and
enhancement from low-dose subtraction images, we train our DL model using
noise-free standard-dose subtraction images as targets. As a result, our model
predicts the contrast enhancement signal only; thereby enabling synthesization
of images beyond the standard dose. Furthermore, we adapt the embedding idea of
recent diffusion-based models to condition our model on physical parameters
affecting the contrast enhancement behavior. We demonstrate the effectiveness
of our approach on synthetic and real datasets using various scanners, field
strengths, and contrast agents.
\\ ( https://arxiv.org/abs/2403.03539 ,  4500kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03545 (*cross-listing*)
Date: Wed, 6 Mar 2024 08:47:31 GMT   (370kb,D)

Title: Diffusion-based Generative Prior for Low-Complexity MIMO Channel
  Estimation
Authors: Benedikt Fesl and Michael Baur and Florian Strasser and Michael Joham
  and Wolfgang Utschick
Categories: eess.SP cs.LG
\\
  This work proposes a novel channel estimator based on diffusion models (DMs),
one of the currently top-rated generative models. Contrary to related works
utilizing generative priors, a lightweight convolutional neural network (CNN)
with positional embedding of the signal-to-noise ratio (SNR) information is
designed by learning the channel distribution in the sparse angular domain.
Combined with an estimation strategy that avoids stochastic resampling and
truncates reverse diffusion steps that account for lower SNR than the given
pilot observation, the resulting DM estimator has both low complexity and
memory overhead. Numerical results exhibit better performance than
state-of-the-art channel estimators utilizing generative priors.
\\ ( https://arxiv.org/abs/2403.03545 ,  370kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03551 (*cross-listing*)
Date: Wed, 6 Mar 2024 08:51:09 GMT   (4144kb,D)

Title: Low-Dose CT Image Reconstruction by Fine-Tuning a UNet Pretrained for
  Gaussian Denoising for the Downstream Task of Image Enhancement
Authors: Tim Selig, Thomas M\"arz, Martin Storath, Andreas Weinmann
Categories: eess.IV cs.CV cs.LG
Comments: 10 pages, 4 figures
\\
  Computed Tomography (CT) is a widely used medical imaging modality, and as it
is based on ionizing radiation, it is desirable to minimize the radiation dose.
However, a reduced radiation dose comes with reduced image quality, and
reconstruction from low-dose CT (LDCT) data is still a challenging task which
is subject to research. According to the LoDoPaB-CT benchmark, a benchmark for
LDCT reconstruction, many state-of-the-art methods use pipelines involving
UNet-type architectures. Specifically the top ranking method, ItNet, employs a
three-stage process involving filtered backprojection (FBP), a UNet trained on
CT data, and an iterative refinement step. In this paper, we propose a less
complex two-stage method. The first stage also employs FBP, while the novelty
lies in the training strategy for the second stage, characterized as the CT
image enhancement stage. The crucial point of our approach is that the neural
network is pretrained on a distinctly different pretraining task with non-CT
data, namely Gaussian noise removal on a variety of natural grayscale images
(photographs). We then fine-tune this network for the downstream task of CT
image enhancement using pairs of LDCT images and corresponding normal-dose CT
images (NDCT). Despite being notably simpler than the state-of-the-art, as the
pretraining did not depend on domain-specific CT data and no further iterative
refinement step was necessary, the proposed two-stage method achieves
competitive results. The proposed method achieves a shared top ranking in the
LoDoPaB-CT challenge and a first position with respect to the SSIM metric.
\\ ( https://arxiv.org/abs/2403.03551 ,  4144kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03552 (*cross-listing*)
Date: Wed, 6 Mar 2024 08:55:34 GMT   (6746kb,D)

Title: Population-aware Online Mirror Descent for Mean-Field Games by Deep
  Reinforcement Learning
Authors: Zida Wu, Mathieu Lauriere, Samuel Jia Cong Chua, Matthieu Geist,
  Olivier Pietquin, Ankur Mehta
Categories: cs.GT cs.LG cs.MA cs.SY eess.SY
\\
  Mean Field Games (MFGs) have the ability to handle large-scale multi-agent
systems, but learning Nash equilibria in MFGs remains a challenging task. In
this paper, we propose a deep reinforcement learning (DRL) algorithm that
achieves population-dependent Nash equilibrium without the need for averaging
or sampling from history, inspired by Munchausen RL and Online Mirror Descent.
Through the design of an additional inner-loop replay buffer, the agents can
effectively learn to achieve Nash equilibrium from any distribution, mitigating
catastrophic forgetting. The resulting policy can be applied to various initial
distributions. Numerical experiments on four canonical examples demonstrate our
algorithm has better convergence properties than SOTA algorithms, in particular
a DRL version of Fictitious Play for population-dependent policies.
\\ ( https://arxiv.org/abs/2403.03552 ,  6746kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03563 (*cross-listing*)
Date: Wed, 6 Mar 2024 09:15:53 GMT   (7762kb,D)

Title: Multimodal Anomaly Detection based on Deep Auto-Encoder for Object Slip
  Perception of Mobile Manipulation Robots
Authors: Youngjae Yoo, Chung-Yeon Lee, and Byoung-Tak Zhang
Categories: cs.RO cs.LG
DOI: 10.1109/ICRA48506.2021.9561586
\\
  Object slip perception is essential for mobile manipulation robots to perform
manipulation tasks reliably in the dynamic real-world. Traditional approaches
to robot arms' slip perception use tactile or vision sensors. However, mobile
robots still have to deal with noise in their sensor signals caused by the
robot's movement in a changing environment. To solve this problem, we present
an anomaly detection method that utilizes multisensory data based on a deep
autoencoder model. The proposed framework integrates heterogeneous data streams
collected from various robot sensors, including RGB and depth cameras, a
microphone, and a force-torque sensor. The integrated data is used to train a
deep autoencoder to construct latent representations of the multisensory data
that indicate the normal status. Anomalies can then be identified by error
scores measured by the difference between the trained encoder's latent values
and the latent values of reconstructed input data. In order to evaluate the
proposed framework, we conducted an experiment that mimics an object slip by a
mobile service robot operating in a real-world environment with diverse
household objects and different moving patterns. The experimental results
verified that the proposed framework reliably detects anomalies in object slip
situations despite various object types and robot behaviors, and visual and
auditory noise in the environment.
\\ ( https://arxiv.org/abs/2403.03563 ,  7762kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03589 (*cross-listing*)
Date: Wed, 6 Mar 2024 10:24:44 GMT   (951kb,D)

Title: Active Adaptive Experimental Design for Treatment Effect Estimation with
  Covariate Choices
Authors: Masahiro Kato and Akihiro Oga and Wataru Komatsubara and Ryo Inokuchi
Categories: stat.ME cs.LG econ.EM stat.ML
Comments: This paper was submitted to ICML 2024 on February 1st, 2024, and is
  currently under review
\\
  This study designs an adaptive experiment for efficiently estimating average
treatment effect (ATEs). We consider an adaptive experiment where an
experimenter sequentially samples an experimental unit from a covariate density
decided by the experimenter and assigns a treatment. After assigning a
treatment, the experimenter observes the corresponding outcome immediately. At
the end of the experiment, the experimenter estimates an ATE using gathered
samples. The objective of the experimenter is to estimate the ATE with a
smaller asymptotic variance. Existing studies have designed experiments that
adaptively optimize the propensity score (treatment-assignment probability). As
a generalization of such an approach, we propose a framework under which an
experimenter optimizes the covariate density, as well as the propensity score,
and find that optimizing both covariate density and propensity score reduces
the asymptotic variance more than optimizing only the propensity score. Based
on this idea, in each round of our experiment, the experimenter optimizes the
covariate density and propensity score based on past observations. To design an
adaptive experiment, we first derive the efficient covariate density and
propensity score that minimizes the semiparametric efficiency bound, a lower
bound for the asymptotic variance given a fixed covariate density and a fixed
propensity score. Next, we design an adaptive experiment using the efficient
covariate density and propensity score sequentially estimated during the
experiment. Lastly, we propose an ATE estimator whose asymptotic variance
aligns with the minimized semiparametric efficiency bound.
\\ ( https://arxiv.org/abs/2403.03589 ,  951kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03590 (*cross-listing*)
Date: Wed, 6 Mar 2024 10:24:47 GMT   (2396kb,D)

Title: DeepEclipse: How to Break White-Box DNN-Watermarking Schemes
Authors: Alessandro Pegoraro, Carlotta Segna, Kavita Kumari, Ahmad-Reza Sadeghi
Categories: cs.CR cs.LG
Comments: To appear in the 33rd USENIX Security Symposium, August 2024,
  Philadelphia, PA, USA. 18 pages, 7 figures, 4 tables, 5 algorithms, 13
  equations
\\
  Deep Learning (DL) models have become crucial in digital transformation, thus
raising concerns about their intellectual property rights. Different
watermarking techniques have been developed to protect Deep Neural Networks
(DNNs) from IP infringement, creating a competitive field for DNN watermarking
and removal methods. The predominant watermarking schemes use white-box
techniques, which involve modifying weights by adding a unique signature to
specific DNN layers. On the other hand, existing attacks on white-box
watermarking usually require knowledge of the specific deployed watermarking
scheme or access to the underlying data for further training and fine-tuning.
We propose DeepEclipse, a novel and unified framework designed to remove
white-box watermarks. We present obfuscation techniques that significantly
differ from the existing white-box watermarking removal schemes. DeepEclipse
can evade watermark detection without prior knowledge of the underlying
watermarking scheme, additional data, or training and fine-tuning. Our
evaluation reveals that DeepEclipse excels in breaking multiple white-box
watermarking schemes, reducing watermark detection to random guessing while
maintaining a similar model accuracy as the original one. Our framework
showcases a promising solution to address the ongoing DNN watermark protection
and removal challenges.
\\ ( https://arxiv.org/abs/2403.03590 ,  2396kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03642 (*cross-listing*)
Date: Wed, 6 Mar 2024 12:02:07 GMT   (34715kb,D)

Title: Generative Active Learning with Variational Autoencoder for Radiology
  Data Generation in Veterinary Medicine
Authors: In-Gyu Lee, Jun-Young Oh, Hee-Jung Yu, Jae-Hwan Kim, Ki-Dong Eom,
  Ji-Hoon Jeong
Categories: eess.IV cs.CV cs.LG
\\
  Recently, with increasing interest in pet healthcare, the demand for
computer-aided diagnosis (CAD) systems in veterinary medicine has increased.
The development of veterinary CAD has stagnated due to a lack of sufficient
radiology data. To overcome the challenge, we propose a generative active
learning framework based on a variational autoencoder. This approach aims to
alleviate the scarcity of reliable data for CAD systems in veterinary medicine.
This study utilizes datasets comprising cardiomegaly radiograph data. After
removing annotations and standardizing images, we employed a framework for data
augmentation, which consists of a data generation phase and a query phase for
filtering the generated data. The experimental results revealed that as the
data generated through this framework was added to the training data of the
generative model, the frechet inception distance consistently decreased from
84.14 to 50.75 on the radiograph. Subsequently, when the generated data were
incorporated into the training of the classification model, the false positive
of the confusion matrix also improved from 0.16 to 0.66 on the radiograph. The
proposed framework has the potential to address the challenges of data scarcity
in medical CAD, contributing to its advancement.
\\ ( https://arxiv.org/abs/2403.03642 ,  34715kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03664 (*cross-listing*)
Date: Wed, 6 Mar 2024 12:34:50 GMT   (32449kb,D)

Title: Environmental Insights: Democratizing Access to Ambient Air Pollution
  Data and Predictive Analytics with an Open-Source Python Package
Authors: Liam J Berrisford, Ronaldo Menezes
Categories: physics.soc-ph cs.LG
Comments: 16 pages, 8 figures, 1 table
\\
  Ambient air pollution is a pervasive issue with wide-ranging effects on human
health, ecosystem vitality, and economic structures. Utilizing data on ambient
air pollution concentrations, researchers can perform comprehensive analyses to
uncover the multifaceted impacts of air pollution across society. To this end,
we introduce Environmental Insights, an open-source Python package designed to
democratize access to air pollution concentration data. This tool enables users
to easily retrieve historical air pollution data and employ a Machine Learning
model for forecasting potential future conditions. Moreover, Environmental
Insights includes a suite of tools aimed at facilitating the dissemination of
analytical findings and enhancing user engagement through dynamic
visualizations. This comprehensive approach ensures that the package caters to
the diverse needs of individuals looking to explore and understand air
pollution trends and their implications.
\\ ( https://arxiv.org/abs/2403.03664 ,  32449kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03669 (*cross-listing*)
Date: Wed, 6 Mar 2024 12:43:53 GMT   (27kb,D)

Title: Spectral Algorithms on Manifolds through Diffusion
Authors: Weichun Xia and Lei Shi
Categories: stat.ML cs.LG
\\
  The existing research on spectral algorithms, applied within a Reproducing
Kernel Hilbert Space (RKHS), has primarily focused on general kernel functions,
often neglecting the inherent structure of the input feature space. Our paper
introduces a new perspective, asserting that input data are situated within a
low-dimensional manifold embedded in a higher-dimensional Euclidean space. We
study the convergence performance of spectral algorithms in the RKHSs,
specifically those generated by the heat kernels, known as diffusion spaces.
Incorporating the manifold structure of the input, we employ integral operator
techniques to derive tight convergence upper bounds concerning generalized
norms, which indicates that the estimators converge to the target function in
strong sense, entailing the simultaneous convergence of the function itself and
its derivatives. These bounds offer two significant advantages: firstly, they
are exclusively contingent on the intrinsic dimension of the input manifolds,
thereby providing a more focused analysis. Secondly, they enable the efficient
derivation of convergence rates for derivatives of any k-th order, all of which
can be accomplished within the ambit of the same spectral algorithms.
Furthermore, we establish minimax lower bounds to demonstrate the asymptotic
optimality of these conclusions in specific contexts. Our study confirms that
the spectral algorithms are practically significant in the broader context of
high-dimensional approximation.
\\ ( https://arxiv.org/abs/2403.03669 ,  27kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03695 (*cross-listing*)
Date: Wed, 6 Mar 2024 13:23:55 GMT   (74kb,D)

Title: Spectral Phase Transition and Optimal PCA in Block-Structured Spiked
  models
Authors: Pierre Mergny, Justin Ko, Florent Krzakala
Categories: stat.ML cond-mat.dis-nn cs.LG math.PR math.ST stat.TH
Comments: 26 pages, 2 figures
\\
  We discuss the inhomogeneous spiked Wigner model, a theoretical framework
recently introduced to study structured noise in various learning scenarios,
through the prism of random matrix theory, with a specific focus on its
spectral properties. Our primary objective is to find an optimal spectral
method and to extend the celebrated \cite{BBP} (BBP) phase transition criterion
-- well-known in the homogeneous case -- to our inhomogeneous,
block-structured, Wigner model. We provide a thorough rigorous analysis of a
transformed matrix and show that the transition for the appearance of 1) an
outlier outside the bulk of the limiting spectral distribution and 2) a
positive overlap between the associated eigenvector and the signal, occurs
precisely at the optimal threshold, making the proposed spectral method optimal
within the class of iterative methods for the inhomogeneous Wigner problem.
\\ ( https://arxiv.org/abs/2403.03695 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03699 (*cross-listing*)
Date: Wed, 6 Mar 2024 13:29:00 GMT   (347kb,D)

Title: Model Parallelism on Distributed Infrastructure: A Literature Review
  from Theory to LLM Case-Studies
Authors: Felix Brakel, Uraz Odyurt, Ana-Lucia Varbanescu
Categories: cs.DC cs.LG
\\
  Neural networks have become a cornerstone of machine learning. As the trend
for these to get more and more complex continues, so does the underlying
hardware and software infrastructure for training and deployment. In this
survey we answer three research questions: "What types of model parallelism
exist?", "What are the challenges of model parallelism?", and "What is a modern
use-case of model parallelism?" We answer the first question by looking at how
neural networks can be parallelised and expressing these as operator graphs
while exploring the available dimensions. The dimensions along which neural
networks can be parallelised are intra-operator and inter-operator. We answer
the second question by collecting and listing both implementation challenges
for the types of parallelism, as well as the problem of optimally partitioning
the operator graph. We answer the last question by collecting and listing how
parallelism is applied in modern multi-billion parameter transformer networks,
to the extend that this is possible with the limited information shared about
these networks.
\\ ( https://arxiv.org/abs/2403.03699 ,  347kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03702 (*cross-listing*)
Date: Wed, 6 Mar 2024 13:36:31 GMT   (10232kb,D)

Title: Online model error correction with neural networks: application to the
  Integrated Forecasting System
Authors: Alban Farchi, Marcin Chrust, Marc Bocquet, Massimo Bonavita
Categories: stat.ML cs.LG
\\
  In recent years, there has been significant progress in the development of
fully data-driven global numerical weather prediction models. These machine
learning weather prediction models have their strength, notably accuracy and
low computational requirements, but also their weakness: they struggle to
represent fundamental dynamical balances, and they are far from being suitable
for data assimilation experiments. Hybrid modelling emerges as a promising
approach to address these limitations. Hybrid models integrate a physics-based
core component with a statistical component, typically a neural network, to
enhance prediction capabilities. In this article, we propose to develop a model
error correction for the operational Integrated Forecasting System (IFS) of the
European Centre for Medium-Range Weather Forecasts using a neural network. The
neural network is initially pre-trained offline using a large dataset of
operational analyses and analysis increments. Subsequently, the trained network
is integrated into the IFS within the Object-Oriented Prediction System (OOPS)
so as to be used in data assimilation and forecast experiments. It is then
further trained online using a recently developed variant of weak-constraint
4D-Var. The results show that the pre-trained neural network already provides a
reliable model error correction, which translates into reduced forecast errors
in many conditions and that the online training further improves the accuracy
of the hybrid model in many conditions.
\\ ( https://arxiv.org/abs/2403.03702 ,  10232kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03736 (*cross-listing*)
Date: Wed, 6 Mar 2024 14:27:02 GMT   (3200kb,D)

Title: Unifying Generation and Compression: Ultra-low bitrate Image Coding Via
  Multi-stage Transformer
Authors: Naifu Xue, Qi Mao, Zijian Wang, Yuan Zhang, Siwei Ma
Categories: cs.CV cs.LG eess.IV
\\
  Recent progress in generative compression technology has significantly
improved the perceptual quality of compressed data. However, these advancements
primarily focus on producing high-frequency details, often overlooking the
ability of generative models to capture the prior distribution of image
content, thus impeding further bitrate reduction in extreme compression
scenarios (<0.05 bpp). Motivated by the capabilities of predictive language
models for lossless compression, this paper introduces a novel Unified Image
Generation-Compression (UIGC) paradigm, merging the processes of generation and
compression. A key feature of the UIGC framework is the adoption of
vector-quantized (VQ) image models for tokenization, alongside a multi-stage
transformer designed to exploit spatial contextual information for modeling the
prior distribution. As such, the dual-purpose framework effectively utilizes
the learned prior for entropy estimation and assists in the regeneration of
lost tokens. Extensive experiments demonstrate the superiority of the proposed
UIGC framework over existing codecs in perceptual quality and human perception,
particularly in ultra-low bitrate scenarios (<=0.03 bpp), pioneering a new
direction in generative compression.
\\ ( https://arxiv.org/abs/2403.03736 ,  3200kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03761 (*cross-listing*)
Date: Wed, 6 Mar 2024 14:53:24 GMT   (262kb,D)

Title: Parameterized quantum comb and simpler circuits for reversing unknown
  qubit-unitary operations
Authors: Yin Mo, Lei Zhang, Yu-Ao Chen, Yingjian Liu, Tengxiang Lin, Xin Wang
Categories: quant-ph cs.IT cs.LG math.IT
Comments: 12 pages including appendix
\\
  Quantum comb is an essential tool for characterizing complex quantum
protocols in quantum information processing. In this work, we introduce PQComb,
a framework leveraging parameterized quantum circuits to explore the
capabilities of quantum combs for general quantum process transformation tasks
and beyond. By optimizing PQComb for time-reversal simulations of unknown
unitary evolutions, we develop a simpler protocol for unknown qubit unitary
inversion that reduces the ancilla qubit overhead from 6 to 3 compared to the
existing method in [Yoshida, Soeda, Murao, PRL 131, 120602, 2023]. This
demonstrates the utility of quantum comb structures and showcases PQComb's
potential for solving complex quantum tasks. Our results pave the way for
broader PQComb applications in quantum computing and quantum information,
emphasizing its versatility for tackling diverse problems in quantum machine
learning.
\\ ( https://arxiv.org/abs/2403.03761 ,  262kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03767 (*cross-listing*)
Date: Wed, 6 Mar 2024 15:03:04 GMT   (744kb,D)

Title: Predicting the Temperature Dependence of Surfactant CMCs Using Graph
  Neural Networks
Authors: Christoforos Brozos, Jan G. Rittig, Sandip Bhattacharya, Elie Akanny,
  Christina Kohlmann, Alexander Mitsos
Categories: physics.chem-ph cs.LG
\\
  The critical micelle concentration (CMC) of surfactant molecules is an
essential property for surfactant applications in industry. Recently, classical
QSPR and Graph Neural Networks (GNNs), a deep learning technique, have been
successfully applied to predict the CMC of surfactants at room temperature.
However, these models have not yet considered the temperature dependency of the
CMC, which is highly relevant for practical applications. We herein develop a
GNN model for temperature-dependent CMC prediction of surfactants. We collect
about 1400 data points from public sources for all surfactant classes, i.e.,
ionic, nonionic, and zwitterionic, at multiple temperatures. We test the
predictive quality of the model for following scenarios: i) when CMC data for
surfactants are present in the training of the model in at least one different
temperature, and ii) CMC data for surfactants are not present in the training,
i.e., generalizing to unseen surfactants. In both test scenarios, our model
exhibits a high predictive performance of R$^2 \geq $ 0.94 on test data. We
also find that the model performance varies by surfactant class. Finally, we
evaluate the model for sugar-based surfactants with complex molecular
structures, as these represent a more sustainable alternative to synthetic
surfactants and are therefore of great interest for future applications in the
personal and home care industries.
\\ ( https://arxiv.org/abs/2403.03767 ,  744kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03771 (*cross-listing*)
Date: Wed, 6 Mar 2024 15:05:39 GMT   (814kb,D)

Title: Joint Sparsity Pattern Learning Based Channel Estimation for Massive
  MIMO-OTFS Systems
Authors: Kuo Meng, Shaoshi Yang, Xiao-Yang Wang, Yan Bu, Yurong Tang, Jianhua
  Zhang, Lajos Hanzo
Categories: eess.SP cs.LG
Comments: 6 pages, 6 figures, accepted to appear on IEEE Transactions on
  Vehicular Technology, Mar. 2024
\\
  We propose a channel estimation scheme based on joint sparsity pattern
learning (JSPL) for massive multi-input multi-output (MIMO) orthogonal
time-frequency-space (OTFS) modulation aided systems. By exploiting the
potential joint sparsity of the delay-Doppler-angle (DDA) domain channel, the
channel estimation problem is transformed into a sparse recovery problem. To
solve it, we first apply the spike and slab prior model to iteratively estimate
the support set of the channel matrix, and a higher-accuracy parameter update
rule relying on the identified support set is introduced into the iteration.
Then the specific values of the channel elements corresponding to the support
set are estimated by the orthogonal matching pursuit (OMP) method. Both our
simulation results and analysis demonstrate that the proposed JSPL channel
estimation scheme achieves an improved performance over the representative
state-of-the-art baseline schemes, despite its reduced pilot overhead.
\\ ( https://arxiv.org/abs/2403.03771 ,  814kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03785 (*cross-listing*)
Date: Wed, 6 Mar 2024 15:30:41 GMT   (1004kb)

Title: A machine learning workflow to address credit default prediction
Authors: Rambod Rahmani, Marco Parola, and Mario G.C.A. Cimino
Categories: cs.CE cs.LG q-fin.RM
\\
  Due to the recent increase in interest in Financial Technology (FinTech),
applications like credit default prediction (CDP) are gaining significant
industrial and academic attention. In this regard, CDP plays a crucial role in
assessing the creditworthiness of individuals and businesses, enabling lenders
to make informed decisions regarding loan approvals and risk management. In
this paper, we propose a workflow-based approach to improve CDP, which refers
to the task of assessing the probability that a borrower will default on his or
her credit obligations. The workflow consists of multiple steps, each designed
to leverage the strengths of different techniques featured in machine learning
pipelines and, thus best solve the CDP task. We employ a comprehensive and
systematic approach starting with data preprocessing using Weight of Evidence
encoding, a technique that ensures in a single-shot data scaling by removing
outliers, handling missing values, and making data uniform for models working
with different data types. Next, we train several families of learning models,
introducing ensemble techniques to build more robust models and hyperparameter
optimization via multi-objective genetic algorithms to consider both predictive
accuracy and financial aspects. Our research aims at contributing to the
FinTech industry in providing a tool to move toward more accurate and reliable
credit risk assessment, benefiting both lenders and borrowers.
\\ ( https://arxiv.org/abs/2403.03785 ,  1004kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03792 (*cross-listing*)
Date: Wed, 6 Mar 2024 15:40:30 GMT   (4446kb,D)

Title: Neural Exec: Learning (and Learning from) Execution Triggers for Prompt
  Injection Attacks
Authors: Dario Pasquini, Martin Strohmeier, and Carmela Troncoso
Categories: cs.CR cs.LG
Comments: v0.1
\\
  We introduce a new family of prompt injection attacks, termed Neural Exec.
Unlike known attacks that rely on handcrafted strings (e.g., "Ignore previous
instructions and..."), we show that it is possible to conceptualize the
creation of execution triggers as a differentiable search problem and use
learning-based methods to autonomously generate them.
  Our results demonstrate that a motivated adversary can forge triggers that
are not only drastically more effective than current handcrafted ones but also
exhibit inherent flexibility in shape, properties, and functionality. In this
direction, we show that an attacker can design and generate Neural Execs
capable of persisting through multi-stage preprocessing pipelines, such as in
the case of Retrieval-Augmented Generation (RAG)-based applications. More
critically, our findings show that attackers can produce triggers that deviate
markedly in form and shape from any known attack, sidestepping existing
blacklist-based detection and sanitation approaches.
\\ ( https://arxiv.org/abs/2403.03792 ,  4446kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03811 (*cross-listing*)
Date: Wed, 6 Mar 2024 16:00:46 GMT   (597kb,D)

Title: Incentivized Learning in Principal-Agent Bandit Games
Authors: Antoine Scheid, Daniil Tiapkin, Etienne Boursier, Aymeric Capitaine,
  El Mahdi El Mhamdi, Eric Moulines, Michael I. Jordan, Alain Durmus
Categories: stat.ML cs.GT cs.LG
\\
  This work considers a repeated principal-agent bandit game, where the
principal can only interact with her environment through the agent. The
principal and the agent have misaligned objectives and the choice of action is
only left to the agent. However, the principal can influence the agent's
decisions by offering incentives which add up to his rewards. The principal
aims to iteratively learn an incentive policy to maximize her own total
utility. This framework extends usual bandit problems and is motivated by
several practical applications, such as healthcare or ecological taxation,
where traditionally used mechanism design theories often overlook the learning
aspect of the problem. We present nearly optimal (with respect to a horizon
$T$) learning algorithms for the principal's regret in both multi-armed and
linear contextual settings. Finally, we support our theoretical guarantees
through numerical experiments.
\\ ( https://arxiv.org/abs/2403.03811 ,  597kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03816 (*cross-listing*)
Date: Wed, 6 Mar 2024 16:03:37 GMT   (781kb,D)

Title: Targeted Variance Reduction: Robust Bayesian Optimization of Black-Box
  Simulators with Noise Parameters
Authors: John Joshua Miller, Simon Mak
Categories: stat.ML cs.LG
\\
  The optimization of a black-box simulator over control parameters
$\mathbf{x}$ arises in a myriad of scientific applications. In such
applications, the simulator often takes the form
$f(\mathbf{x},\boldsymbol{\theta})$, where $\boldsymbol{\theta}$ are parameters
that are uncertain in practice. Robust optimization aims to optimize the
objective $\mathbb{E}[f(\mathbf{x},\boldsymbol{\Theta})]$, where
$\boldsymbol{\Theta} \sim \mathcal{P}$ is a random variable that models
uncertainty on $\boldsymbol{\theta}$. For this, existing black-box methods
typically employ a two-stage approach for selecting the next point
$(\mathbf{x},\boldsymbol{\theta})$, where $\mathbf{x}$ and
$\boldsymbol{\theta}$ are optimized separately via different acquisition
functions. As such, these approaches do not employ a joint acquisition over
$(\mathbf{x},\boldsymbol{\theta})$, and thus may fail to fully exploit
control-to-noise interactions for effective robust optimization. To address
this, we propose a new Bayesian optimization method called Targeted Variance
Reduction (TVR). The TVR leverages a novel joint acquisition function over
$(\mathbf{x},\boldsymbol{\theta})$, which targets variance reduction on the
objective within the desired region of improvement. Under a Gaussian process
surrogate on $f$, the TVR acquisition can be evaluated in closed form, and
reveals an insightful exploration-exploitation-precision trade-off for robust
black-box optimization. The TVR can further accommodate a broad class of
non-Gaussian distributions on $\mathcal{P}$ via a careful integration of
normalizing flows. We demonstrate the improved performance of TVR over the
state-of-the-art in a suite of numerical experiments and an application to the
robust design of automobile brake discs under operational uncertainty.
\\ ( https://arxiv.org/abs/2403.03816 ,  781kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03827 (*cross-listing*)
Date: Wed, 6 Mar 2024 16:17:34 GMT   (65kb,D)

Title: Linear and nonlinear system identification under $\ell_1$- and
  group-Lasso regularization via L-BFGS-B
Authors: Alberto Bemporad
Categories: eess.SY cs.LG cs.SY math.OC
Comments: 23 pages, 4 figures
\\
  In this paper, we propose an approach for identifying linear and nonlinear
discrete-time state-space models, possibly under $\ell_1$- and group-Lasso
regularization, based on the L-BFGS-B algorithm. For the identification of
linear models, we show that, compared to classical linear subspace methods, the
approach often provides better results, is much more general in terms of the
loss and regularization terms used, and is also more stable from a numerical
point of view. The proposed method not only enriches the existing set of linear
system identification tools but can be also applied to identifying a very broad
class of parametric nonlinear state-space models, including recurrent neural
networks. We illustrate the approach on synthetic and experimental datasets and
apply it to solve the challenging industrial robot benchmark for nonlinear
multi-input/multi-output system identification proposed by Weigand et al.
(2022). A Python implementation of the proposed identification method is
available in the package \texttt{jax-sysid}, available at
\url{https://github.com/bemporad/jax-sysid}.
\\ ( https://arxiv.org/abs/2403.03827 ,  65kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03848 (*cross-listing*)
Date: Wed, 6 Mar 2024 16:49:08 GMT   (4214kb,D)

Title: Dexterous Legged Locomotion in Confined 3D Spaces with Reinforcement
  Learning
Authors: Zifan Xu, Amir Hossain Raj, Xuesu Xiao, and Peter Stone
Categories: cs.RO cs.LG
\\
  Recent advances of locomotion controllers utilizing deep reinforcement
learning (RL) have yielded impressive results in terms of achieving rapid and
robust locomotion across challenging terrain, such as rugged rocks, non-rigid
ground, and slippery surfaces. However, while these controllers primarily
address challenges underneath the robot, relatively little research has
investigated legged mobility through confined 3D spaces, such as narrow tunnels
or irregular voids, which impose all-around constraints. The cyclic gait
patterns resulted from existing RL-based methods to learn parameterized
locomotion skills characterized by motion parameters, such as velocity and body
height, may not be adequate to navigate robots through challenging confined 3D
spaces, requiring both agile 3D obstacle avoidance and robust legged
locomotion. Instead, we propose to learn locomotion skills end-to-end from
goal-oriented navigation in confined 3D spaces. To address the inefficiency of
tracking distant navigation goals, we introduce a hierarchical locomotion
controller that combines a classical planner tasked with planning waypoints to
reach a faraway global goal location, and an RL-based policy trained to follow
these waypoints by generating low-level motion commands. This approach allows
the policy to explore its own locomotion skills within the entire solution
space and facilitates smooth transitions between local goals, enabling
long-term navigation towards distant goals. In simulation, our hierarchical
approach succeeds at navigating through demanding confined 3D environments,
outperforming both pure end-to-end learning approaches and parameterized
locomotion skills. We further demonstrate the successful real-world deployment
of our simulation-trained controller on a real robot.
\\ ( https://arxiv.org/abs/2403.03848 ,  4214kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03849 (*cross-listing*)
Date: Wed, 6 Mar 2024 16:49:33 GMT   (135kb,D)

Title: MedMamba: Vision Mamba for Medical Image Classification
Authors: Yubiao Yue, Zhenzhang Li
Categories: eess.IV cs.CV cs.LG
\\
  Medical image classification is a very fundamental and crucial task in the
field of computer vision. These years, CNN-based and Transformer-based models
are widely used in classifying various medical images. Unfortunately, The
limitation of CNNs in long-range modeling capabilities prevent them from
effectively extracting fine-grained features in medical images , while
Transformers are hampered by their quadratic computational complexity. Recent
research has shown that the state space model (SSM) represented by Mamba can
efficiently model long-range interactions while maintaining linear
computational complexity. Inspired by this, we propose Vision Mamba for medical
image classification (MedMamba). More specifically, we introduce a novel
Conv-SSM module, which combines the local feature extraction ability of
convolutional layers with the ability of SSM to capture long-range dependency.
To demonstrate the potential of MedMamba, we conduct extensive experiments
using three publicly available medical datasets with different imaging
techniques (i.e., Kvasir (endoscopic images), FETAL_PLANES_DB (ultrasound
images) and Covid19-Pneumonia-Normal Chest X-Ray (X-ray images)) and two
private datasets built by ourselves. Experimental results show that the
proposed MedMamba performs well in detecting lesions in various medical images.
To the best of our knowledge, this is the first Vision Mamba tailored for
medical image classification. The purpose of this work is to establish a new
baseline for medical image classification tasks and provide valuable insights
for the future development of more efficient and effective SSM-based artificial
intelligence algorithms and application systems in the medical. Source code has
been available at https://github.com/YubiaoYue/MedMamba.
\\ ( https://arxiv.org/abs/2403.03849 ,  135kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03850 (*cross-listing*)
Date: Wed, 6 Mar 2024 16:55:40 GMT   (659kb,D)

Title: Conformal prediction for multi-dimensional time series by ellipsoidal
  sets
Authors: Chen Xu, Hanyang Jiang, Yao Xie
Categories: stat.ML cs.LG
\\
  Conformal prediction (CP) has been a popular method for uncertainty
quantification because it is distribution-free, model-agnostic, and
theoretically sound. For forecasting problems in supervised learning, most CP
methods focus on building prediction intervals for univariate responses. In
this work, we develop a sequential CP method called $\texttt{MultiDimSPCI}$
that builds prediction regions for a multivariate response, especially in the
context of multivariate time series, which are not exchangeable. Theoretically,
we estimate finite-sample high-probability bounds on the conditional coverage
gap. Empirically, we demonstrate that $\texttt{MultiDimSPCI}$ maintains valid
coverage on a wide range of multivariate time series while producing smaller
prediction regions than CP and non-CP baselines.
\\ ( https://arxiv.org/abs/2403.03850 ,  659kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03891 (*cross-listing*)
Date: Wed, 6 Mar 2024 17:51:04 GMT   (1034kb,D)

Title: Joint multi-task learning improves weakly-supervised biomarker
  prediction in computational pathology
Authors: Omar S. M. El Nahhas, Georg W\"olflein, Marta Ligero, Tim Lenz, Marko
  van Treeck, Firas Khader, Daniel Truhn, Jakob Nikolas Kather
Categories: eess.IV cs.CV cs.LG
\\
  Deep Learning (DL) can predict biomarkers directly from digitized cancer
histology in a weakly-supervised setting. Recently, the prediction of
continuous biomarkers through regression-based DL has seen an increasing
interest. Nonetheless, clinical decision making often requires a categorical
outcome. Consequently, we developed a weakly-supervised joint multi-task
Transformer architecture which has been trained and evaluated on four public
patient cohorts for the prediction of two key predictive biomarkers,
microsatellite instability (MSI) and homologous recombination deficiency (HRD),
trained with auxiliary regression tasks related to the tumor microenvironment.
Moreover, we perform a comprehensive benchmark of 16 approaches of task
balancing for weakly-supervised joint multi-task learning in computational
pathology. Using our novel approach, we improve over the state-of-the-art area
under the receiver operating characteristic by +7.7% and +4.1%, as well as
yielding better clustering of latent embeddings by +8% and +5% for the
prediction of MSI and HRD in external cohorts, respectively.
\\ ( https://arxiv.org/abs/2403.03891 ,  1034kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03896 (*cross-listing*)
Date: Wed, 6 Mar 2024 17:54:50 GMT   (6597kb,D)

Title: DART: Implicit Doppler Tomography for Radar Novel View Synthesis
Authors: Tianshu Huang, John Miller, Akarsh Prabhakara, Tao Jin, Tarana Laroia,
  Zico Kolter, Anthony Rowe
Categories: cs.CV cs.LG
Comments: To appear in CVPR 2024; see https://wiselabcmu.github.io/dart/ for
  our project site
\\
  Simulation is an invaluable tool for radio-frequency system designers that
enables rapid prototyping of various algorithms for imaging, target detection,
classification, and tracking. However, simulating realistic radar scans is a
challenging task that requires an accurate model of the scene, radio frequency
material properties, and a corresponding radar synthesis function. Rather than
specifying these models explicitly, we propose DART - Doppler Aided Radar
Tomography, a Neural Radiance Field-inspired method which uses radar-specific
physics to create a reflectance and transmittance-based rendering pipeline for
range-Doppler images. We then evaluate DART by constructing a custom data
collection platform and collecting a novel radar dataset together with accurate
position and instantaneous velocity measurements from lidar-based localization.
In comparison to state-of-the-art baselines, DART synthesizes superior radar
range-Doppler images from novel views across all datasets and additionally can
be used to generate high quality tomographic images.
\\ ( https://arxiv.org/abs/2403.03896 ,  6597kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03905 (*cross-listing*)
Date: Wed, 6 Mar 2024 18:07:20 GMT   (91kb,D)

Title: Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications
Authors: Arun Jambulapati, Syamantak Kumar, Jerry Li, Shourya Pandey, Ankit
  Pensia, Kevin Tian
Categories: math.NA cs.DS cs.LG cs.NA stat.ML
\\
  The $k$-principal component analysis ($k$-PCA) problem is a fundamental
algorithmic primitive that is widely-used in data analysis and dimensionality
reduction applications. In statistical settings, the goal of $k$-PCA is to
identify a top eigenspace of the covariance matrix of a distribution, which we
only have implicit access to via samples. Motivated by these implicit settings,
we analyze black-box deflation methods as a framework for designing $k$-PCA
algorithms, where we model access to the unknown target matrix via a black-box
$1$-PCA oracle which returns an approximate top eigenvector, under two popular
notions of approximation. Despite being arguably the most natural
reduction-based approach to $k$-PCA algorithm design, such black-box methods,
which recursively call a $1$-PCA oracle $k$ times, were previously
poorly-understood.
  Our main contribution is significantly sharper bounds on the approximation
parameter degradation of deflation methods for $k$-PCA. For a quadratic form
notion of approximation we term ePCA (energy PCA), we show deflation methods
suffer no parameter loss. For an alternative well-studied approximation notion
we term cPCA (correlation PCA), we tightly characterize the parameter regimes
where deflation methods are feasible. Moreover, we show that in all feasible
regimes, $k$-cPCA deflation algorithms suffer no asymptotic parameter loss for
any constant $k$. We apply our framework to obtain state-of-the-art $k$-PCA
algorithms robust to dataset contamination, improving prior work both in sample
complexity and approximation quality.
\\ ( https://arxiv.org/abs/2403.03905 ,  91kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03954 (*cross-listing*)
Date: Wed, 6 Mar 2024 18:58:49 GMT   (27515kb,D)

Title: 3D Diffusion Policy
Authors: Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, Huazhe
  Xu
Categories: cs.RO cs.CV cs.LG
Comments: Videos, code, and data: https://3d-diffusion-policy.github.io
\\
  Imitation learning provides an efficient way to teach robots dexterous
skills; however, learning complex skills robustly and generalizablely usually
consumes large amounts of human demonstrations. To tackle this challenging
problem, we present 3D Diffusion Policy (DP3), a novel visual imitation
learning approach that incorporates the power of 3D visual representations into
diffusion policies, a class of conditional action generative models. The core
design of DP3 is the utilization of a compact 3D visual representation,
extracted from sparse point clouds with an efficient point encoder. In our
experiments involving 72 simulation tasks, DP3 successfully handles most tasks
with just 10 demonstrations and surpasses baselines with a 55.3% relative
improvement. In 4 real robot tasks, DP3 demonstrates precise control with a
high success rate of 85%, given only 40 demonstrations of each task, and shows
excellent generalization abilities in diverse aspects, including space,
viewpoint, appearance, and instance. Interestingly, in real robot experiments,
DP3 rarely violates safety requirements, in contrast to baseline methods which
frequently do, necessitating human intervention. Our extensive evaluation
highlights the critical importance of 3D representations in real-world robot
learning. Videos, code, and data are available on
https://3d-diffusion-policy.github.io .
\\ ( https://arxiv.org/abs/2403.03954 ,  27515kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2211.11940
replaced with revised version Wed, 6 Mar 2024 04:10:53 GMT   (7191kb,D)

Title: Decision-making with Speculative Opponent Models
Authors: Jing Sun, Shuo Chen, Cong Zhang, Jie Zhang
Categories: cs.AI cs.LG cs.MA
Comments: 13 pages, 27 figures
\\ ( https://arxiv.org/abs/2211.11940 ,  7191kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08424
replaced with revised version Wed, 6 Mar 2024 05:00:06 GMT   (26220kb,D)

Title: Unstoppable Attack: Label-Only Model Inversion via Conditional Diffusion
  Model
Authors: Rongke Liu, Dong Wang, Yizhi Ren, Zhen Wang, Kaitian Guo, Qianqian
  Qin, Xiaolei Liu
Categories: cs.AI
Comments: 16 pages, 9 figures, 8 tables
DOI: 10.1109/TIFS.2024.3372815
\\ ( https://arxiv.org/abs/2307.08424 ,  26220kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01154
replaced with revised version Wed, 6 Mar 2024 09:39:16 GMT   (1141kb,D)

Title: Arithmetic with Language Models: from Memorization to Computation
Authors: Davide Maltoni and Matteo Ferrara
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2308.01154 ,  1141kb)
------------------------------------------------------------------------------
\\
arXiv:2309.03685
replaced with revised version Tue, 5 Mar 2024 21:56:43 GMT   (130kb,D)

Title: PyGraft: Configurable Generation of Synthetic Schemas and Knowledge
  Graphs at Your Fingertips
Authors: Nicolas Hubert, Pierre Monnin, Mathieu d'Aquin, Davy Monticolo,
  Armelle Brun
Categories: cs.AI cs.SE
Comments: Accepted in ESWC 2024
\\ ( https://arxiv.org/abs/2309.03685 ,  130kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16960
replaced with revised version Wed, 6 Mar 2024 02:02:29 GMT   (2016kb,D)

Title: On Generating Explanations for Reinforcement Learning Policies: An
  Empirical Study
Authors: Mikihisa Yuasa, Huy T. Tran, Ramavarapu S. Sreenivas
Categories: cs.AI
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\ ( https://arxiv.org/abs/2309.16960 ,  2016kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00194
replaced with revised version Wed, 6 Mar 2024 03:24:45 GMT   (505kb,D)

Title: A Prefrontal Cortex-inspired Architecture for Planning in Large Language
  Models
Authors: Taylor Webb, Shanka Subhra Mondal, Chi Wang, Brian Krabach, Ida
  Momennejad
Categories: cs.AI cs.NE
\\ ( https://arxiv.org/abs/2310.00194 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12472
replaced with revised version Wed, 6 Mar 2024 12:57:27 GMT   (3563kb,D)

Title: Self-Supervised Deconfounding Against Spatio-Temporal Shifts: Theory and
  Modeling
Authors: Jiahao Ji, Wentao Zhang, Jingyuan Wang, Yue He and Chao Huang
Categories: cs.AI
Comments: 14 pages, 9 figures
\\ ( https://arxiv.org/abs/2311.12472 ,  3563kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03093
replaced with revised version Wed, 6 Mar 2024 01:37:00 GMT   (388kb)

Title: Explicitly explainable AI solution to the AI black box problem
Authors: V. L. Kalmykov, L.V. Kalmykov
Categories: cs.AI q-bio.PE
Comments: 17 pages, 1 figure, 70 references
\\ ( https://arxiv.org/abs/2401.03093 ,  388kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02547
replaced with revised version Wed, 6 Mar 2024 02:46:40 GMT   (1880kb)

Title: Integration of cognitive tasks into artificial general intelligence test
  for large models
Authors: Youzhi Qu, Chen Wei, Penghui Du, Wenxin Che, Chi Zhang, Wanli Ouyang,
  Yatao Bian, Feiyang Xu, Bin Hu, Kai Du, Haiyan Wu, Jia Liu, Quanying Liu
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.02547 ,  1880kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04154
replaced with revised version Wed, 6 Mar 2024 07:53:43 GMT   (1492kb,D)

Title: Read to Play (R2-Play): Decision Transformer with Multimodal Game
  Instruction
Authors: Yonggang Jin, Ge Zhang, Hao Zhao, Tianyu Zheng, Jiawei Guo, Liuyu
  Xiang, Shawn Yue, Stephen W. Huang, Wenhu Chen, Zhaofeng He and Jie Fu
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.04154 ,  1492kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06326
replaced with revised version Wed, 6 Mar 2024 05:34:12 GMT   (2231kb,D)

Title: Prompt Learning on Temporal Interaction Graphs
Authors: Xi Chen, Siwei Zhang, Yun Xiong, Xixi Wu, Jiawei Zhang, Xiangguo Sun,
  Yao Zhang, Feng Zhao, Yulin Kang
Categories: cs.AI cs.LG cs.SI
Comments: 11 pages, 8 figures
\\ ( https://arxiv.org/abs/2402.06326 ,  2231kb)
------------------------------------------------------------------------------
\\
arXiv:2203.11155
replaced with revised version Wed, 6 Mar 2024 08:53:17 GMT   (650kb)

Title: Quantum Neural Network with Density Matrix for Question Answering and
  Classical Image Classification
Authors: X. Q. Zhao, T. L. Chen
Categories: cs.CL cs.LG quant-ph
\\ ( https://arxiv.org/abs/2203.11155 ,  650kb)
------------------------------------------------------------------------------
\\
arXiv:2305.09329
replaced with revised version Wed, 6 Mar 2024 14:56:28 GMT   (1510kb,D)

Title: CWTM: Leveraging Contextualized Word Embeddings from BERT for Neural
  Topic Modeling
Authors: Zheng Fang, Yulan He and Rob Procter
Categories: cs.CL cs.AI
Comments: The paper has been accepted to appear at the LREC-COLING 2024
  conference
\\ ( https://arxiv.org/abs/2305.09329 ,  1510kb)
------------------------------------------------------------------------------
\\
arXiv:2308.00071
replaced with revised version Wed, 6 Mar 2024 18:49:55 GMT   (1457kb,D)

Title: Interpretable Stereotype Identification through Reasoning
Authors: Jacob-Junqi Tian, Omkar Dige, David Emerson, Faiza Khan Khattak
Categories: cs.CL cs.AI cs.CY cs.LG
\\ ( https://arxiv.org/abs/2308.00071 ,  1457kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13297
replaced with revised version Wed, 6 Mar 2024 16:33:41 GMT   (1984kb,D)

Title: OATS: Opinion Aspect Target Sentiment Quadruple Extraction Dataset for
  Aspect-Based Sentiment Analysis
Authors: Siva Uday Sampreeth Chebolu and Franck Dernoncourt and Nedim Lipka and
  Thamar Solorio
Categories: cs.CL
Comments: Accepted in COLING/LREC-2024. Camera Ready submission
\\ ( https://arxiv.org/abs/2309.13297 ,  1984kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13734
replaced with revised version Tue, 5 Mar 2024 21:26:54 GMT   (1165kb,D)

Title: Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance
  Classification
Authors: Iain J. Cruickshank and Lynnette Hui Xian Ng
Categories: cs.CL cs.AI
Comments: Submitted to ACM Transactions on Intelligent Systems and Technology,
  Special Issue on Evaluations of Large Language Models. 28 Pages, 3 Figures
\\ ( https://arxiv.org/abs/2309.13734 ,  1165kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03668
replaced with revised version Wed, 6 Mar 2024 16:38:03 GMT   (9832kb,D)

Title: GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction
Authors: Oscar Sainz, Iker Garc\'ia-Ferrero, Rodrigo Agerri, Oier Lopez de
  Lacalle, German Rigau, Eneko Agirre
Categories: cs.CL
Comments: The Twelfth International Conference on Learning Representations -
  ICLR 2024
\\ ( https://arxiv.org/abs/2310.03668 ,  9832kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04799
replaced with revised version Wed, 6 Mar 2024 15:50:02 GMT   (12239kb,D)

Title: Chat Vector: A Simple Approach to Equip LLMs with Instruction Following
  and Model Alignment in New Languages
Authors: Shih-Cheng Huang, Pin-Zu Li, Yu-Chi Hsu, Kuang-Ming Chen, Yu Tung Lin,
  Shih-Kai Hsiao, Richard Tzong-Han Tsai, Hung-yi Lee
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.04799 ,  12239kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14985
replaced with revised version Wed, 6 Mar 2024 12:25:09 GMT   (4980kb,D)

Title: LLM-Based Agent Society Investigation: Collaboration and Confrontation
  in Avalon Gameplay
Authors: Yihuai Lan, Zhiqiang Hu, Lei Wang, Yang Wang, Deheng Ye, Peilin Zhao,
  Ee-Peng Lim, Hui Xiong, Hao Wang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.14985 ,  4980kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20707
replaced with revised version Tue, 5 Mar 2024 20:02:31 GMT   (5912kb,D)

Title: What's In My Big Data?
Authors: Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander,
  Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini,
  Sameer Singh, Hanna Hajishirzi, Noah A. Smith, Jesse Dodge
Categories: cs.CL cs.LG
Comments: Published at ICLR 2024 spotlight
\\ ( https://arxiv.org/abs/2310.20707 ,  5912kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14479
replaced with revised version Wed, 6 Mar 2024 09:36:54 GMT   (465kb,D)

Title: Controlled Text Generation via Language Model Arithmetic
Authors: Jasper Dekoninck, Marc Fischer, Luca Beurer-Kellner, Martin Vechev
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.14479 ,  465kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14197
replaced with revised version Wed, 6 Mar 2024 02:19:03 GMT   (1433kb,D)

Title: Benchmarking and Defending Against Indirect Prompt Injection Attacks on
  Large Language Models
Authors: Jingwei Yi, Yueqi Xie, Bin Zhu, Emre Kiciman, Guangzhong Sun, Xing
  Xie, Fangzhao Wu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2312.14197 ,  1433kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11389
replaced with revised version Wed, 6 Mar 2024 03:26:46 GMT   (312kb,D)

Title: MedLM: Exploring Language Models for Medical Question Answering Systems
Authors: Niraj Yagnik, Jay Jhaveri, Vivek Sharma, Gabriel Pila
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.11389 ,  312kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16553
replaced with revised version Tue, 5 Mar 2024 20:55:35 GMT   (10172kb,D)

Title: SelectLLM: Can LLMs Select Important Instructions to Annotate?
Authors: Ritik Sachin Parkar, Jaehyung Kim, Jong Inn Park, Dongyeop Kang
Categories: cs.CL cs.AI
Comments: First Authors: Ritik Sachin Parkar and Jaehyung Kim | Second Author:
  Jong Inn Park | PI: Dongyeop Kang
\\ ( https://arxiv.org/abs/2401.16553 ,  10172kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17919
replaced with revised version Wed, 6 Mar 2024 09:55:55 GMT   (7718kb,D)

Title: LOCOST: State-Space Models for Long Document Abstractive Summarization
Authors: Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen,
  Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick
  Gallinari
Categories: cs.CL cs.LG
Comments: 9 pages, 5 figures, 7 tables, EACL 2024 conference
\\ ( https://arxiv.org/abs/2401.17919 ,  7718kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03223
replaced with revised version Wed, 6 Mar 2024 11:06:42 GMT   (220kb,D)

Title: English Prompts are Better for NLI-based Zero-Shot Emotion
  Classification than Target-Language Prompts
Authors: Patrick Barei{\ss} and Roman Klinger and Jeremy Barnes
Categories: cs.CL
Comments: accepted to the PromptEng workshop at The Web Conf
\\ ( https://arxiv.org/abs/2402.03223 ,  220kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13874
replaced with revised version Wed, 6 Mar 2024 14:46:30 GMT   (3553kb,D)

Title: $Se^2$: Sequential Example Selection for In-Context Learning
Authors: Haoyu Liu, Jianfeng Liu, Shaohan Huang, Yuefeng Zhan, Hao Sun, Weiwei
  Deng, Furu Wei, Qi Zhang
Categories: cs.CL
Comments: 19 pages, 5 figures
\\ ( https://arxiv.org/abs/2402.13874 ,  3553kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14494
replaced with revised version Wed, 6 Mar 2024 07:17:03 GMT   (2222kb,D)

Title: Noise-BERT: A Unified Perturbation-Robust Framework with Noise Alignment
  Pre-training for Noisy Slot Filling Task
Authors: Jinxu Zhao, Guanting Dong, Yueyan Qiu, Tingfeng Hui, Xiaoshuai Song,
  Daichi Guo, Weiran Xu
Categories: cs.CL
Comments: Accepted by ICASSP 2024
\\ ( https://arxiv.org/abs/2402.14494 ,  2222kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16029
replaced with revised version Wed, 6 Mar 2024 13:52:12 GMT   (1180kb,D)

Title: GraphWiz: An Instruction-Following Language Model for Graph Problems
Authors: Nuo Chen, Yuhan Li, Jianheng Tang, Jia Li
Categories: cs.CL
Comments: 27pages, 15 tables
\\ ( https://arxiv.org/abs/2402.16029 ,  1180kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18061
replaced with revised version Wed, 6 Mar 2024 08:42:24 GMT   (849kb,D)

Title: On the use of Silver Standard Data for Zero-shot Classification Tasks in
  Information Extraction
Authors: Jianwei Wang, Tianyin Wang, Ziqian Zeng
Categories: cs.CL cs.AI
Comments: accepted by coling2024. arXiv:2211.13883 is our first edition
\\ ( https://arxiv.org/abs/2402.18061 ,  849kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02472
replaced with revised version Wed, 6 Mar 2024 02:34:05 GMT   (300kb,D)

Title: OffLanDat: A Community Based Implicit Offensive Language Dataset
  Generated by Large Language Model Through Prompt Engineering
Authors: Amit Das, Mostafa Rahgouy, Dongji Feng, Zheng Zhang, Tathagata
  Bhattacharya, Nilanjana Raychawdhary, Mary Sandage, Lauramarie Pope, Gerry
  Dozier and Cheryl Seals
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.02472 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02951
replaced with revised version Wed, 6 Mar 2024 08:43:17 GMT   (2904kb,D)

Title: Benchmarking the Text-to-SQL Capability of Large Language Models: A
  Comprehensive Evaluation
Authors: Bin Zhang, Yuxiao Ye, Guoqing Du, Xiaoru Hu, Zhishuai Li, Sun Yang,
  Chi Harold Liu, Rui Zhao, Ziyue Li, Hangyu Mao
Categories: cs.CL cs.AI
Comments: 26pages, 6figures, 14tables
\\ ( https://arxiv.org/abs/2403.02951 ,  2904kb)
------------------------------------------------------------------------------
\\
arXiv:2205.03014
replaced with revised version Wed, 6 Mar 2024 17:22:11 GMT   (44kb)

Title: Differentially Private Generalized Linear Models Revisited
Authors: Raman Arora, Raef Bassily, Crist\'obal Guzm\'an, Michael Menart,
  Enayat Ullah
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2205.03014 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2209.00225
replaced with revised version Wed, 6 Mar 2024 09:23:41 GMT   (934kb,D)

Title: STDEN: Towards Physics-Guided Neural Networks for Traffic Flow
  Prediction
Authors: Jiahao Ji, Jingyuan Wang, Zhe Jiang, Jiawei Jiang, Hu Zhang
Categories: cs.LG
Comments: 36th AAAI Conference on Artificial Intelligence (AAAI 2022)
DOI: 10.1609/aaai.v36i4.20322
\\ ( https://arxiv.org/abs/2209.00225 ,  934kb)
------------------------------------------------------------------------------
\\
arXiv:2210.09475
replaced with revised version Wed, 6 Mar 2024 18:25:17 GMT   (3976kb,D)

Title: FIMP: Foundation Model-Informed Message Passing for Graph Neural
  Networks
Authors: Syed Asad Rizvi, Nhi Nguyen, Haoran Lyu, Benjamin Christensen, Josue
  Ortega Caro, Antonio H. O. Fonseca, Emanuele Zappala, Maryam Bagherian,
  Christopher Averill, Chadi G. Abdallah, Amin Karbasi, Rex Ying, Maria Brbic,
  Rahul Madhav Dhodapkar, David van Dijk
Categories: cs.LG
Comments: 16 pages (11 + 5 pages appendix). 5 figures and 7 tables
\\ ( https://arxiv.org/abs/2210.09475 ,  3976kb)
------------------------------------------------------------------------------
\\
arXiv:2212.04475
replaced with revised version Wed, 6 Mar 2024 09:06:24 GMT   (2613kb,D)

Title: Spatio-Temporal Self-Supervised Learning for Traffic Flow Prediction
Authors: Jiahao Ji, Jingyuan Wang, Chao Huang, Junjie Wu, Boren Xu, Zhenhe Wu,
  Junbo Zhang, Yu Zheng
Categories: cs.LG cs.AI
Comments: 37th AAAI Conference on Artificial Intelligence (AAAI 2023)
\\ ( https://arxiv.org/abs/2212.04475 ,  2613kb)
------------------------------------------------------------------------------
\\
arXiv:2301.10369
replaced with revised version Wed, 6 Mar 2024 15:25:57 GMT   (132kb,D)

Title: Exact Fractional Inference via Re-Parametrization & Interpolation
  between Tree-Re-Weighted- and Belief Propagation- Algorithms
Authors: Hamidreza Behjoo, Michael Chertkov
Categories: cs.LG cond-mat.stat-mech
\\ ( https://arxiv.org/abs/2301.10369 ,  132kb)
------------------------------------------------------------------------------
\\
arXiv:2302.01477
replaced with revised version Wed, 6 Mar 2024 06:23:21 GMT   (52kb)

Title: A Reduction-based Framework for Sequential Decision Making with Delayed
  Feedback
Authors: Yunchang Yang, Han Zhong, Tianhao Wu, Bin Liu, Liwei Wang, Simon S. Du
Categories: cs.LG
Comments: Accepted by Neurips 2023. arXiv admin note: text overlap with
  arXiv:2110.14555 by other authors
\\ ( https://arxiv.org/abs/2302.01477 ,  52kb)
------------------------------------------------------------------------------
\\
arXiv:2302.08545
replaced with revised version Tue, 5 Mar 2024 21:40:25 GMT   (7605kb,D)

Title: THC: Accelerating Distributed Deep Learning Using Tensor Homomorphic
  Compression
Authors: Minghao Li (1), Ran Ben Basat (2), Shay Vargaftik (3), ChonLam Lao
  (1), Kevin Xu (1), Michael Mitzenmacher (1), Minlan Yu (1) ((1) Harvard
  University, (2) University College London, (3) VMware Research)
Categories: cs.LG cs.AI cs.NI
Comments: 12 pages body, 21 pages total
\\ ( https://arxiv.org/abs/2302.08545 ,  7605kb)
------------------------------------------------------------------------------
\\
arXiv:2303.00286
replaced with revised version Tue, 5 Mar 2024 20:49:10 GMT   (38kb)

Title: Treat Different Negatives Differently: Enriching Loss Functions with
  Domain and Range Constraints for Link Prediction
Authors: Nicolas Hubert, Pierre Monnin, Armelle Brun, Davy Monticolo
Categories: cs.LG cs.AI
Comments: Accepted in ESWC 2024
\\ ( https://arxiv.org/abs/2303.00286 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2303.11525
replaced with revised version Tue, 5 Mar 2024 22:12:38 GMT   (349kb,D)

Title: Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training
  Efficiency
Authors: Vithursan Thangarasa, Shreyas Saxena, Abhay Gupta, Sean Lie
Categories: cs.LG cs.CL cs.CV
Comments: 13 pages, 5 figures (Main Paper) + 9 pages (Supplementary Material)
\\ ( https://arxiv.org/abs/2303.11525 ,  349kb)
------------------------------------------------------------------------------
\\
arXiv:2304.14185
replaced with revised version Wed, 6 Mar 2024 07:41:06 GMT   (21518kb,D)

Title: ClusterNet: A Perception-Based Clustering Model for Scattered Data
Authors: Sebastian Hartwig, Christian van Onzenoodt, Dominik Engel, Pedro
  Hermosilla, Timo Ropinski
Categories: cs.LG cs.HC
Comments: Currently, this manuscript is under revision at CGF
\\ ( https://arxiv.org/abs/2304.14185 ,  21518kb)
------------------------------------------------------------------------------
\\
arXiv:2305.01883
replaced with revised version Wed, 6 Mar 2024 01:45:16 GMT   (182kb,D)

Title: A Lightweight CNN-Transformer Model for Learning Traveling Salesman
  Problems
Authors: Minseop Jung, Jaeseung Lee, Jibum Kim
Categories: cs.LG cs.CG
\\ ( https://arxiv.org/abs/2305.01883 ,  182kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11290
replaced with revised version Tue, 5 Mar 2024 22:07:29 GMT   (1217kb,D)

Title: Massively Scalable Inverse Reinforcement Learning in Google Maps
Authors: Matt Barnes, Matthew Abueg, Oliver F. Lange, Matt Deeds, Jason Trader,
  Denali Molitor, Markus Wulfmeier, Shawn O'Banion
Categories: cs.LG
\\ ( https://arxiv.org/abs/2305.11290 ,  1217kb)
------------------------------------------------------------------------------
\\
arXiv:2306.17486
replaced with revised version Wed, 6 Mar 2024 17:19:09 GMT   (1466kb,D)

Title: Multigrid-Augmented Deep Learning Preconditioners for the Helmholtz
  Equation using Compact Implicit Layers
Authors: Bar Lerer, Ido Ben-Yair and Eran Treister
Categories: cs.LG cs.CE
Comments: Published in SIAM Journal on Scientific Computing Copper Mountain
  Special Section on Multigrid Methods 2023
MSC-class: 68T07, 65N55, 65N22
\\ ( https://arxiv.org/abs/2306.17486 ,  1466kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02117
replaced with revised version Wed, 6 Mar 2024 15:06:27 GMT   (3205kb,D)

Title: VQGraph: Rethinking Graph Representation Space for Bridging GNNs and
  MLPs
Authors: Ling Yang, Ye Tian, Minkai Xu, Zhongyi Liu, Shenda Hong, Wei Qu,
  Wentao Zhang, Bin Cui, Muhan Zhang, Jure Leskovec
Categories: cs.LG cs.AI cs.CV
Comments: ICLR 2024. Code: https://github.com/YangLing0818/VQGraph
\\ ( https://arxiv.org/abs/2308.02117 ,  3205kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11155
replaced with revised version Wed, 6 Mar 2024 02:21:40 GMT   (5143kb,D)

Title: Beyond MD17: the reactive xxMD dataset
Authors: Zihan Pengmei, Junyu Liu, Yinan Shu
Categories: cs.LG cs.AI physics.chem-ph quant-ph
Comments: 19 pages, many figures. Data available at
  https://github.com/zpengmei/xxMD
Journal-ref: Sci Data 11, 222 (2024)
DOI: 10.1038/s41597-024-03019-3
\\ ( https://arxiv.org/abs/2308.11155 ,  5143kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05317
replaced with revised version Wed, 6 Mar 2024 14:57:23 GMT   (3202kb,D)

Title: Neural Koopman prior for data assimilation
Authors: Anthony Frion, Lucas Drumetz, Mauro Dalla Mura, Guillaume Tochon,
  Abdeldjalil A\"issa El Bey
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.05317 ,  3202kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07579
replaced with revised version Wed, 6 Mar 2024 14:27:01 GMT   (206kb,D)

Title: Structure-Preserving Transformers for Sequences of SPD Matrices
Authors: Mathieu Seraphim, Alexis Lechervy, Florian Yger, Luc Brun and Olivier
  Etard
Categories: cs.LG eess.SP
Comments: New year, new version! (updated template, minimal additions -
  including two new references)
\\ ( https://arxiv.org/abs/2309.07579 ,  206kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08776
replaced with revised version Wed, 6 Mar 2024 18:51:45 GMT   (1424kb,D)

Title: Projected Task-Specific Layers for Multi-Task Reinforcement Learning
Authors: Josselin Somerville Roberts, Julia Di
Categories: cs.LG cs.AI cs.RO
Journal-ref: ICRA 2024
\\ ( https://arxiv.org/abs/2309.08776 ,  1424kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13786
replaced with revised version Wed, 6 Mar 2024 14:18:52 GMT   (10186kb,D)

Title: Distribution-Free Statistical Dispersion Control for Societal
  Applications
Authors: Zhun Deng, Thomas P. Zollo, Jake C. Snell, Toniann Pitassi, Richard
  Zemel
Categories: cs.LG stat.ML
Comments: Accepted by NeurIPS as spotlight (top 3% among submissions)
\\ ( https://arxiv.org/abs/2309.13786 ,  10186kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14209
replaced with revised version Wed, 6 Mar 2024 07:55:17 GMT   (2891kb,D)

Title: Continual Driving Policy Optimization with Closed-Loop Individualized
  Curricula
Authors: Haoyi Niu, Yizhou Xu, Xingjian Jiang, Jianming Hu
Categories: cs.LG cs.AI cs.RO
Comments: ICRA 2024
\\ ( https://arxiv.org/abs/2309.14209 ,  2891kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03572
replaced with revised version Wed, 6 Mar 2024 14:54:00 GMT   (9551kb,D)

Title: Residual Multi-Fidelity Neural Network Computing
Authors: Owen Davis, Mohammad Motamed, Raul Tempone
Categories: cs.LG cs.NA math.NA
MSC-class: 65C30, 65C40, 68T07
\\ ( https://arxiv.org/abs/2310.03572 ,  9551kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13225
replaced with revised version Tue, 5 Mar 2024 21:02:04 GMT   (9680kb,D)

Title: Scalable Neural Network Kernels
Authors: Arijit Sehanobish, Krzysztof Choromanski, Yunfan Zhao, Avinava Dubey,
  Valerii Likhosherstov
Categories: cs.LG cs.AI
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.13225 ,  9680kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14814
replaced with revised version Wed, 6 Mar 2024 10:46:41 GMT   (2531kb,D)

Title: Leveraging Ensemble Diversity for Robust Self-Training in the Presence
  of Sample Selection Bias
Authors: Ambroise Odonnat, Vasilii Feofanov, Ievgen Redko
Categories: cs.LG cs.AI stat.ML
Comments: Accepted at AISTATS 2024, Valencia, Spain
\\ ( https://arxiv.org/abs/2310.14814 ,  2531kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03260
replaced with revised version Wed, 6 Mar 2024 01:28:16 GMT   (88kb,D)

Title: From Coupled Oscillators to Graph Neural Networks: Reducing
  Over-smoothing via a Kuramoto Model-based Approach
Authors: Tuan Nguyen, Hirotada Honda, Takashi Sano, Vinh Nguyen, Shugo
  Nakamura, Tan M. Nguyen
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2311.03260 ,  88kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07565
replaced with revised version Wed, 6 Mar 2024 12:28:17 GMT   (247kb,D)

Title: Exploration via linearly perturbed loss minimisation
Authors: David Janz, Shuai Liu, Alex Ayoub, Csaba Szepesv\'ari
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2311.07565 ,  247kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05409
replaced with revised version Wed, 6 Mar 2024 18:18:15 GMT   (1208kb,D)

Title: Large-scale Training of Foundation Models for Wearable Biosignals
Authors: Salar Abbaspourazad, Oussama Elachqar, Andrew C. Miller, Saba Emrani,
  Udhyakumar Nallasamy, Ian Shapiro
Categories: cs.LG cs.AI eess.SP
Comments: Camera ready version for ICLR 2024
\\ ( https://arxiv.org/abs/2312.05409 ,  1208kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12869
replaced with revised version Wed, 6 Mar 2024 15:14:11 GMT   (11176kb,D)

Title: Parameterized Projected Bellman Operator
Authors: Th\'eo Vincent, Alberto Maria Metelli, Boris Belousov, Jan Peters,
  Marcello Restelli and Carlo D'Eramo
Categories: cs.LG cs.AI
Comments: Proceedings of the National Conference on Artificial Intelligence
  (AAAI-24)
\\ ( https://arxiv.org/abs/2312.12869 ,  11176kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03299
replaced with revised version Wed, 6 Mar 2024 04:28:09 GMT   (5101kb,D)

Title: GUARD: Role-playing to Generate Natural-language Jailbreakings to Test
  Guideline Adherence of Large Language Models
Authors: Haibo Jin, Ruoxi Chen, Andy Zhou, Jinyin Chen, Yang Zhang, Haohan Wang
Categories: cs.LG cs.CL cs.CV
Comments: 22 papges
\\ ( https://arxiv.org/abs/2402.03299 ,  5101kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16899
replaced with revised version Wed, 6 Mar 2024 06:59:46 GMT   (249kb,D)

Title: A priori Estimates for Deep Residual Network in Continuous-time
  Reinforcement Learning
Authors: Shuyu Yin, Qixuan Zhou, Fei Wen, Tao Luo
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.16899 ,  249kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18571
replaced with revised version Wed, 6 Mar 2024 08:07:02 GMT   (320kb,D)

Title: Arithmetic Control of LLMs for Diverse User Preferences: Directional
  Preference Alignment with Multi-Objective Rewards
Authors: Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu,
  Han Zhao, Tong Zhang
Categories: cs.LG cs.AI cs.CL stat.ML
Comments: The code and model are released at
  https://github.com/Haoxiang-Wang/directional-preference-alignment
\\ ( https://arxiv.org/abs/2402.18571 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19290
replaced with revised version Wed, 6 Mar 2024 14:14:24 GMT   (498kb)

Title: Estimation and Deconvolution of Second Order Cyclostationary Signals
Authors: Igor Makienko, Michael Grebshtein, Eli Gildish
Categories: cs.LG eess.SP
Comments: 11 pages, 4 figures
\\ ( https://arxiv.org/abs/2402.19290 ,  498kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19322
replaced with revised version Wed, 6 Mar 2024 14:12:56 GMT   (3310kb,D)

Title: Verification of Neural Networks' Global Robustness
Authors: Anan Kabaha, Dana Drachsler-Cohen
Categories: cs.LG cs.CR cs.PL
\\ ( https://arxiv.org/abs/2402.19322 ,  3310kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01317
replaced with revised version Wed, 6 Mar 2024 06:11:33 GMT   (5396kb,D)

Title: Less is More: Hop-Wise Graph Attention for Scalable and Generalizable
  Learning on Circuits
Authors: Chenhui Deng, Zichao Yue, Cunxi Yu, Gokce Sarar, Ryan Carey, Rajeev
  Jain, Zhiru Zhang
Categories: cs.LG cs.AR
Comments: Published as a conference paper at Design Automation Conference (DAC)
  2024
\\ ( https://arxiv.org/abs/2403.01317 ,  5396kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02630
replaced with revised version Wed, 6 Mar 2024 06:46:52 GMT   (1480kb,D)

Title: FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal
  Decoupling
Authors: Hongyu Zhang, Dongyi Zheng, Lin Zhong, Xu Yang, Jiyuan Feng, Yunqing
  Feng, Qing Liao
Categories: cs.LG cs.IR cs.SI
Comments: 16 pages, 5 figures
\\ ( https://arxiv.org/abs/2403.02630 ,  1480kb)
------------------------------------------------------------------------------
\\
arXiv:2107.13509
replaced with revised version Tue, 5 Mar 2024 20:33:44 GMT   (3435kb,D)

Title: The Who in XAI: How AI Background Shapes Perceptions of AI Explanations
Authors: Upol Ehsan, Samir Passi, Q. Vera Liao, Larry Chan, I-Hsiang Lee,
  Michael Muller, Mark O. Riedl
Categories: cs.HC cs.AI cs.CY
Journal-ref: ACM CHI 2024
DOI: 10.1145/3613904.3642474
\\ ( https://arxiv.org/abs/2107.13509 ,  3435kb)
------------------------------------------------------------------------------
\\
arXiv:2209.00381
replaced with revised version Wed, 6 Mar 2024 12:52:22 GMT   (25358kb,D)

Title: SemSegDepth: A Combined Model for Semantic Segmentation and Depth
  Completion
Authors: Juan Pablo Lagos and Esa Rahtu
Categories: cs.CV cs.AI cs.LG
Journal-ref: 17th VISIGRAPP 2022 - Volume 5: VISAPP
DOI: 10.5220/0010838500003124
\\ ( https://arxiv.org/abs/2209.00381 ,  25358kb)
------------------------------------------------------------------------------
\\
arXiv:2210.10971 (*cross-listing*)
replaced with revised version Wed, 6 Mar 2024 02:15:24 GMT   (2017kb)

Title: Optimal Settings for Cryptocurrency Trading Pairs
Authors: Di Zhang, Youzhou Zhou
Categories: q-fin.TR cs.AI math.OC
\\ ( https://arxiv.org/abs/2210.10971 ,  2017kb)
------------------------------------------------------------------------------
\\
arXiv:2211.06753
replaced with revised version Tue, 5 Mar 2024 19:55:14 GMT   (13881kb,D)

Title: Seamful XAI: Operationalizing Seamful Design in Explainable AI
Authors: Upol Ehsan, Q. Vera Liao, Samir Passi, Mark O. Riedl, Hal Daume III
Categories: cs.HC cs.AI
Journal-ref: ACM CSCW 2024
DOI: 10.1145/3637396
\\ ( https://arxiv.org/abs/2211.06753 ,  13881kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09553
replaced with revised version Wed, 6 Mar 2024 16:16:19 GMT   (4698kb,D)

Title: Causal-Story: Local Causal Attention Utilizing Parameter-Efficient
  Tuning For Visual Story Synthesis
Authors: Tianyi Song, Jiuxin Cao, Kun Wang, Bo Liu, Xiaofeng Zhang
Categories: cs.CV cs.AI cs.MM
Comments: Accepted by 2024 International Conference on Acoustics, Speech and
  Signal Processing(ICASSP 2024)
\\ ( https://arxiv.org/abs/2309.09553 ,  4698kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00354
replaced with revised version Wed, 6 Mar 2024 13:05:52 GMT   (3553kb,AD)

Title: AI-Dentify: Deep learning for proximal caries detection on bitewing
  x-ray -- HUNT4 Oral Health Study
Authors: Javier P\'erez de Frutos, Ragnhild Holden Helland, Shreya Desai, Line
  Cathrine Nymoen, Thomas Lang{\o}, Theodor Remman, Abhijit Sen
Categories: cs.CV cs.AI
Comments: 24 pages, 5 figure, 7 tables
ACM-class: I.2.10; I.2.1
\\ ( https://arxiv.org/abs/2310.00354 ,  3553kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04687
replaced with revised version Wed, 6 Mar 2024 18:14:58 GMT   (31674kb,D)

Title: Improving Adversarial Attacks on Latent Diffusion Model
Authors: Boyang Zheng, Chumeng Liang, Xiaoyu Wu, Yan Liu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2310.04687 ,  31674kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17695
replaced with revised version Wed, 6 Mar 2024 11:13:43 GMT   (35902kb,D)

Title: Fair Text-to-Image Diffusion via Fair Mapping
Authors: Jia Li, Lijie Hu, Jingfeng Zhang, Tianhang Zheng, Hua Zhang, Di Wang
Categories: cs.CV cs.AI cs.CY cs.LG
\\ ( https://arxiv.org/abs/2311.17695 ,  35902kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07932 (*cross-listing*)
replaced with revised version Wed, 6 Mar 2024 02:44:35 GMT   (291kb,D)

Title: A Novel Image Classification Framework Based on Variational Quantum
  Algorithms
Authors: Yixiong Chen
Categories: quant-ph cs.AI cs.CV
\\ ( https://arxiv.org/abs/2312.07932 ,  291kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17285
replaced with revised version Tue, 5 Mar 2024 23:40:45 GMT   (38981kb,D)

Title: Understanding Distributed Representations of Concepts in Deep Neural
  Networks without Supervision
Authors: Wonjoon Chang, Dahee Kwon, Jaesik Choi
Categories: cs.CV cs.AI cs.LG
Comments: Published in AAAI2024. First two authors contributed equally. The
  code is available at https://github.com/daheekwon/RDR
\\ ( https://arxiv.org/abs/2312.17285 ,  38981kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04575
replaced with revised version Tue, 5 Mar 2024 21:02:33 GMT   (47099kb,D)

Title: Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual
  Concept Understanding
Authors: Yatong Bai, Utsav Garg, Apaar Shanker, Haoming Zhang, Samyak Parajuli,
  Erhan Bas, Isidora Filipovic, Amelia N. Chu, Eugenia D Fomitcheva, Elliot
  Branson, Aerin Kim, Somayeh Sojoudi, Kyunghyun Cho
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2401.04575 ,  47099kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06401
replaced with revised version Wed, 6 Mar 2024 02:16:51 GMT   (0kb,I)

Title: DevEval: Evaluating Code Generation in Practical Software Projects
Authors: Jia Li, Ge Li, Yunfei Zhao, Yongmin Li, Zhi Jin, Hao Zhu, Huanyu Liu,
  Kaibo Liu, Lecheng Wang, Zheng Fang, Lanshen Wang, Jiazheng Ding, Xuanming
  Zhang, Yihong Dong, Yuqi Zhu, Bin Gu, Mengfei Yang
Categories: cs.SE cs.AI cs.CL
Comments: We are re-checking this benchmark and repeating related experiments.
  New versions of DevEval will be released later
\\ ( https://arxiv.org/abs/2401.06401 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09340
replaced with revised version Wed, 6 Mar 2024 07:54:47 GMT   (32021kb,D)

Title: SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene
  Understanding
Authors: Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu
  Liu, Qing Li, Siyuan Huang
Categories: cs.CV cs.AI cs.CL cs.LG cs.RO
Comments: 21 pages
\\ ( https://arxiv.org/abs/2401.09340 ,  32021kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10712
replaced with revised version Wed, 6 Mar 2024 12:51:11 GMT   (4911kb,D)

Title: Q&A Prompts: Discovering Rich Visual Clues through Mining
  Question-Answer Prompts for VQA requiring Diverse World Knowledge
Authors: Haibi Wang, Weifeng Ge
Categories: cs.CV cs.AI cs.CL
\\ ( https://arxiv.org/abs/2401.10712 ,  4911kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07153
replaced with revised version Wed, 6 Mar 2024 00:26:02 GMT   (3007kb,D)

Title: Error Estimation for Physics-informed Neural Networks Approximating
  Semilinear Wave Equations
Authors: Beatrice Lorenz, Aras Bacho, Gitta Kutyniok
Categories: math.NA cs.AI cs.NA
MSC-class: 35L05, 68T07, 65M15, 35G50, 35A35
\\ ( https://arxiv.org/abs/2402.07153 ,  3007kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08144
replaced with revised version Wed, 6 Mar 2024 00:15:56 GMT   (100kb,D)

Title: Average-Case Analysis of Iterative Voting
Authors: Joshua Kavner, Lirong Xia
Categories: cs.GT cs.AI
Comments: 75 pages, 2 figures
\\ ( https://arxiv.org/abs/2402.08144 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10251 (*cross-listing*)
replaced with revised version Wed, 6 Mar 2024 09:04:32 GMT   (5572kb,D)

Title: Brant-2: Foundation Model for Brain Signals
Authors: Zhizhang Yuan, Daoze Zhang, Junru Chen, Gefei Gu, Yang Yang
Categories: q-bio.NC cs.AI cs.LG eess.SP
Comments: 14 pages, 7 figures
\\ ( https://arxiv.org/abs/2402.10251 ,  5572kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16073
replaced with revised version Wed, 6 Mar 2024 10:03:06 GMT   (685kb,D)

Title: Pfeed: Generating near real-time personalized feeds using precomputed
  embedding similarities
Authors: Binyam Gebre, Karoliina Ranta, Stef van den Elzen, Ernst Kuiper, Thijs
  Baars, Tom Heskes
Categories: cs.IR cs.AI cs.LG
Comments: 9 pages, 8 figures
ACM-class: H.3.3
\\ ( https://arxiv.org/abs/2402.16073 ,  685kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18759
replaced with revised version Wed, 6 Mar 2024 15:53:46 GMT   (24208kb,D)

Title: Learning with Language-Guided State Abstractions
Authors: Andi Peng, Ilia Sucholutsky, Belinda Z. Li, Theodore R. Sumers, Thomas
  L. Griffiths, Jacob Andreas, Julie A. Shah
Categories: cs.RO cs.AI cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2402.18759 ,  24208kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19379
replaced with revised version Wed, 6 Mar 2024 18:44:13 GMT   (248kb,D)

Title: Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Rival
  Human Crowd Accuracy
Authors: Philipp Schoenegger, Indre Tuminauskaite, Peter S. Park, Philip E.
  Tetlock
Categories: cs.CY cs.AI cs.CL cs.LG
Comments: 20 pages; 13 visualizations (nine figures, four tables)
\\ ( https://arxiv.org/abs/2402.19379 ,  248kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01818
replaced with revised version Wed, 6 Mar 2024 10:54:24 GMT   (14637kb,D)

Title: AllSpark: Reborn Labeled Features from Unlabeled in Transformer for
  Semi-Supervised Semantic Segmentation
Authors: Haonan Wang, Qixiang Zhang, Yi Li, Xiaomeng Li
Categories: cs.CV cs.AI
Comments: Accepted by CVPR 2024; correct typos; this is not the camera-ready
  version
\\ ( https://arxiv.org/abs/2403.01818 ,  14637kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02118
replaced with revised version Wed, 6 Mar 2024 12:29:36 GMT   (20900kb,D)

Title: Towards Implicit Prompt For Text-To-Image Models
Authors: Yue Yang, Yuqi lin, Hong Liu, Wenqi Shao, Runjian Chen, Hailong Shang,
  Yu Wang, Yu Qiao, Kaipeng Zhang, Ping Luo
Categories: cs.CY cs.AI cs.CV
\\ ( https://arxiv.org/abs/2403.02118 ,  20900kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02131
replaced with revised version Wed, 6 Mar 2024 10:44:08 GMT   (10527kb,D)

Title: Deep Reinforcement Learning for Dynamic Algorithm Selection: A
  Proof-of-Principle Study on Differential Evolution
Authors: Hongshu Guo, Yining Ma, Zeyuan Ma, Jiacheng Chen, Xinglin Zhang,
  Zhiguang Cao, Jun Zhang, Yue-Jiao Gong
Categories: cs.NE cs.AI
Comments: Accepted by IEEE Transactions on Systems, Man, and Cybernetics:
  Systems at Thu, Feb 29, 2024
\\ ( https://arxiv.org/abs/2403.02131 ,  10527kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02371 (*cross-listing*)
replaced with revised version Wed, 6 Mar 2024 11:08:02 GMT   (978kb,D)

Title: NeuroVoz: a Castillian Spanish corpus of parkinsonian speech
Authors: Jana\'ina Mendes-Laureano, Jorge A. G\'omez-Garc\'ia, Alejandro
  Guerrero-L\'opez, Elisa Luque-Buzo, Juli\'an D. Arias-Londo\~no, Francisco J.
  Grandas-P\'erez, Juan I. Godino-Llorente
Categories: eess.AS cs.AI cs.CL cs.SD
Comments: Preprint version
\\ ( https://arxiv.org/abs/2403.02371 ,  978kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02910
replaced with revised version Wed, 6 Mar 2024 04:29:32 GMT   (9475kb,D)

Title: ImgTrojan: Jailbreaking Vision-Language Models with ONE Image
Authors: Xijia Tao, Shuai Zhong, Lei Li, Qi Liu, Lingpeng Kong
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.02910 ,  9475kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09778
replaced with revised version Wed, 6 Mar 2024 00:38:04 GMT   (26984kb,D)

Title: Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language
  Models
Authors: Navid Rajabi, Jana Kosecka
Categories: cs.CV cs.CL cs.LG
Comments: Accepted to DMLR @ ICLR 2024
\\ ( https://arxiv.org/abs/2308.09778 ,  26984kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07277
replaced with revised version Wed, 6 Mar 2024 17:46:50 GMT   (254kb,D)

Title: AdaCCD: Adaptive Semantic Contrasts Discovery Based Cross Lingual
  Adaptation for Code Clone Detection
Authors: Yangkai Du, Tengfei Ma, Lingfei Wu, Xuhong Zhang, Shouling Ji
Categories: cs.SE cs.CL
\\ ( https://arxiv.org/abs/2311.07277 ,  254kb)
------------------------------------------------------------------------------
\\
arXiv:2210.00212 (*cross-listing*)
replaced with revised version Wed, 6 Mar 2024 06:34:53 GMT   (179kb,D)

Title: Efficient Quantum Agnostic Improper Learning of Decision Trees
Authors: Sagnik Chatterjee, Tharrmashastha SAPV, Debajyoti Bera
Categories: quant-ph cs.LG
Comments: Accepted at AISTATS 2024
\\ ( https://arxiv.org/abs/2210.00212 ,  179kb)
------------------------------------------------------------------------------
\\
arXiv:2302.03658
replaced with revised version Wed, 6 Mar 2024 08:30:48 GMT   (83kb)

Title: Planted Bipartite Graph Detection
Authors: Asaf Rotenberg and Wasim Huleihel and Ofer Shayevitz
Categories: cs.DS cs.IT cs.LG math.IT math.ST stat.TH
Comments: 37 pages
\\ ( https://arxiv.org/abs/2302.03658 ,  83kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18231 (*cross-listing*)
replaced with revised version Wed, 6 Mar 2024 18:27:34 GMT   (41707kb,D)

Title: High-Fidelity Image Compression with Score-based Generative Models
Authors: Emiel Hoogeboom, Eirikur Agustsson, Fabian Mentzer, Luca Versari,
  George Toderici, Lucas Theis
Categories: eess.IV cs.CV cs.LG stat.ML
\\ ( https://arxiv.org/abs/2305.18231 ,  41707kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05366
replaced with revised version Wed, 6 Mar 2024 13:28:48 GMT   (8194kb,D)

Title: Ordinal Potential-based Player Rating
Authors: Nelson Vadori and Rahul Savani
Categories: cs.GT cs.LG
\\ ( https://arxiv.org/abs/2306.05366 ,  8194kb)
------------------------------------------------------------------------------
\\
arXiv:2308.03417
replaced with revised version Wed, 6 Mar 2024 09:58:49 GMT   (3157kb,D)

Title: PURL: Safe and Effective Sanitization of Link Decoration
Authors: Shaoor Munir, Patrick Lee, Umar Iqbal, Zubair Shafiq, Sandra Siby
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2308.03417 ,  3157kb)
------------------------------------------------------------------------------
\\
arXiv:2308.03686 (*cross-listing*)
replaced with revised version Wed, 6 Mar 2024 00:41:30 GMT   (174kb,D)

Title: Nearly $d$-Linear Convergence Bounds for Diffusion Models via Stochastic
  Localization
Authors: Joe Benton, Valentin De Bortoli, Arnaud Doucet, George Deligiannidis
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2308.03686 ,  174kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05027 (*cross-listing*)
replaced with revised version Wed, 6 Mar 2024 17:15:51 GMT   (33517kb,D)

Title: AbDiffuser: Full-Atom Generation of in vitro Functioning Antibodies
Authors: Karolis Martinkus, Jan Ludwiczak, Kyunghyun Cho, Wei-Ching Liang,
  Julien Lafrance-Vanasse, Isidro Hotzel, Arvind Rajpal, Yan Wu, Richard
  Bonneau, Vladimir Gligorijevic, Andreas Loukas
Categories: q-bio.BM cs.LG stat.ML
Comments: NeurIPS 2023
\\ ( https://arxiv.org/abs/2308.05027 ,  33517kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12061
replaced with revised version Wed, 6 Mar 2024 00:45:19 GMT   (20841kb,D)

Title: HarvestNet: A Dataset for Detecting Smallholder Farming Activity Using
  Harvest Piles and Remote Sensing
Authors: Jonathan Xu, Amna Elmustafa, Liya Weldegebriel, Emnet Negash, Richard
  Lee, Chenlin Meng, Stefano Ermon, David Lobell
Categories: cs.CV cs.LG
Comments: submitted to AAAI24
\\ ( https://arxiv.org/abs/2308.12061 ,  20841kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01243
replaced with revised version Tue, 5 Mar 2024 21:23:15 GMT   (7887kb,D)

Title: The Normal Distributions Indistinguishability Spectrum and its
  Application to Privacy-Preserving Machine Learning
Authors: Yun Lu, Malik Magdon-Ismail, Yu Wei, Vassilis Zikas
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2309.01243 ,  7887kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12380 (*cross-listing*)
replaced with revised version Wed, 6 Mar 2024 09:22:40 GMT   (1061kb)

Title: Methods for generating and evaluating synthetic longitudinal patient
  data: a systematic review
Authors: Katariina Perkonoja and Kari Auranen and Joni Virta
Categories: stat.ME cs.CR cs.LG stat.AP
\\ ( https://arxiv.org/abs/2309.12380 ,  1061kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19515 (*cross-listing*)
replaced with revised version Wed, 6 Mar 2024 14:07:24 GMT   (7346kb,D)

Title: Transformer-based nowcasting of radar composites from satellite images
  for severe weather
Authors: \c{C}a\u{g}lar K\"u\c{c}\"uk and Apostolos Giannakos and Stefan
  Schneider and Alexander Jann
Categories: physics.ao-ph cs.CV cs.LG eess.IV
Comments: 17 pages, 3 figures, and further supplementary figures. Accepted to
  Artificial Intelligence for Earth Systems
\\ ( https://arxiv.org/abs/2310.19515 ,  7346kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20172 (*cross-listing*)
replaced with revised version Wed, 6 Mar 2024 03:27:45 GMT   (5545kb,D)

Title: Compact Binary Systems Waveform Generation with Generative Pre-trained
  Transformer
Authors: Ruijun Shi, Yue Zhou, Tianyu Zhao, Zhoujian Cao, Zhixiang Ren
Categories: gr-qc astro-ph.IM cs.LG
\\ ( https://arxiv.org/abs/2310.20172 ,  5545kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08376 (*cross-listing*)
replaced with revised version Wed, 6 Mar 2024 12:31:27 GMT   (45kb)

Title: Ensemble sampling for linear bandits: small ensembles suffice
Authors: David Janz, Alexander E. Litvak, Csaba Szepesv\'ari
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2311.08376 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10187 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 19:40:24 GMT   (462kb,D)

Title: TSRNet: Simple Framework for Real-time ECG Anomaly Detection with
  Multimodal Time and Spectrogram Restoration Network
Authors: Nhat-Tan Bui and Dinh-Hieu Hoang and Thinh Phan and Minh-Triet Tran
  and Brijesh Patel and Donald Adjeroh and Ngan Le
Categories: eess.SP cs.LG
Comments: Accepted at ISBI 2024
\\ ( https://arxiv.org/abs/2312.10187 ,  462kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07342 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 21:12:26 GMT   (734kb,D)

Title: Who Said What? An Automated Approach to Analyzing Speech in Preschool
  Classrooms
Authors: Anchen Sun, Juan J Londono, Batya Elbaum, Luis Estrada, Roberto Jose
  Lazo, Laura Vitale, Hugo Gonzalez Villasanti, Riccardo Fusaroli, Lynn K
  Perry, Daniel S Messinger
Categories: eess.AS cs.LG
Comments: 7 pages, 3 figures, 3 tables, The paper has been accepted to 2024
  IEEE International Conference on Development and Learning (ICDL) as a full
  oral presentation and will appear in the ICDL proceedings
\\ ( https://arxiv.org/abs/2401.07342 ,  734kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03077
replaced with revised version Wed, 6 Mar 2024 12:37:20 GMT   (407kb)

Title: Markov Persuasion Processes: Learning to Persuade from Scratch
Authors: Francesco Bacchiocchi, Francesco Emanuele Stradi, Matteo Castiglioni,
  Alberto Marchesi, Nicola Gatti
Categories: cs.GT cs.LG
\\ ( https://arxiv.org/abs/2402.03077 ,  407kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03302 (*cross-listing*)
replaced with revised version Wed, 6 Mar 2024 13:29:29 GMT   (5212kb,D)

Title: Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining
Authors: Jiarun Liu, Hao Yang, Hong-Yu Zhou, Yan Xi, Lequan Yu, Yizhou Yu, Yong
  Liang, Guangming Shi, Shaoting Zhang, Hairong Zheng, Shanshan Wang
Categories: eess.IV cs.CV cs.LG
Comments: Code and models of Swin-UMamba are publicly available at:
  https://github.com/JiarunLiu/Swin-UMamba
\\ ( https://arxiv.org/abs/2402.03302 ,  5212kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16712 (*cross-listing*)
replaced with revised version Wed, 6 Mar 2024 17:16:38 GMT   (81kb,D)

Title: l1-norm regularized l1-norm best-fit lines
Authors: Xiao Ling, Paul Brooks
Categories: stat.ML cs.LG math.OC
Comments: 10 pages
\\ ( https://arxiv.org/abs/2402.16712 ,  81kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16796
replaced with revised version Wed, 6 Mar 2024 02:19:38 GMT   (25388kb,D)

Title: Expressive Whole-Body Control for Humanoid Robots
Authors: Xuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang, Ge Yang, Xiaolong
  Wang
Categories: cs.RO cs.LG
Comments: Website: https://expressive-humanoid.github.io
\\ ( https://arxiv.org/abs/2402.16796 ,  25388kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01636 (*cross-listing*)
replaced with revised version Wed, 6 Mar 2024 04:34:01 GMT   (1668kb,D)

Title: Sample Efficient Myopic Exploration Through Multitask Reinforcement
  Learning with Diverse Tasks
Authors: Ziping Xu, Zifan Xu, Runxuan Jiang, Peter Stone, Ambuj Tewari
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2403.01636 ,  1668kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
