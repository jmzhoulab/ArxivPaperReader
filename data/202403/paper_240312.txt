paper_240312.txt

Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月12日 16:24
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Fri  8 Mar 24 19:00:00 GMT  to  Mon 11 Mar 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.05632
Date: Fri, 8 Mar 2024 19:16:29 GMT   (210kb,D)

Title: Can Large Language Models Play Games? A Case Study of A Self-Play
  Approach
Authors: Hongyi Guo, Zhihan Liu, Yufeng Zhang, Zhaoran Wang
Categories: cs.AI
\\
  Large Language Models (LLMs) harness extensive data from the Internet,
storing a broad spectrum of prior knowledge. While LLMs have proven beneficial
as decision-making aids, their reliability is hampered by limitations in
reasoning, hallucination phenomenon, and so on. On the other hand, Monte-Carlo
Tree Search (MCTS) is a heuristic search algorithm that provides reliable
decision-making solutions, achieved through recursive rollouts and self-play.
However, the effectiveness of MCTS relies heavily on heuristic pruning and
external value functions, particularly in complex decision scenarios. This work
introduces an innovative approach that bolsters LLMs with MCTS self-play to
efficiently resolve deterministic turn-based zero-sum games (DTZG), such as
chess and go, without the need for additional training. Specifically, we
utilize LLMs as both action pruners and proxies for value functions without the
need for additional training. We theoretically prove that the suboptimality of
the estimated value in our proposed method scales with $\tilde{\mathcal
O}\Bigl(\frac{|\tilde {\mathcal A}|}{\sqrt{N}} + \epsilon_\mathrm{pruner} +
\epsilon_\mathrm{critic}\Bigr)$, where \(N\) is the number of simulations,
$|\tilde {\mathcal A}|$ is the cardinality of the pruned action space by LLM,
and $\epsilon_\mathrm{pruner}$ and $\epsilon_\mathrm{critic}$ quantify the
errors incurred by adopting LLMs as action space pruner and value function
proxy, respectively. Our experiments in chess and go demonstrate the capability
of our method to address challenges beyond the scope of MCTS and improve the
performance of the directly application of LLMs.
\\ ( https://arxiv.org/abs/2403.05632 ,  210kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05636
Date: Fri, 8 Mar 2024 19:18:53 GMT   (2108kb,D)

Title: Tuning-Free Accountable Intervention for LLM Deployment -- A
  Metacognitive Approach
Authors: Zhen Tan, Jie Peng, Tianlong Chen, Huan Liu
Categories: cs.AI cs.CL
\\
  Large Language Models (LLMs) have catalyzed transformative advances across a
spectrum of natural language processing tasks through few-shot or zero-shot
prompting, bypassing the need for parameter tuning. While convenient, this
modus operandi aggravates ``hallucination'' concerns, particularly given the
enigmatic ``black-box'' nature behind their gigantic model sizes. Such concerns
are exacerbated in high-stakes applications (e.g., healthcare), where
unaccountable decision errors can lead to devastating consequences. In
contrast, human decision-making relies on nuanced cognitive processes, such as
the ability to sense and adaptively correct misjudgments through conceptual
understanding. Drawing inspiration from human cognition, we propose an
innovative \textit{metacognitive} approach, dubbed \textbf{CLEAR}, to equip
LLMs with capabilities for self-aware error identification and correction. Our
framework facilitates the construction of concept-specific sparse subnetworks
that illuminate transparent decision pathways. This provides a novel interface
for model \textit{intervention} after deployment. Our intervention offers
compelling advantages: (\textit{i})~at deployment or inference time, our
metacognitive LLMs can self-consciously identify potential mispredictions with
minimum human involvement, (\textit{ii})~the model has the capability to
self-correct its errors efficiently, obviating the need for additional tuning,
and (\textit{iii})~the rectification procedure is not only self-explanatory but
also user-friendly, enhancing the interpretability and accessibility of the
model. By integrating these metacognitive features, our approach pioneers a new
path toward engendering greater trustworthiness and accountability in the
deployment of LLMs.
\\ ( https://arxiv.org/abs/2403.05636 ,  2108kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05641
Date: Fri, 8 Mar 2024 19:26:30 GMT   (1147kb)

Title: A Feature-based Generalizable Prediction Model for Both Perceptual and
  Abstract Reasoning
Authors: Quan Do, Thomas M. Morin, Chantal E. Stern, Michael E. Hasselmo
Categories: cs.AI q-bio.NC
\\
  A hallmark of human intelligence is the ability to infer abstract rules from
limited experience and apply these rules to unfamiliar situations. This
capacity is widely studied in the visual domain using the Raven's Progressive
Matrices. Recent advances in deep learning have led to multiple artificial
neural network models matching or even surpassing human performance. However,
while humans can identify and express the rule underlying these tasks with
little to no exposure, contemporary neural networks often rely on massive
pattern-based training and cannot express or extrapolate the rule inferred from
the task. Furthermore, most Raven's Progressive Matrices or Raven-like tasks
used for neural network training used symbolic representations, whereas humans
can flexibly switch between symbolic and continuous perceptual representations.
In this work, we present an algorithmic approach to rule detection and
application using feature detection, affine transformation estimation and
search. We applied our model to a simplified Raven's Progressive Matrices task,
previously designed for behavioral testing and neuroimaging in humans. The
model exhibited one-shot learning and achieved near human-level performance in
the symbolic reasoning condition of the simplified task. Furthermore, the model
can express the relationships discovered and generate multi-step predictions in
accordance with the underlying rule. Finally, the model can reason using
continuous patterns. We discuss our results and their relevance to studying
abstract reasoning in humans, as well as their implications for improving
intelligent machines.
\\ ( https://arxiv.org/abs/2403.05641 ,  1147kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05680
Date: Fri, 8 Mar 2024 21:16:28 GMT   (1350kb,D)

Title: Decomposing Vision-based LLM Predictions for Auto-Evaluation with GPT-4
Authors: Qingqing Zhu, Benjamin Hou, Tejas S. Mathai, Pritam Mukherjee, Qiao
  Jin, Xiuying Chen, Zhizheng Wang, Ruida Cheng, Ronald M. Summers, and Zhiyong
  Lu
Categories: cs.AI cs.CL cs.CV
\\
  The volume of CT exams being done in the world has been rising every year,
which has led to radiologist burn-out. Large Language Models (LLMs) have the
potential to reduce their burden, but their adoption in the clinic depends on
radiologist trust, and easy evaluation of generated content. Presently, many
automated methods are available to evaluate the reports generated for chest
radiographs, but such an approach is not available for CT presently. In this
paper, we propose a novel evaluation framework to judge the capabilities of
vision-language LLMs in generating accurate summaries of CT-based
abnormalities. CT slices containing an abnormality (e.g., lesion) were input to
a vision-based LLM (GPT-4V, LLaVA-Med, and RadFM), and it generated a free-text
summary of the predicted characteristics of the abnormality. Next, a GPT-4
model decomposed the summary into specific aspects (body part, location, type,
and attributes), automatically evaluated the characteristics against the
ground-truth, and generated a score for each aspect based on its clinical
relevance and factual accuracy. These scores were then contrasted against those
obtained from a clinician, and a high correlation ( 85%, p < .001) was
observed. Although GPT-4V outperformed other models in our evaluation, it still
requires overall improvement. Our evaluation method offers valuable insights
into the specific areas that need the most enhancement, guiding future
development in this field.
\\ ( https://arxiv.org/abs/2403.05680 ,  1350kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05683
Date: Fri, 8 Mar 2024 21:31:00 GMT   (7434kb,D)

Title: Efficient Public Health Intervention Planning Using Decomposition-Based
  Decision-Focused Learning
Authors: Sanket Shah, Arun Suggala, Milind Tambe, Aparna Taneja
Categories: cs.AI cs.LG
Comments: 12 pages, 3 figures, 2 tables
\\
  The declining participation of beneficiaries over time is a key concern in
public health programs. A popular strategy for improving retention is to have
health workers `intervene' on beneficiaries at risk of dropping out. However,
the availability and time of these health workers are limited resources. As a
result, there has been a line of research on optimizing these limited
intervention resources using Restless Multi-Armed Bandits (RMABs). The key
technical barrier to using this framework in practice lies in the need to
estimate the beneficiaries' RMAB parameters from historical data. Recent
research has shown that Decision-Focused Learning (DFL), which focuses on
maximizing the beneficiaries' adherence rather than predictive accuracy,
improves the performance of intervention targeting using RMABs. Unfortunately,
these gains come at a high computational cost because of the need to solve and
evaluate the RMAB in each DFL training step. In this paper, we provide a
principled way to exploit the structure of RMABs to speed up intervention
planning by cleverly decoupling the planning for different beneficiaries. We
use real-world data from an Indian NGO, ARMMAN, to show that our approach is up
to two orders of magnitude faster than the state-of-the-art approach while also
yielding superior model performance. This would enable the NGO to scale up
deployments using DFL to potentially millions of mothers, ultimately advancing
progress toward UNSDG 3.1.
\\ ( https://arxiv.org/abs/2403.05683 ,  7434kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05732
Date: Fri, 8 Mar 2024 23:59:38 GMT   (578kb,D)

Title: Conservative DDPG -- Pessimistic RL without Ensemble
Authors: Nitsan Soffair, Shie Mannor
Categories: cs.AI cs.LG
\\
  DDPG is hindered by the overestimation bias problem, wherein its
$Q$-estimates tend to overstate the actual $Q$-values. Traditional solutions to
this bias involve ensemble-based methods, which require significant
computational resources, or complex log-policy-based approaches, which are
difficult to understand and implement. In contrast, we propose a
straightforward solution using a $Q$-target and incorporating a behavioral
cloning (BC) loss penalty. This solution, acting as an uncertainty measure, can
be easily implemented with minimal code and without the need for an ensemble.
Our empirical findings strongly support the superiority of Conservative DDPG
over DDPG across various MuJoCo and Bullet tasks. We consistently observe
better performance in all evaluated tasks and even competitive or superior
performance compared to TD3 and TD7, all achieved with significantly reduced
computational requirements.
\\ ( https://arxiv.org/abs/2403.05732 ,  578kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05801
Date: Sat, 9 Mar 2024 05:34:07 GMT   (434kb,D)

Title: Enhancing Multi-Hop Knowledge Graph Reasoning through Reward Shaping
  Techniques
Authors: Chen Li, Haotian Zheng, Yiping Sun, Cangqing Wang, Liqiang Yu, Che
  Chang, Xinyu Tian, Bo Liu
Categories: cs.AI
Comments: This paper has been accepted by the 2024 5th International Seminar on
  Artificial Intelligence, Networking and Information Technology (AINIT 2024)
\\
  In the realm of computational knowledge representation, Knowledge Graph
Reasoning (KG-R) stands at the forefront of facilitating sophisticated
inferential capabilities across multifarious domains. The quintessence of this
research elucidates the employment of reinforcement learning (RL) strategies,
notably the REINFORCE algorithm, to navigate the intricacies inherent in
multi-hop KG-R. This investigation critically addresses the prevalent
challenges introduced by the inherent incompleteness of Knowledge Graphs (KGs),
which frequently results in erroneous inferential outcomes, manifesting as both
false negatives and misleading positives. By partitioning the Unified Medical
Language System (UMLS) benchmark dataset into rich and sparse subsets, we
investigate the efficacy of pre-trained BERT embeddings and Prompt Learning
methodologies to refine the reward shaping process. This approach not only
enhances the precision of multi-hop KG-R but also sets a new precedent for
future research in the field, aiming to improve the robustness and accuracy of
knowledge inference within complex KG frameworks. Our work contributes a novel
perspective to the discourse on KG reasoning, offering a methodological
advancement that aligns with the academic rigor and scholarly aspirations of
the Natural journal, promising to invigorate further advancements in the realm
of computational knowledge representation.
\\ ( https://arxiv.org/abs/2403.05801 ,  434kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05921
Date: Sat, 9 Mar 2024 14:04:06 GMT   (2414kb,D)

Title: OntoChat: a Framework for Conversational Ontology Engineering using
  Language Models
Authors: Bohui Zhang and Valentina Anita Carriero and Katrin Schreiberhuber and
  Stefani Tsaneva and Luc\'ia S\'anchez Gonz\'alez and Jongmo Kim and Jacopo de
  Berardinis
Categories: cs.AI
\\
  Ontology engineering (OE) in large projects poses a number of challenges
arising from the heterogeneous backgrounds of the various stakeholders, domain
experts, and their complex interactions with ontology designers. This
multi-party interaction often creates systematic ambiguities and biases from
the elicitation of ontology requirements, which directly affect the design,
evaluation and may jeopardise the target reuse. Meanwhile, current OE
methodologies strongly rely on manual activities (e.g., interviews, discussion
pages). After collecting evidence on the most crucial OE activities, we
introduce OntoChat, a framework for conversational ontology engineering that
supports requirement elicitation, analysis, and testing. By interacting with a
conversational agent, users can steer the creation of user stories and the
extraction of competency questions, while receiving computational support to
analyse the overall requirements and test early versions of the resulting
ontologies. We evaluate OntoChat by replicating the engineering of the Music
Meta Ontology, and collecting preliminary metrics on the effectiveness of each
component from users. We release all code at
https://github.com/King-s-Knowledge-Graph-Lab/OntoChat.
\\ ( https://arxiv.org/abs/2403.05921 ,  2414kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06086
Date: Sun, 10 Mar 2024 04:16:04 GMT   (7942kb,D)

Title: Towards Generalizable and Interpretable Motion Prediction: A Deep
  Variational Bayes Approach
Authors: Juanwu Lu, Wei Zhan, Masayoshi Tomizuka, Yeping Hu
Categories: cs.AI cs.RO
Comments: Accepted at AISTATS 2024
\\
  Estimating the potential behavior of the surrounding human-driven vehicles is
crucial for the safety of autonomous vehicles in a mixed traffic flow. Recent
state-of-the-art achieved accurate prediction using deep neural networks.
However, these end-to-end models are usually black boxes with weak
interpretability and generalizability. This paper proposes the Goal-based
Neural Variational Agent (GNeVA), an interpretable generative model for motion
prediction with robust generalizability to out-of-distribution cases. For
interpretability, the model achieves target-driven motion prediction by
estimating the spatial distribution of long-term destinations with a
variational mixture of Gaussians. We identify a causal structure among maps and
agents' histories and derive a variational posterior to enhance
generalizability. Experiments on motion prediction datasets validate that the
fitted model can be interpretable and generalizable and can achieve comparable
performance to state-of-the-art results.
\\ ( https://arxiv.org/abs/2403.06086 ,  7942kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06221
Date: Sun, 10 Mar 2024 13:58:38 GMT   (1409kb,D)

Title: TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned
  Decision
Authors: Ruiwen Zhou, Yingxuan Yang, Muning Wen, Ying Wen, Wenhao Wang,
  Chunling Xi, Guoqiang Xu, Yong Yu, Weinan Zhang
Categories: cs.AI cs.CL cs.IR
Comments: Codes available at: https://github.com/skyriver-2000/TRAD-Official
\\
  Numerous large language model (LLM) agents have been built for different
tasks like web navigation and online shopping due to LLM's wide knowledge and
text-understanding ability. Among these works, many of them utilize in-context
examples to achieve generalization without the need for fine-tuning, while few
of them have considered the problem of how to select and effectively utilize
these examples. Recently, methods based on trajectory-level retrieval with task
meta-data and using trajectories as in-context examples have been proposed to
improve the agent's overall performance in some sequential decision making
tasks. However, these methods can be problematic due to plausible examples
retrieved without task-specific state transition dynamics and long input with
plenty of irrelevant context. In this paper, we propose a novel framework
(TRAD) to address these issues. TRAD first conducts Thought Retrieval,
achieving step-level demonstration selection via thought matching, leading to
more helpful demonstrations and less irrelevant input noise. Then, TRAD
introduces Aligned Decision, complementing retrieved demonstration steps with
their previous or subsequent steps, which enables tolerance for imperfect
thought and provides a choice for balance between more context and less noise.
Extensive experiments on ALFWorld and Mind2Web benchmarks show that TRAD not
only outperforms state-of-the-art models but also effectively helps in reducing
noise and promoting generalization. Furthermore, TRAD has been deployed in
real-world scenarios of a global business insurance company and improves the
success rate of robotic process automation.
\\ ( https://arxiv.org/abs/2403.06221 ,  1409kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06294
Date: Sun, 10 Mar 2024 19:47:00 GMT   (906kb,D)

Title: ArgMed-Agents: Explainable Clinical Decision Reasoning with Large
  Language Models via Argumentation Schemes
Authors: Shengxin Hong, Liang Xiao, Xin Zhang, Jianxia Chen
Categories: cs.AI cs.MA cs.SC
\\
  There are two main barriers to using large language models (LLMs) in clinical
reasoning. Firstly, while LLMs exhibit significant promise in Natural Language
Processing (NLP) tasks, their performance in complex reasoning and planning
falls short of expectations. Secondly, LLMs use uninterpretable methods to make
clinical decisions that are fundamentally different from the clinician's
cognitive processes. This leads to user distrust. In this paper, we present a
multi-agent framework called ArgMed-Agents, which aims to enable LLM-based
agents to make explainable clinical decision reasoning through interaction.
ArgMed-Agents performs self-argumentation iterations via Argumentation Scheme
for Clinical Decision (a reasoning mechanism for modeling cognitive processes
in clinical reasoning), and then constructs the argumentation process as a
directed graph representing conflicting relationships. Ultimately, Reasoner(a
symbolic solver) identify a series of rational and coherent arguments to
support decision. ArgMed-Agents enables LLMs to mimic the process of clinical
argumentative reasoning by generating explanations of reasoning in a
self-directed manner. The setup experiments show that ArgMed-Agents not only
improves accuracy in complex clinical decision reasoning problems compared to
other prompt methods, but more importantly, it provides users with decision
explanations that increase their confidence.
\\ ( https://arxiv.org/abs/2403.06294 ,  906kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06483
Date: Mon, 11 Mar 2024 07:44:59 GMT   (955kb)

Title: The negation of permutation mass function
Authors: Yongchuan Tang, Rongfei Li
Categories: cs.AI cs.IT math.IT
\\
  Negation is a important perspective of knowledge representation. Existing
negation methods are mainly applied in probability theory, evidence theory and
complex evidence theory. As a generalization of evidence theory, random
permutation sets theory may represent information more precisely. However, how
to apply the concept of negation to random permutation sets theory has not been
studied. In this paper, the negation of permutation mass function is proposed.
Moreover, in the negation process, the convergence of proposed negation method
is verified. The trends of uncertainty and dissimilarity after each negation
operation are investigated. Numerical examples are used to demonstrate the
rationality of the proposed method.
\\ ( https://arxiv.org/abs/2403.06483 ,  955kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06568
Date: Mon, 11 Mar 2024 10:10:35 GMT   (576kb,D)

Title: Better Understandings and Configurations in MaxSAT Local Search Solvers
  via Anytime Performance Analysis
Authors: Furong Ye, Chuan Luo, Shaowei Cai
Categories: cs.AI
\\
  Though numerous solvers have been proposed for the MaxSAT problem, and the
benchmark environment such as MaxSAT Evaluations provides a platform for the
comparison of the state-of-the-art solvers, existing assessments were usually
evaluated based on the quality, e.g., fitness, of the best-found solutions
obtained within a given running time budget. However, concerning solely the
final obtained solutions regarding specific time budgets may restrict us from
comprehending the behavior of the solvers along the convergence process. This
paper demonstrates that Empirical Cumulative Distribution Functions can be used
to compare MaxSAT local search solvers' anytime performance across multiple
problem instances and various time budgets. The assessment reveals distinctions
in solvers' performance and displays that the (dis)advantages of solvers adjust
along different running times. This work also exhibits that the quantitative
and high variance assessment of anytime performance can guide machines, i.e.,
automatic configurators, to search for better parameter settings. Our
experimental results show that the hyperparameter optimization tool, i.e.,
SMAC, generally achieves better parameter settings of local search when using
the anytime performance as the cost function, compared to using the fitness of
the best-found solutions.
\\ ( https://arxiv.org/abs/2403.06568 ,  576kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06734
Date: Mon, 11 Mar 2024 13:56:57 GMT   (6602kb,D)

Title: Real-Time Multimodal Cognitive Assistant for Emergency Medical Services
Authors: Keshara Weerasinghe, Saahith Janapati, Xueren Ge, Sion Kim, Sneha
  Iyer, John A. Stankovic, Homa Alemzadeh
Categories: cs.AI cs.CL cs.CV
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\
  Emergency Medical Services (EMS) responders often operate under
time-sensitive conditions, facing cognitive overload and inherent risks,
requiring essential skills in critical thinking and rapid decision-making. This
paper presents CognitiveEMS, an end-to-end wearable cognitive assistant system
that can act as a collaborative virtual partner engaging in the real-time
acquisition and analysis of multimodal data from an emergency scene and
interacting with EMS responders through Augmented Reality (AR) smart glasses.
CognitiveEMS processes the continuous streams of data in real-time and
leverages edge computing to provide assistance in EMS protocol selection and
intervention recognition. We address key technical challenges in real-time
cognitive assistance by introducing three novel components: (i) a Speech
Recognition model that is fine-tuned for real-world medical emergency
conversations using simulated EMS audio recordings, augmented with synthetic
data generated by large language models (LLMs); (ii) an EMS Protocol Prediction
model that combines state-of-the-art (SOTA) tiny language models with EMS
domain knowledge using graph-based attention mechanisms; (iii) an EMS Action
Recognition module which leverages multimodal audio and video data and protocol
predictions to infer the intervention/treatment actions taken by the responders
at the incident scene. Our results show that for speech recognition we achieve
superior performance compared to SOTA (WER of 0.290 vs. 0.618) on
conversational data. Our protocol prediction component also significantly
outperforms SOTA (top-3 accuracy of 0.800 vs. 0.200) and the action recognition
achieves an accuracy of 0.727, while maintaining an end-to-end latency of 3.78s
for protocol prediction on the edge and 0.31s on the server.
\\ ( https://arxiv.org/abs/2403.06734 ,  6602kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06843
Date: Mon, 11 Mar 2024 16:03:21 GMT   (3168kb,D)

Title: Towards an educational tool for supporting neonatologists in the
  delivery room
Authors: Giorgio Leonardi, Clara Maldarizzi, Stefania Montani, Manuel Striani,
  Mariachiara Martina Strozzi
Categories: cs.AI cs.LG
Comments: 9 pages, 5 figures, conference paper
ACM-class: I.2.1
\\
  Nowadays, there is evidence that several factors may increase the risk, for
an infant, to require stabilisation or resuscitation manoeuvres at birth.
However, this risk factors are not completely known, and a universally
applicable model for predicting high-risk situations is not available yet.
Considering both these limitations and the fact that the need for resuscitation
at birth is a rare event, periodic training of the healthcare personnel
responsible for newborn caring in the delivery room is mandatory.
  In this paper, we propose a machine learning approach for identifying risk
factors and their impact on the birth event from real data, which can be used
by personnel to progressively increase and update their knowledge. Our final
goal will be the one of designing a user-friendly mobile application, able to
improve the recognition rate and the planning of the appropriate interventions
on high-risk patients.
\\ ( https://arxiv.org/abs/2403.06843 ,  3168kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06910
Date: Mon, 11 Mar 2024 17:01:13 GMT   (333kb,D)

Title: Responsible Artificial Intelligence: A Structured Literature Review
Authors: Sabrina Goellner, Marina Tropmann-Frick, Bostjan Brumen
Categories: cs.AI cs.CY cs.LG
\\
  Our research endeavors to advance the concept of responsible artificial
intelligence (AI), a topic of increasing importance within EU policy
discussions. The EU has recently issued several publications emphasizing the
necessity of trust in AI, underscoring the dual nature of AI as both a
beneficial tool and a potential weapon. This dichotomy highlights the urgent
need for international regulation. Concurrently, there is a need for frameworks
that guide companies in AI development, ensuring compliance with such
regulations. Our research aims to assist lawmakers and machine learning
practitioners in navigating the evolving landscape of AI regulation,
identifying focal areas for future attention. This paper introduces a
comprehensive and, to our knowledge, the first unified definition of
responsible AI. Through a structured literature review, we elucidate the
current understanding of responsible AI. Drawing from this analysis, we propose
an approach for developing a future framework centered around this concept. Our
findings advocate for a human-centric approach to Responsible AI. This approach
encompasses the implementation of AI methods with a strong emphasis on ethics,
model explainability, and the pillars of privacy, security, and trust.
\\ ( https://arxiv.org/abs/2403.06910 ,  333kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05640
Date: Fri, 8 Mar 2024 19:25:00 GMT   (307kb,D)

Title: Generating Hard-Negative Out-of-Scope Data with ChatGPT for Intent
  Classification
Authors: Zhijian Li, Stefan Larson, Kevin Leach
Categories: cs.CL
Comments: LREC-COLING 2024
\\
  Intent classifiers must be able to distinguish when a user's utterance does
not belong to any supported intent to avoid producing incorrect and unrelated
system responses. Although out-of-scope (OOS) detection for intent classifiers
has been studied, previous work has not yet studied changes in classifier
performance against hard-negative out-of-scope utterances (i.e., inputs that
share common features with in-scope data, but are actually out-of-scope). We
present an automated technique to generate hard-negative OOS data using
ChatGPT. We use our technique to build five new hard-negative OOS datasets, and
evaluate each against three benchmark intent classifiers. We show that
classifiers struggle to correctly identify hard-negative OOS utterances more
than general OOS utterances. Finally, we show that incorporating hard-negative
OOS data for training improves model robustness when detecting hard-negative
OOS data and general OOS data. Our technique, datasets, and evaluation address
an important void in the field, offering a straightforward and inexpensive way
to collect hard-negative OOS data and improve intent classifiers' robustness.
\\ ( https://arxiv.org/abs/2403.05640 ,  307kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05676
Date: Fri, 8 Mar 2024 21:09:20 GMT   (10178kb,D)

Title: PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System
  Co-design
Authors: Wenqi Jiang, Shuai Zhang, Boran Han, Jie Wang, Bernie Wang, Tim Kraska
Categories: cs.CL
\\
  Retrieval-augmented generation (RAG) can enhance the generation quality of
large language models (LLMs) by incorporating external token databases.
However, retrievals from large databases can constitute a substantial portion
of the overall generation time, particularly when retrievals are periodically
performed to align the retrieved content with the latest states of generation.
In this paper, we introduce PipeRAG, a novel algorithm-system co-design
approach to reduce generation latency and enhance generation quality. PipeRAG
integrates (1) pipeline parallelism to enable concurrent retrieval and
generation processes, (2) flexible retrieval intervals to maximize the
efficiency of pipeline parallelism, and (3) a performance model to
automatically balance retrieval quality and latency based on the generation
states and underlying hardware. Our evaluation shows that, by combining the
three aforementioned methods, PipeRAG achieves up to 2.6$\times$ speedup in
end-to-end generation latency while improving generation quality. These
promising results showcase the effectiveness of co-designing algorithms with
underlying systems, paving the way for the adoption of PipeRAG in future RAG
systems.
\\ ( https://arxiv.org/abs/2403.05676 ,  10178kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05696
Date: Fri, 8 Mar 2024 22:09:58 GMT   (1153kb,D)

Title: SeeGULL Multilingual: a Dataset of Geo-Culturally Situated Stereotypes
Authors: Mukul Bhutani, Kevin Robinson, Vinodkumar Prabhakaran, Shachi Dave,
  Sunipa Dev
Categories: cs.CL cs.CV
\\
  While generative multilingual models are rapidly being deployed, their safety
and fairness evaluations are largely limited to resources collected in English.
This is especially problematic for evaluations targeting inherently
socio-cultural phenomena such as stereotyping, where it is important to build
multi-lingual resources that reflect the stereotypes prevalent in respective
language communities. However, gathering these resources, at scale, in varied
languages and regions pose a significant challenge as it requires broad
socio-cultural knowledge and can also be prohibitively expensive. To overcome
this critical gap, we employ a recently introduced approach that couples LLM
generations for scale with culturally situated validations for reliability, and
build SeeGULL Multilingual, a global-scale multilingual dataset of social
stereotypes, containing over 25K stereotypes, spanning 20 languages, with human
annotations across 23 regions, and demonstrate its utility in identifying gaps
in model evaluations. Content warning: Stereotypes shared in this paper can be
offensive.
\\ ( https://arxiv.org/abs/2403.05696 ,  1153kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05700
Date: Fri, 8 Mar 2024 22:18:13 GMT   (507kb,D)

Title: DADIT: A Dataset for Demographic Classification of Italian Twitter Users
  and a Comparison of Prediction Methods
Authors: Lorenzo Lupo, Paul Bose, Mahyar Habibi, Dirk Hovy, Carlo Schwarz
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  Social scientists increasingly use demographically stratified social media
data to study the attitudes, beliefs, and behavior of the general public. To
facilitate such analyses, we construct, validate, and release publicly the
representative DADIT dataset of 30M tweets of 20k Italian Twitter users, along
with their bios and profile pictures. We enrich the user data with high-quality
labels for gender, age, and location. DADIT enables us to train and compare the
performance of various state-of-the-art models for the prediction of the gender
and age of social media users. In particular, we investigate if tweets contain
valuable information for the task, since popular classifiers like M3 don't
leverage them. Our best XLM-based classifier improves upon the commonly used
competitor M3 by up to 53% F1. Especially for age prediction, classifiers
profit from including tweets as features. We also confirm these findings on a
German test set.
\\ ( https://arxiv.org/abs/2403.05700 ,  507kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05720
Date: Fri, 8 Mar 2024 23:17:55 GMT   (6237kb,D)

Title: A Benchmark of Domain-Adapted Large Language Models for Generating Brief
  Hospital Course Summaries
Authors: Asad Aali, Dave Van Veen, Yamin Ishraq Arefeen, Jason Hom, Christian
  Bluethgen, Eduardo Pontes Reis, Sergios Gatidis, Namuun Clifford, Joseph
  Daws, Arash S. Tehrani, Jangwon Kim, Akshay S. Chaudhari
Categories: cs.CL cs.AI cs.LG
\\
  Brief hospital course (BHC) summaries are common clinical documents generated
by summarizing clinical notes. While large language models (LLMs) depict
remarkable capabilities in automating real-world tasks, their capabilities for
healthcare applications such as BHC synthesis have not been shown. To enable
the adaptation of LLMs for BHC synthesis, we introduce a novel benchmark
consisting of a pre-processed dataset extracted from MIMIC-IV notes,
encapsulating clinical note, and brief hospital course (BHC) pairs. We assess
the performance of two general-purpose LLMs and three healthcare-adapted LLMs
to improve BHC synthesis from clinical notes. Using clinical notes as input for
generating BHCs, we apply prompting-based (using in-context learning) and
fine-tuning-based adaptation strategies to three open-source LLMs
(Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5,
GPT-4). We quantitatively evaluate the performance of these LLMs across varying
context-length inputs using conventional natural language similarity metrics.
We further perform a qualitative study where five diverse clinicians blindly
compare clinician-written BHCs and two LLM-generated BHCs for 30 samples across
metrics of comprehensiveness, conciseness, factual correctness, and fluency.
Overall, we present a new benchmark and pre-processed dataset for using LLMs in
BHC synthesis from clinical notes. We observe high-quality summarization
performance for both in-context proprietary and fine-tuned open-source LLMs
using both quantitative metrics and a qualitative clinical reader study. We
propose our work as a benchmark to motivate future works to adapt and assess
the performance of LLMs in BHC synthesis.
\\ ( https://arxiv.org/abs/2403.05720 ,  6237kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05750
Date: Sat, 9 Mar 2024 01:13:54 GMT   (56kb)

Title: Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated
  Text
Authors: Sara Abdali, Richard Anarfi, CJ Barberan, Jia He
Categories: cs.CL cs.AI cs.LG
\\
  Large Language Models (LLMs) have revolutionized the field of Natural
Language Generation (NLG) by demonstrating an impressive ability to generate
human-like text. However, their widespread usage introduces challenges that
necessitate thoughtful examination, ethical scrutiny, and responsible
practices. In this study, we delve into these challenges, explore existing
strategies for mitigating them, with a particular emphasis on identifying
AI-generated text as the ultimate solution. Additionally, we assess the
feasibility of detection from a theoretical perspective and propose novel
research directions to address the current limitations in this domain.
\\ ( https://arxiv.org/abs/2403.05750 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05766
Date: Sat, 9 Mar 2024 02:27:45 GMT   (7367kb,D)

Title: FLAP: Flow Adhering Planning with Constrained Decoding in LLMs
Authors: Shamik Roy, Sailik Sengupta, Daniele Bonadiman, Saab Mansour, Arshit
  Gupta
Categories: cs.CL
Comments: Under submission
\\
  Planning is a crucial task for agents in task oriented dialogs (TODs). Human
agents typically resolve user issues by following predefined workflows,
decomposing workflow steps into actionable items, and performing actions by
executing APIs in order; all of which require reasoning and planning. With the
recent advances in LLMs, there have been increasing attempts to use LLMs for
task planning and API usage. However, the faithfulness of the plans to
predefined workflows and API dependencies, is not guaranteed with LLMs because
of their bias towards pretraining data. Moreover, in real life, workflows are
custom-defined and prone to change, hence, quickly adapting agents to the
changes is desirable. In this paper, we study faithful planning in TODs to
resolve user intents by following predefined flows and preserving API
dependencies. We propose a constrained decoding algorithm based on lookahead
heuristic for faithful planning. Our algorithm alleviates the need for
finetuning LLMs using domain specific data, outperforms other decoding and
prompting-based baselines, and applying our algorithm on smaller LLMs (7B) we
achieve comparable performance to larger LLMs (30B-40B).
\\ ( https://arxiv.org/abs/2403.05766 ,  7367kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05788
Date: Sat, 9 Mar 2024 04:20:26 GMT   (571kb,D)

Title: On the Benefits of Fine-Grained Loss Truncation: A Case Study on
  Factuality in Summarization
Authors: Lorenzo Jaime Yu Flores, Arman Cohan
Categories: cs.CL cs.AI
Comments: EACL 2024
\\
  Text summarization and simplification are among the most widely used
applications of AI. However, models developed for such tasks are often prone to
hallucination, which can result from training on unaligned data. One efficient
approach to address this issue is Loss Truncation (LT) (Kang and Hashimoto,
2020), an approach to modify the standard log loss to adaptively remove noisy
examples during training. However, we find that LT alone yields a considerable
number of hallucinated entities on various datasets. We study the behavior of
the underlying losses between factual and non-factual examples, to understand
and refine the performance of LT. We demonstrate that LT's performance is
limited when the underlying assumption that noisy targets have higher NLL loss
is not satisfied, and find that word-level NLL among entities provides better
signal for distinguishing factuality. We then leverage this to propose a
fine-grained NLL loss and fine-grained data cleaning strategies, and observe
improvements in hallucination reduction across some datasets. Our work is
available at https://https://github.com/yale-nlp/fine-grained-lt.
\\ ( https://arxiv.org/abs/2403.05788 ,  571kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05789
Date: Sat, 9 Mar 2024 04:20:46 GMT   (8173kb,D)

Title: ItD: Large Language Models Can Teach Themselves Induction through
  Deduction
Authors: Wangtao Sun, Haotian Xu, Xuanqing Yu, Pei Chen, Shizhu He, Jun Zhao,
  Kang Liu
Categories: cs.CL cs.AI
\\
  Although Large Language Models (LLMs) are showing impressive performance on a
wide range of Natural Language Processing tasks, researchers have found that
they still have limited ability to conduct induction. Recent works mainly adopt
``post processes'' paradigms to improve the performance of LLMs on induction
(e.g., the hypothesis search & refinement methods), but their performance is
still constrained by the inherent inductive capability of the LLMs. In this
paper, we propose a novel framework, Induction through Deduction (ItD), to
enable the LLMs to teach themselves induction through deduction. The ItD
framework is composed of two main components: a Deductive Data Generation
module to generate induction data and a Naive Bayesian Induction module to
optimize the fine-tuning and decoding of LLMs. Our empirical results showcase
the effectiveness of ItD on two induction benchmarks, achieving relative
performance improvement of 36% and 10% compared with previous state-of-the-art,
respectively. Our ablation study verifies the effectiveness of two key modules
of ItD. We also verify the effectiveness of ItD across different LLMs and
deductors. The data and code of this paper can be found at
https://anonymous.4open.science/r/ItD-E844.
\\ ( https://arxiv.org/abs/2403.05789 ,  8173kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05795
Date: Sat, 9 Mar 2024 04:58:25 GMT   (6982kb,D)

Title: ClinicalMamba: A Generative Clinical Language Model on Longitudinal
  Clinical Notes
Authors: Zhichao Yang, Avijit Mitra, Sunjae Kwon, Hong Yu
Categories: cs.CL
\\
  The advancement of natural language processing (NLP) systems in healthcare
hinges on language model ability to interpret the intricate information
contained within clinical notes. This process often requires integrating
information from various time points in a patient's medical history. However,
most earlier clinical language models were pretrained with a context length
limited to roughly one clinical document. In this study, We introduce
ClinicalMamba, a specialized version of the Mamba language model, pretrained on
a vast corpus of longitudinal clinical notes to address the unique linguistic
characteristics and information processing needs of the medical domain.
ClinicalMamba, with 130 million and 2.8 billion parameters, demonstrates a
superior performance in modeling clinical language across extended text lengths
compared to Mamba and clinical Llama. With few-shot learning, ClinicalMamba
achieves notable benchmarks in speed and accuracy, outperforming existing
clinical language models and general domain large models like GPT-4 in
longitudinal clinical notes information extraction tasks.
\\ ( https://arxiv.org/abs/2403.05795 ,  6982kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05802
Date: Sat, 9 Mar 2024 05:38:45 GMT   (1733kb,D)

Title: UniSparse: An Intermediate Language for General Sparse Format
  Customization
Authors: Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong,
  Zhiru Zhang
Categories: cs.CL
Comments: to be published in OOPSLA'24
Journal-ref: Proc. ACM Program. Lang. 8, OOPSLA1, Article 99 (April 2024), 29
  pages
DOI: 10.1145/3649816
\\
  The ongoing trend of hardware specialization has led to a growing use of
custom data formats when processing sparse workloads, which are typically
memory-bound. These formats facilitate optimized software/hardware
implementations by utilizing sparsity pattern- or target-aware data structures
and layouts to enhance memory access latency and bandwidth utilization.
However, existing sparse tensor programming models and compilers offer little
or no support for productively customizing the sparse formats. Additionally,
because these frameworks represent formats using a limited set of per-dimension
attributes, they lack the flexibility to accommodate numerous new variations of
custom sparse data structures and layouts. To overcome this deficiency, we
propose UniSparse, an intermediate language that provides a unified abstraction
for representing and customizing sparse formats. Unlike the existing
attribute-based frameworks, UniSparse decouples the logical representation of
the sparse tensor (i.e., the data structure) from its low-level memory layout,
enabling the customization of both. As a result, a rich set of format
customizations can be succinctly expressed in a small set of well-defined
query, mutation, and layout primitives. We also develop a compiler leveraging
the MLIR infrastructure, which supports adaptive customization of formats, and
automatic code generation of format conversion and compute operations for
heterogeneous architectures. We demonstrate the efficacy of our approach
through experiments running commonly-used sparse linear algebra operations with
specialized formats on multiple different hardware targets, including an Intel
CPU, an NVIDIA GPU, an AMD Xilinx FPGA, and a simulated processing-in-memory
(PIM) device.
\\ ( https://arxiv.org/abs/2403.05802 ,  1733kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05812
Date: Sat, 9 Mar 2024 06:26:21 GMT   (1072kb,D)

Title: Algorithmic progress in language models
Authors: Anson Ho, Tamay Besiroglu, Ege Erdil, David Owen, Robi Rahman, Zifan
  Carl Guo, David Atkinson, Neil Thompson, Jaime Sevilla
Categories: cs.CL cs.AI
\\
  We investigate the rate at which algorithms for pre-training language models
have improved since the advent of deep learning. Using a dataset of over 200
language model evaluations on Wikitext and Penn Treebank spanning 2012-2023, we
find that the compute required to reach a set performance threshold has halved
approximately every 8 months, with a 95% confidence interval of around 5 to 14
months, substantially faster than hardware gains per Moore's Law. We estimate
augmented scaling laws, which enable us to quantify algorithmic progress and
determine the relative contributions of scaling models versus innovations in
training algorithms. Despite the rapid pace of algorithmic progress and the
development of new architectures such as the transformer, our analysis reveals
that the increase in compute made an even larger contribution to overall
performance improvements over this time period. Though limited by noisy
benchmark data, our analysis quantifies the rapid progress in language
modeling, shedding light on the relative contributions from compute and
algorithms.
\\ ( https://arxiv.org/abs/2403.05812 ,  1072kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05814
Date: Sat, 9 Mar 2024 06:28:48 GMT   (2265kb,D)

Title: MP2D: An Automated Topic Shift Dialogue Generation Framework Leveraging
  Knowledge Graphs
Authors: Yerin Hwang, Yongil Kim, Yunah Jang, Jeesoo Bang, Hyunkyung Bae,
  Kyomin Jung
Categories: cs.CL cs.AI
Comments: 20 pages
\\
  Despite advancements in on-topic dialogue systems, effectively managing topic
shifts within dialogues remains a persistent challenge, largely attributed to
the limited availability of training datasets. To address this issue, we
propose Multi-Passage to Dialogue (MP2D), a data generation framework that
automatically creates conversational question-answering datasets with natural
topic transitions. By leveraging the relationships between entities in a
knowledge graph, MP2D maps the flow of topics within a dialogue, effectively
mirroring the dynamics of human conversation. It retrieves relevant passages
corresponding to the topics and transforms them into dialogues through the
passage-to-dialogue method. Through quantitative and qualitative experiments,
we demonstrate MP2D's efficacy in generating dialogue with natural topic
shifts. Furthermore, this study introduces a novel benchmark for topic shift
dialogues, TS-WikiDialog. Utilizing the dataset, we demonstrate that even Large
Language Models (LLMs) struggle to handle topic shifts in dialogue effectively,
and we showcase the performance improvements of models trained on datasets
generated by MP2D across diverse topic shift dialogue tasks.
\\ ( https://arxiv.org/abs/2403.05814 ,  2265kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05845
Date: Sat, 9 Mar 2024 09:04:53 GMT   (7171kb,D)

Title: Reverse That Number! Decoding Order Matters in Arithmetic Learning
Authors: Daniel Zhang-Li, Nianyi Lin, Jifan Yu, Zheyuan Zhang, Zijun Yao,
  Xiaokang Zhang, Lei Hou, Jing Zhang, Juanzi Li
Categories: cs.CL cs.AI
\\
  Recent advancements in pretraining have demonstrated that modern Large
Language Models (LLMs) possess the capability to effectively learn arithmetic
operations. However, despite acknowledging the significance of digit order in
arithmetic computation, current methodologies predominantly rely on sequential,
step-by-step approaches for teaching LLMs arithmetic, resulting in a conclusion
where obtaining better performance involves fine-grained step-by-step.
Diverging from this conventional path, our work introduces a novel strategy
that not only reevaluates the digit order by prioritizing output from the least
significant digit but also incorporates a step-by-step methodology to
substantially reduce complexity. We have developed and applied this method in a
comprehensive set of experiments. Compared to the previous state-of-the-art
(SOTA) method, our findings reveal an overall improvement of in accuracy while
requiring only a third of the tokens typically used during training. For the
purpose of facilitating replication and further research, we have made our code
and dataset publicly available at
\url{https://anonymous.4open.science/r/RAIT-9FB7/}.
\\ ( https://arxiv.org/abs/2403.05845 ,  7171kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05881
Date: Sat, 9 Mar 2024 11:23:38 GMT   (401kb,D)

Title: KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge
  Graphs and Ranking Techniques
Authors: Rui Yang, Haoran Liu, Qingcheng Zeng, Yu He Ke, Wanxin Li, Lechao
  Cheng, Qingyu Chen, James Caverlee, Yutaka Matsuo, Irene Li
Categories: cs.CL
\\
  Large Language Models (LLMs) have significantly advanced healthcare
innovation on generation capabilities. However, their application in real
clinical settings is challenging due to potential deviations from medical facts
and inherent biases. In this work, we develop an augmented LLM framework,
KG-Rank, which leverages a medical knowledge graph (KG) with ranking and
re-ranking techniques, aiming to improve free-text question-answering (QA) in
the medical domain. Specifically, upon receiving a question, we initially
retrieve triplets from a medical KG to gather factual information.
Subsequently, we innovatively apply ranking methods to refine the ordering of
these triplets, aiming to yield more precise answers. To the best of our
knowledge, KG-Rank is the first application of ranking models combined with KG
in medical QA specifically for generating long answers. Evaluation of four
selected medical QA datasets shows that KG-Rank achieves an improvement of over
18% in the ROUGE-L score. Moreover, we extend KG-Rank to open domains, where it
realizes a 14% improvement in ROUGE-L, showing the effectiveness and potential
of KG-Rank.
\\ ( https://arxiv.org/abs/2403.05881 ,  401kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05902
Date: Sat, 9 Mar 2024 12:46:53 GMT   (795kb,D)

Title: MaiBaam Annotation Guidelines
Authors: Verena Blaschke, Barbara Kova\v{c}i\'c, Siyao Peng, Barbara Plank
Categories: cs.CL
\\
  This document provides the annotation guidelines for MaiBaam, a Bavarian
corpus annotated with part-of-speech (POS) tags and syntactic dependencies.
MaiBaam belongs to the Universal Dependencies (UD) project, and our annotations
elaborate on the general and German UD version 2 guidelines. In this document,
we detail how to preprocess and tokenize Bavarian data, provide an overview of
the POS tags and dependencies we use, explain annotation decisions that would
also apply to closely related languages like German, and lastly we introduce
and motivate decisions that are specific to Bavarian grammar.
\\ ( https://arxiv.org/abs/2403.05902 ,  795kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05920
Date: Sat, 9 Mar 2024 14:02:59 GMT   (1443kb,D)

Title: High Throughput Phenotyping of Physician Notes with Large Language and
  Hybrid NLP Models
Authors: Syed I. Munzir, Daniel B. Hier, Michael D. Carrithers
Categories: cs.CL cs.AI
Comments: Submitted to IEEE EMBS Summer conference 2024
ACM-class: I.2; J.2
\\
  Deep phenotyping is the detailed description of patient signs and symptoms
using concepts from an ontology. The deep phenotyping of the numerous physician
notes in electronic health records requires high throughput methods. Over the
past thirty years, progress toward making high throughput phenotyping feasible.
In this study, we demonstrate that a large language model and a hybrid NLP
model (combining word vectors with a machine learning classifier) can perform
high throughput phenotyping on physician notes with high accuracy. Large
language models will likely emerge as the preferred method for high throughput
deep phenotyping of physician notes.
\\ ( https://arxiv.org/abs/2403.05920 ,  1443kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05931
Date: Sat, 9 Mar 2024 14:50:20 GMT   (1815kb)

Title: Thread Detection and Response Generation using Transformers with Prompt
  Optimisation
Authors: Kevin Joshua T, Arnav Agarwal, Shriya Sanjay, Yash Sarda, John Sahaya
  Rani Alex, Saurav Gupta, Sushant Kumar, Vishwanath Kamath
Categories: cs.CL cs.LG
Comments: 6 pages, 4 figures, submitted to 2024 IEEE International Conference
  on Signal Processing and Communications (SPCOM)
ACM-class: I.2.7; I.2.6
\\
  Conversational systems are crucial for human-computer interaction, managing
complex dialogues by identifying threads and prioritising responses. This is
especially vital in multi-party conversations, where precise identification of
threads and strategic response prioritisation ensure efficient dialogue
management. To address these challenges an end-to-end model that identifies
threads and prioritises their response generation based on the importance was
developed, involving a systematic decomposition of the problem into discrete
components - thread detection, prioritisation, and performance optimisation
which was meticulously analysed and optimised. These refined components
seamlessly integrate into a unified framework, in conversational systems.
Llama2 7b is used due to its high level of generalisation but the system can be
updated with any open source Large Language Model(LLM). The computational
capabilities of the Llama2 model was augmented by using fine tuning methods and
strategic prompting techniques to optimise the model's performance, reducing
computational time and increasing the accuracy of the model. The model achieves
up to 10x speed improvement, while generating more coherent results compared to
existing models.
\\ ( https://arxiv.org/abs/2403.05931 ,  1815kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05973
Date: Sat, 9 Mar 2024 17:46:24 GMT   (5989kb,D)

Title: Calibrating Large Language Models Using Their Generations Only
Authors: Dennis Ulmer, Martin Gubri, Hwaran Lee, Sangdoo Yun, Seong Joon Oh
Categories: cs.CL cs.AI cs.LG
\\
  As large language models (LLMs) are increasingly deployed in user-facing
applications, building trust and maintaining safety by accurately quantifying a
model's confidence in its prediction becomes even more important. However,
finding effective ways to calibrate LLMs - especially when the only interface
to the models is their generated text - remains a challenge. We propose APRICOT
(auxiliary prediction of confidence targets): A method to set confidence
targets and train an additional model that predicts an LLM's confidence based
on its textual input and output alone. This approach has several advantages: It
is conceptually simple, does not require access to the target model beyond its
output, does not interfere with the language generation, and has a multitude of
potential usages, for instance by verbalizing the predicted confidence or
adjusting the given answer based on the confidence. We show how our approach
performs competitively in terms of calibration error for white-box and
black-box LLMs on closed-book question-answering to detect incorrect LLM
answers.
\\ ( https://arxiv.org/abs/2403.05973 ,  5989kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05975
Date: Sat, 9 Mar 2024 18:24:58 GMT   (353kb,D)

Title: Measuring Bias in a Ranked List using Term-based Representations
Authors: Amin Abolghasemi, Leif Azzopardi, Arian Askari, Maarten de Rijke,
  Suzan Verberne
Categories: cs.CL
Comments: Accepted at the 46th European Conference on Information Retrieval
  (ECIR 2024)
\\
  In most recent studies, gender bias in document ranking is evaluated with the
NFaiRR metric, which measures bias in a ranked list based on an aggregation
over the unbiasedness scores of each ranked document. This perspective in
measuring the bias of a ranked list has a key limitation: individual documents
of a ranked list might be biased while the ranked list as a whole balances the
groups' representations. To address this issue, we propose a novel metric
called TExFAIR (term exposure-based fairness), which is based on two new
extensions to a generic fairness evaluation framework, attention-weighted
ranking fairness (AWRF). TExFAIR assesses fairness based on the term-based
representation of groups in a ranked list: (i) an explicit definition of
associating documents to groups based on probabilistic term-level associations,
and (ii) a rank-biased discounting factor (RBDF) for counting
non-representative documents towards the measurement of the fairness of a
ranked list. We assess TExFAIR on the task of measuring gender bias in passage
ranking, and study the relationship between TExFAIR and NFaiRR. Our experiments
show that there is no strong correlation between TExFAIR and NFaiRR, which
indicates that TExFAIR measures a different dimension of fairness than NFaiRR.
With TExFAIR, we extend the AWRF framework to allow for the evaluation of
fairness in settings with term-based representations of groups in documents in
a ranked list.
\\ ( https://arxiv.org/abs/2403.05975 ,  353kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05982
Date: Sat, 9 Mar 2024 18:43:48 GMT   (802kb)

Title: Enhanced Auto Language Prediction with Dictionary Capsule -- A Novel
  Approach
Authors: Pinni Venkata Abhiram, Ananya Rathore, Abhir Mirikar, Hari Krishna S,
  Sheena Christabel Pravin, Vishwanath Kamath Pethri, Manjunath Lokanath
  Belgod, Reetika Gupta, K Muthukumaran
Categories: cs.CL
Comments: 21 Pages
\\
  The paper presents a novel Auto Language Prediction Dictionary Capsule
(ALPDC) framework for language prediction and machine translation. The model
uses a combination of neural networks and symbolic representations to predict
the language of a given input text and then translate it to a target language
using pre-built dictionaries. This research work also aims to translate the
text of various languages to its literal meaning in English. The proposed model
achieves state-of-the-art results on several benchmark datasets and
significantly improves translation accuracy compared to existing methods. The
results show the potential of the proposed method for practical use in
multilingual communication and natural language processing tasks.
\\ ( https://arxiv.org/abs/2403.05982 ,  802kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06018
Date: Sat, 9 Mar 2024 21:36:13 GMT   (1804kb,D)

Title: Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in
  Low-Resource Languages
Authors: Christopher Toukmaji
Categories: cs.CL cs.AI cs.LG
Comments: 47 pages, 26 figures; a thesis submitted in partial satisfaction of
  the requirements for the degree of Bachelor of Science in Computer Science at
  the University of California - Santa Cruz
\\
  Large pre-trained language models (PLMs) are at the forefront of advances in
Natural Language Processing. One widespread use case of PLMs is "prompting" -
or in-context learning - where a user provides a description of a task and some
completed examples of the task to a PLM as context before prompting the PLM to
perform the task on a new example. Only the largest, most capable PLMs are able
to perform in-context learning effectively, and these models are typically
trained with a predominantly English corpus, leaving all other languages
behind. The data limitations in most languages preclude the training of
language-specific PLMs capable of prompting. Albeit the surge in work of
prompting settings, it is still unclear how PLMs should be adapted
cross-lingually specifically for prompting. We evaluate the possible methods to
adapt LLaMa, a 7B parameter open-source PLM mainly trained in English, for
prompting in low-resource languages, namely for Kinyarwanda, Hausa, and
Luganda. We consider three methods: few-shot prompting (prompt),
language-adaptive fine-tuning (LAFT), and neural machine translation
(translate), and evaluate on abstractive summarization, multi-class topic
classification, and named-entity recognition. Although LAFT carries the
greatest compute cost and intuitively should lead to the best results, our
experiments exhibit that LAFT is only occasionally the optimal choice for
adapting PLMs for prompting. Rather, the translate and prompt settings are a
compute-efficient and cost-effective method of few-shot prompting for the
selected low-resource languages. We find that the results are task and language
dependent but find that the prompting method is the best on average across all
tasks and languages. Results show that the prompt setting performs better than
both translating and LAFT with statistical significance for all shots when
aggregated across all tasks and languages.
\\ ( https://arxiv.org/abs/2403.06018 ,  1804kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06023
Date: Sat, 9 Mar 2024 22:18:26 GMT   (469kb)

Title: Persian Slang Text Conversion to Formal and Deep Learning of Persian
  Short Texts on Social Media for Sentiment Classification
Authors: Mohsen Khazeni, Mohammad Heydari, Amir Albadvi
Categories: cs.CL cs.LG
Comments: 10 pages, 4 figures, 5 tables
\\
  The lack of a suitable tool for the analysis of conversational texts in the
Persian language has made various analyses of these texts, including Sentiment
Analysis, difficult. In this research, we tried to make the understanding of
these texts easier for the machine by providing PSC, Persian Slang Converter, a
tool for converting conversational texts into formal ones, and by using the
most up-to-date and best deep learning methods along with the PSC, the
sentiment learning of short Persian language texts for the machine in a better
way. be made More than 10 million unlabeled texts from various social networks
and movie subtitles (as Conversational texts) and about 10 million news texts
(as formal texts) have been used for training unsupervised models and formal
implementation of the tool. 60,000 texts from the comments of Instagram social
network users with positive, negative, and neutral labels are considered
supervised data for training the emotion classification model of short texts.
Using the formal tool, 57% of the words of the corpus of conversation were
converted. Finally, by using the formalizer, FastText model, and deep LSTM
network, an accuracy of 81.91 was obtained on the test data.
\\ ( https://arxiv.org/abs/2403.06023 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06060
Date: Sun, 10 Mar 2024 01:39:10 GMT   (524kb,D)

Title: Ensemble Language Models for Multilingual Sentiment Analysis
Authors: Md Arid Hasan
Categories: cs.CL cs.LG
Comments: This is one of my graduate course project reports and currently, I'm
  not planning to submit to any conferences
ACM-class: I.2.7
\\
  The rapid advancement of social media enables us to analyze user opinions. In
recent times, sentiment analysis has shown a prominent research gap in
understanding human sentiment based on the content shared on social media.
Although sentiment analysis for commonly spoken languages has advanced
significantly, low-resource languages like Arabic continue to get little
research due to resource limitations. In this study, we explore sentiment
analysis on tweet texts from SemEval-17 and the Arabic Sentiment Tweet dataset.
Moreover, We investigated four pretrained language models and proposed two
ensemble language models. Our findings include monolingual models exhibiting
superior performance and ensemble models outperforming the baseline while the
majority voting ensemble outperforms the English language.
\\ ( https://arxiv.org/abs/2403.06060 ,  524kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06063
Date: Sun, 10 Mar 2024 02:14:24 GMT   (541kb,D)

Title: Target-constrained Bidirectional Planning for Generation of
  Target-oriented Proactive Dialogue
Authors: Jian Wang, Dongding Lin, Wenjie Li
Categories: cs.CL cs.AI
Comments: Accepted by ACM Transactions on Information Systems (TOIS)
\\
  Target-oriented proactive dialogue systems aim to lead conversations from a
dialogue context toward a pre-determined target, such as making recommendations
on designated items or introducing new specific topics. To this end, it is
critical for such dialogue systems to plan reasonable actions to drive the
conversation proactively, and meanwhile, to plan appropriate topics to move the
conversation forward to the target topic smoothly. In this work, we mainly
focus on effective dialogue planning for target-oriented dialogue generation.
Inspired by decision-making theories in cognitive science, we propose a novel
target-constrained bidirectional planning (TRIP) approach, which plans an
appropriate dialogue path by looking ahead and looking back. By formulating the
planning as a generation task, our TRIP bidirectionally generates a dialogue
path consisting of a sequence of <action, topic> pairs using two Transformer
decoders. They are expected to supervise each other and converge on consistent
actions and topics by minimizing the decision gap and contrastive generation of
targets. Moreover, we propose a target-constrained decoding algorithm with a
bidirectional agreement to better control the planning process. Subsequently,
we adopt the planned dialogue paths to guide dialogue generation in a pipeline
manner, where we explore two variants: prompt-based generation and
plan-controlled generation. Extensive experiments are conducted on two
challenging dialogue datasets, which are re-purposed for exploring
target-oriented dialogue. Our automatic and human evaluations demonstrate that
the proposed methods significantly outperform various baseline models.
\\ ( https://arxiv.org/abs/2403.06063 ,  541kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06097
Date: Sun, 10 Mar 2024 05:12:16 GMT   (325kb,D)

Title: Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese
  Address Entity Recognition Dataset for UAV Delivery
Authors: Yuxuan Yao, Sichun Luo, Haohan Zhao, Guanzhi Deng, Linqi Song
Categories: cs.CL cs.AI cs.IR
Comments: Accepted by TheWebConf'24 (WWW'24) as a Resource Paper
\\
  We present CNER-UAV, a fine-grained \textbf{C}hinese \textbf{N}ame
\textbf{E}ntity \textbf{R}ecognition dataset specifically designed for the task
of address resolution in \textbf{U}nmanned \textbf{A}erial \textbf{V}ehicle
delivery systems. The dataset encompasses a diverse range of five categories,
enabling comprehensive training and evaluation of NER models. To construct this
dataset, we sourced the data from a real-world UAV delivery system and
conducted a rigorous data cleaning and desensitization process to ensure
privacy and data integrity. The resulting dataset, consisting of around 12,000
annotated samples, underwent human experts and \textbf{L}arge \textbf{L}anguage
\textbf{M}odel annotation. We evaluated classical NER models on our dataset and
provided in-depth analysis. The dataset and models are publicly available at
\url{https://github.com/zhhvvv/CNER-UAV}.
\\ ( https://arxiv.org/abs/2403.06097 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06108
Date: Sun, 10 Mar 2024 06:30:54 GMT   (1830kb,D)

Title: Large Language Models on Fine-grained Emotion Detection Dataset with
  Data Augmentation and Transfer Learning
Authors: Kaipeng Wang, Zhi Jing, Yongye Su, Yikun Han
Categories: cs.CL cs.AI
\\
  This paper delves into enhancing the classification performance on the
GoEmotions dataset, a large, manually annotated dataset for emotion detection
in text. The primary goal of this paper is to address the challenges of
detecting subtle emotions in text, a complex issue in Natural Language
Processing (NLP) with significant practical applications. The findings offer
valuable insights into addressing the challenges of emotion detection in text
and suggest directions for future research, including the potential for a
survey paper that synthesizes methods and performances across various datasets
in this domain.
\\ ( https://arxiv.org/abs/2403.06108 ,  1830kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06115
Date: Sun, 10 Mar 2024 07:21:31 GMT   (3126kb,D)

Title: FMPAF: How Do Fed Chairs Affect the Financial Market? A Fine-grained
  Monetary Policy Analysis Framework on Their Language
Authors: Yayue Deng, Mohan Xu, Yao Tang
Categories: cs.CL cs.AI cs.CE
Comments: accepted by AAAI 2024 Workshop: AI in Finance for Social Impact
\\
  The effectiveness of central bank communication is a crucial aspect of
monetary policy transmission. While recent research has examined the influence
of policy communication by the chairs of the Federal Reserve on various
financial variables, much of the literature relies on rule-based or
dictionary-based methods in parsing the language of the chairs, leaving nuanced
information about policy stance contained in nonverbal emotion out of the
analysis. In the current study, we propose the Fine-Grained Monetary Policy
Analysis Framework (FMPAF), a novel approach that integrates large language
models (LLMs) with regression analysis to provide a comprehensive analysis of
the impact of the press-conference communications of chairs of the Federal
Reserve on financial markets. We conduct extensive comparisons of model
performance under different levels of granularity, modalities, and
communication scenarios. Based on our preferred specification, a one-unit
increase in the sentiment score is associated with an increase of the price of
S\&P 500 Exchange-Traded Fund by approximately 500 basis points, a
15-basis-point decrease in the policy interest rate, while not leading to a
significant response in exchange rates.
\\ ( https://arxiv.org/abs/2403.06115 ,  3126kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06139
Date: Sun, 10 Mar 2024 08:59:04 GMT   (570kb,D)

Title: Fine-grainedly Synthesize Streaming Data Based On Large Language Models
  With Graph Structure Understanding For Data Sparsity
Authors: Xin Zhang, Linhai Zhang, Deyu Zhou, Guoqiang Xu
Categories: cs.CL cs.AI
\\
  Due to the sparsity of user data, sentiment analysis on user reviews in
e-commerce platforms often suffers from poor performance, especially when faced
with extremely sparse user data or long-tail labels. Recently, the emergence of
LLMs has introduced new solutions to such problems by leveraging graph
structures to generate supplementary user profiles. However, previous
approaches have not fully utilized the graph understanding capabilities of LLMs
and have struggled to adapt to complex streaming data environments. In this
work, we propose a fine-grained streaming data synthesis framework that
categorizes sparse users into three categories: Mid-tail, Long-tail, and
Extreme. Specifically, we design LLMs to comprehensively understand three key
graph elements in streaming data, including Local-global Graph Understanding,
Second-Order Relationship Extraction, and Product Attribute Understanding,
which enables the generation of high-quality synthetic data to effectively
address sparsity across different categories. Experimental results on three
real datasets demonstrate significant performance improvements, with
synthesized data contributing to MSE reductions of 45.85%, 3.16%, and 62.21%,
respectively.
\\ ( https://arxiv.org/abs/2403.06139 ,  570kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06149
Date: Sun, 10 Mar 2024 09:39:00 GMT   (80kb,D)

Title: Can Large Language Models Automatically Score Proficiency of Written
  Essays?
Authors: Watheq Mansour, Salam Albatarni, Sohaila Eltanbouly, Tamer Elsayed
Categories: cs.CL cs.AI
Comments: Accepted at LREC-COLING 2024
\\
  Although several methods were proposed to address the problem of automated
essay scoring (AES) in the last 50 years, there is still much to desire in
terms of effectiveness. Large Language Models (LLMs) are transformer-based
models that demonstrate extraordinary capabilities on various tasks. In this
paper, we test the ability of LLMs, given their powerful linguistic knowledge,
to analyze and effectively score written essays. We experimented with two
popular LLMs, namely ChatGPT and Llama. We aim to check if these models can do
this task and, if so, how their performance is positioned among the
state-of-the-art (SOTA) models across two levels, holistically and per
individual writing trait. We utilized prompt-engineering tactics in designing
four different prompts to bring their maximum potential to this task. Our
experiments conducted on the ASAP dataset revealed several interesting
observations. First, choosing the right prompt depends highly on the model and
nature of the task. Second, the two LLMs exhibited comparable average
performance in AES, with a slight advantage for ChatGPT. Finally, despite the
performance gap between the two LLMs and SOTA models in terms of predictions,
they provide feedback to enhance the quality of the essays, which can
potentially help both teachers and students.
\\ ( https://arxiv.org/abs/2403.06149 ,  80kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06201
Date: Sun, 10 Mar 2024 12:50:35 GMT   (4644kb,D)

Title: Are You Being Tracked? Discover the Power of Zero-Shot Trajectory
  Tracing with LLMs!
Authors: Huanqi Yang, Sijie Ji, Rucheng Wu, Weitao Xu
Categories: cs.CL cs.AI cs.HC cs.LG
\\
  There is a burgeoning discussion around the capabilities of Large Language
Models (LLMs) in acting as fundamental components that can be seamlessly
incorporated into Artificial Intelligence of Things (AIoT) to interpret complex
trajectories. This study introduces LLMTrack, a model that illustrates how LLMs
can be leveraged for Zero-Shot Trajectory Recognition by employing a novel
single-prompt technique that combines role-play and think step-by-step
methodologies with unprocessed Inertial Measurement Unit (IMU) data. We
evaluate the model using real-world datasets designed to challenge it with
distinct trajectories characterized by indoor and outdoor scenarios. In both
test scenarios, LLMTrack not only meets but exceeds the performance benchmarks
set by traditional machine learning approaches and even contemporary
state-of-the-art deep learning models, all without the requirement of training
on specialized datasets. The results of our research suggest that, with
strategically designed prompts, LLMs can tap into their extensive knowledge
base and are well-equipped to analyze raw sensor data with remarkable
effectiveness.
\\ ( https://arxiv.org/abs/2403.06201 ,  4644kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06204
Date: Sun, 10 Mar 2024 13:02:27 GMT   (1247kb,D)

Title: Identifying and interpreting non-aligned human conceptual
  representations using language modeling
Authors: Wanqian Bao and Uri Hasson
Categories: cs.CL
Comments: To appear at the ICLR 2024 Workshop on Representational Alignment
  (Re-Align)
\\
  The question of whether people's experience in the world shapes conceptual
representation and lexical semantics is longstanding. Word-association,
feature-listing and similarity rating tasks aim to address this question but
require a subjective interpretation of the latent dimensions identified. In
this study, we introduce a supervised representational-alignment method that
(i) determines whether two groups of individuals share the same basis of a
certain category, and (ii) explains in what respects they differ. In applying
this method, we show that congenital blindness induces conceptual
reorganization in both a-modal and sensory-related verbal domains, and we
identify the associated semantic shifts. We first apply supervised
feature-pruning to a language model (GloVe) to optimize prediction accuracy of
human similarity judgments from word embeddings. Pruning identifies one subset
of retained GloVe features that optimizes prediction of judgments made by
sighted individuals and another subset that optimizes judgments made by blind.
A linear probing analysis then interprets the latent semantics of these
feature-subsets by learning a mapping from the retained GloVe features to 65
interpretable semantic dimensions. We applied this approach to seven semantic
domains, including verbs related to motion, sight, touch, and amodal verbs
related to knowledge acquisition. We find that blind individuals more strongly
associate social and cognitive meanings to verbs related to motion or those
communicating non-speech vocal utterances (e.g., whimper, moan). Conversely,
for amodal verbs, they demonstrate much sparser information. Finally, for some
verbs, representations of blind and sighted are highly similar. The study
presents a formal approach for studying interindividual differences in word
meaning, and the first demonstration of how blindness impacts conceptual
representation of everyday verbs.
\\ ( https://arxiv.org/abs/2403.06204 ,  1247kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06208
Date: Sun, 10 Mar 2024 13:04:54 GMT   (561kb,D)

Title: Personalized LoRA for Human-Centered Text Understanding
Authors: You Zhang, Jin Wang, Liang-Chih Yu, Dan Xu, Xuejie Zhang
Categories: cs.CL
Comments: Accepted by AAAI 2024
\\
  Effectively and efficiently adapting a pre-trained language model (PLM) for
human-centered text understanding (HCTU) is challenging since user tokens are
million-level in most personalized applications and do not have concrete
explicit semantics. A standard and parameter-efficient approach (e.g., LoRA)
necessitates memorizing numerous suits of adapters for each user. In this work,
we introduce a personalized LoRA (PLoRA) with a plug-and-play (PnP) framework
for the HCTU task. PLoRA is effective, parameter-efficient, and dynamically
deploying in PLMs. Moreover, a personalized dropout and a mutual information
maximizing strategies are adopted and hence the proposed PLoRA can be well
adapted to few/zero-shot learning scenarios for the cold-start issue.
Experiments conducted on four benchmark datasets show that the proposed method
outperforms existing methods in full/few/zero-shot learning scenarios for the
HCTU task, even though it has fewer trainable parameters. For reproducibility,
the code for this paper is available at: https://github.com/yoyo-yun/PLoRA.
\\ ( https://arxiv.org/abs/2403.06208 ,  561kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06259
Date: Sun, 10 Mar 2024 16:57:10 GMT   (8442kb,D)

Title: Editing Conceptual Knowledge for Large Language Models
Authors: Xiaohan Wang, Shengyu Mao, Ningyu Zhang, Shumin Deng, Yunzhi Yao, Yue
  Shen, Lei Liang, Jinjie Gu, Huajun Chen
Categories: cs.CL cs.AI cs.DB cs.IR cs.LG
Comments: Work in progress. Code: https://github.com/zjunlp/EasyEdit Dataset:
  https://huggingface.co/datasets/zjunlp/ConceptEdit
\\
  Recently, there has been a growing interest in knowledge editing for Large
Language Models (LLMs). Current approaches and evaluations merely explore the
instance-level editing, while whether LLMs possess the capability to modify
concepts remains unclear. This paper pioneers the investigation of editing
conceptual knowledge for LLMs, by constructing a novel benchmark dataset
ConceptEdit and establishing a suite of new metrics for evaluation. The
experimental results reveal that, although existing editing methods can
efficiently modify concept-level definition to some extent, they also have the
potential to distort the related instantial knowledge in LLMs, leading to poor
performance. We anticipate this can inspire further progress in better
understanding LLMs. Our project homepage is available at
https://zjunlp.github.io/project/ConceptEdit.
\\ ( https://arxiv.org/abs/2403.06259 ,  8442kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06260
Date: Sun, 10 Mar 2024 16:57:51 GMT   (152kb,D)

Title: SCORE: Self-supervised Correspondence Fine-tuning for Improved Content
  Representations
Authors: Amit Meghanani and Thomas Hain
Categories: cs.CL cs.SD eess.AS
Comments: Accepted at ICASSP 2024
\\
  There is a growing interest in cost-effective self-supervised fine-tuning
(SSFT) of self-supervised learning (SSL)-based speech models to obtain
task-specific representations. These task-specific representations are used for
robust performance on various downstream tasks by fine-tuning on the labelled
data. This work presents a cost-effective SSFT method named Self-supervised
Correspondence (SCORE) fine-tuning to adapt the SSL speech representations for
content-related tasks. The proposed method uses a correspondence training
strategy, aiming to learn similar representations from perturbed speech and
original speech. Commonly used data augmentation techniques for content-related
tasks (ASR) are applied to obtain perturbed speech. SCORE fine-tuned HuBERT
outperforms the vanilla HuBERT on SUPERB benchmark with only a few hours of
fine-tuning (< 5 hrs) on a single GPU for automatic speech recognition, phoneme
recognition, and query-by-example tasks, with relative improvements of 1.09%,
3.58%, and 12.65%, respectively. SCORE provides competitive results with the
recently proposed SSFT method SPIN, using only 1/3 of the processed speech
compared to SPIN.
\\ ( https://arxiv.org/abs/2403.06260 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06265
Date: Sun, 10 Mar 2024 17:02:53 GMT   (2399kb,D)

Title: Unpacking Tokenization: Evaluating Text Compression and its Correlation
  with Model Performance
Authors: Omer Goldman, Avi Caciularu, Matan Eyal, Kris Cao, Idan Szpektor, Reut
  Tsarfaty
Categories: cs.CL cs.AI cs.LG
\\
  Despite it being the cornerstone of BPE, the most common tokenization
algorithm, the importance of compression in the tokenization process is still
unclear. In this paper, we argue for the theoretical importance of compression,
that can be viewed as 0-gram language modeling where equal probability is
assigned to all tokens. We also demonstrate the empirical importance of
compression for downstream success of pre-trained language models. We control
the compression ability of several BPE tokenizers by varying the amount of
documents available during their training: from 1 million documents to a
character-based tokenizer equivalent to no training data at all. We then
pre-train English language models based on those tokenizers and fine-tune them
over several tasks. We show that there is a correlation between tokenizers'
compression and models' downstream performance, suggesting that compression is
a reliable intrinsic indicator of tokenization quality. These correlations are
more pronounced for generation tasks (over classification) or for smaller
models (over large ones). We replicated a representative part of our
experiments on Turkish and found similar results, confirming that our results
hold for languages with typological characteristics dissimilar to English. We
conclude that building better compressing tokenizers is a fruitful avenue for
further research and for improving overall model performance.
\\ ( https://arxiv.org/abs/2403.06265 ,  2399kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06301
Date: Sun, 10 Mar 2024 20:20:16 GMT   (8138kb,D)

Title: LIEDER: Linguistically-Informed Evaluation for Discourse Entity
  Recognition
Authors: Xiaomeng Zhu and Robert Frank
Categories: cs.CL
\\
  Discourse Entity (DE) recognition is the task of identifying novel and known
entities introduced within a text. While previous work has found that large
language models have basic, if imperfect, DE recognition abilities (Schuster
and Linzen, 2022), it remains largely unassessed which of the fundamental
semantic properties that govern the introduction and subsequent reference to
DEs they have knowledge of. We propose the Linguistically-Informed Evaluation
for Discourse Entity Recognition (LIEDER) dataset that allows for a detailed
examination of language models' knowledge of four crucial semantic properties:
existence, uniqueness, plurality, and novelty. We find evidence that
state-of-the-art large language models exhibit sensitivity to all of these
properties except novelty, which demonstrates that they have yet to reach
human-level language understanding abilities.
\\ ( https://arxiv.org/abs/2403.06301 ,  8138kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06326
Date: Sun, 10 Mar 2024 22:14:54 GMT   (9142kb,D)

Title: From Instructions to Constraints: Language Model Alignment with
  Automatic Constraint Verification
Authors: Fei Wang, Chao Shang, Sarthak Jain, Shuai Wang, Qiang Ning, Bonan Min,
  Vittorio Castelli, Yassine Benajiba, Dan Roth
Categories: cs.CL cs.AI cs.LG
\\
  User alignment is crucial for adapting general-purpose language models (LMs)
to downstream tasks, but human annotations are often not available for all
types of instructions, especially those with customized constraints. We observe
that user instructions typically contain constraints. While assessing response
quality in terms of the whole instruction is often costly, efficiently
evaluating the satisfaction rate of constraints is feasible. We investigate
common constraints in NLP tasks, categorize them into three classes based on
the types of their arguments, and propose a unified framework, ACT (Aligning to
ConsTraints), to automatically produce supervision signals for user alignment
with constraints. Specifically, ACT uses constraint verifiers, which are
typically easy to implement in practice, to compute constraint satisfaction
rate (CSR) of each response. It samples multiple responses for each prompt and
collect preference labels based on their CSR automatically. Subsequently, ACT
adapts the LM to the target task through a ranking-based learning process.
Experiments on fine-grained entity typing, abstractive summarization, and
temporal question answering show that ACT is able to enhance LMs' capability to
adhere to different classes of constraints, thereby improving task performance.
Further experiments show that the constraint-following capabilities are
transferable.
\\ ( https://arxiv.org/abs/2403.06326 ,  9142kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06350
Date: Mon, 11 Mar 2024 00:46:56 GMT   (10054kb,D)

Title: IndicLLMSuite: A Blueprint for Creating Pre-training and Fine-Tuning
  Datasets for Indian Languages
Authors: Mohammed Safi Ur Rahman Khan, Priyam Mehta, Ananth Sankar, Umashankar
  Kumaravelan, Sumanth Doddapaneni, Suriyaprasaad G, Varun Balan G, Sparsh
  Jain, Anoop Kunchukuttan, Pratyush Kumar, Raj Dabre, Mitesh M. Khapra
Categories: cs.CL
\\
  Despite the considerable advancements in English LLMs, the progress in
building comparable models for other languages has been hindered due to the
scarcity of tailored resources. Our work aims to bridge this divide by
introducing an expansive suite of resources specifically designed for the
development of Indic LLMs, covering 22 languages, containing a total of 251B
tokens and 74.8M instruction-response pairs. Recognizing the importance of both
data quality and quantity, our approach combines highly curated manually
verified data, unverified yet valuable data, and synthetic data. We build a
clean, open-source pipeline for curating pre-training data from diverse
sources, including websites, PDFs, and videos, incorporating best practices for
crawling, cleaning, flagging, and deduplication. For instruction-fine tuning,
we amalgamate existing Indic datasets, translate/transliterate English datasets
into Indian languages, and utilize LLaMa2 and Mixtral models to create
conversations grounded in articles from Indian Wikipedia and Wikihow.
Additionally, we address toxicity alignment by generating toxic prompts for
multiple scenarios and then generate non-toxic responses by feeding these toxic
prompts to an aligned LLaMa2 model. We hope that the datasets, tools, and
resources released as a part of this work will not only propel the research and
development of Indic LLMs but also establish an open-source blueprint for
extending such efforts to other languages. The data and other artifacts created
as part of this work are released with permissive licenses.
\\ ( https://arxiv.org/abs/2403.06350 ,  10054kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06354
Date: Mon, 11 Mar 2024 01:04:36 GMT   (1785kb)

Title: Amharic LLaMA and LLaVA: Multimodal LLMs for Low Resource Languages
Authors: Michael Andersland
Categories: cs.CL
\\
  Large Language Models (LLMs) like GPT-4 and LLaMA have shown incredible
proficiency at natural language processing tasks and have even begun to excel
at tasks across other modalities such as vision and audio. Despite their
success, LLMs often struggle to perform well on low-resource languages because
there is so little training data available. This shortcoming is especially
prevalent with open source models. In this work, we explore training LLaMA-2 to
speak Amharic, a language which is spoken by over 50 million people world wide,
but has orders of magnitude less data available than languages like English. We
employ methods previously used for training LLMs on other languages with data
scarcity, and use open source translation models to perform data augmentation
and grow our dataset from millions of tokens to billions. We further enhance
the capabilities of our model by connecting an image encoder and training on a
translated visual instruction tuning dataset in the same manner as LLaVA,
resulting in a multimodal Amharic LLM that can understand images along with
text. We introduce an Amharic version of a popular benchmarking dataset to
evaluate our work. Our models and dataset are open sourced and available on
GitHub.
\\ ( https://arxiv.org/abs/2403.06354 ,  1785kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06355
Date: Mon, 11 Mar 2024 01:07:36 GMT   (2680kb,D)

Title: Multi-modal Semantic Understanding with Contrastive Cross-modal Feature
  Alignment
Authors: Ming Zhang, Ke Chang and Yunfang Wu
Categories: cs.CL cs.CV
Comments: 10 pages, 4 figures, accepted by LREC-COLING 2024(main conference,
  long paper)
\\
  Multi-modal semantic understanding requires integrating information from
different modalities to extract users' real intention behind words. Most
previous work applies a dual-encoder structure to separately encode image and
text, but fails to learn cross-modal feature alignment, making it hard to
achieve cross-modal deep information interaction. This paper proposes a novel
CLIP-guided contrastive-learning-based architecture to perform multi-modal
feature alignment, which projects the features derived from different
modalities into a unified deep space. On multi-modal sarcasm detection (MMSD)
and multi-modal sentiment analysis (MMSA) tasks, the experimental results show
that our proposed model significantly outperforms several baselines, and our
feature alignment strategy brings obvious performance gain over models with
different aggregating methods and models even enriched with knowledge. More
importantly, our model is simple to implement without using task-specific
external knowledge, and thus can easily migrate to other multi-modal tasks. Our
source codes are available at https://github.com/ChangKe123/CLFA.
\\ ( https://arxiv.org/abs/2403.06355 ,  2680kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06360
Date: Mon, 11 Mar 2024 01:18:00 GMT   (913kb,D)

Title: Human and Automatic Interpretation of Romanian Noun Compounds
Authors: Ioana Marinescu and Christiane Fellbaum
Categories: cs.CL cs.AI
Comments: 6 pages, 2 figures, 3 tables
\\
  Determining the intended, context-dependent meanings of noun compounds like
"shoe sale" and "fire sale" remains a challenge for NLP. Previous work has
relied on inventories of semantic relations that capture the different meanings
between compound members. Focusing on Romanian compounds, whose morphosyntax
differs from that of their English counterparts, we propose a new set of
relations and test it with human annotators and a neural net classifier.
Results show an alignment of the network's predictions and human judgments,
even where the human agreement rate is low. Agreement tracks with the frequency
of the selected relations, regardless of structural differences. However, the
most frequently selected relation was none of the sixteen labeled semantic
relations, indicating the need for a better relation inventory.
\\ ( https://arxiv.org/abs/2403.06360 ,  913kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06399
Date: Mon, 11 Mar 2024 03:21:15 GMT   (8086kb,D)

Title: GlossLM: Multilingual Pretraining for Low-Resource Interlinear Glossing
Authors: Michael Ginn (1), Lindia Tjuatja (2), Taiqi He (2), Enora Rice (1),
  Graham Neubig (2), Alexis Palmer (1), Lori Levin (2) ((1) University of
  Colorado, (2) Carnegie Mellon University)
Categories: cs.CL
Comments: 18 pages, 3 figures Submitted to ACL ARR Feb 2024 First two authors
  are equal contribution
\\
  A key aspect of language documentation is the creation of annotated text in a
format such as interlinear glossed text (IGT), which captures fine-grained
morphosyntactic analyses in a morpheme-by-morpheme format. Prior work has
explored methods to automatically generate IGT in order to reduce the time cost
of language analysis. However, many languages (particularly those requiring
preservation) lack sufficient IGT data to train effective models, and
crosslingual transfer has been proposed as a method to overcome this
limitation.
  We compile the largest existing corpus of IGT data from a variety of sources,
covering over 450k examples across 1.8k languages, to enable research on
crosslingual transfer and IGT generation. Then, we pretrain a large
multilingual model on a portion of this corpus, and further finetune it to
specific languages. Our model is competitive with state-of-the-art methods for
segmented data and large monolingual datasets. Meanwhile, our model outperforms
SOTA models on unsegmented text and small corpora by up to 6.6% morpheme
accuracy, demonstrating the effectiveness of crosslingual transfer for
low-resource languages.
\\ ( https://arxiv.org/abs/2403.06399 ,  8086kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06402
Date: Mon, 11 Mar 2024 03:28:13 GMT   (8910kb,D)

Title: 'One size doesn't fit all': Learning how many Examples to use for
  In-Context Learning for Improved Text Classification
Authors: Manish Chandra, Debasis Ganguly, Yiwen Li, Iadh Ounis
Categories: cs.CL cs.LG
\\
  Predictive models in natural language processing (NLP) have evolved from
training models from scratch to fine-tuning pre-trained models with labelled
data. An extreme form of this fine-tuning involves in-context learning (ICL),
where the output of a pre-trained generative model (frozen decoder parameters)
is controlled only with variations in the input strings (called instructions or
prompts). An important component of ICL is the use of a small number of
labelled data instances as examples in the prompt. While existing work uses a
static number of examples during inference for each data instance, in this
paper we propose a novel methodology of dynamically adapting the number of
examples as per the data. This is analogous to the use of a variable-sized
neighborhood in k-nearest neighbors (k-NN) classifier. In our proposed workflow
of adaptive ICL (AICL), the number of demonstrations to employ during the
inference on a particular data instance is predicted by the Softmax posteriors
of a classifier. The parameters of this classifier are fitted on the optimal
number of examples in ICL required to correctly infer the label of each
instance in the training set with the hypothesis that a test instance that is
similar to a training instance should use the same (or a closely matching)
number of few-shot examples. Our experiments show that our AICL method results
in improvement in text classification task on several standard datasets.
\\ ( https://arxiv.org/abs/2403.06402 ,  8910kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06410
Date: Mon, 11 Mar 2024 03:45:09 GMT   (1783kb,D)

Title: A Logical Pattern Memory Pre-trained Model for Entailment Tree
  Generation
Authors: Li Yuan, Yi Cai, Haopeng Ren, Jiexin Wang
Categories: cs.CL cs.AI
Comments: Accepted By Coling 2024
\\
  Generating coherent and credible explanations remains a significant challenge
in the field of AI. In recent years, researchers have delved into the
utilization of entailment trees to depict explanations, which exhibit a
reasoning process of how a hypothesis is deduced from the supporting facts.
However, existing models often overlook the importance of generating
intermediate conclusions with logical consistency from the given facts, leading
to inaccurate conclusions and undermining the overall credibility of entailment
trees. To address this limitation, we propose the logical pattern memory
pre-trained model (LMPM). LMPM incorporates an external memory structure to
learn and store the latent representations of logical patterns, which aids in
generating logically consistent conclusions. Furthermore, to mitigate the
influence of logically irrelevant domain knowledge in the Wikipedia-based data,
we introduce an entity abstraction approach to construct the dataset for
pre-training LMPM. The experimental results highlight the effectiveness of our
approach in improving the quality of entailment tree generation. By leveraging
logical entailment patterns, our model produces more coherent and reasonable
conclusions that closely align with the underlying premises. Code and Data are
released at https://github.com/YuanLi95/T5-LMPM
\\ ( https://arxiv.org/abs/2403.06410 ,  1783kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06412
Date: Mon, 11 Mar 2024 03:54:33 GMT   (8945kb,D)

Title: CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in
  Korean
Authors: Eunsu Kim, Juyoung Suk, Philhoon Oh, Haneul Yoo, James Thorne, Alice
  Oh
Categories: cs.CL
\\
  Despite the rapid development of large language models (LLMs) for the Korean
language, there remains an obvious lack of benchmark datasets that test the
requisite Korean cultural and linguistic knowledge. Because many existing
Korean benchmark datasets are derived from the English counterparts through
translation, they often overlook the different cultural contexts. For the few
benchmark datasets that are sourced from Korean data capturing cultural
knowledge, only narrow tasks such as bias and hate speech detection are
offered. To address this gap, we introduce a benchmark of Cultural and
Linguistic Intelligence in Korean (CLIcK), a dataset comprising 1,995 QA pairs.
CLIcK sources its data from official Korean exams and textbooks, partitioning
the questions into eleven categories under the two main categories of language
and culture. For each instance in CLIcK, we provide fine-grained annotation of
which cultural and linguistic knowledge is required to answer the question
correctly. Using CLIcK, we test 13 language models to assess their performance.
Our evaluation uncovers insights into their performances across the categories,
as well as the diverse factors affecting their comprehension. CLIcK offers the
first large-scale comprehensive Korean-centric analysis of LLMs' proficiency in
Korean culture and language.
\\ ( https://arxiv.org/abs/2403.06412 ,  8945kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06414
Date: Mon, 11 Mar 2024 03:55:24 GMT   (1781kb,D)

Title: Evolving Knowledge Distillation with Large Language Models and Active
  Learning
Authors: Chengyuan Liu, Yangyang Kang, Fubang Zhao, Kun Kuang, Zhuoren Jiang,
  Changlong Sun, Fei Wu
Categories: cs.CL
Comments: Accepted by COLING 2024
\\
  Large language models (LLMs) have demonstrated remarkable capabilities across
various NLP tasks. However, their computational costs are prohibitively high.
To address this issue, previous research has attempted to distill the knowledge
of LLMs into smaller models by generating annotated data. Nonetheless, these
works have mainly focused on the direct use of LLMs for text generation and
labeling, without fully exploring their potential to comprehend the target task
and acquire valuable knowledge. In this paper, we propose EvoKD: Evolving
Knowledge Distillation, which leverages the concept of active learning to
interactively enhance the process of data generation using large language
models, simultaneously improving the task capabilities of small domain model
(student model). Different from previous work, we actively analyze the student
model's weaknesses, and then synthesize labeled samples based on the analysis.
In addition, we provide iterative feedback to the LLMs regarding the student
model's performance to continuously construct diversified and challenging
samples. Experiments and analysis on different NLP tasks, namely, text
classification and named entity recognition show the effectiveness of EvoKD.
\\ ( https://arxiv.org/abs/2403.06414 ,  1781kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06448
Date: Mon, 11 Mar 2024 05:51:03 GMT   (8027kb,D)

Title: Unsupervised Real-Time Hallucination Detection based on the Internal
  States of Large Language Models
Authors: Weihang Su, Changyue Wang, Qingyao Ai, Yiran HU, Zhijing Wu, Yujia
  Zhou, Yiqun Liu
Categories: cs.CL cs.AI
\\
  Hallucinations in large language models (LLMs) refer to the phenomenon of
LLMs producing responses that are coherent yet factually inaccurate. This issue
undermines the effectiveness of LLMs in practical applications, necessitating
research into detecting and mitigating hallucinations of LLMs. Previous studies
have mainly concentrated on post-processing techniques for hallucination
detection, which tend to be computationally intensive and limited in
effectiveness due to their separation from the LLM's inference process. To
overcome these limitations, we introduce MIND, an unsupervised training
framework that leverages the internal states of LLMs for real-time
hallucination detection without requiring manual annotations. Additionally, we
present HELM, a new benchmark for evaluating hallucination detection across
multiple LLMs, featuring diverse LLM outputs and the internal states of LLMs
during their inference process. Our experiments demonstrate that MIND
outperforms existing state-of-the-art methods in hallucination detection.
\\ ( https://arxiv.org/abs/2403.06448 ,  8027kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06487
Date: Mon, 11 Mar 2024 07:50:29 GMT   (1506kb,D)

Title: Multilingual Turn-taking Prediction Using Voice Activity Projection
Authors: Koji Inoue, Bing'er Jiang, Erik Ekstedt, Tatsuya Kawahara, Gabriel
  Skantze
Categories: cs.CL cs.SD eess.AS
Comments: This paper has been accepted for presentation at The 2024 Joint
  International Conference on Computational Linguistics, Language Resources and
  Evaluation (LREC-COLING 2024) and represents the author's version of the work
\\
  This paper investigates the application of voice activity projection (VAP), a
predictive turn-taking model for spoken dialogue, on multilingual data,
encompassing English, Mandarin, and Japanese. The VAP model continuously
predicts the upcoming voice activities of participants in dyadic dialogue,
leveraging a cross-attention Transformer to capture the dynamic interplay
between participants. The results show that a monolingual VAP model trained on
one language does not make good predictions when applied to other languages.
However, a multilingual model, trained on all three languages, demonstrates
predictive performance on par with monolingual models across all languages.
Further analyses show that the multilingual model has learned to discern the
language of the input signal. We also analyze the sensitivity to pitch, a
prosodic cue that is thought to be important for turn-taking. Finally, we
compare two different audio encoders, contrastive predictive coding (CPC)
pre-trained on English, with a recent model based on multilingual wav2vec 2.0
(MMS).
\\ ( https://arxiv.org/abs/2403.06487 ,  1506kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06520
Date: Mon, 11 Mar 2024 08:52:52 GMT   (5113kb,D)

Title: How to Understand Named Entities: Using Common Sense for News Captioning
Authors: Ning Xu, Yanhui Wang, Tingting Zhang, Hongshuo Tian, Mohan
  Kankanhalli, An-An Liu
Categories: cs.CL cs.AI
\\
  News captioning aims to describe an image with its news article body as
input. It greatly relies on a set of detected named entities, including
real-world people, organizations, and places. This paper exploits commonsense
knowledge to understand named entities for news captioning. By ``understand'',
we mean correlating the news content with common sense in the wild, which helps
an agent to 1) distinguish semantically similar named entities and 2) describe
named entities using words outside of training corpora. Our approach consists
of three modules: (a) Filter Module aims to clarify the common sense concerning
a named entity from two aspects: what does it mean? and what is it related to?,
which divide the common sense into explanatory knowledge and relevant
knowledge, respectively. (b) Distinguish Module aggregates explanatory
knowledge from node-degree, dependency, and distinguish three aspects to
distinguish semantically similar named entities. (c) Enrich Module attaches
relevant knowledge to named entities to enrich the entity description by
commonsense information (e.g., identity and social position). Finally, the
probability distributions from both modules are integrated to generate the news
captions. Extensive experiments on two challenging datasets (i.e., GoodNews and
NYTimes) demonstrate the superiority of our method. Ablation studies and
visualization further validate its effectiveness in understanding named
entities.
\\ ( https://arxiv.org/abs/2403.06520 ,  5113kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06537
Date: Mon, 11 Mar 2024 09:24:06 GMT   (78kb,D)

Title: On the Consideration of AI Openness: Can Good Intent Be Abused?
Authors: Yeeun Kim, Eunkyung Choi, Hyunjun Kim, Hongseok Oh, Hyunseo Shin,
  Wonseok Hwang
Categories: cs.CL
Comments: 10 pages
\\
  Openness is critical for the advancement of science. In particular, recent
rapid progress in AI has been made possible only by various open-source models,
datasets, and libraries. However, this openness also means that technologies
can be freely used for socially harmful purposes. Can open-source models or
datasets be used for malicious purposes? If so, how easy is it to adapt
technology for such goals? Here, we conduct a case study in the legal domain, a
realm where individual decisions can have profound social consequences. To this
end, we build EVE, a dataset consisting of 200 examples of questions and
corresponding answers about criminal activities based on 200 Korean precedents.
We found that a widely accepted open-source LLM, which initially refuses to
answer unethical questions, can be easily tuned with EVE to provide unethical
and informative answers about criminal activities. This implies that although
open-source technologies contribute to scientific progress, some care must be
taken to mitigate possible malicious use cases. Warning: This paper contains
contents that some may find unethical.
\\ ( https://arxiv.org/abs/2403.06537 ,  78kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06570
Date: Mon, 11 Mar 2024 10:11:29 GMT   (1954kb,D)

Title: Improving Speaker Assignment in Speaker-Attributed ASR for Real Meeting
  Applications
Authors: Can Cui (MULTISPEECH), Imran Ahamad Sheikh, Mostafa Sadeghi
  (MULTISPEECH), Emmanuel Vincent (MULTISPEECH)
Categories: cs.CL
Comments: Submitted to Odyssey 2024
\\
  Past studies on end-to-end meeting transcription have focused on model
architecture and have mostly been evaluated on simulated meeting data. We
present a novel study aiming to optimize the use of a Speaker-Attributed ASR
(SA-ASR) system in real-life scenarios, such as the AMI meeting corpus, for
improved speaker assignment of speech segments. First, we propose a pipeline
tailored to real-life applications involving Voice Activity Detection (VAD),
Speaker Diarization (SD), and SA-ASR. Second, we advocate using VAD output
segments to fine-tune the SA-ASR model, considering that it is also applied to
VAD segments during test, and show that this results in a relative reduction of
Speaker Error Rate (SER) up to 28%. Finally, we explore strategies to enhance
the extraction of the speaker embedding templates used as inputs by the SA-ASR
system. We show that extracting them from SD output rather than annotated
speaker segments results in a relative SER reduction up to 20%.
\\ ( https://arxiv.org/abs/2403.06570 ,  1954kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06574
Date: Mon, 11 Mar 2024 10:24:37 GMT   (1076kb,D)

Title: AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large
  Language Models
Authors: Yuting Wei, Yuanxing Xu, Xinru Wei, Simin Yang, Yangfu Zhu, Yuqing Li,
  Di Liu, Bin Wu
Categories: cs.CL
\\
  Given the importance of ancient Chinese in capturing the essence of rich
historical and cultural heritage, the rapid advancements in Large Language
Models (LLMs) necessitate benchmarks that can effectively evaluate their
understanding of ancient contexts. To meet this need, we present AC-EVAL, an
innovative benchmark designed to assess the advanced knowledge and reasoning
capabilities of LLMs within the context of ancient Chinese. AC-EVAL is
structured across three levels of difficulty reflecting different facets of
language comprehension: general historical knowledge, short text understanding,
and long text comprehension. The benchmark comprises 13 tasks, spanning
historical facts, geography, social customs, art, philosophy, classical poetry
and prose, providing a comprehensive assessment framework. Our extensive
evaluation of top-performing LLMs, tailored for both English and Chinese,
reveals a substantial potential for enhancing ancient text comprehension. By
highlighting the strengths and weaknesses of LLMs, AC-EVAL aims to promote
their development and application forward in the realms of ancient Chinese
language education and scholarly research. The AC-EVAL data and evaluation code
are available at https://github.com/yuting-wei/AC-EVAL.
\\ ( https://arxiv.org/abs/2403.06574 ,  1076kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06591
Date: Mon, 11 Mar 2024 10:35:53 GMT   (808kb,D)

Title: Academically intelligent LLMs are not necessarily socially intelligent
Authors: Ruoxi Xu, Hongyu Lin, Xianpei Han, Le Sun, Yingfei Sun
Categories: cs.CL cs.CY
\\
  The academic intelligence of large language models (LLMs) has made remarkable
progress in recent times, but their social intelligence performance remains
unclear. Inspired by established human social intelligence frameworks,
particularly Daniel Goleman's social intelligence theory, we have developed a
standardized social intelligence test based on real-world social scenarios to
comprehensively assess the social intelligence of LLMs, termed as the
Situational Evaluation of Social Intelligence (SESI). We conducted an extensive
evaluation with 13 recent popular and state-of-art LLM agents on SESI. The
results indicate the social intelligence of LLMs still has significant room for
improvement, with superficially friendliness as a primary reason for errors.
Moreover, there exists a relatively low correlation between the social
intelligence and academic intelligence exhibited by LLMs, suggesting that
social intelligence is distinct from academic intelligence for LLMs.
Additionally, while it is observed that LLMs can't ``understand'' what social
intelligence is, their social intelligence, similar to that of humans, is
influenced by social factors.
\\ ( https://arxiv.org/abs/2403.06591 ,  808kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06609
Date: Mon, 11 Mar 2024 10:53:20 GMT   (364kb,D)

Title: Guiding Clinical Reasoning with Large Language Models via Knowledge
  Seeds
Authors: Jiageng WU, Xian Wu, Jie Yang
Categories: cs.CL cs.AI
\\
  Clinical reasoning refers to the cognitive process that physicians employ in
evaluating and managing patients. This process typically involves suggesting
necessary examinations, diagnosing patients' diseases, and deciding on
appropriate therapies, etc. Accurate clinical reasoning requires extensive
medical knowledge and rich clinical experience, setting a high bar for
physicians. This is particularly challenging in developing countries due to the
overwhelming number of patients and limited physician resources, contributing
significantly to global health inequity and necessitating automated clinical
reasoning approaches. Recently, the emergence of large language models (LLMs)
such as ChatGPT and GPT-4 have demonstrated their potential in clinical
reasoning. However, these LLMs are prone to hallucination problems, and the
reasoning process of LLMs may not align with the clinical decision path of
physicians. In this study, we introduce a novel framework, In-Context Padding
(ICP), designed to enhance LLMs with medical knowledge. Specifically, we infer
critical clinical reasoning elements (referred to as knowledge seeds) and use
these as anchors to guide the generation process of LLMs. Experiments on two
clinical question datasets demonstrate that ICP significantly improves the
clinical reasoning ability of LLMs.
\\ ( https://arxiv.org/abs/2403.06609 ,  364kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06611
Date: Mon, 11 Mar 2024 10:57:45 GMT   (356kb,D)

Title: MedKP: Medical Dialogue with Knowledge Enhancement and Clinical Pathway
  Encoding
Authors: Jiageng Wu, Xian Wu, Yefeng Zheng, Jie Yang
Categories: cs.CL cs.AI
\\
  With appropriate data selection and training techniques, Large Language
Models (LLMs) have demonstrated exceptional success in various medical
examinations and multiple-choice questions. However, the application of LLMs in
medical dialogue generation-a task more closely aligned with actual medical
practice-has been less explored. This gap is attributed to the insufficient
medical knowledge of LLMs, which leads to inaccuracies and hallucinated
information in the generated medical responses. In this work, we introduce the
Medical dialogue with Knowledge enhancement and clinical Pathway encoding
(MedKP) framework, which integrates an external knowledge enhancement module
through a medical knowledge graph and an internal clinical pathway encoding via
medical entities and physician actions. Evaluated with comprehensive metrics,
our experiments on two large-scale, real-world online medical consultation
datasets (MedDG and KaMed) demonstrate that MedKP surpasses multiple baselines
and mitigates the incidence of hallucinations, achieving a new
state-of-the-art. Extensive ablation studies further reveal the effectiveness
of each component of MedKP. This enhancement advances the development of
reliable, automated medical consultation responses using LLMs, thereby
broadening the potential accessibility of precise and real-time medical
assistance.
\\ ( https://arxiv.org/abs/2403.06611 ,  356kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06682
Date: Mon, 11 Mar 2024 12:57:28 GMT   (7527kb,D)

Title: Restoring Ancient Ideograph: A Multimodal Multitask Neural Network
  Approach
Authors: Siyu Duan, Jun Wang, Qi Su
Categories: cs.CL cs.CV cs.CY
Comments: Accept by Lrec-Coling 2024
\\
  Cultural heritage serves as the enduring record of human thought and history.
Despite significant efforts dedicated to the preservation of cultural relics,
many ancient artefacts have been ravaged irreversibly by natural deterioration
and human actions. Deep learning technology has emerged as a valuable tool for
restoring various kinds of cultural heritages, including ancient text
restoration. Previous research has approached ancient text restoration from
either visual or textual perspectives, often overlooking the potential of
synergizing multimodal information. This paper proposes a novel Multimodal
Multitask Restoring Model (MMRM) to restore ancient texts, particularly
emphasising the ideograph. This model combines context understanding with
residual visual information from damaged ancient artefacts, enabling it to
predict damaged characters and generate restored images simultaneously. We
tested the MMRM model through experiments conducted on both simulated datasets
and authentic ancient inscriptions. The results show that the proposed method
gives insightful restoration suggestions in both simulation experiments and
real-world scenarios. To the best of our knowledge, this work represents the
pioneering application of multimodal deep learning in ancient text restoration,
which will contribute to the understanding of ancient society and culture in
digital humanities fields.
\\ ( https://arxiv.org/abs/2403.06682 ,  7527kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06745
Date: Mon, 11 Mar 2024 14:10:57 GMT   (5180kb,D)

Title: ACT-MNMT Auto-Constriction Turning for Multilingual Neural Machine
  Translation
Authors: Shaojie Dai, Xin Liu, Ping Luo and Yue Yu
Categories: cs.CL cs.AI
\\
  Large language model (LLM) has achieved promising performance in multilingual
machine translation tasks through zero/few-shot prompts or prompt-tuning.
However, due to the mixture of multilingual data during the pre-training of
LLM, the LLM-based translation models face the off-target issue in both
prompt-based methods, including a series of phenomena, namely instruction
misunderstanding, translation with wrong language and over-generation. For this
issue, this paper introduces an
\textbf{\underline{A}}uto-\textbf{\underline{C}}onstriction
\textbf{\underline{T}}urning mechanism for \textbf{\underline{M}}ultilingual
\textbf{\underline{N}}eural \textbf{\underline{M}}achine
\textbf{\underline{T}}ranslation (\model), which is a novel supervised
fine-tuning mechanism and orthogonal to the traditional prompt-based methods.
In this method, \model automatically constructs a constrained template in the
target side by adding trigger tokens ahead of the ground truth. Furthermore,
trigger tokens can be arranged and combined freely to represent different task
semantics, and they can be iteratively updated to maximize the label
likelihood. Experiments are performed on WMT test sets with multiple metrics,
and the experimental results demonstrate that \model achieves substantially
improved performance across multiple translation directions and reduce the
off-target phenomena in the translation.
\\ ( https://arxiv.org/abs/2403.06745 ,  5180kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06754
Date: Mon, 11 Mar 2024 14:28:40 GMT   (760kb,D)

Title: ALaRM: Align Language Models via Hierarchical Rewards Modeling
Authors: Yuhang Lai, Siyuan Wang, Shujun Liu, Xuanjing Huang, Zhongyu Wei
Categories: cs.CL cs.AI cs.LG
Comments: 15 pages, 6 figures
\\
  We introduce ALaRM, the first framework modeling hierarchical rewards in
reinforcement learning from human feedback (RLHF), which is designed to enhance
the alignment of large language models (LLMs) with human preferences. The
framework addresses the limitations of current alignment approaches, which
often struggle with the inconsistency and sparsity of human supervision
signals, by integrating holistic rewards with aspect-specific rewards. This
integration enables more precise and consistent guidance of language models
towards desired outcomes, particularly in complex and open text generation
tasks. By employing a methodology that filters and combines multiple rewards
based on their consistency, the framework provides a reliable mechanism for
improving model alignment. We validate our approach through applications in
long-form question answering and machine translation tasks, employing
gpt-3.5-turbo for pairwise comparisons, and demonstrate improvements over
existing baselines. Our work underscores the effectiveness of hierarchical
rewards modeling in refining LLM training processes for better human preference
alignment. We release our code at https://ALaRM-fdu.github.io.
\\ ( https://arxiv.org/abs/2403.06754 ,  760kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06765
Date: Mon, 11 Mar 2024 14:35:45 GMT   (1794kb,D)

Title: ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large
  Language Model
Authors: Zhiwei Liu, Boyang Liu, Paul Thompson, Kailai Yang, Raghav Jain,
  Sophia Ananiadou
Categories: cs.CL
Comments: Work in progress
\\
  The internet has brought both benefits and harms to society. A prime example
of the latter is misinformation, including conspiracy theories, which flood the
web. Recent advances in natural language processing, particularly the emergence
of large language models (LLMs), have improved the prospects of accurate
misinformation detection. However, most LLM-based approaches to conspiracy
theory detection focus only on binary classification and fail to account for
the important relationship between misinformation and affective features (i.e.,
sentiment and emotions). Driven by a comprehensive analysis of conspiracy text
that reveals its distinctive affective features, we propose ConspEmoLLM, the
first open-source LLM that integrates affective information and is able to
perform diverse tasks relating to conspiracy theories. These tasks include not
only conspiracy theory detection, but also classification of theory type and
detection of related discussion (e.g., opinions towards theories). ConspEmoLLM
is fine-tuned based on an emotion-oriented LLM using our novel ConDID dataset,
which includes five tasks to support LLM instruction tuning and evaluation. We
demonstrate that when applied to these tasks, ConspEmoLLM largely outperforms
several open-source general domain LLMs and ChatGPT, as well as an LLM that has
been fine-tuned using ConDID, but which does not use affective features. This
project will be released on https://github.com/lzw108/ConspEmoLLM/.
\\ ( https://arxiv.org/abs/2403.06765 ,  1794kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06769
Date: Mon, 11 Mar 2024 14:38:16 GMT   (1539kb,D)

Title: Strength Lies in Differences! Towards Effective Non-collaborative
  Dialogues via Tailored Strategy Planning
Authors: Tong Zhang, Chen Huang, Yang Deng, Hongru Liang, Jia Liu, Zujie Wen,
  Wenqiang Lei, Tat-Seng Chua
Categories: cs.CL
Comments: A draft version and appendix will be coming soon!
\\
  We investigate non-collaborative dialogue agents that must engage in tailored
strategic planning for diverse users to secure a favorable agreement. This
poses challenges for existing dialogue agents due to two main reasons: their
inability to integrate user-specific characteristics into their strategic
planning and their training paradigm's failure to produce strategic planners
that can generalize to diverse users. To address these challenges, we propose
TRIP to enhance the capability in tailored strategic planning, incorporating a
user-aware strategic planning module and a population-based training paradigm.
Through experiments on benchmark non-collaborative dialogue tasks, we
demonstrate the effectiveness of TRIP in catering to diverse users.
\\ ( https://arxiv.org/abs/2403.06769 ,  1539kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06832
Date: Mon, 11 Mar 2024 15:48:43 GMT   (469kb,D)

Title: The Power of Noise: Toward a Unified Multi-modal Knowledge Graph
  Representation Framework
Authors: Zhuo Chen, Yin Fang, Yichi Zhang, Lingbing Guo, Jiaoyan Chen, Huajun
  Chen, Wen Zhang
Categories: cs.CL cs.AI
Comments: Ongoing work; 10 pages, 6 Tables, 2 Figures; Repo is available at
  https://github.com/zjukg/SNAG
\\
  The advancement of Multi-modal Pre-training highlights the necessity for a
robust Multi-Modal Knowledge Graph (MMKG) representation learning framework.
This framework is crucial for integrating structured knowledge into multi-modal
Large Language Models (LLMs) at scale, aiming to alleviate issues like
knowledge misconceptions and multi-modal hallucinations. In this work, to
evaluate models' ability to accurately embed entities within MMKGs, we focus on
two widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and
Multi-modal Entity Alignment (MMEA). Building on this foundation, we propose a
novel SNAG method that utilizes a Transformer-based architecture equipped with
modality-level noise masking for the robust integration of multi-modal entity
features in KGs. By incorporating specific training objectives for both MKGC
and MMEA, our approach achieves SOTA performance across a total of ten datasets
(three for MKGC and seven for MEMA), demonstrating its robustness and
versatility. Besides, SNAG can not only function as a standalone model but also
enhance other existing methods, providing stable performance improvements. Our
code and data are available at: https://github.com/zjukg/SNAG.
\\ ( https://arxiv.org/abs/2403.06832 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06840
Date: Mon, 11 Mar 2024 16:01:05 GMT   (1582kb,D)

Title: RA-ISF: Learning to Answer and Understand from Retrieval Augmentation
  via Iterative Self-Feedback
Authors: Yanming Liu, Xinyue Peng, Xuhong Zhang, Weihao Liu, Jianwei Yin,
  Jiannan Cao, Tianyu Du
Categories: cs.CL cs.AI
Comments: 15 pages, 4 figures. Providing first version RA-ISF
\\
  Large language models (LLMs) demonstrate exceptional performance in numerous
tasks but still heavily rely on knowledge stored in their parameters. Moreover,
updating this knowledge incurs high training costs. Retrieval-augmented
generation (RAG) methods address this issue by integrating external knowledge.
The model can answer questions it couldn't previously by retrieving knowledge
relevant to the query. This approach improves performance in certain scenarios
for specific tasks. However, if irrelevant texts are retrieved, it may impair
model performance. In this paper, we propose Retrieval Augmented Iterative
Self-Feedback (RA-ISF), a framework that iteratively decomposes tasks and
processes them in three submodules to enhance the model's problem-solving
capabilities. Experiments show that our method outperforms existing benchmarks,
performing well on models like GPT3.5, Llama2, significantly enhancing factual
reasoning capabilities and reducing hallucinations.
\\ ( https://arxiv.org/abs/2403.06840 ,  1582kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06857
Date: Mon, 11 Mar 2024 16:12:34 GMT   (1050kb)

Title: Development of a Reliable and Accessible Caregiving Language Model
  (CaLM)
Authors: Bambang Parmanto, Bayu Aryoyudanta, Wilbert Soekinto, I Made Agus
  Setiawan, Yuhan Wang, Haomin Hu, Andi Saptono, Yong K. Choi
Categories: cs.CL
\\
  Unlike professional caregivers, family caregivers often assume this role
without formal preparation or training. Because of this, there is an urgent
need to enhance the capacity of family caregivers to provide quality care.
Large language models can potentially be used as a foundation technology for
supporting caregivers as educational tools or as adjunct to care. This study
aimed to develop a reliable Caregiving Language Model (CaLM) by using FMs and a
caregiving knowledge base, develop an accessible CaLM using a small FM that
requires fewer computing resources, and evaluate the performance of the model
compared to a large FM. We developed CaLM using the Retrieval Augmented
Generation (RAG) framework combined with FM fine-tuning for improving the
quality of FM answers by grounding the model on a caregiving knowledge base. We
used two small FMs as candidates for the FM of CaLM (LLaMA-2 and Falcon with 7B
parameters) and larger FM GPT-3.5 as a benchmark. We developed the caregiving
knowledge base by gathering various types of documents from the Internet. In
this study, we focused on caregivers of individuals with Alzheimer's Disease
Related Dementias. We evaluated the models' performance using the benchmark
metrics commonly used in evaluating language models and their reliability to
provide accurate references with the answers. The RAG framework improved the
performance of all FMs used in this study across all measures. As expected, the
large FM performed better than small FMs across all metrics. The most
interesting result is that small fine-tuned FMs with RAG performed
significantly better than GPT 3.5 across all metrics. The fine-tuned LLaMA-2
small FM performed better than GPT 3.5 (even with RAG) in returning references
with the answers. The study shows that reliable and accessible CaLM can be
developed by using small FMs with a knowledge base specific to the caregiving
domain.
\\ ( https://arxiv.org/abs/2403.06857 ,  1050kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06872
Date: Mon, 11 Mar 2024 16:24:08 GMT   (420kb,D)

Title: Exploring Large Language Models and Hierarchical Frameworks for
  Classification of Large Unstructured Legal Documents
Authors: Nishchal Prasad, Mohand Boughanem, Taoufiq Dkaki
Categories: cs.CL cs.AI
Comments: This paper was accepted as a long paper at ECIR 2024. arXiv admin
  note: substantial text overlap with arXiv:2309.10563
\\
  Legal judgment prediction suffers from the problem of long case documents
exceeding tens of thousands of words, in general, and having a non-uniform
structure. Predicting judgments from such documents becomes a challenging task,
more so on documents with no structural annotation. We explore the
classification of these large legal documents and their lack of structural
information with a deep-learning-based hierarchical framework which we call
MESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgment
prediction. Specifically, we divide a document into parts to extract their
embeddings from the last four layers of a custom fine-tuned Large Language
Model, and try to approximate their structure through unsupervised clustering.
Which we use in another set of transformer encoder layers to learn the
inter-chunk representations. We analyze the adaptability of Large Language
Models (LLMs) with multi-billion parameters (GPT-Neo, and GPT-J) with the
hierarchical framework of MESc and compare them with their standalone
performance on legal texts. We also study their intra-domain(legal) transfer
learning capability and the impact of combining embeddings from their last
layers in MESc. We test these methods and their effectiveness with extensive
experiments and ablation studies on legal documents from India, the European
Union, and the United States with the ILDC dataset and a subset of the LexGLUE
dataset. Our approach achieves a minimum total performance gain of
approximately 2 points over previous state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.06872 ,  420kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06914
Date: Mon, 11 Mar 2024 17:03:04 GMT   (339kb,D)

Title: MEND: Meta dEmonstratioN Distillation for Efficient and Effective
  In-Context Learning
Authors: Yichuan Li, Xiyao Ma, Sixing Lu, Kyumin Lee, Xiaohu Liu, Chenlei Guo
Categories: cs.CL cs.AI
Comments: ICLR 2024
\\
  Large Language models (LLMs) have demonstrated impressive in-context learning
(ICL) capabilities, where a LLM makes predictions for a given test input
together with a few input-output pairs (demonstrations). Nevertheless, the
inclusion of demonstrations leads to a quadratic increase in the computational
overhead of the self-attention mechanism. Existing solutions attempt to distill
lengthy demonstrations into compact vectors. However, they often require
task-specific retraining or compromise LLM's in-context learning performance.
To mitigate these challenges, we present Meta dEmonstratioN Distillation
(MEND), where a language model learns to distill any lengthy demonstrations
into vectors without retraining for a new downstream task. We exploit the
knowledge distillation to enhance alignment between MEND and LLM, achieving
both efficiency and effectiveness simultaneously. MEND is endowed with the
meta-knowledge of distilling demonstrations through a two-stage training
process, which includes meta-distillation pretraining and fine-tuning.
Comprehensive evaluations across seven diverse ICL task partitions using
decoder-only (GPT-2) and encoder-decoder (T5) attest to MEND's prowess. It not
only matches but often outperforms the Vanilla ICL as well as other
state-of-the-art distillation models, while significantly reducing the
computational demands. This innovation promises enhanced scalability and
efficiency for the practical deployment of large language models
\\ ( https://arxiv.org/abs/2403.06914 ,  339kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06932
Date: Mon, 11 Mar 2024 17:18:53 GMT   (1079kb,D)

Title: ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis
Authors: Yanming Liu, Xinyue Peng, Tianyu Du, Jianwei Yin, Weihao Liu, Xuhong
  Zhang
Categories: cs.CL
Comments: 14 pages, first version of ERA-CoT
\\
  Large language models (LLMs) have achieved commendable accomplishments in
various natural language processing tasks. However, LLMs still encounter
significant challenges when dealing with complex scenarios involving multiple
entities. These challenges arise from the presence of implicit relationships
that demand multi-step reasoning. In this paper, we propose a novel approach
ERA-CoT, which aids LLMs in understanding context by capturing relationships
between entities and supports the reasoning of diverse tasks through
Chain-of-Thoughts (CoT). Experimental results show that ERA-CoT demonstrates
the superior performance of our proposed method compared to current CoT
prompting methods, achieving a significant improvement of an average of 5.1\%
on GPT3.5 compared to previous SOTA baselines. Our analysis indicates that
ERA-CoT increases the LLM's understanding of entity relationships,
significantly improves the accuracy of question answering, and enhances the
reasoning ability of LLMs.
\\ ( https://arxiv.org/abs/2403.06932 ,  1079kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06935
Date: Mon, 11 Mar 2024 17:20:12 GMT   (9270kb,D)

Title: Naming, Describing, and Quantifying Visual Objects in Humans and LLMs
Authors: Alberto Testoni, Juell Sprott, Sandro Pezzelle
Categories: cs.CL
\\
  While human speakers use a variety of different expressions when describing
the same object in an image, giving rise to a distribution of plausible labels
driven by pragmatic constraints, the extent to which current Vision \& Language
Large Language Models (VLLMs) can mimic this crucial feature of language use is
an open question. This applies to common, everyday objects, but it is
particularly interesting for uncommon or novel objects for which a category
label may be lacking or fuzzy. Furthermore, humans show clear production
preferences for highly context-sensitive expressions, such as the quantifiers
`few' or `most'. In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) on
three categories (nouns, attributes, and quantifiers) where humans show great
subjective variability concerning the distribution over plausible labels, using
datasets and resources mostly under-explored in previous work. Our results
reveal mixed evidence on the ability of VLLMs to capture human naming
preferences, with all models failing in tasks that require high-level reasoning
such as assigning quantifiers.
\\ ( https://arxiv.org/abs/2403.06935 ,  9270kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06963
Date: Mon, 11 Mar 2024 17:47:30 GMT   (1064kb,D)

Title: The pitfalls of next-token prediction
Authors: Gregor Bachmann, Vaishnavh Nagarajan
Categories: cs.CL cs.AI cs.LG
\\
  Can a mere next-token predictor faithfully model human intelligence? We
crystallize this intuitive concern, which is fragmented in the literature. As a
starting point, we argue that the two often-conflated phases of next-token
prediction -- autoregressive inference and teacher-forced training -- must be
treated distinctly. The popular criticism that errors can compound during
autoregressive inference, crucially assumes that teacher-forcing has learned an
accurate next-token predictor. This assumption sidesteps a more deep-rooted
problem we expose: in certain classes of tasks, teacher-forcing can simply fail
to learn an accurate next-token predictor in the first place. We describe a
general mechanism of how teacher-forcing can fail, and design a minimal
planning task where both the Transformer and the Mamba architecture empirically
fail in that manner -- remarkably, despite the task being straightforward to
learn. We provide preliminary evidence that this failure can be resolved when
training to predict multiple tokens in advance. We hope this finding can ground
future debates and inspire explorations beyond the next-token prediction
paradigm. We make our code available under
https://github.com/gregorbachmann/Next-Token-Failures
\\ ( https://arxiv.org/abs/2403.06963 ,  1064kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06965
Date: Mon, 11 Mar 2024 17:47:47 GMT   (8806kb,D)

Title: Hybrid Human-LLM Corpus Construction and LLM Evaluation for Rare
  Linguistic Phenomena
Authors: Leonie Weissweiler, Abdullatif K\"oksal, Hinrich Sch\"utze
Categories: cs.CL
\\
  Argument Structure Constructions (ASCs) are one of the most well-studied
construction groups, providing a unique opportunity to demonstrate the
usefulness of Construction Grammar (CxG). For example, the caused-motion
construction (CMC, ``She sneezed the foam off her cappuccino'') demonstrates
that constructions must carry meaning, otherwise the fact that ``sneeze'' in
this context causes movement cannot be explained. We form the hypothesis that
this remains challenging even for state-of-the-art Large Language Models
(LLMs), for which we devise a test based on substituting the verb with a
prototypical motion verb. To be able to perform this test at statistically
significant scale, in the absence of adequate CxG corpora, we develop a novel
pipeline of NLP-assisted collection of linguistically annotated text. We show
how dependency parsing and GPT-3.5 can be used to significantly reduce
annotation cost and thus enable the annotation of rare phenomena at scale. We
then evaluate GPT, Gemini, Llama2 and Mistral models for their understanding of
the CMC using the newly collected corpus. We find that all models struggle with
understanding the motion component that the CMC adds to a sentence.
\\ ( https://arxiv.org/abs/2403.06965 ,  8806kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06970
Date: Mon, 11 Mar 2024 17:54:33 GMT   (7554kb,D)

Title: MRL Parsing Without Tears: The Case of Hebrew
Authors: Shaltiel Shmidman, Avi Shmidman, Moshe Koppel, Reut Tsarfaty
Categories: cs.CL
\\
  Syntactic parsing remains a critical tool for relation extraction and
information extraction, especially in resource-scarce languages where LLMs are
lacking. Yet in morphologically rich languages (MRLs), where parsers need to
identify multiple lexical units in each token, existing systems suffer in
latency and setup complexity. Some use a pipeline to peel away the layers:
first segmentation, then morphology tagging, and then syntax parsing; however,
errors in earlier layers are then propagated forward. Others use a joint
architecture to evaluate all permutations at once; while this improves
accuracy, it is notoriously slow. In contrast, and taking Hebrew as a test
case, we present a new "flipped pipeline": decisions are made directly on the
whole-token units by expert classifiers, each one dedicated to one specific
task. The classifiers are independent of one another, and only at the end do we
synthesize their predictions. This blazingly fast approach sets a new SOTA in
Hebrew POS tagging and dependency parsing, while also reaching near-SOTA
performance on other Hebrew NLP tasks. Because our architecture does not rely
on any language-specific resources, it can serve as a model to develop similar
parsers for other MRLs.
\\ ( https://arxiv.org/abs/2403.06970 ,  7554kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05600
Date: Thu, 7 Mar 2024 23:20:34 GMT   (12111kb,D)

Title: Density-Regression: Efficient and Distance-Aware Deep Regressor for
  Uncertainty Estimation under Distribution Shifts
Authors: Manh Ha Bui and Anqi Liu
Categories: cs.LG stat.ML
Comments: International Conference on Artificial Intelligence and Statistics,
  2024
\\
  Morden deep ensembles technique achieves strong uncertainty estimation
performance by going through multiple forward passes with different models.
This is at the price of a high storage space and a slow speed in the inference
(test) time. To address this issue, we propose Density-Regression, a method
that leverages the density function in uncertainty estimation and achieves fast
inference by a single forward pass. We prove it is distance aware on the
feature space, which is a necessary condition for a neural network to produce
high-quality uncertainty estimation under distribution shifts. Empirically, we
conduct experiments on regression tasks with the cubic toy dataset, benchmark
UCI, weather forecast with time series, and depth estimation under real-world
shifted applications. We show that Density-Regression has competitive
uncertainty estimation performance under distribution shifts with modern deep
regressors while using a lower model size and a faster inference speed.
\\ ( https://arxiv.org/abs/2403.05600 ,  12111kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05601
Date: Fri, 8 Mar 2024 00:02:42 GMT   (95kb)

Title: Select High-Level Features: Efficient Experts from a Hierarchical
  Classification Network
Authors: Andr\'e Kelm, Niels Hannemann, Bruno Heberle, Lucas Schmidt, Tim
  Rolff, Christian Wilms, Ehsan Yaghoubi, Simone Frintrop
Categories: cs.LG
Comments: This two-page paper was accepted for a poster presentation at the 5th
  ICLR 2024 Workshop on Practical ML for Limited/Low Resource Settings
  (PML4LRS)
\\
  This study introduces a novel expert generation method that dynamically
reduces task and computational complexity without compromising predictive
performance. It is based on a new hierarchical classification network topology
that combines sequential processing of generic low-level features with
parallelism and nesting of high-level features. This structure allows for the
innovative extraction technique: the ability to select only high-level features
of task-relevant categories. In certain cases, it is possible to skip almost
all unneeded high-level features, which can significantly reduce the inference
cost and is highly beneficial in resource-constrained conditions. We believe
this method paves the way for future network designs that are lightweight and
adaptable, making them suitable for a wide range of applications, from compact
edge devices to large-scale clouds. In terms of dynamic inference our
methodology can achieve an exclusion of up to 88.7\,\% of parameters and
73.4\,\% fewer giga-multiply accumulate (GMAC) operations, analysis against
comparative baselines showing an average reduction of 47.6\,\% in parameters
and 5.8\,\% in GMACs across the cases we evaluated.
\\ ( https://arxiv.org/abs/2403.05601 ,  95kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05606
Date: Fri, 8 Mar 2024 07:15:53 GMT   (21058kb,D)

Title: A Concept-based Interpretable Model for the Diagnosis of Choroid
  Neoplasias using Multimodal Data
Authors: Yifan Wu, Yang Liu, Yue Yang, Michael S. Yao, Wenli Yang, Xuehui Shi,
  Lihong Yang, Dongjun Li, Yueming Liu, James C. Gee, Xuan Yang, Wenbin Wei,
  Shi Gu
Categories: cs.LG cs.AI cs.CL cs.CV
\\
  Diagnosing rare diseases presents a common challenge in clinical practice,
necessitating the expertise of specialists for accurate identification. The
advent of machine learning offers a promising solution, while the development
of such technologies is hindered by the scarcity of data on rare conditions and
the demand for models that are both interpretable and trustworthy in a clinical
context. Interpretable AI, with its capacity for human-readable outputs, can
facilitate validation by clinicians and contribute to medical education. In the
current work, we focus on choroid neoplasias, the most prevalent form of eye
cancer in adults, albeit rare with 5.1 per million. We built the so-far largest
dataset consisting of 750 patients, incorporating three distinct imaging
modalities collected from 2004 to 2022. Our work introduces a concept-based
interpretable model that distinguishes between three types of choroidal tumors,
integrating insights from domain experts via radiological reports. Remarkably,
this model not only achieves an F1 score of 0.91, rivaling that of black-box
models, but also boosts the diagnostic accuracy of junior doctors by 42%. This
study highlights the significant potential of interpretable machine learning in
improving the diagnosis of rare diseases, laying a groundwork for future
breakthroughs in medical AI that could tackle a wider array of complex health
scenarios.
\\ ( https://arxiv.org/abs/2403.05606 ,  21058kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05610
Date: Fri, 8 Mar 2024 13:23:42 GMT   (15kb)

Title: Evidence, Definitions and Algorithms regarding the Existence of
  Cohesive-Convergence Groups in Neural Network Optimization
Authors: Thien An L. Nguyen
Categories: cs.LG cs.CV
\\
  Understanding the convergence process of neural networks is one of the most
complex and crucial issues in the field of machine learning. Despite the close
association of notable successes in this domain with the convergence of
artificial neural networks, this concept remains predominantly theoretical. In
reality, due to the non-convex nature of the optimization problems that
artificial neural networks tackle, very few trained networks actually achieve
convergence. To expand recent research efforts on artificial-neural-network
convergence, this paper will discuss a different approach based on observations
of cohesive-convergence groups emerging during the optimization process of an
artificial neural network.
\\ ( https://arxiv.org/abs/2403.05610 ,  15kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05612
Date: Fri, 8 Mar 2024 18:28:13 GMT   (1515kb,D)

Title: Unfamiliar Finetuning Examples Control How Language Models Hallucinate
Authors: Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, Sergey Levine
Categories: cs.LG cs.AI cs.CL
\\
  Large language models (LLMs) have a tendency to generate plausible-sounding
yet factually incorrect responses, especially when queried on unfamiliar
concepts. In this work, we explore the underlying mechanisms that govern how
finetuned LLMs hallucinate. Our investigation reveals an interesting pattern:
as inputs become more unfamiliar, LLM outputs tend to default towards a
``hedged'' prediction, whose form is determined by how the unfamiliar examples
in the finetuning data are supervised. Thus, by strategically modifying these
examples' supervision, we can control LLM predictions for unfamiliar inputs
(e.g., teach them to say ``I don't know''). Based on these principles, we
develop an RL approach that more reliably mitigates hallucinations for
long-form generation tasks, by tackling the challenges presented by reward
model hallucinations. We validate our findings with a series of controlled
experiments in multiple-choice QA on MMLU, as well as long-form biography and
book/movie plot generation tasks.
\\ ( https://arxiv.org/abs/2403.05612 ,  1515kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05652
Date: Fri, 8 Mar 2024 19:52:39 GMT   (40168kb,D)

Title: What is different between these datasets?
Authors: Varun Babbar, Zhicheng Guo, Cynthia Rudin
Categories: cs.LG cs.AI
\\
  The performance of machine learning models heavily depends on the quality of
input data, yet real-world applications often encounter various data-related
challenges. One such challenge could arise when curating training data or
deploying the model in the real world - two comparable datasets in the same
domain may have different distributions. While numerous techniques exist for
detecting distribution shifts, the literature lacks comprehensive approaches
for explaining dataset differences in a human-understandable manner. To address
this gap, we propose a suite of interpretable methods (toolbox) for comparing
two datasets. We demonstrate the versatility of our approach across diverse
data modalities, including tabular data, language, images, and signals in both
low and high-dimensional settings. Our methods not only outperform comparable
and related approaches in terms of explanation quality and correctness, but
also provide actionable, complementary insights to understand and mitigate
dataset differences effectively.
\\ ( https://arxiv.org/abs/2403.05652 ,  40168kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05693
Date: Fri, 8 Mar 2024 22:04:25 GMT   (507kb,D)

Title: Shielded Deep Reinforcement Learning for Complex Spacecraft Tasking
Authors: Robert Reed, Hanspeter Schaub, Morteza Lahijanian
Categories: cs.LG
Comments: 9 pages, 2 figures, 2 tables, ACC 2024
\\
  Autonomous spacecraft control via Shielded Deep Reinforcement Learning (SDRL)
has become a rapidly growing research area. However, the construction of
shields and the definition of tasking remains informal, resulting in policies
with no guarantees on safety and ambiguous goals for the RL agent. In this
paper, we first explore the use of formal languages, namely Linear Temporal
Logic (LTL), to formalize spacecraft tasks and safety requirements. We then
define a manner in which to construct a reward function from a co-safe LTL
specification automatically for effective training in SDRL framework. We also
investigate methods for constructing a shield from a safe LTL specification for
spacecraft applications and propose three designs that provide probabilistic
guarantees. We show how these shields interact with different policies and the
flexibility of the reward structure through several experiments.
\\ ( https://arxiv.org/abs/2403.05693 ,  507kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05713
Date: Fri, 8 Mar 2024 22:59:41 GMT   (1482kb,D)

Title: $\mathtt{tsGT}$: Stochastic Time Series Modeling With Transformer
Authors: {\L}ukasz Kuci\'nski, Witold Drzewakowski, Mateusz Olko, Piotr
  Kozakowski, {\L}ukasz Maziarka, Marta Emilia Nowakowska, {\L}ukasz Kaiser,
  Piotr Mi{\l}o\'s
Categories: cs.LG
\\
  Time series methods are of fundamental importance in virtually any field of
science that deals with temporally structured data. Recently, there has been a
surge of deterministic transformer models with time series-specific
architectural biases. In this paper, we go in a different direction by
introducing $\mathtt{tsGT}$, a stochastic time series model built on a
general-purpose transformer architecture. We focus on using a well-known and
theoretically justified rolling window backtesting and evaluation protocol. We
show that $\mathtt{tsGT}$ outperforms the state-of-the-art models on MAD and
RMSE, and surpasses its stochastic peers on QL and CRPS, on four commonly used
datasets. We complement these results with a detailed analysis of
$\mathtt{tsGT}$'s ability to model the data distribution and predict marginal
quantile values.
\\ ( https://arxiv.org/abs/2403.05713 ,  1482kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05726
Date: Fri, 8 Mar 2024 23:42:06 GMT   (681kb,D)

Title: Augmentations vs Algorithms: What Works in Self-Supervised Learning
Authors: Warren Morningstar, Alex Bijamov, Chris Duvarney, Luke Friedman, Neha
  Kalibhat, Luyang Liu, Philip Mansfield, Renan Rojas-Gomez, Karan Singhal,
  Bradley Green, Sushant Prakash
Categories: cs.LG cs.CV
Comments: 18 pages, 1 figure
\\
  We study the relative effects of data augmentations, pretraining algorithms,
and model architectures in Self-Supervised Learning (SSL). While the recent
literature in this space leaves the impression that the pretraining algorithm
is of critical importance to performance, understanding its effect is
complicated by the difficulty in making objective and direct comparisons
between methods. We propose a new framework which unifies many seemingly
disparate SSL methods into a single shared template. Using this framework, we
identify aspects in which methods differ and observe that in addition to
changing the pretraining algorithm, many works also use new data augmentations
or more powerful model architectures. We compare several popular SSL methods
using our framework and find that many algorithmic additions, such as
prediction networks or new losses, have a minor impact on downstream task
performance (often less than $1\%$), while enhanced augmentation techniques
offer more significant performance improvements ($2-4\%$). Our findings
challenge the premise that SSL is being driven primarily by algorithmic
improvements, and suggest instead a bitter lesson for SSL: that augmentation
diversity and data / model scale are more critical contributors to recent
advances in self-supervised learning.
\\ ( https://arxiv.org/abs/2403.05726 ,  681kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05738
Date: Sat, 9 Mar 2024 00:20:33 GMT   (727kb,D)

Title: Provable Policy Gradient Methods for Average-Reward Markov Potential
  Games
Authors: Min Cheng, Ruida Zhou, P. R. Kumar and Chao Tian
Categories: cs.LG cs.GT
Comments: 38 pages, 7 figures, published to AISTAT-24
\\
  We study Markov potential games under the infinite horizon average reward
criterion. Most previous studies have been for discounted rewards. We prove
that both algorithms based on independent policy gradient and independent
natural policy gradient converge globally to a Nash equilibrium for the average
reward criterion. To set the stage for gradient-based methods, we first
establish that the average reward is a smooth function of policies and provide
sensitivity bounds for the differential value functions, under certain
conditions on ergodicity and the second largest eigenvalue of the underlying
Markov decision process (MDP). We prove that three algorithms, policy gradient,
proximal-Q, and natural policy gradient (NPG), converge to an $\epsilon$-Nash
equilibrium with time complexity $O(\frac{1}{\epsilon^2})$, given a
gradient/differential Q function oracle. When policy gradients have to be
estimated, we propose an algorithm with
$\tilde{O}(\frac{1}{\min_{s,a}\pi(a|s)\delta})$ sample complexity to achieve
$\delta$ approximation error w.r.t~the $\ell_2$ norm. Equipped with the
estimator, we derive the first sample complexity analysis for a policy gradient
ascent algorithm, featuring a sample complexity of $\tilde{O}(1/\epsilon^5)$.
Simulation studies are presented.
\\ ( https://arxiv.org/abs/2403.05738 ,  727kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05751
Date: Sat, 9 Mar 2024 01:15:03 GMT   (1365kb,D)

Title: MG-TSD: Multi-Granularity Time Series Diffusion Models with Guided
  Learning Process
Authors: Xinyao Fan, Yueying Wu, Chang Xu, Yuhao Huang, Weiqing Liu, Jiang Bian
Categories: cs.LG cs.AI
Comments: International Conference on Learning Representations (ICLR) 2024
\\
  Recently, diffusion probabilistic models have attracted attention in
generative time series forecasting due to their remarkable capacity to generate
high-fidelity samples. However, the effective utilization of their strong
modeling ability in the probabilistic time series forecasting task remains an
open question, partially due to the challenge of instability arising from their
stochastic nature. To address this challenge, we introduce a novel
Multi-Granularity Time Series Diffusion (MG-TSD) model, which achieves
state-of-the-art predictive performance by leveraging the inherent granularity
levels within the data as given targets at intermediate diffusion steps to
guide the learning process of diffusion models. The way to construct the
targets is motivated by the observation that the forward process of the
diffusion model, which sequentially corrupts the data distribution to a
standard normal distribution, intuitively aligns with the process of smoothing
fine-grained data into a coarse-grained representation, both of which result in
a gradual loss of fine distribution features. In the study, we derive a novel
multi-granularity guidance diffusion loss function and propose a concise
implementation method to effectively utilize coarse-grained data across various
granularity levels. More importantly, our approach does not rely on additional
external data, making it versatile and applicable across various domains.
Extensive experiments conducted on real-world datasets demonstrate that our
MG-TSD model outperforms existing time series prediction methods.
\\ ( https://arxiv.org/abs/2403.05751 ,  1365kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05752
Date: Sat, 9 Mar 2024 01:17:26 GMT   (1224kb,D)

Title: Task-Oriented GNNs Training on Large Knowledge Graphs for Accurate and
  Efficient Modeling
Authors: Hussein Abdallah, Waleed Afandi, Panos Kalnis, Essam Mansour
Categories: cs.LG cs.AI
Comments: 12 pages,9 Figures, 3 Tables, ICDE:2024
\\
  A Knowledge Graph (KG) is a heterogeneous graph encompassing a diverse range
of node and edge types. Heterogeneous Graph Neural Networks (HGNNs) are popular
for training machine learning tasks like node classification and link
prediction on KGs. However, HGNN methods exhibit excessive complexity
influenced by the KG's size, density, and the number of node and edge types. AI
practitioners handcraft a subgraph of a KG G relevant to a specific task. We
refer to this subgraph as a task-oriented subgraph (TOSG), which contains a
subset of task-related node and edge types in G. Training the task using TOSG
instead of G alleviates the excessive computation required for a large KG.
Crafting the TOSG demands a deep understanding of the KG's structure and the
task's objectives. Hence, it is challenging and time-consuming. This paper
proposes KG-TOSA, an approach to automate the TOSG extraction for task-oriented
HGNN training on a large KG. In KG-TOSA, we define a generic graph pattern that
captures the KG's local and global structure relevant to a specific task. We
explore different techniques to extract subgraphs matching our graph pattern:
namely (i) two techniques sampling around targeted nodes using biased random
walk or influence scores, and (ii) a SPARQL-based extraction method leveraging
RDF engines' built-in indices. Hence, it achieves negligible preprocessing
overhead compared to the sampling techniques. We develop a benchmark of real
KGs of large sizes and various tasks for node classification and link
prediction. Our experiments show that KG-TOSA helps state-of-the-art HGNN
methods reduce training time and memory usage by up to 70% while improving the
model performance, e.g., accuracy and inference time.
\\ ( https://arxiv.org/abs/2403.05752 ,  1224kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05754
Date: Sat, 9 Mar 2024 01:34:26 GMT   (2071kb,D)

Title: Hybrid Quantum-inspired Resnet and Densenet for Pattern Recognition with
  Completeness Analysis
Authors: Andi Chen, Hua-Lei Yin, Zeng-Bing Chen, Shengjun Wu
Categories: cs.LG cs.ET
Comments: 12 pages for main paper with 13-page supplementary materials with a
  hyperlink in the last page of the main paper
\\
  With the contemporary digital technology approaching, deep neural networks
are emerging as the foundational algorithm of the artificial intelligence boom.
Whereas, the evolving social demands have been emphasizing the necessity of
novel methodologies to substitute traditional neural networks. Concurrently,
the advent of the post-Moore era has spurred the development of
quantum-inspired neural networks with outstanding potentials at certain
circumstances. Nonetheless, a definitive evaluating system with detailed
metrics is tremendously vital and indispensable owing to the vague indicators
in comparison between the novel and traditional deep learning models at
present. Hence, to improve and evaluate the performances of the novel neural
networks more comprehensively in complex and unpredictable environments, we
propose two hybrid quantum-inspired neural networks which are rooted in
residual and dense connections respectively for pattern recognitions with
completeness representation theory for model assessment. Comparative analyses
against pure classical models with detailed frameworks reveal that our hybrid
models with lower parameter complexity not only match the generalization power
of pure classical models, but also outperform them notably in resistance to
parameter attacks with various asymmetric noises. Moreover, our hybrid models
indicate unique superiority to prevent gradient explosion problems through
theoretical argumentation. Eventually, We elaborate on the application
scenarios where our hybrid models are applicable and efficient, which paves the
way for their industrialization and commercialization.
\\ ( https://arxiv.org/abs/2403.05754 ,  2071kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05759
Date: Sat, 9 Mar 2024 02:10:08 GMT   (463kb,D)

Title: Membership Testing in Markov Equivalence Classes via Independence Query
  Oracles
Authors: Jiaqi Zhang, Kirankumar Shiragur, Caroline Uhler
Categories: cs.LG cs.AI stat.ME stat.ML
\\
  Understanding causal relationships between variables is a fundamental problem
with broad impact in numerous scientific fields. While extensive research has
been dedicated to learning causal graphs from data, its complementary concept
of testing causal relationships has remained largely unexplored. While learning
involves the task of recovering the Markov equivalence class (MEC) of the
underlying causal graph from observational data, the testing counterpart
addresses the following critical question: Given a specific MEC and
observational data from some causal graph, can we determine if the
data-generating causal graph belongs to the given MEC?
  We explore constraint-based testing methods by establishing bounds on the
required number of conditional independence tests. Our bounds are in terms of
the size of the maximum undirected clique ($s$) of the given MEC. In the worst
case, we show a lower bound of $\exp(\Omega(s))$ independence tests. We then
give an algorithm that resolves the task with $\exp(O(s))$ tests, matching our
lower bound. Compared to the learning problem, where algorithms often use a
number of independence tests that is exponential in the maximum in-degree, this
shows that testing is relatively easier. In particular, it requires
exponentially less independence tests in graphs featuring high in-degrees and
small clique sizes. Additionally, using the DAG associahedron, we provide a
geometric interpretation of testing versus learning and discuss how our testing
result can aid learning.
\\ ( https://arxiv.org/abs/2403.05759 ,  463kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05767
Date: Sat, 9 Mar 2024 02:30:04 GMT   (647kb,D)

Title: Extending Activation Steering to Broad Skills and Multiple Behaviours
Authors: Teun van der Weij, Massimo Poesio, Nandi Schoots
Categories: cs.LG cs.AI cs.CL cs.CY
Comments: Code is available at:
  https://github.com/TeunvdWeij/extending-activation-addition
\\
  Current large language models have dangerous capabilities, which are likely
to become more problematic in the future. Activation steering techniques can be
used to reduce risks from these capabilities. In this paper, we investigate the
efficacy of activation steering for broad skills and multiple behaviours.
First, by comparing the effects of reducing performance on general coding
ability and Python-specific ability, we find that steering broader skills is
competitive to steering narrower skills. Second, we steer models to become more
or less myopic and wealth-seeking, among other behaviours. In our experiments,
combining steering vectors for multiple different behaviours into one steering
vector is largely unsuccessful. On the other hand, injecting individual
steering vectors at different places in a model simultaneously is promising.
\\ ( https://arxiv.org/abs/2403.05767 ,  647kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05778
Date: Sat, 9 Mar 2024 03:21:18 GMT   (27031kb)

Title: Spatial Clustering Approach for Vessel Path Identification
Authors: Mohamed Abuella, M. Amine Atoui, Slawomir Nowaczyk, Simon Johansson,
  Ethan Faghan
Categories: cs.LG cs.CE
Comments: Keywords: Spatial clustering, vessel path identification, maritime
  transportation, average nearest neighbor distance, hierarchical clustering,
  likelihood estimation. This preprint has 12 pages, 14 figures, 2 tables
MSC-class: 68T10 (Primary), 68T20 (Secondary)
ACM-class: G.1.3; H.3.4
\\
  This paper addresses the challenge of identifying the paths for vessels with
operating routes of repetitive paths, partially repetitive paths, and new
paths. We propose a spatial clustering approach for labeling the vessel paths
by using only position information. We develop a path clustering framework
employing two methods: a distance-based path modeling and a likelihood
estimation method. The former enhances the accuracy of path clustering through
the integration of unsupervised machine learning techniques, while the latter
focuses on likelihood-based path modeling and introduces segmentation for a
more detailed analysis. The result findings highlight the superior performance
and efficiency of the developed approach, as both methods for clustering vessel
paths into five classes achieve a perfect F1-score. The approach aims to offer
valuable insights for route planning, ultimately contributing to improving
safety and efficiency in maritime transportation.
\\ ( https://arxiv.org/abs/2403.05778 ,  27031kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05786
Date: Sat, 9 Mar 2024 04:01:39 GMT   (980kb,D)

Title: Optimistic Safety for Linearly-Constrained Online Convex Optimization
Authors: Spencer Hutchinson, Tianyi Chen, Mahnoosh Alizadeh
Categories: cs.LG math.OC
Comments: 30 pages, 4 figures
\\
  The setting of online convex optimization (OCO) under unknown constraints has
garnered significant attention in recent years. In this work, we consider a
version of this problem with static linear constraints that the player receives
noisy feedback of and must always satisfy. By leveraging our novel design
paradigm of optimistic safety, we give an algorithm for this problem that
enjoys $\tilde{\mathcal{O}}(\sqrt{T})$ regret. This improves on the previous
best regret bound of $\tilde{\mathcal{O}}(T^{2/3})$ while using only slightly
stronger assumptions of independent noise and an oblivious adversary. Then, by
recasting this problem as OCO under time-varying stochastic linear constraints,
we show that our algorithm enjoys the same regret guarantees in such a setting
and never violates the constraints in expectation. This contributes to the
literature on OCO under time-varying stochastic constraints, where the
state-of-the-art algorithms enjoy $\tilde{\mathcal{O}}(\sqrt{T})$ regret and
$\tilde{\mathcal{O}}(\sqrt{T})$ violation when the constraints are convex and
the player receives full feedback. Additionally, we provide a version of our
algorithm that is more computationally efficient and give numerical experiments
comparing it with benchmark algorithms.
\\ ( https://arxiv.org/abs/2403.05786 ,  980kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05798
Date: Sat, 9 Mar 2024 05:20:48 GMT   (3924kb,D)

Title: $\textbf{S}^2$IP-LLM: Semantic Space Informed Prompt Learning with LLM
  for Time Series Forecasting
Authors: Zijie Pan, Yushan Jiang, Sahil Garg, Anderson Schneider, Yuriy
  Nevmyvaka and Dongjin Song
Categories: cs.LG
\\
  Recently, there has been a growing interest in leveraging pre-trained large
language models (LLMs) for various time series applications. However, the
semantic space of LLMs, established through the pre-training, is still
underexplored and may help yield more distinctive and informative
representations to facilitate time series forecasting. To this end, we propose
Semantic Space Informed Prompt learning with LLM ($S^2$IP-LLM) to align the
pre-trained semantic space with time series embeddings space and perform time
series forecasting based on learned prompts from the joint space. We first
design a tokenization module tailored for cross-modality alignment, which
explicitly concatenates patches of decomposed time series components to create
embeddings that effectively encode the temporal dynamics. Next, we leverage the
pre-trained word token embeddings to derive semantic anchors and align selected
anchors with time series embeddings by maximizing the cosine similarity in the
joint space. This way, $S^2$IP-LLM can retrieve relevant semantic anchors as
prompts to provide strong indicators (context) for time series that exhibit
different temporal dynamics. With thorough empirical studies on multiple
benchmark datasets, we demonstrate that the proposed $S^2$IP-LLM can achieve
superior forecasting performance over state-of-the-art baselines. Furthermore,
our ablation studies and visualizations verify the necessity of prompt learning
informed by semantic space.
\\ ( https://arxiv.org/abs/2403.05798 ,  3924kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05818
Date: Sat, 9 Mar 2024 06:58:21 GMT   (1300kb)

Title: PR-NET: Leveraging Pathway Refined Network Structures for Prostate
  Cancer Patient Condition Prediction
Authors: R. Li, J. Liu, X.L. Deng, X. Liu, J.C. Guo, W.Y. Wu, L. Yang
Categories: cs.LG q-bio.QM
\\
  Motivation: The diagnosis and monitoring of Castrate Resistant Prostate
Cancer (CRPC) are crucial for cancer patients, but the current models (such as
P-NET) have limitations in terms of parameter count, generalization, and cost.
Results: To address the above issues, we develop a more accurate and efficient
Prostate Cancer patient condition prediction model, named PR-NET. By
compressing and optimizing the network structure of P-NET, the model complexity
is reduced while maintaining high accuracy and interpretability. The PR-NET
demonstrated superior performance in predicting prostate cancer patient
outcomes, outshining P-NET and six other traditional models with a significant
margin. In our rigorous evaluation, PR-NET not only achieved impressive average
AUC and Recall scores of 0.94 and 0.83, respectively, on known data but also
maintained robust generalizability on five unknown datasets with a higher
average AUC of 0.73 and Recall of 0.72, compared to P-NET's 0.68 and 0.5.
PR-NET's efficiency was evidenced by its shorter average training and inference
times, and its gene-level analysis revealed 46 key genes, demonstrating its
enhanced predictive power and efficiency in identifying critical biomarkers for
prostate cancer. Future research can further expand its application domains and
optimize the model's performance and reliability.
\\ ( https://arxiv.org/abs/2403.05818 ,  1300kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05821
Date: Sat, 9 Mar 2024 07:01:44 GMT   (12692kb,D)

Title: Optimizing LLM Queries in Relational Workloads
Authors: Shu Liu, Asim Biswal, Audrey Cheng, Xiangxi Mo, Shiyi Cao, Joseph E.
  Gonzalez, Ion Stoica, Matei Zaharia
Categories: cs.LG cs.DB
\\
  Analytical database providers (e.g., Redshift, Databricks, BigQuery) have
rapidly added support for invoking Large Language Models (LLMs) through native
user-defined functions (UDFs) to help users perform natural language tasks,
such as classification, entity extraction, and translation, inside analytical
workloads. For instance, an analyst might want to extract customer sentiments
on millions of product reviews. However, LLM inference is highly expensive in
both computational and economic terms: for example, an NVIDIA L4 GPU running
Llama2-7B can only process 6 KB of text per second. In this paper, we explore
how to optimize LLM inference for analytical workloads that invoke LLMs within
relational queries. We show that relational queries present novel opportunities
for accelerating LLM inference, including reordering rows to maximize key-value
(KV) cache reuse within the LLM inference engine, reordering columns within a
row to further increase cache reuse, and deduplicating redundant inference
requests. We implement these optimizations in Apache Spark, with vLLM as the
model serving backend and achieve up to 4.4x improvement in end-to-end latency
on a benchmark of diverse LLM-based queries on real datasets. To the best of
our knowledge, this is the first work to explicitly address the problem of
optimizing LLM invocations within SQL queries.
\\ ( https://arxiv.org/abs/2403.05821 ,  12692kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05822
Date: Sat, 9 Mar 2024 07:19:37 GMT   (736kb,D)

Title: TrafficGPT: Breaking the Token Barrier for Efficient Long Traffic
  Analysis and Generation
Authors: Jian Qu, Xiaobo Ma, Jianfeng Li
Categories: cs.LG
\\
  Over the years, network traffic analysis and generation have advanced
significantly. From traditional statistical methods, the field has progressed
to sophisticated deep learning techniques. This progress has improved the
ability to detect complex patterns and security threats, as well as to test and
optimize network performance. However, obstacles persist, such as the
dependence on labeled data for analysis and the difficulty of generating
traffic samples that follow realistic patterns. Pre-trained deep neural
networks have emerged as powerful tools to resolve these issues, offering
improved performance by learning robust data representations from large
unlabeled datasets. Despite their benefits, existing pre-trained models face
challenges like token length limitation, which restricts their usefulness in
comprehensive traffic analysis and realistic traffic generation. To address
these challenges, we introduce TrafficGPT, a deep learning model that can
tackle complex challenges related to long flow classification and generation
tasks. This model uses generative pre-training with the linear attention
mechanism, which allows for a substantially increased capacity of up to 12,032
tokens from the previous limit of only 512 tokens. TrafficGPT demonstrates
superior performance in classification tasks, reaching state-of-the-art levels.
In generation tasks, it closely resembles real traffic flows, with low JS
divergence and an F1 score close to 0.5 (representing a random guess) in
discriminating generated data. These advancements hold promise for future
applications in both traffic flow classification and generation tasks.
\\ ( https://arxiv.org/abs/2403.05822 ,  736kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05848
Date: Sat, 9 Mar 2024 09:17:23 GMT   (6098kb,D)

Title: tLaSDI: Thermodynamics-informed latent space dynamics identification
Authors: Jun Sur Richard Park, Siu Wun Cheung, Youngsoo Choi, and Yeonjong Shin
Categories: cs.LG math.DS
Comments: 33 pages, 8 figures
\\
  We propose a data-driven latent space dynamics identification method (tLaSDI)
that embeds the first and second principles of thermodynamics. The latent
variables are learned through an autoencoder as a nonlinear dimension reduction
model. The dynamics of the latent variables are constructed by a neural
network-based model that preserves certain structures to respect the
thermodynamic laws through the GENERIC formalism. An abstract error estimate of
the approximation is established, which provides a new loss formulation
involving the Jacobian computation of autoencoder. Both the autoencoder and the
latent dynamics are trained to minimize the new loss. Numerical examples are
presented to demonstrate the performance of tLaSDI, which exhibits robust
generalization ability, even in extrapolation. In addition, an intriguing
correlation is empirically observed between the entropy production rates in the
latent space and the behaviors of the full-state solution.
\\ ( https://arxiv.org/abs/2403.05848 ,  6098kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05864
Date: Sat, 9 Mar 2024 10:24:12 GMT   (14804kb,D)

Title: PAPER-HILT: Personalized and Adaptive Privacy-Aware Early-Exit for
  Reinforcement Learning in Human-in-the-Loop Systems
Authors: Mojtaba Taherisadr, Salma Elmalaki
Categories: cs.LG cs.CR cs.HC
Comments: 18 pages
MSC-class: F.2.2
ACM-class: I.2.7
\\
  Reinforcement Learning (RL) has increasingly become a preferred method over
traditional rule-based systems in diverse human-in-the-loop (HITL) applications
due to its adaptability to the dynamic nature of human interactions. However,
integrating RL in such settings raises significant privacy concerns, as it
might inadvertently expose sensitive user information. Addressing this, our
paper focuses on developing PAPER-HILT, an innovative, adaptive RL strategy
through exploiting an early-exit approach designed explicitly for privacy
preservation in HITL environments. This approach dynamically adjusts the
tradeoff between privacy protection and system utility, tailoring its operation
to individual behavioral patterns and preferences. We mainly highlight the
challenge of dealing with the variable and evolving nature of human behavior,
which renders static privacy models ineffective. PAPER-HILT's effectiveness is
evaluated through its application in two distinct contexts: Smart Home
environments and Virtual Reality (VR) Smart Classrooms. The empirical results
demonstrate PAPER-HILT's capability to provide a personalized equilibrium
between user privacy and application utility, adapting effectively to
individual user needs and preferences. On average for both experiments, utility
(performance) drops by 24%, and privacy (state prediction) improves by 31%.
\\ ( https://arxiv.org/abs/2403.05864 ,  14804kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05882
Date: Sat, 9 Mar 2024 11:24:34 GMT   (1146kb,D)

Title: DiffRed: Dimensionality Reduction guided by stable rank
Authors: Prarabdh Shukla, Gagan Raj Gupta, Kunal Dutta
Categories: cs.LG
\\
  In this work, we propose a novel dimensionality reduction technique, DiffRed,
which first projects the data matrix, A, along first $k_1$ principal components
and the residual matrix $A^{*}$ (left after subtracting its $k_1$-rank
approximation) along $k_2$ Gaussian random vectors. We evaluate M1, the
distortion of mean-squared pair-wise distance, and Stress, the normalized value
of RMS of distortion of the pairwise distances. We rigorously prove that
DiffRed achieves a general upper bound of
$O\left(\sqrt{\frac{1-p}{k_2}}\right)$ on Stress and
$O\left(\frac{(1-p)}{\sqrt{k_2*\rho(A^{*})}}\right)$ on M1 where $p$ is the
fraction of variance explained by the first $k_1$ principal components and
$\rho(A^{*})$ is the stable rank of $A^{*}$. These bounds are tighter than the
currently known results for Random maps. Our extensive experiments on a variety
of real-world datasets demonstrate that DiffRed achieves near zero M1 and much
lower values of Stress as compared to the well-known dimensionality reduction
techniques. In particular, DiffRed can map a 6 million dimensional dataset to
10 dimensions with 54% lower Stress than PCA.
\\ ( https://arxiv.org/abs/2403.05882 ,  1146kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05890
Date: Sat, 9 Mar 2024 12:04:56 GMT   (1838kb,D)

Title: Towards Efficient Replay in Federated Incremental Learning
Authors: Yichen Li, Qunwei Li, Haozhao Wang, Ruixuan Li, Wenliang Zhong,
  Guannan Zhang
Categories: cs.LG cs.DC
\\
  In Federated Learning (FL), the data in each client is typically assumed
fixed or static. However, data often comes in an incremental manner in
real-world applications, where the data domain may increase dynamically. In
this work, we study catastrophic forgetting with data heterogeneity in
Federated Incremental Learning (FIL) scenarios where edge clients may lack
enough storage space to retain full data. We propose to employ a simple,
generic framework for FIL named Re-Fed, which can coordinate each client to
cache important samples for replay. More specifically, when a new task arrives,
each client first caches selected previous samples based on their global and
local importance. Then, the client trains the local model with both the cached
samples and the samples from the new task. Theoretically, we analyze the
ability of Re-Fed to discover important samples for replay thus alleviating the
catastrophic forgetting problem. Moreover, we empirically show that Re-Fed
achieves competitive performance compared to state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.05890 ,  1838kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05918
Date: Sat, 9 Mar 2024 14:01:04 GMT   (2408kb)

Title: SEMRes-DDPM: Residual Network Based Diffusion Modelling Applied to
  Imbalanced Data
Authors: Ming Zheng, Yang Yang, Zhi-Hang Zhao, Shan-Chao Gan, Yang Chen, Si-Kai
  Ni and Yang Lu
Categories: cs.LG cs.AI
\\
  In the field of data mining and machine learning, commonly used
classification models cannot effectively learn in unbalanced data. In order to
balance the data distribution before model training,oversamplingmethods are
often used to generate data for a small number of classes to solve the problem
of classifying unbalanced data. Most of the classical oversampling methods are
based on theSMOTE technique, which only focuses on the local information of the
data, and therefore the generated data may have the problem of not being
realistic enough. In the current oversampling methods based on generative
networks, the methods based on GANs can capture the true distribution of data,
but there is the problem of pattern collapse and training instability in
training; in the oversampling methods based on denoising diffusion probability
models, the neural network of the inverse diffusion process using the U-Net is
not applicable to tabular data, and although the MLP can be used to replace the
U-Net, the problem exists due to the simplicity of the structure and the poor
effect of removing noise. problem of poor noise removal. In order to overcome
the above problems, we propose a novel oversampling method SEMRes-DDPM.In the
SEMRes?DDPM backward diffusion process, a new neural network structure
SEMST-ResNet is used, which is suitable for tabular data and has good noise
removal effect, and it can generate tabular data with higher quality.
Experiments show that the SEMResNet network removes noise better than MLP;
SEMRes?DDPM generates data distributions that are closer to the real data
distributions than TabDDPM with CWGAN-GP; on 20 real unbalanced tabular
datasets with 9 classification models, SEMRes-DDPM improves the quality of the
generated tabular data in terms of three evaluation metrics (F1, G-mean, AUC)
with better classification performance than other SOTA oversampling methods.
\\ ( https://arxiv.org/abs/2403.05918 ,  2408kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05979
Date: Sat, 9 Mar 2024 18:34:59 GMT   (346kb)

Title: Enhancing Classification Performance via Reinforcement Learning for
  Feature Selection
Authors: Younes Ghazagh Jahed, Seyyed Ali Sadat Tavana
Categories: cs.LG
Comments: 5 pages, 4 figures
MSC-class: 68T99
ACM-class: E.1
\\
  Feature selection plays a crucial role in improving predictive accuracy by
identifying relevant features while filtering out irrelevant ones. This study
investigates the importance of effective feature selection in enhancing the
performance of classification models. By employing reinforcement learning (RL)
algorithms, specifically Q-learning (QL) and SARSA learning, this paper
addresses the feature selection challenge. Using the Breast Cancer Coimbra
dataset (BCCDS) and three normalization methods (Min-Max, l1, and l2), the
study evaluates the performance of these algorithms. Results show that
QL@Min-Max and SARSA@l2 achieve the highest classification accuracies, reaching
87% and 88%, respectively. This highlights the effectiveness of RL-based
feature selection methods in optimizing classification tasks, contributing to
improved model accuracy and efficiency.
\\ ( https://arxiv.org/abs/2403.05979 ,  346kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05996
Date: Sat, 9 Mar 2024 19:56:40 GMT   (6450kb,D)

Title: Dissecting Deep RL with High Update Ratios: Combatting Value
  Overestimation and Divergence
Authors: Marcel Hussing, Claas Voelcker, Igor Gilitschenski, Amir-massoud
  Farahmand, Eric Eaton
Categories: cs.LG cs.AI
\\
  We show that deep reinforcement learning can maintain its ability to learn
without resetting network parameters in settings where the number of gradient
updates greatly exceeds the number of environment samples. Under such large
update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the
emergence of a primacy bias, in which agents overfit early interactions and
downplay later experience, impairing their ability to learn. In this work, we
dissect the phenomena underlying the primacy bias. We inspect the early stages
of training that ought to cause the failure to learn and find that a
fundamental challenge is a long-standing acquaintance: value overestimation.
Overinflated Q-values are found not only on out-of-distribution but also
in-distribution data and can be traced to unseen action prediction propelled by
optimizer momentum. We employ a simple unit-ball normalization that enables
learning under large update ratios, show its efficacy on the widely used
dm_control suite, and obtain strong performance on the challenging dog tasks,
competitive with model-based approaches. Our results question, in parts, the
prior explanation for sub-optimal learning due to overfitting on early data.
\\ ( https://arxiv.org/abs/2403.05996 ,  6450kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06009
Date: Sat, 9 Mar 2024 21:07:16 GMT   (1135kb,D)

Title: Detectors for Safe and Reliable LLMs: Implementations, Uses, and
  Limitations
Authors: Swapnaja Achintalwar, Adriana Alvarado Garcia, Ateret Anaby-Tavor,
  Ioana Baldini, Sara E. Berger, Bishwaranjan Bhattacharjee, Djallel
  Bouneffouf, Subhajit Chaudhury, Pin-Yu Chen, Lamogha Chiazor, Elizabeth M.
  Daly, Rog\'erio Abreu de Paula, Pierre Dognin, Eitan Farchi, Soumya Ghosh,
  Michael Hind, Raya Horesh, George Kour, Ja Young Lee, Erik Miehling,
  Keerthiram Murugesan, Manish Nagireddy, Inkit Padhi, David Piorkowski,
  Ambrish Rawat, Orna Raz, Prasanna Sattigeri, Hendrik Strobelt, Sarathkrishna
  Swaminathan, Christoph Tillmann, Aashka Trivedi, Kush R. Varshney, Dennis
  Wei, Shalisha Witherspooon, Marcel Zalmanovici
Categories: cs.LG
\\
  Large language models (LLMs) are susceptible to a variety of risks, from
non-faithful output to biased and toxic generations. Due to several limiting
factors surrounding LLMs (training cost, API access, data availability, etc.),
it may not always be feasible to impose direct safety constraints on a deployed
model. Therefore, an efficient and reliable alternative is required. To this
end, we present our ongoing efforts to create and deploy a library of
detectors: compact and easy-to-build classification models that provide labels
for various harms. In addition to the detectors themselves, we discuss a wide
range of uses for these detector models - from acting as guardrails to enabling
effective AI governance. We also deep dive into inherent challenges in their
development and discuss future work aimed at making the detectors more reliable
and broadening their scope.
\\ ( https://arxiv.org/abs/2403.06009 ,  1135kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06011
Date: Sat, 9 Mar 2024 21:10:10 GMT   (413kb,D)

Title: Reinforcement Learning Paycheck Optimization for Multivariate Financial
  Goals
Authors: Melda Alaluf, Giulia Crippa, Sinong Geng, Zijian Jing, Nikhil
  Krishnan, Sanjeev Kulkarni, Wyatt Navarro, Ronnie Sircar, Jonathan Tang
Categories: cs.LG math.OC
Journal-ref: Risk and Decision Analysis, Volume 9, 2023
\\
  We study paycheck optimization, which examines how to allocate income in
order to achieve several competing financial goals. For paycheck optimization,
a quantitative methodology is missing, due to a lack of a suitable problem
formulation. To deal with this issue, we formulate the problem as a utility
maximization problem. The proposed formulation is able to (i) unify different
financial goals; (ii) incorporate user preferences regarding the goals; (iii)
handle stochastic interest rates. The proposed formulation also facilitates an
end-to-end reinforcement learning solution, which is implemented on a variety
of problem settings.
\\ ( https://arxiv.org/abs/2403.06011 ,  413kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06013
Date: Sat, 9 Mar 2024 21:26:10 GMT   (4264kb,D)

Title: Are Classification Robustness and Explanation Robustness Really Strongly
  Correlated? An Analysis Through Input Loss Landscape
Authors: Tiejin Chen, Wenwang Huang, Linsey Pang, Dongsheng Luo, Hua Wei
Categories: cs.LG cs.CV
\\
  This paper delves into the critical area of deep learning robustness,
challenging the conventional belief that classification robustness and
explanation robustness in image classification systems are inherently
correlated. Through a novel evaluation approach leveraging clustering for
efficient assessment of explanation robustness, we demonstrate that enhancing
explanation robustness does not necessarily flatten the input loss landscape
with respect to explanation loss - contrary to flattened loss landscapes
indicating better classification robustness. To deeply investigate this
contradiction, a groundbreaking training method designed to adjust the loss
landscape with respect to explanation loss is proposed. Through the new
training method, we uncover that although such adjustments can impact the
robustness of explanations, they do not have an influence on the robustness of
classification. These findings not only challenge the prevailing assumption of
a strong correlation between the two forms of robustness but also pave new
pathways for understanding relationship between loss landscape and explanation
loss.
\\ ( https://arxiv.org/abs/2403.06013 ,  4264kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06014
Date: Sat, 9 Mar 2024 21:26:22 GMT   (1024kb,D)

Title: Hard-label based Small Query Black-box Adversarial Attack
Authors: Jeonghwan Park, Paul Miller, Niall McLaughlin
Categories: cs.LG cs.AI cs.CV
Comments: 11 pages, 3 figures
Journal-ref: IEEE/CVF Winter Conference on Applications of Computer Vision,
  2024
\\
  We consider the hard label based black box adversarial attack setting which
solely observes predicted classes from the target model. Most of the attack
methods in this setting suffer from impractical number of queries required to
achieve a successful attack. One approach to tackle this drawback is utilising
the adversarial transferability between white box surrogate models and black
box target model. However, the majority of the methods adopting this approach
are soft label based to take the full advantage of zeroth order optimisation.
Unlike mainstream methods, we propose a new practical setting of hard label
based attack with an optimisation process guided by a pretrained surrogate
model. Experiments show the proposed method significantly improves the query
efficiency of the hard label based black-box attack across various target model
architectures. We find the proposed method achieves approximately 5 times
higher attack success rate compared to the benchmarks, especially at the small
query budgets as 100 and 250.
\\ ( https://arxiv.org/abs/2403.06014 ,  1024kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06017
Date: Sat, 9 Mar 2024 21:33:26 GMT   (74kb)

Title: Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New
  Benchmark
Authors: Xiaowei Qian, Zhimeng Guo, Jialiang Li, Haitao Mao, Bingheng Li,
  Suhang Wang, Yao Ma
Categories: cs.LG cs.CY
\\
  Fair graph learning plays a pivotal role in numerous practical applications.
Recently, many fair graph learning methods have been proposed; however, their
evaluation often relies on poorly constructed semi-synthetic datasets or
substandard real-world datasets. In such cases, even a basic Multilayer
Perceptron (MLP) can outperform Graph Neural Networks (GNNs) in both utility
and fairness. In this work, we illustrate that many datasets fail to provide
meaningful information in the edges, which may challenge the necessity of using
graph structures in these problems. To address these issues, we develop and
introduce a collection of synthetic, semi-synthetic, and real-world datasets
that fulfill a broad spectrum of requirements. These datasets are thoughtfully
designed to include relevant graph structures and bias information crucial for
the fair evaluation of models. The proposed synthetic and semi-synthetic
datasets offer the flexibility to create data with controllable bias
parameters, thereby enabling the generation of desired datasets with
user-defined bias values with ease. Moreover, we conduct systematic evaluations
of these proposed datasets and establish a unified evaluation approach for fair
graph learning models. Our extensive experimental results with fair graph
learning methods across our datasets demonstrate their effectiveness in
benchmarking the performance of these methods. Our datasets and the code for
reproducing our experiments are available at
https://github.com/XweiQ/Benchmark-GraphFairness.
\\ ( https://arxiv.org/abs/2403.06017 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06020
Date: Sat, 9 Mar 2024 21:45:31 GMT   (1895kb,D)

Title: Multi-conditioned Graph Diffusion for Neural Architecture Search
Authors: Rohan Asthana, Joschua Conrad, Youssef Dawoud, Maurits Ortmanns,
  Vasileios Belagiannis
Categories: cs.LG cs.CV
Comments: Transactions on Machine Learning Research (TMLR)
\\
  Neural architecture search automates the design of neural network
architectures usually by exploring a large and thus complex architecture search
space. To advance the architecture search, we present a graph diffusion-based
NAS approach that uses discrete conditional graph diffusion processes to
generate high-performing neural network architectures. We then propose a
multi-conditioned classifier-free guidance approach applied to graph diffusion
networks to jointly impose constraints such as high accuracy and low hardware
latency. Unlike the related work, our method is completely differentiable and
requires only a single model training. In our evaluations, we show promising
results on six standard benchmarks, yielding novel and unique architectures at
a fast speed, i.e. less than 0.2 seconds per architecture. Furthermore, we
demonstrate the generalisability and efficiency of our method through
experiments on ImageNet dataset.
\\ ( https://arxiv.org/abs/2403.06020 ,  1895kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06026
Date: Sat, 9 Mar 2024 22:28:46 GMT   (192kb,D)

Title: Towards a Generic Representation of Cominatorial Problems for
  Learning-Based Approaches
Authors: L\'eo Boisvert, H\'el\`ene Verhaeghe, Quentin Cappart
Categories: cs.LG cs.AI
\\
  In recent years, there has been a growing interest in using learning-based
approaches for solving combinatorial problems, either in an end-to-end manner
or in conjunction with traditional optimization algorithms. In both scenarios,
the challenge lies in encoding the targeted combinatorial problems into a
structure compatible with the learning algorithm. Many existing works have
proposed problem-specific representations, often in the form of a graph, to
leverage the advantages of \textit{graph neural networks}. However, these
approaches lack generality, as the representation cannot be easily transferred
from one combinatorial problem to another one. While some attempts have been
made to bridge this gap, they still offer a partial generality only. In
response to this challenge, this paper advocates for progress toward a fully
generic representation of combinatorial problems for learning-based approaches.
The approach we propose involves constructing a graph by breaking down any
constraint of a combinatorial problem into an abstract syntax tree and
expressing relationships (e.g., a variable involved in a constraint) through
the edges. Furthermore, we introduce a graph neural network architecture
capable of efficiently learning from this representation. The tool provided
operates on combinatorial problems expressed in the XCSP3 format, handling all
the constraints available in the 2023 mini-track competition. Experimental
results on four combinatorial problems demonstrate that our architecture
achieves performance comparable to dedicated architectures while maintaining
generality. Our code and trained models are publicly available at
\url{https://github.com/corail-research/learning-generic-csp}.
\\ ( https://arxiv.org/abs/2403.06026 ,  192kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06027
Date: Sat, 9 Mar 2024 22:29:24 GMT   (733kb,D)

Title: Multimodal deep learning approach to predicting neurological recovery
  from coma after cardiac arrest
Authors: Felix H. Krones, Ben Walker, Guy Parsons, Terry Lyons, Adam Mahdi
Categories: cs.LG eess.SP
Comments: 5 figures, 2 tables
\\
  This work showcases our team's (The BEEGees) contributions to the 2023 George
B. Moody PhysioNet Challenge. The aim was to predict neurological recovery from
coma following cardiac arrest using clinical data and time-series such as
multi-channel EEG and ECG signals. Our modelling approach is multimodal, based
on two-dimensional spectrogram representations derived from numerous EEG
channels, alongside the integration of clinical data and features extracted
directly from EEG recordings. Our submitted model achieved a Challenge score of
$0.53$ on the hidden test set for predictions made $72$ hours after return of
spontaneous circulation. Our study shows the efficacy and limitations of
employing transfer learning in medical classification. With regard to
prospective implementation, our analysis reveals that the performance of the
model is strongly linked to the selection of a decision threshold and exhibits
strong variability across data splits.
\\ ( https://arxiv.org/abs/2403.06027 ,  733kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06031
Date: Sat, 9 Mar 2024 22:41:33 GMT   (863kb,D)

Title: FairTargetSim: An Interactive Simulator for Understanding and Explaining
  the Fairness Effects of Target Variable Definition
Authors: Dalia Gala, Milo Phillips-Brown, Naman Goel, Carinal Prunkl, Laura
  Alvarez Jubete, medb corcoran, Ray Eitel-Porter
Categories: cs.LG cs.AI cs.CY
\\
  Machine learning requires defining one's target variable for predictions or
decisions, a process that can have profound implications on fairness: biases
are often encoded in target variable definition itself, before any data
collection or training. We present an interactive simulator, FairTargetSim
(FTS), that illustrates how target variable definition impacts fairness. FTS is
a valuable tool for algorithm developers, researchers, and non-technical
stakeholders. FTS uses a case study of algorithmic hiring, using real-world
data and user-defined target variables. FTS is open-source and available at:
http://tinyurl.com/ftsinterface. The video accompanying this paper is here:
http://tinyurl.com/ijcaifts.
\\ ( https://arxiv.org/abs/2403.06031 ,  863kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06033
Date: Sat, 9 Mar 2024 22:49:04 GMT   (319kb,D)

Title: Predicting Depression and Anxiety: A Multi-Layer Perceptron for
  Analyzing the Mental Health Impact of COVID-19
Authors: David Fong and Tianshu Chu and Matthew Heflin and Xiaosi Gu and Oshani
  Seneviratne
Categories: cs.LG cs.CY
\\
  We introduce a multi-layer perceptron (MLP) called the COVID-19 Depression
and Anxiety Predictor (CoDAP) to predict mental health trends, particularly
anxiety and depression, during the COVID-19 pandemic. Our method utilizes a
comprehensive dataset, which tracked mental health symptoms weekly over ten
weeks during the initial COVID-19 wave (April to June 2020) in a diverse cohort
of U.S. adults. This period, characterized by a surge in mental health symptoms
and conditions, offers a critical context for our analysis. Our focus was to
extract and analyze patterns of anxiety and depression through a unique lens of
qualitative individual attributes using CoDAP. This model not only predicts
patterns of anxiety and depression during the pandemic but also unveils key
insights into the interplay of demographic factors, behavioral changes, and
social determinants of mental health. These findings contribute to a more
nuanced understanding of the complexity of mental health issues in times of
global health crises, potentially guiding future early interventions.
\\ ( https://arxiv.org/abs/2403.06033 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06064
Date: Sun, 10 Mar 2024 02:16:13 GMT   (2440kb,D)

Title: L$^2$GC: Lorentzian Linear Graph Convolutional Networks For Node
  Classification
Authors: Qiuyu Liang, Weihua Wang, Feilong Bao, Guanglai Gao
Categories: cs.LG cs.AI cs.CL
Comments: Accepted by LREC-COLING 2024
\\
  Linear Graph Convolutional Networks (GCNs) are used to classify the node in
the graph data. However, we note that most existing linear GCN models perform
neural network operations in Euclidean space, which do not explicitly capture
the tree-like hierarchical structure exhibited in real-world datasets that
modeled as graphs. In this paper, we attempt to introduce hyperbolic space into
linear GCN and propose a novel framework for Lorentzian linear GCN.
Specifically, we map the learned features of graph nodes into hyperbolic space,
and then perform a Lorentzian linear feature transformation to capture the
underlying tree-like structure of data. Experimental results on standard
citation networks datasets with semi-supervised learning show that our approach
yields new state-of-the-art results of accuracy 74.7$\%$ on Citeseer and
81.3$\%$ on PubMed datasets. Furthermore, we observe that our approach can be
trained up to two orders of magnitude faster than other nonlinear GCN models on
PubMed dataset. Our code is publicly available at
https://github.com/llqy123/LLGC-master.
\\ ( https://arxiv.org/abs/2403.06064 ,  2440kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06079
Date: Sun, 10 Mar 2024 03:51:59 GMT   (132kb,D)

Title: Generalization of Graph Neural Networks through the Lens of Homomorphism
Authors: Shouheng Li, Dongwoo Kim, Qing Wang
Categories: cs.LG
Comments: 17 pages, 3 figures
\\
  Despite the celebrated popularity of Graph Neural Networks (GNNs) across
numerous applications, the ability of GNNs to generalize remains less explored.
In this work, we propose to study the generalization of GNNs through a novel
perspective - analyzing the entropy of graph homomorphism. By linking graph
homomorphism with information-theoretic measures, we derive generalization
bounds for both graph and node classifications. These bounds are capable of
capturing subtleties inherent in various graph structures, including but not
limited to paths, cycles and cliques. This enables a data-dependent
generalization analysis with robust theoretical guarantees. To shed light on
the generality of of our proposed bounds, we present a unifying framework that
can characterize a broad spectrum of GNN models through the lens of graph
homomorphism. We validate the practical applicability of our theoretical
findings by showing the alignment between the proposed bounds and the
empirically observed generalization gaps over both real-world and synthetic
datasets.
\\ ( https://arxiv.org/abs/2403.06079 ,  132kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06080
Date: Sun, 10 Mar 2024 03:59:24 GMT   (3762kb,D)

Title: Local Vertex Colouring Graph Neural Networks
Authors: Shouheng Li, Dongwoo Kim, Qing Wang
Categories: cs.LG
Comments: 22 pages, 8 figures
Journal-ref: Proceedings of the 40th International Conference on Machine
  Learning, PMLR 202:19616-19637, 2023
\\
  In recent years, there has been a significant amount of research focused on
expanding the expressivity of Graph Neural Networks (GNNs) beyond the
Weisfeiler-Lehman (1-WL) framework. While many of these studies have yielded
advancements in expressivity, they have frequently come at the expense of
decreased efficiency or have been restricted to specific types of graphs. In
this study, we investigate the expressivity of GNNs from the perspective of
graph search. Specifically, we propose a new vertex colouring scheme and
demonstrate that classical search algorithms can efficiently compute graph
representations that extend beyond the 1-WL. We show the colouring scheme
inherits useful properties from graph search that can help solve problems like
graph biconnectivity. Furthermore, we show that under certain conditions, the
expressivity of GNNs increases hierarchically with the radius of the search
neighbourhood. To further investigate the proposed scheme, we develop a new
type of GNN based on two search strategies, breadth-first search and
depth-first search, highlighting the graph properties they can capture on top
of 1-WL. Our code is available at https://github.com/seanli3/lvc.
\\ ( https://arxiv.org/abs/2403.06080 ,  3762kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06082
Date: Sun, 10 Mar 2024 04:01:49 GMT   (9767kb,D)

Title: FrameQuant: Flexible Low-Bit Quantization for Transformers
Authors: Harshavardhan Adepu, Zhanpeng Zeng, Li Zhang, Vikas Singh
Categories: cs.LG cs.CL
Comments: 25 pages, 15 figures
\\
  Transformers are the backbone of powerful foundation models for many Vision
and Natural Language Processing tasks. But their compute and memory/storage
footprint is large, and so, serving such models is expensive often requiring
high-end hardware. To mitigate this difficulty, Post-Training Quantization
seeks to modify a pre-trained model and quantize it to eight bits or lower,
significantly boosting compute/memory/latency efficiency. Such models have been
successfully quantized to four bits with some performance loss. In this work,
we outline a simple scheme to quantize Transformer-based models to just two
bits (plus some overhead) with only a small drop in accuracy. Key to our
formulation is a concept borrowed from Harmonic analysis called Fusion Frames.
Our main finding is that the quantization must take place not in the original
weight space, but instead in the Fusion Frame representations. If quantization
is interpreted as the addition of noise, our casting of the problem allows
invoking an extensive body of known consistent recovery and noise robustness
guarantees. Further, if desired, de-noising filters are known in closed form.
We show empirically, via a variety of experiments, that (almost) two-bit
quantization for Transformer models promises sizable efficiency gains.
\\ ( https://arxiv.org/abs/2403.06082 ,  9767kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06087
Date: Sun, 10 Mar 2024 04:17:42 GMT   (644kb,D)

Title: Learning the irreversible progression trajectory of Alzheimer's disease
Authors: Yipei Wang, Bing He, Shannon Risacher, Andrew Saykin, Jingwen Yan,
  Xiaoqian Wang
Categories: cs.LG eess.IV
Comments: accepted by ISBI 2024
\\
  Alzheimer's disease (AD) is a progressive and irreversible brain disorder
that unfolds over the course of 30 years. Therefore, it is critical to capture
the disease progression in an early stage such that intervention can be applied
before the onset of symptoms. Machine learning (ML) models have been shown
effective in predicting the onset of AD. Yet for subjects with follow-up
visits, existing techniques for AD classification only aim for accurate group
assignment, where the monotonically increasing risk across follow-up visits is
usually ignored. Resulted fluctuating risk scores across visits violate the
irreversibility of AD, hampering the trustworthiness of models and also
providing little value to understanding the disease progression. To address
this issue, we propose a novel regularization approach to predict AD
longitudinally. Our technique aims to maintain the expected monotonicity of
increasing disease risk during progression while preserving expressiveness.
Specifically, we introduce a monotonicity constraint that encourages the model
to predict disease risk in a consistent and ordered manner across follow-up
visits. We evaluate our method using the longitudinal structural MRI and
amyloid-PET imaging data from the Alzheimer's Disease Neuroimaging Initiative
(ADNI). Our model outperforms existing techniques in capturing the
progressiveness of disease risk, and at the same time preserves prediction
accuracy.
\\ ( https://arxiv.org/abs/2403.06087 ,  644kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06174
Date: Sun, 10 Mar 2024 10:59:22 GMT   (2865kb,D)

Title: Domain Adversarial Active Learning for Domain Generalization
  Classification
Authors: Jianting Chen, Ling Ding, Yunxiao Yang, Zaiyuan Di, and Yang Xiang
Categories: cs.LG cs.AI
\\
  Domain generalization models aim to learn cross-domain knowledge from source
domain data, to improve performance on unknown target domains. Recent research
has demonstrated that diverse and rich source domain samples can enhance domain
generalization capability. This paper argues that the impact of each sample on
the model's generalization ability varies. Despite its small scale, a
high-quality dataset can still attain a certain level of generalization
ability. Motivated by this, we propose a domain-adversarial active learning
(DAAL) algorithm for classification tasks in domain generalization. First, we
analyze that the objective of tasks is to maximize the inter-class distance
within the same domain and minimize the intra-class distance across different
domains. To achieve this objective, we design a domain adversarial selection
method that prioritizes challenging samples. Second, we posit that even in a
converged model, there are subsets of features that lack discriminatory power
within each domain. We attempt to identify these feature subsets and optimize
them by a constraint loss. We validate and analyze our DAAL algorithm on
multiple domain generalization datasets, comparing it with various domain
generalization algorithms and active learning algorithms. Our results
demonstrate that the DAAL algorithm can achieve strong generalization ability
with fewer data resources, thereby reducing data annotation costs in domain
generalization tasks.
\\ ( https://arxiv.org/abs/2403.06174 ,  2865kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06183
Date: Sun, 10 Mar 2024 11:50:34 GMT   (523kb,D)

Title: An Improved Analysis of Langevin Algorithms with Prior Diffusion for
  Non-Log-Concave Sampling
Authors: Xunpeng Huang, Hanze Dong, Difan Zou, Tong Zhang
Categories: cs.LG math.OC math.ST stat.ML stat.TH
Comments: 32 pages
\\
  Understanding the dimension dependency of computational complexity in
high-dimensional sampling problem is a fundamental problem, both from a
practical and theoretical perspective. Compared with samplers with unbiased
stationary distribution, e.g., Metropolis-adjusted Langevin algorithm (MALA),
biased samplers, e.g., Underdamped Langevin Dynamics (ULD), perform better in
low-accuracy cases just because a lower dimension dependency in their
complexities. Along this line, Freund et al. (2022) suggest that the modified
Langevin algorithm with prior diffusion is able to converge dimension
independently for strongly log-concave target distributions. Nonetheless, it
remains open whether such property establishes for more general cases. In this
paper, we investigate the prior diffusion technique for the target
distributions satisfying log-Sobolev inequality (LSI), which covers a much
broader class of distributions compared to the strongly log-concave ones. In
particular, we prove that the modified Langevin algorithm can also obtain the
dimension-independent convergence of KL divergence with different step size
schedules. The core of our proof technique is a novel construction of an
interpolating SDE, which significantly helps to conduct a more accurate
characterization of the discrete updates of the overdamped Langevin dynamics.
Our theoretical analysis demonstrates the benefits of prior diffusion for a
broader class of target distributions and provides new insights into developing
faster sampling algorithms.
\\ ( https://arxiv.org/abs/2403.06183 ,  523kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06230
Date: Sun, 10 Mar 2024 15:01:50 GMT   (330kb,D)

Title: LinearAPT: An Adaptive Algorithm for the Fixed-Budget Thresholding
  Linear Bandit Problem
Authors: Yun-Ang Wu, Yun-Da Tsai, Shou-De Lin
Categories: cs.LG stat.ML
\\
  In this study, we delve into the Thresholding Linear Bandit (TLB) problem, a
nuanced domain within stochastic Multi-Armed Bandit (MAB) problems, focusing on
maximizing decision accuracy against a linearly defined threshold under
resource constraints. We present LinearAPT, a novel algorithm designed for the
fixed budget setting of TLB, providing an efficient solution to optimize
sequential decision-making. This algorithm not only offers a theoretical upper
bound for estimated loss but also showcases robust performance on both
synthetic and real-world datasets. Our contributions highlight the
adaptability, simplicity, and computational efficiency of LinearAPT, making it
a valuable addition to the toolkit for addressing complex sequential
decision-making challenges.
\\ ( https://arxiv.org/abs/2403.06230 ,  330kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06235
Date: Sun, 10 Mar 2024 15:25:49 GMT   (375kb)

Title: Probabilistic Neural Circuits
Authors: Pedro Zuidberg Dos Martires
Categories: cs.LG cs.AI cs.NE stat.ML
Comments: Proceedings of the AAAI Conference on Artificial Intelligence
\\
  Probabilistic circuits (PCs) have gained prominence in recent years as a
versatile framework for discussing probabilistic models that support tractable
queries and are yet expressive enough to model complex probability
distributions. Nevertheless, tractability comes at a cost: PCs are less
expressive than neural networks. In this paper we introduce probabilistic
neural circuits (PNCs), which strike a balance between PCs and neural nets in
terms of tractability and expressive power. Theoretically, we show that PNCs
can be interpreted as deep mixtures of Bayesian networks. Experimentally, we
demonstrate that PNCs constitute powerful function approximators.
\\ ( https://arxiv.org/abs/2403.06235 ,  375kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06239
Date: Sun, 10 Mar 2024 15:38:20 GMT   (695kb,D)

Title: Cooperative Classification and Rationalization for Graph Generalization
Authors: Linan Yue, Qi Liu, Ye Liu, Weibo Gao, Fangzhou Yao, Wenfeng Li
Categories: cs.LG cs.AI
Comments: Accepted to WWW 2024
\\
  Graph Neural Networks (GNNs) have achieved impressive results in graph
classification tasks, but they struggle to generalize effectively when faced
with out-of-distribution (OOD) data. Several approaches have been proposed to
address this problem. Among them, one solution is to diversify training
distributions in vanilla classification by modifying the data environment, yet
accessing the environment information is complex. Besides, another promising
approach involves rationalization, extracting invariant rationales for
predictions. However, extracting rationales is difficult due to limited
learning signals, resulting in less accurate rationales and diminished
predictions. To address these challenges, in this paper, we propose a
Cooperative Classification and Rationalization (C2R) method, consisting of the
classification and the rationalization module. Specifically, we first assume
that multiple environments are available in the classification module. Then, we
introduce diverse training distributions using an environment-conditional
generative network, enabling robust graph representations. Meanwhile, the
rationalization module employs a separator to identify relevant rationale
subgraphs while the remaining non-rationale subgraphs are de-correlated with
labels. Next, we align graph representations from the classification module
with rationale subgraph representations using the knowledge distillation
methods, enhancing the learning signal for rationales. Finally, we infer
multiple environments by gathering non-rationale representations and
incorporate them into the classification module for cooperative learning.
Extensive experimental results on both benchmarks and synthetic datasets
demonstrate the effectiveness of C2R. Code is available at
https://github.com/yuelinan/Codes-of-C2R.
\\ ( https://arxiv.org/abs/2403.06239 ,  695kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06298
Date: Sun, 10 Mar 2024 20:07:14 GMT   (16kb)

Title: Analysis of Total Variation Minimization for Clustered Federated
  Learning
Authors: A. Jung
Categories: cs.LG
ACM-class: I.2.11; I.5.3
\\
  A key challenge in federated learning applications is the statistical
heterogeneity of local datasets. Clustered federated learning addresses this
challenge by identifying clusters of local datasets that are approximately
homogeneous. One recent approach to clustered federated learning is generalized
total variation minimization (GTVMin). This approach requires a similarity
graph which can be obtained by domain expertise or in a data-driven fashion via
graph learning techniques. Under a widely applicable clustering assumption, we
derive an upper bound the deviation between GTVMin solutions and their
cluster-wise averages. This bound provides valuable insights into the
effectiveness and robustness of GTVMin in addressing statistical heterogeneity
within federated learning environments.
\\ ( https://arxiv.org/abs/2403.06298 ,  16kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06311
Date: Sun, 10 Mar 2024 21:08:29 GMT   (963kb,D)

Title: How much data do you need? Part 2: Predicting DL class specific training
  dataset sizes
Authors: Thomas M\"uhlenst\"adt, Jelena Frtunikj
Categories: cs.LG stat.ML
Comments: 17 pages, 10 figures
MSC-class: 68T99
\\
  This paper targets the question of predicting machine learning classification
model performance, when taking into account the number of training examples per
class and not just the overall number of training examples. This leads to the a
combinatorial question, which combinations of number of training examples per
class should be considered, given a fixed overall training dataset size. In
order to solve this question, an algorithm is suggested which is motivated from
special cases of space filling design of experiments. The resulting data are
modeled using models like powerlaw curves and similar models, extended like
generalized linear models i.e. by replacing the overall training dataset size
by a parametrized linear combination of the number of training examples per
label class. The proposed algorithm has been applied on the CIFAR10 and the
EMNIST datasets.
\\ ( https://arxiv.org/abs/2403.06311 ,  963kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06313
Date: Sun, 10 Mar 2024 21:18:54 GMT   (5604kb)

Title: Optimal Policy Sparsification and Low Rank Decomposition for Deep
  Reinforcement Learning
Authors: Vikram Goddla
Categories: cs.LG cs.AI
\\
  Deep reinforcement learning(DRL) has shown significant promise in a wide
range of applications including computer games and robotics. Yet, training DRL
policies consume extraordinary computing resources resulting in dense policies
which are prone to overfitting. Moreover, inference with dense DRL policies
limit their practical applications, especially in edge computing. Techniques
such as pruning and singular value decomposition have been used with deep
learning models to achieve sparsification and model compression to limit
overfitting and reduce memory consumption. However, these techniques resulted
in sub-optimal performance with notable decay in rewards. $L_1$ and $L_2$
regularization techniques have been proposed for neural network sparsification
and sparse auto-encoder development, but their implementation in DRL
environments has not been apparent. We propose a novel
$L_0$-norm-regularization technique using an optimal sparsity map to sparsify
DRL policies and promote their decomposition to a lower rank without decay in
rewards. We evaluated our $L_0$-norm-regularization technique across five
different environments (Cartpole-v1, Acrobat-v1, LunarLander-v2,
SuperMarioBros-7.1.v0 and Surgical Robot Learning) using several on-policy and
off-policy algorithms. We demonstrated that the $L_0$-norm-regularized DRL
policy in the SuperMarioBros environment achieved 93% sparsity and gained 70%
compression when subjected to low-rank decomposition, while significantly
outperforming the dense policy. Additionally, the $L_0$-norm-regularized DRL
policy in the Surgical Robot Learning environment achieved a 36% sparsification
and gained 46% compression when decomposed to a lower rank, while being
performant. The results suggest that our custom $L_0$-norm-regularization
technique for sparsification of DRL policies is a promising avenue to reduce
computational resources and limit overfitting.
\\ ( https://arxiv.org/abs/2403.06313 ,  5604kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06319
Date: Sun, 10 Mar 2024 21:37:21 GMT   (2506kb,D)

Title: Fake or Compromised? Making Sense of Malicious Clients in Federated
  Learning
Authors: Hamid Mozaffari, Sunav Choudhary, and Amir Houmansadr
Categories: cs.LG cs.CR
\\
  Federated learning (FL) is a distributed machine learning paradigm that
enables training models on decentralized data. The field of FL security against
poisoning attacks is plagued with confusion due to the proliferation of
research that makes different assumptions about the capabilities of adversaries
and the adversary models they operate under. Our work aims to clarify this
confusion by presenting a comprehensive analysis of the various poisoning
attacks and defensive aggregation rules (AGRs) proposed in the literature, and
connecting them under a common framework. To connect existing adversary models,
we present a hybrid adversary model, which lies in the middle of the spectrum
of adversaries, where the adversary compromises a few clients, trains a
generative (e.g., DDPM) model with their compromised samples, and generates new
synthetic data to solve an optimization for a stronger (e.g., cheaper, more
practical) attack against different robust aggregation rules. By presenting the
spectrum of FL adversaries, we aim to provide practitioners and researchers
with a clear understanding of the different types of threats they need to
consider when designing FL systems, and identify areas where further research
is needed.
\\ ( https://arxiv.org/abs/2403.06319 ,  2506kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06323
Date: Sun, 10 Mar 2024 21:45:12 GMT   (186kb,D)

Title: Risk-Sensitive RL with Optimized Certainty Equivalents via Reduction to
  Standard RL
Authors: Kaiwen Wang, Dawen Liang, Nathan Kallus, Wen Sun
Categories: cs.LG
\\
  We study Risk-Sensitive Reinforcement Learning (RSRL) with the Optimized
Certainty Equivalent (OCE) risk, which generalizes Conditional Value-at-risk
(CVaR), entropic risk and Markowitz's mean-variance. Using an augmented Markov
Decision Process (MDP), we propose two general meta-algorithms via reductions
to standard RL: one based on optimistic algorithms and another based on policy
optimization. Our optimistic meta-algorithm generalizes almost all prior RSRL
theory with entropic risk or CVaR. Under discrete rewards, our optimistic
theory also certifies the first RSRL regret bounds for MDPs with bounded
coverability, e.g., exogenous block MDPs. Under discrete rewards, our policy
optimization meta-algorithm enjoys both global convergence and local
improvement guarantees in a novel metric that lower bounds the true OCE risk.
Finally, we instantiate our framework with PPO, construct an MDP, and show that
it learns the optimal risk-sensitive policy while prior algorithms provably
fail.
\\ ( https://arxiv.org/abs/2403.06323 ,  186kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06328
Date: Sun, 10 Mar 2024 22:27:21 GMT   (5158kb,D)

Title: Transferable Reinforcement Learning via Generalized Occupancy Models
Authors: Chuning Zhu, Xinqi Wang, Tyler Han, Simon S. Du, Abhishek Gupta
Categories: cs.LG
\\
  Intelligent agents must be generalists - showing the ability to quickly adapt
and generalize to varying tasks. Within the framework of reinforcement learning
(RL), model-based RL algorithms learn a task-agnostic dynamics model of the
world, in principle allowing them to generalize to arbitrary rewards. However,
one-step models naturally suffer from compounding errors, making them
ineffective for problems with long horizons and large state spaces. In this
work, we propose a novel class of models - generalized occupancy models (GOMs)
- that retain the generality of model-based RL while avoiding compounding
error. The key idea behind GOMs is to model the distribution of all possible
long-term outcomes from a given state under the coverage of a stationary
dataset, along with a policy that realizes a particular outcome from the given
state. These models can then quickly be used to select the optimal action for
arbitrary new tasks, without having to redo policy optimization. By directly
modeling long-term outcomes, GOMs avoid compounding error while retaining
generality across arbitrary reward functions. We provide a practical
instantiation of GOMs using diffusion models and show its efficacy as a new
class of transferable models, both theoretically and empirically across a
variety of simulated robotics problems. Videos and code at
https://weirdlabuw.github.io/gom/.
\\ ( https://arxiv.org/abs/2403.06328 ,  5158kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06366
Date: Mon, 11 Mar 2024 01:36:37 GMT   (73kb)

Title: Finite-Time Error Analysis of Soft Q-Learning: Switching System Approach
Authors: Narim Jeong and Donghwan Lee
Categories: cs.LG
Comments: 15 pages
\\
  Soft Q-learning is a variation of Q-learning designed to solve entropy
regularized Markov decision problems where an agent aims to maximize the
entropy regularized value function. Despite its empirical success, there have
been limited theoretical studies of soft Q-learning to date. This paper aims to
offer a novel and unified finite-time, control-theoretic analysis of soft
Q-learning algorithms. We focus on two types of soft Q-learning algorithms: one
utilizing the log-sum-exp operator and the other employing the Boltzmann
operator. By using dynamical switching system models, we derive novel
finite-time error bounds for both soft Q-learning algorithms. We hope that our
analysis will deepen the current understanding of soft Q-learning by
establishing connections with switching system models and may even pave the way
for new frameworks in the finite-time analysis of other reinforcement learning
algorithms.
\\ ( https://arxiv.org/abs/2403.06366 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06367
Date: Mon, 11 Mar 2024 01:44:14 GMT   (1442kb,D)

Title: FeatAug: Automatic Feature Augmentation From One-to-Many Relationship
  Tables
Authors: Danrui Qi, Weiling Zheng, Jiannan Wang
Categories: cs.LG cs.DB
\\
  Feature augmentation from one-to-many relationship tables is a critical but
challenging problem in ML model development. To augment good features, data
scientists need to come up with SQL queries manually, which is time-consuming.
Featuretools [1] is a widely used tool by the data science community to
automatically augment the training data by extracting new features from
relevant tables. It represents each feature as a group-by aggregation SQL query
on relevant tables and can automatically generate these SQL queries. However,
it does not include predicates in these queries, which significantly limits its
application in many real-world scenarios. To overcome this limitation, we
propose FEATAUG, a new feature augmentation framework that automatically
extracts predicate-aware SQL queries from one-to-many relationship tables. This
extension is not trivial because considering predicates will exponentially
increase the number of candidate queries. As a result, the original
Featuretools framework, which materializes all candidate queries, will not work
and needs to be redesigned. We formally define the problem and model it as a
hyperparameter optimization problem. We discuss how the Bayesian Optimization
can be applied here and propose a novel warm-up strategy to optimize it. To
make our algorithm more practical, we also study how to identify promising
attribute combinations for predicates. We show that how the beam search idea
can partially solve the problem and propose several techniques to further
optimize it. Our experiments on four real-world datasets demonstrate that
FeatAug extracts more effective features compared to Featuretools and other
baselines. The code is open-sourced at https://github.com/sfu-db/FeatAug
\\ ( https://arxiv.org/abs/2403.06367 ,  1442kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06392
Date: Mon, 11 Mar 2024 02:57:27 GMT   (6547kb,D)

Title: Towards Robust Out-of-Distribution Generalization Bounds via Sharpness
Authors: Yingtian Zou, Kenji Kawaguchi, Yingnan Liu, Jiashuo Liu, Mong-Li Lee,
  Wynne Hsu
Categories: cs.LG
Comments: 40 pages, 9 figures, ICLR 2024 Spotlight Presentation
\\
  Generalizing to out-of-distribution (OOD) data or unseen domain, termed OOD
generalization, still lacks appropriate theoretical guarantees. Canonical OOD
bounds focus on different distance measurements between source and target
domains but fail to consider the optimization property of the learned model. As
empirically shown in recent work, the sharpness of learned minima influences
OOD generalization. To bridge this gap between optimization and OOD
generalization, we study the effect of sharpness on how a model tolerates data
change in domain shift which is usually captured by "robustness" in
generalization. In this paper, we give a rigorous connection between sharpness
and robustness, which gives better OOD guarantees for robust algorithms. It
also provides a theoretical backing for "flat minima leads to better OOD
generalization". Overall, we propose a sharpness-based OOD generalization bound
by taking robustness into consideration, resulting in a tighter bound than
non-robust guarantees. Our findings are supported by the experiments on a ridge
regression model, as well as the experiments on deep learning classification
tasks.
\\ ( https://arxiv.org/abs/2403.06392 ,  6547kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06397
Date: Mon, 11 Mar 2024 03:17:33 GMT   (4693kb,D)

Title: DeepSafeMPC: Deep Learning-Based Model Predictive Control for Safe
  Multi-Agent Reinforcement Learning
Authors: Xuefeng Wang, Henglin Pu, Hyung Jun Kim and Husheng Li
Categories: cs.LG cs.AI cs.SY eess.SY
Comments: 8 pages, 5 figures
\\
  Safe Multi-agent reinforcement learning (safe MARL) has increasingly gained
attention in recent years, emphasizing the need for agents to not only optimize
the global return but also adhere to safety requirements through behavioral
constraints. Some recent work has integrated control theory with multi-agent
reinforcement learning to address the challenge of ensuring safety. However,
there have been only very limited applications of Model Predictive Control
(MPC) methods in this domain, primarily due to the complex and implicit
dynamics characteristic of multi-agent environments. To bridge this gap, we
propose a novel method called Deep Learning-Based Model Predictive Control for
Safe Multi-Agent Reinforcement Learning (DeepSafeMPC). The key insight of
DeepSafeMPC is leveraging a entralized deep learning model to well predict
environmental dynamics. Our method applies MARL principles to search for
optimal solutions. Through the employment of MPC, the actions of agents can be
restricted within safe states concurrently. We demonstrate the effectiveness of
our approach using the Safe Multi-agent MuJoCo environment, showcasing
significant advancements in addressing safety concerns in MARL.
\\ ( https://arxiv.org/abs/2403.06397 ,  4693kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06398
Date: Mon, 11 Mar 2024 03:19:45 GMT   (1141kb,D)

Title: On the Diminishing Returns of Width for Continual Learning
Authors: Etash Guha, Vihan Lakshman
Categories: cs.LG cs.AI
Comments: 10 pages
\\
  While deep neural networks have demonstrated groundbreaking performance in
various settings, these models often suffer from \emph{catastrophic forgetting}
when trained on new tasks in sequence. Several works have empirically
demonstrated that increasing the width of a neural network leads to a decrease
in catastrophic forgetting but have yet to characterize the exact relationship
between width and continual learning. We design one of the first frameworks to
analyze Continual Learning Theory and prove that width is directly related to
forgetting in Feed-Forward Networks (FFN). Specifically, we demonstrate that
increasing network widths to reduce forgetting yields diminishing returns. We
empirically verify our claims at widths hitherto unexplored in prior studies
where the diminishing returns are clearly observed as predicted by our theory.
\\ ( https://arxiv.org/abs/2403.06398 ,  1141kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06408
Date: Mon, 11 Mar 2024 03:42:51 GMT   (761kb,D)

Title: What Makes Quantization for Large Language Models Hard? An Empirical
  Study from the Lens of Perturbation
Authors: Zhuocheng Gong, Jiahao Liu, Jingang Wang, Xunliang Cai, Dongyan Zhao,
  Rui Yan
Categories: cs.LG cs.AI
\\
  Quantization has emerged as a promising technique for improving the memory
and computational efficiency of large language models (LLMs). Though the
trade-off between performance and efficiency is well-known, there is still much
to be learned about the relationship between quantization and LLM performance.
To shed light on this relationship, we propose a new perspective on
quantization, viewing it as perturbations added to the weights and activations
of LLMs. We call this approach "the lens of perturbation". Using this lens, we
conduct experiments with various artificial perturbations to explore their
impact on LLM performance. Our findings reveal several connections between the
properties of perturbations and LLM performance, providing insights into the
failure cases of uniform quantization and suggesting potential solutions to
improve the robustness of LLM quantization. To demonstrate the significance of
our findings, we implement a simple non-uniform quantization approach based on
our insights. Our experiments show that this approach achieves minimal
performance degradation on both 4-bit weight quantization and 8-bit
quantization for weights and activations. These results validate the
correctness of our approach and highlight its potential to improve the
efficiency of LLMs without sacrificing performance.
\\ ( https://arxiv.org/abs/2403.06408 ,  761kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06419
Date: Mon, 11 Mar 2024 04:11:48 GMT   (1467kb,D)

Title: Causal Multi-Label Feature Selection in Federated Setting
Authors: Yukun Song, Dayuan Cao, Jiali Miao, Shuai Yang, Kui Yu
Categories: cs.LG
\\
  Multi-label feature selection serves as an effective mean for dealing with
high-dimensional multi-label data. To achieve satisfactory performance,
existing methods for multi-label feature selection often require the
centralization of substantial data from multiple sources. However, in Federated
setting, centralizing data from all sources and merging them into a single
dataset is not feasible. To tackle this issue, in this paper, we study a
challenging problem of causal multi-label feature selection in federated
setting and propose a Federated Causal Multi-label Feature Selection (FedCMFS)
algorithm with three novel subroutines. Specifically, FedCMFS first uses the
FedCFL subroutine that considers the correlations among label-label,
label-feature, and feature-feature to learn the relevant features (candidate
parents and children) of each class label while preserving data privacy without
centralizing data. Second, FedCMFS employs the FedCFR subroutine to selectively
recover the missed true relevant features. Finally, FedCMFS utilizes the FedCFC
subroutine to remove false relevant features. The extensive experiments on 8
datasets have shown that FedCMFS is effect for causal multi-label feature
selection in federated setting.
\\ ( https://arxiv.org/abs/2403.06419 ,  1467kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06425
Date: Mon, 11 Mar 2024 04:26:18 GMT   (6084kb,D)

Title: A Differential Geometric View and Explainability of GNN on Evolving
  Graphs
Authors: Yazheng Liu, Xi Zhang, Sihong Xie
Categories: cs.LG cs.AI
Comments: Accepted into ICLR 2023
\\
  Graphs are ubiquitous in social networks and biochemistry, where Graph Neural
Networks (GNN) are the state-of-the-art models for prediction. Graphs can be
evolving and it is vital to formally model and understand how a trained GNN
responds to graph evolution. We propose a smooth parameterization of the GNN
predicted distributions using axiomatic attribution, where the distributions
are on a low-dimensional manifold within a high-dimensional embedding space. We
exploit the differential geometric viewpoint to model distributional evolution
as smooth curves on the manifold. We reparameterize families of curves on the
manifold and design a convex optimization problem to find a unique curve that
concisely approximates the distributional evolution for human interpretation.
Extensive experiments on node classification, link prediction, and graph
classification tasks with evolving graphs demonstrate the better sparsity,
faithfulness, and intuitiveness of the proposed method over the
state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.06425 ,  6084kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06432
Date: Mon, 11 Mar 2024 04:49:41 GMT   (1427kb,D)

Title: Joint-Embedding Masked Autoencoder for Self-supervised Learning of
  Dynamic Functional Connectivity from the Human Brain
Authors: Jungwon Choi, Hyungi Lee, Byung-Hoon Kim, Juho Lee
Categories: cs.LG q-bio.NC
Comments: Under review
\\
  Graph Neural Networks (GNNs) have shown promise in learning dynamic
functional connectivity for distinguishing phenotypes from human brain
networks. However, obtaining extensive labeled clinical data for training is
often resource-intensive, making practical application difficult. Leveraging
unlabeled data thus becomes crucial for representation learning in a
label-scarce setting. Although generative self-supervised learning techniques,
especially masked autoencoders, have shown promising results in representation
learning in various domains, their application to dynamic graphs for dynamic
functional connectivity remains underexplored, facing challenges in capturing
high-level semantic representations. Here, we introduce the Spatio-Temporal
Joint Embedding Masked Autoencoder (ST-JEMA), drawing inspiration from the
Joint Embedding Predictive Architecture (JEPA) in computer vision. ST-JEMA
employs a JEPA-inspired strategy for reconstructing dynamic graphs, which
enables the learning of higher-level semantic representations considering
temporal perspectives, addressing the challenges in fMRI data representation
learning. Utilizing the large-scale UK Biobank dataset for self-supervised
learning, ST-JEMA shows exceptional representation learning performance on
dynamic functional connectivity demonstrating superiority over previous methods
in predicting phenotypes and psychiatric diagnoses across eight benchmark fMRI
datasets even with limited samples and effectiveness of temporal reconstruction
on missing data scenarios. These findings highlight the potential of our
approach as a robust representation learning method for leveraging label-scarce
fMRI data.
\\ ( https://arxiv.org/abs/2403.06432 ,  1427kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06458
Date: Mon, 11 Mar 2024 06:36:33 GMT   (7351kb,D)

Title: Prediction of Wort Density with LSTM Network
Authors: Derk Rembold, Bernd Stauss, Stefan Schwarzkopf
Categories: cs.LG
Comments: 12 pages
\\
  Many physical target values in technical processes are error-prone,
cumbersome, or expensive to measure automatically. One example of a physical
target value is the wort density, which is an important value needed for beer
production. This article introduces a system that helps the brewer measure wort
density through sensors in order to reduce errors in manual data collection.
Instead of a direct measurement of wort density, a method is developed that
calculates the density from measured values acquired by inexpensive standard
sensors such as pressure or temperature. The model behind the calculation is a
neural network, known as LSTM.
\\ ( https://arxiv.org/abs/2403.06458 ,  7351kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06466
Date: Mon, 11 Mar 2024 07:07:05 GMT   (10074kb,D)

Title: RL-MSA: a Reinforcement Learning-based Multi-line bus Scheduling
  Approach
Authors: Yingzhuo Liu
Categories: cs.LG cs.AI
\\
  Multiple Line Bus Scheduling Problem (MLBSP) is vital to save operational
cost of bus company and guarantee service quality for passengers. Existing
approaches typically generate a bus scheduling scheme in an offline manner and
then schedule buses according to the scheme. In practice, uncertain events such
as traffic congestion occur frequently, which may make the pre-determined bus
scheduling scheme infeasible. In this paper, MLBSP is modeled as a Markov
Decision Process (MDP). A Reinforcement Learning-based Multi-line bus
Scheduling Approach (RL-MSA) is proposed for bus scheduling at both the offline
and online phases. At the offline phase, deadhead decision is integrated into
bus selection decision for the first time to simplify the learning problem. At
the online phase, deadhead decision is made through a time window mechanism
based on the policy learned at the offline phase. We develop several new and
useful state features including the features for control points, bus lines and
buses. A bus priority screening mechanism is invented to construct bus-related
features. Considering the interests of both the bus company and passengers, a
reward function combining the final reward and the step-wise reward is devised.
Experiments at the offline phase demonstrate that the number of buses used of
RL-MSA is decreased compared with offline optimization approaches. At the
online phase, RL-MSA can cover all departure times in a timetable (i.e.,
service quality) without increasing the number of buses used (i.e., operational
cost).
\\ ( https://arxiv.org/abs/2403.06466 ,  10074kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06489
Date: Mon, 11 Mar 2024 07:51:27 GMT   (398kb,D)

Title: Graph Neural Network with Two Uplift Estimators for Label-Scarcity
  Individual Uplift Modeling
Authors: Dingyuan Zhu, Daixin Wang, Zhiqiang Zhang, Kun Kuang, Yan Zhang, Yulin
  Kang, Jun Zhou
Categories: cs.LG
\\
  Uplift modeling aims to measure the incremental effect, which we call uplift,
of a strategy or action on the users from randomized experiments or
observational data. Most existing uplift methods only use individual data,
which are usually not informative enough to capture the unobserved and complex
hidden factors regarding the uplift. Furthermore, uplift modeling scenario
usually has scarce labeled data, especially for the treatment group, which also
poses a great challenge for model training. Considering that the neighbors'
features and the social relationships are very informative to characterize a
user's uplift, we propose a graph neural network-based framework with two
uplift estimators, called GNUM, to learn from the social graph for uplift
estimation. Specifically, we design the first estimator based on a
class-transformed target. The estimator is general for all types of outcomes,
and is able to comprehensively model the treatment and control group data
together to approach the uplift. When the outcome is discrete, we further
design the other uplift estimator based on our defined partial labels, which is
able to utilize more labeled data from both the treatment and control groups,
to further alleviate the label scarcity problem. Comprehensive experiments on a
public dataset and two industrial datasets show a superior performance of our
proposed framework over state-of-the-art methods under various evaluation
metrics. The proposed algorithms have been deployed online to serve real-world
uplift estimation scenarios.
\\ ( https://arxiv.org/abs/2403.06489 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06524
Date: Mon, 11 Mar 2024 08:58:42 GMT   (1133kb,D)

Title: Tactical Decision Making for Autonomous Trucks by Deep Reinforcement
  Learning with Total Cost of Operation Based Reward
Authors: Deepthi Pathare, Leo Laine, Morteza Haghir Chehreghani
Categories: cs.LG cs.AI cs.RO
\\
  We develop a deep reinforcement learning framework for tactical decision
making in an autonomous truck, specifically for Adaptive Cruise Control (ACC)
and lane change maneuvers in a highway scenario. Our results demonstrate that
it is beneficial to separate high-level decision-making processes and low-level
control actions between the reinforcement learning agent and the low-level
controllers based on physical models. In the following, we study optimizing the
performance with a realistic and multi-objective reward function based on Total
Cost of Operation (TCOP) of the truck using different approaches; by adding
weights to reward components, by normalizing the reward components and by using
curriculum learning techniques.
\\ ( https://arxiv.org/abs/2403.06524 ,  1133kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06528
Date: Mon, 11 Mar 2024 09:10:37 GMT   (417kb,D)

Title: Adaptive Federated Learning Over the Air
Authors: Chenhao Wang, Zihan Chen, Nikolaos Pappas, Howard H. Yang, Tony Q. S.
  Quek, H. Vincent Poor
Categories: cs.LG cs.IT cs.NI math.IT
\\
  We propose a federated version of adaptive gradient methods, particularly
AdaGrad and Adam, within the framework of over-the-air model training. This
approach capitalizes on the inherent superposition property of wireless
channels, facilitating fast and scalable parameter aggregation. Meanwhile, it
enhances the robustness of the model training process by dynamically adjusting
the stepsize in accordance with the global gradient update. We derive the
convergence rate of the training algorithms, encompassing the effects of
channel fading and interference, for a broad spectrum of nonconvex loss
functions. Our analysis shows that the AdaGrad-based algorithm converges to a
stationary point at the rate of $\mathcal{O}( \ln{(T)} /{ T^{ 1 -
\frac{1}{\alpha} } } )$, where $\alpha$ represents the tail index of the
electromagnetic interference. This result indicates that the level of
heavy-tailedness in interference distribution plays a crucial role in the
training efficiency: the heavier the tail, the slower the algorithm converges.
In contrast, an Adam-like algorithm converges at the $\mathcal{O}( 1/T )$ rate,
demonstrating its advantage in expediting the model training process. We
conduct extensive experiments that corroborate our theoretical findings and
affirm the practical efficacy of our proposed federated adaptive gradient
methods.
\\ ( https://arxiv.org/abs/2403.06528 ,  417kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06535
Date: Mon, 11 Mar 2024 09:21:11 GMT   (8603kb,D)

Title: Decentralized and Lifelong-Adaptive Multi-Agent Collaborative Learning
Authors: Shuo Tang, Rui Ye, Chenxin Xu, Xiaowen Dong, Siheng Chen, Yanfeng Wang
Categories: cs.LG cs.AI cs.MA
Comments: 23 pages, 15 figures
\\
  Decentralized and lifelong-adaptive multi-agent collaborative learning aims
to enhance collaboration among multiple agents without a central server, with
each agent solving varied tasks over time. To achieve efficient collaboration,
agents should: i) autonomously identify beneficial collaborative relationships
in a decentralized manner; and ii) adapt to dynamically changing task
observations. In this paper, we propose DeLAMA, a decentralized multi-agent
lifelong collaborative learning algorithm with dynamic collaboration graphs. To
promote autonomous collaboration relationship learning, we propose a
decentralized graph structure learning algorithm, eliminating the need for
external priors. To facilitate adaptation to dynamic tasks, we design a memory
unit to capture the agents' accumulated learning history and knowledge, while
preserving finite storage consumption. To further augment the system's
expressive capabilities and computational efficiency, we apply algorithm
unrolling, leveraging the advantages of both mathematical optimization and
neural networks. This allows the agents to `learn to collaborate' through the
supervision of training tasks. Our theoretical analysis verifies that
inter-agent collaboration is communication efficient under a small number of
communication rounds. The experimental results verify its ability to facilitate
the discovery of collaboration strategies and adaptation to dynamic learning
scenarios, achieving a 98.80% reduction in MSE and a 188.87% improvement in
classification accuracy. We expect our work can serve as a foundational
technique to facilitate future works towards an intelligent, decentralized, and
dynamic multi-agent system. Code is available at
https://github.com/ShuoTang123/DeLAMA.
\\ ( https://arxiv.org/abs/2403.06535 ,  8603kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06560
Date: Mon, 11 Mar 2024 10:01:21 GMT   (8301kb,D)

Title: Sliced-Wasserstein Distances and Flows on Cartan-Hadamard Manifolds
Authors: Cl\'ement Bonet and Lucas Drumetz and Nicolas Courty
Categories: cs.LG stat.ML
\\
  While many Machine Learning methods were developed or transposed on
Riemannian manifolds to tackle data with known non Euclidean geometry, Optimal
Transport (OT) methods on such spaces have not received much attention. The
main OT tool on these spaces is the Wasserstein distance which suffers from a
heavy computational burden. On Euclidean spaces, a popular alternative is the
Sliced-Wasserstein distance, which leverages a closed-form solution of the
Wasserstein distance in one dimension, but which is not readily available on
manifolds. In this work, we derive general constructions of Sliced-Wasserstein
distances on Cartan-Hadamard manifolds, Riemannian manifolds with non-positive
curvature, which include among others Hyperbolic spaces or the space of
Symmetric Positive Definite matrices. Then, we propose different applications.
Additionally, we derive non-parametric schemes to minimize these new distances
by approximating their Wasserstein gradient flows.
\\ ( https://arxiv.org/abs/2403.06560 ,  8301kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06563
Date: Mon, 11 Mar 2024 10:05:29 GMT   (67kb,D)

Title: Unraveling the Mystery of Scaling Laws: Part I
Authors: Hui Su, Zhi Tian, Xiaoyu Shen, Xunliang Cai
Categories: cs.LG cs.CL
\\
  Scaling law principles indicate a power-law correlation between loss and
variables such as model size, dataset size, and computational resources
utilized during training. These principles play a vital role in optimizing
various aspects of model pre-training, ultimately contributing to the success
of large language models such as GPT-4, Llama and Gemini. However, the original
scaling law paper by OpenAI did not disclose the complete details necessary to
derive the precise scaling law formulas, and their conclusions are only based
on models containing up to 1.5 billion parameters. Though some subsequent works
attempt to unveil these details and scale to larger models, they often neglect
the training dependency of important factors such as the learning rate, context
length and batch size, leading to their failure to establish a reliable formula
for predicting the test loss trajectory. In this technical report, we confirm
that the scaling law formulations proposed in the original OpenAI paper remain
valid when scaling the model size up to 33 billion, but the constant
coefficients in these formulas vary significantly with the experiment setup. We
meticulously identify influential factors and provide transparent, step-by-step
instructions to estimate all constant terms in scaling-law formulas by training
on models with only 1M~60M parameters. Using these estimated formulas, we
showcase the capability to accurately predict various attributes for models
with up to 33B parameters before their training, including (1) the minimum
possible test loss; (2) the minimum required training steps and processed
tokens to achieve a specific loss; (3) the critical batch size with an optimal
time/computation trade-off at any loss value; and (4) the complete test loss
trajectory with arbitrary batch size.
\\ ( https://arxiv.org/abs/2403.06563 ,  67kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06569
Date: Mon, 11 Mar 2024 10:10:45 GMT   (1063kb,D)

Title: Enhancing Joint Motion Prediction for Individuals with Limb Loss Through
  Model Reprogramming
Authors: Sharmita Dey, Sarath R. Nair
Categories: cs.LG cs.RO
Comments: Accepted at ICLR 2024 Workshop on Learning from Time Series for
  Health
\\
  Mobility impairment caused by limb loss is a significant challenge faced by
millions of individuals worldwide. The development of advanced assistive
technologies, such as prosthetic devices, has the potential to greatly improve
the quality of life for amputee patients. A critical component in the design of
such technologies is the accurate prediction of reference joint motion for the
missing limb. However, this task is hindered by the scarcity of joint motion
data available for amputee patients, in contrast to the substantial quantity of
data from able-bodied subjects. To overcome this, we leverage deep learning's
reprogramming property to repurpose well-trained models for a new goal without
altering the model parameters. With only data-level manipulation, we adapt
models originally designed for able-bodied people to forecast joint motion in
amputees. The findings in this study have significant implications for
advancing assistive tech and amputee mobility.
\\ ( https://arxiv.org/abs/2403.06569 ,  1063kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06571
Date: Mon, 11 Mar 2024 10:14:06 GMT   (2117kb,D)

Title: Scalable Online Exploration via Coverability
Authors: Philip Amortila, Dylan J. Foster, Akshay Krishnamurthy
Categories: cs.LG math.OC stat.ML
\\
  Exploration is a major challenge in reinforcement learning, especially for
high-dimensional domains that require function approximation. We propose
exploration objectives -- policy optimization objectives that enable downstream
maximization of any reward function -- as a conceptual framework to systematize
the study of exploration. Within this framework, we introduce a new objective,
$L_1$-Coverage, which generalizes previous exploration schemes and supports
three fundamental desiderata:
  1. Intrinsic complexity control. $L_1$-Coverage is associated with a
structural parameter, $L_1$-Coverability, which reflects the intrinsic
statistical difficulty of the underlying MDP, subsuming Block and Low-Rank
MDPs.
  2. Efficient planning. For a known MDP, optimizing $L_1$-Coverage efficiently
reduces to standard policy optimization, allowing flexible integration with
off-the-shelf methods such as policy gradient and Q-learning approaches.
  3. Efficient exploration. $L_1$-Coverage enables the first computationally
efficient model-based and model-free algorithms for online (reward-free or
reward-driven) reinforcement learning in MDPs with low coverability.
  Empirically, we find that $L_1$-Coverage effectively drives off-the-shelf
policy optimization algorithms to explore the state space.
\\ ( https://arxiv.org/abs/2403.06571 ,  2117kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06576
Date: Mon, 11 Mar 2024 10:26:04 GMT   (1999kb,D)

Title: FFAD: A Novel Metric for Assessing Generated Time Series Data Utilizing
  Fourier Transform and Auto-encoder
Authors: Yang Chen, Dustin J. Kempton, Rafal A. Angryk
Categories: cs.LG
Comments: 13 pages, 6 figures, accepted by ICTIS-2024 on March 8th, 2024
\\
  The success of deep learning-based generative models in producing realistic
images, videos, and audios has led to a crucial consideration: how to
effectively assess the quality of synthetic samples. While the Fr\'{e}chet
Inception Distance (FID) serves as the standard metric for evaluating
generative models in image synthesis, a comparable metric for time series data
is notably absent. This gap in assessment capabilities stems from the absence
of a widely accepted feature vector extractor pre-trained on benchmark time
series datasets. In addressing these challenges related to assessing the
quality of time series, particularly in the context of Fr\'echet Distance, this
work proposes a novel solution leveraging the Fourier transform and
Auto-encoder, termed the Fr\'{e}chet Fourier-transform Auto-encoder Distance
(FFAD). Through our experimental results, we showcase the potential of FFAD for
effectively distinguishing samples from different classes. This novel metric
emerges as a fundamental tool for the evaluation of generative time series
data, contributing to the ongoing efforts of enhancing assessment methodologies
in the realm of deep learning-based generative models.
\\ ( https://arxiv.org/abs/2403.06576 ,  1999kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06586
Date: Mon, 11 Mar 2024 10:32:23 GMT   (2950kb,D)

Title: ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity
  Recognition Models
Authors: Luca Arrotta, Claudio Bettini, Gabriele Civitarese, Michele Fiori
Categories: cs.LG cs.AI cs.CL
\\
  Context-aware Human Activity Recognition (HAR) is a hot research area in
mobile computing, and the most effective solutions in the literature are based
on supervised deep learning models. However, the actual deployment of these
systems is limited by the scarcity of labeled data that is required for
training. Neuro-Symbolic AI (NeSy) provides an interesting research direction
to mitigate this issue, by infusing common-sense knowledge about human
activities and the contexts in which they can be performed into HAR deep
learning classifiers. Existing NeSy methods for context-aware HAR rely on
knowledge encoded in logic-based models (e.g., ontologies) whose design,
implementation, and maintenance to capture new activities and contexts require
significant human engineering efforts, technical knowledge, and domain
expertise. Recent works show that pre-trained Large Language Models (LLMs)
effectively encode common-sense knowledge about human activities. In this work,
we propose ContextGPT: a novel prompt engineering approach to retrieve from
LLMs common-sense knowledge about the relationship between human activities and
the context in which they are performed. Unlike ontologies, ContextGPT requires
limited human effort and expertise. An extensive evaluation carried out on two
public datasets shows how a NeSy model obtained by infusing common-sense
knowledge from ContextGPT is effective in data scarcity scenarios, leading to
similar (and sometimes better) recognition rates than logic-based approaches
with a fraction of the effort.
\\ ( https://arxiv.org/abs/2403.06586 ,  2950kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06631
Date: Mon, 11 Mar 2024 11:41:30 GMT   (3895kb,D)

Title: Evaluating the Energy Efficiency of Few-Shot Learning for Object
  Detection in Industrial Settings
Authors: Georgios Tsoumplekas, Vladislav Li, Ilias Siniosoglou, Vasileios
  Argyriou, Sotirios K. Goudos, Ioannis D. Moscholios, Panagiotis
  Radoglou-Grammatikis, Panagiotis Sarigiannidis
Categories: cs.LG cs.AI cs.CV
Comments: 7 pages, 6 figures, 4 tables
\\
  In the ever-evolving era of Artificial Intelligence (AI), model performance
has constituted a key metric driving innovation, leading to an exponential
growth in model size and complexity. However, sustainability and energy
efficiency have been critical requirements during deployment in contemporary
industrial settings, necessitating the use of data-efficient approaches such as
few-shot learning. In this paper, to alleviate the burden of lengthy model
training and minimize energy consumption, a finetuning approach to adapt
standard object detection models to downstream tasks is examined. Subsequently,
a thorough case study and evaluation of the energy demands of the developed
models, applied in object detection benchmark datasets from volatile industrial
environments is presented. Specifically, different finetuning strategies as
well as utilization of ancillary evaluation data during training are examined,
and the trade-off between performance and efficiency is highlighted in this
low-data regime. Finally, this paper introduces a novel way to quantify this
trade-off through a customized Efficiency Factor metric.
\\ ( https://arxiv.org/abs/2403.06631 ,  3895kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06643
Date: Mon, 11 Mar 2024 12:04:28 GMT   (22495kb,D)

Title: Spatial features of CO2 for occupancy detection in a naturally
  ventilated school building
Authors: Qirui Huang, Marc Syndicus, J\'er\^ome Frisch, Christoph van Treeck
Categories: cs.LG eess.SP
\\
  Accurate occupancy information helps to improve building energy efficiency
and occupant comfort. Occupancy detection methods based on CO2 sensors have
received attention due to their low cost and low intrusiveness. In naturally
ventilated buildings, the accuracy of CO2-based occupancy detection is
generally low in related studies due to the complex ventilation behavior and
the difficulty in measuring the actual air exchange through windows. In this
study, we present two novel features for occupancy detection based on the
spatial distribution of the CO2 concentration. After a quantitative analysis
with Support Vector Machine (SVM) as classifier, it was found that the accuracy
of occupancy state detection in naturally ventilated rooms could be improved by
up to 14.8 percentage points compared to the baseline, reaching 83.2 % (F1
score 0.84) without any ventilation information. With ventilation information,
the accuracy reached 87.6 % (F1 score 0.89). The performance of occupancy
quantity detection was significantly improved by up to 25.3 percentage points
versus baseline, reaching 56 %, with root mean square error (RMSE) of 11.44
occupants, using only CO2-related features. Additional ventilation information
further enhanced the performance to 61.8 % (RMSE 9.02 occupants). By
incorporating spatial features, the model using only CO2-related features
revealed similar performance as the model containing additional ventilation
information, resulting in a better low-cost occupancy detection method for
naturally ventilated buildings.
\\ ( https://arxiv.org/abs/2403.06643 ,  22495kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06644
Date: Mon, 11 Mar 2024 12:07:13 GMT   (1517kb,D)

Title: Elephants Never Forget: Testing Language Models for Memorization of
  Tabular Data
Authors: Sebastian Bordt, Harsha Nori, Rich Caruana
Categories: cs.LG cs.CL
Comments: Table Representation Learning Workshop at NeurIPS 2023
\\
  While many have shown how Large Language Models (LLMs) can be applied to a
diverse set of tasks, the critical issues of data contamination and
memorization are often glossed over. In this work, we address this concern for
tabular data. Starting with simple qualitative tests for whether an LLM knows
the names and values of features, we introduce a variety of different
techniques to assess the degrees of contamination, including statistical tests
for conditional distribution modeling and four tests that identify
memorization. Our investigation reveals that LLMs are pre-trained on many
popular tabular datasets. This exposure can lead to invalid performance
evaluation on downstream tasks because the LLMs have, in effect, been fit to
the test set. Interestingly, we also identify a regime where the language model
reproduces important statistics of the data, but fails to reproduce the dataset
verbatim. On these datasets, although seen during training, good performance on
downstream tasks might not be due to overfitting. Our findings underscore the
need for ensuring data integrity in machine learning tasks with LLMs. To
facilitate future research, we release an open-source tool that can perform
various tests for memorization
\url{https://github.com/interpretml/LLM-Tabular-Memorization-Checker}.
\\ ( https://arxiv.org/abs/2403.06644 ,  1517kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06668
Date: Mon, 11 Mar 2024 12:36:14 GMT   (4144kb,D)

Title: PeerAiD: Improving Adversarial Distillation from a Specialized Peer
  Tutor
Authors: Jaewon Jung, Hongsun Jang, Jaeyong Song, Jinho Lee
Categories: cs.LG cs.CV
Comments: Accepted to CVPR 2024
\\
  Adversarial robustness of the neural network is a significant concern when it
is applied to security-critical domains. In this situation, adversarial
distillation is a promising option which aims to distill the robustness of the
teacher network to improve the robustness of a small student network. Previous
works pretrain the teacher network to make it robust to the adversarial
examples aimed at itself. However, the adversarial examples are dependent on
the parameters of the target network. The fixed teacher network inevitably
degrades its robustness against the unseen transferred adversarial examples
which targets the parameters of the student network in the adversarial
distillation process. We propose PeerAiD to make a peer network learn the
adversarial examples of the student network instead of adversarial examples
aimed at itself. PeerAiD is an adversarial distillation that trains the peer
network and the student network simultaneously in order to make the peer
network specialized for defending the student network. We observe that such
peer networks surpass the robustness of pretrained robust teacher network
against student-attacked adversarial samples. With this peer network and
adversarial distillation, PeerAiD achieves significantly higher robustness of
the student network with AutoAttack (AA) accuracy up to 1.66%p and improves the
natural accuracy of the student network up to 4.72%p with ResNet-18 and
TinyImageNet dataset.
\\ ( https://arxiv.org/abs/2403.06668 ,  4144kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06677
Date: Mon, 11 Mar 2024 12:49:37 GMT   (704kb,D)

Title: Streamlining in the Riemannian Realm: Efficient Riemannian Optimization
  with Loopless Variance Reduction
Authors: Yury Demidovich, Grigory Malinovsky, Peter Richt\'arik
Categories: cs.LG cs.AI cs.DC
\\
  In this study, we investigate stochastic optimization on Riemannian
manifolds, focusing on the crucial variance reduction mechanism used in both
Euclidean and Riemannian settings. Riemannian variance-reduced methods usually
involve a double-loop structure, computing a full gradient at the start of each
loop. Determining the optimal inner loop length is challenging in practice, as
it depends on strong convexity or smoothness constants, which are often unknown
or hard to estimate. Motivated by Euclidean methods, we introduce the
Riemannian Loopless SVRG (R-LSVRG) and PAGE (R-PAGE) methods. These methods
replace the outer loop with probabilistic gradient computation triggered by a
coin flip in each iteration, ensuring simpler proofs, efficient hyperparameter
selection, and sharp convergence guarantees. Using R-PAGE as a framework for
non-convex Riemannian optimization, we demonstrate its applicability to various
important settings. For example, we derive Riemannian MARINA (R-MARINA) for
distributed settings with communication compression, providing the best
theoretical communication complexity guarantees for non-convex distributed
optimization over Riemannian manifolds. Experimental results support our
theoretical findings.
\\ ( https://arxiv.org/abs/2403.06677 ,  704kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06687
Date: Mon, 11 Mar 2024 13:04:21 GMT   (3523kb,D)

Title: Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and
  Attention Mechanism Approach for Heterogeneous Graph-Structured Data
Authors: Jinghan Huang, Qiufeng Chen, Yijun Bian, Pengli Zhu, Nanguang Chen,
  Moo K. Chung, Anqi Qiu
Categories: cs.LG cs.CV
\\
  Graph neural networks (GNNs) have proven effective in capturing relationships
among nodes in a graph. This study introduces a novel perspective by
considering a graph as a simplicial complex, encompassing nodes, edges,
triangles, and $k$-simplices, enabling the definition of graph-structured data
on any $k$-simplices. Our contribution is the Hodge-Laplacian heterogeneous
graph attention network (HL-HGAT), designed to learn heterogeneous signal
representations across $k$-simplices. The HL-HGAT incorporates three key
components: HL convolutional filters (HL-filters), simplicial projection (SP),
and simplicial attention pooling (SAP) operators, applied to $k$-simplices.
HL-filters leverage the unique topology of $k$-simplices encoded by the
Hodge-Laplacian (HL) operator, operating within the spectral domain of the
$k$-th HL operator. To address computation challenges, we introduce a
polynomial approximation for HL-filters, exhibiting spatial localization
properties. Additionally, we propose a pooling operator to coarsen
$k$-simplices, combining features through simplicial attention mechanisms of
self-attention and cross-attention via transformers and SP operators, capturing
topological interconnections across multiple dimensions of simplices. The
HL-HGAT is comprehensively evaluated across diverse graph applications,
including NP-hard problems, graph multi-label and classification challenges,
and graph regression tasks in logistics, computer vision, biology, chemistry,
and neuroscience. The results demonstrate the model's efficacy and versatility
in handling a wide range of graph-based scenarios.
\\ ( https://arxiv.org/abs/2403.06687 ,  3523kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06726
Date: Mon, 11 Mar 2024 13:44:49 GMT   (1118kb,D)

Title: Probabilistic Contrastive Learning for Long-Tailed Visual Recognition
Authors: Chaoqun Du, Yulin Wang, Shiji Song, and Gao Huang
Categories: cs.LG cs.CV
Comments: Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (T-PAMI)
\\
  Long-tailed distributions frequently emerge in real-world data, where a large
number of minority categories contain a limited number of samples. Such
imbalance issue considerably impairs the performance of standard supervised
learning algorithms, which are mainly designed for balanced training sets.
Recent investigations have revealed that supervised contrastive learning
exhibits promising potential in alleviating the data imbalance. However, the
performance of supervised contrastive learning is plagued by an inherent
challenge: it necessitates sufficiently large batches of training data to
construct contrastive pairs that cover all categories, yet this requirement is
difficult to meet in the context of class-imbalanced data. To overcome this
obstacle, we propose a novel probabilistic contrastive (ProCo) learning
algorithm that estimates the data distribution of the samples from each class
in the feature space, and samples contrastive pairs accordingly. In fact,
estimating the distributions of all classes using features in a small batch,
particularly for imbalanced data, is not feasible. Our key idea is to introduce
a reasonable and simple assumption that the normalized features in contrastive
learning follow a mixture of von Mises-Fisher (vMF) distributions on unit
space, which brings two-fold benefits. First, the distribution parameters can
be estimated using only the first sample moment, which can be efficiently
computed in an online manner across different batches. Second, based on the
estimated distribution, the vMF distribution allows us to sample an infinite
number of contrastive pairs and derive a closed form of the expected
contrastive loss for efficient optimization. Our code is available at
https://github.com/LeapLabTHU/ProCo.
\\ ( https://arxiv.org/abs/2403.06726 ,  1118kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06757
Date: Mon, 11 Mar 2024 14:29:56 GMT   (1079kb,D)

Title: Koopman Ensembles for Probabilistic Time Series Forecasting
Authors: Anthony Frion, Lucas Drumetz, Guillaume Tochon, Mauro Dalla Mura,
  Albdeldjalil A\"issa El Bey
Categories: cs.LG
\\
  In the context of an increasing popularity of data-driven models to represent
dynamical systems, many machine learning-based implementations of the Koopman
operator have recently been proposed. However, the vast majority of those works
are limited to deterministic predictions, while the knowledge of uncertainty is
critical in fields like meteorology and climatology. In this work, we
investigate the training of ensembles of models to produce stochastic outputs.
We show through experiments on real remote sensing image time series that
ensembles of independently trained models are highly overconfident and that
using a training criterion that explicitly encourages the members to produce
predictions with high inter-model variances greatly improves the uncertainty
quantification of the ensembles.
\\ ( https://arxiv.org/abs/2403.06757 ,  1079kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06768
Date: Mon, 11 Mar 2024 14:37:57 GMT   (7040kb,D)

Title: XB-MAML: Learning Expandable Basis Parameters for Effective
  Meta-Learning with Wide Task Coverage
Authors: Jae-Jun Lee, Sung Whan Yoon
Categories: cs.LG
Comments: In Proceedings of the International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024, Valencia, Spain
\\
  Meta-learning, which pursues an effective initialization model, has emerged
as a promising approach to handling unseen tasks. However, a limitation remains
to be evident when a meta-learner tries to encompass a wide range of task
distribution, e.g., learning across distinctive datasets or domains. Recently,
a group of works has attempted to employ multiple model initializations to
cover widely-ranging tasks, but they are limited in adaptively expanding
initializations. We introduce XB-MAML, which learns expandable basis
parameters, where they are linearly combined to form an effective
initialization to a given task. XB-MAML observes the discrepancy between the
vector space spanned by the basis and fine-tuned parameters to decide whether
to expand the basis. Our method surpasses the existing works in the
multi-domain meta-learning benchmarks and opens up new chances of meta-learning
for obtaining the diverse inductive bias that can be combined to stretch toward
the effective initialization for diverse unseen tasks.
\\ ( https://arxiv.org/abs/2403.06768 ,  7040kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06771
Date: Mon, 11 Mar 2024 14:39:24 GMT   (1883kb,D)

Title: Redefining Event Types and Group Evolution in Temporal Data
Authors: Andrea Failla and R\'emy Cazabet and Giulio Rossetti and Salvatore
  Citraro
Categories: cs.LG cs.SI
\\
  Groups -- such as clusters of points or communities of nodes -- are
fundamental when addressing various data mining tasks. In temporal data, the
predominant approach for characterizing group evolution has been through the
identification of ``events". However, the events usually described in the
literature, e.g., shrinks/growths, splits/merges, are often arbitrarily
defined, creating a gap between such theoretical/predefined types and real-data
group observations. Moving beyond existing taxonomies, we think of events as
``archetypes" characterized by a unique combination of quantitative dimensions
that we call ``facets". Group dynamics are defined by their position within the
facet space, where archetypal events occupy extremities. Thus, rather than
enforcing strict event types, our approach can allow for hybrid descriptions of
dynamics involving group proximity to multiple archetypes. We apply our
framework to evolving groups from several face-to-face interaction datasets,
showing it enables richer, more reliable characterization of group dynamics
with respect to state-of-the-art methods, especially when the groups are
subject to complex relationships. Our approach also offers intuitive solutions
to common tasks related to dynamic group analysis, such as choosing an
appropriate aggregation scale, quantifying partition stability, and evaluating
event quality.
\\ ( https://arxiv.org/abs/2403.06771 ,  1883kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06797
Date: Mon, 11 Mar 2024 15:15:50 GMT   (358kb)

Title: Leveraging Internal Representations of Model for Magnetic Image
  Classification
Authors: Adarsh N L, Arun P V, Alok Porwal, Malcolm Aranha
Categories: cs.LG cs.CV
Comments: 5 Pages, 6 Figures
\\
  Data generated by edge devices has the potential to train intelligent
autonomous systems across various domains. Despite the emergence of diverse
machine learning approaches addressing privacy concerns and utilizing
distributed data, security issues persist due to the sensitive storage of data
shards in disparate locations. This paper introduces a potentially
groundbreaking paradigm for machine learning model training, specifically
designed for scenarios with only a single magnetic image and its corresponding
label image available. We harness the capabilities of Deep Learning to generate
concise yet informative samples, aiming to overcome data scarcity. Through the
utilization of deep learning's internal representations, our objective is to
efficiently address data scarcity issues and produce meaningful results. This
methodology presents a promising avenue for training machine learning models
with minimal data.
\\ ( https://arxiv.org/abs/2403.06797 ,  358kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06806
Date: Mon, 11 Mar 2024 15:25:03 GMT   (198kb,D)

Title: On the Global Convergence of Policy Gradient in Average Reward Markov
  Decision Processes
Authors: Navdeep Kumar, Yashaswini Murthy, Itai Shufaro, Kfir Y. Levy, R.
  Srikant and Shie Mannor
Categories: cs.LG cs.SY eess.SY
Comments: 29 pages, 5 figures
\\
  We present the first finite time global convergence analysis of policy
gradient in the context of infinite horizon average reward Markov decision
processes (MDPs). Specifically, we focus on ergodic tabular MDPs with finite
state and action spaces. Our analysis shows that the policy gradient iterates
converge to the optimal policy at a sublinear rate of
$O\left({\frac{1}{T}}\right),$ which translates to $O\left({\log(T)}\right)$
regret, where $T$ represents the number of iterations. Prior work on
performance bounds for discounted reward MDPs cannot be extended to average
reward MDPs because the bounds grow proportional to the fifth power of the
effective horizon. Thus, our primary contribution is in proving that the policy
gradient algorithm converges for average-reward MDPs and in obtaining
finite-time performance guarantees. In contrast to the existing discounted
reward performance bounds, our performance bounds have an explicit dependence
on constants that capture the complexity of the underlying MDP. Motivated by
this observation, we reexamine and improve the existing performance bounds for
discounted reward MDPs. We also present simulations to empirically evaluate the
performance of average reward policy gradient algorithm.
\\ ( https://arxiv.org/abs/2403.06806 ,  198kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06807
Date: Mon, 11 Mar 2024 15:26:34 GMT   (2798kb,D)

Title: Multistep Consistency Models
Authors: Jonathan Heek, Emiel Hoogeboom, Tim Salimans
Categories: cs.LG cs.CV stat.ML
\\
  Diffusion models are relatively easy to train but require many steps to
generate samples. Consistency models are far more difficult to train, but
generate samples in a single step.
  In this paper we propose Multistep Consistency Models: A unification between
Consistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that
can interpolate between a consistency model and a diffusion model: a trade-off
between sampling speed and sampling quality. Specifically, a 1-step consistency
model is a conventional consistency model whereas we show that a $\infty$-step
consistency model is a diffusion model.
  Multistep Consistency Models work really well in practice. By increasing the
sample budget from a single step to 2-8 steps, we can train models more easily
that generate higher quality samples, while retaining much of the sampling
speed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1
FID on Imagenet128 in 8 steps with consistency distillation. We also show that
our method scales to a text-to-image diffusion model, generating samples that
are very close to the quality of the original model.
\\ ( https://arxiv.org/abs/2403.06807 ,  2798kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06812
Date: Mon, 11 Mar 2024 15:32:56 GMT   (51kb)

Title: Monotone Individual Fairness
Authors: Yahav Bechavod
Categories: cs.LG cs.CY stat.ML
\\
  We revisit the problem of online learning with individual fairness, where an
online learner strives to maximize predictive accuracy while ensuring that
similar individuals are treated similarly. We first extend the frameworks of
Gillen et al. (2018); Bechavod et al. (2020), which rely on feedback from human
auditors regarding fairness violations, as we consider auditing schemes that
are capable of aggregating feedback from any number of auditors, using a rich
class we term monotone aggregation functions. We then prove a characterization
for such auditing schemes, practically reducing the analysis of auditing for
individual fairness by multiple auditors to that of auditing by
(instance-specific) single auditors. Using our generalized framework, we
present an oracle-efficient algorithm achieving an upper bound frontier of
$(\mathcal{O}(T^{1/2+2b}),\mathcal{O}(T^{3/4-b}))$ respectively for regret,
number of fairness violations, for $0\leq b \leq 1/4$. We then study an online
classification setting where label feedback is available for
positively-predicted individuals only, and present an oracle-efficient
algorithm achieving an upper bound frontier of
$(\mathcal{O}(T^{2/3+2b}),\mathcal{O}(T^{5/6-b}))$ for regret, number of
fairness violations, for $0\leq b \leq 1/6$. In both settings, our algorithms
improve on the best known bounds for oracle-efficient algorithms. Furthermore,
our algorithms offer significant improvements in computational efficiency,
greatly reducing the number of required calls to an (offline) optimization
oracle per round, to $\tilde{\mathcal{O}}(\alpha^{-2})$ in the full information
setting, and $\tilde{\mathcal{O}}(\alpha^{-2} + k^2T^{1/3})$ in the partial
information setting, where $\alpha$ is the sensitivity for reporting fairness
violations, and $k$ is the number of individuals in a round.
\\ ( https://arxiv.org/abs/2403.06812 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06814
Date: Mon, 11 Mar 2024 15:33:40 GMT   (1760kb,D)

Title: {\epsilon}-Neural Thompson Sampling of Deep Brain Stimulation for
  Parkinson Disease Treatment
Authors: Hao-Lun Hsu, Qitong Gao, Miroslav Pajic
Categories: cs.LG q-bio.NC
Comments: 11 pages, 12 figures, 2 tables. To appear in the 15th ACM/IEEE
  International Conference on Cyber-Physical Systems (ICCPS'2024)
\\
  Deep Brain Stimulation (DBS) stands as an effective intervention for
alleviating the motor symptoms of Parkinson's disease (PD). Traditional
commercial DBS devices are only able to deliver fixed-frequency periodic pulses
to the basal ganglia (BG) regions of the brain, i.e., continuous DBS (cDBS).
However, they in general suffer from energy inefficiency and side effects, such
as speech impairment. Recent research has focused on adaptive DBS (aDBS) to
resolve the limitations of cDBS. Specifically, reinforcement learning (RL)
based approaches have been developed to adapt the frequencies of the stimuli in
order to achieve both energy efficiency and treatment efficacy. However, RL
approaches in general require significant amount of training data and
computational resources, making it intractable to integrate RL policies into
real-time embedded systems as needed in aDBS. In contrast, contextual
multi-armed bandits (CMAB) in general lead to better sample efficiency compared
to RL. In this study, we propose a CMAB solution for aDBS. Specifically, we
define the context as the signals capturing irregular neuronal firing
activities in the BG regions (i.e., beta-band power spectral density), while
each arm signifies the (discretized) pulse frequency of the stimulation.
Moreover, an {\epsilon}-exploring strategy is introduced on top of the classic
Thompson sampling method, leading to an algorithm called {\epsilon}-Neural
Thompson sampling ({\epsilon}-NeuralTS), such that the learned CMAB policy can
better balance exploration and exploitation of the BG environment. The
{\epsilon}-NeuralTS algorithm is evaluated using a computation BG model that
captures the neuronal activities in PD patients' brains. The results show that
our method outperforms both existing cDBS methods and CMAB baselines.
\\ ( https://arxiv.org/abs/2403.06814 ,  1760kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06826
Date: Mon, 11 Mar 2024 15:43:14 GMT   (989kb,D)

Title: In-context Exploration-Exploitation for Reinforcement Learning
Authors: Zhenwen Dai, Federico Tomasi, Sina Ghiassian
Categories: cs.LG cs.AI stat.ML
Comments: Published at ICLR 2024
\\
  In-context learning is a promising approach for online policy learning of
offline reinforcement learning (RL) methods, which can be achieved at inference
time without gradient optimization. However, this method is hindered by
significant computational costs resulting from the gathering of large training
trajectory sets and the need to train large Transformer models. We address this
challenge by introducing an In-context Exploration-Exploitation (ICEE)
algorithm, designed to optimize the efficiency of in-context policy learning.
Unlike existing models, ICEE performs an exploration-exploitation trade-off at
inference time within a Transformer model, without the need for explicit
Bayesian inference. Consequently, ICEE can solve Bayesian optimization problems
as efficiently as Gaussian process biased methods do, but in significantly less
time. Through experiments in grid world environments, we demonstrate that ICEE
can learn to solve new RL tasks using only tens of episodes, marking a
substantial improvement over the hundreds of episodes needed by the previous
in-context learning method.
\\ ( https://arxiv.org/abs/2403.06826 ,  989kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06829
Date: Mon, 11 Mar 2024 15:44:40 GMT   (3541kb,D)

Title: Constructing Variables Using Classifiers as an Aid to Regression: An
  Empirical Assessment
Authors: Colin Troisemaine, Vincent Lemaire
Categories: cs.LG
\\
  This paper proposes a method for the automatic creation of variables (in the
case of regression) that complement the information contained in the initial
input vector. The method works as a pre-processing step in which the continuous
values of the variable to be regressed are discretized into a set of intervals
which are then used to define value thresholds. Then classifiers are trained to
predict whether the value to be regressed is less than or equal to each of
these thresholds. The different outputs of the classifiers are then
concatenated in the form of an additional vector of variables that enriches the
initial vector of the regression problem. The implemented system can thus be
considered as a generic pre-processing tool. We tested the proposed enrichment
method with 5 types of regressors and evaluated it in 33 regression datasets.
Our experimental results confirm the interest of the approach.
\\ ( https://arxiv.org/abs/2403.06829 ,  3541kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06833
Date: Mon, 11 Mar 2024 15:48:56 GMT   (181kb,D)

Title: Can LLMs Separate Instructions From Data? And What Do We Even Mean By
  That?
Authors: Egor Zverev, Sahar Abdelnabi, Mario Fritz, Christoph H. Lampert
Categories: cs.LG cs.CL
Comments: Accepted for ICLR 2024 Workshop on Secure and Trustworthy Large
  Language Models, GitHub:
  https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed. 5 pages main
  text, 17 pages in total
\\
  Instruction-tuned Large Language Models (LLMs) have achieved breakthrough
results, opening countless new possibilities for many practical applications.
However, LLMs lack elementary safety features that are established norms in
other areas of computer science, such as the separation between instructions
and data, causing them to malfunction or rendering them vulnerable to
manipulation and interference by third parties e.g., via indirect
prompt/command injection. Even worse, so far, there is not even an established
definition of what precisely such a separation would mean and how its violation
could be tested. In this work, we aim to close this gap. We introduce a formal
measure to quantify the phenomenon of instruction-data separation as well as an
empirical variant of the measure that can be computed from a model`s black-box
outputs. We also introduce a new dataset, SEP (Should it be Executed or
Processed?), which allows estimating the measure, and we report results on
several state-of-the-art open-source and closed LLMs. Finally, we
quantitatively demonstrate that all evaluated LLMs fail to achieve a high
amount of separation, according to our measure. The source code and SEP dataset
are openly accessible at
https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.
\\ ( https://arxiv.org/abs/2403.06833 ,  181kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06854
Date: Mon, 11 Mar 2024 16:09:39 GMT   (382kb,D)

Title: Quantifying the Sensitivity of Inverse Reinforcement Learning to
  Misspecification
Authors: Joar Skalse and Alessandro Abate
Categories: cs.LG
\\
  Inverse reinforcement learning (IRL) aims to infer an agent's preferences
(represented as a reward function $R$) from their behaviour (represented as a
policy $\pi$). To do this, we need a behavioural model of how $\pi$ relates to
$R$. In the current literature, the most common behavioural models are
optimality, Boltzmann-rationality, and causal entropy maximisation. However,
the true relationship between a human's preferences and their behaviour is much
more complex than any of these behavioural models. This means that the
behavioural models are misspecified, which raises the concern that they may
lead to systematic errors if applied to real data. In this paper, we analyse
how sensitive the IRL problem is to misspecification of the behavioural model.
Specifically, we provide necessary and sufficient conditions that completely
characterise how the observed data may differ from the assumed behavioural
model without incurring an error above a given threshold. In addition to this,
we also characterise the conditions under which a behavioural model is robust
to small perturbations of the observed policy, and we analyse how robust many
behavioural models are to misspecification of their parameter values (such as
e.g.\ the discount rate). Our analysis suggests that the IRL problem is highly
sensitive to misspecification, in the sense that very mild misspecification can
lead to very large errors in the inferred reward function.
\\ ( https://arxiv.org/abs/2403.06854 ,  382kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06860
Date: Mon, 11 Mar 2024 16:13:58 GMT   (7006kb,D)

Title: A Geospatial Approach to Predicting Desert Locust Breeding Grounds in
  Africa
Authors: Ibrahim Salihu Yusuf, Mukhtar Opeyemi Yusuf, Kobby Panford-Quainoo,
  Arnu Pretorius
Categories: cs.LG cs.CV
\\
  Desert locust swarms present a major threat to agriculture and food security.
Addressing this challenge, our study develops an operationally-ready model for
predicting locust breeding grounds, which has the potential to enhance early
warning systems and targeted control measures. We curated a dataset from the
United Nations Food and Agriculture Organization's (UN-FAO) locust observation
records and analyzed it using two types of spatio-temporal input features:
remotely-sensed environmental and climate data as well as multi-spectral earth
observation images. Our approach employed custom deep learning models
(three-dimensional and LSTM-based recurrent convolutional networks), along with
the geospatial foundational model Prithvi recently released by Jakubik et al.,
2023. These models notably outperformed existing baselines, with the
Prithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized
Landsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and
ROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant finding
from our research is that multi-spectral earth observation images alone are
sufficient for effective locust breeding ground prediction without the need to
explicitly incorporate climatic or environmental features.
\\ ( https://arxiv.org/abs/2403.06860 ,  7006kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06869
Date: Mon, 11 Mar 2024 16:22:41 GMT   (11311kb,D)

Title: Learning with Noisy Foundation Models
Authors: Hao Chen, Jindong Wang, Zihan Wang, Ran Tao, Hongxin Wei, Xing Xie,
  Masashi Sugiyama, Bhiksha Raj
Categories: cs.LG cs.AI cs.CL cs.CV
Comments: 18 pages, 10 figures, 6 tables, preprint. arXiv admin note:
  substantial text overlap with arXiv:2309.17002
\\
  Foundation models are usually pre-trained on large-scale datasets and then
adapted to downstream tasks through tuning. However, the large-scale
pre-training datasets, often inaccessible or too expensive to handle, can
contain label noise that may adversely affect the generalization of the model
and pose unexpected risks. This paper stands out as the first work to
comprehensively understand and analyze the nature of noise in pre-training
datasets and then effectively mitigate its impacts on downstream tasks.
Specifically, through extensive experiments of fully-supervised and image-text
contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M
datasets, we demonstrate that, while slight noise in pre-training can benefit
in-domain (ID) performance, where the training and testing data share a similar
distribution, it always deteriorates out-of-domain (OOD) performance, where
training and testing distributions are significantly different. These
observations are agnostic to scales of pre-training datasets, pre-training
noise types, model architectures, pre-training objectives, downstream tuning
methods, and downstream applications. We empirically ascertain that the reason
behind this is that the pre-training noise shapes the feature space
differently. We then propose a tuning method (NMTune) to affine the feature
space to mitigate the malignant effect of noise and improve generalization,
which is applicable in both parameter-efficient and black-box tuning manners.
We additionally conduct extensive experiments on popular vision and language
models, including APIs, which are supervised and self-supervised pre-trained on
realistic noisy data for evaluation. Our analysis and results demonstrate the
importance of this novel and fundamental research direction, which we term as
Noisy Model Learning.
\\ ( https://arxiv.org/abs/2403.06869 ,  11311kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06870
Date: Mon, 11 Mar 2024 16:23:38 GMT   (849kb,D)

Title: Semantic Residual Prompts for Continual Learning
Authors: Martin Menabue, Emanuele Frascaroli, Matteo Boschini, Enver Sangineto,
  Lorenzo Bonicelli, Angelo Porrello, Simone Calderara
Categories: cs.LG
Comments: 17 pages, 3 figures
\\
  Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained
model and focus training on a few parameter vectors termed prompts. Most of
these methods organize these vectors in a pool of key-value pairs, and use the
input image as query to retrieve the prompts (values). However, as keys are
learned while tasks progress, the prompting selection strategy is itself
subject to catastrophic forgetting, an issue often overlooked by existing
approaches. For instance, prompts introduced to accommodate new tasks might end
up interfering with previously learned prompts. To make the selection strategy
more stable, we ask a foundational model (CLIP) to select our prompt within a
two-level adaptation mechanism. Specifically, the first level leverages
standard textual prompts for the CLIP textual encoder, leading to stable class
prototypes. The second level, instead, uses these prototypes along with the
query image as keys to index a second pool. The retrieved prompts serve to
adapt a pre-trained ViT, granting plasticity. In doing so, we also propose a
novel residual mechanism to transfer CLIP semantics to the ViT layers. Through
extensive analysis on established CL benchmarks, we show that our method
significantly outperforms both state-of-the-art CL approaches and the zero-shot
CLIP test. Notably, our findings hold true even for datasets with a substantial
domain gap w.r.t. the pre-training knowledge of the backbone model, as
showcased by experiments on satellite imagery and medical datasets.
\\ ( https://arxiv.org/abs/2403.06870 ,  849kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06871
Date: Mon, 11 Mar 2024 16:23:42 GMT   (232kb,D)

Title: On the Generalization Ability of Unsupervised Pretraining
Authors: Yuyang Deng, Junyuan Hong, Jiayu Zhou, Mehrdad Mahdavi
Categories: cs.LG stat.ML
\\
  Recent advances in unsupervised learning have shown that unsupervised
pre-training, followed by fine-tuning, can improve model generalization.
However, a rigorous understanding of how the representation function learned on
an unlabeled dataset affects the generalization of the fine-tuned model is
lacking. Existing theoretical research does not adequately account for the
heterogeneity of the distribution and tasks in pre-training and fine-tuning
stage. To bridge this gap, this paper introduces a novel theoretical framework
that illuminates the critical factor influencing the transferability of
knowledge acquired during unsupervised pre-training to the subsequent
fine-tuning phase, ultimately affecting the generalization capabilities of the
fine-tuned model on downstream tasks. We apply our theoretical framework to
analyze generalization bound of two distinct scenarios: Context Encoder
pre-training with deep neural networks and Masked Autoencoder pre-training with
deep transformers, followed by fine-tuning on a binary classification task.
Finally, inspired by our findings, we propose a novel regularization method
during pre-training to further enhances the generalization of fine-tuned model.
Overall, our results contribute to a better understanding of unsupervised
pre-training and fine-tuning paradigm, and can shed light on the design of more
effective pre-training algorithms.
\\ ( https://arxiv.org/abs/2403.06871 ,  232kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06880
Date: Mon, 11 Mar 2024 16:34:23 GMT   (13840kb,D)

Title: Unveiling the Significance of Toddler-Inspired Reward Transition in
  Goal-Oriented Reinforcement Learning
Authors: Junseok Park, Yoonsung Kim, Hee Bin Yoo, Min Whoo Lee, Kibeom Kim,
  Won-Seok Choi, Minsu Lee, Byoung-Tak Zhang
Categories: cs.LG cs.AI
Comments: Accepted as a full paper at AAAI 2024 (Oral presentation): The 38th
  Annual AAAI Conference on Artificial Intelligence (Main Tech Track). 7 pages
  (main paper), 2 pages (references), 17 pages (appendix) each
ACM-class: I.2.0
\\
  Toddlers evolve from free exploration with sparse feedback to exploiting
prior experiences for goal-directed learning with denser rewards. Drawing
inspiration from this Toddler-Inspired Reward Transition, we set out to explore
the implications of varying reward transitions when incorporated into
Reinforcement Learning (RL) tasks. Central to our inquiry is the transition
from sparse to potential-based dense rewards, which share optimal strategies
regardless of reward changes. Through various experiments, including those in
egocentric navigation and robotic arm manipulation tasks, we found that proper
reward transitions significantly influence sample efficiency and success rates.
Of particular note is the efficacy of the toddler-inspired Sparse-to-Dense
(S2D) transition. Beyond these performance metrics, using Cross-Density
Visualizer technique, we observed that transitions, especially the S2D, smooth
the policy loss landscape, promoting wide minima that enhance generalization in
RL models.
\\ ( https://arxiv.org/abs/2403.06880 ,  13840kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06903
Date: Mon, 11 Mar 2024 16:56:01 GMT   (62kb)

Title: Benign overfitting in leaky ReLU networks with moderate input dimension
Authors: Kedar Karhadkar, Erin George, Michael Murray, Guido Mont\'ufar, Deanna
  Needell
Categories: cs.LG stat.ML
Comments: 36 pages
\\
  The problem of benign overfitting asks whether it is possible for a model to
perfectly fit noisy training data and still generalize well. We study benign
overfitting in two-layer leaky ReLU networks trained with the hinge loss on a
binary classification task. We consider input data which can be decomposed into
the sum of a common signal and a random noise component, which lie on subspaces
orthogonal to one another. We characterize conditions on the signal to noise
ratio (SNR) of the model parameters giving rise to benign versus non-benign, or
harmful, overfitting: in particular, if the SNR is high then benign overfitting
occurs, conversely if the SNR is low then harmful overfitting occurs. We
attribute both benign and non-benign overfitting to an approximate margin
maximization property and show that leaky ReLU networks trained on hinge loss
with Gradient Descent (GD) satisfy this property. In contrast to prior work we
do not require near orthogonality conditions on the training data: notably, for
input dimension $d$ and training sample size $n$, while prior work shows
asymptotically optimal error when $d = \Omega(n^2 \log n)$, here we require
only $d = \Omega\left(n \log \frac{1}{\epsilon}\right)$ to obtain error within
$\epsilon$ of optimal.
\\ ( https://arxiv.org/abs/2403.06903 ,  62kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06906
Date: Mon, 11 Mar 2024 16:57:20 GMT   (450kb,D)

Title: Cost-Sensitive Learning to Defer to Multiple Experts with Workload
  Constraints
Authors: Jean V. Alves, Diogo Leit\~ao, S\'ergio Jesus, Marco O. P. Sampaio,
  Javier Li\'ebana, Pedro Saleiro, M\'ario A. T. Figueiredo, Pedro Bizarro
Categories: cs.LG cs.AI
\\
  Learning to defer (L2D) aims to improve human-AI collaboration systems by
learning how to defer decisions to humans when they are more likely to be
correct than an ML classifier. Existing research in L2D overlooks key aspects
of real-world systems that impede its practical adoption, namely: i) neglecting
cost-sensitive scenarios, where type 1 and type 2 errors have different costs;
ii) requiring concurrent human predictions for every instance of the training
dataset and iii) not dealing with human work capacity constraints. To address
these issues, we propose the deferral under cost and capacity constraints
framework (DeCCaF). DeCCaF is a novel L2D approach, employing supervised
learning to model the probability of human error under less restrictive data
requirements (only one expert prediction per instance) and using constraint
programming to globally minimize the error cost subject to workload
limitations. We test DeCCaF in a series of cost-sensitive fraud detection
scenarios with different teams of 9 synthetic fraud analysts, with individual
work capacity constraints. The results demonstrate that our approach performs
significantly better than the baselines in a wide array of scenarios, achieving
an average 8.4% reduction in the misclassification cost.
\\ ( https://arxiv.org/abs/2403.06906 ,  450kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06925
Date: Mon, 11 Mar 2024 17:12:09 GMT   (8461kb,D)

Title: Simplicity Bias of Transformers to Learn Low Sensitivity Functions
Authors: Bhavya Vasudeva, Deqing Fu, Tianyi Zhou, Elliott Kau, Youqi Huang,
  Vatsal Sharan
Categories: cs.LG cs.AI cs.CL stat.ML
Comments: 24 pages, 19 figures, 3 tables
\\
  Transformers achieve state-of-the-art accuracy and robustness across many
tasks, but an understanding of the inductive biases that they have and how
those biases are different from other neural network architectures remains
elusive. Various neural network architectures such as fully connected networks
have been found to have a simplicity bias towards simple functions of the data;
one version of this simplicity bias is a spectral bias to learn simple
functions in the Fourier space. In this work, we identify the notion of
sensitivity of the model to random changes in the input as a notion of
simplicity bias which provides a unified metric to explain the simplicity and
spectral bias of transformers across different data modalities. We show that
transformers have lower sensitivity than alternative architectures, such as
LSTMs, MLPs and CNNs, across both vision and language tasks. We also show that
low-sensitivity bias correlates with improved robustness; furthermore, it can
also be used as an efficient intervention to further improve the robustness of
transformers.
\\ ( https://arxiv.org/abs/2403.06925 ,  8461kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06936
Date: Mon, 11 Mar 2024 17:21:39 GMT   (227kb,D)

Title: Counterfactual Reasoning with Knowledge Graph Embeddings
Authors: Lena Zellinger, Andreas Stephan, Benjamin Roth
Categories: cs.LG cs.AI cs.CL
Comments: Accepted to EACL 2024
\\
  Knowledge graph embeddings (KGEs) were originally developed to infer true but
missing facts in incomplete knowledge repositories. In this paper, we link
knowledge graph completion and counterfactual reasoning via our new task CFKGR.
We model the original world state as a knowledge graph, hypothetical scenarios
as edges added to the graph, and plausible changes to the graph as inferences
from logical rules. We create corresponding benchmark datasets, which contain
diverse hypothetical scenarios with plausible changes to the original knowledge
graph and facts that should be retained. We develop COULDD, a general method
for adapting existing knowledge graph embeddings given a hypothetical premise,
and evaluate it on our benchmark. Our results indicate that KGEs learn patterns
in the graph without explicit training. We further observe that KGEs adapted
with COULDD solidly detect plausible counterfactual changes to the graph that
follow these patterns. An evaluation on human-annotated data reveals that KGEs
adapted with COULDD are mostly unable to recognize changes to the graph that do
not follow learned inference rules. In contrast, ChatGPT mostly outperforms
KGEs in detecting plausible changes to the graph but has poor knowledge
retention. In summary, CFKGR connects two previously distinct areas, namely KG
completion and counterfactual reasoning.
\\ ( https://arxiv.org/abs/2403.06936 ,  227kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06966
Date: Mon, 11 Mar 2024 17:49:18 GMT   (6712kb,D)

Title: Acquiring Diverse Skills using Curriculum Reinforcement Learning with
  Mixture of Experts
Authors: Onur Celik, Aleksandar Taranovic, Gerhard Neumann
Categories: cs.LG cs.RO
\\
  Reinforcement learning (RL) is a powerful approach for acquiring a
good-performing policy. However, learning diverse skills is challenging in RL
due to the commonly used Gaussian policy parameterization. We propose
\textbf{Di}verse \textbf{Skil}l \textbf{L}earning (Di-SkilL), an RL method for
learning diverse skills using Mixture of Experts, where each expert formalizes
a skill as a contextual motion primitive. Di-SkilL optimizes each expert and
its associate context distribution to a maximum entropy objective that
incentivizes learning diverse skills in similar contexts. The per-expert
context distribution enables automatic curricula learning, allowing each expert
to focus on its best-performing sub-region of the context space. To overcome
hard discontinuities and multi-modalities without any prior knowledge of the
environment's unknown context probability space, we leverage energy-based
models to represent the per-expert context distributions and demonstrate how we
can efficiently train them using the standard policy gradient objective. We
show on challenging robot simulation tasks that Di-SkilL can learn diverse and
performant skills.
\\ ( https://arxiv.org/abs/2403.06966 ,  6712kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06971
Date: Mon, 11 Mar 2024 17:54:42 GMT   (420kb)

Title: A representation-learning game for classes of prediction tasks
Authors: Neria Uzan and Nir Weinberger
Categories: cs.LG cs.IT math.IT
Comments: ICLR 2024
\\
  We propose a game-based formulation for learning dimensionality-reducing
representations of feature vectors, when only a prior knowledge on future
prediction tasks is available. In this game, the first player chooses a
representation, and then the second player adversarially chooses a prediction
task from a given class, representing the prior knowledge. The first player
aims is to minimize, and the second player to maximize, the regret: The minimal
prediction loss using the representation, compared to the same loss using the
original features. For the canonical setting in which the representation, the
response to predict and the predictors are all linear functions, and under the
mean squared error loss function, we derive the theoretically optimal
representation in pure strategies, which shows the effectiveness of the prior
knowledge, and the optimal regret in mixed strategies, which shows the
usefulness of randomizing the representation. For general representations and
loss functions, we propose an efficient algorithm to optimize a randomized
representation. The algorithm only requires the gradients of the loss function,
and is based on incrementally adding a representation rule to a mixture of such
rules.
\\ ( https://arxiv.org/abs/2403.06971 ,  420kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2403.05541 (*cross-listing*)
Date: Sat, 3 Feb 2024 02:14:47 GMT   (1094kb)

Title: AI in ESG for Financial Institutions: An Industrial Survey
Authors: Jun Xu
Categories: cs.CY cs.AI cs.LG q-fin.CP
Comments: 31 pages, 14 tables, 3 figures
\\
  The burgeoning integration of Artificial Intelligence (AI) into
Environmental, Social, and Governance (ESG) initiatives within the financial
sector represents a paradigm shift towards more sus-tainable and equitable
financial practices. This paper surveys the industrial landscape to delineate
the necessity and impact of AI in bolstering ESG frameworks. With the advent of
stringent regulatory requirements and heightened stakeholder awareness,
financial institutions (FIs) are increasingly compelled to adopt ESG criteria.
AI emerges as a pivotal tool in navigating the complex in-terplay of financial
activities and sustainability goals. Our survey categorizes AI applications
across three main pillars of ESG, illustrating how AI enhances analytical
capabilities, risk assessment, customer engagement, reporting accuracy and
more. Further, we delve into the critical con-siderations surrounding the use
of data and the development of models, underscoring the importance of data
quality, privacy, and model robustness. The paper also addresses the imperative
of responsible and sustainable AI, emphasizing the ethical dimensions of AI
deployment in ESG-related banking processes. Conclusively, our findings suggest
that while AI offers transformative potential for ESG in banking, it also poses
significant challenges that necessitate careful consideration. The final part
of the paper synthesizes the survey's insights, proposing a forward-looking
stance on the adoption of AI in ESG practices. We conclude with recommendations
with a reference architecture for future research and development, advocating
for a balanced approach that leverages AI's strengths while mitigating its
risks within the ESG domain.
\\ ( https://arxiv.org/abs/2403.05541 ,  1094kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05544 (*cross-listing*)
Date: Mon, 5 Feb 2024 16:12:14 GMT   (397kb)

Title: From Algorithm Worship to the Art of Human Learning: Insights from
  50-year journey of AI in Education
Authors: Kaska Porayska-Pomsta
Categories: cs.CY cs.AI
Comments: 12 pages; opinion piece
\\
  Current discourse surrounding Artificial Intelligence (AI) oscillates between
hope and apprehension, painting a future where AI reshapes every facet of human
life, including Education. This paper delves into the complexities of AI's role
in Education, addressing the mixed messages that have both enthused and alarmed
educators, policymakers, and the public. It explores the promises that AI holds
for enhancing learning through personalisation at scale, against the backdrop
of concerns about ethical implications, the devaluation of non-STEM subjects,
and the potential transformative impact on our neurocognitive and
socio-emotional functioning. Drawing on recent research and global discourse,
the paper seeks to unpack the reasons behind the vagueness of current
discussions on AI in Education (AIED) and the implications of this ambiguity
for future educational practices and policies. By highlighting insights from
educational research and synthesising evidence-based best practices in AIED,
the aim is to provide a clearer understanding of how AI technologies can be
aligned with the fundamental principles of learning and teaching, and explore
what concrete actions may need to be prioritised now to truly enhance learning
experiences and outcomes for all in the future.
\\ ( https://arxiv.org/abs/2403.05544 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05547 (*cross-listing*)
Date: Tue, 6 Feb 2024 17:26:24 GMT   (2492kb,D)

Title: AI for non-programmers: Applied AI in the lectures for students without
  programming skills
Authors: Julius Sch\"oning and Tim Wawer and Kai-Michael Griese
Categories: cs.CY cs.AI cs.LG
Comments: 10 pages, 6 figures, Translated from the German of "KI f\"ur
  Nicht-Programmierer*innen: Angewandte KI im H\"orsaal f\"ur Studierende ohne
  Programmierkenntnisse". Translated from the German of
  https://nbn-resolving.org/urn:nbn:de:bsz:959-opus-52866
ACM-class: K.3.2; I.2.0
Journal-ref: Voneinander Lehren lernen (5) (2024)
\\
  Applications such as ChatGPT and WOMBO Dream make it easy to inspire students
without programming knowledge to use artificial intelligence (AI). Therefore,
given the increasing importance of AI in all disciplines, innovative strategies
are needed to educate students in AI without programming knowledge so that AI
can be integrated into their study modules as a future skill. This work
presents a didactic planning script for applied AI. The didactic planning
script is based on the AI application pipeline and links AI concepts with
study-relevant topics. These linkages open up a new solution space and promote
students' interest in and understanding of the potentials and risks of AI. An
example lecture series for master students in energy management shows how AI
can be seamlessly integrated into discipline-specific lectures. To this end,
the planning script for applied AI is adapted to fit the study programs' topic.
This specific teaching scenario enables students to solve a discipline-specific
task step by step using the AI application pipeline. Thus, the application of
the didactic planning script for applied AI shows the practical implementation
of the theoretical concepts of AI. In addition, a checklist is presented that
can be used to assess whether AI can be used in the discipline-specific
lecture. AI as a future skill must be learned by students based on use cases
that are relevant to the course of studies. For this reason, AI education
should fit seamlessly into various curricula, even if the students do not have
a programming background due to their field of study.
\\ ( https://arxiv.org/abs/2403.05547 ,  2492kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05548 (*cross-listing*)
Date: Tue, 6 Feb 2024 20:34:49 GMT   (3738kb,D)

Title: Monitoring the evolution of antisemitic discourse on extremist social
  media using BERT
Authors: Raza Ul Mustafa and Nathalie Japkowicz
Categories: cs.CY cs.AI cs.CL cs.IR cs.LG cs.SI
Comments: 11 pages; 4 figures; 4 pages
\\
  Racism and intolerance on social media contribute to a toxic online
environment which may spill offline to foster hatred, and eventually lead to
physical violence. That is the case with online antisemitism, the specific
category of hatred considered in this study. Tracking antisemitic themes and
their associated terminology over time in online discussions could help monitor
the sentiments of their participants and their evolution, and possibly offer
avenues for intervention that may prevent the escalation of hatred. Due to the
large volume and constant evolution of online traffic, monitoring conversations
manually is impractical. Instead, we propose an automated method that extracts
antisemitic themes and terminology from extremist social media over time and
captures their evolution. Since supervised learning would be too limited for
such a task, we created an unsupervised online machine learning approach that
uses large language models to assess the contextual similarity of posts. The
method clusters similar posts together, dividing, and creating additional
clusters over time when sub-themes emerge from existing ones or new themes
appear. The antisemitic terminology used within each theme is extracted from
the posts in each cluster. Our experiments show that our methodology
outperforms existing baselines and demonstrates the kind of themes and
sub-themes it discovers within antisemitic discourse along with their
associated terminology. We believe that our approach will be useful for
monitoring the evolution of all kinds of hatred beyond antisemitism on social
platforms.
\\ ( https://arxiv.org/abs/2403.05548 ,  3738kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05550 (*cross-listing*)
Date: Wed, 7 Feb 2024 15:50:54 GMT   (1270kb,D)

Title: Teranga Go!: Carpooling Collaborative Consumption Community with
  multi-criteria hesitant fuzzy linguistic term set opinions to build
  confidence and trust
Authors: Rosana Montes, Ana M. Sanchez, Pedro Villar, Francisco Herrera
Categories: cs.CY cs.AI
Comments: project at https://github.com/rosanamontes/teranga.go. arXiv admin
  note: substantial text overlap with arXiv:2402.01775
Journal-ref: Applied Soft Computing 67, 2018, Pages 941-952
DOI: 10.1016/j.asoc.2017.05.039
\\
  Classic Delphi and Fuzzy Delphi methods are used to test content validity of
a data collection tools such as questionnaires. Fuzzy Delphi takes the opinion
issued by judges from a linguistic perspective reducing ambiguity in opinions
by using fuzzy numbers. We propose an extension named 2-Tuple Fuzzy Linguistic
Delphi method to deal with scenarios in which judges show different expertise
degrees by using fuzzy multigranular semantics of the linguistic terms and to
obtain intermediate and final results expressed by 2-tuple linguistic values.
The key idea of our proposal is to validate the full questionnaire by means of
the evaluation of its parts, defining the validity of each item as a Decision
Making problem. Taking the opinion of experts, we measure the degree of
consensus, the degree of consistency, and the linguistic score of each item, in
order to detect those items that affect, positively or negatively, the quality
of the instrument. Considering the real need to evaluate a b-learning
educational experience with a consensual questionnaire, we present a Decision
Making model for questionnaire validation that solve it. Additionally, we
contribute to this consensus reaching problem by developing an online tool
under GPL v3 license. The software visualizes the collective valuations for
each iteration and assists to determine which parts of the questionnaire should
be modified to reach a consensual solution.
\\ ( https://arxiv.org/abs/2403.05550 ,  1270kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05552 (*cross-listing*)
Date: Thu, 8 Feb 2024 21:29:41 GMT   (1044kb)

Title: Multi-source and multimodal data fusion for predicting academic
  performance in blended learning university courses
Authors: W. Chango, R. Cerezo, and C. Romero
Categories: cs.CY cs.AI cs.LG
Journal-ref: Chango, W., Cerezo, R., & Romero, C. (2021). Multi-source and
  multimodal data fusion for predicting academic performance in blended
  learning university courses. Computers & Electrical Engineering, 89, 106908
DOI: 10.1016/j.compeleceng.2020.106908
\\
  In this paper we applied data fusion approaches for predicting the final
academic performance of university students using multiple-source, multimodal
data from blended learning environments. We collected and preprocessed data
about first-year university students from different sources: theory classes,
practical sessions, on-line Moodle sessions, and a final exam. Our objective
was to discover which data fusion approach produced the best results using our
data. We carried out experiments by applying four different data fusion
approaches and six classification algorithms. The results showed that the best
predictions were produced using ensembles and selecting the best attributes
approach with discretized data. The best prediction models showed us that the
level of attention in theory classes, scores in Moodle quizzes, and the level
of activity in Moodle forums were the best set of attributes for predicting
students' final performance in our courses.
\\ ( https://arxiv.org/abs/2403.05552 ,  1044kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05562 (*cross-listing*)
Date: Tue, 20 Feb 2024 08:20:08 GMT   (25449kb,D)

Title: SDXL Finetuned with LoRA for Coloring Therapy: Generating Graphic
  Templates Inspired by United Arab Emirates Culture
Authors: Abdulla Alfalasi, Esrat Khan, Mohamed Alhashmi, Raed Aldweik, Davor
  Svetinovic
Categories: cs.HC cs.AI cs.CY
\\
  A transformative approach to mental health therapy lies at the crossroads of
cultural heritage and advanced technology. This paper introduces an innovative
method that fuses machine learning techniques with traditional Emirati motifs,
focusing on the United Arab Emirates (UAE). We utilize the Stable Diffusion XL
(SDXL) model, enhanced with Low-Rank Adaptation (LoRA), to create culturally
significant coloring templates featuring Al-Sadu weaving patterns. This novel
approach leverages coloring therapy for its recognized stress-relieving
benefits and embeds deep cultural resonance, making it a potent tool for
therapeutic intervention and cultural preservation. Specifically targeting
Generalized Anxiety Disorder (GAD), our method demonstrates significant
potential in reducing associated symptoms. Additionally, the paper delves into
the broader implications of color and music therapy, emphasizing the importance
of culturally tailored content. The technical aspects of the SDXL model and its
LoRA fine-tuning showcase its capability to generate high-quality, culturally
specific images. This research stands at the forefront of integrating mental
wellness practices with cultural heritage, providing a groundbreaking
perspective on the synergy between technology, culture, and healthcare. In
future work, we aim to employ biosignals to assess the level of engagement and
effectiveness of color therapy. A key focus will be to examine the impact of
the Emirati heritage Al Sadu art on Emirati individuals and compare their
responses with those of other nationalities. This will provide deeper insights
into the cultural specificity of therapeutic interventions and further the
understanding of the unique interplay between cultural identity and mental
health therapy.
\\ ( https://arxiv.org/abs/2403.05562 ,  25449kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05565 (*cross-listing*)
Date: Tue, 20 Feb 2024 22:17:59 GMT   (763kb,D)

Title: OpenHEXAI: An Open-Source Framework for Human-Centered Evaluation of
  Explainable Machine Learning
Authors: Jiaqi Ma, Vivian Lai, Yiming Zhang, Chacha Chen, Paul Hamilton, Davor
  Ljubenkov, Himabindu Lakkaraju, Chenhao Tan
Categories: cs.HC cs.AI
\\
  Recently, there has been a surge of explainable AI (XAI) methods driven by
the need for understanding machine learning model behaviors in high-stakes
scenarios. However, properly evaluating the effectiveness of the XAI methods
inevitably requires the involvement of human subjects, and conducting
human-centered benchmarks is challenging in a number of ways: designing and
implementing user studies is complex; numerous design choices in the design
space of user study lead to problems of reproducibility; and running user
studies can be challenging and even daunting for machine learning researchers.
To address these challenges, this paper presents OpenHEXAI, an open-source
framework for human-centered evaluation of XAI methods. OpenHEXAI features (1)
a collection of diverse benchmark datasets, pre-trained models, and post hoc
explanation methods; (2) an easy-to-use web application for user study; (3)
comprehensive evaluation metrics for the effectiveness of post hoc explanation
methods in the context of human-AI decision making tasks; (4) best practice
recommendations of experiment documentation; and (5) convenient tools for power
analysis and cost estimation. OpenHEAXI is the first large-scale
infrastructural effort to facilitate human-centered benchmarks of XAI methods.
It simplifies the design and implementation of user studies for XAI methods,
thus allowing researchers and practitioners to focus on the scientific
questions. Additionally, it enhances reproducibility through standardized
designs. Based on OpenHEXAI, we further conduct a systematic benchmark of four
state-of-the-art post hoc explanation methods and compare their impacts on
human-AI decision making tasks in terms of accuracy, fairness, as well as
users' trust and understanding of the machine learning model.
\\ ( https://arxiv.org/abs/2403.05565 ,  763kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05572 (*cross-listing*)
Date: Thu, 22 Feb 2024 09:52:45 GMT   (11941kb,D)

Title: Is ChatGPT More Empathetic than Humans?
Authors: Anuradha Welivita and Pearl Pu
Categories: cs.HC cs.AI cs.CL
Comments: 21 pages, 16 figures
\\
  This paper investigates the empathetic responding capabilities of ChatGPT,
particularly its latest iteration, GPT-4, in comparison to human-generated
responses to a wide range of emotional scenarios, both positive and negative.
We employ a rigorous evaluation methodology, involving a between-groups study
with 600 participants, to evaluate the level of empathy in responses generated
by humans and ChatGPT. ChatGPT is prompted in two distinct ways: a standard
approach and one explicitly detailing empathy's cognitive, affective, and
compassionate counterparts. Our findings indicate that the average empathy
rating of responses generated by ChatGPT exceeds those crafted by humans by
approximately 10%. Additionally, instructing ChatGPT to incorporate a clear
understanding of empathy in its responses makes the responses align
approximately 5 times more closely with the expectations of individuals
possessing a high degree of empathy, compared to human responses. The proposed
evaluation framework serves as a scalable and adaptable framework to assess the
empathetic capabilities of newer and updated versions of large language models,
eliminating the need to replicate the current study's results in future
research.
\\ ( https://arxiv.org/abs/2403.05572 ,  11941kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05574 (*cross-listing*)
Date: Mon, 26 Feb 2024 09:10:34 GMT   (1684kb,D)

Title: HealMe: Harnessing Cognitive Reframing in Large Language Models for
  Psychotherapy
Authors: Mengxi Xiao, Qianqian Xie, Ziyan Kuang, Zhicheng Liu, Kailai Yang, Min
  Peng, Weiguang Han, Jimin Huang
Categories: cs.HC cs.AI cs.CL
Comments: 17 pages, 4 figures
ACM-class: J.4
\\
  Large Language Models (LLMs) can play a vital role in psychotherapy by
adeptly handling the crucial task of cognitive reframing and overcoming
challenges such as shame, distrust, therapist skill variability, and resource
scarcity. Previous LLMs in cognitive reframing mainly converted negative
emotions to positive ones, but these approaches have limited efficacy, often
not promoting clients' self-discovery of alternative perspectives. In this
paper, we unveil the Helping and Empowering through Adaptive Language in Mental
Enhancement (HealMe) model. This novel cognitive reframing therapy method
effectively addresses deep-rooted negative thoughts and fosters rational,
balanced perspectives. Diverging from traditional LLM methods, HealMe employs
empathetic dialogue based on psychotherapeutic frameworks. It systematically
guides clients through distinguishing circumstances from feelings,
brainstorming alternative viewpoints, and developing empathetic, actionable
suggestions. Moreover, we adopt the first comprehensive and expertly crafted
psychological evaluation metrics, specifically designed to rigorously assess
the performance of cognitive reframing, in both AI-simulated dialogues and
real-world therapeutic conversations. Experimental results show that our model
outperforms others in terms of empathy, guidance, and logical coherence,
demonstrating its effectiveness and potential positive impact on psychotherapy.
\\ ( https://arxiv.org/abs/2403.05574 ,  1684kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05576 (*cross-listing*)
Date: Tue, 27 Feb 2024 01:16:55 GMT   (789kb)

Title: Understanding Subjectivity through the Lens of Motivational Context in
  Model-Generated Image Satisfaction
Authors: Senjuti Dutta, Sherol Chen, Sunny Mak, Amnah Ahmad, Katherine Collins,
  Alena Butryna, Deepak Ramachandran, Krishnamurthy Dvijotham, Ellie Pavlick,
  Ravi Rajakumar
Categories: cs.HC cs.AI
\\
  Image generation models are poised to become ubiquitous in a range of
applications. These models are often fine-tuned and evaluated using human
quality judgments that assume a universal standard, failing to consider the
subjectivity of such tasks. To investigate how to quantify subjectivity, and
the scale of its impact, we measure how assessments differ among human
annotators across different use cases. Simulating the effects of ordinarily
latent elements of annotators subjectivity, we contrive a set of motivations
(t-shirt graphics, presentation visuals, and phone background images) to
contextualize a set of crowdsourcing tasks. Our results show that human
evaluations of images vary within individual contexts and across combinations
of contexts. Three key factors affecting this subjectivity are image
appearance, image alignment with text, and representation of objects mentioned
in the text. Our study highlights the importance of taking individual users and
contexts into account, both when building and evaluating generative models
\\ ( https://arxiv.org/abs/2403.05576 ,  789kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05578 (*cross-listing*)
Date: Wed, 28 Feb 2024 07:56:04 GMT   (4189kb,D)

Title: Chaining text-to-image and large language model: A novel approach for
  generating personalized e-commerce banners
Authors: Shanu Vashishtha, Abhinav Prakash, Lalitesh Morishetti, Kaushiki Nag,
  Yokila Arora, Sushant Kumar and Kannan Achan
Categories: cs.HC cs.AI cs.CV cs.IR cs.LG
Comments: 10 pages
\\
  Text-to-image models such as stable diffusion have opened a plethora of
opportunities for generating art. Recent literature has surveyed the use of
text-to-image models for enhancing the work of many creative artists. Many
e-commerce platforms employ a manual process to generate the banners, which is
time-consuming and has limitations of scalability. In this work, we demonstrate
the use of text-to-image models for generating personalized web banners with
dynamic content for online shoppers based on their interactions. The novelty in
this approach lies in converting users' interaction data to meaningful prompts
without human intervention. To this end, we utilize a large language model
(LLM) to systematically extract a tuple of attributes from item
meta-information. The attributes are then passed to a text-to-image model via
prompt engineering to generate images for the banner. Our results show that the
proposed approach can create high-quality personalized banners for users.
\\ ( https://arxiv.org/abs/2403.05578 ,  4189kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05579 (*cross-listing*)
Date: Wed, 28 Feb 2024 19:30:32 GMT   (939kb)

Title: Cultural Bias in Explainable AI Research: A Systematic Analysis
Authors: Uwe Peters, Mary Carman
Categories: cs.HC cs.AI cs.CY
\\
  For synergistic interactions between humans and artificial intelligence (AI)
systems, AI outputs often need to be explainable to people. Explainable AI
(XAI) systems are commonly tested in human user studies. However, whether XAI
researchers consider potential cultural differences in human explanatory needs
remains unexplored. We highlight psychological research that found significant
differences in human explanations between many people from Western, commonly
individualist countries and people from non-Western, often collectivist
countries. We argue that XAI research currently overlooks these variations and
that many popular XAI designs implicitly and problematically assume that
Western explanatory needs are shared cross-culturally. Additionally, we
systematically reviewed over 200 XAI user studies and found that most studies
did not consider relevant cultural variations, sampled only Western
populations, but drew conclusions about human-XAI interactions more generally.
We also analyzed over 30 literature reviews of XAI studies. Most reviews did
not mention cultural differences in explanatory needs or flag overly broad
cross-cultural extrapolations of XAI user study results. Combined, our analyses
provide evidence of a cultural bias toward Western populations in XAI research,
highlighting an important knowledge gap regarding how culturally diverse users
may respond to widely used XAI systems that future work can and should address.
\\ ( https://arxiv.org/abs/2403.05579 ,  939kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05581 (*cross-listing*)
Date: Fri, 1 Mar 2024 13:25:54 GMT   (152kb,D)

Title: Can Interpretability Layouts Influence Human Perception of Offensive
  Sentences?
Authors: Thiago Freitas dos Santos, Nardine Osman, Marco Schorlemmer
Categories: cs.HC cs.AI cs.LG
\\
  This paper conducts a user study to assess whether three machine learning
(ML) interpretability layouts can influence participants' views when evaluating
sentences containing hate speech, focusing on the "Misogyny" and "Racism"
classes. Given the existence of divergent conclusions in the literature, we
provide empirical evidence on using ML interpretability in online communities
through statistical and qualitative analyses of questionnaire responses. The
Generalized Additive Model estimates participants' ratings, incorporating
within-subject and between-subject designs. While our statistical analysis
indicates that none of the interpretability layouts significantly influences
participants' views, our qualitative analysis demonstrates the advantages of ML
interpretability: 1) triggering participants to provide corrective feedback in
case of discrepancies between their views and the model, and 2) providing
insights to evaluate a model's behavior beyond traditional performance metrics.
\\ ( https://arxiv.org/abs/2403.05581 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05583 (*cross-listing*)
Date: Sat, 2 Mar 2024 21:15:24 GMT   (720kb,D)

Title: A Cross-Modal Approach to Silent Speech with LLM-Enhanced Recognition
Authors: Tyler Benster, Guy Wilson, Reshef Elisha, Francis R Willett, Shaul
  Druckmann
Categories: cs.HC cs.AI cs.SD eess.AS
\\
  Silent Speech Interfaces (SSIs) offer a noninvasive alternative to
brain-computer interfaces for soundless verbal communication. We introduce
Multimodal Orofacial Neural Audio (MONA), a system that leverages cross-modal
alignment through novel loss functions--cross-contrast (crossCon) and
supervised temporal contrast (supTcon)--to train a multimodal model with a
shared latent representation. This architecture enables the use of audio-only
datasets like LibriSpeech to improve silent speech recognition. Additionally,
our introduction of Large Language Model (LLM) Integrated Scoring Adjustment
(LISA) significantly improves recognition accuracy. Together, MONA LISA reduces
the state-of-the-art word error rate (WER) from 28.8% to 12.2% in the Gaddy
(2020) benchmark dataset for silent speech on an open vocabulary. For vocal EMG
recordings, our method improves the state-of-the-art from 23.3% to 3.7% WER. In
the Brain-to-Text 2024 competition, LISA performs best, improving the top WER
from 9.8% to 8.9%. To the best of our knowledge, this work represents the first
instance where noninvasive silent speech recognition on an open vocabulary has
cleared the threshold of 15% WER, demonstrating that SSIs can be a viable
alternative to automatic speech recognition (ASR). Our work not only narrows
the performance gap between silent and vocalized speech but also opens new
possibilities in human-computer interaction, demonstrating the potential of
cross-modal approaches in noisy and data-limited regimes.
\\ ( https://arxiv.org/abs/2403.05583 ,  720kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05584 (*cross-listing*)
Date: Sun, 3 Mar 2024 06:57:48 GMT   (27417kb,D)

Title: Time2Stop: Adaptive and Explainable Human-AI Loop for Smartphone Overuse
  Intervention
Authors: Adiba Orzikulova, Han Xiao, Zhipeng Li, Yukang Yan, Yuntao Wang,
  Yuanchun Shi, Marzyeh Ghassemi, Sung-Ju Lee, Anind K Dey, Xuhai "Orson" Xu
Categories: cs.HC cs.AI
DOI: 10.1145/3613904.3642747
\\
  Despite a rich history of investigating smartphone overuse intervention
techniques, AI-based just-in-time adaptive intervention (JITAI) methods for
overuse reduction are lacking. We develop Time2Stop, an intelligent, adaptive,
and explainable JITAI system that leverages machine learning to identify
optimal intervention timings, introduces interventions with transparent AI
explanations, and collects user feedback to establish a human-AI loop and adapt
the intervention model over time. We conducted an 8-week field experiment
(N=71) to evaluate the effectiveness of both the adaptation and explanation
aspects of Time2Stop. Our results indicate that our adaptive models
significantly outperform the baseline methods on intervention accuracy (>32.8\%
relatively) and receptivity (>8.0\%). In addition, incorporating explanations
further enhances the effectiveness by 53.8\% and 11.4\% on accuracy and
receptivity, respectively. Moreover, Time2Stop significantly reduces overuse,
decreasing app visit frequency by 7.0$\sim$8.9\%. Our subjective data also
echoed these quantitative measures. Participants preferred the adaptive
interventions and rated the system highly on intervention time accuracy,
effectiveness, and level of trust. We envision our work can inspire future
research on JITAI systems with a human-AI loop to evolve with users.
\\ ( https://arxiv.org/abs/2403.05584 ,  27417kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05585 (*cross-listing*)
Date: Sun, 3 Mar 2024 17:46:12 GMT   (4432kb)

Title: Plasmon Resonance Model: Investigation of Analysis of Fake News
  Diffusion Model with Third Mover Intervention Using Soliton Solution in
  Non-Complete Information Game under Repeated Dilemma Condition
Authors: Yasuko Kawahata
Categories: physics.soc-ph cs.AI
Comments: Plasmon Resonance Model, Soliton Solution, Third Mover,Fake News,
  Non-Complete Information Game, Nonlinear Partial Differential Equations,
  First Mover, Second Mover, Third Mover, Diffusion Dynamics, Iteration Dilemma
\\
  In this research note, we propose a new approach to model the fake news
diffusion process within the framework of incomplete information games. In
particular, we use nonlinear partial differential equations to represent the
phenomenon of plasmon resonance, in which the diffusion of fake news is rapidly
amplified within a particular social group or communication network, and
analyze its dynamics through a soliton solution approach. In addition, we
consider how first mover, second mover, and third mover strategies interact
within this nonlinear system and contribute to the amplification or suppression
of fake news diffusion. The model aims to understand the mechanisms of fake
news proliferation and provide insights into how to prevent or combat it. By
combining concepts from the social sciences and the physical sciences, this
study attempts to develop a new theoretical framework for the contemporary
problem of fake news.
\\ ( https://arxiv.org/abs/2403.05585 ,  4432kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05589 (*cross-listing*)
Date: Mon, 4 Mar 2024 17:44:18 GMT   (2388kb,D)

Title: Optimizing Computer Lab Ergonomics in Universities: A Study on
  Anthropometric Measurements, Furniture Design, and ANOVA Test
Authors: Anik Kumar Saha, Md Abrar Jahin, Md. Rafiquzzaman, and M. F. Mridha
Categories: cs.HC cs.AI
\\
  Many studies have shown how ergonomically designed furniture improves
productivity and well-being. As computers have become a part of students'
academic lives, they will grow further in the future. We propose
anthropometric-based furniture dimensions suitable for university students to
improve computer laboratory ergonomics. We collected data from 380 participants
and analyzed 11 anthropometric measurements, correlating them to 11 furniture
dimensions. Two types of furniture were studied: a non-adjustable chair with a
non-adjustable table and an adjustable chair with a non-adjustable table. The
mismatch calculation showed a significant difference between furniture
dimensions and anthropometric measurements. The one-way ANOVA test with a
significance level of 5% also showed a significant difference between proposed
and existing furniture dimensions. The proposed dimensions were found to be
more compatible and reduced mismatch percentages for both males and females
compared to existing furniture. The proposed dimensions of the furniture set
with adjustable seat height showed slightly improved results compared to the
non-adjustable furniture set. This suggests that the proposed dimensions can
improve comfort levels and reduce the risk of musculoskeletal disorders among
students. Further studies on the implementation and long-term effects of these
proposed dimensions in real-world computer laboratory settings are recommended.
\\ ( https://arxiv.org/abs/2403.05589 ,  2388kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05592 (*cross-listing*)
Date: Wed, 6 Mar 2024 13:23:57 GMT   (174kb)

Title: Eternal Sunshine of the Mechanical Mind: The Irreconcilability of
  Machine Learning and the Right to be Forgotten
Authors: Meem Arafat Manab
Categories: cs.GL cs.AI
MSC-class: 68P27
ACM-class: K.4.1; K.5.2; I.2.0
\\
  As we keep rapidly advancing toward an era where artificial intelligence is a
constant and normative experience for most of us, we must also be aware of what
this vision and this progress entail. By first approximating neural connections
and activities in computer circuits and then creating more and more
sophisticated versions of this crude approximation, we are now facing an age to
come where modern deep learning-based artificial intelligence systems can
rightly be called thinking machines, and they are sometimes even lauded for
their emergent behavior and black-box approaches. But as we create more
powerful electronic brains, with billions of neural connections and parameters,
can we guarantee that these mammoths built of artificial neurons will be able
to forget the data that we store in them? If they are at some level like a
brain, can the right to be forgotten still be protected while dealing with
these AIs? The essential gap between machine learning and the RTBF is explored
in this article, with a premonition of far-reaching conclusions if the gap is
not bridged or reconciled any time soon. The core argument is that deep
learning models, due to their structure and size, cannot be expected to forget
or delete a data as it would be expected from a tabular database, and they
should be treated more like a mechanical brain, albeit still in development.
\\ ( https://arxiv.org/abs/2403.05592 ,  174kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05593 (*cross-listing*)
Date: Wed, 6 Mar 2024 15:00:11 GMT   (1051kb)

Title: Introducing First-Principles Calculations: New Approach to Group
  Dynamics and Bridging Social Phenomena in TeNP-Chain Based Social Dynamics
  Simulations
Authors: Yasuko Kawahata
Categories: physics.soc-ph cs.AI physics.ed-ph
Comments: TeNP Chains, First-principles calculations, Tellurium nanoparticles
  (TeNPs), Graphene, Fake news dissemination, Social cohesion, Information Flow
  Disruption, Quantum Mechanics, Interdisciplinary approach, Misinformation
  mitigation
\\
  This note considers an innovative interdisciplinary methodology that bridges
the gap between the fundamental principles of quantum mechanics applied to the
study of materials such as tellurium nanoparticles (TeNPs) and graphene and the
complex dynamics of social systems. The basis for this approach lies in the
metaphorical parallels drawn between the structural features of TeNPs and
graphene and the behavioral patterns of social groups in the face of
misinformation. TeNPs exhibit unique properties such as the strengthening of
covalent bonds within telluric chains and the disruption of secondary structure
leading to the separation of these chains. This is analogous to increased
cohesion within social groups and disruption of information flow between
different subgroups, respectively. . Similarly, the outstanding properties of
graphene, such as high electrical conductivity, strength, and flexibility,
provide additional aspects for understanding the resilience and adaptability of
social structures in response to external stimuli such as fake news. This
research note proposes a novel metaphorical framework for analyzing the spread
of fake news within social groups, analogous to the structural features of
telluric nanoparticles (TeNPs). We investigate how the strengthening of
covalent bonds within TeNPs reflects the strengthening of social cohesion in
groups that share common beliefs and values.
\\ ( https://arxiv.org/abs/2403.05593 ,  1051kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05645 (*cross-listing*)
Date: Fri, 8 Mar 2024 19:36:20 GMT   (1906kb,D)

Title: Geometric Neural Network based on Phase Space for BCI decoding
Authors: Igor Carrara, Bruno Aristimunha, Marie-Constance Corsi, Raphael Y. de
  Camargo, Sylvain Chevallier, Th\'eodore Papadopoulo
Categories: eess.SP cs.AI cs.LG q-bio.NC
ACM-class: I.5.1; I.6.3; I.2.6
\\
  The integration of Deep Learning (DL) algorithms on brain signal analysis is
still in its nascent stages compared to their success in fields like Computer
Vision, especially in Brain-Computer Interface (BCI), where the brain activity
is decoded to control external devices without requiring muscle control.
Electroencephalography (EEG) is a widely adopted choice for designing BCI
systems due to its non-invasive and cost-effective nature and excellent
temporal resolution. Still, it comes at the expense of limited training data,
poor signal-to-noise, and a large variability across and within-subject
recordings. Finally, setting up a BCI system with many electrodes takes a long
time, hindering the widespread adoption of reliable DL architectures in BCIs
outside research laboratories. To improve adoption, we need to improve user
comfort using, for instance, reliable algorithms that operate with few
electrodes. \textbf{Approach:} Our research aims to develop a DL algorithm that
delivers effective results with a limited number of electrodes. Taking
advantage of the Augmented Covariance Method with SPDNet, we propose the
SPDNet$_{\psi}$ architecture and analyze its performance and computational
impact, as well as the interpretability of the results. The evaluation is
conducted on 5-fold cross-validation, using only three electrodes positioned
above the Motor Cortex. The methodology was tested on nearly 100 subjects from
several open-source datasets using the Mother Of All BCI Benchmark (MOABB)
framework. \textbf{Main results:} The results of our SPDNet$_{\psi}$
demonstrate that the augmented approach combined with the SPDNet significantly
outperforms all the current state-of-the-art DL architecture in MI decoding.
\textbf{Significance:} This new architecture is explainable, with a low number
of trainable parameters and a reduced carbon footprint.
\\ ( https://arxiv.org/abs/2403.05645 ,  1906kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05658 (*cross-listing*)
Date: Fri, 8 Mar 2024 20:16:00 GMT   (18951kb,D)

Title: Feature CAM: Interpretable AI in Image Classification
Authors: Frincy Clement, Ji Yang and Irene Cheng
Categories: cs.CV cs.AI cs.MM
\\
  Deep Neural Networks have often been called the black box because of the
complex, deep architecture and non-transparency presented by the inner layers.
There is a lack of trust to use Artificial Intelligence in critical and
high-precision fields such as security, finance, health, and manufacturing
industries. A lot of focused work has been done to provide interpretable
models, intending to deliver meaningful insights into the thoughts and behavior
of neural networks. In our research, we compare the state-of-the-art methods in
the Activation-based methods (ABM) for interpreting predictions of CNN models,
specifically in the application of Image Classification. We then extend the
same for eight CNN-based architectures to compare the differences in
visualization and thus interpretability. We introduced a novel technique
Feature CAM, which falls in the perturbation-activation combination, to create
fine-grained, class-discriminative visualizations. The resulting saliency maps
from our experiments proved to be 3-4 times better human interpretable than the
state-of-the-art in ABM. At the same time it reserves machine interpretability,
which is the average confidence scores in classification.
\\ ( https://arxiv.org/abs/2403.05658 ,  18951kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05681 (*cross-listing*)
Date: Fri, 8 Mar 2024 21:19:01 GMT   (987kb,D)

Title: DP-TabICL: In-Context Learning with Differentially Private Tabular Data
Authors: Alycia N. Carey, Karuna Bhaila, Kennedy Edemacu, Xintao Wu
Categories: cs.CR cs.AI cs.LG
Comments: 15 pages, 2 figures, 9 tables
\\
  In-context learning (ICL) enables large language models (LLMs) to adapt to
new tasks by conditioning on demonstrations of question-answer pairs and it has
been shown to have comparable performance to costly model retraining and
fine-tuning. Recently, ICL has been extended to allow tabular data to be used
as demonstration examples by serializing individual records into natural
language formats. However, it has been shown that LLMs can leak information
contained in prompts, and since tabular data often contain sensitive
information, understanding how to protect the underlying tabular data used in
ICL is a critical area of research. This work serves as an initial
investigation into how to use differential privacy (DP) -- the long-established
gold standard for data privacy and anonymization -- to protect tabular data
used in ICL. Specifically, we investigate the application of DP mechanisms for
private tabular ICL via data privatization prior to serialization and
prompting. We formulate two private ICL frameworks with provable privacy
guarantees in both the local (LDP-TabICL) and global (GDP-TabICL) DP scenarios
via injecting noise into individual records or group statistics, respectively.
We evaluate our DP-based frameworks on eight real-world tabular datasets and
across multiple ICL and DP settings. Our evaluations show that DP-based ICL can
protect the privacy of the underlying tabular data while achieving comparable
performance to non-LLM baselines, especially under high privacy regimes.
\\ ( https://arxiv.org/abs/2403.05681 ,  987kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05701 (*cross-listing*)
Date: Fri, 8 Mar 2024 22:23:23 GMT   (4754kb,D)

Title: Are Large Language Models Aligned with People's Social Intuitions for
  Human-Robot Interactions?
Authors: Lennart Wachowiak, Andrew Coles, Oya Celiktutan, Gerard Canal
Categories: cs.RO cs.AI cs.HC
\\
  Large language models (LLMs) are increasingly used in robotics, especially
for high-level action planning. Meanwhile, many robotics applications involve
human supervisors or collaborators. Hence, it is crucial for LLMs to generate
socially acceptable actions that align with people's preferences and values. In
this work, we test whether LLMs capture people's intuitions about behavior
judgments and communication preferences in human-robot interaction (HRI)
scenarios. For evaluation, we reproduce three HRI user studies, comparing the
output of LLMs with that of real participants. We find that GPT-4 strongly
outperforms other models, generating answers that correlate strongly with
users' answers in two studies $\unicode{x2014}$ the first study dealing with
selecting the most appropriate communicative act for a robot in various
situations ($r_s$ = 0.82), and the second with judging the desirability,
intentionality, and surprisingness of behavior ($r_s$ = 0.83). However, for the
last study, testing whether people judge the behavior of robots and humans
differently, no model achieves strong correlations. Moreover, we show that
vision models fail to capture the essence of video stimuli and that LLMs tend
to rate different communicative acts and behavior desirability higher than
people.
\\ ( https://arxiv.org/abs/2403.05701 ,  4754kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05715 (*cross-listing*)
Date: Fri, 8 Mar 2024 23:02:20 GMT   (282kb,D)

Title: A Framework for Effective AI Recommendations in Cyber-Physical-Human
  Systems
Authors: Aditya Dave, Heeseung Bang, Andreas A. Malikopoulos
Categories: eess.SY cs.AI cs.HC cs.LG cs.SY
\\
  Many cyber-physical-human systems (CPHS) involve a human decision-maker who
may receive recommendations from an artificial intelligence (AI) platform while
holding the ultimate responsibility of making decisions. In such CPHS
applications, the human decision-maker may depart from an optimal recommended
decision and instead implement a different one for various reasons. In this
letter, we develop a rigorous framework to overcome this challenge. In our
framework, we consider that humans may deviate from AI recommendations as they
perceive and interpret the system's state in a different way than the AI
platform. We establish the structural properties of optimal recommendation
strategies and develop an approximate human model (AHM) used by the AI. We
provide theoretical bounds on the optimality gap that arises from an AHM and
illustrate the efficacy of our results in a numerical example.
\\ ( https://arxiv.org/abs/2403.05715 ,  282kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05763 (*cross-listing*)
Date: Sat, 9 Mar 2024 02:17:43 GMT   (1124kb,D)

Title: HDReason: Algorithm-Hardware Codesign for Hyperdimensional Knowledge
  Graph Reasoning
Authors: Hanning Chen, Yang Ni, Ali Zakeri, Zhuowen Zou, Sanggeon Yun, Fei Wen,
  Behnam Khaleghi, Narayan Srinivasa, Hugo Latapie, and Mohsen Imani
Categories: cs.AR cs.AI cs.LG
\\
  In recent times, a plethora of hardware accelerators have been put forth for
graph learning applications such as vertex classification and graph
classification. However, previous works have paid little attention to Knowledge
Graph Completion (KGC), a task that is well-known for its significantly higher
algorithm complexity. The state-of-the-art KGC solutions based on graph
convolution neural network (GCN) involve extensive vertex/relation embedding
updates and complicated score functions, which are inherently cumbersome for
acceleration. As a result, existing accelerator designs are no longer optimal,
and a novel algorithm-hardware co-design for KG reasoning is needed.
  Recently, brain-inspired HyperDimensional Computing (HDC) has been introduced
as a promising solution for lightweight machine learning, particularly for
graph learning applications. In this paper, we leverage HDC for an
intrinsically more efficient and acceleration-friendly KGC algorithm. We also
co-design an acceleration framework named HDReason targeting FPGA platforms. On
the algorithm level, HDReason achieves a balance between high reasoning
accuracy, strong model interpretability, and less computation complexity. In
terms of architecture, HDReason offers reconfigurability, high training
throughput, and low energy consumption. When compared with NVIDIA RTX 4090 GPU,
the proposed accelerator achieves an average 10.6x speedup and 65x energy
efficiency improvement. When conducting cross-models and cross-platforms
comparison, HDReason yields an average 4.2x higher performance and 3.4x better
energy efficiency with similar accuracy versus the state-of-the-art FPGA-based
GCN training platform.
\\ ( https://arxiv.org/abs/2403.05763 ,  1124kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05764 (*cross-listing*)
Date: Sat, 9 Mar 2024 02:18:48 GMT   (659kb)

Title: Investigation into the Potential of Parallel Quantum Annealing for
  Simultaneous Optimization of Multiple Problems: A Comprehensive Study
Authors: Arit Kumar Bishwas, Anuraj Som, Saurabh Choudhary
Categories: quant-ph cs.AI
\\
  Parallel Quantum Annealing is a technique to solve multiple optimization
problems simultaneously. Parallel quantum annealing aims to optimize the
utilization of available qubits on a quantum topology by addressing multiple
independent problems in a single annealing cycle. This study provides insights
into the potential and the limitations of this parallelization method. The
experiments consisting of two different problems are integrated, and various
problem dimensions are explored including normalization techniques using
specific methods such as DWaveSampler with Default Embedding, DWaveSampler with
Custom Embedding and LeapHybridSampler. This method minimizes idle qubits and
holds promise for substantial speed-up, as indicated by the Time-to-Solution
(TTS) metric, compared to traditional quantum annealing, which solves problems
sequentially and may leave qubits unutilized.
\\ ( https://arxiv.org/abs/2403.05764 ,  659kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05770 (*cross-listing*)
Date: Sat, 9 Mar 2024 02:34:13 GMT   (10853kb,D)

Title: Towards Deviation-Robust Agent Navigation via Perturbation-Aware
  Contrastive Learning
Authors: Bingqian Lin, Yanxin Long, Yi Zhu, Fengda Zhu, Xiaodan Liang, Qixiang
  Ye, Liang Lin
Categories: cs.CV cs.AI cs.RO
Comments: Accepted by TPAMI 2023
Journal-ref: IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI,2023)
DOI: 10.1109/TPAMI.2023.3273594
\\
  Vision-and-language navigation (VLN) asks an agent to follow a given language
instruction to navigate through a real 3D environment. Despite significant
advances, conventional VLN agents are trained typically under disturbance-free
environments and may easily fail in real-world scenarios, since they are
unaware of how to deal with various possible disturbances, such as sudden
obstacles or human interruptions, which widely exist and may usually cause an
unexpected route deviation. In this paper, we present a model-agnostic training
paradigm, called Progressive Perturbation-aware Contrastive Learning (PROPER)
to enhance the generalization ability of existing VLN agents, by requiring them
to learn towards deviation-robust navigation. Specifically, a simple yet
effective path perturbation scheme is introduced to implement the route
deviation, with which the agent is required to still navigate successfully
following the original instruction. Since directly enforcing the agent to learn
perturbed trajectories may lead to inefficient training, a progressively
perturbed trajectory augmentation strategy is designed, where the agent can
self-adaptively learn to navigate under perturbation with the improvement of
its navigation performance for each specific trajectory. For encouraging the
agent to well capture the difference brought by perturbation, a
perturbation-aware contrastive learning mechanism is further developed by
contrasting perturbation-free trajectory encodings and perturbation-based
counterparts. Extensive experiments on R2R show that PROPER can benefit
multiple VLN baselines in perturbation-free scenarios. We further collect the
perturbed path data to construct an introspection subset based on the R2R,
called Path-Perturbed R2R (PP-R2R). The results on PP-R2R show unsatisfying
robustness of popular VLN agents and the capability of PROPER in improving the
navigation robustness.
\\ ( https://arxiv.org/abs/2403.05770 ,  10853kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05794 (*cross-listing*)
Date: Sat, 9 Mar 2024 04:56:57 GMT   (13255kb,D)

Title: Privacy-Preserving Diffusion Model Using Homomorphic Encryption
Authors: Yaojian Chen and Qiben Yan
Categories: cs.CR cs.AI
\\
  In this paper, we introduce a privacy-preserving stable diffusion framework
leveraging homomorphic encryption, called HE-Diffusion, which primarily focuses
on protecting the denoising phase of the diffusion process. HE-Diffusion is a
tailored encryption framework specifically designed to align with the unique
architecture of stable diffusion, ensuring both privacy and functionality. To
address the inherent computational challenges, we propose a novel
min-distortion method that enables efficient partial image encryption,
significantly reducing the overhead without compromising the model's output
quality. Furthermore, we adopt a sparse tensor representation to expedite
computational operations, enhancing the overall efficiency of the
privacy-preserving diffusion process. We successfully implement HE-based
privacy-preserving stable diffusion inference. The experimental results show
that HE-Diffusion achieves 500 times speedup compared with the baseline method,
and reduces time cost of the homomorphically encrypted inference to the minute
level. Both the performance and accuracy of the HE-Diffusion are on par with
the plaintext counterpart. Our approach marks a significant step towards
integrating advanced cryptographic techniques with state-of-the-art generative
models, paving the way for privacy-preserving and efficient image generation in
critical applications.
\\ ( https://arxiv.org/abs/2403.05794 ,  13255kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05810 (*cross-listing*)
Date: Sat, 9 Mar 2024 06:17:09 GMT   (1074kb,D)

Title: Recurrent Aligned Network for Generalized Pedestrian Trajectory
  Prediction
Authors: Yonghao Dong, Le Wang, Sanping Zhou, Gang Hua, Changyin Sun
Categories: cs.CV cs.AI
\\
  Pedestrian trajectory prediction is a crucial component in computer vision
and robotics, but remains challenging due to the domain shift problem. Previous
studies have tried to tackle this problem by leveraging a portion of the
trajectory data from the target domain to adapt the model. However, such domain
adaptation methods are impractical in real-world scenarios, as it is infeasible
to collect trajectory data from all potential target domains. In this paper, we
study a task named generalized pedestrian trajectory prediction, with the aim
of generalizing the model to unseen domains without accessing their
trajectories. To tackle this task, we introduce a Recurrent Aligned
Network~(RAN) to minimize the domain gap through domain alignment.
Specifically, we devise a recurrent alignment module to effectively align the
trajectory feature spaces at both time-state and time-sequence levels by the
recurrent alignment strategy.Furthermore, we introduce a pre-aligned
representation module to combine social interactions with the recurrent
alignment strategy, which aims to consider social interactions during the
alignment process instead of just target trajectories. We extensively evaluate
our method and compare it with state-of-the-art methods on three widely used
benchmarks. The experimental results demonstrate the superior generalization
capability of our method. Our work not only fills the gap in the generalization
setting for practical pedestrian trajectory prediction but also sets strong
baselines in this field.
\\ ( https://arxiv.org/abs/2403.05810 ,  1074kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05828 (*cross-listing*)
Date: Sat, 9 Mar 2024 07:38:45 GMT   (2016kb,D)

Title: Quantum-HPC Framework with multi-GPU-Enabled Hybrid Quantum-Classical
  Workflow: Applications in Quantum Simulations
Authors: Kuan-Cheng Chen, Xiaoren Li, Xiaotian Xu, Yun-Yuan Wang and Chen-Yu
  Liu
Categories: quant-ph cs.AI cs.AR cs.DC
Comments: 8 pages, 8 figures
\\
  Achieving high-performance computation on quantum systems presents a
formidable challenge that necessitates bridging the capabilities between
quantum hardware and classical computing resources. This study introduces an
innovative distribution-aware Quantum-Classical-Quantum (QCQ) architecture,
which integrates cutting-edge quantum software framework works with
high-performance classical computing resources to address challenges in quantum
simulation for materials and condensed matter physics. At the heart of this
architecture is the seamless integration of VQE algorithms running on QPUs for
efficient quantum state preparation, Tensor Network states, and QCNNs for
classifying quantum states on classical hardware.
  For benchmarking quantum simulators, the QCQ architecture utilizes the
cuQuantum SDK to leverage multi-GPU acceleration, integrated with PennyLane's
Lightning plugin, demonstrating up to tenfold increases in computational speed
for complex phase transition classification tasks compared to traditional
CPU-based methods. This significant acceleration enables models such as the
transverse field Ising and XXZ systems to accurately predict phase transitions
with a 99.5% accuracy. The architecture's ability to distribute computation
between QPUs and classical resources addresses critical bottlenecks in
Quantum-HPC, paving the way for scalable quantum simulation.
  The QCQ framework embodies a synergistic combination of quantum algorithms,
machine learning, and Quantum-HPC capabilities, enhancing its potential to
provide transformative insights into the behavior of quantum systems across
different scales. As quantum hardware continues to improve, this hybrid
distribution-aware framework will play a crucial role in realizing the full
potential of quantum computing by seamlessly integrating distributed quantum
resources with the state-of-the-art classical computing infrastructure.
\\ ( https://arxiv.org/abs/2403.05828 ,  2016kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05839 (*cross-listing*)
Date: Sat, 9 Mar 2024 08:49:50 GMT   (29083kb,D)

Title: Long-term Frame-Event Visual Tracking: Benchmark Dataset and Baseline
Authors: Xiao Wang, Ju Huang, Shiao Wang, Chuanming Tang, Bo Jiang, Yonghong
  Tian, Jin Tang, Bin Luo
Categories: cs.CV cs.AI cs.NE
Comments: In Peer Review
\\
  Current event-/frame-event based trackers undergo evaluation on short-term
tracking datasets, however, the tracking of real-world scenarios involves
long-term tracking, and the performance of existing tracking algorithms in
these scenarios remains unclear. In this paper, we first propose a new
long-term and large-scale frame-event single object tracking dataset, termed
FELT. It contains 742 videos and 1,594,474 RGB frames and event stream pairs
and has become the largest frame-event tracking dataset to date. We re-train
and evaluate 15 baseline trackers on our dataset for future works to compare.
More importantly, we find that the RGB frames and event streams are naturally
incomplete due to the influence of challenging factors and spatially sparse
event flow. In response to this, we propose a novel associative memory
Transformer network as a unified backbone by introducing modern Hopfield layers
into multi-head self-attention blocks to fuse both RGB and event data.
Extensive experiments on both FELT and RGB-T tracking dataset LasHeR fully
validated the effectiveness of our model. The dataset and source code can be
found at \url{https://github.com/Event-AHU/FELT_SOT_Benchmark}.
\\ ( https://arxiv.org/abs/2403.05839 ,  29083kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05842 (*cross-listing*)
Date: Sat, 9 Mar 2024 08:54:52 GMT   (1911kb,D)

Title: Hufu: A Modality-Agnositc Watermarking System for Pre-Trained
  Transformers via Permutation Equivariance
Authors: Hengyuan Xu, Liyao Xiang, Xingjun Ma, Borui Yang, Baochun Li
Categories: cs.CR cs.AI
\\
  With the blossom of deep learning models and services, it has become an
imperative concern to safeguard the valuable model parameters from being
stolen. Watermarking is considered an important tool for ownership
verification. However, current watermarking schemes are customized for
different models and tasks, hard to be integrated as an integrated intellectual
protection service. We propose Hufu, a modality-agnostic watermarking system
for pre-trained Transformer-based models, relying on the permutation
equivariance property of Transformers. Hufu embeds watermark by fine-tuning the
pre-trained model on a set of data samples specifically permuted, and the
embedded model essentially contains two sets of weights -- one for normal use
and the other for watermark extraction which is triggered on permuted inputs.
The permutation equivariance ensures minimal interference between these two
sets of model weights and thus high fidelity on downstream tasks. Since our
method only depends on the model itself, it is naturally modality-agnostic,
task-independent, and trigger-sample-free. Extensive experiments on the
state-of-the-art vision Transformers, BERT, and GPT2 have demonstrated Hufu's
superiority in meeting watermarking requirements including effectiveness,
efficiency, fidelity, and robustness, showing its great potential to be
deployed as a uniform ownership verification service for various Transformers.
\\ ( https://arxiv.org/abs/2403.05842 ,  1911kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05911 (*cross-listing*)
Date: Sat, 9 Mar 2024 13:30:00 GMT   (726kb,D)

Title: Towards Optimizing Human-Centric Objectives in AI-Assisted
  Decision-Making With Offline Reinforcement Learning
Authors: Zana Bu\c{c}inca, Siddharth Swaroop, Amanda E. Paluch, Susan A.
  Murphy, Krzysztof Z. Gajos
Categories: cs.HC cs.AI
\\
  As AI assistance is increasingly infused into decision-making processes, we
may seek to optimize human-centric objectives beyond decision accuracy, such as
skill improvement or task enjoyment of individuals interacting with these
systems. With this aspiration in mind, we propose offline reinforcement
learning (RL) as a general approach for modeling human-AI decision-making to
optimize such human-centric objectives. Our approach seeks to optimize
different objectives by adaptively providing decision support to humans -- the
right type of assistance, to the right person, at the right time. We
instantiate our approach with two objectives: human-AI accuracy on the
decision-making task and human learning about the task, and learn policies that
optimize these two objectives from previous human-AI interaction data. We
compare the optimized policies against various baselines in AI-assisted
decision-making. Across two experiments (N = 316 and N = 964), our results
consistently demonstrate that people interacting with policies optimized for
accuracy achieve significantly better accuracy -- and even human-AI
complementarity -- compared to those interacting with any other type of AI
support. Our results further indicate that human learning is more difficult to
optimize than accuracy, with participants who interacted with
learning-optimized policies showing significant learning improvement only at
times. Our research (1) demonstrates offline RL to be a promising approach to
model dynamics of human-AI decision-making, leading to policies that may
optimize various human-centric objectives and provide novel insights about the
AI-assisted decision-making space, and (2) emphasizes the importance of
considering human-centric objectives beyond decision accuracy in AI-assisted
decision-making, while also opening up the novel research challenge of
optimizing such objectives.
\\ ( https://arxiv.org/abs/2403.05911 ,  726kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05916 (*cross-listing*)
Date: Sat, 9 Mar 2024 13:56:25 GMT   (4940kb,D)

Title: GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual
  Affective Computing
Authors: Hao Lu, Xuesong Niu, Jiyao Wang, Yin Wang, Qingyong Hu, Jiaqi Tang,
  Yuting Zhang, Kaishen Yuan, Bin Huang, Zitong Yu, Dengbo He, Shuiguang Deng,
  Hao Chen, Yingcong Chen, Shiguang Shan
Categories: cs.CV cs.AI
\\
  Multimodal language models (MLMs) are designed to process and integrate
information from multiple sources, such as text, speech, images, and videos.
Despite its success in language understanding, it is critical to evaluate the
performance of downstream tasks for better human-centric applications. This
paper assesses the application of MLMs with 5 crucial abilities for affective
computing, spanning from visual affective tasks and reasoning tasks. The
results show that GPT4 has high accuracy in facial action unit recognition and
micro-expression detection while its general facial expression recognition
performance is not accurate. We also highlight the challenges of achieving
fine-grained micro-expression recognition and the potential for further study
and demonstrate the versatility and potential of GPT4 for handling advanced
tasks in emotion recognition and related fields by integrating with
task-related agents for more complex tasks, such as heart rate estimation
through signal processing. In conclusion, this paper provides valuable insights
into the potential applications and challenges of MLMs in human-centric
computing. The interesting samples are available at
\url{https://github.com/LuPaoPao/GPT4Affectivity}.
\\ ( https://arxiv.org/abs/2403.05916 ,  4940kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05932 (*cross-listing*)
Date: Sat, 9 Mar 2024 14:57:03 GMT   (27367kb,D)

Title: Learned 3D volumetric recovery of clouds and its uncertainty for climate
  analysis
Authors: Roi Ronen and Ilan Koren and Aviad Levis and Eshkol Eytan and Vadim
  Holodovsky and Yoav Y. Schechner
Categories: cs.CV cs.AI
\\
  Significant uncertainty in climate prediction and cloud physics is tied to
observational gaps relating to shallow scattered clouds. Addressing these
challenges requires remote sensing of their three-dimensional (3D)
heterogeneous volumetric scattering content. This calls for passive scattering
computed tomography (CT). We design a learning-based model (ProbCT) to achieve
CT of such clouds, based on noisy multi-view spaceborne images. ProbCT infers -
for the first time - the posterior probability distribution of the
heterogeneous extinction coefficient, per 3D location. This yields arbitrary
valuable statistics, e.g., the 3D field of the most probable extinction and its
uncertainty. ProbCT uses a neural-field representation, making essentially
real-time inference. ProbCT undergoes supervised training by a new labeled
multi-class database of physics-based volumetric fields of clouds and their
corresponding images. To improve out-of-distribution inference, we incorporate
self-supervised learning through differential rendering. We demonstrate the
approach in simulations and on real-world data, and indicate the relevance of
3D recovery and uncertainty to precipitation and renewable energy.
\\ ( https://arxiv.org/abs/2403.05932 ,  27367kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05950 (*cross-listing*)
Date: Sat, 9 Mar 2024 16:05:31 GMT   (747kb)

Title: Classifying Objects in 3D Point Clouds Using Recurrent Neural Network: A
  GRU LSTM Hybrid Approach
Authors: Ramin Mousa, Mitra Khezli, Saba Hesaraki
Categories: cs.CV cs.AI
\\
  Accurate classification of objects in 3D point clouds is a significant
problem in several applications, such as autonomous navigation and
augmented/virtual reality scenarios, which has become a research hot spot. In
this paper, we presented a deep learning strategy for 3D object classification
in augmented reality. The proposed approach is a combination of the GRU and
LSTM. LSTM networks learn longer dependencies well, but due to the number of
gates, it takes longer to train; on the other hand, GRU networks have a weaker
performance than LSTM, but their training speed is much higher than GRU, which
is The speed is due to its fewer gates. The proposed approach used the
combination of speed and accuracy of these two networks. The proposed approach
achieved an accuracy of 0.99 in the 4,499,0641 points dataset, which includes
eight classes (unlabeled, man-made terrain, natural terrain, high vegetation,
low vegetation, buildings, hardscape, scanning artifacts, cars). Meanwhile, the
traditional machine learning approaches could achieve a maximum accuracy of
0.9489 in the best case. Keywords: Point Cloud Classification, Virtual Reality,
Hybrid Model, GRULSTM, GRU, LSTM
\\ ( https://arxiv.org/abs/2403.05950 ,  747kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06003 (*cross-listing*)
Date: Sat, 9 Mar 2024 20:32:17 GMT   (1190kb,D)

Title: A Generalized Acquisition Function for Preference-based Reward Learning
Authors: Evan Ellis, Gaurav R. Ghosal, Stuart J. Russell, Anca Dragan, Erdem
  B{\i}y{\i}k
Categories: cs.RO cs.AI cs.LG
\\
  Preference-based reward learning is a popular technique for teaching robots
and autonomous systems how a human user wants them to perform a task. Previous
works have shown that actively synthesizing preference queries to maximize
information gain about the reward function parameters improves data efficiency.
The information gain criterion focuses on precisely identifying all parameters
of the reward function. This can potentially be wasteful as many parameters may
result in the same reward, and many rewards may result in the same behavior in
the downstream tasks. Instead, we show that it is possible to optimize for
learning the reward function up to a behavioral equivalence class, such as
inducing the same ranking over behaviors, distribution over choices, or other
related definitions of what makes two rewards similar. We introduce a tractable
framework that can capture such definitions of similarity. Our experiments in a
synthetic environment, an assistive robotics environment with domain transfer,
and a natural language processing problem with real datasets demonstrate the
superior performance of our querying method over the state-of-the-art
information gain method.
\\ ( https://arxiv.org/abs/2403.06003 ,  1190kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06025 (*cross-listing*)
Date: Sat, 9 Mar 2024 22:25:14 GMT   (13565kb,D)

Title: CarbonNet: How Computer Vision Plays a Role in Climate Change?
  Application: Learning Geomechanics from Subsurface Geometry of CCS to
  Mitigate Global Warming
Authors: Wei Chen, Yunan Li and Yuan Tian
Categories: cs.CV cs.AI
\\
  We introduce a new approach using computer vision to predict the land surface
displacement from subsurface geometry images for Carbon Capture and
Sequestration (CCS). CCS has been proved to be a key component for a carbon
neutral society. However, scientists see there are challenges along the way
including the high computational cost due to the large model scale and
limitations to generalize a pre-trained model with complex physics. We tackle
those challenges by training models directly from the subsurface geometry
images. The goal is to understand the respons of land surface displacement due
to carbon injection and utilize our trained models to inform decision making in
CCS projects.
  We implement multiple models (CNN, ResNet, and ResNetUNet) for static
mechanics problem, which is a image prediction problem. Next, we use the LSTM
and transformer for transient mechanics scenario, which is a video prediction
problem. It shows ResNetUNet outperforms the others thanks to its architecture
in static mechanics problem, and LSTM shows comparable performance to
transformer in transient problem. This report proceeds by outlining our dataset
in detail followed by model descriptions in method section. Result and
discussion state the key learning, observations, and conclusion with future
work rounds out the paper.
\\ ( https://arxiv.org/abs/2403.06025 ,  13565kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06039 (*cross-listing*)
Date: Sat, 9 Mar 2024 23:22:56 GMT   (6476kb,D)

Title: A Preliminary Exploration of YouTubers' Use of Generative-AI in Content
  Creation
Authors: Yao Lyu, He Zhang, Shuo Niu, Jie Cai
Categories: cs.HC cs.AI
Comments: Accepted at CHI LBW 2024
DOI: 10.1145/3613905.3651057
\\
  Content creators increasingly utilize generative artificial intelligence
(Gen-AI) on platforms such as YouTube, TikTok, Instagram, and various blogging
sites to produce imaginative images, AI-generated videos, and articles using
Large Language Models (LLMs). Despite its growing popularity, there remains an
underexplored area concerning the specific domains where AI-generated content
is being applied, and the methodologies content creators employ with Gen-AI
tools during the creation process. This study initially explores this emerging
area through a qualitative analysis of 68 YouTube videos demonstrating Gen-AI
usage. Our research focuses on identifying the content domains, the variety of
tools used, the activities performed, and the nature of the final products
generated by Gen-AI in the context of user-generated content.
\\ ( https://arxiv.org/abs/2403.06039 ,  6476kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06041 (*cross-listing*)
Date: Sat, 9 Mar 2024 23:28:54 GMT   (5394kb,D)

Title: MATRIX: Multi-Agent Trajectory Generation with Diverse Contexts
Authors: Zhuo Xu, Rui Zhou, Yida Yin, Huidong Gao, Masayoshi Tomizuka, Jiachen
  Li
Categories: cs.RO cs.AI cs.CV cs.LG cs.MA
Comments: IEEE International Conference on Robotics and Automation (ICRA 2024)
\\
  Data-driven methods have great advantages in modeling complicated human
behavioral dynamics and dealing with many human-robot interaction applications.
However, collecting massive and annotated real-world human datasets has been a
laborious task, especially for highly interactive scenarios. On the other hand,
algorithmic data generation methods are usually limited by their model
capacities, making them unable to offer realistic and diverse data needed by
various application users. In this work, we study trajectory-level data
generation for multi-human or human-robot interaction scenarios and propose a
learning-based automatic trajectory generation model, which we call Multi-Agent
TRajectory generation with dIverse conteXts (MATRIX). MATRIX is capable of
generating interactive human behaviors in realistic diverse contexts. We
achieve this goal by modeling the explicit and interpretable objectives so that
MATRIX can generate human motions based on diverse destinations and
heterogeneous behaviors. We carried out extensive comparison and ablation
studies to illustrate the effectiveness of our approach across various metrics.
We also presented experiments that demonstrate the capability of MATRIX to
serve as data augmentation for imitation-based motion planning.
\\ ( https://arxiv.org/abs/2403.06041 ,  5394kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06054 (*cross-listing*)
Date: Sun, 10 Mar 2024 00:47:05 GMT   (13983kb,D)

Title: Decoupled Data Consistency with Diffusion Purification for Image
  Restoration
Authors: Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishanka, Qing
  Qu
Categories: eess.IV cs.AI cs.CV cs.LG eess.SP
\\
  Diffusion models have recently gained traction as a powerful class of deep
generative priors, excelling in a wide range of image restoration tasks due to
their exceptional ability to model data distributions. To solve image
restoration problems, many existing techniques achieve data consistency by
incorporating additional likelihood gradient steps into the reverse sampling
process of diffusion models. However, the additional gradient steps pose a
challenge for real-world practical applications as they incur a large
computational overhead, thereby increasing inference time. They also present
additional difficulties when using accelerated diffusion model samplers, as the
number of data consistency steps is limited by the number of reverse sampling
steps. In this work, we propose a novel diffusion-based image restoration
solver that addresses these issues by decoupling the reverse process from the
data consistency steps. Our method involves alternating between a
reconstruction phase to maintain data consistency and a refinement phase that
enforces the prior via diffusion purification. Our approach demonstrates
versatility, making it highly adaptable for efficient problem-solving in latent
space. Additionally, it reduces the necessity for numerous sampling steps
through the integration of consistency models. The efficacy of our approach is
validated through comprehensive experiments across various image restoration
tasks, including image denoising, deblurring, inpainting, and super-resolution.
\\ ( https://arxiv.org/abs/2403.06054 ,  13983kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06088 (*cross-listing*)
Date: Sun, 10 Mar 2024 04:17:54 GMT   (21379kb,D)

Title: Towards In-Vehicle Multi-Task Facial Attribute Recognition:
  Investigating Synthetic Data and Vision Foundation Models
Authors: Esmaeil Seraj and Walter Talamonti
Categories: cs.CV cs.AI cs.LG eess.IV
Comments: Manuscript under peer review
\\
  In the burgeoning field of intelligent transportation systems, enhancing
vehicle-driver interaction through facial attribute recognition, such as facial
expression, eye gaze, age, etc., is of paramount importance for safety,
personalization, and overall user experience. However, the scarcity of
comprehensive large-scale, real-world datasets poses a significant challenge
for training robust multi-task models. Existing literature often overlooks the
potential of synthetic datasets and the comparative efficacy of
state-of-the-art vision foundation models in such constrained settings. This
paper addresses these gaps by investigating the utility of synthetic datasets
for training complex multi-task models that recognize facial attributes of
passengers of a vehicle, such as gaze plane, age, and facial expression.
Utilizing transfer learning techniques with both pre-trained Vision Transformer
(ViT) and Residual Network (ResNet) models, we explore various training and
adaptation methods to optimize performance, particularly when data availability
is limited. We provide extensive post-evaluation analysis, investigating the
effects of synthetic data distributions on model performance in in-distribution
data and out-of-distribution inference. Our study unveils counter-intuitive
findings, notably the superior performance of ResNet over ViTs in our specific
multi-task context, which is attributed to the mismatch in model complexity
relative to task complexity. Our results highlight the challenges and
opportunities for enhancing the use of synthetic data and vision foundation
models in practical applications.
\\ ( https://arxiv.org/abs/2403.06088 ,  21379kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06095 (*cross-listing*)
Date: Sun, 10 Mar 2024 05:10:34 GMT   (856kb,D)

Title: RepoHyper: Better Context Retrieval Is All You Need for Repository-Level
  Code Completion
Authors: Huy N. Phan and Hoang N. Phan and Tien N. Nguyen and Nghi D. Q. Bui
Categories: cs.SE cs.AI
Comments: 8 pages
\\
  Code Large Language Models (CodeLLMs) have demonstrated impressive
proficiency in code completion tasks. However, they often fall short of fully
understanding the extensive context of a project repository, such as the
intricacies of relevant files and class hierarchies, which can result in less
precise completions. To overcome these limitations, we present RepoHyper, a
multifaceted framework designed to address the complex challenges associated
with repository-level code completion. Central to RepoHyper is the Repo-level
Semantic Graph (RSG), a novel semantic graph structure that encapsulates the
vast context of code repositories. Furthermore, RepoHyper leverages Expand and
Refine retrieval method, including a graph expansion and a link prediction
algorithm applied to the RSG, enabling the effective retrieval and
prioritization of relevant code snippets. Our evaluations show that RepoHyper
markedly outperforms existing techniques in repository-level code completion,
showcasing enhanced accuracy across various datasets when compared to several
strong baselines.
\\ ( https://arxiv.org/abs/2403.06095 ,  856kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06131 (*cross-listing*)
Date: Sun, 10 Mar 2024 08:41:22 GMT   (8020kb,D)

Title: FedPIT: Towards Privacy-preserving and Few-shot Federated Instruction
  Tuning
Authors: Zhuo Zhang, Jingyuan Zhang, Jintao Huang, Lizhen Qu, Hongzhi Zhang,
  Zenglin Xu
Categories: cs.CR cs.AI
Comments: Work in process
\\
  Instruction tuning has proven essential for enhancing the performance of
large language models (LLMs) in generating human-aligned responses. However,
collecting diverse, high-quality instruction data for tuning poses challenges,
particularly in privacy-sensitive domains. Federated instruction tuning (FedIT)
has emerged as a solution, leveraging federated learning from multiple data
owners while preserving privacy. Yet, it faces challenges due to limited
instruction data and vulnerabilities to training data extraction attacks. To
address these issues, we propose a novel federated algorithm, FedPIT, which
utilizes LLMs' in-context learning capability to self-generate task-specific
synthetic data for training autonomously. Our method employs parameter-isolated
training to maintain global parameters trained on synthetic data and local
parameters trained on augmented local data, effectively thwarting data
extraction attacks. Extensive experiments on real-world medical data
demonstrate the effectiveness of FedPIT in improving federated few-shot
performance while preserving privacy and robustness against data heterogeneity.
\\ ( https://arxiv.org/abs/2403.06131 ,  8020kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06135 (*cross-listing*)
Date: Sun, 10 Mar 2024 08:50:56 GMT   (46570kb,D)

Title: MACE: Mass Concept Erasure in Diffusion Models
Authors: Shilin Lu, Zilan Wang, Leyang Li, Yanzhu Liu, Adams Wai-Kin Kong
Categories: cs.CV cs.AI cs.LG
Comments: Accepted by CVPR 2024
\\
  The rapid expansion of large-scale text-to-image diffusion models has raised
growing concerns regarding their potential misuse in creating harmful or
misleading content. In this paper, we introduce MACE, a finetuning framework
for the task of mass concept erasure. This task aims to prevent models from
generating images that embody unwanted concepts when prompted. Existing concept
erasure methods are typically restricted to handling fewer than five concepts
simultaneously and struggle to find a balance between erasing concept synonyms
(generality) and maintaining unrelated concepts (specificity). In contrast,
MACE differs by successfully scaling the erasure scope up to 100 concepts and
by achieving an effective balance between generality and specificity. This is
achieved by leveraging closed-form cross-attention refinement along with LoRA
finetuning, collectively eliminating the information of undesirable concepts.
Furthermore, MACE integrates multiple LoRAs without mutual interference. We
conduct extensive evaluations of MACE against prior methods across four
different tasks: object erasure, celebrity erasure, explicit content erasure,
and artistic style erasure. Our results reveal that MACE surpasses prior
methods in all evaluated tasks. Code is available at
https://github.com/Shilin-LU/MACE.
\\ ( https://arxiv.org/abs/2403.06135 ,  46570kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06143 (*cross-listing*)
Date: Sun, 10 Mar 2024 09:11:57 GMT   (9302kb,D)

Title: Fluent: Round-efficient Secure Aggregation for Private Federated
  Learning
Authors: Xincheng Li, Jianting Ning, Geong Sen Poh, Leo Yu Zhang, Xinchun Yin,
  Tianwei Zhang
Categories: cs.CR cs.AI
\\
  Federated learning (FL) facilitates collaborative training of machine
learning models among a large number of clients while safeguarding the privacy
of their local datasets. However, FL remains susceptible to vulnerabilities
such as privacy inference and inversion attacks. Single-server secure
aggregation schemes were proposed to address these threats. Nonetheless, they
encounter practical constraints due to their round and communication
complexities. This work introduces Fluent, a round and communication-efficient
secure aggregation scheme for private FL. Fluent has several improvements
compared to state-of-the-art solutions like Bell et al. (CCS 2020) and Ma et
al. (SP 2023): (1) it eliminates frequent handshakes and secret sharing
operations by efficiently reusing the shares across multiple training
iterations without leaking any private information; (2) it accomplishes both
the consistency check and gradient unmasking in one logical step, thereby
reducing another round of communication. With these innovations, Fluent
achieves the fewest communication rounds (i.e., two in the collection phase) in
the malicious server setting, in contrast to at least three rounds in existing
schemes. This significantly minimizes the latency for geographically
distributed clients; (3) Fluent also introduces Fluent-Dynamic with a
participant selection algorithm and an alternative secret sharing scheme. This
can facilitate dynamic client joining and enhance the system flexibility and
scalability. We implemented Fluent and compared it with existing solutions.
Experimental results show that Fluent improves the computational cost by at
least 75% and communication overhead by at least 25% for normal clients. Fluent
also reduces the communication overhead for the server at the expense of a
marginal increase in computational cost.
\\ ( https://arxiv.org/abs/2403.06143 ,  9302kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06145 (*cross-listing*)
Date: Sun, 10 Mar 2024 09:24:53 GMT   (3520kb,D)

Title: All-in-one platform for AI R&D in medical imaging, encompassing data
  collection, selection, annotation, and pre-processing
Authors: Changhee Han, Kyohei Shibano, Wataru Ozaki, Keishiro Osaki, Takafumi
  Haraguchi, Daisuke Hirahara, Shumon Kimura, Yasuyuki Kobayashi, Gento Mogi
Categories: cs.CV cs.AI
Comments: 5 pages, 3 figures, accepted to SPIE Medical Imaging 2024
\\
  Deep Learning is advancing medical imaging Research and Development (R&D),
leading to the frequent clinical use of Artificial Intelligence/Machine
Learning (AI/ML)-based medical devices. However, to advance AI R&D, two
challenges arise: 1) significant data imbalance, with most data from
Europe/America and under 10% from Asia, despite its 60% global population
share; and 2) hefty time and investment needed to curate proprietary datasets
for commercial use. In response, we established the first commercial medical
imaging platform, encompassing steps like: 1) data collection, 2) data
selection, 3) annotation, and 4) pre-processing. Moreover, we focus on
harnessing under-represented data from Japan and broader Asia, including
Computed Tomography, Magnetic Resonance Imaging, and Whole Slide Imaging scans.
Using the collected data, we are preparing/providing ready-to-use datasets for
medical AI R&D by 1) offering these datasets to AI firms, biopharma, and
medical device makers and 2) using them as training/test data to develop
tailored AI solutions for such entities. We also aim to merge Blockchain for
data security and plan to synthesize rare disease data via generative AI.
DataHub Website: https://medical-datahub.ai/
\\ ( https://arxiv.org/abs/2403.06145 ,  3520kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06168 (*cross-listing*)
Date: Sun, 10 Mar 2024 10:39:32 GMT   (35584kb,D)

Title: DiffuMatting: Synthesizing Arbitrary Objects with Matting-level
  Annotation
Authors: Xiaobin Hu and Xu Peng and Donghao Luo and Xiaozhong Ji and Jinlong
  Peng and Zhengkai Jiang and Jiangning Zhang and Taisong Jin and Chengjie Wang
  and Rongrong Ji
Categories: cs.CV cs.AI
\\
  Due to the difficulty and labor-consuming nature of getting highly accurate
or matting annotations, there only exists a limited amount of highly accurate
labels available to the public. To tackle this challenge, we propose a
DiffuMatting which inherits the strong Everything generation ability of
diffusion and endows the power of "matting anything". Our DiffuMatting can 1).
act as an anything matting factory with high accurate annotations 2). be
well-compatible with community LoRAs or various conditional control approaches
to achieve the community-friendly art design and controllable generation.
Specifically, inspired by green-screen-matting, we aim to teach the diffusion
model to paint on a fixed green screen canvas. To this end, a large-scale
greenscreen dataset (Green100K) is collected as a training dataset for
DiffuMatting. Secondly, a green background control loss is proposed to keep the
drawing board as a pure green color to distinguish the foreground and
background. To ensure the synthesized object has more edge details, a
detailed-enhancement of transition boundary loss is proposed as a guideline to
generate objects with more complicated edge structures. Aiming to
simultaneously generate the object and its matting annotation, we build a
matting head to make a green color removal in the latent space of the VAE
decoder. Our DiffuMatting shows several potential applications (e.g.,
matting-data generator, community-friendly art design and controllable
generation). As a matting-data generator, DiffuMatting synthesizes general
object and portrait matting sets, effectively reducing the relative MSE error
by 15.4% in General Object Matting and 11.4% in Portrait Matting tasks.
\\ ( https://arxiv.org/abs/2403.06168 ,  35584kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06206 (*cross-listing*)
Date: Sun, 10 Mar 2024 13:04:09 GMT   (1858kb)

Title: Limit of the Maximum Random Permutation Set Entropy
Authors: Jiefeng Zhou, Zhen Li, Kang Hao Cheong, Yong Deng
Categories: cs.IT cs.AI math.IT
Comments: 22 pages, 5 figures
\\
  The Random Permutation Set (RPS) is a new type of set proposed recently,
which can be regarded as the generalization of evidence theory. To measure the
uncertainty of RPS, the entropy of RPS and its corresponding maximum entropy
have been proposed. Exploring the maximum entropy provides a possible way of
understanding the physical meaning of RPS. In this paper, a new concept, the
envelope of entropy function, is defined. In addition, the limit of the
envelope of RPS entropy is derived and proved. Compared with the existing
method, the computational complexity of the proposed method to calculate the
envelope of RPS entropy decreases greatly. The result shows that when $N \to
\infty$, the limit form of the envelope of the entropy of RPS converges to $e
\times (N!)^2$, which is highly connected to the constant $e$ and factorial.
Finally, numerical examples validate the efficiency and conciseness of the
proposed envelope, which provides a new insight into the maximum entropy
function.
\\ ( https://arxiv.org/abs/2403.06206 ,  1858kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06213 (*cross-listing*)
Date: Sun, 10 Mar 2024 13:26:24 GMT   (3660kb,D)

Title: $V_kD:$ Improving Knowledge Distillation using Orthogonal Projections
Authors: Roy Miles, Ismail Elezi, Jiankang Deng
Categories: cs.CV cs.AI
Comments: CVPR 2024. Code available at https://github.com/roymiles/vkd
\\
  Knowledge distillation is an effective method for training small and
efficient deep learning models. However, the efficacy of a single method can
degenerate when transferring to other tasks, modalities, or even other
architectures. To address this limitation, we propose a novel constrained
feature distillation method. This method is derived from a small set of core
principles, which results in two emerging components: an orthogonal projection
and a task-specific normalisation. Equipped with both of these components, our
transformer models can outperform all previous methods on ImageNet and reach up
to a 4.4% relative improvement over the previous state-of-the-art methods. To
further demonstrate the generality of our method, we apply it to object
detection and image generation, whereby we obtain consistent and substantial
performance improvements over state-of-the-art. Code and models are publicly
available: https://github.com/roymiles/vkd
\\ ( https://arxiv.org/abs/2403.06213 ,  3660kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06225 (*cross-listing*)
Date: Sun, 10 Mar 2024 14:11:25 GMT   (47523kb,D)

Title: MoST: Motion Style Transformer between Diverse Action Contents
Authors: Boeun Kim, Jungho Kim, Hyung Jin Chang, Jin Young Choi
Categories: cs.CV cs.AI
Comments: Accepted by CVPR 2024
\\
  While existing motion style transfer methods are effective between two
motions with identical content, their performance significantly diminishes when
transferring style between motions with different contents. This challenge lies
in the lack of clear separation between content and style of a motion. To
tackle this challenge, we propose a novel motion style transformer that
effectively disentangles style from content and generates a plausible motion
with transferred style from a source motion. Our distinctive approach to
achieving the goal of disentanglement is twofold: (1) a new architecture for
motion style transformer with 'part-attentive style modulator across body
parts' and 'Siamese encoders that encode style and content features
separately'; (2) style disentanglement loss. Our method outperforms existing
methods and demonstrates exceptionally high quality, particularly in motion
pairs with different contents, without the need for heuristic post-processing.
Codes are available at https://github.com/Boeun-Kim/MoST.
\\ ( https://arxiv.org/abs/2403.06225 ,  47523kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06247 (*cross-listing*)
Date: Sun, 10 Mar 2024 16:11:17 GMT   (1182kb,D)

Title: Text-Guided Variational Image Generation for Industrial Anomaly
  Detection and Segmentation
Authors: Mingyu Lee, Jongwon Choi
Categories: cs.CV cs.AI
Comments: 10 pages, CVPR2024 Accepted
\\
  We propose a text-guided variational image generation method to address the
challenge of getting clean data for anomaly detection in industrial
manufacturing. Our method utilizes text information about the target object,
learned from extensive text library documents, to generate non-defective data
images resembling the input image. The proposed framework ensures that the
generated non-defective images align with anticipated distributions derived
from textual and image-based knowledge, ensuring stability and generality.
Experimental results demonstrate the effectiveness of our approach, surpassing
previous methods even with limited non-defective data. Our approach is
validated through generalization tests across four baseline models and three
distinct datasets. We present an additional analysis to enhance the
effectiveness of anomaly detection models by utilizing the generated images.
\\ ( https://arxiv.org/abs/2403.06247 ,  1182kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06267 (*cross-listing*)
Date: Sun, 10 Mar 2024 17:07:20 GMT   (5443kb,D)

Title: FARPLS: A Feature-Augmented Robot Trajectory Preference Labeling System
  to Assist Human Labelers' Preference Elicitation
Authors: Hanfang Lyu, Yuanchen Bai, Xin Liang, Ujaan Das, Chuhan Shi, Leiliang
  Gong, Yingchi Li, Mingfei Sun, Ming Ge, Xiaojuan Ma
Categories: cs.HC cs.AI
Comments: Accepted to ACM Conference on Intelligent User Interfaces (IUI) 2024,
  March 18-21, 2024, Greenville, SC, USA
DOI: 10.1145/3640543.3645145
\\
  Preference-based learning aims to align robot task objectives with human
values. One of the most common methods to infer human preferences is by
pairwise comparisons of robot task trajectories. Traditional comparison-based
preference labeling systems seldom support labelers to digest and identify
critical differences between complex trajectories recorded in videos. Our
formative study (N = 12) suggests that individuals may overlook non-salient
task features and establish biased preference criteria during their preference
elicitation process because of partial observations. In addition, they may
experience mental fatigue when given many pairs to compare, causing their label
quality to deteriorate. To mitigate these issues, we propose FARPLS, a
Feature-Augmented Robot trajectory Preference Labeling System. FARPLS
highlights potential outliers in a wide variety of task features that matter to
humans and extracts the corresponding video keyframes for easy review and
comparison. It also dynamically adjusts the labeling order according to users'
familiarities, difficulties of the trajectory pair, and level of disagreements.
At the same time, the system monitors labelers' consistency and provides
feedback on labeling progress to keep labelers engaged. A between-subjects
study (N = 42, 105 pairs of robot pick-and-place trajectories per person) shows
that FARPLS can help users establish preference criteria more easily and notice
more relevant details in the presented trajectories than the conventional
interface. FARPLS also improves labeling consistency and engagement, mitigating
challenges in preference elicitation without raising cognitive loads
significantly
\\ ( https://arxiv.org/abs/2403.06267 ,  5443kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06268 (*cross-listing*)
Date: Sun, 10 Mar 2024 17:07:28 GMT   (5697kb,D)

Title: Physics-Guided Abnormal Trajectory Gap Detection
Authors: Arun Sharma, Shashi Shekhar
Categories: cs.CV cs.AI cs.CG cs.DB cs.LG
\\
  Given trajectories with gaps (i.e., missing data), we investigate algorithms
to identify abnormal gaps in trajectories which occur when a given moving
object did not report its location, but other moving objects in the same
geographic region periodically did. The problem is important due to its
societal applications, such as improving maritime safety and regulatory
enforcement for global security concerns such as illegal fishing, illegal oil
transfers, and trans-shipments. The problem is challenging due to the
difficulty of bounding the possible locations of the moving object during a
trajectory gap, and the very high computational cost of detecting gaps in such
a large volume of location data. The current literature on anomalous trajectory
detection assumes linear interpolation within gaps, which may not be able to
detect abnormal gaps since objects within a given region may have traveled away
from their shortest path. In preliminary work, we introduced an abnormal gap
measure that uses a classical space-time prism model to bound an object's
possible movement during the trajectory gap and provided a scalable memoized
gap detection algorithm (Memo-AGD). In this paper, we propose a Space
Time-Aware Gap Detection (STAGD) approach to leverage space-time indexing and
merging of trajectory gaps. We also incorporate a Dynamic Region Merge-based
(DRM) approach to efficiently compute gap abnormality scores. We provide
theoretical proofs that both algorithms are correct and complete and also
provide analysis of asymptotic time complexity. Experimental results on
synthetic and real-world maritime trajectory data show that the proposed
approach substantially improves computation time over the baseline technique.
\\ ( https://arxiv.org/abs/2403.06268 ,  5697kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06275 (*cross-listing*)
Date: Sun, 10 Mar 2024 18:05:41 GMT   (18332kb,D)

Title: UNICORN: Ultrasound Nakagami Imaging via Score Matching and Adaptation
Authors: Kwanyoung Kim, Jaa-Yeon Lee, Jong Chul Ye
Categories: cs.CV cs.AI cs.LG physics.med-ph
Comments: 12 pages, 5 figure
\\
  Nakagami imaging holds promise for visualizing and quantifying tissue
scattering in ultrasound waves, with potential applications in tumor diagnosis
and fat fraction estimation which are challenging to discern by conventional
ultrasound B-mode images. Existing methods struggle with optimal window size
selection and suffer from estimator instability, leading to degraded resolution
images. To address this, here we propose a novel method called UNICORN
(Ultrasound Nakagami Imaging via Score Matching and Adaptation), that offers an
accurate, closed-form estimator for Nakagami parameter estimation in terms of
the score function of ultrasonic envelope. Extensive experiments using
simulation and real ultrasound RF data demonstrate UNICORN's superiority over
conventional approaches in accuracy and resolution quality.
\\ ( https://arxiv.org/abs/2403.06275 ,  18332kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06289 (*cross-listing*)
Date: Sun, 10 Mar 2024 19:05:12 GMT   (6401kb,D)

Title: Understanding and Mitigating Human-Labelling Errors in Supervised
  Contrastive Learning
Authors: Zijun Long and Lipeng Zhuang and George Killick and Richard McCreadie
  and Gerardo Aragon Camarasa and Paul Henderson
Categories: cs.CV cs.AI cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2311.16481
\\
  Human-annotated vision datasets inevitably contain a fraction of human
mislabelled examples. While the detrimental effects of such mislabelling on
supervised learning are well-researched, their influence on Supervised
Contrastive Learning (SCL) remains largely unexplored. In this paper, we show
that human-labelling errors not only differ significantly from synthetic label
errors, but also pose unique challenges in SCL, different to those in
traditional supervised learning methods. Specifically, our results indicate
they adversely impact the learning process in the ~99% of cases when they occur
as false positive samples. Existing noise-mitigating methods primarily focus on
synthetic label errors and tackle the unrealistic setting of very high
synthetic noise rates (40-80%), but they often underperform on common image
datasets due to overfitting. To address this issue, we introduce a novel SCL
objective with robustness to human-labelling errors, SCL-RHE. SCL-RHE is
designed to mitigate the effects of real-world mislabelled examples, typically
characterized by much lower noise rates (<5%). We demonstrate that SCL-RHE
consistently outperforms state-of-the-art representation learning and
noise-mitigating methods across various vision benchmarks, by offering improved
resilience against human-labelling errors.
\\ ( https://arxiv.org/abs/2403.06289 ,  6401kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06317 (*cross-listing*)
Date: Sun, 10 Mar 2024 21:33:53 GMT   (3749kb,D)

Title: An End-to-End Deep Learning Generative Framework for Refinable Shape
  Matching and Generation
Authors: Soodeh Kalaie, Andy Bulpitt, Alejandro F. Frangi, and Ali Gooya
Categories: cs.CV cs.AI
\\
  Generative modelling for shapes is a prerequisite for In-Silico Clinical
Trials (ISCTs), which aim to cost-effectively validate medical device
interventions using synthetic anatomical shapes, often represented as 3D
surface meshes. However, constructing AI models to generate shapes closely
resembling the real mesh samples is challenging due to variable vertex counts,
connectivities, and the lack of dense vertex-wise correspondences across the
training data. Employing graph representations for meshes, we develop a novel
unsupervised geometric deep-learning model to establish refinable shape
correspondences in a latent space, construct a population-derived atlas and
generate realistic synthetic shapes. We additionally extend our proposed base
model to a joint shape generative-clustering multi-atlas framework to
incorporate further variability and preserve more details in the generated
shapes. Experimental results using liver and left-ventricular models
demonstrate the approach's applicability to computational medicine,
highlighting its suitability for ISCTs through a comparative analysis.
\\ ( https://arxiv.org/abs/2403.06317 ,  3749kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06322 (*cross-listing*)
Date: Sun, 10 Mar 2024 21:43:47 GMT   (2685kb,D)

Title: Leveraging Computer Vision in the Intensive Care Unit (ICU) for
  Examining Visitation and Mobility
Authors: Scott Siegel, Jiaqing Zhang, Sabyasachi Bandyopadhyay, Subhash
  Nerella, Brandon Silva, Tezcan Baslanti, Azra Bihorac, Parisa Rashidi
Categories: cs.CV cs.AI
\\
  Despite the importance of closely monitoring patients in the Intensive Care
Unit (ICU), many aspects are still assessed in a limited manner due to the time
constraints imposed on healthcare providers. For example, although excessive
visitations during rest hours can potentially exacerbate the risk of circadian
rhythm disruption and delirium, it is not captured in the ICU. Likewise, while
mobility can be an important indicator of recovery or deterioration in ICU
patients, it is only captured sporadically or not captured at all. In the past
few years, the computer vision field has found application in many domains by
reducing the human burden. Using computer vision systems in the ICU can also
potentially enable non-existing assessments or enhance the frequency and
accuracy of existing assessments while reducing the staff workload. In this
study, we leverage a state-of-the-art noninvasive computer vision system based
on depth imaging to characterize ICU visitations and patients' mobility. We
then examine the relationship between visitation and several patient outcomes,
such as pain, acuity, and delirium. We found an association between
deteriorating patient acuity and the incidence of delirium with increased
visitations. In contrast, self-reported pain, reported using the Defense and
Veteran Pain Rating Scale (DVPRS), was correlated with decreased visitations.
Our findings highlight the feasibility and potential of using noninvasive
autonomous systems to monitor ICU patients.
\\ ( https://arxiv.org/abs/2403.06322 ,  2685kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06332 (*cross-listing*)
Date: Sun, 10 Mar 2024 22:40:07 GMT   (29kb)

Title: Exploiting the Margin: How Capitalism Fuels AI at the Expense of
  Minoritized Groups
Authors: Nelson Col\'on Vargas
Categories: cs.CY cs.AI
\\
  This article investigates the complex nexus of capitalism, racial oppression,
and artificial intelligence (AI), revealing how these elements coalesce to
deepen social inequities. By tracing the historical exploitation of
marginalized communities through capitalist practices, the study demonstrates
how AI technologies not only reflect but also amplify societal biases,
particularly in exacerbating racial disparities. Through a focused analysis,
the paper presents how AI's development and application exploit marginalized
groups via mechanisms such as gig economy labor abuses, biased facial
recognition technologies, and the disproportionate mental health burdens placed
on these communities. These examples underscore the critical role of AI in
reinforcing and intensifying existing inequalities. Concluding that unregulated
AI significantly threatens to compound current oppressions, the article calls
for a concerted effort towards responsible AI development. This entails
adopting a holistic approach that rectifies systemic flaws and champions the
empowerment of marginalized individuals, ensuring that technological
advancement contributes to societal healing rather than perpetuating cycles of
exploitation.
\\ ( https://arxiv.org/abs/2403.06332 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06349 (*cross-listing*)
Date: Mon, 11 Mar 2024 00:33:28 GMT   (1899kb,D)

Title: MOAB: Multi-Modal Outer Arithmetic Block For Fusion Of Histopathological
  Images And Genetic Data For Brain Tumor Grading
Authors: Omnia Alwazzan (1 and 2), Abbas Khan (1 and 2), Ioannis Patras (1 and
  2), Gregory Slabaugh (1 and 2) ((1) School of Electronic Engineering and
  Computer Science, Queen Mary University of London, UK, (2) Queen Mary Digital
  Environment Research Institute (DERI), London, UK)
Categories: cs.CV cs.AI
Journal-ref: pages={1--5},year={2023},organization={IEEE}
DOI: 10.1109/ISBI53787.2023.10230698
\\
  Brain tumors are an abnormal growth of cells in the brain. They can be
classified into distinct grades based on their growth. Often grading is
performed based on a histological image and is one of the most significant
predictors of a patients prognosis, the higher the grade, the more aggressive
the tumor. Correct diagnosis of a tumor grade remains challenging. Though
histopathological grading has been shown to be prognostic, results are subject
to interobserver variability, even among experienced pathologists. Recently,
the World Health Organization reported that advances in molecular genetics have
led to improvements in tumor classification. This paper seeks to integrate
histological images and genetic data for improved computer-aided diagnosis. We
propose a novel Multi-modal Outer Arithmetic Block (MOAB) based on arithmetic
operations to combine latent representations of the different modalities for
predicting the tumor grade (Grade \rom{2}, \rom{3} and \rom{4}). Extensive
experiments evaluate the effectiveness of our approach. By applying MOAB to The
Cancer Genome Atlas (TCGA) glioma dataset, we show that it can improve
separation between similar classes (Grade \rom{2} and \rom{3}) and outperform
prior state-of-the-art grade classification techniques.
\\ ( https://arxiv.org/abs/2403.06349 ,  1899kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06356 (*cross-listing*)
Date: Mon, 11 Mar 2024 01:11:28 GMT   (9052kb,D)

Title: Video Generation with Consistency Tuning
Authors: Chaoyi Wang, Yaozhe Song, Yafeng Zhang, Jun Pei, Lijie Xia, Jianpo Liu
Categories: cs.CV cs.AI
\\
  Currently, various studies have been exploring generation of long videos.
However, the generated frames in these videos often exhibit jitter and noise.
Therefore, in order to generate the videos without these noise, we propose a
novel framework composed of four modules: separate tuning module, average
fusion module, combined tuning module, and inter-frame consistency module. By
applying our newly proposed modules subsequently, the consistency of the
background and foreground in each video frames is optimized. Besides, the
experimental results demonstrate that videos generated by our method exhibit a
high quality in comparison of the state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.06356 ,  9052kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06382 (*cross-listing*)
Date: Mon, 11 Mar 2024 02:24:32 GMT   (1745kb,D)

Title: Pre-Trained Model Recommendation for Downstream Fine-tuning
Authors: Jiameng Bai, Sai Wu, Jie Song, Junbo Zhao, Gang Chen
Categories: cs.CV cs.AI cs.LG
\\
  As a fundamental problem in transfer learning, model selection aims to rank
off-the-shelf pre-trained models and select the most suitable one for the new
target task. Existing model selection techniques are often constrained in their
scope and tend to overlook the nuanced relationships between models and tasks.
In this paper, we present a pragmatic framework \textbf{Fennec}, delving into a
diverse, large-scale model repository while meticulously considering the
intricate connections between tasks and models. The key insight is to map all
models and historical tasks into a transfer-related subspace, where the
distance between model vectors and task vectors represents the magnitude of
transferability. A large vision model, as a proxy, infers a new task's
representation in the transfer space, thereby circumventing the computational
burden of extensive forward passes. We also investigate the impact of the
inherent inductive bias of models on transfer results and propose a novel
method called \textbf{archi2vec} to encode the intricate structures of models.
The transfer score is computed through straightforward vector arithmetic with a
time complexity of $\mathcal{O}(1)$. Finally, we make a substantial
contribution to the field by releasing a comprehensive benchmark. We validate
the effectiveness of our framework through rigorous testing on two benchmarks.
The benchmark and the code will be publicly available in the near future.
\\ ( https://arxiv.org/abs/2403.06382 ,  1745kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06420 (*cross-listing*)
Date: Mon, 11 Mar 2024 04:13:26 GMT   (7314kb,D)

Title: RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic
  Manipulations With Large Language Models
Authors: Liangliang Chen, Yutian Lei, Shiyu Jin, Ying Zhang, Liangjun Zhang
Categories: cs.RO cs.AI cs.HC cs.LG
\\
  Reinforcement learning (RL) has demonstrated its capability in solving
various tasks but is notorious for its low sample efficiency. In this paper, we
propose RLingua, a framework that can leverage the internal knowledge of large
language models (LLMs) to reduce the sample complexity of RL in robotic
manipulations. To this end, we first present how to extract the prior knowledge
of LLMs by prompt engineering so that a preliminary rule-based robot controller
for a specific task can be generated. Despite being imperfect, the
LLM-generated robot controller is utilized to produce action samples during
rollouts with a decaying probability, thereby improving RL's sample efficiency.
We employ the actor-critic framework and modify the actor loss to regularize
the policy learning towards the LLM-generated controller. RLingua also provides
a novel method of improving the imperfect LLM-generated robot controllers by
RL. We demonstrated that RLingua can significantly reduce the sample complexity
of TD3 in the robot tasks of panda_gym and achieve high success rates in
sparsely rewarded robot tasks in RLBench, where the standard TD3 fails.
Additionally, We validated RLingua's effectiveness in real-world robot
experiments through Sim2Real, demonstrating that the learned policies are
effectively transferable to real robot tasks. Further details and videos about
our work are available at our project website https://rlingua.github.io.
\\ ( https://arxiv.org/abs/2403.06420 ,  7314kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06433 (*cross-listing*)
Date: Mon, 11 Mar 2024 04:58:36 GMT   (393kb,D)

Title: Fine-Grained Pillar Feature Encoding Via Spatio-Temporal Virtual Grid
  for 3D Object Detection
Authors: Konyul Park, Yecheol Kim, Junho Koh, Byungwoo Park, Jun Won Choi
Categories: cs.CV cs.AI
Comments: ICRA 2024
\\
  Developing high-performance, real-time architectures for LiDAR-based 3D
object detectors is essential for the successful commercialization of
autonomous vehicles. Pillar-based methods stand out as a practical choice for
onboard deployment due to their computational efficiency. However, despite
their efficiency, these methods can sometimes underperform compared to
alternative point encoding techniques such as Voxel-encoding or PointNet++. We
argue that current pillar-based methods have not sufficiently captured the
fine-grained distributions of LiDAR points within each pillar structure.
Consequently, there exists considerable room for improvement in pillar feature
encoding. In this paper, we introduce a novel pillar encoding architecture
referred to as Fine-Grained Pillar Feature Encoding (FG-PFE). FG-PFE utilizes
Spatio-Temporal Virtual (STV) grids to capture the distribution of point clouds
within each pillar across vertical, temporal, and horizontal dimensions.
Through STV grids, points within each pillar are individually encoded using
Vertical PFE (V-PFE), Temporal PFE (T-PFE), and Horizontal PFE (H-PFE). These
encoded features are then aggregated through an Attentive Pillar Aggregation
method. Our experiments conducted on the nuScenes dataset demonstrate that
FG-PFE achieves significant performance improvements over baseline models such
as PointPillar, CenterPoint-Pillar, and PillarNet, with only a minor increase
in computational overhead.
\\ ( https://arxiv.org/abs/2403.06433 ,  393kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06447 (*cross-listing*)
Date: Mon, 11 Mar 2024 05:49:34 GMT   (448kb,D)

Title: CoRAL: Collaborative Retrieval-Augmented Large Language Models Improve
  Long-tail Recommendation
Authors: Junda Wu, Cheng-Chun Chang, Tong Yu, Zhankui He, Jianing Wang, Yupeng
  Hou, Julian McAuley
Categories: cs.IR cs.AI
Comments: 11 pages
\\
  The long-tail recommendation is a challenging task for traditional
recommender systems, due to data sparsity and data imbalance issues. The recent
development of large language models (LLMs) has shown their abilities in
complex reasoning, which can help to deduce users' preferences based on very
few previous interactions. However, since most LLM-based systems rely on items'
semantic meaning as the sole evidence for reasoning, the collaborative
information of user-item interactions is neglected, which can cause the LLM's
reasoning to be misaligned with task-specific collaborative information of the
dataset. To further align LLMs' reasoning to task-specific user-item
interaction knowledge, we introduce collaborative retrieval-augmented LLMs,
CoRAL, which directly incorporate collaborative evidence into the prompts.
Based on the retrieved user-item interactions, the LLM can analyze shared and
distinct preferences among users, and summarize the patterns indicating which
types of users would be attracted by certain items. The retrieved collaborative
evidence prompts the LLM to align its reasoning with the user-item interaction
patterns in the dataset. However, since the capacity of the input prompt is
limited, finding the minimally-sufficient collaborative information for
recommendation tasks can be challenging. We propose to find the optimal
interaction set through a sequential decision-making process and develop a
retrieval policy learned through a reinforcement learning (RL) framework,
CoRAL. Our experimental results show that CoRAL can significantly improve LLMs'
reasoning abilities on specific recommendation tasks. Our analysis also reveals
that CoRAL can more efficiently explore collaborative information through
reinforcement learning.
\\ ( https://arxiv.org/abs/2403.06447 ,  448kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06465 (*cross-listing*)
Date: Mon, 11 Mar 2024 07:07:02 GMT   (803kb,D)

Title: RecAI: Leveraging Large Language Models for Next-Generation Recommender
  Systems
Authors: Jianxun Lian, Yuxuan Lei, Xu Huang, Jing Yao, Wei Xu, Xing Xie
Categories: cs.IR cs.AI
Comments: 4 pages. Webconf 2024 demo track
MSC-class: 68T50
DOI: 10.1145/3589335.3651242
\\
  This paper introduces RecAI, a practical toolkit designed to augment or even
revolutionize recommender systems with the advanced capabilities of Large
Language Models (LLMs). RecAI provides a suite of tools, including Recommender
AI Agent, Recommendation-oriented Language Models, Knowledge Plugin,
RecExplainer, and Evaluator, to facilitate the integration of LLMs into
recommender systems from multifaceted perspectives. The new generation of
recommender systems, empowered by LLMs, are expected to be more versatile,
explainable, conversational, and controllable, paving the way for more
intelligent and user-centric recommendation experiences. We hope the
open-source of RecAI can help accelerate evolution of new advanced recommender
systems. The source code of RecAI is available at
\url{https://github.com/microsoft/RecAI}.
\\ ( https://arxiv.org/abs/2403.06465 ,  803kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06479 (*cross-listing*)
Date: Mon, 11 Mar 2024 07:42:40 GMT   (5614kb,D)

Title: Ada-Tracker: Soft Tissue Tracking via Inter-Frame and Adaptive-Template
  Matching
Authors: Jiaxin Guo, Jiangliu Wang, Zhaoshuo Li, Tongyu Jia, Qi Dou, Yun-Hui
  Liu
Categories: cs.CV cs.AI
\\
  Soft tissue tracking is crucial for computer-assisted interventions. Existing
approaches mainly rely on extracting discriminative features from the template
and videos to recover corresponding matches. However, it is difficult to adopt
these techniques in surgical scenes, where tissues are changing in shape and
appearance throughout the surgery. To address this problem, we exploit optical
flow to naturally capture the pixel-wise tissue deformations and adaptively
correct the tracked template. Specifically, we first implement an inter-frame
matching mechanism to extract a coarse region of interest based on optical flow
from consecutive frames. To accommodate appearance change and alleviate drift,
we then propose an adaptive-template matching method, which updates the tracked
template based on the reliability of the estimates. Our approach, Ada-Tracker,
enjoys both short-term dynamics modeling by capturing local deformations and
long-term dynamics modeling by introducing global temporal compensation. We
evaluate our approach on the public SurgT benchmark, which is generated from
Hamlyn, SCARED, and Kidney boundary datasets. The experimental results show
that Ada-Tracker achieves superior accuracy and performs more robustly against
prior works. Code is available at https://github.com/wrld/Ada-Tracker.
\\ ( https://arxiv.org/abs/2403.06479 ,  5614kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06514 (*cross-listing*)
Date: Mon, 11 Mar 2024 08:40:37 GMT   (29324kb,D)

Title: Structure Your Data: Towards Semantic Graph Counterfactuals
Authors: Angeliki Dimitriou, Maria Lymperaiou, Giorgos Filandrianos,
  Konstantinos Thomas, Giorgos Stamou
Categories: cs.CV cs.AI
\\
  Counterfactual explanations (CEs) based on concepts are explanations that
consider alternative scenarios to understand which high-level semantic features
contributed to particular model predictions. In this work, we propose CEs based
on the semantic graphs accompanying input data to achieve more descriptive,
accurate, and human-aligned explanations. Building upon state-of-the-art (SoTA)
conceptual attempts, we adopt a model-agnostic edit-based approach and
introduce leveraging GNNs for efficient Graph Edit Distance (GED) computation.
With a focus on the visual domain, we represent images as scene graphs and
obtain their GNN embeddings to bypass solving the NP-hard graph similarity
problem for all input pairs, an integral part of the CE computation process. We
apply our method to benchmark and real-world datasets with varying difficulty
and availability of semantic annotations. Testing on diverse classifiers, we
find that our CEs outperform previous SoTA explanation models based on
semantics, including both white and black-box as well as conceptual and
pixel-level approaches. Their superiority is proven quantitatively and
qualitatively, as validated by human subjects, highlighting the significance of
leveraging semantic edges in the presence of intricate relationships. Our
model-agnostic graph-based approach is widely applicable and easily extensible,
producing actionable explanations across different contexts.
\\ ( https://arxiv.org/abs/2403.06514 ,  29324kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06517 (*cross-listing*)
Date: Mon, 11 Mar 2024 08:45:31 GMT   (2536kb,D)

Title: Active Generation for Image Classification
Authors: Tao Huang, Jiaqi Liu, Shan You, Chang Xu
Categories: cs.CV cs.AI
\\
  Recently, the growing capabilities of deep generative models have underscored
their potential in enhancing image classification accuracy. However, existing
methods often demand the generation of a disproportionately large number of
images compared to the original dataset, while having only marginal
improvements in accuracy. This computationally expensive and time-consuming
process hampers the practicality of such approaches. In this paper, we propose
to address the efficiency of image generation by focusing on the specific needs
and characteristics of the model. With a central tenet of active learning, our
method, named ActGen, takes a training-aware approach to image generation. It
aims to create images akin to the challenging or misclassified samples
encountered by the current model and incorporates these generated images into
the training set to augment model performance. ActGen introduces an attentive
image guidance technique, using real images as guides during the denoising
process of a diffusion model. The model's attention on class prompt is
leveraged to ensure the preservation of similar foreground object while
diversifying the background. Furthermore, we introduce a gradient-based
generation guidance method, which employs two losses to generate more
challenging samples and prevent the generated images from being too similar to
previously generated ones. Experimental results on the CIFAR and ImageNet
datasets demonstrate that our method achieves better performance with a
significantly reduced number of generated images.
\\ ( https://arxiv.org/abs/2403.06517 ,  2536kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06534 (*cross-listing*)
Date: Mon, 11 Mar 2024 09:20:40 GMT   (2278kb,D)

Title: SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale
  SAR Object Detection
Authors: Yuxuan Li, Xiang Li, Weijie Li, Qibin Hou, Li Liu, Ming-Ming Cheng,
  Jian Yang
Categories: cs.CV cs.AI cs.CE cs.LG
Comments: 22 Pages, 10 Figures, 9 Tables
\\
  Synthetic Aperture Radar (SAR) object detection has gained significant
attention recently due to its irreplaceable all-weather imaging capabilities.
However, this research field suffers from both limited public datasets (mostly
comprising <2K images with only mono-category objects) and inaccessible source
code. To tackle these challenges, we establish a new benchmark dataset and an
open-source method for large-scale SAR object detection. Our dataset,
SARDet-100K, is a result of intense surveying, collecting, and standardizing 10
existing SAR detection datasets, providing a large-scale and diverse dataset
for research purposes. To the best of our knowledge, SARDet-100K is the first
COCO-level large-scale multi-class SAR object detection dataset ever created.
With this high-quality dataset, we conducted comprehensive experiments and
uncovered a crucial challenge in SAR object detection: the substantial
disparities between the pretraining on RGB datasets and finetuning on SAR
datasets in terms of both data domain and model structure. To bridge these
gaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA)
pretraining framework that tackles the problems from the perspective of data
input, domain transition, and model migration. The proposed MSFA method
significantly enhances the performance of SAR object detection models while
demonstrating exceptional generalizability and flexibility across diverse
models. This work aims to pave the way for further advancements in SAR object
detection. The dataset and code is available at
https://github.com/zcablii/SARDet_100K.
\\ ( https://arxiv.org/abs/2403.06534 ,  2278kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06545 (*cross-listing*)
Date: Mon, 11 Mar 2024 09:45:34 GMT   (14054kb,D)

Title: ReStainGAN: Leveraging IHC to IF Stain Domain Translation for in-silico
  Data Generation
Authors: Dominik Winter, Nicolas Triltsch, Philipp Plewa, Marco Rosati, Thomas
  Padel, Ross Hill, Markus Schick, Nicolas Brieu
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: 4 pages, 1 figure
MSC-class: I.2.10, J.3, I.4.6
\\
  The creation of in-silico datasets can expand the utility of existing
annotations to new domains with different staining patterns in computational
pathology. As such, it has the potential to significantly lower the cost
associated with building large and pixel precise datasets needed to train
supervised deep learning models. We propose a novel approach for the generation
of in-silico immunohistochemistry (IHC) images by disentangling morphology
specific IHC stains into separate image channels in immunofluorescence (IF)
images. The proposed approach qualitatively and quantitatively outperforms
baseline methods as proven by training nucleus segmentation models on the
created in-silico datasets.
\\ ( https://arxiv.org/abs/2403.06545 ,  14054kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06592 (*cross-listing*)
Date: Mon, 11 Mar 2024 10:35:58 GMT   (813kb,D)

Title: Exploiting Style Latent Flows for Generalizing Deepfake Detection Video
  Detection
Authors: Jongwook Choi, Taehoon Kim, Yonghyun Jeong, Seungryul Baek, Jongwon
  Choi
Categories: cs.CV cs.AI
Comments: Preprint version, final version will be available at
  https://openaccess.thecvf.com The IEEE / CVF Computer Vision and Pattern
  Recognition Conference (CVPR) (2024) Published by: IEEE & CVF
\\
  This paper presents a new approach for the detection of fake videos, based on
the analysis of style latent vectors and their abnormal behavior in temporal
changes in the generated videos. We discovered that the generated facial videos
suffer from the temporal distinctiveness in the temporal changes of style
latent vectors, which are inevitable during the generation of temporally stable
videos with various facial expressions and geometric transformations. Our
framework utilizes the StyleGRU module, trained by contrastive learning, to
represent the dynamic properties of style latent vectors. Additionally, we
introduce a style attention module that integrates StyleGRU-generated features
with content-based features, enabling the detection of visual and temporal
artifacts. We demonstrate our approach across various benchmark scenarios in
deepfake detection, showing its superiority in cross-dataset and
cross-manipulation scenarios. Through further analysis, we also validate the
importance of using temporal changes of style latent vectors to improve the
generality of deepfake video detection.
\\ ( https://arxiv.org/abs/2403.06592 ,  813kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06601 (*cross-listing*)
Date: Mon, 11 Mar 2024 10:48:56 GMT   (43391kb,D)

Title: Cross-domain and Cross-dimension Learning for Image-to-Graph
  Transformers
Authors: Alexander H. Berger, Laurin Lux, Suprosanna Shit, Ivan Ezhov, Georgios
  Kaissis, Martin J. Menten, Daniel Rueckert, Johannes C. Paetzold
Categories: cs.CV cs.AI
\\
  Direct image-to-graph transformation is a challenging task that solves object
detection and relationship prediction in a single model. Due to the complexity
of this task, large training datasets are rare in many domains, which makes the
training of large networks challenging. This data sparsity necessitates the
establishment of pre-training strategies akin to the state-of-the-art in
computer vision. In this work, we introduce a set of methods enabling
cross-domain and cross-dimension transfer learning for image-to-graph
transformers. We propose (1) a regularized edge sampling loss for sampling the
optimal number of object relationships (edges) across domains, (2) a domain
adaptation framework for image-to-graph transformers that aligns features from
different domains, and (3) a simple projection function that allows us to
pretrain 3D transformers on 2D input data. We demonstrate our method's utility
in cross-domain and cross-dimension experiments, where we pretrain our models
on 2D satellite images before applying them to vastly different target domains
in 2D and 3D. Our method consistently outperforms a series of baselines on
challenging benchmarks, such as retinal or whole-brain vessel graph extraction.
\\ ( https://arxiv.org/abs/2403.06601 ,  43391kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06621 (*cross-listing*)
Date: Mon, 11 Mar 2024 11:26:44 GMT   (13483kb,D)

Title: Forest Inspection Dataset for Aerial Semantic Segmentation and Depth
  Estimation
Authors: Bianca-Cerasela-Zelia Blaga and Sergiu Nedevschi
Categories: cs.CV cs.AI
\\
  Humans use UAVs to monitor changes in forest environments since they are
lightweight and provide a large variety of surveillance data. However, their
information does not present enough details for understanding the scene which
is needed to assess the degree of deforestation. Deep learning algorithms must
be trained on large amounts of data to output accurate interpretations, but
ground truth recordings of annotated forest imagery are not available. To solve
this problem, we introduce a new large aerial dataset for forest inspection
which contains both real-world and virtual recordings of natural environments,
with densely annotated semantic segmentation labels and depth maps, taken in
different illumination conditions, at various altitudes and recording angles.
We test the performance of two multi-scale neural networks for solving the
semantic segmentation task (HRNet and PointFlow network), studying the impact
of the various acquisition conditions and the capabilities of transfer learning
from virtual to real data. Our results showcase that the best results are
obtained when the training is done on a dataset containing a large variety of
scenarios, rather than separating the data into specific categories. We also
develop a framework to assess the deforestation degree of an area.
\\ ( https://arxiv.org/abs/2403.06621 ,  13483kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06642 (*cross-listing*)
Date: Mon, 11 Mar 2024 12:04:20 GMT   (132kb,D)

Title: KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation
Authors: Weiqing Luo, Chonggang Song, Lingling Yi, Gong Cheng
Categories: cs.IR cs.AI cs.CL
Comments: 9 pages, 1 figure
\\
  The utilization of semantic information is an important research problem in
the field of recommender systems, which aims to complement the missing parts of
mainstream ID-based approaches. With the rise of LLM, its ability to act as a
knowledge base and its reasoning capability have opened up new possibilities
for this research area, making LLM-based recommendation an emerging research
direction. However, directly using LLM to process semantic information for
recommendation scenarios is unreliable and sub-optimal due to several problems
such as hallucination. A promising way to cope with this is to use external
knowledge to aid LLM in generating truthful and usable text. Inspired by the
above motivation, we propose a Knowledge-Enhanced LLMRec method. In addition to
using external knowledge in prompts, the proposed method also includes a
knowledge-based contrastive learning scheme for training. Experiments on public
datasets and in-enterprise datasets validate the effectiveness of the proposed
method.
\\ ( https://arxiv.org/abs/2403.06642 ,  132kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06659 (*cross-listing*)
Date: Mon, 11 Mar 2024 12:28:55 GMT   (31833kb,D)

Title: Zero-Shot ECG Classification with Multimodal Learning and Test-time
  Clinical Knowledge Enhancement
Authors: Che Liu, Zhongwei Wan, Cheng Ouyang, Anand Shah, Wenjia Bai, Rossella
  Arcucci
Categories: eess.SP cs.AI cs.LG
Comments: Under Review
\\
  Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for
detecting cardiac arrhythmic diseases in clinical practice. While ECG
Self-supervised Learning (eSSL) methods show promise in representation learning
from unannotated ECG data, they often overlook the clinical knowledge that can
be found in reports. This oversight and the requirement for annotated samples
for downstream tasks limit eSSL's versatility. In this work, we address these
issues with the Multimodal ECG Representation Learning (MERL}) framework.
Through multimodal learning on ECG records and associated reports, MERL is
capable of performing zero-shot ECG classification with text prompts,
eliminating the need for training data in downstream tasks. At test time, we
propose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach,
which uses Large Language Models (LLMs) to exploit external expert-verified
clinical knowledge databases, generating more descriptive prompts and reducing
hallucinations in LLM-generated content to boost zero-shot classification.
Based on MERL, we perform the first benchmark across six public ECG datasets,
showing the superior performance of MERL compared against eSSL methods.
Notably, MERL achieves an average AUC score of 75.2% in zero-shot
classification (without training data), 3.2% higher than linear probed eSSL
methods with 10\% annotated training data, averaged across all six datasets.
\\ ( https://arxiv.org/abs/2403.06659 ,  31833kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06660 (*cross-listing*)
Date: Mon, 11 Mar 2024 12:29:35 GMT   (954kb,D)

Title: FashionReGen: LLM-Empowered Fashion Report Generation
Authors: Yujuan Ding, Yunshan Ma, Wenqi Fan, Yige Yao, Tat-Seng Chua, Qing Li
Categories: cs.MM cs.AI cs.MA
\\
  Fashion analysis refers to the process of examining and evaluating trends,
styles, and elements within the fashion industry to understand and interpret
its current state, generating fashion reports. It is traditionally performed by
fashion professionals based on their expertise and experience, which requires
high labour cost and may also produce biased results for relying heavily on a
small group of people. In this paper, to tackle the Fashion Report Generation
(FashionReGen) task, we propose an intelligent Fashion Analyzing and Reporting
system based the advanced Large Language Models (LLMs), debbed as GPT-FAR.
Specifically, it tries to deliver FashionReGen based on effective catwalk
analysis, which is equipped with several key procedures, namely, catwalk
understanding, collective organization and analysis, and report generation. By
posing and exploring such an open-ended, complex and domain-specific task of
FashionReGen, it is able to test the general capability of LLMs in fashion
domain. It also inspires the explorations of more high-level tasks with
industrial significance in other domains. Video illustration and more materials
of GPT-FAR can be found in https://github.com/CompFashion/FashionReGen.
\\ ( https://arxiv.org/abs/2403.06660 ,  954kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06670 (*cross-listing*)
Date: Mon, 11 Mar 2024 12:40:12 GMT   (6526kb,D)

Title: CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar
  Class-Incremental Learnin
Authors: Xinyuan Gao, Songlin Dong, Yuhang He, Xing Wei, Yihong Gong
Categories: cs.CV cs.AI
\\
  In real-world applications, dynamic scenarios require the models to possess
the capability to learn new tasks continuously without forgetting the old
knowledge. Experience-Replay methods store a subset of the old images for joint
training. In the scenario of more strict privacy protection, storing the old
images becomes infeasible, which leads to a more severe plasticity-stability
dilemma and classifier bias. To meet the above challenges, we propose a new
architecture, named continual expansion and absorption transformer~(CEAT). The
model can learn the novel knowledge by extending the expanded-fusion layers in
parallel with the frozen previous parameters. After the task ends, we
losslessly absorb the extended parameters into the backbone to ensure that the
number of parameters remains constant. To improve the learning ability of the
model, we designed a novel prototype contrastive loss to reduce the overlap
between old and new classes in the feature space. Besides, to address the
classifier bias towards the new classes, we propose a novel approach to
generate the pseudo-features to correct the classifier. We experiment with our
methods on three standard Non-Exemplar Class-Incremental Learning~(NECIL)
benchmarks. Extensive experiments demonstrate that our model gets a significant
improvement compared with the previous works and achieves 5.38%, 5.20%, and
4.92% improvement on CIFAR-100, TinyImageNet, and ImageNet-Subset.
\\ ( https://arxiv.org/abs/2403.06670 ,  6526kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06674 (*cross-listing*)
Date: Mon, 11 Mar 2024 12:46:53 GMT   (471kb)

Title: Car Damage Detection and Patch-to-Patch Self-supervised Image Alignment
Authors: Hanxiao Chen
Categories: cs.CV cs.AI
Comments: The paper has been accepted and given a poster presentation at
  NeurIPS 2021 WiML Workshop
  (https://nips.cc/virtual/2021/affinity-workshop/22882)
\\
  Most computer vision applications aim to identify pixels in a scene and use
them for diverse purposes. One intriguing application is car damage detection
for insurance carriers which tends to detect all car damages by comparing both
pre-trip and post-trip images, even requiring two components: (i) car damage
detection; (ii) image alignment. Firstly, we implemented a Mask R-CNN model to
detect car damages on custom images. Whereas for the image alignment section,
we especially propose a novel self-supervised Patch-to-Patch SimCLR inspired
alignment approach to find perspective transformations between custom pre/post
car rental images except for traditional computer vision methods.
\\ ( https://arxiv.org/abs/2403.06674 ,  471kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06675 (*cross-listing*)
Date: Mon, 11 Mar 2024 12:47:04 GMT   (963kb,D)

Title: Poisoning Programs by Un-Repairing Code: Security Concerns of
  AI-generated Code
Authors: Cristina Improta
Categories: cs.CR cs.AI cs.SE
Comments: Accepted at The 1st IEEE International Workshop on Reliable and
  Secure AI for Software Engineering (ReSAISE), co-located with ISSRE 2023
DOI: 10.1109/ISSREW60843.2023.00060
\\
  AI-based code generators have gained a fundamental role in assisting
developers in writing software starting from natural language (NL). However,
since these large language models are trained on massive volumes of data
collected from unreliable online sources (e.g., GitHub, Hugging Face), AI
models become an easy target for data poisoning attacks, in which an attacker
corrupts the training data by injecting a small amount of poison into it, i.e.,
astutely crafted malicious samples. In this position paper, we address the
security of AI code generators by identifying a novel data poisoning attack
that results in the generation of vulnerable code. Next, we devise an extensive
evaluation of how these attacks impact state-of-the-art models for code
generation. Lastly, we discuss potential solutions to overcome this threat.
\\ ( https://arxiv.org/abs/2403.06675 ,  963kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06725 (*cross-listing*)
Date: Mon, 11 Mar 2024 13:44:43 GMT   (455kb,D)

Title: Improving Low-Resource Knowledge Tracing Tasks by Supervised
  Pre-training and Importance Mechanism Fine-tuning
Authors: Hengyuan Zhang, Zitao Liu, Shuyan Huang, Chenming Shang, Bojun Zhan,
  Yong Jiang
Categories: cs.CY cs.AI cs.CL cs.LG
Comments: 29 pages, 4 figures
\\
  Knowledge tracing (KT) aims to estimate student's knowledge mastery based on
their historical interactions. Recently, the deep learning based KT (DLKT)
approaches have achieved impressive performance in the KT task. These DLKT
models heavily rely on the large number of available student interactions.
However, due to various reasons such as budget constraints and privacy
concerns, observed interactions are very limited in many real-world scenarios,
a.k.a, low-resource KT datasets. Directly training a DLKT model on a
low-resource KT dataset may lead to overfitting and it is difficult to choose
the appropriate deep neural architecture. Therefore, in this paper, we propose
a low-resource KT framework called LoReKT to address above challenges. Inspired
by the prevalent "pre-training and fine-tuning" paradigm, we aim to learn
transferable parameters and representations from rich-resource KT datasets
during the pre-training stage and subsequently facilitate effective adaptation
to low-resource KT datasets. Specifically, we simplify existing sophisticated
DLKT model architectures with purely a stack of transformer decoders. We design
an encoding mechanism to incorporate student interactions from multiple KT data
sources and develop an importance mechanism to prioritize updating parameters
with high importance while constraining less important ones during the
fine-tuning stage. We evaluate LoReKT on six public KT datasets and
experimental results demonstrate the superiority of our approach in terms of
AUC and Accuracy. To encourage reproducible research, we make our data and code
publicly available at https://anonymous.4open.science/r/LoReKT-C619.
\\ ( https://arxiv.org/abs/2403.06725 ,  455kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06735 (*cross-listing*)
Date: Mon, 11 Mar 2024 13:57:05 GMT   (730kb)

Title: Enhancing Image Caption Generation Using Reinforcement Learning with
  Human Feedback
Authors: Adarsh N L, Arun P V, Aravindh N L
Categories: cs.CV cs.AI
Comments: 6 Pages, 8 figures
\\
  Research on generative models to produce human-aligned / human-preferred
outputs has seen significant recent contributions. Between text and
image-generative models, we narrowed our focus to text-based generative models,
particularly to produce captions for images that align with human preferences.
In this research, we explored a potential method to amplify the performance of
the Deep Neural Network Model to generate captions that are preferred by
humans. This was achieved by integrating Supervised Learning and Reinforcement
Learning with Human Feedback (RLHF) using the Flickr8k dataset. Also, a novel
loss function that is capable of optimizing the model based on human feedback
is introduced. In this paper, we provide a concise sketch of our approach and
results, hoping to contribute to the ongoing advances in the field of
human-aligned generative AI models.
\\ ( https://arxiv.org/abs/2403.06735 ,  730kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06764 (*cross-listing*)
Date: Mon, 11 Mar 2024 14:35:32 GMT   (8772kb,D)

Title: An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference
  Acceleration for Large Vision-Language Models
Authors: Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang
  Zhou, Baobao Chang
Categories: cs.CV cs.AI cs.CL
Comments: 21 papes, 8 figures, code is released at
  https://github.com/pkunlp-icler/FastV
\\
  In this study, we identify the inefficient attention phenomena in Large
Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5,
QwenVL-Chat and Video-LLaVA. We find out that the attention computation over
visual tokens is of extreme inefficiency in the deep layers of popular LVLMs,
suggesting a need for a sparser approach compared to textual data handling. To
this end, we introduce FastV, a versatile plug-and-play method designed to
optimize computational efficiency by learning adaptive attention patterns in
early layers and pruning visual tokens in subsequent ones. Our evaluations
demonstrate FastV's ability to dramatically reduce computational costs (e.g., a
45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a
wide range of image and video understanding tasks. The computational efficiency
and performance trade-off of FastV are highly customizable and
pareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve
a lower budget than that of a 7B-parameter model, while still maintaining
superior performance. We believe FastV has practical values for deployment of
LVLMs in edge devices and commercial models. Code is released at
https://github.com/pkunlp-icler/FastV.
\\ ( https://arxiv.org/abs/2403.06764 ,  8772kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06786 (*cross-listing*)
Date: Mon, 11 Mar 2024 15:00:56 GMT   (130kb,D)

Title: Genetic Learning for Designing Sim-to-Real Data Augmentations
Authors: Bram Vanherle, Nick Michiels, Frank Van Reeth
Categories: cs.CV cs.AI
Comments: 21 pages; accepted at DMLR Workshop @ ICRL 2024
\\
  Data augmentations are useful in closing the sim-to-real domain gap when
training on synthetic data. This is because they widen the training data
distribution, thus encouraging the model to generalize better to other domains.
Many image augmentation techniques exist, parametrized by different settings,
such as strength and probability. This leads to a large space of different
possible augmentation policies. Some policies work better than others for
overcoming the sim-to-real gap for specific datasets, and it is unclear why.
This paper presents two different interpretable metrics that can be combined to
predict how well a certain augmentation policy will work for a specific
sim-to-real setting, focusing on object detection. We validate our metrics by
training many models with different augmentation policies and showing a strong
correlation with performance on real data. Additionally, we introduce
GeneticAugment, a genetic programming method that can leverage these metrics to
automatically design an augmentation policy for a specific dataset without
needing to train a model.
\\ ( https://arxiv.org/abs/2403.06786 ,  130kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06817 (*cross-listing*)
Date: Mon, 11 Mar 2024 15:34:57 GMT   (60kb,D)

Title: Are Targeted Messages More Effective?
Authors: Martin Grohe, Eran Rosenbluth
Categories: cs.LO cs.AI cs.LG
MSC-class: 68T05, 68T07
ACM-class: I.2.6
\\
  Graph neural networks (GNN) are deep learning architectures for graphs.
Essentially, a GNN is a distributed message passing algorithm, which is
controlled by parameters learned from data. It operates on the vertices of a
graph: in each iteration, vertices receive a message on each incoming edge,
aggregate these messages, and then update their state based on their current
state and the aggregated messages. The expressivity of GNNs can be
characterised in terms of certain fragments of first-order logic with counting
and the Weisfeiler-Lehman algorithm.
  The core GNN architecture comes in two different versions. In the first
version, a message only depends on the state of the source vertex, whereas in
the second version it depends on the states of the source and target vertices.
In practice, both of these versions are used, but the theory of GNNs so far
mostly focused on the first one. On the logical side, the two versions
correspond to two fragments of first-order logic with counting that we call
modal and guarded.
  The question whether the two versions differ in their expressivity has been
mostly overlooked in the GNN literature and has only been asked recently
(Grohe, LICS'23). We answer this question here. It turns out that the answer is
not as straightforward as one might expect. By proving that the modal and
guarded fragment of first-order logic with counting have the same expressivity
over labelled undirected graphs, we show that in a non-uniform setting the two
GNN versions have the same expressivity. However, we also prove that in a
uniform setting the second version is strictly more expressive.
\\ ( https://arxiv.org/abs/2403.06817 ,  60kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06828 (*cross-listing*)
Date: Mon, 11 Mar 2024 15:44:38 GMT   (17061kb,D)

Title: NeuPAN: Direct Point Robot Navigation with End-to-End Model-based
  Learning
Authors: Ruihua Han, Shuai Wang, Shuaijun Wang, Zeqing Zhang, Jianjun Chen,
  Shijie Lin, Chengyang Li, Chengzhong Xu, Yonina C. Eldar, Qi Hao, Jia Pan
Categories: cs.RO cs.AI
Comments: submit to TRO
\\
  Navigating a nonholonomic robot in a cluttered environment requires extremely
accurate perception and locomotion for collision avoidance. This paper presents
NeuPAN: a real-time, highly-accurate, map-free, robot-agnostic, and
environment-invariant robot navigation solution. Leveraging a tightly-coupled
perception-locomotion framework, NeuPAN has two key innovations compared to
existing approaches: 1) it directly maps raw points to a learned multi-frame
distance space, avoiding error propagation from perception to control; 2) it is
interpretable from an end-to-end model-based learning perspective, enabling
provable convergence. The crux of NeuPAN is to solve a high-dimensional
end-to-end mathematical model with various point-level constraints using the
plug-and-play (PnP) proximal alternating-minimization network (PAN) with
neurons in the loop. This allows NeuPAN to generate real-time, end-to-end,
physically-interpretable motions directly from point clouds, which seamlessly
integrates data- and knowledge-engines, where its network parameters are
adjusted via back propagation. We evaluate NeuPAN on car-like robot,
wheel-legged robot, and passenger autonomous vehicle, in both simulated and
real-world environments. Experiments demonstrate that NeuPAN outperforms
various benchmarks, in terms of accuracy, efficiency, robustness, and
generalization capability across various environments, including the cluttered
sandbox, office, corridor, and parking lot. We show that NeuPAN works well in
unstructured environments with arbitrary-shape undetectable objects, making
impassable ways passable.
\\ ( https://arxiv.org/abs/2403.06828 ,  17061kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06835 (*cross-listing*)
Date: Mon, 11 Mar 2024 15:56:17 GMT   (3686kb,D)

Title: Medical Image Synthesis via Fine-Grained Image-Text Alignment and
  Anatomy-Pathology Prompting
Authors: Wenting Chen, Pengyu Wang, Hui Ren, Lichao Sun, Quanzheng Li, Yixuan
  Yuan, and Xiang Li
Categories: cs.CV cs.AI cs.CL
Comments: 5 figures
\\
  Data scarcity and privacy concerns limit the availability of high-quality
medical images for public use, which can be mitigated through medical image
synthesis. However, current medical image synthesis methods often struggle to
accurately capture the complexity of detailed anatomical structures and
pathological conditions. To address these challenges, we propose a novel
medical image synthesis model that leverages fine-grained image-text alignment
and anatomy-pathology prompts to generate highly detailed and accurate
synthetic medical images. Our method integrates advanced natural language
processing techniques with image generative modeling, enabling precise
alignment between descriptive text prompts and the synthesized images'
anatomical and pathological details. The proposed approach consists of two key
components: an anatomy-pathology prompting module and a fine-grained
alignment-based synthesis module. The anatomy-pathology prompting module
automatically generates descriptive prompts for high-quality medical images. To
further synthesize high-quality medical images from the generated prompts, the
fine-grained alignment-based synthesis module pre-defines a visual codebook for
the radiology dataset and performs fine-grained alignment between the codebook
and generated prompts to obtain key patches as visual clues, facilitating
accurate image synthesis. We validate the superiority of our method through
experiments on public chest X-ray datasets and demonstrate that our synthetic
images preserve accurate semantic information, making them valuable for various
medical applications.
\\ ( https://arxiv.org/abs/2403.06835 ,  3686kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06901 (*cross-listing*)
Date: Mon, 11 Mar 2024 16:54:44 GMT   (6123kb,D)

Title: LIBR+: Improving Intraoperative Liver Registration by Learning the
  Residual of Biomechanics-Based Deformable Registration
Authors: Dingrong Wang, Soheil Azadvar, Jon Heiselman, Xiajun Jiang, Michael
  Miga, Linwei Wang
Categories: eess.IV cs.AI cs.LG
Comments: 12 pages, Medical Image Computing and Computer Assisted Intervention
  2024
\\
  The surgical environment imposes unique challenges to the intraoperative
registration of organ shapes to their preoperatively-imaged geometry.
Biomechanical model-based registration remains popular, while deep learning
solutions remain limited due to the sparsity and variability of intraoperative
measurements and the limited ground-truth deformation of an organ that can be
obtained during the surgery. In this paper, we propose a novel \textit{hybrid}
registration approach that leverage a linearized iterative boundary
reconstruction (LIBR) method based on linear elastic biomechanics, and use deep
neural networks to learn its residual to the ground-truth deformation (LIBR+).
We further formulate a dual-branch spline-residual graph convolutional neural
network (SR-GCN) to assimilate information from sparse and variable
intraoperative measurements and effectively propagate it through the geometry
of the 3D organ. Experiments on a large intraoperative liver registration
dataset demonstrated the consistent improvements achieved by LIBR+ in
comparison to existing rigid, biomechnical model-based non-rigid, and
deep-learning based non-rigid approaches to intraoperative liver registration.
\\ ( https://arxiv.org/abs/2403.06901 ,  6123kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06952 (*cross-listing*)
Date: Mon, 11 Mar 2024 17:35:33 GMT   (6998kb,D)

Title: SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with
  Auto-Generated Data
Authors: Jialu Li, Jaemin Cho, Yi-Lin Sung, Jaehong Yoon, Mohit Bansal
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: First two authors contributed equally; Project website:
  https://selma-t2i.github.io/
\\
  Recent text-to-image (T2I) generation models have demonstrated impressive
capabilities in creating images from text descriptions. However, these T2I
generation models often fall short of generating images that precisely match
the details of the text inputs, such as incorrect spatial relationship or
missing objects. In this paper, we introduce SELMA: Skill-Specific Expert
Learning and Merging with Auto-Generated Data, a novel paradigm to improve the
faithfulness of T2I models by fine-tuning models on automatically generated,
multi-skill image-text datasets, with skill-specific expert learning and
merging. First, SELMA leverages an LLM's in-context learning capability to
generate multiple datasets of text prompts that can teach different skills, and
then generates the images with a T2I model based on the prompts. Next, SELMA
adapts the T2I model to the new skills by learning multiple single-skill LoRA
(low-rank adaptation) experts followed by expert merging. Our independent
expert fine-tuning specializes multiple models for different skills, and expert
merging helps build a joint multi-skill T2I model that can generate faithful
images given diverse text prompts, while mitigating the knowledge conflict from
different datasets. We empirically demonstrate that SELMA significantly
improves the semantic alignment and text faithfulness of state-of-the-art T2I
diffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human
preference metrics (PickScore, ImageReward, and HPS), as well as human
evaluation. Moreover, fine-tuning with image-text pairs auto-collected via
SELMA shows comparable performance to fine-tuning with ground truth data.
Lastly, we show that fine-tuning with images from a weaker T2I model can help
improve the generation quality of a stronger T2I model, suggesting promising
weak-to-strong generalization in T2I models.
\\ ( https://arxiv.org/abs/2403.06952 ,  6998kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05553 (*cross-listing*)
Date: Sat, 10 Feb 2024 08:24:29 GMT   (8213kb,D)

Title: Understanding the Progression of Educational Topics via Semantic
  Matching
Authors: Tamador Alkhidir (1), Edmond Awad (2), Aamena Alshamsi (3) ((1)
  Curriculum Department, Ministry of Education, United Arab Emirates, (2)
  Department of Economics and Institute for Data Science and AI, University of
  Exeter, United Kingdom,(3) Heuristic World, Dubai, United Arab Emirates)
Categories: cs.CY cs.CL cs.LG
\\
  Education systems are dynamically changing to accommodate technological
advances, industrial and societal needs, and to enhance students' learning
journeys. Curriculum specialists and educators constantly revise taught
subjects across educational grades to identify gaps, introduce new learning
topics, and enhance the learning outcomes. This process is usually done within
the same subjects (e.g. math) or across related subjects (e.g. math and
physics) considering the same and different educational levels, leading to
massive multi-layer comparisons. Having nuanced data about subjects, topics,
and learning outcomes structured within a dataset, empowers us to leverage data
science to better understand the progression of various learning topics. In
this paper, Bidirectional Encoder Representations from Transformers (BERT)
topic modeling was used to extract topics from the curriculum, which were then
used to identify relationships between subjects, track their progression, and
identify conceptual gaps. We found that grouping learning outcomes by common
topics helped specialists reduce redundancy and introduce new concepts in the
curriculum. We built a dashboard to avail the methodology to curriculum
specials. Finally, we tested the validity of the approach with subject matter
experts.
\\ ( https://arxiv.org/abs/2403.05553 ,  8213kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05561 (*cross-listing*)
Date: Sat, 17 Feb 2024 10:32:43 GMT   (1212kb,D)

Title: Detecting a Proxy for Potential Comorbid ADHD in People Reporting
  Anxiety Symptoms from Social Media Data
Authors: Claire S. Lee, Noelle Lim, and Michael Guerzhoy
Categories: cs.CY cs.CL
Comments: Forthcoming in Proc. of the Workshop on Computational Linguistics and
  Clinical Psychology (CLPsych) at EACL 2024
\\
  We present a novel task that can elucidate the connection between anxiety and
ADHD; use Transformers to make progress toward solving a task that is not
solvable by keyword-based classifiers; and discuss a method for visualization
of our classifier illuminating the connection between anxiety and ADHD
presentations.
  Up to approximately 50% of adults with ADHD may also have an anxiety disorder
and approximately 30\% of adults with anxiety may also have ADHD. Patients
presenting with anxiety may be treated for anxiety without ADHD ever being
considered, possibly affecting treatment. We show how data that bears on ADHD
that is comorbid with anxiety can be obtained from social media data, and show
that Transformers can be used to detect a proxy for possible comorbid ADHD in
people with anxiety symptoms.
  We collected data from anxiety and ADHD online forums (subreddits). We
identified posters who first started posting in the Anxiety subreddit and later
started posting in the ADHD subreddit as well. We use this subset of the
posters as a proxy for people who presented with anxiety symptoms and then
became aware that they might have ADHD. We fine-tune a Transformer
architecture-based classifier to classify people who started posting in the
Anxiety subreddit and then started posting in the ADHD subreddit vs. people who
posted in the Anxiety subreddit without later posting in the ADHD subreddit. We
show that a Transformer architecture is capable of achieving reasonable results
(76% correct for RoBERTa vs. under 60% correct for the best keyword-based
model, both with 50% base rate).
\\ ( https://arxiv.org/abs/2403.05561 ,  1212kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05602 (*cross-listing*)
Date: Fri, 8 Mar 2024 01:43:21 GMT   (2012kb,D)

Title: Extracting Protein-Protein Interactions (PPIs) from Biomedical
  Literature using Attention-based Relational Context Information
Authors: Gilchan Park, Sean McCorkle, Carlos Soto, Ian Blaby, Shinjae Yoo
Categories: q-bio.BM cs.CL cs.LG
Comments: 10 pages, 3 figures, 7 tables, 2022 IEEE International Conference on
  Big Data (Big Data)
Journal-ref: Park, G., McCorkle, S., Soto, C., Blaby, I., & Yoo, S. (2022).
  Extracting Protein-Protein Interactions (PPIs) from Biomedical Literature
  using Attention-based Relational Context Information. In 2022 IEEE Big Data
  (pp. 2052-2061)
DOI: 10.1109/BigData55660.2022.10021099
\\
  Because protein-protein interactions (PPIs) are crucial to understand living
systems, harvesting these data is essential to probe disease development and
discern gene/protein functions and biological processes. Some curated datasets
contain PPI data derived from the literature and other sources (e.g., IntAct,
BioGrid, DIP, and HPRD). However, they are far from exhaustive, and their
maintenance is a labor-intensive process. On the other hand, machine learning
methods to automate PPI knowledge extraction from the scientific literature
have been limited by a shortage of appropriate annotated data. This work
presents a unified, multi-source PPI corpora with vetted interaction
definitions augmented by binary interaction type labels and a Transformer-based
deep learning method that exploits entities' relational context information for
relation representation to improve relation classification performance. The
model's performance is evaluated on four widely studied biomedical relation
extraction datasets, as well as this work's target PPI datasets, to observe the
effectiveness of the representation to relation extraction tasks in various
data. Results show the model outperforms prior state-of-the-art models. The
code and data are available at:
https://github.com/BNLNLP/PPI-Relation-Extraction
\\ ( https://arxiv.org/abs/2403.05602 ,  2012kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05820 (*cross-listing*)
Date: Sat, 9 Mar 2024 06:59:47 GMT   (277kb,D)

Title: An Audio-textual Diffusion Model For Converting Speech Signals Into
  Ultrasound Tongue Imaging Data
Authors: Yudong Yang, Rongfeng Su, Xiaokang Liu, Nan Yan, and Lan Wang
Categories: cs.SD cs.CL eess.AS
Comments: ICASSP2024 Accept
\\
  Acoustic-to-articulatory inversion (AAI) is to convert audio into articulator
movements, such as ultrasound tongue imaging (UTI) data. An issue of existing
AAI methods is only using the personalized acoustic information to derive the
general patterns of tongue motions, and thus the quality of generated UTI data
is limited. To address this issue, this paper proposes an audio-textual
diffusion model for the UTI data generation task. In this model, the inherent
acoustic characteristics of individuals related to the tongue motion details
are encoded by using wav2vec 2.0, while the ASR transcriptions related to the
universality of tongue motions are encoded by using BERT. UTI data are then
generated by using a diffusion module. Experimental results showed that the
proposed diffusion model could generate high-quality UTI data with clear tongue
contour that is crucial for the linguistic analysis and clinical assessment.
The project can be found on the
website\footnote{https://yangyudong2020.github.io/wav2uti/
\\ ( https://arxiv.org/abs/2403.05820 ,  277kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05846 (*cross-listing*)
Date: Sat, 9 Mar 2024 09:11:49 GMT   (21951kb,D)

Title: Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines
Authors: Michael Toker, Hadas Orgad, Mor Ventura, Dana Arad, Yonatan Belinkov
Categories: cs.CV cs.CL
Comments: Project webpage: tokeron.github.io/DiffusionLensWeb
ACM-class: I.2.7; I.4.0
\\
  Text-to-image diffusion models (T2I) use a latent representation of a text
prompt to guide the image generation process. However, the process by which the
encoder produces the text representation is unknown. We propose the Diffusion
Lens, a method for analyzing the text encoder of T2I models by generating
images from its intermediate representations. Using the Diffusion Lens, we
perform an extensive analysis of two recent T2I models. Exploring compound
prompts, we find that complex scenes describing multiple objects are composed
progressively and more slowly compared to simple scenes; Exploring knowledge
retrieval, we find that representation of uncommon concepts requires further
computation compared to common concepts, and that knowledge retrieval is
gradual across layers. Overall, our findings provide valuable insights into the
text encoder component in T2I pipelines.
\\ ( https://arxiv.org/abs/2403.05846 ,  21951kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06016 (*cross-listing*)
Date: Sat, 9 Mar 2024 21:29:40 GMT   (88kb,D)

Title: End-to-end solution for linked open data query logs analytics
Authors: Dihia Lanasri
Categories: cs.DB cs.CL
\\
  Important advances in pillar domains are derived from exploiting query-logs
which represents users interest and preferences. Deep understanding of users
provides useful knowledge which can influence strongly decision-making. In this
work, we want to extract valuable information from Linked Open Data (LOD)
query-logs. LOD logs have experienced significant growth due to the large
exploitation of LOD datasets. However, exploiting these logs is a difficult
task because of their complex structure. Moreover, these logs suffer from many
risks related to their Quality and Provenance, impacting their trust. To tackle
these issues, we start by clearly defining the ecosystem of LOD query-logs.
Then, we provide an end-to-end solution to exploit these logs. At the end, real
LOD logs are used and a set of experiments are conducted to validate the
proposed solution.
\\ ( https://arxiv.org/abs/2403.06016 ,  88kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06098 (*cross-listing*)
Date: Sun, 10 Mar 2024 05:40:12 GMT   (14272kb,D)

Title: VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video
  Diffusion Models
Authors: Wenhao Wang, Yi Yang
Categories: cs.CV cs.CL
Comments: Please download the collected dataset from
  https://github.com/WangWenhao0716/VidProM and
  https://huggingface.co/datasets/WenhaoWang/VidProM
\\
  The arrival of Sora marks a new era for text-to-video diffusion models,
bringing significant advancements in video generation and potential
applications. However, Sora, as well as other text-to-video diffusion models,
highly relies on the prompts, and there is no publicly available dataset
featuring a study of text-to-video prompts. In this paper, we introduce
VidProM, the first large-scale dataset comprising 1.67 million unique
text-to-video prompts from real users. Additionally, the dataset includes 6.69
million videos generated by four state-of-the-art diffusion models and some
related data. We initially demonstrate the curation of this large-scale
dataset, which is a time-consuming and costly process. Subsequently, we show
how the proposed VidProM differs from DiffusionDB, a large-scale prompt-gallery
dataset for image generation. Based on the analysis of these prompts, we
identify the necessity for a new prompt dataset specifically designed for
text-to-video generation and gain insights into the preferences of real users
when creating videos. Our large-scale and diverse dataset also inspires many
exciting new research areas. For instance, to develop better, more efficient,
and safer text-to-video diffusion models, we suggest exploring text-to-video
prompt engineering, efficient video generation, and video copy detection for
diffusion models. We make the collected dataset VidProM publicly available at
GitHub and Hugging Face under the CC-BY- NC 4.0 License.
\\ ( https://arxiv.org/abs/2403.06098 ,  14272kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06100 (*cross-listing*)
Date: Sun, 10 Mar 2024 05:55:00 GMT   (1477kb,D)

Title: Automatic design optimization of preference-based subjective evaluation
  with online learning in crowdsourcing environment
Authors: Yusuke Yasuda and Tomoki Toda
Categories: cs.HC cs.CL cs.LG eess.AS stat.ML
\\
  A preference-based subjective evaluation is a key method for evaluating
generative media reliably. However, its huge combinations of pairs prohibit it
from being applied to large-scale evaluation using crowdsourcing. To address
this issue, we propose an automatic optimization method for preference-based
subjective evaluation in terms of pair combination selections and allocation of
evaluation volumes with online learning in a crowdsourcing environment. We use
a preference-based online learning method based on a sorting algorithm to
identify the total order of evaluation targets with minimum sample volumes. Our
online learning algorithm supports parallel and asynchronous execution under
fixed-budget conditions required for crowdsourcing. Our experiment on
preference-based subjective evaluation of synthetic speech shows that our
method successfully optimizes the test by reducing pair combinations from 351
to 83 and allocating optimal evaluation volumes for each pair ranging from 30
to 663 without compromising evaluation accuracies and wasting budget
allocations.
\\ ( https://arxiv.org/abs/2403.06100 ,  1477kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06199 (*cross-listing*)
Date: Sun, 10 Mar 2024 12:43:27 GMT   (8354kb,D)

Title: A Comprehensive Overhaul of Multimodal Assistant with Small Language
  Models
Authors: Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu, Chaomin Shen,
  Yaxin Peng, Zhicai Ou, Feifei Feng, Jian Tang
Categories: cs.CV cs.CL
\\
  Multimodal Large Language Models (MLLMs) have showcased impressive skills in
tasks related to visual understanding and reasoning. Yet, their widespread
application faces obstacles due to the high computational demands during both
the training and inference phases, restricting their use to a limited audience
within the research and user communities. In this paper, we investigate the
design aspects of Multimodal Small Language Models (MSLMs) and propose an
efficient multimodal assistant named Mipha, which is designed to create synergy
among various aspects: visual representation, language models, and optimization
strategies. We show that without increasing the volume of training data, our
Mipha-3B outperforms the state-of-the-art large MLLMs, especially
LLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide
insights and guidelines for developing strong MSLMs that rival the capabilities
of MLLMs. Our code is available at https://github.com/zhuyiche/Mipha.
\\ ( https://arxiv.org/abs/2403.06199 ,  8354kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06249 (*cross-listing*)
Date: Sun, 10 Mar 2024 16:22:20 GMT   (559kb,D)

Title: No Language is an Island: Unifying Chinese and English in Financial
  Large Language Models, Instruction Data, and Benchmarks
Authors: Gang Hu, Ke Qin, Chenhan Yuan, Min Peng, Alejandro Lopez-Lira, Benyou
  Wang, Sophia Ananiadou, Wanlong Yu, Jimin Huang, and Qianqian Xie
Categories: cs.CE cs.CL
Comments: 23 pages, 5 figures, 11 tables, including Appendix
\\
  While the progression of Large Language Models (LLMs) has notably propelled
financial analysis, their application has largely been confined to singular
language realms, leaving untapped the potential of bilingual Chinese-English
capacity. To bridge this chasm, we introduce ICE-PIXIU, seamlessly amalgamating
the ICE-INTENT model and ICE-FLARE benchmark for bilingual financial analysis.
ICE-PIXIU uniquely integrates a spectrum of Chinese tasks, alongside translated
and original English datasets, enriching the breadth and depth of bilingual
financial modeling. It provides unrestricted access to diverse model variants,
a substantial compilation of diverse cross-lingual and multi-modal instruction
data, and an evaluation benchmark with expert annotations, comprising 10 NLP
tasks, 20 bilingual specific tasks, totaling 1,185k datasets. Our thorough
evaluation emphasizes the advantages of incorporating these bilingual datasets,
especially in translation tasks and utilizing original English data, enhancing
both linguistic flexibility and analytical acuity in financial contexts.
Notably, ICE-INTENT distinguishes itself by showcasing significant enhancements
over conventional LLMs and existing financial LLMs in bilingual milieus,
underscoring the profound impact of robust bilingual data on the accuracy and
efficacy of financial NLP.
\\ ( https://arxiv.org/abs/2403.06249 ,  559kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06292 (*cross-listing*)
Date: Sun, 10 Mar 2024 19:31:13 GMT   (1115kb,D)

Title: Transformer based Multitask Learning for Image Captioning and Object
  Detection
Authors: Debolena Basak, P.K. Srijith, and Maunendra Sankar Desarkar
Categories: cs.CV cs.CL
Comments: Accepted at PAKDD 2024
\\
  In several real-world scenarios like autonomous navigation and mobility, to
obtain a better visual understanding of the surroundings, image captioning and
object detection play a crucial role. This work introduces a novel multitask
learning framework that combines image captioning and object detection into a
joint model. We propose TICOD, Transformer-based Image Captioning and Object
detection model for jointly training both tasks by combining the losses
obtained from image captioning and object detection networks. By leveraging
joint training, the model benefits from the complementary information shared
between the two tasks, leading to improved performance for image captioning.
Our approach utilizes a transformer-based architecture that enables end-to-end
network integration for image captioning and object detection and performs both
tasks jointly. We evaluate the effectiveness of our approach through
comprehensive experiments on the MS-COCO dataset. Our model outperforms the
baselines from image captioning literature by achieving a 3.65% improvement in
BERTScore.
\\ ( https://arxiv.org/abs/2403.06292 ,  1115kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06485 (*cross-listing*)
Date: Mon, 11 Mar 2024 07:48:35 GMT   (1921kb,D)

Title: Knowledge-aware Alert Aggregation in Large-scale Cloud Systems: a Hybrid
  Approach
Authors: Jinxi Kuang, Jinyang Liu, Junjie Huang, Renyi Zhong, Jiazhen Gu, Lan
  Yu, Rui Tan, Zengyin Yang, Michael R. Lyu
Categories: cs.SE cs.CL cs.LG
Comments: Accepted by Proceedings of the 46th International Conference on
  Software Engineering: Software Engineering in Practice (ICSE SEIP 2024)
DOI: 10.1145/3639477.3639745
\\
  Due to the scale and complexity of cloud systems, a system failure would
trigger an "alert storm", i.e., massive correlated alerts. Although these
alerts can be traced back to a few root causes, the overwhelming number makes
it infeasible for manual handling. Alert aggregation is thus critical to help
engineers concentrate on the root cause and facilitate failure resolution.
Existing methods typically utilize semantic similarity-based methods or
statistical methods to aggregate alerts. However, semantic similarity-based
methods overlook the causal rationale of alerts, while statistical methods can
hardly handle infrequent alerts.
  To tackle these limitations, we introduce leveraging external knowledge,
i.e., Standard Operation Procedure (SOP) of alerts as a supplement. We propose
COLA, a novel hybrid approach based on correlation mining and LLM (Large
Language Model) reasoning for online alert aggregation. The correlation mining
module effectively captures the temporal and spatial relations between alerts,
measuring their correlations in an efficient manner. Subsequently, only
uncertain pairs with low confidence are forwarded to the LLM reasoning module
for detailed analysis. This hybrid design harnesses both statistical evidence
for frequent alerts and the reasoning capabilities of computationally intensive
LLMs, ensuring the overall efficiency of COLA in handling large volumes of
alerts in practical scenarios. We evaluate COLA on three datasets collected
from the production environment of a large-scale cloud platform. The
experimental results show COLA achieves F1-scores from 0.901 to 0.930,
outperforming state-of-the-art methods and achieving comparable efficiency. We
also share our experience in deploying COLA in our real-world cloud system,
Cloud X.
\\ ( https://arxiv.org/abs/2403.06485 ,  1921kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06503 (*cross-listing*)
Date: Mon, 11 Mar 2024 08:25:52 GMT   (592kb,D)

Title: Automatic Generation of Python Programs Using Context-Free Grammars
Authors: Kamel Yamani, Marwa Na\"ir, Riyadh Baghdadi
Categories: cs.PL cs.CL cs.LG
Comments: This work was presented at the 2nd Languages, Architectures, and
  Tools for Heterogeneous Computing (LATHC) Workshop 2024, organized in
  conjunction with the IEEE/ACM International Symposium on Code Generation and
  Optimization (CGO)
\\
  In recent years, data has emerged as the new gold, serving as a powerful tool
for creating intelligent systems. However, procuring high-quality data remains
challenging, especially for code. To address this, we developed TinyPy
Generator, a tool that generates random Python programs using a context-free
grammar. The generated programs are guaranteed to be correct by construction.
Our system uses custom production rules (in the Backus-Naur Form (BNF) format)
to recursively generate code. This allows us to generate code with different
levels of complexity, ranging from code containing only assignments to more
complex code containing conditionals and loops. Our proposed tool enables
effortless large-scale Python code generation, beneficial for a wide range of
applications. TinyPy Generator is particularly useful in the field of machine
learning, where it can generate substantial amounts of Python code for training
Python language models. Additionally, researchers who are studying programming
languages can utilize this tool to create datasets for their experiments, which
can help validate the robustness of code interpreters or compilers. Unlike
existing research, we have open-sourced our implementation. This allows
customization according to user needs and extends potential usage to other
languages.
\\ ( https://arxiv.org/abs/2403.06503 ,  592kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06789 (*cross-listing*)
Date: Mon, 11 Mar 2024 15:04:55 GMT   (3127kb,D)

Title: SPLADE-v3: New baselines for SPLADE
Authors: Carlos Lassance, Herv\'e D\'ejean, Thibault Formal, St\'ephane
  Clinchant
Categories: cs.IR cs.CL
Comments: Technical report
\\
  A companion to the release of the latest version of the SPLADE library. We
describe changes to the training structure and present our latest series of
models -- SPLADE-v3. We compare this new version to BM25, SPLADE++, as well as
re-rankers, and showcase its effectiveness via a meta-analysis over more than
40 query sets. SPLADE-v3 further pushes the limit of SPLADE models: it is
statistically significantly more effective than both BM25 and SPLADE++, while
comparing well to cross-encoder re-rankers. Specifically, it gets more than 40
MRR@10 on the MS MARCO dev set, and improves by 2% the out-of-domain results on
the BEIR benchmark.
\\ ( https://arxiv.org/abs/2403.06789 ,  3127kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06892 (*cross-listing*)
Date: Mon, 11 Mar 2024 16:48:25 GMT   (1705kb,D)

Title: Real-time Transformer-based Open-Vocabulary Detection with Efficient
  Fusion Head
Authors: Tiancheng Zhao, Peng Liu, Xuan He, Lu Zhang and Kyusong Lee
Categories: cs.CV cs.CL
Comments: Preprint
\\
  End-to-end transformer-based detectors (DETRs) have shown exceptional
performance in both closed-set and open-vocabulary object detection (OVD) tasks
through the integration of language modalities. However, their demanding
computational requirements have hindered their practical application in
real-time object detection (OD) scenarios. In this paper, we scrutinize the
limitations of two leading models in the OVDEval benchmark, OmDet and
Grounding-DINO, and introduce OmDet-Turbo. This novel transformer-based
real-time OVD model features an innovative Efficient Fusion Head (EFH) module
designed to alleviate the bottlenecks observed in OmDet and Grounding-DINO.
Notably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with
TensorRT and language cache techniques applied. Notably, in zero-shot scenarios
on COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on
par with current state-of-the-art supervised models. Furthermore, it
establishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an
AP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of
OmDet-Turbo in industrial applications is underscored by its exceptional
performance on benchmark datasets and superior inference speed, positioning it
as a compelling choice for real-time object detection tasks. Code:
\url{https://github.com/om-ai-lab/OmDet}
\\ ( https://arxiv.org/abs/2403.06892 ,  1705kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06949 (*cross-listing*)
Date: Mon, 11 Mar 2024 17:34:25 GMT   (9378kb,D)

Title: Materials science in the era of large language models: a perspective
Authors: Ge Lei, Ronan Docherty, Samuel J. Cooper
Categories: cond-mat.mtrl-sci cs.CL
\\
  Large Language Models (LLMs) have garnered considerable interest due to their
impressive natural language capabilities, which in conjunction with various
emergent properties make them versatile tools in workflows ranging from complex
code generation to heuristic finding for combinatorial problems. In this paper
we offer a perspective on their applicability to materials science research,
arguing their ability to handle ambiguous requirements across a range of tasks
and disciplines mean they could be a powerful tool to aid researchers. We
qualitatively examine basic LLM theory, connecting it to relevant properties
and techniques in the literature before providing two case studies that
demonstrate their use in task automation and knowledge extraction at-scale. At
their current stage of development, we argue LLMs should be viewed less as
oracles of novel insight, and more as tireless workers that can accelerate and
unify exploration across domains. It is our hope that this paper can
familiarise material science researchers with the concepts needed to leverage
these tools in their own research.
\\ ( https://arxiv.org/abs/2403.06949 ,  9378kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04837 (*cross-listing*)
Date: Thu, 7 Mar 2024 19:00:02 GMT   (9998kb)

Title: Cell reprogramming design by transfer learning of functional
  transcriptional networks
Authors: Thomas P. Wytock and Adilson E. Motter
Categories: q-bio.MN cond-mat.dis-nn cs.LG q-bio.GN
Comments: 27 pages, 12 figures, 4 tables
Journal-ref: Proceedings of the National Academy of Sciences, 121(11)
  e2312942121 (2024)
DOI: 10.1073/pnas.2312942121
\\
  Recent developments in synthetic biology, next-generation sequencing, and
machine learning provide an unprecedented opportunity to rationally design new
disease treatments based on measured responses to gene perturbations and drugs
to reprogram cells. The main challenges to seizing this opportunity are the
incomplete knowledge of the cellular network and the combinatorial explosion of
possible interventions, both of which are insurmountable by experiments. To
address these challenges, we develop a transfer learning approach to control
cell behavior that is pre-trained on transcriptomic data associated with human
cell fates, thereby generating a model of the network dynamics that can be
transferred to specific reprogramming goals. The approach combines
transcriptional responses to gene perturbations to minimize the difference
between a given pair of initial and target transcriptional states. We
demonstrate our approach's versatility by applying it to a microarray dataset
comprising >9,000 microarrays across 54 cell types and 227 unique
perturbations, and an RNASeq dataset consisting of >10,000 sequencing runs
across 36 cell types and 138 perturbations. Our approach reproduces known
reprogramming protocols with an AUROC of 0.91 while innovating over existing
methods by pre-training an adaptable model that can be tailored to specific
reprogramming transitions. We show that the number of gene perturbations
required to steer from one fate to another increases with decreasing
developmental relatedness and that fewer genes are needed to progress along
developmental paths than to regress. These findings establish a
proof-of-concept for our approach to computationally design control strategies
and provide insights into how gene regulatory networks govern phenotype.
\\ ( https://arxiv.org/abs/2403.04837 ,  9998kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05540 (*cross-listing*)
Date: Fri, 2 Feb 2024 23:04:13 GMT   (184kb,D)

Title: Extinction Risks from AI: Invisible to Science?
Authors: Vojtech Kovarik, Christian van Merwijk, Ida Mattsson
Categories: cs.CY cs.LG
\\
  In an effort to inform the discussion surrounding existential risks from AI,
we formulate Extinction-level Goodhart's Law as "Virtually any goal
specification, pursued to the extreme, will result in the extinction of
humanity", and we aim to understand which formal models are suitable for
investigating this hypothesis. Note that we remain agnostic as to whether
Extinction-level Goodhart's Law holds or not. As our key contribution, we
identify a set of conditions that are necessary for a model that aims to be
informative for evaluating specific arguments for Extinction-level Goodhart's
Law. Since each of the conditions seems to significantly contribute to the
complexity of the resulting model, formally evaluating the hypothesis might be
exceedingly difficult. This raises the possibility that whether the risk of
extinction from artificial intelligence is real or not, the underlying dynamics
might be invisible to current scientific methods.
\\ ( https://arxiv.org/abs/2403.05540 ,  184kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05546 (*cross-listing*)
Date: Tue, 6 Feb 2024 16:33:56 GMT   (2008kb,D)

Title: Unified Occupancy on a Public Transport Network through Combination of
  AFC and APC Data
Authors: Amir Dib, No\"elie Cherrier, Martin Graive, Baptiste R\'erolle,
  Eglantine Schmitt
Categories: cs.CY cs.CE cs.LG stat.AP
Comments: Accepted in this version to ITSC 2023
\\
  In a transport network, the onboard occupancy is key for gaining insights
into travelers' habits and adjusting the offer. Traditionally, operators have
relied on field studies to evaluate ridership of a typical workday. However,
automated fare collection (AFC) and automatic passenger counting (APC) data,
which provide complete temporal coverage, are often available but
underexploited. It should be noted, however, that each data source comes with
its own biases: AFC data may not account for fraud, while not all vehicles are
equipped with APC systems.
  This paper introduces the unified occupancy method, a geostatistical model to
extrapolate occupancy to every course of a public transportation network by
combining AFC and APC data with partial coverage. Unified occupancy completes
missing APC information for courses on lines where other courses have APC
measures, as well as for courses on lines where no APC data is available at
all. The accuracy of this method is evaluated on real data from several public
transportation networks in France.
\\ ( https://arxiv.org/abs/2403.05546 ,  2008kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05555 (*cross-listing*)
Date: Sat, 10 Feb 2024 16:07:38 GMT   (1302kb)

Title: Subgroup Discovery in MOOCs: A Big Data Application for Describing
  Different Types of Learners
Authors: J. M. Luna, H. M. Fardoun, F. Padillo, C. Romero and S. Ventura
Categories: cs.CY cs.DB cs.LG
Journal-ref: Knowledge and Information Systems (2022); 64:1349-1384
DOI: 10.1007/s10115-022-01674-9
\\
  The aim of this paper is to categorize and describe different types of
learners in massive open online courses (MOOCs) by means of a subgroup
discovery approach based on MapReduce. The final objective is to discover
IF-THEN rules that appear in different MOOCs. The proposed subgroup discovery
approach, which is an extension of the well-known FP-Growth algorithm,
considers emerging parallel methodologies like MapReduce to be able to cope
with extremely large datasets. As an additional feature, the proposal includes
a threshold value to denote the number of courses that each discovered rule
should satisfy. A post-processing step is also included so redundant subgroups
can be removed. The experimental stage is carried out by considering
de-identified data from the first year of 16 MITx and HarvardX courses on the
edX platform. Experimental results demonstrate that the proposed MapReduce
approach outperforms traditional sequential subgroup discovery approaches,
achieving a runtime that is almost constant for different courses.
Additionally, thanks to the final post-processing step, only interesting and
not-redundant rules are discovered, hence reducing the number of subgroups in
one or two orders of magnitude. Finally, the discovered subgroups are easily
used by courses' instructors not only for descriptive purposes but also for
additional tasks such as recommendation or personalization.
\\ ( https://arxiv.org/abs/2403.05555 ,  1302kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05556 (*cross-listing*)
Date: Sat, 10 Feb 2024 19:03:06 GMT   (1923kb)

Title: Modeling and predicting students' engagement behaviors using mixture
  Markov models
Authors: R. Maqsood, P. Ceravolo, C. Romero, and S. Ventura
Categories: cs.CY cs.LG
Journal-ref: Knowledge and Information System (2022); 64:1349-1384
\\
  Students' engagements reflect their level of involvement in an ongoing
learning process which can be estimated through their interactions with a
computer-based learning or assessment system. A pre-requirement for stimulating
student engagement lies in the capability to have an approximate representation
model for comprehending students' varied (dis)engagement behaviors. In this
paper, we utilized model-based clustering for this purpose which generates K
mixture Markov models to group students' traces containing their
(dis)engagement behavioral patterns. To prevent the Expectation-Maximization
(EM) algorithm from getting stuck in a local maxima, we also introduced a
K-means-based initialization method named as K-EM. We performed an experimental
work on two real datasets using the three variants of the EM algorithm: the
original EM, emEM, K-EM; and, non-mixture baseline models for both datasets.
The proposed K-EM has shown very promising results and achieved significant
performance difference in comparison with the other approaches particularly
using the Dataset. Hence, we suggest to perform further experiments using large
dataset(s) to validate our method. Additionally, visualization of the resultant
clusters through first-order Markov chains reveals very useful insights about
(dis)engagement behaviors depicted by the students. We conclude the paper with
a discussion on the usefulness of our approach, limitations and potential
extensions of this work.
\\ ( https://arxiv.org/abs/2403.05556 ,  1923kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05557 (*cross-listing*)
Date: Sun, 11 Feb 2024 12:23:21 GMT   (258kb,D)

Title: Re-thinking Human Activity Recognition with Hierarchy-aware Label
  Relationship Modeling
Authors: Jingwei Zuo and Hakim Hacid
Categories: eess.SP cs.HC cs.LG
Comments: Accepted by PAKDD 2024
\\
  Human Activity Recognition (HAR) has been studied for decades, from data
collection, learning models, to post-processing and result interpretations.
However, the inherent hierarchy in the activities remains relatively
under-explored, despite its significant impact on model performance and
interpretation. In this paper, we propose H-HAR, by rethinking the HAR tasks
from a fresh perspective by delving into their intricate global label
relationships. Rather than building multiple classifiers separately for
multi-layered activities, we explore the efficacy of a flat model enhanced with
graph-based label relationship modeling. Being hierarchy-aware, the graph-based
label modeling enhances the fundamental HAR model, by incorporating intricate
label relationships into the model. We validate the proposal with a multi-label
classifier on complex human activity data. The results highlight the advantages
of the proposal, which can be vertically integrated into advanced HAR models to
further enhance their performances.
\\ ( https://arxiv.org/abs/2403.05557 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05559 (*cross-listing*)
Date: Thu, 15 Feb 2024 14:12:38 GMT   (370kb,D)

Title: Improving Cognitive Diagnosis Models with Adaptive Relational Graph
  Neural Networks
Authors: Pengyang Shao, Chen Gao, Lei Chen, Yonghui Yang, Kun Zhang, Meng Wang
Categories: cs.CY cs.LG
\\
  Cognitive Diagnosis (CD) algorithms receive growing research interest in
intelligent education. Typically, these CD algorithms assist students by
inferring their abilities (i.e., their proficiency levels on various knowledge
concepts). The proficiency levels can enable further targeted skill training
and personalized exercise recommendations, thereby promoting students' learning
efficiency in online education. Recently, researchers have found that building
and incorporating a student-exercise bipartite graph is beneficial for
enhancing diagnostic performance. However, there are still limitations in their
studies. On one hand, researchers overlook the heterogeneity within edges,
where there can be both correct and incorrect answers. On the other hand, they
disregard the uncertainty within edges, e.g., a correct answer can indicate
true mastery or fortunate guessing. To address the limitations, we propose
Adaptive Semantic-aware Graph-based Cognitive Diagnosis model (ASG-CD), which
introduces a novel and effective way to leverage bipartite graph information in
CD. Specifically, we first map students, exercises, and knowledge concepts into
a latent representation space and combine these latent representations to
obtain student abilities and exercise difficulties. After that, we propose a
Semantic-aware Graph Neural Network Layer to address edge heterogeneity. This
layer splits the original bipartite graph into two subgraphs according to edge
semantics, and aggregates information based on these two subgraphs separately.
To mitigate the impact of edge uncertainties, we propose an Adaptive Edge
Differentiation Layer that dynamically differentiates edges, followed by
keeping reliable edges and filtering out uncertain edges. Extensive experiments
on three real-world datasets have demonstrated the effectiveness of ASG-CD.
\\ ( https://arxiv.org/abs/2403.05559 ,  370kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05571 (*cross-listing*)
Date: Thu, 22 Feb 2024 03:52:17 GMT   (7496kb,D)

Title: Efficient and Guaranteed-Safe Non-Convex Trajectory Optimization with
  Constrained Diffusion Model
Authors: Anjian Li, Zihan Ding, Adji Bousso Dieng, Ryne Beeson
Categories: cs.RO cs.LG
\\
  Trajectory optimization in robotics poses a challenging non-convex problem
due to complex dynamics and environmental settings. Traditional numerical
optimization methods are time-consuming in finding feasible solutions, whereas
data-driven approaches lack safety guarantees for the output trajectories. In
this paper, we introduce a general and fully parallelizable framework that
combines diffusion models and numerical solvers for non-convex trajectory
optimization, ensuring both computational efficiency and constraint
satisfaction. A novel constrained diffusion model is proposed with an
additional constraint violation loss for training. It aims to approximate the
distribution of locally optimal solutions while minimizing constraint
violations during sampling. The samples are then used as initial guesses for a
numerical solver to refine and derive final solutions with formal verification
of feasibility and optimality. Experimental evaluations on three tasks over
different robotics domains verify the improved constraint satisfaction and
computational efficiency with 4$\times$ to 22$\times$ acceleration using our
proposed method, which generalizes across trajectory optimization problems and
scales well with problem complexity.
\\ ( https://arxiv.org/abs/2403.05571 ,  7496kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05573 (*cross-listing*)
Date: Mon, 26 Feb 2024 08:59:46 GMT   (210kb,D)

Title: Beyond Predictive Algorithms in Child Welfare
Authors: Erina Seh-Young Moon, Devansh Saxena, Tegan Maharaj, Shion Guha
Categories: cs.CY cs.HC cs.LG
\\
  Caseworkers in the child welfare (CW) sector use predictive decision-making
algorithms built on risk assessment (RA) data to guide and support CW
decisions. Researchers have highlighted that RAs can contain biased signals
which flatten CW case complexities and that the algorithms may benefit from
incorporating contextually rich case narratives, i.e. - casenotes written by
caseworkers. To investigate this hypothesized improvement, we quantitatively
deconstructed two commonly used RAs from a United States CW agency. We trained
classifier models to compare the predictive validity of RAs with and without
casenote narratives and applied computational text analysis on casenotes to
highlight topics uncovered in the casenotes. Our study finds that common risk
metrics used to assess families and build CWS predictive risk models (PRMs) are
unable to predict discharge outcomes for children who are not reunified with
their birth parent(s). We also find that although casenotes cannot predict
discharge outcomes, they contain contextual case signals. Given the lack of
predictive validity of RA scores and casenotes, we propose moving beyond
quantitative risk assessments for public sector algorithms and towards using
contextual sources of information such as narratives to study public
sociotechnical systems.
\\ ( https://arxiv.org/abs/2403.05573 ,  210kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05591 (*cross-listing*)
Date: Tue, 5 Mar 2024 23:32:45 GMT   (14895kb,D)

Title: Data-Driven Ergonomic Risk Assessment of Complex Hand-intensive
  Manufacturing Processes
Authors: Anand Krishnan, Xingjian Yang, Utsav Seth, Jonathan M. Jeyachandran,
  Jonathan Y. Ahn, Richard Gardner, Samuel F. Pedigo, Adriana (Agnes)
  Blom-Schieber, Ashis G. Banerjee, Krithika Manohar
Categories: cs.HC cs.LG
Comments: 26 pages, 7 figures
\\
  Hand-intensive manufacturing processes, such as composite layup and textile
draping, require significant human dexterity to accommodate task complexity.
These strenuous hand motions often lead to musculoskeletal disorders and
rehabilitation surgeries. We develop a data-driven ergonomic risk assessment
system with a special focus on hand and finger activity to better identify and
address ergonomic issues related to hand-intensive manufacturing processes. The
system comprises a multi-modal sensor testbed to collect and synchronize
operator upper body pose, hand pose and applied forces; a Biometric Assessment
of Complete Hand (BACH) formulation to measure high-fidelity hand and finger
risks; and industry-standard risk scores associated with upper body posture,
RULA, and hand activity, HAL. Our findings demonstrate that BACH captures
injurious activity with a higher granularity in comparison to the existing
metrics. Machine learning models are also used to automate RULA and HAL
scoring, and generalize well to unseen participants. Our assessment system,
therefore, provides ergonomic interpretability of the manufacturing processes
studied, and could be used to mitigate risks through minor workplace
optimization and posture corrections.
\\ ( https://arxiv.org/abs/2403.05591 ,  14895kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05595 (*cross-listing*)
Date: Thu, 7 Mar 2024 10:05:09 GMT   (903kb)

Title: Comparison of gait phase detection using traditional machine learning
  and deep learning techniques
Authors: Farhad Nazari, Navid Mohajer, Darius Nahavandi, and Abbas Khosravi
Categories: eess.SP cs.CV cs.HC cs.LG
Comments: Copyright \c{opyright} This is the accepted version of an article
  published in the proceedings of the 2022 IEEE International Conference on
  Systems, Man, and Cybernetics (SMC)
Journal-ref: 2022 IEEE International Conference on Systems, Man, and
  Cybernetics (SMC)
DOI: 10.1109/SMC53654.2022.9945397
\\
  Human walking is a complex activity with a high level of cooperation and
interaction between different systems in the body. Accurate detection of the
phases of the gait in real-time is crucial to control lower-limb assistive
devices like exoskeletons and prostheses. There are several ways to detect the
walking gait phase, ranging from cameras and depth sensors to the sensors
attached to the device itself or the human body. Electromyography (EMG) is one
of the input methods that has captured lots of attention due to its precision
and time delay between neuromuscular activity and muscle movement. This study
proposes a few Machine Learning (ML) based models on lower-limb EMG data for
human walking. The proposed models are based on Gaussian Naive Bayes (NB),
Decision Tree (DT), Random Forest (RF), Linear Discriminant Analysis (LDA) and
Deep Convolutional Neural Networks (DCNN). The traditional ML models are
trained on hand-crafted features or their reduced components using Principal
Component Analysis (PCA). On the contrary, the DCNN model utilises
convolutional layers to extract features from raw data. The results show up to
75% average accuracy for traditional ML models and 79% for Deep Learning (DL)
model. The highest achieved accuracy in 50 trials of the training DL model is
89.5%.
\\ ( https://arxiv.org/abs/2403.05595 ,  903kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05598 (*cross-listing*)
Date: Thu, 7 Mar 2024 21:22:07 GMT   (400kb,D)

Title: Privacy Amplification for the Gaussian Mechanism via Bounded Support
Authors: Shengyuan Hu, Saeed Mahloujifar, Virginia Smith, Kamalika Chaudhuri,
  Chuan Guo
Categories: cs.CR cs.LG
Comments: 23 pages, 4 figures
\\
  Data-dependent privacy accounting frameworks such as per-instance
differential privacy (pDP) and Fisher information loss (FIL) confer
fine-grained privacy guarantees for individuals in a fixed training dataset.
These guarantees can be desirable compared to vanilla DP in real world settings
as they tightly upper-bound the privacy leakage for a $\textit{specific}$
individual in an $\textit{actual}$ dataset, rather than considering worst-case
datasets. While these frameworks are beginning to gain popularity, to date,
there is a lack of private mechanisms that can fully leverage advantages of
data-dependent accounting. To bridge this gap, we propose simple modifications
of the Gaussian mechanism with bounded support, showing that they amplify
privacy guarantees under data-dependent accounting. Experiments on model
training with DP-SGD show that using bounded support Gaussian mechanisms can
provide a reduction of the pDP bound $\epsilon$ by as much as 30% without
negative effects on model utility.
\\ ( https://arxiv.org/abs/2403.05598 ,  400kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05618 (*cross-listing*)
Date: Fri, 8 Mar 2024 19:00:01 GMT   (2221kb,D)

Title: OmniJet-$\alpha$: The first cross-task foundation model for particle
  physics
Authors: Joschka Birk, Anna Hallin, Gregor Kasieczka
Categories: hep-ph cs.LG hep-ex physics.data-an
\\
  Foundation models are multi-dataset and multi-task machine learning methods
that once pre-trained can be fine-tuned for a large variety of downstream
applications. The successful development of such general-purpose models for
physics data would be a major breakthrough as they could improve the achievable
physics performance while at the same time drastically reduce the required
amount of training time and data.
  We report significant progress on this challenge on several fronts. First, a
comprehensive set of evaluation methods is introduced to judge the quality of
an encoding from physics data into a representation suitable for the
autoregressive generation of particle jets with transformer architectures (the
common backbone of foundation models). These measures motivate the choice of a
higher-fidelity tokenization compared to previous works. Finally, we
demonstrate transfer learning between an unsupervised problem (jet generation)
and a classic supervised task (jet tagging) with our new OmniJet-$\alpha$
model. This is the first successful transfer between two different and actively
studied classes of tasks and constitutes a major step in the building of
foundation models for particle physics.
\\ ( https://arxiv.org/abs/2403.05618 ,  2221kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05666 (*cross-listing*)
Date: Fri, 8 Mar 2024 20:43:57 GMT   (8417kb,D)

Title: Prepared for the Worst: A Learning-Based Adversarial Attack for
  Resilience Analysis of the ICP Algorithm
Authors: Ziyu Zhang, Johann Laconte, Daniil Lisus, Timothy D. Barfoot
Categories: cs.RO cs.LG
Comments: 8 pages (7 content, 1 reference). 5 figures, submitted to the IEEE
  Robotics and Automation Letters (RA-L)
\\
  This paper presents a novel method to assess the resilience of the Iterative
Closest Point (ICP) algorithm via deep-learning-based attacks on lidar point
clouds. For safety-critical applications such as autonomous navigation,
ensuring the resilience of algorithms prior to deployments is of utmost
importance. The ICP algorithm has become the standard for lidar-based
localization. However, the pose estimate it produces can be greatly affected by
corruption in the measurements. Corruption can arise from a variety of
scenarios such as occlusions, adverse weather, or mechanical issues in the
sensor. Unfortunately, the complex and iterative nature of ICP makes assessing
its resilience to corruption challenging. While there have been efforts to
create challenging datasets and develop simulations to evaluate the resilience
of ICP empirically, our method focuses on finding the maximum possible ICP pose
error using perturbation-based adversarial attacks. The proposed attack induces
significant pose errors on ICP and outperforms baselines more than 88% of the
time across a wide range of scenarios. As an example application, we
demonstrate that our attack can be used to identify areas on a map where ICP is
particularly vulnerable to corruption in the measurements.
\\ ( https://arxiv.org/abs/2403.05666 ,  8417kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05669 (*cross-listing*)
Date: Fri, 8 Mar 2024 20:49:49 GMT   (252kb,D)

Title: Spectral Clustering of Categorical and Mixed-type Data via Extra Graph
  Nodes
Authors: Dylan Soemitro, Jeova Farias Sales Rocha Neto
Categories: stat.ML cs.LG
\\
  Clustering data objects into homogeneous groups is one of the most important
tasks in data mining. Spectral clustering is arguably one of the most important
algorithms for clustering, as it is appealing for its theoretical soundness and
is adaptable to many real-world data settings. For example, mixed data, where
the data is composed of numerical and categorical features, is typically
handled via numerical discretization, dummy coding, or similarity computation
that takes into account both data types. This paper explores a more natural way
to incorporate both numerical and categorical information into the spectral
clustering algorithm, avoiding the need for data preprocessing or the use of
sophisticated similarity functions. We propose adding extra nodes corresponding
to the different categories the data may belong to and show that it leads to an
interpretable clustering objective function. Furthermore, we demonstrate that
this simple framework leads to a linear-time spectral clustering algorithm for
categorical-only data. Finally, we compare the performance of our algorithms
against other related methods and show that it provides a competitive
alternative to them in terms of performance and runtime.
\\ ( https://arxiv.org/abs/2403.05669 ,  252kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05743 (*cross-listing*)
Date: Sat, 9 Mar 2024 00:41:30 GMT   (612kb)

Title: Generative Probabilistic Forecasting with Applications in Market
  Operations
Authors: Xinyi Wang, Lang Tong
Categories: eess.SP cs.LG econ.GN q-fin.EC
\\
  This paper presents a novel generative probabilistic forecasting approach
derived from the Wiener-Kallianpur innovation representation of nonparametric
time series. Under the paradigm of generative artificial intelligence, the
proposed forecasting architecture includes an autoencoder that transforms
nonparametric multivariate random processes into canonical innovation
sequences, from which future time series samples are generated according to
their probability distributions conditioned on past samples. A novel
deep-learning algorithm is proposed that constrains the latent process to be an
independent and identically distributed sequence with matching autoencoder
input-output conditional probability distributions. Asymptotic optimality and
structural convergence properties of the proposed generative forecasting
approach are established. Three applications involving highly dynamic and
volatile time series in real-time market operations are considered: (i)
locational marginal price forecasting for merchant storage participants, {(ii)
interregional price spread forecasting for interchange markets,} and (iii) area
control error forecasting for frequency regulations. Numerical studies based on
market data from multiple independent system operators demonstrate superior
performance against leading traditional and machine learning-based forecasting
techniques under both probabilistic and point forecast metrics.
\\ ( https://arxiv.org/abs/2403.05743 ,  612kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05756 (*cross-listing*)
Date: Sat, 9 Mar 2024 01:58:45 GMT   (19962kb,D)

Title: Model-Free Local Recalibration of Neural Networks
Authors: R. Torres (1), D. J. Nott (2), S. A. Sisson (3), T. Rodrigues (1), J.
  G. Reis (1), G. S. Rodrigues (1) ((1) University of Bras\'ilia, (2) National
  University of Singapore, (3) University of New South Wales, Sydney)
Categories: stat.ME cs.LG
Comments: 25 pages, 5 figures
MSC-class: 62G07 (Primary), 68T07, 68T37 (Secondary), 68Q10
ACM-class: G.3; I.5.1; I.6.4
\\
  Artificial neural networks (ANNs) are highly flexible predictive models.
However, reliably quantifying uncertainty for their predictions is a continuing
challenge. There has been much recent work on "recalibration" of predictive
distributions for ANNs, so that forecast probabilities for events of interest
are consistent with certain frequency evaluations of them. Uncalibrated
probabilistic forecasts are of limited use for many important decision-making
tasks. To address this issue, we propose a localized recalibration of ANN
predictive distributions using the dimension-reduced representation of the
input provided by the ANN hidden layers. Our novel method draws inspiration
from recalibration techniques used in the literature on approximate Bayesian
computation and likelihood-free inference methods. Most existing calibration
methods for ANNs can be thought of as calibrating either on the input layer,
which is difficult when the input is high-dimensional, or the output layer,
which may not be sufficiently flexible. Through a simulation study, we
demonstrate that our method has good performance compared to alternative
approaches, and explore the benefits that can be achieved by localizing the
calibration based on different layers of the network. Finally, we apply our
proposed method to a diamond price prediction problem, demonstrating the
potential of our approach to improve prediction and uncertainty quantification
in real-world applications.
\\ ( https://arxiv.org/abs/2403.05756 ,  19962kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05765 (*cross-listing*)
Date: Sat, 9 Mar 2024 02:24:02 GMT   (813kb,D)

Title: Physics-informed Neural Motion Planning on Constraint Manifolds
Authors: Ruiqi Ni and Ahmed H. Qureshi
Categories: cs.RO cs.LG
Comments: Accepted at the IEEE International Conference on Robotics and
  Automation (ICRA), 2024
\\
  Constrained Motion Planning (CMP) aims to find a collision-free path between
the given start and goal configurations on the kinematic constraint manifolds.
These problems appear in various scenarios ranging from object manipulation to
legged-robot locomotion. However, the zero-volume nature of manifolds makes the
CMP problem challenging, and the state-of-the-art methods still take several
seconds to find a path and require a computationally expansive path dataset for
imitation learning. Recently, physics-informed motion planning methods have
emerged that directly solve the Eikonal equation through neural networks for
motion planning and do not require expert demonstrations for learning. Inspired
by these approaches, we propose the first physics-informed CMP framework that
solves the Eikonal equation on the constraint manifolds and trains neural
function for CMP without expert data. Our results show that the proposed
approach efficiently solves various CMP problems in both simulation and
real-world, including object manipulation under orientation constraints and
door opening with a high-dimensional 6-DOF robot manipulator. In these complex
settings, our method exhibits high success rates and finds paths in
sub-seconds, which is many times faster than the state-of-the-art CMP methods.
\\ ( https://arxiv.org/abs/2403.05765 ,  813kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05783 (*cross-listing*)
Date: Sat, 9 Mar 2024 03:33:07 GMT   (19691kb,D)

Title: Large Generative Model Assisted 3D Semantic Communication
Authors: Feibo Jiang, Yubo Peng, Li Dong, Kezhi Wang, Kun Yang, Cunhua Pan,
  Xiaohu You
Categories: cs.IT cs.LG math.IT
Comments: 13 pages,13 figures,1 table
\\
  Semantic Communication (SC) is a novel paradigm for data transmission in 6G.
However, there are several challenges posed when performing SC in 3D scenarios:
1) 3D semantic extraction; 2) Latent semantic redundancy; and 3) Uncertain
channel estimation. To address these issues, we propose a Generative AI Model
assisted 3D SC (GAM-3DSC) system. Firstly, we introduce a 3D Semantic Extractor
(3DSE), which employs generative AI models, including Segment Anything Model
(SAM) and Neural Radiance Field (NeRF), to extract key semantics from a 3D
scenario based on user requirements. The extracted 3D semantics are represented
as multi-perspective images of the goal-oriented 3D object. Then, we present an
Adaptive Semantic Compression Model (ASCM) for encoding these multi-perspective
images, in which we use a semantic encoder with two output heads to perform
semantic encoding and mask redundant semantics in the latent semantic space,
respectively. Next, we design a conditional Generative adversarial network and
Diffusion model aided-Channel Estimation (GDCE) to estimate and refine the
Channel State Information (CSI) of physical channels. Finally, simulation
results demonstrate the advantages of the proposed GAM-3DSC system in
effectively transmitting the goal-oriented 3D scenario.
\\ ( https://arxiv.org/abs/2403.05783 ,  19691kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05809 (*cross-listing*)
Date: Sat, 9 Mar 2024 06:12:06 GMT   (105kb,D)

Title: Shallow ReLU neural networks and finite elements
Authors: Pengzhan Jin
Categories: math.NA cs.LG cs.NA
\\
  We point out that (continuous or discontinuous) piecewise linear functions on
a convex polytope mesh can be represented by two-hidden-layer ReLU neural
networks in a weak sense. In addition, the numbers of neurons of the two hidden
layers required to weakly represent are accurately given based on the numbers
of polytopes and hyperplanes involved in this mesh. The results naturally hold
for constant and linear finite element functions. Such weak representation
establishes a bridge between shallow ReLU neural networks and finite element
functions, and leads to a perspective for analyzing approximation capability of
ReLU neural networks in $L^p$ norm via finite element functions. Moreover, we
discuss the strict representation for tensor finite element functions via the
recent tensor neural networks.
\\ ( https://arxiv.org/abs/2403.05809 ,  105kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05811 (*cross-listing*)
Date: Sat, 9 Mar 2024 06:19:53 GMT   (24kb)

Title: Statistical Efficiency of Distributional Temporal Difference
Authors: Yang Peng, Liangyu Zhang, Zhihua Zhang
Categories: stat.ML cs.LG
\\
  Distributional reinforcement learning (DRL), which cares about the full
distribution of returns instead of just the mean, has achieved empirical
success in various domains. One of the core tasks in the field of DRL is
distributional policy evaluation, which involves estimating the return
distribution $\eta^\pi$ for a given policy $\pi$. A distributional temporal
difference (TD) algorithm has been accordingly proposed, which is an extension
of the temporal difference algorithm in the classic RL literature. In the
tabular case, \citet{rowland2018analysis} and \citet{rowland2023analysis}
proved the asymptotic convergence of two instances of distributional TD, namely
categorical temporal difference algorithm (CTD) and quantile temporal
difference algorithm (QTD), respectively. In this paper, we go a step further
and analyze the finite-sample performance of distributional TD. To facilitate
theoretical analysis, we propose non-parametric distributional TD algorithm
(NTD). For a $\gamma$-discounted infinite-horizon tabular Markov decision
process with state space $S$ and action space $A$, we show that in the case of
NTD we need $\wtilde O\prn{\frac{1}{\varepsilon^{2p}(1-\gamma)^{2p+2}}}$
iterations to achieve an $\varepsilon$-optimal estimator with high probability,
when the estimation error is measured by the $p$-Wasserstein distance. Under
some mild assumptions, $\wtilde O\prn{\frac{1}{\varepsilon^{2}(1-\gamma)^{4}}}$
iterations suffices to ensure the Kolmogorov-Smirnov distance between the NTD
estimator $\hat\eta^\pi$ and $\eta^\pi$ less than $\varepsilon$ with high
probability. And we revisit CTD, showing that the same non-asymptotic
convergence bounds hold for CTD in the case of the $p$-Wasserstein distance.
\\ ( https://arxiv.org/abs/2403.05811 ,  24kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05873 (*cross-listing*)
Date: Sat, 9 Mar 2024 10:49:31 GMT   (1424kb,D)

Title: LEGION: Harnessing Pre-trained Language Models for GitHub Topic
  Recommendations with Distribution-Balance Loss
Authors: Yen-Trang Dang, Thanh-Le Cong, Phuc-Thanh Nguyen, Anh M. T. Bui,
  Phuong T. Nguyen, Bach Le, Quyet-Thang Huynh
Categories: cs.SE cs.IR cs.LG
Comments: Accepted to EASE'24
\\
  Open-source development has revolutionized the software industry by promoting
collaboration, transparency, and community-driven innovation. Today, a vast
amount of various kinds of open-source software, which form networks of
repositories, is often hosted on GitHub - a popular software development
platform. To enhance the discoverability of the repository networks, i.e.,
groups of similar repositories, GitHub introduced repository topics in 2017
that enable users to more easily explore relevant projects by type, technology,
and more. It is thus crucial to accurately assign topics for each GitHub
repository. Current methods for automatic topic recommendation rely heavily on
TF-IDF for encoding textual data, presenting challenges in understanding
semantic nuances. This paper addresses the limitations of existing techniques
by proposing Legion, a novel approach that leverages Pre-trained Language
Models (PTMs) for recommending topics for GitHub repositories. The key novelty
of Legion is three-fold. First, Legion leverages the extensive capabilities of
PTMs in language understanding to capture contextual information and semantic
meaning in GitHub repositories. Second, Legion overcomes the challenge of
long-tailed distribution, which results in a bias toward popular topics in
PTMs, by proposing a Distribution-Balanced Loss (DB Loss) to better train the
PTMs. Third, Legion employs a filter to eliminate vague recommendations,
thereby improving the precision of PTMs. Our empirical evaluation on a
benchmark dataset of real-world GitHub repositories shows that Legion can
improve vanilla PTMs by up to 26% on recommending GitHubs topics. Legion also
can suggest GitHub topics more precisely and effectively than the
state-of-the-art baseline with an average improvement of 20% and 5% in terms of
Precision and F1-score, respectively.
\\ ( https://arxiv.org/abs/2403.05873 ,  1424kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05879 (*cross-listing*)
Date: Sat, 9 Mar 2024 11:09:45 GMT   (5496kb,D)

Title: Deep Learning based acoustic measurement approach for robotic
  applications on orthopedics
Authors: Bangyu Lan, Momen Abayazid, Nico Verdonschot, Stefano Stramigioli,
  Kenan Niu
Categories: eess.SP cs.LG cs.RO
\\
  In Total Knee Replacement Arthroplasty (TKA), surgical robotics can provide
image-guided navigation to fit implants with high precision. Its tracking
approach highly relies on inserting bone pins into the bones tracked by the
optical tracking system. This is normally done by invasive, radiative manners
(implantable markers and CT scans), which introduce unnecessary trauma and
prolong the preparation time for patients. To tackle this issue,
ultrasound-based bone tracking could offer an alternative. In this study, we
proposed a novel deep learning structure to improve the accuracy of bone
tracking by an A-mode ultrasound (US). We first obtained a set of ultrasound
dataset from the cadaver experiment, where the ground truth locations of bones
were calculated using bone pins. These data were used to train the proposed
CasAtt-UNet to predict bone location automatically and robustly. The ground
truth bone locations and those locations of US were recorded simultaneously.
Therefore, we could label bone peaks in the raw US signals. As a result, our
method achieved sub millimeter precision across all eight bone areas with the
only exception of one channel in the ankle. This method enables the robust
measurement of lower extremity bone positions from 1D raw ultrasound signals.
It shows great potential to apply A-mode ultrasound in orthopedic surgery from
safe, convenient, and efficient perspectives.
\\ ( https://arxiv.org/abs/2403.05879 ,  5496kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05899 (*cross-listing*)
Date: Sat, 9 Mar 2024 12:33:09 GMT   (2935kb,D)

Title: Online Identification of Stochastic Continuous-Time Wiener Models Using
  Sampled Data
Authors: Mohamed Abdalmoaty, Efe C. Balta, John Lygeros, Roy S. Smith
Categories: stat.ME cs.LG cs.SY eess.SP eess.SY
\\
  It is well known that ignoring the presence of stochastic disturbances in the
identification of stochastic Wiener models leads to asymptotically biased
estimators. On the other hand, optimal statistical identification, via
likelihood-based methods, is sensitive to the assumptions on the data
distribution and is usually based on relatively complex sequential Monte Carlo
algorithms. We develop a simple recursive online estimation algorithm based on
an output-error predictor, for the identification of continuous-time stochastic
parametric Wiener models through stochastic approximation. The method is
applicable to generic model parameterizations and, as demonstrated in the
numerical simulation examples, it is robust with respect to the assumptions on
the spectrum of the disturbance process.
\\ ( https://arxiv.org/abs/2403.05899 ,  2935kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05949 (*cross-listing*)
Date: Sat, 9 Mar 2024 16:02:46 GMT   (14157kb,D)

Title: General surgery vision transformer: A video pre-trained foundation model
  for general surgery
Authors: Samuel Schmidgall, Ji Woong Kim, Jeffery Jopling, Axel Krieger
Categories: cs.CV cs.LG q-bio.TO
\\
  The absence of openly accessible data and specialized foundation models is a
major barrier for computational research in surgery. Toward this, (i) we
open-source the largest dataset of general surgery videos to-date, consisting
of 680 hours of surgical videos, including data from robotic and laparoscopic
techniques across 28 procedures; (ii) we propose a technique for video
pre-training a general surgery vision transformer (GSViT) on surgical videos
based on forward video prediction that can run in real-time for surgical
applications, toward which we open-source the code and weights of GSViT; (iii)
we also release code and weights for procedure-specific fine-tuned versions of
GSViT across 10 procedures; (iv) we demonstrate the performance of GSViT on the
Cholec80 phase annotation task, displaying improved performance over
state-of-the-art single frame predictors.
\\ ( https://arxiv.org/abs/2403.05949 ,  14157kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05963 (*cross-listing*)
Date: Sat, 9 Mar 2024 17:05:43 GMT   (1089kb,D)

Title: Robust Emotion Recognition in Context Debiasing
Authors: Dingkang Yang, Kun Yang, Mingcheng Li, Shunli Wang, Shuaibing Wang,
  Lihua Zhang
Categories: cs.CV cs.LG
\\
  Context-aware emotion recognition (CAER) has recently boosted the practical
applications of affective computing techniques in unconstrained environments.
Mainstream CAER methods invariably extract ensemble representations from
diverse contexts and subject-centred characteristics to perceive the target
person's emotional state. Despite advancements, the biggest challenge remains
due to context bias interference. The harmful bias forces the models to rely on
spurious correlations between background contexts and emotion labels in
likelihood estimation, causing severe performance bottlenecks and confounding
valuable context priors. In this paper, we propose a counterfactual emotion
inference (CLEF) framework to address the above issue. Specifically, we first
formulate a generalized causal graph to decouple the causal relationships among
the variables in CAER. Following the causal graph, CLEF introduces a
non-invasive context branch to capture the adverse direct effect caused by the
context bias. During the inference, we eliminate the direct context effect from
the total causal effect by comparing factual and counterfactual outcomes,
resulting in bias mitigation and robust prediction. As a model-agnostic
framework, CLEF can be readily integrated into existing methods, bringing
consistent performance gains.
\\ ( https://arxiv.org/abs/2403.05963 ,  1089kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05966 (*cross-listing*)
Date: Sat, 9 Mar 2024 17:17:07 GMT   (5797kb,D)

Title: Can Generative Models Improve Self-Supervised Representation Learning?
Authors: Arash Afkanpour, Vahid Reza Khazaie, Sana Ayromlou, Fereshteh Forghani
Categories: cs.CV cs.LG
\\
  The rapid advancement in self-supervised learning (SSL) has highlighted its
potential to leverage unlabeled data for learning powerful visual
representations. However, existing SSL approaches, particularly those employing
different views of the same image, often rely on a limited set of predefined
data augmentations. This constrains the diversity and quality of
transformations, which leads to sub-optimal representations. In this paper, we
introduce a novel framework that enriches the SSL paradigm by utilizing
generative models to produce semantically consistent image augmentations. By
directly conditioning generative models on a source image representation, our
method enables the generation of diverse augmentations while maintaining the
semantics of the source image, thus offering a richer set of data for
self-supervised learning. Our experimental results demonstrate that our
framework significantly enhances the quality of learned visual representations.
This research demonstrates that incorporating generative models into the SSL
workflow opens new avenues for exploring the potential of unlabeled visual
data. This development paves the way for more robust and versatile
representation learning techniques.
\\ ( https://arxiv.org/abs/2403.05966 ,  5797kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06015 (*cross-listing*)
Date: Sat, 9 Mar 2024 21:29:25 GMT   (559kb,D)

Title: Grafting: Making Random Forests Consistent
Authors: Nicholas Waltz
Categories: stat.ML cs.LG
\\
  Despite their performance and widespread use, little is known about the
theory of Random Forests. A major unanswered question is whether, or when, the
Random Forest algorithm is consistent. The literature explores various variants
of the classic Random Forest algorithm to address this question and known
short-comings of the method. This paper is a contribution to this literature.
Specifically, the suitability of grafting consistent estimators onto a shallow
CART is explored. It is shown that this approach has a consistency guarantee
and performs well in empirical settings.
\\ ( https://arxiv.org/abs/2403.06015 ,  559kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06021 (*cross-listing*)
Date: Sat, 9 Mar 2024 21:55:55 GMT   (207kb,D)

Title: Hierarchical Query Classification in E-commerce Search
Authors: Bing He, Sreyashi Nag, Limeng Cui, Suhang Wang, Zheng Li, Rahul
  Goutam, Zhen Li, Haiyang Zhang
Categories: cs.IR cs.LG
Comments: Published at: the ACM Web Conference 2024 in the industry track
  (WWW'24)
\\
  E-commerce platforms typically store and structure product information and
search data in a hierarchy. Efficiently categorizing user search queries into a
similar hierarchical structure is paramount in enhancing user experience on
e-commerce platforms as well as news curation and academic research. The
significance of this task is amplified when dealing with sensitive query
categorization or critical information dissemination, where inaccuracies can
lead to considerable negative impacts. The inherent complexity of hierarchical
query classification is compounded by two primary challenges: (1) the
pronounced class imbalance that skews towards dominant categories, and (2) the
inherent brevity and ambiguity of search queries that hinder accurate
classification.
  To address these challenges, we introduce a novel framework that leverages
hierarchical information through (i) enhanced representation learning that
utilizes the contrastive loss to discern fine-grained instance relationships
within the hierarchy, called ''instance hierarchy'', and (ii) a nuanced
hierarchical classification loss that attends to the intrinsic label taxonomy,
named ''label hierarchy''. Additionally, based on our observation that certain
unlabeled queries share typographical similarities with labeled queries, we
propose a neighborhood-aware sampling technique to intelligently select these
unlabeled queries to boost the classification performance. Extensive
experiments demonstrate that our proposed method is better than
state-of-the-art (SOTA) on the proprietary Amazon dataset, and comparable to
SOTA on the public datasets of Web of Science and RCV1-V2. These results
underscore the efficacy of our proposed solution, and pave the path toward the
next generation of hierarchy-aware query classification systems.
\\ ( https://arxiv.org/abs/2403.06021 ,  207kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06024 (*cross-listing*)
Date: Sat, 9 Mar 2024 22:23:45 GMT   (6979kb,D)

Title: Semi-Supervised Multimodal Multi-Instance Learning for Aortic Stenosis
  Diagnosis
Authors: Zhe Huang, Xiaowei Yu, Benjamin S. Wessler and Michael C. Hughes
Categories: cs.CV cs.ET cs.LG
Comments: Echocardiography; Multimodal; Semi-supervised Learning;
  Multiple-Instance Learning
\\
  Automated interpretation of ultrasound imaging of the heart (echocardiograms)
could improve the detection and treatment of aortic stenosis (AS), a deadly
heart disease. However, existing deep learning pipelines for assessing AS from
echocardiograms have two key limitations. First, most methods rely on limited
2D cineloops, thereby ignoring widely available Doppler imaging that contains
important complementary information about pressure gradients and blood flow
abnormalities associated with AS. Second, obtaining labeled data is difficult.
There are often far more unlabeled echocardiogram recordings available, but
these remain underutilized by existing methods. To overcome these limitations,
we introduce Semi-supervised Multimodal Multiple-Instance Learning (SMMIL), a
new deep learning framework for automatic interpretation for structural heart
diseases like AS. When deployed, SMMIL can combine information from two input
modalities, spectral Dopplers and 2D cineloops, to produce a study-level AS
diagnosis. During training, SMMIL can combine a smaller labeled set and an
abundant unlabeled set of both modalities to improve its classifier.
Experiments demonstrate that SMMIL outperforms recent alternatives at 3-level
AS severity classification as well as several clinically relevant AS detection
tasks.
\\ ( https://arxiv.org/abs/2403.06024 ,  6979kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06048 (*cross-listing*)
Date: Sun, 10 Mar 2024 00:07:47 GMT   (2830kb)

Title: Texture image retrieval using a classification and contourlet-based
  features
Authors: Asal Rouhafzay, Nadia Baaziz and Mohand Said Allili
Categories: cs.CV cs.LG
Comments: 14 pages, 6 figures, The 25th International Conference on Image
  Processing, Computer Vision, & Pattern Recognition (IPCV'21: July 26-29,
  2021, USA)
\\
  In this paper, we propose a new framework for improving Content Based Image
Retrieval (CBIR) for texture images. This is achieved by using a new image
representation based on the RCT-Plus transform which is a novel variant of the
Redundant Contourlet transform that extracts a richer directional information
in the image. Moreover, the process of image search is improved through a
learning-based approach where the images of the database are classified using
an adapted similarity metric to the statistical modeling of the RCT-Plus
transform. A query is then first classified to select the best texture class
after which the retained class images are ranked to select top ones. By this,
we have achieved significant improvements in the retrieval rates compared to
previous CBIR schemes.
\\ ( https://arxiv.org/abs/2403.06048 ,  2830kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06056 (*cross-listing*)
Date: Sun, 10 Mar 2024 01:07:22 GMT   (1007kb,D)

Title: Absence of spurious solutions far from ground truth: A low-rank analysis
  with high-order losses
Authors: Ziye Ma, Ying Chen, Javad Lavaei, Somayeh Sojoudi
Categories: math.OC cs.LG eess.SP
Comments: Accepted by AISTATS 2024
\\
  Matrix sensing problems exhibit pervasive non-convexity, plaguing
optimization with a proliferation of suboptimal spurious solutions. Avoiding
convergence to these critical points poses a major challenge. This work
provides new theoretical insights that help demystify the intricacies of the
non-convex landscape. In this work, we prove that under certain conditions,
critical points sufficiently distant from the ground truth matrix exhibit
favorable geometry by being strict saddle points rather than troublesome local
minima. Moreover, we introduce the notion of higher-order losses for the matrix
sensing problem and show that the incorporation of such losses into the
objective function amplifies the negative curvature around those distant
critical points. This implies that increasing the complexity of the objective
function via high-order losses accelerates the escape from such critical points
and acts as a desirable alternative to increasing the complexity of the
optimization problem via over-parametrization. By elucidating key
characteristics of the non-convex optimization landscape, this work makes
progress towards a comprehensive framework for tackling broader machine
learning objectives plagued by non-convexity.
\\ ( https://arxiv.org/abs/2403.06056 ,  1007kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06066 (*cross-listing*)
Date: Sun, 10 Mar 2024 03:04:13 GMT   (1475kb)

Title: CausalCellSegmenter: Causal Inference inspired Diversified Aggregation
  Convolution for Pathology Image Segmentation
Authors: Dawei Fan, Yifan Gao, Jiaming Yu, Yanping Chen, Wencheng Li, Chuancong
  Lin, Kaibin Li, Changcai Yang, Riqing Chen, Lifang Wei
Categories: eess.IV cs.CV cs.LG
Comments: 10 pages, 5 figures, 2 tables, MICCAI
\\
  Deep learning models have shown promising performance for cell nucleus
segmentation in the field of pathology image analysis. However, training a
robust model from multiple domains remains a great challenge for cell nucleus
segmentation. Additionally, the shortcomings of background noise, highly
overlapping between cell nucleus, and blurred edges often lead to poor
performance. To address these challenges, we propose a novel framework termed
CausalCellSegmenter, which combines Causal Inference Module (CIM) with
Diversified Aggregation Convolution (DAC) techniques. The DAC module is
designed which incorporates diverse downsampling features through a simple,
parameter-free attention module (SimAM), aiming to overcome the problems of
false-positive identification and edge blurring. Furthermore, we introduce CIM
to leverage sample weighting by directly removing the spurious correlations
between features for every input sample and concentrating more on the
correlation between features and labels. Extensive experiments on the
MoNuSeg-2018 dataset achieves promising results, outperforming other
state-of-the-art methods, where the mIoU and DSC scores growing by 3.6% and
2.65%.
\\ ( https://arxiv.org/abs/2403.06066 ,  1475kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06069 (*cross-listing*)
Date: Sun, 10 Mar 2024 03:22:57 GMT   (906kb,D)

Title: Implicit Image-to-Image Schrodinger Bridge for CT Super-Resolution and
  Denoising
Authors: Yuang Wang, Siyeop Yoon, Pengfei Jin, Matthew Tivnan, Zhennong Chen,
  Rui Hu, Li Zhang, Zhiqiang Chen, Quanzheng Li, and Dufan Wu
Categories: eess.IV cs.CV cs.LG
\\
  Conditional diffusion models have gained recognition for their effectiveness
in image restoration tasks, yet their iterative denoising process, starting
from Gaussian noise, often leads to slow inference speeds. As a promising
alternative, the Image-to-Image Schr\"odinger Bridge (I2SB) initializes the
generative process from corrupted images and integrates training techniques
from conditional diffusion models. In this study, we extended the I2SB method
by introducing the Implicit Image-to-Image Schrodinger Bridge (I3SB),
transitioning its generative process to a non-Markovian process by
incorporating corrupted images in each generative step. This enhancement
empowers I3SB to generate images with better texture restoration using a small
number of generative steps. The proposed method was validated on CT
super-resolution and denoising tasks and outperformed existing methods,
including the conditional denoising diffusion probabilistic model (cDDPM) and
I2SB, in both visual quality and quantitative metrics. These findings
underscore the potential of I3SB in improving medical image restoration by
providing fast and accurate generative modeling.
\\ ( https://arxiv.org/abs/2403.06069 ,  906kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06153 (*cross-listing*)
Date: Sun, 10 Mar 2024 09:54:56 GMT   (9863kb,D)

Title: The ALL0CORE Tensor Decomposition for Sparse Count Data
Authors: John Hood, Aaron Schein
Categories: stat.ML cs.LG
\\
  This paper introduces ALL0CORE, a new form of probabilistic non-negative
tensor decomposition. ALL0CORE is a Tucker decomposition where the number of
non-zero elements (i.e., the L0-norm) of the core tensor is constrained to a
preset value Q much smaller than the size of the core. While the user dictates
the total budget Q, the locations and values of the non-zero elements are
latent variables and allocated across the core tensor during inference.
ALL0CORE -- i.e., allocated L0-constrained core -- thus enjoys both the
computational tractability of CP decomposition and the qualitatively appealing
latent structure of Tucker. In a suite of real-data experiments, we demonstrate
that ALL0CORE typically requires only tiny fractions (e.g.,~1%) of the full
core to achieve the same results as full Tucker decomposition at only a
correspondingly tiny fraction of the cost.
\\ ( https://arxiv.org/abs/2403.06153 ,  9863kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06173 (*cross-listing*)
Date: Sun, 10 Mar 2024 10:58:54 GMT   (10595kb,D)

Title: Speeding up 6-DoF Grasp Sampling with Quality-Diversity
Authors: Johann Huber, Fran\c{c}ois H\'el\'enon, Mathilde Kappel, Elie Chelly,
  Mahdi Khoramshahi, Fa\"iz Ben Amar, St\'ephane Doncieux
Categories: cs.RO cs.LG
Comments: 7 pages, 8 figures. Preprint version
\\
  Recent advances in AI have led to significant results in robotic learning,
including natural language-conditioned planning and efficient optimization of
controllers using generative models. However, the interaction data remains the
bottleneck for generalization. Getting data for grasping is a critical
challenge, as this skill is required to complete many manipulation tasks.
Quality-Diversity (QD) algorithms optimize a set of solutions to get diverse,
high-performing solutions to a given problem. This paper investigates how QD
can be combined with priors to speed up the generation of diverse grasps poses
in simulation compared to standard 6-DoF grasp sampling schemes. Experiments
conducted on 4 grippers with 2-to-5 fingers on standard objects show that QD
outperforms commonly used methods by a large margin. Further experiments show
that QD optimization automatically finds some efficient priors that are usually
hard coded. The deployment of generated grasps on a 2-finger gripper and an
Allegro hand shows that the diversity produced maintains sim-to-real
transferability. We believe these results to be a significant step toward the
generation of large datasets that can lead to robust and generalizing robotic
grasping policies.
\\ ( https://arxiv.org/abs/2403.06173 ,  10595kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06197 (*cross-listing*)
Date: Sun, 10 Mar 2024 12:41:34 GMT   (1406kb,D)

Title: DrFuse: Learning Disentangled Representation for Clinical Multi-Modal
  Fusion with Missing Modality and Modal Inconsistency
Authors: Wenfang Ya, Kejing Yin, William K. Cheung, Jia Liu and Jing Qin
Categories: eess.IV cs.CV cs.LG
Comments: Accepted by AAAI-24
\\
  The combination of electronic health records (EHR) and medical images is
crucial for clinicians in making diagnoses and forecasting prognosis.
Strategically fusing these two data modalities has great potential to improve
the accuracy of machine learning models in clinical prediction tasks. However,
the asynchronous and complementary nature of EHR and medical images presents
unique challenges. Missing modalities due to clinical and administrative
factors are inevitable in practice, and the significance of each data modality
varies depending on the patient and the prediction target, resulting in
inconsistent predictions and suboptimal model performance. To address these
challenges, we propose DrFuse to achieve effective clinical multi-modal fusion.
It tackles the missing modality issue by disentangling the features shared
across modalities and those unique within each modality. Furthermore, we
address the modal inconsistency issue via a disease-wise attention layer that
produces the patient- and disease-wise weighting for each modality to make the
final prediction. We validate the proposed method using real-world large-scale
datasets, MIMIC-IV and MIMIC-CXR. Experimental results show that the proposed
method significantly outperforms the state-of-the-art models. Our
implementation is publicly available at https://github.com/dorothy-yao/drfuse.
\\ ( https://arxiv.org/abs/2403.06197 ,  1406kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06251 (*cross-listing*)
Date: Sun, 10 Mar 2024 16:34:21 GMT   (23690kb,D)

Title: Online Multi-spectral Neuron Tracing
Authors: Bin Duan, Yuzhang Shang, Dawen Cai, and Yan Yan
Categories: q-bio.NC cs.CV cs.LG
\\
  In this paper, we propose an online multi-spectral neuron tracing method with
uniquely designed modules, where no offline training are required. Our method
is trained online to update our enhanced discriminative correlation filter to
conglutinate the tracing process. This distinctive offline-training-free schema
differentiates us from other training-dependent tracing approaches like deep
learning methods since no annotation is needed for our method. Besides,
compared to other tracing methods requiring complicated set-up such as for
clustering and graph multi-cut, our approach is much easier to be applied to
new images. In fact, it only needs a starting bounding box of the tracing
neuron, significantly reducing users' configuration effort. Our extensive
experiments show that our training-free and easy-configured methodology allows
fast and accurate neuron reconstructions in multi-spectral images.
\\ ( https://arxiv.org/abs/2403.06251 ,  23690kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06279 (*cross-listing*)
Date: Sun, 10 Mar 2024 18:13:22 GMT   (17kb)

Title: Fine-tuning of diffusion models via stochastic control: entropy
  regularization and beyond
Authors: Wenpin Tang
Categories: math.OC cs.LG
Comments: 15 pages
\\
  This paper aims to develop and provide a rigorous treatment to the problem of
entropy regularized fine-tuning in the context of continuous-time diffusion
models, which was recently proposed by Uehara et al. ( arXiv:2402.15194, 2024).
We also show how the analysis can be extended to fine-tuning involving a
general $f$-divergence regularizer.
\\ ( https://arxiv.org/abs/2403.06279 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06302 (*cross-listing*)
Date: Sun, 10 Mar 2024 20:22:06 GMT   (10386kb,D)

Title: Nonparametric Automatic Differentiation Variational Inference with
  Spline Approximation
Authors: Yuda Shao, Shan Yu, Tianshu Feng
Categories: stat.ML cs.LG
\\
  Automatic Differentiation Variational Inference (ADVI) is efficient in
learning probabilistic models. Classic ADVI relies on the parametric approach
to approximate the posterior. In this paper, we develop a spline-based
nonparametric approximation approach that enables flexible posterior
approximation for distributions with complicated structures, such as skewness,
multimodality, and bounded support. Compared with widely-used nonparametric
variational inference methods, the proposed method is easy to implement and
adaptive to various data structures. By adopting the spline approximation, we
derive a lower bound of the importance weighted autoencoder and establish the
asymptotic consistency. Experiments demonstrate the efficiency of the proposed
method in approximating complex posterior distributions and improving the
performance of generative models with incomplete data.
\\ ( https://arxiv.org/abs/2403.06302 ,  10386kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06315 (*cross-listing*)
Date: Sun, 10 Mar 2024 21:30:22 GMT   (1232kb,D)

Title: A Study on Domain Generalization for Failure Detection through Human
  Reactions in HRI
Authors: Maria Teresa Parreira, Sukruth Gowdru Lingaraju, Adolfo
  Ramirez-Aristizabal, Manaswi Saha, Michael Kuniavsky, Wendy Ju
Categories: cs.RO cs.HC cs.LG
\\
  Machine learning models are commonly tested in-distribution (same dataset);
performance almost always drops in out-of-distribution settings. For HRI
research, the goal is often to develop generalized models. This makes domain
generalization - retaining performance in different settings - a critical
issue. In this study, we present a concise analysis of domain generalization in
failure detection models trained on human facial expressions. Using two
distinct datasets of humans reacting to videos where error occurs, one from a
controlled lab setting and another collected online, we trained deep learning
models on each dataset. When testing these models on the alternate dataset, we
observed a significant performance drop. We reflect on the causes for the
observed model behavior and leave recommendations. This work emphasizes the
need for HRI research focusing on improving model robustness and real-life
applicability.
\\ ( https://arxiv.org/abs/2403.06315 ,  1232kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06338 (*cross-listing*)
Date: Sun, 10 Mar 2024 23:11:05 GMT   (5126kb,D)

Title: Disentangling shared and private latent factors in multimodal
  Variational Autoencoders
Authors: Kaspar M\"artens and Christopher Yau
Categories: stat.ML cs.LG q-bio.GN
Comments: Accepted for publication in the Proceedings of Machine Learning in
  Computational Biology (MLCB 2023)
\\
  Generative models for multimodal data permit the identification of latent
factors that may be associated with important determinants of observed data
heterogeneity. Common or shared factors could be important for explaining
variation across modalities whereas other factors may be private and important
only for the explanation of a single modality. Multimodal Variational
Autoencoders, such as MVAE and MMVAE, are a natural choice for inferring those
underlying latent factors and separating shared variation from private. In this
work, we investigate their capability to reliably perform this disentanglement.
In particular, we highlight a challenging problem setting where
modality-specific variation dominates the shared signal. Taking a cross-modal
prediction perspective, we demonstrate limitations of existing models, and
propose a modification how to make them more robust to modality-specific
variation. Our findings are supported by experiments on synthetic as well as
various real-world multi-omics data sets.
\\ ( https://arxiv.org/abs/2403.06338 ,  5126kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06342 (*cross-listing*)
Date: Sun, 10 Mar 2024 23:44:55 GMT   (1555kb,D)

Title: Separable Physics-informed Neural Networks for Solving the BGK Model of
  the Boltzmann Equation
Authors: Jaemin Oh, Seung Yeon Cho, Seok-Bae Yun, Eunbyung Park, and Youngjoon
  Hong
Categories: math.NA cs.LG cs.NA
MSC-class: 68T20, 35R09
\\
  In this study, we introduce a method based on Separable Physics-Informed
Neural Networks (SPINNs) for effectively solving the BGK model of the Boltzmann
equation. While the mesh-free nature of PINNs offers significant advantages in
handling high-dimensional partial differential equations (PDEs), challenges
arise when applying quadrature rules for accurate integral evaluation in the
BGK operator, which can compromise the mesh-free benefit and increase
computational costs. To address this, we leverage the canonical polyadic
decomposition structure of SPINNs and the linear nature of moment calculation,
achieving a substantial reduction in computational expense for quadrature rule
application. The multi-scale nature of the particle density function poses
difficulties in precisely approximating macroscopic moments using neural
networks. To improve SPINN training, we introduce the integration of Gaussian
functions into SPINNs, coupled with a relative loss approach. This modification
enables SPINNs to decay as rapidly as Maxwellian distributions, thereby
enhancing the accuracy of macroscopic moment approximations. The relative loss
design further ensures that both large and small-scale features are effectively
captured by the SPINNs. The efficacy of our approach is demonstrated through a
series of five numerical experiments, including the solution to a challenging
3D Riemann problem. These results highlight the potential of our novel method
in efficiently and accurately addressing complex challenges in computational
physics.
\\ ( https://arxiv.org/abs/2403.06342 ,  1555kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06388 (*cross-listing*)
Date: Mon, 11 Mar 2024 02:47:21 GMT   (554kb,D)

Title: A Zero Trust Framework for Realization and Defense Against Generative AI
  Attacks in Power Grid
Authors: Md. Shirajum Munir, Sravanthi Proddatoori, Manjushree Muralidhara,
  Walid Saad, Zhu Han, Sachin Shetty
Categories: cs.CR cs.LG
Comments: Accepted article by IEEE International Conference on Communications
  (ICC 2024), Copyright 2024 IEEE
\\
  Understanding the potential of generative AI (GenAI)-based attacks on the
power grid is a fundamental challenge that must be addressed in order to
protect the power grid by realizing and validating risk in new attack vectors.
In this paper, a novel zero trust framework for a power grid supply chain
(PGSC) is proposed. This framework facilitates early detection of potential
GenAI-driven attack vectors (e.g., replay and protocol-type attacks),
assessment of tail risk-based stability measures, and mitigation of such
threats. First, a new zero trust system model of PGSC is designed and
formulated as a zero-trust problem that seeks to guarantee for a stable PGSC by
realizing and defending against GenAI-driven cyber attacks. Second, in which a
domain-specific generative adversarial networks (GAN)-based attack generation
mechanism is developed to create a new vulnerability cyberspace for further
understanding that threat. Third, tail-based risk realization metrics are
developed and implemented for quantifying the extreme risk of a potential
attack while leveraging a trust measurement approach for continuous validation.
Fourth, an ensemble learning-based bootstrap aggregation scheme is devised to
detect the attacks that are generating synthetic identities with convincing
user and distributed energy resources device profiles. Experimental results
show the efficacy of the proposed zero trust framework that achieves an
accuracy of 95.7% on attack vector generation, a risk measure of 9.61% for a
95% stable PGSC, and a 99% confidence in defense against GenAI-driven attack.
\\ ( https://arxiv.org/abs/2403.06388 ,  554kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06404 (*cross-listing*)
Date: Mon, 11 Mar 2024 03:31:35 GMT   (368kb,D)

Title: Cosine Scoring with Uncertainty for Neural Speaker Embedding
Authors: Qiongqiong Wang, Kong Aik Lee
Categories: cs.SD cs.LG eess.AS
Comments: 5 pages, 4 figures
Journal-ref: IEEE Signal Processing Letters 2024
DOI: 10.1109/LSP.2024.3375080
\\
  Uncertainty modeling in speaker representation aims to learn the variability
present in speech utterances. While the conventional cosine-scoring is
computationally efficient and prevalent in speaker recognition, it lacks the
capability to handle uncertainty. To address this challenge, this paper
proposes an approach for estimating uncertainty at the speaker embedding
front-end and propagating it to the cosine scoring back-end. Experiments
conducted on the VoxCeleb and SITW datasets confirmed the efficacy of the
proposed method in handling uncertainty arising from embedding estimation. It
achieved improvement with 8.5% and 9.8% average reductions in EER and minDCF
compared to the conventional cosine similarity. It is also computationally
efficient in practice.
\\ ( https://arxiv.org/abs/2403.06404 ,  368kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06424 (*cross-listing*)
Date: Mon, 11 Mar 2024 04:25:41 GMT   (6432kb,D)

Title: Bridging Domains with Approximately Shared Features
Authors: Ziliang Samuel Zhong, Xiang Pan, Qi Lei
Categories: stat.ML cs.CV cs.LG
\\
  Multi-source domain adaptation aims to reduce performance degradation when
applying machine learning models to unseen domains. A fundamental challenge is
devising the optimal strategy for feature selection. Existing literature is
somewhat paradoxical: some advocate for learning invariant features from source
domains, while others favor more diverse features. To address the challenge, we
propose a statistical framework that distinguishes the utilities of features
based on the variance of their correlation to label $y$ across domains. Under
our framework, we design and analyze a learning procedure consisting of
learning approximately shared feature representation from source tasks and
fine-tuning it on the target task. Our theoretical analysis necessitates the
importance of learning approximately shared features instead of only the
strictly invariant features and yields an improved population risk compared to
previous results on both source and target tasks, thus partly resolving the
paradox mentioned above. Inspired by our theory, we proposed a more practical
way to isolate the content (invariant+approximately shared) from environmental
features and further consolidate our theoretical findings.
\\ ( https://arxiv.org/abs/2403.06424 ,  6432kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06456 (*cross-listing*)
Date: Mon, 11 Mar 2024 06:32:32 GMT   (1290kb,D)

Title: A Survey of Learned Indexes for the Multi-dimensional Space
Authors: Abdullah Al-Mamun, Hao Wu, Qiyang He, Jianguo Wang, Walid G. Aref
Categories: cs.DB cs.LG
\\
  A recent research trend involves treating database index structures as
Machine Learning (ML) models. In this domain, single or multiple ML models are
trained to learn the mapping from keys to positions inside a data set. This
class of indexes is known as "Learned Indexes." Learned indexes have
demonstrated improved search performance and reduced space requirements for
one-dimensional data. The concept of one-dimensional learned indexes has
naturally been extended to multi-dimensional (e.g., spatial) data, leading to
the development of "Learned Multi-dimensional Indexes". This survey focuses on
learned multi-dimensional index structures. Specifically, it reviews the
current state of this research area, explains the core concepts behind each
proposed method, and classifies these methods based on several well-defined
criteria. We present a taxonomy that classifies and categorizes each learned
multi-dimensional index, and survey the existing literature on learned
multi-dimensional indexes according to this taxonomy. Additionally, we present
a timeline to illustrate the evolution of research on learned indexes. Finally,
we highlight several open challenges and future research directions in this
emerging and highly active field.
\\ ( https://arxiv.org/abs/2403.06456 ,  1290kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06482 (*cross-listing*)
Date: Mon, 11 Mar 2024 07:44:56 GMT   (484kb,D)

Title: Financial Default Prediction via Motif-preserving Graph Neural Network
  with Curriculum Learning
Authors: Daixin Wang, Zhiqiang Zhang, Yeyu Zhao, Kai Huang, Yulin Kang, Jun
  Zhou
Categories: q-fin.RM cs.LG
\\
  User financial default prediction plays a critical role in credit risk
forecasting and management. It aims at predicting the probability that the user
will fail to make the repayments in the future. Previous methods mainly extract
a set of user individual features regarding his own profiles and behaviors and
build a binary-classification model to make default predictions. However, these
methods cannot get satisfied results, especially for users with limited
information. Although recent efforts suggest that default prediction can be
improved by social relations, they fail to capture the higher-order topology
structure at the level of small subgraph patterns. In this paper, we fill in
this gap by proposing a motif-preserving Graph Neural Network with curriculum
learning (MotifGNN) to jointly learn the lower-order structures from the
original graph and higherorder structures from multi-view motif-based graphs
for financial default prediction. Specifically, to solve the problem of weak
connectivity in motif-based graphs, we design the motif-based gating mechanism.
It utilizes the information learned from the original graph with good
connectivity to strengthen the learning of the higher-order structure. And
considering that the motif patterns of different samples are highly unbalanced,
we propose a curriculum learning mechanism on the whole learning process to
more focus on the samples with uncommon motif distributions. Extensive
experiments on one public dataset and two industrial datasets all demonstrate
the effectiveness of our proposed method.
\\ ( https://arxiv.org/abs/2403.06482 ,  484kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06499 (*cross-listing*)
Date: Mon, 11 Mar 2024 08:11:52 GMT   (1456kb,D)

Title: Detection of Unobserved Common Causes based on NML Code in Discrete,
  Mixed, and Continuous Variables
Authors: Masatoshi Kobayashi, Kohei Miyagichi, Shin Matsushima
Categories: stat.ML cs.IT cs.LG math.IT
Comments: submitted to Journal of Data Mining and Knowledge Discovery
\\
  Causal discovery in the presence of unobserved common causes from
observational data only is a crucial but challenging problem. We categorize all
possible causal relationships between two random variables into the following
four categories and aim to identify one from observed data: two cases in which
either of the direct causality exists, a case that variables are independent,
and a case that variables are confounded by latent confounders. Although
existing methods have been proposed to tackle this problem, they require
unobserved variables to satisfy assumptions on the form of their equation
models. In our previous study (Kobayashi et al., 2022), the first causal
discovery method without such assumptions is proposed for discrete data and
named CLOUD. Using Normalized Maximum Likelihood (NML) Code, CLOUD selects a
model that yields the minimum codelength of the observed data from a set of
model candidates. This paper extends CLOUD to apply for various data types
across discrete, mixed, and continuous. We not only performed theoretical
analysis to show the consistency of CLOUD in terms of the model selection, but
also demonstrated that CLOUD is more effective than existing methods in
inferring causal relationships by extensive experiments on both synthetic and
real-world data.
\\ ( https://arxiv.org/abs/2403.06499 ,  1456kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06546 (*cross-listing*)
Date: Mon, 11 Mar 2024 09:46:41 GMT   (3670kb,D)

Title: OMH: Structured Sparsity via Optimally Matched Hierarchy for
  Unsupervised Semantic Segmentation
Authors: Baran Ozaydin, Tong Zhang, Deblina Bhattacharjee, Sabine S\"usstrunk,
  Mathieu Salzmann
Categories: cs.CV cs.LG
Comments: 11 pages
\\
  Unsupervised Semantic Segmentation (USS) involves segmenting images without
relying on predefined labels, aiming to alleviate the burden of extensive human
labeling. Existing methods utilize features generated by self-supervised models
and specific priors for clustering. However, their clustering objectives are
not involved in the optimization of the features during training. Additionally,
due to the lack of clear class definitions in USS, the resulting segments may
not align well with the clustering objective. In this paper, we introduce a
novel approach called Optimally Matched Hierarchy (OMH) to simultaneously
address the above issues. The core of our method lies in imposing structured
sparsity on the feature space, which allows the features to encode information
with different levels of granularity. The structure of this sparsity stems from
our hierarchy (OMH). To achieve this, we learn a soft but sparse hierarchy
among parallel clusters through Optimal Transport. Our OMH yields better
unsupervised segmentation performance compared to existing USS methods. Our
extensive experiments demonstrate the benefits of OMH when utilizing our
differentiable paradigm. We will make our code publicly available.
\\ ( https://arxiv.org/abs/2403.06546 ,  3670kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06557 (*cross-listing*)
Date: Mon, 11 Mar 2024 10:00:26 GMT   (376kb,D)

Title: Data-driven architecture to encode information in the kinematics of
  robots and artificial avatars
Authors: Francesco De Lellis, Marco Coraggio, Nathan C. Foster, Riccardo Villa,
  Cristina Becchio, Mario di Bernardo
Categories: eess.SY cs.LG cs.RO cs.SY
\\
  We present a data-driven control architecture for modifying the kinematics of
robots and artificial avatars to encode specific information such as the
presence or not of an emotion in the movements of an avatar or robot driven by
a human operator. We validate our approach on an experimental dataset obtained
during the reach-to-grasp phase of a pick-and-place task.
\\ ( https://arxiv.org/abs/2403.06557 ,  376kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06606 (*cross-listing*)
Date: Mon, 11 Mar 2024 10:50:53 GMT   (3501kb,D)

Title: Distributionally Generative Augmentation for Fair Facial Attribute
  Classification
Authors: Fengda Zhang, Qianpei He, Kun Kuang, Jiashuo Liu, Long Chen, Chao Wu,
  Jun Xiao, Hanwang Zhang
Categories: cs.CV cs.LG
Comments: CVPR 2024
\\
  Facial Attribute Classification (FAC) holds substantial promise in widespread
applications. However, FAC models trained by traditional methodologies can be
unfair by exhibiting accuracy inconsistencies across varied data
subpopulations. This unfairness is largely attributed to bias in data, where
some spurious attributes (e.g., Male) statistically correlate with the target
attribute (e.g., Smiling). Most of existing fairness-aware methods rely on the
labels of spurious attributes, which may be unavailable in practice. This work
proposes a novel, generation-based two-stage framework to train a fair FAC
model on biased data without additional annotation. Initially, we identify the
potential spurious attributes based on generative models. Notably, it enhances
interpretability by explicitly showing the spurious attributes in image space.
Following this, for each image, we first edit the spurious attributes with a
random degree sampled from a uniform distribution, while keeping target
attribute unchanged. Then we train a fair FAC model by fostering model
invariance to these augmentation. Extensive experiments on three common
datasets demonstrate the effectiveness of our method in promoting fairness in
FAC without compromising accuracy. Codes are in
https://github.com/heqianpei/DiGA.
\\ ( https://arxiv.org/abs/2403.06606 ,  3501kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06612 (*cross-listing*)
Date: Mon, 11 Mar 2024 10:59:55 GMT   (223kb)

Title: Pulling back symmetric Riemannian geometry for data analysis
Authors: Willem Diepeveen
Categories: math.DG cs.LG
MSC-class: 53Z50, 53C35, 53C22
\\
  Data sets tend to live in low-dimensional non-linear subspaces. Ideal data
analysis tools for such data sets should therefore account for such non-linear
geometry. The symmetric Riemannian geometry setting can be suitable for a
variety of reasons. First, it comes with a rich mathematical structure to
account for a wide range of non-linear geometries that has been shown to be
able to capture the data geometry through empirical evidence from classical
non-linear embedding. Second, many standard data analysis tools initially
developed for data in Euclidean space can also be generalised efficiently to
data on a symmetric Riemannian manifold. A conceptual challenge comes from the
lack of guidelines for constructing a symmetric Riemannian structure on the
data space itself and the lack of guidelines for modifying successful
algorithms on symmetric Riemannian manifolds for data analysis to this setting.
This work considers these challenges in the setting of pullback Riemannian
geometry through a diffeomorphism. The first part of the paper characterises
diffeomorphisms that result in proper, stable and efficient data analysis. The
second part then uses these best practices to guide construction of such
diffeomorphisms through deep learning. As a proof of concept, different types
of pullback geometries -- among which the proposed construction -- are tested
on several data analysis tasks and on several toy data sets. The numerical
experiments confirm the predictions from theory, i.e., that the diffeomorphisms
generating the pullback geometry need to map the data manifold into a geodesic
subspace of the pulled back Riemannian manifold while preserving local isometry
around the data manifold for proper, stable and efficient data analysis, and
that pulling back positive curvature can be problematic in terms of stability.
\\ ( https://arxiv.org/abs/2403.06612 ,  223kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06645 (*cross-listing*)
Date: Mon, 11 Mar 2024 12:07:33 GMT   (11851kb,D)

Title: Ricci flow-based brain surface covariance descriptors for Alzheimer
  disease
Authors: Fatemeh Ahmadi, Mohamad Ebrahim Shiri, Behroz Bidabad, Maral Sedaghat,
  Pooran Memari
Categories: eess.IV cs.CV cs.LG
Comments: Accepted for publication in Biomedical Signal Processing and Control
  journal
\\
  Automated feature extraction from MRI brain scans and diagnosis of
Alzheimer's disease are ongoing challenges. With advances in 3D imaging
technology, 3D data acquisition is becoming more viable and efficient than its
2D counterpart. Rather than using feature-based vectors, in this paper, for the
first time, we suggest a pipeline to extract novel covariance-based descriptors
from the cortical surface using the Ricci energy optimization. The covariance
descriptors are components of the nonlinear manifold of symmetric
positive-definite matrices, thus we focus on using the Gaussian radial basis
function to apply manifold-based classification to the 3D shape problem.
Applying this novel signature to the analysis of abnormal cortical brain
morphometry allows for diagnosing Alzheimer's disease. Experimental studies
performed on about two hundred 3D MRI brain models, gathered from Alzheimer's
Disease Neuroimaging Initiative (ADNI) dataset demonstrate the effectiveness of
our descriptors in achieving remarkable classification accuracy.
\\ ( https://arxiv.org/abs/2403.06645 ,  11851kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06664 (*cross-listing*)
Date: Mon, 11 Mar 2024 12:32:14 GMT   (5225kb,D)

Title: Smart-Infinity: Fast Large Language Model Training using Near-Storage
  Processing on a Real System
Authors: Hongsun Jang, Jaeyong Song, Jaewon Jung, Jaeyoung Park, Youngsok Kim,
  and Jinho Lee
Categories: cs.AR cs.LG
Comments: Published at HPCA 2024 (Best Paper Award Honorable Mention)
\\
  The recent huge advance of Large Language Models (LLMs) is mainly driven by
the increase in the number of parameters. This has led to substantial memory
capacity requirements, necessitating the use of dozens of GPUs just to meet the
capacity. One popular solution to this is storage-offloaded training, which
uses host memory and storage as an extended memory hierarchy. However, this
obviously comes at the cost of storage bandwidth bottleneck because storage
devices have orders of magnitude lower bandwidth compared to that of GPU device
memories. Our work, Smart-Infinity, addresses the storage bandwidth bottleneck
of storage-offloaded LLM training using near-storage processing devices on a
real system. The main component of Smart-Infinity is SmartUpdate, which
performs parameter updates on custom near-storage accelerators. We identify
that moving parameter updates to the storage side removes most of the storage
traffic. In addition, we propose an efficient data transfer handler structure
to address the system integration issues for Smart-Infinity. The handler allows
overlapping data transfers with fixed memory consumption by reusing the device
buffer. Lastly, we propose accelerator-assisted gradient
compression/decompression to enhance the scalability of Smart-Infinity. When
scaling to multiple near-storage processing devices, the write traffic on the
shared channel becomes the bottleneck. To alleviate this, we compress the
gradients on the GPU and decompress them on the accelerators. It provides
further acceleration from reduced traffic. As a result, Smart-Infinity achieves
a significant speedup compared to the baseline. Notably, Smart-Infinity is a
ready-to-use approach that is fully integrated into PyTorch on a real system.
We will open-source Smart-Infinity to facilitate its use.
\\ ( https://arxiv.org/abs/2403.06664 ,  5225kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06671 (*cross-listing*)
Date: Mon, 11 Mar 2024 12:42:31 GMT   (136kb,D)

Title: Untangling Gaussian Mixtures
Authors: Eva Fluck, Sandra Kiefer, Christoph Standke
Categories: math.ST cs.DM cs.LG math.CO stat.TH
MSC-class: 05C40, 62H30, 68R10
\\
  Tangles were originally introduced as a concept to formalize regions of high
connectivity in graphs. In recent years, they have also been discovered as a
link between structural graph theory and data science: when interpreting
similarity in data sets as connectivity between points, finding clusters in the
data essentially amounts to finding tangles in the underlying graphs. This
paper further explores the potential of tangles in data sets as a means for a
formal study of clusters. Real-world data often follow a normal distribution.
Accounting for this, we develop a quantitative theory of tangles in data sets
drawn from Gaussian mixtures. To this end, we equip the data with a graph
structure that models similarity between the points and allows us to apply
tangle theory to the data. We provide explicit conditions under which tangles
associated with the marginal Gaussian distributions exist asymptotically almost
surely. This can be considered as a sufficient formal criterion for the
separabability of clusters in the data.
\\ ( https://arxiv.org/abs/2403.06671 ,  136kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06672 (*cross-listing*)
Date: Mon, 11 Mar 2024 12:43:44 GMT   (62kb)

Title: Provable Mutual Benefits from Federated Learning in Privacy-Sensitive
  Domains
Authors: Nikita Tsoy, Anna Mihalkova, Teodora Todorova, Nikola Konstantinov
Categories: stat.ML cs.CR cs.GT cs.LG
Comments: AISTATS 2024; Camera-ready version
\\
  Cross-silo federated learning (FL) allows data owners to train accurate
machine learning models by benefiting from each others private datasets.
Unfortunately, the model accuracy benefits of collaboration are often
undermined by privacy defenses. Therefore, to incentivize client participation
in privacy-sensitive domains, a FL protocol should strike a delicate balance
between privacy guarantees and end-model accuracy. In this paper, we study the
question of when and how a server could design a FL protocol provably
beneficial for all participants. First, we provide necessary and sufficient
conditions for the existence of mutually beneficial protocols in the context of
mean estimation and convex stochastic optimization. We also derive protocols
that maximize the total clients' utility, given symmetric privacy preferences.
Finally, we design protocols maximizing end-model accuracy and demonstrate
their benefits in synthetic experiments.
\\ ( https://arxiv.org/abs/2403.06672 ,  62kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06731 (*cross-listing*)
Date: Mon, 11 Mar 2024 13:50:07 GMT   (24kb)

Title: On the Approximation of Kernel functions
Authors: Paul Dommel and Alois Pichler
Categories: stat.ML cs.LG
\\
  Various methods in statistical learning build on kernels considered in
reproducing kernel Hilbert spaces. In applications, the kernel is often
selected based on characteristics of the problem and the data. This kernel is
then employed to infer response variables at points, where no explanatory data
were observed. The data considered here are located in compact sets in higher
dimensions and the paper addresses approximations of the kernel itself. The new
approach considers Taylor series approximations of radial kernel functions. For
the Gauss kernel on the unit cube, the paper establishes an upper bound of the
associated eigenfunctions, which grows only polynomially with respect to the
index. The novel approach substantiates smaller regularization parameters than
considered in the literature, overall leading to better approximations. This
improvement confirms low rank approximation methods such as the Nystr\"om
method.
\\ ( https://arxiv.org/abs/2403.06731 ,  24kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06748 (*cross-listing*)
Date: Mon, 11 Mar 2024 14:14:52 GMT   (4510kb,D)

Title: Shortcut Learning in Medical Image Segmentation
Authors: Manxi Lin, Nina Weng, Kamil Mikolaj, Zahra Bashir, Morten Bo
  S{\o}ndergaard Svendsen, Martin Tolsgaard, Anders Nymark Christensen, Aasa
  Feragen
Categories: eess.IV cs.CV cs.LG
\\
  Shortcut learning is a phenomenon where machine learning models prioritize
learning simple, potentially misleading cues from data that do not generalize
well beyond the training set. While existing research primarily investigates
this in the realm of image classification, this study extends the exploration
of shortcut learning into medical image segmentation. We demonstrate that
clinical annotations such as calipers, and the combination of zero-padded
convolutions and center-cropped training sets in the dataset can inadvertently
serve as shortcuts, impacting segmentation accuracy. We identify and evaluate
the shortcut learning on two different but common medical image segmentation
tasks. In addition, we suggest strategies to mitigate the influence of shortcut
learning and improve the generalizability of the segmentation models. By
uncovering the presence and implications of shortcuts in medical image
segmentation, we provide insights and methodologies for evaluating and
overcoming this pervasive challenge and call for attention in the community for
shortcuts in segmentation.
\\ ( https://arxiv.org/abs/2403.06748 ,  4510kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06750 (*cross-listing*)
Date: Mon, 11 Mar 2024 14:20:13 GMT   (2436kb,D)

Title: Generalising Multi-Agent Cooperation through Task-Agnostic Communication
Authors: Dulhan Jayalath, Steven Morad, Amanda Prorok
Categories: cs.MA cs.LG cs.RO
Comments: 12 pages, 6 figures, submitted to Distributed Autonomous Robotic
  Systems (DARS 2024)
\\
  Existing communication methods for multi-agent reinforcement learning (MARL)
in cooperative multi-robot problems are almost exclusively task-specific,
training new communication strategies for each unique task. We address this
inefficiency by introducing a communication strategy applicable to any task
within a given environment. We pre-train the communication strategy without
task-specific reward guidance in a self-supervised manner using a set
autoencoder. Our objective is to learn a fixed-size latent Markov state from a
variable number of agent observations. Under mild assumptions, we prove that
policies using our latent representations are guaranteed to converge, and upper
bound the value error introduced by our Markov state approximation. Our method
enables seamless adaptation to novel tasks without fine-tuning the
communication strategy, gracefully supports scaling to more agents than present
during training, and detects out-of-distribution events in an environment.
Empirical results on diverse MARL scenarios validate the effectiveness of our
approach, surpassing task-specific communication strategies in unseen tasks.
Our implementation of this work is available at
https://github.com/proroklab/task-agnostic-comms.
\\ ( https://arxiv.org/abs/2403.06750 ,  2436kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06759 (*cross-listing*)
Date: Mon, 11 Mar 2024 14:31:03 GMT   (238kb,D)

Title: Average Calibration Error: A Differentiable Loss for Improved
  Reliability in Image Segmentation
Authors: Theodore Barfoot and Luis Garcia-Peraza-Herrera and Ben Glocker and
  Tom Vercauteren
Categories: cs.CV cs.LG
\\
  Deep neural networks for medical image segmentation often produce
overconfident results misaligned with empirical observations. Such
miscalibration, challenges their clinical translation. We propose to use
marginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss
function to improve pixel-wise calibration without compromising segmentation
quality. We show that this loss, despite using hard binning, is directly
differentiable, bypassing the need for approximate but differentiable surrogate
or soft binning approaches. Our work also introduces the concept of dataset
reliability histograms which generalises standard reliability diagrams for
refined visual assessment of calibration in semantic segmentation aggregated at
the dataset level. Using mL1-ACE, we reduce average and maximum calibration
error by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS
2021 dataset. We share our code here: https://github.com/cai4cai/ACE-DLIRIS
\\ ( https://arxiv.org/abs/2403.06759 ,  238kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06798 (*cross-listing*)
Date: Mon, 11 Mar 2024 15:16:20 GMT   (1864kb,D)

Title: Dynamic Perturbation-Adaptive Adversarial Training on Medical Image
  Classification
Authors: Shuai Li, Xiaoguang Ma, Shancheng Jiang, and Lu Meng
Categories: eess.IV cs.CV cs.LG
Comments: 9 pages, 4 figures, 2 tables
\\
  Remarkable successes were made in Medical Image Classification (MIC)
recently, mainly due to wide applications of convolutional neural networks
(CNNs). However, adversarial examples (AEs) exhibited imperceptible similarity
with raw data, raising serious concerns on network robustness. Although
adversarial training (AT), in responding to malevolent AEs, was recognized as
an effective approach to improve robustness, it was challenging to overcome
generalization decline of networks caused by the AT. In this paper, in order to
reserve high generalization while improving robustness, we proposed a dynamic
perturbation-adaptive adversarial training (DPAAT) method, which placed AT in a
dynamic learning environment to generate adaptive data-level perturbations and
provided a dynamically updated criterion by loss information collections to
handle the disadvantage of fixed perturbation sizes in conventional AT methods
and the dependence on external transference. Comprehensive testing on
dermatology HAM10000 dataset showed that the DPAAT not only achieved better
robustness improvement and generalization preservation but also significantly
enhanced mean average precision and interpretability on various CNNs,
indicating its great potential as a generic adversarial training method on the
MIC.
\\ ( https://arxiv.org/abs/2403.06798 ,  1864kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06816 (*cross-listing*)
Date: Mon, 11 Mar 2024 15:33:55 GMT   (8083kb,D)

Title: Efficient first-order algorithms for large-scale, non-smooth maximum
  entropy models with application to wildfire science
Authors: Gabriel P. Langlois, Jatan Buch and J\'er\^ome Darbon
Categories: stat.ML cs.LG cs.NA math.NA math.OC
Comments: The main text of our manuscript is 20 pages long, the appendices are
  4 pages long, and the references are 4 pages long,for a total of 28 pages
MSC-class: 90C30, 90C06, 90C90, 62P12
ACM-class: G.1.6; G.3; J.2
\\
  Maximum entropy (Maxent) models are a class of statistical models that use
the maximum entropy principle to estimate probability distributions from data.
Due to the size of modern data sets, Maxent models need efficient optimization
algorithms to scale well for big data applications. State-of-the-art algorithms
for Maxent models, however, were not originally designed to handle big data
sets; these algorithms either rely on technical devices that may yield
unreliable numerical results, scale poorly, or require smoothness assumptions
that many practical Maxent models lack. In this paper, we present novel
optimization algorithms that overcome the shortcomings of state-of-the-art
algorithms for training large-scale, non-smooth Maxent models. Our proposed
first-order algorithms leverage the Kullback-Leibler divergence to train
large-scale and non-smooth Maxent models efficiently. For Maxent models with
discrete probability distribution of $n$ elements built from samples, each
containing $m$ features, the stepsize parameters estimation and iterations in
our algorithms scale on the order of $O(mn)$ operations and can be trivially
parallelized. Moreover, the strong $\ell_{1}$ convexity of the
Kullback--Leibler divergence allows for larger stepsize parameters, thereby
speeding up the convergence rate of our algorithms. To illustrate the
efficiency of our novel algorithms, we consider the problem of estimating
probabilities of fire occurrences as a function of ecological features in the
Western US MTBS-Interagency wildfire data set. Our numerical results show that
our algorithms outperform the state of the arts by one order of magnitude and
yield results that agree with physical models of wildfire occurrence and
previous statistical analyses of wildfire drivers.
\\ ( https://arxiv.org/abs/2403.06816 ,  8083kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06873 (*cross-listing*)
Date: Mon, 11 Mar 2024 16:24:26 GMT   (52kb)

Title: Last Iterate Convergence of Incremental Methods and Applications in
  Continual Learning
Authors: Xufeng Cai, Jelena Diakonikolas
Categories: math.OC cs.LG
\\
  Incremental gradient methods and incremental proximal methods are a
fundamental class of optimization algorithms used for solving finite sum
problems, broadly studied in the literature. Yet, when it comes to their
convergence guarantees, nonasymptotic (first-order or proximal) oracle
complexity bounds have been obtained fairly recently, almost exclusively
applying to the average iterate. Motivated by applications in continual
learning, we obtain the first convergence guarantees for the last iterate of
both incremental gradient and incremental proximal methods, in general convex
smooth (for both) and convex Lipschitz (for the proximal variants) settings.
Our oracle complexity bounds for the last iterate nearly match (i.e., match up
to a square-root-log or a log factor) the best known oracle complexity bounds
for the average iterate, for both classes of methods. We further obtain
generalizations of our results to weighted averaging of the iterates with
increasing weights, which can be seen as interpolating between the last iterate
and the average iterate guarantees. Additionally, we discuss how our results
can be generalized to variants of studied incremental methods with permuted
ordering of updates. Our results generalize last iterate guarantees for
incremental methods compared to state of the art, as such results were
previously known only for overparameterized linear models, which correspond to
convex quadratic problems with infinitely many solutions.
\\ ( https://arxiv.org/abs/2403.06873 ,  52kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06874 (*cross-listing*)
Date: Mon, 11 Mar 2024 16:26:35 GMT   (5398kb,D)

Title: COOD: Combined out-of-distribution detection using multiple measures for
  anomaly & novel class detection in large-scale hierarchical classification
Authors: L. E. Hogeweg, R. Gangireddy, D. Brunink, V. J. Kalkman, L.
  Cornelissen, J.W. Kamminga
Categories: cs.CV cs.LG
\\
  High-performing out-of-distribution (OOD) detection, both anomaly and novel
class, is an important prerequisite for the practical use of classification
models. In this paper, we focus on the species recognition task in images
concerned with large databases, a large number of fine-grained hierarchical
classes, severe class imbalance, and varying image quality. We propose a
framework for combining individual OOD measures into one combined OOD (COOD)
measure using a supervised model. The individual measures are several existing
state-of-the-art measures and several novel OOD measures developed with novel
class detection and hierarchical class structure in mind. COOD was extensively
evaluated on three large-scale (500k+ images) biodiversity datasets in the
context of anomaly and novel class detection. We show that COOD outperforms
individual, including state-of-the-art, OOD measures by a large margin in terms
of TPR@1% FPR in the majority of experiments, e.g., improving detecting
ImageNet images (OOD) from 54.3% to 85.4% for the iNaturalist 2018 dataset.
SHAP (feature contribution) analysis shows that different individual OOD
measures are essential for various tasks, indicating that multiple OOD measures
and combinations are needed to generalize. Additionally, we show that
explicitly considering ID images that are incorrectly classified for the
original (species) recognition task is important for constructing
high-performing OOD detection methods and for practical applicability. The
framework can easily be extended or adapted to other tasks and media
modalities.
\\ ( https://arxiv.org/abs/2403.06874 ,  5398kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06888 (*cross-listing*)
Date: Mon, 11 Mar 2024 16:45:19 GMT   (1918kb)

Title: HiRA-Pro: High resolution alignment of multimodal spatio-temporal data:
  a process physics driven approach
Authors: Abhishek Hanchate, Himanshu Balhara, Vishal S. Chindepalli, Satish
  T.S. Bukkapatnam
Categories: physics.data-an cs.LG physics.app-ph
\\
  We present HiRA-Pro, a novel procedure to align, at high spatio-temporal
resolutions, multimodal signals from real-world processes and systems that
exhibit diverse transient, nonlinear stochastic dynamics, such as manufacturing
machines. It is based on discerning and synchronizing the process signatures of
salient kinematic and dynamic events in these disparate signals. HiRA-Pro
addresses the challenge of aligning data with sub-millisecond phenomena, where
traditional timestamp, external trigger, or clock-based alignment methods fall
short. The effectiveness of HiRA-Pro is demonstrated in a smart manufacturing
context, where it aligns data from 13+ channels acquired during 3D-printing and
milling operations on an Optomec-LENS MTS 500 hybrid machine. The aligned data
is then voxelized to generate 0.25 second aligned data chunks that correspond
to physical voxels on the produced part. The superiority of HiRA-Pro is further
showcased through case studies in additive manufacturing, demonstrating
improved machine learning-based predictive performance due to precise
multimodal data alignment. Specifically, testing classification accuracies
improved by almost 35% with the application of HiRA-Pro, even with limited
data, allowing for precise localization of artifacts. The paper also provides a
comprehensive discussion on the proposed method, its applications, and
comparative qualitative analysis with a few other alignment methods. HiRA-Pro
achieves temporal-spatial resolutions of 10-1000 us and 100 um in order to
generate datasets that register with physical voxels on the 3D-printed and
milled part. These resolutions are at least an order of magnitude finer than
the existing alignment methods that employ individual timestamps, statistical
correlations, or common clocks, which achieve precision of hundreds of
milliseconds.
\\ ( https://arxiv.org/abs/2403.06888 ,  1918kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06890 (*cross-listing*)
Date: Mon, 11 Mar 2024 16:47:09 GMT   (934kb,D)

Title: Application of Quantum Tensor Networks for Protein Classification
Authors: Debarshi Kundu, Archisman Ghosh, Srinivasan Ekambaram, Jian Wang,
  Nikolay Dokholyan, Swaroop Ghosh
Categories: quant-ph cs.LG q-bio.BM
Comments: 7 pages, 8 figures
\\
  We show that protein sequences can be thought of as sentences in natural
language processing and can be parsed using the existing Quantum Natural
Language framework into parameterized quantum circuits of reasonable qubits,
which can be trained to solve various protein-related machine-learning
problems. We classify proteins based on their subcellular locations, a pivotal
task in bioinformatics that is key to understanding biological processes and
disease mechanisms. Leveraging the quantum-enhanced processing capabilities, we
demonstrate that Quantum Tensor Networks (QTN) can effectively handle the
complexity and diversity of protein sequences. We present a detailed
methodology that adapts QTN architectures to the nuanced requirements of
protein data, supported by comprehensive experimental results. We demonstrate
two distinct QTNs, inspired by classical recurrent neural networks (RNN) and
convolutional neural networks (CNN), to solve the binary classification task
mentioned above. Our top-performing quantum model has achieved a 94% accuracy
rate, which is comparable to the performance of a classical model that uses the
ESM2 protein language model embeddings. It's noteworthy that the ESM2 model is
extremely large, containing 8 million parameters in its smallest configuration,
whereas our best quantum model requires only around 800 parameters. We
demonstrate that these hybrid models exhibit promising performance, showcasing
their potential to compete with classical models of similar complexity.
\\ ( https://arxiv.org/abs/2403.06890 ,  934kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06940 (*cross-listing*)
Date: Mon, 11 Mar 2024 17:26:18 GMT   (15315kb,D)

Title: Conditional Score-Based Diffusion Model for Cortical Thickness
  Trajectory Prediction
Authors: Qing Xiao, Siyeop Yoon, Hui Ren, Matthew Tivnan, Lichao Sun, Quanzheng
  Li, Tianming Liu, Yu Zhang, and Xiang Li
Categories: eess.IV cs.LG q-bio.QM
\\
  Alzheimer's Disease (AD) is a neurodegenerative condition characterized by
diverse progression rates among individuals, with changes in cortical thickness
(CTh) closely linked to its progression. Accurately forecasting CTh
trajectories can significantly enhance early diagnosis and intervention
strategies, providing timely care. However, the longitudinal data essential for
these studies often suffer from temporal sparsity and incompleteness,
presenting substantial challenges in modeling the disease's progression
accurately. Existing methods are limited, focusing primarily on datasets
without missing entries or requiring predefined assumptions about CTh
progression. To overcome these obstacles, we propose a conditional score-based
diffusion model specifically designed to generate CTh trajectories with the
given baseline information, such as age, sex, and initial diagnosis. Our
conditional diffusion model utilizes all available data during the training
phase to make predictions based solely on baseline information during inference
without needing prior history about CTh progression. The prediction accuracy of
the proposed CTh prediction pipeline using a conditional score-based model was
compared for sub-groups consisting of cognitively normal, mild cognitive
impairment, and AD subjects. The Bland-Altman analysis shows our
diffusion-based prediction model has a near-zero bias with narrow 95%
confidential interval compared to the ground-truth CTh in 6-36 months. In
addition, our conditional diffusion model has a stochastic generative nature,
therefore, we demonstrated an uncertainty analysis of patient-specific CTh
prediction through multiple realizations.
\\ ( https://arxiv.org/abs/2403.06940 ,  15315kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06942 (*cross-listing*)
Date: Mon, 11 Mar 2024 17:28:46 GMT   (26769kb,D)

Title: Grid Monitoring and Protection with Continuous Point-on-Wave
  Measurements and Generative AI
Authors: Lang Tong, Xinyi Wang, Qing Zhao
Categories: eess.SY cs.LG cs.SY stat.ML
\\
  Purpose This article presents a case for a next-generation grid monitoring
and control system, leveraging recent advances in generative artificial
intelligence (AI), machine learning, and statistical inference. Advancing
beyond earlier generations of wide-area monitoring systems built upon
supervisory control and data acquisition (SCADA) and synchrophasor
technologies, we argue for a monitoring and control framework based on the
streaming of continuous point-on-wave (CPOW) measurements with AI-powered data
compression and fault detection.
  Methods and Results: The architecture of the proposed design originates from
the Wiener-Kallianpur innovation representation of a random process that
transforms causally a stationary random process into an innovation sequence
with independent and identically distributed random variables. This work
presents a generative AI approach that (i) learns an innovation autoencoder
that extracts innovation sequence from CPOW time series, (ii) compresses the
CPOW streaming data with innovation autoencoder and subband coding, and (iii)
detects unknown faults and novel trends via nonparametric sequential hypothesis
testing.
  Conclusion: This work argues that conventional monitoring using SCADA and
phasor measurement unit (PMU) technologies is ill-suited for a future grid with
deep penetration of inverter-based renewable generations and distributed energy
resources. A monitoring system based on CPOW data streaming and AI data
analytics should be the basic building blocks for situational awareness of a
highly dynamic future grid.
\\ ( https://arxiv.org/abs/2403.06942 ,  26769kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06955 (*cross-listing*)
Date: Mon, 11 Mar 2024 17:39:08 GMT   (10512kb,D)

Title: Accurate Crystal Structure Prediction of New 2D Hybrid Organic Inorganic
  Perovskites
Authors: Nima Karimitari, William J. Baldwin, Evan W. Muller, Zachary J. L.
  Bare, W. Joshua Kennedy, G\'abor Cs\'anyi, Christopher Sutton
Categories: cond-mat.mtrl-sci cs.LG
Comments: 14 pages and 9 figures in the main text. Supplementary included in
  pdf
\\
  Low dimensional hybrid organic-inorganic perovskites (HOIPs) represent a
promising class of electronically active materials for both light absorption
and emission. The design space of HOIPs is extremely large, since a diverse
space of organic cations can be combined with different inorganic frameworks.
This immense design space allows for tunable electronic and mechanical
properties, but also necessitates the development of new tools for in silico
high throughput analysis of candidate structures. In this work, we present an
accurate, efficient, transferable and widely applicable machine learning
interatomic potential (MLIP) for predicting the structure of new 2D HOIPs.
Using the MACE architecture, an MLIP is trained on 86 diverse experimentally
reported HOIP structures. The model is tested on 73 unseen perovskite
compositions, and achieves chemical accuracy with respect to the reference
electronic structure method. Our model is then combined with a simple random
structure search algorithm to predict the structure of hypothetical HOIPs given
only the proposed composition. Success is demonstrated by correctly and
reliably recovering the crystal structure of a set of experimentally known 2D
perovskites. Such a random structure search is impossible with ab initio
methods due to the associated computational cost, but is relatively inexpensive
with the MACE potential. Finally, the procedure is used to predict the
structure formed by a new organic cation with no previously known corresponding
perovskite. Laboratory synthesis of the new hybrid perovskite confirms the
accuracy of our prediction. This capability, applied at scale, enables
efficient screening of thousands of combinations of organic cations and
inorganic layers.
\\ ( https://arxiv.org/abs/2403.06955 ,  10512kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06973 (*cross-listing*)
Date: Mon, 11 Mar 2024 17:55:53 GMT   (4820kb,D)

Title: Bayesian Diffusion Models for 3D Shape Reconstruction
Authors: Haiyang Xu, Yu Lei, Zeyuan Chen, Xiang Zhang, Yue Zhao, Yilin Wang,
  Zhuowen Tu
Categories: cs.CV cs.LG
Comments: Accepted by CVPR 2024
\\
  We present Bayesian Diffusion Models (BDM), a prediction algorithm that
performs effective Bayesian inference by tightly coupling the top-down (prior)
information with the bottom-up (data-driven) procedure via joint diffusion
processes. We show the effectiveness of BDM on the 3D shape reconstruction
task. Compared to prototypical deep learning data-driven approaches trained on
paired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM
brings in rich prior information from standalone labels (e.g. point clouds) to
improve the bottom-up 3D reconstruction. As opposed to the standard Bayesian
frameworks where explicit prior and likelihood are required for the inference,
BDM performs seamless information fusion via coupled diffusion processes with
learned gradient computation networks. The specialty of our BDM lies in its
capability to engage the active and effective information exchange and fusion
of the top-down and bottom-up processes where each itself is a diffusion
process. We demonstrate state-of-the-art results on both synthetic and
real-world benchmarks for 3D shape reconstruction.
\\ ( https://arxiv.org/abs/2403.06973 ,  4820kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2007.00714
replaced with revised version Fri, 8 Mar 2024 13:33:03 GMT   (512kb,D)

Title: Quantifying intrinsic causal contributions via structure preserving
  interventions
Authors: Dominik Janzing, Patrick Bl\"obaum, Atalanti A. Mastakouri, Philipp M.
  Faller, Lenon Minorics, Kailash Budhathoki
Categories: cs.AI cs.IT math.IT stat.ML
Comments: to appear at AISTATS 2024
\\ ( https://arxiv.org/abs/2007.00714 ,  512kb)
------------------------------------------------------------------------------
\\
arXiv:2201.07381
replaced with revised version Mon, 11 Mar 2024 07:52:47 GMT   (7653kb,D)

Title: Unveiling Project-Specific Bias in Neural Code Models
Authors: Zhiming Li, Yanzhou Li, Tianlin Li, Mengnan Du, Bozhi Wu, Yushi Cao,
  Junzhe Jiang, Yang Liu
Categories: cs.AI cs.SE
Comments: Accepted by LREC-COLING 2024
\\ ( https://arxiv.org/abs/2201.07381 ,  7653kb)
------------------------------------------------------------------------------
\\
arXiv:2210.08569
replaced with revised version Fri, 8 Mar 2024 21:31:36 GMT   (5176kb,D)

Title: Limited or Biased: Modeling Sub-Rational Human Investors in Financial
  Markets
Authors: Penghang Liu, Kshama Dwarakanath, Svitlana S Vyetrenko, Tucker Balch
Categories: cs.AI cs.MA q-fin.TR
\\ ( https://arxiv.org/abs/2210.08569 ,  5176kb)
------------------------------------------------------------------------------
\\
arXiv:2211.09935
replaced with revised version Sat, 9 Mar 2024 13:53:47 GMT   (18749kb,D)

Title: CAPE: Corrective Actions from Precondition Errors using Large Language
  Models
Authors: Shreyas Sundara Raman, Vanya Cohen, Ifrah Idrees, Eric Rosen, Ray
  Mooney, Stefanie Tellex and David Paulius
Categories: cs.AI cs.CL cs.LG cs.RO
Comments: 17 pages, 6 figures, accepted at ICRA 2024
MSC-class: 68T20, 68T50
ACM-class: I.2.7; I.2.8; I.2.2; I.2.4
\\ ( https://arxiv.org/abs/2211.09935 ,  18749kb)
------------------------------------------------------------------------------
\\
arXiv:2306.15035
replaced with revised version Sat, 9 Mar 2024 14:41:50 GMT   (10073kb,D)

Title: Optimized Vectorizing of Building Structures with Switch:
  High-Efficiency Convolutional Channel-Switch Hybridization Strategy
Authors: Moule Lin, Weipeng Jing, Chao Li and Andr\'as Jung
Categories: cs.AI cs.CV
Comments: 5 pages
\\ ( https://arxiv.org/abs/2306.15035 ,  10073kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03067
replaced with revised version Sat, 9 Mar 2024 02:17:42 GMT   (327kb,D)

Title: DeepOnto: A Python Package for Ontology Engineering with Deep Learning
Authors: Yuan He, Jiaoyan Chen, Hang Dong, Ian Horrocks, Carlo Allocca, Taehun
  Kim, Brahmananda Sapkota
Categories: cs.AI cs.CL cs.LG cs.LO
Comments: Accepted by the Semantic Web Journal
\\ ( https://arxiv.org/abs/2307.03067 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14363
replaced with revised version Mon, 11 Mar 2024 16:18:17 GMT   (9017kb,D)

Title: Rethinking Mobile AI Ecosystem in the LLM Era
Authors: Jinliang Yuan, Chen Yang, Dongqi Cai, Shihe Wang, Xin Yuan, Zeling
  Zhang, Xiang Li, Dingge Zhang, Hanzi Mei, Xianqing Jia, Shangguang Wang,
  Mengwei Xu
Categories: cs.AI
Comments: 17 pages, 15 figures, published to ACM MobiCom'24
Journal-ref: The 30th Annual International Conference on Mobile Computing and
  Networking, 2024
DOI: 10.1145/3636534.3649361
\\ ( https://arxiv.org/abs/2308.14363 ,  9017kb)
------------------------------------------------------------------------------
\\
arXiv:2308.15272
replaced with revised version Sat, 9 Mar 2024 09:38:51 GMT   (6520kb,D)

Title: AutoDroid: LLM-powered Task Automation in Android
Authors: Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun
  Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, Yunxin Liu
Categories: cs.AI cs.SE
Comments: Published in MobiCom 2024; Original title: "Empowering LLM to use
  Smartphone for Intelligent Task Automation"
\\ ( https://arxiv.org/abs/2308.15272 ,  6520kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10444
replaced with revised version Sun, 10 Mar 2024 13:48:41 GMT   (246kb,D)

Title: Exploring Iterative Enhancement for Improving Learnersourced
  Multiple-Choice Question Explanations with Large Language Models
Authors: Qiming Bao, Juho Leinonen, Alex Yuxuan Peng, Wanjun Zhong, Ga\"el
  Gendron, Timothy Pistotti, Alice Huang, Paul Denny, Michael Witbrock and
  Jiamou Liu
Categories: cs.AI cs.CL
Comments: The short version (v4) was accepted as a non-archival workshop paper
  at AGI@ICLR 2024; the full version is under review
\\ ( https://arxiv.org/abs/2309.10444 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04963
replaced with revised version Sun, 10 Mar 2024 21:05:28 GMT   (50kb)

Title: LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation
Authors: Christian Munley, Aaron Jarmusch and Sunita Chandrasekaran
Categories: cs.AI
\\ ( https://arxiv.org/abs/2310.04963 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05136
replaced with revised version Mon, 11 Mar 2024 07:32:31 GMT   (23584kb,D)

Title: InstructDET: Diversifying Referring Object Detection with Generalized
  Instructions
Authors: Ronghao Dang, Jiangyan Feng, Haodong Zhang, Chongjian Ge, Lin Song,
  Lijun Gong, Chengju Liu, Qijun Chen, Feng Zhu, Rui Zhao, Yibing Song
Categories: cs.AI cs.CV
Comments: 29 pages (include Appendix) Published in ICLR
\\ ( https://arxiv.org/abs/2310.05136 ,  23584kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05473
replaced with revised version Sat, 9 Mar 2024 08:51:41 GMT   (16789kb,D)

Title: Self Model for Embodied Intelligence: Modeling Full-Body Human
  Musculoskeletal System and Locomotion Control with Hierarchical
  Low-Dimensional Representation
Authors: Kaibo He, Chenhui Zuo, Jing Shao, Yanan Sui
Categories: cs.AI
Comments: ICRA 2024
\\ ( https://arxiv.org/abs/2312.05473 ,  16789kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17445
replaced with revised version Sat, 9 Mar 2024 02:16:07 GMT   (1218kb,D)

Title: State Machine of Thoughts: Leveraging Past Reasoning Trajectories for
  Enhancing Problem Solving
Authors: Jia Liu, Jie Shuai, Xiyao Li
Categories: cs.AI
Comments: 9 pages, 4 figures
\\ ( https://arxiv.org/abs/2312.17445 ,  1218kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03093
replaced with revised version Mon, 11 Mar 2024 12:57:32 GMT   (413kb)

Title: XXAI: Explicitly Explainable AI provides transparency in automatic
  decision-making by overcoming the limitations of symbolic AI
Authors: V. L. Kalmykov, L.V. Kalmykov
Categories: cs.AI q-bio.PE
Comments: 18 pages, 1 graphical abstract, 1 figure, 72 references
\\ ( https://arxiv.org/abs/2401.03093 ,  413kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04846
replaced with revised version Sat, 9 Mar 2024 13:58:07 GMT   (5082kb,D)

Title: The inherent goodness of well educated intelligence
Authors: Michael E. Glinsky and Sharon Sievert
Categories: cs.AI physics.soc-ph
Comments: 13 pages, 12 figures, 15 equations, to be submitted to Nature
\\ ( https://arxiv.org/abs/2401.04846 ,  5082kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04559
replaced with revised version Sun, 10 Mar 2024 13:48:43 GMT   (4231kb,D)

Title: Can Large Language Model Agents Simulate Human Trust Behaviors?
Authors: Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Kai Shu, Adel Bibi,
  Ziniu Hu, Philip Torr, Bernard Ghanem, Guohao Li
Categories: cs.AI cs.CL cs.HC
Comments: The first two authors contributed equally. Project website:
  https://www.camel-ai.org/research/agent-trust
\\ ( https://arxiv.org/abs/2402.04559 ,  4231kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13058
replaced with revised version Sat, 9 Mar 2024 08:43:20 GMT   (735kb,D)

Title: Random Graph Set and Evidence Pattern Reasoning Model
Authors: Tianxiang Zhan, Zhen Li, Yong Deng
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.13058 ,  735kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16278
replaced with revised version Sun, 10 Mar 2024 10:04:41 GMT   (1254kb,D)

Title: A Self-matching Training Method with Annotation Embedding Models for
  Ontology Subsumption Prediction
Authors: Yukihiro Shiraishi, Ken Kaneiwa
Categories: cs.AI cs.CL cs.LG
Comments: 21 pages, 6 figures
\\ ( https://arxiv.org/abs/2402.16278 ,  1254kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03357
replaced with revised version Mon, 11 Mar 2024 16:16:22 GMT   (3268kb,D)

Title: The Case for Globalizing Fairness: A Mixed Methods Study on Colonialism,
  AI, and Health in Africa
Authors: Mercy Asiedu, Awa Dieng, Iskandar Haykel, Negar Rostamzadeh, Stephen
  Pfohl, Chirag Nagpal, Maria Nagawa, Abigail Oppong, Sanmi Koyejo, Katherine
  Heller
Categories: cs.AI cs.CY
Comments: 11 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:2304.02190
\\ ( https://arxiv.org/abs/2403.03357 ,  3268kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03768
replaced with revised version Sat, 9 Mar 2024 15:01:52 GMT   (19589kb,D)

Title: DeepCRE: Revolutionizing Drug R&D with Cutting-Edge Computational Models
Authors: Yushuai Wu
Categories: cs.AI cs.LG q-bio.QM
\\ ( https://arxiv.org/abs/2403.03768 ,  19589kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04035
replaced with revised version Sat, 9 Mar 2024 02:47:28 GMT   (738kb,D)

Title: Personalizing explanations of AI-driven hints to users' cognitive
  abilities: an empirical evaluation
Authors: Vedant Bahel, Harshinee Sriram and Cristina Conati
Categories: cs.AI cs.CY cs.HC
\\ ( https://arxiv.org/abs/2403.04035 ,  738kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04121
replaced with revised version Fri, 8 Mar 2024 19:51:14 GMT   (5418kb,D)

Title: Can Large Language Models Reason and Plan?
Authors: Subbarao Kambhampati
Categories: cs.AI cs.CL cs.LG
Comments: arXiv admin note: text overlap with arXiv:2402.01817 (v2 add creative
  commons attribution to Figure 2 graphic)
Journal-ref: Annals of The New York Academy of Sciences; March 2024
DOI: 10.1111/nyas.15125
\\ ( https://arxiv.org/abs/2403.04121 ,  5418kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04264
replaced with revised version Sat, 9 Mar 2024 20:17:25 GMT   (1499kb,D)

Title: Competitive Facility Location under Random Utilities and Routing
  Constraints
Authors: Hoang Giang Pham, Tien Thanh Dam, Ngan Ha Duong, Tien Mai and Minh
  Hoang Ha
Categories: cs.AI
\\ ( https://arxiv.org/abs/2403.04264 ,  1499kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04859
replaced with revised version Mon, 11 Mar 2024 09:32:20 GMT   (372kb)

Title: Self-Supervision in Time for Satellite Images(S3-TSS): A novel method of
  SSL technique in Satellite images
Authors: Akansh Maurya, Hewan Shrestha, Mohammad Munem Shahriar
Categories: cs.AI
\\ ( https://arxiv.org/abs/2403.04859 ,  372kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05525
replaced with revised version Mon, 11 Mar 2024 16:47:41 GMT   (7262kb,D)

Title: DeepSeek-VL: Towards Real-World Vision-Language Understanding
Authors: Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu,
  Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi
  Deng, Hanwei Xu, Zhenda Xie, Chong Ruan
Categories: cs.AI
Comments: https://github.com/deepseek-ai/DeepSeek-VL
\\ ( https://arxiv.org/abs/2403.05525 ,  7262kb)
------------------------------------------------------------------------------
\\
arXiv:1808.09334
replaced with revised version Sun, 10 Mar 2024 18:35:27 GMT   (335kb,D)

Title: A Discriminative Latent-Variable Model for Bilingual Lexicon Induction
Authors: Sebastian Ruder, Ryan Cotterell, Yova Kementchedjhieva, Anders
  S{\o}gaard
Categories: cs.CL cs.LG stat.ML
Comments: Proceedings of the 2018 Conference on Empirical Methods in Natural
  Language Processing
\\ ( https://arxiv.org/abs/1808.09334 ,  335kb)
------------------------------------------------------------------------------
\\
arXiv:2301.03765
replaced with revised version Sat, 9 Mar 2024 07:12:52 GMT   (1301kb,D)

Title: Cross-Model Comparative Loss for Enhancing Neuronal Utility in Language
  Understanding
Authors: Yunchang Zhu, Liang Pang, Kangxi Wu, Yanyan Lan, Huawei Shen, Xueqi
  Cheng
Categories: cs.CL cs.IR cs.LG
\\ ( https://arxiv.org/abs/2301.03765 ,  1301kb)
------------------------------------------------------------------------------
\\
arXiv:2304.01665
replaced with revised version Sat, 9 Mar 2024 15:27:33 GMT   (3235kb,D)

Title: Mastering Symbolic Operations: Augmenting Language Models with Compiled
  Neural Networks
Authors: Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, Jun
  Zhao
Categories: cs.CL
Comments: Accepted in ICLR 2024
\\ ( https://arxiv.org/abs/2304.01665 ,  3235kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14902
replaced with revised version Sun, 10 Mar 2024 01:04:48 GMT   (4382kb,D)

Title: M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box
  Machine-Generated Text Detection
Authors: Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem
  Shelmanov, Akim Tsvigun, Chenxi Whitehouse, Osama Mohammed Afzal, Tarek
  Mahmoud, Toru Sasaki, Thomas Arnold, Alham Fikri Aji, Nizar Habash, Iryna
  Gurevych, Preslav Nakov
Categories: cs.CL
Comments: 41 pages
\\ ( https://arxiv.org/abs/2305.14902 ,  4382kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05783
replaced with revised version Mon, 11 Mar 2024 09:49:04 GMT   (5464kb,D)

Title: Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge
  Evaluation
Authors: Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen Wang, Yixin
  Zhu, Sihang Jiang, Zhuozhi Xiong, Zihan Li, Weijie Wu, Qianyu He, Rui Xu,
  Wenhao Huang, Jingping Liu, Zili Wang, Shusen Wang, Weiguo Zheng, Hongwei
  Feng, Yanghua Xiao
Categories: cs.CL
Comments: Accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2306.05783 ,  5464kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14580
replaced with revised version Sat, 9 Mar 2024 09:15:41 GMT   (1615kb,D)

Title: TransERR: Translation-based Knowledge Graph Embedding via Efficient
  Relation Rotation
Authors: Jiang Li and Xiangdong Su and Fujun Zhang and Guanglai Gao
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2306.14580 ,  1615kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00920
replaced with revised version Mon, 11 Mar 2024 14:56:47 GMT   (4571kb,D)

Title: Node-weighted Graph Convolutional Network for Depression Detection in
  Transcribed Clinical Interviews
Authors: Sergio Burdisso, Esa\'u Villatoro-Tello, Srikanth Madikeri, Petr
  Motlicek
Categories: cs.CL cs.AI
Comments: Paper Accepted to Interspeech 2023
Journal-ref: Interspeech 2023
DOI: 10.21437/Interspeech.2023-1923
\\ ( https://arxiv.org/abs/2307.00920 ,  4571kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08153
replaced with revised version Sat, 9 Mar 2024 14:18:41 GMT   (3033kb,D)

Title: Analyzing Dataset Annotation Quality Management in the Wild
Authors: Jan-Christoph Klie, Richard Eckart de Castilho, Iryna Gurevych
Categories: cs.CL
\\ ( https://arxiv.org/abs/2307.08153 ,  3033kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08488
replaced with revised version Sat, 9 Mar 2024 01:36:06 GMT   (11207kb,D)

Title: Improving Audio-Visual Speech Recognition by Lip-Subword Correlation
  Based Visual Pre-training and Cross-Modal Fusion Encoder
Authors: Yusheng Dai, Hang Chen, Jun Du, Xiaofei Ding, Ning Ding, Feijun Jiang,
  Chin-Hui Lee
Categories: cs.CL cs.AI cs.SD eess.AS
Comments: 6 pages, 2 figures, published in ICME2023
\\ ( https://arxiv.org/abs/2308.08488 ,  11207kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13139
replaced with revised version Mon, 11 Mar 2024 14:50:03 GMT   (866kb,D)

Title: MatchXML: An Efficient Text-label Matching Framework for Extreme
  Multi-label Text Classification
Authors: Hui Ye, Rajshekhar Sunderraman, Shihao Ji
Categories: cs.CL cs.LG
Comments: Accepted to TKDE 2024
\\ ( https://arxiv.org/abs/2308.13139 ,  866kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01940
replaced with revised version Mon, 11 Mar 2024 08:07:28 GMT   (3254kb,D)

Title: CodeApex: A Bilingual Programming Evaluation Benchmark for Large
  Language Models
Authors: Lingyue Fu, Huacan Chai, Shuang Luo, Kounianhua Du, Weiming Zhang,
  Longteng Fan, Jiayi Lei, Renting Rui, Jianghao Lin, Yuchen Fang, Yifan Liu,
  Jingkuan Wang, Siyuan Qi, Kangning Zhang, Weinan Zhang, Yong Yu
Categories: cs.CL cs.AI
Comments: 33pages
\\ ( https://arxiv.org/abs/2309.01940 ,  3254kb)
------------------------------------------------------------------------------
\\
arXiv:2309.03883
replaced with revised version Mon, 11 Mar 2024 02:01:09 GMT   (243kb,D)

Title: DoLa: Decoding by Contrasting Layers Improves Factuality in Large
  Language Models
Authors: Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass,
  Pengcheng He
Categories: cs.CL cs.AI cs.LG
Comments: ICLR 2024 main conference paper. The source code is available at
  https://github.com/voidism/DoLa
\\ ( https://arxiv.org/abs/2309.03883 ,  243kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11998
replaced with revised version Sun, 10 Mar 2024 19:34:57 GMT   (1034kb,D)

Title: LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset
Authors: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang,
  Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E.
  Gonzalez, Ion Stoica, Hao Zhang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2309.11998 ,  1034kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02107
replaced with revised version Sat, 9 Mar 2024 19:07:00 GMT   (11032kb,D)

Title: Instances Need More Care: Rewriting Prompts for Instances with LLMs in
  the Loop Yields Better Zero-Shot Performance
Authors: Saurabh Srivastava, Chengyue Huang, Weiguo Fan, Ziyu Yao
Categories: cs.CL
Comments: Work in progress
\\ ( https://arxiv.org/abs/2310.02107 ,  11032kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07225
replaced with revised version Sat, 9 Mar 2024 23:26:34 GMT   (143kb)

Title: Exploring the landscape of large language models in medical question
  answering
Authors: Andrew M. Bean, Karolina Korgul, Felix Krones, Robert McCraith, Adam
  Mahdi
Categories: cs.CL
Comments: 11 pages, 8 figures
\\ ( https://arxiv.org/abs/2310.07225 ,  143kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08491
replaced with revised version Sat, 9 Mar 2024 10:44:58 GMT   (12942kb,D)

Title: Prometheus: Inducing Fine-grained Evaluation Capability in Language
  Models
Authors: Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran
  Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, Minjoon Seo
Categories: cs.CL cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.08491 ,  12942kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10638
replaced with revised version Sat, 9 Mar 2024 22:22:48 GMT   (1088kb,D)

Title: In-Context Pretraining: Language Modeling Beyond Document Boundaries
Authors: Weijia Shi and Sewon Min and Maria Lomeli and Chunting Zhou and
  Margaret Li and Gergely Szilvasy and Rich James and Xi Victoria Lin and Noah
  A. Smith and Luke Zettlemoyer and Scott Yih and Mike Lewis
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2310.10638 ,  1088kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14505
replaced with revised version Mon, 11 Mar 2024 04:13:50 GMT   (367kb)

Title: Sentiment analysis with adaptive multi-head attention in Transformer
Authors: Fanfei Meng, Chen-Ao Wang
Categories: cs.CL
Comments: Accepted by the 4th International Conference on Signal Processing and
  Machine Learning
\\ ( https://arxiv.org/abs/2310.14505 ,  367kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16789
replaced with revised version Sat, 9 Mar 2024 22:26:06 GMT   (2404kb,D)

Title: Detecting Pretraining Data from Large Language Models
Authors: Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu,
  Terra Blevins, Danqi Chen, Luke Zettlemoyer
Categories: cs.CL cs.CR cs.LG
\\ ( https://arxiv.org/abs/2310.16789 ,  2404kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18152
replaced with revised version Sat, 9 Mar 2024 16:08:16 GMT   (153kb,D)

Title: Disentangled Representation Learning with Large Language Models for
  Text-Attributed Graphs
Authors: Yijian Qin, Xin Wang, Ziwei Zhang, Wenwu Zhu
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2310.18152 ,  153kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00262
replaced with revised version Mon, 11 Mar 2024 08:30:31 GMT   (196kb,D)

Title: Plug-and-Play Policy Planner for Large Language Model Powered Dialogue
  Agents
Authors: Yang Deng, Wenxuan Zhang, Wai Lam, See-Kiong Ng, Tat-Seng Chua
Categories: cs.CL cs.AI
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2311.00262 ,  196kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08299
replaced with revised version Fri, 8 Mar 2024 19:29:47 GMT   (9260kb,D)

Title: VERVE: Template-based ReflectiVE Rewriting for MotiVational IntErviewing
Authors: Do June Min and Ver\'onica P\'erez-Rosas and Kenneth Resnicow and Rada
  Mihalcea
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2311.08299 ,  9260kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09533
replaced with revised version Mon, 11 Mar 2024 05:36:36 GMT   (302kb,D)

Title: Effective Large Language Model Adaptation for Improved Grounding and
  Citation Generation
Authors: Xi Ye, Ruoxi Sun, Sercan \"O. Arik, Tomas Pfister
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.09533 ,  302kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09889
replaced with revised version Mon, 11 Mar 2024 11:05:21 GMT   (3944kb,D)

Title: Language Generation from Brain Recordings
Authors: Ziyi Ye, Qingyao Ai, Yiqun Liu, Maarten de Rijke, Min Zhang, Christina
  Lioma, Tuukka Ruotsalo
Categories: cs.CL
Comments: Preprint. Under Submission
\\ ( https://arxiv.org/abs/2311.09889 ,  3944kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13729
replaced with revised version Sun, 10 Mar 2024 01:11:20 GMT   (2212kb,D)

Title: Comparison of pipeline, sequence-to-sequence, and GPT models for
  end-to-end relation extraction: experiments with the rare disease use-case
Authors: Shashank Gupta, Xuguang Ai, Ramakanth Kavuluru
Categories: cs.CL
Comments: In V2 we added new experiments with T5 models. The dataset and code
  for all our experiments are publicly available:
  https://github.com/shashank140195/Raredis
\\ ( https://arxiv.org/abs/2311.13729 ,  2212kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05990
replaced with revised version Sat, 9 Mar 2024 04:55:47 GMT   (9510kb,D)

Title: Constructing Vec-tionaries to Extract Message Features from Texts: A
  Case Study of Moral Appeals
Authors: Zening Duan, Anqi Shao, Yicheng Hu, Heysung Lee, Xining Liao, Yoo Ji
  Suh, Jisoo Kim, Kai-Cheng Yang, Kaiping Chen, and Sijia Yang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2312.05990 ,  9510kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12021
replaced with revised version Mon, 11 Mar 2024 14:42:28 GMT   (2881kb,D)

Title: Synergistic Anchored Contrastive Pre-training for Few-Shot Relation
  Extraction
Authors: Da Luo, Yanglei Gan, Rui Hou, Run Lin, Qiao Liu, Yuxiang Cai, Wannian
  Gao
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2312.12021 ,  2881kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15710
replaced with revised version Mon, 11 Mar 2024 07:50:05 GMT   (236kb,D)

Title: Alleviating Hallucinations of Large Language Models through Induced
  Hallucinations
Authors: Yue Zhang, Leyang Cui, Wei Bi, Shuming Shi
Categories: cs.CL cs.AI
Comments: Work in progress
\\ ( https://arxiv.org/abs/2312.15710 ,  236kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05507
replaced with revised version Mon, 11 Mar 2024 07:57:59 GMT   (1090kb,D)

Title: InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks
Authors: Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Qianli Ma, Guoyin Wang,
  Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, Yao Cheng, Jianbo Yuan, Jiwei Li,
  Kun Kuang, Yang Yang, Hongxia Yang, Fei Wu
Categories: cs.CL cs.AI
Comments: 27 pages, 7 figures, work in progress
\\ ( https://arxiv.org/abs/2401.05507 ,  1090kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10286
replaced with revised version Mon, 11 Mar 2024 01:23:47 GMT   (165kb,D)

Title: Code-Based English Models Surprising Performance on Chinese QA Pair
  Extraction Task
Authors: Linghan Zheng, Hui Liu, Xiaojun Lin, Jiayuan Dong, Yue Sheng, Gang
  Shi, Zhiwei Liu, Hongwei Chen
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.10286 ,  165kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07577
replaced with revised version Sat, 9 Mar 2024 05:35:21 GMT   (407kb,D)

Title: Topic Modeling as Multi-Objective Contrastive Optimization
Authors: Thong Nguyen, Xiaobao Wu, Xinshuai Dong, Cong-Duy T Nguyen, See-Kiong
  Ng, Anh Tuan Luu
Categories: cs.CL
Comments: Accepted at ICLR 2024 (poster)
\\ ( https://arxiv.org/abs/2402.07577 ,  407kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09282
replaced with revised version Sat, 9 Mar 2024 16:45:40 GMT   (454kb,D)

Title: Distilling Large Language Models into Tiny Models for Named Entity
  Recognition
Authors: Yining Huang
Categories: cs.CL
Comments: 16 pages, 3 figures
\\ ( https://arxiv.org/abs/2402.09282 ,  454kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11941
replaced with revised version Sat, 9 Mar 2024 12:15:34 GMT   (16640kb,D)

Title: Comprehensive Cognitive LLM Agent for Smartphone GUI Automation
Authors: Xinbei Ma, Zhuosheng Zhang, Hai Zhao
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.11941 ,  16640kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12749
replaced with revised version Mon, 11 Mar 2024 04:04:59 GMT   (860kb)

Title: Me LLaMA: Foundation Large Language Models for Medical Applications
Authors: Qianqian Xie, Qingyu Chen, Aokun Chen, Cheng Peng, Yan Hu, Fongci Lin,
  Xueqing Peng, Jimin Huang, Jeffrey Zhang, Vipina Keloth, Xingyu Zhou, Huan
  He, Lucila Ohno-Machido, Yonghui Wu, Hua Xu, Jiang Bian
Categories: cs.CL cs.AI
Comments: 21 pages, 3 figures, 8 tables
\\ ( https://arxiv.org/abs/2402.12749 ,  860kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13350
replaced with revised version Sun, 10 Mar 2024 16:13:37 GMT   (142kb,D)

Title: PIRB: A Comprehensive Benchmark of Polish Dense and Hybrid Text
  Retrieval Methods
Authors: S{\l}awomir Dadas, Micha{\l} Pere{\l}kiewicz, Rafa{\l} Po\'swiata
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.13350 ,  142kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14702
replaced with revised version Sat, 9 Mar 2024 19:13:54 GMT   (265kb,D)

Title: InfFeed: Influence Functions as a Feedback to Improve the Performance of
  Subjective Tasks
Authors: Somnath Banerjee, Maulindu Sarkar, Punyajoy Saha, Binny Mathew,
  Animesh Mukherjee
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024 (Long Paper)
\\ ( https://arxiv.org/abs/2402.14702 ,  265kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16034
replaced with revised version Sun, 10 Mar 2024 15:58:56 GMT   (665kb,D)

Title: Emotion Classification in Short English Texts using Deep Learning
  Techniques
Authors: Siddhanth Bhat
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.16034 ,  665kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16159
replaced with revised version Mon, 11 Mar 2024 08:11:08 GMT   (2142kb,D)

Title: DistALANER: Distantly Supervised Active Learning Augmented Named Entity
  Recognition in the Open Source Software Ecosystem
Authors: Somnath Banerjee, Avik Dutta, Aaditya Agrawal, Rima Hazra, Animesh
  Mukherjee
Categories: cs.CL
Comments: Under review
\\ ( https://arxiv.org/abs/2402.16159 ,  2142kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16363
replaced with revised version Mon, 11 Mar 2024 17:46:49 GMT   (1639kb,D)

Title: LLM Inference Unveiled: Survey and Roofline Model Insights
Authors: Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao
  Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, Beidi Chen,
  Guangyu Sun, Kurt Keutzer
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.16363 ,  1639kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16827
replaced with revised version Fri, 8 Mar 2024 20:04:01 GMT   (1764kb,D)

Title: A Survey on Data Selection for Language Models
Authors: Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan
  Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon
  Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, William Yang Wang
Categories: cs.CL cs.LG
Comments: Paper list available at
  https://github.com/alon-albalak/data-selection-survey
\\ ( https://arxiv.org/abs/2402.16827 ,  1764kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19088
replaced with revised version Mon, 11 Mar 2024 15:21:57 GMT   (398kb,D)

Title: Survey in Characterization of Semantic Change
Authors: Jader Martins Camboim de S\'a, Marcos Da Silveira, C\'edric Pruski
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.19088 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19467
replaced with revised version Mon, 11 Mar 2024 01:31:58 GMT   (10967kb,D)

Title: TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning
Authors: Kate Sanders, Nathaniel Weir, Benjamin Van Durme
Categories: cs.CL cs.AI cs.CV
Comments: 9 pages, preprint
ACM-class: I.2.7; I.2.10
\\ ( https://arxiv.org/abs/2402.19467 ,  10967kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03102
replaced with revised version Mon, 11 Mar 2024 11:25:48 GMT   (11338kb,D)

Title: "In Dialogues We Learn": Towards Personalized Dialogue Without
  Pre-defined Profiles through In-Dialogue Learning
Authors: Chuanqi Cheng, Quan Tu, Wei Wu, Shuo Shang, Cunli Mao, Zhengtao Yu,
  Rui Yan
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.03102 ,  11338kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03640
replaced with revised version Sat, 9 Mar 2024 13:02:11 GMT   (14155kb,D)

Title: Apollo: An Lightweight Multilingual Medical LLM towards Democratizing
  Medical AI to 6B People
Authors: Xidong Wang, Nuo Chen, Junyin Chen, Yan Hu, Yidong Wang, Xiangbo Wu,
  Anningzhe Gao, Xiang Wan, Haizhou Li, Benyou Wang
Categories: cs.CL cs.AI
Comments: Preprint
\\ ( https://arxiv.org/abs/2403.03640 ,  14155kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04224
replaced with revised version Mon, 11 Mar 2024 07:04:42 GMT   (1034kb,D)

Title: Aligners: Decoupling LLMs and Alignment
Authors: Lilian Ngweta, Mayank Agarwal, Subha Maity, Alex Gittens, Yuekai Sun,
  Mikhail Yurochkin
Categories: cs.CL cs.AI cs.LG
Comments: Tiny Papers Track at the International Conference on Learning
  Representations (ICLR) 2024
\\ ( https://arxiv.org/abs/2403.04224 ,  1034kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04656
replaced with revised version Sat, 9 Mar 2024 15:37:36 GMT   (9684kb,D)

Title: Chain of Thought Explanation for Dialogue State Tracking
Authors: Lin Xu, Ningxin Peng, Daquan Zhou, See-Kiong Ng, Jinlan Fu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.04656 ,  9684kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04789
replaced with revised version Mon, 11 Mar 2024 01:04:28 GMT   (1730kb,D)

Title: TopicDiff: A Topic-enriched Diffusion Approach for Multimodal
  Conversational Emotion Detection
Authors: Jiamin Luo, Jingjing Wang, Guodong Zhou
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.04789 ,  1730kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05101
replaced with revised version Mon, 11 Mar 2024 04:59:51 GMT   (471kb,D)

Title: Rule-driven News Captioning
Authors: Ning Xu, Tingting Zhang, Hongshuo Tian, An-An Liu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.05101 ,  471kb)
------------------------------------------------------------------------------
\\
arXiv:1511.03086
replaced with revised version Mon, 11 Mar 2024 17:09:24 GMT   (11kb)

Title: The CTU Prague Relational Learning Repository
Authors: Jan Motl and Oliver Schulte
Categories: cs.LG cs.DB
Comments: 7 pages
ACM-class: I.2.6; H.2.8
\\ ( https://arxiv.org/abs/1511.03086 ,  11kb)
------------------------------------------------------------------------------
\\
arXiv:2007.07876
replaced with revised version Sat, 9 Mar 2024 20:11:36 GMT   (34kb)

Title: Upper Counterfactual Confidence Bounds: a New Optimism Principle for
  Contextual Bandits
Authors: Yunbei Xu and Assaf Zeevi
Categories: cs.LG math.ST stat.ML stat.TH
\\ ( https://arxiv.org/abs/2007.07876 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2109.03396
replaced with revised version Mon, 11 Mar 2024 10:06:17 GMT   (31kb)

Title: A Bayesian Learning Algorithm for Unknown Zero-sum Stochastic Games with
  an Arbitrary Opponent
Authors: Mehdi Jafarnia-Jahromi, Rahul Jain, Ashutosh Nayyar
Categories: cs.LG cs.GT
\\ ( https://arxiv.org/abs/2109.03396 ,  31kb)
------------------------------------------------------------------------------
\\
arXiv:2109.07582
replaced with revised version Sat, 9 Mar 2024 00:01:32 GMT   (25797kb,D)

Title: Pareto-wise Ranking Classifier for Multi-objective Evolutionary Neural
  Architecture Search
Authors: Lianbo Ma, Nan Li, Guo Yu, Xiaoyu Geng, Min Huang and Xingwei Wang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2109.07582 ,  25797kb)
------------------------------------------------------------------------------
\\
arXiv:2111.10657
replaced with revised version Sun, 10 Mar 2024 07:52:04 GMT   (5068kb,D)

Title: Generalizing Graph Neural Networks on Out-Of-Distribution Graphs
Authors: Shaohua Fan, Xiao Wang, Chuan Shi, Peng Cui and Bai Wang
Categories: cs.LG cs.AI
Comments: IEEE TPAMI 2023
\\ ( https://arxiv.org/abs/2111.10657 ,  5068kb)
------------------------------------------------------------------------------
\\
arXiv:2112.05321
replaced with revised version Sat, 9 Mar 2024 02:42:38 GMT   (912kb,D)

Title: PMFL: Partial Meta-Federated Learning for heterogeneous tasks and its
  applications on real-world medical records
Authors: Tianyi Zhang, Shirui Zhang, Ziwei Chen, Dianbo Liu
Categories: cs.LG
Comments: 11 pages, 7 figures
\\ ( https://arxiv.org/abs/2112.05321 ,  912kb)
------------------------------------------------------------------------------
\\
arXiv:2205.09056
replaced with revised version Sun, 10 Mar 2024 02:52:58 GMT   (55kb)

Title: Slowly Changing Adversarial Bandit Algorithms are Efficient for
  Discounted MDPs
Authors: Ian A. Kash, Lev Reyzin and Zishun Yu
Categories: cs.LG
Comments: ALT 24
\\ ( https://arxiv.org/abs/2205.09056 ,  55kb)
------------------------------------------------------------------------------
\\
arXiv:2206.09166
replaced with revised version Sat, 9 Mar 2024 18:32:48 GMT   (2820kb,D)

Title: NAS-Bench-Graph: Benchmarking Graph Neural Architecture Search
Authors: Yijian Qin, Ziwei Zhang, Xin Wang, Zeyang Zhang, Wenwu Zhu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2206.09166 ,  2820kb)
------------------------------------------------------------------------------
\\
arXiv:2206.11104
replaced with revised version Mon, 11 Mar 2024 17:31:58 GMT   (1567kb,D)

Title: OpenXAI: Towards a Transparent Evaluation of Model Explanations
Authors: Chirag Agarwal, Dan Ley, Eshika Saxena, Satyapriya Krishna, Martin
  Pawelczyk, Nari Johnson, Isha Puri, Marinka Zitnik, and Himabindu Lakkaraju
Categories: cs.LG cs.AI
Comments: Newer version with updated results and code
\\ ( https://arxiv.org/abs/2206.11104 ,  1567kb)
------------------------------------------------------------------------------
\\
arXiv:2206.11673
replaced with revised version Sun, 10 Mar 2024 11:56:19 GMT   (1830kb,D)

Title: Is your model predicting the past?
Authors: Moritz Hardt and Michael P. Kim
Categories: cs.LG stat.ML
Comments: Code available at:
  https://github.com/socialfoundations/backward_baselines
DOI: 10.1145/3617694.3623225
\\ ( https://arxiv.org/abs/2206.11673 ,  1830kb)
------------------------------------------------------------------------------
\\
arXiv:2209.00372
replaced with revised version Sat, 9 Mar 2024 03:16:12 GMT   (9110kb,D)

Title: Sample Efficient Learning of Factored Embeddings of Tensor Fields
Authors: Taemin Heo, Chandrajit Bajaj
Categories: cs.LG cs.NA math.NA
\\ ( https://arxiv.org/abs/2209.00372 ,  9110kb)
------------------------------------------------------------------------------
\\
arXiv:2210.01834
replaced with revised version Fri, 8 Mar 2024 20:32:07 GMT   (297kb,D)

Title: Invariant Aggregator for Defending against Federated Backdoor Attacks
Authors: Xiaoyang Wang, Dimitrios Dimitriadis, Sanmi Koyejo, Shruti Tople
Categories: cs.LG cs.CR
Comments: AISTATS 2024 camera-ready
\\ ( https://arxiv.org/abs/2210.01834 ,  297kb)
------------------------------------------------------------------------------
\\
arXiv:2211.12020
replaced with revised version Mon, 11 Mar 2024 15:50:55 GMT   (603kb,D)

Title: PhAST: Physics-Aware, Scalable, and Task-specific GNNs for Accelerated
  Catalyst Design
Authors: Alexandre Duval, Victor Schmidt, Santiago Miret, Yoshua Bengio, Alex
  Hern\'andez-Garc\'ia, David Rolnick
Categories: cs.LG physics.comp-ph
Comments: Journal of Machine Learning Research (JMLR)
\\ ( https://arxiv.org/abs/2211.12020 ,  603kb)
------------------------------------------------------------------------------
\\
arXiv:2301.01333
replaced with revised version Mon, 11 Mar 2024 05:10:17 GMT   (1630kb)

Title: oneDNN Graph Compiler: A Hybrid Approach for High-Performance Deep
  Learning Compilation
Authors: Jianhui Li, Zhennan Qin, Yijie Mei, Jingze Cui, Yunfei Song, Ciyong
  Chen, Yifei Zhang, Longsheng Du, Xianhang Cheng, Baihui Jin, Yan Zhang, Jason
  Ye, Eric Lin, Dan Lavery
Categories: cs.LG cs.PF
Comments: 10 pages excluding reference, 9 figures, 1 table
\\ ( https://arxiv.org/abs/2301.01333 ,  1630kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13629
replaced with revised version Sun, 10 Mar 2024 01:31:24 GMT   (4903kb,D)

Title: DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting with Denoising
  Diffusion Models
Authors: Haomin Wen, Youfang Lin, Yutong Xia, Huaiyu Wan, Qingsong Wen, Roger
  Zimmermann, Yuxuan Liang
Categories: cs.LG
Comments: Accepted to the 31st ACM SIGSPATIAL International Conference on
  Advances in Geographic Information Systems
\\ ( https://arxiv.org/abs/2301.13629 ,  4903kb)
------------------------------------------------------------------------------
\\
arXiv:2302.00482
replaced with revised version Mon, 11 Mar 2024 14:27:48 GMT   (9711kb,D)

Title: Improving and generalizing flow-based generative models with minibatch
  optimal transport
Authors: Alexander Tong, Kilian Fatras, Nikolay Malkin, Guillaume Huguet,
  Yanlei Zhang, Jarrid Rector-Brooks, Guy Wolf, Yoshua Bengio
Categories: cs.LG
Comments: TMLR. Code: https://github.com/atong01/conditional-flow-matching
\\ ( https://arxiv.org/abs/2302.00482 ,  9711kb)
------------------------------------------------------------------------------
\\
arXiv:2302.05451
replaced with revised version Sat, 9 Mar 2024 19:14:57 GMT   (34238kb,D)

Title: Brain Effective Connectome based on fMRI and DTI Data: Bayesian Causal
  Learning and Assessment
Authors: Abdolmahdi Bagheri, Mahdi Dehshiri, Yamin Bagheri, Alireza
  Akhondi-Asl, Babak Nadjar Araabi
Categories: cs.LG cs.CE
\\ ( https://arxiv.org/abs/2302.05451 ,  34238kb)
------------------------------------------------------------------------------
\\
arXiv:2304.00155
replaced with revised version Mon, 11 Mar 2024 00:38:26 GMT   (218kb,D)

Title: Online Reinforcement Learning in Markov Decision Process Using Linear
  Programming
Authors: Vincent Leon, S. Rasoul Etesami
Categories: cs.LG cs.SY eess.SY math.OC
Journal-ref: 2023 62nd IEEE Conference on Decision and Control (CDC)
DOI: 10.1109/CDC49753.2023.10383839
\\ ( https://arxiv.org/abs/2304.00155 ,  218kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15572
replaced with revised version Fri, 8 Mar 2024 21:39:18 GMT   (460kb,D)

Title: The Behavior and Convergence of Local Bayesian Optimization
Authors: Kaiwen Wu, Kyurae Kim, Roman Garnett and Jacob R. Gardner
Categories: cs.LG stat.ML
Comments: 27 pages; NeurIPS 2023
\\ ( https://arxiv.org/abs/2305.15572 ,  460kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15901
replaced with revised version Sat, 9 Mar 2024 16:01:39 GMT   (3507kb,D)

Title: Consistent Optimal Transport with Empirical Conditional Measures
Authors: Piyushi Manupriya, Rachit Keerti Das, Sayantan Biswas, Saketha Nath
  Jagarlapudi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2305.15901 ,  3507kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16363
replaced with revised version Mon, 11 Mar 2024 14:18:30 GMT   (788kb,D)

Title: Subpopulation-Specific Synthetic EHR for Better Mortality Prediction
Authors: Oriel Perets, Nadav Rappoport
Categories: cs.LG
Comments: 10 pages, 4 figures, submitted to AIME 2024
\\ ( https://arxiv.org/abs/2305.16363 ,  788kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17126
replaced with revised version Mon, 11 Mar 2024 01:15:09 GMT   (1093kb,D)

Title: Large Language Models as Tool Makers
Authors: Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, Denny Zhou
Categories: cs.LG cs.AI cs.CL stat.ML
Comments: Code available at https://github.com/ctlllll/LLM-ToolMaker
\\ ( https://arxiv.org/abs/2305.17126 ,  1093kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18511
replaced with revised version Fri, 8 Mar 2024 22:16:17 GMT   (3874kb,D)

Title: Contextual Bandits with Budgeted Information Reveal
Authors: Kyra Gan, Esmaeil Keyvanshokooh, Xueqing Liu, Susan Murphy
Categories: cs.LG math.OC
Comments: International Conference on Artificial Intelligence and Statistics,
  2024
\\ ( https://arxiv.org/abs/2305.18511 ,  3874kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18806
replaced with revised version Sat, 9 Mar 2024 09:28:20 GMT   (552kb,D)

Title: Prediction Error-based Classification for Class-Incremental Learning
Authors: Micha{\l} Zaj\k{a}c, Tinne Tuytelaars, Gido M. van de Ven
Categories: cs.LG cs.AI cs.CV stat.ML
Comments: ICLR 2024 camera ready
\\ ( https://arxiv.org/abs/2305.18806 ,  552kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19101
replaced with revised version Mon, 11 Mar 2024 12:48:37 GMT   (19574kb,D)

Title: Which Models have Perceptually-Aligned Gradients? An Explanation via
  Off-Manifold Robustness
Authors: Suraj Srinivas, Sebastian Bordt, Hima Lakkaraju
Categories: cs.LG cs.CV
Comments: NeurIPS 2023
\\ ( https://arxiv.org/abs/2305.19101 ,  19574kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00010
replaced with revised version Mon, 11 Mar 2024 09:10:15 GMT   (1601kb,D)

Title: Trainable and Explainable Simplicial Map Neural Networks
Authors: Eduardo Paluzo-Hidalgo, Miguel A. Guti\'errez-Naranjo, Rocio
  Gonzalez-Diaz
Categories: cs.LG cs.AI math.AT
\\ ( https://arxiv.org/abs/2306.00010 ,  1601kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02090
replaced with revised version Mon, 11 Mar 2024 14:48:34 GMT   (1092kb,D)

Title: Deep Classifier Mimicry without Data Access
Authors: Steven Braun, Martin Mundt, Kristian Kersting
Categories: cs.LG cs.AI
Comments: 11 pages main, 4 figures, 2 tables, 4 pages appendix
\\ ( https://arxiv.org/abs/2306.02090 ,  1092kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02879
replaced with revised version Mon, 11 Mar 2024 08:56:58 GMT   (5814kb,D)

Title: Neuron Activation Coverage: Rethinking Out-of-distribution Detection and
  Generalization
Authors: Yibing Liu, Chris Xing Tian, Haoliang Li, Lei Ma, Shiqi Wang
Categories: cs.LG
Comments: ICLR 2024 Spotlight
\\ ( https://arxiv.org/abs/2306.02879 ,  5814kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04107
replaced with revised version Fri, 8 Mar 2024 22:40:45 GMT   (2694kb,D)

Title: BeMap: Balanced Message Passing for Fair Graph Neural Network
Authors: Xiao Lin, Jian Kang, Weilin Cong, Hanghang Tong
Categories: cs.LG cs.AI cs.SI
Comments: Accepted at the Second Learning on Graphs Conference (LoG 2023)
\\ ( https://arxiv.org/abs/2306.04107 ,  2694kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06098
replaced with revised version Sat, 9 Mar 2024 14:54:41 GMT   (108kb,D)

Title: Error Feedback Can Accurately Compress Preconditioners
Authors: Ionut-Vlad Modoranu, Aleksei Kalinov, Eldar Kurtic, Elias Frantar, Dan
  Alistarh
Categories: cs.LG cs.NA math.NA math.OC
\\ ( https://arxiv.org/abs/2306.06098 ,  108kb)
------------------------------------------------------------------------------
\\
arXiv:2306.07473
replaced with revised version Fri, 8 Mar 2024 19:30:22 GMT   (10059kb,D)

Title: 3D molecule generation by denoising voxel grids
Authors: Pedro O. Pinheiro, Joshua Rackers, Joseph Kleinhenz, Michael Maser,
  Omar Mahmood, Andrew Martin Watkins, Stephen Ra, Vishnu Sresht, Saeed Saremi
Categories: cs.LG q-bio.QM
\\ ( https://arxiv.org/abs/2306.07473 ,  10059kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14348
replaced with revised version Sat, 9 Mar 2024 23:37:40 GMT   (5257kb,D)

Title: Collaborative and Distributed Bayesian Optimization via Consensus:
  Showcasing the Power of Collaboration for Optimal Design
Authors: Xubo Yue, Raed Al Kontar, Albert S. Berahas, Yang Liu, Blake N.
  Johnson
Categories: cs.LG cs.DC
Comments: 41 pages
\\ ( https://arxiv.org/abs/2306.14348 ,  5257kb)
------------------------------------------------------------------------------
\\
arXiv:2306.15155
replaced with revised version Sat, 9 Mar 2024 00:00:36 GMT   (289kb,D)

Title: SENSEi: Input-Sensitive Compilation for Accelerating GNNs
Authors: Damitha Lenadora, Vimarsh Sathia, Gerasimos Gerogiannis, Serif Yesil,
  Josep Torrellas, Charith Mendis
Categories: cs.LG cs.PF
\\ ( https://arxiv.org/abs/2306.15155 ,  289kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16614
replaced with revised version Sun, 10 Mar 2024 05:02:04 GMT   (4808kb,D)

Title: Group-based Robustness: A General Framework for Customized Robustness in
  the Real World
Authors: Weiran Lin and Keane Lucas and Neo Eyal and Lujo Bauer and Michael K.
  Reiter and Mahmood Sharif
Categories: cs.LG cs.AI cs.CR cs.CV
\\ ( https://arxiv.org/abs/2306.16614 ,  4808kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00543
replaced with revised version Sun, 10 Mar 2024 15:08:38 GMT   (11075kb,D)

Title: Defending Against Malicious Behaviors in Federated Learning with
  Blockchain
Authors: Nanqing Dong, Zhipeng Wang, Jiahao Sun, Michael Kampffmeyer, William
  Knottenbelt, Eric Xing
Categories: cs.LG cs.AI cs.CR cs.GT
\\ ( https://arxiv.org/abs/2307.00543 ,  11075kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03672
replaced with revised version Mon, 11 Mar 2024 14:42:58 GMT   (12428kb,D)

Title: Simulation-free Schr\"odinger bridges via score and flow matching
Authors: Alexander Tong, Nikolay Malkin, Kilian Fatras, Lazar Atanackovic,
  Yanlei Zhang, Guillaume Huguet, Guy Wolf, Yoshua Bengio
Categories: cs.LG
Comments: AISTATS 2024. Code:
  https://github.com/atong01/conditional-flow-matching
\\ ( https://arxiv.org/abs/2307.03672 ,  12428kb)
------------------------------------------------------------------------------
\\
arXiv:2307.05358
replaced with revised version Mon, 11 Mar 2024 15:48:08 GMT   (365kb,D)

Title: Combating Data Imbalances in Federated Semi-supervised Learning with
  Dual Regulators
Authors: Sikai Bai, Shuaicheng Li, Weiming Zhuang, Jie Zhang, Song Guo, Kunlin
  Yang, Jun Hou, Shuai Zhang, Junyu Gao, Shuai Yi
Categories: cs.LG cs.AI
Journal-ref: The 38th Annual AAAI Conference on Artificial Intelligence, 2024
\\ ( https://arxiv.org/abs/2307.05358 ,  365kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06148
replaced with revised version Sat, 9 Mar 2024 04:24:21 GMT   (4304kb,D)

Title: NetGPT: A Native-AI Network Architecture Beyond Provisioning
  Personalized Generative Services
Authors: Yuxuan Chen, Rongpeng Li, Zhifeng Zhao, Chenghui Peng, Jianjun Wu,
  Ekram Hossain, and Honggang Zhang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2307.06148 ,  4304kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09423
replaced with revised version Sun, 10 Mar 2024 14:50:52 GMT   (694kb,D)

Title: Scaling Laws for Imitation Learning in Single-Agent Games
Authors: Jens Tuyls, Dhruv Madeka, Kari Torkkola, Dean Foster, Karthik
  Narasimhan, Sham Kakade
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2307.09423 ,  694kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09476
replaced with revised version Mon, 11 Mar 2024 04:18:10 GMT   (490kb,D)

Title: Overthinking the Truth: Understanding how Language Models Process False
  Demonstrations
Authors: Danny Halawi, Jean-Stanislas Denain, Jacob Steinhardt
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2307.09476 ,  490kb)
------------------------------------------------------------------------------
\\
arXiv:2307.11888
replaced with revised version Mon, 11 Mar 2024 17:30:54 GMT   (4158kb,D)

Title: Universality of Linear Recurrences Followed by Non-linear Projections:
  Finite-Width Guarantees and Benefits of Complex Eigenvalues
Authors: Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, Samuel L.
  Smith
Categories: cs.LG cs.NE
Comments: v1: Accepted at HLD 2023: 1st Workshop on High-dimensional Learning
  Dynamics v2: Preprint
\\ ( https://arxiv.org/abs/2307.11888 ,  4158kb)
------------------------------------------------------------------------------
\\
arXiv:2307.11949
replaced with revised version Sun, 10 Mar 2024 04:26:48 GMT   (7780kb,D)

Title: HIQL: Offline Goal-Conditioned RL with Latent States as Actions
Authors: Seohong Park, Dibya Ghosh, Benjamin Eysenbach, Sergey Levine
Categories: cs.LG cs.AI cs.RO
Comments: NeurIPS 2023
\\ ( https://arxiv.org/abs/2307.11949 ,  7780kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13709
replaced with revised version Mon, 11 Mar 2024 10:45:41 GMT   (215kb)

Title: Neural Bradley-Terry Rating: Quantifying Properties from Comparisons
Authors: Satoru Fujii
Categories: cs.LG cs.AI
MSC-class: 68T99
Journal-ref: In Proceedings of the 16th International Conference on Agents and
  Artificial Intelligence - Volume 3: ICAART, 422-429, 2024 , Rome, Italy
DOI: 10.5220/0012355900003636
\\ ( https://arxiv.org/abs/2307.13709 ,  215kb)
------------------------------------------------------------------------------
\\
arXiv:2307.14025
replaced with revised version Mon, 11 Mar 2024 11:14:15 GMT   (4446kb,D)

Title: Topologically Regularized Multiple Instance Learning to Harness Data
  Scarcity
Authors: Salome Kazeminia, Carsten Marr, Bastian Rieck
Categories: cs.LG cs.CV eess.IV q-bio.QM stat.ML
\\ ( https://arxiv.org/abs/2307.14025 ,  4446kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14412
replaced with revised version Mon, 11 Mar 2024 11:19:32 GMT   (2373kb,D)

Title: Task-Aware Machine Unlearning and Its Application in Load Forecasting
Authors: Wangkun Xu, Fei Teng
Categories: cs.LG cs.SY eess.SY
Comments: This paper has been accepted by IEEE trans on Power Systems. The
  copyright is transfered and preserved by IEEE
\\ ( https://arxiv.org/abs/2308.14412 ,  2373kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14675
replaced with revised version Mon, 11 Mar 2024 16:28:15 GMT   (21602kb,D)

Title: FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous
  Client Devices using a Computing Power Aware Scheduler
Authors: Zilinghan Li, Pranshu Chaturvedi, Shilan He, Han Chen, Gagandeep
  Singh, Volodymyr Kindratenko, E. A. Huerta, Kibaek Kim, Ravi Madduri
Categories: cs.LG cs.DC
Comments: Accepted as poster at The Twelfth International Conference on
  Learning Representations (ICLR 2024)
\\ ( https://arxiv.org/abs/2309.14675 ,  21602kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15257
replaced with revised version Mon, 11 Mar 2024 16:29:17 GMT   (450kb,D)

Title: STARC: A General Framework For Quantifying Differences Between Reward
  Functions
Authors: Joar Skalse, Lucy Farnik, Sumeet Ramesh Motwani, Erik Jenner, Adam
  Gleave, Alessandro Abate
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2309.15257 ,  450kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16119
replaced with revised version Sun, 10 Mar 2024 03:24:06 GMT   (7136kb)

Title: ModuLoRA: Finetuning 2-Bit LLMs on Consumer GPUs by Integrating with
  Modular Quantizers
Authors: Junjie Yin, Jiahao Dong, Yingheng Wang, Christopher De Sa, Volodymyr
  Kuleshov
Categories: cs.LG cs.AI
Comments: Update since being accepted to TMLR. Updated 2Bit results
\\ ( https://arxiv.org/abs/2309.16119 ,  7136kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17002
replaced with revised version Mon, 11 Mar 2024 15:59:28 GMT   (6203kb,D)

Title: Understanding and Mitigating the Label Noise in Pre-training on
  Downstream Tasks
Authors: Hao Chen, Jindong Wang, Ankit Shah, Ran Tao, Hongxin Wei, Xing Xie,
  Masashi Sugiyama, Bhiksha Raj
Categories: cs.LG cs.AI cs.CV
Comments: ICLR 2024 Spotlight
\\ ( https://arxiv.org/abs/2309.17002 ,  6203kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01706
replaced with revised version Mon, 11 Mar 2024 03:31:52 GMT   (6593kb,D)

Title: On Representation Complexity of Model-based and Model-free Reinforcement
  Learning
Authors: Hanlin Zhu, Baihe Huang, Stuart Russell
Categories: cs.LG
Comments: 23 pages, 9 figures, to be published in ICLR 2024
\\ ( https://arxiv.org/abs/2310.01706 ,  6593kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01714
replaced with revised version Sat, 9 Mar 2024 05:54:39 GMT   (303kb,D)

Title: Large Language Models as Analogical Reasoners
Authors: Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure
  Leskovec, Percy Liang, Ed H. Chi, Denny Zhou
Categories: cs.LG
Comments: Published at ICLR 2024
\\ ( https://arxiv.org/abs/2310.01714 ,  303kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02679
replaced with revised version Sat, 9 Mar 2024 21:05:43 GMT   (10304kb,D)

Title: Diffusion Generative Flow Samplers: Improving learning signals through
  partial trajectory optimization
Authors: Dinghuai Zhang, Ricky T. Q. Chen, Cheng-Hao Liu, Aaron Courville,
  Yoshua Bengio
Categories: cs.LG cs.AI stat.CO stat.ME stat.ML
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2310.02679 ,  10304kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02743
replaced with revised version Sun, 10 Mar 2024 16:14:58 GMT   (7997kb,D)

Title: Reward Model Ensembles Help Mitigate Overoptimization
Authors: Thomas Coste, Usman Anwar, Robert Kirk, David Krueger
Categories: cs.LG
Comments: Accepted at ICLR 2024
\\ ( https://arxiv.org/abs/2310.02743 ,  7997kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02807
replaced with revised version Mon, 11 Mar 2024 10:51:14 GMT   (1069kb,D)

Title: A Deep Instance Generative Framework for MILP Solvers Under Limited Data
  Availability
Authors: Zijie Geng, Xijun Li, Jie Wang, Xiao Li, Yongdong Zhang, Feng Wu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.02807 ,  1069kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04742
replaced with revised version Mon, 11 Mar 2024 10:07:08 GMT   (396kb,D)

Title: Parameter Efficient Multi-task Model Fusion with Partial Linearization
Authors: Anke Tang, Li Shen, Yong Luo, Yibing Zhan, Han Hu, Bo Du, Yixin Chen,
  Dacheng Tao
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.04742 ,  396kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05179
replaced with revised version Mon, 11 Mar 2024 15:36:19 GMT   (475kb,D)

Title: Distributional Reinforcement Learning with Online Risk-awareness
  Adaption
Authors: Yupeng Wu, Wenjie Huang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.05179 ,  475kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06282
replaced with revised version Sat, 9 Mar 2024 18:28:34 GMT   (39503kb,D)

Title: MuseChat: A Conversational Music Recommendation System for Videos
Authors: Zhikang Dong, Bin Chen, Xiulong Liu, Pawel Polak, Peng Zhang
Categories: cs.LG cs.CV cs.IR
\\ ( https://arxiv.org/abs/2310.06282 ,  39503kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06625
replaced with revised version Sat, 9 Mar 2024 13:23:57 GMT   (5170kb,D)

Title: iTransformer: Inverted Transformers Are Effective for Time Series
  Forecasting
Authors: Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma,
  Mingsheng Long
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.06625 ,  5170kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07365
replaced with revised version Mon, 11 Mar 2024 07:33:51 GMT   (1145kb)

Title: GraphControl: Adding Conditional Control to Universal Graph Pre-trained
  Models for Graph Domain Transfer Learning
Authors: Yun Zhu, Yaoke Wang, Haizhou Shi, Zhenshuo Zhang, Dian Jiao, Siliang
  Tang
Categories: cs.LG
Comments: Accepted by The Web Conference 2024 (WWW 2024)
Journal-ref: The Web Conference 2024
DOI: 10.1145/3589334.3645439
\\ ( https://arxiv.org/abs/2310.07365 ,  1145kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08887
replaced with revised version Sun, 10 Mar 2024 04:30:17 GMT   (4975kb,D)

Title: METRA: Scalable Unsupervised RL with Metric-Aware Abstraction
Authors: Seohong Park, Oleh Rybkin, Sergey Levine
Categories: cs.LG cs.AI cs.RO
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.08887 ,  4975kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09203
replaced with revised version Fri, 8 Mar 2024 19:11:41 GMT   (23724kb,D)

Title: SiamAF: Learning Shared Information from ECG and PPG Signals for Robust
  Atrial Fibrillation Detection
Authors: Zhicheng Guo, Cheng Ding, Duc H. Do, Amit Shah, Randall J. Lee, Xiao
  Hu, Cynthia Rudin
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.09203 ,  23724kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10207
replaced with revised version Sun, 10 Mar 2024 02:37:36 GMT   (35980kb,D)

Title: Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in
  the Real World
Authors: Rujie Wu, Xiaojian Ma, Zhenliang Zhang, Wei Wang, Qing Li, Song-Chun
  Zhu, Yizhou Wang
Categories: cs.LG
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2310.10207 ,  35980kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12713
replaced with revised version Sun, 10 Mar 2024 16:17:08 GMT   (19122kb,D)

Title: Learn from the Past: A Proxy Guided Adversarial Defense Framework with
  Self Distillation Regularization
Authors: Yaohua Liu, Jiaxin Gao, Xianghao Jiao, Zhu Liu, Xin Fan, Risheng Liu
Categories: cs.LG
Comments: 13 Pages
\\ ( https://arxiv.org/abs/2310.12713 ,  19122kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12955
replaced with revised version Sat, 9 Mar 2024 17:46:44 GMT   (2779kb,D)

Title: Towards Robust Offline Reinforcement Learning under Diverse Data
  Corruption
Authors: Rui Yang, Han Zhong, Jiawei Xu, Amy Zhang, Chongjie Zhang, Lei Han,
  Tong Zhang
Categories: cs.LG cs.AI
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2310.12955 ,  2779kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17137
replaced with revised version Fri, 8 Mar 2024 21:25:44 GMT   (3174kb,D)

Title: Large-Scale Gaussian Processes via Alternating Projection
Authors: Kaiwen Wu, Jonathan Wenger, Haydn Jones, Geoff Pleiss, Jacob R.
  Gardner
Categories: cs.LG stat.ML
Comments: AISTATS 2024
\\ ( https://arxiv.org/abs/2310.17137 ,  3174kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17159
replaced with revised version Sat, 9 Mar 2024 10:29:52 GMT   (5740kb,D)

Title: MaxEnt Loss: Constrained Maximum Entropy for Calibration under
  Out-of-Distribution Shift
Authors: Dexter Neo, Stefan Winkler, Tsuhan Chen
Categories: cs.LG
Comments: 38th AAAI Conference on Artificial Intelligence, AAAI24 (Oral)
\\ ( https://arxiv.org/abs/2310.17159 ,  5740kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03096
replaced with revised version Mon, 11 Mar 2024 02:47:47 GMT   (317kb,D)

Title: Weight-Sharing Regularization
Authors: Mehran Shakerinava, Motahareh Sohrabi, Siamak Ravanbakhsh, Simon
  Lacoste-Julien
Categories: cs.LG stat.ML
Comments: Our code is available at
  https://github.com/motahareh-sohrabi/weight-sharing-regularization
\\ ( https://arxiv.org/abs/2311.03096 ,  317kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18206
replaced with revised version Mon, 11 Mar 2024 00:38:57 GMT   (4150kb,D)

Title: SCOPE-RL: A Python Library for Offline Reinforcement Learning and
  Off-Policy Evaluation
Authors: Haruka Kiyohara, Ren Kishimoto, Kosuke Kawakami, Ken Kobayashi,
  Kazuhide Nakata, Yuta Saito
Categories: cs.LG cs.AI
Comments: preprint, open-source software:
  https://github.com/hakuhodo-technologies/scope-rl
\\ ( https://arxiv.org/abs/2311.18206 ,  4150kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18207
replaced with revised version Mon, 11 Mar 2024 00:29:24 GMT   (7566kb,D)

Title: Towards Assessing and Benchmarking Risk-Return Tradeoff of Off-Policy
  Evaluation
Authors: Haruka Kiyohara, Ren Kishimoto, Kosuke Kawakami, Ken Kobayashi,
  Kazuhide Nakata, Yuta Saito
Categories: cs.LG cs.AI
Comments: ICLR2024
\\ ( https://arxiv.org/abs/2311.18207 ,  7566kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04111
replaced with revised version Mon, 11 Mar 2024 01:25:39 GMT   (3740kb,D)

Title: Breaking the Entanglement of Homophily and Heterophily in
  Semi-supervised Node Classification
Authors: Henan Sun, Xunkai Li, Zhengyu Wu, Daohan Su, Rong-Hua Li, Guoren Wang
Categories: cs.LG cs.AI cs.SI
Comments: Accepted by ICDE 2024
\\ ( https://arxiv.org/abs/2312.04111 ,  3740kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06091
replaced with revised version Sat, 9 Mar 2024 11:38:55 GMT   (204kb,D)

Title: Learning Unknown Intervention Targets in Structural Causal Models from
  Heterogeneous Data
Authors: Yuqin Yang, Saber Salehkaleybar, Negar Kiyavash
Categories: cs.LG cs.AI cs.IT math.IT stat.ML
Comments: Accepted at 27th International Conference on Artificial Intelligence
  and Statistics (AISTATS 2024)
\\ ( https://arxiv.org/abs/2312.06091 ,  204kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08150
replaced with revised version Mon, 11 Mar 2024 09:43:47 GMT   (2676kb,D)

Title: Active learning with biased non-response to label requests
Authors: Thomas Robinson, Niek Tax, Richard Mudd, and Ido Guy
Categories: cs.LG stat.ME stat.ML
\\ ( https://arxiv.org/abs/2312.08150 ,  2676kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13611
replaced with revised version Mon, 11 Mar 2024 02:27:57 GMT   (1110kb,D)

Title: Topology Learning for Heterogeneous Decentralized Federated Learning
  over Unreliable D2D Networks
Authors: Zheshun Wu, Zenglin Xu, Dun Zeng, Junfan Li, Jie Liu
Categories: cs.LG cs.NI eess.SP
Comments: To appear in IEEE Transactions on Vehicular Technology
\\ ( https://arxiv.org/abs/2312.13611 ,  1110kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14196
replaced with revised version Sun, 10 Mar 2024 20:37:47 GMT   (772kb,D)

Title: Optimizing Heat Alert Issuance with Reinforcement Learning
Authors: Ellen M. Considine, Rachel C. Nethery, Gregory A. Wellenius, Francesca
  Dominici, Mauricio Tec
Categories: cs.LG stat.AP
Comments: Main text has 21 pages with 3 tables and 7 figures
\\ ( https://arxiv.org/abs/2312.14196 ,  772kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14712
replaced with revised version Mon, 11 Mar 2024 10:06:37 GMT   (1296kb,D)

Title: Robustness, Efficiency, or Privacy: Pick Two in Machine Learning
Authors: Youssef Allouah, Rachid Guerraoui, and John Stephan
Categories: cs.LG cs.CR cs.DC
\\ ( https://arxiv.org/abs/2312.14712 ,  1296kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01626
replaced with revised version Fri, 8 Mar 2024 19:57:35 GMT   (0kb,I)

Title: On the Expressive Power of Graph Neural Networks
Authors: Ashwin Nalwade, Kelly Marshall, Axel Eladi, Umang Sharma
Categories: cs.LG cs.AI
Comments: We felt that significantly more work was needed to improve the
  quality before it should be put out in its current state. No replacement is
  available at the moment or in the near future
\\ ( https://arxiv.org/abs/2401.01626 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05653
replaced with revised version Mon, 11 Mar 2024 13:55:01 GMT   (519kb)

Title: Quantifying Marketing Performance at Channel-Partner Level by Using
  Marketing Mix Modeling (MMM) and Shapley Value Regression
Authors: Sean Tang, Sriya Musunuru, Baoshi Zong, Brooks Thornton
Categories: cs.LG
Comments: Corrected typos
\\ ( https://arxiv.org/abs/2401.05653 ,  519kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08819
replaced with revised version Mon, 11 Mar 2024 14:43:52 GMT   (2591kb,D)

Title: Learning from Sparse Offline Datasets via Conservative Density
  Estimation
Authors: Zhepeng Cen, Zuxin Liu, Zitong Wang, Yihang Yao, Henry Lam, Ding Zhao
Categories: cs.LG cs.AI
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2401.08819 ,  2591kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09574
replaced with revised version Mon, 11 Mar 2024 00:50:45 GMT   (8744kb)

Title: Towards Scalable and Robust Model Versioning
Authors: Wenxin Ding, Arjun Nitin Bhagoji, Ben Y. Zhao, Haitao Zheng
Categories: cs.LG cs.CR
Comments: Published in IEEE SaTML 2024
\\ ( https://arxiv.org/abs/2401.09574 ,  8744kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13171
replaced with revised version Mon, 11 Mar 2024 15:25:57 GMT   (7329kb,D)

Title: Compositional Generative Inverse Design
Authors: Tailin Wu, Takashi Maruyama, Long Wei, Tao Zhang, Yilun Du, Gianluca
  Iaccarino, Jure Leskovec
Categories: cs.LG cs.AI cs.CE
Comments: ICLR 2024 spotlight. 30 pages, 17 figures
\\ ( https://arxiv.org/abs/2401.13171 ,  7329kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14151
replaced with revised version Mon, 11 Mar 2024 03:15:58 GMT   (31930kb,D)

Title: True Knowledge Comes from Practice: Aligning LLMs with Embodied
  Environments via Reinforcement Learning
Authors: Weihao Tan, Wentao Zhang, Shanqi Liu, Longtao Zheng, Xinrun Wang, Bo
  An
Categories: cs.LG cs.AI cs.CL
Comments: Accepted by ICLR2024
\\ ( https://arxiv.org/abs/2401.14151 ,  31930kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16422
replaced with revised version Fri, 8 Mar 2024 21:01:08 GMT   (3579kb,D)

Title: Strategic Usage in a Multi-Learner Setting
Authors: Eliot Shekhtman and Sarah Dean
Categories: cs.LG cs.GT
Comments: 18 pages, 9 figures
MSC-class: 91A10
\\ ( https://arxiv.org/abs/2401.16422 ,  3579kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17505
replaced with revised version Sun, 10 Mar 2024 14:33:49 GMT   (1423kb,D)

Title: Arrows of Time for Large Language Models
Authors: Vassilis Papadopoulos, J\'er\'emie Wenger, Cl\'ement Hongler
Categories: cs.LG cs.AI cs.CL
Comments: Updated 1 figure, minor corrections to text. 11 figures, 16 pages
\\ ( https://arxiv.org/abs/2401.17505 ,  1423kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01261
replaced with revised version Sun, 10 Mar 2024 14:51:56 GMT   (10716kb,D)

Title: TEDDY: Trimming Edges with Degree-based Discrimination strategY
Authors: Hyunjin Seo, Jihun Yun, Eunho Yang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.01261 ,  10716kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04676
replaced with revised version Mon, 11 Mar 2024 06:56:54 GMT   (14436kb,D)

Title: Group Distributionally Robust Dataset Distillation with Risk
  Minimization
Authors: Saeed Vahidian, Mingyu Wang, Jianyang Gu, Vyacheslav Kungurtsev, Wei
  Jiang, Yiran Chen
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2402.04676 ,  14436kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04852
replaced with revised version Sun, 10 Mar 2024 01:53:40 GMT   (8368kb,D)

Title: Multi-Patch Prediction: Adapting LLMs for Time Series Representation
  Learning
Authors: Yuxuan Bian, Xuan Ju, Jiangtong Li, Zhijian Xu, Dawei Cheng, Qiang Xu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.04852 ,  8368kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09702
replaced with revised version Sat, 9 Mar 2024 01:01:27 GMT   (11844kb,D)

Title: Sparse and Faithful Explanations Without Sparse Models
Authors: Yiyang Sun, Zhi Chen, Vittorio Orlandi, Tong Wang, Cynthia Rudin
Categories: cs.LG stat.ML
Comments: Accepted in AISTATS 2024
\\ ( https://arxiv.org/abs/2402.09702 ,  11844kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10695
replaced with revised version Mon, 11 Mar 2024 17:08:36 GMT   (629kb,D)

Title: Unlink to Unlearn: Simplifying Edge Unlearning in GNNs
Authors: Jiajun Tan, Fei Sun, Ruichen Qiu, Du Su, Huawei Shen
Categories: cs.LG cs.AI cs.CR
Comments: Accepted by WWW 2024 as a Short Research Paper
\\ ( https://arxiv.org/abs/2402.10695 ,  629kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11078
replaced with revised version Sun, 10 Mar 2024 13:53:45 GMT   (26kb,D)

Title: Model Editing by Pure Fine-Tuning
Authors: Govind Gangadhar, Karl Stratos
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.11078 ,  26kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12263
replaced with revised version Fri, 8 Mar 2024 21:16:13 GMT   (516kb,D)

Title: Towards a tailored mixed-precision sub-8-bit quantization scheme for
  Gated Recurrent Units using Genetic Algorithms
Authors: Riccardo Miccini, Alessandro Cerioli, Cl\'ement Laroche, Tobias
  Piechowiak, Jens Spars{\o}, Luca Pezzarossa
Categories: cs.LG cs.NE eess.SP
Comments: Accepted as a full paper by the tinyML Research Symposium 2024
\\ ( https://arxiv.org/abs/2402.12263 ,  516kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18603
replaced with revised version Sun, 10 Mar 2024 11:17:58 GMT   (924kb,D)

Title: MMSR: Symbolic Regression is a Multimodal Task
Authors: Yanjie Li, Jingyi Liu, Weijun Li, Lina Yu, Min Wu, Wenqiang Li, Meilan
  Hao, Su Wei, Yusong Deng
Categories: cs.LG cs.AI cs.CL
Comments: 12 page
\\ ( https://arxiv.org/abs/2402.18603 ,  924kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18609
replaced with revised version Sat, 9 Mar 2024 03:51:53 GMT   (3903kb,D)

Title: ICE-SEARCH: A Language Model-Driven Feature Selection Approach
Authors: Tianze Yang, Tianyi Yang, Shaoshan Liu, Fuyuan Lvu, Xue Liu
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.18609 ,  3903kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00715
replaced with revised version Sun, 10 Mar 2024 15:12:16 GMT   (43kb)

Title: Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive
  Analysis and Best-of-Both-Worlds
Authors: Shinji Ito, Taira Tsuchiya, Junya Honda
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2403.00715 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00858
replaced with revised version Fri, 8 Mar 2024 23:23:48 GMT   (939kb,D)

Title: Direct Alignment of Draft Model for Speculative Decoding with
  Chat-Fine-Tuned LLMs
Authors: Raghavv Goel, Mukul Gagrani, Wonseok Jeon, Junyoung Park, Mingu Lee,
  Christopher Lott
Categories: cs.LG cs.AI cs.CL
Comments: 8 pages, 3 figures, Published at the ICLR 2024 Workshop on
  Understanding of Foundation Models (ME-FoMo)
\\ ( https://arxiv.org/abs/2403.00858 ,  939kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01845
replaced with revised version Sun, 10 Mar 2024 05:49:03 GMT   (1159kb,D)

Title: NASH: Neural Architecture Search for Hardware-Optimized Machine Learning
  Models
Authors: Mengfei Ji, Yuchun Chang, Baolin Zhang and Zaid Al-Ars
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2403.01845 ,  1159kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04161
replaced with revised version Mon, 11 Mar 2024 05:37:36 GMT   (19319kb,D)

Title: Estimating Neural Network Performance through Sample-Wise Activation
  Patterns
Authors: Yameng Peng, Andy Song, Haytham M. Fayek, Vic Ciesielski, Xiaojun
  Chang
Categories: cs.LG cs.CV cs.NE
Comments: ICLR2024 Spotlight
\\ ( https://arxiv.org/abs/2403.04161 ,  19319kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04493
replaced with revised version Mon, 11 Mar 2024 12:01:43 GMT   (62kb)

Title: What makes an image realistic?
Authors: Lucas Theis
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2403.04493 ,  62kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05385
replaced with revised version Mon, 11 Mar 2024 15:59:08 GMT   (135kb,D)

Title: Switching the Loss Reduces the Cost in Batch Reinforcement Learning
Authors: Alex Ayoub, Kaiwen Wang, Vincent Liu, Samuel Robertson, James
  McInerney, Dawen Liang, Nathan Kallus, and Csaba Szepesv\'ari
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.05385 ,  135kb)
------------------------------------------------------------------------------
\\
arXiv:2104.12294
replaced with revised version Mon, 11 Mar 2024 14:38:54 GMT   (1733kb,D)

Title: Wise-SrNet: A Novel Architecture for Enhancing Image Classification by
  Learning Spatial Resolution of Feature Maps
Authors: Mohammad Rahimzadeh, AmirAli Askari, Soroush Parvin, Elnaz Safi,
  Mohammad Reza Mohammadi
Categories: cs.CV cs.AI
Comments: The code is shared at
  https://github.com/mr7495/image-classification-spatial
DOI: 10.1007/s10044-024-01211-0
\\ ( https://arxiv.org/abs/2104.12294 ,  1733kb)
------------------------------------------------------------------------------
\\
arXiv:2201.09754
replaced with revised version Sun, 10 Mar 2024 08:02:12 GMT   (2045kb,D)

Title: Deep Reinforcement Learning with Spiking Q-learning
Authors: Ding Chen, Peixi Peng, Tiejun Huang, Yonghong Tian
Categories: cs.NE cs.AI cs.LG
Comments: 15 pages, 7 figures
\\ ( https://arxiv.org/abs/2201.09754 ,  2045kb)
------------------------------------------------------------------------------
\\
arXiv:2210.00954
replaced with revised version Sat, 9 Mar 2024 18:43:45 GMT   (390kb,D)

Title: Machine Learning-Powered Course Allocation
Authors: Ermis Soumalias, Behnoosh Zamanlooy, Jakob Weissteiner and Sven Seuken
Categories: cs.GT cs.AI cs.LG
\\ ( https://arxiv.org/abs/2210.00954 ,  390kb)
------------------------------------------------------------------------------
\\
arXiv:2212.14400
replaced with revised version Mon, 11 Mar 2024 16:49:16 GMT   (6365kb,D)

Title: Visual CPG-RL: Learning Central Pattern Generators for Visually-Guided
  Quadruped Locomotion
Authors: Guillaume Bellegarda, Milad Shafiee, Auke Ijspeert
Categories: cs.RO cs.AI cs.LG cs.SY eess.SY
Comments: Accepted for 2024 IEEE International Conference on Robotics and
  Automation (ICRA)
\\ ( https://arxiv.org/abs/2212.14400 ,  6365kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13096
replaced with revised version Sun, 10 Mar 2024 08:01:03 GMT   (232kb,D)

Title: Language-Driven Anchors for Zero-Shot Adversarial Robustness
Authors: Xiao Li and Wei Zhang and Yining Liu and Zhanhao Hu and Bo Zhang and
  Xiaolin Hu
Categories: cs.CV cs.AI cs.LG
Comments: Accepted by CVPR 2024
\\ ( https://arxiv.org/abs/2301.13096 ,  232kb)
------------------------------------------------------------------------------
\\
arXiv:2304.09653
replaced with revised version Mon, 11 Mar 2024 03:06:46 GMT   (5648kb,D)

Title: ReelFramer: Human-AI Co-Creation for News-to-Video Translation
Authors: Sitong Wang, Samia Menon, Tao Long, Keren Henderson, Dingzeyu Li,
  Kevin Crowston, Mark Hansen, Jeffrey V. Nickerson, Lydia B. Chilton
Categories: cs.HC cs.AI
\\ ( https://arxiv.org/abs/2304.09653 ,  5648kb)
------------------------------------------------------------------------------
\\
arXiv:2304.10985
replaced with revised version Mon, 11 Mar 2024 17:14:40 GMT   (5496kb,D)

Title: RSBA: Robust Statistical Backdoor Attack under Privilege-Constrained
  Scenarios
Authors: Xiaolei Liu, Ming Yi, Kangyi Ding, Bangzhou Xin, Yixiao Xu, Li Yan,
  Chao Shen
Categories: cs.CR cs.AI cs.CV
Comments: 11 pages, 10 figures
\\ ( https://arxiv.org/abs/2304.10985 ,  5496kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12553
replaced with revised version Sat, 9 Mar 2024 18:41:01 GMT   (1025kb,D)

Title: Markov $\alpha$-Potential Games
Authors: Xin Guo and Xinyu Li and Chinmay Maheshwari and Shankar Sastry and
  Manxi Wu
Categories: cs.GT cs.AI cs.MA cs.SY eess.SY math.DS
Comments: 32 pages, 3 figures
MSC-class: 91A68, 91A50, 91A15, 91A14, 91A10
\\ ( https://arxiv.org/abs/2305.12553 ,  1025kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13860
replaced with revised version Sun, 10 Mar 2024 13:58:08 GMT   (460kb,D)

Title: Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study
Authors: Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang,
  Lida Zhao, Tianwei Zhang, Kailong Wang and Yang Liu
Categories: cs.SE cs.AI cs.CL
\\ ( https://arxiv.org/abs/2305.13860 ,  460kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18766
replaced with revised version Mon, 11 Mar 2024 06:14:31 GMT   (10783kb,D)

Title: HiFA: High-fidelity Text-to-3D Generation with Advanced Diffusion
  Guidance
Authors: Junzhe Zhu and Peiye Zhuang and Sanmi Koyejo
Categories: cs.CV cs.AI cs.LG
Comments: Project page: https://hifa-team.github.io/HiFA-site/
\\ ( https://arxiv.org/abs/2305.18766 ,  10783kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00209
replaced with revised version Sat, 9 Mar 2024 02:30:11 GMT   (3290kb,D)

Title: Image Matters: A New Dataset and Empirical Study for Multimodal
  Hyperbole Detection
Authors: Huixuan Zhang, Xiaojun Wan
Categories: cs.CV cs.AI cs.CL
Comments: Accepted by LREC-COLING 2024
\\ ( https://arxiv.org/abs/2307.00209 ,  3290kb)
------------------------------------------------------------------------------
\\
arXiv:2308.03166
replaced with revised version Sun, 10 Mar 2024 09:43:34 GMT   (2813kb,D)

Title: Strategic Preys Make Acute Predators: Enhancing Camouflaged Object
  Detectors by Generating Camouflaged Objects
Authors: Chunming He, Kai Li, Yachao Zhang, Yulun Zhang, Zhenhua Guo, Xiu Li,
  Martin Danelljan, Fisher Yu
Categories: cs.CV cs.AI
Comments: Accepted at ICLR 2024
\\ ( https://arxiv.org/abs/2308.03166 ,  2813kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10620
replaced with revised version Sun, 10 Mar 2024 06:28:33 GMT   (1489kb,D)

Title: Large Language Models for Software Engineering: A Systematic Literature
  Review
Authors: Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu
  Luo, David Lo, John Grundy, Haoyu Wang
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2308.10620 ,  1489kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12591 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 10:02:49 GMT   (2132kb,D)

Title: SICNN: Soft Interference Cancellation Inspired Neural Network Equalizers
Authors: Stefan Baumgartner and Oliver Lang and Mario Huemer
Categories: eess.SP cs.AI
\\ ( https://arxiv.org/abs/2308.12591 ,  2132kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13893
replaced with revised version Sat, 9 Mar 2024 00:40:08 GMT   (29130kb,D)

Title: Scene Informer: Anchor-based Occlusion Inference and Trajectory
  Prediction in Partially Observable Environments
Authors: Bernard Lange, Jiachen Li, and Mykel J. Kochenderfer
Categories: cs.RO cs.AI cs.CV
Comments: Accepted to 2024 IEEE International Conference on Robotics and
  Automation (ICRA)
\\ ( https://arxiv.org/abs/2309.13893 ,  29130kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14859
replaced with revised version Mon, 11 Mar 2024 09:39:33 GMT   (87480kb,D)

Title: Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to
  Model Evaluation
Authors: Shih-Ying Yeh, Yu-Guan Hsieh, Zhidong Gao, Bernard B W Yang, Giyeong
  Oh, Yanmin Gong
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: In International Conference on Learning Representations 12 (ICLR
  2024) [79 pages, 54 figures, 7 tables]
\\ ( https://arxiv.org/abs/2309.14859 ,  87480kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00349
replaced with revised version Sat, 9 Mar 2024 13:22:29 GMT   (5073kb)

Title: Visual Political Communication in a Polarized Society: A Longitudinal
  Study of Brazilian Presidential Elections on Instagram
Authors: Mathias-Felipe de-Lima-Santos, Isabella Gon\c{c}alves, Marcos G.
  Quiles, Lucia Mesquita, Wilson Ceron, Maria Clara Couto Lorena
Categories: cs.CY cs.AI cs.CV cs.LG cs.SI
\\ ( https://arxiv.org/abs/2310.00349 ,  5073kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03813
replaced with revised version Sun, 10 Mar 2024 05:13:41 GMT   (714kb,D)

Title: Cold-start Bundle Recommendation via Popularity-based Coalescence and
  Curriculum Heating
Authors: Hyunsik Jeon, Jong-eun Lee, Jeongin Yun, U Kang
Categories: cs.IR cs.AI
Comments: 8 pages, 4 figures, 4 tables
\\ ( https://arxiv.org/abs/2310.03813 ,  714kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03976
replaced with revised version Sun, 10 Mar 2024 01:19:41 GMT   (10785kb)

Title: From Text to Self: Users' Perceptions of Potential of AI on
  Interpersonal Communication and Self
Authors: Yue Fu, Sami Foell, Xuhai Xu, Alexis Hiniker
Categories: cs.HC cs.AI cs.CL
Journal-ref: Proceedings of the CHI Conference on Human Factors in Computing
  Systems (CHI 2024)
DOI: 10.1145/3613904.3641955
\\ ( https://arxiv.org/abs/2310.03976 ,  10785kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04432
replaced with revised version Sun, 10 Mar 2024 22:01:18 GMT   (41250kb,D)

Title: Training-free Linear Image Inverses via Flows
Authors: Ashwini Pokle, Matthew J. Muckley, Ricky T. Q. Chen, Brian Karrer
Categories: cs.CV cs.AI cs.LG
Comments: 40 pages, 30 figures. Added additional qualitative results in the
  appendix
\\ ( https://arxiv.org/abs/2310.04432 ,  41250kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13165
replaced with revised version Sat, 9 Mar 2024 20:58:55 GMT   (11357kb,D)

Title: CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for
  Image Manipulation
Authors: Sihan Xu, Ziqiao Ma, Yidong Huang, Honglak Lee, Joyce Chai
Categories: cs.CV cs.AI cs.LG
Comments: NeurIPS 2023
\\ ( https://arxiv.org/abs/2310.13165 ,  11357kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04193
replaced with revised version Sun, 10 Mar 2024 01:55:47 GMT   (41328kb,D)

Title: Selective Visual Representations Improve Convergence and Generalization
  for Embodied AI
Authors: Ainaz Eftekhar, Kuo-Hao Zeng, Jiafei Duan, Ali Farhadi, Ani Kembhavi,
  Ranjay Krishna
Categories: cs.CV cs.AI
Comments: See project website: https://embodied-codebook.github.io
\\ ( https://arxiv.org/abs/2311.04193 ,  41328kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04400
replaced with revised version Sat, 9 Mar 2024 10:47:51 GMT   (3798kb,D)

Title: LRM: Large Reconstruction Model for Single Image to 3D
Authors: Yicong Hong and Kai Zhang and Jiuxiang Gu and Sai Bi and Yang Zhou and
  Difan Liu and Feng Liu and Kalyan Sunkavalli and Trung Bui and Hao Tan
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2311.04400 ,  3798kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12316
replaced with revised version Sat, 9 Mar 2024 15:30:08 GMT   (15963kb)

Title: Generating Progressive Images from Pathological Transitions via
  Diffusion Model
Authors: Zeyu Liu, Tianyi Zhang, Yufang He, Yunlu Feng, Yu Zhao, Guanglei Zhang
Categories: cs.CV cs.AI eess.IV
Comments: 13 pages, 9 figs, 4 tabs
\\ ( https://arxiv.org/abs/2311.12316 ,  15963kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12651
replaced with revised version Mon, 11 Mar 2024 04:12:45 GMT   (4493kb,D)

Title: Mobile-Seed: Joint Semantic Segmentation and Boundary Detection for
  Mobile Robots
Authors: Youqi Liao, Shuhao Kang, Jianping Li, Yang Liu, Yun Liu, Zhen Dong,
  Bisheng Yang, Xieyuanli Chen
Categories: cs.CV cs.AI cs.RO
Comments: Accepted by IEEE Robotics and Automation Letters (RA-L) 2024. Code,
  pre-trained models and additional results are available at
  https://whu-usi3dv.github.io/Mobile-Seed/
DOI: 10.1109/LRA.2024.3373235
\\ ( https://arxiv.org/abs/2311.12651 ,  4493kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15100
replaced with revised version Mon, 11 Mar 2024 17:23:24 GMT   (9292kb,D)

Title: Unbalancedness in Neural Monge Maps Improves Unpaired Domain Translation
Authors: Luca Eyring, Dominik Klein, Th\'eo Uscidda, Giovanni Palla, Niki
  Kilbertus, Zeynep Akata, Fabian Theis
Categories: cs.CV cs.AI cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2311.15100 ,  9292kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16503
replaced with revised version Mon, 11 Mar 2024 10:40:40 GMT   (29856kb,D)

Title: TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models
Authors: Yushi Huang, Ruihao Gong, Jing Liu, Tianlong Chen, Xianglong Liu
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2311.16503 ,  29856kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05626
replaced with revised version Sat, 9 Mar 2024 18:34:43 GMT   (1093kb,D)

Title: Redefining Developer Assistance: Through Large Language Models in
  Software Ecosystem
Authors: Somnath Banerjee, Avik Dutta, Sayan Layek, Amruit Sahoo, Sam Conrad
  Joyce, Rima Hazra
Categories: cs.SE cs.AI
Comments: Under review
\\ ( https://arxiv.org/abs/2312.05626 ,  1093kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03196
replaced with revised version Sun, 10 Mar 2024 14:55:39 GMT   (402kb)

Title: SecureReg: A Combined Framework for Proactively Exposing Malicious
  Domain Name Registrations
Authors: Furkan \c{C}olhak, Mert \.Ilhan Ecevit, Hasan Da\u{g}, Reiner
  Creutzburg
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2401.03196 ,  402kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04820
replaced with revised version Sun, 10 Mar 2024 11:13:32 GMT   (475kb)

Title: Phishing Website Detection through Multi-Model Analysis of HTML Content
Authors: Furkan \c{C}olhak, Mert \.Ilhan Ecevit, Bilal Emir U\c{c}ar, Reiner
  Creutzburg, Hasan Da\u{g}
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2401.04820 ,  475kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07348
replaced with revised version Mon, 11 Mar 2024 11:35:22 GMT   (381kb)

Title: Generative AI in EU Law: Liability, Privacy, Intellectual Property, and
  Cybersecurity
Authors: Claudio Novelli, Federico Casolari, Philipp Hacker, Giorgio Spedicato,
  Luciano Floridi
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2401.07348 ,  381kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11913
replaced with revised version Sun, 10 Mar 2024 10:37:21 GMT   (23626kb,D)

Title: Large receptive field strategy and important feature extraction strategy
  in 3D object detection
Authors: Leichao Cui, Xiuxian Li, Min Meng and Guangyu Jia
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2401.11913 ,  23626kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16352
replaced with revised version Sun, 10 Mar 2024 06:49:58 GMT   (3776kb,D)

Title: Adversarial Training on Purification (AToP): Advancing Both Robustness
  and Generalization
Authors: Guang Lin, Chao Li, Jianhai Zhang, Toshihisa Tanaka, Qibin Zhao
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2401.16352 ,  3776kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17592
replaced with revised version Mon, 11 Mar 2024 01:32:03 GMT   (9187kb,D)

Title: Local Feature Matching Using Deep Learning: A Survey
Authors: Shibiao Xu, Shunpeng Chen, Rongtao Xu, Changwei Wang, Peng Lu, Li Guo
Categories: cs.CV cs.AI
Comments: Accepted by Information Fusion 2024. Project page:
  https://github.com/vignywang/Awesome-Local-Feature-Matching
DOI: 10.1016/j.inffus.2024.102344
\\ ( https://arxiv.org/abs/2401.17592 ,  9187kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17828
replaced with revised version Mon, 11 Mar 2024 04:59:43 GMT   (2019kb,D)

Title: Leveraging Swin Transformer for Local-to-Global Weakly Supervised
  Semantic Segmentation
Authors: Rozhan Ahmadi, Shohreh Kasaei
Categories: cs.CV cs.AI
Comments: 7 pages, 4 figures, 3 tables
\\ ( https://arxiv.org/abs/2401.17828 ,  2019kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07860
replaced with revised version Sun, 10 Mar 2024 23:46:41 GMT   (287kb,D)

Title: On the Detection of Reviewer-Author Collusion Rings From Paper Bidding
Authors: Steven Jecmen, Nihar B. Shah, Fei Fang, Leman Akoglu
Categories: cs.SI cs.AI cs.GT
\\ ( https://arxiv.org/abs/2402.07860 ,  287kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09450 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 11:16:33 GMT   (18857kb,D)

Title: Guiding Masked Representation Learning to Capture Spatio-Temporal
  Relationship of Electrocardiogram
Authors: Yeongyeon Na, Minje Park, Yunwon Tae, Sunghoon Joo
Categories: eess.SP cs.AI cs.LG
Comments: ICLR 2024. The first three authors contribute equally
\\ ( https://arxiv.org/abs/2402.09450 ,  18857kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16898
replaced with revised version Sun, 10 Mar 2024 07:35:15 GMT   (8709kb,D)

Title: MIM-Reasoner: Learning with Theoretical Guarantees for Multiplex
  Influence Maximization
Authors: Nguyen Do, Tanmoy Chowdhury, Chen Ling, Liang Zhao, My T. Thai
Categories: cs.SI cs.AI cs.LG math.PR stat.ML
Journal-ref: International Conference on Artificial Intelligence and Statistics
  (AISTATS) 2024
\\ ( https://arxiv.org/abs/2402.16898 ,  8709kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16906
replaced with revised version Sun, 10 Mar 2024 06:16:01 GMT   (782kb,D)

Title: LDB: A Large Language Model Debugger via Verifying Runtime Execution
  Step-by-step
Authors: Lily Zhong, Zilong Wang, Jingbo Shang
Categories: cs.SE cs.AI cs.CL
Comments: Preprint
\\ ( https://arxiv.org/abs/2402.16906 ,  782kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00929
replaced with revised version Sun, 10 Mar 2024 08:55:18 GMT   (3497kb,D)

Title: PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for
  Data-Efficient Imitation Learning
Authors: Tian Gao, Soroush Nasiriany, Huihan Liu, Quantao Yang, Yuke Zhu
Categories: cs.RO cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.00929 ,  3497kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01437
replaced with revised version Sun, 10 Mar 2024 09:56:22 GMT   (923kb,D)

Title: GPTSee: Enhancing Moment Retrieval and Highlight Detection via
  Description-Based Similarity Features
Authors: Yunzhuo Sun, Yifang Xu, Zien Xie, Yukun Shu, and Sidan Du
Categories: cs.CV cs.AI
Comments: 5 pages, 3 figures
DOI: 10.1109/LSP.2023.3340103
\\ ( https://arxiv.org/abs/2403.01437 ,  923kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03864
replaced with revised version Sun, 10 Mar 2024 00:59:35 GMT   (1737kb,D)

Title: Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious
  Challenges in Multimodal Reasoning
Authors: Deepanway Ghosal, Vernon Toh Yan Han, Chia Yew Ken, Soujanya Poria
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.03864 ,  1737kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04769
replaced with revised version Mon, 11 Mar 2024 01:21:32 GMT   (6457kb,D)

Title: Using Hallucinations to Bypass GPT4's Filter
Authors: Benjamin Lemkin
Categories: cs.CR cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2403.04769 ,  6457kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04917
replaced with revised version Mon, 11 Mar 2024 03:47:04 GMT   (403kb)

Title: A Mixed-Integer Conic Program for the Moving-Target Traveling Salesman
  Problem based on a Graph of Convex Sets
Authors: Allen George Philip, Zhongqiang Ren, Sivakumar Rathinam, Howie Choset
Categories: cs.RO cs.AI cs.DS
Comments: 7 pages, 4 figures
\\ ( https://arxiv.org/abs/2403.04917 ,  403kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04954
replaced with revised version Mon, 11 Mar 2024 09:37:39 GMT   (14444kb)

Title: Fooling Neural Networks for Motion Forecasting via Adversarial Attacks
Authors: Edgar Medina, Leyong Loh
Categories: cs.CV cs.AI
Comments: 11 pages, 8 figures, VISSAP 2024
\\ ( https://arxiv.org/abs/2403.04954 ,  14444kb)
------------------------------------------------------------------------------
\\
arXiv:2301.09209
replaced with revised version Sun, 10 Mar 2024 17:21:25 GMT   (22889kb,D)

Title: Summarize the Past to Predict the Future: Natural Language Descriptions
  of Context Boost Multimodal Object Interaction Anticipation
Authors: Razvan-George Pasca, Alexey Gavryushin, Muhammad Hamza, Yen-Ling Kuo,
  Kaichun Mo, Luc Van Gool, Otmar Hilliges, Xi Wang
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2301.09209 ,  22889kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05961
replaced with revised version Sat, 9 Mar 2024 15:50:17 GMT   (306kb,D)

Title: Evaluating the Ebb and Flow: An In-depth Analysis of Question-Answering
  Trends across Diverse Platforms
Authors: Rima Hazra, Agnik Saha, Somnath Banerjee and Animesh Mukherjee
Categories: cs.SI cs.CL cs.IR cs.LG
Comments: Accepted as POSTER
\\ ( https://arxiv.org/abs/2309.05961 ,  306kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10435
replaced with revised version Sat, 9 Mar 2024 04:04:09 GMT   (1483kb,D)

Title: Reformulating Sequential Recommendation: Learning Dynamic User Interest
  with Content-enriched Language Modeling
Authors: Junzhe Jiang, Shang Qu, Mingyue Cheng, Qi Liu
Categories: cs.IR cs.CL
\\ ( https://arxiv.org/abs/2309.10435 ,  1483kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01381
replaced with revised version Sun, 10 Mar 2024 22:31:45 GMT   (12934kb,D)

Title: DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform
  Generation
Authors: Roi Benita, Michael Elad, Joseph Keshet
Categories: cs.SD cs.CL eess.AS
\\ ( https://arxiv.org/abs/2310.01381 ,  12934kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14356
replaced with revised version Sat, 9 Mar 2024 20:47:30 GMT   (76015kb,D)

Title: Computer Vision Datasets and Models Exhibit Cultural and Linguistic
  Diversity in Perception
Authors: Andre Ye, Sebastin Santy, Jena D. Hwang, Amy X. Zhang, Ranjay Krishna
Categories: cs.CV cs.CL cs.CY cs.HC
\\ ( https://arxiv.org/abs/2310.14356 ,  76015kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19181
replaced with revised version Sun, 10 Mar 2024 04:12:27 GMT   (9084kb,D)

Title: From Chatbots to PhishBots? -- Preventing Phishing scams created using
  ChatGPT, Google Bard and Claude
Authors: Sayak Saha Roy, Poojitha Thota, Krishna Vamsi Naragam, Shirin
  Nilizadeh
Categories: cs.CR cs.CL
\\ ( https://arxiv.org/abs/2310.19181 ,  9084kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11791
replaced with revised version Mon, 11 Mar 2024 04:01:50 GMT   (3467kb,D)

Title: SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic
  Segmentation
Authors: Ci-Siang Lin, Chien-Yi Wang, Yu-Chiang Frank Wang, Min-Hung Chen
Categories: cs.CV cs.CL cs.LG
\\ ( https://arxiv.org/abs/2401.11791 ,  3467kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11895
replaced with revised version Sun, 10 Mar 2024 05:38:20 GMT   (1841kb,D)

Title: Bridging or Breaking: Impact of Intergroup Interactions on Religious
  Polarization
Authors: Rochana Chaturvedi, Sugat Chaturvedi and Elena Zheleva
Categories: cs.SI cs.CL physics.soc-ph
DOI: 10.1145/3589334.3645675
\\ ( https://arxiv.org/abs/2402.11895 ,  1841kb)
------------------------------------------------------------------------------
\\
arXiv:1902.03589
replaced with revised version Sat, 9 Mar 2024 23:21:18 GMT   (3530kb,D)

Title: NeurAll: Towards a Unified Visual Perception Model for Automated Driving
Authors: Ganesh Sistu, Isabelle Leang, Sumanth Chennupati, Senthil Yogamani,
  Ciaran Hughes, Stefan Milz and Samir Rawashdeh
Categories: cs.CV cs.LG cs.RO stat.ML
Comments: Accepted for Oral Presentation at IEEE Intelligent Transportation
  Systems Conference (ITSC) 2019
\\ ( https://arxiv.org/abs/1902.03589 ,  3530kb)
------------------------------------------------------------------------------
\\
arXiv:1906.08691
replaced with revised version Sun, 10 Mar 2024 00:30:11 GMT   (693kb,D)

Title: ENCORE: Ensemble Learning using Convolution Neural Machine Translation
  for Automatic Program Repair
Authors: Thibaud Lutellier, Lawrence Pang, Viet Hung Pham, Moshi Wei, Lin Tan
Categories: cs.SE cs.LG
\\ ( https://arxiv.org/abs/1906.08691 ,  693kb)
------------------------------------------------------------------------------
\\
arXiv:2009.13040 (*cross-listing*)
replaced with revised version Sat, 9 Mar 2024 15:27:29 GMT   (920kb,D)

Title: Local Minima Structures in Gaussian Mixture Models
Authors: Yudong Chen, Dogyoon Song, Xumei Xi and Yuqian Zhang
Categories: stat.ML cs.LG math.ST stat.TH
Comments: 73 pages, 6 figures, 2Tables. To appear in Transactions on
  Information Theory
\\ ( https://arxiv.org/abs/2009.13040 ,  920kb)
------------------------------------------------------------------------------
\\
arXiv:2112.14249 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 02:12:52 GMT   (436kb,D)

Title: Nested Nonparametric Instrumental Variable Regression: Long Term,
  Mediated, and Time Varying Treatment Effects
Authors: Isaac Meza and Rahul Singh
Categories: stat.ML cs.LG econ.EM math.ST stat.TH
\\ ( https://arxiv.org/abs/2112.14249 ,  436kb)
------------------------------------------------------------------------------
\\
arXiv:2201.13192 (*cross-listing*)
replaced with revised version Sun, 10 Mar 2024 13:49:43 GMT   (1533kb,D)

Title: Uncertainty-aware Pseudo-label Selection for Positive-Unlabeled Learning
Authors: Emilio Dorigatti, Jann Goschenhofer, Benjamin Schubert, Mina Rezaei,
  Bernd Bischl
Categories: stat.ML cs.LG
Comments: 25 pages, 4 figures
\\ ( https://arxiv.org/abs/2201.13192 ,  1533kb)
------------------------------------------------------------------------------
\\
arXiv:2206.12191 (*cross-listing*)
replaced with revised version Sun, 10 Mar 2024 21:00:09 GMT   (972kb,D)

Title: Computational Complexity Evaluation of Neural Network Applications in
  Signal Processing
Authors: Pedro Freire, Sasipim Srivallapanondh, Antonio Napoli, Jaroslaw E.
  Prilepsky, Sergei K. Turitsyn
Categories: eess.SP cs.CC cs.LG
\\ ( https://arxiv.org/abs/2206.12191 ,  972kb)
------------------------------------------------------------------------------
\\
arXiv:2208.05845
replaced with revised version Mon, 11 Mar 2024 11:17:36 GMT   (14594kb,D)

Title: Analyzing Fairness in Deepfake Detection With Massively Annotated
  Databases
Authors: Ying Xu, Philipp Terh\"orst, Kiran Raja, Marius Pedersen
Categories: cs.CV cs.CY cs.LG
\\ ( https://arxiv.org/abs/2208.05845 ,  14594kb)
------------------------------------------------------------------------------
\\
arXiv:2210.15182
replaced with revised version Sat, 9 Mar 2024 22:35:57 GMT   (9433kb,D)

Title: Text2Model: Text-based Model Induction for Zero-shot Image
  Classification
Authors: Ohad Amosy, Tomer Volk, Eilam Shapira, Eyal Ben-David, Roi Reichart
  and Gal Chechik
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2210.15182 ,  9433kb)
------------------------------------------------------------------------------
\\
arXiv:2212.14076 (*cross-listing*)
replaced with revised version Sun, 10 Mar 2024 21:29:53 GMT   (1427kb,D)

Title: Quantum-Inspired Tensor Neural Networks for Option Pricing
Authors: Raj G. Patel, Chia-Wei Hsing, Serkan Sahin, Samuel Palmer, Saeed S.
  Jahromi, Shivam Sharma, Tomas Dominguez, Kris Tziritas, Christophe Michel,
  Vincent Porte, Mustafa Abid, Stephane Aubert, Pierre Castellani, Samuel
  Mugel, Roman Orus
Categories: q-fin.PR cs.CE cs.LG quant-ph
Comments: 11 pages, 8 figures, minor changes. arXiv admin note: substantial
  text overlap with arXiv:2208.02235
\\ ( https://arxiv.org/abs/2212.14076 ,  1427kb)
------------------------------------------------------------------------------
\\
arXiv:2303.10257 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 04:59:57 GMT   (7320kb,D)

Title: Recent Developments in Machine Learning Methods for Stochastic Control
  and Games
Authors: Ruimeng Hu, Mathieu Lauri\`ere
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2303.10257 ,  7320kb)
------------------------------------------------------------------------------
\\
arXiv:2303.14877 (*cross-listing*)
replaced with revised version Sat, 9 Mar 2024 20:01:31 GMT   (4322kb,AD)

Title: Quantum approximate optimization via learning-based adaptive
  optimization
Authors: Lixue Cheng, Yu-Qin Chen, Shi-Xin Zhang, Shengyu Zhang
Categories: quant-ph cs.LG
Comments: Main text: 11 pages, 4 figures, SI: 5 pages, 5 figures
Journal-ref: Commun Phys 7, 83 (2024)
DOI: 10.1038/s42005-024-01577-x
\\ ( https://arxiv.org/abs/2303.14877 ,  4322kb)
------------------------------------------------------------------------------
\\
arXiv:2304.09750 (*cross-listing*)
replaced with revised version Sun, 10 Mar 2024 21:51:28 GMT   (1031kb,D)

Title: Application of Tensor Neural Networks to Pricing Bermudan Swaptions
Authors: Raj G. Patel, Tomas Dominguez, Mohammad Dib, Samuel Palmer, Andrea
  Cadarso, Fernando De Lope Contreras, Abdelkader Ratnani, Francisco Gomez
  Casanova, Senaida Hern\'andez-Santana, \'Alvaro D\'iaz-Fern\'andez, Eva
  Andr\'es, Jorge Luis-Hita, Escol\'astico S\'anchez-Mart\'inez, Samuel Mugel,
  Roman Orus
Categories: q-fin.CP cs.CE cs.LG quant-ph
Comments: 16 pages, 9 figures, 2 tables, minor changes
\\ ( https://arxiv.org/abs/2304.09750 ,  1031kb)
------------------------------------------------------------------------------
\\
arXiv:2304.14907 (*cross-listing*)
replaced with revised version Fri, 8 Mar 2024 22:20:52 GMT   (2781kb,D)

Title: A Stochastic-Gradient-based Interior-Point Algorithm for Solving Smooth
  Bound-Constrained Optimization Problems
Authors: Frank E. Curtis, Vyacheslav Kungurtsev, Daniel P. Robinson, Qi Wang
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2304.14907 ,  2781kb)
------------------------------------------------------------------------------
\\
arXiv:2305.00418
replaced with revised version Sat, 9 Mar 2024 00:59:18 GMT   (581kb,D)

Title: Using Large Language Models to Generate JUnit Tests: An Empirical Study
Authors: Mohammed Latif Siddiq, Joanna C. S. Santos, Ridwanul Hasan Tanvir,
  Noshin Ulfat, Fahmid Al Rifat, Vinicius Carvalho Lopes
Categories: cs.SE cs.LG
Comments: Accepted in Research Track of The 28th International Conference on
  Evaluation and Assessment in Software Engineering (EASE 2024)
\\ ( https://arxiv.org/abs/2305.00418 ,  581kb)
------------------------------------------------------------------------------
\\
arXiv:2305.05773
replaced with revised version Mon, 11 Mar 2024 06:27:26 GMT   (604kb,D)

Title: DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for
  Identifying Large Language Model Generated Text
Authors: Travis Munyer, Abdullah Tanvir, Arjon Das, Xin Zhong
Categories: cs.MM cs.LG
Comments: The paper has been accpeted for publication by IEEE Access
\\ ( https://arxiv.org/abs/2305.05773 ,  604kb)
------------------------------------------------------------------------------
\\
arXiv:2305.09744 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 14:09:44 GMT   (374kb,D)

Title: Assessment of few-hits machine learning classification algorithms for
  low energy physics in liquid argon detectors
Authors: Roberto Moretti, Marco Rossi, Matteo Biassoni, Andrea Giachero,
  Michele Grossi, Daniele Guffanti, Danilo Labranca, Francesco Terranova, Sofia
  Vallecorsa
Categories: physics.ins-det cs.LG physics.data-an
\\ ( https://arxiv.org/abs/2305.09744 ,  374kb)
------------------------------------------------------------------------------
\\
arXiv:2305.09792 (*cross-listing*)
replaced with revised version Sat, 9 Mar 2024 15:09:45 GMT   (1536kb,D)

Title: Score Operator Newton transport
Authors: Nisha Chandramoorthy, Florian Schaefer and Youssef Marzouk
Categories: math.ST cs.LG cs.NA math.NA stat.TH
Comments: 24 pages; AISTATS 2024
\\ ( https://arxiv.org/abs/2305.09792 ,  1536kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16534 (*cross-listing*)
replaced with revised version Sat, 9 Mar 2024 23:11:27 GMT   (3604kb,D)

Title: Variation Spaces for Multi-Output Neural Networks: Insights on
  Multi-Task Learning and Network Compression
Authors: Joseph Shenouda, Rahul Parhi, Kangwook Lee, Robert D. Nowak
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2305.16534 ,  3604kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17000
replaced with revised version Mon, 11 Mar 2024 10:07:03 GMT   (500kb,D)

Title: DistriBlock: Identifying adversarial audio samples by leveraging
  characteristics of the output distribution
Authors: Mat\'ias Pizarro, Dorothea Kolossa and Asja Fischer
Categories: cs.SD cs.CR cs.LG eess.AS
\\ ( https://arxiv.org/abs/2305.17000 ,  500kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17283 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 17:37:33 GMT   (3852kb,D)

Title: Sharpened Lazy Incremental Quasi-Newton Method
Authors: Aakash Lahoti, Spandan Senapati, Ketan Rajawat, Alec Koppel
Categories: math.OC cs.LG
Comments: 39 pages, 3 figures; Accepted to AISTATS 2024
\\ ( https://arxiv.org/abs/2305.17283 ,  3852kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03143 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 15:46:57 GMT   (8784kb,D)

Title: Machine learning reveals features of spinon Fermi surface
Authors: Kevin Zhang, Shi Feng, Yuri D. Lensky, Nandini Trivedi, Eun-Ah Kim
Categories: cond-mat.str-el cond-mat.dis-nn cs.LG quant-ph
Comments: 9 pages + 7 pages supplemental
Journal-ref: Commun. Phys. 7, 54 (2024)
DOI: 10.1038/s42005-024-01542-8
\\ ( https://arxiv.org/abs/2306.03143 ,  8784kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06138 (*cross-listing*)
replaced with revised version Fri, 8 Mar 2024 20:11:55 GMT   (9605kb)

Title: Extraction and Recovery of Spatio-Temporal Structure in Latent Dynamics
  Alignment with Diffusion Models
Authors: Yule Wang, Zijing Wu, Chengrui Li, Anqi Wu
Categories: q-bio.NC cs.LG
\\ ( https://arxiv.org/abs/2306.06138 ,  9605kb)
------------------------------------------------------------------------------
\\
arXiv:2306.07254 (*cross-listing*)
replaced with revised version Sat, 9 Mar 2024 03:20:06 GMT   (220kb,D)

Title: On the Expected Size of Conformal Prediction Sets
Authors: Guneet S. Dhillon and George Deligiannidis and Tom Rainforth
Categories: stat.ML cs.LG
Comments: International Conference on Artificial Intelligence and Statistics
  (AISTATS), 2024
\\ ( https://arxiv.org/abs/2306.07254 ,  220kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12041
replaced with revised version Sat, 9 Mar 2024 20:43:33 GMT   (17782kb,D)

Title: Self-Distilled Masked Auto-Encoders are Efficient Video Anomaly
  Detectors
Authors: Nicolae-Catalin Ristea, Florinel-Alin Croitoru, Radu Tudor Ionescu,
  Marius Popescu, Fahad Shahbaz Khan, Mubarak Shah
Categories: cs.CV cs.LG
Comments: Accepted at CVPR 2024
\\ ( https://arxiv.org/abs/2306.12041 ,  17782kb)
------------------------------------------------------------------------------
\\
arXiv:2306.15620
replaced with revised version Mon, 11 Mar 2024 06:20:07 GMT   (20250kb,D)

Title: SCENEREPLICA: Benchmarking Real-World Robot Manipulation by Creating
  Replicable Scenes
Authors: Ninad Khargonkar, Sai Haneesh Allu, Yangxiao Lu, Jishnu Jaykumar P,
  Balakrishnan Prabhakaran, Yu Xiang
Categories: cs.RO cs.CV cs.LG
Comments: Accepted to ICRA 2024. Project page is available at
  https://irvlutd.github.io/SceneReplica
\\ ( https://arxiv.org/abs/2306.15620 ,  20250kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03410 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 03:56:13 GMT   (460kb,D)

Title: Scalable High-Dimensional Multivariate Linear Regression for
  Feature-Distributed Data
Authors: Shuo-Chieh Huang, Ruey S. Tsay
Categories: stat.ML cs.DC cs.LG
\\ ( https://arxiv.org/abs/2307.03410 ,  460kb)
------------------------------------------------------------------------------
\\
arXiv:2307.14642 (*cross-listing*)
replaced with revised version Sat, 9 Mar 2024 01:10:21 GMT   (327kb)

Title: Linear Convergence of Black-Box Variational Inference: Should We Stick
  the Landing?
Authors: Kyurae Kim, Yian Ma, and Jacob R. Gardner
Categories: stat.ML cs.LG stat.CO
Comments: Accepted to AISTATS'24
\\ ( https://arxiv.org/abs/2307.14642 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08138
replaced with revised version Sat, 9 Mar 2024 04:18:46 GMT   (45kb)

Title: Data-Driven Adversarial Online Control for Unknown Linear Systems
Authors: Zishun Liu and Yongxin Chen
Categories: eess.SY cs.LG cs.SY math.OC
\\ ( https://arxiv.org/abs/2308.08138 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00854
replaced with revised version Fri, 8 Mar 2024 23:16:12 GMT   (1743kb,D)

Title: A Unifying Variational Framework for Gaussian Process Motion Planning
Authors: Lucas Cosier, Rares Iordan, Sicelukwanda Zwane, Giovanni Franzese,
  James T. Wilson, Marc Peter Deisenroth, Alexander Terenin, Yasemin Bekiroglu
Categories: cs.RO cs.LG
Comments: Code and supplementary video available at:
  https://github.com/luke-ck/vgpmp
Journal-ref: Artificial Intelligence and Statistics, 2024
\\ ( https://arxiv.org/abs/2309.00854 ,  1743kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00958
replaced with revised version Sat, 9 Mar 2024 10:54:04 GMT   (3660kb,D)

Title: Index-aware learning of circuits
Authors: Idoia Cortes Garcia, Peter F\"orster, Lennart Jansen, Wil Schilders,
  Sebastian Sch\"ops
Categories: cs.CE cs.LG
Comments: 21 pages, 16 figures
MSC-class: 34A09, 65L80 (Primary)
ACM-class: G.1.7; J.2; J.6
\\ ( https://arxiv.org/abs/2309.00958 ,  3660kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06679 (*cross-listing*)
replaced with revised version Sun, 10 Mar 2024 18:33:54 GMT   (893kb,D)

Title: Generalizable improvement of the Spalart-Allmaras model through
  assimilation of experimental data
Authors: Deepinder Jot Singh Aulakh, Xiang Yang and Romit Maulik
Categories: physics.flu-dyn cs.LG physics.comp-ph physics.data-an
\\ ( https://arxiv.org/abs/2309.06679 ,  893kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07080 (*cross-listing*)
replaced with revised version Sat, 9 Mar 2024 17:56:38 GMT   (13322kb,D)

Title: Discovering Dynamic Effective Connectome of Brain with Bayesian Dynamic
  DAG Learning
Authors: Abdolmahdi Bagheri, Mohammad Pasande, Kevin Bello, Babak Nadjar
  Araabi, Alireza Akhondi-Asl
Categories: q-bio.NC cs.LG
\\ ( https://arxiv.org/abs/2309.07080 ,  13322kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12819 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 03:09:55 GMT   (294kb,D)

Title: Doubly Robust Proximal Causal Learning for Continuous Treatments
Authors: Yong Wu, Yanwei Fu, Shouyan Wang, Xinwei Sun
Categories: stat.ME cs.LG
Comments: Published as a conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2309.12819 ,  294kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16578 (*cross-listing*)
replaced with revised version Sun, 10 Mar 2024 04:00:39 GMT   (2756kb,D)

Title: Overcoming the Barrier of Orbital-Free Density Functional Theory for
  Molecular Systems Using Deep Learning
Authors: He Zhang, Siyuan Liu, Jiacheng You, Chang Liu, Shuxin Zheng, Ziheng
  Lu, Tong Wang, Nanning Zheng, Bin Shao
Categories: stat.ML cs.LG physics.chem-ph
Comments: Published in Nature Computational Science, March 2024. Full paper
  with supplementary information
DOI: 10.1038/s43588-024-00605-8
\\ ( https://arxiv.org/abs/2309.16578 ,  2756kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16992
replaced with revised version Mon, 11 Mar 2024 02:38:01 GMT   (33213kb,D)

Title: Segment Anything Model is a Good Teacher for Local Feature Learning
Authors: Jingqian Wu, Rongtao Xu, Zach Wood-Doughty, Changwei Wang, Shibiao Xu,
  Edmund Lam
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2309.16992 ,  33213kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02011
replaced with revised version Sat, 9 Mar 2024 19:26:46 GMT   (437kb,D)

Title: Decoding Human Activities: Analyzing Wearable Accelerometer and
  Gyroscope Data for Activity Recognition
Authors: Utsab Saha, Sawradip Saha, Tahmid Kabir, Shaikh Anowarul Fattah,
  Mohammad Saquib
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2310.02011 ,  437kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03597 (*cross-listing*)
replaced with revised version Sat, 9 Mar 2024 15:35:46 GMT   (4414kb,D)

Title: Sampling via Gradient Flows in the Space of Probability Measures
Authors: Yifan Chen, Daniel Zhengyu Huang, Jiaoyang Huang, Sebastian Reich,
  Andrew M Stuart
Categories: stat.ML cs.LG cs.NA math.DS math.NA
Comments: Related and text overlap with arXiv:2302.11024
\\ ( https://arxiv.org/abs/2310.03597 ,  4414kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08381
replaced with revised version Sun, 10 Mar 2024 19:00:00 GMT   (4068kb,D)

Title: AutoVP: An Automated Visual Prompting Framework and Benchmark
Authors: Hsi-Ai Tsao, Lei Hsiung, Pin-Yu Chen, Sijia Liu, Tsung-Yi Ho
Categories: cs.CV cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.08381 ,  4068kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09804 (*cross-listing*)
replaced with revised version Sat, 9 Mar 2024 13:09:30 GMT   (6827kb,D)

Title: Communication Compression for Byzantine Robust Learning: New Efficient
  Algorithms and Improved Rates
Authors: Ahmad Rammal, Kaja Gruntkowska, Nikita Fedin, Eduard Gorbunov, Peter
  Richt\'arik
Categories: math.OC cs.LG
Comments: 47 pages, 10 figures
MSC-class: 90C26
\\ ( https://arxiv.org/abs/2310.09804 ,  6827kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13349 (*cross-listing*)
replaced with revised version Sun, 10 Mar 2024 19:29:06 GMT   (15347kb,D)

Title: DeepFDR: A Deep Learning-based False Discovery Rate Control Method for
  Neuroimaging Data
Authors: Taehyo Kim, Hai Shu, Qiran Jia, Mony J. de Leon
Categories: stat.ML cs.CV cs.LG
Journal-ref: The 27th International Conference on Artificial Intelligence and
  Statistics (AISTATS 2024)
\\ ( https://arxiv.org/abs/2310.13349 ,  15347kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16047
replaced with revised version Mon, 11 Mar 2024 15:14:51 GMT   (16849kb,D)

Title: From Posterior Sampling to Meaningful Diversity in Image Restoration
Authors: Noa Cohen, Hila Manor, Yuval Bahat, Tomer Michaeli
Categories: cs.CV cs.LG eess.IV
Comments: Accepted for ICLR 2024. Code and examples are available at
  https://noa-cohen.github.io/MeaningfulDiversityInIR
\\ ( https://arxiv.org/abs/2310.16047 ,  16849kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17848 (*cross-listing*)
replaced with revised version Sun, 10 Mar 2024 18:53:50 GMT   (21438kb,D)

Title: Boosting Data Analytics With Synthetic Volume Expansion
Authors: Xiaotong Shen, Yifei Liu, Rex Shen
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2310.17848 ,  21438kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18554 (*cross-listing*)
replaced with revised version Sat, 9 Mar 2024 13:11:23 GMT   (1889kb,D)

Title: Improved Regret Bounds of (Multinomial) Logistic Bandits via
  Regret-to-Confidence-Set Conversion
Authors: Junghyun Lee, Se-Young Yun, Kwang-Sung Jun
Categories: stat.ML cs.LG
Comments: 39 pages, 1 figure, 1 table; Accepted to the 27th International
  Conference on Artificial Intelligence and Statistics (AISTATS 2024) (ver2:
  fixed some errors and significantly expanded discussions on various parts,
  such as related work.)
\\ ( https://arxiv.org/abs/2310.18554 ,  1889kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19788 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 00:56:02 GMT   (2884kb,D)

Title: Worst-Case Optimal Multi-Armed Gaussian Best Arm Identification with a
  Fixed Budget
Authors: Masahiro Kato
Categories: math.ST cs.LG econ.EM stat.ME stat.ML stat.TH
\\ ( https://arxiv.org/abs/2310.19788 ,  2884kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02557 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 10:44:54 GMT   (5575kb,D)

Title: Fast Minimization of Expected Logarithmic Loss via Stochastic Dual
  Averaging
Authors: Chung-En Tsai and Hao-Chung Cheng and Yen-Huan Li
Categories: math.OC cs.LG quant-ph
Comments: 26 pages, AISTATS 2024
\\ ( https://arxiv.org/abs/2311.02557 ,  5575kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05866 (*cross-listing*)
replaced with revised version Sat, 9 Mar 2024 06:31:46 GMT   (3977kb,D)

Title: Fair Supervised Learning with A Simple Random Sampler of Sensitive
  Attributes
Authors: Jinwon Sohn, Qifan Song, Guang Lin
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2311.05866 ,  3977kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06293 (*cross-listing*)
replaced with revised version Sun, 10 Mar 2024 15:49:57 GMT   (754kb)

Title: Quantum Neural Networks for Power Flow Analysis
Authors: Zeynab Kaseb, Matthias Moller, Giorgio Tosti Balducci, Peter Palensky,
  Pedro P. Vergara
Categories: quant-ph cs.LG cs.SY eess.SY
Comments: 8 pages, 13 figures
\\ ( https://arxiv.org/abs/2311.06293 ,  754kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07620
replaced with revised version Sat, 9 Mar 2024 02:45:35 GMT   (2519kb,D)

Title: EPIM: Efficient Processing-In-Memory Accelerators based on Epitome
Authors: Chenyu Wang, Zhen Dong, Daquan Zhou, Zhenhua Zhu, Yu Wang, Jiashi
  Feng, Kurt Keutzer
Categories: cs.AR cs.LG
\\ ( https://arxiv.org/abs/2311.07620 ,  2519kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12570 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 09:49:06 GMT   (959kb,D)

Title: BEND: Benchmarking DNA Language Models on biologically meaningful tasks
Authors: Frederikke Isa Marin, Felix Teufel, Marc Horlacher, Dennis Madsen,
  Dennis Pultz, Ole Winther, Wouter Boomsma
Categories: q-bio.GN cs.LG
Comments: 9 pages, 1 figure, 3 tables, code available at
  https://github.com/frederikkemarin/BEND, to be published in ICLR 2024
\\ ( https://arxiv.org/abs/2311.12570 ,  959kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12831
replaced with revised version Sat, 9 Mar 2024 16:15:38 GMT   (33179kb,D)

Title: ECNR: Efficient Compressive Neural Representation of Time-Varying
  Volumetric Datasets
Authors: Kaiyuan Tang and Chaoli Wang
Categories: cs.CV cs.GR cs.LG
Comments: Accepted by IEEE PacificVis 2024 (conference papers track)
\\ ( https://arxiv.org/abs/2311.12831 ,  33179kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15327
replaced with revised version Sun, 10 Mar 2024 15:50:36 GMT   (961kb)

Title: FRAC-Q-Learning: A Reinforcement Learning with Boredom Avoidance
  Processes for Social Robots
Authors: Akinari Onishi
Categories: cs.RO cs.HC cs.LG
\\ ( https://arxiv.org/abs/2311.15327 ,  961kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16632 (*cross-listing*)
replaced with revised version Sat, 9 Mar 2024 12:49:11 GMT   (259kb,D)

Title: Opening the Black Box: Towards inherently interpretable energy data
  imputation models using building physics insight
Authors: Antonio Liguori, Matias Quintana, Chun Fu, Clayton Miller, J\'er\^ome
  Frisch, Christoph van Treeck
Categories: stat.ML cs.LG
Comments: Accepted for publication in Energy and Buildings
\\ ( https://arxiv.org/abs/2311.16632 ,  259kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17740 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 12:17:58 GMT   (1580kb,D)

Title: A transductive few-shot learning approach for classification of digital
  histopathological slides from liver cancer
Authors: Aymen Sadraoui (OPIS, CVN), S\'egol\`ene Martin (OPIS, CVN), Eliott
  Barbot (OPIS, CVN), Astrid Laurent-Bellue, Jean-Christophe Pesquet (OPIS,
  CVN), Catherine Guettier, Ismail Ben Ayed (ETS)
Categories: eess.IV cs.LG q-bio.TO
Journal-ref: ISBI 2024 - 21st IEEE International Symposium on Biomedical
  Imaging, May 2024, Ath{\`e}nes, Greece
\\ ( https://arxiv.org/abs/2311.17740 ,  1580kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15698
replaced with revised version Mon, 11 Mar 2024 08:31:19 GMT   (309kb,D)

Title: RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for
  Program Repair
Authors: Andr\'e Silva, Sen Fang, Martin Monperrus
Categories: cs.SE cs.LG
\\ ( https://arxiv.org/abs/2312.15698 ,  309kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13045 (*cross-listing*)
replaced with revised version Sat, 9 Mar 2024 11:36:52 GMT   (342kb)

Title: Assessment of Sports Concussion in Female Athletes: A Role for
  Neuroinformatics?
Authors: Rachel Edelstein, Sterling Gutterman, Benjamin Newman, John Darrell
  Van Horn
Categories: stat.ML cs.LG stat.AP stat.ME
\\ ( https://arxiv.org/abs/2401.13045 ,  342kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14512 (*cross-listing*)
replaced with revised version Sun, 10 Mar 2024 18:08:27 GMT   (1801kb,D)

Title: Who Are We Missing? A Principled Approach to Characterizing the
  Underrepresented Population
Authors: Harsh Parikh, Rachael Ross, Elizabeth Stuart, Kara Rudolph
Categories: stat.ME cs.CY cs.LG stat.AP
Comments: MOUD Analysis Included
\\ ( https://arxiv.org/abs/2401.14512 ,  1801kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03838 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 12:16:24 GMT   (3148kb,D)

Title: Gaussian process regression with Sliced Wasserstein Weisfeiler-Lehman
  graph kernels
Authors: Rapha\"el Carpintero Perez (CMAP), S\'ebastien da Veiga (ENSAI,
  CREST), Josselin Garnier (CMAP), Brian Staber
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2402.03838 ,  3148kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03917
replaced with revised version Mon, 11 Mar 2024 13:35:29 GMT   (1095kb,D)

Title: Elastic Feature Consolidation for Cold Start Exemplar-free Incremental
  Learning
Authors: Simone Magistri, Tomaso Trinci, Albin Soutif-Cormerais, Joost van de
  Weijer, Andrew D. Bagdanov
Categories: cs.CV cs.LG
Comments: Accepted at Twelfth International Conference on Learning
  Representations (ICLR 2024)
\\ ( https://arxiv.org/abs/2402.03917 ,  1095kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04613 (*cross-listing*)
replaced with revised version Sat, 9 Mar 2024 22:58:57 GMT   (10656kb,D)

Title: Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in
  Reproducing Kernel Hilbert Spaces
Authors: Sebastian Neumayer, Viktor Stein, Gabriele Steidl, Nicolaj Rux
Categories: stat.ML cs.LG math.FA math.OC
Comments: 46 pages, 13 figures
MSC-class: 46N10 (Primary) 46E22, 94A15 (Secondary)
\\ ( https://arxiv.org/abs/2402.04613 ,  10656kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05067 (*cross-listing*)
replaced with revised version Sat, 9 Mar 2024 15:28:21 GMT   (11961kb,D)

Title: Insights into Multiscale Complexity: from Macroscopic Patterns to
  Microscopic Simulations via Deep Learning
Authors: Jing Wang and Zheng Li and Pengyu Lai and Rui Wang and Di Yang and
  Dewu Yang and Hui Xu
Categories: physics.flu-dyn cs.LG physics.comp-ph
\\ ( https://arxiv.org/abs/2402.05067 ,  11961kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05210 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 17:50:31 GMT   (32310kb,D)

Title: Anatomically-Controllable Medical Image Generation with
  Segmentation-Guided Diffusion Models
Authors: Nicholas Konz, Yuwen Chen, Haoyu Dong, Maciej A. Mazurowski
Categories: eess.IV cs.CV cs.LG stat.ML
Comments: Code and synthetic dataset:
  https://github.com/mazurowski-lab/segmentation-guided-diffusion
\\ ( https://arxiv.org/abs/2402.05210 ,  32310kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10045
replaced with revised version Sat, 9 Mar 2024 13:38:46 GMT   (1268kb)

Title: Short-Form Videos and Mental Health: A Knowledge-Guided Multimodal
  Neural Topic Model
Authors: Jiaheng Xie, Ruicheng Liang, Yidong Chai, Yang Liu, Daniel Zeng
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2402.10045 ,  1268kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11816
replaced with revised version Mon, 11 Mar 2024 06:46:22 GMT   (607kb,D)

Title: Learning the Unlearned: Mitigating Feature Suppression in Contrastive
  Learning
Authors: Jihai Zhang, Xiang Lan, Xiaoye Qu, Yu Cheng, Mengling Feng, Bryan Hooi
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2402.11816 ,  607kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14515 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 15:40:18 GMT   (38kb,D)

Title: Spectral invariance and maximality properties of the frequency spectrum
  of quantum neural networks
Authors: Patrick Holzer, Ivica Turkalj
Categories: quant-ph cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.14515 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18781
replaced with revised version Fri, 8 Mar 2024 21:36:48 GMT   (258kb)

Title: Conjectural Online Learning with First-order Beliefs in Asymmetric
  Information Stochastic Games
Authors: Tao Li, Kim Hammar, Rolf Stadler, and Quanyan Zhu
Categories: cs.GT cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2402.18781 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00269
replaced with revised version Mon, 11 Mar 2024 05:58:55 GMT   (48758kb,D)

Title: Large Convolutional Model Tuning via Filter Subspace
Authors: Wei Chen, Zichen Miao, Qiang Qiu
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2403.00269 ,  48758kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01865 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 13:11:51 GMT   (4258kb,D)

Title: Improving generalisation via anchor multivariate analysis
Authors: Homer Durand, Gherardo Varando, Nathan Mankovich, Gustau Camps-Valls
Categories: stat.ML cs.LG stat.AP stat.ME
Comments: 21 pages, 15 figures
MSC-class: 62Hxx
\\ ( https://arxiv.org/abs/2403.01865 ,  4258kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02645 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 17:25:14 GMT   (6454kb,D)

Title: DT-DDNN: A Physical Layer Security Attack Detector in 5G RF Domain for
  CAVs
Authors: Ghazal Asemian, Mohammadreza Amini, Burak Kantarci, Melike
  Erol-Kantarci
Categories: eess.SP cs.CR cs.LG cs.NI
Comments: 15 pages, 16 figures
\\ ( https://arxiv.org/abs/2403.02645 ,  6454kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03353 (*cross-listing*)
replaced with revised version Mon, 11 Mar 2024 14:37:42 GMT   (27kb)

Title: Hypothesis Spaces for Deep Learning
Authors: Rui Wang, Yuesheng Xu, Mingsong Yan
Categories: stat.ML cs.LG math.FA
\\ ( https://arxiv.org/abs/2403.03353 ,  27kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03434
replaced with revised version Sat, 9 Mar 2024 05:01:03 GMT   (701kb)

Title: An AI-enabled Agent-Based Model and Its Application in Measles Outbreak
  Simulation for New Zealand
Authors: Sijin Zhang, Alvaro Orsi, Lei Chen
Categories: cs.MA cs.CY cs.LG
Comments: 11 pages, 9 figures
\\ ( https://arxiv.org/abs/2403.03434 ,  701kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
