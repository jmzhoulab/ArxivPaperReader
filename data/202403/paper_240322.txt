paper_240322.txt

Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 100021 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月22日 13:29
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Wed 20 Mar 24 18:00:00 GMT  to  Thu 21 Mar 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.14077
Date: Thu, 21 Mar 2024 01:57:30 GMT   (34762kb,D)

Title: Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language
  Models for Media Forensics
Authors: Shan Jia, Reilin Lyu, Kangran Zhao, Yize Chen, Zhiyuan Yan, Yan Ju,
  Chuanbo Hu, Xin Li, Baoyuan Wu, Siwei Lyu
Categories: cs.AI
\\
  DeepFakes, which refer to AI-generated media content, have become an
increasing concern due to their use as a means for disinformation. Detecting
DeepFakes is currently solved with programmed machine learning algorithms. In
this work, we investigate the capabilities of multimodal large language models
(LLMs) in DeepFake detection. We conducted qualitative and quantitative
experiments to demonstrate multimodal LLMs and show that they can expose
AI-generated images through careful experimental design and prompt engineering.
This is interesting, considering that LLMs are not inherently tailored for
media forensic tasks, and the process does not require programming. We discuss
the limitations of multimodal LLMs for these tasks and suggest possible
improvements.
\\ ( https://arxiv.org/abs/2403.14077 ,  34762kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14100
Date: Thu, 21 Mar 2024 03:23:34 GMT   (15143kb,D)

Title: Causal knowledge engineering: A case study from COVID-19
Authors: Steven Mascaro, Yue Wu, Ross Pearson, Owen Woodberry, Jessica Ramsay,
  Tom Snelling, Ann E. Nicholson
Categories: cs.AI
Comments: 22 pages (plus 19 pages in appendices), 9 figures, submitted for
  review
\\
  COVID-19 appeared abruptly in early 2020, requiring a rapid response amid a
context of great uncertainty. Good quality data and knowledge was initially
lacking, and many early models had to be developed with causal assumptions and
estimations built in to supplement limited data, often with no reliable
approach for identifying, validating and documenting these causal assumptions.
Our team embarked on a knowledge engineering process to develop a causal
knowledge base consisting of several causal BNs for diverse aspects of
COVID-19. The unique challenges of the setting lead to experiments with the
elicitation approach, and what emerged was a knowledge engineering method we
call Causal Knowledge Engineering (CKE). The CKE provides a structured approach
for building a causal knowledge base that can support the development of a
variety of application-specific models. Here we describe the CKE method, and
use our COVID-19 work as a case study to provide a detailed discussion and
analysis of the method.
\\ ( https://arxiv.org/abs/2403.14100 ,  15143kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14102
Date: Thu, 21 Mar 2024 03:25:49 GMT   (745kb,D)

Title: DouRN: Improving DouZero by Residual Neural Networks
Authors: Yiquan Chen, Yingchao Lyu, Di Zhang
Categories: cs.AI cs.LG
Journal-ref: CyberC 2023: 96-99
\\
  Deep reinforcement learning has made significant progress in games with
imperfect information, but its performance in the card game Doudizhu (Chinese
Poker/Fight the Landlord) remains unsatisfactory. Doudizhu is different from
conventional games as it involves three players and combines elements of
cooperation and confrontation, resulting in a large state and action space. In
2021, a Doudizhu program called DouZero\cite{zha2021douzero} surpassed previous
models without prior knowledge by utilizing traditional Monte Carlo methods and
multilayer perceptrons. Building on this work, our study incorporates residual
networks into the model, explores different architectural designs, and conducts
multi-role testing. Our findings demonstrate that this model significantly
improves the winning rate within the same training time. Additionally, we
introduce a call scoring system to assist the agent in deciding whether to
become a landlord. With these enhancements, our model consistently outperforms
the existing version of DouZero and even experienced human players.
\footnote{The source code is available at
\url{https://github.com/Yingchaol/Douzero_Resnet.git.}
\\ ( https://arxiv.org/abs/2403.14102 ,  745kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14443
Date: Thu, 21 Mar 2024 14:48:37 GMT   (1363kb,D)

Title: Language Models Can Reduce Asymmetry in Information Markets
Authors: Nasim Rahaman, Martin Weiss, Manuel W\"uthrich, Yoshua Bengio, Li
  Erran Li, Chris Pal, Bernhard Sch\"olkopf
Categories: cs.AI cs.CL cs.LG cs.MA cs.SI
\\
  This work addresses the buyer's inspection paradox for information markets.
The paradox is that buyers need to access information to determine its value,
while sellers need to limit access to prevent theft. To study this, we
introduce an open-source simulated digital marketplace where intelligent
agents, powered by language models, buy and sell information on behalf of
external participants. The central mechanism enabling this marketplace is the
agents' dual capabilities: they not only have the capacity to assess the
quality of privileged information but also come equipped with the ability to
forget. This ability to induce amnesia allows vendors to grant temporary access
to proprietary information, significantly reducing the risk of unauthorized
retention while enabling agents to accurately gauge the information's relevance
to specific queries or tasks. To perform well, agents must make rational
decisions, strategically explore the marketplace through generated sub-queries,
and synthesize answers from purchased information. Concretely, our experiments
(a) uncover biases in language models leading to irrational behavior and
evaluate techniques to mitigate these biases, (b) investigate how price affects
demand in the context of informational goods, and (c) show that inspection and
higher budgets both lead to higher quality outcomes.
\\ ( https://arxiv.org/abs/2403.14443 ,  1363kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14566
Date: Thu, 21 Mar 2024 17:09:20 GMT   (223kb,D)

Title: A survey on Concept-based Approaches For Model Improvement
Authors: Avani Gupta, P J Narayanan
Categories: cs.AI cs.LG
\\
  The focus of recent research has shifted from merely increasing the Deep
Neural Networks (DNNs) performance in various tasks to DNNs, which are more
interpretable to humans. The field of eXplainable Artificial Intelligence (XAI)
has observed various techniques, including saliency-based and concept-based
approaches. Concept-based approaches explain the model's decisions in simple
human understandable terms called Concepts. Concepts are human interpretable
units of data and are the thinking ground of humans. Explanations in terms of
concepts enable detecting spurious correlations, inherent biases, or
clever-hans. With the advent of concept-based explanations, there have been
various concept representation methods and automatic concept discovery
algorithms. Some recent methods use concepts for post-hoc model disentanglement
evaluation, while others use them for ante-hoc training. The concept-based
approaches are new, with many representations coming up, and there is very
limited work on Concept-based Model improvement. We provide a systematic review
and taxonomy of various concept representations and their discovery algorithms
in DNNs, specifically in vision. We also provide details on concept-based model
improvement literature, which is the first to survey concept-based model
improvement methods.
\\ ( https://arxiv.org/abs/2403.14566 ,  223kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14589
Date: Thu, 21 Mar 2024 17:43:44 GMT   (340kb,D)

Title: ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for
  Contrastive Self-Training
Authors: Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu
Categories: cs.AI cs.CL cs.LG
\\
  Language agents have demonstrated autonomous decision-making abilities by
reasoning with foundation models. Recently, efforts have been made to train
language agents for performance improvement, with multi-step reasoning and
action trajectories as the training data. However, collecting such trajectories
still requires considerable human effort, by either artificial annotations or
implementations of diverse prompting frameworks. In this work, we propose
A$^3$T, a framework that enables the Autonomous Annotation of Agent
Trajectories in the style of ReAct. The central role is an ActRe prompting
agent, which explains the reason for an arbitrary action. When randomly
sampling an external action, the ReAct-style agent could query the ActRe agent
with the action to obtain its textual rationales. Novel trajectories are then
synthesized by prepending the posterior reasoning from ActRe to the sampled
action. In this way, the ReAct-style agent executes multiple trajectories for
the failed tasks, and selects the successful ones to supplement its failed
trajectory for contrastive self-training. Realized by policy gradient methods
with binarized rewards, the contrastive self-training with accumulated
trajectories facilitates a closed loop for multiple rounds of language agent
self-improvement. We conduct experiments using QLoRA fine-tuning with the
open-sourced Mistral-7B-Instruct-v0.2. In AlfWorld, the agent trained with
A$^3$T obtains a 1-shot success rate of 96%, and 100% success with 4 iterative
rounds. In WebShop, the 1-shot performance of the A$^3$T agent matches human
average, and 4 rounds of iterative refinement lead to the performance
approaching human experts. A$^3$T agents significantly outperform existing
techniques, including prompting with GPT-4, advanced agent frameworks, and
fully fine-tuned LLMs.
\\ ( https://arxiv.org/abs/2403.14589 ,  340kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13840
Date: Fri, 15 Mar 2024 04:02:24 GMT   (1048kb)

Title: Whose Side Are You On? Investigating the Political Stance of Large
  Language Models
Authors: Pagnarasmey Pit, Xingjun Ma, Mike Conway, Qingyu Chen, James Bailey,
  Henry Pit, Putrasmey Keo, Watey Diep and Yu-Gang Jiang
Categories: cs.CL cs.AI cs.SI
\\
  Large Language Models (LLMs) have gained significant popularity for their
application in various everyday tasks such as text generation, summarization,
and information retrieval. As the widespread adoption of LLMs continues to
surge, it becomes increasingly crucial to ensure that these models yield
responses that are politically impartial, with the aim of preventing
information bubbles, upholding fairness in representation, and mitigating
confirmation bias. In this paper, we propose a quantitative framework and
pipeline designed to systematically investigate the political orientation of
LLMs. Our investigation delves into the political alignment of LLMs across a
spectrum of eight polarizing topics, spanning from abortion to LGBTQ issues.
Across topics, the results indicate that LLMs exhibit a tendency to provide
responses that closely align with liberal or left-leaning perspectives rather
than conservative or right-leaning ones when user queries include details
pertaining to occupation, race, or political affiliation. The findings
presented in this study not only reaffirm earlier observations regarding the
left-leaning characteristics of LLMs but also surface particular attributes,
such as occupation, that are particularly susceptible to such inclinations even
when directly steered towards conservatism. As a recommendation to avoid these
models providing politicised responses, users should be mindful when crafting
queries, and exercise caution in selecting neutral prompt language.
\\ ( https://arxiv.org/abs/2403.13840 ,  1048kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13901
Date: Wed, 20 Mar 2024 18:13:17 GMT   (1043kb,D)

Title: Train & Constrain: Phonologically Informed Tongue-Twister Generation
  from Topics and Paraphrases
Authors: Tyler Loakman, Chen Tang, Chenghua Lin
Categories: cs.CL
Comments: Submitted to Computational Linguistics
\\
  Previous work in phonologically and phonetically grounded language generation
has mainly focused on domains such as puns and poetry. In this article, we
present new work on the generation of tongue-twisters - a form of language that
is required to be conditioned on a phoneme level to maximize sound overlap,
whilst maintaining semantic consistency with an input topic and still being
grammatically correct. We present TwisterLister, a pipeline for generating
phonologically informed tongue-twisters from Large Language Models (LLMs) that
we use to generate TwistList 2.0, the largest annotated dataset of
tongue-twisters to date, consisting of 17K+ examples from a combination of
human and LLM authors. Our generation pipeline involves the use of a
phonologically constrained vocabulary alongside LLM prompting to generate
novel, non-derivative tongue-twister examples. We additionally present the
results of automatic and human evaluation of smaller models trained on our
generated dataset to demonstrate the extent to which phonologically motivated
language types can be generated without explicit injection of phonological
knowledge. Additionally, we introduce a Phoneme-Aware Constrained Decoding
module (PACD) that can be integrated into any causal language model and
demonstrate that this method generates good quality tongue-twisters both with
and without fine-tuning the underlying language model. We also design and
implement a range of automatic metrics for the task of tongue-twister
generation that is phonologically motivated and captures the unique essence of
tongue-twisters based on Phonemic Edit Distance (PED).
\\ ( https://arxiv.org/abs/2403.13901 ,  1043kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13903
Date: Wed, 20 Mar 2024 18:18:48 GMT   (1003kb,D)

Title: Leveraging Linguistically Enhanced Embeddings for Open Information
  Extraction
Authors: Fauzan Farooqui, Thanmay Jayakumar, Pulkit Mathur, Mansi Radke
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024 Main Conference, Long Paper
\\
  Open Information Extraction (OIE) is a structured prediction (SP) task in
Natural Language Processing (NLP) that aims to extract structured $n$-ary
tuples - usually subject-relation-object triples - from free text. The word
embeddings in the input text can be enhanced with linguistic features, usually
Part-of-Speech (PoS) and Syntactic Dependency Parse (SynDP) labels. However,
past enhancement techniques cannot leverage the power of pretrained language
models (PLMs), which themselves have been hardly used for OIE. To bridge this
gap, we are the first to leverage linguistic features with a Seq2Seq PLM for
OIE. We do so by introducing two methods - Weighted Addition and Linearized
Concatenation. Our work can give any neural OIE architecture the key
performance boost from both PLMs and linguistic features in one go. In our
settings, this shows wide improvements of up to 24.9%, 27.3% and 14.9% on
Precision, Recall and F1 scores respectively over the baseline. Beyond this, we
address other important challenges in the field: to reduce compute overheads
with the features, we are the first ones to exploit Semantic Dependency Parse
(SemDP) tags; to address flaws in current datasets, we create a clean synthetic
dataset; finally, we contribute the first known study of OIE behaviour in SP
models.
\\ ( https://arxiv.org/abs/2403.13903 ,  1003kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13922
Date: Wed, 20 Mar 2024 18:49:59 GMT   (1931kb,D)

Title: Visually Grounded Speech Models have a Mutual Exclusivity Bias
Authors: Leanne Nortje, Dan Onea\c{t}\u{a}, Yevgen Matusevych, Herman Kamper
Categories: cs.CL
Comments: Accepted to TACL, pre-MIT Press publication version
\\
  When children learn new words, they employ constraints such as the mutual
exclusivity (ME) bias: a novel word is mapped to a novel object rather than a
familiar one. This bias has been studied computationally, but only in models
that use discrete word representations as input, ignoring the high variability
of spoken words. We investigate the ME bias in the context of visually grounded
speech models that learn from natural images and continuous speech audio.
Concretely, we train a model on familiar words and test its ME bias by asking
it to select between a novel and a familiar object when queried with a novel
word. To simulate prior acoustic and visual knowledge, we experiment with
several initialisation strategies using pretrained speech and vision networks.
Our findings reveal the ME bias across the different initialisation approaches,
with a stronger bias in models with more prior (in particular, visual)
knowledge. Additional tests confirm the robustness of our results, even when
different loss functions are considered.
\\ ( https://arxiv.org/abs/2403.13922 ,  1931kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13925
Date: Wed, 20 Mar 2024 18:59:18 GMT   (99kb,D)

Title: Reducing Large Language Model Bias with Emphasis on 'Restricted
  Industries': Automated Dataset Augmentation and Prejudice Quantification
Authors: Devam Mondal, Carlo Lipizzi
Categories: cs.CL cs.AI cs.LG
\\
  Despite the growing capabilities of large language models, there exists
concerns about the biases they develop. In this paper, we propose a novel,
automated mechanism for debiasing through specified dataset augmentation in the
lens of bias producers and in the context of 'restricted industries' with
limited data. We additionally create two new additional metrics, the mb-index
and db-index, to quantify bias, considering the idea that bias occurs due to
both intrinsic model architecture and dataset.
\\ ( https://arxiv.org/abs/2403.13925 ,  99kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14001
Date: Wed, 20 Mar 2024 21:58:32 GMT   (1905kb,D)

Title: Evaluating Unsupervised Dimensionality Reduction Methods for Pretrained
  Sentence Embeddings
Authors: Gaifan Zhang, Yi Zhou, Danushka Bollegala
Categories: cs.CL
\\
  Sentence embeddings produced by Pretrained Language Models (PLMs) have
received wide attention from the NLP community due to their superior
performance when representing texts in numerous downstream applications.
However, the high dimensionality of the sentence embeddings produced by PLMs is
problematic when representing large numbers of sentences in memory- or
compute-constrained devices. As a solution, we evaluate unsupervised
dimensionality reduction methods to reduce the dimensionality of sentence
embeddings produced by PLMs. Our experimental results show that simple methods
such as Principal Component Analysis (PCA) can reduce the dimensionality of
sentence embeddings by almost $50\%$, without incurring a significant loss in
performance in multiple downstream tasks. Surprisingly, reducing the
dimensionality further improves performance over the original high-dimensional
versions for the sentence embeddings produced by some PLMs in some tasks.
\\ ( https://arxiv.org/abs/2403.14001 ,  1905kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14006
Date: Wed, 20 Mar 2024 22:11:01 GMT   (126kb,D)

Title: On Prompt Sensitivity of ChatGPT in Affective Computing
Authors: Mostafa M. Amin and Bj\"orn W. Schuller
Categories: cs.CL cs.AI
Comments: 2 Tables, 1 Figure, preprint submission to ACII 2024
\\
  Recent studies have demonstrated the emerging capabilities of foundation
models like ChatGPT in several fields, including affective computing. However,
accessing these emerging capabilities is facilitated through prompt
engineering. Despite the existence of some prompting techniques, the field is
still rapidly evolving and many prompting ideas still require investigation. In
this work, we introduce a method to evaluate and investigate the sensitivity of
the performance of foundation models based on different prompts or generation
parameters. We perform our evaluation on ChatGPT within the scope of affective
computing on three major problems, namely sentiment analysis, toxicity
detection, and sarcasm detection. First, we carry out a sensitivity analysis on
pivotal parameters in auto-regressive text generation, specifically the
temperature parameter $T$ and the top-$p$ parameter in Nucleus sampling,
dictating how conservative or creative the model should be during generation.
Furthermore, we explore the efficacy of several prompting ideas, where we
explore how giving different incentives or structures affect the performance.
Our evaluation takes into consideration performance measures on the affective
computing tasks, and the effectiveness of the model to follow the stated
instructions, hence generating easy-to-parse responses to be smoothly used in
downstream applications.
\\ ( https://arxiv.org/abs/2403.14006 ,  126kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14009
Date: Wed, 20 Mar 2024 22:14:39 GMT   (4637kb,D)

Title: A New Massive Multilingual Dataset for High-Performance Language
  Technologies
Authors: Ona de Gibert, Graeme Nail, Nikolay Arefyev, Marta Ba\~n\'on, Jelmer
  van der Linde, Shaoxiong Ji, Jaume Zaragoza-Bernabeu, Mikko Aulamo, Gema
  Ram\'irez-S\'anchez, Andrey Kutuzov, Sampo Pyysalo, Stephan Oepen, J\"org
  Tiedemann
Categories: cs.CL
Comments: LREC-COLING 2024
\\
  We present the HPLT (High Performance Language Technologies) language
resources, a new massive multilingual dataset including both monolingual and
bilingual corpora extracted from CommonCrawl and previously unused web crawls
from the Internet Archive. We describe our methods for data acquisition,
management and processing of large corpora, which rely on open-source software
tools and high-performance computing. Our monolingual collection focuses on
low- to medium-resourced languages and covers 75 languages and a total of ~5.6
trillion word tokens de-duplicated on the document level. Our English-centric
parallel corpus is derived from its monolingual counterpart and covers 18
language pairs and more than 96 million aligned sentence pairs with roughly 1.4
billion English tokens. The HPLT language resources are one of the largest open
text corpora ever released, providing a great resource for language modeling
and machine translation training. We publicly release the corpora, the
software, and the tools used in this work.
\\ ( https://arxiv.org/abs/2403.14009 ,  4637kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14037
Date: Wed, 20 Mar 2024 23:21:35 GMT   (1093kb)

Title: Ax-to-Grind Urdu: Benchmark Dataset for Urdu Fake News Detection
Authors: Sheetal Harris, Jinshuo Liu, Hassan Jalil Hadi, Yue Cao
Categories: cs.CL cs.AI
\\
  Misinformation can seriously impact society, affecting anything from public
opinion to institutional confidence and the political horizon of a state. Fake
News (FN) proliferation on online websites and Online Social Networks (OSNs)
has increased profusely. Various fact-checking websites include news in English
and barely provide information about FN in regional languages. Thus the Urdu FN
purveyors cannot be discerned using factchecking portals. SOTA approaches for
Fake News Detection (FND) count upon appropriately labelled and large datasets.
FND in regional and resource-constrained languages lags due to the lack of
limited-sized datasets and legitimate lexical resources. The previous datasets
for Urdu FND are limited-sized, domain-restricted, publicly unavailable and not
manually verified where the news is translated from English into Urdu. In this
paper, we curate and contribute the first largest publicly available dataset
for Urdu FND, Ax-to-Grind Urdu, to bridge the identified gaps and limitations
of existing Urdu datasets in the literature. It constitutes 10,083 fake and
real news on fifteen domains collected from leading and authentic Urdu
newspapers and news channel websites in Pakistan and India. FN for the
Ax-to-Grind dataset is collected from websites and crowdsourcing. The dataset
contains news items in Urdu from the year 2017 to the year 2023. Expert
journalists annotated the dataset. We benchmark the dataset with an ensemble
model of mBERT,XLNet, and XLM RoBERTa. The selected models are originally
trained on multilingual large corpora. The results of the proposed model are
based on performance metrics, F1-score, accuracy, precision, recall and MCC
value.
\\ ( https://arxiv.org/abs/2403.14037 ,  1093kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14072
Date: Thu, 21 Mar 2024 01:47:22 GMT   (6620kb,D)

Title: A Taxonomy of Ambiguity Types for NLP
Authors: Margaret Y. Li, Alisa Liu, Zhaofeng Wu, Noah A. Smith
Categories: cs.CL
Comments: To appear at the UnImplicit workshop at EACL 2024
\\
  Ambiguity is an critical component of language that allows for more effective
communication between speakers, but is often ignored in NLP. Recent work
suggests that NLP systems may struggle to grasp certain elements of human
language understanding because they may not handle ambiguities at the level
that humans naturally do in communication. Additionally, different types of
ambiguity may serve different purposes and require different approaches for
resolution, and we aim to investigate how language models' abilities vary
across types. We propose a taxonomy of ambiguity types as seen in English to
facilitate NLP analysis. Our taxonomy can help make meaningful splits in
language ambiguity data, allowing for more fine-grained assessments of both
datasets and model performance.
\\ ( https://arxiv.org/abs/2403.14072 ,  6620kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14112
Date: Thu, 21 Mar 2024 03:52:01 GMT   (1301kb,D)

Title: Benchmarking Chinese Commonsense Reasoning of LLMs: From
  Chinese-Specifics to Reasoning-Memorization Correlations
Authors: Jiaxing Sun, Weiquan Huang, Jiang Wu, Chenya Gu, Wei Li, Songyang
  Zhang, Hang Yan, Conghui He
Categories: cs.CL
Comments: Equal contribution: Jiaxing Sun, Weiquan Huang, Jiang Wu;
  Corresponding author: Conghui He
\\
  We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
\\ ( https://arxiv.org/abs/2403.14112 ,  1301kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14118
Date: Thu, 21 Mar 2024 04:07:40 GMT   (994kb,D)

Title: From Handcrafted Features to LLMs: A Brief Survey for Machine
  Translation Quality Estimation
Authors: Haofei Zhao, Yilun Liu, Shimin Tao, Weibin Meng, Yimeng Chen, Xiang
  Geng, Chang Su, Min Zhang, Hao Yang
Categories: cs.CL
Comments: Accepted by IJCNN 2024
\\
  Machine Translation Quality Estimation (MTQE) is the task of estimating the
quality of machine-translated text in real time without the need for reference
translations, which is of great importance for the development of MT. After two
decades of evolution, QE has yielded a wealth of results. This article provides
a comprehensive overview of QE datasets, annotation methods, shared tasks,
methodologies, challenges, and future research directions. It begins with an
introduction to the background and significance of QE, followed by an
explanation of the concepts and evaluation metrics for word-level QE,
sentence-level QE, document-level QE, and explainable QE. The paper categorizes
the methods developed throughout the history of QE into those based on
handcrafted features, deep learning, and Large Language Models (LLMs), with a
further division of deep learning-based methods into classic deep learning and
those incorporating pre-trained language models (LMs). Additionally, the
article details the advantages and limitations of each method and offers a
straightforward comparison of different approaches. Finally, the paper
discusses the current challenges in QE research and provides an outlook on
future research directions.
\\ ( https://arxiv.org/abs/2403.14118 ,  994kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14168
Date: Thu, 21 Mar 2024 06:43:59 GMT   (3227kb,D)

Title: M$^3$AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual
  Academic Lecture Dataset
Authors: Zhe Chen, Heyang Liu, Wenyi Yu, Guangzhi Sun, Hongcheng Liu, Ji Wu,
  Chao Zhang, Yu Wang, Yanfeng Wang
Categories: cs.CL
\\
  Publishing open-source academic video recordings is an emergent and prevalent
approach to sharing knowledge online. Such videos carry rich multimodal
information including speech, the facial and body movements of the speakers, as
well as the texts and pictures in the slides and possibly even the papers.
Although multiple academic video datasets have been constructed and released,
few of them support both multimodal content recognition and understanding
tasks, which is partially due to the lack of high-quality human annotations. In
this paper, we propose a novel multimodal, multigenre, and multipurpose
audio-visual academic lecture dataset (M$^3$AV), which has almost 367 hours of
videos from five sources covering computer science, mathematics, and medical
and biology topics. With high-quality human annotations of the spoken and
written words, in particular high-valued name entities, the dataset can be used
for multiple audio-visual recognition and understanding tasks. Evaluations
performed on contextual speech recognition, speech synthesis, and slide and
script generation tasks demonstrate that the diversity of M$^3$AV makes it a
challenging dataset.
\\ ( https://arxiv.org/abs/2403.14168 ,  3227kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14171
Date: Thu, 21 Mar 2024 06:47:28 GMT   (2312kb,D)

Title: MMIDR: Teaching Large Language Model to Interpret Multimodal
  Misinformation via Knowledge Distillation
Authors: Longzheng Wang, Xiaohan Xu, Lei Zhang, Jiarui Lu, Yongxiu Xu, Hongbo
  Xu, Chuang Zhang
Categories: cs.CL
Comments: 10 pages, 3 figures
\\
  Automatic detection of multimodal misinformation has gained a widespread
attention recently. However, the potential of powerful Large Language Models
(LLMs) for multimodal misinformation detection remains underexplored. Besides,
how to teach LLMs to interpret multimodal misinformation in cost-effective and
accessible way is still an open question. To address that, we propose MMIDR, a
framework designed to teach LLMs in providing fluent and high-quality textual
explanations for their decision-making process of multimodal misinformation. To
convert multimodal misinformation into an appropriate instruction-following
format, we present a data augmentation perspective and pipeline. This pipeline
consists of a visual information processing module and an evidence retrieval
module. Subsequently, we prompt the proprietary LLMs with processed contents to
extract rationales for interpreting the authenticity of multimodal
misinformation. Furthermore, we design an efficient knowledge distillation
approach to distill the capability of proprietary LLMs in explaining multimodal
misinformation into open-source LLMs. To explore several research questions
regarding the performance of LLMs in multimodal misinformation detection tasks,
we construct an instruction-following multimodal misinformation dataset and
conduct comprehensive experiments. The experimental findings reveal that our
MMIDR exhibits sufficient detection performance and possesses the capacity to
provide compelling rationales to support its assessments.
\\ ( https://arxiv.org/abs/2403.14171 ,  2312kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14197
Date: Thu, 21 Mar 2024 07:47:57 GMT   (817kb,D)

Title: Context Quality Matters in Training Fusion-in-Decoder for Extractive
  Open-Domain Question Answering
Authors: Kosuke Akimoto, Kunihiro Takeoka, Masafumi Oyamada
Categories: cs.CL
Comments: EMNLP Findings 2023
DOI: 10.18653/v1/2023.findings-emnlp.784
\\
  Retrieval-augmented generation models augment knowledge encoded in a language
model by providing additional relevant external knowledge (context) during
generation. Although it has been shown that the quantity and quality of context
impact the performance of retrieval-augmented generation models during
inference, limited research explores how these characteristics affect model
training. This paper explores how context quantity and quality during model
training affect the performance of Fusion-in-Decoder (FiD), the
state-of-the-art retrieval-augmented generation model, in extractive
open-domain question answering tasks. Experimental results suggest that FiD
models overfit to context quality during training and show suboptimal
performance when evaluated on different context quality. Through the
experimental results, we also reveal FiD models trained with different context
quality have different cross-attention distribution patterns. Specifically, as
context quality during training increases, FiD models tend to attend more
uniformly to each passage in context. Finally, based on these observations, we
propose a method to mitigate overfitting to specific context quality by
introducing bias to the cross-attention distribution, which we demonstrate to
be effective in improving the performance of FiD models on different context
quality.
\\ ( https://arxiv.org/abs/2403.14197 ,  817kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14208
Date: Thu, 21 Mar 2024 08:00:05 GMT   (892kb,D)

Title: Automatic Annotation of Grammaticality in Child-Caregiver Conversations
Authors: Mitja Nikolaus (ILCB, LPL, LIS, TALEP), Abhishek Agrawal (ILCB, LIS,
  TALEP), Petros Kaklamanis, Alex Warstadt (SED), Abdellah Fourtassi (ILCB,
  LIS, TALEP)
Categories: cs.CL
Journal-ref: LREC-Coling 2024, May 2024, Turin, Italy
\\
  The acquisition of grammar has been a central question to adjudicate between
theories of language acquisition. In order to conduct faster, more
reproducible, and larger-scale corpus studies on grammaticality in
child-caregiver conversations, tools for automatic annotation can offer an
effective alternative to tedious manual annotation. We propose a coding scheme
for context-dependent grammaticality in child-caregiver conversations and
annotate more than 4,000 utterances from a large corpus of transcribed
conversations. Based on these annotations, we train and evaluate a range of NLP
models. Our results show that fine-tuned Transformer-based models perform best,
achieving human inter-annotation agreement levels.As a first application and
sanity check of this tool, we use the trained models to annotate a corpus
almost two orders of magnitude larger than the manually annotated data and
verify that children's grammaticality shows a steady increase with age.This
work contributes to the growing literature on applying state-of-the-art NLP
methods to help study child language acquisition at scale.
\\ ( https://arxiv.org/abs/2403.14208 ,  892kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14221
Date: Thu, 21 Mar 2024 08:21:12 GMT   (3488kb,D)

Title: Improving the Robustness of Large Language Models via Consistency
  Alignment
Authors: Zhao Yukun, Yan Lingyong, Sun Weiwei, Xing Guoliang, Wang Shuaiqiang,
  Meng Chong, Cheng Zhicong, Ren Zhaochun, Yin Dawei
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\
  Large language models (LLMs) have shown tremendous success in following user
instructions and generating helpful responses. Nevertheless, their robustness
is still far from optimal, as they may generate significantly inconsistent
responses due to minor changes in the verbalized instructions. Recent
literature has explored this inconsistency issue, highlighting the importance
of continued improvement in the robustness of response generation. However,
systematic analysis and solutions are still lacking. In this paper, we
quantitatively define the inconsistency problem and propose a two-stage
training framework consisting of instruction-augmented supervised fine-tuning
and consistency alignment training. The first stage helps a model generalize on
following instructions via similar instruction augmentations. In the second
stage, we improve the diversity and help the model understand which responses
are more aligned with human expectations by differentiating subtle differences
in similar responses. The training process is accomplished by self-rewards
inferred from the trained model at the first stage without referring to
external human preference resources. We conduct extensive experiments on recent
publicly available LLMs on instruction-following tasks and demonstrate the
effectiveness of our training framework.
\\ ( https://arxiv.org/abs/2403.14221 ,  3488kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14222
Date: Thu, 21 Mar 2024 08:22:44 GMT   (5367kb,D)

Title: Large-Scale Label Interpretation Learning for Few-Shot Named Entity
  Recognition
Authors: Jonas Golde, Felix Hamborg, Alan Akbik
Categories: cs.CL
Comments: 8 pages
\\
  Few-shot named entity recognition (NER) detects named entities within text
using only a few annotated examples. One promising line of research is to
leverage natural language descriptions of each entity type: the common label
PER might, for example, be verbalized as ''person entity.'' In an initial label
interpretation learning phase, the model learns to interpret such verbalized
descriptions of entity types. In a subsequent few-shot tagset extension phase,
this model is then given a description of a previously unseen entity type (such
as ''music album'') and optionally a few training examples to perform few-shot
NER for this type. In this paper, we systematically explore the impact of a
strong semantic prior to interpret verbalizations of new entity types by
massively scaling up the number and granularity of entity types used for label
interpretation learning. To this end, we leverage an entity linking benchmark
to create a dataset with orders of magnitude of more distinct entity types and
descriptions as currently used datasets. We find that this increased signal
yields strong results in zero- and few-shot NER in in-domain, cross-domain, and
even cross-lingual settings. Our findings indicate significant potential for
improving few-shot NER through heuristical data-based optimization.
\\ ( https://arxiv.org/abs/2403.14222 ,  5367kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14238
Date: Thu, 21 Mar 2024 08:57:27 GMT   (1021kb,D)

Title: Reinforcement Learning from Reflective Feedback (RLRF): Aligning and
  Improving LLMs via Fine-Grained Self-Reflection
Authors: Kyungjae Lee, Dasol Hwang, Sunghyun Park, Youngsoo Jang, Moontae Lee
Categories: cs.CL cs.AI
Comments: 22 pages, 5 figures, Submitted to ACL 2024
\\
  Despite the promise of RLHF in aligning LLMs with human preferences, it often
leads to superficial alignment, prioritizing stylistic changes over improving
downstream performance of LLMs. Underspecified preferences could obscure
directions to align the models. Lacking exploration restricts identification of
desirable outputs to improve the models. To overcome these challenges, we
propose a novel framework: Reinforcement Learning from Reflective Feedback
(RLRF), which leverages fine-grained feedback based on detailed criteria to
improve the core capabilities of LLMs. RLRF employs a self-reflection mechanism
to systematically explore and refine LLM responses, then fine-tuning the models
via a RL algorithm along with promising responses. Our experiments across
Just-Eval, Factuality, and Mathematical Reasoning demonstrate the efficacy and
transformative potential of RLRF beyond superficial surface-level adjustment.
\\ ( https://arxiv.org/abs/2403.14238 ,  1021kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14243
Date: Thu, 21 Mar 2024 09:02:17 GMT   (4759kb,D)

Title: Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large
  Language Models with Machine Learning in tele-dermatology
Authors: Dimitrios P. Panagoulias and Evridiki Tsoureli-Nikita and Maria Virvou
  and George A. Tsihrintzis
Categories: cs.CL cs.AI cs.CV
\\
  The rise of Artificial Intelligence creates great promise in the field of
medical discovery, diagnostics and patient management. However, the vast
complexity of all medical domains require a more complex approach that combines
machine learning algorithms, classifiers, segmentation algorithms and, lately,
large language models. In this paper, we describe, implement and assess an
Artificial Intelligence-empowered system and methodology aimed at assisting the
diagnosis process of skin lesions and other skin conditions within the field of
dermatology that aims to holistically address the diagnostic process in this
domain. The workflow integrates large language, transformer-based vision models
and sophisticated machine learning tools. This holistic approach achieves a
nuanced interpretation of dermatological conditions that simulates and
facilitates a dermatologist's workflow. We assess our proposed methodology
through a thorough cross-model validation technique embedded in an evaluation
pipeline that utilizes publicly available medical case studies of skin
conditions and relevant images. To quantitatively score the system performance,
advanced machine learning and natural language processing tools are employed
which focus on similarity comparison and natural language inference.
Additionally, we incorporate a human expert evaluation process based on a
structured checklist to further validate our results. We implemented the
proposed methodology in a system which achieved approximate (weighted) scores
of 0.87 for both contextual understanding and diagnostic accuracy,
demonstrating the efficacy of our approach in enhancing dermatological
analysis. The proposed methodology is expected to prove useful in the
development of next-generation tele-dermatology applications, enhancing remote
consultation capabilities and access to care, especially in underserved areas.
\\ ( https://arxiv.org/abs/2403.14243 ,  4759kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14252
Date: Thu, 21 Mar 2024 09:25:24 GMT   (943kb,D)

Title: LayoutLLM: Large Language Model Instruction Tuning for Visually Rich
  Document Understanding
Authors: Masato Fujitake
Categories: cs.CL cs.AI cs.CV cs.LG
Comments: LREC-COLING 2024
\\
  This paper proposes LayoutLLM, a more flexible document analysis method for
understanding imaged documents. Visually Rich Document Understanding tasks,
such as document image classification and information extraction, have gained
significant attention due to their importance. Existing methods have been
developed to enhance document comprehension by incorporating pre-training
awareness of images, text, and layout structure. However, these methods require
fine-tuning for each task and dataset, and the models are expensive to train
and operate. To overcome this limitation, we propose a new LayoutLLM that
integrates these with large-scale language models (LLMs). By leveraging the
strengths of existing research in document image understanding and LLMs'
superior language understanding capabilities, the proposed model, fine-tuned
with multimodal instruction datasets, performs an understanding of document
images in a single model. Our experiments demonstrate improvement over the
baseline model in various document analysis tasks.
\\ ( https://arxiv.org/abs/2403.14252 ,  943kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14253
Date: Thu, 21 Mar 2024 09:26:04 GMT   (8595kb,D)

Title: K-Act2Emo: Korean Commonsense Knowledge Graph for Indirect Emotional
  Expression
Authors: Kyuhee Kim, Surin Lee and Sangah Lee
Categories: cs.CL
Comments: 10 pages
\\
  In many literary texts, emotions are indirectly conveyed through descriptions
of actions, facial expressions, and appearances, necessitating emotion
inference for narrative understanding. In this paper, we introduce K-Act2Emo, a
Korean commonsense knowledge graph (CSKG) comprising 1,900 indirect emotional
expressions and the emotions inferable from them. We categorize reasoning types
into inferences in positive situations, inferences in negative situations, and
inferences when expressions do not serve as emotional cues. Unlike existing
CSKGs, K-Act2Emo specializes in emotional contexts, and experimental results
validate its effectiveness for training emotion inference models.
Significantly, the BART-based knowledge model fine-tuned with K-Act2Emo
outperforms various existing Korean large language models, achieving
performance levels comparable to GPT-4 Turbo.
\\ ( https://arxiv.org/abs/2403.14253 ,  8595kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14255
Date: Thu, 21 Mar 2024 09:28:38 GMT   (350kb,D)

Title: ERD: A Framework for Improving LLM Reasoning for Cognitive Distortion
  Classification
Authors: Sehee Lim, Yejin Kim, Chi-Hyun Choi, Jy-yong Sohn, Byung-Hoon Kim
Categories: cs.CL cs.LG
\\
  Improving the accessibility of psychotherapy with the aid of Large Language
Models (LLMs) is garnering a significant attention in recent years. Recognizing
cognitive distortions from the interviewee's utterances can be an essential
part of psychotherapy, especially for cognitive behavioral therapy. In this
paper, we propose ERD, which improves LLM-based cognitive distortion
classification performance with the aid of additional modules of (1) extracting
the parts related to cognitive distortion, and (2) debating the reasoning steps
by multiple agents. Our experimental results on a public dataset show that ERD
improves the multi-class F1 score as well as binary specificity score.
Regarding the latter score, it turns out that our method is effective in
debiasing the baseline method which has high false positive rate, especially
when the summary of multi-agent debate is provided to LLMs.
\\ ( https://arxiv.org/abs/2403.14255 ,  350kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14258
Date: Thu, 21 Mar 2024 09:36:36 GMT   (800kb)

Title: LLM-based Extraction of Contradictions from Patents
Authors: Stefan Trapp and Joachim Warschat
Categories: cs.CL
Comments: 10 pages, 2 tables
ACM-class: I.2.7; H.3.1
\\
  Already since the 1950s TRIZ shows that patents and the technical
contradictions they solve are an important source of inspiration for the
development of innovative products. However, TRIZ is a heuristic based on a
historic patent analysis and does not make use of the ever-increasing number of
latest technological solutions in current patents. Because of the huge number
of patents, their length, and, last but not least, their complexity there is a
need for modern patent retrieval and patent analysis to go beyond
keyword-oriented methods. Recent advances in patent retrieval and analysis
mainly focus on dense vectors based on neural AI Transformer language models
like Google BERT. They are, for example, used for dense retrieval, question
answering or summarization and key concept extraction. A research focus within
the methods for patent summarization and key concept extraction are generic
inventive concepts respectively TRIZ concepts like problems, solutions,
advantage of invention, parameters, and contradictions. Succeeding rule-based
approaches, finetuned BERT-like language models for sentence-wise
classification represent the state-of-the-art of inventive concept extraction.
While they work comparatively well for basic concepts like problems or
solutions, contradictions - as a more complex abstraction - remain a challenge
for these models. This paper goes one step further, as it presents a method to
extract TRIZ contradictions from patent texts based on Prompt Engineering using
a generative Large Language Model (LLM), namely OpenAI's GPT-4. Contradiction
detection, sentence extraction, contradiction summarization, parameter
extraction and assignment to the 39 abstract TRIZ engineering parameters are
all performed in a single prompt using the LangChain framework. Our results
show that "off-the-shelf" GPT-4 is a serious alternative to existing
approaches.
\\ ( https://arxiv.org/abs/2403.14258 ,  800kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14275
Date: Thu, 21 Mar 2024 10:31:11 GMT   (210kb,D)

Title: Is Reference Necessary in the Evaluation of NLG Systems? When and Where?
Authors: Shuqian Sheng, Yi Xu, Luoyi Fu, Jiaxin Ding, Lei Zhou, Xinbing Wang,
  Chenghu Zhou
Categories: cs.CL
\\
  The majority of automatic metrics for evaluating NLG systems are
reference-based. However, the challenge of collecting human annotation results
in a lack of reliable references in numerous application scenarios. Despite
recent advancements in reference-free metrics, it has not been well understood
when and where they can be used as an alternative to reference-based metrics.
In this study, by employing diverse analytical approaches, we comprehensively
assess the performance of both metrics across a wide range of NLG tasks,
encompassing eight datasets and eight evaluation models. Based on solid
experiments, the results show that reference-free metrics exhibit a higher
correlation with human judgment and greater sensitivity to deficiencies in
language quality. However, their effectiveness varies across tasks and is
influenced by the quality of candidate texts. Therefore, it's important to
assess the performance of reference-free metrics before applying them to a new
task, especially when inputs are in uncommon form or when the answer space is
highly variable. Our study can provide insight into the appropriate application
of automatic metrics and the impact of metric choice on evaluation performance.
\\ ( https://arxiv.org/abs/2403.14275 ,  210kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14312
Date: Thu, 21 Mar 2024 11:34:26 GMT   (346kb,D)

Title: ChainLM: Empowering Large Language Models with Improved Chain-of-Thought
  Prompting
Authors: Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  Chain-of-Thought (CoT) prompting can enhance the reasoning capabilities of
large language models (LLMs), establishing itself as a primary approach to
solving complex reasoning tasks. Existing CoT synthesis approaches usually
focus on simpler reasoning tasks and thus result in low-quality and
inconsistent CoT prompts. In response to this challenge, we present an
empirical investigation of CoT prompting and introduce CoTGenius, a novel
framework designed for the automatic generation of superior CoT prompts.
CoTGenius is developed based on three major evolution strategies, i.e.,
complicate, diversify, and specify-alongside two filtering mechanisms:
evolutionary success judgement and correctness verification. We further employ
CoTGenius to create an extensive CoT dataset, and subsequently fine-tune the
Llama 2-Chat 7B and 13B models on this dataset. We call the resulting model
ChainLM. To deal with the cumulative error issue in reasoning steps, we propose
a step-level debating method, wherein multiple debaters discuss each reasoning
step to arrive at the correct answer. Extensive experiments demonstrate that
our ChainLM models exhibit enhanced proficiency in addressing a spectrum of
complex reasoning problems compared to existing models. In addition, we conduct
an in-depth analysis of the impact of data categories within CoTGenius on the
model performance. We release our dataset and code at
https://github.com/RUCAIBox/ChainLM.
\\ ( https://arxiv.org/abs/2403.14312 ,  346kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14341
Date: Thu, 21 Mar 2024 12:17:59 GMT   (9602kb,D)

Title: Beyond Surface Similarity: Detecting Subtle Semantic Shifts in Financial
  Narratives
Authors: Jiaxin Liu and Yi Yang and Kar Yan Tam
Categories: cs.CL
\\
  In this paper, we introduce the Financial-STS task, a financial
domain-specific NLP task designed to measure the nuanced semantic similarity
between pairs of financial narratives. These narratives originate from the
financial statements of the same company but correspond to different periods,
such as year-over-year comparisons. Measuring the subtle semantic differences
between these paired narratives enables market stakeholders to gauge changes
over time in the company's financial and operational situations, which is
critical for financial decision-making. We find that existing pretrained
embedding models and LLM embeddings fall short in discerning these subtle
financial narrative shifts. To address this gap, we propose an LLM-augmented
pipeline specifically designed for the Financial-STS task. Evaluation on a
human-annotated dataset demonstrates that our proposed method outperforms
existing methods trained on classic STS tasks and generic LLM embeddings.
\\ ( https://arxiv.org/abs/2403.14341 ,  9602kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14364
Date: Thu, 21 Mar 2024 12:45:12 GMT   (287kb)

Title: WikiFactDiff: A Large, Realistic, and Temporally Adaptable Dataset for
  Atomic Factual Knowledge Update in Causal Language Models
Authors: Hichem Ammar Khodja, Fr\'ed\'eric B\'echet, Quentin Brabant, Alexis
  Nasr, Gw\'enol\'e Lecorv\'e
Categories: cs.CL
Comments: Accepted for publication at LREC-COLING 2024
\\
  The factuality of large language model (LLMs) tends to decay over time since
events posterior to their training are "unknown" to them. One way to keep
models up-to-date could be factual update: the task of inserting, replacing, or
removing certain simple (atomic) facts within the model. To study this task, we
present WikiFactDiff, a dataset that describes the evolution of factual
knowledge between two dates as a collection of simple facts divided into three
categories: new, obsolete, and static. We describe several update scenarios
arising from various combinations of these three types of basic update. The
facts are represented by subject-relation-object triples; indeed, WikiFactDiff
was constructed by comparing the state of the Wikidata knowledge base at 4
January 2021 and 27 February 2023. Those fact are accompanied by verbalization
templates and cloze tests that enable running update algorithms and their
evaluation metrics. Contrary to other datasets, such as zsRE and CounterFact,
WikiFactDiff constitutes a realistic update setting that involves various
update scenarios, including replacements, archival, and new entity insertions.
We also present an evaluation of existing update algorithms on WikiFactDiff.
\\ ( https://arxiv.org/abs/2403.14364 ,  287kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14374
Date: Thu, 21 Mar 2024 13:05:18 GMT   (2040kb,D)

Title: FIT-RAG: Black-Box RAG with Factual Information and Token Reduction
Authors: Yuren Mao, Xuemei Dong, Wenyi Xu, Yunjun Gao, Bin Wei, Ying Zhang
Categories: cs.CL cs.IR
\\
  Due to the extraordinarily large number of parameters, fine-tuning Large
Language Models (LLMs) to update long-tail or out-of-date knowledge is
impractical in lots of applications. To avoid fine-tuning, we can alternatively
treat a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment
it with a Retrieval-Augmented Generation (RAG) system, namely black-box RAG.
Recently, black-box RAG has achieved success in knowledge-intensive tasks and
has gained much attention. Existing black-box RAG methods typically fine-tune
the retriever to cater to LLMs' preferences and concatenate all the retrieved
documents as the input, which suffers from two issues: (1) Ignorance of Factual
Information. The LLM preferred documents may not contain the factual
information for the given question, which can mislead the retriever and hurt
the effectiveness of black-box RAG; (2) Waste of Tokens. Simply concatenating
all the retrieved documents brings large amounts of unnecessary tokens for
LLMs, which degenerates the efficiency of black-box RAG. To address these
issues, this paper proposes a novel black-box RAG framework which utilizes the
factual information in the retrieval and reduces the number of tokens for
augmentation, dubbed FIT-RAG. FIT-RAG utilizes the factual information by
constructing a bi-label document scorer. Besides, it reduces the tokens by
introducing a self-knowledge recognizer and a sub-document-level token reducer.
FIT-RAG achieves both superior effectiveness and efficiency, which is validated
by extensive experiments across three open-domain question-answering datasets:
TriviaQA, NQ and PopQA. FIT-RAG can improve the answering accuracy of
Llama2-13B-Chat by 14.3\% on TriviaQA, 19.9\% on NQ and 27.5\% on PopQA,
respectively. Furthermore, it can save approximately half of the tokens on
average across the three datasets.
\\ ( https://arxiv.org/abs/2403.14374 ,  2040kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14381
Date: Thu, 21 Mar 2024 13:15:25 GMT   (595kb,D)

Title: Editing Knowledge Representation of Language Lodel via Rephrased Prefix
  Prompts
Authors: Yuchen Cai and Ding Cao and Rongxi Guo and Yaqin Wen and Guiquan Liu
  and Enhong Chen
Categories: cs.CL cs.AI
Comments: 19pages,3figures
\\
  Neural language models (LMs) have been extensively trained on vast corpora to
store factual knowledge about various aspects of the world described in texts.
Current technologies typically employ knowledge editing methods or specific
prompts to modify LM outputs. However, existing knowledge editing methods are
costly and inefficient, struggling to produce appropriate text. Additionally,
prompt engineering is opaque and requires significant effort to find suitable
prompts. To address these issues, we introduce a new method called PSPEM
(Prefix Soft Prompt Editing Method), that can be used for a lifetime with just
one training. It resolves the inefficiencies and generalizability issues in
knowledge editing methods and overcomes the opacity of prompt engineering by
automatically seeking optimal soft prompts. Specifically, PSPEM utilizes a
prompt encoder and an encoding converter to refine key information in prompts
and uses prompt alignment techniques to guide model generation, ensuring text
consistency and adherence to the intended structure and content, thereby
maintaining an optimal balance between efficiency and accuracy. We have
validated the effectiveness of PSPEM through knowledge editing and attribute
inserting. On the COUNTERFACT dataset, PSPEM achieved nearly 100\% editing
accuracy and demonstrated the highest level of fluency. We further analyzed the
similarities between PSPEM and original prompts and their impact on the model's
internals. The results indicate that PSPEM can serve as an alternative to
original prompts, supporting the model in effective editing.
\\ ( https://arxiv.org/abs/2403.14381 ,  595kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14390
Date: Thu, 21 Mar 2024 13:29:54 GMT   (8687kb,D)

Title: From Large to Tiny: Distilling and Refining Mathematical Expertise for
  Math Word Problems with Weakly Supervision
Authors: Qingwen Lin, Boyan Xu, Zhengting Huang, Ruichu Cai
Categories: cs.CL
\\
  Addressing the challenge of high annotation costs in solving Math Word
Problems (MWPs) through full supervision with intermediate equations, recent
works have proposed weakly supervised task settings that rely solely on the
final answer as a supervised signal. Existing leading approaches typically
employ various search techniques to infer intermediate equations, but cannot
ensure their semantic consistency with natural language descriptions. The rise
of Large Language Models (LLMs) like ChatGPT has opened up new possibilities
for addressing MWPs directly. However, the computational demands of LLMs make
them less than ideal for use in settings where resources are tight. In light of
these challenges, we introduce an innovative two-stage framework that adeptly
transfers mathematical Expertise from large to tiny language models. In
\emph{Distillation Stage}, we propose a series of extraction processes that
satisfy the properties of MWPs to distill mathematical knowledge from LLMs to
construct problem-equation pairs required for supervised training. In
\emph{Refinement Stage}, Due to Knowledge distilling method cannot guarantee
the full utilization of all data, we further utilize the unsuccessfully
searched data effectively by Knowledge Refine method. Finally, We train a small
model using distilled data generated through two-stage methods. As our method
fully leverages the semantic understanding capabilities during the searching
'problem-equation' pair, it demonstrates significantly improved performance on
the Math23K and Weak12K datasets compared to existing small model methods,
while maintaining a much lower computational cost than ChatGPT.
\\ ( https://arxiv.org/abs/2403.14390 ,  8687kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14399
Date: Thu, 21 Mar 2024 13:47:40 GMT   (901kb,D)

Title: Building Accurate Translation-Tailored LLMs with Language Aware
  Instruction Tuning
Authors: Changtong Zan, Liang Ding, Li Shen, Yibing Zhen, Weifeng Liu, Dacheng
  Tao
Categories: cs.CL cs.AI
\\
  Translation-tailored Large language models (LLMs) exhibit remarkable
translation capabilities, even competing with supervised-trained commercial
translation systems. However, off-target translation remains an unsolved
problem, especially for low-resource languages, hindering us from developing
accurate LLMs-based translation models. To mitigate the off-target translation
problem and enhance the performance of LLMs on translation, recent works have
either designed advanced prompting strategies to highlight the functionality of
translation instructions or exploited the in-context learning ability of LLMs
by feeding few-shot demonstrations. However, these methods essentially do not
improve LLM's ability to follow translation instructions, especially the
language direction information. In this work, we design a two-stage fine-tuning
algorithm to improve the instruction-following ability (especially the
translation direction) of LLMs. Specifically, we first tune LLMs with the
maximum likelihood estimation loss on the translation dataset to elicit the
basic translation capabilities. In the second stage, we construct
instruction-conflicting samples by randomly replacing the translation
directions with a wrong one within the instruction, and then introduce an extra
unlikelihood loss to learn those samples. Experiments on IWSLT and WMT
benchmarks upon the LLaMA model spanning 16 zero-shot directions show that,
compared to the competitive baseline -- translation-finetuned LLama, our method
could effectively reduce the off-target translation ratio (averagely -53.3\%),
thus improving translation quality with average +5.7 SacreBLEU and +16.4
BLEURT. Analysis shows that our method could preserve the model's general task
performance on AlpacaEval. Code and models will be released at
\url{https://github.com/alphadl/LanguageAware_Tuning}.
\\ ( https://arxiv.org/abs/2403.14399 ,  901kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14402
Date: Thu, 21 Mar 2024 13:52:17 GMT   (159kb,D)

Title: XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for
  Noise-Robust Speech Perception
Authors: HyoJung Han, Mohamed Anwar, Juan Pino, Wei-Ning Hsu, Marine Carpuat,
  Bowen Shi, Changhan Wang
Categories: cs.CL cs.SD eess.AS
\\
  Speech recognition and translation systems perform poorly on noisy inputs,
which are frequent in realistic environments. Augmenting these systems with
visual signals has the potential to improve robustness to noise. However,
audio-visual (AV) data is only available in limited amounts and for fewer
languages than audio-only resources. To address this gap, we present XLAVS-R, a
cross-lingual audio-visual speech representation model for noise-robust speech
recognition and translation in over 100 languages. It is designed to maximize
the benefits of limited multilingual AV pre-training data, by building on top
of audio-only multilingual pre-training and simplifying existing pre-training
schemes. Extensive evaluation on the MuAViC benchmark shows the strength of
XLAVS-R on downstream audio-visual speech recognition and translation tasks,
where it outperforms the previous state of the art by up to 18.5% WER and 4.7
BLEU given noisy AV inputs, and enables strong zero-shot audio-visual ability
with audio-only fine-tuning.
\\ ( https://arxiv.org/abs/2403.14402 ,  159kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14403
Date: Thu, 21 Mar 2024 13:52:30 GMT   (8186kb,D)

Title: Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language
  Models through Question Complexity
Authors: Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park
Categories: cs.CL cs.AI
Comments: NAACL 2024
\\
  Retrieval-Augmented Large Language Models (LLMs), which incorporate the
non-parametric knowledge from external knowledge bases into LLMs, have emerged
as a promising approach to enhancing response accuracy in several tasks, such
as Question-Answering (QA). However, even though there are various approaches
dealing with queries of different complexities, they either handle simple
queries with unnecessary computational overhead or fail to adequately address
complex multi-step queries; yet, not all user requests fall into only one of
the simple or complex categories. In this work, we propose a novel adaptive QA
framework, that can dynamically select the most suitable strategy for
(retrieval-augmented) LLMs from the simplest to the most sophisticated ones
based on the query complexity. Also, this selection process is operationalized
with a classifier, which is a smaller LM trained to predict the complexity
level of incoming queries with automatically collected labels, obtained from
actual predicted outcomes of models and inherent inductive biases in datasets.
This approach offers a balanced strategy, seamlessly adapting between the
iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval
methods, in response to a range of query complexities. We validate our model on
a set of open-domain QA datasets, covering multiple query complexities, and
show that ours enhances the overall efficiency and accuracy of QA systems,
compared to relevant baselines including the adaptive retrieval approaches.
Code is available at: https://github.com/starsuzi/Adaptive-RAG.
\\ ( https://arxiv.org/abs/2403.14403 ,  8186kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14409
Date: Thu, 21 Mar 2024 13:57:43 GMT   (1004kb,D)

Title: Locating and Mitigating Gender Bias in Large Language Models
Authors: Yuchen Cai and Ding Cao and Rongxi Guo and Yaqin Wen and Guiquan Liu
  and Enhong Chen
Categories: cs.CL cs.AI
Comments: 23 pages, 5 figures
\\
  Large language models(LLM) are pre-trained on extensive corpora to learn
facts and human cognition which contain human preferences. However, this
process can inadvertently lead to these models acquiring biases and stereotypes
prevalent in society. Prior research has typically tackled the issue of bias
through a one-dimensional perspective, concentrating either on locating or
mitigating it. This limited perspective has created obstacles in facilitating
research on bias to synergistically complement and progressively build upon one
another. In this study, we integrate the processes of locating and mitigating
bias within a unified framework. Initially, we use causal mediation analysis to
trace the causal effects of different components' activation within a large
language model. Building on this, we propose the LSDM (Least Square Debias
Method), a knowledge-editing based method for mitigating gender bias in
occupational pronouns, and compare it against two baselines on three gender
bias datasets and seven knowledge competency test datasets. The experimental
results indicate that the primary contributors to gender bias are the bottom
MLP modules acting on the last token of occupational pronouns and the top
attention module acting on the final word in the sentence. Furthermore, LSDM
mitigates gender bias in the model more effectively than the other baselines,
while fully preserving the model's capabilities in all other aspects.
\\ ( https://arxiv.org/abs/2403.14409 ,  1004kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14427
Date: Thu, 21 Mar 2024 14:33:34 GMT   (151kb,D)

Title: Emergent communication and learning pressures in language models: a
  language evolution perspective
Authors: Lukas Galke and Limor Raviv
Categories: cs.CL
Comments: 12 pages
ACM-class: I.2.7
\\
  Language models and humans are two types of learning systems. Finding or
facilitating commonalities could enable major breakthroughs in our
understanding of the acquisition and evolution of language. Many theories of
language evolution rely heavily on learning biases and learning pressures. Yet
due to substantial differences in learning pressures, it is questionable
whether the similarity between humans and machines is sufficient for insights
to carry over and to be worth testing with human participants. Here, we review
the emergent communication literature, a subfield of multi-agent reinforcement
learning, from a language evolution perspective. We find that the emergent
communication literature excels at designing and adapting models to recover
initially absent linguistic phenomena of natural languages. Based on a short
literature review, we identify key pressures that have recovered initially
absent human patterns in emergent communication models: communicative success,
efficiency, learnability, and other psycho-/sociolinguistic factors. We argue
that this may serve as inspiration for how to design language models for
language acquisition and language evolution research.
\\ ( https://arxiv.org/abs/2403.14427 ,  151kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14438
Date: Thu, 21 Mar 2024 14:44:03 GMT   (1819kb,D)

Title: A Multimodal Approach to Device-Directed Speech Detection with Large
  Language Models
Authors: Dominik Wager, Alexander Churchill, Siddharth Sigtia, Panayiotis
  Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi
Categories: cs.CL cs.LG eess.AS
Comments: arXiv admin note: text overlap with arXiv:2312.03632
DOI: 10.1109/ICASSP48485.2024.10446224
\\
  Interactions with virtual assistants typically start with a predefined
trigger phrase followed by the user command. To make interactions with the
assistant more intuitive, we explore whether it is feasible to drop the
requirement that users must begin each command with a trigger phrase. We
explore this task in three ways: First, we train classifiers using only
acoustic information obtained from the audio waveform. Second, we take the
decoder outputs of an automatic speech recognition (ASR) system, such as 1-best
hypotheses, as input features to a large language model (LLM). Finally, we
explore a multimodal system that combines acoustic and lexical features, as
well as ASR decoder signals in an LLM. Using multimodal information yields
relative equal-error-rate improvements over text-only and audio-only models of
up to 39% and 61%. Increasing the size of the LLM and training with low-rank
adaption leads to further relative EER reductions of up to 18% on our dataset.
\\ ( https://arxiv.org/abs/2403.14438 ,  1819kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14444
Date: Thu, 21 Mar 2024 14:51:51 GMT   (173kb,D)

Title: More than Just Statistical Recurrence: Human and Machine Unsupervised
  Learning of M\=aori Word Segmentation across Morphological Processes
Authors: Ashvini Varatharaj, Simon Todd
Categories: cs.CL
Comments: 10 pages, 1 Figure, 2 tables
\\
  Non-M\=aori-speaking New Zealanders (NMS)are able to segment M\=aori words in
a highlysimilar way to fluent speakers (Panther et al.,2024). This ability is
assumed to derive through the identification and extraction of statistically
recurrent forms. We examine this assumption by asking how NMS segmentations
compare to those produced by Morfessor, an unsupervised machine learning model
that operates based on statistical recurrence, across words formed by a variety
of morphological processes. Both NMS and Morfessor succeed in segmenting words
formed by concatenative processes (compounding and affixation without
allomorphy), but NMS also succeed for words that invoke templates
(reduplication and allomorphy) and other cues to morphological structure,
implying that their learning process is sensitive to more than just statistical
recurrence.
\\ ( https://arxiv.org/abs/2403.14444 ,  173kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14454
Date: Thu, 21 Mar 2024 15:02:03 GMT   (1452kb,D)

Title: Prediction of Translation Techniques for the Translation Process
Authors: Fan Zhou, Vincent Vandeghinste
Categories: cs.CL
Comments: 11 pages, 6 figures, conference
\\
  Machine translation (MT) encompasses a variety of methodologies aimed at
enhancing the accuracy of translations. In contrast, the process of
human-generated translation relies on a wide range of translation techniques,
which are crucial for ensuring linguistic adequacy and fluency. This study
suggests that these translation techniques could further optimize machine
translation if they are automatically identified before being applied to guide
the translation process effectively. The study differentiates between two
scenarios of the translation process: from-scratch translation and
post-editing. For each scenario, a specific set of experiments has been
designed to forecast the most appropriate translation techniques. The findings
indicate that the predictive accuracy for from-scratch translation reaches 82%,
while the post-editing process exhibits even greater potential, achieving an
accuracy rate of 93%.
\\ ( https://arxiv.org/abs/2403.14454 ,  1452kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14457
Date: Thu, 21 Mar 2024 15:04:32 GMT   (9283kb,D)

Title: gTBLS: Generating Tables from Text by Conditional Question Answering
Authors: Anirudh Sundar, Christopher Richardson, Larry Heck
Categories: cs.CL cs.LG
Comments: 12 pages, 1 figure
\\
  Distilling large, unstructured text into a structured, condensed form such as
tables is an open research problem. One of the primary challenges in
automatically generating tables is ensuring their syntactic validity. Prior
approaches address this challenge by including additional parameters in the
Transformer's attention mechanism to attend to specific rows and column
headers. In contrast to this single-stage method, this paper presents a
two-stage approach called Generative Tables (gTBLS). The first stage infers
table structure (row and column headers) from the text. The second stage
formulates questions using these headers and fine-tunes a causal language model
to answer them. Furthermore, the gTBLS approach is amenable to the utilization
of pre-trained Large Language Models in a zero-shot configuration, presenting a
solution for table generation in situations where fine-tuning is not feasible.
gTBLS improves prior approaches by up to 10% in BERTScore on the table
construction task and up to 20% on the table content generation task of the
E2E, WikiTableText, WikiBio, and RotoWire datasets.
\\ ( https://arxiv.org/abs/2403.14457 ,  9283kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14459
Date: Thu, 21 Mar 2024 15:06:14 GMT   (3545kb,D)

Title: Multi-Level Explanations for Generative Language Models
Authors: Lucas Monteiro Paes, Dennis Wei, Hyo Jin Do, Hendrik Strobelt, Ronny
  Luss, Amit Dhurandhar, Manish Nagireddy, Karthikeyan Natesan Ramamurthy,
  Prasanna Sattigeri, Werner Geyer, Soumya Ghosh
Categories: cs.CL cs.AI
\\
  Perturbation-based explanation methods such as LIME and SHAP are commonly
applied to text classification. This work focuses on their extension to
generative language models. To address the challenges of text as output and
long text inputs, we propose a general framework called MExGen that can be
instantiated with different attribution algorithms. To handle text output, we
introduce the notion of scalarizers for mapping text to real numbers and
investigate multiple possibilities. To handle long inputs, we take a
multi-level approach, proceeding from coarser levels of granularity to finer
ones, and focus on algorithms with linear scaling in model queries. We conduct
a systematic evaluation, both automated and human, of perturbation-based
attribution methods for summarization and context-grounded question answering.
The results show that our framework can provide more locally faithful
explanations of generated outputs.
\\ ( https://arxiv.org/abs/2403.14459 ,  3545kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14469
Date: Thu, 21 Mar 2024 15:16:50 GMT   (3800kb,D)

Title: ChatGPT Alternative Solutions: Large Language Models Survey
Authors: Hanieh Alipour, Nick Pendar, Kohinoor Roy
Categories: cs.CL cs.AI
Journal-ref: David C. Wyld et al. (Eds): NBIoT, MLCL, NMCO, ARIN, CSITA, ISPR,
  NATAP-2024. pp. 153-173, 2024. CS & IT - CSCP 2024
DOI: 10.5121/csit.2024.140514
\\
  In recent times, the grandeur of Large Language Models (LLMs) has not only
shone in the realm of natural language processing but has also cast its
brilliance across a vast array of applications. This remarkable display of LLM
capabilities has ignited a surge in research contributions within this domain,
spanning a diverse spectrum of topics. These contributions encompass
advancements in neural network architecture, context length enhancements, model
alignment, training datasets, benchmarking, efficiency improvements, and more.
Recent years have witnessed a dynamic synergy between academia and industry,
propelling the field of LLM research to new heights. A notable milestone in
this journey is the introduction of ChatGPT, a powerful AI chatbot grounded in
LLMs, which has garnered widespread societal attention. The evolving technology
of LLMs has begun to reshape the landscape of the entire AI community,
promising a revolutionary shift in the way we create and employ AI algorithms.
Given this swift-paced technical evolution, our survey embarks on a journey to
encapsulate the recent strides made in the world of LLMs. Through an
exploration of the background, key discoveries, and prevailing methodologies,
we offer an up-to-the-minute review of the literature. By examining multiple
LLM models, our paper not only presents a comprehensive overview but also
charts a course that identifies existing challenges and points toward potential
future research trajectories. This survey furnishes a well-rounded perspective
on the current state of generative AI, shedding light on opportunities for
further exploration, enhancement, and innovation.
\\ ( https://arxiv.org/abs/2403.14469 ,  3800kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14472
Date: Thu, 21 Mar 2024 15:18:30 GMT   (9785kb,D)

Title: Detoxifying Large Language Models via Knowledge Editing
Authors: Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi
  Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen
Categories: cs.CL cs.AI cs.CV cs.HC cs.LG
Comments: Ongoing work. Project website:
  https://zjunlp.github.io/project/SafeEdit Benchmark:
  https://huggingface.co/datasets/zjunlp/SafeEdit Code:
  https://github.com/zjunlp/EasyEdit
\\
  This paper investigates using knowledge editing techniques to detoxify Large
Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine
unsafe categories with various powerful attack prompts and equips comprehensive
metrics for systematic evaluation. We conduct experiments to compare knowledge
editing approaches with previous baselines, indicating that knowledge editing
has the potential to efficiently detoxify LLMs with limited impact on general
performance. Then, we propose a simple yet effective baseline, dubbed
Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the
toxicity of LLMs within a few tuning steps via only one instance. We further
provide an in-depth analysis of the internal mechanism for various detoxify
approaches, demonstrating that previous methods like SFT and DPO may merely
suppress the activations of toxic parameters, while DINM mitigates the toxicity
of the toxic parameters to a certain extent, making permanent adjustments. We
hope that these insights could shed light on future work of developing
detoxifying approaches and the underlying knowledge mechanisms of LLMs. Code
and benchmark are available at https://github.com/zjunlp/EasyEdit.
\\ ( https://arxiv.org/abs/2403.14472 ,  9785kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14515
Date: Thu, 21 Mar 2024 16:11:44 GMT   (543kb,D)

Title: Building a Language-Learning Game for Brazilian Indigenous Languages: A
  Case of Study
Authors: Gustavo Polleti
Categories: cs.CL
Comments: First Workshop on NLP for Indigenous Languages of Lusophone
  Countries, 16th International Conference on Computational Processing of
  Portuguese (PROPOR 2024)
\\
  In this paper we discuss a first attempt to build a language learning game
for brazilian indigenous languages and the challenges around it. We present a
design for the tool with gamification aspects. Then we describe a process to
automatically generate language exercises and questions from a dependency
treebank and a lexical database for Tupian languages. We discuss the
limitations of our prototype highlighting ethical and practical implementation
concerns. Finally, we conclude that new data gathering processes should be
established in partnership with indigenous communities and oriented for
educational purposes.
\\ ( https://arxiv.org/abs/2403.14515 ,  543kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14541
Date: Thu, 21 Mar 2024 16:41:12 GMT   (7827kb,D)

Title: EDT: Improving Large Language Models' Generation by Entropy-based
  Dynamic Temperature Sampling
Authors: Shimao Zhang, Yu Bao, Shujian Huang
Categories: cs.CL
\\
  Recently, Large Language Models (LLMs) have demonstrated outstanding
performance across a wide range of downstream language tasks. Temperature
sampling is a commonly used decoding strategy for LLMs' generation process.
However, a fixed temperature parameter is used in most cases, which may not
always be an optimal choice for balancing generation quality and diversity. In
this paper, we propose an effective Entropy-based Dynamic Temperature (EDT)
Sampling method, to achieve a more balanced performance in terms of both
generation quality and diversity by dynamically selecting the temperature
parameter. Additionally, we also show model performance and comprehensive
analyses for 4 different generation benchmarks. Our experiments show that EDT
significantly outperforms the existing strategies across different tasks.
\\ ( https://arxiv.org/abs/2403.14541 ,  7827kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14551
Date: Thu, 21 Mar 2024 16:52:01 GMT   (7661kb,D)

Title: Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling
Authors: Chengxu Zhuang, Evelina Fedorenko, Jacob Andreas
Categories: cs.CL cs.AI cs.LG
\\
  Today's most accurate language models are trained on orders of magnitude more
language data than human language learners receive - but with no supervision
from other sensory modalities that play a crucial role in human learning. Can
we make LMs' representations and predictions more accurate (and more
human-like) with more ecologically plausible supervision? This paper describes
LexiContrastive Grounding (LCG), a grounded language learning procedure that
leverages visual supervision to improve textual representations.
LexiContrastive Grounding combines a next token prediction strategy with a
contrastive visual grounding objective, focusing on early-layer representations
that encode lexical information. Across multiple word-learning and
sentence-understanding benchmarks, LexiContrastive Grounding not only
outperforms standard language-only models in learning efficiency, but also
improves upon vision-and-language learning procedures including CLIP, GIT,
Flamingo, and Vokenization. Moreover, LexiContrastive Grounding improves
perplexity by around 5% on multiple language modeling tasks. This work
underscores the potential of incorporating visual grounding into language
models, aligning more closely with the multimodal nature of human language
acquisition.
\\ ( https://arxiv.org/abs/2403.14551 ,  7661kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14562
Date: Thu, 21 Mar 2024 17:06:17 GMT   (8151kb,D)

Title: The Era of Semantic Decoding
Authors: Maxime Peyrard, Martin Josifoski, Robert West
Categories: cs.CL cs.AI cs.HC cs.MA
Comments: 25 pages, 3 figures
\\
  Recent work demonstrated great promise in the idea of orchestrating
collaborations between LLMs, human input, and various tools to address the
inherent limitations of LLMs. We propose a novel perspective called semantic
decoding, which frames these collaborative processes as optimization procedures
in semantic space. Specifically, we conceptualize LLMs as semantic processors
that manipulate meaningful pieces of information that we call semantic tokens
(known thoughts). LLMs are among a large pool of other semantic processors,
including humans and tools, such as search engines or code executors.
Collectively, semantic processors engage in dynamic exchanges of semantic
tokens to progressively construct high-utility outputs. We refer to these
orchestrated interactions among semantic processors, optimizing and searching
in semantic space, as semantic decoding algorithms. This concept draws a direct
parallel to the well-studied problem of syntactic decoding, which involves
crafting algorithms to best exploit auto-regressive language models for
extracting high-utility sequences of syntactic tokens. By focusing on the
semantic level and disregarding syntactic details, we gain a fresh perspective
on the engineering of AI systems, enabling us to imagine systems with much
greater complexity and capabilities. In this position paper, we formalize the
transition from syntactic to semantic tokens as well as the analogy between
syntactic and semantic decoding. Subsequently, we explore the possibilities of
optimizing within the space of semantic tokens via semantic decoding
algorithms. We conclude with a list of research opportunities and questions
arising from this fresh perspective. The semantic decoding perspective offers a
powerful abstraction for search and optimization directly in the space of
meaningful concepts, with semantic tokens as the fundamental units of a new
type of computation.
\\ ( https://arxiv.org/abs/2403.14562 ,  8151kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14565
Date: Thu, 21 Mar 2024 17:09:08 GMT   (1704kb,D)

Title: A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students'
  Formative Assessment Responses in Science
Authors: Clayton Cohn, Nicole Hutchins, Tuan Le, Gautam Biswas
Categories: cs.CL
Comments: In press at EAAI-24: The 14th Symposium on Educational Advances in
  Artificial Intelligence
\\
  This paper explores the use of large language models (LLMs) to score and
explain short-answer assessments in K-12 science. While existing methods can
score more structured math and computer science assessments, they often do not
provide explanations for the scores. Our study focuses on employing GPT-4 for
automated assessment in middle school Earth Science, combining few-shot and
active learning with chain-of-thought reasoning. Using a human-in-the-loop
approach, we successfully score and provide meaningful explanations for
formative assessment responses. A systematic analysis of our method's pros and
cons sheds light on the potential for human-in-the-loop techniques to enhance
automated grading for open-ended science assessments.
\\ ( https://arxiv.org/abs/2403.14565 ,  1704kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14582
Date: Thu, 21 Mar 2024 17:36:08 GMT   (265kb,D)

Title: Large Language Models for Multi-Choice Question Classification of
  Medical Subjects
Authors: V\'ictor Ponce-L\'opez
Categories: cs.CL cs.AI
\\
  The aim of this paper is to evaluate whether large language models trained on
multi-choice question data can be used to discriminate between medical
subjects. This is an important and challenging task for automatic question
answering. To achieve this goal, we train deep neural networks for multi-class
classification of questions into the inferred medical subjects. Using our
Multi-Question (MQ) Sequence-BERT method, we outperform the state-of-the-art
results on the MedMCQA dataset with an accuracy of 0.68 and 0.60 on their
development and test sets, respectively. In this sense, we show the capability
of AI and LLMs in particular for multi-classification tasks in the Healthcare
domain.
\\ ( https://arxiv.org/abs/2403.14582 ,  265kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13819
Date: Thu, 29 Feb 2024 10:05:37 GMT   (5601kb,D)

Title: A machine learning approach to predict university enrolment choices
  through students' high school background in Italy
Authors: Andrea Priulla, Alessandro Albano, Nicoletta D'Angelo, Massimo
  Attanasio
Categories: cs.LG stat.AP stat.OT
\\
  This paper explores the influence of Italian high school students'
proficiency in mathematics and the Italian language on their university
enrolment choices, specifically focusing on STEM (Science, Technology,
Engineering, and Mathematics) courses. We distinguish between students from
scientific and humanistic backgrounds in high school, providing valuable
insights into their enrolment preferences. Furthermore, we investigate
potential gender differences in response to similar previous educational
choices and achievements. The study employs gradient boosting methodology,
known for its high predicting performance and ability to capture non-linear
relationships within data, and adjusts for variables related to the
socio-demographic characteristics of the students and their previous
educational achievements. Our analysis reveals significant differences in the
enrolment choices based on previous high school achievements. The findings shed
light on the complex interplay of academic proficiency, gender, and high school
background in shaping students' choices regarding university education, with
implications for educational policy and future research endeavours.
\\ ( https://arxiv.org/abs/2403.13819 ,  5601kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13820
Date: Sat, 2 Mar 2024 17:18:40 GMT   (13611kb,D)

Title: Identity information based on human magnetocardiography signals
Authors: Pengju Zhang, Chenxi Sun, Jianwei Zhang and Hong Guo
Categories: cs.LG cs.CR
Comments: 7 pages, 5 figures. Author manuscript accepted for AAAI 2024 Spring
  Symposium on Clinical Foundation Models
\\
  We have developed an individual identification system based on
magnetocardiography (MCG) signals captured using optically pumped magnetometers
(OPMs). Our system utilizes pattern recognition to analyze the signals obtained
at different positions on the body, by scanning the matrices composed of MCG
signals with a 2*2 window. In order to make use of the spatial information of
MCG signals, we transform the signals from adjacent small areas into four
channels of a dataset. We further transform the data into time-frequency
matrices using wavelet transforms and employ a convolutional neural network
(CNN) for classification. As a result, our system achieves an accuracy rate of
97.04% in identifying individuals. This finding indicates that the MCG signal
holds potential for use in individual identification systems, offering a
valuable tool for personalized healthcare management.
\\ ( https://arxiv.org/abs/2403.13820 ,  13611kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13821
Date: Mon, 4 Mar 2024 03:09:25 GMT   (1195kb,D)

Title: Offensive Lineup Analysis in Basketball with Clustering Players Based on
  Shooting Style and Offensive Role
Authors: Kazuhiro Yamada, Keisuke Fujii
Categories: cs.LG stat.AP
Comments: 28 pages
\\
  In a basketball game, scoring efficiency holds significant importance due to
the numerous offensive possessions per game. Enhancing scoring efficiency
necessitates effective collaboration among players with diverse playing styles.
In previous studies, basketball lineups have been analyzed, but their playing
style compatibility has not been quantitatively examined. The purpose of this
study is to analyze more specifically the impact of playing style compatibility
on scoring efficiency, focusing only on offense. This study employs two methods
to capture the playing styles of players on offense: shooting style clustering
using tracking data, and offensive role clustering based on annotated playtypes
and advanced statistics. For the former, interpretable hand-crafted shot
features and Wasserstein distances between shooting style distributions were
utilized. For the latter, soft clustering was applied to playtype data for the
first time. Subsequently, based on the lineup information derived from these
two clusterings, machine learning models Bayesian models that predict
statistics representing scoring efficiency were trained and interpreted. These
approaches provide insights into which combinations of five players tend to be
effective and which combinations of two players tend to produce good effects.
\\ ( https://arxiv.org/abs/2403.13821 ,  1195kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13834
Date: Sun, 10 Mar 2024 00:43:36 GMT   (140kb,D)

Title: Few-shot Learning on Heterogeneous Graphs: Challenges, Progress, and
  Prospects
Authors: Pengfei Ding and Yan Wang and Guanfeng Liu
Categories: cs.LG
\\
  Few-shot learning on heterogeneous graphs (FLHG) is attracting more attention
from both academia and industry because prevailing studies on heterogeneous
graphs often suffer from label sparsity. FLHG aims to tackle the performance
degradation in the face of limited annotated data and there have been numerous
recent studies proposing various methods and applications. In this paper, we
provide a comprehensive review of existing FLHG methods, covering challenges,
research progress, and future prospects. Specifically, we first formalize FLHG
and categorize its methods into three types: single-heterogeneity FLHG,
dual-heterogeneity FLHG, and multi-heterogeneity FLHG. Then, we analyze the
research progress within each category, highlighting the most recent and
representative developments. Finally, we identify and discuss promising
directions for future research in FLHG. To the best of our knowledge, this
paper is the first systematic and comprehensive review of FLHG.
\\ ( https://arxiv.org/abs/2403.13834 ,  140kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13835
Date: Mon, 11 Mar 2024 17:45:47 GMT   (200kb,D)

Title: SMART: Automatically Scaling Down Language Models with Accuracy
  Guarantees for Reduced Processing Fees
Authors: Saehan Jo and Immanuel Trummer
Categories: cs.LG cs.AI cs.CL cs.DB
\\
  The advancement of Large Language Models (LLMs) has significantly boosted
performance in natural language processing (NLP) tasks. However, the deployment
of high-performance LLMs incurs substantial costs, primarily due to the
increased number of parameters aimed at enhancing model performance. This has
made the use of state-of-the-art LLMs more expensive for end-users. AI service
providers, such as OpenAI and Anthropic, often offer multiple versions of LLMs
with varying prices and performance. However, end-users still face challenges
in choosing the appropriate LLM for their tasks that balance result quality
with cost.
  We introduce SMART, Scaling Models Adaptively for Reduced Token Fees, a novel
LLM framework designed to minimize the inference costs of NLP tasks while
ensuring sufficient result quality. It enables users to specify an accuracy
constraint in terms of the equivalence of outputs to those of the most powerful
LLM. SMART then generates results that deviate from the outputs of this LLM
only with a probability below a user-defined threshold. SMART employs a
profiling phase that evaluates the performance of multiple LLMs to identify
those that meet the user-defined accuracy level. SMART optimizes the tradeoff
between profiling overheads and the anticipated cost savings resulting from
profiling. Moreover, our approach significantly reduces inference costs by
strategically leveraging a mix of LLMs. Our experiments on three real-world
datasets show that, based on OpenAI models, SMART achieves significant cost
savings, up to 25.6x in comparison to GPT-4.
\\ ( https://arxiv.org/abs/2403.13835 ,  200kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13836
Date: Tue, 12 Mar 2024 01:16:29 GMT   (6440kb,D)

Title: Tree-based Learning for High-Fidelity Prediction of Chaos
Authors: Adam Giammarese, Kamal Rana, Erik M. Bollt, Nishant Malik
Categories: cs.LG math.DS nlin.CD physics.data-an stat.ML
\\
  Model-free forecasting of the temporal evolution of chaotic systems is
crucial but challenging. Existing solutions require hyperparameter tuning,
significantly hindering their wider adoption. In this work, we introduce a
tree-based approach not requiring hyperparameter tuning: TreeDOX. It uses time
delay overembedding as explicit short-term memory and Extra-Trees Regressors to
perform feature reduction and forecasting. We demonstrate the state-of-the-art
performance of TreeDOX using the Henon map, Lorenz and Kuramoto-Sivashinsky
systems, and the real-world Southern Oscillation Index.
\\ ( https://arxiv.org/abs/2403.13836 ,  6440kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13838
Date: Thu, 14 Mar 2024 03:24:14 GMT   (485kb,D)

Title: Circuit Transformer: End-to-end Circuit Design by Predicting the Next
  Gate
Authors: Xihan Li, Xing Li, Lei Chen, Xing Zhang, Mingxuan Yuan, Jun Wang
Categories: cs.LG cs.AR cs.LO
\\
  Language, a prominent human ability to express through sequential symbols,
has been computationally mastered by recent advances of large language models
(LLMs). By predicting the next word recurrently with huge neural models, LLMs
have shown unprecedented capabilities in understanding and reasoning. Circuit,
as the "language" of electronic design, specifies the functionality of an
electronic device by cascade connections of logic gates. Then, can circuits
also be mastered by a a sufficiently large "circuit model", which can conquer
electronic design tasks by simply predicting the next logic gate? In this work,
we take the first step to explore such possibilities. Two primary barriers
impede the straightforward application of LLMs to circuits: their complex,
non-sequential structure, and the intolerance of hallucination due to strict
constraints (e.g., equivalence). For the first barrier, we encode a circuit as
a memory-less, depth-first traversal trajectory, which allows Transformer-based
neural models to better leverage its structural information, and predict the
next gate on the trajectory as a circuit model. For the second barrier, we
introduce an equivalence-preserving decoding process, which ensures that every
token in the generated trajectory adheres to the specified equivalence
constraints. Moreover, the circuit model can also be regarded as a stochastic
policy to tackle optimization-oriented circuit design tasks. Experimentally, we
trained a Transformer-based model of 88M parameters, named "Circuit
Transformer", which demonstrates impressive performance in end-to-end logic
synthesis. With Monte-Carlo tree search, Circuit Transformer significantly
improves over resyn2 while retaining strict equivalence, showcasing the
potential of generative AI in conquering electronic design challenges.
\\ ( https://arxiv.org/abs/2403.13838 ,  485kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13839
Date: Thu, 14 Mar 2024 16:17:14 GMT   (276kb,D)

Title: depyf: Open the Opaque Box of PyTorch Compiler for Machine Learning
  Researchers
Authors: Kaichao You, Runsheng Bai, Meng Cao, Jianmin Wang, Ion Stoica,
  Mingsheng Long
Categories: cs.LG cs.AI cs.PL
Comments: 16 pages, 2 figures
\\
  PyTorch \texttt{2.x} introduces a compiler designed to accelerate deep
learning programs. However, for machine learning researchers, adapting to the
PyTorch compiler to full potential can be challenging. The compiler operates at
the Python bytecode level, making it appear as an opaque box. To address this,
we introduce \texttt{depyf}, a tool designed to demystify the inner workings of
the PyTorch compiler. \texttt{depyf} decompiles bytecode generated by PyTorch
back into equivalent source code, and establishes connections between in-memory
code objects and their on-disk source code counterparts. This feature enables
users to step through the source code line by line using debuggers, thus
enhancing their understanding of the underlying processes. Notably,
\texttt{depyf} is non-intrusive and user-friendly, primarily relying on two
convenient context managers for its core functionality. The project is
\href{https://github.com/thuml/depyf}{ openly available} and is recognized as a
\href{https://pytorch.org/ecosystem/}{PyTorch ecosystem project}.
\\ ( https://arxiv.org/abs/2403.13839 ,  276kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13841
Date: Sat, 16 Mar 2024 17:24:38 GMT   (4389kb,D)

Title: Integrating Wearable Sensor Data and Self-reported Diaries for
  Personalized Affect Forecasting
Authors: Zhongqi Yang, Yuning Wang, Ken S. Yamashita, Maryam Sabah, Elahe
  Khatibi, Iman Azimi, Nikil Dutt, Jessica L. Borelli, and Amir M. Rahmani
Categories: cs.LG cs.AI
Comments: Accepted by Connected Health: Applications, Systems and Engineering
  Technologies (CHASE) 2024
\\
  Emotional states, as indicators of affect, are pivotal to overall health,
making their accurate prediction before onset crucial. Current studies are
primarily centered on immediate short-term affect detection using data from
wearable and mobile devices. These studies typically focus on objective sensory
measures, often neglecting other forms of self-reported information like
diaries and notes. In this paper, we propose a multimodal deep learning model
for affect status forecasting. This model combines a transformer encoder with a
pre-trained language model, facilitating the integrated analysis of objective
metrics and self-reported diaries. To validate our model, we conduct a
longitudinal study, enrolling college students and monitoring them over a year,
to collect an extensive dataset including physiological, environmental, sleep,
metabolic, and physical activity parameters, alongside open-ended textual
diaries provided by the participants. Our results demonstrate that the proposed
model achieves predictive accuracy of 82.50% for positive affect and 82.76% for
negative affect, a full week in advance. The effectiveness of our model is
further elevated by its explainability.
\\ ( https://arxiv.org/abs/2403.13841 ,  4389kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13842
Date: Sun, 17 Mar 2024 11:48:40 GMT   (1169kb)

Title: Analyzing the Variations in Emergency Department Boarding and Testing
  the Transferability of Forecasting Models across COVID-19 Pandemic Waves in
  Hong Kong: Hybrid CNN-LSTM approach to quantifying building-level
  socioecological risk
Authors: Eman Leung (1), Jingjing Guan (1), Kin On Kwok (1), CT Hung (1), CC.
  Ching (1), CK. Chung (1), Hector Tsang (2), EK Yeoh (1), Albert Lee (1) ((1)
  JC School of Public Health and Primary Care, The Chinese University of Hong
  Kong, (2) Department of Rehabilitation Science, Hong Kong Polytechnic
  University)
Categories: cs.LG
\\
  Emergency department's (ED) boarding (defined as ED waiting time greater than
four hours) has been linked to poor patient outcomes and health system
performance. Yet, effective forecasting models is rare before COVID-19, lacking
during the peri-COVID era. Here, a hybrid convolutional neural network
(CNN)-Long short-term memory (LSTM) model was applied to public-domain data
sourced from Hong Kong's Hospital Authority, Department of Health, and Housing
Authority. In addition, we sought to identify the phase of the COVID-19
pandemic that most significantly perturbed our complex adaptive healthcare
system, thereby revealing a stable pattern of interconnectedness among its
components, using deep transfer learning methodology.
  Our result shows that 1) the greatest proportion of days with ED boarding was
found between waves four and five; 2) the best-performing model for forecasting
ED boarding was observed between waves four and five, which was based on
features representing time-invariant residential buildings' built environment
and sociodemographic profiles and the historical time series of ED boarding and
case counts, compared to during the waves when best-performing forecasting is
based on time-series features alone; and 3) when the model built from the
period between waves four and five was applied to data from other waves via
deep transfer learning, the transferred model enhanced the performance of
indigenous models.
\\ ( https://arxiv.org/abs/2403.13842 ,  1169kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13843
Date: Sun, 17 Mar 2024 17:45:04 GMT   (4033kb,D)

Title: Machine Learning and Vision Transformers for Thyroid Carcinoma
  Diagnosis: A review
Authors: Yassine Habchi, Hamza Kheddar, Yassine Himeur, Abdelkrim Boukabou,
  Ammar Chouchane, Abdelmalik Ouamane, Shadi Atalla, Wathiq Mansoor
Categories: cs.LG cs.AI eess.IV
\\
  The growing interest in developing smart diagnostic systems to help medical
experts process extensive data for treating incurable diseases has been
notable. In particular, the challenge of identifying thyroid cancer (TC) has
seen progress with the use of machine learning (ML) and big data analysis,
incorporating transformers to evaluate TC prognosis and determine the risk of
malignancy in individuals. This review article presents a summary of various
studies on AIbased approaches, especially those employing transformers, for
diagnosing TC. It introduces a new categorization system for these methods
based on artifcial intelligence (AI) algorithms, the goals of the framework,
and the computing environments used. Additionally, it scrutinizes and contrasts
the available TC datasets by their features. The paper highlights the
importance of AI instruments in aiding the diagnosis and treatment of TC
through supervised, unsupervised, or mixed approaches, with a special focus on
the ongoing importance of transformers in medical diagnostics and disease
management. It further discusses the progress made and the continuing obstacles
in this area. Lastly, it explores future directions and focuses within this
research feld.
\\ ( https://arxiv.org/abs/2403.13843 ,  4033kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13844
Date: Mon, 18 Mar 2024 01:06:29 GMT   (7745kb,D)

Title: Scheduled Knowledge Acquisition on Lightweight Vector Symbolic
  Architectures for Brain-Computer Interfaces
Authors: Yejia Liu, Shijin Duan, Xiaolin Xu, Shaolei Ren
Categories: cs.LG cs.AI
Comments: Accepted as a full paper by the tinyML Research Symposium 2024
\\
  Brain-Computer interfaces (BCIs) are typically designed to be lightweight and
responsive in real-time to provide users timely feedback. Classical feature
engineering is computationally efficient but has low accuracy, whereas the
recent neural networks (DNNs) improve accuracy but are computationally
expensive and incur high latency. As a promising alternative, the
low-dimensional computing (LDC) classifier based on vector symbolic
architecture (VSA), achieves small model size yet higher accuracy than
classical feature engineering methods. However, its accuracy still lags behind
that of modern DNNs, making it challenging to process complex brain signals. To
improve the accuracy of a small model, knowledge distillation is a popular
method. However, maintaining a constant level of distillation between the
teacher and student models may not be the best way for a growing student during
its progressive learning stages. In this work, we propose a simple scheduled
knowledge distillation method based on curriculum data order to enable the
student to gradually build knowledge from the teacher model, controlled by an
$\alpha$ scheduler. Meanwhile, we employ the LDC/VSA as the student model to
enhance the on-device inference efficiency for tiny BCI devices that demand low
latency. The empirical results have demonstrated that our approach achieves
better tradeoff between accuracy and hardware efficiency compared to other
methods.
\\ ( https://arxiv.org/abs/2403.13844 ,  7745kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13845
Date: Mon, 18 Mar 2024 02:50:42 GMT   (3716kb,D)

Title: Learning to better see the unseen: Broad-Deep Mixed Anti-Forgetting
  Framework for Incremental Zero-Shot Fault Diagnosis
Authors: Jiancheng Zhao, Jiaqi Yue, Chunhui Zhao
Categories: cs.LG cs.AI
\\
  Zero-shot fault diagnosis (ZSFD) is capable of identifying unseen faults via
predicting fault attributes labeled by human experts. We first recognize the
demand of ZSFD to deal with continuous changes in industrial processes, i.e.,
the model's ability to adapt to new fault categories and attributes while
avoiding forgetting the diagnosis ability learned previously. To overcome the
issue that the existing ZSFD paradigm cannot learn from evolving streams of
training data in industrial scenarios, the incremental ZSFD (IZSFD) paradigm is
proposed for the first time, which incorporates category increment and
attribute increment for both traditional ZSFD and generalized ZSFD paradigms.
To achieve IZSFD, we present a broad-deep mixed anti-forgetting framework
(BDMAFF) that aims to learn from new fault categories and attributes. To tackle
the issue of forgetting, BDMAFF effectively accumulates previously acquired
knowledge from two perspectives: features and attribute prototypes. The feature
memory is established through a deep generative model that employs
anti-forgetting training strategies, ensuring the generation quality of
historical categories is supervised and maintained. The diagnosis model SEEs
the UNSEEN faults with the help of generated samples from the generative model.
The attribute prototype memory is established through a diagnosis model
inspired by the broad learning system. Unlike traditional incremental learning
algorithms, BDMAFF introduces a memory-driven iterative update strategy for the
diagnosis model, which allows the model to learn new faults and attributes
without requiring the storage of all historical training samples. The
effectiveness of the proposed method is verified by a real hydraulic system and
the Tennessee-Eastman benchmark process.
\\ ( https://arxiv.org/abs/2403.13845 ,  3716kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13846
Date: Mon, 18 Mar 2024 05:18:19 GMT   (1080kb,D)

Title: A Clustering Method with Graph Maximum Decoding Information
Authors: Xinrun Xu, Manying Lv, Yurong Wu, Zhanbiao Lian, Zhiming Ding, Jin
  Yan, Shan Jiang
Categories: cs.LG cs.AI
Comments: 9 pages, 9 figures, IJCNN 2024
\\
  The clustering method based on graph models has garnered increased attention
for its widespread applicability across various knowledge domains. Its
adaptability to integrate seamlessly with other relevant applications endows
the graph model-based clustering analysis with the ability to robustly extract
"natural associations" or "graph structures" within datasets, facilitating the
modelling of relationships between data points. Despite its efficacy, the
current clustering method utilizing the graph-based model overlooks the
uncertainty associated with random walk access between nodes and the embedded
structural information in the data. To address this gap, we present a novel
Clustering method for Maximizing Decoding Information within graph-based
models, named CMDI. CMDI innovatively incorporates two-dimensional structural
information theory into the clustering process, consisting of two phases: graph
structure extraction and graph vertex partitioning. Within CMDI, graph
partitioning is reformulated as an abstract clustering problem, leveraging
maximum decoding information to minimize uncertainty associated with random
visits to vertices. Empirical evaluations on three real-world datasets
demonstrate that CMDI outperforms classical baseline methods, exhibiting a
superior decoding information ratio (DI-R). Furthermore, CMDI showcases
heightened efficiency, particularly when considering prior knowledge (PK).
These findings underscore the effectiveness of CMDI in enhancing decoding
information quality and computational efficiency, positioning it as a valuable
tool in graph-based clustering analyses.
\\ ( https://arxiv.org/abs/2403.13846 ,  1080kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13847
Date: Mon, 18 Mar 2024 09:32:33 GMT   (4641kb,D)

Title: Optimal Transport for Domain Adaptation through Gaussian Mixture Models
Authors: Eduardo Fernandes Montesuma, Fred Maurice Ngol\`e Mboula, Antoine
  Souloumiac
Categories: cs.LG cs.AI
Comments: 10 pages,5 figures,under review
\\
  In this paper we explore domain adaptation through optimal transport. We
propose a novel approach, where we model the data distributions through
Gaussian mixture models. This strategy allows us to solve continuous optimal
transport through an equivalent discrete problem. The optimal transport
solution gives us a matching between source and target domain mixture
components. From this matching, we can map data points between domains, or
transfer the labels from the source domain components towards the target
domain. We experiment with 2 domain adaptation benchmarks in fault diagnosis,
showing that our methods have state-of-the-art performance.
\\ ( https://arxiv.org/abs/2403.13847 ,  4641kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13848
Date: Mon, 18 Mar 2024 10:44:22 GMT   (968kb,D)

Title: Smooth Sensitivity for Learning Differentially-Private yet Accurate Rule
  Lists
Authors: Timoth\'ee Ly (LAAS-ROC), Julien Ferry (EPM), Marie-Jos\'e Huguet
  (LAAS-ROC), S\'ebastien Gambs (UQAM), Ulrich Aivodji (ETS)
Categories: cs.LG cs.AI cs.CR
\\
  Differentially-private (DP) mechanisms can be embedded into the design of a
machine learningalgorithm to protect the resulting model against privacy
leakage, although this often comes with asignificant loss of accuracy. In this
paper, we aim at improving this trade-off for rule lists modelsby establishing
the smooth sensitivity of the Gini impurity and leveraging it to propose a DP
greedyrule list algorithm. In particular, our theoretical analysis and
experimental results demonstrate thatthe DP rule lists models integrating
smooth sensitivity have higher accuracy that those using otherDP frameworks
based on global sensitivity.
\\ ( https://arxiv.org/abs/2403.13848 ,  968kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13849
Date: Mon, 18 Mar 2024 14:37:27 GMT   (869kb)

Title: Graphs Unveiled: Graph Neural Networks and Graph Generation
Authors: L\'aszl\'o Kov\'acs, Ali Jlidi
Categories: cs.LG cs.AI
DOI: 10.32968/psaie.2023.1.5
\\
  One of the hot topics in machine learning is the field of GNN. The complexity
of graph data has imposed significant challenges on existing machine learning
algorithms. Recently, many studies on extending deep learning approaches for
graph data have emerged. This paper represents a survey, providing a
comprehensive overview of Graph Neural Networks (GNNs). We discuss the
applications of graph neural networks across various domains. Finally, we
present an advanced field in GNNs: graph generation.
\\ ( https://arxiv.org/abs/2403.13849 ,  869kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13850
Date: Mon, 18 Mar 2024 14:57:47 GMT   (25582kb,D)

Title: Spatio-Temporal Fluid Dynamics Modeling via Physical-Awareness and
  Parameter Diffusion Guidance
Authors: Hao Wu, Fan Xu, Yifan Duan, Ziwei Niu, Weiyan Wang, Gaofeng Lu, Kun
  Wang, Yuxuan Liang, and Yang Wang
Categories: cs.LG cs.AI physics.flu-dyn
\\
  This paper proposes a two-stage framework named ST-PAD for spatio-temporal
fluid dynamics modeling in the field of earth sciences, aiming to achieve
high-precision simulation and prediction of fluid dynamics through
spatio-temporal physics awareness and parameter diffusion guidance. In the
upstream stage, we design a vector quantization reconstruction module with
temporal evolution characteristics, ensuring balanced and resilient parameter
distribution by introducing general physical constraints. In the downstream
stage, a diffusion probability network involving parameters is utilized to
generate high-quality future states of fluids, while enhancing the model's
generalization ability by perceiving parameters in various physical setups.
Extensive experiments on multiple benchmark datasets have verified the
effectiveness and robustness of the ST-PAD framework, which showcase that
ST-PAD outperforms current mainstream models in fluid dynamics modeling and
prediction, especially in effectively capturing local representations and
maintaining significant advantages in OOD generations.
\\ ( https://arxiv.org/abs/2403.13850 ,  25582kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13861
Date: Wed, 20 Mar 2024 01:12:44 GMT   (521kb)

Title: Machine Learning-based Layer-wise Detection of Overheating Anomaly in
  LPBF using Photodiode Data
Authors: Nazmul Hasan, Apurba Kumar Saha, Andrew Wessman, Mohammed Shafae
Categories: cs.LG stat.AP
Comments: 12 pages (including references); 5 figures; 4 tables
\\
  Overheating anomaly detection is essential for the quality and reliability of
parts produced by laser powder bed fusion (LPBF) additive manufacturing (AM).
In this research, we focus on the detection of overheating anomalies using
photodiode sensor data. Photodiode sensors can collect high-frequency data from
the melt pool, reflecting the process dynamics and thermal history. Hence, the
proposed method offers a machine learning (ML) framework to utilize photodiode
sensor data for layer-wise detection of overheating anomalies. In doing so,
three sets of features are extracted from the raw photodiode data: MSMM (mean,
standard deviation, median, maximum), MSQ (mean, standard deviation,
quartiles), and MSD (mean, standard deviation, deciles). These three datasets
are used to train several ML classifiers. Cost-sensitive learning is used to
handle the class imbalance between the "anomalous" layers (affected by
overheating) and "nominal" layers in the benchmark dataset. To boost detection
accuracy, our proposed ML framework involves utilizing the majority voting
ensemble (MVE) approach. The proposed method is demonstrated using a case study
including an open benchmark dataset of photodiode measurements from an LPBF
specimen with deliberate overheating anomalies at some layers. The results from
the case study demonstrate that the MSD features yield the best performance for
all classifiers, and the MVE classifier (with a mean F1-score of 0.8654)
surpasses the individual ML classifiers. Moreover, our machine learning
methodology achieves superior results (9.66% improvement in mean F1-score) in
detecting layer-wise overheating anomalies, surpassing the existing methods in
the literature that use the same benchmark dataset.
\\ ( https://arxiv.org/abs/2403.13861 ,  521kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13863
Date: Wed, 20 Mar 2024 08:45:31 GMT   (4512kb,D)

Title: DiffImpute: Tabular Data Imputation With Denoising Diffusion
  Probabilistic Model
Authors: Yizhu Wen, Kai Yi, Jing Ke, Yiqing Shen
Categories: cs.LG cs.AI cs.DB
Comments: 26 pages, 6 figures
\\
  Tabular data plays a crucial role in various domains but often suffers from
missing values, thereby curtailing its potential utility. Traditional
imputation techniques frequently yield suboptimal results and impose
substantial computational burdens, leading to inaccuracies in subsequent
modeling tasks. To address these challenges, we propose DiffImpute, a novel
Denoising Diffusion Probabilistic Model (DDPM). Specifically, DiffImpute is
trained on complete tabular datasets, ensuring that it can produce credible
imputations for missing entries without undermining the authenticity of the
existing data. Innovatively, it can be applied to various settings of Missing
Completely At Random (MCAR) and Missing At Random (MAR). To effectively handle
the tabular features in DDPM, we tailor four tabular denoising networks,
spanning MLP, ResNet, Transformer, and U-Net. We also propose Harmonization to
enhance coherence between observed and imputed data by infusing the data back
and denoising them multiple times during the sampling stage. To enable
efficient inference while maintaining imputation performance, we propose a
refined non-Markovian sampling process that works along with Harmonization.
Empirical evaluations on seven diverse datasets underscore the prowess of
DiffImpute. Specifically, when paired with the Transformer as the denoising
network, it consistently outperforms its competitors, boasting an average
ranking of 1.7 and the most minimal standard deviation. In contrast, the next
best method lags with a ranking of 2.8 and a standard deviation of 0.9. The
code is available at https://github.com/Dendiiiii/DiffImpute.
\\ ( https://arxiv.org/abs/2403.13863 ,  4512kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13864
Date: Wed, 20 Mar 2024 09:23:20 GMT   (364kb,D)

Title: Optimal Transport for Fairness: Archival Data Repair using Small
  Research Data Sets
Authors: Abigail Langbridge and Anthony Quinn and Robert Shorten
Categories: cs.LG cs.CY math.ST stat.TH
\\
  With the advent of the AI Act and other regulations, there is now an urgent
need for algorithms that repair unfairness in training data. In this paper, we
define fairness in terms of conditional independence between protected
attributes ($S$) and features ($X$), given unprotected attributes ($U$). We
address the important setting in which torrents of archival data need to be
repaired, using only a small proportion of these data, which are $S|U$-labelled
(the research data). We use the latter to design optimal transport (OT)-based
repair plans on interpolated supports. This allows {\em off-sample}, labelled,
archival data to be repaired, subject to stationarity assumptions. It also
significantly reduces the size of the supports of the OT plans, with
correspondingly large savings in the cost of their design and of their {\em
sequential\/} application to the off-sample data. We provide detailed
experimental results with simulated and benchmark real data (the Adult data
set). Our performance figures demonstrate effective repair -- in the sense of
quenching conditional dependence -- of large quantities of off-sample, labelled
(archival) data.
\\ ( https://arxiv.org/abs/2403.13864 ,  364kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13866
Date: Wed, 20 Mar 2024 11:47:42 GMT   (3038kb,D)

Title: The Bid Picture: Auction-Inspired Multi-player Generative Adversarial
  Networks Training
Authors: Joo Yong Shim, Jean Seong Bjorn Choe and Jong-Kook Kim
Categories: cs.LG cs.AI
\\
  This article proposes auction-inspired multi-player generative adversarial
networks training, which mitigates the mode collapse problem of GANs. Mode
collapse occurs when an over-fitted generator generates a limited range of
samples, often concentrating on a small subset of the data distribution.
Despite the restricted diversity of generated samples, the discriminator can
still be deceived into distinguishing these samples as real samples from the
actual distribution. In the absence of external standards, a model cannot
recognize its failure during the training phase. We extend the two-player game
of generative adversarial networks to the multi-player game. During the
training, the values of each model are determined by the bids submitted by
other players in an auction-like process.
\\ ( https://arxiv.org/abs/2403.13866 ,  3038kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13867
Date: Wed, 20 Mar 2024 12:17:49 GMT   (734kb)

Title: Capsule Neural Networks as Noise Stabilizer for Time Series Data
Authors: Soyeon Kim, Jihyeon Seong, Hyunkyung Han, Jaesik Choi
Categories: cs.LG
Comments: 3 pages, 3 figures
Journal-ref: KCC2023
\\
  Capsule Neural Networks utilize capsules, which bind neurons into a single
vector and learn position equivariant features, which makes them more robust
than original Convolutional Neural Networks. CapsNets employ an affine
transformation matrix and dynamic routing with coupling coefficients to learn
robustly. In this paper, we investigate the effectiveness of CapsNets in
analyzing highly sensitive and noisy time series sensor data. To demonstrate
CapsNets robustness, we compare their performance with original CNNs on
electrocardiogram data, a medical time series sensor data with complex patterns
and noise. Our study provides empirical evidence that CapsNets function as
noise stabilizers, as investigated by manual and adversarial attack experiments
using the fast gradient sign method and three manual attacks, including offset
shifting, gradual drift, and temporal lagging. In summary, CapsNets outperform
CNNs in both manual and adversarial attacked data. Our findings suggest that
CapsNets can be effectively applied to various sensor systems to improve their
resilience to noise attacks. These results have significant implications for
designing and implementing robust machine learning models in real world
applications. Additionally, this study contributes to the effectiveness of
CapsNet models in handling noisy data and highlights their potential for
addressing the challenges of noise data in time series analysis.
\\ ( https://arxiv.org/abs/2403.13867 ,  734kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13869
Date: Wed, 20 Mar 2024 14:00:29 GMT   (5458kb,D)

Title: Accurately Predicting Probabilities of Safety-Critical Rare Events for
  Intelligent Systems
Authors: Ruoxuan Bai, Jingxuan Yang, Weiduo Gong, Yi Zhang, Qiujing Lu and Shuo
  Feng
Categories: cs.LG cs.AI
\\
  Intelligent systems are increasingly integral to our daily lives, yet rare
safety-critical events present significant latent threats to their practical
deployment. Addressing this challenge hinges on accurately predicting the
probability of safety-critical events occurring within a given time step from
the current state, a metric we define as 'criticality'. The complexity of
predicting criticality arises from the extreme data imbalance caused by rare
events in high dimensional variables associated with the rare events, a
challenge we refer to as the curse of rarity. Existing methods tend to be
either overly conservative or prone to overlooking safety-critical events, thus
struggling to achieve both high precision and recall rates, which severely
limits their applicability. This study endeavors to develop a criticality
prediction model that excels in both precision and recall rates for evaluating
the criticality of safety-critical autonomous systems. We propose a multi-stage
learning framework designed to progressively densify the dataset, mitigating
the curse of rarity across stages. To validate our approach, we evaluate it in
two cases: lunar lander and bipedal walker scenarios. The results demonstrate
that our method surpasses traditional approaches, providing a more accurate and
dependable assessment of criticality in intelligent systems.
\\ ( https://arxiv.org/abs/2403.13869 ,  5458kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13872
Date: Wed, 20 Mar 2024 15:27:17 GMT   (1389kb,D)

Title: Spatial-Temporal Graph Representation Learning for Tactical Networks
  Future State Prediction
Authors: Liu Junhua, Albrethsen Justin, Goh Lincoln, Yau David, Lim Kwan Hui
Categories: cs.LG cs.SI
\\
  Resource allocation in tactical ad-hoc networks presents unique challenges
due to their dynamic and multi-hop nature. Accurate prediction of future
network connectivity is essential for effective resource allocation in such
environments. In this paper, we introduce the Spatial-Temporal Graph
Encoder-Decoder (STGED) framework for Tactical Communication Networks that
leverages both spatial and temporal features of network states to learn latent
tactical behaviors effectively. STGED hierarchically utilizes graph-based
attention mechanism to spatially encode a series of communication network
states, leverages a recurrent neural network to temporally encode the evolution
of states, and a fully-connected feed-forward network to decode the
connectivity in the future state. Through extensive experiments, we demonstrate
that STGED consistently outperforms baseline models by large margins across
different time-steps input, achieving an accuracy of up to 99.2\% for the
future state prediction task of tactical communication networks.
\\ ( https://arxiv.org/abs/2403.13872 ,  1389kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13893
Date: Wed, 20 Mar 2024 18:05:52 GMT   (3907kb,D)

Title: Data Acquisition via Experimental Design for Decentralized Data Markets
Authors: Charles Lu, Baihe Huang, Sai Praneeth Karimireddy, Praneeth Vepakomma,
  Michael Jordan, Ramesh Raskar
Categories: cs.LG
Comments: 26 pages, 20 figures
\\
  Acquiring high-quality training data is essential for current machine
learning models. Data markets provide a way to increase the supply of data,
particularly in data-scarce domains such as healthcare, by incentivizing
potential data sellers to join the market. A major challenge for a data buyer
in such a market is selecting the most valuable data points from a data seller.
Unlike prior work in data valuation, which assumes centralized data access, we
propose a federated approach to the data selection problem that is inspired by
linear experimental design. Our proposed data selection method achieves lower
prediction error without requiring labeled validation data and can be optimized
in a fast and federated procedure. The key insight of our work is that a method
that directly estimates the benefit of acquiring data for test set prediction
is particularly compatible with a decentralized market setting.
\\ ( https://arxiv.org/abs/2403.13893 ,  3907kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13909
Date: Wed, 20 Mar 2024 18:29:55 GMT   (1103kb,D)

Title: Sequential Modeling of Complex Marine Navigation: Case Study on a
  Passenger Vessel (Student Abstract)
Authors: Yimeng Fan, Pedram Agand, Mo Chen, Edward J. Park, Allison Kennedy,
  and Chanwoo Bae
Categories: cs.LG cs.SY eess.SY
Comments: 5 pages, 3 figures, AAAI 2024 student abstract
\\
  The maritime industry's continuous commitment to sustainability has led to a
dedicated exploration of methods to reduce vessel fuel consumption. This paper
undertakes this challenge through a machine learning approach, leveraging a
real-world dataset spanning two years of a ferry in west coast Canada. Our
focus centers on the creation of a time series forecasting model given the
dynamic and static states, actions, and disturbances. This model is designed to
predict dynamic states based on the actions provided, subsequently serving as
an evaluative tool to assess the proficiency of the ferry's operation under the
captain's guidance. Additionally, it lays the foundation for future
optimization algorithms, providing valuable feedback on decision-making
processes. To facilitate future studies, our code is available at
\url{https://github.com/pagand/model_optimze_vessel/tree/AAAI}
\\ ( https://arxiv.org/abs/2403.13909 ,  1103kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13940
Date: Wed, 20 Mar 2024 19:25:11 GMT   (769kb,D)

Title: Multi-criteria approach for selecting an explanation from the set of
  counterfactuals produced by an ensemble of explainers
Authors: Ignacy St\k{e}pka, Mateusz Lango, Jerzy Stefanowski
Categories: cs.LG cs.AI
Comments: 17 pages, 2 figures
\\
  Counterfactuals are widely used to explain ML model predictions by providing
alternative scenarios for obtaining the more desired predictions. They can be
generated by a variety of methods that optimize different, sometimes
conflicting, quality measures and produce quite different solutions. However,
choosing the most appropriate explanation method and one of the generated
counterfactuals is not an easy task. Instead of forcing the user to test many
different explanation methods and analysing conflicting solutions, in this
paper, we propose to use a multi-stage ensemble approach that will select
single counterfactual based on the multiple-criteria analysis. It offers a
compromise solution that scores well on several popular quality measures. This
approach exploits the dominance relation and the ideal point decision aid
method, which selects one counterfactual from the Pareto front. The conducted
experiments demonstrated that the proposed approach generates fully actionable
counterfactuals with attractive compromise values of the considered quality
measures.
\\ ( https://arxiv.org/abs/2403.13940 ,  769kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14058
Date: Thu, 21 Mar 2024 01:06:47 GMT   (17807kb,D)

Title: Hypothesis-Driven Deep Learning for Out of Distribution Detection
Authors: Yasith Jayawardana, Azeem Ahmad, Balpreet S. Ahluwalia, Rafi Ahmad,
  Sampath Jayarathna, Dushan N. Wadduwage
Categories: cs.LG stat.ML
\\
  Predictions of opaque black-box systems are frequently deployed in
high-stakes applications such as healthcare. For such applications, it is
crucial to assess how models handle samples beyond the domain of training data.
While several metrics and tests exist to detect out-of-distribution (OoD) data
from in-distribution (InD) data to a deep neural network (DNN), their
performance varies significantly across datasets, models, and tasks, which
limits their practical use. In this paper, we propose a hypothesis-driven
approach to quantify whether a new sample is InD or OoD. Given a trained DNN
and some input, we first feed the input through the DNN and compute an ensemble
of OoD metrics, which we term latent responses. We then formulate the OoD
detection problem as a hypothesis test between latent responses of different
groups, and use permutation-based resampling to infer the significance of the
observed latent responses under a null hypothesis. We adapt our method to
detect an unseen sample of bacteria to a trained deep learning model, and show
that it reveals interpretable differences between InD and OoD latent responses.
Our work has implications for systematic novelty detection and informed
decision-making from classifiers trained on a subset of labels.
\\ ( https://arxiv.org/abs/2403.14058 ,  17807kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14063
Date: Thu, 21 Mar 2024 01:20:32 GMT   (2280kb,D)

Title: DiffSTOCK: Probabilistic relational Stock Market Predictions using
  Diffusion Models
Authors: Divyanshu Daiya, Monika Yadav, Harshit Singh Rao
Categories: cs.LG cs.CE q-fin.CP q-fin.PM
Comments: Accepted for presentation to the 2024 IEEE International Conference
  on Acoustics, Speech, and Signal Processing (ICASSP 2024), Seoul, Korea
\\
  In this work, we propose an approach to generalize denoising diffusion
probabilistic models for stock market predictions and portfolio management.
Present works have demonstrated the efficacy of modeling interstock relations
for market time-series forecasting and utilized Graph-based learning models for
value prediction and portfolio management. Though convincing, these
deterministic approaches still fall short of handling uncertainties i.e., due
to the low signal-to-noise ratio of the financial data, it is quite challenging
to learn effective deterministic models. Since the probabilistic methods have
shown to effectively emulate higher uncertainties for time-series predictions.
To this end, we showcase effective utilisation of Denoising Diffusion
Probabilistic Models (DDPM), to develop an architecture for providing better
market predictions conditioned on the historical financial indicators and
inter-stock relations. Additionally, we also provide a novel deterministic
architecture MaTCHS which uses Masked Relational Transformer(MRT) to exploit
inter-stock relations along with historical stock features. We demonstrate that
our model achieves SOTA performance for movement predication and Portfolio
management.
\\ ( https://arxiv.org/abs/2403.14063 ,  2280kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14069
Date: Thu, 21 Mar 2024 01:35:03 GMT   (2814kb,D)

Title: Sampling Audit Evidence Using a Naive Bayes Classifier
Authors: Guang-Yih Sheu and Nai-Ru Liu
Categories: cs.LG
Comments: 16 pages, 11 figures, 4 tables
MSC-class: 62D05, 62H30
\\
  Taiwan's auditors have suffered from processing excessive audit data,
including drawing audit evidence. This study advances sampling techniques by
integrating machine learning with sampling. This machine learning integration
helps avoid sampling bias, keep randomness and variability, and target risker
samples. We first classify data using a Naive Bayes classifier into some
classes. Next, a user-based, item-based, or hybrid approach is employed to draw
audit evidence. The representativeness index is the primary metric for
measuring its representativeness. The user-based approach samples data
symmetric around the median of a class as audit evidence. It may be equivalent
to a combination of monetary and variable samplings. The item-based approach
represents asymmetric sampling based on posterior probabilities for obtaining
risky samples as audit evidence. It may be identical to a combination of
non-statistical and monetary samplings. Auditors can hybridize those user-based
and item-based approaches to balance representativeness and riskiness in
selecting audit evidence. Three experiments show that sampling using machine
learning integration has the benefits of drawing unbiased samples, handling
complex patterns, correlations, and unstructured data, and improving efficiency
in sampling big data. However, the limitations are the classification accuracy
output by machine learning algorithms and the range of prior probabilities.
\\ ( https://arxiv.org/abs/2403.14069 ,  2814kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14092
Date: Thu, 21 Mar 2024 02:59:56 GMT   (1212kb)

Title: Carbon Footprint Reduction for Sustainable Data Centers in Real-Time
Authors: Soumyendu Sarkar, Avisek Naug, Ricardo Luna, Antonio Guillen, Vineet
  Gundecha, Sahand Ghorbanpour, Sajad Mousavi, Dejan Markovikj, Ashwin Ramesh
  Babu
Categories: cs.LG cs.AI cs.MA cs.SY eess.SY
Journal-ref: 2024 Proceedings of the AAAI Conference on Artificial Intelligence
\\
  As machine learning workloads significantly increase energy consumption,
sustainable data centers with low carbon emissions are becoming a top priority
for governments and corporations worldwide. This requires a paradigm shift in
optimizing power consumption in cooling and IT loads, shifting flexible loads
based on the availability of renewable energy in the power grid, and leveraging
battery storage from the uninterrupted power supply in data centers, using
collaborative agents. The complex association between these optimization
strategies and their dependencies on variable external factors like weather and
the power grid carbon intensity makes this a hard problem. Currently, a
real-time controller to optimize all these goals simultaneously in a dynamic
real-world setting is lacking. We propose a Data Center Carbon Footprint
Reduction (DC-CFR) multi-agent Reinforcement Learning (MARL) framework that
optimizes data centers for the multiple objectives of carbon footprint
reduction, energy consumption, and energy cost. The results show that the
DC-CFR MARL agents effectively resolved the complex interdependencies in
optimizing cooling, load shifting, and energy storage in real-time for various
locations under real-world dynamic weather and grid carbon intensity
conditions. DC-CFR significantly outperformed the industry standard ASHRAE
controller with a considerable reduction in carbon emissions (14.5%), energy
usage (14.4%), and energy cost (13.7%) when evaluated over one year across
multiple geographical regions.
\\ ( https://arxiv.org/abs/2403.14092 ,  1212kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14110
Date: Thu, 21 Mar 2024 03:42:39 GMT   (2696kb,D)

Title: Heuristic Algorithm-based Action Masking Reinforcement Learning
  (HAAM-RL) with Ensemble Inference Method
Authors: Kyuwon Choi, Cheolkyun Rho, Taeyoun Kim, Daewoo Choi
Categories: cs.LG cs.AI
Comments: 7 pages, 8 figures
\\
  This paper presents a novel reinforcement learning (RL) approach called
HAAM-RL (Heuristic Algorithm-based Action Masking Reinforcement Learning) for
optimizing the color batching re-sequencing problem in automobile painting
processes. The existing heuristic algorithms have limitations in adequately
reflecting real-world constraints and accurately predicting logistics
performance. Our methodology incorporates several key techniques including a
tailored Markov Decision Process (MDP) formulation, reward setting including
Potential-Based Reward Shaping, action masking using heuristic algorithms
(HAAM-RL), and an ensemble inference method that combines multiple RL models.
The RL agent is trained and evaluated using FlexSim, a commercial 3D simulation
software, integrated with our RL MLOps platform BakingSoDA. Experimental
results across 30 scenarios demonstrate that HAAM-RL with an ensemble inference
method achieves a 16.25% performance improvement over the conventional
heuristic algorithm, with stable and consistent results. The proposed approach
exhibits superior performance and generalization capability, indicating its
effectiveness in optimizing complex manufacturing processes. The study also
discusses future research directions, including alternative state
representations, incorporating model-based RL methods, and integrating
additional real-world constraints.
\\ ( https://arxiv.org/abs/2403.14110 ,  2696kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14120
Date: Thu, 21 Mar 2024 04:15:56 GMT   (2289kb,D)

Title: Advancing IIoT with Over-the-Air Federated Learning: The Role of
  Iterative Magnitude Pruning
Authors: Fazal Muhammad Ali Khan, Hatem Abou-Zeid, Aryan Kaushik, Syed Ali
  Hassan
Categories: cs.LG cs.AI eess.SP
Comments: 6 pages, 6 figures
\\
  The industrial Internet of Things (IIoT) under Industry 4.0 heralds an era of
interconnected smart devices where data-driven insights and machine learning
(ML) fuse to revolutionize manufacturing. A noteworthy development in IIoT is
the integration of federated learning (FL), which addresses data privacy and
security among devices. FL enables edge sensors, also known as peripheral
intelligence units (PIUs) to learn and adapt using their data locally, without
explicit sharing of confidential data, to facilitate a collaborative yet
confidential learning process. However, the lower memory footprint and
computational power of PIUs inherently require deep neural network (DNN) models
that have a very compact size. Model compression techniques such as pruning can
be used to reduce the size of DNN models by removing unnecessary connections
that have little impact on the model's performance, thus making the models more
suitable for the limited resources of PIUs. Targeting the notion of compact yet
robust DNN models, we propose the integration of iterative magnitude pruning
(IMP) of the DNN model being trained in an over-the-air FL (OTA-FL) environment
for IIoT. We provide a tutorial overview and also present a case study of the
effectiveness of IMP in OTA-FL for an IIoT environment. Finally, we present
future directions for enhancing and optimizing these deep compression
techniques further, aiming to push the boundaries of IIoT capabilities in
acquiring compact yet robust and high-performing DNN models.
\\ ( https://arxiv.org/abs/2403.14120 ,  2289kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14123
Date: Thu, 21 Mar 2024 04:31:59 GMT   (293kb,D)

Title: AI and Memory Wall
Authors: Amir Gholami, Zhewei Yao, Sehoon Kim, Coleman Hooper, Michael W.
  Mahoney, Kurt Keutzer
Categories: cs.LG cs.AR
Comments: Published in IEEE Micro Journal
\\
  The availability of unprecedented unsupervised training data, along with
neural scaling laws, has resulted in an unprecedented surge in model size and
compute requirements for serving/training LLMs. However, the main performance
bottleneck is increasingly shifting to memory bandwidth. Over the past 20
years, peak server hardware FLOPS has been scaling at 3.0x/2yrs, outpacing the
growth of DRAM and interconnect bandwidth, which have only scaled at 1.6 and
1.4 times every 2 years, respectively. This disparity has made memory, rather
than compute, the primary bottleneck in AI applications, particularly in
serving. Here, we analyze encoder and decoder Transformer models and show how
memory bandwidth can become the dominant bottleneck for decoder models. We
argue for a redesign in model architecture, training, and deployment strategies
to overcome this memory limitation.
\\ ( https://arxiv.org/abs/2403.14123 ,  293kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14151
Date: Thu, 21 Mar 2024 05:57:27 GMT   (4971kb,D)

Title: Deep Learning for Trajectory Data Management and Mining: A Survey and
  Beyond
Authors: Wei Chen, Yuxuan Liang, Yuanshao Zhu, Yanchuan Chang, Kang Luo, Haomin
  Wen, Lei Li, Yanwei Yu, Qingsong Wen, Chao Chen, Kai Zheng, Yunjun Gao,
  Xiaofang Zhou and Yu Zheng
Categories: cs.LG cs.AI cs.CY cs.DB
Comments: 25 pages, 12 figures, 5 tables
\\
  Trajectory computing is a pivotal domain encompassing trajectory data
management and mining, garnering widespread attention due to its crucial role
in various practical applications such as location services, urban traffic, and
public safety. Traditional methods, focusing on simplistic spatio-temporal
features, face challenges of complex calculations, limited scalability, and
inadequate adaptability to real-world complexities. In this paper, we present a
comprehensive review of the development and recent advances in deep learning
for trajectory computing (DL4Traj). We first define trajectory data and provide
a brief overview of widely-used deep learning models. Systematically, we
explore deep learning applications in trajectory management (pre-processing,
storage, analysis, and visualization) and mining (trajectory-related
forecasting, trajectory-related recommendation, trajectory classification,
travel time estimation, anomaly detection, and mobility generation). Notably,
we encapsulate recent advancements in Large Language Models (LLMs) that hold
the potential to augment trajectory computing. Additionally, we summarize
application scenarios, public datasets, and toolkits. Finally, we outline
current challenges in DL4Traj research and propose future directions. Relevant
papers and open-source resources have been collated and are continuously
updated at:
\href{https://github.com/yoshall/Awesome-Trajectory-Computing}{DL4Traj Repo}.
\\ ( https://arxiv.org/abs/2403.14151 ,  4971kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14156
Date: Thu, 21 Mar 2024 06:10:51 GMT   (871kb,D)

Title: Policy Mirror Descent with Lookahead
Authors: Kimon Protopapas, Anas Barakat
Categories: cs.LG cs.AI stat.ML
\\
  Policy Mirror Descent (PMD) stands as a versatile algorithmic framework
encompassing several seminal policy gradient algorithms such as natural policy
gradient, with connections with state-of-the-art reinforcement learning (RL)
algorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iteration
algorithm implementing regularized 1-step greedy policy improvement. However,
1-step greedy policies might not be the best choice and recent remarkable
empirical successes in RL such as AlphaGo and AlphaZero have demonstrated that
greedy approaches with respect to multiple steps outperform their 1-step
counterpart. In this work, we propose a new class of PMD algorithms called
$h$-PMD which incorporates multi-step greedy policy improvement with lookahead
depth $h$ to the PMD update rule. To solve discounted infinite horizon Markov
Decision Processes with discount factor $\gamma$, we show that $h$-PMD which
generalizes the standard PMD enjoys a faster dimension-free $\gamma^h$-linear
convergence rate, contingent on the computation of multi-step greedy policies.
We propose an inexact version of $h$-PMD where lookahead action values are
estimated. Under a generative model, we establish a sample complexity for
$h$-PMD which improves over prior work. Finally, we extend our result to linear
function approximation to scale to large state spaces. Under suitable
assumptions, our sample complexity only involves dependence on the dimension of
the feature map space instead of the state space size.
\\ ( https://arxiv.org/abs/2403.14156 ,  871kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14200
Date: Thu, 21 Mar 2024 07:50:45 GMT   (603kb,D)

Title: Debiasing surgeon: fantastic weights and how to find them
Authors: R\'emi Nahon and Ivan Luiz De Moura Matos and Van-Tam Nguyen and Enzo
  Tartaglione
Categories: cs.LG cs.AI cs.CV
\\
  Nowadays an ever-growing concerning phenomenon, the emergence of algorithmic
biases that can lead to unfair models, emerges. Several debiasing approaches
have been proposed in the realm of deep learning, employing more or less
sophisticated approaches to discourage these models from massively employing
these biases. However, a question emerges: is this extra complexity really
necessary? Is a vanilla-trained model already embodying some ``unbiased
sub-networks'' that can be used in isolation and propose a solution without
relying on the algorithmic biases? In this work, we show that such a
sub-network typically exists, and can be extracted from a vanilla-trained model
without requiring additional training. We further validate that such specific
architecture is incapable of learning a specific bias, suggesting that there
are possible architectural countermeasures to the problem of biases in deep
neural networks.
\\ ( https://arxiv.org/abs/2403.14200 ,  603kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14232
Date: Thu, 21 Mar 2024 08:41:53 GMT   (3008kb,D)

Title: Contrastive Balancing Representation Learning for Heterogeneous
  Dose-Response Curves Estimation
Authors: Minqin Zhu, Anpeng Wu, Haoxuan Li, Ruoxuan Xiong, Bo Li, Xiaoqing
  Yang, Xuan Qin, Peng Zhen, Jiecheng Guo, Fei Wu, Kun Kuang
Categories: cs.LG
\\
  Estimating the individuals' potential response to varying treatment doses is
crucial for decision-making in areas such as precision medicine and management
science. Most recent studies predict counterfactual outcomes by learning a
covariate representation that is independent of the treatment variable.
However, such independence constraints neglect much of the covariate
information that is useful for counterfactual prediction, especially when the
treatment variables are continuous. To tackle the above issue, in this paper,
we first theoretically demonstrate the importance of the balancing and
prognostic representations for unbiased estimation of the heterogeneous
dose-response curves, that is, the learned representations are constrained to
satisfy the conditional independence between the covariates and both of the
treatment variables and the potential responses. Based on this, we propose a
novel Contrastive balancing Representation learning Network using a partial
distance measure, called CRNet, for estimating the heterogeneous dose-response
curves without losing the continuity of treatments. Extensive experiments are
conducted on synthetic and real-world datasets demonstrating that our proposal
significantly outperforms previous methods.
\\ ( https://arxiv.org/abs/2403.14232 ,  3008kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14236
Date: Thu, 21 Mar 2024 08:54:24 GMT   (2937kb,D)

Title: A Unified Framework for Model Editing
Authors: Akshat Gupta, Dev Sajnani, Gopala Anumanchipalli
Categories: cs.LG cs.AI cs.CL
\\
  Model editing is a growing area focused on updating the knowledge embedded
within models. Among the various methodologies, ROME and MEMIT stand out as
leading "locate-and-edit" model editing techniques. While MEMIT enables batched
editing of memories, ROME is limited to changing one fact at a time. This paper
introduces a unifying framework that brings ROME and MEMIT under a single
conceptual umbrella, optimizing for the same goal, which we call the
"preservation-memorization" objective. This objective aims to preserve the
representations of certain selected vectors while memorizing the
representations of new factual information. Specifically, ROME optimizes this
objective using an equality constraint, whereas MEMIT employs a more flexible
least-square constraint. In addition to making batched edits, MEMIT also edits
the model at multiple layers. We disentangle the distribution of edits to
multiple layers from the optimization objective of MEMIT and show that these
edit-distribution algorithms should be considered separate entities worthy of
their own line of research.
  Finally, we present EMMET - an Equality-constrained Mass Model Editing
algorithm for Transformers, a new batched memory-editing algorithm. With EMMET,
we present a closed form solution for the equality-constrained version of the
preservation-memorization objective. We show that EMMET is able to perform
batched-edits on par with MEMIT up to a batch-size of 256 and discuss the
challenges in stabilizing EMMET. By articulating the "locate-and-edit" model
editing algorithms under a simple conceptual framework of
"preservation-memorization", we aim to bridge the gap between intuition and
mathematics and hope to simplify the journey for future researchers in model
editing.
\\ ( https://arxiv.org/abs/2403.14236 ,  2937kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14282
Date: Thu, 21 Mar 2024 10:43:55 GMT   (647kb,D)

Title: How to be fair? A study of label and selection bias
Authors: Marco Favier, Toon Calders, Sam Pinxteren, Jonathan Meyer
Categories: cs.LG cs.AI
DOI: 10.1007/s10994-023-06401-1
\\
  It is widely accepted that biased data leads to biased and thus potentially
unfair models. Therefore, several measures for bias in data and model
predictions have been proposed, as well as bias mitigation techniques whose aim
is to learn models that are fair by design. Despite the myriad of mitigation
techniques developed in the past decade, however, it is still poorly understood
under what circumstances which methods work. Recently, Wick et al. showed, with
experiments on synthetic data, that there exist situations in which bias
mitigation techniques lead to more accurate models when measured on unbiased
data. Nevertheless, in the absence of a thorough mathematical analysis, it
remains unclear which techniques are effective under what circumstances. We
propose to address this problem by establishing relationships between the type
of bias and the effectiveness of a mitigation technique, where we categorize
the mitigation techniques by the bias measure they optimize. In this paper we
illustrate this principle for label and selection bias on the one hand, and
demographic parity and ``We're All Equal'' on the other hand. Our theoretical
analysis allows to explain the results of Wick et al. and we also show that
there are situations where minimizing fairness measures does not result in the
fairest possible distribution.
\\ ( https://arxiv.org/abs/2403.14282 ,  647kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14297
Date: Thu, 21 Mar 2024 11:03:56 GMT   (64kb,D)

Title: Impact Assessment of Missing Data in Model Predictions for Earth
  Observation Applications
Authors: Francisco Mena, Diego Arenas, Marcela Charfuelan, Marlon Nuske,
  Andreas Dengel
Categories: cs.LG cs.AI cs.CV
Comments: Accepted at IEEE International Geoscience and Remote Sensing
  Symposium 2024
\\
  Earth observation (EO) applications involving complex and heterogeneous data
sources are commonly approached with machine learning models. However, there is
a common assumption that data sources will be persistently available. Different
situations could affect the availability of EO sources, like noise, clouds, or
satellite mission failures. In this work, we assess the impact of missing
temporal and static EO sources in trained models across four datasets with
classification and regression tasks. We compare the predictive quality of
different methods and find that some are naturally more robust to missing data.
The Ensemble strategy, in particular, achieves a prediction robustness up to
100%. We evidence that missing scenarios are significantly more challenging in
regression than classification tasks. Finally, we find that the optical view is
the most critical view when it is missing individually.
\\ ( https://arxiv.org/abs/2403.14297 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14327
Date: Thu, 21 Mar 2024 11:51:42 GMT   (3267kb,D)

Title: Investigating the validity of structure learning algorithms in
  identifying risk factors for intervention in patients with diabetes
Authors: Sheresh Zahoor, Anthony C. Constantinou, Tim M Curtis, Mohammed
  Hasanuzzaman
Categories: cs.LG
Comments: 20 pages, 17 figures, 4 tables
\\
  Diabetes, a pervasive and enduring health challenge, imposes significant
global implications on health, financial healthcare systems, and societal
well-being. This study undertakes a comprehensive exploration of various
structural learning algorithms to discern causal pathways amongst potential
risk factors influencing diabetes progression. The methodology involves the
application of these algorithms to relevant diabetes data, followed by the
conversion of their output graphs into Causal Bayesian Networks (CBNs),
enabling predictive analysis and the evaluation of discrepancies in the effect
of hypothetical interventions within our context-specific case study.
  This study highlights the substantial impact of algorithm selection on
intervention outcomes. To consolidate insights from diverse algorithms, we
employ a model-averaging technique that helps us obtain a unique causal model
for diabetes derived from a varied set of structural learning algorithms. We
also investigate how each of those individual graphs, as well as the average
graph, compare to the structures elicited by a domain expert who categorised
graph edges into high confidence, moderate, and low confidence types, leading
into three individual graphs corresponding to the three levels of confidence.
  The resulting causal model and data are made available online, and serve as a
valuable resource and a guide for informed decision-making by healthcare
practitioners, offering a comprehensive understanding of the interactions
between relevant risk factors and the effect of hypothetical interventions.
Therefore, this research not only contributes to the academic discussion on
diabetes, but also provides practical guidance for healthcare professionals in
developing efficient intervention and risk management strategies.
\\ ( https://arxiv.org/abs/2403.14327 ,  3267kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14339
Date: Thu, 21 Mar 2024 12:11:26 GMT   (659kb,D)

Title: $\nabla \tau$: Gradient-based and Task-Agnostic machine Unlearning
Authors: Daniel Trippa, Cesare Campagnano, Maria Sofia Bucarelli, Gabriele
  Tolomei, Fabrizio Silvestri
Categories: cs.LG cs.AI cs.CL cs.CV
Comments: 14 pages, 2 figures
\\
  Machine Unlearning, the process of selectively eliminating the influence of
certain data examples used during a model's training, has gained significant
attention as a means for practitioners to comply with recent data protection
regulations. However, existing unlearning methods face critical drawbacks,
including their prohibitively high cost, often associated with a large number
of hyperparameters, and the limitation of forgetting only relatively small data
portions. This often makes retraining the model from scratch a quicker and more
effective solution. In this study, we introduce Gradient-based and
Task-Agnostic machine Unlearning ($\nabla \tau$), an optimization framework
designed to remove the influence of a subset of training data efficiently. It
applies adaptive gradient ascent to the data to be forgotten while using
standard gradient descent for the remaining data. $\nabla \tau$ offers multiple
benefits over existing approaches. It enables the unlearning of large sections
of the training dataset (up to 30%). It is versatile, supporting various
unlearning tasks (such as subset forgetting or class removal) and applicable
across different domains (images, text, etc.). Importantly, $\nabla \tau$
requires no hyperparameter adjustments, making it a more appealing option than
retraining the model from scratch. We evaluate our framework's effectiveness
using a set of well-established Membership Inference Attack metrics,
demonstrating up to 10% enhancements in performance compared to
state-of-the-art methods without compromising the original model's accuracy.
\\ ( https://arxiv.org/abs/2403.14339 ,  659kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14340
Date: Thu, 21 Mar 2024 12:14:02 GMT   (766kb,D)

Title: Exploring Task Unification in Graph Representation Learning via
  Generative Approach
Authors: Yulan Hu, Sheng Ouyang, Zhirui Yang, Ge Chen, Junchen Wan, Xiao Wang,
  Yong Liu
Categories: cs.LG cs.AI
\\
  Graphs are ubiquitous in real-world scenarios and encompass a diverse range
of tasks, from node-, edge-, and graph-level tasks to transfer learning.
However, designing specific tasks for each type of graph data is often costly
and lacks generalizability. Recent endeavors under the "Pre-training +
Fine-tuning" or "Pre-training + Prompt" paradigms aim to design a unified
framework capable of generalizing across multiple graph tasks. Among these,
graph autoencoders (GAEs), generative self-supervised models, have demonstrated
their potential in effectively addressing various graph tasks. Nevertheless,
these methods typically employ multi-stage training and require adaptive
designs, which on one hand make it difficult to be seamlessly applied to
diverse graph tasks and on the other hand overlook the negative impact caused
by discrepancies in task objectives between the different stages. To address
these challenges, we propose GA^2E, a unified adversarially masked autoencoder
capable of addressing the above challenges seamlessly. Specifically, GA^2E
proposes to use the subgraph as the meta-structure, which remains consistent
across all graph tasks (ranging from node-, edge-, and graph-level to transfer
learning) and all stages (both during training and inference). Further, GA^2E
operates in a \textbf{"Generate then Discriminate"} manner. It leverages the
masked GAE to reconstruct the input subgraph whilst treating it as a generator
to compel the reconstructed graphs resemble the input subgraph. Furthermore,
GA^2E introduces an auxiliary discriminator to discern the authenticity between
the reconstructed (generated) subgraph and the input subgraph, thus ensuring
the robustness of the graph representation through adversarial training
mechanisms. We validate GA^2E's capabilities through extensive experiments on
21 datasets across four types of graph tasks.
\\ ( https://arxiv.org/abs/2403.14340 ,  766kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14356
Date: Thu, 21 Mar 2024 12:35:46 GMT   (1645kb,D)

Title: DomainLab: A modular Python package for domain generalization in deep
  learning
Authors: Xudong Sun, Carla Feistner, Alexej Gossmann, George Schwarz, Rao
  Muhammad Umer, Lisa Beer, Patrick Rockenschaub, Rahul Babu Shrestha, Armin
  Gruber, Nutan Chen, Sayedali Shetab Boushehri, Florian Buettner, Carsten Marr
Categories: cs.LG cs.SE
\\
  Poor generalization performance caused by distribution shifts in unseen
domains often hinders the trustworthy deployment of deep neural networks. Many
domain generalization techniques address this problem by adding a domain
invariant regularization loss terms during training. However, there is a lack
of modular software that allows users to combine the advantages of different
methods with minimal effort for reproducibility. DomainLab is a modular Python
package for training user specified neural networks with composable
regularization loss terms. Its decoupled design allows the separation of neural
networks from regularization loss construction. Hierarchical combinations of
neural networks, different domain generalization methods, and associated
hyperparameters, can all be specified together with other experimental setup in
a single configuration file. Hierarchical combinations of neural networks,
different domain generalization methods, and associated hyperparameters, can
all be specified together with other experimental setup in a single
configuration file. In addition, DomainLab offers powerful benchmarking
functionality to evaluate the generalization performance of neural networks in
out-of-distribution data. The package supports running the specified benchmark
on an HPC cluster or on a standalone machine. The package is well tested with
over 95 percent coverage and well documented. From the user perspective, it is
closed to modification but open to extension. The package is under the MIT
license, and its source code, tutorial and documentation can be found at
https://github.com/marrlab/DomainLab.
\\ ( https://arxiv.org/abs/2403.14356 ,  1645kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14358
Date: Thu, 21 Mar 2024 12:37:54 GMT   (479kb,D)

Title: Exploring the Potential of Large Language Models in Graph Generation
Authors: Yang Yao, Xin Wang, Zeyang Zhang, Yijian Qin, Ziwei Zhang, Xu Chu,
  Yuekui Yang, Wenwu Zhu, Hong Mei
Categories: cs.LG cs.AI
\\
  Large language models (LLMs) have achieved great success in many fields, and
recent works have studied exploring LLMs for graph discriminative tasks such as
node classification. However, the abilities of LLMs for graph generation remain
unexplored in the literature. Graph generation requires the LLM to generate
graphs with given properties, which has valuable real-world applications such
as drug discovery, while tends to be more challenging. In this paper, we
propose LLM4GraphGen to explore the ability of LLMs for graph generation with
systematical task designs and extensive experiments. Specifically, we propose
several tasks tailored with comprehensive experiments to address key questions
regarding LLMs' understanding of different graph structure rules, their ability
to capture structural type distributions, and their utilization of domain
knowledge for property-based graph generation. Our evaluations demonstrate that
LLMs, particularly GPT-4, exhibit preliminary abilities in graph generation
tasks, including rule-based and distribution-based generation. We also observe
that popular prompting methods, such as few-shot and chain-of-thought
prompting, do not consistently enhance performance. Besides, LLMs show
potential in generating molecules with specific properties. These findings may
serve as foundations for designing good LLMs based models for graph generation
and provide valuable insights and further research.
\\ ( https://arxiv.org/abs/2403.14358 ,  479kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14371
Date: Thu, 21 Mar 2024 12:59:24 GMT   (1049kb)

Title: Loop Improvement: An Efficient Approach for Extracting Shared Features
  from Heterogeneous Data without Central Server
Authors: Fei Li, Chu Kiong Loo, Wei Shiung Liew, Xiaofeng Liu
Categories: cs.LG cs.AI
Comments: 11 pages, 11 figures
\\
  In federated learning, data heterogeneity significantly impacts performance.
A typical solution involves segregating these parameters into shared and
personalized components, a concept also relevant in multi-task learning.
Addressing this, we propose "Loop Improvement" (LI), a novel method enhancing
this separation and feature extraction without necessitating a central server
or data interchange among participants. Our experiments reveal LI's superiority
in several aspects: In personalized federated learning environments, LI
consistently outperforms the advanced FedALA algorithm in accuracy across
diverse scenarios. Additionally, LI's feature extractor closely matches the
performance achieved when aggregating data from all clients. In global model
contexts, employing LI with stacked personalized layers and an additional
network also yields comparable results to combined client data scenarios.
Furthermore, LI's adaptability extends to multi-task learning, streamlining the
extraction of common features across tasks and obviating the need for
simultaneous training. This approach not only enhances individual task
performance but also achieves accuracy levels on par with classic multi-task
learning methods where all tasks are trained simultaneously. LI integrates a
loop topology with layer-wise and end-to-end training, compatible with various
neural network models. This paper also delves into the theoretical
underpinnings of LI's effectiveness, offering insights into its potential
applications. The code is on https://github.com/axedge1983/LI
\\ ( https://arxiv.org/abs/2403.14371 ,  1049kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14398
Date: Thu, 21 Mar 2024 13:43:49 GMT   (751kb,D)

Title: Regularized Adaptive Momentum Dual Averaging with an Efficient Inexact
  Subproblem Solver for Training Structured Neural Network
Authors: Zih-Syuan Huang, Ching-pei Lee
Categories: cs.LG math.OC
\\
  We propose a Regularized Adaptive Momentum Dual Averaging (RAMDA) algorithm
for training structured neural networks. Similar to existing regularized
adaptive methods, the subproblem for computing the update direction of RAMDA
involves a nonsmooth regularizer and a diagonal preconditioner, and therefore
does not possess a closed-form solution in general. We thus also carefully
devise an implementable inexactness condition that retains convergence
guarantees similar to the exact versions, and propose a companion efficient
solver for the subproblems of both RAMDA and existing methods to make them
practically feasible. We leverage the theory of manifold identification in
variational analysis to show that, even in the presence of such inexactness,
the iterates of RAMDA attain the ideal structure induced by the regularizer at
the stationary point of asymptotic convergence. This structure is locally
optimal near the point of convergence, so RAMDA is guaranteed to obtain the
best structure possible among all methods converging to the same point, making
it the first regularized adaptive method outputting models that possess
outstanding predictive performance while being (locally) optimally structured.
Extensive numerical experiments in large-scale modern computer vision, language
modeling, and speech tasks show that the proposed RAMDA is efficient and
consistently outperforms state of the art for training structured neural
network. Implementation of our algorithm is available at
http://www.github.com/ismoptgroup/RAMDA/.
\\ ( https://arxiv.org/abs/2403.14398 ,  751kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14404
Date: Thu, 21 Mar 2024 13:52:55 GMT   (1376kb,D)

Title: Physics-Informed Diffusion Models
Authors: Jan-Hendrik Bastek, WaiChing Sun, Dennis M. Kochmann
Categories: cs.LG cs.CE
Comments: 15 pages, 4 figures
\\
  Generative models such as denoising diffusion models are quickly advancing
their ability to approximate highly complex data distributions. They are also
increasingly leveraged in scientific machine learning, where samples from the
implied data distribution are expected to adhere to specific governing
equations. We present a framework to inform denoising diffusion models on
underlying constraints on such generated samples during model training. Our
approach improves the alignment of the generated samples with the imposed
constraints and significantly outperforms existing methods without affecting
inference speed. Additionally, our findings suggest that incorporating such
constraints during training provides a natural regularization against
overfitting. Our framework is easy to implement and versatile in its
applicability for imposing equality and inequality constraints as well as
auxiliary optimization objectives.
\\ ( https://arxiv.org/abs/2403.14404 ,  1376kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14421
Date: Thu, 21 Mar 2024 14:17:28 GMT   (9349kb,D)

Title: DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning
Authors: Jonathan Lebensold, Maziar Sanjabi, Pietro Astolfi, Adriana
  Romero-Soriano, Kamalika Chaudhuri, Mike Rabbat, Chuan Guo
Categories: cs.LG cs.CV
\\
  Text-to-image diffusion models have been shown to suffer from sample-level
memorization, possibly reproducing near-perfect replica of images that they are
trained on, which may be undesirable. To remedy this issue, we develop the
first differentially private (DP) retrieval-augmented generation algorithm that
is capable of generating high-quality image samples while providing provable
privacy guarantees. Specifically, we assume access to a text-to-image diffusion
model trained on a small amount of public data, and design a DP retrieval
mechanism to augment the text prompt with samples retrieved from a private
retrieval dataset. Our \emph{differentially private retrieval-augmented
diffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to
adapt to another domain, and can use state-of-the-art generative models to
generate high-quality image samples while satisfying rigorous DP guarantees.
For instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a
privacy budget of $\epsilon=10$, while providing a $3.5$ point improvement in
FID compared to public-only retrieval for up to $10,000$ queries.
\\ ( https://arxiv.org/abs/2403.14421 ,  9349kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14425
Date: Thu, 21 Mar 2024 14:28:43 GMT   (716kb,D)

Title: Task-optimal data-driven surrogate models for eNMPC via differentiable
  simulation and optimization
Authors: Daniel Mayfrank, Na Young Ahn, Alexander Mitsos, Manuel Dahmen
Categories: cs.LG math.OC
Comments: 6 pages, 4 figures, 1 table
\\
  We present a method for end-to-end learning of Koopman surrogate models for
optimal performance in control. In contrast to previous contributions that
employ standard reinforcement learning (RL) algorithms, we use a training
algorithm that exploits the potential differentiability of environments based
on mechanistic simulation models. We evaluate the performance of our method by
comparing it to that of other controller type and training algorithm
combinations on a literature known eNMPC case study. Our method exhibits
superior performance on this problem, thereby constituting a promising avenue
towards more capable controllers that employ dynamic surrogate models.
\\ ( https://arxiv.org/abs/2403.14425 ,  716kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14466
Date: Thu, 21 Mar 2024 15:13:54 GMT   (917kb,D)

Title: Universal Feature Selection for Simultaneous Interpretability of
  Multitask Datasets
Authors: Matt Raymond, Jacob Charles Saldinger, Paolo Elvati, Clayton Scott,
  Angela Violi
Categories: cs.LG
Comments: Main text: 14 pages, 3 figures, 1 table; SI: 7 pages, 1 figure, 4
  tables, 3 algorithms
\\
  Extracting meaningful features from complex, high-dimensional datasets across
scientific domains remains challenging. Current methods often struggle with
scalability, limiting their applicability to large datasets, or make
restrictive assumptions about feature-property relationships, hindering their
ability to capture complex interactions. BoUTS's general and scalable feature
selection algorithm surpasses these limitations to identify both universal
features relevant to all datasets and task-specific features predictive for
specific subsets. Evaluated on seven diverse chemical regression datasets,
BoUTS achieves state-of-the-art feature sparsity while maintaining prediction
accuracy comparable to specialized methods. Notably, BoUTS's universal features
enable domain-specific knowledge transfer between datasets, and suggest deep
connections in seemingly-disparate chemical datasets. We expect these results
to have important repercussions in manually-guided inverse problems. Beyond its
current application, BoUTS holds immense potential for elucidating data-poor
systems by leveraging information from similar data-rich systems. BoUTS
represents a significant leap in cross-domain feature selection, potentially
leading to advancements in various scientific fields.
\\ ( https://arxiv.org/abs/2403.14466 ,  917kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14483
Date: Thu, 21 Mar 2024 15:29:24 GMT   (122kb,D)

Title: Utilizing the LightGBM Algorithm for Operator User Credit Assessment
  Research
Authors: Shaojie Li, Xinqi Dong, Danqing Ma, Bo Dang, Hengyi Zang, and Yulu
  Gong
Categories: cs.LG cs.AI
\\
  Mobile Internet user credit assessment is an important way for communication
operators to establish decisions and formulate measures, and it is also a
guarantee for operators to obtain expected benefits. However, credit evaluation
methods have long been monopolized by financial industries such as banks and
credit. As supporters and providers of platform network technology and network
resources, communication operators are also builders and maintainers of
communication networks. Internet data improves the user's credit evaluation
strategy. This paper uses the massive data provided by communication operators
to carry out research on the operator's user credit evaluation model based on
the fusion LightGBM algorithm. First, for the massive data related to user
evaluation provided by operators, key features are extracted by data
preprocessing and feature engineering methods, and a multi-dimensional feature
set with statistical significance is constructed; then, linear regression,
decision tree, LightGBM, and other machine learning algorithms build multiple
basic models to find the best basic model; finally, integrates Averaging,
Voting, Blending, Stacking and other integrated algorithms to refine multiple
fusion models, and finally establish the most suitable fusion model for
operator user evaluation.
\\ ( https://arxiv.org/abs/2403.14483 ,  122kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14484
Date: Thu, 21 Mar 2024 15:31:28 GMT   (11662kb,D)

Title: HyperGALE: ASD Classification via Hypergraph Gated Attention with
  Learnable Hyperedges
Authors: Mehul Arora, Chirag Shantilal Jain, Lalith Bharadwaj Baru, Kamalaker
  Dadi, Bapi Raju Surampudi
Categories: cs.LG cs.AI cs.CV cs.NE
Comments: Accepted to IJCNN 2024
\\
  Autism Spectrum Disorder (ASD) is a neurodevelopmental condition
characterized by varied social cognitive challenges and repetitive behavioral
patterns. Identifying reliable brain imaging-based biomarkers for ASD has been
a persistent challenge due to the spectrum's diverse symptomatology. Existing
baselines in the field have made significant strides in this direction, yet
there remains room for improvement in both performance and interpretability. We
propose \emph{HyperGALE}, which builds upon the hypergraph by incorporating
learned hyperedges and gated attention mechanisms. This approach has led to
substantial improvements in the model's ability to interpret complex brain
graph data, offering deeper insights into ASD biomarker characterization.
Evaluated on the extensive ABIDE II dataset, \emph{HyperGALE} not only improves
interpretability but also demonstrates statistically significant enhancements
in key performance metrics compared to both previous baselines and the
foundational hypergraph model. The advancement \emph{HyperGALE} brings to ASD
research highlights the potential of sophisticated graph-based techniques in
neurodevelopmental studies. The source code and implementation instructions are
available at GitHub:https://github.com/mehular0ra/HyperGALE.
\\ ( https://arxiv.org/abs/2403.14484 ,  11662kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14504
Date: Thu, 21 Mar 2024 15:56:15 GMT   (473kb,D)

Title: Soft Learning Probabilistic Circuits
Authors: Soroush Ghandi, Benjamin Quost, Cassio de Campos
Categories: cs.LG cs.AI
\\
  Probabilistic Circuits (PCs) are prominent tractable probabilistic models,
allowing for a range of exact inferences. This paper focuses on the main
algorithm for training PCs, LearnSPN, a gold standard due to its efficiency,
performance, and ease of use, in particular for tabular data. We show that
LearnSPN is a greedy likelihood maximizer under mild assumptions. While
inferences in PCs may use the entire circuit structure for processing queries,
LearnSPN applies a hard method for learning them, propagating at each sum node
a data point through one and only one of the children/edges as in a hard
clustering process. We propose a new learning procedure named SoftLearn, that
induces a PC using a soft clustering process. We investigate the effect of this
learning-inference compatibility in PCs. Our experiments show that SoftLearn
outperforms LearnSPN in many situations, yielding better likelihoods and
arguably better samples. We also analyze comparable tractable models to
highlight the differences between soft/hard learning and model querying.
\\ ( https://arxiv.org/abs/2403.14504 ,  473kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14508
Date: Thu, 21 Mar 2024 16:02:52 GMT   (8781kb,D)

Title: Constrained Reinforcement Learning with Smoothed Log Barrier Function
Authors: Baohe Zhang, Yuan Zhang, Lilli Frison, Thomas Brox, Joschka B\"odecker
Categories: cs.LG cs.AI cs.SY eess.SY
\\
  Reinforcement Learning (RL) has been widely applied to many control tasks and
substantially improved the performances compared to conventional control
methods in many domains where the reward function is well defined. However, for
many real-world problems, it is often more convenient to formulate optimization
problems in terms of rewards and constraints simultaneously. Optimizing such
constrained problems via reward shaping can be difficult as it requires tedious
manual tuning of reward functions with several interacting terms. Recent
formulations which include constraints mostly require a pre-training phase,
which often needs human expertise to collect data or assumes having a
sub-optimal policy readily available. We propose a new constrained RL method
called CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), which
achieves competitive performance without any pre-training by applying a linear
smoothed log barrier function to an additional safety critic. It implements an
adaptive penalty for policy learning and alleviates the numerical issues that
are known to complicate the application of the log barrier function method. As
a result, we show that with CSAC-LB, we achieve state-of-the-art performance on
several constrained control tasks with different levels of difficulty and
evaluate our methods in a locomotion task on a real quadruped robot platform.
\\ ( https://arxiv.org/abs/2403.14508 ,  8781kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14578
Date: Thu, 21 Mar 2024 17:30:59 GMT   (73kb,D)

Title: RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants
  in the Biomedical Domain
Authors: William James Bolton, Rafael Poyiadzi, Edward R. Morrell, Gabriela van
  Bergen Gonzalez Bueno, Lea Goetz
Categories: cs.LG
Comments: Published at ICLR 2024 Workshop on Reliable and Responsible
  Foundation Models
\\
  Large Language Models (LLMs) increasingly support applications in a wide
range of domains, some with potential high societal impact such as biomedicine,
yet their reliability in realistic use cases is under-researched. In this work
we introduce the Reliability AssesMent for Biomedical LLM Assistants (RAmBLA)
framework and evaluate whether four state-of-the-art foundation LLMs can serve
as reliable assistants in the biomedical domain. We identify prompt robustness,
high recall, and a lack of hallucinations as necessary criteria for this use
case. We design shortform tasks and tasks requiring LLM freeform responses
mimicking real-world user interactions. We evaluate LLM performance using
semantic similarity with a ground truth response, through an evaluator LLM.
\\ ( https://arxiv.org/abs/2403.14578 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14587
Date: Thu, 21 Mar 2024 17:42:45 GMT   (768kb,D)

Title: An Analysis of Linear Time Series Forecasting Models
Authors: William Toner, Luke Darlow
Categories: cs.LG
\\
  Despite their simplicity, linear models perform well at time series
forecasting, even when pitted against deeper and more expensive models. A
number of variations to the linear model have been proposed, often including
some form of feature normalisation that improves model generalisation. In this
paper we analyse the sets of functions expressible using these linear model
architectures. In so doing we show that several popular variants of linear
models for time series forecasting are equivalent and functionally
indistinguishable from standard, unconstrained linear regression. We
characterise the model classes for each linear variant. We demonstrate that
each model can be reinterpreted as unconstrained linear regression over a
suitably augmented feature set, and therefore admit closed-form solutions when
using a mean-squared loss function. We provide experimental evidence that the
models under inspection learn nearly identical solutions, and finally
demonstrate that the simpler closed form solutions are superior forecasters
across 72% of test settings.
\\ ( https://arxiv.org/abs/2403.14587 ,  768kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14593
Date: Thu, 21 Mar 2024 17:48:38 GMT   (736kb,D)

Title: Rethinking Adversarial Inverse Reinforcement Learning: From the Angles
  of Policy Imitation and Transferable Reward Recovery
Authors: Yangchun Zhang, Yirui Zhou
Categories: cs.LG stat.ML
\\
  Adversarial inverse reinforcement learning (AIRL) stands as a cornerstone
approach in imitation learning. This paper rethinks the two different angles of
AIRL: policy imitation and transferable reward recovery. We begin with
substituting the built-in algorithm in AIRL with soft actor-critic (SAC) during
the policy optimization process to enhance sample efficiency, thanks to the
off-policy formulation of SAC and identifiable Markov decision process (MDP)
models with respect to AIRL. It indeed exhibits a significant improvement in
policy imitation but accidentally brings drawbacks to transferable reward
recovery. To learn this issue, we illustrate that the SAC algorithm itself is
not feasible to disentangle the reward function comprehensively during the AIRL
training process, and propose a hybrid framework, PPO-AIRL + SAC, for
satisfactory transfer effect. Additionally, we analyze the capability of
environments to extract disentangled rewards from an algebraic theory
perspective.
\\ ( https://arxiv.org/abs/2403.14593 ,  736kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14606
Date: Thu, 21 Mar 2024 17:55:16 GMT   (1921kb,D)

Title: The Elements of Differentiable Programming
Authors: Mathieu Blondel, Vincent Roulet
Categories: cs.LG
Comments: Draft version 1
\\
  Artificial intelligence has recently experienced remarkable advances, fueled
by large models, vast datasets, accelerated hardware, and, last but not least,
the transformative power of differentiable programming. This new programming
paradigm enables end-to-end differentiation of complex computer programs
(including those with control flows and data structures), making gradient-based
optimization of program parameters possible.
  As an emerging paradigm, differentiable programming builds upon several areas
of computer science and applied mathematics, including automatic
differentiation, graphical models, optimization and statistics. This book
presents a comprehensive review of the fundamental concepts useful for
differentiable programming. We adopt two main perspectives, that of
optimization and that of probability, with clear analogies between the two.
  Differentiable programming is not merely the differentiation of programs, but
also the thoughtful design of programs intended for differentiation. By making
programs differentiable, we inherently introduce probability distributions over
their execution, providing a means to quantify the uncertainty associated with
program outputs.
\\ ( https://arxiv.org/abs/2403.14606 ,  1921kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14608
Date: Thu, 21 Mar 2024 17:55:50 GMT   (3747kb,D)

Title: Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey
Authors: Zeyu Han, Chao Gao, Jinyang Liu, Jeff (Jun) Zhang, Sai Qian Zhang
Categories: cs.LG
Comments: 25 pages, 13 figures
\\
  Large models represent a groundbreaking advancement in multiple application
fields, enabling remarkable achievements across various tasks. However, their
unprecedented scale comes with significant computational costs. These models,
often consisting of billions of parameters, require vast amounts of
computational resources for execution. Especially, the expansive scale and
computational demands pose considerable challenges when customizing them for
particular downstream tasks, particularly over the hardware platforms
constrained by computational capabilities. Parameter Efficient Fine-Tuning
(PEFT) provides a practical solution by efficiently adapt the large models over
the various downstream tasks. In particular, PEFT refers to the process of
adjusting the parameters of a pre-trained large models to adapt it to a
specific task while minimizing the number of additional parameters introduced
or computational resources required. This approach is particularly important
when dealing with large language models with high parameter counts, as
fine-tuning these models from scratch can be computationally expensive and
resource-intensive, posing considerable challenges in the supporting system
platform design. In this survey, we present comprehensive studies of various
PEFT algorithms, examining their performance and computational overhead.
Moreover, we provide an overview of applications developed using different PEFT
algorithms and discuss common techniques employed to mitigate computation costs
for PEFT. In addition to the algorithmic perspective, we overview various
real-world system designs to investigate the implementation costs associated
with different PEFT algorithms. This survey serves as an indispensable resource
for researchers aiming to understand both the PEFT algorithm and its system
implementation, offering detailed insights into recent advancements and
practical applications.
\\ ( https://arxiv.org/abs/2403.14608 ,  3747kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14623
Date: Thu, 21 Mar 2024 17:59:41 GMT   (16363kb,D)

Title: Simplified Diffusion Schr\"odinger Bridge
Authors: Zhicong Tang, Tiankai Hang, Shuyang Gu, Dong Chen, Baining Guo
Categories: cs.LG cs.CV
\\
  This paper introduces a novel theoretical simplification of the Diffusion
Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based
Generative Models (SGMs), addressing the limitations of DSB in complex data
generation and enabling faster convergence and enhanced performance. By
employing SGMs as an initial solution for DSB, our approach capitalizes on the
strengths of both frameworks, ensuring a more efficient training process and
improving the performance of SGM. We also propose a reparameterization
technique that, despite theoretical approximations, practically improves the
network's fitting capabilities. Our extensive experimental evaluations confirm
the effectiveness of the simplified DSB, demonstrating its significant
improvements. We believe the contributions of this work pave the way for
advanced generative modeling. The code is available at
https://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.
\\ ( https://arxiv.org/abs/2403.14623 ,  16363kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2402.17128 (*cross-listing*)
Date: Tue, 27 Feb 2024 01:48:19 GMT   (11983kb,D)
Date (revised v2): Wed, 28 Feb 2024 02:48:31 GMT   (11972kb,D)
Date (revised v3): Wed, 20 Mar 2024 14:49:16 GMT   (12023kb,D)

Title: OSCaR: Object State Captioning and State Change Representation
Authors: Nguyen Nguyen, Jing Bi, Ali Vosoughi, Yapeng Tian, Pooyan Fazli,
  Chenliang Xu
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: NAACL 2024
\\
  The capability of intelligent models to extrapolate and comprehend changes in
object states is a crucial yet demanding aspect of AI research, particularly
through the lens of human interaction in real-world settings. This task
involves describing complex visual environments, identifying active objects,
and interpreting their changes as conveyed through language. Traditional
methods, which isolate object captioning and state change detection, offer a
limited view of dynamic environments. Moreover, relying on a small set of
symbolic words to represent changes has restricted the expressiveness of the
language. To address these challenges, in this paper, we introduce the Object
State Captioning and State Change Representation (OSCaR) dataset and benchmark.
OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique
objects from various egocentric video collections. It sets a new testbed for
evaluating multimodal large language models (MLLMs). Our experiments
demonstrate that while MLLMs show some skill, they lack a full understanding of
object state changes. The benchmark includes a fine-tuned model that, despite
initial capabilities, requires significant improvements in accuracy and
generalization ability for effective understanding of these changes. Our code
and dataset are available at https://github.com/nguyennm1024/OSCaR.
\\ ( https://arxiv.org/abs/2402.17128 ,  12023kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13809 (*cross-listing*)
Date: Fri, 22 Dec 2023 17:27:50 GMT   (1826kb)

Title: Predicting Confinement Effect of Carbon Fiber Reinforced Polymers on
  Strength of Concrete using Metaheuristics-based Artificial Neural Networks
Authors: Sarmed Wahab, Mohamed Suleiman, Faisal Shabbir, Nasim Shakouri
  Mahmoudabadi, Sarmad Waqas, Nouman Herl, Afaq Ahmad
Categories: cs.NE cs.AI
Comments: 28 Pages, 19 Figures
\\
  This article deals with the study of predicting the confinement effect of
carbon fiber reinforced polymers (CFRPs) on concrete cylinder strength using
metaheuristics-based artificial neural networks. A detailed database of 708
CFRP confined concrete cylinders is developed from previously published
research with information on 8 parameters including geometrical parameters like
the diameter (d) and height (h) of a cylinder, unconfined compressive strength
of concrete (fco'), thickness (nt), the elastic modulus of CFRP (Ef),
unconfined concrete strain confined concrete strain and the ultimate
compressive strength of confined concrete fcc'. Three metaheuristic models are
implemented including particle swarm optimization (PSO), grey wolf optimizer
(GWO), and bat algorithm (BA). These algorithms are trained on the data using
an objective function of mean square error and their predicted results are
validated against the experimental studies and finite element analysis. The
study shows that the hybrid model of PSO predicted the strength of
CFRP-confined concrete cylinders with maximum accuracy of 99.13% and GWO
predicted the results with an accuracy of 98.17%. The high accuracy of axial
compressive strength predictions demonstrated that these prediction models are
a reliable solution to the empirical methods. The prediction models are
especially suitable for avoiding full-scale time-consuming experimental tests
that make the process quick and economical.
\\ ( https://arxiv.org/abs/2403.13809 ,  1826kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13812 (*cross-listing*)
Date: Fri, 9 Feb 2024 17:20:48 GMT   (529kb)

Title: Quantitative Analysis of AI-Generated Texts in Academic Research: A
  Study of AI Presence in Arxiv Submissions using AI Detection Tool
Authors: Arslan Akram
Categories: cs.DL cs.AI cs.CL cs.CY cs.LG stat.OT
Comments: 8 pages, 6 figures, 1 table
MSC-class: 62P25
ACM-class: I.7; G.1; G.3
\\
  Many people are interested in ChatGPT since it has become a prominent AIGC
model that provides high-quality responses in various contexts, such as
software development and maintenance. Misuse of ChatGPT might cause significant
issues, particularly in public safety and education, despite its immense
potential. The majority of researchers choose to publish their work on Arxiv.
The effectiveness and originality of future work depend on the ability to
detect AI components in such contributions. To address this need, this study
will analyze a method that can see purposely manufactured content that academic
organizations use to post on Arxiv. For this study, a dataset was created using
physics, mathematics, and computer science articles. Using the newly built
dataset, the following step is to put originality.ai through its paces. The
statistical analysis shows that Originality.ai is very accurate, with a rate of
98%.
\\ ( https://arxiv.org/abs/2403.13812 ,  529kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13825 (*cross-listing*)
Date: Tue, 5 Mar 2024 23:12:47 GMT   (18311kb,D)

Title: Deep Generative Models for Ultra-High Granularity Particle Physics
  Detector Simulation: A Voyage From Emulation to Extrapolation
Authors: Baran Hashemi
Categories: physics.ins-det cs.AI cs.LG hep-ex hep-ph
Comments: PhD thesis, 234 pages
\\
  Simulating ultra-high-granularity detector responses in Particle Physics
represents a critical yet computationally demanding task. This thesis aims to
overcome this challenge for the Pixel Vertex Detector (PXD) at the Belle II
experiment, which features over 7.5M pixel channels-the highest spatial
resolution detector simulation dataset ever analysed with generative models.
This thesis starts off by a comprehensive and taxonomic review on generative
models for simulating detector signatures. Then, it presents the Intra-Event
Aware Generative Adversarial Network (IEA-GAN), a new geometry-aware generative
model that introduces a relational attentive reasoning and Self-Supervised
Learning to approximate an "event" in the detector. This study underscores the
importance of intra-event correlation for downstream physics analyses. Building
upon this, the work drifts towards a more generic approach and presents
YonedaVAE, a Category Theory-inspired generative model that tackles the open
problem of Out-of-Distribution (OOD) simulation. YonedaVAE introduces a
learnable Yoneda embedding to capture the entirety of an event based on its
sensor relationships, formulating a Category theoretical language for
intra-event relational reasoning. This is complemented by introducing a
Self-Supervised learnable prior for VAEs and an Adaptive Top-q sampling
mechanism, enabling the model to sample point clouds with variable
intra-category cardinality in a zero-shot manner. Variable Intra-event
cardinality has not been approached before and is vital for simulating
irregular detector geometries. Trained on an early experiment data, YonedaVAE
can reach a reasonable OOD simulation precision of a later experiment with
almost double luminosity. This study introduces, for the first time, the
results of using deep generative models for ultra-high granularity detector
simulation in Particle Physics.
\\ ( https://arxiv.org/abs/2403.13825 ,  18311kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13890 (*cross-listing*)
Date: Wed, 20 Mar 2024 18:01:57 GMT   (2289kb,D)

Title: Towards Learning Contrast Kinetics with Multi-Condition Latent Diffusion
  Models
Authors: Richard Osuala and Daniel Lang and Preeti Verma and Smriti Joshi and
  Apostolia Tsirikoglou and Grzegorz Skorupko and Kaisar Kushibar and Lidia
  Garrucho and Walter H. L. Pinaya and Oliver Diaz and Julia Schnabel and Karim
  Lekadir
Categories: cs.CV cs.AI cs.LG
\\
  Contrast agents in dynamic contrast enhanced magnetic resonance imaging allow
to localize tumors and observe their contrast kinetics, which is essential for
cancer characterization and respective treatment decision-making. However,
contrast agent administration is not only associated with adverse health risks,
but also restricted for patients during pregnancy, and for those with kidney
malfunction, or other adverse reactions. With contrast uptake as key biomarker
for lesion malignancy, cancer recurrence risk, and treatment response, it
becomes pivotal to reduce the dependency on intravenous contrast agent
administration. To this end, we propose a multi-conditional latent diffusion
model capable of acquisition time-conditioned image synthesis of DCE-MRI
temporal sequences. To evaluate medical image synthesis, we additionally
propose and validate the Fr\'echet radiomics distance as an image quality
measure based on biomarker variability between synthetic and real imaging data.
Our results demonstrate our method's ability to generate realistic
multi-sequence fat-saturated breast DCE-MRI and uncover the emerging potential
of deep learning based contrast kinetics simulation. We publicly share our
accessible codebase at https://github.com/RichardObi/ccnet.
\\ ( https://arxiv.org/abs/2403.13890 ,  2289kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13947 (*cross-listing*)
Date: Wed, 20 Mar 2024 19:41:05 GMT   (27930kb,D)

Title: BlendScape: Enabling Unified and Personalized Video-Conferencing
  Environments through Generative AI
Authors: Shwetha Rajaram, Nels Numan, Balasaravanan Thoravi Kumaravel, Nicolai
  Marquardt, Andrew D. Wilson
Categories: cs.HC cs.AI
\\
  Today's video-conferencing tools support a rich range of professional and
social activities, but their generic, grid-based environments cannot be easily
adapted to meet the varying needs of distributed collaborators. To enable
end-user customization, we developed BlendScape, a system for meeting
participants to compose video-conferencing environments tailored to their
collaboration context by leveraging AI image generation techniques. BlendScape
supports flexible representations of task spaces by blending users' physical or
virtual backgrounds into unified environments and implements multimodal
interaction techniques to steer the generation. Through an evaluation with 15
end-users, we investigated their customization preferences for work and social
scenarios. Participants could rapidly express their design intentions with
BlendScape and envisioned using the system to structure collaboration in future
meetings, but experienced challenges with preventing distracting elements. We
implement scenarios to demonstrate BlendScape's expressiveness in supporting
distributed collaboration techniques from prior work and propose composition
techniques to improve the quality of environments.
\\ ( https://arxiv.org/abs/2403.13947 ,  27930kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13950 (*cross-listing*)
Date: Wed, 20 Mar 2024 19:42:11 GMT   (4583kb)

Title: Evo* 2023 -- Late-Breaking Abstracts Volume
Authors: A.M. Mora, A.I. Esparcia-Alc\'azar
Categories: cs.NE cs.AI cs.LG
Comments: LBAs accepted in Evo* 2023. Part of the Conference Proceedings
MSC-class: 68T05, 68W20
ACM-class: I.0; I.2; K.4
\\
  Volume with the Late-Breaking Abstracts submitted to the Evo* 2023
Conference, held in Brno (Czech Republic), from 12 to 14 of April. These papers
present ongoing research and preliminary results investigating on the
application of different approaches of Bioinspired Methods (mainly Evolutionary
Computation) to different problems, most of them real world ones.
\\ ( https://arxiv.org/abs/2403.13950 ,  4583kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13951 (*cross-listing*)
Date: Wed, 20 Mar 2024 19:45:06 GMT   (41276kb,D)

Title: ACDG-VTON: Accurate and Contained Diffusion Generation for Virtual
  Try-On
Authors: Jeffrey Zhang, Kedan Li, Shao-Yu Chang, David Forsyth
Categories: cs.CV cs.AI
\\
  Virtual Try-on (VTON) involves generating images of a person wearing selected
garments. Diffusion-based methods, in particular, can create high-quality
images, but they struggle to maintain the identities of the input garments. We
identified this problem stems from the specifics in the training formulation
for diffusion. To address this, we propose a unique training scheme that limits
the scope in which diffusion is trained. We use a control image that perfectly
aligns with the target image during training. In turn, this accurately
preserves garment details during inference. We demonstrate our method not only
effectively conserves garment details but also allows for layering, styling,
and shoe try-on. Our method runs multi-garment try-on in a single inference
cycle and can support high-quality zoomed-in generations without training in
higher resolutions. Finally, we show our method surpasses prior methods in
accuracy and quality.
\\ ( https://arxiv.org/abs/2403.13951 ,  41276kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13960 (*cross-listing*)
Date: Wed, 20 Mar 2024 20:13:39 GMT   (615kb,D)

Title: Open Access NAO (OAN): a ROS2-based software framework for HRI
  applications with the NAO robot
Authors: Antonio Bono, Kenji Brameld, Luigi D'Alfonso and Giuseppe Fedele
Categories: cs.RO cs.AI
Comments: 7 pages, 3 figures
\\
  This paper presents a new software framework for HRI experimentation with the
sixth version of the common NAO robot produced by the United Robotics Group.
Embracing the common demand of researchers for better performance and new
features for NAO, the authors took advantage of the ability to run ROS2 onboard
on the NAO to develop a framework independent of the APIs provided by the
manufacturer. Such a system provides NAO with not only the basic skills of a
humanoid robot such as walking and reproducing movements of interest but also
features often used in HRI such as: speech recognition/synthesis, face and
object detention, and the use of Generative Pre-trained Transformer (GPT)
models for conversation. The developed code is therefore configured as a
ready-to-use but also highly expandable and improvable tool thanks to the
possibilities provided by the ROS community.
\\ ( https://arxiv.org/abs/2403.13960 ,  615kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13969 (*cross-listing*)
Date: Wed, 20 Mar 2024 20:46:41 GMT   (630kb,D)

Title: "This is not a data problem": Algorithms and Power in Public Higher
  Education in Canada
Authors: Kelly McConvey and Shion Guha
Categories: cs.HC cs.AI cs.CY
Comments: In CHI '24 Proceedings of the CHI Conference on Human Factors in
  Computing Systems Honolulu, HI, USA
DOI: 10.1145/3613904.3642451
\\
  Algorithmic decision-making is increasingly being adopted across public
higher education. The expansion of data-driven practices by post-secondary
institutions has occurred in parallel with the adoption of New Public
Management approaches by neoliberal administrations. In this study, we conduct
a qualitative analysis of an in-depth ethnographic case study of data and
algorithms in use at a public college in Ontario, Canada. We identify the data,
algorithms, and outcomes in use at the college. We assess how the college's
processes and relationships support those outcomes and the different
stakeholders' perceptions of the college's data-driven systems. In addition, we
find that the growing reliance on algorithmic decisions leads to increased
student surveillance, exacerbation of existing inequities, and the automation
of the faculty-student relationship. Finally, we identify a cycle of increased
institutional power perpetuated by algorithmic decision-making, and driven by a
push towards financial sustainability.
\\ ( https://arxiv.org/abs/2403.13969 ,  630kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14019 (*cross-listing*)
Date: Wed, 20 Mar 2024 22:40:53 GMT   (2266kb,D)

Title: Searching Search Spaces: Meta-evolving a Geometric Encoding for Neural
  Networks
Authors: Tarek Kunze, Paul Templier, Dennis G Wilson
Categories: cs.NE cs.AI
Comments: 9 pages, 8 figures
ACM-class: I.2.6
\\
  In evolutionary policy search, neural networks are usually represented using
a direct mapping: each gene encodes one network weight. Indirect encoding
methods, where each gene can encode for multiple weights, shorten the genome to
reduce the dimensions of the search space and better exploit permutations and
symmetries. The Geometric Encoding for Neural network Evolution (GENE)
introduced an indirect encoding where the weight of a connection is computed as
the (pseudo-)distance between the two linked neurons, leading to a genome size
growing linearly with the number of genes instead of quadratically in direct
encoding. However GENE still relies on hand-crafted distance functions with no
prior optimization. Here we show that better performing distance functions can
be found for GENE using Cartesian Genetic Programming (CGP) in a meta-evolution
approach, hence optimizing the encoding to create a search space that is easier
to exploit. We show that GENE with a learned function can outperform both
direct encoding and the hand-crafted distances, generalizing on unseen
problems, and we study how the encoding impacts neural network properties.
\\ ( https://arxiv.org/abs/2403.14019 ,  2266kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14049 (*cross-listing*)
Date: Thu, 21 Mar 2024 00:14:53 GMT   (6416kb,D)

Title: A Roadmap Towards Automated and Regulated Robotic Systems
Authors: Yihao Liu and Mehran Armand
Categories: cs.RO cs.AI cs.HC
Comments: 17 pages, 9 figures
\\
  The rapid development of generative technology opens up possibility for
higher level of automation, and artificial intelligence (AI) embodiment in
robotic systems is imminent. However, due to the blackbox nature of the
generative technology, the generation of the knowledge and workflow scheme is
uncontrolled, especially in a dynamic environment and a complex scene. This
poses challenges to regulations in safety-demanding applications such as
medical scenes. We argue that the unregulated generative processes from AI is
fitted for low level end tasks, but intervention in the form of manual or
automated regulation should happen post-workflow-generation and
pre-robotic-execution. To address this, we propose a roadmap that can lead to
fully automated and regulated robotic systems. In this paradigm, the high level
policies are generated as structured graph data, enabling regulatory oversight
and reusability, while the code base for lower level tasks is generated by
generative models. Our approach aims the transitioning from expert knowledge to
regulated action, akin to the iterative processes of study, practice, scrutiny,
and execution in human tasks. We identify the generative and deterministic
processes in a design cycle, where generative processes serve as a text-based
world simulator and the deterministic processes generate the executable system.
We propose State Machine Seralization Language (SMSL) to be the conversion
point between text simulator and executable workflow control. From there, we
analyze the modules involved based on the current literature, and discuss human
in the loop. As a roadmap, this work identifies the current possible
implementation and future work. This work does not provide an implemented
system but envisions to inspire the researchers working on the direction in the
roadmap. We implement the SMSL and D-SFO paradigm that serve as the starting
point of the roadmap.
\\ ( https://arxiv.org/abs/2403.14049 ,  6416kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14119 (*cross-listing*)
Date: Thu, 21 Mar 2024 04:08:29 GMT   (5980kb,D)

Title: C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via
  Text Feature Dispersion
Authors: Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Mark
  Hasegawa-Johnson, Yingzhen Li, Chang D. Yoo
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: ICLR 2024
\\
  In deep learning, test-time adaptation has gained attention as a method for
model fine-tuning without the need for labeled data. A prime exemplification is
the recently proposed test-time prompt tuning for large-scale vision-language
models such as CLIP. Unfortunately, these prompts have been mainly developed to
improve accuracy, overlooking the importance of calibration-a crucial aspect
for quantifying prediction uncertainty. However, traditional calibration
methods rely on substantial amounts of labeled data, making them impractical
for test-time scenarios. To this end, this paper explores calibration during
test-time prompt tuning by leveraging the inherent properties of CLIP. Through
a series of observations, we find that the prompt choice significantly affects
the calibration in CLIP, where the prompts leading to higher text feature
dispersion result in better-calibrated predictions. Introducing the Average
Text Feature Dispersion (ATFD), we establish its relationship with calibration
error and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT),
for optimizing prompts during test-time with enhanced calibration. Through
extensive experiments on different CLIP architectures and datasets, we show
that C-TPT can effectively improve the calibration of test-time prompt tuning
without needing labeled data.
\\ ( https://arxiv.org/abs/2403.14119 ,  5980kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14146 (*cross-listing*)
Date: Thu, 21 Mar 2024 05:42:17 GMT   (33400kb,D)

Title: Evolving Benchmark Functions to Compare Evolutionary Algorithms via
  Genetic Programming
Authors: Yifan He and Claus Aranha
Categories: cs.NE cs.AI
\\
  In this study, we use Genetic Programming (GP) to compose new optimization
benchmark functions. Optimization benchmarks have the important role of showing
the differences between evolutionary algorithms, making it possible for further
analysis and comparisons. We show that the benchmarks generated by GP are able
to differentiate algorithms better than human-made benchmark functions. The
fitness measure of the GP is the Wasserstein distance of the solutions found by
a pair of optimizers. Additionally, we use MAP-Elites to both enhance the
search power of the GP and also illustrate how the difference between
optimizers changes by various landscape features. Our approach provides a novel
way to automate the design of benchmark functions and to compare evolutionary
algorithms.
\\ ( https://arxiv.org/abs/2403.14146 ,  33400kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14163 (*cross-listing*)
Date: Thu, 21 Mar 2024 06:32:36 GMT   (26855kb,D)

Title: Leveraging Large Language Model-based Room-Object Relationships
  Knowledge for Enhancing Multimodal-Input Object Goal Navigation
Authors: Leyuan Sun, Asako Kanezaki, Guillaume Caron, Yusuke Yoshiyasu
Categories: cs.RO cs.AI cs.CV
Comments: will soon submit to the Elsevier journal, Advanced Engineering
  Informatics
\\
  Object-goal navigation is a crucial engineering task for the community of
embodied navigation; it involves navigating to an instance of a specified
object category within unseen environments. Although extensive investigations
have been conducted on both end-to-end and modular-based, data-driven
approaches, fully enabling an agent to comprehend the environment through
perceptual knowledge and perform object-goal navigation as efficiently as
humans remains a significant challenge. Recently, large language models have
shown potential in this task, thanks to their powerful capabilities for
knowledge extraction and integration. In this study, we propose a data-driven,
modular-based approach, trained on a dataset that incorporates common-sense
knowledge of object-to-room relationships extracted from a large language
model. We utilize the multi-channel Swin-Unet architecture to conduct
multi-task learning incorporating with multimodal inputs. The results in the
Habitat simulator demonstrate that our framework outperforms the baseline by an
average of 10.6% in the efficiency metric, Success weighted by Path Length
(SPL). The real-world demonstration shows that the proposed approach can
efficiently conduct this task by traversing several rooms. For more details and
real-world demonstrations, please check our project webpage
(https://sunleyuan.github.io/ObjectNav).
\\ ( https://arxiv.org/abs/2403.14163 ,  26855kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14183 (*cross-listing*)
Date: Thu, 21 Mar 2024 07:15:37 GMT   (29827kb,D)

Title: OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic
  Segmentation
Authors: Kwanyoung Kim, Yujin Oh, Jong Chul Ye
Categories: cs.CV cs.AI cs.LG stat.ML
Comments: 22 pages, 7 figures
\\
  The recent success of CLIP has demonstrated promising results in zero-shot
semantic segmentation by transferring muiltimodal knowledge to pixel-level
classification. However, leveraging pre-trained CLIP knowledge to closely align
text embeddings with pixel embeddings still has limitations in existing
approaches. To address this issue, we propose OTSeg, a novel multimodal
attention mechanism aimed at enhancing the potential of multiple text prompts
for matching associated pixel embeddings. We first propose Multi-Prompts
Sinkhorn (MPS) based on the Optimal Transport (OT) algorithm, which leads
multiple text prompts to selectively focus on various semantic features within
image pixels. Moreover, inspired by the success of Sinkformers in unimodal
settings, we introduce the extension of MPS, called Multi-Prompts Sinkhorn
Attention (MPSA), which effectively replaces cross-attention mechanisms within
Transformer framework in multimodal settings. Through extensive experiments, we
demonstrate that OTSeg achieves state-of-the-art (SOTA) performance with
significant gains on Zero-Shot Semantic Segmentation (ZS3) tasks across three
benchmark datasets.
\\ ( https://arxiv.org/abs/2403.14183 ,  29827kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14186 (*cross-listing*)
Date: Thu, 21 Mar 2024 07:21:51 GMT   (48188kb,D)

Title: StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained
  StyleGAN
Authors: Jongwoo Choi, Kwanggyoon Seo, Amirsaman Ashtari, Junyong Noh
Categories: cs.CV cs.AI cs.GR
Comments: Project website: https://jeolpyeoni.github.io/stylecinegan_project/
\\
  We propose a method that can generate cinemagraphs automatically from a still
landscape image using a pre-trained StyleGAN. Inspired by the success of recent
unconditional video generation, we leverage a powerful pre-trained image
generator to synthesize high-quality cinemagraphs. Unlike previous approaches
that mainly utilize the latent space of a pre-trained StyleGAN, our approach
utilizes its deep feature space for both GAN inversion and cinemagraph
generation. Specifically, we propose multi-scale deep feature warping (MSDFW),
which warps the intermediate features of a pre-trained StyleGAN at different
resolutions. By using MSDFW, the generated cinemagraphs are of high resolution
and exhibit plausible looping animation. We demonstrate the superiority of our
method through user studies and quantitative comparisons with state-of-the-art
cinemagraph generation methods and a video generation method that uses a
pre-trained StyleGAN.
\\ ( https://arxiv.org/abs/2403.14186 ,  48188kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14188 (*cross-listing*)
Date: Thu, 21 Mar 2024 07:25:52 GMT   (15222kb,D)

Title: Quantum-activated neural reservoirs on-chip open up large hardware
  security models for resilient authentication
Authors: Zhao He, Maxim S. Elizarov, Ning Li, Fei Xiang, and Andrea Fratalocchi
Categories: cond-mat.dis-nn cs.AI cs.CR
\\
  Quantum artificial intelligence is a frontier of artificial intelligence
research, pioneering quantum AI-powered circuits to address problems beyond the
reach of deep learning with classical architectures. This work implements a
large-scale quantum-activated recurrent neural network possessing more than 3
trillion hardware nodes/cm$^2$, originating from repeatable atomic-scale
nucleation dynamics in an amorphous material integrated on-chip, controlled
with 0.07 nW electric power per readout channel. Compared to the
best-performing reservoirs currently reported, this implementation increases
the scale of the network by two orders of magnitude and reduces the power
consumption by six, reaching power efficiencies in the range of the human
brain, dissipating 0.2 nW/neuron. When interrogated by a classical input, the
chip implements a large-scale hardware security model, enabling dictionary-free
authentication secure against statistical inference attacks, including AI's
present and future development, even for an adversary with a copy of all the
classical components available. Experimental tests report 99.6% reliability,
100% user authentication accuracy, and an ideal 50% key uniqueness. Due to its
quantum nature, the chip supports a bit density per feature size area three
times higher than the best technology available, with the capacity to store
more than $2^{1104}$ keys in a footprint of 1 cm$^2$. Such a quantum-powered
platform could help counteract the emerging form of warfare led by the
cybercrime industry in breaching authentication to target small to large-scale
facilities, from private users to intelligent energy grids.
\\ ( https://arxiv.org/abs/2403.14188 ,  15222kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14203 (*cross-listing*)
Date: Thu, 21 Mar 2024 07:56:09 GMT   (6195kb,D)

Title: Unsupervised Audio-Visual Segmentation with Modality Alignment
Authors: Swapnil Bhosale, Haosen Yang, Diptesh Kanojia, Jiangkang Deng, Xiatian
  Zhu
Categories: cs.CV cs.AI
\\
  Audio-Visual Segmentation (AVS) aims to identify, at the pixel level, the
object in a visual scene that produces a given sound. Current AVS methods rely
on costly fine-grained annotations of mask-audio pairs, making them impractical
for scalability. To address this, we introduce unsupervised AVS, eliminating
the need for such expensive annotation. To tackle this more challenging
problem, we propose an unsupervised learning method, named Modality
Correspondence Alignment (MoCA), which seamlessly integrates off-the-shelf
foundation models like DINO, SAM, and ImageBind. This approach leverages their
knowledge complementarity and optimizes their joint usage for multi-modality
association. Initially, we estimate positive and negative image pairs in the
feature space. For pixel-level association, we introduce an audio-visual
adapter and a novel pixel matching aggregation strategy within the image-level
contrastive learning framework. This allows for a flexible connection between
object appearance and audio signal at the pixel level, with tolerance to
imaging variations such as translation and rotation. Extensive experiments on
the AVSBench (single and multi-object splits) and AVSS datasets demonstrate
that our MoCA outperforms strongly designed baseline methods and approaches
supervised counterparts, particularly in complex scenarios with multiple
auditory objects. Notably when comparing mIoU, MoCA achieves a substantial
improvement over baselines in both the AVSBench (S4: +17.24%; MS3: +67.64%) and
AVSS (+19.23%) audio-visual segmentation challenges.
\\ ( https://arxiv.org/abs/2403.14203 ,  6195kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14227 (*cross-listing*)
Date: Thu, 21 Mar 2024 08:37:15 GMT   (2790kb,D)

Title: PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators
  and Participants in Children's Collaborative Learning
Authors: Jiawen Liu, Yuanyuan Yao, Pengcheng An, Qi Wang
Categories: cs.HC cs.AI
Comments: To appear at CHI EA '24
DOI: 10.1145/3613905.3651008
\\
  In children's collaborative learning, effective peer conversations can
significantly enhance the quality of children's collaborative interactions. The
integration of Large Language Model (LLM) agents into this setting explores
their novel role as peers, assessing impacts as team moderators and
participants. We invited two groups of participants to engage in a
collaborative learning workshop, where they discussed and proposed conceptual
solutions to a design problem. The peer conversation transcripts were analyzed
using thematic analysis. We discovered that peer agents, while managing
discussions effectively as team moderators, sometimes have their instructions
disregarded. As participants, they foster children's creative thinking but may
not consistently provide timely feedback. These findings highlight potential
design improvements and considerations for peer agents in both roles.
\\ ( https://arxiv.org/abs/2403.14227 ,  2790kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14233 (*cross-listing*)
Date: Thu, 21 Mar 2024 08:49:34 GMT   (2664kb,D)

Title: SoftPatch: Unsupervised Anomaly Detection with Noisy Data
Authors: Xi Jiang, Ying Chen, Qiang Nie, Yong Liu, Jianlin Liu, Bin-Bin Gao,
  Jun Liu, Chengjie Wang, Feng Zheng
Categories: cs.CV cs.AI cs.LG
Comments: 36th Conference on Neural Information Processing Systems
Journal-ref: Advances in Neural Information Processing Systems 35, ISBN:
  9781713871088, (2022)
\\
  Although mainstream unsupervised anomaly detection (AD) algorithms perform
well in academic datasets, their performance is limited in practical
application due to the ideal experimental setting of clean training data.
Training with noisy data is an inevitable problem in real-world anomaly
detection but is seldom discussed. This paper considers label-level noise in
image sensory anomaly detection for the first time. To solve this problem, we
proposed a memory-based unsupervised AD method, SoftPatch, which efficiently
denoises the data at the patch level. Noise discriminators are utilized to
generate outlier scores for patch-level noise elimination before coreset
construction. The scores are then stored in the memory bank to soften the
anomaly detection boundary. Compared with existing methods, SoftPatch maintains
a strong modeling ability of normal data and alleviates the overconfidence
problem in coreset. Comprehensive experiments in various noise scenes
demonstrate that SoftPatch outperforms the state-of-the-art AD methods on the
MVTecAD and BTAD benchmarks and is comparable to those methods under the
setting without noise.
\\ ( https://arxiv.org/abs/2403.14233 ,  2664kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14244 (*cross-listing*)
Date: Thu, 21 Mar 2024 09:02:31 GMT   (2865kb,D)

Title: Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering
Authors: Yuanhao Gong, Lantao Yu, Guanghui Yue
Categories: cs.CV cs.AI cs.LG eess.IV
\\
  The 3D Gaussian splatting method has drawn a lot of attention, thanks to its
high performance in training and high quality of the rendered image. However,
it uses anisotropic Gaussian kernels to represent the scene. Although such
anisotropic kernels have advantages in representing the geometry, they lead to
difficulties in terms of computation, such as splitting or merging two kernels.
In this paper, we propose to use isotropic Gaussian kernels to avoid such
difficulties in the computation, leading to a higher performance method. The
experiments confirm that the proposed method is about {\bf 100X} faster without
losing the geometry representation accuracy. The proposed method can be applied
in a large range applications where the radiance field is needed, such as 3D
reconstruction, view synthesis, and dynamic object modeling.
\\ ( https://arxiv.org/abs/2403.14244 ,  2865kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14246 (*cross-listing*)
Date: Thu, 21 Mar 2024 09:06:28 GMT   (1020kb,D)

Title: CATSE: A Context-Aware Framework for Causal Target Sound Extraction
Authors: Shrishail Baligar, Mikolaj Kegler, Bryce Irvin, Marko Stamenovic,
  Shawn Newsam
Categories: eess.AS cs.AI
Comments: Submitted to EUSIPCO 2024
\\
  Target Sound Extraction (TSE) focuses on the problem of separating sources of
interest, indicated by a user's cue, from the input mixture. Most existing
solutions operate in an offline fashion and are not suited to the low-latency
causal processing constraints imposed by applications in live-streamed content
such as augmented hearing. We introduce a family of context-aware low-latency
causal TSE models suitable for real-time processing. First, we explore the
utility of context by providing the TSE model with oracle information about
what sound classes make up the input mixture, where the objective of the model
is to extract one or more sources of interest indicated by the user. Since the
practical applications of oracle models are limited due to their assumptions,
we introduce a composite multi-task training objective involving separation and
classification losses. Our evaluation involving single- and multi-source
extraction shows the benefit of using context information in the model either
by means of providing full context or via the proposed multi-task training loss
without the need for full context information. Specifically, we show that our
proposed model outperforms size- and latency-matched Waveformer, a
state-of-the-art model for real-time TSE.
\\ ( https://arxiv.org/abs/2403.14246 ,  1020kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14264 (*cross-listing*)
Date: Thu, 21 Mar 2024 09:59:53 GMT   (28398kb,D)

Title: A Framework for Portrait Stylization with Skin-Tone Awareness and Nudity
  Identification
Authors: Seungkwon Kim, Sangyeon Kim, Seung-Hun Nam
Categories: cs.CV cs.AI
Comments: Accepted to ICASSP 2024
\\
  Portrait stylization is a challenging task involving the transformation of an
input portrait image into a specific style while preserving its inherent
characteristics. The recent introduction of Stable Diffusion (SD) has
significantly improved the quality of outcomes in this field. However, a
practical stylization framework that can effectively filter harmful input
content and preserve the distinct characteristics of an input, such as
skin-tone, while maintaining the quality of stylization remains lacking. These
challenges have hindered the wide deployment of such a framework. To address
these issues, this study proposes a portrait stylization framework that
incorporates a nudity content identification module (NCIM) and a
skin-tone-aware portrait stylization module (STAPSM). In experiments, NCIM
showed good performance in enhancing explicit content filtering, and STAPSM
accurately represented a diverse range of skin tones. Our proposed framework
has been successfully deployed in practice, and it has effectively satisfied
critical requirements of real-world applications.
\\ ( https://arxiv.org/abs/2403.14264 ,  28398kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14273 (*cross-listing*)
Date: Thu, 21 Mar 2024 10:26:47 GMT   (3067kb,D)

Title: Reactor Optimization Benchmark by Reinforcement Learning
Authors: Deborah Schwarcz, Nadav Schneider, Gal Oren, Uri Steinitz
Categories: cs.NE cs.AI
\\
  Neutronic calculations for reactors are a daunting task when using Monte
Carlo (MC) methods. As high-performance computing has advanced, the simulation
of a reactor is nowadays more readily done, but design and optimization with
multiple parameters is still a computational challenge. MC transport
simulations, coupled with machine learning techniques, offer promising avenues
for enhancing the efficiency and effectiveness of nuclear reactor optimization.
This paper introduces a novel benchmark problem within the OpenNeoMC framework
designed specifically for reinforcement learning. The benchmark involves
optimizing a unit cell of a research reactor with two varying parameters (fuel
density and water spacing) to maximize neutron flux while maintaining reactor
criticality. The test case features distinct local optima, representing
different physical regimes, thus posing a challenge for learning algorithms.
Through extensive simulations utilizing evolutionary and neuroevolutionary
algorithms, we demonstrate the effectiveness of reinforcement learning in
navigating complex optimization landscapes with strict constraints.
Furthermore, we propose acceleration techniques within the OpenNeoMC framework,
including model updating and cross-section usage by RAM utilization, to
expedite simulation times. Our findings emphasize the importance of machine
learning integration in reactor optimization and contribute to advancing
methodologies for addressing intricate optimization challenges in nuclear
engineering. The sources of this work are available at our GitHub repository:
https://github.com/Scientific-Computing-Lab-NRCN/RLOpenNeoMC
\\ ( https://arxiv.org/abs/2403.14273 ,  3067kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14274 (*cross-listing*)
Date: Thu, 21 Mar 2024 10:28:18 GMT   (97kb,D)

Title: Multi-role Consensus through LLMs Discussions for Vulnerability
  Detection
Authors: Zhenyu Mao, Jialong Li, Munan Li, Kenji Tei
Categories: cs.SE cs.AI
\\
  Recent advancements in large language models (LLMs) have highlighted the
potential for vulnerability detection, a crucial component of software quality
assurance. Despite this progress, most studies have been limited to the
perspective of a single role, usually testers, lacking diverse viewpoints from
different roles in a typical software development life-cycle, including both
developers and testers. To this end, this paper introduces an approach to
employ LLMs to act as different roles to simulate real-life code review
process, engaging in discussions towards a consensus on the existence and
classification of vulnerabilities in the code. Preliminary evaluation of the
proposed approach indicates a 4.73% increase in the precision rate, 58.9%
increase in the recall rate, and a 28.1% increase in the F1 score.
\\ ( https://arxiv.org/abs/2403.14274 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14287 (*cross-listing*)
Date: Thu, 21 Mar 2024 10:51:19 GMT   (2702kb,D)

Title: Enhancing Historical Image Retrieval with Compositional Cues
Authors: Tingyu Lin, Robert Sablatnig
Categories: cs.CV cs.AI eess.IV
\\
  In analyzing vast amounts of digitally stored historical image data, existing
content-based retrieval methods often overlook significant non-semantic
information, limiting their effectiveness for flexible exploration across
varied themes. To broaden the applicability of image retrieval methods for
diverse purposes and uncover more general patterns, we innovatively introduce a
crucial factor from computational aesthetics, namely image composition, into
this topic. By explicitly integrating composition-related information extracted
by CNN into the designed retrieval model, our method considers both the image's
composition rules and semantic information. Qualitative and quantitative
experiments demonstrate that the image retrieval network guided by composition
information outperforms those relying solely on content information,
facilitating the identification of images in databases closer to the target
image in human perception. Please visit https://github.com/linty5/CCBIR to try
our codes.
\\ ( https://arxiv.org/abs/2403.14287 ,  2702kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14298 (*cross-listing*)
Date: Thu, 21 Mar 2024 11:04:41 GMT   (319kb)

Title: From Perils to Possibilities: Understanding how Human (and AI) Biases
  affect Online Fora
Authors: Virginia Morini, Valentina Pansanella, Katherine Abramski, Erica Cau,
  Andrea Failla, Salvatore Citraro, Giulio Rossetti
Categories: cs.SI cs.AI cs.HC
\\
  Social media platforms are online fora where users engage in discussions,
share content, and build connections. This review explores the dynamics of
social interactions, user-generated contents, and biases within the context of
social media analysis (analyzing works that use the tools offered by complex
network analysis and natural language processing) through the lens of three key
points of view: online debates, online support, and human-AI interactions. On
the one hand, we delineate the phenomenon of online debates, where
polarization, misinformation, and echo chamber formation often proliferate,
driven by algorithmic biases and extreme mechanisms of homophily. On the other
hand, we explore the emergence of online support groups through users'
self-disclosure and social support mechanisms. Online debates and support
mechanisms present a duality of both perils and possibilities within social
media; perils of segregated communities and polarized debates, and
possibilities of empathy narratives and self-help groups. This dichotomy also
extends to a third perspective: users' reliance on AI-generated content, such
as the ones produced by Large Language Models, which can manifest both human
biases hidden in training sets and non-human biases that emerge from their
artificial neural architectures. Analyzing interdisciplinary approaches, we aim
to deepen the understanding of the complex interplay between social
interactions, user-generated content, and biases within the realm of social
media ecosystems.
\\ ( https://arxiv.org/abs/2403.14298 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14300 (*cross-listing*)
Date: Thu, 21 Mar 2024 11:16:28 GMT   (12675kb,D)

Title: DexDribbler: Learning Dexterous Soccer Manipulation via Dynamic
  Supervision
Authors: Yutong Hu, Kehan Wen and Fisher Yu
Categories: cs.RO cs.AI
Comments: 8 pages, 7 figures, submitted to IROS 2024
\\
  Learning dexterous locomotion policy for legged robots is becoming
increasingly popular due to its ability to handle diverse terrains and resemble
intelligent behaviors. However, joint manipulation of moving objects and
locomotion with legs, such as playing soccer, receive scant attention in the
learning community, although it is natural for humans and smart animals. A key
challenge to solve this multitask problem is to infer the objectives of
locomotion from the states and targets of the manipulated objects. The implicit
relation between the object states and robot locomotion can be hard to capture
directly from the training experience. We propose adding a feedback control
block to compute the necessary body-level movement accurately and using the
outputs as dynamic joint-level locomotion supervision explicitly. We further
utilize an improved ball dynamic model, an extended context-aided estimator,
and a comprehensive ball observer to facilitate transferring policy learned in
simulation to the real world. We observe that our learning scheme can not only
make the policy network converge faster but also enable soccer robots to
perform sophisticated maneuvers like sharp cuts and turns on flat surfaces, a
capability that was lacking in previous methods. Video and code are available
at https://github.com/SysCV/soccer-player
\\ ( https://arxiv.org/abs/2403.14300 ,  12675kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14328 (*cross-listing*)
Date: Thu, 21 Mar 2024 11:54:45 GMT   (27582kb,D)

Title: Distilling Reinforcement Learning Policies for Interpretable Robot
  Locomotion: Gradient Boosting Machines and Symbolic Regression
Authors: Fernando Acero and Zhibin Li
Categories: cs.RO cs.AI cs.LG
\\
  Recent advancements in reinforcement learning (RL) have led to remarkable
achievements in robot locomotion capabilities. However, the complexity and
``black-box'' nature of neural network-based RL policies hinder their
interpretability and broader acceptance, particularly in applications demanding
high levels of safety and reliability. This paper introduces a novel approach
to distill neural RL policies into more interpretable forms using Gradient
Boosting Machines (GBMs), Explainable Boosting Machines (EBMs) and Symbolic
Regression. By leveraging the inherent interpretability of generalized additive
models, decision trees, and analytical expressions, we transform opaque neural
network policies into more transparent ``glass-box'' models. We train expert
neural network policies using RL and subsequently distill them into (i) GBMs,
(ii) EBMs, and (iii) symbolic policies. To address the inherent distribution
shift challenge of behavioral cloning, we propose to use the Dataset
Aggregation (DAgger) algorithm with a curriculum of episode-dependent
alternation of actions between expert and distilled policies, to enable
efficient distillation of feedback control policies. We evaluate our approach
on various robot locomotion gaits -- walking, trotting, bounding, and pacing --
and study the importance of different observations in joint actions for
distilled policies using various methods. We train neural expert policies for
205 hours of simulated experience and distill interpretable policies with only
10 minutes of simulated interaction for each gait using the proposed method.
\\ ( https://arxiv.org/abs/2403.14328 ,  27582kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14410 (*cross-listing*)
Date: Thu, 21 Mar 2024 13:57:45 GMT   (8651kb,D)

Title: GLC++: Source-Free Universal Domain Adaptation through Global-Local
  Clustering and Contrastive Affinity Learning
Authors: Sanqing Qu, Tianpei Zou, Florian R\"ohrbein, Cewu Lu, Guang Chen,
  Dacheng Tao, Changjun Jiang
Categories: cs.CV cs.AI cs.LG
Comments: This is a substantial extension of the CVPR 2023 paper "Upcycling
  Models under Domain and Category Shift"
\\
  Deep neural networks often exhibit sub-optimal performance under covariate
and category shifts. Source-Free Domain Adaptation (SFDA) presents a promising
solution to this dilemma, yet most SFDA approaches are restricted to closed-set
scenarios. In this paper, we explore Source-Free Universal Domain Adaptation
(SF-UniDA) aiming to accurately classify "known" data belonging to common
categories and segregate them from target-private "unknown" data. We propose a
novel Global and Local Clustering (GLC) technique, which comprises an adaptive
one-vs-all global clustering algorithm to discern between target classes,
complemented by a local k-NN clustering strategy to mitigate negative transfer.
Despite the effectiveness, the inherent closed-set source architecture leads to
uniform treatment of "unknown" data, impeding the identification of distinct
"unknown" categories. To address this, we evolve GLC to GLC++, integrating a
contrastive affinity learning strategy. We examine the superiority of GLC and
GLC++ across multiple benchmarks and category shift scenarios. Remarkably, in
the most challenging open-partial-set scenarios, GLC and GLC++ surpass GATE by
16.7% and 18.6% in H-score on VisDA, respectively. GLC++ enhances the novel
category clustering accuracy of GLC by 4.3% in open-set scenarios on
Office-Home. Furthermore, the introduced contrastive learning strategy not only
enhances GLC but also significantly facilitates existing methodologies.
\\ ( https://arxiv.org/abs/2403.14410 ,  8651kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14429 (*cross-listing*)
Date: Thu, 21 Mar 2024 14:36:59 GMT   (43768kb,D)

Title: Style-Extracting Diffusion Models for Semi-Supervised Histopathology
  Segmentation
Authors: Mathias \"Ottl, Frauke Wilm, Jana Steenpass, Jingna Qiu, Matthias
  R\"ubner, Arndt Hartmann, Matthias Beckmann, Peter Fasching, Andreas Maier,
  Ramona Erber, Bernhard Kainz, Katharina Breininger
Categories: cs.CV cs.AI cs.LG
\\
  Deep learning-based image generation has seen significant advancements with
diffusion models, notably improving the quality of generated images. Despite
these developments, generating images with unseen characteristics beneficial
for downstream tasks has received limited attention. To bridge this gap, we
propose Style-Extracting Diffusion Models, featuring two conditioning
mechanisms. Specifically, we utilize 1) a style conditioning mechanism which
allows to inject style information of previously unseen images during image
generation and 2) a content conditioning which can be targeted to a downstream
task, e.g., layout for segmentation. We introduce a trainable style encoder to
extract style information from images, and an aggregation block that merges
style information from multiple style inputs. This architecture enables the
generation of images with unseen styles in a zero-shot manner, by leveraging
styles from unseen images, resulting in more diverse generations. In this work,
we use the image layout as target condition and first show the capability of
our method on a natural image dataset as a proof-of-concept. We further
demonstrate its versatility in histopathology, where we combine prior knowledge
about tissue composition and unannotated data to create diverse synthetic
images with known layouts. This allows us to generate additional synthetic data
to train a segmentation network in a semi-supervised fashion. We verify the
added value of the generated images by showing improved segmentation results
and lower performance variability between patients when synthetic images are
included during segmentation training. Our code will be made publicly available
at [LINK].
\\ ( https://arxiv.org/abs/2403.14429 ,  43768kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14432 (*cross-listing*)
Date: Thu, 21 Mar 2024 14:39:28 GMT   (703kb,D)

Title: On the continuity and smoothness of the value function in reinforcement
  learning and optimal control
Authors: Hans Harder and Sebastian Peitz
Categories: eess.SY cs.AI cs.SY
MSC-class: 37H99, 37N35, 93E03
ACM-class: I.2.8
\\
  The value function plays a crucial role as a measure for the cumulative
future reward an agent receives in both reinforcement learning and optimal
control. It is therefore of interest to study how similar the values of
neighboring states are, i.e., to investigate the continuity of the value
function. We do so by providing and verifying upper bounds on the value
function's modulus of continuity. Additionally, we show that the value function
is always H\"older continuous under relatively weak assumptions on the
underlying system and that non-differentiable value functions can be made
differentiable by slightly "disturbing" the system.
\\ ( https://arxiv.org/abs/2403.14432 ,  703kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14435 (*cross-listing*)
Date: Thu, 21 Mar 2024 14:41:58 GMT   (24538kb,D)

Title: Biased Binary Attribute Classifiers Ignore the Majority Classes
Authors: Xinyi Zhang and Johanna Sophie Bieri and Manuel G\"unther
Categories: cs.CV cs.AI cs.LG
\\
  To visualize the regions of interest that classifiers base their decisions
on, different Class Activation Mapping (CAM) methods have been developed.
However, all of these techniques target categorical classifiers only, though
most real-world tasks are binary classification. In this paper, we extend
gradient-based CAM techniques to work with binary classifiers and visualize the
active regions for binary facial attribute classifiers. When training an
unbalanced binary classifier on an imbalanced dataset, it is well-known that
the majority class, i.e. the class with many training samples, is mostly
predicted much better than minority class with few training instances. In our
experiments on the CelebA dataset, we verify these results, when training an
unbalanced classifier to extract 40 facial attributes simultaneously. One would
expect that the biased classifier has learned to extract features mainly for
the majority classes and that the proportional energy of the activations mainly
reside in certain specific regions of the image where the attribute is located.
However, we find very little regular activation for samples of majority
classes, while the active regions for minority classes seem mostly reasonable
and overlap with our expectations. These results suggest that biased
classifiers mainly rely on bias activation for majority classes. When training
a balanced classifier on the imbalanced data by employing attribute-specific
class weights, majority and minority classes are classified similarly well and
show expected activations for almost all attributes
\\ ( https://arxiv.org/abs/2403.14435 ,  24538kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14440 (*cross-listing*)
Date: Thu, 21 Mar 2024 14:45:54 GMT   (4739kb,D)

Title: Analysing Diffusion Segmentation for Medical Images
Authors: Mathias \"Ottl, Siyuan Mei, Frauke Wilm, Jana Steenpass, Matthias
  R\"ubner, Arndt Hartmann, Matthias Beckmann, Peter Fasching, Andreas Maier,
  Ramona Erber, Katharina Breininger
Categories: cs.CV cs.AI cs.LG
\\
  Denoising Diffusion Probabilistic models have become increasingly popular due
to their ability to offer probabilistic modeling and generate diverse outputs.
This versatility inspired their adaptation for image segmentation, where
multiple predictions of the model can produce segmentation results that not
only achieve high quality but also capture the uncertainty inherent in the
model. Here, powerful architectures were proposed for improving diffusion
segmentation performance. However, there is a notable lack of analysis and
discussions on the differences between diffusion segmentation and image
generation, and thorough evaluations are missing that distinguish the
improvements these architectures provide for segmentation in general from their
benefit for diffusion segmentation specifically. In this work, we critically
analyse and discuss how diffusion segmentation for medical images differs from
diffusion image generation, with a particular focus on the training behavior.
Furthermore, we conduct an assessment how proposed diffusion segmentation
architectures perform when trained directly for segmentation. Lastly, we
explore how different medical segmentation tasks influence the diffusion
segmentation behavior and the diffusion process could be adapted accordingly.
With these analyses, we aim to provide in-depth insights into the behavior of
diffusion segmentation that allow for a better design and evaluation of
diffusion segmentation methods in the future.
\\ ( https://arxiv.org/abs/2403.14440 ,  4739kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14460 (*cross-listing*)
Date: Thu, 21 Mar 2024 15:07:57 GMT   (1012kb,D)

Title: Towards Single-System Illusion in Software-Defined Vehicles --
  Automated, AI-Powered Workflow
Authors: Krzysztof Lebioda, Viktor Vorobev, Nenad Petrovic, Fengjunjie Pan,
  Vahid Zolfaghari, Alois Knoll
Categories: cs.SE cs.AI cs.CL
Report-no: TUM-I24108
ACM-class: D.2.1; D.2.2; D.2.4; I.2.7; I.2.2; I.7.0
\\
  We propose a novel model- and feature-based approach to development of
vehicle software systems, where the end architecture is not explicitly defined.
Instead, it emerges from an iterative process of search and optimization given
certain constraints, requirements and hardware architecture, while retaining
the property of single-system illusion, where applications run in a logically
uniform environment. One of the key points of the presented approach is the
inclusion of modern generative AI, specifically Large Language Models (LLMs),
in the loop. With the recent advances in the field, we expect that the LLMs
will be able to assist in processing of requirements, generation of formal
system models, as well as generation of software deployment specification and
test code. The resulting pipeline is automated to a large extent, with feedback
being generated at each step.
\\ ( https://arxiv.org/abs/2403.14460 ,  1012kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14468 (*cross-listing*)
Date: Thu, 21 Mar 2024 15:15:00 GMT   (47066kb,D)

Title: AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks
Authors: Max Ku and Cong Wei and Weiming Ren and Huan Yang and Wenhu Chen
Categories: cs.CV cs.AI cs.MM
Comments: preprint
\\
  Video-to-video editing involves editing a source video along with additional
control (such as text prompts, subjects, or styles) to generate a new video
that aligns with the source video and the provided control. Traditional methods
have been constrained to certain editing types, limiting their ability to meet
the wide range of user demands. In this paper, we introduce AnyV2V, a novel
training-free framework designed to simplify video editing into two primary
steps: (1) employing an off-the-shelf image editing model (e.g.
InstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an
existing image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion
and feature injection. In the first stage, AnyV2V can plug in any existing
image editing tools to support an extensive array of video editing tasks.
Beyond the traditional prompt-based editing methods, AnyV2V also can support
novel video editing tasks, including reference-based style transfer,
subject-driven editing, and identity manipulation, which were unattainable by
previous methods. In the second stage, AnyV2V can plug in any existing
image-to-video models to perform DDIM inversion and intermediate feature
injection to maintain the appearance and motion consistency with the source
video. On the prompt-based editing, we show that AnyV2V can outperform the
previous best approach by 35\% on prompt alignment, and 25\% on human
preference. On the three novel tasks, we show that AnyV2V also achieves a high
success rate. We believe AnyV2V will continue to thrive due to its ability to
seamlessly integrate the fast-evolving image editing methods. Such
compatibility can help AnyV2V to increase its versatility to cater to diverse
user demands.
\\ ( https://arxiv.org/abs/2403.14468 ,  47066kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14488 (*cross-listing*)
Date: Thu, 21 Mar 2024 15:36:26 GMT   (12079kb,D)

Title: Physics-Based Causal Reasoning for Safe & Robust Next-Best Action
  Selection in Robot Manipulation Tasks
Authors: Ricardo Cannizzaro, Michael Groom, Jonathan Routley, Robert Osazuwa
  Ness, Lars Kunze
Categories: cs.RO cs.AI cs.LG stat.AP
Comments: 8 pages, 9 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)
ACM-class: I.2.9; I.2.8; I.2.3; G.3; I.2.6; I.6.8; I.2.4; I.2.10
\\
  Safe and efficient object manipulation is a key enabler of many real-world
robot applications. However, this is challenging because robot operation must
be robust to a range of sensor and actuator uncertainties. In this paper, we
present a physics-informed causal-inference-based framework for a robot to
probabilistically reason about candidate actions in a block stacking task in a
partially observable setting. We integrate a physics-based simulation of the
rigid-body system dynamics with a causal Bayesian network (CBN) formulation to
define a causal generative probabilistic model of the robot decision-making
process. Using simulation-based Monte Carlo experiments, we demonstrate our
framework's ability to successfully: (1) predict block tower stability with
high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best
action for the block stacking task, for execution by an integrated robot
system, achieving 94.2% task success rate. We also demonstrate our framework's
suitability for real-world robot systems by demonstrating successful task
executions with a domestic support robot, with perception and manipulation
sub-system integration. Hence, we show that by embedding physics-based causal
reasoning into robots' decision-making processes, we can make robot task
execution safer, more reliable, and more robust to various types of
uncertainty.
\\ ( https://arxiv.org/abs/2403.14488 ,  12079kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14494 (*cross-listing*)
Date: Thu, 21 Mar 2024 15:42:17 GMT   (2133kb,D)

Title: Learning to Project for Cross-Task Knowledge Distillation
Authors: Dylan Auty, Roy Miles, Benedikt Kolbeinsson, Krystian Mikolajczyk
Categories: cs.CV cs.AI
\\
  Traditional knowledge distillation (KD) relies on a proficient teacher
trained on the target task, which is not always available. In this setting,
cross-task distillation can be used, enabling the use of any teacher model
trained on a different task. However, many KD methods prove ineffective when
applied to this cross-task setting. To address this limitation, we propose a
simple modification: the use of an inverted projection. We show that this
drop-in replacement for a standard projector is effective by learning to
disregard any task-specific features which might degrade the student's
performance. We find that this simple modification is sufficient for extending
many KD methods to the cross-task setting, where the teacher and student tasks
can be very different. In doing so, we obtain up to a 1.9% improvement in the
cross-task setting compared to the traditional projection, at no additional
cost. Our method can obtain significant performance improvements (up to 7%)
when using even a randomly-initialised teacher on various tasks such as depth
estimation, image translation, and semantic segmentation, despite the lack of
any learned knowledge to transfer. To provide conceptual and analytical
insights into this result, we show that using an inverted projection allows the
distillation loss to be decomposed into a knowledge transfer and a spectral
regularisation component. Through this analysis we are additionally able to
propose a novel regularisation loss that allows teacher-free distillation,
enabling performance improvements of up to 8.57% on ImageNet with no additional
training costs.
\\ ( https://arxiv.org/abs/2403.14494 ,  2133kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14496 (*cross-listing*)
Date: Thu, 21 Mar 2024 15:44:56 GMT   (15406kb,D)

Title: How Human-Centered Explainable AI Interface Are Designed and Evaluated:
  A Systematic Survey
Authors: Thu Nguyen, Alessandro Canossa, Jichen Zhu
Categories: cs.HC cs.AI
\\
  Despite its technological breakthroughs, eXplainable Artificial Intelligence
(XAI) research has limited success in producing the {\em effective
explanations} needed by users. In order to improve XAI systems' usability,
practical interpretability, and efficacy for real users, the emerging area of
{\em Explainable Interfaces} (EIs) focuses on the user interface and user
experience design aspects of XAI. This paper presents a systematic survey of 53
publications to identify current trends in human-XAI interaction and promising
directions for EI design and development. This is among the first systematic
survey of EI research.
\\ ( https://arxiv.org/abs/2403.14496 ,  15406kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14526 (*cross-listing*)
Date: Thu, 21 Mar 2024 16:26:19 GMT   (12387kb,D)

Title: Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion
  Descriptors
Authors: Nikolaos Tsagkas, Jack Rome, Subramanian Ramamoorthy, Oisin Mac Aodha,
  Chris Xiaoxuan Lu
Categories: cs.RO cs.AI cs.CV
Comments: 8 pages, 4 figures
\\
  Precise manipulation that is generalizable across scenes and objects remains
a persistent challenge in robotics. Current approaches for this task heavily
depend on having a significant number of training instances to handle objects
with pronounced visual and/or geometric part ambiguities. Our work explores the
grounding of fine-grained part descriptors for precise manipulation in a
zero-shot setting by utilizing web-trained text-to-image diffusion-based
generative models. We tackle the problem by framing it as a dense semantic part
correspondence task. Our model returns a gripper pose for manipulating a
specific part, using as reference a user-defined click from a source image of a
visually different instance of the same object. We require no manual grasping
demonstrations as we leverage the intrinsic object geometry and features.
Practical experiments in a real-world tabletop scenario validate the efficacy
of our approach, demonstrating its potential for advancing semantic-aware
robotics manipulation. Web page: https://tsagkas.github.io/click2grasp
\\ ( https://arxiv.org/abs/2403.14526 ,  12387kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14539 (*cross-listing*)
Date: Thu, 21 Mar 2024 16:40:10 GMT   (6641kb,D)

Title: Object-Centric Domain Randomization for 3D Shape Reconstruction in the
  Wild
Authors: Junhyeong Cho, Kim Youwang, Hunmin Yang, Tae-Hyun Oh
Categories: cs.CV cs.AI cs.LG
Comments: Project Page: https://ObjectDR.github.io
\\
  One of the biggest challenges in single-view 3D shape reconstruction in the
wild is the scarcity of <3D shape, 2D image>-paired data from real-world
environments. Inspired by remarkable achievements via domain randomization, we
propose ObjectDR which synthesizes such paired data via a random simulation of
visual variations in object appearances and backgrounds. Our data synthesis
framework exploits a conditional generative model (e.g., ControlNet) to
generate images conforming to spatial conditions such as 2.5D sketches, which
are obtainable through a rendering process of 3D shapes from object collections
(e.g., Objaverse-XL). To simulate diverse variations while preserving object
silhouettes embedded in spatial conditions, we also introduce a disentangled
framework which leverages an initial object guidance. After synthesizing a wide
range of data, we pre-train a model on them so that it learns to capture a
domain-invariant geometry prior which is consistent across various domains. We
validate its effectiveness by substantially improving 3D shape reconstruction
models on a real-world benchmark. In a scale-up evaluation, our pre-training
achieves 23.6% superior results compared with the pre-training on high-quality
computer graphics renderings.
\\ ( https://arxiv.org/abs/2403.14539 ,  6641kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14550 (*cross-listing*)
Date: Thu, 21 Mar 2024 16:50:12 GMT   (6114kb,D)

Title: Dynamic Explanation Emphasis in Human-XAI Interaction with Communication
  Robot
Authors: Yosuke Fukuchi and Seiji Yamada
Categories: cs.HC cs.AI
\\
  Communication robots have the potential to contribute to effective human-XAI
interaction as an interface that goes beyond textual or graphical explanations.
One of their strengths is that they can use physical and vocal expressions to
add detailed nuances to explanations. However, it is not clear how a robot can
apply such expressions, or in particular, how we can develop a strategy to
adaptively use such expressions depending on the task and user in dynamic
interactions. To address this question, this paper proposes DynEmph, a method
for a communication robot to decide where to emphasize XAI-generated
explanations with physical expressions. It predicts the effect of emphasizing
certain points on a user and aims to minimize the expected difference between
predicted user decisions and AI-suggested ones. DynEmph features a strategy for
deciding where to emphasize in a data-driven manner, relieving engineers from
the need to manually design a strategy. We further conducted experiments to
investigate how emphasis selection strategies affect the performance of user
decisions. The results suggest that, while a naive strategy (emphasizing
explanations for an AI's most probable class) does not necessarily work better,
DynEmph effectively guides users to better decisions under the condition that
the performance of the AI suggestion is high.
\\ ( https://arxiv.org/abs/2403.14550 ,  6114kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14592 (*cross-listing*)
Date: Thu, 21 Mar 2024 17:47:28 GMT   (616kb,D)

Title: Envisioning the Next-Generation AI Coding Assistants: Insights &
  Proposals
Authors: Khanh Nghiem, Anh Minh Nguyen, Nghi D. Q. Bui
Categories: cs.SE cs.AI cs.HC
\\
  As a research-product hybrid group in AI for Software Engineering (AI4SE), we
present four key takeaways from our experience developing in-IDE AI coding
assistants. AI coding assistants should set clear expectations for usage,
integrate with advanced IDE capabilities and existing extensions, use
extendable backend designs, and collect app data responsibly for downstream
analyses. We propose open questions and challenges that academia and industry
should address to realize the vision of next-generation AI coding assistants.
\\ ( https://arxiv.org/abs/2403.14592 ,  616kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14617 (*cross-listing*)
Date: Thu, 21 Mar 2024 17:59:03 GMT   (5297kb,D)

Title: Videoshop: Localized Semantic Video Editing with Noise-Extrapolated
  Diffusion Inversion
Authors: Xiang Fan, Anand Bhattad, Ranjay Krishna
Categories: cs.CV cs.AI cs.LG
\\
  We introduce Videoshop, a training-free video editing algorithm for localized
semantic edits. Videoshop allows users to use any editing software, including
Photoshop and generative inpainting, to modify the first frame; it
automatically propagates those changes, with semantic, spatial, and temporally
consistent motion, to the remaining frames. Unlike existing methods that enable
edits only through imprecise textual instructions, Videoshop allows users to
add or remove objects, semantically change objects, insert stock photos into
videos, etc. with fine-grained control over locations and appearance. We
achieve this through image-based video editing by inverting latents with noise
extrapolation, from which we generate videos conditioned on the edited image.
Videoshop produces higher quality edits against 6 baselines on 2 editing
benchmarks using 10 evaluation metrics.
\\ ( https://arxiv.org/abs/2403.14617 ,  5297kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14624 (*cross-listing*)
Date: Thu, 21 Mar 2024 17:59:50 GMT   (4818kb,D)

Title: MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual
  Math Problems?
Authors: Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo,
  Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, Hongsheng Li
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: 46 Pages, Work in Progress, Benchmark Project Page:
  https://mathverse-cuhk.github.io
\\
  The remarkable progress of Multi-modal Large Language Models (MLLMs) has
garnered unparalleled attention, due to their superior performance in visual
contexts. However, their capabilities in visual math problem-solving remain
insufficiently evaluated and understood. We investigate current benchmarks to
incorporate excessive visual content within textual questions, which
potentially assist MLLMs in deducing answers without truly interpreting the
input diagrams. To this end, we introduce MathVerse, an all-around visual math
benchmark designed for an equitable and in-depth evaluation of MLLMs. We
meticulously collect 2,612 high-quality, multi-subject math problems with
diagrams from publicly available sources. Each problem is then transformed by
human annotators into six distinct versions, each offering varying degrees of
information content in multi-modality, contributing to 15K test samples in
total. This approach allows MathVerse to comprehensively assess whether and how
much MLLMs can truly understand the visual diagrams for mathematical reasoning.
In addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a
fine-grained assessment of the output answers. Rather than naively judging True
or False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and
then score each step with detailed error analysis, which can reveal the
intermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark
may provide unique insights to guide the future development of MLLMs. Project
page: https://mathverse-cuhk.github.io
\\ ( https://arxiv.org/abs/2403.14624 ,  4818kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13826 (*cross-listing*)
Date: Wed, 6 Mar 2024 01:55:14 GMT   (20399kb,D)

Title: Measuring Diversity in Co-creative Image Generation
Authors: Francisco Ibarrola and Kazjon Grace
Categories: cs.CV cs.CL cs.LG
ACM-class: I.2.0
\\
  Quality and diversity have been proposed as reasonable heuristics for
assessing content generated by co-creative systems, but to date there has been
little agreement around what constitutes the latter or how to measure it.
Proposed approaches for assessing generative models in terms of diversity have
limitations in that they compare the model's outputs to a ground truth that in
the era of large pre-trained generative models might not be available, or
entail an impractical number of computations. We propose an alternative based
on entropy of neural network encodings for comparing diversity between sets of
images that does not require ground-truth knowledge and is easy to compute. We
also compare two pre-trained networks and show how the choice relates to the
notion of diversity that we want to evaluate. We conclude with a discussion of
the potential applications of these measures for ideation in interactive
systems, model evaluation, and more broadly within computational creativity.
\\ ( https://arxiv.org/abs/2403.13826 ,  20399kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13830 (*cross-listing*)
Date: Thu, 7 Mar 2024 03:03:13 GMT   (2973kb,D)

Title: Bridging Text and Molecule: A Survey on Multimodal Frameworks for
  Molecule
Authors: Yi Xiao, Xiangxin Zhou, Qiang Liu, Liang Wang
Categories: q-bio.BM cs.CL cs.LG
\\
  Artificial intelligence has demonstrated immense potential in scientific
research. Within molecular science, it is revolutionizing the traditional
computer-aided paradigm, ushering in a new era of deep learning. With recent
progress in multimodal learning and natural language processing, an emerging
trend has targeted at building multimodal frameworks to jointly model molecules
with textual domain knowledge. In this paper, we present the first systematic
survey on multimodal frameworks for molecules research. Specifically,we begin
with the development of molecular deep learning and point out the necessity to
involve textual modality. Next, we focus on recent advances in text-molecule
alignment methods, categorizing current models into two groups based on their
architectures and listing relevant pre-training tasks. Furthermore, we delves
into the utilization of large language models and prompting techniques for
molecular tasks and present significant applications in drug discovery.
Finally, we discuss the limitations in this field and highlight several
promising directions for future research.
\\ ( https://arxiv.org/abs/2403.13830 ,  2973kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14048 (*cross-listing*)
Date: Thu, 21 Mar 2024 00:13:59 GMT   (115kb)

Title: The NeurIPS 2023 Machine Learning for Audio Workshop: Affective Audio
  Benchmarks and Novel Data
Authors: Alice Baird, Rachel Manzelli, Panagiotis Tzirakis, Chris Gagne, Haoqi
  Li, Sadie Allen, Sander Dieleman, Brian Kulis, Shrikanth S. Narayanan, Alan
  Cowen
Categories: cs.SD cs.CL eess.AS
\\
  The NeurIPS 2023 Machine Learning for Audio Workshop brings together machine
learning (ML) experts from various audio domains. There are several valuable
audio-driven ML tasks, from speech emotion recognition to audio event
detection, but the community is sparse compared to other ML areas, e.g.,
computer vision or natural language processing. A major limitation with audio
is the available data; with audio being a time-dependent modality, high-quality
data collection is time-consuming and costly, making it challenging for
academic groups to apply their often state-of-the-art strategies to a larger,
more generalizable dataset. In this short white paper, to encourage researchers
with limited access to large-datasets, the organizers first outline several
open-source datasets that are available to the community, and for the duration
of the workshop are making several propriety datasets available. Namely, three
vocal datasets, Hume-Prosody, Hume-VocalBurst, an acted emotional speech
dataset Modulate-Sonata, and an in-game streamer dataset Modulate-Stream. We
outline the current baselines on these datasets but encourage researchers from
across audio to utilize them outside of the initial baseline tasks.
\\ ( https://arxiv.org/abs/2403.14048 ,  115kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14074 (*cross-listing*)
Date: Thu, 21 Mar 2024 01:52:07 GMT   (420kb,D)

Title: M3: A Multi-Task Mixed-Objective Learning Framework for Open-Domain
  Multi-Hop Dense Sentence Retrieval
Authors: Yang Bai, Anthony Colas, Christan Grant, Daisy Zhe Wang
Categories: cs.IR cs.CL cs.LG
Comments: Accepted by LREC-COLING 2024
\\
  In recent research, contrastive learning has proven to be a highly effective
method for representation learning and is widely used for dense retrieval.
However, we identify that relying solely on contrastive learning can lead to
suboptimal retrieval performance. On the other hand, despite many retrieval
datasets supporting various learning objectives beyond contrastive learning,
combining them efficiently in multi-task learning scenarios can be challenging.
In this paper, we introduce M3, an advanced recursive Multi-hop dense sentence
retrieval system built upon a novel Multi-task Mixed-objective approach for
dense text representation learning, addressing the aforementioned challenges.
Our approach yields state-of-the-art performance on a large-scale open-domain
fact verification benchmark dataset, FEVER. Code and data are available at:
https://github.com/TonyBY/M3
\\ ( https://arxiv.org/abs/2403.14074 ,  420kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14117 (*cross-listing*)
Date: Thu, 21 Mar 2024 04:03:16 GMT   (1069kb,D)

Title: A Design Space for Intelligent and Interactive Writing Assistants
Authors: Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham
  Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss,
  David Zhou, Emad A. Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi,
  Senjuti Dutta, Jin L.C. Guo, Md Naimul Hoque, Yewon Kim, Seyed Parsa Neshaei,
  Agnia Sergeyuk, Antonette Shibani, Disha Shrivastava, Lila Shroff, Jessi
  Stark, Sarah Sterman, Sitong Wang, Antoine Bosselut, Daniel Buschek, Joseph
  Chee Chang, Sherol Chen, Max Kreminski, Joonsuk Park, Roy Pea, Eugenia H.
  Rho, Shannon Zejiang Shen, Pao Siangliulue
Categories: cs.HC cs.CL
Comments: Published as a conference paper at CHI 2024
DOI: 10.1145/3613904.3642697
\\
  In our era of rapid technological advancement, the research landscape for
writing assistants has become increasingly fragmented across various research
communities. We seek to address this challenge by proposing a design space as a
structured way to examine and explore the multidimensional space of intelligent
and interactive writing assistants. Through a large community collaboration, we
explore five aspects of writing assistants: task, user, technology,
interaction, and ecosystem. Within each aspect, we define dimensions (i.e.,
fundamental components of an aspect) and codes (i.e., potential options for
each dimension) by systematically reviewing 115 papers. Our design space aims
to offer researchers and designers a practical tool to navigate, comprehend,
and compare the various possibilities of writing assistants, and aid in the
envisioning and design of new writing assistants.
\\ ( https://arxiv.org/abs/2403.14117 ,  1069kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14270 (*cross-listing*)
Date: Thu, 21 Mar 2024 10:15:57 GMT   (18890kb,D)

Title: Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship
  Detection
Authors: Tim Salzmann, Markus Ryll, Alex Bewley and Matthias Minderer
Categories: cs.CV cs.CL cs.LG cs.RO
\\
  Visual relationship detection aims to identify objects and their
relationships in images. Prior methods approach this task by adding separate
relationship modules or decoders to existing object detection architectures.
This separation increases complexity and hinders end-to-end training, which
limits performance. We propose a simple and highly efficient decoder-free
architecture for open-vocabulary visual relationship detection. Our model
consists of a Transformer-based image encoder that represents objects as tokens
and models their relationships implicitly. To extract relationship information,
we introduce an attention mechanism that selects object pairs likely to form a
relationship. We provide a single-stage recipe to train this model on a mixture
of object and relationship detection data. Our approach achieves
state-of-the-art relationship detection performance on Visual Genome and on the
large-vocabulary GQA benchmark at real-time inference speeds. We provide
analyses of zero-shot performance, ablations, and real-world qualitative
examples.
\\ ( https://arxiv.org/abs/2403.14270 ,  18890kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14467 (*cross-listing*)
Date: Thu, 21 Mar 2024 15:14:25 GMT   (2716kb,D)

Title: Recourse for reclamation: Chatting with generative language models
Authors: Jennifer Chien, Kevin R. McKee, Jackie Kay, William Isaac
Categories: cs.HC cs.CL cs.CY
Comments: Extended Abstracts of the CHI Conference on Human Factors in
  Computing Systems (CHI EA 2024)
DOI: 10.1145/3613905.3650999
\\
  Researchers and developers increasingly rely on toxicity scoring to moderate
generative language model outputs, in settings such as customer service,
information retrieval, and content generation. However, toxicity scoring may
render pertinent information inaccessible, rigidify or "value-lock" cultural
norms, and prevent language reclamation processes, particularly for
marginalized people. In this work, we extend the concept of algorithmic
recourse to generative language models: we provide users a novel mechanism to
achieve their desired prediction by dynamically setting thresholds for toxicity
filtering. Users thereby exercise increased agency relative to interactions
with the baseline system. A pilot study ($n = 30$) supports the potential of
our proposed recourse mechanism, indicating improvements in usability compared
to fixed-threshold toxicity-filtering of model outputs. Future work should
explore the intersection of toxicity scoring, model controllability, user
agency, and language reclamation processes -- particularly with regard to the
bias that many communities encounter when interacting with generative language
models.
\\ ( https://arxiv.org/abs/2403.14467 ,  2716kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13815 (*cross-listing*)
Date: Fri, 16 Feb 2024 13:41:05 GMT   (7812kb,D)

Title: Autonomous microARPES
Authors: Steinn Ymir Agustsson, Alfred J. H. Jones, Davide Curcio, S{\o}ren
  Ulstrup, Jill Miwa, Davide Mottin, Panagiotis Karras and Philip Hofmann
Categories: cond-mat.mtrl-sci cs.LG
\\
  Angle-resolved photoemission spectroscopy (ARPES) is a technique used to map
the occupied electronic structure of solids. Recent progress in X-ray focusing
optics has led to the development of ARPES into a microscopic tool, permitting
the electronic structure to be spatially mapped across the surface of a sample.
This comes at the expense of a time-consuming scanning process to cover not
only a three-dimensional energy-momentum ($E, k_z, k_y$) space but also the
two-dimensional surface area. Here, we implement a protocol to autonomously
search both $\mathbf{k}$- and real space in order to find positions of
particular interest, either because of their high photoemission intensity or
because of sharp spectral features. The search is based on the use of Gaussian
process regression and can easily be expanded to include additional parameters
or optimisation criteria. This autonomous experimental control is implemented
on the SGM4 micro-focus beamline of the synchrotron radiation source ASTRID2.
\\ ( https://arxiv.org/abs/2403.13815 ,  7812kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13827 (*cross-listing*)
Date: Wed, 6 Mar 2024 09:58:35 GMT   (699kb,D)

Title: Self-Supervised Path Planning in UAV-aided Wireless Networks based on
  Active Inference
Authors: Ali Krayani, Khalid Khan, Lucio Marcenaro, Mario Marchese, Carlo
  Regazzoni
Categories: cs.RO cs.LG eess.SP
Comments: Accepted for publication in the 2024 IEEE International Conference on
  Acoustics, Speech, and Signal Processing (ICASSP 2024)
\\
  This paper presents a novel self-supervised path-planning method for
UAV-aided networks. First, we employed an optimizer to solve training examples
offline and then used the resulting solutions as demonstrations from which the
UAV can learn the world model to understand the environment and implicitly
discover the optimizer's policy. UAV equipped with the world model can make
real-time autonomous decisions and engage in online planning using active
inference. During planning, UAV can score different policies based on the
expected surprise, allowing it to choose among alternative futures.
Additionally, UAV can anticipate the outcomes of its actions using the world
model and assess the expected surprise in a self-supervised manner. Our method
enables quicker adaptation to new situations and better performance than
traditional RL, leading to broader generalizability.
\\ ( https://arxiv.org/abs/2403.13827 ,  699kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13829 (*cross-listing*)
Date: Thu, 7 Mar 2024 02:53:40 GMT   (23749kb,D)

Title: DecompOpt: Controllable and Decomposed Diffusion Models for
  Structure-based Molecular Optimization
Authors: Xiangxin Zhou, Xiwei Cheng, Yuwei Yang, Yu Bao, Liang Wang, Quanquan
  Gu
Categories: q-bio.BM cs.LG
Comments: Accepted to ICLR 2024
\\
  Recently, 3D generative models have shown promising performances in
structure-based drug design by learning to generate ligands given target
binding sites. However, only modeling the target-ligand distribution can hardly
fulfill one of the main goals in drug discovery -- designing novel ligands with
desired properties, e.g., high binding affinity, easily synthesizable, etc.
This challenge becomes particularly pronounced when the target-ligand pairs
used for training do not align with these desired properties. Moreover, most
existing methods aim at solving \textit{de novo} design task, while many
generative scenarios requiring flexible controllability, such as R-group
optimization and scaffold hopping, have received little attention. In this
work, we propose DecompOpt, a structure-based molecular optimization method
based on a controllable and decomposed diffusion model. DecompOpt presents a
new generation paradigm which combines optimization with conditional diffusion
models to achieve desired properties while adhering to the molecular grammar.
Additionally, DecompOpt offers a unified framework covering both \textit{de
novo} design and controllable generation. To achieve so, ligands are decomposed
into substructures which allows fine-grained control and local optimization.
Experiments show that DecompOpt can efficiently generate molecules with
improved properties than strong de novo baselines, and demonstrate great
potential in controllable generation tasks.
\\ ( https://arxiv.org/abs/2403.13829 ,  23749kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13833 (*cross-listing*)
Date: Fri, 8 Mar 2024 01:01:24 GMT   (403kb,D)

Title: Linearly Constrained Weights: Reducing Activation Shift for Faster
  Training of Neural Networks
Authors: Takuro Kutsuna
Categories: cs.NE cs.LG stat.ML
Comments: 19 pages
DOI: 10.1007/978-3-030-46147-8_16
\\
  In this paper, we first identify activation shift, a simple but remarkable
phenomenon in a neural network in which the preactivation value of a neuron has
non-zero mean that depends on the angle between the weight vector of the neuron
and the mean of the activation vector in the previous layer. We then propose
linearly constrained weights (LCW) to reduce the activation shift in both fully
connected and convolutional layers. The impact of reducing the activation shift
in a neural network is studied from the perspective of how the variance of
variables in the network changes through layer operations in both forward and
backward chains. We also discuss its relationship to the vanishing gradient
problem. Experimental results show that LCW enables a deep feedforward network
with sigmoid activation functions to be trained efficiently by resolving the
vanishing gradient problem. Moreover, combined with batch normalization, LCW
improves generalization performance of both feedforward and convolutional
networks.
\\ ( https://arxiv.org/abs/2403.13833 ,  403kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13851 (*cross-listing*)
Date: Mon, 18 Mar 2024 19:30:46 GMT   (1037kb,D)

Title: Control of Medical Digital Twins with Artificial Neural Networks
Authors: Lucas B\"ottcher, Luis L. Fonseca, Reinhard C. Laubenbacher
Categories: q-bio.QM cs.LG cs.SY eess.SY math.DS math.OC
Comments: 13 pages, 5 figures
\\
  The objective of personalized medicine is to tailor interventions to an
individual patient's unique characteristics. A key technology for this purpose
involves medical digital twins, computational models of human biology that can
be personalized and dynamically updated to incorporate patient-specific data
collected over time. Certain aspects of human biology, such as the immune
system, are not easily captured with physics-based models, such as differential
equations. Instead, they are often multi-scale, stochastic, and hybrid. This
poses a challenge to existing model-based control and optimization approaches
that cannot be readily applied to such models. Recent advances in automatic
differentiation and neural-network control methods hold promise in addressing
complex control problems. However, the application of these approaches to
biomedical systems is still in its early stages. This work introduces
dynamics-informed neural-network controllers as an alternative approach to
control of medical digital twins. As a first use case for this method, the
focus is on agent-based models, a versatile and increasingly common modeling
platform in biomedicine. The effectiveness of the proposed neural-network
control method is illustrated and benchmarked against other methods with two
widely-used agent-based model types. The relevance of the method introduced
here extends beyond medical digital twins to other complex dynamical systems.
\\ ( https://arxiv.org/abs/2403.13851 ,  1037kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13858 (*cross-listing*)
Date: Tue, 19 Mar 2024 22:05:17 GMT   (42409kb,D)

Title: A conditional latent autoregressive recurrent model for generation and
  forecasting of beam dynamics in particle accelerators
Authors: Mahindra Rautela, Alan Williams, Alexander Scheinker
Categories: physics.acc-ph cs.CV cs.LG
\\
  Particle accelerators are complex systems that focus, guide, and accelerate
intense charged particle beams to high energy. Beam diagnostics present a
challenging problem due to limited non-destructive measurements,
computationally demanding simulations, and inherent uncertainties in the
system. We propose a two-step unsupervised deep learning framework named as
Conditional Latent Autoregressive Recurrent Model (CLARM) for learning the
spatiotemporal dynamics of charged particles in accelerators. CLARM consists of
a Conditional Variational Autoencoder (CVAE) transforming six-dimensional phase
space into a lower-dimensional latent distribution and a Long Short-Term Memory
(LSTM) network capturing temporal dynamics in an autoregressive manner. The
CLARM can generate projections at various accelerator modules by sampling and
decoding the latent space representation. The model also forecasts future
states (downstream locations) of charged particles from past states (upstream
locations). The results demonstrate that the generative and forecasting ability
of the proposed approach is promising when tested against a variety of
evaluation metrics.
\\ ( https://arxiv.org/abs/2403.13858 ,  42409kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13865 (*cross-listing*)
Date: Wed, 20 Mar 2024 10:13:54 GMT   (551kb,D)

Title: Graph Neural Network for Crawling Target Nodes in Social Networks
Authors: Kirill Lukyanov, Mikhail Drobyshevskiy, Danil Shaikhelislamov, Denis
  Turdakov
Categories: cs.SI cs.LG
ACM-class: I.2.6
Journal-ref: 2022 Ivannikov Ispras Open Conference (ISPRAS), 2022, pp. 31-36
DOI: 10.1109/ISPRAS57371.2022.10076873
\\
  Social networks crawling is in the focus of active research the last years.
One of the challenging task is to collect target nodes in an initially unknown
graph given a budget of crawling steps. Predicting a node property based on its
partially known neighbourhood is at the heart of a successful crawler. In this
paper we adopt graph neural networks for this purpose and show they are
competitive to traditional classifiers and are better for individual cases.
Additionally we suggest a training sample boosting technique, which helps to
diversify the training set at early stages of crawling and thus improves the
predictor quality. The experimental study on three types of target set topology
indicates GNN based approach has a potential in crawling task, especially in
the case of distributed target nodes.
\\ ( https://arxiv.org/abs/2403.13865 ,  551kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13868 (*cross-listing*)
Date: Wed, 20 Mar 2024 13:39:19 GMT   (43kb,D)

Title: Analysing heavy-tail properties of Stochastic Gradient Descent by means
  of Stochastic Recurrence Equations
Authors: Ewa Damek and Sebastian Mentemeier
Categories: stat.ML cs.LG math.OC math.PR math.ST stat.TH
Comments: 25 pages, 2 figures
MSC-class: 60J05, 90C15, 60B20
\\
  In recent works on the theory of machine learning, it has been observed that
heavy tail properties of Stochastic Gradient Descent (SGD) can be studied in
the probabilistic framework of stochastic recursions. In particular,
G\"{u}rb\"{u}zbalaban et al. (arXiv:2006.04740) considered a setup
corresponding to linear regression for which iterations of SGD can be modelled
by a multivariate affine stochastic recursion $X_k=A_k X_{k-1}+B_k$, for
independent and identically distributed pairs $(A_k, B_k)$, where $A_k$ is a
random symmetric matrix and $B_k$ is a random vector. In this work, we will
answer several open questions of the quoted paper and extend their results by
applying the theory of irreducible-proximal (i-p) matrices.
\\ ( https://arxiv.org/abs/2403.13868 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13870 (*cross-listing*)
Date: Wed, 20 Mar 2024 14:47:28 GMT   (17691kb,D)

Title: ExMap: Leveraging Explainability Heatmaps for Unsupervised Group
  Robustness to Spurious Correlations
Authors: Rwiddhi Chakraborty, Adrian Sletten, Michael Kampffmeyer
Categories: cs.CV cs.LG
\\
  Group robustness strategies aim to mitigate learned biases in deep learning
models that arise from spurious correlations present in their training
datasets. However, most existing methods rely on the access to the label
distribution of the groups, which is time-consuming and expensive to obtain. As
a result, unsupervised group robustness strategies are sought. Based on the
insight that a trained model's classification strategies can be inferred
accurately based on explainability heatmaps, we introduce ExMap, an
unsupervised two stage mechanism designed to enhance group robustness in
traditional classifiers. ExMap utilizes a clustering module to infer
pseudo-labels based on a model's explainability heatmaps, which are then used
during training in lieu of actual labels. Our empirical studies validate the
efficacy of ExMap - We demonstrate that it bridges the performance gap with its
supervised counterparts and outperforms existing partially supervised and
unsupervised methods. Additionally, ExMap can be seamlessly integrated with
existing group robustness learning strategies. Finally, we demonstrate its
potential in tackling the emerging issue of multiple shortcut
mitigation\footnote{Code available at \url{https://github.com/rwchakra/exmap}}.
\\ ( https://arxiv.org/abs/2403.13870 ,  17691kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13910 (*cross-listing*)
Date: Wed, 20 Mar 2024 18:30:12 GMT   (4137kb,D)

Title: Augmented Reality Demonstrations for Scalable Robot Imitation Learning
Authors: Yue Yang, Bryce Ikeda, Gedas Bertasius, Daniel Szafir
Categories: cs.RO cs.GR cs.LG
\\
  Robot Imitation Learning (IL) is a widely used method for training robots to
perform manipulation tasks that involve mimicking human demonstrations to
acquire skills. However, its practicality has been limited due to its
requirement that users be trained in operating real robot arms to provide
demonstrations. This paper presents an innovative solution: an Augmented
Reality (AR)-assisted framework for demonstration collection, empowering
non-roboticist users to produce demonstrations for robot IL using devices like
the HoloLens 2. Our framework facilitates scalable and diverse demonstration
collection for real-world tasks. We validate our approach with experiments on
three classical robotics tasks: reach, push, and pick-and-place. The real robot
performs each task successfully while replaying demonstrations collected via
AR.
\\ ( https://arxiv.org/abs/2403.13910 ,  4137kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13916 (*cross-listing*)
Date: Wed, 20 Mar 2024 18:36:30 GMT   (2269kb,D)

Title: Enhancing Fingerprint Image Synthesis with GANs, Diffusion Models, and
  Style Transfer Techniques
Authors: W. Tang, D. Figueroa, D. Liu, K. Johnsson, A. Sopasakis
Categories: cs.CV cs.LG
\\
  We present novel approaches involving generative adversarial networks and
diffusion models in order to synthesize high quality, live and spoof
fingerprint images while preserving features such as uniqueness and diversity.
We generate live fingerprints from noise with a variety of methods, and we use
image translation techniques to translate live fingerprint images to spoof. To
generate different types of spoof images based on limited training data we
incorporate style transfer techniques through a cycle autoencoder equipped with
a Wasserstein metric along with Gradient Penalty (CycleWGAN-GP) in order to
avoid mode collapse and instability. We find that when the spoof training data
includes distinct spoof characteristics, it leads to improved live-to-spoof
translation. We assess the diversity and realism of the generated live
fingerprint images mainly through the Fr\'echet Inception Distance (FID) and
the False Acceptance Rate (FAR). Our best diffusion model achieved a FID of
15.78. The comparable WGAN-GP model achieved slightly higher FID while
performing better in the uniqueness assessment due to a slightly lower FAR when
matched against the training data, indicating better creativity. Moreover, we
give example images showing that a DDPM model clearly can generate realistic
fingerprint images.
\\ ( https://arxiv.org/abs/2403.13916 ,  2269kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13952 (*cross-listing*)
Date: Wed, 20 Mar 2024 19:49:21 GMT   (6960kb)

Title: Considerations in the use of ML interaction potentials for free energy
  calculations
Authors: Orlando A. Mendible, Jonathan K. Whitmer, and Yamil J. Col\'on
Categories: physics.chem-ph cond-mat.mtrl-sci cs.LG
\\
  Machine learning potentials (MLPs) offer the potential to accurately model
the energy and free energy landscapes of molecules with the precision of
quantum mechanics and an efficiency similar to classical simulations. This
research focuses on using equivariant graph neural networks MLPs due to their
proven effectiveness in modeling equilibrium molecular trajectories. A key
issue addressed is the capability of MLPs to accurately predict free energies
and transition states by considering both the energy and the diversity of
molecular configurations. We examined how the distribution of collective
variables (CVs) in the training data affects MLP accuracy in determining the
free energy surface (FES) of systems, using Metadynamics simulations for butane
and alanine dipeptide (ADP). The study involved training forty-three MLPs, half
based on classical molecular dynamics data and the rest on ab initio computed
energies. The MLPs were trained using different distributions that aim to
replicate hypothetical scenarios of sampled CVs obtained if the underlying FES
of the system was unknown. Findings for butane revealed that training data
coverage of key FES regions ensures model accuracy regardless of CV
distribution. However, missing significant FES regions led to correct potential
energy predictions but failed free energy reconstruction. For ADP, models
trained on classical dynamics data were notably less accurate, while ab
initio-based MLPs predicted potential energy well but faltered on free energy
predictions. These results emphasize the challenge of assembling an
all-encompassing training set for accurate FES prediction and highlight the
importance of understanding the FES in preparing training data. The study
points out the limitations of MLPs in free energy calculations, stressing the
need for comprehensive data that encompasses the system's full FES for
effective model training.
\\ ( https://arxiv.org/abs/2403.13952 ,  6960kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14002 (*cross-listing*)
Date: Wed, 20 Mar 2024 22:03:40 GMT   (16248kb,D)

Title: Uncertainty Driven Active Learning for Image Segmentation in Underwater
  Inspection
Authors: Luiza Ribeiro Marnet, Yury Brodskiy, Stella Grasshof, Andrzej Wasowski
Categories: cs.CV cs.LG
Comments: 16 pages, 8 figures, to be published in the Proceedings of the 4th
  International Conference on Robotics, Computer Vision and Intelligent
  Systems, Springer Nature, Feb 2024
\\
  Active learning aims to select the minimum amount of data to train a model
that performs similarly to a model trained with the entire dataset. We study
the potential of active learning for image segmentation in underwater
infrastructure inspection tasks, where large amounts of data are typically
collected. The pipeline inspection images are usually semantically repetitive
but with great variations in quality. We use mutual information as the
acquisition function, calculated using Monte Carlo dropout. To assess the
effectiveness of the framework, DenseNet and HyperSeg are trained with the
CamVid dataset using active learning. In addition, HyperSeg is trained with a
pipeline inspection dataset of over 50,000 images. For the pipeline dataset,
HyperSeg with active learning achieved 67.5% meanIoU using 12.5% of the data,
and 61.4% with the same amount of randomly selected images. This shows that
using active learning for segmentation models in underwater inspection tasks
can lower the cost significantly.
\\ ( https://arxiv.org/abs/2403.14002 ,  16248kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14003 (*cross-listing*)
Date: Wed, 20 Mar 2024 22:05:18 GMT   (4037kb,D)

Title: Multi-Modal Hallucination Control by Visual Information Grounding
Authors: Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary,
  Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, Stefano Soatto
Categories: cs.CV cs.LG
Journal-ref: IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR) 2024
\\
  Generative Vision-Language Models (VLMs) are prone to generate
plausible-sounding textual answers that, however, are not always grounded in
the input image. We investigate this phenomenon, usually referred to as
"hallucination" and show that it stems from an excessive reliance on the
language prior. In particular, we show that as more tokens are generated, the
reliance on the visual prompt decreases, and this behavior strongly correlates
with the emergence of hallucinations. To reduce hallucinations, we introduce
Multi-Modal Mutual-Information Decoding (M3ID), a new sampling method for
prompt amplification. M3ID amplifies the influence of the reference image over
the language prior, hence favoring the generation of tokens with higher mutual
information with the visual prompt. M3ID can be applied to any pre-trained
autoregressive VLM at inference time without necessitating further training and
with minimal computational overhead. If training is an option, we show that
M3ID can be paired with Direct Preference Optimization (DPO) to improve the
model's reliance on the prompt image without requiring any labels. Our
empirical findings show that our algorithms maintain the fluency and linguistic
capabilities of pre-trained VLMs while reducing hallucinations by mitigating
visually ungrounded answers. Specifically, for the LLaVA 13B model, M3ID and
M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by
25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as
POPE by 21% and 24%.
\\ ( https://arxiv.org/abs/2403.14003 ,  4037kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14013 (*cross-listing*)
Date: Wed, 20 Mar 2024 22:24:36 GMT   (738kb,D)

Title: Towards a connection between the capacitated vehicle routing problem and
  the constrained centroid-based clustering
Authors: Abdelhakim Abdellaoui, Loubna Benabbou, Issmail El Hallaoui
Categories: math.OC cs.LG
\\
  Efficiently solving a vehicle routing problem (VRP) in a practical runtime is
a critical challenge for delivery management companies. This paper explores
both a theoretical and experimental connection between the Capacitated Vehicle
Routing Problem (CVRP) and the Constrained Centroid-Based Clustering (CCBC).
Reducing a CVRP to a CCBC is a synonym for a transition from an exponential to
a polynomial complexity using commonly known algorithms for clustering, i.e
K-means. At the beginning, we conduct an exploratory analysis to highlight the
existence of such a relationship between the two problems through illustrative
small-size examples and simultaneously deduce some mathematically-related
formulations and properties. On a second level, the paper proposes a CCBC based
approach endowed with some enhancements. The proposed framework consists of
three stages. At the first step, a constrained centroid-based clustering
algorithm generates feasible clusters of customers. This methodology
incorporates three enhancement tools to achieve near-optimal clusters, namely:
a multi-start procedure for initial centroids, a customer assignment metric,
and a self-adjustment mechanism for choosing the number of clusters. At the
second step, a traveling salesman problem (T SP) solver is used to optimize the
order of customers within each cluster. Finally, we introduce a process relying
on routes cutting and relinking procedure, which calls upon solving a linear
and integer programming model to further improve the obtained routes. This step
is inspired by the ruin & recreate algorithm. This approach is an extension of
the classical cluster-first, route-second method and provides near-optimal
solutions on well-known benchmark instances in terms of solution quality and
computational runtime, offering a milestone in solving VRP.
\\ ( https://arxiv.org/abs/2403.14013 ,  738kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14067 (*cross-listing*)
Date: Thu, 21 Mar 2024 01:30:24 GMT   (3977kb,D)

Title: Automatic Outlier Rectification via Optimal Transport
Authors: Jose Blanchet, Jiajin Li, Markus Pelger, Greg Zanotti
Categories: stat.ML cs.LG math.OC stat.ME
\\
  In this paper, we propose a novel conceptual framework to detect outliers
using optimal transport with a concave cost function. Conventional outlier
detection approaches typically use a two-stage procedure: first, outliers are
detected and removed, and then estimation is performed on the cleaned data.
However, this approach does not inform outlier removal with the estimation
task, leaving room for improvement. To address this limitation, we propose an
automatic outlier rectification mechanism that integrates rectification and
estimation within a joint optimization framework. We take the first step to
utilize an optimal transport distance with a concave cost function to construct
a rectification set in the space of probability distributions. Then, we select
the best distribution within the rectification set to perform the estimation
task. Notably, the concave cost function we introduced in this paper is the key
to making our estimator effectively identify the outlier during the
optimization process. We discuss the fundamental differences between our
estimator and optimal transport-based distributionally robust optimization
estimator. finally, we demonstrate the effectiveness and superiority of our
approach over conventional approaches in extensive simulation and empirical
analyses for mean estimation, least absolute regression, and the fitting of
option implied volatility surfaces.
\\ ( https://arxiv.org/abs/2403.14067 ,  3977kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14076 (*cross-listing*)
Date: Thu, 21 Mar 2024 01:54:00 GMT   (348kb,D)

Title: Improving $\Lambda$ Signal Extraction with Domain Adaptation via
  Normalizing Flows
Authors: Rowan Kelleher, Matthew McEneaney, Anselm Vossen
Categories: hep-ex cs.LG
Comments: Proceedings for the 25th International Spin Physics Symposium (SPIN
  2023)
\\
  The present study presents a novel application for normalizing flows for
domain adaptation. The study investigates the ability of flow based neural
networks to improve signal extraction of $\Lambda$ Hyperons at CLAS12.
Normalizing Flows can help model complex probability density functions that
describe physics processes, enabling uses such as event generation. $\Lambda$
signal extraction has been improved through the use of classifier networks, but
differences in simulation and data domains limit classifier performance; this
study utilizes the flows for domain adaptation between Monte Carlo simulation
and data. We were successful in training a flow network to transform between
the latent physics space and a normal distribution. We also found that applying
the flows lessened the dependence of the figure of merit on the cut on the
classifier output, meaning that there was a broader range where the cut results
in a similar figure of merit.
\\ ( https://arxiv.org/abs/2403.14076 ,  348kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14083 (*cross-listing*)
Date: Thu, 21 Mar 2024 02:26:30 GMT   (879kb,D)

Title: emoDARTS: Joint Optimisation of CNN & Sequential Neural Network
  Architectures for Superior Speech Emotion Recognition
Authors: Thejan Rajapakshe, Rajib Rana, Sara Khalifa, Berrak Sisman, Bjorn W.
  Schuller, Carlos Busso
Categories: cs.SD cs.LG eess.AS
Comments: Submitted to IEEE Transactions on Affective Computing on February 19,
  2024. arXiv admin note: text overlap with arXiv:2305.14402
\\
  Speech Emotion Recognition (SER) is crucial for enabling computers to
understand the emotions conveyed in human communication. With recent
advancements in Deep Learning (DL), the performance of SER models has
significantly improved. However, designing an optimal DL architecture requires
specialised knowledge and experimental assessments. Fortunately, Neural
Architecture Search (NAS) provides a potential solution for automatically
determining the best DL model. The Differentiable Architecture Search (DARTS)
is a particularly efficient method for discovering optimal models. This study
presents emoDARTS, a DARTS-optimised joint CNN and Sequential Neural Network
(SeqNN: LSTM, RNN) architecture that enhances SER performance. The literature
supports the selection of CNN and LSTM coupling to improve performance.
  While DARTS has previously been used to choose CNN and LSTM operations
independently, our technique adds a novel mechanism for selecting CNN and SeqNN
operations in conjunction using DARTS. Unlike earlier work, we do not impose
limits on the layer order of the CNN. Instead, we let DARTS choose the best
layer order inside the DARTS cell. We demonstrate that emoDARTS outperforms
conventionally designed CNN-LSTM models and surpasses the best-reported SER
results achieved through DARTS on CNN-LSTM by evaluating our approach on the
IEMOCAP, MSP-IMPROV, and MSP-Podcast datasets.
\\ ( https://arxiv.org/abs/2403.14083 ,  879kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14084 (*cross-listing*)
Date: Thu, 21 Mar 2024 02:30:56 GMT   (361kb,D)

Title: Learning-based Multi-continuum Model for Multiscale Flow Problems
Authors: Fan Wang, Yating Wang, Wing Tat Leung and Zongben Xu
Categories: math.NA cs.LG cs.NA
\\
  Multiscale problems can usually be approximated through numerical
homogenization by an equation with some effective parameters that can capture
the macroscopic behavior of the original system on the coarse grid to speed up
the simulation. However, this approach usually assumes scale separation and
that the heterogeneity of the solution can be approximated by the solution
average in each coarse block. For complex multiscale problems, the computed
single effective properties/continuum might be inadequate. In this paper, we
propose a novel learning-based multi-continuum model to enrich the homogenized
equation and improve the accuracy of the single continuum model for multiscale
problems with some given data. Without loss of generalization, we consider a
two-continuum case. The first flow equation keeps the information of the
original homogenized equation with an additional interaction term. The second
continuum is newly introduced, and the effective permeability in the second
flow equation is determined by a neural network. The interaction term between
the two continua aligns with that used in the Dual-porosity model but with a
learnable coefficient determined by another neural network. The new model with
neural network terms is then optimized using trusted data. We discuss both
direct back-propagation and the adjoint method for the PDE-constraint
optimization problem. Our proposed learning-based multi-continuum model can
resolve multiple interacted media within each coarse grid block and describe
the mass transfer among them, and it has been demonstrated to significantly
improve the simulation results through numerical experiments involving both
linear and nonlinear flow equations.
\\ ( https://arxiv.org/abs/2403.14084 ,  361kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14088 (*cross-listing*)
Date: Thu, 21 Mar 2024 02:44:08 GMT   (6992kb,D)

Title: Protein Conformation Generation via Force-Guided SE(3) Diffusion Models
Authors: Yan Wang, Lihao Wang, Yuning Shen, Yiqun Wang, Huizhuo Yuan, Yue Wu,
  Quanquan Gu
Categories: q-bio.BM cs.LG
\\
  The conformational landscape of proteins is crucial to understanding their
functionality in complex biological processes. Traditional physics-based
computational methods, such as molecular dynamics (MD) simulations, suffer from
rare event sampling and long equilibration time problems, hindering their
applications in general protein systems. Recently, deep generative modeling
techniques, especially diffusion models, have been employed to generate novel
protein conformations. However, existing score-based diffusion methods cannot
properly incorporate important physical prior knowledge to guide the generation
process, causing large deviations in the sampled protein conformations from the
equilibrium distribution. In this paper, to overcome these limitations, we
propose a force-guided SE(3) diffusion model, ConfDiff, for protein
conformation generation. By incorporating a force-guided network with a mixture
of data-based score models, ConfDiff can can generate protein conformations
with rich diversity while preserving high fidelity. Experiments on a variety of
protein conformation prediction tasks, including 12 fast-folding proteins and
the Bovine Pancreatic Trypsin Inhibitor (BPTI), demonstrate that our method
surpasses the state-of-the-art method.
\\ ( https://arxiv.org/abs/2403.14088 ,  6992kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14101 (*cross-listing*)
Date: Thu, 21 Mar 2024 03:24:01 GMT   (1809kb,D)

Title: Text-Enhanced Data-free Approach for Federated Class-Incremental
  Learning
Authors: Minh-Tuan Tran, Trung Le, Xuan-May Le, Mehrtash Harandi, and Dinh
  Phung
Categories: cs.CV cs.LG
Comments: Accepted at CVPR 2024
\\
  Federated Class-Incremental Learning (FCIL) is an underexplored yet pivotal
issue, involving the dynamic addition of new classes in the context of
federated learning. In this field, Data-Free Knowledge Transfer (DFKT) plays a
crucial role in addressing catastrophic forgetting and data privacy problems.
However, prior approaches lack the crucial synergy between DFKT and the model
training phases, causing DFKT to encounter difficulties in generating
high-quality data from a non-anchored latent space of the old task model. In
this paper, we introduce LANDER (Label Text Centered Data-Free Knowledge
Transfer) to address this issue by utilizing label text embeddings (LTE)
produced by pretrained language models. Specifically, during the model training
phase, our approach treats LTE as anchor points and constrains the feature
embeddings of corresponding training samples around them, enriching the
surrounding area with more meaningful information. In the DFKT phase, by using
these LTE anchors, LANDER can synthesize more meaningful samples, thereby
effectively addressing the forgetting problem. Additionally, instead of tightly
constraining embeddings toward the anchor, the Bounding Loss is introduced to
encourage sample embeddings to remain flexible within a defined radius. This
approach preserves the natural differences in sample embeddings and mitigates
the embedding overlap caused by heterogeneous federated settings. Extensive
experiments conducted on CIFAR100, Tiny-ImageNet, and ImageNet demonstrate that
LANDER significantly outperforms previous methods and achieves state-of-the-art
performance in FCIL. The code is available at
https://github.com/tmtuan1307/lander.
\\ ( https://arxiv.org/abs/2403.14101 ,  1809kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14111 (*cross-listing*)
Date: Thu, 21 Mar 2024 03:47:26 GMT   (440kb,D)

Title: HETAL: Efficient Privacy-preserving Transfer Learning with Homomorphic
  Encryption
Authors: Seewoo Lee, Garam Lee, Jung Woo Kim, Junbum Shin, Mun-Kyu Lee
Categories: cs.CR cs.LG
Comments: ICML 2023, Appendix D includes some updates after official
  publication
\\
  Transfer learning is a de facto standard method for efficiently training
machine learning models for data-scarce problems by adding and fine-tuning new
classification layers to a model pre-trained on large datasets. Although
numerous previous studies proposed to use homomorphic encryption to resolve the
data privacy issue in transfer learning in the machine learning as a service
setting, most of them only focused on encrypted inference. In this study, we
present HETAL, an efficient Homomorphic Encryption based Transfer Learning
algorithm, that protects the client's privacy in training tasks by encrypting
the client data using the CKKS homomorphic encryption scheme. HETAL is the
first practical scheme that strictly provides encrypted training, adopting
validation-based early stopping and achieving the accuracy of nonencrypted
training. We propose an efficient encrypted matrix multiplication algorithm,
which is 1.8 to 323 times faster than prior methods, and a highly precise
softmax approximation algorithm with increased coverage. The experimental
results for five well-known benchmark datasets show total training times of
567-3442 seconds, which is less than an hour.
\\ ( https://arxiv.org/abs/2403.14111 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14125 (*cross-listing*)
Date: Thu, 21 Mar 2024 04:42:04 GMT   (1272kb,D)

Title: Learning causal graphs using variable grouping according to ancestral
  relationship
Authors: Ming Cai and Hisayuki Hara
Categories: stat.ML cs.LG
Comments: 12 pages, 5 figures
\\
  Several causal discovery algorithms have been proposed. However, when the
sample size is small relative to the number of variables, the accuracy of
estimating causal graphs using existing methods decreases. And some methods are
not feasible when the sample size is smaller than the number of variables. To
circumvent these problems, some researchers proposed causal structure learning
algorithms using divide-and-conquer approaches. For learning the entire causal
graph, the approaches first split variables into several subsets according to
the conditional independence relationships among the variables, then apply a
conventional causal discovery algorithm to each subset and merge the estimated
results. Since the divide-and-conquer approach reduces the number of variables
to which a causal structure learning algorithm is applied, it is expected to
improve the estimation accuracy of causal graphs, especially when the sample
size is small relative to the number of variables and the model is sparse.
However, existing methods are either computationally expensive or do not
provide sufficient accuracy when the sample size is small. This paper proposes
a new algorithm for grouping variables based the ancestral relationships among
the variables, under the LiNGAM assumption, where the causal relationships are
linear, and the mutually independent noise are distributed as continuous
non-Gaussian distributions. We call the proposed algorithm CAG. The time
complexity of the ancestor finding in CAG is shown to be cubic to the number of
variables. Extensive computer experiments confirm that the proposed method
outperforms the original DirectLiNGAM without grouping variables and other
divide-and-conquer approaches not only in estimation accuracy but also in
computation time when the sample size is small relative to the number of
variables and the model is sparse.
\\ ( https://arxiv.org/abs/2403.14125 ,  1272kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14137 (*cross-listing*)
Date: Thu, 21 Mar 2024 05:13:12 GMT   (7519kb,D)

Title: Improving Image Classification Accuracy through Complementary
  Intra-Class and Inter-Class Mixup
Authors: Ye Xu, Ya Gao, Xiaorong Qiu, Yang Chen, Ying Ji
Categories: cs.CV cs.LG
Comments: 25 pages,12 figures
\\
  MixUp and its variants, such as Manifold MixUp, have two key limitations in
image classification tasks. First, they often neglect mixing within the same
class (intra-class mixup), leading to an underutilization of the relationships
among samples within the same class. Second, although these methods effectively
enhance inter-class separability by mixing between different classes
(inter-class mixup), they fall short in improving intra-class cohesion through
their mixing operations, limiting their classification performance. To tackle
these issues, we propose a novel mixup method and a comprehensive integrated
solution.Our mixup approach specifically targets intra-class mixup, an aspect
commonly overlooked, to strengthen intra-class cohesion-a feature not provided
by current mixup techniques.For each mini-batch, our method utilizes feature
representations of unaugmented original images from each class within the
mini-batch to generate a single synthesized feature representation through
random linear interpolation. All synthesized representations for this
mini-batch are then fed into the classification and loss layers to calculate an
average classification loss that can markedly enhance intra-class cohesion.
Moreover, our integrated solution seamlessly combines our intra-class mixup
method with an existing mixup approach such as MixUp or Manifold MixUp. This
comprehensive solution incorporates inter- and intra-class mixup in a balanced
manner while concurrently improving intra-class cohesion and inter-class
separability. Experimental results on six public datasets demonstrate that our
integrated solution achieves a 0.1% to 3.43% higher accuracy than the best of
either MixUp or our intra-class mixup method, averaging a 1.16% gain. It also
outperforms the better performer of either Manifold MixUp or our intra-class
mixup method by 0.12% to 5.16%, with an average gain of 1.11%.
\\ ( https://arxiv.org/abs/2403.14137 ,  7519kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14139 (*cross-listing*)
Date: Thu, 21 Mar 2024 05:17:22 GMT   (15902kb,D)

Title: Genetic Programming for Explainable Manifold Learning
Authors: Ben Cravens, Andrew Lensen, Paula Maddigan, Bing Xue
Categories: cs.NE cs.LG
\\
  Manifold learning techniques play a pivotal role in machine learning by
revealing lower-dimensional embeddings within high-dimensional data, thus
enhancing both the efficiency and interpretability of data analysis by
transforming the data into a lower-dimensional representation. However, a
notable challenge with current manifold learning methods is their lack of
explicit functional mappings, crucial for explainability in many real-world
applications. Genetic programming, known for its interpretable functional
tree-based models, has emerged as a promising approach to address this
challenge. Previous research leveraged multi-objective GP to balance manifold
quality against embedding dimensionality, producing functional mappings across
a range of embedding sizes. Yet, these mapping trees often became complex,
hindering explainability. In response, in this paper, we introduce Genetic
Programming for Explainable Manifold Learning (GP-EMaL), a novel approach that
directly penalises tree complexity. Our new method is able to maintain high
manifold quality while significantly enhancing explainability and also allows
customisation of complexity measures, such as symmetry balancing, scaling, and
node complexity, catering to diverse application needs. Our experimental
analysis demonstrates that GP-EMaL is able to match the performance of the
existing approach in most cases, while using simpler, smaller, and more
interpretable tree structures. This advancement marks a significant step
towards achieving interpretable manifold learning.
\\ ( https://arxiv.org/abs/2403.14139 ,  15902kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14140 (*cross-listing*)
Date: Thu, 21 Mar 2024 05:33:49 GMT   (19528kb,D)

Title: Learning Decomposable and Debiased Representations via Attribute-Centric
  Information Bottlenecks
Authors: Jinyung Hong, Eun Som Jeon, Changhoon Kim, Keun Hee Park, Utkarsh
  Nath, Yezhou Yang, Pavan Turaga, Theodore P. Pavlic
Categories: cs.CV cs.LG
Comments: 24 pages, 16 figures, 3 tables
\\
  Biased attributes, spuriously correlated with target labels in a dataset, can
problematically lead to neural networks that learn improper shortcuts for
classifications and limit their capabilities for out-of-distribution (OOD)
generalization. Although many debiasing approaches have been proposed to ensure
correct predictions from biased datasets, few studies have considered learning
latent embedding consisting of intrinsic and biased attributes that contribute
to improved performance and explain how the model pays attention to attributes.
In this paper, we propose a novel debiasing framework, Debiasing Global
Workspace, introducing attention-based information bottlenecks for learning
compositional representations of attributes without defining specific bias
types. Based on our observation that learning shape-centric representation
helps robust performance on OOD datasets, we adopt those abilities to learn
robust and generalizable representations of decomposable latent embeddings
corresponding to intrinsic and biasing attributes. We conduct comprehensive
evaluations on biased datasets, along with both quantitative and qualitative
analyses, to showcase our approach's efficacy in attribute-centric
representation learning and its ability to differentiate between intrinsic and
bias-related features.
\\ ( https://arxiv.org/abs/2403.14140 ,  19528kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14148 (*cross-listing*)
Date: Thu, 21 Mar 2024 05:48:48 GMT   (36303kb,D)

Title: Efficient Video Diffusion Models via Content-Frame Motion-Latent
  Decomposition
Authors: Sihyun Yu, Weili Nie, De-An Huang, Boyi Li, Jinwoo Shin, Anima
  Anandkumar
Categories: cs.CV cs.LG
Comments: ICLR 2024. Project page: https://sihyun.me/CMD
\\
  Video diffusion models have recently made great progress in generation
quality, but are still limited by the high memory and computational
requirements. This is because current video diffusion models often attempt to
process high-dimensional videos directly. To tackle this issue, we propose
content-motion latent diffusion model (CMD), a novel efficient extension of
pretrained image diffusion models for video generation. Specifically, we
propose an autoencoder that succinctly encodes a video as a combination of a
content frame (like an image) and a low-dimensional motion latent
representation. The former represents the common content, and the latter
represents the underlying motion in the video, respectively. We generate the
content frame by fine-tuning a pretrained image diffusion model, and we
generate the motion latent representation by training a new lightweight
diffusion model. A key innovation here is the design of a compact latent space
that can directly utilizes a pretrained image diffusion model, which has not
been done in previous latent video diffusion models. This leads to considerably
better quality generation and reduced computational costs. For instance, CMD
can sample a video 7.7$\times$ faster than prior approaches by generating a
video of 512$\times$1024 resolution and length 16 in 3.1 seconds. Moreover, CMD
achieves an FVD score of 212.7 on WebVid-10M, 27.3% better than the previous
state-of-the-art of 292.4.
\\ ( https://arxiv.org/abs/2403.14148 ,  36303kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14225 (*cross-listing*)
Date: Thu, 21 Mar 2024 08:31:36 GMT   (137kb,D)

Title: Posterior concentrations of fully-connected Bayesian neural networks
  with general priors on the weights
Authors: Insung Kong and Yongdai Kim
Categories: stat.ML cs.LG
\\
  Bayesian approaches for training deep neural networks (BNNs) have received
significant interest and have been effectively utilized in a wide range of
applications. There have been several studies on the properties of posterior
concentrations of BNNs. However, most of these studies only demonstrate results
in BNN models with sparse or heavy-tailed priors. Surprisingly, no theoretical
results currently exist for BNNs using Gaussian priors, which are the most
commonly used one. The lack of theory arises from the absence of approximation
results of Deep Neural Networks (DNNs) that are non-sparse and have bounded
parameters. In this paper, we present a new approximation theory for non-sparse
DNNs with bounded parameters. Additionally, based on the approximation theory,
we show that BNNs with non-sparse general priors can achieve near-minimax
optimal posterior concentration rates to the true model.
\\ ( https://arxiv.org/abs/2403.14225 ,  137kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14228 (*cross-listing*)
Date: Thu, 21 Mar 2024 08:39:13 GMT   (5542kb,D)

Title: Recovering Latent Confounders from High-dimensional Proxy Variables
Authors: Nathan Mankovich, Homer Durand, Emiliano Diaz, Gherardo Varando, and
  Gustau Camps-Valls
Categories: stat.ML cs.LG
\\
  Detecting latent confounders from proxy variables is an essential problem in
causal effect estimation. Previous approaches are limited to low-dimensional
proxies, sorted proxies, and binary treatments. We remove these assumptions and
present a novel Proxy Confounder Factorization (PCF) framework for continuous
treatment effect estimation when latent confounders manifest through
high-dimensional, mixed proxy variables. For specific sample sizes, our
two-step PCF implementation, using Independent Component Analysis (ICA-PCF),
and the end-to-end implementation, using Gradient Descent (GD-PCF), achieve
high correlation with the latent confounder and low absolute error in causal
effect estimation with synthetic datasets in the high sample size regime. Even
when faced with climate data, ICA-PCF recovers four components that explain
$75.9\%$ of the variance in the North Atlantic Oscillation, a known confounder
of precipitation patterns in Europe. Code for our PCF implementations and
experiments can be found here: https://github.com/IPL-UV/confound_it. The
proposed methodology constitutes a stepping stone towards discovering latent
confounders and can be applied to many problems in disciplines dealing with
high-dimensional observed proxies, e.g., spatiotemporal fields.
\\ ( https://arxiv.org/abs/2403.14228 ,  5542kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14235 (*cross-listing*)
Date: Thu, 21 Mar 2024 08:52:39 GMT   (7902kb,D)

Title: RG-CAT: Detection Pipeline and Catalogue of Radio Galaxies in the EMU
  Pilot Survey
Authors: Nikhel Gupta, Ray P. Norris, Zeeshan Hayder, Minh Huynh, Lars
  Petersson, X. Rosalind Wang, Andrew M. Hopkins, Heinz Andernach, Yjan Gordon,
  Simone Riggi, Miranda Yew, Evan J. Crawford, B\"arbel Koribalski, Miroslav D.
  Filipovi\'c, Anna D. Kapin\'ska, Stanislav Shabala, Tessa Vernstrom, and
  Joshua R. Marvil
Categories: astro-ph.GA astro-ph.CO astro-ph.IM cs.CV cs.LG
Comments: Accepted for publication in PASA. The paper has 22 pages, 12 figures
  and 5 tables
\\
  We present source detection and catalogue construction pipelines to build the
first catalogue of radio galaxies from the 270 $\rm deg^2$ pilot survey of the
Evolutionary Map of the Universe (EMU-PS) conducted with the Australian Square
Kilometre Array Pathfinder (ASKAP) telescope. The detection pipeline uses
Gal-DINO computer-vision networks (Gupta et al., 2024) to predict the
categories of radio morphology and bounding boxes for radio sources, as well as
their potential infrared host positions. The Gal-DINO network is trained and
evaluated on approximately 5,000 visually inspected radio galaxies and their
infrared hosts, encompassing both compact and extended radio morphologies. We
find that the Intersection over Union (IoU) for the predicted and ground truth
bounding boxes is larger than 0.5 for 99% of the radio sources, and 98% of
predicted host positions are within $3^{\prime \prime}$ of the ground truth
infrared host in the evaluation set. The catalogue construction pipeline uses
the predictions of the trained network on the radio and infrared image cutouts
based on the catalogue of radio components identified using the Selavy source
finder algorithm. Confidence scores of the predictions are then used to
prioritize Selavy components with higher scores and incorporate them first into
the catalogue. This results in identifications for a total of 211,625 radio
sources, with 201,211 classified as compact and unresolved. The remaining
10,414 are categorized as extended radio morphologies, including 582 FR-I,
5,602 FR-II, 1,494 FR-x (uncertain whether FR-I or FR-II), 2,375 R (single-peak
resolved) radio galaxies, and 361 with peculiar and other rare morphologies. We
cross-match the radio sources in the catalogue with the infrared and optical
catalogues, finding infrared cross-matches for 73% and photometric redshifts
for 36% of the radio galaxies.
\\ ( https://arxiv.org/abs/2403.14235 ,  7902kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14262 (*cross-listing*)
Date: Thu, 21 Mar 2024 09:50:39 GMT   (257kb,D)

Title: Diffusion Models with Ensembled Structure-Based Anomaly Scoring for
  Unsupervised Anomaly Detection
Authors: Finn Behrendt, Debayan Bhattacharya, Lennart Maack, Julia Kr\"uger,
  Roland Opfer, Robin Mieling, Alexander Schlaefer
Categories: eess.IV cs.CV cs.LG
Comments: Accepted at IEEE ISBI 2024
\\
  Supervised deep learning techniques show promise in medical image analysis.
However, they require comprehensive annotated data sets, which poses
challenges, particularly for rare diseases. Consequently, unsupervised anomaly
detection (UAD) emerges as a viable alternative for pathology segmentation, as
only healthy data is required for training. However, recent UAD anomaly scoring
functions often focus on intensity only and neglect structural differences,
which impedes the segmentation performance. This work investigates the
potential of Structural Similarity (SSIM) to bridge this gap. SSIM captures
both intensity and structural disparities and can be advantageous over the
classical $l1$ error. However, we show that there is more than one optimal
kernel size for the SSIM calculation for different pathologies. Therefore, we
investigate an adaptive ensembling strategy for various kernel sizes to offer a
more pathology-agnostic scoring mechanism. We demonstrate that this ensembling
strategy can enhance the performance of DMs and mitigate the sensitivity to
different kernel sizes across varying pathologies, highlighting its promise for
brain MRI anomaly detection.
\\ ( https://arxiv.org/abs/2403.14262 ,  257kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14286 (*cross-listing*)
Date: Thu, 21 Mar 2024 10:49:54 GMT   (1293kb,D)

Title: Assessing the Robustness of Spectral Clustering for Deep Speaker
  Diarization
Authors: Nikhil Raghav, Md Sahidullah
Categories: cs.SD cs.CV cs.LG
Comments: Manuscript Under Review
\\
  Clustering speaker embeddings is crucial in speaker diarization but hasn't
received as much focus as other components. Moreover, the robustness of speaker
diarization across various datasets hasn't been explored when the development
and evaluation data are from different domains. To bridge this gap, this study
thoroughly examines spectral clustering for both same-domain and cross-domain
speaker diarization. Our extensive experiments on two widely used corpora, AMI
and DIHARD, reveal the performance trend of speaker diarization in the presence
of domain mismatch. We observe that the performance difference between two
different domain conditions can be attributed to the role of spectral
clustering. In particular, keeping other modules unchanged, we show that
differences in optimal tuning parameters as well as speaker count estimation
originates due to the mismatch. This study opens several future directions for
speaker diarization research.
\\ ( https://arxiv.org/abs/2403.14286 ,  1293kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14290 (*cross-listing*)
Date: Thu, 21 Mar 2024 10:54:21 GMT   (897kb,D)

Title: Exploring Green AI for Audio Deepfake Detection
Authors: Subhajit Saha, Md Sahidullah, Swagatam Das
Categories: cs.SD cs.CV cs.LG
Comments: This manuscript is under review in a conference
\\
  The state-of-the-art audio deepfake detectors leveraging deep neural networks
exhibit impressive recognition performance. Nonetheless, this advantage is
accompanied by a significant carbon footprint. This is mainly due to the use of
high-performance computing with accelerators and high training time. Studies
show that average deep NLP model produces around 626k lbs of
CO\textsubscript{2} which is equivalent to five times of average US car
emission at its lifetime. This is certainly a massive threat to the
environment. To tackle this challenge, this study presents a novel framework
for audio deepfake detection that can be seamlessly trained using standard CPU
resources. Our proposed framework utilizes off-the-shelve self-supervised
learning (SSL) based models which are pre-trained and available in public
repositories. In contrast to existing methods that fine-tune SSL models and
employ additional deep neural networks for downstream tasks, we exploit
classical machine learning algorithms such as logistic regression and shallow
neural networks using the SSL embeddings extracted using the pre-trained model.
Our approach shows competitive results compared to the commonly used
high-carbon footprint approaches. In experiments with the ASVspoof 2019 LA
dataset, we achieve a 0.90\% equal error rate (EER) with less than 1k trainable
model parameters. To encourage further research in this direction and support
reproducible results, the Python code will be made publicly accessible
following
acceptance\footnote{\href{https://github.com/sahasubhajit/Speech-Spoofing-}{GitHub
link}}.
\\ ( https://arxiv.org/abs/2403.14290 ,  897kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14302 (*cross-listing*)
Date: Thu, 21 Mar 2024 11:16:42 GMT   (529kb,D)

Title: SpikingResformer: Bridging ResNet and Vision Transformer in Spiking
  Neural Networks
Authors: Xinyu Shi, Zecheng Hao, Zhaofei Yu
Categories: cs.NE cs.CV cs.LG
Comments: To be published in the 2024 IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)
\\
  The remarkable success of Vision Transformers in Artificial Neural Networks
(ANNs) has led to a growing interest in incorporating the self-attention
mechanism and transformer-based architecture into Spiking Neural Networks
(SNNs). While existing methods propose spiking self-attention mechanisms that
are compatible with SNNs, they lack reasonable scaling methods, and the overall
architectures proposed by these methods suffer from a bottleneck in effectively
extracting local features. To address these challenges, we propose a novel
spiking self-attention mechanism named Dual Spike Self-Attention (DSSA) with a
reasonable scaling method. Based on DSSA, we propose a novel spiking Vision
Transformer architecture called SpikingResformer, which combines the
ResNet-based multi-stage architecture with our proposed DSSA to improve both
performance and energy efficiency while reducing parameters. Experimental
results show that SpikingResformer achieves higher accuracy with fewer
parameters and lower energy consumption than other spiking Vision Transformer
counterparts. Notably, our SpikingResformer-L achieves 79.40% top-1 accuracy on
ImageNet with 4 time-steps, which is the state-of-the-art result in the SNN
field.
\\ ( https://arxiv.org/abs/2403.14302 ,  529kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14324 (*cross-listing*)
Date: Thu, 21 Mar 2024 11:44:25 GMT   (2122kb)

Title: Neural Network-Based Processing and Reconstruction of Compromised
  Biophotonic Image Data
Authors: Michael John Fanous, Paloma Casteleiro Costa, Cagatay Isil, Luzhe
  Huang, Aydogan Ozcan
Categories: physics.optics cs.CV cs.LG physics.app-ph
Comments: 17 Pages, 4 Figures, 1 Table
\\
  The integration of deep learning techniques with biophotonic setups has
opened new horizons in bioimaging. A compelling trend in this field involves
deliberately compromising certain measurement metrics to engineer better
bioimaging tools in terms of cost, speed, and form-factor, followed by
compensating for the resulting defects through the utilization of deep learning
models trained on a large amount of ideal, superior or alternative data. This
strategic approach has found increasing popularity due to its potential to
enhance various aspects of biophotonic imaging. One of the primary motivations
for employing this strategy is the pursuit of higher temporal resolution or
increased imaging speed, critical for capturing fine dynamic biological
processes. This approach also offers the prospect of simplifying hardware
requirements/complexities, thereby making advanced imaging standards more
accessible in terms of cost and/or size. This article provides an in-depth
review of the diverse measurement aspects that researchers intentionally impair
in their biophotonic setups, including the point spread function,
signal-to-noise ratio, sampling density, and pixel resolution. By deliberately
compromising these metrics, researchers aim to not only recuperate them through
the application of deep learning networks, but also bolster in return other
crucial parameters, such as the field-of-view, depth-of-field, and
space-bandwidth product. Here, we discuss various biophotonic methods that have
successfully employed this strategic approach. These techniques span broad
applications and showcase the versatility and effectiveness of deep learning in
the context of compromised biophotonic data. Finally, by offering our
perspectives on the future possibilities of this rapidly evolving concept, we
hope to motivate our readers to explore novel ways of balancing hardware
compromises with compensation via AI.
\\ ( https://arxiv.org/abs/2403.14324 ,  2122kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14332 (*cross-listing*)
Date: Thu, 21 Mar 2024 11:57:16 GMT   (57kb)

Title: A Differentially Private Clustering Algorithm for Well-Clustered Graphs
Authors: Weiqiang He, Hendrik Fichtenberger, Pan Peng
Categories: cs.DS cs.CR cs.LG
\\
  We study differentially private (DP) algorithms for recovering clusters in
well-clustered graphs, which are graphs whose vertex set can be partitioned
into a small number of sets, each inducing a subgraph of high inner conductance
and small outer conductance. Such graphs have widespread application as a
benchmark in the theoretical analysis of spectral clustering. We provide an
efficient ($\epsilon$,$\delta$)-DP algorithm tailored specifically for such
graphs. Our algorithm draws inspiration from the recent work of Chen et al.,
who developed DP algorithms for recovery of stochastic block models in cases
where the graph comprises exactly two nearly-balanced clusters. Our algorithm
works for well-clustered graphs with $k$ nearly-balanced clusters, and the
misclassification ratio almost matches the one of the best-known non-private
algorithms. We conduct experimental evaluations on datasets with known ground
truth clusters to substantiate the prowess of our algorithm. We also show that
any (pure) $\epsilon$-DP algorithm would result in substantial error.
\\ ( https://arxiv.org/abs/2403.14332 ,  57kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14353 (*cross-listing*)
Date: Thu, 21 Mar 2024 12:28:44 GMT   (770kb,D)

Title: DaCapo: Accelerating Continuous Learning in Autonomous Systems for Video
  Analytics
Authors: Yoonsung Kim, Changhun Oh, Jinwoo Hwang, Wonung Kim, Seongryong Oh,
  Yubin Lee, Hardik Sharma, Amir Yazdanbakhsh, Jongse Park
Categories: cs.AR cs.LG cs.RO
\\
  Deep neural network (DNN) video analytics is crucial for autonomous systems
such as self-driving vehicles, unmanned aerial vehicles (UAVs), and security
robots. However, real-world deployment faces challenges due to their limited
computational resources and battery power. To tackle these challenges,
continuous learning exploits a lightweight "student" model at deployment
(inference), leverages a larger "teacher" model for labeling sampled data
(labeling), and continuously retrains the student model to adapt to changing
scenarios (retraining). This paper highlights the limitations in
state-of-the-art continuous learning systems: (1) they focus on computations
for retraining, while overlooking the compute needs for inference and labeling,
(2) they rely on power-hungry GPUs, unsuitable for battery-operated autonomous
systems, and (3) they are located on a remote centralized server, intended for
multi-tenant scenarios, again unsuitable for autonomous systems due to privacy,
network availability, and latency concerns. We propose a hardware-algorithm
co-designed solution for continuous learning, DaCapo, that enables autonomous
systems to perform concurrent executions of inference, labeling, and training
in a performant and energy-efficient manner. DaCapo comprises (1) a
spatially-partitionable and precision-flexible accelerator enabling parallel
execution of kernels on sub-accelerators at their respective precisions, and
(2) a spatiotemporal resource allocation algorithm that strategically navigates
the resource-accuracy tradeoff space, facilitating optimal decisions for
resource allocation to achieve maximal accuracy. Our evaluation shows that
DaCapo achieves 6.5% and 5.5% higher accuracy than a state-of-the-art GPU-based
continuous learning systems, Ekya and EOMU, respectively, while consuming 254x
less power.
\\ ( https://arxiv.org/abs/2403.14353 ,  770kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14359 (*cross-listing*)
Date: Thu, 21 Mar 2024 12:40:41 GMT   (1766kb,D)

Title: Varroa destructor detection on honey bees using hyperspectral imagery
Authors: Zina-Sabrina Duma and Tomas Zemcik and Simon Bilik and Tuomas Sihvonen
  and Peter Honec and Satu-Pia Reinikainen and Karel Horak
Categories: cs.CV cs.LG
\\
  Hyperspectral (HS) imagery in agriculture is becoming increasingly common.
These images have the advantage of higher spectral resolution. Advanced
spectral processing techniques are required to unlock the information potential
in these HS images. The present paper introduces a method rooted in
multivariate statistics designed to detect parasitic Varroa destructor mites on
the body of western honey bee Apis mellifera, enabling easier and continuous
monitoring of the bee hives. The methodology explores unsupervised (K-means++)
and recently developed supervised (Kernel Flows - Partial Least-Squares,
KF-PLS) methods for parasitic identification. Additionally, in light of the
emergence of custom-band multispectral cameras, the present research outlines a
strategy for identifying the specific wavelengths necessary for effective
bee-mite separation, suitable for implementation in a custom-band camera.
Illustrated with a real-case dataset, our findings demonstrate that as few as
four spectral bands are sufficient for accurate parasite identification.
\\ ( https://arxiv.org/abs/2403.14359 ,  1766kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14377 (*cross-listing*)
Date: Thu, 21 Mar 2024 13:09:23 GMT   (1009kb,D)

Title: Knowledge-Enhanced Recommendation with User-Centric Subgraph Network
Authors: Guangyi Liu, Quanming Yao, Yongqi Zhang, Lei Chen
Categories: cs.IR cs.LG
\\
  Recommendation systems, as widely implemented nowadays on various platforms,
recommend relevant items to users based on their preferences. The classical
methods which rely on user-item interaction matrices has limitations,
especially in scenarios where there is a lack of interaction data for new
items. Knowledge graph (KG)-based recommendation systems have emerged as a
promising solution. However, most KG-based methods adopt node embeddings, which
do not provide personalized recommendations for different users and cannot
generalize well to the new items. To address these limitations, we propose
Knowledge-enhanced User-Centric subgraph Network (KUCNet), a subgraph learning
approach with graph neural network (GNN) for effective recommendation. KUCNet
constructs a U-I subgraph for each user-item pair that captures both the
historical information of user-item interactions and the side information
provided in KG. An attention-based GNN is designed to encode the U-I subgraphs
for recommendation. Considering efficiency, the pruned user-centric computation
graph is further introduced such that multiple U-I subgraphs can be
simultaneously computed and that the size can be pruned by Personalized
PageRank. Our proposed method achieves accurate, efficient, and interpretable
recommendations especially for new items. Experimental results demonstrate the
superiority of KUCNet over state-of-the-art KG-based and collaborative
filtering (CF)-based methods.
\\ ( https://arxiv.org/abs/2403.14377 ,  1009kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14379 (*cross-listing*)
Date: Thu, 21 Mar 2024 13:12:33 GMT   (15170kb,D)

Title: Tensor network compressibility of convolutional models
Authors: Sukhbinder Singh, Saeed S. Jahromi, Roman Orus
Categories: cs.CV cs.LG quant-ph
Comments: 20 pages, 21 images
\\
  Convolutional neural networks (CNNs) represent one of the most widely used
neural network architectures, showcasing state-of-the-art performance in
computer vision tasks. Although larger CNNs generally exhibit higher accuracy,
their size can be effectively reduced by "tensorization" while maintaining
accuracy. Tensorization consists of replacing the convolution kernels with
compact decompositions such as Tucker, Canonical Polyadic decompositions, or
quantum-inspired decompositions such as matrix product states, and directly
training the factors in the decompositions to bias the learning towards
low-rank decompositions. But why doesn't tensorization seem to impact the
accuracy adversely? We explore this by assessing how truncating the convolution
kernels of dense (untensorized) CNNs impact their accuracy. Specifically, we
truncated the kernels of (i) a vanilla four-layer CNN and (ii) ResNet-50
pre-trained for image classification on CIFAR-10 and CIFAR-100 datasets. We
found that kernels (especially those inside deeper layers) could often be
truncated along several cuts resulting in significant loss in kernel norm but
not in classification accuracy. This suggests that such ``correlation
compression'' (underlying tensorization) is an intrinsic feature of how
information is encoded in dense CNNs. We also found that aggressively truncated
models could often recover the pre-truncation accuracy after only a few epochs
of re-training, suggesting that compressing the internal correlations of
convolution layers does not often transport the model to a worse minimum. Our
results can be applied to tensorize and compress CNN models more effectively.
\\ ( https://arxiv.org/abs/2403.14379 ,  15170kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14385 (*cross-listing*)
Date: Thu, 21 Mar 2024 13:21:33 GMT   (1422kb,D)

Title: Estimating Causal Effects with Double Machine Learning -- A Method
  Evaluation
Authors: Jonathan Fuhr (1), Philipp Berens (2), Dominik Papies (1) ((1) School
  of Business and Economics, University of T\"ubingen, (2) Hertie Institute for
  AI in Brain Health, University of T\"ubingen)
Categories: stat.ML cs.LG econ.EM stat.ME
\\
  The estimation of causal effects with observational data continues to be a
very active research area. In recent years, researchers have developed new
frameworks which use machine learning to relax classical assumptions necessary
for the estimation of causal effects. In this paper, we review one of the most
prominent methods - "double/debiased machine learning" (DML) - and empirically
evaluate it by comparing its performance on simulated data relative to more
traditional statistical methods, before applying it to real-world data. Our
findings indicate that the application of a suitably flexible machine learning
algorithm within DML improves the adjustment for various nonlinear confounding
relationships. This advantage enables a departure from traditional functional
form assumptions typically necessary in causal effect estimation. However, we
demonstrate that the method continues to critically depend on standard
assumptions about causal structure and identification. When estimating the
effects of air pollution on housing prices in our application, we find that DML
estimates are consistently larger than estimates of less flexible methods. From
our overall results, we provide actionable recommendations for specific choices
researchers must make when applying DML in practice.
\\ ( https://arxiv.org/abs/2403.14385 ,  1422kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14392 (*cross-listing*)
Date: Thu, 21 Mar 2024 13:33:00 GMT   (337kb,D)

Title: A Bag of Tricks for Few-Shot Class-Incremental Learning
Authors: Shuvendu Roy, Chunjong Park, Aldi Fahrezi, Ali Etemad
Categories: cs.CV cs.LG
\\
  We present a bag of tricks framework for few-shot class-incremental learning
(FSCIL), which is a challenging form of continual learning that involves
continuous adaptation to new tasks with limited samples. FSCIL requires both
stability and adaptability, i.e., preserving proficiency in previously learned
tasks while learning new ones. Our proposed bag of tricks brings together eight
key and highly influential techniques that improve stability, adaptability, and
overall performance under a unified framework for FSCIL. We organize these
tricks into three categories: stability tricks, adaptability tricks, and
training tricks. Stability tricks aim to mitigate the forgetting of previously
learned classes by enhancing the separation between the embeddings of learned
classes and minimizing interference when learning new ones. On the other hand,
adaptability tricks focus on the effective learning of new classes. Finally,
training tricks improve the overall performance without compromising stability
or adaptability. We perform extensive experiments on three benchmark datasets,
CIFAR-100, CUB-200, and miniIMageNet, to evaluate the impact of our proposed
framework. Our detailed analysis shows that our approach substantially improves
both stability and adaptability, establishing a new state-of-the-art by
outperforming prior works in the area. We believe our method provides a go-to
solution and establishes a robust baseline for future research in this area.
\\ ( https://arxiv.org/abs/2403.14392 ,  337kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14413 (*cross-listing*)
Date: Thu, 21 Mar 2024 13:59:19 GMT   (354kb,D)

Title: Model Uncertainty in Evolutionary Optimization and Bayesian
  Optimization: A Comparative Analysis
Authors: Hao Hao, Xiaoqun Zhang, Aimin Zhou
Categories: cs.NE cs.LG
\\
  Black-box optimization problems, which are common in many real-world
applications, require optimization through input-output interactions without
access to internal workings. This often leads to significant computational
resources being consumed for simulations. Bayesian Optimization (BO) and
Surrogate-Assisted Evolutionary Algorithm (SAEA) are two widely used
gradient-free optimization techniques employed to address such challenges. Both
approaches follow a similar iterative procedure that relies on surrogate models
to guide the search process. This paper aims to elucidate the similarities and
differences in the utilization of model uncertainty between these two methods,
as well as the impact of model inaccuracies on algorithmic performance. A novel
model-assisted strategy is introduced, which utilizes unevaluated solutions to
generate offspring, leveraging the population-based search capabilities of
evolutionary algorithm to enhance the effectiveness of model-assisted
optimization. Experimental results demonstrate that the proposed approach
outperforms mainstream Bayesian optimization algorithms in terms of accuracy
and efficiency.
\\ ( https://arxiv.org/abs/2403.14413 ,  354kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14514 (*cross-listing*)
Date: Thu, 21 Mar 2024 16:10:42 GMT   (1163kb,D)

Title: Machine-learning invariant foliations in forced systems for reduced
  order modelling
Authors: Robert Szalai
Categories: math.DS cs.LG
\\
  We identify reduced order models (ROM) of forced systems from data using
invariant foliations. The forcing can be external, parametric, periodic or
quasi-periodic. The process has four steps: 1. identify an approximate
invariant torus and the linear dynamics about the torus; 2. identify a globally
defined invariant foliation about the torus; 3. identify a local foliation
about an invariant manifold that complements the global foliation 4. extract
the invariant manifold as the leaf going through the torus and interpret the
result. We combine steps 2 and 3, so that we can track the location of the
invariant torus and scale the invariance equations appropriately. We highlight
some fundamental limitations of invariant manifolds and foliations when fitting
them to data, that require further mathematics to resolve.
\\ ( https://arxiv.org/abs/2403.14514 ,  1163kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14547 (*cross-listing*)
Date: Thu, 21 Mar 2024 16:48:45 GMT   (4511kb,D)

Title: Estimating Physical Information Consistency of Channel Data Augmentation
  for Remote Sensing Images
Authors: Tom Burgert, Beg\"um Demir
Categories: cs.CV cs.LG
Comments: Accepted at the IEEE International Geoscience and Remote Sensing
  Symposium
\\
  The application of data augmentation for deep learning (DL) methods plays an
important role in achieving state-of-the-art results in supervised,
semi-supervised, and self-supervised image classification. In particular,
channel transformations (e.g., solarize, grayscale, brightness adjustments) are
integrated into data augmentation pipelines for remote sensing (RS) image
classification tasks. However, contradicting beliefs exist about their proper
applications to RS images. A common point of critique is that the application
of channel augmentation techniques may lead to physically inconsistent spectral
data (i.e., pixel signatures). To shed light on the open debate, we propose an
approach to estimate whether a channel augmentation technique affects the
physical information of RS images. To this end, the proposed approach estimates
a score that measures the alignment of a pixel signature within a time series
that can be naturally subject to deviations caused by factors such as
acquisition conditions or phenological states of vegetation. We compare the
scores associated with original and augmented pixel signatures to evaluate the
physical consistency. Experimental results on a multi-label image
classification task show that channel augmentations yielding a score that
exceeds the expected deviation of original pixel signatures can not improve the
performance of a baseline model trained without augmentation.
\\ ( https://arxiv.org/abs/2403.14547 ,  4511kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14583 (*cross-listing*)
Date: Thu, 21 Mar 2024 17:37:43 GMT   (1900kb,D)

Title: Co-Optimization of Environment and Policies for Decentralized
  Multi-Agent Navigation
Authors: Zhan Gao and Guang Yang and Amanda Prorok
Categories: cs.RO cs.LG cs.MA
\\
  This work views the multi-agent system and its surrounding environment as a
co-evolving system, where the behavior of one affects the other. The goal is to
take both agent actions and environment configurations as decision variables,
and optimize these two components in a coordinated manner to improve some
measure of interest. Towards this end, we consider the problem of decentralized
multi-agent navigation in cluttered environments. By introducing two
sub-objectives of multi-agent navigation and environment optimization, we
propose an $\textit{agent-environment co-optimization}$ problem and develop a
$\textit{coordinated algorithm}$ that alternates between these sub-objectives
to search for an optimal synthesis of agent actions and obstacle configurations
in the environment; ultimately, improving the navigation performance. Due to
the challenge of explicitly modeling the relation between agents, environment
and performance, we leverage policy gradient to formulate a model-free learning
mechanism within the coordinated framework. A formal convergence analysis shows
that our coordinated algorithm tracks the local minimum trajectory of an
associated time-varying non-convex optimization problem. Extensive numerical
results corroborate theoretical findings and show the benefits of
co-optimization over baselines. Interestingly, the results also indicate that
optimized environment configurations are able to offer structural guidance that
is key to de-conflicting agents in motion.
\\ ( https://arxiv.org/abs/2403.14583 ,  1900kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14597 (*cross-listing*)
Date: Thu, 21 Mar 2024 17:50:22 GMT   (1072kb,D)

Title: Extended Reality for Enhanced Human-Robot Collaboration: a
  Human-in-the-Loop Approach
Authors: Yehor Karpichev, Todd Charter, Homayoun Najjaran
Categories: cs.RO cs.HC cs.LG
\\
  The rise of automation has provided an opportunity to achieve higher
efficiency in manufacturing processes, yet it often compromises the flexibility
required to promptly respond to evolving market needs and meet the demand for
customization. Human-robot collaboration attempts to tackle these challenges by
combining the strength and precision of machines with human ingenuity and
perceptual understanding. In this paper, we conceptualize and propose an
implementation framework for an autonomous, machine learning-based manipulator
that incorporates human-in-the-loop principles and leverages Extended Reality
(XR) to facilitate intuitive communication and programming between humans and
robots. Furthermore, the conceptual framework foresees human involvement
directly in the robot learning process, resulting in higher adaptability and
task generalization. The paper highlights key technologies enabling the
proposed framework, emphasizing the importance of developing the digital
ecosystem as a whole. Additionally, we review the existent implementation
approaches of XR in human-robot collaboration, showcasing diverse perspectives
and methodologies. The challenges and future outlooks are discussed, delving
into the major obstacles and potential research avenues of XR for more natural
human-robot interaction and integration in the industrial landscape.
\\ ( https://arxiv.org/abs/2403.14597 ,  1072kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14602 (*cross-listing*)
Date: Thu, 21 Mar 2024 17:52:08 GMT   (13772kb,D)

Title: ReNoise: Real Image Inversion Through Iterative Noising
Authors: Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor and
  Daniel Cohen-Or
Categories: cs.CV cs.GR cs.LG
Comments: project page at: https://garibida.github.io/ReNoise-Inversion/
\\
  Recent advancements in text-guided diffusion models have unlocked powerful
image manipulation capabilities. However, applying these methods to real images
necessitates the inversion of the images into the domain of the pretrained
diffusion model. Achieving faithful inversion remains a challenge, particularly
for more recent models trained to generate images with a small number of
denoising steps. In this work, we introduce an inversion method with a high
quality-to-operation ratio, enhancing reconstruction accuracy without
increasing the number of operations. Building on reversing the diffusion
sampling process, our method employs an iterative renoising mechanism at each
inversion sampling step. This mechanism refines the approximation of a
predicted point along the forward diffusion trajectory, by iteratively applying
the pretrained diffusion model, and averaging these predictions. We evaluate
the performance of our ReNoise technique using various sampling algorithms and
models, including recent accelerated diffusion models. Through comprehensive
evaluations and comparisons, we show its effectiveness in terms of both
accuracy and speed. Furthermore, we confirm that our method preserves
editability by demonstrating text-driven image editing on real images.
\\ ( https://arxiv.org/abs/2403.14602 ,  13772kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14613 (*cross-listing*)
Date: Thu, 21 Mar 2024 17:58:04 GMT   (5751kb,D)

Title: DreamReward: Text-to-3D Generation with Human Preference
Authors: Junliang Ye, Fangfu Liu, Qixiu Li, Zhengyi Wang, Yikai Wang, Xinzhou
  Wang, Yueqi Duan, Jun Zhu
Categories: cs.CV cs.LG
Comments: Project page: https://jamesyjl.github.io/DreamReward
\\
  3D content creation from text prompts has shown remarkable success recently.
However, current text-to-3D methods often generate 3D results that do not align
well with human preferences. In this paper, we present a comprehensive
framework, coined DreamReward, to learn and improve text-to-3D models from
human preference feedback. To begin with, we collect 25k expert comparisons
based on a systematic annotation pipeline including rating and ranking. Then,
we build Reward3D -- the first general-purpose text-to-3D human preference
reward model to effectively encode human preferences. Building upon the 3D
reward model, we finally perform theoretical analysis and present the Reward3D
Feedback Learning (DreamFL), a direct tuning algorithm to optimize the
multi-view diffusion models with a redefined scorer. Grounded by theoretical
proof and extensive experiment comparisons, our DreamReward successfully
generates high-fidelity and 3D consistent results with significant boosts in
prompt alignment with human intention. Our results demonstrate the great
potential for learning from human feedback to improve text-to-3D models.
\\ ( https://arxiv.org/abs/2403.14613 ,  5751kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2303.15662
replaced with revised version Thu, 21 Mar 2024 01:42:43 GMT   (1935kb,D)

Title: ChatGPT4PCG Competition: Character-like Level Generation for Science
  Birds
Authors: Pittawat Taveekitworachai, Febri Abdullah, Mury F. Dewantoro, Ruck
  Thawonmas, Julian Togelius, Jochen Renz
Categories: cs.AI cs.CL
Comments: This paper accepted for presentation at IEEE CoG 2023 is made
  available for participants of ChatGPT4PCG Competition
  (https://chatgpt4pcg.github.io/) and readers interested in relevant areas. In
  this PDF version, the affiliation symbol of Julian Togelius has been revised
ACM-class: I.2.7; I.2.8
\\ ( https://arxiv.org/abs/2303.15662 ,  1935kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05374
replaced with revised version Thu, 21 Mar 2024 00:21:14 GMT   (966kb,D)

Title: Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language
  Models' Alignment
Authors: Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng
  Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li
Categories: cs.AI cs.LG
Comments: Fixed several typos
\\ ( https://arxiv.org/abs/2308.05374 ,  966kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07234
replaced with revised version Thu, 21 Mar 2024 12:39:09 GMT   (1470kb,D)

Title: CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for
  Chinese Public Security Domain
Authors: Xin Tong, Bo Jin, Zhi Lin, Binjun Wang, Ting Yu and Qiang Cheng
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.07234 ,  1470kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09099
replaced with revised version Thu, 21 Mar 2024 05:33:23 GMT   (26400kb,D)

Title: Exploring Neuron Interactions and Emergence in LLMs: From the
  Multifractal Analysis Perspective
Authors: Xiongye Xiao, Chenyu Zhou, Heng Ping, Defu Cao, Yaxing Li, Yizhuo
  Zhou, Shixuan Li, Paul Bogdan
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.09099 ,  26400kb)
------------------------------------------------------------------------------
\\
arXiv:2109.01048
replaced with revised version Thu, 21 Mar 2024 09:23:38 GMT   (781kb,D)

Title: Pre-training Language Model Incorporating Domain-specific Heterogeneous
  Knowledge into A Unified Representation
Authors: Hongyin Zhu, Hao Peng, Zhiheng Lyu, Lei Hou, Juanzi Li, Jinghui Xiao
Categories: cs.CL
\\ ( https://arxiv.org/abs/2109.01048 ,  781kb)
------------------------------------------------------------------------------
\\
arXiv:2209.00568
replaced with revised version Wed, 20 Mar 2024 19:09:57 GMT   (1700kb,D)

Title: Multi-Scale Contrastive Knowledge Co-Distillation for Event Temporal
  Relation Extraction
Authors: Hao-Ren Yao, Luke Breitfeller, Aakanksha Naik, Chunxiao Zhou, Carolyn
  Rose
Categories: cs.CL cs.AI cs.LG
Comments: update
\\ ( https://arxiv.org/abs/2209.00568 ,  1700kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00618
replaced with revised version Thu, 21 Mar 2024 13:37:23 GMT   (329kb,D)

Title: Effective Structured Prompting by Meta-Learning and Representative
  Verbalizer
Authors: Weisen Jiang, Yu Zhang, James T. Kwok
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at ICML 2023
\\ ( https://arxiv.org/abs/2306.00618 ,  329kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06029
replaced with revised version Thu, 21 Mar 2024 09:46:31 GMT   (1236kb,D)

Title: Pluggable Neural Machine Translation Models via Memory-augmented
  Adapters
Authors: Yuzhuang Xu, Shuo Wang, Peng Li, Xuebo Liu, Xiaolong Wang, Weidong
  Liu, Yang Liu
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\ ( https://arxiv.org/abs/2307.06029 ,  1236kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05342
replaced with revised version Wed, 20 Mar 2024 20:37:17 GMT   (337kb,D)

Title: Metacognitive Prompting Improves Understanding in Large Language Models
Authors: Yuqing Wang, Yun Zhao
Categories: cs.CL cs.AI
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2308.05342 ,  337kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12022
replaced with revised version Thu, 21 Mar 2024 09:11:22 GMT   (305kb,D)

Title: Reranking Passages with Coarse-to-Fine Neural Retriever Enhanced by
  List-Context Information
Authors: Hongyin Zhu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2308.12022 ,  305kb)
------------------------------------------------------------------------------
\\
arXiv:2309.03378
replaced with revised version Wed, 20 Mar 2024 20:57:51 GMT   (847kb,D)

Title: RoDia: A New Dataset for Romanian Dialect Identification from Speech
Authors: Codrut Rotaru, Nicolae-Catalin Ristea, Radu Tudor Ionescu
Categories: cs.CL cs.SD eess.AS
Comments: Accepted at NAACL 2024
\\ ( https://arxiv.org/abs/2309.03378 ,  847kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09749
replaced with revised version Thu, 21 Mar 2024 01:57:38 GMT   (0kb,I)

Title: Facilitating NSFW Text Detection in Open-Domain Dialogue Systems via
  Knowledge Distillation
Authors: Huachuan Qiu, Shuai Zhang, Hongliang He, Anqi Li, Zhenzhong Lan
Categories: cs.CL
Comments: As we have submitted a final version arXiv:2403.13250, we decide to
  withdraw it
\\ ( https://arxiv.org/abs/2309.09749 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11259
replaced with revised version Thu, 21 Mar 2024 13:41:35 GMT   (66kb)

Title: Sequence-to-Sequence Spanish Pre-trained Language Models
Authors: Vladimir Araujo, Maria Mihaela Trusca, Rodrigo Tufi\~no,
  Marie-Francine Moens
Categories: cs.CL cs.AI cs.LG
Comments: Accepted paper at LREC-Coling2024
\\ ( https://arxiv.org/abs/2309.11259 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11566
replaced with revised version Thu, 21 Mar 2024 15:31:45 GMT   (530kb)

Title: SignBank+: Preparing a Multilingual Sign Language Dataset for Machine
  Translation Using Large Language Models
Authors: Amit Moryossef, Zifan Jiang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.11566 ,  530kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14974
replaced with revised version Thu, 21 Mar 2024 08:50:44 GMT   (314kb,D)

Title: Detecting Sexual Content at the Sentence Level in First Millennium Latin
  Texts
Authors: Thibault Cl\'erice (ALMAnaCH, CJM)
Categories: cs.CL cs.AI
Journal-ref: Joint International Conference on Computational Linguistics,
  Language Resources and Evaluation (LREC-COLING 2024), ELRA Language Resources
  Association (ELRA); International Committee on Computational Linguistics
  (ICCL), May 2024, Torino, Italy
\\ ( https://arxiv.org/abs/2309.14974 ,  314kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04451
replaced with revised version Wed, 20 Mar 2024 21:34:56 GMT   (445kb,D)

Title: AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language
  Models
Authors: Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao
Categories: cs.CL cs.AI
Comments: Published as a conference paper at ICLR 2024. Code is available at
  https://github.com/SheltonLiu-N/AutoDAN
\\ ( https://arxiv.org/abs/2310.04451 ,  445kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14564
replaced with revised version Thu, 21 Mar 2024 02:56:22 GMT   (514kb,D)

Title: Language Models Hallucinate, but May Excel at Fact Verification
Authors: Jian Guan, Jesse Dodge, David Wadden, Minlie Huang, Hao Peng
Categories: cs.CL
Comments: Accepted in NAACL 2024
\\ ( https://arxiv.org/abs/2310.14564 ,  514kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16343
replaced with revised version Thu, 21 Mar 2024 08:29:35 GMT   (1618kb,D)

Title: Evaluating, Understanding, and Improving Constrained Text Generation for
  Large Language Models
Authors: Xiang Chen and Xiaojun Wan
Categories: cs.CL
Comments: Work in progress
\\ ( https://arxiv.org/abs/2310.16343 ,  1618kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17918
replaced with revised version Thu, 21 Mar 2024 10:57:23 GMT   (292kb,D)

Title: Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection
  Method
Authors: Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Chong Meng,
  Shuaiqiang Wang, Zhicong Cheng, Zhaochun Ren, Dawei Yin
Categories: cs.CL cs.AI
Comments: Accepted by NAACL 2024
\\ ( https://arxiv.org/abs/2310.17918 ,  292kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08921
replaced with revised version Thu, 21 Mar 2024 00:27:37 GMT   (464kb,D)

Title: Self-Improving for Zero-Shot Named Entity Recognition with Large
  Language Models
Authors: Tingyu Xie, Qi Li, Yan Zhang, Zuozhu Liu, Hongwei Wang
Categories: cs.CL
Comments: Accepted to NAACL 2024 (Main Conference)
\\ ( https://arxiv.org/abs/2311.08921 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09206
replaced with revised version Thu, 21 Mar 2024 17:56:37 GMT   (20715kb,D)

Title: TableLlama: Towards Open Large Generalist Models for Tables
Authors: Tianshu Zhang, Xiang Yue, Yifei Li, Huan Sun
Categories: cs.CL
Comments: NAACL 2024 long paper
\\ ( https://arxiv.org/abs/2311.09206 ,  20715kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13246
replaced with revised version Thu, 21 Mar 2024 03:50:32 GMT   (2759kb,D)

Title: CoachLM: Automatic Instruction Revisions Improve the Data Quality in LLM
  Instruction Tuning
Authors: Yilun Liu, Shimin Tao, Xiaofeng Zhao, Ming Zhu, Wenbing Ma, Junhao
  Zhu, Chang Su, Yutai Hou, Miao Zhang, Min Zhang, Hongxia Ma, Li Zhang, Hao
  Yang, Yanfei Jiang
Categories: cs.CL
Comments: Accepted by ICDE 2024
\\ ( https://arxiv.org/abs/2311.13246 ,  2759kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04877
replaced with revised version Thu, 21 Mar 2024 07:22:54 GMT   (583kb,D)

Title: Generating Explanations to Understand and Repair Embedding-based Entity
  Alignment
Authors: Xiaobin Tian and Zequn Sun and Wei Hu
Categories: cs.CL cs.DB
Comments: Accepted in the 40th IEEE International Conference on Data
  Engineering (ICDE 2024)
\\ ( https://arxiv.org/abs/2312.04877 ,  583kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02622
replaced with revised version Thu, 21 Mar 2024 10:57:40 GMT   (4007kb,D)

Title: DenseFormer: Enhancing Information Flow in Transformers via Depth
  Weighted Averaging
Authors: Matteo Pagliardini, Amirkeivan Mohtashami, Francois Fleuret, Martin
  Jaggi
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.02622 ,  4007kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03049
replaced with revised version Thu, 21 Mar 2024 15:33:34 GMT   (2687kb,D)

Title: EasyInstruct: An Easy-to-use Instruction Processing Framework for Large
  Language Models
Authors: Yixin Ou, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Yida Xue,
  Runnan Fang, Kangwei Liu, Lei Li, Zhen Bi, Guozhou Zheng, Huajun Chen
Categories: cs.CL cs.AI cs.HC cs.IR cs.LG
Comments: Project website: https://zjunlp.github.io/project/EasyInstruct Code:
  https://github.com/zjunlp/EasyInstruct Video: https://youtu.be/rfQOWYfziFo
  Demo: https://huggingface.co/spaces/zjunlp/EasyInstruct
\\ ( https://arxiv.org/abs/2402.03049 ,  2687kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03848
replaced with revised version Thu, 21 Mar 2024 05:58:10 GMT   (185kb,D)

Title: ANLS* -- A Universal Document Processing Metric for Generative Large
  Language Models
Authors: David Peer, Philemon Sch\"opf, Volckmar Nebendahl, Alexander Rietzler,
  Sebastian Stabinger
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.03848 ,  185kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08403
replaced with revised version Thu, 21 Mar 2024 09:02:26 GMT   (16kb,D)

Title: LLMs and the Human Condition
Authors: Peter Wallis
Categories: cs.CL
Comments: A 2nd draft with a better abstract and introduction. target is IVA in
  2024
\\ ( https://arxiv.org/abs/2402.08403 ,  16kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10735
replaced with revised version Wed, 20 Mar 2024 19:14:54 GMT   (177kb,D)

Title: Assessing the Reasoning Abilities of ChatGPT in the Context of Claim
  Verification
Authors: John Dougrez-Lewis, Mahmud Elahi Akhter, Yulan He, Maria Liakata
Categories: cs.CL
Comments: 19 pages, 1 figure
\\ ( https://arxiv.org/abs/2402.10735 ,  177kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00862
replaced with revised version Thu, 21 Mar 2024 10:14:09 GMT   (165kb,D)

Title: NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and
  Safety Adherence in Chinese Journalistic Editorial Applications
Authors: Miao Li and Ming-Bin Chen and Bo Tang and Shengbin Hou and Pengyu Wang
  and Haiying Deng and Zhiyu Li and Feiyu Xiong and Keming Mao and Peng Cheng
  and Yi Luo
Categories: cs.CL cs.AI
Comments: 27 pages
\\ ( https://arxiv.org/abs/2403.00862 ,  165kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04521
replaced with revised version Thu, 21 Mar 2024 04:28:45 GMT   (5308kb,D)

Title: Uncertainty-Aware Relational Graph Neural Network for Few-Shot Knowledge
  Graph Completion
Authors: Qian Li, Shu Guo, Yinjia Chen, Cheng Ji, Jiawei Sheng, and Jianxin Li
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.04521 ,  5308kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05020
replaced with revised version Wed, 20 Mar 2024 20:44:17 GMT   (1312kb,D)

Title: Is this the real life? Is this just fantasy? The Misleading Success of
  Simulating Social Interactions With LLMs
Authors: Xuhui Zhou, Zhe Su, Tiwalayo Eisape, Hyunwoo Kim, Maarten Sap
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.05020 ,  1312kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08002
replaced with revised version Wed, 20 Mar 2024 23:31:22 GMT   (44423kb,D)

Title: Training Small Multimodal Models to Bridge Biomedical Competency Gap: A
  Case Study in Radiology Imaging
Authors: Juan Manuel Zambrano Chaves, Shih-Cheng Huang, Yanbo Xu, Hanwen Xu,
  Naoto Usuyama, Sheng Zhang, Fei Wang, Yujia Xie, Mahmoud Khademi, Ziyi Yang,
  Hany Awadalla, Julia Gong, Houdong Hu, Jianwei Yang, Chunyuan Li, Jianfeng
  Gao, Yu Gu, Cliff Wong, Mu Wei, Tristan Naumann, Muhao Chen, Matthew P.
  Lungren, Serena Yeung-Levy, Curtis P. Langlotz, Sheng Wang, Hoifung Poon
Categories: cs.CL cs.CV
\\ ( https://arxiv.org/abs/2403.08002 ,  44423kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09559
replaced with revised version Thu, 21 Mar 2024 06:51:16 GMT   (1210kb,D)

Title: Less is More: Data Value Estimation for Visual Instruction Tuning
Authors: Zikang Liu, Kun Zhou, Wayne Xin Zhao, Dawei Gao, Yaliang Li, Ji-Rong
  Wen
Categories: cs.CL cs.CV
\\ ( https://arxiv.org/abs/2403.09559 ,  1210kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10882
replaced with revised version Thu, 21 Mar 2024 14:50:18 GMT   (3755kb,D)

Title: Optimizing Language Augmentation for Multilingual Large Language Models:
  A Case Study on Korean
Authors: ChangSu Choi, Yongbin Jeong, Seoyoon Park, InHo Won, HyeonSeok Lim,
  SangMin Kim, Yejee Kang, Chanhyuk Yoon, Jaewan Park, Yiseul Lee, HyeJin Lee,
  Younggyun Hahm, Hansaem Kim and KyungTae Lim
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.10882 ,  3755kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11145
replaced with revised version Thu, 21 Mar 2024 06:22:56 GMT   (1090kb,D)

Title: A Challenge Dataset and Effective Models for Conversational Stance
  Detection
Authors: Fuqiang Niu, Min Yang, Ang Li, Baoquan Zhang, Xiaojiang Peng and Bowen
  Zhang
Categories: cs.CL
Journal-ref: LREC-COLING 2024
\\ ( https://arxiv.org/abs/2403.11145 ,  1090kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12373
replaced with revised version Thu, 21 Mar 2024 06:01:48 GMT   (132kb,D)

Title: RankPrompt: Step-by-Step Comparisons Make Language Models Better
  Reasoners
Authors: Chi Hu, Yuan Ge, Xiangnan Ma, Hang Cao, Qiang Li, Yonghua Yang, Tong
  Xiao, Jingbo Zhu
Categories: cs.CL
Comments: LREC-Coling 2024 Long Paper
\\ ( https://arxiv.org/abs/2403.12373 ,  132kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13257
replaced with revised version Thu, 21 Mar 2024 03:13:30 GMT   (2086kb,D)

Title: Arcee's MergeKit: A Toolkit for Merging Large Language Models
Authors: Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers,
  Vlad Karpukhin, Brian Benedict, Mark McQuade, Jacob Solawetz
Categories: cs.CL cs.AI cs.LG
Comments: 11 pages, 4 figures
\\ ( https://arxiv.org/abs/2403.13257 ,  2086kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13372
replaced with revised version Thu, 21 Mar 2024 08:36:39 GMT   (51kb,D)

Title: LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models
Authors: Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo,
  Yongqiang Ma
Categories: cs.CL cs.AI
Comments: 12 pages, preprint
\\ ( https://arxiv.org/abs/2403.13372 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13638
replaced with revised version Thu, 21 Mar 2024 04:03:59 GMT   (8205kb,D)

Title: Do Not Worry if You Do Not Have Data: Building Pretrained Language
  Models Using Translationese
Authors: Meet Doshi, Raj Dabre, Pushpak Bhattacharyya
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.13638 ,  8205kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13679
replaced with revised version Thu, 21 Mar 2024 04:06:06 GMT   (8386kb,D)

Title: RoleInteract: Evaluating the Social Interaction of Role-Playing Agents
Authors: Hongzhan Chen, Hehong Chen, Ming Yan, Wenshen Xu, Xing Gao, Weizhou
  Shen, Xiaojun Quan, Chenliang Li, Ji Zhang, Fei Huang, Jingren Zhou
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.13679 ,  8386kb)
------------------------------------------------------------------------------
\\
arXiv:2202.08982
replaced with revised version Thu, 21 Mar 2024 05:55:29 GMT   (1825kb)

Title: PGCN: Progressive Graph Convolutional Networks for Spatial-Temporal
  Traffic Forecasting
Authors: Yuyol Shin and Yoonjin Yoon
Categories: cs.LG
Comments: 12 pages, 6 figures
Journal-ref: IEEE Transactions on Intelligent Transportation Systems (2024)
  1-12
DOI: 10.1109/TITS.2024.3349565.
\\ ( https://arxiv.org/abs/2202.08982 ,  1825kb)
------------------------------------------------------------------------------
\\
arXiv:2205.14116
replaced with revised version Thu, 21 Mar 2024 16:14:01 GMT   (1694kb,D)

Title: Don't Explain Noise: Robust Counterfactuals for Randomized Ensembles
Authors: Alexandre Forel, Axel Parmentier, Thibaut Vidal
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2205.14116 ,  1694kb)
------------------------------------------------------------------------------
\\
arXiv:2207.05827
replaced with revised version Thu, 21 Mar 2024 08:53:03 GMT   (526kb,D)

Title: Differentially Private Linear Bandits with Partial Distributed Feedback
Authors: Fengjiao Li, Xingyu Zhou, and Bo Ji
Categories: cs.LG cs.CR cs.NA math.NA
Comments: 69 pages, this version is an extension from the preliminary one
  presented at IEEE/IFIP WiOpt 2022 and was accepted to IEEE Transactions on
  Network Science and Engineering (TNSE)
\\ ( https://arxiv.org/abs/2207.05827 ,  526kb)
------------------------------------------------------------------------------
\\
arXiv:2209.07603
replaced with revised version Wed, 20 Mar 2024 22:44:15 GMT   (558kb,D)

Title: Hub-aware Random Walk Graph Embedding Methods for Classification
Authors: Aleksandar Tom\v{c}i\'c and Milo\v{s} Savi\'c and Milo\v{s}
  Radovanovi\'c
Categories: cs.LG cs.SI
Comments: Submitted to journal for possible publication
\\ ( https://arxiv.org/abs/2209.07603 ,  558kb)
------------------------------------------------------------------------------
\\
arXiv:2211.16237
replaced with revised version Wed, 20 Mar 2024 18:04:19 GMT   (1558kb,D)

Title: Closing the gap between SVRG and TD-SVRG with Gradient Splitting
Authors: Arsenii Mustafin, Alex Olshevsky, Ioannis Ch. Paschalidis
Categories: cs.LG
Comments: 37 pages, 8 figures, 6 tables
\\ ( https://arxiv.org/abs/2211.16237 ,  1558kb)
------------------------------------------------------------------------------
\\
arXiv:2301.11624
replaced with revised version Thu, 21 Mar 2024 12:34:14 GMT   (11835kb,D)

Title: Neural Wasserstein Gradient Flows for Maximum Mean Discrepancies with
  Riesz Kernels
Authors: Fabian Altekr\"uger, Johannes Hertrich, Gabriele Steidl
Categories: cs.LG math.OC math.PR
Comments: Accepted at ICML 2023
Journal-ref: Proceedings of the 40th International Conference on Machine
  Learning, PMLR 202:664-690, 2023
\\ ( https://arxiv.org/abs/2301.11624 ,  11835kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13395
replaced with revised version Thu, 21 Mar 2024 13:16:27 GMT   (1689kb,D)

Title: Learning to Solve Integer Linear Programs with Davis-Yin Splitting
Authors: Daniel McKenzie, Samy Wu Fung, Howard Heaton
Categories: cs.LG
\\ ( https://arxiv.org/abs/2301.13395 ,  1689kb)
------------------------------------------------------------------------------
\\
arXiv:2302.01385
replaced with revised version Thu, 21 Mar 2024 04:16:58 GMT   (1390kb,D)

Title: Hyper-parameter Tuning for Fair Classification without Sensitive
  Attribute Access
Authors: Akshaj Kumar Veldanda, Ivan Brugere, Sanghamitra Dutta, Alan Mishler,
  Siddharth Garg
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2302.01385 ,  1390kb)
------------------------------------------------------------------------------
\\
arXiv:2302.13425
replaced with revised version Thu, 21 Mar 2024 03:08:53 GMT   (9206kb,D)

Title: A Survey on Uncertainty Quantification for Deep Learning: An Uncertainty
  Source Perspective
Authors: Wenchong He and Zhe Jiang
Categories: cs.LG stat.ML
Comments: 39 pages, 14 figures
\\ ( https://arxiv.org/abs/2302.13425 ,  9206kb)
------------------------------------------------------------------------------
\\
arXiv:2305.08886
replaced with revised version Wed, 20 Mar 2024 18:29:53 GMT   (961kb)

Title: Building Energy Efficiency through Advanced Regression Models and
  Metaheuristic Techniques for Sustainable Management
Authors: Hamed Khosravi, Hadi Sahebi, Rahim khanizad, Imtiaz Ahmed
Categories: cs.LG
\\ ( https://arxiv.org/abs/2305.08886 ,  961kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15141
replaced with revised version Thu, 21 Mar 2024 10:15:19 GMT   (77kb,D)

Title: From Tempered to Benign Overfitting in ReLU Neural Networks
Authors: Guy Kornowski, Gilad Yehudai, Ohad Shamir
Categories: cs.LG cs.NE stat.ML
Comments: NeurIPS 2023; fixed bug
\\ ( https://arxiv.org/abs/2305.15141 ,  77kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01181
replaced with revised version Thu, 21 Mar 2024 15:57:29 GMT   (1689kb,D)

Title: TMI! Finetuned Models Leak Private Information from their Pretraining
  Data
Authors: John Abascal, Stanley Wu, Alina Oprea, Jonathan Ullman
Categories: cs.LG cs.CR
\\ ( https://arxiv.org/abs/2306.01181 ,  1689kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02090
replaced with revised version Thu, 21 Mar 2024 09:58:15 GMT   (1092kb,D)

Title: Deep Classifier Mimicry without Data Access
Authors: Steven Braun, Martin Mundt, Kristian Kersting
Categories: cs.LG cs.AI
Comments: 11 pages main, 4 figures, 2 tables, 4 pages appendix
\\ ( https://arxiv.org/abs/2306.02090 ,  1092kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02094
replaced with revised version Thu, 21 Mar 2024 09:03:48 GMT   (360kb)

Title: TensorBank: Tensor Lakehouse for Foundation Model Training
Authors: Romeo Kienzler, Leonardo Pondian Tizzei, Benedikt Blumenstiel, Zoltan
  Arnold Nagy, S. Karthik Mukkavilli, Johannes Schmude, Marcus Freitag, Michael
  Behrendt, Daniel Salles Civitarese, Naomi Simumba, Daiki Kimura, Hendrik
  Hamann
Categories: cs.LG cs.AI cs.DB cs.IR
\\ ( https://arxiv.org/abs/2309.02094 ,  360kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02632
replaced with revised version Wed, 20 Mar 2024 18:15:09 GMT   (8841kb,D)

Title: Deep Reinforcement Learning with Hierarchical Reward Modeling
Authors: Alexander Bukharin, Yixiao Li, Pengcheng He, Weizhu Chen, Tuo Zhao
Categories: cs.LG cs.AI
Comments: 29 Pages, 15 figures
\\ ( https://arxiv.org/abs/2309.02632 ,  8841kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15284
replaced with revised version Thu, 21 Mar 2024 04:36:22 GMT   (1638kb)

Title: A Physics Enhanced Residual Learning (PERL) Framework for Vehicle
  Trajectory Prediction
Authors: Keke Long, Zihao Sheng, Haotian Shi, Xiaopeng Li, Sikai Chen, Sue Ahn
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.15284 ,  1638kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01232
replaced with revised version Wed, 20 Mar 2024 21:48:05 GMT   (209kb,D)

Title: Modality-aware Transformer for Financial Time series Forecasting
Authors: Hajar Emami, Xuan-Hong Dang, Yousaf Shah, Petros Zerfos
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.01232 ,  209kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03049
replaced with revised version Thu, 21 Mar 2024 16:21:45 GMT   (13836kb,D)

Title: QuATON: Quantization Aware Training of Optical Neurons
Authors: Hasindu Kariyawasam, Ramith Hettiarachchi, Quansan Yang, Alex Matlock,
  Takahiro Nambara, Hiroyuki Kusaka, Yuichiro Kunai, Peter T C So, Edward S
  Boyden, and Dushan Wadduwage
Categories: cs.LG eess.IV physics.optics
\\ ( https://arxiv.org/abs/2310.03049 ,  13836kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07756
replaced with revised version Wed, 20 Mar 2024 18:00:04 GMT   (4170kb,D)

Title: Self-supervised Representation Learning From Random Data Projectors
Authors: Yi Sui, Tongzi Wu, Jesse C. Cresswell, Ga Wu, George Stein, Xiao Shi
  Huang, Xiaochen Zhang, Maksims Volkovs
Categories: cs.LG
Comments: Published as a conference paper of ICLR 2024.
  https://openreview.net/pdf?id=EpYnZpDpsQ
\\ ( https://arxiv.org/abs/2310.07756 ,  4170kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11287
replaced with revised version Thu, 21 Mar 2024 16:11:17 GMT   (45kb,D)

Title: Assessing the Causal Impact of Humanitarian Aid on Food Security
Authors: Jordi Cerd\`a-Bautista, Jos\'e Mar\'ia T\'arraga, Vasileios
  Sitokonstantinou and Gustau Camps-Valls
Categories: cs.LG
Comments: Accepted for publication and presentation at the International
  Geoscience and Remote Sensing Symposium (IGARSS) 2024
\\ ( https://arxiv.org/abs/2310.11287 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12032
replaced with revised version Thu, 21 Mar 2024 14:36:26 GMT   (816kb,D)

Title: Exact and general decoupled solutions of the LMC Multitask Gaussian
  Process model
Authors: Olivier Truffinet (CEA Saclay), Karim Ammar (CEA Saclay),
  Jean-Philippe Argaud (EDF R&D), Bertrand Bouriquet (EDF)
Categories: cs.LG stat.ML
Comments: 29 pages, 10 figures, submitted to UAI
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2310.12032 ,  816kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14525
replaced with revised version Thu, 21 Mar 2024 12:32:53 GMT   (1160kb,D)

Title: Graph Ranking Contrastive Learning: A Extremely Simple yet Efficient
  Method
Authors: Yulan Hu, Sheng Ouyang, Jingyu Liu, Ge Chen, Zhirui Yang, Junchen Wan,
  Fuzheng Zhang, Zhongyuan Wang, Yong Liu
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.14525 ,  1160kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16828
replaced with revised version Thu, 21 Mar 2024 17:56:19 GMT   (6855kb,D)

Title: TD-MPC2: Scalable, Robust World Models for Continuous Control
Authors: Nicklas Hansen, Hao Su, Xiaolong Wang
Categories: cs.LG cs.AI cs.CV cs.RO
Comments: ICLR 2024. Explore videos, models, data, code, and more at
  https://tdmpc2.com
\\ ( https://arxiv.org/abs/2310.16828 ,  6855kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20082
replaced with revised version Wed, 20 Mar 2024 18:22:16 GMT   (219kb,D)

Title: Efficient Subgraph GNNs by Learning Effective Selection Policies
Authors: Beatrice Bevilacqua, Moshe Eliasof, Eli Meirom, Bruno Ribeiro, Haggai
  Maron
Categories: cs.LG
Comments: ICLR 2024 Camera Ready; 27 pages, 3 figures
\\ ( https://arxiv.org/abs/2310.20082 ,  219kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08977
replaced with revised version Thu, 21 Mar 2024 04:04:25 GMT   (617kb,D)

Title: Weighted Ensemble Models Are Strong Continual Learners
Authors: Imad Eddine Marouf, Subhankar Roy, Enzo Tartaglione, St\'ephane
  Lathuili\`ere
Categories: cs.LG cs.AI cs.CV
Comments: Code: https://github.com/IemProg/CoFiMA
\\ ( https://arxiv.org/abs/2312.08977 ,  617kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09234
replaced with revised version Thu, 21 Mar 2024 16:26:09 GMT   (14462kb,D)

Title: Let's do the time-warp-attend: Learning topological invariants of
  dynamical systems
Authors: Noa Moriel, Matthew Ricci, Mor Nitzan
Categories: cs.LG math.DS stat.ML
\\ ( https://arxiv.org/abs/2312.09234 ,  14462kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10569
replaced with revised version Wed, 20 Mar 2024 21:06:43 GMT   (3166kb,D)

Title: Interpretable Causal Inference for Analyzing Wearable, Sensor, and
  Distributional Data
Authors: Srikar Katta, Harsh Parikh, Cynthia Rudin, Alexander Volfovsky
Categories: cs.LG eess.SP stat.ME
\\ ( https://arxiv.org/abs/2312.10569 ,  3166kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13927
replaced with revised version Thu, 21 Mar 2024 12:37:21 GMT   (577kb,D)

Title: On the convergence of loss and uncertainty-based active learning
  algorithms
Authors: Daniel Haimovich, Dima Karamshuk, Fridolin Linder, Niek Tax, Milan
  Vojnovic
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2312.13927 ,  577kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17244
replaced with revised version Wed, 20 Mar 2024 20:21:58 GMT   (1350kb,D)

Title: The LLM Surgeon
Authors: Tycho F.A. van der Ouderaa, Markus Nagel, Mart van Baalen, Yuki M.
  Asano, Tijmen Blankevoort
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2312.17244 ,  1350kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00081
replaced with revised version Wed, 20 Mar 2024 20:21:35 GMT   (4600kb,D)

Title: Synthetic Data Applications in Finance
Authors: Vamsi K. Potluru, Daniel Borrajo, Andrea Coletta, Niccol\`o Dalmasso,
  Yousef El-Laham, Elizabeth Fons, Mohsen Ghassemi, Sriram Gopalakrishnan,
  Vikesh Gosai, Eleonora Krea\v{c}i\'c, Ganapathy Mani, Saheed Obitayo, Deepak
  Paramanand, Natraj Raman, Mikhail Solonin, Srijan Sood, Svitlana Vyetrenko,
  Haibei Zhu, Manuela Veloso, Tucker Balch
Categories: cs.LG q-fin.GN
Comments: 50 pages, journal submission; updated 6 privacy levels
\\ ( https://arxiv.org/abs/2401.00081 ,  4600kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11609
replaced with revised version Wed, 20 Mar 2024 19:12:28 GMT   (21086kb,D)

Title: Graph Edits for Counterfactual Explanations: A comparative study
Authors: Angeliki Dimitriou, Nikolaos Chaidos, Maria Lymperaiou, Giorgos Stamou
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.11609 ,  21086kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12648
replaced with revised version Thu, 21 Mar 2024 13:23:44 GMT   (0kb,I)

Title: Consistency Enhancement-Based Deep Multiview Clustering via Contrastive
  Learning
Authors: Hao Yang, Hua Mao, Wai Lok Woo, Jie Chen and Xi Peng
Categories: cs.LG cs.CV
Comments: There are multiple errors that need to be corrected, including some
  formulas and concept descriptions. We will re upload the paper after the
  modifications are completed
\\ ( https://arxiv.org/abs/2401.12648 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00823
replaced with revised version Thu, 21 Mar 2024 10:21:37 GMT   (1698kb,D)

Title: SLIM: Skill Learning with Multiple Critics
Authors: David Emukpere, Bingbing Wu, Julien Perez, Jean-Michel Renders
Categories: cs.LG cs.AI cs.RO
Comments: Accepted at IEEE ICRA 2024
\\ ( https://arxiv.org/abs/2402.00823 ,  1698kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06137
replaced with revised version Thu, 21 Mar 2024 14:03:39 GMT   (4099kb,D)

Title: On the Privacy of Selection Mechanisms with Gaussian Noise
Authors: Jonathan Lebensold, Doina Precup and Borja Balle
Categories: cs.LG cs.CR
Comments: AISTATS 2024
\\ ( https://arxiv.org/abs/2402.06137 ,  4099kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06563
replaced with revised version Thu, 21 Mar 2024 17:08:43 GMT   (67kb,D)

Title: Unraveling the Mystery of Scaling Laws: Part I
Authors: Hui Su, Zhi Tian, Xiaoyu Shen, Xunliang Cai
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2403.06563 ,  67kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06860
replaced with revised version Thu, 21 Mar 2024 17:06:49 GMT   (7006kb,D)

Title: A Geospatial Approach to Predicting Desert Locust Breeding Grounds in
  Africa
Authors: Ibrahim Salihu Yusuf, Mukhtar Opeyemi Yusuf, Kobby Panford-Quainoo,
  Arnu Pretorius
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2403.06860 ,  7006kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06906
replaced with revised version Thu, 21 Mar 2024 12:30:16 GMT   (449kb,D)

Title: Cost-Sensitive Learning to Defer to Multiple Experts with Workload
  Constraints
Authors: Jean V. Alves, Diogo Leit\~ao, S\'ergio Jesus, Marco O. P. Sampaio,
  Javier Li\'ebana, Pedro Saleiro, M\'ario A. T. Figueiredo, Pedro Bizarro
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.06906 ,  449kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12821
replaced with revised version Thu, 21 Mar 2024 10:02:39 GMT   (952kb,D)

Title: FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware
  Graph Transformer
Authors: Dongyeong Hwang, Hyunju Kim, Sunwoo Kim, Kijung Shin
Categories: cs.LG cs.AI
Comments: CVPR 2024 Camera-Ready
\\ ( https://arxiv.org/abs/2403.12821 ,  952kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13213
replaced with revised version Thu, 21 Mar 2024 02:27:57 GMT   (1069kb,D)

Title: From Representational Harms to Quality-of-Service Harms: A Case Study on
  Llama 2 Safety Safeguards
Authors: Khaoula Chehbouni, Megha Roshan, Emmanuel Ma, Futian Andrew Wei, Afaf
  Taik, Jackie CK Cheung, Golnoosh Farnadi
Categories: cs.LG cs.CL cs.CY
Comments: 9 pages, 4 figures
\\ ( https://arxiv.org/abs/2403.13213 ,  1069kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13502
replaced with revised version Thu, 21 Mar 2024 13:18:47 GMT   (1796kb,D)

Title: Adversarial Attacks and Defenses in Automated Control Systems: A
  Comprehensive Benchmark
Authors: Vitaliy Pozdnyakov, Aleksandr Kovalenko, Ilya Makarov, Mikhail
  Drobyshevskiy, Kirill Lukyanov
Categories: cs.LG cs.CR cs.SY eess.SY
ACM-class: I.2.6; I.2.1
\\ ( https://arxiv.org/abs/2403.13502 ,  1796kb)
------------------------------------------------------------------------------
\\
arXiv:2211.13854
replaced with revised version Thu, 21 Mar 2024 00:53:19 GMT   (8556kb,D)

Title: ComCLIP: Training-Free Compositional Image and Text Matching
Authors: Kenan Jiang, Xuehai He, Ruize Xu, Xin Eric Wang
Categories: cs.CV cs.AI cs.CL
\\ ( https://arxiv.org/abs/2211.13854 ,  8556kb)
------------------------------------------------------------------------------
\\
arXiv:2302.03788
replaced with revised version Thu, 21 Mar 2024 13:30:22 GMT   (7231kb,D)

Title: Toward a Theory of Causation for Interpreting Neural Code Models
Authors: David N. Palacio and Alejandro Velasco and Nathan Cooper and Alvaro
  Rodriguez and Kevin Moran and Denys Poshyvanyk
Categories: cs.SE cs.AI cs.LG stat.ME
\\ ( https://arxiv.org/abs/2302.03788 ,  7231kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09549 (*cross-listing*)
replaced with revised version Thu, 21 Mar 2024 07:16:03 GMT   (671kb,D)

Title: QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules
Authors: Haiyang Yu, Meng Liu, Youzhi Luo, Alex Strasser, Xiaofeng Qian,
  Xiaoning Qian, Shuiwang Ji
Categories: physics.chem-ph cs.AI cs.LG
Comments: Accepted by NeurIPS 2023, Track on Datasets and Benchmarks
\\ ( https://arxiv.org/abs/2306.09549 ,  671kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14296
replaced with revised version Wed, 20 Mar 2024 18:13:10 GMT   (1598kb,D)

Title: RecMind: Large Language Model Powered Agent For Recommendation
Authors: Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah
  Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, Yingzhen Yang
Categories: cs.IR cs.AI
Comments: Accepted by NAACL 2024 (Findings)
\\ ( https://arxiv.org/abs/2308.14296 ,  1598kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00903
replaced with revised version Thu, 21 Mar 2024 15:12:36 GMT   (6565kb,D)

Title: An explainable three dimension framework to uncover learning patterns: A
  unified look in variable sulci recognition
Authors: Michail Mamalakis, Heloise de Vareilles, Atheer AI-Manea, Samantha C.
  Mitchell, Ingrid Arartz, Lynn Egeland Morch-Johnsen, Jane Garrison, Jon
  Simons, Pietro Lio, John Suckling, Graham Murray
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2309.00903 ,  6565kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06255
replaced with revised version Thu, 21 Mar 2024 03:21:24 GMT   (1581kb,D)

Title: Enhancing Multimodal Cooperation via Fine-grained Modality Valuation
Authors: Yake Wei, Ruoxuan Feng, Zihe Wang, Di Hu
Categories: cs.CV cs.AI cs.LG cs.MM
Comments: Accepted by CVPR 2024
\\ ( https://arxiv.org/abs/2309.06255 ,  1581kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02712
replaced with revised version Thu, 21 Mar 2024 07:20:35 GMT   (32430kb,D)

Title: ED-NeRF: Efficient Text-Guided Editing of 3D Scene with Latent Space
  NeRF
Authors: Jangho Park, Gihyun Kwon, Jong Chul Ye
Categories: cs.CV cs.AI cs.LG stat.ML
Comments: ICLR 2024; Project Page: https://jhq1234.github.io/ed-nerf.github.io/
\\ ( https://arxiv.org/abs/2310.02712 ,  32430kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01623
replaced with revised version Thu, 21 Mar 2024 00:05:23 GMT   (10619kb,D)

Title: VQPy: An Object-Oriented Approach to Modern Video Analytics
Authors: Shan Yu, Zhenting Zhu, Yu Chen, Hanchen Xu, Pengzhan Zhao, Yang Wang,
  Arthi Padmanabhan, Hugo Latapie, Harry Xu
Categories: cs.CV cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2311.01623 ,  10619kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01753
replaced with revised version Thu, 21 Mar 2024 12:36:22 GMT   (29722kb,D)

Title: RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value
  Factorization
Authors: Siqi Shen, Chennan Ma, Chao Li, Weiquan Liu, Yongquan Fu, Songzhu Mei,
  Xinwang Liu, Cheng Wang
Categories: cs.MA cs.AI cs.LG
Comments: Accepted at NeurIPS 2023
\\ ( https://arxiv.org/abs/2311.01753 ,  29722kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10678
replaced with revised version Thu, 21 Mar 2024 05:47:22 GMT   (1912kb,D)

Title: Distilling and Retrieving Generalizable Knowledge for Robot Manipulation
  via Language Corrections
Authors: Lihan Zha, Yuchen Cui, Li-Heng Lin, Minae Kwon, Montserrat Gonzalez
  Arenas, Andy Zeng, Fei Xia, Dorsa Sadigh
Categories: cs.RO cs.AI cs.LG
Comments: 8 pages, 4 figures, videos and code links on website
  https://sites.google.com/stanford.edu/droc
\\ ( https://arxiv.org/abs/2311.10678 ,  1912kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14758
replaced with revised version Thu, 21 Mar 2024 12:43:32 GMT   (3734kb,D)

Title: Point2RBox: Combine Knowledge from Synthetic Visual Patterns for
  End-to-end Oriented Object Detection with Single Point Supervision
Authors: Yi Yu, Xue Yang, Qingyun Li, Feipeng Da, Jifeng Dai, Yu Qiao, Junchi
  Yan
Categories: cs.CV cs.AI
Comments: 10 pages, 3 figures, 5 tables, code:
  https://github.com/yuyi1005/point2rbox-mmrotate
\\ ( https://arxiv.org/abs/2311.14758 ,  3734kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15619
replaced with revised version Wed, 20 Mar 2024 18:27:25 GMT   (2848kb,D)

Title: Align before Adapt: Leveraging Entity-to-Region Alignments for
  Generalizable Video Action Recognition
Authors: Yifei Chen, Dapeng Chen, Ruijin Liu, Sai Zhou, Wenyuan Xue, Wei Peng
Categories: cs.CV cs.AI
Comments: Accepted at CVPR 2024
\\ ( https://arxiv.org/abs/2311.15619 ,  2848kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15876
replaced with revised version Thu, 21 Mar 2024 07:38:51 GMT   (11454kb,D)

Title: LMM-Assisted Breast Cancer Treatment Target Segmentation with
  Consistency Embedding
Authors: Kwanyoung Kim, Yujin Oh, Sangjoon Park, Hwa Kyung Byun, Jin Sung Kim,
  Yong Bae Kim, Jong Chul Ye
Categories: cs.CV cs.AI cs.LG
Comments: 30 pages, 16 table, 5 figures
\\ ( https://arxiv.org/abs/2311.15876 ,  11454kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02003
replaced with revised version Wed, 20 Mar 2024 19:00:24 GMT   (2228kb,D)

Title: A Survey on Large Language Model (LLM) Security and Privacy: The Good,
  the Bad, and the Ugly
Authors: Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun and Yue
  Zhang
Categories: cs.CR cs.AI
DOI: 10.1016/j.hcc.2024.100211
\\ ( https://arxiv.org/abs/2312.02003 ,  2228kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02352
replaced with revised version Wed, 20 Mar 2024 19:57:24 GMT   (49119kb,D)

Title: Working Backwards: Learning to Place by Picking
Authors: Oliver Limoyo, Abhisek Konar, Trevor Ablett, Jonathan Kelly, Francois
  R. Hogan, Gregory Dudek
Categories: cs.RO cs.AI cs.LG
Comments: Submitted to the IEEE/RSJ International Conference on Intelligent
  Robotics and Systems (IROS'24), Abu Dhabi, UAE, Oct 14-18, 2024
\\ ( https://arxiv.org/abs/2312.02352 ,  49119kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07214
replaced with revised version Thu, 21 Mar 2024 11:12:31 GMT   (17820kb,D)

Title: Exploring Large Language Models to Facilitate Variable Autonomy for
  Human-Robot Teaming
Authors: Younes Lakhnati, Max Pascher, Jens Gerken
Categories: cs.HC cs.AI cs.RO
Comments: Frontiers in Robotics and AI, Variable Autonomy for Human-Robot
  Teaming
Journal-ref: Front. Robot. AI 11:1347538 2024
DOI: 10.3389/frobt.2024.1347538
\\ ( https://arxiv.org/abs/2312.07214 ,  17820kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12274
replaced with revised version Thu, 21 Mar 2024 12:51:31 GMT   (7065kb,D)

Title: Intrinsic Image Diffusion for Indoor Single-view Material Estimation
Authors: Peter Kocsis (1), Vincent Sitzmann (2), Matthias Nie{\ss}ner (1) ((1)
  Technical University of Munich, (2) MIT EECS)
Categories: cs.CV cs.AI cs.GR
Comments: Project page: https://peter-kocsis.github.io/IntrinsicImageDiffusion/
  Video: https://youtu.be/lz0meJlj5cA
ACM-class: I.4.8; I.2.10
\\ ( https://arxiv.org/abs/2312.12274 ,  7065kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05584
replaced with revised version Thu, 21 Mar 2024 00:42:39 GMT   (1891kb)

Title: FourCastNeXt: Optimizing FourCastNet Training for Limited Compute
Authors: Edison Guo, Maruf Ahmed, Yue Sun, Rui Yang, Harrison Cook, Tennessee
  Leeuwenburg, Ben Evans
Categories: cs.CV cs.AI
Comments: Major revision. All prior content (text, figures, table) has been
  updated. Additionally, new text, tables and figures have been added. Updated
  title. Updated author list
\\ ( https://arxiv.org/abs/2401.05584 ,  1891kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11061
replaced with revised version Wed, 20 Mar 2024 19:44:07 GMT   (23863kb,D)

Title: PhotoBot: Reference-Guided Interactive Photography via Natural Language
Authors: Oliver Limoyo, Jimmy Li, Dmitriy Rivkin, Jonathan Kelly, and Gregory
  Dudek
Categories: cs.CV cs.AI cs.RO
Comments: Submitted to the IEEE/RSJ International Conference on Intelligent
  Robotics and Systems (IROS'24), Abu Dhabi, UAE, Oct 14-18, 2024
\\ ( https://arxiv.org/abs/2401.11061 ,  23863kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12258
replaced with revised version Thu, 21 Mar 2024 17:29:37 GMT   (1402kb,D)

Title: Emergent Dominance Hierarchies in Reinforcement Learning Agents
Authors: Ram Rachum, Yonatan Nakar, Bill Tomlinson, Nitay Alon, Reuth Mirsky
Categories: cs.MA cs.AI cs.GT cs.LG
\\ ( https://arxiv.org/abs/2401.12258 ,  1402kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08812
replaced with revised version Thu, 21 Mar 2024 16:44:41 GMT   (8617kb,D)

Title: Intelligent Canvas: Enabling Design-Like Exploratory Visual Data
  Analysis with Generative AI through Rapid Prototyping, Iteration and Curation
Authors: Zijian Ding, Joel Chan
Categories: cs.HC cs.AI
\\ ( https://arxiv.org/abs/2402.08812 ,  8617kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16068
replaced with revised version Thu, 21 Mar 2024 11:58:49 GMT   (847kb,D)

Title: ROS-Causal: A ROS-based Causal Analysis Framework for Human-Robot
  Interaction Applications
Authors: Luca Castri, Gloria Beraldo, Sariah Mghames, Marc Hanheide, Nicola
  Bellotto
Categories: cs.RO cs.AI
Comments: Accepted by the "Causal-HRI: Causal Learning for Human-Robot
  Interaction" workshop at the 2024 ACM/IEEE International Conference on
  Human-Robot Interaction (HRI)
\\ ( https://arxiv.org/abs/2402.16068 ,  847kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02302
replaced with revised version Wed, 20 Mar 2024 20:05:45 GMT   (17791kb,D)

Title: Beyond Specialization: Assessing the Capabilities of MLLMs in Age and
  Gender Estimation
Authors: Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh
Categories: cs.CV cs.AI cs.LG
ACM-class: I.2.0; I.4.0; I.4.9
\\ ( https://arxiv.org/abs/2403.02302 ,  17791kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11879
replaced with revised version Thu, 21 Mar 2024 16:15:52 GMT   (104kb,D)

Title: Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton
Authors: Tobias Hallmen, Fabian Deuser, Norbert Oswald, Elisabeth Andr\'e
Categories: cs.SD cs.AI eess.AS
\\ ( https://arxiv.org/abs/2403.11879 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12777
replaced with revised version Wed, 20 Mar 2024 19:18:27 GMT   (24440kb,D)

Title: Discover and Mitigate Multiple Biased Subgroups in Image Classifiers
Authors: Zeliang Zhang, Mingqian Feng, Zhiheng Li, Chenliang Xu
Categories: cs.CV cs.AI
Comments: CVPR 2024. Code is available at https://github.com/ZhangAIPI/DIM
\\ ( https://arxiv.org/abs/2403.12777 ,  24440kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03296 (*cross-listing*)
replaced with revised version Wed, 20 Mar 2024 19:08:06 GMT   (1934kb)

Title: Gammatonegram Representation for End-to-End Dysarthric Speech Processing
  Tasks: Speech Recognition, Speaker Identification, and Intelligibility
  Assessment
Authors: Aref Farhadipour and Hadi Veisi
Categories: eess.AS cs.CL cs.SD
Comments: 12 pages, 8 figures. Iran J Comput Sci (2024)
DOI: 10.1007/s42044-024-00175-y
\\ ( https://arxiv.org/abs/2307.03296 ,  1934kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16226
replaced with revised version Thu, 21 Mar 2024 04:47:27 GMT   (7367kb,D)

Title: TiC-CLIP: Continual Training of CLIP Models
Authors: Saurabh Garg, Mehrdad Farajtabar, Hadi Pouransari, Raviteja
  Vemulapalli, Sachin Mehta, Oncel Tuzel, Vaishaal Shankar, Fartash Faghri
Categories: cs.CV cs.CL cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.16226 ,  7367kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19268
replaced with revised version Wed, 20 Mar 2024 21:24:33 GMT   (15832kb,D)

Title: Moral Judgments in Narratives on Reddit: Investigating Moral Sparks via
  Social Commonsense and Linguistic Signals
Authors: Ruijie Xi, Munindar P. Singh
Categories: cs.SI cs.CL cs.CY
\\ ( https://arxiv.org/abs/2310.19268 ,  15832kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04302
replaced with revised version Wed, 20 Mar 2024 23:32:08 GMT   (9431kb,D)

Title: Prompt Highlighter: Interactive Control for Multi-Modal LLMs
Authors: Yuechen Zhang, Shengju Qian, Bohao Peng, Shu Liu, Jiaya Jia
Categories: cs.CV cs.CL
Comments: CVPR 2024; Project Page:
  https://julianjuaner.github.io/projects/PromptHighlighter
\\ ( https://arxiv.org/abs/2312.04302 ,  9431kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11085
replaced with revised version Thu, 21 Mar 2024 17:25:23 GMT   (10921kb,D)

Title: m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks
Authors: Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, Ranjay Krishna
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2403.11085 ,  10921kb)
------------------------------------------------------------------------------
\\
arXiv:1902.06931 (*cross-listing*)
replaced with revised version Thu, 21 Mar 2024 09:01:19 GMT   (869kb,D)

Title: On the consistency of supervised learning with missing values
Authors: Julie Josse (XPOP, CMAP), Jacob M. Chen, Nicolas Prost (CMAP, XPOP,
  PARIETAL), Erwan Scornet (X, CMAP, SU), Ga\"el Varoquaux (PARIETAL)
Categories: stat.ML cs.LG math.ST stat.TH
\\ ( https://arxiv.org/abs/1902.06931 ,  869kb)
------------------------------------------------------------------------------
\\
arXiv:2209.05812 (*cross-listing*)
replaced with revised version Wed, 20 Mar 2024 20:19:28 GMT   (1371kb,D)

Title: A Non-Parametric Bootstrap for Spectral Clustering
Authors: Liam Welsh and Phillip Shreeves
Categories: stat.ML cs.LG
Comments: 19 pages, 4 figures, 4 tables
\\ ( https://arxiv.org/abs/2209.05812 ,  1371kb)
------------------------------------------------------------------------------
\\
arXiv:2211.10938
replaced with revised version Thu, 21 Mar 2024 13:51:10 GMT   (3472kb,D)

Title: AI-KD: Adversarial learning and Implicit regularization for
  self-Knowledge Distillation
Authors: Hyungmin Kim, Sungho Suh, Sunghyun Baek, Daehwan Kim, Daun Jeong,
  Hansang Cho, and Junmo Kim
Categories: cs.CV cs.LG
Comments: Accepted to KBS
\\ ( https://arxiv.org/abs/2211.10938 ,  3472kb)
------------------------------------------------------------------------------
\\
arXiv:2212.01729 (*cross-listing*)
replaced with revised version Thu, 21 Mar 2024 05:45:15 GMT   (2043kb)

Title: Time-Synchronized Full System State Estimation Considering Practical
  Implementation Challenges
Authors: Antos Cheeramban Varghese, Hritik Shah, Behrouz Azimian, Anamitra Pal,
  and Evangelos Farantatos
Categories: eess.SP cs.LG
\\ ( https://arxiv.org/abs/2212.01729 ,  2043kb)
------------------------------------------------------------------------------
\\
arXiv:2303.09780 (*cross-listing*)
replaced with revised version Thu, 21 Mar 2024 12:05:47 GMT   (18244kb,D)

Title: Mpox-AISM: AI-Mediated Super Monitoring for Mpox and Like-Mpox
Authors: Yubiao Yue, Minghua Jiang, Xinyue Zhang, Jialong Xu, Huacong Ye, Fan
  Zhang, Zhenzhang Li, Yang Li
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2303.09780 ,  18244kb)
------------------------------------------------------------------------------
\\
arXiv:2303.12157
replaced with revised version Thu, 21 Mar 2024 16:09:57 GMT   (8594kb,D)

Title: Learning a Depth Covariance Function
Authors: Eric Dexheimer and Andrew J. Davison
Categories: cs.CV cs.LG cs.RO
Comments: CVPR 2023. Project page: https://edexheim.github.io/DepthCov/
\\ ( https://arxiv.org/abs/2303.12157 ,  8594kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09478
replaced with revised version Thu, 21 Mar 2024 10:28:38 GMT   (130kb,D)

Title: The Role of Transparency in Repeated First-Price Auctions with Unknown
  Valuations
Authors: Nicol\`o Cesa-Bianchi, Tommaso Cesari, Roberto Colomboni, Federico
  Fusco, and Stefano Leonardi
Categories: cs.GT cs.DS cs.LG
Comments: Accepted at STOC 2024
\\ ( https://arxiv.org/abs/2307.09478 ,  130kb)
------------------------------------------------------------------------------
\\
arXiv:2308.03574 (*cross-listing*)
replaced with revised version Thu, 21 Mar 2024 09:13:17 GMT   (3973kb,D)

Title: Generalized Early Stopping in Evolutionary Direct Policy Search
Authors: Etor Arza, Leni K. Le Goff and Emma Hart
Categories: stat.ML cs.LG cs.NE cs.RO
DOI: 10.1145/3653024
\\ ( https://arxiv.org/abs/2308.03574 ,  3973kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10569
replaced with revised version Thu, 21 Mar 2024 07:12:06 GMT   (1299kb,D)

Title: Task Graph offloading via Deep Reinforcement Learning in Mobile Edge
  Computing
Authors: Jiagang Liu, Yun Mi, Xinyu Zhang, Xiaocui Li
Categories: cs.DC cs.LG
Comments: 13 figures
\\ ( https://arxiv.org/abs/2309.10569 ,  1299kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16584
replaced with revised version Thu, 21 Mar 2024 16:21:23 GMT   (162kb,D)

Title: Collaborative Distributed Machine Learning
Authors: David Jin, Niclas Kannengie{\ss}er, Sascha Rank, Ali Sunyaev
Categories: cs.MA cs.ET cs.LG cs.SE
\\ ( https://arxiv.org/abs/2309.16584 ,  162kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00817 (*cross-listing*)
replaced with revised version Thu, 21 Mar 2024 01:56:13 GMT   (1539kb,D)

Title: Learning to Make Adherence-Aware Advice
Authors: Guanting Chen, Xiaocheng Li, Chunlin Sun, Hanzhao Wang
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2310.00817 ,  1539kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03054 (*cross-listing*)
replaced with revised version Thu, 21 Mar 2024 12:43:34 GMT   (14218kb,D)

Title: Posterior Sampling Based on Gradient Flows of the MMD with Negative
  Distance Kernel
Authors: Paul Hagemann, Johannes Hertrich, Fabian Altekr\"uger, Robert Beinert,
  Jannis Chemseddine, Gabriele Steidl
Categories: stat.ML cs.LG math.OC math.PR
Comments: Published as a conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2310.03054 ,  14218kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18744 (*cross-listing*)
replaced with revised version Thu, 21 Mar 2024 03:09:15 GMT   (5909kb,D)

Title: $\mathbb{Z}_2\times \mathbb{Z}_2$ Equivariant Quantum Neural Networks:
  Benchmarking against Classical Neural Networks
Authors: Zhongtian Dong, Mar\c{c}al Comajoan Cara, Gopal Ramesh Dahale, Roy T.
  Forestano, Sergei Gleyzer, Daniel Justice, Kyoungchul Kong, Tom Magorsch,
  Konstantin T. Matchev, Katia Matcheva, Eyup B. Unlu
Categories: quant-ph cs.LG hep-ph stat.ML
Comments: 13 pages, 7 figures
Journal-ref: Axioms 2024, 13 (3), 188
DOI: 10.3390/axioms13030188
\\ ( https://arxiv.org/abs/2311.18744 ,  5909kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02914
replaced with revised version Thu, 21 Mar 2024 13:53:48 GMT   (6706kb,D)

Title: Unsupervised Video Domain Adaptation with Masked Pre-Training and
  Collaborative Self-Training
Authors: Arun Reddy, William Paul, Corban Rivera, Ketul Shah, Celso M. de Melo,
  Rama Chellappa
Categories: cs.CV cs.LG
Comments: Accepted at CVPR 2024. 13 pages, 4 figures
\\ ( https://arxiv.org/abs/2312.02914 ,  6706kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10132
replaced with revised version Thu, 21 Mar 2024 15:42:06 GMT   (690kb,D)

Title: Closing the Gap: Achieving Better Accuracy-Robustness Tradeoffs against
  Query-Based Attacks
Authors: Pascal Zimmer, S\'ebastien Andreina, Giorgia Azzurra Marson, Ghassan
  Karame
Categories: cs.CV cs.CR cs.LG
Comments: To appear in the Proceedings of the AAAI Conference on Artificial
  Intelligence (AAAI) 2024
\\ ( https://arxiv.org/abs/2312.10132 ,  690kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14057
replaced with revised version Thu, 21 Mar 2024 08:29:32 GMT   (297kb,D)

Title: Weighted least-squares approximation with determinantal point processes
  and generalized volume sampling
Authors: Anthony Nouy and Bertrand Michel
Categories: math.NA cs.LG cs.NA math.ST stat.TH
Comments: In this second version, conjecture (13) on DPP and (16) on volume
  sampling have been modified, including a convexity requirement. Proofs of
  propositions 5.4 and 5.12 have been modified accordingly. Remarks 5.5 and 5.6
  have been added to discuss alternatives to conjecture (13) on DPP
\\ ( https://arxiv.org/abs/2312.14057 ,  297kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12190 (*cross-listing*)
replaced with revised version Wed, 20 Mar 2024 21:21:48 GMT   (15510kb,D)

Title: Towards AI-Based Precision Oncology: A Machine Learning Framework for
  Personalized Counterfactual Treatment Suggestions based on Multi-Omics Data
Authors: Manuel Sch\"urch, Laura Boos, Viola Heinzelmann-Schwarz, Gabriele Gut,
  Michael Krauthammer, Andreas Wicki, Tumor Profiler Consortium
Categories: stat.ML cs.LG q-bio.QM
\\ ( https://arxiv.org/abs/2402.12190 ,  15510kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03849 (*cross-listing*)
replaced with revised version Thu, 21 Mar 2024 16:49:20 GMT   (3356kb,D)

Title: MedMamba: Vision Mamba for Medical Image Classification
Authors: Yubiao Yue, Zhenzhang Li
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2403.03849 ,  3356kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13795
replaced with revised version Thu, 21 Mar 2024 08:14:36 GMT   (134kb,D)

Title: PyVRP: a high-performance VRP solver package
Authors: Niels A. Wouda and Leon Lan and Wouter Kool
Categories: cs.NE cs.LG
Comments: Pre-print of accepted paper in INFORMS Journal on Computing. 24
  pages, 1 figure, 2 listings
DOI: 10.1287/ijoc.2023.0055
\\ ( https://arxiv.org/abs/2403.13795 ,  134kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
