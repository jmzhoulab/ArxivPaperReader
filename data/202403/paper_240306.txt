paper_240306.txt

Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月6日 17:24
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Mon  4 Mar 24 19:00:00 GMT  to  Tue  5 Mar 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.02454
Date: Mon, 4 Mar 2024 20:14:38 GMT   (1432kb,D)

Title: The Ink Splotch Effect: A Case Study on ChatGPT as a Co-Creative Game
  Designer
Authors: Asad Anjum, Yuting Li, Noelle Law, M Charity, and Julian Togelius
Categories: cs.AI
Comments: 12 pages
\\
  This paper studies how large language models (LLMs) can act as effective,
high-level creative collaborators and ``muses'' for game design. We model the
design of this study after the exercises artists use by looking at amorphous
ink splotches for creative inspiration. Our goal is to determine whether
AI-assistance can improve, hinder, or provide an alternative quality to games
when compared to the creative intents implemented by human designers. The
capabilities of LLMs as game designers are stress tested by placing it at the
forefront of the decision making process. Three prototype games are designed
across 3 different genres: (1) a minimalist base game, (2) a game with features
and game feel elements added by a human game designer, and (3) a game with
features and feel elements directly implemented from prompted outputs of the
LLM, ChatGPT. A user study was conducted and participants were asked to blindly
evaluate the quality and their preference of these games. We discuss both the
development process of communicating creative intent to an AI chatbot and the
synthesized open feedback of the participants. We use this data to determine
both the benefits and shortcomings of AI in a more design-centric role.
\\ ( https://arxiv.org/abs/2403.02454 ,  1432kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02482
Date: Mon, 4 Mar 2024 21:04:54 GMT   (579kb,D)

Title: MORBDD: Multiobjective Restricted Binary Decision Diagrams by Learning
  to Sparsify
Authors: Rahul Patel, Elias B. Khalil, David Bergman
Categories: cs.AI
\\
  In multicriteria decision-making, a user seeks a set of non-dominated
solutions to a (constrained) multiobjective optimization problem, the so-called
Pareto frontier. In this work, we seek to bring a state-of-the-art method for
exact multiobjective integer linear programming into the heuristic realm. We
focus on binary decision diagrams (BDDs) which first construct a graph that
represents all feasible solutions to the problem and then traverse the graph to
extract the Pareto frontier. Because the Pareto frontier may be exponentially
large, enumerating it over the BDD can be time-consuming. We explore how
restricted BDDs, which have already been shown to be effective as heuristics
for single-objective problems, can be adapted to multiobjective optimization
through the use of machine learning (ML). MORBDD, our ML-based BDD sparsifier,
first trains a binary classifier to eliminate BDD nodes that are unlikely to
contribute to Pareto solutions, then post-processes the sparse BDD to ensure
its connectivity via optimization. Experimental results on multiobjective
knapsack problems show that MORBDD is highly effective at producing very small
restricted BDDs with excellent approximation quality, outperforming
width-limited restricted BDDs and the well-known evolutionary algorithm
NSGA-II.
\\ ( https://arxiv.org/abs/2403.02482 ,  579kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02523
Date: Mon, 4 Mar 2024 22:27:11 GMT   (498kb,D)

Title: Transformer for Times Series: an Application to the S&P500
Authors: Pierre Brugiere and Gabriel Turinici
Categories: cs.AI q-fin.PM q-fin.ST stat.ML
\\
  The transformer models have been extensively used with good results in a wide
area of machine learning applications including Large Language Models and image
generation. Here, we inquire on the applicability of this approach to financial
time series. We first describe the dataset construction for two prototypical
situations: a mean reverting synthetic Ornstein-Uhlenbeck process on one hand
and real S&P500 data on the other hand. Then, we present in detail the proposed
Transformer architecture and finally we discuss some encouraging results. For
the synthetic data we predict rather accurately the next move, and for the
S&P500 we get some interesting results related to quadratic variation and
volatility prediction.
\\ ( https://arxiv.org/abs/2403.02523 ,  498kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02610
Date: Tue, 5 Mar 2024 02:58:57 GMT   (88kb)

Title: ChatGPT4PCG 2 Competition: Prompt Engineering for Science Birds Level
  Generation
Authors: Pittawat Taveekitworachai, Febri Abdullah, Mury F. Dewantoro, Yi Xia,
  Pratch Suntichaikul, Ruck Thawonmas, Julian Togelius, Jochen Renz
Categories: cs.AI
ACM-class: I.2.7; I.2.8
\\
  This paper presents the second ChatGPT4PCG competition at the 2024 IEEE
Conference on Games. In this edition of the competition, we follow the first
edition, but make several improvements and changes. We introduce a new
evaluation metric along with allowing a more flexible format for participants'
submissions and making several improvements to the evaluation pipeline.
Continuing from the first edition, we aim to foster and explore the realm of
prompt engineering (PE) for procedural content generation (PCG). While the
first competition saw success, it was hindered by various limitations; we aim
to mitigate these limitations in this edition. We introduce diversity as a new
metric to discourage submissions aimed at producing repetitive structures.
Furthermore, we allow submission of a Python program instead of a prompt text
file for greater flexibility in implementing advanced PE approaches, which may
require control flow, including conditions and iterations. We also make several
improvements to the evaluation pipeline with a better classifier for similarity
evaluation and better-performing function signatures. We thoroughly evaluate
the effectiveness of the new metric and the improved classifier. Additionally,
we perform an ablation study to select a function signature to instruct ChatGPT
for level generation. Finally, we provide implementation examples of various PE
techniques in Python and evaluate their preliminary performance. We hope this
competition serves as a resource and platform for learning about PE and PCG in
general.
\\ ( https://arxiv.org/abs/2403.02610 ,  88kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02635
Date: Tue, 5 Mar 2024 03:59:01 GMT   (7370kb,D)

Title: PPS-QMIX: Periodically Parameter Sharing for Accelerating Convergence of
  Multi-Agent Reinforcement Learning
Authors: Ke Zhang, DanDan Zhu, Qiuhan Xu, Hao Zhou and Ce Zheng
Categories: cs.AI
Comments: 10 pages, 5 figures
\\
  Training for multi-agent reinforcement learning(MARL) is a time-consuming
process caused by distribution shift of each agent. One drawback is that
strategy of each agent in MARL is independent but actually in cooperation.
Thus, a vertical issue in multi-agent reinforcement learning is how to
efficiently accelerate training process. To address this problem, current
research has leveraged a centralized function(CF) across multiple agents to
learn contribution of the team reward for each agent. However, CF based methods
introduce joint error from other agents in estimation of value network. In so
doing, inspired by federated learning, we propose three simple novel approaches
called Average Periodically Parameter Sharing(A-PPS), Reward-Scalability
Periodically Parameter Sharing(RS-PPS) and Partial Personalized Periodically
Parameter Sharing(PP-PPS) mechanism to accelerate training of MARL. Agents
share Q-value network periodically during the training process. Agents which
has same identity adapt collected reward as scalability and update partial
neural network during period to share different parameters. We apply our
approaches in classical MARL method QMIX and evaluate our approaches on various
tasks in StarCraft Multi-Agent Challenge(SMAC) environment. Performance of
numerical experiments yield enormous enhancement, with an average improvement
of 10\%-30\%, and enable to win tasks that QMIX cannot. Our code can be
downloaded from https://github.com/ColaZhang22/PPS-QMIX
\\ ( https://arxiv.org/abs/2403.02635 ,  7370kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02719
Date: Tue, 5 Mar 2024 07:17:18 GMT   (999kb)

Title: Multi-Scale Subgraph Contrastive Learning
Authors: Yanbei Liu, Yu Zhao, Xiao Wang, Lei Geng and Zhitao Xiao
Categories: cs.AI
\\
  Graph-level contrastive learning, aiming to learn the representations for
each graph by contrasting two augmented graphs, has attracted considerable
attention. Previous studies usually simply assume that a graph and its
augmented graph as a positive pair, otherwise as a negative pair. However, it
is well known that graph structure is always complex and multi-scale, which
gives rise to a fundamental question: after graph augmentation, will the
previous assumption still hold in reality? By an experimental analysis, we
discover the semantic information of an augmented graph structure may be not
consistent as original graph structure, and whether two augmented graphs are
positive or negative pairs is highly related with the multi-scale structures.
Based on this finding, we propose a multi-scale subgraph contrastive learning
method which is able to characterize the fine-grained semantic information.
Specifically, we generate global and local views at different scales based on
subgraph sampling, and construct multiple contrastive relationships according
to their semantic associations to provide richer self-supervised signals.
Extensive experiments and parametric analysis on eight graph classification
real-world datasets well demonstrate the effectiveness of the proposed method.
\\ ( https://arxiv.org/abs/2403.02719 ,  999kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02723
Date: Tue, 5 Mar 2024 07:29:12 GMT   (21963kb,D)

Title: Minimum Topology Attacks for Graph Neural Networks
Authors: Mengmei Zhang, Xiao Wang, Chuan Shi, Lingjuan Lyu, Tianchi Yang,
  Junping Du
Categories: cs.AI
Comments: Published on WWW 2023. Proceedings of the ACM Web Conference 2023
DOI: 10.1145/3543507.3583509
\\
  With the great popularity of Graph Neural Networks (GNNs), their robustness
to adversarial topology attacks has received significant attention. Although
many attack methods have been proposed, they mainly focus on fixed-budget
attacks, aiming at finding the most adversarial perturbations within a fixed
budget for target node. However, considering the varied robustness of each
node, there is an inevitable dilemma caused by the fixed budget, i.e., no
successful perturbation is found when the budget is relatively small, while if
it is too large, the yielding redundant perturbations will hurt the
invisibility. To break this dilemma, we propose a new type of topology attack,
named minimum-budget topology attack, aiming to adaptively find the minimum
perturbation sufficient for a successful attack on each node. To this end, we
propose an attack model, named MiBTack, based on a dynamic projected gradient
descent algorithm, which can effectively solve the involving non-convex
constraint optimization on discrete topology. Extensive results on three GNNs
and four real-world datasets show that MiBTack can successfully lead all target
nodes misclassified with the minimum perturbation edges. Moreover, the obtained
minimum budget can be used to measure node robustness, so we can explore the
relationships of robustness, topology, and uncertainty for nodes, which is
beyond what the current fixed-budget topology attacks can offer.
\\ ( https://arxiv.org/abs/2403.02723 ,  21963kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02745
Date: Tue, 5 Mar 2024 07:58:12 GMT   (1142kb,D)

Title: CURATRON: Complete Robust Preference Data for Robust Alignment of Large
  Language Models
Authors: Son The Nguyen, Niranjan Uma Naresh, Theja Tulabandhula
Categories: cs.AI cs.CL
\\
  This paper addresses the challenges of aligning large language models (LLMs)
with human values via preference learning (PL), with a focus on the issues of
incomplete and corrupted data in preference datasets. We propose a novel method
for robustly and completely recalibrating values within these datasets to
enhance LLMs resilience against the issues. In particular, we devise a
guaranteed polynomial time ranking algorithm that robustifies several existing
models, such as the classic Bradley--Terry--Luce (BTL) (Bradley and Terry,
1952) model and certain generalizations of it. To the best of our knowledge,
our present work is the first to propose an algorithm that provably recovers an
{\epsilon}-optimal ranking with high probability while allowing as large as
O(n) perturbed pairwise comparison results per model response. Furthermore, we
show robust recovery results in the partially observed setting. Our experiments
confirm that our algorithms handle adversarial noise and unobserved comparisons
well in both general and LLM preference dataset settings. This work contributes
to the development and scaling of more reliable and ethically aligned AI models
by equipping the dataset curation pipeline with the ability to handle missing
and maliciously manipulated inputs.
\\ ( https://arxiv.org/abs/2403.02745 ,  1142kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02760
Date: Tue, 5 Mar 2024 08:31:00 GMT   (736kb)

Title: Emerging Synergies Between Large Language Models and Machine Learning in
  Ecommerce Recommendations
Authors: Xiaonan Xu, Zheng Xu, Zhipeng Ling, Zhengyu Jin, ShuQian Du
Categories: cs.AI
\\
  With the boom of e-commerce and web applications, recommender systems have
become an important part of our daily lives, providing personalized
recommendations based on the user's preferences. Although deep neural networks
(DNNs) have made significant progress in improving recommendation systems by
simulating the interaction between users and items and incorporating their
textual information, these DNN-based approaches still have some limitations,
such as the difficulty of effectively understanding users' interests and
capturing textual information. It is not possible to generalize to different
seen/unseen recommendation scenarios and reason about their predictions. At the
same time, the emergence of large language models (LLMs), represented by
ChatGPT and GPT-4, has revolutionized the fields of natural language processing
(NLP) and artificial intelligence (AI) due to their superior capabilities in
the basic tasks of language understanding and generation, and their impressive
generalization and reasoning capabilities. As a result, recent research has
sought to harness the power of LLM to improve recommendation systems. Given the
rapid development of this research direction in the field of recommendation
systems, there is an urgent need for a systematic review of existing LLM-driven
recommendation systems for researchers and practitioners in related fields to
gain insight into. More specifically, we first introduced a representative
approach to learning user and item representations using LLM as a feature
encoder. We then reviewed the latest advances in LLMs techniques for
collaborative filtering enhanced recommendation systems from the three
paradigms of pre-training, fine-tuning, and prompting. Finally, we had a
comprehensive discussion on the future direction of this emerging field.
\\ ( https://arxiv.org/abs/2403.02760 ,  736kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02775
Date: Tue, 5 Mar 2024 08:45:30 GMT   (95kb,D)

Title: EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs
Authors: Hanlin Tang, Yifu Sun, Decheng Wu, Kai Liu, Jianchen Zhu, Zhanhui Kang
Categories: cs.AI cs.LG
\\
  Large language models (LLMs) have proven to be very superior to conventional
methods in various tasks. However, their expensive computations and high memory
requirements are prohibitive for deployment. Model quantization is an effective
method for reducing this overhead. The problem is that in most previous works,
the quantized model was calibrated using few samples from the training data,
which might affect the generalization of the quantized LLMs to unknown cases
and tasks. Hence in this work, we explore an important question: Can we design
a data-independent quantization method for LLMs to guarantee its generalization
performance? In this work, we propose EasyQuant, a training-free and
data-independent weight-only quantization algorithm for LLMs. Our observation
indicates that two factors: outliers in the weight and quantization ranges, are
essential for reducing the quantization error. Therefore, in EasyQuant, we
leave the outliers (less than 1%) unchanged and optimize the quantization range
to reduce the reconstruction error. With these methods, we surprisingly find
that EasyQuant achieves comparable performance to the original model. Since
EasyQuant does not depend on any training data, the generalization performance
of quantized LLMs is safely guaranteed. Moreover, EasyQuant can be implemented
in parallel so that the quantized model could be attained in a few minutes even
for LLMs over 100B. To our best knowledge, we are the first work that achieves
almost lossless quantization performance for LLMs under a data-independent
setting and our algorithm runs over 10 times faster than the data-dependent
methods.
\\ ( https://arxiv.org/abs/2403.02775 ,  95kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02783
Date: Tue, 5 Mar 2024 08:56:30 GMT   (322kb,D)

Title: Where the Really Hard Quadratic Assignment Problems Are: the QAP-SAT
  instances
Authors: S\'ebastien Verel (LISIC), Sarah Thomson, Omar Rifki (LISIC)
Categories: cs.AI
Journal-ref: Evolutionary Computation in Combinatorial Optimization Conference
  (evoCOP), Apr 2024, Aberystwyth, United Kingdom
\\
  The Quadratic Assignment Problem (QAP) is one of the major domains in the
field of evolutionary computation, and more widely in combinatorial
optimization. This paper studies the phase transition of the QAP, which can be
described as a dramatic change in the problem's computational complexity and
satisfiability, within a narrow range of the problem parameters. To approach
this phenomenon, we introduce a new QAP-SAT design of the initial problem based
on submodularity to capture its difficulty with new features. This
decomposition is studied experimentally using branch-and-bound and tabu search
solvers. A phase transition parameter is then proposed. The critical parameter
of phase transition satisfaction and that of the solving effort are shown to be
highly correlated for tabu search, thus allowing the prediction of difficult
instances.
\\ ( https://arxiv.org/abs/2403.02783 ,  322kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02795
Date: Tue, 5 Mar 2024 09:09:15 GMT   (1658kb,D)

Title: Evaluating and Optimizing Educational Content with Large Language Model
  Judgments
Authors: Joy He-Yueya, Noah D. Goodman, Emma Brunskill
Categories: cs.AI cs.CL
\\
  Creating effective educational materials generally requires expensive and
time-consuming studies of student learning outcomes. To overcome this barrier,
one idea is to build computational models of student learning and use them to
optimize instructional materials. However, it is difficult to model the
cognitive processes of learning dynamics. We propose an alternative approach
that uses Language Models (LMs) as educational experts to assess the impact of
various instructions on learning outcomes. Specifically, we use GPT-3.5 to
evaluate the overall effect of instructional materials on different student
groups and find that it can replicate well-established educational findings
such as the Expertise Reversal Effect and the Variability Effect. This
demonstrates the potential of LMs as reliable evaluators of educational
content. Building on this insight, we introduce an instruction optimization
approach in which one LM generates instructional materials using the judgments
of another LM as a reward function. We apply this approach to create math word
problem worksheets aimed at maximizing student learning gains. Human teachers'
evaluations of these LM-generated worksheets show a significant alignment
between the LM judgments and human teacher preferences. We conclude by
discussing potential divergences between human and LM opinions and the
resulting pitfalls of automating instructional design.
\\ ( https://arxiv.org/abs/2403.02795 ,  1658kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02820
Date: Tue, 5 Mar 2024 09:44:19 GMT   (3165kb,D)

Title: Reconstruction for Sparse View Tomography of Long Objects Applied to
  Imaging in the Wood Industry
Authors: Buda Baji\'c, Johannes A. J. Huber, Benedikt Neyses, Linus Olofsson,
  Ozan \"Oktem
Categories: cs.AI
\\
  In the wood industry, logs are commonly quality screened by discrete X-ray
scans on a moving conveyor belt from a few source positions. Typically,
two-dimensional (2D) slice-wise measurements are obtained by a sequential
scanning geometry. Each 2D slice alone does not carry sufficient information
for a three-dimensional tomographic reconstruction in which biological features
of interest in the log are well preserved. In the present work, we propose a
learned iterative reconstruction method based on the Learned Primal-Dual neural
network, suited for sequential scanning geometries. Our method accumulates
information between neighbouring slices, instead of only accounting for single
slices during reconstruction. Our quantitative and qualitative evaluations with
as few as five source positions show that our method yields reconstructions of
logs that are sufficiently accurate to identify biological features like knots
(branches), heartwood and sapwood.
\\ ( https://arxiv.org/abs/2403.02820 ,  3165kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02870
Date: Tue, 5 Mar 2024 11:26:22 GMT   (5985kb,D)

Title: Precise Extraction of Deep Learning Models via Side-Channel Attacks on
  Edge/Endpoint Devices
Authors: Younghan Lee, Sohee Jun, Yungi Cho, Woorim Han, Hyungon Moon, and
  Yunheung Paek
Categories: cs.AI cs.CR cs.LG
Comments: Accepted by 27th European Symposium on Research in Computer Security
  (ESORICS 2022)
\\
  With growing popularity, deep learning (DL) models are becoming larger-scale,
and only the companies with vast training datasets and immense computing power
can manage their business serving such large models. Most of those DL models
are proprietary to the companies who thus strive to keep their private models
safe from the model extraction attack (MEA), whose aim is to steal the model by
training surrogate models. Nowadays, companies are inclined to offload the
models from central servers to edge/endpoint devices. As revealed in the latest
studies, adversaries exploit this opportunity as new attack vectors to launch
side-channel attack (SCA) on the device running victim model and obtain various
pieces of the model information, such as the model architecture (MA) and image
dimension (ID). Our work provides a comprehensive understanding of such a
relationship for the first time and would benefit future MEA studies in both
offensive and defensive sides in that they may learn which pieces of
information exposed by SCA are more important than the others. Our analysis
additionally reveals that by grasping the victim model information from SCA,
MEA can get highly effective and successful even without any prior knowledge of
the model. Finally, to evince the practicality of our analysis results, we
empirically apply SCA, and subsequently, carry out MEA under realistic threat
assumptions. The results show up to 5.8 times better performance than when the
adversary has no model information about the victim model.
\\ ( https://arxiv.org/abs/2403.02870 ,  5985kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02899
Date: Tue, 5 Mar 2024 12:06:48 GMT   (3263kb,D)

Title: Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation
Authors: Zhekai Du, Xinyao Li, Fengling Li, Ke Lu, Lei Zhu, Jingjing Li
Categories: cs.AI
\\
  Conventional Unsupervised Domain Adaptation (UDA) strives to minimize
distribution discrepancy between domains, which neglects to harness rich
semantics from data and struggles to handle complex domain shifts. A promising
technique is to leverage the knowledge of large-scale pre-trained
vision-language models for more guided adaptation. Despite some endeavors,
current methods often learn textual prompts to embed domain semantics for
source and target domains separately and perform classification within each
domain, limiting cross-domain knowledge transfer. Moreover, prompting only the
language branch lacks flexibility to adapt both modalities dynamically. To
bridge this gap, we propose Domain-Agnostic Mutual Prompting (DAMP) to exploit
domain-invariant semantics by mutually aligning visual and textual embeddings.
Specifically, the image contextual information is utilized to prompt the
language branch in a domain-agnostic and instance-conditioned way. Meanwhile,
visual prompts are imposed based on the domain-agnostic textual prompt to
elicit domain-invariant visual embeddings. These two branches of prompts are
learned mutually with a cross-attention module and regularized with a
semantic-consistency loss and an instance-discrimination contrastive loss.
Experiments on three UDA benchmarks demonstrate the superiority of DAMP over
state-of-the-art approaches.
\\ ( https://arxiv.org/abs/2403.02899 ,  3263kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02901
Date: Tue, 5 Mar 2024 12:11:07 GMT   (1276kb,D)

Title: A Comprehensive Survey on Process-Oriented Automatic Text Summarization
  with Exploration of LLM-Based Methods
Authors: Hanlei Jin, Yang Zhang, Dan Meng, Jun Wang, Jinghua Tan
Categories: cs.AI
\\
  Automatic Text Summarization (ATS), utilizing Natural Language Processing
(NLP) algorithms, aims to create concise and accurate summaries, thereby
significantly reducing the human effort required in processing large volumes of
text. ATS has drawn considerable interest in both academic and industrial
circles. Many studies have been conducted in the past to survey ATS methods;
however, they generally lack practicality for real-world implementations, as
they often categorize previous methods from a theoretical standpoint. Moreover,
the advent of Large Language Models (LLMs) has altered conventional ATS
methods. In this survey, we aim to 1) provide a comprehensive overview of ATS
from a ``Process-Oriented Schema'' perspective, which is best aligned with
real-world implementations; 2) comprehensively review the latest LLM-based ATS
works; and 3) deliver an up-to-date survey of ATS, bridging the two-year gap in
the literature. To the best of our knowledge, this is the first survey to
specifically investigate LLM-based ATS methods.
\\ ( https://arxiv.org/abs/2403.02901 ,  1276kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02914
Date: Tue, 5 Mar 2024 12:31:24 GMT   (7643kb)

Title: DynST: Dynamic Sparse Training for Resource-Constrained Spatio-Temporal
  Forecasting
Authors: Hao Wu, Haomin Wen, Guibin Zhang, Yutong Xia, Kai Wang, Yuxuan Liang,
  Yu Zheng, Kun Wang
Categories: cs.AI
\\
  The ever-increasing sensor service, though opening a precious path and
providing a deluge of earth system data for deep-learning-oriented earth
science, sadly introduce a daunting obstacle to their industrial level
deployment. Concretely, earth science systems rely heavily on the extensive
deployment of sensors, however, the data collection from sensors is constrained
by complex geographical and social factors, making it challenging to achieve
comprehensive coverage and uniform deployment. To alleviate the obstacle,
traditional approaches to sensor deployment utilize specific algorithms to
design and deploy sensors. These methods dynamically adjust the activation
times of sensors to optimize the detection process across each sub-region.
Regrettably, formulating an activation strategy generally based on historical
observations and geographic characteristics, which make the methods and
resultant models were neither simple nor practical. Worse still, the complex
technical design may ultimately lead to a model with weak generalizability. In
this paper, we introduce for the first time the concept of spatio-temporal data
dynamic sparse training and are committed to adaptively, dynamically filtering
important sensor distributions. To our knowledge, this is the first proposal
(termed DynST) of an industry-level deployment optimization concept at the data
level. However, due to the existence of the temporal dimension, pruning of
spatio-temporal data may lead to conflicts at different timestamps. To achieve
this goal, we employ dynamic merge technology, along with ingenious dimensional
mapping to mitigate potential impacts caused by the temporal aspect. During the
training process, DynST utilize iterative pruning and sparse training,
repeatedly identifying and dynamically removing sensor perception areas that
contribute the least to future predictions.
\\ ( https://arxiv.org/abs/2403.02914 ,  7643kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02933
Date: Tue, 5 Mar 2024 12:51:40 GMT   (907kb,D)

Title: Fuzzy Datalog$^\exists$ over Arbitrary t-Norms
Authors: Matthias Lanzinger, Stefano Sferrazza, Przemys{\l}aw A. Wa{\l}\k{e}ga,
  Georg Gottlob
Categories: cs.AI cs.LO
\\
  One of the main challenges in the area of Neuro-Symbolic AI is to perform
logical reasoning in the presence of both neural and symbolic data. This
requires combining heterogeneous data sources such as knowledge graphs, neural
model predictions, structured databases, crowd-sourced data, and many more. To
allow for such reasoning, we generalise the standard rule-based language
Datalog with existential rules (commonly referred to as tuple-generating
dependencies) to the fuzzy setting, by allowing for arbitrary t-norms in the
place of classical conjunctions in rule bodies. The resulting formalism allows
us to perform reasoning about data associated with degrees of uncertainty while
preserving computational complexity results and the applicability of reasoning
techniques established for the standard Datalog setting. In particular, we
provide fuzzy extensions of Datalog chases which produce fuzzy universal models
and we exploit them to show that in important fragments of the language,
reasoning has the same complexity as in the classical setting.
\\ ( https://arxiv.org/abs/2403.02933 ,  907kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02936
Date: Tue, 5 Mar 2024 13:03:31 GMT   (3707kb,D)

Title: AdAM: Adaptive Fault-Tolerant Approximate Multiplier for Edge DNN
  Accelerators
Authors: Mahdi Taheri, Natalia Cherezova, Samira Nazari, Ahsan Rafiq, Ali
  Azarpeyvand, Tara Ghasempouri, Masoud Daneshtalab, Jaan Raik and Maksim
  Jenihhin
Categories: cs.AI cs.AR cs.LG
\\
  In this paper, we propose an architecture of a novel adaptive fault-tolerant
approximate multiplier tailored for ASIC-based DNN accelerators.
\\ ( https://arxiv.org/abs/2403.02936 ,  3707kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02946
Date: Tue, 5 Mar 2024 13:17:09 GMT   (3376kb,D)

Title: SAFFIRA: a Framework for Assessing the Reliability of
  Systolic-Array-Based DNN Accelerators
Authors: Mahdi Taheri, Masoud Daneshtalab, Jaan Raik, Maksim Jenihhin,
  Salvatore Pappalardo, Paul Jimenez, Bastien Deveautour, and Alberto Bosio
Categories: cs.AI cs.AR cs.LG
\\
  Systolic array has emerged as a prominent architecture for Deep Neural
Network (DNN) hardware accelerators, providing high-throughput and low-latency
performance essential for deploying DNNs across diverse applications. However,
when used in safety-critical applications, reliability assessment is mandatory
to guarantee the correct behavior of DNN accelerators. While fault injection
stands out as a well-established practical and robust method for reliability
assessment, it is still a very time-consuming process. This paper addresses the
time efficiency issue by introducing a novel hierarchical software-based
hardware-aware fault injection strategy tailored for systolic array-based DNN
accelerators.
\\ ( https://arxiv.org/abs/2403.02946 ,  3376kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02950
Date: Tue, 5 Mar 2024 13:21:20 GMT   (2895kb,D)

Title: A general approach to enhance the survivability of backdoor attacks by
  decision path coupling
Authors: Yufei Zhao, Dingji Wang, Bihuan Chen, Ziqian Chen, Xin Peng
Categories: cs.AI cs.CR
\\
  Backdoor attacks have been one of the emerging security threats to deep
neural networks (DNNs), leading to serious consequences. One of the mainstream
backdoor defenses is model reconstruction-based. Such defenses adopt model
unlearning or pruning to eliminate backdoors. However, little attention has
been paid to survive from such defenses. To bridge the gap, we propose Venom,
the first generic backdoor attack enhancer to improve the survivability of
existing backdoor attacks against model reconstruction-based defenses. We
formalize Venom as a binary-task optimization problem. The first is the
original backdoor attack task to preserve the original attack capability, while
the second is the attack enhancement task to improve the attack survivability.
To realize the second task, we propose attention imitation loss to force the
decision path of poisoned samples in backdoored models to couple with the
crucial decision path of benign samples, which makes backdoors difficult to
eliminate. Our extensive evaluation on two DNNs and three datasets has
demonstrated that Venom significantly improves the survivability of eight
state-of-the-art attacks against eight state-of-the-art defenses without
impacting the capability of the original attacks.
\\ ( https://arxiv.org/abs/2403.02950 ,  2895kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02962
Date: Tue, 5 Mar 2024 13:33:12 GMT   (2211kb,D)

Title: WikiTableEdit: A Benchmark for Table Editing by Natural Language
  Instruction
Authors: Zheng Li and Xiang Chen and Xiaojun Wan
Categories: cs.AI
\\
  Tabular data, as a crucial form of data representation, exists in diverse
formats on the Web. When confronted with complex and irregular tables, manual
modification becomes a laborious task. This paper investigates the performance
of Large Language Models (LLMs) in the context of table editing tasks. Existing
research mainly focuses on regular-shaped tables, wherein instructions are used
to generate code in SQL, Python, or Excel Office-script for manipulating the
tables. Nevertheless, editing tables with irregular structures, particularly
those containing merged cells spanning multiple rows, poses a challenge when
using code. To address this, we introduce the WikiTableEdit dataset. Leveraging
26,531 tables from the WikiSQL dataset, we automatically generate natural
language instructions for six distinct basic operations and the corresponding
outcomes, resulting in over 200,000 instances. Subsequently, we evaluate
several representative large language models on the WikiTableEdit dataset to
demonstrate the challenge of this task. The dataset will be released to the
community to promote related researches.
\\ ( https://arxiv.org/abs/2403.02962 ,  2211kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02985
Date: Tue, 5 Mar 2024 14:04:13 GMT   (4348kb,D)

Title: Evolution Transformer: In-Context Evolutionary Optimization
Authors: Robert Tjarko Lange, Yingtao Tian, Yujin Tang
Categories: cs.AI cs.NE
\\
  Evolutionary optimization algorithms are often derived from loose biological
analogies and struggle to leverage information obtained during the sequential
course of optimization. An alternative promising approach is to leverage data
and directly discover powerful optimization principles via meta-optimization.
In this work, we follow such a paradigm and introduce Evolution Transformer, a
causal Transformer architecture, which can flexibly characterize a family of
Evolution Strategies. Given a trajectory of evaluations and search distribution
statistics, Evolution Transformer outputs a performance-improving update to the
search distribution. The architecture imposes a set of suitable inductive
biases, i.e. the invariance of the distribution update to the order of
population members within a generation and equivariance to the order of the
search dimensions. We train the model weights using Evolutionary Algorithm
Distillation, a technique for supervised optimization of sequence models using
teacher algorithm trajectories. The resulting model exhibits strong in-context
optimization performance and shows strong generalization capabilities to
otherwise challenging neuroevolution tasks. We analyze the resulting properties
of the Evolution Transformer and propose a technique to fully
self-referentially train the Evolution Transformer, starting from a random
initialization and bootstrapping its own learning progress. We provide an open
source implementation under https://github.com/RobertTLange/evosax.
\\ ( https://arxiv.org/abs/2403.02985 ,  4348kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02993
Date: Tue, 5 Mar 2024 14:18:15 GMT   (5360kb,D)

Title: Localized Zeroth-Order Prompt Optimization
Authors: Wenyang Hu, Yao Shu, Zongmin Yu, Zhaoxuan Wu, Xiangqiang Lin,
  Zhongxiang Dai, See-Kiong Ng, Bryan Kian Hsiang Low
Categories: cs.AI
\\
  The efficacy of large language models (LLMs) in understanding and generating
natural language has aroused a wide interest in developing prompt-based methods
to harness the power of black-box LLMs. Existing methodologies usually
prioritize a global optimization for finding the global optimum, which however
will perform poorly in certain tasks. This thus motivates us to re-think the
necessity of finding a global optimum in prompt optimization. To answer this,
we conduct a thorough empirical study on prompt optimization and draw two major
insights. Contrasting with the rarity of global optimum, local optima are
usually prevalent and well-performed, which can be more worthwhile for
efficient prompt optimization (Insight I). The choice of the input domain,
covering both the generation and the representation of prompts, affects the
identification of well-performing local optima (Insight II). Inspired by these
insights, we propose a novel algorithm, namely localized zeroth-order prompt
optimization (ZOPO), which incorporates a Neural Tangent Kernel-based derived
Gaussian process into standard zeroth-order optimization for an efficient
search of well-performing local optima in prompt optimization. Remarkably, ZOPO
outperforms existing baselines in terms of both the optimization performance
and the query efficiency, which we demonstrate through extensive experiments.
\\ ( https://arxiv.org/abs/2403.02993 ,  5360kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03008
Date: Tue, 5 Mar 2024 14:41:12 GMT   (1002kb,D)

Title: Knowledge Graphs as Context Sources for LLM-Based Explanations of
  Learning Recommendations
Authors: Hasan Abu-Rasheed, Christian Weber, Madjid Fathi
Categories: cs.AI
\\
  In the era of personalized education, the provision of comprehensible
explanations for learning recommendations is of a great value to enhance the
learner's understanding and engagement with the recommended learning content.
Large language models (LLMs) and generative AI in general have recently opened
new doors for generating human-like explanations, for and along learning
recommendations. However, their precision is still far away from acceptable in
a sensitive field like education. To harness the abilities of LLMs, while still
ensuring a high level of precision towards the intent of the learners, this
paper proposes an approach to utilize knowledge graphs (KG) as a source of
factual context, for LLM prompts, reducing the risk of model hallucinations,
and safeguarding against wrong or imprecise information, while maintaining an
application-intended learning context. We utilize the semantic relations in the
knowledge graph to offer curated knowledge about learning recommendations. With
domain-experts in the loop, we design the explanation as a textual template,
which is filled and completed by the LLM. Domain experts were integrated in the
prompt engineering phase as part of a study, to ensure that explanations
include information that is relevant to the learner. We evaluate our approach
quantitatively using Rouge-N and Rouge-L measures, as well as qualitatively
with experts and learners. Our results show an enhanced recall and precision of
the generated explanations compared to those generated solely by the GPT model,
with a greatly reduced risk of generating imprecise information in the final
learning explanation.
\\ ( https://arxiv.org/abs/2403.03008 ,  1002kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03017
Date: Tue, 5 Mar 2024 14:53:53 GMT   (8317kb,D)

Title: OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied
  Instruction Following
Authors: Haochen Shi, Zhiyuan Sun, Xingdi Yuan, Marc-Alexandre C\^ot\'e, Bang
  Liu
Categories: cs.AI
\\
  Embodied Instruction Following (EIF) is a crucial task in embodied learning,
requiring agents to interact with their environment through egocentric
observations to fulfill natural language instructions. Recent advancements have
seen a surge in employing large language models (LLMs) within a
framework-centric approach to enhance performance in embodied learning tasks,
including EIF. Despite these efforts, there exists a lack of a unified
understanding regarding the impact of various components-ranging from visual
perception to action execution-on task performance. To address this gap, we
introduce OPEx, a comprehensive framework that delineates the core components
essential for solving embodied learning tasks: Observer, Planner, and Executor.
Through extensive evaluations, we provide a deep analysis of how each component
influences EIF task performance. Furthermore, we innovate within this space by
deploying a multi-agent dialogue strategy on a TextWorld counterpart, further
enhancing task performance. Our findings reveal that LLM-centric design
markedly improves EIF outcomes, identify visual perception and low-level action
execution as critical bottlenecks, and demonstrate that augmenting LLMs with a
multi-agent framework further elevates performance.
\\ ( https://arxiv.org/abs/2403.03017 ,  8317kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03028
Date: Tue, 5 Mar 2024 15:04:18 GMT   (2991kb,D)

Title: Word Importance Explains How Prompts Affect Language Model Outputs
Authors: Stefan Hackmann, Haniyeh Mahmoudian, Mark Steadman and Michael Schmidt
Categories: cs.AI cs.CL
ACM-class: I.2.7; I.5.2
\\
  The emergence of large language models (LLMs) has revolutionized numerous
applications across industries. However, their "black box" nature often hinders
the understanding of how they make specific decisions, raising concerns about
their transparency, reliability, and ethical use. This study presents a method
to improve the explainability of LLMs by varying individual words in prompts to
uncover their statistical impact on the model outputs. This approach, inspired
by permutation importance for tabular data, masks each word in the system
prompt and evaluates its effect on the outputs based on the available text
scores aggregated over multiple user inputs. Unlike classical attention, word
importance measures the impact of prompt words on arbitrarily-defined text
scores, which enables decomposing the importance of words into the specific
measures of interest--including bias, reading level, verbosity, etc. This
procedure also enables measuring impact when attention weights are not
available. To test the fidelity of this approach, we explore the effect of
adding different suffixes to multiple different system prompts and comparing
subsequent generations with different large language models. Results show that
word importance scores are closely related to the expected suffix importances
for multiple scoring functions.
\\ ( https://arxiv.org/abs/2403.03028 ,  2991kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03165
Date: Tue, 5 Mar 2024 17:58:26 GMT   (487kb)

Title: Leveraging Federated Learning and Edge Computing for Recommendation
  Systems within Cloud Computing Networks
Authors: Yaqian Qi, Yaqian Qi, Xiangxiang Wang, Hanzhe Li, Jingxiao Tian
Categories: cs.AI
\\
  To enable large-scale and efficient deployment of artificial intelligence
(AI), the combination of AI and edge computing has spawned Edge Intelligence,
which leverages the computing and communication capabilities of end devices and
edge servers to process data closer to where it is generated. A key technology
for edge intelligence is the privacy-protecting machine learning paradigm known
as Federated Learning (FL), which enables data owners to train models without
having to transfer raw data to third-party servers. However, FL networks are
expected to involve thousands of heterogeneous distributed devices. As a
result, communication efficiency remains a key bottleneck. To reduce node
failures and device exits, a Hierarchical Federated Learning (HFL) framework is
proposed, where a designated cluster leader supports the data owner through
intermediate model aggregation. Therefore, based on the improvement of edge
server resource utilization, this paper can effectively make up for the
limitation of cache capacity. In order to mitigate the impact of soft clicks on
the quality of user experience (QoE), the authors model the user QoE as a
comprehensive system cost. To solve the formulaic problem, the authors propose
a decentralized caching algorithm with federated deep reinforcement learning
(DRL) and federated learning (FL), where multiple agents learn and make
decisions independently
\\ ( https://arxiv.org/abs/2403.03165 ,  487kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03172
Date: Tue, 5 Mar 2024 18:07:34 GMT   (4799kb,D)

Title: Reaching Consensus in Cooperative Multi-Agent Reinforcement Learning
  with Goal Imagination
Authors: Liangzhou Wang, Kaiwen Zhu, Fengming Zhu, Xinghu Yao, Shujie Zhang,
  Deheng Ye, Haobo Fu, Qiang Fu, Wei Yang
Categories: cs.AI cs.LG
\\
  Reaching consensus is key to multi-agent coordination. To accomplish a
cooperative task, agents need to coherently select optimal joint actions to
maximize the team reward. However, current cooperative multi-agent
reinforcement learning (MARL) methods usually do not explicitly take consensus
into consideration, which may cause miscoordination problem. In this paper, we
propose a model-based consensus mechanism to explicitly coordinate multiple
agents. The proposed Multi-agent Goal Imagination (MAGI) framework guides
agents to reach consensus with an Imagined common goal. The common goal is an
achievable state with high value, which is obtained by sampling from the
distribution of future states. We directly model this distribution with a
self-supervised generative model, thus alleviating the "curse of dimensinality"
problem induced by multi-agent multi-step policy rollout commonly used in
model-based methods. We show that such efficient consensus mechanism can guide
all agents cooperatively reaching valuable future states. Results on
Multi-agent Particle-Environments and Google Research Football environment
demonstrate the superiority of MAGI in both sample efficiency and performance.
\\ ( https://arxiv.org/abs/2403.03172 ,  4799kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03176
Date: Tue, 5 Mar 2024 18:13:18 GMT   (25kb)

Title: Unifying and Certifying Top-Quality Planning
Authors: Michael Katz, Junkyu Lee, Shirin Sohrabi
Categories: cs.AI
Comments: To appear at ICAPS 2024
\\
  The growing utilization of planning tools in practical scenarios has sparked
an interest in generating multiple high-quality plans. Consequently, a range of
computational problems under the general umbrella of top-quality planning were
introduced over a short time period, each with its own definition. In this
work, we show that the existing definitions can be unified into one, based on a
dominance relation. The different computational problems, therefore, simply
correspond to different dominance relations. Given the unified definition, we
can now certify the top-quality of the solutions, leveraging existing
certification of unsolvability and optimality. We show that task
transformations found in the existing literature can be employed for the
efficient certification of various top-quality planning problems and propose a
novel transformation to efficiently certify loopless top-quality planning.
\\ ( https://arxiv.org/abs/2403.03176 ,  25kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03186
Date: Tue, 5 Mar 2024 18:22:29 GMT   (18505kb,D)

Title: Towards General Computer Control: A Multimodal Agent for Red Dead
  Redemption II as a Case Study
Authors: Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng
  Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, Yifei Bi,
  Pengjie Gu, Xinrun Wang, B\"orje F. Karlsson, Bo An, Zongqing Lu
Categories: cs.AI
\\
  Recent studies have demonstrated the success of foundation agents in specific
tasks or scenarios. However, existing agents cannot generalize across different
scenarios, mainly due to their diverse observation and action spaces and
semantic gaps, or reliance on task-specific resources. In this work, we propose
the General Computer Control (GCC) setting: building foundation agents that can
master any computer task by taking only screen images (and possibly audio) of
the computer as input, and producing keyboard and mouse operations as output,
similar to human-computer interaction. To target GCC, we propose Cradle, an
agent framework with strong reasoning abilities, including self-reflection,
task inference, and skill curation, to ensure generalizability and
self-improvement across various tasks. To demonstrate the capabilities of
Cradle, we deploy it in the complex AAA game Red Dead Redemption II, serving as
a preliminary attempt towards GCC with a challenging target. Our agent can
follow the main storyline and finish real missions in this complex AAA game,
with minimal reliance on prior knowledge and application-specific resources.
The project website is at https://baai-agents.github.io/Cradle/.
\\ ( https://arxiv.org/abs/2403.03186 ,  18505kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03188
Date: Tue, 5 Mar 2024 18:24:52 GMT   (1651kb)

Title: Towards Democratized Flood Risk Management: An Advanced AI Assistant
  Enabled by GPT-4 for Enhanced Interpretability and Public Engagement
Authors: Rafaela Martelo, Ruo-Qian Wang (Rutgers University)
Categories: cs.AI cs.CY cs.HC
Comments: 48 pages, 3 figures and an appendix with 2 supplementary tables
  detailing experimental results and observations. Supported by Rutgers's
  Research Incubator in Climate and Health, Seed Funding Initiative and
  Research Council Award - "Engaged Climate Action". Source code and data
  available at https://github.com/RafaelaMartelo/FloodGPT-4_Prototype
ACM-class: I.2.1; I.2.7; J.2
\\
  Real-time flood forecasting plays a crucial role in enabling timely and
effective emergency responses. However, a significant challenge lies in
bridging the gap between complex numerical flood models and practical
decision-making. Decision-makers often rely on experts to interpret these
models for optimizing flood mitigation strategies. And the public requires
complex techniques to inquiry and understand socio-cultural and institutional
factors, often hinders the public's understanding of flood risks. To overcome
these challenges, our study introduces an innovative solution: a customized AI
Assistant powered by the GPT-4 Large Language Model. This AI Assistant is
designed to facilitate effective communication between decision-makers, the
general public, and flood forecasters, without the requirement of specialized
knowledge. The new framework utilizes GPT-4's advanced natural language
understanding and function calling capabilities to provide immediate flood
alerts and respond to various flood-related inquiries. Our developed prototype
integrates real-time flood warnings with flood maps and social vulnerability
data. It also effectively translates complex flood zone information into
actionable risk management advice. To assess its performance, we evaluated the
prototype using six criteria within three main categories: relevance, error
resilience, and understanding of context. Our research marks a significant step
towards a more accessible and user-friendly approach in flood risk management.
This study highlights the potential of advanced AI tools like GPT-4 in
democratizing information and enhancing public engagement in critical social
and environmental issues.
\\ ( https://arxiv.org/abs/2403.03188 ,  1651kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03203
Date: Tue, 5 Mar 2024 18:41:37 GMT   (2957kb,D)

Title: CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially
  Observable Environments
Authors: Savitha Sam Abraham and Marjan Alirezaie and Luc De Raedt
Categories: cs.AI
Comments: 17 pages, 10 images, Accepted at LREC-COLING 2024 - The 2024 Joint
  International Conference on Computational Linguistics, Language Resources and
  Evaluation
\\
  The integration of learning and reasoning is high on the research agenda in
AI. Nevertheless, there is only a little attention to use existing background
knowledge for reasoning about partially observed scenes to answer questions
about the scene. Yet, we as humans use such knowledge frequently to infer
plausible answers to visual questions (by eliminating all inconsistent ones).
Such knowledge often comes in the form of constraints about objects and it
tends to be highly domain or environment-specific. We contribute a novel
benchmark called CLEVR-POC for reasoning-intensive visual question answering
(VQA) in partially observable environments under constraints. In CLEVR-POC,
knowledge in the form of logical constraints needs to be leveraged to generate
plausible answers to questions about a hidden object in a given partial scene.
For instance, if one has the knowledge that all cups are colored either red,
green or blue and that there is only one green cup, it becomes possible to
deduce the color of an occluded cup as either red or blue, provided that all
other cups, including the green one, are observed. Through experiments, we
observe that the low performance of pre-trained vision language models like
CLIP (~ 22%) and a large language model (LLM) like GPT-4 (~ 46%) on CLEVR-POC
ascertains the necessity for frameworks that can handle reasoning-intensive
tasks where environment-specific background knowledge is available and crucial.
Furthermore, our demonstration illustrates that a neuro-symbolic model, which
integrates an LLM like GPT-4 with a visual perception network and a formal
logical reasoner, exhibits exceptional performance on CLEVR-POC.
\\ ( https://arxiv.org/abs/2403.03203 ,  2957kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02366
Date: Mon, 4 Mar 2024 11:45:46 GMT   (1740kb,D)

Title: Human Evaluation of English--Irish Transformer-Based NMT
Authors: S\'eamus Lankford, Haithem Afli and Andy Way
Categories: cs.CL cs.AI
Comments: arXiv admin note: text overlap with arXiv:2403.01985
Journal-ref: Information 2022, 13(7), 309
DOI: 10.3390/info13070309
\\
  In this study, a human evaluation is carried out on how hyperparameter
settings impact the quality of Transformer-based Neural Machine Translation
(NMT) for the low-resourced English--Irish pair. SentencePiece models using
both Byte Pair Encoding (BPE) and unigram approaches were appraised. Variations
in model architectures included modifying the number of layers, evaluating the
optimal number of heads for attention and testing various regularisation
techniques. The greatest performance improvement was recorded for a
Transformer-optimized model with a 16k BPE subword model. Compared with a
baseline Recurrent Neural Network (RNN) model, a Transformer-optimized model
demonstrated a BLEU score improvement of 7.8 points. When benchmarked against
Google Translate, our translation engines demonstrated significant
improvements. Furthermore, a quantitative fine-grained manual evaluation was
conducted which compared the performance of machine translation systems. Using
the Multidimensional Quality Metrics (MQM) error taxonomy, a human evaluation
of the error types generated by an RNN-based system and a Transformer-based
system was explored. Our findings show the best-performing Transformer system
significantly reduces both accuracy and fluency errors when compared with an
RNN-based model.
\\ ( https://arxiv.org/abs/2403.02366 ,  1740kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02367
Date: Mon, 4 Mar 2024 12:10:17 GMT   (1363kb,D)

Title: adaptNMT: an open-source, language-agnostic development environment for
  Neural Machine Translation
Authors: S\'eamus Lankford, Haithem Afli and Andy Way
Categories: cs.CL cs.AI
Journal-ref: Language Resources and Evaluation 57, 1671-1696, (2023)
DOI: 10.1007/s10579-023-09671-2
\\
  adaptNMT streamlines all processes involved in the development and deployment
of RNN and Transformer neural translation models. As an open-source
application, it is designed for both technical and non-technical users who work
in the field of machine translation. Built upon the widely-adopted OpenNMT
ecosystem, the application is particularly useful for new entrants to the field
since the setup of the development environment and creation of train,
validation and test splits is greatly simplified. Graphing, embedded within the
application, illustrates the progress of model training, and SentencePiece is
used for creating subword segmentation models. Hyperparameter customization is
facilitated through an intuitive user interface, and a single-click model
development approach has been implemented. Models developed by adaptNMT can be
evaluated using a range of metrics, and deployed as a translation service
within the application. To support eco-friendly research in the NLP space, a
green report also flags the power consumption and kgCO$_{2}$ emissions
generated during model development. The application is freely available.
\\ ( https://arxiv.org/abs/2403.02367 ,  1363kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02370
Date: Mon, 4 Mar 2024 14:49:18 GMT   (1759kb,D)

Title: adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource
  Languages with Integrated LLM Playgrounds
Authors: S\'eamus Lankford, Haithem Afli and Andy Way
Categories: cs.CL cs.AI
Journal-ref: Information 2023, 14(12), 638
DOI: 10.3390/info14120638
\\
  The advent of Multilingual Language Models (MLLMs) and Large Language Models
has spawned innovation in many areas of natural language processing. Despite
the exciting potential of this technology, its impact on developing
high-quality Machine Translation (MT) outputs for low-resource languages
remains relatively under-explored. Furthermore, an open-source application,
dedicated to both fine-tuning MLLMs and managing the complete MT workflow for
low-resources languages, remains unavailable. We aim to address these
imbalances through the development of adaptMLLM, which streamlines all
processes involved in the fine-tuning of MLLMs for MT. This open-source
application is tailored for developers, translators, and users who are engaged
in MT. An intuitive interface allows for easy customisation of hyperparameters,
and the application offers a range of metrics for model evaluation and the
capability to deploy models as a translation service directly within the
application. As a multilingual tool, we used adaptMLLM to fine-tune models for
two low-resource language pairs: English to Irish (EN$\leftrightarrow$GA) and
English to Marathi (EN$\leftrightarrow$MR). Compared with baselines from the
LoResMT2021 Shared Task, the adaptMLLM system demonstrated significant
improvements. In the EN$\rightarrow$GA direction, an improvement of 5.2 BLEU
points was observed and an increase of 40.5 BLEU points was recorded in the
GA$\rightarrow$EN direction. Significant improvements in the translation
performance of the EN$\leftrightarrow$MR pair were also observed notably in the
MR$\rightarrow$EN direction with an increase of 21.3 BLEU points. Finally, a
fine-grained human evaluation of the MLLM output on the EN$\rightarrow$GA pair
was conducted using the Multidimensional Quality Metrics and Scalar Quality
Metrics error taxonomies. The application and models are freely available.
\\ ( https://arxiv.org/abs/2403.02370 ,  1759kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02436
Date: Mon, 4 Mar 2024 19:33:39 GMT   (7940kb,D)

Title: How does Architecture Influence the Base Capabilities of Pre-trained
  Language Models? A Case Study Based on FFN-Wider Transformer Models
Authors: Xin Lu, Yanyan Zhao, Bing Qin
Categories: cs.CL
\\
  Pre-trained language models have been proven to possess strong base
capabilities, which not only excel in in-distribution language modeling but
also show powerful abilities in out-of-distribution language modeling, transfer
learning and few-shot learning. Unlike existing work focusing on the influence
of scale on base capabilities, our work examines the influence of architecture
on those. Specifically, our concern is: How does architecture influence the
base capabilities of pre-trained language models? In this work, we attempt to
explain and reverse the decline in base capabilities caused by the architecture
of FFN-Wider Transformers, seeking to provide some insights. Through analysis,
we found the contribution ratio of Multi-Head Attention (a combination
function) to pre-trained language modeling is a key factor affecting base
capabilities. FFN-Wider Transformers reduce the contribution ratio of this
combination function, leading to a decline in base capabilities. We confirmed
this by experiments and proposed Combination Enhancement Architecture (CEA) to
address the decline in base capabilities of such models. Significantly, we
extended our explanation and CEA to Mixture of Experts (MoE) architecture
Transformers, which also alleviated their decline in base capabilities to some
extent, proving our work can offer useful guidance for architecture analysis,
architecture improvement and architecture design.
\\ ( https://arxiv.org/abs/2403.02436 ,  7940kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02451
Date: Mon, 4 Mar 2024 20:07:17 GMT   (7899kb,D)

Title: Views Are My Own, But Also Yours: Benchmarking Theory of Mind using
  Common Ground
Authors: Adil Soubki, John Murzaku, Arash Yousefi Jordehi, Peter Zeng,
  Magdalena Markowska, Seyed Abolghasem Mirroshandel, Owen Rambow
Categories: cs.CL
\\
  Evaluating the theory of mind (ToM) capabilities of language models (LMs) has
recently received much attention. However, many existing benchmarks rely on
synthetic data which risks misaligning the resulting experiments with human
behavior. We introduce the first ToM dataset based on naturally occurring
spoken dialogs, Common-ToM, and show that LMs struggle to demonstrate ToM. We
then show that integrating a simple, explicit representation of beliefs
improves LM performance on Common-ToM.
\\ ( https://arxiv.org/abs/2403.02451 ,  7899kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02472
Date: Mon, 4 Mar 2024 20:34:58 GMT   (300kb,D)

Title: OffLanDat: A Community Based Implicit Offensive Language Dataset
  Generated by Large Language Model Through Prompt Engineering
Authors: Amit Das, Mostafa Rahgouy, Dongji Feng, Zheng Zhang, Tathagata
  Bhattacharya, Nilanjana Raychawdhary, Mary Sandage, Lauramarie Pope, Gerry
  Dozier and Cheryl Seals
Categories: cs.CL
\\
  The widespread presence of offensive languages on social media has resulted
in adverse effects on societal well-being. As a result, it has become very
important to address this issue with high priority. Offensive languages exist
in both explicit and implicit forms, with the latter being more challenging to
detect. Current research in this domain encounters several challenges. Firstly,
the existing datasets primarily rely on the collection of texts containing
explicit offensive keywords, making it challenging to capture implicitly
offensive contents that are devoid of these keywords. Secondly, usual
methodologies tend to focus solely on textual analysis, neglecting the valuable
insights that community information can provide. In this research paper, we
introduce a novel dataset OffLanDat, a community based implicit offensive
language dataset generated by ChatGPT containing data for 38 different target
groups. Despite limitations in generating offensive texts using ChatGPT due to
ethical constraints, we present a prompt-based approach that effectively
generates implicit offensive languages. To ensure data quality, we evaluate our
data with human. Additionally, we employ a prompt-based Zero-Shot method with
ChatGPT and compare the detection results between human annotation and ChatGPT
annotation. We utilize existing state-of-the-art models to see how effective
they are in detecting such languages. We will make our code and dataset public
for other researchers.
\\ ( https://arxiv.org/abs/2403.02472 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02474
Date: Mon, 4 Mar 2024 20:39:21 GMT   (1651kb,D)

Title: The Emotion Dynamics of Literary Novels
Authors: Krishnapriya Vishnubhotla, Adam Hammond, Graeme Hirst, Saif M.
  Mohammad
Categories: cs.CL
Comments: 8 pages plus appendices
\\
  Stories are rich in the emotions they exhibit in their narratives and evoke
in the readers. The emotional journeys of the various characters within a story
are central to their appeal. Computational analysis of the emotions of novels,
however, has rarely examined the variation in the emotional trajectories of the
different characters within them, instead considering the entire novel to
represent a single story arc. In this work, we use character dialogue to
distinguish between the emotion arcs of the narration and the various
characters. We analyze the emotion arcs of the various characters in a dataset
of English literary novels using the framework of Utterance Emotion Dynamics.
Our findings show that the narration and the dialogue largely express disparate
emotions through the course of a novel, and that the commonalities or
differences in the emotional arcs of stories are more accurately captured by
those associated with individual characters.
\\ ( https://arxiv.org/abs/2403.02474 ,  1651kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02496
Date: Mon, 4 Mar 2024 21:43:59 GMT   (2158kb)

Title: Choose Your Own Adventure: Interactive E-Books to Improve Word Knowledge
  and Comprehension Skills
Authors: Stephanie Day, Jin K. Hwang, Tracy Arner, Danielle McNamara, Carol
  Connor
Categories: cs.CL
\\
  The purpose of this feasibility study was to examine the potential impact of
reading digital interactive e-books on essential skills that support reading
comprehension with third-fifth grade students. Students read two e-Books that
taught word learning and comprehension monitoring strategies in the service of
learning difficult vocabulary and targeted science concepts about hurricanes.
We investigated whether specific comprehension strategies including word
learning and strategies that supported general reading comprehension,
summarization, and question generation, show promise of effectiveness in
building vocabulary knowledge and comprehension skills in the e-Books. Students
were assigned to read one of three versions of each of the e-Books, each
version implemented one strategy. The books employed a choose-your-adventure
format with embedded comprehension questions that provided students with
immediate feedback on their responses. Paired samples t-tests were run to
examine pre-to-post differences in learning the targeted vocabulary and science
concepts taught in both e-Books. For both e-Books, students demonstrated
significant gains in word learning and on the targeted hurricane concepts.
Additionally, Hierarchical Linear Modeling (HLM) revealed that no one strategy
was more associated with larger gains than the other. Performance on the
embedded questions in the books was also associated with greater posttest
outcomes for both e-Books. This work discusses important considerations for
implementation and future development of e-books that can enhance student
engagement and improve reading comprehension.
\\ ( https://arxiv.org/abs/2403.02496 ,  2158kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02502
Date: Mon, 4 Mar 2024 21:50:29 GMT   (2909kb,D)

Title: Trial and Error: Exploration-Based Trajectory Optimization for LLM
  Agents
Authors: Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, Bill Yuchen Lin
Categories: cs.CL cs.AI cs.LG
\\
  Large Language Models (LLMs) have become integral components in various
autonomous agent systems. In this study, we present an exploration-based
trajectory optimization approach, referred to as ETO. This learning method is
designed to enhance the performance of open LLM agents. Contrary to previous
studies that exclusively train on successful expert trajectories, our method
allows agents to learn from their exploration failures. This leads to improved
performance through an iterative optimization framework. During the exploration
phase, the agent interacts with the environment while completing given tasks,
gathering failure trajectories to create contrastive trajectory pairs. In the
subsequent training phase, the agent utilizes these trajectory preference pairs
to update its policy using contrastive learning methods like DPO. This
iterative cycle of exploration and training fosters continued improvement in
the agents. Our experiments on three complex tasks demonstrate that ETO
consistently surpasses baseline performance by a large margin. Furthermore, an
examination of task-solving efficiency and potential in scenarios lacking
expert trajectory underscores the effectiveness of our approach.
\\ ( https://arxiv.org/abs/2403.02502 ,  2909kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02504
Date: Mon, 4 Mar 2024 21:51:11 GMT   (1091kb,D)

Title: A Tutorial on the Pretrain-Finetune Paradigm for Natural Language
  Processing
Authors: Yu Wang
Categories: cs.CL cs.AI
Comments: 16 pages, 6 figures, 2 tables
\\
  The pretrain-finetune paradigm represents a transformative approach in
natural language processing (NLP). This paradigm distinguishes itself through
the use of large pretrained language models, demonstrating remarkable
efficiency in finetuning tasks, even with limited training data. This
efficiency is especially beneficial for research in social sciences, where the
number of annotated samples is often quite limited. Our tutorial offers a
comprehensive introduction to the pretrain-finetune paradigm. We first delve
into the fundamental concepts of pretraining and finetuning, followed by
practical exercises using real-world applications. We demonstrate the
application of the paradigm across various tasks, including multi-class
classification and regression. Emphasizing its efficacy and user-friendliness,
the tutorial aims to encourage broader adoption of this paradigm. To this end,
we have provided open access to all our code and datasets. The tutorial is
particularly valuable for quantitative researchers in psychology, offering them
an insightful guide into this innovative approach.
\\ ( https://arxiv.org/abs/2403.02504 ,  1091kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02509
Date: Mon, 4 Mar 2024 21:55:22 GMT   (1452kb,D)

Title: SPUQ: Perturbation-Based Uncertainty Quantification for Large Language
  Models
Authors: Xiang Gao, Jiaxin Zhang, Lalla Mouatadid, Kamalika Das
Categories: cs.CL cs.AI
Comments: Accepted to appear at EACL 2024
\\
  In recent years, large language models (LLMs) have become increasingly
prevalent, offering remarkable text generation capabilities. However, a
pressing challenge is their tendency to make confidently wrong predictions,
highlighting the critical need for uncertainty quantification (UQ) in LLMs.
While previous works have mainly focused on addressing aleatoric uncertainty,
the full spectrum of uncertainties, including epistemic, remains inadequately
explored. Motivated by this gap, we introduce a novel UQ method, sampling with
perturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic
uncertainties. The method entails generating a set of perturbations for LLM
inputs, sampling outputs for each perturbation, and incorporating an
aggregation module that generalizes the sampling uncertainty approach for text
generation tasks. Through extensive experiments on various datasets, we
investigated different perturbation and aggregation techniques. Our findings
show a substantial improvement in model uncertainty calibration, with a
reduction in Expected Calibration Error (ECE) by 50\% on average. Our findings
suggest that our proposed UQ method offers promising steps toward enhancing the
reliability and trustworthiness of LLMs.
\\ ( https://arxiv.org/abs/2403.02509 ,  1452kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02513
Date: Mon, 4 Mar 2024 22:02:12 GMT   (710kb,D)

Title: Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing
  Conversational LLMs with Direct RLHF
Authors: Chen Zheng, Ke Sun, Hang Wu, Chenguang Xi, Xun Zhou
Categories: cs.CL
\\
  In recent advancements in Conversational Large Language Models (LLMs), a
concerning trend has emerged, showing that many new base LLMs experience a
knowledge reduction in their foundational capabilities following Supervised
Fine-Tuning (SFT). This process often leads to issues such as forgetting or a
decrease in the base model's abilities. Moreover, fine-tuned models struggle to
align with user preferences, inadvertently increasing the generation of toxic
outputs when specifically prompted. To overcome these challenges, we adopted an
innovative approach by completely bypassing SFT and directly implementing
Harmless Reinforcement Learning from Human Feedback (RLHF). Our method not only
preserves the base model's general capabilities but also significantly enhances
its conversational abilities, while notably reducing the generation of toxic
outputs. Our approach holds significant implications for fields that demand a
nuanced understanding and generation of responses, such as customer service. We
applied this methodology to Mistral, the most popular base model, thereby
creating Mistral-Plus. Our validation across 11 general tasks demonstrates that
Mistral-Plus outperforms similarly sized open-source base models and their
corresponding instruct versions. Importantly, the conversational abilities of
Mistral-Plus were significantly improved, indicating a substantial advancement
over traditional SFT models in both safety and user preference alignment.
\\ ( https://arxiv.org/abs/2403.02513 ,  710kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02528
Date: Mon, 4 Mar 2024 22:47:58 GMT   (3442kb,D)

Title: DACO: Towards Application-Driven and Comprehensive Data Analysis via
  Code Generation
Authors: Xueqing Wu, Rui Zheng, Jingzhen Sha, Te-Lin Wu, Hanyu Zhou, Mohan
  Tang, Kai-Wei Chang, Nanyun Peng, Haoran Huang
Categories: cs.CL cs.AI
\\
  Data analysis is a crucial analytical process to generate in-depth studies
and conclusive insights to comprehensively answer a given user query for
tabular data. In this work, we aim to propose new resources and benchmarks to
inspire future research on this crucial yet challenging and under-explored
task. However, collecting data analysis annotations curated by experts can be
prohibitively expensive. We propose to automatically generate high-quality
answer annotations leveraging the code-generation capabilities of LLMs with a
multi-turn prompting technique. We construct the DACO dataset, containing (1)
440 databases (of tabular data) collected from real-world scenarios, (2) ~2k
query-answer pairs that can serve as weak supervision for model training, and
(3) a concentrated but high-quality test set with human refined annotations
that serves as our main evaluation benchmark. We train a 6B supervised
fine-tuning (SFT) model on DACO dataset, and find that the SFT model learns
reasonable data analysis capabilities. To further align the models with human
preference, we use reinforcement learning to encourage generating analysis
perceived by human as helpful, and design a set of dense rewards to propagate
the sparse human preference reward to intermediate code generation steps. Our
DACO-RL algorithm is evaluated by human annotators to produce more helpful
answers than SFT model in 57.72% cases, validating the effectiveness of our
proposed algorithm. Data and code are released at
https://github.com/shirley-wu/daco
\\ ( https://arxiv.org/abs/2403.02528 ,  3442kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02558
Date: Tue, 5 Mar 2024 00:27:43 GMT   (168kb)

Title: Updating the Minimum Information about CLinical Artificial Intelligence
  (MI-CLAIM) checklist for generative modeling research
Authors: Brenda Y. Miao, Irene Y. Chen, Christopher YK Williams, Jays\'on
  Davidson, Augusto Garcia-Agundez, Harry Sun, Travis Zack, Atul J. Butte,
  Madhumita Sushil
Categories: cs.CL cs.CV
\\
  Recent advances in generative models, including large language models (LLMs),
vision language models (VLMs), and diffusion models, have accelerated the field
of natural language and image processing in medicine and marked a significant
paradigm shift in how biomedical models can be developed and deployed. While
these models are highly adaptable to new tasks, scaling and evaluating their
usage presents new challenges not addressed in previous frameworks. In
particular, the ability of these models to produce useful outputs with little
to no specialized training data ("zero-" or "few-shot" approaches), as well as
the open-ended nature of their outputs, necessitate the development of updated
guidelines in using and evaluating these models. In response to gaps in
standards and best practices for the development of clinical AI tools
identified by US Executive Order 141103 and several emerging national networks
for clinical AI evaluation, we begin to formalize some of these guidelines by
building on the "Minimum information about clinical artificial intelligence
modeling" (MI-CLAIM) checklist. The MI-CLAIM checklist, originally developed in
2020, provided a set of six steps with guidelines on the minimum information
necessary to encourage transparent, reproducible research for artificial
intelligence (AI) in medicine. Here, we propose modifications to the original
checklist that highlight differences in training, evaluation, interpretability,
and reproducibility of generative models compared to traditional AI models for
clinical research. This updated checklist also seeks to clarify cohort
selection reporting and adds additional items on alignment with ethical
standards.
\\ ( https://arxiv.org/abs/2403.02558 ,  168kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02567
Date: Tue, 5 Mar 2024 00:48:56 GMT   (1195kb,D)

Title: Eliciting Better Multilingual Structured Reasoning from LLMs through
  Code
Authors: Bryan Li and Tamer Alkhouli and Daniele Bonadiman and Nikolaos Pappas
  and Saab Mansour
Categories: cs.CL cs.AI
\\
  Development of large language models (LLM) have shown progress on reasoning,
though studies have been limited to English or simple reasoning tasks. We thus
introduce a multilingual structured reasoning and explanation dataset, termed
xSTREET, that covers four tasks across six languages. xSTREET exposes a gap in
base LLM performance between English and non-English reasoning tasks. We then
propose two methods to remedy this gap, building on the insight that LLMs
trained on code are better reasoners. First, at training time, we augment a
code dataset with multi-lingual comments using machine translation while
keeping program code as-is. Second, at inference time, we bridge the gap
between training and inference by employing a prompt structure that
incorporates step-by-step code primitives to derive new facts and find a
solution. Our methods show improved multilingual performance on xSTREET, most
notably on the scientific commonsense reasoning subtask. Furthermore, the
models show no regression on non-reasoning tasks, thus showing our techniques
maintain general-purpose abilities.
\\ ( https://arxiv.org/abs/2403.02567 ,  1195kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02586
Date: Tue, 5 Mar 2024 01:46:50 GMT   (6115kb,D)

Title: Improving Event Definition Following For Zero-Shot Event Detection
Authors: Zefan Cai, Po-Nien Kung, Ashima Suvarna, Mingyu Derek Ma, Hritik
  Bansal, Baobao Chang, P. Jeffrey Brantingham, Wei Wang, Nanyun Peng
Categories: cs.CL
\\
  Existing approaches on zero-shot event detection usually train models on
datasets annotated with known event types, and prompt them with unseen event
definitions. These approaches yield sporadic successes, yet generally fall
short of expectations. In this work, we aim to improve zero-shot event
detection by training models to better follow event definitions. We hypothesize
that a diverse set of event types and definitions are the key for models to
learn to follow event definitions while existing event extraction datasets
focus on annotating many high-quality examples for a few event types. To verify
our hypothesis, we construct an automatically generated Diverse Event
Definition (DivED) dataset and conduct comparative studies. Our experiments
reveal that a large number of event types (200) and diverse event definitions
can significantly boost event extraction performance; on the other hand, the
performance does not scale with over ten examples per event type. Beyond
scaling, we incorporate event ontology information and hard-negative samples
during training, further boosting the performance. Based on these findings, we
fine-tuned a LLaMA-2-7B model on our DivED dataset, yielding performance that
surpasses SOTA large language models like GPT-3.5 across three open benchmarks
on zero-shot event detection.
\\ ( https://arxiv.org/abs/2403.02586 ,  6115kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02615
Date: Tue, 5 Mar 2024 03:07:10 GMT   (1061kb,D)

Title: Exploring the Limitations of Large Language Models in Compositional
  Relation Reasoning
Authors: Jinman Zhao, Xueyan Zhang
Categories: cs.CL
Comments: 20 pages, 7 figures, 7 tables, submitted to ICML 2024
\\
  We present a comprehensive evaluation of large language models(LLMs)' ability
to reason about composition relations through a benchmark encompassing 1,500
test cases in English, designed to cover six distinct types of composition
relations: Positional, Comparative, Personal, Mathematical, Identity, and
Other. Acknowledging the significance of multilingual capabilities, we expanded
our assessment to include translations of these cases into Chinese, Japanese,
French, and Korean. Our Multilingual Composition Relation (MCR) benchmark aims
at investigating the robustness and adaptability of LLMs in handling
composition relation reasoning across diverse linguistic contexts.
\\ ( https://arxiv.org/abs/2403.02615 ,  1061kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02647
Date: Tue, 5 Mar 2024 04:33:36 GMT   (1233kb,D)

Title: FinReport: Explainable Stock Earnings Forecasting via News Factor
  Analyzing Model
Authors: Xiangyu Li, Xinjie Shen, Yawen Zeng, Xiaofen Xing, Jin Xu
Categories: cs.CL cs.AI
Comments: Accepted by WWW 2024
\\
  The task of stock earnings forecasting has received considerable attention
due to the demand investors in real-world scenarios. However, compared with
financial institutions, it is not easy for ordinary investors to mine factors
and analyze news. On the other hand, although large language models in the
financial field can serve users in the form of dialogue robots, it still
requires users to have financial knowledge to ask reasonable questions. To
serve the user experience, we aim to build an automatic system, FinReport, for
ordinary investors to collect information, analyze it, and generate reports
after summarizing.
  Specifically, our FinReport is based on financial news announcements and a
multi-factor model to ensure the professionalism of the report. The FinReport
consists of three modules: news factorization module, return forecasting
module, risk assessment module. The news factorization module involves
understanding news information and combining it with stock factors, the return
forecasting module aim to analysis the impact of news on market sentiment, and
the risk assessment module is adopted to control investment risk. Extensive
experiments on real-world datasets have well verified the effectiveness and
explainability of our proposed FinReport. Our codes and datasets are available
at https://github.com/frinkleko/FinReport.
\\ ( https://arxiv.org/abs/2403.02647 ,  1233kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02674
Date: Tue, 5 Mar 2024 05:53:09 GMT   (1405kb,D)

Title: Revisiting Meta-evaluation for Grammatical Error Correction
Authors: Masamune Kobayashi, Masato Mita, Mamoru Komachi
Categories: cs.CL
Comments: Accepted to TACL. This arXiv version is a pre-MIT Press publication
  version
\\
  Metrics are the foundation for automatic evaluation in grammatical error
correction (GEC), with their evaluation of the metrics (meta-evaluation)
relying on their correlation with human judgments. However, conventional
meta-evaluations in English GEC encounter several challenges including biases
caused by inconsistencies in evaluation granularity, and an outdated setup
using classical systems. These problems can lead to misinterpretation of
metrics and potentially hinder the applicability of GEC techniques. To address
these issues, this paper proposes SEEDA, a new dataset for GEC meta-evaluation.
SEEDA consists of corrections with human ratings along two different
granularities: edit-based and sentence-based, covering 12 state-of-the-art
systems including large language models (LLMs), and two human corrections with
different focuses. The results of improved correlations by aligning the
granularity in the sentence-level meta-evaluation, suggest that edit-based
metrics may have been underestimated in existing studies. Furthermore,
correlations of most metrics decrease when changing from classical to neural
systems, indicating that traditional metrics are relatively poor at evaluating
fluently corrected sentences with many edits.
\\ ( https://arxiv.org/abs/2403.02674 ,  1405kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02691
Date: Tue, 5 Mar 2024 06:21:45 GMT   (2149kb,D)

Title: InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated
  Large Language Model Agents
Authors: Qiusi Zhan, Zhixiang Liang, Zifan Ying, Daniel Kang
Categories: cs.CL cs.CR
Comments: 26 pages, 5 figures, 7 tables
\\
  Recent work has embodied LLMs as agents, allowing them to access tools,
perform actions, and interact with external content (e.g., emails or websites).
However, external content introduces the risk of indirect prompt injection
(IPI) attacks, where malicious instructions are embedded within the content
processed by LLMs, aiming to manipulate these agents into executing detrimental
actions against users. Given the potentially severe consequences of such
attacks, establishing benchmarks to assess and mitigate these risks is
imperative.
  In this work, we introduce InjecAgent, a benchmark designed to assess the
vulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent
comprises 1,054 test cases covering 17 different user tools and 62 attacker
tools. We categorize attack intentions into two primary types: direct harm to
users and exfiltration of private data. We evaluate 30 different LLM agents and
show that agents are vulnerable to IPI attacks, with ReAct-prompted GPT-4
vulnerable to attacks 24% of the time. Further investigation into an enhanced
setting, where the attacker instructions are reinforced with a hacking prompt,
shows additional increases in success rates, nearly doubling the attack success
rate on the ReAct-prompted GPT-4. Our findings raise questions about the
widespread deployment of LLM Agents. Our benchmark is available at
https://github.com/uiuc-kang-lab/InjecAgent.
\\ ( https://arxiv.org/abs/2403.02691 ,  2149kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02698
Date: Tue, 5 Mar 2024 06:28:02 GMT   (758kb,D)

Title: Causal Walk: Debiasing Multi-Hop Fact Verification with Front-Door
  Adjustment
Authors: Congzhi Zhang, Linhai Zhang, Deyu Zhou
Categories: cs.CL
Comments: Accepted by AAAI 2024
\\
  Conventional multi-hop fact verification models are prone to rely on spurious
correlations from the annotation artifacts, leading to an obvious performance
decline on unbiased datasets. Among the various debiasing works, the causal
inference-based methods become popular by performing theoretically guaranteed
debiasing such as casual intervention or counterfactual reasoning. However,
existing causal inference-based debiasing methods, which mainly formulate fact
verification as a single-hop reasoning task to tackle shallow bias patterns,
cannot deal with the complicated bias patterns hidden in multiple hops of
evidence. To address the challenge, we propose Causal Walk, a novel method for
debiasing multi-hop fact verification from a causal perspective with front-door
adjustment. Specifically, in the structural causal model, the reasoning path
between the treatment (the input claim-evidence graph) and the outcome (the
veracity label) is introduced as the mediator to block the confounder. With the
front-door adjustment, the causal effect between the treatment and the outcome
is decomposed into the causal effect between the treatment and the mediator,
which is estimated by applying the idea of random walk, and the causal effect
between the mediator and the outcome, which is estimated with normalized
weighted geometric mean approximation. To investigate the effectiveness of the
proposed method, an adversarial multi-hop fact verification dataset and a
symmetric multi-hop fact verification dataset are proposed with the help of the
large language model. Experimental results show that Causal Walk outperforms
some previous debiasing methods on both existing datasets and the newly
constructed datasets. Code and data will be released at
https://github.com/zcccccz/CausalWalk.
\\ ( https://arxiv.org/abs/2403.02698 ,  758kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02712
Date: Tue, 5 Mar 2024 07:08:06 GMT   (43kb,D)

Title: Breeze-7B Technical Report
Authors: Chan-Jan Hsu, Chang-Le Liu, Feng-Ting Liao, Po-Chun Hsu, Yi-Chang
  Chen, Da-Shan Shiu
Categories: cs.CL
\\
  Breeze-7B is an open-source language model based on Mistral-7B, designed to
address the need for improved language comprehension and chatbot-oriented
capabilities in Traditional Chinese. This technical report provides an overview
of the additional pretraining, finetuning, and evaluation stages for the
Breeze-7B model. The Breeze-7B family of base and chat models exhibits good
performance on language comprehension and chatbot-oriented tasks, reaching the
top in several benchmarks among models comparable in its complexity class.
\\ ( https://arxiv.org/abs/2403.02712 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02713
Date: Tue, 5 Mar 2024 07:09:35 GMT   (3795kb,D)

Title: Android in the Zoo: Chain-of-Action-Thought for GUI Agents
Authors: Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao,
  Zhongyu Wei, Duyu Tang
Categories: cs.CL cs.CV cs.HC cs.LG
Comments: Dataset could be found in https://github.com/IMNearth/CoAT
\\
  Large language model (LLM) leads to a surge of autonomous GUI agents for
smartphone, which completes a task triggered by natural language through
predicting a sequence of actions of API. Even though the task highly relies on
past actions and visual observations, existing studies typical consider little
semantic information carried out by intermediate screenshots and screen
operations. To address this, this work presents Chain-of-Action-Thought (dubbed
CoAT), which takes the description of the previous actions, the current screen,
and more importantly the action thinking of what actions should be performed
and the outcomes led by the chosen action. We demonstrate that, in a zero-shot
setting upon an off-the-shell LLM, CoAT significantly improves the goal
progress compared to standard context modeling. To further facilitate the
research in this line, we construct a benchmark Android-In-The-Zoo (AitZ),
which contains 18,643 screen-action pairs together with chain-of-action-thought
annotations. Experiments show that fine-tuning a 200M model on our AitZ dataset
achieves on par performance with CogAgent-Chat-18B.
\\ ( https://arxiv.org/abs/2403.02713 ,  3795kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02715
Date: Tue, 5 Mar 2024 07:13:28 GMT   (84kb,D)

Title: Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of
  Vietnamese Large Language Models
Authors: Sang T. Truong, Duc Q. Nguyen, Toan Nguyen, Dong D. Le, Nhi N. Truong,
  Tho Quan, Sanmi Koyejo
Categories: cs.CL cs.AI
Comments: 33 pages
MSC-class: 68T50
\\
  Recent advancements in large language models (LLMs) have underscored their
importance in the evolution of artificial intelligence. However, despite
extensive pretraining on multilingual datasets, available open-sourced LLMs
exhibit limited effectiveness in processing Vietnamese. The challenge is
exacerbated by the absence of systematic benchmark datasets and metrics
tailored for Vietnamese LLM evaluation. To mitigate these issues, we have
finetuned LLMs specifically for Vietnamese and developed a comprehensive
evaluation framework encompassing 10 common tasks and 31 metrics. Our
evaluation results reveal that the fine-tuned LLMs exhibit enhanced
comprehension and generative capabilities in Vietnamese. Moreover, our analysis
indicates that models with more parameters can introduce more biases and
uncalibrated outputs and the key factor influencing LLM performance is the
quality of the training or fine-tuning datasets. These insights underscore the
significance of meticulous fine-tuning with high-quality datasets in enhancing
LLM performance.
\\ ( https://arxiv.org/abs/2403.02715 ,  84kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02718
Date: Tue, 5 Mar 2024 07:16:51 GMT   (1762kb,D)

Title: DP-CRE: Continual Relation Extraction via Decoupled Contrastive Learning
  and Memory Structure Preservation
Authors: Mengyi Huang, Meng Xiao, Ludi Wang, Yi Du
Categories: cs.CL
Comments: Accepted By LREC-Coling-2024, 10 pages with 2 pages of appendix
\\
  Continuous Relation Extraction (CRE) aims to incrementally learn relation
knowledge from a non-stationary stream of data. Since the introduction of new
relational tasks can overshadow previously learned information, catastrophic
forgetting becomes a significant challenge in this domain. Current replay-based
training paradigms prioritize all data uniformly and train memory samples
through multiple rounds, which would result in overfitting old tasks and
pronounced bias towards new tasks because of the imbalances of the replay set.
To handle the problem, we introduce the DecouPled CRE (DP-CRE) framework that
decouples the process of prior information preservation and new knowledge
acquisition. This framework examines alterations in the embedding space as new
relation classes emerge, distinctly managing the preservation and acquisition
of knowledge. Extensive experiments show that DP-CRE significantly outperforms
other CRE baselines across two datasets.
\\ ( https://arxiv.org/abs/2403.02718 ,  1762kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02727
Date: Tue, 5 Mar 2024 07:34:51 GMT   (2188kb,D)

Title: HARGPT: Are LLMs Zero-Shot Human Activity Recognizers?
Authors: Sijie Ji, Xinzhe Zheng, Chenshu Wu
Categories: cs.CL cs.AI cs.HC
\\
  There is an ongoing debate regarding the potential of Large Language Models
(LLMs) as foundational models seamlessly integrated with Cyber-Physical Systems
(CPS) for interpreting the physical world. In this paper, we carry out a case
study to answer the following question: Are LLMs capable of zero-shot human
activity recognition (HAR). Our study, HARGPT, presents an affirmative answer
by demonstrating that LLMs can comprehend raw IMU data and perform HAR tasks in
a zero-shot manner, with only appropriate prompts. HARGPT inputs raw IMU data
into LLMs and utilizes the role-play and think step-by-step strategies for
prompting. We benchmark HARGPT on GPT4 using two public datasets of different
inter-class similarities and compare various baselines both based on
traditional machine learning and state-of-the-art deep classification models.
Remarkably, LLMs successfully recognize human activities from raw IMU data and
consistently outperform all the baselines on both datasets. Our findings
indicate that by effective prompting, LLMs can interpret raw IMU data based on
their knowledge base, possessing a promising potential to analyze raw sensor
data of the physical world effectively.
\\ ( https://arxiv.org/abs/2403.02727 ,  2188kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02738
Date: Tue, 5 Mar 2024 07:47:34 GMT   (322kb,D)

Title: Causal Prompting: Debiasing Large Language Model Prompting based on
  Front-Door Adjustment
Authors: Congzhi Zhang, Linhai Zhang, Deyu Zhou, Guoqiang Xu
Categories: cs.CL
\\
  Despite the significant achievements of existing prompting methods such as
in-context learning and chain-of-thought for large language models (LLMs), they
still face challenges of various biases. Traditional debiasing methods
primarily focus on the model training stage, including data augmentation-based
and reweight-based approaches, with the limitations of addressing the complex
biases of LLMs. To address such limitations, the causal relationship behind the
prompting methods is uncovered using a structural causal model, and a novel
causal prompting method based on front-door adjustment is proposed to
effectively mitigate the bias of LLMs. In specific, causal intervention is
implemented by designing the prompts without accessing the parameters and
logits of LLMs.The chain-of-thoughts generated by LLMs are employed as the
mediator variable and the causal effect between the input prompt and the output
answers is calculated through front-door adjustment to mitigate model biases.
Moreover, to obtain the representation of the samples precisely and estimate
the causal effect more accurately, contrastive learning is used to fine-tune
the encoder of the samples by aligning the space of the encoder with the LLM.
Experimental results show that the proposed causal prompting approach achieves
excellent performance on 3 natural language processing datasets on both
open-source and closed-source LLMs.
\\ ( https://arxiv.org/abs/2403.02738 ,  322kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02742
Date: Tue, 5 Mar 2024 07:53:49 GMT   (1775kb,D)

Title: Towards Training A Chinese Large Language Model for Anesthesiology
Authors: Zhonghai Wang, Jie Jiang, Yibing Zhan, Bohao Zhou, Yanhong Li, Chong
  Zhang, Liang Ding, Hua Jin, Jun Peng, Xu Lin, and Weifeng Liu
Categories: cs.CL
\\
  Medical large language models (LLMs) have gained popularity recently due to
their significant practical utility. However, most existing research focuses on
general medicine, and there is a need for in-depth study of LLMs in specific
fields like anesthesiology. To fill the gap, we introduce Hypnos, a Chinese
Anesthesia model built upon existing LLMs, e.g., Llama. Hypnos' contributions
have three aspects: 1) The data, such as utilizing Self-Instruct, acquired from
current LLMs likely includes inaccuracies. Hypnos implements a cross-filtering
strategy to improve the data quality. This strategy involves using one LLM to
assess the quality of the generated data from another LLM and filtering out the
data with low quality. 2) Hypnos employs a general-to-specific training
strategy that starts by fine-tuning LLMs using the general medicine data and
subsequently improving the fine-tuned LLMs using data specifically from
Anesthesiology. The general medical data supplement the medical expertise in
Anesthesiology and enhance the effectiveness of Hypnos' generation. 3) We
introduce a standardized benchmark for evaluating medical LLM in
Anesthesiology. Our benchmark includes both publicly available instances from
the Internet and privately obtained cases from the Hospital. Hypnos outperforms
other medical LLMs in anesthesiology in metrics, GPT-4, and human evaluation on
the benchmark dataset.
\\ ( https://arxiv.org/abs/2403.02742 ,  1775kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02756
Date: Tue, 5 Mar 2024 08:22:41 GMT   (1409kb,D)

Title: Role Prompting Guided Domain Adaptation with General Capability Preserve
  for Large Language Models
Authors: Rui Wang, Fei Mi, Yi Chen, Boyang Xue, Hongru Wang, Qi Zhu, Kam-Fai
  Wong, Ruifeng Xu
Categories: cs.CL
\\
  The growing interest in Large Language Models (LLMs) for specialized
applications has revealed a significant challenge: when tailored to specific
domains, LLMs tend to experience catastrophic forgetting, compromising their
general capabilities and leading to a suboptimal user experience. Additionally,
crafting a versatile model for multiple domains simultaneously often results in
a decline in overall performance due to confusion between domains. In response
to these issues, we present the RolE Prompting Guided Multi-Domain Adaptation
(REGA) strategy. This novel approach effectively manages multi-domain LLM
adaptation through three key components: 1) Self-Distillation constructs and
replays general-domain exemplars to alleviate catastrophic forgetting. 2) Role
Prompting assigns a central prompt to the general domain and a unique role
prompt to each specific domain to minimize inter-domain confusion during
training. 3) Role Integration reuses and integrates a small portion of
domain-specific data to the general-domain data, which are trained under the
guidance of the central prompt. The central prompt is used for a streamlined
inference process, removing the necessity to switch prompts for different
domains. Empirical results demonstrate that REGA effectively alleviates
catastrophic forgetting and inter-domain confusion. This leads to improved
domain-specific performance compared to standard fine-tuned models, while still
preserving robust general capabilities.
\\ ( https://arxiv.org/abs/2403.02756 ,  1409kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02757
Date: Tue, 5 Mar 2024 08:25:11 GMT   (4502kb,D)

Title: In-Memory Learning: A Declarative Learning Framework for Large Language
  Models
Authors: Bo Wang, Tianxiang Sun, Hang Yan, Siyin Wang, Qingyuan Cheng, Xipeng
  Qiu
Categories: cs.CL
\\
  The exploration of whether agents can align with their environment without
relying on human-labeled data presents an intriguing research topic. Drawing
inspiration from the alignment process observed in intelligent organisms, where
declarative memory plays a pivotal role in summarizing past experiences, we
propose a novel learning framework. The agents adeptly distill insights from
past experiences, refining and updating existing notes to enhance their
performance in the environment. This entire process transpires within the
memory components and is implemented through natural language, so we character
this framework as In-memory Learning. We also delve into the key features of
benchmarks designed to evaluate the self-improvement process. Through
systematic experiments, we demonstrate the effectiveness of our framework and
provide insights into this problem.
\\ ( https://arxiv.org/abs/2403.02757 ,  4502kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02799
Date: Tue, 5 Mar 2024 09:12:49 GMT   (6204kb,D)

Title: DPPA: Pruning Method for Large Language Model to Model Merging
Authors: Yaochen Zhu, Rui Xia, Jiajun Zhang
Categories: cs.CL cs.AI
\\
  Model merging is to combine fine-tuned models derived from multiple domains,
with the intent of enhancing the model's proficiency across various domains.
The principal concern is the resolution of parameter conflicts. A substantial
amount of existing research remedy this issue during the merging stage, with
the latest study focusing on resolving this issue throughout the pruning stage.
The DARE approach has exhibited promising outcomes when applied to a simplistic
fine-tuned model. However, the efficacy of this method tends to wane when
employed on complex fine-tuned models that show a significant parameter bias
relative to the baseline model. In this paper, we introduce a dual-stage method
termed Dynamic Pruning Partition Amplification (DPPA), devised to tackle the
challenge of merging complex fine-tuned models. Initially, we introduce
Dynamically Pruning (DP), an improved approach based on magnitude pruning,
which aim is to enhance performance at higher pruning rates. Subsequently, we
propose Dynamically Partition Amplification (DPA), a rescaling strategy, is
designed to dynamically amplify parameter partitions in relation to their
significance levels. The experimental results show that our method maintains a
mere 20% of domain-specific parameters and yet delivers a performance
comparable to other methodologies that preserve up to 90% of parameters.
Furthermore, our method displays outstanding performance post-pruning, leading
to a significant improvement of nearly 20% performance in model merging. We
make our code on Github.
\\ ( https://arxiv.org/abs/2403.02799 ,  6204kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02839
Date: Tue, 5 Mar 2024 10:20:52 GMT   (2892kb,D)

Title: An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned
  Judge Models are Task-specific Classifiers
Authors: Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, Tiejun Zhao
Categories: cs.CL
\\
  Recently, there has been a growing trend of utilizing Large Language Model
(LLM) to evaluate the quality of other LLMs. Many studies have employed
proprietary close-source models, especially GPT4, as the evaluator.
Alternatively, other works have fine-tuned judge models based on open-source
LLMs as the evaluator. In this study, we conduct an empirical study of
different judge models on their evaluation capability. Our findings indicate
that although the fine-tuned judge models achieve high accuracy on in-domain
test sets, even surpassing GPT4, they are inherently task-specific classifiers,
and their generalizability and fairness severely underperform GPT4.
\\ ( https://arxiv.org/abs/2403.02839 ,  2892kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02884
Date: Tue, 5 Mar 2024 11:42:59 GMT   (505kb,D)

Title: MathScale: Scaling Instruction Tuning for Mathematical Reasoning
Authors: Zhengyang Tang, Xingxing Zhang, Benyou Wan, Furu Wei
Categories: cs.CL cs.AI cs.LG
Comments: Work in progress
\\
  Large language models (LLMs) have demonstrated remarkable capabilities in
problem-solving. However, their proficiency in solving mathematical problems
remains inadequate. We propose MathScale, a simple and scalable method to
create high-quality mathematical reasoning data using frontier LLMs (e.g., {\tt
GPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning,
it first extracts topics and knowledge points from seed math questions and then
build a concept graph, which is subsequently used to generate new math
questions. MathScale exhibits effective scalability along the size axis of the
math dataset that we generate. As a result, we create a mathematical reasoning
dataset (MathScaleQA) containing two million math question-answer pairs. To
evaluate mathematical reasoning abilities of LLMs comprehensively, we construct
{\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten
datasets (including GSM8K and MATH) covering K-12, college, and competition
level math problems. We apply MathScaleQA to fine-tune open-source LLMs (e.g.,
LLaMA-2 and Mistral), resulting in significantly improved capabilities in
mathematical reasoning. Evaluated on {\sc MwpBench}, MathScale-7B achieves
state-of-the-art performance across all datasets, surpassing its best peers of
equivalent size by 42.9\% in micro average accuracy and 43.7\% in macro average
accuracy, respectively.
\\ ( https://arxiv.org/abs/2403.02884 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02889
Date: Tue, 5 Mar 2024 11:50:01 GMT   (7038kb,D)

Title: In Search of Truth: An Interrogation Approach to Hallucination Detection
Authors: Yakir Yehuda, Itzik Malkiel, Oren Barkan, Jonathan Weill, Royi Ronen
  and Noam Koenigstein
Categories: cs.CL cs.LG
\\
  Despite the many advances of Large Language Models (LLMs) and their
unprecedented rapid evolution, their impact and integration into every facet of
our daily lives is limited due to various reasons. One critical factor
hindering their widespread adoption is the occurrence of hallucinations, where
LLMs invent answers that sound realistic, yet drift away from factual truth. In
this paper, we present a novel method for detecting hallucinations in large
language models, which tackles a critical issue in the adoption of these models
in various real-world scenarios. Through extensive evaluations across multiple
datasets and LLMs, including Llama-2, we study the hallucination levels of
various recent LLMs and demonstrate the effectiveness of our method to
automatically detect them. Notably, we observe up to 62% hallucinations for
Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy
(B-ACC) of 87%, all without relying on external knowledge.
\\ ( https://arxiv.org/abs/2403.02889 ,  7038kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02893
Date: Tue, 5 Mar 2024 11:57:21 GMT   (1251kb,D)

Title: Zero-Shot Cross-Lingual Document-Level Event Causality Identification
  with Heterogeneous Graph Contrastive Transfer Learning
Authors: Zhitao He, Pengfei Cao, Yubo Chen, Kang Liu, Zhiqiang Zhang, Mengshu
  Sun, Jun Zhao
Categories: cs.CL cs.AI
Comments: Accepted at LREC-COLING 2024
\\
  Event Causality Identification (ECI) refers to detect causal relations
between events in texts. However, most existing studies focus on sentence-level
ECI with high-resource language, leaving more challenging document-level ECI
(DECI) with low-resource languages under-explored. In this paper, we propose a
Heterogeneous Graph Interaction Model with Multi-granularity Contrastive
Transfer Learning (GIMC) for zero-shot cross-lingual document-level ECI.
Specifically, we introduce a heterogeneous graph interaction network to model
the long-distance dependencies between events that are scattered over document.
Then, to improve cross-lingual transferability of causal knowledge learned from
source language, we propose a multi-granularity contrastive transfer learning
module to align the causal representations across languages. Extensive
experiments show our framework outperforms previous state-of-the-art model by
9.4% and 8.2% of average F1 score on monolingual and multilingual scenarios
respectively. Notably, in multilingual scenario, our zero-shot framework even
exceeds GPT-3.5 with few-shot learning by 24.3% in overall performance.
\\ ( https://arxiv.org/abs/2403.02893 ,  1251kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02902
Date: Tue, 5 Mar 2024 12:11:32 GMT   (9255kb,D)

Title: Demonstrating Mutual Reinforcement Effect through Information Flow
Authors: Chengguang Gan, Xuzheng He, Qinghao Zhang, Tatsunori Mori
Categories: cs.CL
Comments: 9 pages, 15 figure
\\
  The Mutual Reinforcement Effect (MRE) investigates the synergistic
relationship between word-level and text-level classifications in text
classification tasks. It posits that the performance of both classification
levels can be mutually enhanced. However, this mechanism has not been
adequately demonstrated or explained in prior research. To address this gap, we
employ information flow analysis to observe and substantiate the MRE theory.
Our experiments on six MRE hybrid datasets revealed the presence of MRE in the
model and its impact. Additionally, we conducted fine-tuning experiments, whose
results were consistent with those of the information flow experiments. The
convergence of findings from both experiments corroborates the existence of
MRE. Furthermore, we extended the application of MRE to prompt learning,
utilizing word-level information as a verbalizer to bolster the model's
prediction of text-level classification labels. In our final experiment, the
F1-score significantly surpassed the baseline in five out of six datasets,
further validating the notion that word-level information enhances the language
model's comprehension of the text as a whole.
\\ ( https://arxiv.org/abs/2403.02902 ,  9255kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02930
Date: Tue, 5 Mar 2024 12:48:29 GMT   (66kb,D)

Title: A Second Look on BASS -- Boosting Abstractive Summarization with Unified
  Semantic Graphs -- A Replication Study
Authors: Osman Alperen Kora\c{s}, J\"org Schl\"otterer, Christin Seifert
Categories: cs.CL cs.LG
Comments: This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this contribution is
  published in Advances in Information Retrieval, 46th European Conference on
  Information Retrieval, ECIR 2024. 16 pages, 4 figures
\\
  We present a detailed replication study of the BASS framework, an abstractive
summarization system based on the notion of Unified Semantic Graphs. Our
investigation includes challenges in replicating key components and an ablation
study to systematically isolate error sources rooted in replicating novel
components. Our findings reveal discrepancies in performance compared to the
original work. We highlight the significance of paying careful attention even
to reasonably omitted details for replicating advanced frameworks like BASS,
and emphasize key practices for writing replicable papers.
\\ ( https://arxiv.org/abs/2403.02930 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02932
Date: Tue, 5 Mar 2024 12:50:36 GMT   (230kb,D)

Title: RulePrompt: Weakly Supervised Text Classification with Prompting PLMs
  and Self-Iterative Logical Rules
Authors: Miaomiao Li, Jiaqi Zhu, Yang Wang, Yi Yang, Yilin Li, Hongan Wang
Categories: cs.CL
Comments: Accepted by WWW 2024
\\
  Weakly supervised text classification (WSTC), also called zero-shot or
dataless text classification, has attracted increasing attention due to its
applicability in classifying a mass of texts within the dynamic and open Web
environment, since it requires only a limited set of seed words (label names)
for each category instead of labeled data. With the help of recently popular
prompting Pre-trained Language Models (PLMs), many studies leveraged manually
crafted and/or automatically identified verbalizers to estimate the likelihood
of categories, but they failed to differentiate the effects of these
category-indicative words, let alone capture their correlations and realize
adaptive adjustments according to the unlabeled corpus. In this paper, in order
to let the PLM effectively understand each category, we at first propose a
novel form of rule-based knowledge using logical expressions to characterize
the meanings of categories. Then, we develop a prompting PLM-based approach
named RulePrompt for the WSTC task, consisting of a rule mining module and a
rule-enhanced pseudo label generation module, plus a self-supervised
fine-tuning module to make the PLM align with this task. Within this framework,
the inaccurate pseudo labels assigned to texts and the imprecise logical rules
associated with categories mutually enhance each other in an alternative
manner. That establishes a self-iterative closed loop of knowledge (rule)
acquisition and utilization, with seed words serving as the starting point.
Extensive experiments validate the effectiveness and robustness of our
approach, which markedly outperforms state-of-the-art weakly supervised
methods. What is more, our approach yields interpretable category rules,
proving its advantage in disambiguating easily-confused categories.
\\ ( https://arxiv.org/abs/2403.02932 ,  230kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02938
Date: Tue, 5 Mar 2024 13:08:52 GMT   (8589kb,D)

Title: AIx Speed: Playback Speed Optimization Using Listening Comprehension of
  Speech Recognition Models
Authors: Kazuki Kawamura and Jun Rekimoto
Categories: cs.CL cs.HC cs.LG cs.SD eess.AS
Journal-ref: AHs '23: Proceedings of the Augmented Humans International
  Conference 2023
DOI: 10.1145/3582700.3582722
\\
  Since humans can listen to audio and watch videos at faster speeds than
actually observed, we often listen to or watch these pieces of content at
higher playback speeds to increase the time efficiency of content
comprehension. To further utilize this capability, systems that automatically
adjust the playback speed according to the user's condition and the type of
content to assist in more efficient comprehension of time-series content have
been developed. However, there is still room for these systems to further
extend human speed-listening ability by generating speech with playback speed
optimized for even finer time units and providing it to humans. In this study,
we determine whether humans can hear the optimized speech and propose a system
that automatically adjusts playback speed at units as small as phonemes while
ensuring speech intelligibility. The system uses the speech recognizer score as
a proxy for how well a human can hear a certain unit of speech and maximizes
the speech playback speed to the extent that a human can hear. This method can
be used to produce fast but intelligible speech. In the evaluation experiment,
we compared the speech played back at a constant fast speed and the flexibly
speed-up speech generated by the proposed method in a blind test and confirmed
that the proposed method produced speech that was easier to listen to.
\\ ( https://arxiv.org/abs/2403.02938 ,  8589kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02951
Date: Tue, 5 Mar 2024 13:23:48 GMT   (6143kb,D)

Title: Benchmarking the Text-to-SQL Capability of Large Language Models: A
  Comprehensive Evaluation
Authors: Bin Zhang, Yuxiao Ye, Guoqing Du, Xiaoru Hu, Zhishuai Li, Sun Yang,
  Chi Harold Liu, Rui Zhao, Ziyue Li, Hangyu Mao
Categories: cs.CL cs.AI
Comments: 26pages
\\
  Large Language Models (LLMs) have emerged as a powerful tool in advancing the
Text-to-SQL task, significantly outperforming traditional methods.
Nevertheless, as a nascent research field, there is still no consensus on the
optimal prompt templates and design frameworks. Additionally, existing
benchmarks inadequately explore the performance of LLMs across the various
sub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs'
cognitive capabilities and the optimization of LLM-based solutions.To address
the aforementioned issues, we firstly construct a new dataset designed to
mitigate the risk of overfitting in LLMs. Then we formulate five evaluation
tasks to comprehensively assess the performance of diverse methods across
various LLMs throughout the Text-to-SQL process.Our study highlights the
performance disparities among LLMs and proposes optimal in-context learning
solutions tailored to each task. These findings offer valuable insights for
enhancing the development of LLM-based Text-to-SQL systems.
\\ ( https://arxiv.org/abs/2403.02951 ,  6143kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02959
Date: Tue, 5 Mar 2024 13:30:02 GMT   (8795kb,D)

Title: SimuCourt: Building Judicial Decision-Making Agents with Real-world
  Judgement Documents
Authors: Zhitao He, Pengfei Cao, Chenhao Wang, Zhuoran Jin, Yubo Chen, Jiexin
  Xu, Huaijun Li, Xiaojian Jiang, Kang Liu, Jun Zhao
Categories: cs.CL cs.AI
\\
  With the development of deep learning, natural language processing technology
has effectively improved the efficiency of various aspects of the traditional
judicial industry. However, most current efforts focus solely on individual
judicial stage, overlooking cross-stage collaboration. As the autonomous agents
powered by large language models are becoming increasingly smart and able to
make complex decisions in real-world settings, offering new insights for
judicial intelligence. In this paper, (1) we introduce SimuCourt, a judicial
benchmark that encompasses 420 judgment documents from real-world, spanning the
three most common types of judicial cases, and a novel task Judicial
Decision-Making to evaluate the judicial analysis and decision-making power of
agents. To support this task, we construct a large-scale judicial knowledge
base, JudicialKB, with multiple legal knowledge. (2) we propose a novel
multi-agent framework, AgentsCourt. Our framework follows the real-world
classic court trial process, consisting of court debate simulation, legal
information retrieval and judgement refinement to simulate the decision-making
of judge. (3) we perform extensive experiments, the results demonstrate that,
our framework outperforms the existing advanced methods in various aspects,
especially in generating legal grounds, where our model achieves significant
improvements of 8.6% and 9.1% F1 score in the first and second instance
settings, respectively.
\\ ( https://arxiv.org/abs/2403.02959 ,  8795kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02966
Date: Tue, 5 Mar 2024 13:43:58 GMT   (6745kb,D)

Title: Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot
  Question Answering
Authors: Sungho Ko, Hyunjin Cho, Hyungjoo Chae, Jinyoung Yeo, Dongha Lee
Categories: cs.CL cs.AI cs.LG
\\
  Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance
Quesetion Answering (QA) performance of Large Language Models (LLMs), yet
structured KG verbalization remains challengin. Existing methods, such as
triple-form or free-form textual conversion of triple-form facts, encounter
several issues. These include reduced evidence density due to duplicated
entities or relationships, and reduced evidence clarity due to an inability to
emphasize crucial evidence. To address these issues, we propose EFSum, an
Evidence-focused Fact Summarization framework for enhanced QA with
knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer
through distillation and preference alignment. Our extensive experiments show
that EFSum improves LLM's zero-shot QA performance, and it is possible to
ensure both the helpfulness and faithfulness of the summary.
\\ ( https://arxiv.org/abs/2403.02966 ,  6745kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02975
Date: Tue, 5 Mar 2024 13:55:16 GMT   (455kb,D)

Title: A General and Flexible Multi-concept Parsing Framework for Multilingual
  Semantic Matching
Authors: Dong Yao, Asaad Alghamdi, Qingrong Xia, Xiaoye Qu, Xinyu Duan, Zhefeng
  Wang, Yi Zheng, Baoxing Huai, Peilun Cheng, Zhou Zhao
Categories: cs.CL cs.AI
\\
  Sentence semantic matching is a research hotspot in natural language
processing, which is considerably significant in various key scenarios, such as
community question answering, searching, chatbot, and recommendation. Since
most of the advanced models directly model the semantic relevance among words
between two sentences while neglecting the \textit{keywords} and
\textit{intents} concepts of them, DC-Match is proposed to disentangle keywords
from intents and utilizes them to optimize the matching performance. Although
DC-Match is a simple yet effective method for semantic matching, it highly
depends on the external NER techniques to identify the keywords of sentences,
which limits the performance of semantic matching for minor languages since
satisfactory NER tools are usually hard to obtain. In this paper, we propose to
generally and flexibly resolve the text into multi concepts for multilingual
semantic matching to liberate the model from the reliance on NER models. To
this end, we devise a \underline{M}ulti-\underline{C}oncept \underline{P}arsed
\underline{S}emantic \underline{M}atching framework based on the pre-trained
language models, abbreviated as \textbf{MCP-SM}, to extract various concepts
and infuse them into the classification tokens. We conduct comprehensive
experiments on English datasets QQP and MRPC, and Chinese dataset Medical-SM.
Besides, we experiment on Arabic datasets MQ2Q and XNLI, the outstanding
performance further prove MCP-SM's applicability in low-resource languages.
\\ ( https://arxiv.org/abs/2403.02975 ,  455kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02990
Date: Tue, 5 Mar 2024 14:11:54 GMT   (1468kb,D)

Title: Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and
  Challenges
Authors: Bosheng Ding, Chengwei Qin, Ruochen Zhao, Tianze Luo, Xinze Li,
  Guizhen Chen, Wenhan Xia, Junjie Hu, Anh Tuan Luu, Shafiq Joty
Categories: cs.CL cs.AI
\\
  In the rapidly evolving field of machine learning (ML), data augmentation
(DA) has emerged as a pivotal technique for enhancing model performance by
diversifying training examples without the need for additional data collection.
This survey explores the transformative impact of Large Language Models (LLMs)
on DA, particularly addressing the unique challenges and opportunities they
present in the context of natural language processing (NLP) and beyond. From a
data perspective and a learning perspective, we examine various strategies that
utilize Large Language Models for data augmentation, including a novel
exploration of learning paradigms where LLM-generated data is used for further
training. Additionally, this paper delineates the primary challenges faced in
this domain, ranging from controllable data augmentation to multi modal data
augmentation. This survey highlights the paradigm shift introduced by LLMs in
DA, aims to serve as a foundational guide for researchers and practitioners in
this field.
\\ ( https://arxiv.org/abs/2403.02990 ,  1468kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03014
Date: Tue, 5 Mar 2024 14:49:52 GMT   (731kb,D)

Title: The Case for Evaluating Multimodal Translation Models on Text Datasets
Authors: Vipin Vijayan, Braeden Bowen, Scott Grigsby, Timothy Anderson, and
  Jeremy Gwinnup
Categories: cs.CL
\\
  A good evaluation framework should evaluate multimodal machine translation
(MMT) models by measuring 1) their use of visual information to aid in the
translation task and 2) their ability to translate complex sentences such as
done for text-only machine translation. However, most current work in MMT is
evaluated against the Multi30k testing sets, which do not measure these
properties. Namely, the use of visual information by the MMT model cannot be
shown directly from the Multi30k test set results and the sentences in Multi30k
are are image captions, i.e., short, descriptive sentences, as opposed to
complex sentences that typical text-only machine translation models are
evaluated against.
  Therefore, we propose that MMT models be evaluated using 1) the CoMMuTE
evaluation framework, which measures the use of visual information by MMT
models, 2) the text-only WMT news translation task test sets, which evaluates
translation performance against complex sentences, and 3) the Multi30k test
sets, for measuring MMT model performance against a real MMT dataset. Finally,
we evaluate recent MMT models trained solely against the Multi30k dataset
against our proposed evaluation framework and demonstrate the dramatic drop
performance against text-only testing sets compared to recent text-only MT
models.
\\ ( https://arxiv.org/abs/2403.03014 ,  731kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03029
Date: Tue, 5 Mar 2024 15:05:06 GMT   (445kb,D)

Title: Socratic Reasoning Improves Positive Text Rewriting
Authors: Anmol Goel, Nico Daheim, Iryna Gurevych
Categories: cs.CL
\\
  Reframing a negative into a positive thought is at the crux of several
cognitive approaches to mental health and psychotherapy that could be made more
accessible by large language model-based solutions. Such reframing is typically
non-trivial and requires multiple rationalization steps to uncover the
underlying issue of a negative thought and transform it to be more positive.
However, this rationalization process is currently neglected by both datasets
and models which reframe thoughts in one step. In this work, we address this
gap by augmenting open-source datasets for positive text rewriting with
synthetically-generated Socratic rationales using a novel framework called
\textsc{SocraticReframe}. \textsc{SocraticReframe} uses a sequence of
question-answer pairs to rationalize the thought rewriting process. We show
that such Socratic rationales significantly improve positive text rewriting for
different open-source LLMs according to both automatic and human evaluations
guided by criteria from psychotherapy research.
\\ ( https://arxiv.org/abs/2403.03029 ,  445kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03031
Date: Tue, 5 Mar 2024 15:08:16 GMT   (1443kb,D)

Title: Learning to Use Tools via Cooperative and Interactive Agents
Authors: Zhengliang Shi, Shen Gao, Xiuyi Chen, Lingyong Yan, Haibo Shi, Dawei
  Yin, Zhumin Chen, Pengjie Ren, Suzan Verberne, Zhaochun Ren
Categories: cs.CL
Comments: 20 pages
\\
  Tool learning empowers large language models (LLMs) as agents to use external
tools to extend their capability. Existing methods employ one single LLM-based
agent to iteratively select and execute tools, thereafter incorporating the
result into the next action prediction. However, they still suffer from
potential performance degradation when addressing complex tasks due to: (1) the
limitation of the inherent capability of a single LLM to perform diverse
actions, and (2) the struggle to adaptively correct mistakes when the task
fails. To mitigate these problems, we propose the ConAgents, a Cooperative and
interactive Agents framework, which modularizes the workflow of tool learning
into Grounding, Execution, and Observing agents. We also introduce an iterative
calibration (IterCali) method, enabling the agents to adapt themselves based on
the feedback from the tool environment. Experiments conducted on three datasets
demonstrate the superiority of our ConAgents (e.g., 6 point improvement over
the SOTA baseline). We further provide fine-granularity analysis for the
efficiency and consistency of our framework.
\\ ( https://arxiv.org/abs/2403.03031 ,  1443kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03045
Date: Tue, 5 Mar 2024 15:28:24 GMT   (655kb,D)

Title: Adding Multimodal Capabilities to a Text-only Translation Model
Authors: Vipin Vijayan, Braeden Bowen, Scott Grigsby, Timothy Anderson, and
  Jeremy Gwinnup
Categories: cs.CL
\\
  While most current work in multimodal machine translation (MMT) uses the
Multi30k dataset for training and evaluation, we find that the resulting models
overfit to the Multi30k dataset to an extreme degree. Consequently, these
models perform very badly when evaluated against typical text-only testing sets
such as the WMT newstest datasets. In order to perform well on both Multi30k
and typical text-only datasets, we use a performant text-only machine
translation (MT) model as the starting point of our MMT model. We add
vision-text adapter layers connected via gating mechanisms to the MT model, and
incrementally transform the MT model into an MMT model by 1) pre-training using
vision-based masking of the source text and 2) fine-tuning on Multi30k.
\\ ( https://arxiv.org/abs/2403.03045 ,  655kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03075
Date: Tue, 5 Mar 2024 16:01:09 GMT   (8137kb,D)

Title: Detecting Concrete Visual Tokens for Multimodal Machine Translation
Authors: Braeden Bowen, Vipin Vijayan, Scott Grigsby, Timothy Anderson, and
  Jeremy Gwinnup
Categories: cs.CL
\\
  The challenge of visual grounding and masking in multimodal machine
translation (MMT) systems has encouraged varying approaches to the detection
and selection of visually-grounded text tokens for masking. We introduce new
methods for detection of visually and contextually relevant (concrete) tokens
from source sentences, including detection with natural language processing
(NLP), detection with object detection, and a joint detection-verification
technique. We also introduce new methods for selection of detected tokens,
including shortest $n$ tokens, longest $n$ tokens, and all detected concrete
tokens. We utilize the GRAM MMT architecture to train models against
synthetically collated multimodal datasets of source images with masked
sentences, showing performance improvements and improved usage of visual
context during translation tasks over the baseline model.
\\ ( https://arxiv.org/abs/2403.03075 ,  8137kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03101
Date: Tue, 5 Mar 2024 16:39:12 GMT   (9366kb,D)

Title: KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents
Authors: Yuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng, Ningyu Zhang, Shiwei
  Lyu, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen
Categories: cs.CL cs.AI cs.HC cs.LG cs.MA
Comments: Work in progress. Project page:
  https://zjunlp.github.io/project/KnowAgent/ Code:
  https://github.com/zjunlp/KnowAgent
\\
  Large Language Models (LLMs) have demonstrated great potential in complex
reasoning tasks, yet they fall short when tackling more sophisticated
challenges, especially when interacting with environments through generating
executable actions. This inadequacy primarily stems from the lack of built-in
action knowledge in language agents, which fails to effectively guide the
planning trajectories during task solving and results in planning
hallucination. To address this issue, we introduce KnowAgent, a novel approach
designed to enhance the planning capabilities of LLMs by incorporating explicit
action knowledge. Specifically, KnowAgent employs an action knowledge base and
a knowledgeable self-learning strategy to constrain the action path during
planning, enabling more reasonable trajectory synthesis, and thereby enhancing
the planning performance of language agents. Experimental results on HotpotQA
and ALFWorld based on various backbone models demonstrate that KnowAgent can
achieve comparable or superior performance to existing baselines. Further
analysis indicates the effectiveness of KnowAgent in terms of planning
hallucinations mitigation. Code is available in
https://github.com/zjunlp/KnowAgent.
\\ ( https://arxiv.org/abs/2403.03101 ,  9366kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03102
Date: Tue, 5 Mar 2024 16:43:03 GMT   (11338kb,D)

Title: "In Dialogues We Learn": Towards Personalized Dialogue Without
  Pre-defined Profiles through In-Dialogue Learning
Authors: Chuanqi Cheng, Quan Tu, Wei Wu, Shuo Shang, Cunli Mao, Zhengtao Yu,
  Rui Yan
Categories: cs.CL cs.AI
\\
  Personalized dialogue systems have gained significant attention in recent
years for their ability to generate responses in alignment with different
personas. However, most existing approaches rely on pre-defined personal
profiles, which are not only time-consuming and labor-intensive to create but
also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning
framework that enhances the ability of pre-trained large language models to
leverage dialogue history to characterize persona for completing personalized
dialogue generation tasks without pre-defined profiles. Our experiments on
three datasets demonstrate that IDL brings substantial improvements, with BLEU
and ROUGE scores increasing by up to 200% and 247%, respectively. Additionally,
the results of human evaluations further validate the efficacy of our proposed
method.
\\ ( https://arxiv.org/abs/2403.03102 ,  11338kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03121
Date: Tue, 5 Mar 2024 17:04:05 GMT   (8278kb,D)

Title: Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes
  in Emotion Attribution
Authors: Flor Miriam Plaza-del-Arco, Amanda Cercas Curry, Alba Curry, Gavin
  Abercrombie, Dirk Hovy
Categories: cs.CL
\\
  Large language models (LLMs) reflect societal norms and biases, especially
about gender. While societal biases and stereotypes have been extensively
researched in various NLP applications, there is a surprising gap for emotion
analysis. However, emotion and gender are closely linked in societal discourse.
E.g., women are often thought of as more empathetic, while men's anger is more
socially accepted. To fill this gap, we present the first comprehensive study
of gendered emotion attribution in five state-of-the-art LLMs (open- and
closed-source). We investigate whether emotions are gendered, and whether these
variations are based on societal stereotypes. We prompt the models to adopt a
gendered persona and attribute emotions to an event like 'When I had a serious
argument with a dear person'. We then analyze the emotions generated by the
models in relation to the gender-event pairs. We find that all models
consistently exhibit gendered emotions, influenced by gender stereotypes. These
findings are in line with established research in psychology and gender
studies. Our study sheds light on the complex societal interplay between
language, gender, and emotion. The reproduction of emotion stereotypes in LLMs
allows us to use those models to study the topic in detail, but raises
questions about the predictive use of those same LLMs for emotion applications.
\\ ( https://arxiv.org/abs/2403.03121 ,  8278kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03129
Date: Tue, 5 Mar 2024 17:15:28 GMT   (434kb,D)

Title: CoGenesis: A Framework Collaborating Large and Small Language Models for
  Secure Context-Aware Instruction Following
Authors: Kaiyan Zhang, Jianyu Wang, Ermo Hua, Biqing Qi, Ning Ding, Bowen Zhou
Categories: cs.CL
\\
  With the advancement of language models (LMs), their exposure to private data
is increasingly inevitable, and their deployment (especially for smaller ones)
on personal devices, such as PCs and smartphones, has become a prevailing
trend. In contexts laden with user information, enabling models to both
safeguard user privacy and execute commands efficiently emerges as an essential
research imperative. In this paper, we propose CoGenesis, a collaborative
generation framework integrating large (hosted on cloud infrastructure) and
small models (deployed on local devices) to address privacy concerns logically.
Initially, we design a pipeline to create personalized writing instruction
datasets enriched with extensive context details as the testbed of this
research issue. Subsequently, we introduce two variants of CoGenesis based on
sketch and logits respectively. Our experimental findings, based on our
synthesized dataset and two additional open-source datasets, indicate that: 1)
Large-scale models perform well when provided with user context but struggle in
the absence of such context. 2) While specialized smaller models fine-tuned on
the synthetic dataset show promise, they still lag behind their larger
counterparts. 3) Our CoGenesis framework, utilizing mixed-scale models,
showcases competitive performance, providing a feasible solution to privacy
issues.
\\ ( https://arxiv.org/abs/2403.03129 ,  434kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03141
Date: Tue, 5 Mar 2024 17:26:41 GMT   (294kb,D)

Title: Language Guided Exploration for RL Agents in Text Environments
Authors: Hitesh Golchha, Sahil Yerawar, Dhruvesh Patel, Soham Dan, Keerthiram
  Murugesan
Categories: cs.CL
\\
  Real-world sequential decision making is characterized by sparse rewards and
large decision spaces, posing significant difficulty for experiential learning
systems like $\textit{tabula rasa}$ reinforcement learning (RL) agents. Large
Language Models (LLMs), with a wealth of world knowledge, can help RL agents
learn quickly and adapt to distribution shifts. In this work, we introduce
Language Guided Exploration (LGE) framework, which uses a pre-trained language
model (called GUIDE ) to provide decision-level guidance to an RL agent (called
EXPLORER). We observe that on ScienceWorld (Wang et al.,2022), a challenging
text environment, LGE outperforms vanilla RL agents significantly and also
outperforms other sophisticated methods like Behaviour Cloning and Text
Decision Transformer.
\\ ( https://arxiv.org/abs/2403.03141 ,  294kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03163
Date: Tue, 5 Mar 2024 17:56:27 GMT   (3151kb,D)

Title: Design2Code: How Far Are We From Automating Front-End Engineering?
Authors: Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu, Diyi Yang
Categories: cs.CL cs.CV cs.CY
Comments: Technical Report; The first two authors contributed equally
\\
  Generative AI has made rapid advancements in recent years, achieving
unprecedented capabilities in multimodal understanding and code generation.
This can enable a new paradigm of front-end development, in which multimodal
LLMs might directly convert visual designs into code implementations. In this
work, we formalize this as a Design2Code task and conduct comprehensive
benchmarking. Specifically, we manually curate a benchmark of 484 diverse
real-world webpages as test cases and develop a set of automatic evaluation
metrics to assess how well current multimodal LLMs can generate the code
implementations that directly render into the given reference webpages, given
the screenshots as input. We also complement automatic metrics with
comprehensive human evaluations. We develop a suite of multimodal prompting
methods and show their effectiveness on GPT-4V and Gemini Pro Vision. We
further finetune an open-source Design2Code-18B model that successfully matches
the performance of Gemini Pro Vision. Both human evaluation and automatic
metrics show that GPT-4V performs the best on this task compared to other
models. Moreover, annotators think GPT-4V generated webpages can replace the
original reference webpages in 49% of cases in terms of visual appearance and
content; and perhaps surprisingly, in 64% of cases GPT-4V generated webpages
are considered better than the original reference webpages. Our fine-grained
break-down metrics indicate that open-source models mostly lag in recalling
visual elements from the input webpages and in generating correct layout
designs, while aspects like text content and coloring can be drastically
improved with proper finetuning.
\\ ( https://arxiv.org/abs/2403.03163 ,  3151kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03167
Date: Tue, 5 Mar 2024 18:01:59 GMT   (12605kb,D)

Title: PARADISE: Evaluating Implicit Planning Skills of Language Models with
  Procedural Warnings and Tips Dataset
Authors: Arda Uzuno\u{g}lu, Abdalfatah Rashid Safa, G\"ozde G\"ul \c{S}ahin
Categories: cs.CL
\\
  Recently, there has been growing interest within the community regarding
whether large language models are capable of planning or executing plans.
However, most prior studies use LLMs to generate high-level plans for
simplified scenarios lacking linguistic complexity and domain diversity,
limiting analysis of their planning abilities. These setups constrain
evaluation methods (e.g., predefined action space), architectural choices
(e.g., only generative models), and overlook the linguistic nuances essential
for realistic analysis. To tackle this, we present PARADISE, an abductive
reasoning task using Q\&A format on practical procedural text sourced from
wikiHow. It involves warning and tip inference tasks directly associated with
goals, excluding intermediary steps, with the aim of testing the ability of the
models to infer implicit knowledge of the plan solely from the given goal. Our
experiments, utilizing fine-tuned language models and zero-shot prompting,
reveal the effectiveness of task-specific small models over large language
models in most scenarios. Despite advancements, all models fall short of human
performance. Notably, our analysis uncovers intriguing insights, such as
variations in model behavior with dropped keywords, struggles of BERT-family
and GPT-4 with physical and abstract goals, and the proposed tasks offering
valuable prior knowledge for other unseen procedural tasks. The PARADISE
dataset and associated resources are publicly available for further research
exploration with https://github.com/GGLAB-KU/paradise.
\\ ( https://arxiv.org/abs/2403.03167 ,  12605kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03187
Date: Tue, 5 Mar 2024 18:22:33 GMT   (2170kb,D)

Title: Reliable, Adaptable, and Attributable Language Models with Retrieval
Authors: Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer,
  Hannaneh Hajishirzi, Wen-tau Yih
Categories: cs.CL cs.AI cs.LG
\\
  Parametric language models (LMs), which are trained on vast amounts of web
data, exhibit remarkable flexibility and capability. However, they still face
practical challenges such as hallucinations, difficulty in adapting to new data
distributions, and a lack of verifiability. In this position paper, we advocate
for retrieval-augmented LMs to replace parametric LMs as the next generation of
LMs. By incorporating large-scale datastores during inference,
retrieval-augmented LMs can be more reliable, adaptable, and attributable.
Despite their potential, retrieval-augmented LMs have yet to be widely adopted
due to several obstacles: specifically, current retrieval-augmented LMs
struggle to leverage helpful text beyond knowledge-intensive tasks such as
question answering, have limited interaction between retrieval and LM
components, and lack the infrastructure for scaling. To address these, we
propose a roadmap for developing general-purpose retrieval-augmented LMs. This
involves a reconsideration of datastores and retrievers, the exploration of
pipelines with improved retriever-LM interaction, and significant investment in
infrastructure for efficient training and inference.
\\ ( https://arxiv.org/abs/2403.03187 ,  2170kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03194
Date: Tue, 5 Mar 2024 18:31:28 GMT   (22647kb,D)

Title: MAGID: An Automated Pipeline for Generating Synthetic Multi-modal
  Datasets
Authors: Hossein Aboutalebi, Hwanjun Song, Yusheng Xie, Arshit Gupta, Justin
  Sun, Hang Su, Igor Shalyminov, Nikolaos Pappas, Siffi Singh, Saab Mansour
Categories: cs.CL
\\
  Development of multimodal interactive systems is hindered by the lack of
rich, multimodal (text, images) conversational data, which is needed in large
quantities for LLMs. Previous approaches augment textual dialogues with
retrieved images, posing privacy, diversity, and quality constraints. In this
work, we introduce \textbf{M}ultimodal \textbf{A}ugmented \textbf{G}enerative
\textbf{I}mages \textbf{D}ialogues (MAGID), a framework to augment text-only
dialogues with diverse and high-quality images. Subsequently, a diffusion model
is applied to craft corresponding images, ensuring alignment with the
identified text. Finally, MAGID incorporates an innovative feedback loop
between an image description generation module (textual LLM) and image quality
modules (addressing aesthetics, image-text matching, and safety), that work in
tandem to generate high-quality and multi-modal dialogues. We compare MAGID to
other SOTA baselines on three dialogue datasets, using automated and human
evaluation. Our results show that MAGID is comparable to or better than
baselines, with significant improvements in human evaluation, especially
against retrieval baselines where the image database is small.
\\ ( https://arxiv.org/abs/2403.03194 ,  22647kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02347
Date: Thu, 29 Feb 2024 23:20:19 GMT   (6882kb,D)

Title: On the Convergence of Federated Learning Algorithms without Data
  Similarity
Authors: Ali Beikmohammadi, Sarit Khirirat, Sindri Magn\'usson
Categories: cs.LG cs.GT
\\
  Data similarity assumptions have traditionally been relied upon to understand
the convergence behaviors of federated learning methods. Unfortunately, this
approach often demands fine-tuning step sizes based on the level of data
similarity. When data similarity is low, these small step sizes result in an
unacceptably slow convergence speed for federated methods. In this paper, we
present a novel and unified framework for analyzing the convergence of
federated learning algorithms without the need for data similarity conditions.
Our analysis centers on an inequality that captures the influence of step sizes
on algorithmic convergence performance. By applying our theorems to well-known
federated algorithms, we derive precise expressions for three widely used step
size schedules: fixed, diminishing, and step-decay step sizes, which are
independent of data similarity conditions. Finally, we conduct comprehensive
evaluations of the performance of these federated learning algorithms,
employing the proposed step size strategies to train deep neural network models
on benchmark datasets under varying data similarity conditions. Our findings
demonstrate significant improvements in convergence speed and overall
performance, marking a substantial advancement in federated learning research.
\\ ( https://arxiv.org/abs/2403.02347 ,  6882kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02352
Date: Fri, 1 Mar 2024 19:24:37 GMT   (7668kb,D)

Title: ATP: Enabling Fast LLM Serving via Attention on Top Principal Keys
Authors: Yue Niu, Saurav Prakash, Salman Avestimehr
Categories: cs.LG cs.AI
Comments: 10 pages, 7 figures, 8 tables
\\
  We propose a new attention mechanism with linear complexity, ATP, that
fixates \textbf{A}ttention on \textbf{T}op \textbf{P}rincipal keys, rather than
on each individual token. Particularly, ATP is driven by an important
observation that input sequences are typically low-rank, i.e., input sequences
can be represented by a few principal bases. Therefore, instead of directly
iterating over all the input tokens, ATP transforms inputs into an orthogonal
space and computes attention only on the top principal bases (keys). Owing to
the observed low-rank structure in input sequences, ATP is able to capture
semantic relationships in input sequences with a few principal keys.
Furthermore, the attention complexity is reduced from \emph{quadratic} to
\emph{linear} without incurring a noticeable performance drop. ATP further
reduces complexity for other linear layers with low-rank inputs, leading to
more speedup compared to prior works that solely target the attention module.
Our evaluations on various models (e.g., BERT and Llama) demonstrate that ATP
achieves comparable accuracy with much lower computation and memory complexity
than the standard attention mechanism. In particular, ATP barely loses accuracy
with only $1/2$ principal keys, and only incurs around $2\%$ accuracy drops
with $1/4$ principal keys.
\\ ( https://arxiv.org/abs/2403.02352 ,  7668kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02354
Date: Sat, 2 Mar 2024 10:14:42 GMT   (3927kb,D)

Title: Spatio-Temporal Field Neural Networks for Air Quality Inference
Authors: Yutong Feng, Qiongyan Wang, Yutong Xia, Junlin Huang, Siru Zhong, Kun
  Wang, Shifen Cheng, Yuxuan Liang
Categories: cs.LG cs.AI
\\
  The air quality inference problem aims to utilize historical data from a
limited number of observation sites to infer the air quality index at an
unknown location. Considering the sparsity of data due to the high maintenance
cost of the stations, good inference algorithms can effectively save the cost
and refine the data granularity. While spatio-temporal graph neural networks
have made excellent progress on this problem, their non-Euclidean and discrete
data structure modeling of reality limits its potential. In this work, we make
the first attempt to combine two different spatio-temporal perspectives, fields
and graphs, by proposing a new model, Spatio-Temporal Field Neural Network, and
its corresponding new framework, Pyramidal Inference. Extensive experiments
validate that our model achieves state-of-the-art performance in nationwide air
quality inference in the Chinese Mainland, demonstrating the superiority of our
proposed model and framework.
\\ ( https://arxiv.org/abs/2403.02354 ,  3927kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02355
Date: Sat, 2 Mar 2024 16:50:48 GMT   (141kb,D)

Title: Temporal Knowledge Graph Completion with Time-sensitive Relations in
  Hypercomplex Space
Authors: Li Cai, Xin Mao, Zhihong Wang, Shangqing Zhao, Yuhao Zhou, Changxu Wu,
  Man Lan
Categories: cs.LG cs.AI
\\
  Temporal knowledge graph completion (TKGC) aims to fill in missing facts
within a given temporal knowledge graph at a specific time. Existing methods,
operating in real or complex spaces, have demonstrated promising performance in
this task. This paper advances beyond conventional approaches by introducing
more expressive quaternion representations for TKGC within hypercomplex space.
Unlike existing quaternion-based methods, our study focuses on capturing
time-sensitive relations rather than time-aware entities. Specifically, we
model time-sensitive relations through time-aware rotation and periodic time
translation, effectively capturing complex temporal variability. Furthermore,
we theoretically demonstrate our method's capability to model symmetric,
asymmetric, inverse, compositional, and evolutionary relation patterns.
Comprehensive experiments on public datasets validate that our proposed
approach achieves state-of-the-art performance in the field of TKGC.
\\ ( https://arxiv.org/abs/2403.02355 ,  141kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02360
Date: Mon, 4 Mar 2024 05:10:28 GMT   (13039kb,D)

Title: Towards Optimal Customized Architecture for Heterogeneous Federated
  Learning with Contrastive Cloud-Edge Model Decoupling
Authors: Xingyan Chen and Tian Du and Mu Wang and Tiancheng Gu and Yu Zhao and
  Gang Kou and Changqiao Xu and Dapeng Oliver Wu
Categories: cs.LG cs.AI
\\
  Federated learning, as a promising distributed learning paradigm, enables
collaborative training of a global model across multiple network edge clients
without the need for central data collecting. However, the heterogeneity of
edge data distribution drags the model towards the local minima, which can be
distant from the global optimum. Such heterogeneity often leads to slow
convergence and substantial communication overhead. To address these issues, we
propose a novel federated learning framework called FedCMD, a model decoupling
tailored to the Cloud-edge supported federated learning that separates deep
neural networks into a body for capturing shared representations in Cloud and a
personalized head for migrating data heterogeneity. Our motivation is that, by
the deep investigation of the performance of selecting different neural network
layers as the personalized head, we found rigidly assigning the last layer as
the personalized head in current studies is not always optimal. Instead, it is
necessary to dynamically select the personalized layer that maximizes the
training performance by taking the representation difference between neighbor
layers into account. To find the optimal personalized layer, we utilize the
low-dimensional representation of each layer to contrast feature distribution
transfer and introduce a Wasserstein-based layer selection method, aimed at
identifying the best-match layer for personalization. Additionally, a weighted
global aggregation algorithm is proposed based on the selected personalized
layer for the practical application of FedCMD. Extensive experiments on ten
benchmarks demonstrate the efficiency and superior performance of our solution
compared with nine state-of-the-art solutions. All code and results are
available at https://github.com/elegy112138/FedCMD.
\\ ( https://arxiv.org/abs/2403.02360 ,  13039kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02363
Date: Mon, 4 Mar 2024 08:06:57 GMT   (1010kb,D)

Title: Addressing Long-Tail Noisy Label Learning Problems: a Two-Stage Solution
  with Label Refurbishment Considering Label Rarity
Authors: Ying-Hsuan Wu, Jun-Wei Hsieh, Li Xin, Shin-You Teng, Yi-Kuan Hsieh,
  Ming-Ching Chang
Categories: cs.LG cs.AI
\\
  Real-world datasets commonly exhibit noisy labels and class imbalance, such
as long-tailed distributions. While previous research addresses this issue by
differentiating noisy and clean samples, reliance on information from
predictions based on noisy long-tailed data introduces potential errors. To
overcome the limitations of prior works, we introduce an effective two-stage
approach by combining soft-label refurbishing with multi-expert ensemble
learning. In the first stage of robust soft label refurbishing, we acquire
unbiased features through contrastive learning, making preliminary predictions
using a classifier trained with a carefully designed BAlanced Noise-tolerant
Cross-entropy (BANC) loss. In the second stage, our label refurbishment method
is applied to obtain soft labels for multi-expert ensemble learning, providing
a principled solution to the long-tail noisy label problem. Experiments
conducted across multiple benchmarks validate the superiority of our approach,
Label Refurbishment considering Label Rarity (LR^2), achieving remarkable
accuracies of 94.19% and 77.05% on simulated noisy CIFAR-10 and CIFAR-100
long-tail datasets, as well as 77.74% and 81.40% on real-noise long-tail
datasets, Food-101N and Animal-10N, surpassing existing state-of-the-art
methods.
\\ ( https://arxiv.org/abs/2403.02363 ,  1010kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02368
Date: Mon, 4 Mar 2024 13:22:53 GMT   (853kb)

Title: A Novel Hybrid Feature Importance and Feature Interaction Detection
  Framework for Predictive Optimization in Industry 4.0 Applications
Authors: Zhipeng Ma, Bo N{\o}rregaard J{\o}rgensen, Zheng Grace Ma
Categories: cs.LG cs.AI
Journal-ref: IECON 2023- 49th Annual Conference of the IEEE Industrial
  Electronics Society
DOI: 10.1109/IECON51785.2023.10312491
\\
  Advanced machine learning algorithms are increasingly utilized to provide
data-based prediction and decision-making support in Industry 4.0. However, the
prediction accuracy achieved by the existing models is insufficient to warrant
practical implementation in real-world applications. This is because not all
features present in real-world datasets possess a direct relevance to the
predictive analysis being conducted. Consequently, the careful incorporation of
select features has the potential to yield a substantial positive impact on the
outcome. To address the research gap, this paper proposes a novel hybrid
framework that combines the feature importance detector - local interpretable
model-agnostic explanations (LIME) and the feature interaction detector -
neural interaction detection (NID), to improve prediction accuracy. By applying
the proposed framework, unnecessary features can be eliminated, and
interactions are encoded to generate a more conducive dataset for predictive
purposes. Subsequently, the proposed model is deployed to refine the prediction
of electricity consumption in foundry processing. The experimental outcomes
reveal an augmentation of up to 9.56% in the R2 score, and a diminution of up
to 24.05% in the root mean square error.
\\ ( https://arxiv.org/abs/2403.02368 ,  853kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02372
Date: Mon, 4 Mar 2024 18:23:55 GMT   (1698kb,D)

Title: OTClean: Data Cleaning for Conditional Independence Violations using
  Optimal Transport
Authors: Alireza Pirhadi, Mohammad Hossein Moslemi, Alexander Cloninger,
  Mostafa Milani, Babak Salimi
Categories: cs.LG cs.AI cs.DB
\\
  Ensuring Conditional Independence (CI) constraints is pivotal for the
development of fair and trustworthy machine learning models. In this paper, we
introduce \sys, a framework that harnesses optimal transport theory for data
repair under CI constraints. Optimal transport theory provides a rigorous
framework for measuring the discrepancy between probability distributions,
thereby ensuring control over data utility. We formulate the data repair
problem concerning CIs as a Quadratically Constrained Linear Program (QCLP) and
propose an alternating method for its solution. However, this approach faces
scalability issues due to the computational cost associated with computing
optimal transport distances, such as the Wasserstein distance. To overcome
these scalability challenges, we reframe our problem as a regularized
optimization problem, enabling us to develop an iterative algorithm inspired by
Sinkhorn's matrix scaling algorithm, which efficiently addresses
high-dimensional and large-scale data. Through extensive experiments, we
demonstrate the efficacy and efficiency of our proposed methods, showcasing
their practical utility in real-world data cleaning and preprocessing tasks.
Furthermore, we provide comparisons with traditional approaches, highlighting
the superiority of our techniques in terms of preserving data utility while
ensuring adherence to the desired CI constraints.
\\ ( https://arxiv.org/abs/2403.02372 ,  1698kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02418
Date: Mon, 4 Mar 2024 19:12:13 GMT   (1252kb,D)

Title: From Zero to Hero: How local curvature at artless initial conditions
  leads away from bad minima
Authors: Tony Bonnaire, Giulio Biroli, Chiara Cammarota
Categories: cs.LG cond-mat.dis-nn cond-mat.stat-mech
Comments: 21 pages, 10 figures
\\
  We investigate the optimization dynamics of gradient descent in a non-convex
and high-dimensional setting, with a focus on the phase retrieval problem as a
case study for complex loss landscapes. We first study the high-dimensional
limit where both the number $M$ and the dimension $N$ of the data are going to
infinity at fixed signal-to-noise ratio $\alpha = M/N$. By analyzing how the
local curvature changes during optimization, we uncover that for intermediate
$\alpha$, the Hessian displays a downward direction pointing towards good
minima in the first regime of the descent, before being trapped in bad minima
at the end. Hence, the local landscape is benign and informative at first,
before gradient descent brings the system into a uninformative maze. The
transition between the two regimes is associated to a BBP-type threshold in the
time-dependent Hessian. Through both theoretical analysis and numerical
experiments, we show that in practical cases, i.e. for finite but even very
large $N$, successful optimization via gradient descent in phase retrieval is
achieved by falling towards the good minima before reaching the bad ones. This
mechanism explains why successful recovery is obtained well before the
algorithmic transition corresponding to the high-dimensional limit.
Technically, this is associated to strong logarithmic corrections of the
algorithmic transition at large $N$ with respect to the one expected in the
$N\to\infty$ limit. Our analysis sheds light on such a new mechanism that
facilitate gradient descent dynamics in finite large dimensions, also
highlighting the importance of good initialization of spectral properties for
optimization in complex high-dimensional landscapes.
\\ ( https://arxiv.org/abs/2403.02418 ,  1252kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02419
Date: Mon, 4 Mar 2024 19:12:48 GMT   (1401kb,D)

Title: Are More LLM Calls All You Need? Towards Scaling Laws of Compound
  Inference Systems
Authors: Lingjiao Chen and Jared Quincy Davis and Boris Hanin and Peter Bailis
  and Ion Stoica and Matei Zaharia and James Zou
Categories: cs.LG cs.AI cs.CL cs.SY eess.SY
\\
  Many recent state-of-the-art results in language tasks were achieved using
compound systems that perform multiple Large Language Model (LLM) calls and
aggregate their responses. However, there is little understanding of how the
number of LLM calls -- e.g., when asking the LLM to answer each question
multiple times and taking a consensus -- affects such a compound system's
performance. In this paper, we initiate the study of scaling laws of compound
inference systems. We analyze, theoretically and empirically, how the number of
LLM calls affects the performance of one-layer Voting Inference Systems -- one
of the simplest compound systems, which aggregates LLM responses via majority
voting. We find empirically that across multiple language tasks, surprisingly,
Voting Inference Systems' performance first increases but then decreases as a
function of the number of LLM calls. Our theoretical results suggest that this
non-monotonicity is due to the diversity of query difficulties within a task:
more LLM calls lead to higher performance on "easy" queries, but lower
performance on "hard" queries, and non-monotone behavior emerges when a task
contains both types of queries. This insight then allows us to compute, from a
small number of samples, the number of LLM calls that maximizes system
performance, and define a scaling law of Voting Inference Systems. Experiments
show that our scaling law can predict the performance of Voting Inference
Systems and find the optimal number of LLM calls to make.
\\ ( https://arxiv.org/abs/2403.02419 ,  1401kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02429
Date: Mon, 4 Mar 2024 19:22:09 GMT   (333kb,D)

Title: Towards efficient deep autoencoders for multivariate time series anomaly
  detection
Authors: Marcin Pietro\'n, Dominik \.Zurek, Kamil Faber, Roberto Corizzo
Categories: cs.LG cs.AI
\\
  Multivariate time series anomaly detection is a crucial problem in many
industrial and research applications. Timely detection of anomalies allows, for
instance, to prevent defects in manufacturing processes and failures in
cyberphysical systems. Deep learning methods are preferred among others for
their accuracy and robustness for the analysis of complex multivariate data.
However, a key aspect is being able to extract predictions in a timely manner,
to accommodate real-time requirements in different applications. In the case of
deep learning models, model reduction is extremely important to achieve optimal
results in real-time systems with limited time and memory constraints. In this
paper, we address this issue by proposing a novel compression method for deep
autoencoders that involves three key factors. First, pruning reduces the number
of weights, while preventing catastrophic drops in accuracy by means of a fast
search process that identifies high sparsity levels. Second, linear and
non-linear quantization reduces model complexity by reducing the number of bits
for every single weight. The combined contribution of these three aspects allow
the model size to be reduced, by removing a subset of the weights (pruning),
and decreasing their bit-width (quantization). As a result, the compressed
model is faster and easier to adopt in highly constrained hardware
environments. Experiments performed on popular multivariate anomaly detection
benchmarks, show that our method is capable of achieving significant model
compression ratio (between 80% and 95%) without a significant reduction in the
anomaly detection performance.
\\ ( https://arxiv.org/abs/2403.02429 ,  333kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02437
Date: Mon, 4 Mar 2024 19:35:08 GMT   (1235kb,D)

Title: SoK: Challenges and Opportunities in Federated Unlearning
Authors: Hyejun Jeong, Shiqing Ma, Amir Houmansadr
Categories: cs.LG cs.AI cs.DC
\\
  Federated learning (FL), introduced in 2017, facilitates collaborative
learning between non-trusting parties with no need for the parties to
explicitly share their data among themselves. This allows training models on
user data while respecting privacy regulations such as GDPR and CPRA. However,
emerging privacy requirements may mandate model owners to be able to
\emph{forget} some learned data, e.g., when requested by data owners or law
enforcement. This has given birth to an active field of research called
\emph{machine unlearning}. In the context of FL, many techniques developed for
unlearning in centralized settings are not trivially applicable! This is due to
the unique differences between centralized and distributed learning, in
particular, interactivity, stochasticity, heterogeneity, and limited
accessibility in FL. In response, a recent line of work has focused on
developing unlearning mechanisms tailored to FL.
  This SoK paper aims to take a deep look at the \emph{federated unlearning}
literature, with the goal of identifying research trends and challenges in this
emerging field. By carefully categorizing papers published on FL unlearning
(since 2020), we aim to pinpoint the unique complexities of federated
unlearning, highlighting limitations on directly applying centralized
unlearning methods. We compare existing federated unlearning methods regarding
influence removal and performance recovery, compare their threat models and
assumptions, and discuss their implications and limitations. For instance, we
analyze the experimental setup of FL unlearning studies from various
perspectives, including data heterogeneity and its simulation, the datasets
used for demonstration, and evaluation metrics. Our work aims to offer insights
and suggestions for future research on federated unlearning.
\\ ( https://arxiv.org/abs/2403.02437 ,  1235kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02439
Date: Mon, 4 Mar 2024 19:38:50 GMT   (59kb,D)

Title: Root Causing Prediction Anomalies Using Explainable AI
Authors: Ramanathan Vishnampet, Rajesh Shenoy, Jianhui Chen, Anuj Gupta
Categories: cs.LG cs.AI
Comments: Submitted to The 2nd World Conference on eXplainable Artificial
  Intelligence, 17-19 July, 2024, Malta, Valletta
\\
  This paper presents a novel application of explainable AI (XAI) for
root-causing performance degradation in machine learning models that learn
continuously from user engagement data. In such systems a single feature
corruption can cause cascading feature, label and concept drifts. We have
successfully applied this technique to improve the reliability of models used
in personalized advertising. Performance degradation in such systems manifest
as prediction anomalies in the models. These models are typically trained
continuously using features that are produced by hundreds of real time data
processing pipelines or derived from other upstream models. A failure in any of
these pipelines or an instability in any of the upstream models can cause
feature corruption, causing the model's predicted output to deviate from the
actual output and the training data to become corrupted. The causal
relationship between the features and the predicted output is complex, and
root-causing is challenging due to the scale and dynamism of the system. We
demonstrate how temporal shifts in the global feature importance distribution
can effectively isolate the cause of a prediction anomaly, with better recall
than model-to-feature correlation methods. The technique appears to be
effective even when approximating the local feature importance using a simple
perturbation-based method, and aggregating over a few thousand examples. We
have found this technique to be a model-agnostic, cheap and effective way to
monitor complex data pipelines in production and have deployed a system for
continuously analyzing the global feature importance distribution of
continuously trained models.
\\ ( https://arxiv.org/abs/2403.02439 ,  59kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02446
Date: Mon, 4 Mar 2024 19:59:32 GMT   (728kb,D)

Title: On Latency Predictors for Neural Architecture Search
Authors: Yash Akhauri, Mohamed S. Abdelfattah
Categories: cs.LG cs.AR cs.CV cs.PF
Comments: Accepted at MLSys'24
\\
  Efficient deployment of neural networks (NN) requires the co-optimization of
accuracy and latency. For example, hardware-aware neural architecture search
has been used to automatically find NN architectures that satisfy a latency
constraint on a specific hardware device. Central to these search algorithms is
a prediction model that is designed to provide a hardware latency estimate for
a candidate NN architecture. Recent research has shown that the sample
efficiency of these predictive models can be greatly improved through
pre-training on some \textit{training} devices with many samples, and then
transferring the predictor on the \textit{test} (target) device. Transfer
learning and meta-learning methods have been used for this, but often exhibit
significant performance variability. Additionally, the evaluation of existing
latency predictors has been largely done on hand-crafted training/test device
sets, making it difficult to ascertain design features that compose a robust
and general latency predictor. To address these issues, we introduce a
comprehensive suite of latency prediction tasks obtained in a principled way
through automated partitioning of hardware device sets. We then design a
general latency predictor to comprehensively study (1) the predictor
architecture, (2) NN sample selection methods, (3) hardware device
representations, and (4) NN operation encoding schemes. Building on conclusions
from our study, we present an end-to-end latency predictor training strategy
that outperforms existing methods on 11 out of 12 difficult latency prediction
tasks, improving latency prediction by 22.5\% on average, and up to to 87.6\%
on the hardest tasks. Focusing on latency prediction, our HW-Aware NAS reports
a $5.8\times$ speedup in wall-clock time. Our code is available on
\href{https://github.com/abdelfattah-lab/nasflat_latency}{https://github.com/abdelfattah-lab/nasflat\_latency}.
\\ ( https://arxiv.org/abs/2403.02446 ,  728kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02475
Date: Mon, 4 Mar 2024 20:39:24 GMT   (1640kb,D)

Title: Enhancing LLM Safety via Constrained Direct Preference Optimization
Authors: Zixuan Liu, Xiaolin Sun, Zizhan Zheng
Categories: cs.LG cs.CL
\\
  The rapidly increasing capabilities of large language models (LLMs) raise an
urgent need to align AI systems with diverse human preferences to
simultaneously enhance their usefulness and safety, despite the often
conflicting nature of these goals. To address this important problem, a
promising approach is to enforce a safety constraint at the fine-tuning stage
through a constrained Reinforcement Learning from Human Feedback (RLHF)
framework. This approach, however, is computationally expensive and often
unstable. In this work, we introduce Constrained DPO (C-DPO), a novel extension
of the recently proposed Direct Preference Optimization (DPO) approach for
fine-tuning LLMs that is both efficient and lightweight. By integrating dual
gradient descent and DPO, our method identifies a nearly optimal trade-off
between helpfulness and harmlessness without using reinforcement learning.
Empirically, our approach provides a safety guarantee to LLMs that is missing
in DPO while achieving significantly higher rewards under the same safety
constraint compared to a recently proposed safe RLHF approach.
  Warning: This paper contains example data that may be offensive or harmful.
\\ ( https://arxiv.org/abs/2403.02475 ,  1640kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02476
Date: Mon, 4 Mar 2024 20:40:02 GMT   (22kb)

Title: A Simple Finite-Time Analysis of TD Learning with Linear Function
  Approximation
Authors: Aritra Mitra
Categories: cs.LG cs.SY eess.SY math.OC
\\
  We study the finite-time convergence of TD learning with linear function
approximation under Markovian sampling. Existing proofs for this setting either
assume a projection step in the algorithm to simplify the analysis, or require
a fairly intricate argument to ensure stability of the iterates. We ask:
\textit{Is it possible to retain the simplicity of a projection-based analysis
without actually performing a projection step in the algorithm?} Our main
contribution is to show this is possible via a novel two-step argument. In the
first step, we use induction to prove that under a standard choice of a
constant step-size $\alpha$, the iterates generated by TD learning remain
uniformly bounded in expectation. In the second step, we establish a recursion
that mimics the steady-state dynamics of TD learning up to a bounded
perturbation on the order of $O(\alpha^2)$ that captures the effect of
Markovian sampling. Combining these pieces leads to an overall approach that
considerably simplifies existing proofs. We conjecture that our inductive proof
technique will find applications in the analyses of more complex stochastic
approximation algorithms, and conclude by providing some examples of such
applications.
\\ ( https://arxiv.org/abs/2403.02476 ,  22kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02484
Date: Mon, 4 Mar 2024 21:05:52 GMT   (20437kb,D)

Title: Encodings for Prediction-based Neural Architecture Search
Authors: Yash Akhauri, Mohamed S. Abdelfattah
Categories: cs.LG cs.AI cs.CV cs.NE
\\
  Predictor-based methods have substantially enhanced Neural Architecture
Search (NAS) optimization. The efficacy of these predictors is largely
influenced by the method of encoding neural network architectures. While
traditional encodings used an adjacency matrix describing the graph structure
of a neural network, novel encodings embrace a variety of approaches from
unsupervised pretraining of latent representations to vectors of zero-cost
proxies. In this paper, we categorize and investigate neural encodings from
three main types: structural, learned, and score-based. Furthermore, we extend
these encodings and introduce \textit{unified encodings}, that extend NAS
predictors to multiple search spaces. Our analysis draws from experiments
conducted on over 1.5 million neural network architectures on NAS spaces such
as NASBench-101 (NB101), NB201, NB301, Network Design Spaces (NDS), and
TransNASBench-101. Building on our study, we present our predictor
\textbf{FLAN}: \textbf{Fl}ow \textbf{A}ttention for \textbf{N}AS. FLAN
integrates critical insights on predictor design, transfer learning, and
\textit{unified encodings} to enable more than an order of magnitude cost
reduction for training NAS accuracy predictors. Our implementation and
encodings for all neural networks are open-sourced at
\href{https://github.com/abdelfattah-lab/flan_nas}{https://github.com/abdelfattah-lab/flan\_nas}.
\\ ( https://arxiv.org/abs/2403.02484 ,  20437kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02522
Date: Mon, 4 Mar 2024 22:26:25 GMT   (4705kb,D)

Title: HeAR -- Health Acoustic Representations
Authors: Sebastien Baur, Zaid Nabulsi, Wei-Hung Weng, Jake Garrison, Louis
  Blankemeier, Sam Fishman, Christina Chen, Sujay Kakarmath, Minyoi Maimbolwa,
  Nsala Sanjase, Brian Shuma, Yossi Matias, Greg S. Corrado, Shwetak Patel,
  Shravya Shetty, Shruthi Prabhakara, Monde Muyoyeta, Diego Ardila
Categories: cs.LG cs.AI
Comments: 4 tables, 4 figures, 6 supplementary tables, 3 supplementary figures
\\
  Health acoustic sounds such as coughs and breaths are known to contain useful
health signals with significant potential for monitoring health and disease,
yet are underexplored in the medical machine learning community. The existing
deep learning systems for health acoustics are often narrowly trained and
evaluated on a single task, which is limited by data and may hinder
generalization to other tasks. To mitigate these gaps, we develop HeAR, a
scalable self-supervised learning-based deep learning system using masked
autoencoders trained on a large dataset of 313 million two-second long audio
clips. Through linear probes, we establish HeAR as a state-of-the-art health
audio embedding model on a benchmark of 33 health acoustic tasks across 6
datasets. By introducing this work, we hope to enable and accelerate further
health acoustics research.
\\ ( https://arxiv.org/abs/2403.02522 ,  4705kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02531
Date: Mon, 4 Mar 2024 22:51:51 GMT   (10418kb,D)

Title: Density-based Isometric Mapping
Authors: Bardia Yousefi, M\'elina Khansari, Ryan Trask, Patrick Tallon, Carina
  Carino, Arman Afrasiyabi, Vikas Kundra, Lan Ma, Lei Ren, Keyvan Farahani,
  Michelle Hershman
Categories: cs.LG cs.CV
Comments: This is the author's version
\\
  The isometric mapping method employs the shortest path algorithm to estimate
the Euclidean distance between points on High dimensional (HD) manifolds. This
may not be sufficient for weakly uniformed HD data as it could lead to
overestimating distances between far neighboring points, resulting in
inconsistencies between the intrinsic (local) and extrinsic (global) distances
during the projection. To address this issue, we modify the shortest path
algorithm by adding a novel constraint inspired by the Parzen-Rosenblatt (PR)
window, which helps to maintain the uniformity of the constructed shortest-path
graph in Isomap. Multiple imaging datasets overall of 72,236 cases, 70,000
MINST data, 1596 from multiple Chest-XRay pneumonia datasets, and three NSCLC
CT/PET datasets with a total of 640 lung cancer patients, were used to
benchmark and validate PR-Isomap. 431 imaging biomarkers were extracted from
each modality. Our results indicate that PR-Isomap projects HD attributes into
a lower-dimensional (LD) space while preserving information, visualized by the
MNIST dataset indicating the maintaining local and global distances. PR-Isomap
achieved the highest comparative accuracies of 80.9% (STD:5.8) for pneumonia
and 78.5% (STD:4.4), 88.4% (STD:1.4), and 61.4% (STD:11.4) for three NSCLC
datasets, with a confidence interval of 95% for outcome prediction. Similarly,
the multivariate Cox model showed higher overall survival, measured with
c-statistics and log-likelihood test, of PR-Isomap compared to other
dimensionality reduction methods. Kaplan Meier survival curve also signifies
the notable ability of PR-Isomap to distinguish between high-risk and low-risk
patients using multimodal imaging biomarkers preserving HD imaging
characteristics for precision medicine.
\\ ( https://arxiv.org/abs/2403.02531 ,  10418kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02534
Date: Mon, 4 Mar 2024 23:03:17 GMT   (326kb,D)

Title: Towards Foundation Time Series Model: To Synthesize Or Not To
  Synthesize?
Authors: Kseniia Kuvshinova, Olga Tsymboi, Alina Kostromina, Dmitry Simakov,
  Elizaveta Kovtun
Categories: cs.LG
\\
  The industry is rich in cases when we are required to make forecasting for
large amounts of time series at once. However, we might be in a situation where
we can not afford to train a separate model for each of them. Such issue in
time series modeling remains without due attention. The remedy for this setting
is the establishment of a foundation model. Such a model is expected to work in
zero-shot and few-shot regimes. However, what should we take as a training
dataset for such kind of model?
  Witnessing the benefits from the enrichment of NLP datasets with
artificially-generated data, we might want to adopt their experience for time
series. In contrast to natural language, the process of generation of synthetic
time series data is even more favorable because it provides full control of
series patterns, time horizons, and number of samples. In this work, we
consider the essential question if it is advantageous to train a foundation
model on synthetic data or it is better to utilize only a limited number of
real-life examples. Our experiments are conducted only for regular time series
and speak in favor of leveraging solely the real time series. Moreover, the
choice of the proper source dataset strongly influences the performance during
inference. When provided access even to a limited quantity of short time series
data, employing it within a supervised framework yields more favorable results
than training on a larger volume of synthetic data. The code for our
experiments is publicly available on Github
\url{https://github.com/sb-ai-lab/synthesize_or_not}.
\\ ( https://arxiv.org/abs/2403.02534 ,  326kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02545
Date: Mon, 4 Mar 2024 23:40:20 GMT   (972kb,D)

Title: Wukong: Towards a Scaling Law for Large-Scale Recommendation
Authors: Buyun Zhang, Liang Luo, Yuxin Chen, Jade Nie, Xi Liu, Daifeng Guo,
  Yanli Zhao, Shen Li, Yuchen Hao, Yantao Yao, Guna Lakshminarayanan, Ellie
  Dingqiao Wen, Jongsoo Park, Maxim Naumov, Wenlin Chen
Categories: cs.LG cs.AI
Comments: 12 pages
\\
  Scaling laws play an instrumental role in the sustainable improvement in
model quality. Unfortunately, recommendation models to date do not exhibit such
laws similar to those observed in the domain of large language models, due to
the inefficiencies of their upscaling mechanisms. This limitation poses
significant challenges in adapting these models to increasingly more complex
real-world datasets. In this paper, we propose an effective network
architecture based purely on stacked factorization machines, and a synergistic
upscaling strategy, collectively dubbed Wukong, to establish a scaling law in
the domain of recommendation. Wukong's unique design makes it possible to
capture diverse, any-order of interactions simply through taller and wider
layers. We conducted extensive evaluations on six public datasets, and our
results demonstrate that Wukong consistently outperforms state-of-the-art
models quality-wise. Further, we assessed Wukong's scalability on an internal,
large-scale dataset. The results show that Wukong retains its superiority in
quality over state-of-the-art models, while holding the scaling law across two
orders of magnitude in model complexity, extending beyond 100 Gflop or
equivalently up to GPT-3/LLaMa-2 scale of total training compute, where prior
arts fall short.
\\ ( https://arxiv.org/abs/2403.02545 ,  972kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02571
Date: Tue, 5 Mar 2024 00:58:34 GMT   (2330kb,D)

Title: DPAdapter: Improving Differentially Private Deep Learning through Noise
  Tolerance Pre-training
Authors: Zihao Wang, Rui Zhu, Dongruo Zhou, Zhikun Zhang, John Mitchell, Haixu
  Tang, and XiaoFeng Wang
Categories: cs.LG cs.CV
Comments: To appear in the 33rd USENIX Security Symposium, August 2024,
  Philadelphia Marriott Downtown, PA, USA
\\
  Recent developments have underscored the critical role of
\textit{differential privacy} (DP) in safeguarding individual data for training
machine learning models. However, integrating DP oftentimes incurs significant
model performance degradation due to the perturbation introduced into the
training process, presenting a formidable challenge in the {differentially
private machine learning} (DPML) field. To this end, several mitigative efforts
have been proposed, typically revolving around formulating new DPML algorithms
or relaxing DP definitions to harmonize with distinct contexts. In spite of
these initiatives, the diminishment induced by DP on models, particularly
large-scale models, remains substantial and thus, necessitates an innovative
solution that adeptly circumnavigates the consequential impairment of model
utility.
  In response, we introduce DPAdapter, a pioneering technique designed to
amplify the model performance of DPML algorithms by enhancing parameter
robustness. The fundamental intuition behind this strategy is that models with
robust parameters are inherently more resistant to the noise introduced by DP,
thereby retaining better performance despite the perturbations. DPAdapter
modifies and enhances the sharpness-aware minimization (SAM) technique,
utilizing a two-batch strategy to provide a more accurate perturbation estimate
and an efficient gradient descent, thereby improving parameter robustness
against noise. Notably, DPAdapter can act as a plug-and-play component and be
combined with existing DPML algorithms to further improve their performance.
Our experiments show that DPAdapter vastly enhances state-of-the-art DPML
algorithms, increasing average accuracy from 72.92\% to 77.09\% with a privacy
budget of $\epsilon=4$.
\\ ( https://arxiv.org/abs/2403.02571 ,  2330kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02573
Date: Tue, 5 Mar 2024 01:06:25 GMT   (4089kb,D)

Title: Learning-augmented Online Minimization of Age of Information and
  Transmission Costs
Authors: Zhongdong Liu, Keyuan Zhang, Bin Li, Yin Sun, Y. Thomas Hou, and Bo Ji
Categories: cs.LG
Comments: A preliminary version of this work is to be presented at IEEE INFOCOM
  2024 Age and Semantics of Information Workshop
\\
  We consider a discrete-time system where a resource-constrained source (e.g.,
a small sensor) transmits its time-sensitive data to a destination over a
time-varying wireless channel. Each transmission incurs a fixed transmission
cost (e.g., energy cost), and no transmission results in a staleness cost
represented by the Age-of-Information. The source must balance the tradeoff
between transmission and staleness costs. To address this challenge, we develop
a robust online algorithm to minimize the sum of transmission and staleness
costs, ensuring a worst-case performance guarantee. While online algorithms are
robust, they are usually overly conservative and may have a poor average
performance in typical scenarios. In contrast, by leveraging historical data
and prediction models, machine learning (ML) algorithms perform well in average
cases. However, they typically lack worst-case performance guarantees. To
achieve the best of both worlds, we design a learning-augmented online
algorithm that exhibits two desired properties: (i) consistency: closely
approximating the optimal offline algorithm when the ML prediction is accurate
and trusted; (ii) robustness: ensuring worst-case performance guarantee even ML
predictions are inaccurate. Finally, we perform extensive simulations to show
that our online algorithm performs well empirically and that our
learning-augmented algorithm achieves both consistency and robustness.
\\ ( https://arxiv.org/abs/2403.02573 ,  4089kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02598
Date: Tue, 5 Mar 2024 02:20:33 GMT   (2838kb)

Title: Pooling Image Datasets With Multiple Covariate Shift and Imbalance
Authors: Sotirios Panagiotis Chytas, Vishnu Suresh Lokhande, Peiran Li, Vikas
  Singh
Categories: cs.LG cs.CV
Comments: The International Conference on Learning Representations (ICLR)
\\
  Small sample sizes are common in many disciplines, which necessitates pooling
roughly similar datasets across multiple institutions to study weak but
relevant associations between images and disease outcomes. Such data often
manifest shift/imbalance in covariates (i.e., secondary non-imaging data).
Controlling for such nuisance variables is common within standard statistical
analysis, but the ideas do not directly apply to overparameterized models.
Consequently, recent work has shown how strategies from invariant
representation learning provides a meaningful starting point, but the current
repertoire of methods is limited to accounting for shifts/imbalances in just a
couple of covariates at a time. In this paper, we show how viewing this problem
from the perspective of Category theory provides a simple and effective
solution that completely avoids elaborate multi-stage training pipelines that
would otherwise be needed. We show the effectiveness of this approach via
extensive experiments on real datasets. Further, we discuss how this style of
formulation offers a unified perspective on at least 5+ distinct problem
settings, from self-supervised learning to matching problems in 3D
reconstruction.
\\ ( https://arxiv.org/abs/2403.02598 ,  2838kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02600
Date: Tue, 5 Mar 2024 02:27:52 GMT   (10795kb,D)

Title: TESTAM: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of
  Experts
Authors: Hyunwook Lee, Sungahn Ko
Categories: cs.LG cs.SI
Comments: 19 pages, 7 figures, Accepted as poster to ICLR 2024. Code:
  https://github.com/HyunWookL/TESTAM
Journal-ref: International Conference on Learning Representations (ICLR 2024)
\\
  Accurate traffic forecasting is challenging due to the complex dependency on
road networks, various types of roads, and the abrupt speed change due to the
events. Recent works mainly focus on dynamic spatial modeling with adaptive
graph embedding or graph attention having less consideration for temporal
characteristics and in-situ modeling. In this paper, we propose a novel deep
learning model named TESTAM, which individually models recurring and
non-recurring traffic patterns by a mixture-of-experts model with three experts
on temporal modeling, spatio-temporal modeling with static graph, and dynamic
spatio-temporal dependency modeling with dynamic graph. By introducing
different experts and properly routing them, TESTAM could better model various
circumstances, including spatially isolated nodes, highly related nodes, and
recurring and non-recurring events. For the proper routing, we reformulate a
gating problem into a classification problem with pseudo labels. Experimental
results on three public traffic network datasets, METR-LA, PEMS-BAY, and
EXPY-TKY, demonstrate that TESTAM achieves a better indication and modeling of
recurring and non-recurring traffic. We published the official code at
https://github.com/HyunWookL/TESTAM
\\ ( https://arxiv.org/abs/2403.02600 ,  10795kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02608
Date: Tue, 5 Mar 2024 02:49:00 GMT   (3142kb,D)

Title: DNNLasso: Scalable Graph Learning for Matrix-Variate Data
Authors: Meixia Lin and Yangjing Zhang
Categories: cs.LG math.OC
Comments: Proceedings of the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024
\\
  We consider the problem of jointly learning row-wise and column-wise
dependencies of matrix-variate observations, which are modelled separately by
two precision matrices. Due to the complicated structure of Kronecker-product
precision matrices in the commonly used matrix-variate Gaussian graphical
models, a sparser Kronecker-sum structure was proposed recently based on the
Cartesian product of graphs. However, existing methods for estimating
Kronecker-sum structured precision matrices do not scale well to large scale
datasets. In this paper, we introduce DNNLasso, a diagonally non-negative
graphical lasso model for estimating the Kronecker-sum structured precision
matrix, which outperforms the state-of-the-art methods by a large margin in
both accuracy and computational time. Our code is available at
https://github.com/YangjingZhang/DNNLasso.
\\ ( https://arxiv.org/abs/2403.02608 ,  3142kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02616
Date: Tue, 5 Mar 2024 03:11:02 GMT   (1953kb)

Title: Unsupervised Spatio-Temporal State Estimation for Fine-grained Adaptive
  Anomaly Diagnosis of Industrial Cyber-physical Systems
Authors: Haili Sun, Yan Huang, Lansheng Han, Cai Fu, Chunjie Zhou
Categories: cs.LG cs.AI cs.CR cs.NI cs.SY eess.SY
Comments: 23 pages, 7 figures
\\
  Accurate detection and diagnosis of abnormal behaviors such as network
attacks from multivariate time series (MTS) are crucial for ensuring the stable
and effective operation of industrial cyber-physical systems (CPS). However,
existing researches pay little attention to the logical dependencies among
system working states, and have difficulties in explaining the evolution
mechanisms of abnormal signals. To reveal the spatio-temporal association
relationships and evolution mechanisms of the working states of industrial CPS,
this paper proposes a fine-grained adaptive anomaly diagnosis method (i.e.
MAD-Transformer) to identify and diagnose anomalies in MTS. MAD-Transformer
first constructs a temporal state matrix to characterize and estimate the
change patterns of the system states in the temporal dimension. Then, to better
locate the anomalies, a spatial state matrix is also constructed to capture the
inter-sensor state correlation relationships within the system. Subsequently,
based on these two types of state matrices, a three-branch structure of
series-temporal-spatial attention module is designed to simultaneously capture
the series, temporal, and space dependencies among MTS. Afterwards, three
associated alignment loss functions and a reconstruction loss are constructed
to jointly optimize the model. Finally, anomalies are determined and diagnosed
by comparing the residual matrices with the original matrices. We conducted
comparative experiments on five publicly datasets spanning three application
domains (service monitoring, spatial and earth exploration, and water
treatment), along with a petroleum refining simulation dataset collected by
ourselves. The results demonstrate that MAD-Transformer can adaptively detect
fine-grained anomalies with short duration, and outperforms the
state-of-the-art baselines in terms of noise robustness and localization
performance.
\\ ( https://arxiv.org/abs/2403.02616 ,  1953kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02619
Date: Tue, 5 Mar 2024 03:18:43 GMT   (295kb,D)

Title: Training Machine Learning models at the Edge: A Survey
Authors: Aymen Rayane Khouas, Mohamed Reda Bouadjenek, Hakim Hacid, and Sunil
  Aryal
Categories: cs.LG cs.DC
Comments: 29 pages, 7 figures
\\
  Edge Computing (EC) has gained significant traction in recent years,
promising enhanced efficiency by integrating Artificial Intelligence (AI)
capabilities at the edge. While the focus has primarily been on the deployment
and inference of Machine Learning (ML) models at the edge, the training aspect
remains less explored. This survey delves into Edge Learning (EL), specifically
the optimization of ML model training at the edge. The objective is to
comprehensively explore diverse approaches and methodologies in EL, synthesize
existing knowledge, identify challenges, and highlight future trends. Utilizing
Scopus' advanced search, relevant literature on EL was identified, revealing a
concentration of research efforts in distributed learning methods, particularly
Federated Learning (FL). This survey further provides a guideline for comparing
techniques used to optimize ML for edge learning, along with an exploration of
different frameworks, libraries, and simulation tools available for EL. In
doing so, the paper contributes to a holistic understanding of the current
landscape and future directions in the intersection of edge computing and
machine learning, paving the way for informed comparisons between optimization
methods and techniques designed for edge learning.
\\ ( https://arxiv.org/abs/2403.02619 ,  295kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02622
Date: Tue, 5 Mar 2024 03:23:55 GMT   (23655kb,D)

Title: World Models for Autonomous Driving: An Initial Survey
Authors: Yanchen Guan, Haicheng Liao, Zhenning Li, Guohui Zhang, Chengzhong Xu
Categories: cs.LG cs.AI cs.RO
\\
  In the rapidly evolving landscape of autonomous driving, the capability to
accurately predict future events and assess their implications is paramount for
both safety and efficiency, critically aiding the decision-making process.
World models have emerged as a transformative approach, enabling autonomous
driving systems to synthesize and interpret vast amounts of sensor data,
thereby predicting potential future scenarios and compensating for information
gaps. This paper provides an initial review of the current state and
prospective advancements of world models in autonomous driving, spanning their
theoretical underpinnings, practical applications, and the ongoing research
efforts aimed at overcoming existing limitations. Highlighting the significant
role of world models in advancing autonomous driving technologies, this survey
aspires to serve as a foundational reference for the research community,
facilitating swift access to and comprehension of this burgeoning field, and
inspiring continued innovation and exploration.
\\ ( https://arxiv.org/abs/2403.02622 ,  23655kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02624
Date: Tue, 5 Mar 2024 03:32:02 GMT   (9920kb,D)

Title: Pareto-Optimal Estimation and Policy Learning on Short-term and
  Long-term Treatment Effects
Authors: Yingrong Wang, Anpeng Wu, Haoxuan Li, Weiming Liu, Qiaowei Miao,
  Ruoxuan Xiong, Fei Wu, Kun Kuang
Categories: cs.LG cs.AI
\\
  This paper focuses on developing Pareto-optimal estimation and policy
learning to identify the most effective treatment that maximizes the total
reward from both short-term and long-term effects, which might conflict with
each other. For example, a higher dosage of medication might increase the speed
of a patient's recovery (short-term) but could also result in severe long-term
side effects. Although recent works have investigated the problems about
short-term or long-term effects or the both, how to trade-off between them to
achieve optimal treatment remains an open challenge. Moreover, when multiple
objectives are directly estimated using conventional causal representation
learning, the optimization directions among various tasks can conflict as well.
In this paper, we systematically investigate these issues and introduce a
Pareto-Efficient algorithm, comprising Pareto-Optimal Estimation (POE) and
Pareto-Optimal Policy Learning (POPL), to tackle them. POE incorporates a
continuous Pareto module with representation balancing, enhancing estimation
efficiency across multiple tasks. As for POPL, it involves deriving short-term
and long-term outcomes linked with various treatment levels, facilitating an
exploration of the Pareto frontier emanating from these outcomes. Results on
both the synthetic and real-world datasets demonstrate the superiority of our
method.
\\ ( https://arxiv.org/abs/2403.02624 ,  9920kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02630
Date: Tue, 5 Mar 2024 03:40:39 GMT   (1480kb,D)

Title: FedHCDR: Federated Cross-Domain Recommendation with Hypergraph Signal
  Decoupling
Authors: Hongyu Zhang, Dongyi Zheng, Lin Zhong, Xu Yang, Jiyuan Feng, Yunqing
  Feng, Qing Liao
Categories: cs.LG cs.IR cs.SI
Comments: 16 pages, 5 figures
\\
  In recent years, Cross-Domain Recommendation (CDR) has drawn significant
attention, which utilizes user data from multiple domains to enhance the
recommendation performance. However, current CDR methods require sharing user
data across domains, thereby violating the General Data Protection Regulation
(GDPR). Consequently, numerous approaches have been proposed for Federated
Cross-Domain Recommendation (FedCDR). Nevertheless, the data heterogeneity
across different domains inevitably influences the overall performance of
federated learning. In this study, we propose FedHCDR, a novel Federated
Cross-Domain Recommendation framework with Hypergraph signal decoupling.
Specifically, to address the data heterogeneity across domains, we introduce an
approach called hypergraph signal decoupling (HSD) to decouple the user
features into domain-exclusive and domain-shared features. The approach employs
high-pass and low-pass hypergraph filters to decouple domain-exclusive and
domain-shared user representations, which are trained by the local-global
bi-directional transfer algorithm. In addition, a hypergraph contrastive
learning (HCL) module is devised to enhance the learning of domain-shared user
relationship information by perturbing the user hypergraph. Extensive
experiments conducted on three real-world scenarios demonstrate that FedHCDR
outperforms existing baselines significantly.
\\ ( https://arxiv.org/abs/2403.02630 ,  1480kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02648
Date: Tue, 5 Mar 2024 04:35:59 GMT   (152kb,D)

Title: Remove that Square Root: A New Efficient Scale-Invariant Version of
  AdaGrad
Authors: Sayantan Choudhury, Nazarii Tupitsa, Nicolas Loizou, Samuel Horvath,
  Martin Takac, Eduard Gorbunov
Categories: cs.LG cs.AI math.OC
Comments: 26 pages, 9 figures
\\
  Adaptive methods are extremely popular in machine learning as they make
learning rate tuning less expensive. This paper introduces a novel optimization
algorithm named KATE, which presents a scale-invariant adaptation of the
well-known AdaGrad algorithm. We prove the scale-invariance of KATE for the
case of Generalized Linear Models. Moreover, for general smooth non-convex
problems, we establish a convergence rate of $O \left(\frac{\log T}{\sqrt{T}}
\right)$ for KATE, matching the best-known ones for AdaGrad and Adam. We also
compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in
numerical experiments with different problems, including complex machine
learning tasks like image classification and text classification on real data.
The results indicate that KATE consistently outperforms AdaGrad and
matches/surpasses the performance of Adam in all considered scenarios.
\\ ( https://arxiv.org/abs/2403.02648 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02681
Date: Tue, 5 Mar 2024 06:10:21 GMT   (175kb,D)

Title: SGD with Partial Hessian for Deep Neural Networks Optimization
Authors: Ying Sun, Hongwei Yong, Lei Zhang
Categories: cs.LG math.OC
\\
  Due to the effectiveness of second-order algorithms in solving classical
optimization problems, designing second-order optimizers to train deep neural
networks (DNNs) has attracted much research interest in recent years. However,
because of the very high dimension of intermediate features in DNNs, it is
difficult to directly compute and store the Hessian matrix for network
optimization. Most of the previous second-order methods approximate the Hessian
information imprecisely, resulting in unstable performance. In this work, we
propose a compound optimizer, which is a combination of a second-order
optimizer with a precise partial Hessian matrix for updating channel-wise
parameters and the first-order stochastic gradient descent (SGD) optimizer for
updating the other parameters. We show that the associated Hessian matrices of
channel-wise parameters are diagonal and can be extracted directly and
precisely from Hessian-free methods. The proposed method, namely SGD with
Partial Hessian (SGD-PH), inherits the advantages of both first-order and
second-order optimizers. Compared with first-order optimizers, it adopts a
certain amount of information from the Hessian matrix to assist optimization,
while compared with the existing second-order optimizers, it keeps the good
generalization performance of first-order optimizers. Experiments on image
classification tasks demonstrate the effectiveness of our proposed optimizer
SGD-PH. The code is publicly available at
\url{https://github.com/myingysun/SGDPH}.
\\ ( https://arxiv.org/abs/2403.02681 ,  175kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02682
Date: Tue, 5 Mar 2024 06:10:22 GMT   (41005kb,D)

Title: Time Weaver: A Conditional Time Series Generation Model
Authors: Sai Shankar Narasimhan, Shubhankar Agarwal, Oguzhan Akcin, Sujay
  Sanghavi, Sandeep Chinchali
Categories: cs.LG eess.SP
\\
  Imagine generating a city's electricity demand pattern based on weather, the
presence of an electric vehicle, and location, which could be used for capacity
planning during a winter freeze. Such real-world time series are often enriched
with paired heterogeneous contextual metadata (weather, location, etc.).
Current approaches to time series generation often ignore this paired metadata,
and its heterogeneity poses several practical challenges in adapting existing
conditional generation approaches from the image, audio, and video domains to
the time series domain. To address this gap, we introduce Time Weaver, a novel
diffusion-based model that leverages the heterogeneous metadata in the form of
categorical, continuous, and even time-variant variables to significantly
improve time series generation. Additionally, we show that naive extensions of
standard evaluation metrics from the image to the time series domain are
insufficient. These metrics do not penalize conditional generation approaches
for their poor specificity in reproducing the metadata-specific features in the
generated time series. Thus, we innovate a novel evaluation metric that
accurately captures the specificity of conditional generation and the realism
of the generated time series. We show that Time Weaver outperforms
state-of-the-art benchmarks, such as Generative Adversarial Networks (GANs), by
up to 27% in downstream classification tasks on real-world energy, medical, air
quality, and traffic data sets.
\\ ( https://arxiv.org/abs/2403.02682 ,  41005kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02683
Date: Tue, 5 Mar 2024 06:10:28 GMT   (3052kb,D)

Title: Learning to Defer to a Population: A Meta-Learning Approach
Authors: Dharmesh Tailor, Aditya Patra, Rajeev Verma, Putra Manggala, Eric
  Nalisnick
Categories: cs.LG stat.ML
Comments: Accepted at the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS 2024)
\\
  The learning to defer (L2D) framework allows autonomous systems to be safe
and robust by allocating difficult decisions to a human expert. All existing
work on L2D assumes that each expert is well-identified, and if any expert were
to change, the system should be re-trained. In this work, we alleviate this
constraint, formulating an L2D system that can cope with never-before-seen
experts at test-time. We accomplish this by using meta-learning, considering
both optimization- and model-based variants. Given a small context set to
characterize the currently available expert, our framework can quickly adapt
its deferral policy. For the model-based approach, we employ an attention
mechanism that is able to look for points in the context set that are similar
to a given test point, leading to an even more precise assessment of the
expert's abilities. In the experiments, we validate our methods on image
recognition, traffic sign detection, and skin lesion diagnosis benchmarks.
\\ ( https://arxiv.org/abs/2403.02683 ,  3052kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02690
Date: Tue, 5 Mar 2024 06:20:49 GMT   (14831kb,D)

Title: Dirichlet-based Per-Sample Weighting by Transition Matrix for Noisy
  Label Learning
Authors: HeeSun Bae, Seungjae Shin, Byeonghu Na, Il-Chul Moon
Categories: cs.LG cs.CV
Comments: 35 pages, 20 figures, Accepted to the twelfth International
  Conference on Learninig Representations (ICLR 24)
\\
  For learning with noisy labels, the transition matrix, which explicitly
models the relation between noisy label distribution and clean label
distribution, has been utilized to achieve the statistical consistency of
either the classifier or the risk. Previous researches have focused more on how
to estimate this transition matrix well, rather than how to utilize it. We
propose good utilization of the transition matrix is crucial and suggest a new
utilization method based on resampling, coined RENT. Specifically, we first
demonstrate current utilizations can have potential limitations for
implementation. As an extension to Reweighting, we suggest the Dirichlet
distribution-based per-sample Weight Sampling (DWS) framework, and compare
reweighting and resampling under DWS framework. With the analyses from DWS, we
propose RENT, a REsampling method with Noise Transition matrix. Empirically,
RENT consistently outperforms existing transition matrix utilization methods,
which includes reweighting, on various benchmark datasets. Our code is
available at \url{https://github.com/BaeHeeSun/RENT}.
\\ ( https://arxiv.org/abs/2403.02690 ,  14831kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02694
Date: Tue, 5 Mar 2024 06:23:50 GMT   (269kb,D)

Title: Privacy-Aware Semantic Cache for Large Language Models
Authors: Waris Gill (1), Mohamed Elidrisi (2), Pallavi Kalapatapu (2), Ali
  Anwar (3), Muhammad Ali Gulzar (1) ((1) Virginia Tech, USA, (2) Cisco, USA
  (3) University of Minnesota, Minneapolis, USA)
Categories: cs.LG cs.AI cs.CL cs.CR cs.DC
Comments: This study presents the first privacy aware semantic cache for LLMs
  based on Federated Learning. Total pages 12
ACM-class: I.2.7
\\
  Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2
have revolutionized natural language processing and search engine dynamics.
However, these models incur exceptionally high computational costs. For
instance, GPT-3 consists of 175 billion parameters and inference on these
models also demands billions of floating-point operations. Caching is a natural
solution to reduce LLM inference costs on repeated queries. However, existing
caching methods are incapable of finding semantic similarities among LLM
queries, leading to unacceptable false hit-and-miss rates.
  This paper introduces MeanCache, a semantic cache for LLMs that identifies
semantically similar queries to determine cache hit or miss. Using MeanCache,
the response to a user's semantically similar query can be retrieved from a
local cache rather than re-querying the LLM, thus reducing costs, service
provider load, and environmental impact. MeanCache leverages Federated Learning
(FL) to collaboratively train a query similarity model in a distributed manner
across numerous users without violating privacy. By placing a local cache in
each user's device and using FL, MeanCache reduces the latency and costs and
enhances model performance, resulting in lower cache false hit rates. Our
experiments, benchmarked against the GPTCache, reveal that MeanCache attains an
approximately 17% higher F-score and a 20% increase in precision during
semantic cache hit-and-miss decisions. Furthermore, MeanCache reduces the
storage requirement by 83% and accelerates semantic cache hit-and-miss
decisions by 11%, while still surpassing GPTCache.
\\ ( https://arxiv.org/abs/2403.02694 ,  269kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02695
Date: Tue, 5 Mar 2024 06:23:55 GMT   (6597kb,D)

Title: Controllable Prompt Tuning For Balancing Group Distributional Robustness
Authors: Hoang Phan and Andrew Gordon Wilson and Qi Lei
Categories: cs.LG
Comments: 29 pages, 11 figures, 13 tables
\\
  Models trained on data composed of different groups or domains can suffer
from severe performance degradation under distribution shifts. While recent
methods have largely focused on optimizing the worst-group objective, this
often comes at the expense of good performance on other groups. To address this
problem, we introduce an optimization scheme to achieve good performance across
groups and find a good solution for all without severely sacrificing
performance on any of them. However, directly applying such optimization
involves updating the parameters of the entire network, making it both
computationally expensive and challenging. Thus, we introduce Controllable
Prompt Tuning (CPT), which couples our approach with prompt-tuning techniques.
On spurious correlation benchmarks, our procedures achieve state-of-the-art
results across both transformer and non-transformer architectures, as well as
unimodal and multimodal data, while requiring only 0.4% tunable parameters.
\\ ( https://arxiv.org/abs/2403.02695 ,  6597kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02730
Date: Tue, 5 Mar 2024 07:37:47 GMT   (836kb,D)

Title: A Two-Stage Training Method for Modeling Constrained Systems With Neural
  Networks
Authors: C. Coelho, M. Fernanda P. Costa, L.L. Ferr\'as
Categories: cs.LG cs.CE math.OC
MSC-class: 35A01, 65L10, 65L12, 65L20, 65L70
ACM-class: I.5.1; G.1.6
\\
  Real-world systems are often formulated as constrained optimization problems.
Techniques to incorporate constraints into Neural Networks (NN), such as Neural
Ordinary Differential Equations (Neural ODEs), have been used. However, these
introduce hyperparameters that require manual tuning through trial and error,
raising doubts about the successful incorporation of constraints into the
generated model. This paper describes in detail the two-stage training method
for Neural ODEs, a simple, effective, and penalty parameter-free approach to
model constrained systems. In this approach the constrained optimization
problem is rewritten as two unconstrained sub-problems that are solved in two
stages. The first stage aims at finding feasible NN parameters by minimizing a
measure of constraints violation. The second stage aims to find the optimal NN
parameters by minimizing the loss function while keeping inside the feasible
region. We experimentally demonstrate that our method produces models that
satisfy the constraints and also improves their predictive performance. Thus,
ensuring compliance with critical system properties and also contributing to
reducing data quantity requirements. Furthermore, we show that the proposed
method improves the convergence to an optimal solution and improves the
explainability of Neural ODE models. Our proposed two-stage training method can
be used with any NN architectures.
\\ ( https://arxiv.org/abs/2403.02730 ,  836kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02737
Date: Tue, 5 Mar 2024 07:45:29 GMT   (629kb,D)

Title: Neural Fractional Differential Equations
Authors: C. Coelho, M. Fernanda P. Costa, L.L. Ferr\'as
Categories: cs.LG cs.CE cs.NA math.NA
MSC-class: G.1, G.1.10, G.4, I.5.1
\\
  Fractional Differential Equations (FDEs) are essential tools for modelling
complex systems in science and engineering. They extend the traditional
concepts of differentiation and integration to non-integer orders, enabling a
more precise representation of processes characterised by non-local and
memory-dependent behaviours.
  This property is useful in systems where variables do not respond to changes
instantaneously, but instead exhibit a strong memory of past interactions.
  Having this in mind, and drawing inspiration from Neural Ordinary
Differential Equations (Neural ODEs), we propose the Neural FDE, a novel deep
neural network architecture that adjusts a FDE to the dynamics of data.
  This work provides a comprehensive overview of the numerical method employed
in Neural FDEs and the Neural FDE architecture. The numerical outcomes suggest
that, despite being more computationally demanding, the Neural FDE may
outperform the Neural ODE in modelling systems with memory or dependencies on
past states, and it can effectively be applied to learn more intricate
dynamical systems.
\\ ( https://arxiv.org/abs/2403.02737 ,  629kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02765
Date: Tue, 5 Mar 2024 08:34:04 GMT   (2466kb,D)

Title: G4-Attention: Deep Learning Model with Attention for predicting DNA
  G-Quadruplexes
Authors: Shrimon Mukherjee, Pulakesh Pramanik, Partha Basuchowdhuri, Santanu
  Bhattacharya
Categories: cs.LG q-bio.BM
\\
  G-Quadruplexes are the four-stranded non-canonical nucleic acid secondary
structures, formed by the stacking arrangement of the guanine tetramers. They
are involved in a wide range of biological roles because of their exceptionally
unique and distinct structural characteristics. After the completion of the
human genome sequencing project, a lot of bioinformatic algorithms were
introduced to predict the active G4s regions \textit{in vitro} based on the
canonical G4 sequence elements, G-\textit{richness}, and G-\textit{skewness},
as well as the non-canonical sequence features. Recently, sequencing techniques
like G4-seq and G4-ChIP-seq were developed to map the G4s \textit{in vitro},
and \textit{in vivo} respectively at a few hundred base resolution.
Subsequently, several machine learning approaches were developed for predicting
the G4 regions using the existing databases. However, their prediction models
were simplistic, and the prediction accuracy was notably poor. In response,
here, we propose a novel convolutional neural network with Bi-LSTM and
attention layers, named G4-attention, to predict the G4 forming sequences with
improved accuracy. G4-attention achieves high accuracy and attains
state-of-the-art results in the G4 prediction task. Our model also predicts the
G4 regions accurately in the highly class-imbalanced datasets. In addition, the
developed model trained on the human genome dataset can be applied to any
non-human genome DNA sequences to predict the G4 formation propensities.
\\ ( https://arxiv.org/abs/2403.02765 ,  2466kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02772
Date: Tue, 5 Mar 2024 08:38:25 GMT   (1045kb,D)

Title: Rehabilitation Exercise Quality Assessment through Supervised
  Contrastive Learning with Hard and Soft Negatives
Authors: Mark Karlov, Ali Abedi, Shehroz S. Khan
Categories: cs.LG cs.AI cs.CV cs.CY
\\
  Exercise-based rehabilitation programs have proven to be effective in
enhancing the quality of life and reducing mortality and rehospitalization
rates. AI-driven virtual rehabilitation, which allows patients to independently
complete exercises at home, utilizes AI algorithms to analyze exercise data,
providing feedback to patients and updating clinicians on their progress. These
programs commonly prescribe a variety of exercise types, leading to a distinct
challenge in rehabilitation exercise assessment datasets: while abundant in
overall training samples, these datasets often have a limited number of samples
for each individual exercise type. This disparity hampers the ability of
existing approaches to train generalizable models with such a small sample size
per exercise. Addressing this issue, our paper introduces a novel supervised
contrastive learning framework with hard and soft negative samples that
effectively utilizes the entire dataset to train a single model applicable to
all exercise types. This model, with a Spatial-Temporal Graph Convolutional
Network (ST-GCN) architecture, demonstrated enhanced generalizability across
exercises and a decrease in overall complexity. Through extensive experiments
on three publicly available rehabilitation exercise assessment datasets, the
University of Idaho-Physical Rehabilitation Movement Data (UI-PRMD),
IntelliRehabDS (IRDS), and KInematic assessment of MOvement and clinical scores
for remote monitoring of physical REhabilitation (KIMORE), our method has shown
to surpass existing methods, setting a new benchmark in rehabilitation exercise
assessment accuracy.
\\ ( https://arxiv.org/abs/2403.02772 ,  1045kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02777
Date: Tue, 5 Mar 2024 08:46:54 GMT   (895kb,D)

Title: A Zero-Shot Reinforcement Learning Strategy for Autonomous Guidewire
  Navigation
Authors: Valentina Scarponi (MIMESIS, ICube), Michel Duprez (ICube, MIMESIS),
  Florent Nageotte (ICube), St\'ephane Cotin (ICube, MIMESIS)
Categories: cs.LG cs.RO physics.med-ph
Comments: International Journal of Computer Assisted Radiology and Surgery, In
  press
\\
  Purpose: The treatment of cardiovascular diseases requires complex and
challenging navigation of a guidewire and catheter. This often leads to lengthy
interventions during which the patient and clinician are exposed to X-ray
radiation. Deep Reinforcement Learning approaches have shown promise in
learning this task and may be the key to automating catheter navigation during
robotized interventions. Yet, existing training methods show limited
capabilities at generalizing to unseen vascular anatomies, requiring to be
retrained each time the geometry changes. Methods: In this paper, we propose a
zero-shot learning strategy for three-dimensional autonomous endovascular
navigation. Using a very small training set of branching patterns, our
reinforcement learning algorithm is able to learn a control that can then be
applied to unseen vascular anatomies without retraining. Results: We
demonstrate our method on 4 different vascular systems, with an average success
rate of 95% at reaching random targets on these anatomies. Our strategy is also
computationally efficient, allowing the training of our controller to be
performed in only 2 hours. Conclusion: Our training method proved its ability
to navigate unseen geometries with different characteristics, thanks to a
nearly shape-invariant observation space.
\\ ( https://arxiv.org/abs/2403.02777 ,  895kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02780
Date: Tue, 5 Mar 2024 08:52:16 GMT   (384kb)

Title: Data Collaboration Analysis Over Matrix Manifolds
Authors: Keiyu Nosaka, Akiko Yoshise
Categories: cs.LG math.OC
Comments: 29 pages
\\
  The effectiveness of machine learning (ML) algorithms is deeply intertwined
with the quality and diversity of their training datasets. Improved datasets,
marked by superior quality, enhance the predictive accuracy and broaden the
applicability of models across varied scenarios. Researchers often integrate
data from multiple sources to mitigate biases and limitations of single-source
datasets. However, this extensive data amalgamation raises significant ethical
concerns, particularly regarding user privacy and the risk of unauthorized data
disclosure. Various global legislative frameworks have been established to
address these privacy issues. While crucial for safeguarding privacy, these
regulations can complicate the practical deployment of ML technologies.
Privacy-Preserving Machine Learning (PPML) addresses this challenge by
safeguarding sensitive information, from health records to geolocation data,
while enabling the secure use of this data in developing robust ML models.
Within this realm, the Non-Readily Identifiable Data Collaboration (NRI-DC)
framework emerges as an innovative approach, potentially resolving the 'data
island' issue among institutions through non-iterative communication and robust
privacy protections. However, in its current state, the NRI-DC framework faces
model performance instability due to theoretical unsteadiness in creating
collaboration functions. This study establishes a rigorous theoretical
foundation for these collaboration functions and introduces new formulations
through optimization problems on matrix manifolds and efficient solutions.
Empirical analyses demonstrate that the proposed approach, particularly the
formulation over orthogonal matrix manifolds, significantly enhances
performance, maintaining consistency and efficiency without compromising
communication efficiency or privacy protections.
\\ ( https://arxiv.org/abs/2403.02780 ,  384kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02786
Date: Tue, 5 Mar 2024 08:59:45 GMT   (1419kb,D)

Title: Semi-Supervised Graph Representation Learning with Human-centric
  Explanation for Predicting Fatty Liver Disease
Authors: So Yeon Kim, Sehee Wang, Eun Kyung Choe
Categories: cs.LG cs.AI
Comments: Paper accepted in Human-Centric Representation Learning workshop at
  AAAI 2024 (https://hcrl-workshop.github.io/2024/)
\\
  Addressing the challenge of limited labeled data in clinical settings,
particularly in the prediction of fatty liver disease, this study explores the
potential of graph representation learning within a semi-supervised learning
framework. Leveraging graph neural networks (GNNs), our approach constructs a
subject similarity graph to identify risk patterns from health checkup data.
The effectiveness of various GNN approaches in this context is demonstrated,
even with minimal labeled samples. Central to our methodology is the inclusion
of human-centric explanations through explainable GNNs, providing personalized
feature importance scores for enhanced interpretability and clinical relevance,
thereby underscoring the potential of our approach in advancing healthcare
practices with a keen focus on graph representation learning and human-centric
explanation.
\\ ( https://arxiv.org/abs/2403.02786 ,  1419kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02810
Date: Tue, 5 Mar 2024 09:25:31 GMT   (3714kb)

Title: Dynamic Gaussian Graph Operator: Learning parametric partial
  differential equations in arbitrary discrete mechanics problems
Authors: Chu Wang, Jinhong Wu, Yanzhi Wang, Zhijian Zha, Qi Zhou
Categories: cs.LG cs.AI
Comments: The number of figures is 13. The number of tables is 7. The number of
  words is 9854
\\
  Deep learning methods have access to be employed for solving physical systems
governed by parametric partial differential equations (PDEs) due to massive
scientific data. It has been refined to operator learning that focuses on
learning non-linear mapping between infinite-dimensional function spaces,
offering interface from observations to solutions. However, state-of-the-art
neural operators are limited to constant and uniform discretization, thereby
leading to deficiency in generalization on arbitrary discretization schemes for
computational domain. In this work, we propose a novel operator learning
algorithm, referred to as Dynamic Gaussian Graph Operator (DGGO) that expands
neural operators to learning parametric PDEs in arbitrary discrete mechanics
problems. The Dynamic Gaussian Graph (DGG) kernel learns to map the observation
vectors defined in general Euclidean space to metric vectors defined in
high-dimensional uniform metric space. The DGG integral kernel is parameterized
by Gaussian kernel weighted Riemann sum approximating and using dynamic message
passing graph to depict the interrelation within the integral term. Fourier
Neural Operator is selected to localize the metric vectors on spatial and
frequency domains. Metric vectors are regarded as located on latent uniform
domain, wherein spatial and spectral transformation offer highly regular
constraints on solution space. The efficiency and robustness of DGGO are
validated by applying it to solve numerical arbitrary discrete mechanics
problems in comparison with mainstream neural operators. Ablation experiments
are implemented to demonstrate the effectiveness of spatial transformation in
the DGG kernel. The proposed method is utilized to forecast stress field of
hyper-elastic material with geometrically variable void as engineering
application.
\\ ( https://arxiv.org/abs/2403.02810 ,  3714kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02814
Date: Tue, 5 Mar 2024 09:33:36 GMT   (298kb,D)

Title: InjectTST: A Transformer Method of Injecting Global Information into
  Independent Channels for Long Time Series Forecasting
Authors: Ce Chi, Xing Wang, Kexin Yang, Zhiyan Song, Di Jin, Lin Zhu, Chao
  Deng, Junlan Feng
Categories: cs.LG cs.AI
\\
  Transformer has become one of the most popular architectures for multivariate
time series (MTS) forecasting. Recent Transformer-based MTS models generally
prefer channel-independent structures with the observation that channel
independence can alleviate noise and distribution drift issues, leading to more
robustness. Nevertheless, it is essential to note that channel dependency
remains an inherent characteristic of MTS, carrying valuable information.
Designing a model that incorporates merits of both channel-independent and
channel-mixing structures is a key to further improvement of MTS forecasting,
which poses a challenging conundrum. To address the problem, an injection
method for global information into channel-independent Transformer, InjectTST,
is proposed in this paper. Instead of designing a channel-mixing model
directly, we retain the channel-independent backbone and gradually inject
global information into individual channels in a selective way. A channel
identifier, a global mixing module and a self-contextual attention module are
devised in InjectTST. The channel identifier can help Transformer distinguish
channels for better representation. The global mixing module produces
cross-channel global information. Through the self-contextual attention module,
the independent channels can selectively concentrate on useful global
information without robustness degradation, and channel mixing is achieved
implicitly. Experiments indicate that InjectTST can achieve stable improvement
compared with state-of-the-art models.
\\ ( https://arxiv.org/abs/2403.02814 ,  298kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02821
Date: Tue, 5 Mar 2024 09:44:51 GMT   (217kb,D)

Title: An Adaptive Hydropower Management Approach for Downstream Ecosystem
  Preservation
Authors: C. Coelho, M. Jing, M. Fernanda P. Costa, L.L. Ferr\'as
Categories: cs.LG cs.CE math.OC
ACM-class: J.2; I.5.1; G.1.6
\\
  Hydropower plants play a pivotal role in advancing clean and sustainable
energy production, contributing significantly to the global transition towards
renewable energy sources. However, hydropower plants are currently perceived
both positively as sources of renewable energy and negatively as disruptors of
ecosystems. In this work, we highlight the overlooked potential of using
hydropower plant as protectors of ecosystems by using adaptive ecological
discharges. To advocate for this perspective, we propose using a neural network
to predict the minimum ecological discharge value at each desired time.
Additionally, we present a novel framework that seamlessly integrates it into
hydropower management software, taking advantage of the well-established
approach of using traditional constrained optimisation algorithms. This novel
approach not only protects the ecosystems from climate change but also
contributes to potentially increase the electricity production.
\\ ( https://arxiv.org/abs/2403.02821 ,  217kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02833
Date: Tue, 5 Mar 2024 10:09:31 GMT   (862kb,D)

Title: SOFIM: Stochastic Optimization Using Regularized Fisher Information
  Matrix
Authors: Gayathri C, Mrinmay Sen, A. K. Qin, Raghu Kishore N, Yen-Wei Chen,
  Balasubramanian Raman
Categories: cs.LG cs.NE
\\
  This paper introduces a new stochastic optimization method based on the
regularized Fisher information matrix (FIM), named SOFIM, which can efficiently
utilize the FIM to approximate the Hessian matrix for finding Newton's gradient
update in large-scale stochastic optimization of machine learning models. It
can be viewed as a variant of natural gradient descent (NGD), where the
challenge of storing and calculating the full FIM is addressed through making
use of the regularized FIM and directly finding the gradient update direction
via Sherman-Morrison matrix inversion. Additionally, like the popular Adam
method, SOFIM uses the first moment of the gradient to address the issue of
non-stationary objectives across mini-batches due to heterogeneous data. The
utilization of the regularized FIM and Sherman-Morrison matrix inversion leads
to the improved convergence rate with the same space and time complexities as
stochastic gradient descent (SGD) with momentum. The extensive experiments on
training deep learning models on several benchmark image classification
datasets demonstrate that the proposed SOFIM outperforms SGD with momentum and
several state-of-the-art Newton optimization methods, such as Nystrom-SGD,
L-BFGS, and AdaHessian, in term of the convergence speed for achieving the
pre-specified objectives of training and test losses as well as test accuracy.
\\ ( https://arxiv.org/abs/2403.02833 ,  862kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02846
Date: Tue, 5 Mar 2024 10:36:27 GMT   (677kb,D)

Title: FLGuard: Byzantine-Robust Federated Learning via Ensemble of Contrastive
  Models
Authors: Younghan Lee, Yungi Cho, Woorim Han, Ho Bae, and Yunheung Paek
Categories: cs.LG cs.AI cs.CR cs.DC
Comments: Accepted by 28th European Symposium on Research in Computer Security
  (ESORICS 2023)
\\
  Federated Learning (FL) thrives in training a global model with numerous
clients by only sharing the parameters of their local models trained with their
private training datasets. Therefore, without revealing the private dataset,
the clients can obtain a deep learning (DL) model with high performance.
However, recent research proposed poisoning attacks that cause a catastrophic
loss in the accuracy of the global model when adversaries, posed as benign
clients, are present in a group of clients. Therefore, recent studies suggested
byzantine-robust FL methods that allow the server to train an accurate global
model even with the adversaries present in the system. However, many existing
methods require the knowledge of the number of malicious clients or the
auxiliary (clean) dataset or the effectiveness reportedly decreased hugely when
the private dataset was non-independently and identically distributed
(non-IID). In this work, we propose FLGuard, a novel byzantine-robust FL method
that detects malicious clients and discards malicious local updates by
utilizing the contrastive learning technique, which showed a tremendous
improvement as a self-supervised learning method. With contrastive models, we
design FLGuard as an ensemble scheme to maximize the defensive capability. We
evaluate FLGuard extensively under various poisoning attacks and compare the
accuracy of the global model with existing byzantine-robust FL methods. FLGuard
outperforms the state-of-the-art defense methods in most cases and shows
drastic improvement, especially in non-IID settings.
https://github.com/201younghanlee/FLGuard
\\ ( https://arxiv.org/abs/2403.02846 ,  677kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02873
Date: Tue, 5 Mar 2024 11:38:20 GMT   (11kb,D)

Title: A Note on High-Probability Analysis of Algorithms with Exponential,
  Sub-Gaussian, and General Light Tails
Authors: Amit Attia, Tomer Koren
Categories: cs.LG cs.DS math.PR
Comments: 9 pages
\\
  This short note describes a simple technique for analyzing probabilistic
algorithms that rely on a light-tailed (but not necessarily bounded) source of
randomization. We show that the analysis of such an algorithm can be reduced,
in a black-box manner and with only a small loss in logarithmic factors, to an
analysis of a simpler variant of the same algorithm that uses bounded random
variables and often easier to analyze. This approach simultaneously applies to
any light-tailed randomization, including exponential, sub-Gaussian, and more
general fast-decaying distributions, without needing to appeal to specialized
concentration inequalities. Analyses of a generalized Azuma inequality and
stochastic optimization with general light-tailed noise are provided to
illustrate the technique.
\\ ( https://arxiv.org/abs/2403.02873 ,  11kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02920
Date: Tue, 5 Mar 2024 12:38:14 GMT   (128kb,D)

Title: TaylorShift: Shifting the Complexity of Self-Attention from Squared to
  Linear (and Back) using Taylor-Softmax
Authors: Tobias Christian Nauen, Sebastian Palacio, Andreas Dengel
Categories: cs.LG cs.AI
MSC-class: 68T07
ACM-class: I.5.1; I.2.10; I.2.7
\\
  The quadratic complexity of the attention mechanism represents one of the
biggest hurdles for processing long sequences using Transformers. Current
methods, relying on sparse representations or stateful recurrence, sacrifice
token-to-token interactions, which ultimately leads to compromises in
performance. This paper introduces TaylorShift, a novel reformulation of the
Taylor softmax that enables computing full token-to-token interactions in
linear time and space. We analytically determine the crossover points where
employing TaylorShift becomes more efficient than traditional attention,
aligning closely with empirical measurements. Specifically, our findings
demonstrate that TaylorShift enhances memory efficiency for sequences as short
as 800 tokens and accelerates inference for inputs of approximately 1700 tokens
and beyond. For shorter sequences, TaylorShift scales comparably with the
vanilla attention. Furthermore, a classification benchmark across five tasks
involving long sequences reveals no degradation in accuracy when employing
Transformers equipped with TaylorShift. For reproducibility, we provide access
to our code under https://github.com/tobna/TaylorShift.
\\ ( https://arxiv.org/abs/2403.02920 ,  128kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02922
Date: Tue, 5 Mar 2024 12:38:54 GMT   (35001kb,D)

Title: From Spectra to Biophysical Insights: End-to-End Learning with a Biased
  Radiative Transfer Model
Authors: Yihang She, Clement Atzberger, Andrew Blake, Srinivasan Keshav
Categories: cs.LG
\\
  Advances in machine learning have boosted the use of Earth observation data
for climate change research. Yet, the interpretability of machine-learned
representations remains a challenge, particularly in understanding forests'
biophysical reactions to climate change. Traditional methods in remote sensing
that invert radiative transfer models (RTMs) to retrieve biophysical variables
from spectral data often fail to account for biases inherent in the RTM,
especially for complex forests. We propose to integrate RTMs into an
auto-encoder architecture, creating an end-to-end learning approach. Our method
not only corrects biases in RTMs but also outperforms traditional techniques
for variable retrieval like neural network regression. Furthermore, our
framework has potential generally for inverting biased physical models. The
code is available on https://github.com/yihshe/ai-refined-rtm.git.
\\ ( https://arxiv.org/abs/2403.02922 ,  35001kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02945
Date: Tue, 5 Mar 2024 13:16:37 GMT   (1707kb,D)

Title: Unsupervised Learning Approaches for Identifying ICU Patient Subgroups:
  Do Results Generalise?
Authors: Harry Mayne, Guy Parsons and Adam Mahdi
Categories: cs.LG
\\
  The use of unsupervised learning to identify patient subgroups has emerged as
a potentially promising direction to improve the efficiency of Intensive Care
Units (ICUs). By identifying subgroups of patients with similar levels of
medical resource need, ICUs could be restructured into a collection of smaller
subunits, each catering to a specific group. However, it is unclear whether
common patient subgroups exist across different ICUs, which would determine
whether ICU restructuring could be operationalised in a standardised manner. In
this paper, we tested the hypothesis that common ICU patient subgroups exist by
examining whether the results from one existing study generalise to a different
dataset. We extracted 16 features representing medical resource need and used
consensus clustering to derive patient subgroups, replicating the previous
study. We found limited similarities between our results and those of the
previous study, providing evidence against the hypothesis. Our findings imply
that there is significant variation between ICUs; thus, a standardised
restructuring approach is unlikely to be appropriate. Instead, potential
efficiency gains might be greater when the number and nature of the subunits
are tailored to each ICU individually.
\\ ( https://arxiv.org/abs/2403.02945 ,  1707kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02957
Date: Tue, 5 Mar 2024 13:25:44 GMT   (794kb,D)

Title: On the Asymptotic Mean Square Error Optimality of Diffusion
  Probabilistic Models
Authors: Benedikt Fesl and Benedikt B\"ock and Florian Strasser and Michael
  Baur and Michael Joham and Wolfgang Utschick
Categories: cs.LG stat.ML
\\
  Diffusion probabilistic models (DPMs) have recently shown great potential for
denoising tasks. Despite their practical utility, there is a notable gap in
their theoretical understanding. This paper contributes novel theoretical
insights by rigorously proving the asymptotic convergence of a specific DPM
denoising strategy to the mean square error (MSE)-optimal conditional mean
estimator (CME) over a large number of diffusion steps. The studied DPM-based
denoiser shares the training procedure of DPMs but distinguishes itself by
forwarding only the conditional mean during the reverse inference process after
training. We highlight the unique perspective that DPMs are composed of an
asymptotically optimal denoiser while simultaneously inheriting a powerful
generator by switching re-sampling in the reverse process on and off. The
theoretical findings are validated by numerical results.
\\ ( https://arxiv.org/abs/2403.02957 ,  794kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03018
Date: Tue, 5 Mar 2024 14:55:14 GMT   (720kb,D)

Title: CRISPR: Ensemble Model
Authors: Mohammad Rostami, Amin Ghariyazi, Hamed Dashti, Mohammad Hossein
  Rohban, Hamid R. Rabiee
Categories: cs.LG q-bio.GN
\\
  Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) is a gene
editing technology that has revolutionized the fields of biology and medicine.
However, one of the challenges of using CRISPR is predicting the on-target
efficacy and off-target sensitivity of single-guide RNAs (sgRNAs). This is
because most existing methods are trained on separate datasets with different
genes and cells, which limits their generalizability. In this paper, we propose
a novel ensemble learning method for sgRNA design that is accurate and
generalizable. Our method combines the predictions of multiple machine learning
models to produce a single, more robust prediction. This approach allows us to
learn from a wider range of data, which improves the generalizability of our
model. We evaluated our method on a benchmark dataset of sgRNA designs and
found that it outperformed existing methods in terms of both accuracy and
generalizability. Our results suggest that our method can be used to design
sgRNAs with high sensitivity and specificity, even for new genes or cells. This
could have important implications for the clinical use of CRISPR, as it would
allow researchers to design more effective and safer treatments for a variety
of diseases.
\\ ( https://arxiv.org/abs/2403.03018 ,  720kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03020
Date: Tue, 5 Mar 2024 14:57:04 GMT   (14580kb,D)

Title: SplAgger: Split Aggregation for Meta-Reinforcement Learning
Authors: Jacob Beck, Matthew Jackson, Risto Vuorio, Zheng Xiong, Shimon
  Whiteson
Categories: cs.LG cs.AI
\\
  A core ambition of reinforcement learning (RL) is the creation of agents
capable of rapid learning in novel tasks. Meta-RL aims to achieve this by
directly learning such agents. One category of meta-RL methods, called black
box methods, does so by training off-the-shelf sequence models end-to-end. In
contrast, another category of methods have been developed that explicitly infer
a posterior distribution over the unknown task. These methods generally have
distinct objectives and sequence models designed to enable task inference, and
so are known as task inference methods. However, recent evidence suggests that
task inference objectives are unnecessary in practice. Nonetheless, it remains
unclear whether task inference sequence models are beneficial even when task
inference objectives are not. In this paper, we present strong evidence that
task inference sequence models are still beneficial. In particular, we
investigate sequence models with permutation invariant aggregation, which
exploit the fact that, due to the Markov property, the task posterior does not
depend on the order of data. We empirically confirm the advantage of
permutation invariant sequence models without the use of task inference
objectives. However, we also find, surprisingly, that there are multiple
conditions under which permutation variance remains useful. Therefore, we
propose SplAgger, which uses both permutation variant and invariant components
to achieve the best of both worlds, outperforming all baselines on continuous
control and memory environments.
\\ ( https://arxiv.org/abs/2403.03020 ,  14580kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03069
Date: Tue, 5 Mar 2024 15:57:52 GMT   (2740kb,D)

Title: Improving Variational Autoencoder Estimation from Incomplete Data with
  Mixture Variational Families
Authors: Vaidotas Simkus, Michael U. Gutmann
Categories: cs.LG stat.ML
MSC-class: 62D10
ACM-class: I.2.6; G.3
\\
  We consider the task of estimating variational autoencoders (VAEs) when the
training data is incomplete. We show that missing data increases the complexity
of the model's posterior distribution over the latent variables compared to the
fully-observed case. The increased complexity may adversely affect the fit of
the model due to a mismatch between the variational and model posterior
distributions. We introduce two strategies based on (i) finite
variational-mixture and (ii) imputation-based variational-mixture distributions
to address the increased posterior complexity. Through a comprehensive
evaluation of the proposed approaches, we show that variational mixtures are
effective at improving the accuracy of VAE estimation from incomplete data.
\\ ( https://arxiv.org/abs/2403.03069 ,  2740kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03082
Date: Tue, 5 Mar 2024 16:08:59 GMT   (2317kb,D)

Title: Recall-Oriented Continual Learning with Generative Adversarial
  Meta-Model
Authors: Haneol Kang, Dong-Wan Choi
Categories: cs.LG cs.AI cs.CV
Comments: Accepted in AAAI-2024 (Oral presentation)
\\
  The stability-plasticity dilemma is a major challenge in continual learning,
as it involves balancing the conflicting objectives of maintaining performance
on previous tasks while learning new tasks. In this paper, we propose the
recall-oriented continual learning framework to address this challenge.
Inspired by the human brain's ability to separate the mechanisms responsible
for stability and plasticity, our framework consists of a two-level
architecture where an inference network effectively acquires new knowledge and
a generative network recalls past knowledge when necessary. In particular, to
maximize the stability of past knowledge, we investigate the complexity of
knowledge depending on different representations, and thereby introducing
generative adversarial meta-model (GAMM) that incrementally learns
task-specific parameters instead of input data samples of the task. Through our
experiments, we show that our framework not only effectively learns new
knowledge without any disruption but also achieves high stability of previous
knowledge in both task-aware and task-agnostic learning scenarios. Our code is
available at: https://github.com/bigdata-inha/recall-oriented-cl-framework.
\\ ( https://arxiv.org/abs/2403.03082 ,  2317kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03103
Date: Tue, 5 Mar 2024 16:43:25 GMT   (361kb,D)

Title: Emergent Equivariance in Deep Ensembles
Authors: Jan E. Gerken and Pan Kessel
Categories: cs.LG
Comments: 10 pages + 15 pages appendices
\\
  We demonstrate that deep ensembles are secretly equivariant models. More
precisely, we show that deep ensembles become equivariant for all inputs and at
all training times by simply using data augmentation. Crucially, equivariance
holds off-manifold and for any architecture in the infinite width limit. The
equivariance is emergent in the sense that predictions of individual ensemble
members are not equivariant but their collective prediction is. Neural tangent
kernel theory is used to derive this result and we verify our theoretical
insights using detailed numerical experiments.
\\ ( https://arxiv.org/abs/2403.03103 ,  361kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03150
Date: Tue, 5 Mar 2024 17:42:39 GMT   (1867kb,D)

Title: Deep-Learned Compression for Radio-Frequency Signal Classification
Authors: Armani Rodriguez, Yagna Kaasaragadda, Silvija Kokalj-Filipovic
Categories: cs.LG cs.NI eess.SP
\\
  Next-generation cellular concepts rely on the processing of large quantities
of radio-frequency (RF) samples. This includes Radio Access Networks (RAN)
connecting the cellular front-end based on software defined radios (SDRs) and a
framework for the AI processing of spectrum-related data. The RF data collected
by the dense RAN radio units and spectrum sensors may need to be jointly
processed for intelligent decision making. Moving large amounts of data to AI
agents may result in significant bandwidth and latency costs. We propose a deep
learned compression (DLC) model, HQARF, based on learned vector quantization
(VQ), to compress the complex-valued samples of RF signals comprised of 6
modulation classes. We are assessing the effects of HQARF on the performance of
an AI model trained to infer the modulation class of the RF signal. Compression
of narrow-band RF samples for the training and off-the-site inference will
allow for an efficient use of the bandwidth and storage for non-real-time
analytics, and for a decreased delay in real-time applications. While exploring
the effectiveness of the HQARF signal reconstructions in modulation
classification tasks, we highlight the DLC optimization space and some open
problems related to the training of the VQ embedded in HQARF.
\\ ( https://arxiv.org/abs/2403.03150 ,  1867kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03181
Date: Tue, 5 Mar 2024 18:19:29 GMT   (6341kb,D)

Title: Behavior Generation with Latent Actions
Authors: Seungjae Lee and Yibin Wang and Haritheja Etukuru and H. Jin Kim and
  Nur Muhammad Mahi Shafiullah and Lerrel Pinto
Categories: cs.LG cs.AI cs.RO
Comments: Github repo: https://github.com/jayLEE0301/vq_bet_official
\\
  Generative modeling of complex behaviors from labeled datasets has been a
longstanding problem in decision making. Unlike language or image generation,
decision making requires modeling actions - continuous-valued vectors that are
multimodal in their distribution, potentially drawn from uncurated sources,
where generation errors can compound in sequential prediction. A recent class
of models called Behavior Transformers (BeT) addresses this by discretizing
actions using k-means clustering to capture different modes. However, k-means
struggles to scale for high-dimensional action spaces or long sequences, and
lacks gradient information, and thus BeT suffers in modeling long-range
actions. In this work, we present Vector-Quantized Behavior Transformer
(VQ-BeT), a versatile model for behavior generation that handles multimodal
action prediction, conditional generation, and partial observations. VQ-BeT
augments BeT by tokenizing continuous actions with a hierarchical vector
quantization module. Across seven environments including simulated
manipulation, autonomous driving, and robotics, VQ-BeT improves on
state-of-the-art models such as BeT and Diffusion Policies. Importantly, we
demonstrate VQ-BeT's improved ability to capture behavior modes while
accelerating inference speed 5x over Diffusion Policies. Videos and code can be
found https://sjlee.cc/vq-bet
\\ ( https://arxiv.org/abs/2403.03181 ,  6341kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03183
Date: Tue, 5 Mar 2024 18:20:10 GMT   (1176kb,D)

Title: How Well Can Transformers Emulate In-context Newton's Method?
Authors: Angeliki Giannou, Liu Yang, Tianhao Wang, Dimitris Papailiopoulos,
  Jason D. Lee
Categories: cs.LG cs.AI math.OC stat.ML
\\
  Transformer-based models have demonstrated remarkable in-context learning
capabilities, prompting extensive research into its underlying mechanisms.
Recent studies have suggested that Transformers can implement first-order
optimization algorithms for in-context learning and even second order ones for
the case of linear regression. In this work, we study whether Transformers can
perform higher order optimization methods, beyond the case of linear
regression. We establish that linear attention Transformers with ReLU layers
can approximate second order optimization algorithms for the task of logistic
regression and achieve $\epsilon$ error with only a logarithmic to the error
more layers. As a by-product we demonstrate the ability of even linear
attention-only Transformers in implementing a single step of Newton's iteration
for matrix inversion with merely two layers. These results suggest the ability
of the Transformer architecture to implement complex algorithms, beyond
gradient descent.
\\ ( https://arxiv.org/abs/2403.03183 ,  1176kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03185
Date: Tue, 5 Mar 2024 18:22:15 GMT   (192kb,D)

Title: Preventing Reward Hacking with Occupancy Measure Regularization
Authors: Cassidy Laidlaw, Shivam Singhal, Anca Dragan
Categories: cs.LG cs.AI
\\
  Reward hacking occurs when an agent performs very well with respect to a
"proxy" reward function (which may be hand-specified or learned), but poorly
with respect to the unknown true reward. Since ensuring good alignment between
the proxy and true reward is extremely difficult, one approach to prevent
reward hacking is optimizing the proxy conservatively. Prior work has
particularly focused on enforcing the learned policy to behave similarly to a
"safe" policy by penalizing the KL divergence between their action
distributions (AD). However, AD regularization doesn't always work well since a
small change in action distribution at a single state can lead to potentially
calamitous outcomes, while large changes might not be indicative of any
dangerous activity. Our insight is that when reward hacking, the agent visits
drastically different states from those reached by the safe policy, causing
large deviations in state occupancy measure (OM). Thus, we propose regularizing
based on the OM divergence between policies instead of AD divergence to prevent
reward hacking. We theoretically establish that OM regularization can more
effectively avoid large drops in true reward. Then, we empirically demonstrate
in a variety of realistic environments that OM divergence is superior to AD
divergence for preventing reward hacking by regularizing towards a safe policy.
Furthermore, we show that occupancy measure divergence can also regularize
learned policies away from reward hacking behavior. Our code and data are
available at https://github.com/cassidylaidlaw/orpo
\\ ( https://arxiv.org/abs/2403.03185 ,  192kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03218
Date: Tue, 5 Mar 2024 18:59:35 GMT   (721kb,D)

Title: The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning
Authors: Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios,
  Alice Gatti, Justin D. Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan,
  Gabriel Mukobi, Nathan Helm-Burger, Rassin Lababidi, Lennart Justen, Andrew
  B. Liu, Michael Chen, Isabelle Barrass, Oliver Zhang, Xiaoyuan Zhu, Rishub
  Tamirisa, Bhrugu Bharathi, Adam Khoja, Ariel Herbert-Voss, Cort B. Breuer,
  Andy Zou, Mantas Mazeika, Zifan Wang, Palash Oswal, Weiran Liu, Adam A. Hunt,
  Justin Tienken-Harder, Kevin Y. Shih, Kemper Talley, John Guan, Russell
  Kaplan, Ian Steneker, David Campbell, Brad Jokubaitis, Alex Levinson, Jean
  Wang, William Qian, Kallol Krishna Karmakar, Steven Basart, Stephen Fitz,
  Mindy Levine, Ponnurangam Kumaraguru, Uday Tupakula, Vijay Varadharajan, Yan
  Shoshitaishvili, Jimmy Ba, Kevin M. Esvelt, Alexandr Wang and Dan Hendrycks
Categories: cs.LG cs.AI cs.CL cs.CY
Comments: See the project page at https://wmdp.ai
\\
  The White House Executive Order on Artificial Intelligence highlights the
risks of large language models (LLMs) empowering malicious actors in developing
biological, cyber, and chemical weapons. To measure these risks of malicious
use, government institutions and major AI labs are developing evaluations for
hazardous capabilities in LLMs. However, current evaluations are private,
preventing further research into mitigating risk. Furthermore, they focus on
only a few, highly specific pathways for malicious use. To fill these gaps, we
publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a
dataset of 4,157 multiple-choice questions that serve as a proxy measurement of
hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP
was developed by a consortium of academics and technical consultants, and was
stringently filtered to eliminate sensitive information prior to public
release. WMDP serves two roles: first, as an evaluation for hazardous knowledge
in LLMs, and second, as a benchmark for unlearning methods to remove such
hazardous knowledge. To guide progress on unlearning, we develop CUT, a
state-of-the-art unlearning method based on controlling model representations.
CUT reduces model performance on WMDP while maintaining general capabilities in
areas such as biology and computer science, suggesting that unlearning may be a
concrete path towards reducing malicious use from LLMs. We release our
benchmark and code publicly at https://wmdp.ai
\\ ( https://arxiv.org/abs/2403.03218 ,  721kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03219
Date: Tue, 5 Mar 2024 18:59:47 GMT   (39kb)

Title: LC-Tsalis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits
Authors: Masahiro Kato and Shinji Ito
Categories: cs.LG stat.ML
\\
  This study considers the linear contextual bandit problem with independent
and identically distributed (i.i.d.) contexts. In this problem, existing
studies have proposed Best-of-Both-Worlds (BoBW) algorithms whose regrets
satisfy $O(\log^2(T))$ for the number of rounds $T$ in a stochastic regime with
a suboptimality gap lower-bounded by a positive constant, while satisfying
$O(\sqrt{T})$ in an adversarial regime. However, the dependency on $T$ has room
for improvement, and the suboptimality-gap assumption can be relaxed. For this
issue, this study proposes an algorithm whose regret satisfies $O(\log(T))$ in
the setting when the suboptimality gap is lower-bounded. Furthermore, we
introduce a margin condition, a milder assumption on the suboptimality gap.
That condition characterizes the problem difficulty linked to the suboptimality
gap using a parameter $\beta \in (0, \infty]$. We then show that the
algorithm's regret satisfies
$O\left(\left\{\log(T)\right\}^{\frac{1+\beta}{2+\beta}}T^{\frac{1}{2+\beta}}\right)$.
Here, $\beta= \infty$ corresponds to the case in the existing studies where a
lower bound exists in the suboptimality gap, and our regret satisfies
$O(\log(T))$ in that case. Our proposed algorithm is based on the
Follow-The-Regularized-Leader with the Tsallis entropy and referred to as the
$\alpha$-Linear-Contextual (LC)-Tsallis-INF.
\\ ( https://arxiv.org/abs/2403.03219 ,  39kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2403.00632 (*cross-listing*)
Date: Fri, 1 Mar 2024 16:09:32 GMT   (46656kb,D)

Title: Metamorpheus: Interactive, Affective, and Creative Dream Narration
  Through Metaphorical Visual Storytelling
Authors: Qian Wan, Xin Feng, Yining Bei, Zhiqi Gao, Zhicong Lu
Categories: cs.HC cs.AI cs.CL cs.CY
Comments: Accepted by CHI 2024
\\
  Human emotions are essentially molded by lived experiences, from which we
construct personalised meaning. The engagement in such meaning-making process
has been practiced as an intervention in various psychotherapies to promote
wellness. Nevertheless, to support recollecting and recounting lived
experiences in everyday life remains under explored in HCI. It also remains
unknown how technologies such as generative AI models can facilitate the
meaning making process, and ultimately support affective mindfulness. In this
paper we present Metamorpheus, an affective interface that engages users in a
creative visual storytelling of emotional experiences during dreams.
Metamorpheus arranges the storyline based on a dream's emotional arc, and
provokes self-reflection through the creation of metaphorical images and text
depictions. The system provides metaphor suggestions, and generates visual
metaphors and text depictions using generative AI models, while users can apply
generations to recolour and re-arrange the interface to be visually affective.
Our experience-centred evaluation manifests that, by interacting with
Metamorpheus, users can recall their dreams in vivid detail, through which they
relive and reflect upon their experiences in a meaningful way.
\\ ( https://arxiv.org/abs/2403.00632 ,  46656kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00801 (*cross-listing*)
Date: Fri, 23 Feb 2024 18:45:35 GMT   (516kb,D)

Title: Self-Retrieval: Building an Information Retrieval System with One Large
  Language Model
Authors: Qiaoyu Tang, Jiawei Chen, Bowen Yu, Yaojie Lu, Cheng Fu, Haiyang Yu,
  Hongyu Lin, Fei Huang, Ben He, Xianpei Han, Le Sun, Yongbin Li
Categories: cs.IR cs.AI cs.CL
\\
  The rise of large language models (LLMs) has transformed the role of
information retrieval (IR) systems in the way to humans accessing information.
Due to the isolated architecture and the limited interaction, existing IR
systems are unable to fully accommodate the shift from directly providing
information to humans to indirectly serving large language models. In this
paper, we propose Self-Retrieval, an end-to-end, LLM-driven information
retrieval architecture that can fully internalize the required abilities of IR
systems into a single LLM and deeply leverage the capabilities of LLMs during
IR process. Specifically, Self-retrieval internalizes the corpus to retrieve
into a LLM via a natural language indexing architecture. Then the entire
retrieval process is redefined as a procedure of document generation and
self-assessment, which can be end-to-end executed using a single large language
model. Experimental results demonstrate that Self-Retrieval not only
significantly outperforms previous retrieval approaches by a large margin, but
also can significantly boost the performance of LLM-driven downstream
applications like retrieval augumented generation.
\\ ( https://arxiv.org/abs/2403.00801 ,  516kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00957 (*cross-listing*)
Date: Fri, 1 Mar 2024 20:15:28 GMT   (313kb,D)

Title: Resolution of Simpson's paradox via the common cause principle
Authors: A. Hovhannisyan and A. E. Allahverdyan
Categories: stat.ME cs.AI math.PR physics.data-an stat.AP
\\
  Simpson's paradox is an obstacle to establishing a probabilistic association
between two events $a_1$ and $a_2$, given the third (lurking) random variable
$B$. We focus on scenarios when the random variables $A$ (which combines $a_1$,
$a_2$, and their complements) and $B$ have a common cause $C$ that need not be
observed. Alternatively, we can assume that $C$ screens out $A$ from $B$. For
such cases, the correct association between $a_1$ and $a_2$ is to be defined
via conditioning over $C$. This set-up generalizes the original Simpson's
paradox. Now its two contradicting options simply refer to two particular and
different causes $C$. We show that if $B$ and $C$ are binary and $A$ is
quaternary (the minimal and the most widespread situation for valid Simpson's
paradox), the conditioning over any binary common cause $C$ establishes the
same direction of the association between $a_1$ and $a_2$ as the conditioning
over $B$ in the original formulation of the paradox. Thus, for the minimal
common cause, one should choose the option of Simpson's paradox that assumes
conditioning over $B$ and not its marginalization. For tertiary (unobserved)
common causes $C$ all three options of Simpson's paradox become possible (i.e.
marginalized, conditional, and none of them), and one needs prior information
on $C$ to choose the right option.
\\ ( https://arxiv.org/abs/2403.00957 ,  313kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02342 (*cross-listing*)
Date: Sun, 18 Feb 2024 18:26:50 GMT   (2520kb)

Title: Entanglement: Balancing Punishment and Compensation, Repeated Dilemma
  Game-Theoretic Analysis of Maximum Compensation Problem for Bypass and Least
  Cost Paths in Fact-Checking, Case of Fake News with Weak Wallace's Law
Authors: Yasuko Kawahata
Categories: physics.soc-ph cs.AI econ.TH
Comments: Recurring Dilemma, Wallace's Law, Entanglement, Detour Path, Least
  Cost Path, Metzler Function, Metzler Matrix, Fake News, Fact-Checking,
  Punitive Dominance Problem, Maximum Compensation Problem, Informational
  health
\\
  This research note is organized with respect to a novel approach to solving
problems related to the spread of fake news and effective fact-checking.
Focusing on the least-cost routing problem, the discussion is organized with
respect to the use of Metzler functions and Metzler matrices to model the
dynamics of information propagation among news providers. With this approach,
we designed a strategy to minimize the spread of fake news, which is
detrimental to informational health, while at the same time maximizing the
spread of credible information. In particular, through the punitive dominance
problem and the maximum compensation problem, we developed and examined a path
to reassess the incentives of news providers to act and to analyze their impact
on the equilibrium of the information market. By applying the concept of
entanglement to the context of information propagation, we shed light on the
complexity of interactions among news providers and contribute to the
formulation of more effective information management strategies. This study
provides new theoretical and practical insights into issues related to fake
news and fact-checking, and will be examined against improving informational
health and public digital health.
\\ ( https://arxiv.org/abs/2403.02342 ,  2520kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02371 (*cross-listing*)
Date: Mon, 4 Mar 2024 16:17:39 GMT   (978kb,D)

Title: NeuroVoz: a Castillian Spanish corpus of parkinsonian speech
Authors: Jana\'ina Mendes-Laureano, Jorge A. G\'omez-Garc\'ia, Alejandro
  Guerrero-L\'opez, Elisa Luque-Buzo, Juli\'an D. Arias-Londo\~no, Francisco J.
  Grandas-P\'erez, Juan I. Godino-Llorente
Categories: eess.AS cs.AI cs.CL cs.SD
Comments: Preprint version
\\
  The advancement of Parkinson's Disease (PD) diagnosis through speech analysis
is hindered by a notable lack of publicly available, diverse language datasets,
limiting the reproducibility and further exploration of existing research.
  In response to this gap, we introduce a comprehensive corpus from 108 native
Castilian Spanish speakers, comprising 55 healthy controls and 53 individuals
diagnosed with PD, all of whom were under pharmacological treatment and
recorded in their medication-optimized state. This unique dataset features a
wide array of speech tasks, including sustained phonation of the five Spanish
vowels, diadochokinetic tests, 16 listen-and-repeat utterances, and free
monologues. The dataset emphasizes accuracy and reliability through specialist
manual transcriptions of the listen-and-repeat tasks and utilizes Whisper for
automated monologue transcriptions, making it the most complete public corpus
of Parkinsonian speech, and the first in Castillian Spanish.
  NeuroVoz is composed by 2,903 audio recordings averaging $26.88 \pm 3.35$
recordings per participant, offering a substantial resource for the scientific
exploration of PD's impact on speech. This dataset has already underpinned
several studies, achieving a benchmark accuracy of 89% in PD speech pattern
identification, indicating marked speech alterations attributable to PD.
Despite these advances, the broader challenge of conducting a
language-agnostic, cross-corpora analysis of Parkinsonian speech patterns
remains an open area for future research. This contribution not only fills a
critical void in PD speech analysis resources but also sets a new standard for
the global research community in leveraging speech as a diagnostic tool for
neurodegenerative diseases.
\\ ( https://arxiv.org/abs/2403.02371 ,  978kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02444 (*cross-listing*)
Date: Mon, 4 Mar 2024 19:56:19 GMT   (38480kb,D)

Title: Anatomically Constrained Tractography of the Fetal Brain
Authors: Camilo Calixto, Camilo Jaimes, Matheus D. Soldatelli, Simon K.
  Warfield, Ali Gholipour, Davood Karimi
Categories: cs.CV cs.AI cs.LG
\\
  Diffusion-weighted Magnetic Resonance Imaging (dMRI) is increasingly used to
study the fetal brain in utero. An important computation enabled by dMRI is
streamline tractography, which has unique applications such as tract-specific
analysis of the brain white matter and structural connectivity assessment.
However, due to the low fetal dMRI data quality and the challenging nature of
tractography, existing methods tend to produce highly inaccurate results. They
generate many false streamlines while failing to reconstruct streamlines that
constitute the major white matter tracts. In this paper, we advocate for
anatomically constrained tractography based on an accurate segmentation of the
fetal brain tissue directly in the dMRI space. We develop a deep learning
method to compute the segmentation automatically. Experiments on independent
test data show that this method can accurately segment the fetal brain tissue
and drastically improve tractography results. It enables the reconstruction of
highly curved tracts such as optic radiations. Importantly, our method infers
the tissue segmentation and streamline propagation direction from a diffusion
tensor fit to the dMRI data, making it applicable to routine fetal dMRI scans.
The proposed method can lead to significant improvements in the accuracy and
reproducibility of quantitative assessment of the fetal brain with dMRI.
\\ ( https://arxiv.org/abs/2403.02444 ,  38480kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02495 (*cross-listing*)
Date: Mon, 4 Mar 2024 21:41:27 GMT   (31009kb,D)

Title: Pseudo-Labeling and Contextual Curriculum Learning for Online Grasp
  Learning in Robotic Bin Picking
Authors: Huy Le, Philipp Schillinger, Miroslav Gabriel, Alexander Qualmann, Ngo
  Anh Vien
Categories: cs.RO cs.AI
Comments: Accepted to ICRA 2024
\\
  The prevailing grasp prediction methods predominantly rely on offline
learning, overlooking the dynamic grasp learning that occurs during real-time
adaptation to novel picking scenarios. These scenarios may involve previously
unseen objects, variations in camera perspectives, and bin configurations,
among other factors. In this paper, we introduce a novel approach, SSL-ConvSAC,
that combines semi-supervised learning and reinforcement learning for online
grasp learning. By treating pixels with reward feedback as labeled data and
others as unlabeled, it efficiently exploits unlabeled data to enhance
learning. In addition, we address the imbalance between labeled and unlabeled
data by proposing a contextual curriculum-based method. We ablate the proposed
approach on real-world evaluation data and demonstrate promise for improving
online grasp learning on bin picking tasks using a physical 7-DoF Franka Emika
robot arm with a suction gripper. Video: https://youtu.be/OAro5pg8I9U
\\ ( https://arxiv.org/abs/2403.02495 ,  31009kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02514 (*cross-listing*)
Date: Mon, 4 Mar 2024 22:03:49 GMT   (951kb,D)

Title: Purpose for Open-Ended Learning Robots: A Computational Taxonomy,
  Definition, and Operationalisation
Authors: Gianluca Baldassarre, Richard J. Duro, Emilio Cartoni, Mehdi Khamassi,
  Alejandro Romero, Vieri Giuliano Santucci
Categories: cs.RO cs.AI cs.LG
Comments: 15 pages, 6 figures
\\
  Autonomous open-ended learning (OEL) robots are able to cumulatively acquire
new skills and knowledge through direct interaction with the environment, for
example relying on the guidance of intrinsic motivations and self-generated
goals. OEL robots have a high relevance for applications as they can use the
autonomously acquired knowledge to accomplish tasks relevant for their human
users. OEL robots, however, encounter an important limitation: this may lead to
the acquisition of knowledge that is not so much relevant to accomplish the
users' tasks. This work analyses a possible solution to this problem that
pivots on the novel concept of `purpose'. Purposes indicate what the designers
and/or users want from the robot. The robot should use internal representations
of purposes, called here `desires', to focus its open-ended exploration towards
the acquisition of knowledge relevant to accomplish them. This work contributes
to develop a computational framework on purpose in two ways. First, it
formalises a framework on purpose based on a three-level motivational hierarchy
involving: (a) the purposes; (b) the desires, which are domain independent; (c)
specific domain dependent state-goals. Second, the work highlights key
challenges highlighted by the framework such as: the `purpose-desire alignment
problem', the `purpose-goal grounding problem', and the `arbitration between
desires'. Overall, the approach enables OEL robots to learn in an autonomous
way but also to focus on acquiring goals and skills that meet the purposes of
the designers and users.
\\ ( https://arxiv.org/abs/2403.02514 ,  951kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02574 (*cross-listing*)
Date: Tue, 5 Mar 2024 01:13:56 GMT   (1054kb,D)

Title: ChatCite: LLM Agent with Human Workflow Guidance for Comparative
  Literature Summary
Authors: Yutong Li, Lu Chen, Aiwei Liu, Kai Yu, Lijie Wen
Categories: cs.IR cs.AI cs.CL
Comments: 18 pages, 5 figures
MSC-class: 68T50
ACM-class: I.2.7
\\
  The literature review is an indispensable step in the research process. It
provides the benefit of comprehending the research problem and understanding
the current research situation while conducting a comparative analysis of prior
works. However, literature summary is challenging and time consuming. The
previous LLM-based studies on literature review mainly focused on the complete
process, including literature retrieval, screening, and summarization. However,
for the summarization step, simple CoT method often lacks the ability to
provide extensive comparative summary. In this work, we firstly focus on the
independent literature summarization step and introduce ChatCite, an LLM agent
with human workflow guidance for comparative literature summary. This agent, by
mimicking the human workflow, first extracts key elements from relevant
literature and then generates summaries using a Reflective Incremental
Mechanism. In order to better evaluate the quality of the generated summaries,
we devised a LLM-based automatic evaluation metric, G-Score, in refer to the
human evaluation criteria. The ChatCite agent outperformed other models in
various dimensions in the experiments. The literature summaries generated by
ChatCite can also be directly used for drafting literature reviews.
\\ ( https://arxiv.org/abs/2403.02574 ,  1054kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02589 (*cross-listing*)
Date: Tue, 5 Mar 2024 02:02:00 GMT   (644kb)

Title: MUSIC: Accelerated Convergence for Distributed Optimization With Inexact
  and Exact Methods
Authors: Mou Wu, Haibin Liao, Zhengtao Ding, Yonggang Xiao
Categories: math.OC cs.AI
\\
  Gradient-type distributed optimization methods have blossomed into one of the
most important tools for solving a minimization learning task over a networked
agent system. However, only one gradient update per iteration is difficult to
achieve a substantive acceleration of convergence. In this paper, we propose an
accelerated framework named as MUSIC allowing each agent to perform multiple
local updates and a single combination in each iteration. More importantly, we
equip inexact and exact distributed optimization methods into this framework,
thereby developing two new algorithms that exhibit accelerated linear
convergence and high communication efficiency. Our rigorous convergence
analysis reveals the sources of steady-state errors arising from inexact
policies and offers effective solutions. Numerical results based on synthetic
and real datasets demonstrate both our theoretical motivations and analysis, as
well as performance advantages.
\\ ( https://arxiv.org/abs/2403.02589 ,  644kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02607 (*cross-listing*)
Date: Tue, 5 Mar 2024 02:44:58 GMT   (1410kb)

Title: MEBS: Multi-task End-to-end Bid Shading for Multi-slot Display
  Advertising
Authors: Zhen Gong, Lvyin Niu, Yang Zhao, Miao Xu, Zhenzhe Zheng, Haoqi Zhang,
  Zhilin Zhang, Fan Wu, Rongquan Bai, Chuan Yu, Jian Xu and Bo Zheng
Categories: cs.GT cs.AI
\\
  Online bidding and auction are crucial aspects of the online advertising
industry. Conventionally, there is only one slot for ad display and most
current studies focus on it. Nowadays, multi-slot display advertising is
gradually becoming popular where many ads could be displayed in a list and
shown as a whole to users. However, multi-slot display advertising leads to
different cost-effectiveness. Advertisers have the incentive to adjust bid
prices so as to win the most economical ad positions. In this study, we
introduce bid shading into multi-slot display advertising for bid price
adjustment with a Multi-task End-to-end Bid Shading(MEBS) method. We prove the
optimality of our method theoretically and examine its performance
experimentally. Through extensive offline and online experiments, we
demonstrate the effectiveness and efficiency of our method, and we obtain a
7.01% lift in Gross Merchandise Volume, a 7.42% lift in Return on Investment,
and a 3.26% lift in ad buy count.
\\ ( https://arxiv.org/abs/2403.02607 ,  1410kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02611 (*cross-listing*)
Date: Tue, 5 Mar 2024 02:59:35 GMT   (20294kb,D)

Title: A Unified Framework for Microscopy Defocus Deblur with Multi-Pyramid
  Transformer and Contrastive Learning
Authors: Yuelin Zhang, Pengyu Zheng, Wanquan Yan, Chengyu Fang, Shing Shin
  Cheng
Categories: cs.CV cs.AI
Comments: Accepted in CVPR 2024
\\
  Defocus blur is a persistent problem in microscope imaging that poses harm to
pathology interpretation and medical intervention in cell microscopy and
microscope surgery. To address this problem, a unified framework including
multi-pyramid transformer (MPT) and extended frequency contrastive
regularization (EFCR) is proposed to tackle two outstanding challenges in
microscopy deblur: longer attention span and feature deficiency. The MPT
employs an explicit pyramid structure at each network stage that integrates the
cross-scale window attention (CSWA), the intra-scale channel attention (ISCA),
and the feature-enhancing feed-forward network (FEFN) to capture long-range
cross-scale spatial interaction and global channel context. The EFCR addresses
the feature deficiency problem by exploring latent deblur signals from
different frequency bands. It also enables deblur knowledge transfer to learn
cross-domain information from extra data, improving deblur performance for
labeled and unlabeled data. Extensive experiments and downstream task
validation show the framework achieves state-of-the-art performance across
multiple datasets. Project page: https://github.com/PieceZhang/MPT-CataBlur.
\\ ( https://arxiv.org/abs/2403.02611 ,  20294kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02613 (*cross-listing*)
Date: Tue, 5 Mar 2024 03:04:35 GMT   (138kb,D)

Title: Large Language Models and Video Games: A Preliminary Scoping Review
Authors: Penny Sweetser
Categories: cs.HC cs.AI
Comments: under review
\\
  Large language models (LLMs) hold interesting potential for the design,
development, and research of video games. Building on the decades of prior
research on generative AI in games, many researchers have sped to investigate
the power and potential of LLMs for games. Given the recent spike in
LLM-related research in games, there is already a wealth of relevant research
to survey. In order to capture a snapshot of the state of LLM research in
games, and to help lay the foundation for future work, we carried out an
initial scoping review of relevant papers published so far. In this paper, we
review 76 papers published between 2022 to early 2024 on LLMs and video games,
with key focus areas in game AI, game development, narrative, and game research
and reviews. Our paper provides an early state of the field and lays the
groundwork for future research and reviews on this topic.
\\ ( https://arxiv.org/abs/2403.02613 ,  138kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02651 (*cross-listing*)
Date: Tue, 5 Mar 2024 04:48:24 GMT   (1483kb,D)

Title: Learning at the Speed of Wireless: Online Real-Time Learning for
  AI-Enabled MIMO in NextG
Authors: Jiarui Xu, Shashank Jere, Yifei Song, Yi-Hung Kao, Lizhong Zheng, and
  Lingjia Liu
Categories: eess.SP cs.AI
Comments: 7 pages, 4 figures, 1 table, magazine paper
\\
  Integration of artificial intelligence (AI) and machine learning (ML) into
the air interface has been envisioned as a key technology for next-generation
(NextG) cellular networks. At the air interface, multiple-input multiple-output
(MIMO) and its variants such as multi-user MIMO (MU-MIMO) and
massive/full-dimension MIMO have been key enablers across successive
generations of cellular networks with evolving complexity and design
challenges. Initiating active investigation into leveraging AI/ML tools to
address these challenges for MIMO becomes a critical step towards an AI-enabled
NextG air interface. At the NextG air interface, the underlying wireless
environment will be extremely dynamic with operation adaptations performed on a
sub-millisecond basis by MIMO operations such as MU-MIMO scheduling and
rank/link adaptation. Given the enormously large number of operation adaptation
possibilities, we contend that online real-time AI/ML-based approaches
constitute a promising paradigm. To this end, we outline the inherent
challenges and offer insights into the design of such online real-time
AI/ML-based solutions for MIMO operations. An online real-time AI/ML-based
method for MIMO-OFDM channel estimation is then presented, serving as a
potential roadmap for developing similar techniques across various MIMO
operations in NextG.
\\ ( https://arxiv.org/abs/2403.02651 ,  1483kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02687 (*cross-listing*)
Date: Tue, 5 Mar 2024 06:15:48 GMT   (2644kb,D)

Title: Enhanced DareFightingICE Competitions: Sound Design and AI Competitions
Authors: Ibrahim Khan, Chollakorn Nimpattanavong, Thai Van Nguyen, Kantinan
  Plupattanakit, Ruck Thawonmas
Categories: cs.HC cs.AI cs.SD eess.AS
ACM-class: I.2; H.5.2; H.5.5
\\
  This paper presents a new and improved DareFightingICE platform, a fighting
game platform with a focus on visually impaired players (VIPs), in the Unity
game engine. It also introduces the separation of the DareFightingICE
Competition into two standalone competitions called DareFightingICE Sound
Design Competition and DareFightingICE AI Competition--at the 2024 IEEE
Conference on Games (CoG)--in which a new platform will be used. This new
platform is an enhanced version of the old DareFightingICE platform, having a
better audio system to convey 3D sound and a better way to send audio data to
AI agents. With this enhancement and by utilizing Unity, the new
DareFightingICE platform is more accessible in terms of adding new features for
VIPs and future audio research. This paper also improves the evaluation method
for evaluating sound designs in the Sound Design Competition which will ensure
a better sound design for VIPs as this competition continues to run at future
CoG. To the best of our knowledge, both of our competitions are first of their
kind, and the connection between the competitions to mutually improve the
entries' quality with time makes these competitions an important part of
representing an often overlooked segment within the broader gaming community,
VIPs.
\\ ( https://arxiv.org/abs/2403.02687 ,  2644kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02688 (*cross-listing*)
Date: Tue, 5 Mar 2024 06:17:13 GMT   (2266kb,D)

Title: DOCTOR: Dynamic On-Chip Remediation Against Temporally-Drifting Thermal
  Variations Toward Self-Corrected Photonic Tensor Accelerators
Authors: Haotian Lu, Sanmitra Banerjee, Jiaqi Gu
Categories: cs.ET cs.AI cs.LG
Comments: 8 pages
\\
  Photonic computing has emerged as a promising solution for accelerating
computation-intensive artificial intelligence (AI) workloads, offering
unparalleled speed and energy efficiency, especially in resource-limited,
latency-sensitive edge computing environments. However, the deployment of
analog photonic tensor accelerators encounters reliability challenges due to
hardware noises and environmental variations. While off-chip noise-aware
training and on-chip training have been proposed to enhance the variation
tolerance of optical neural accelerators with moderate, static noises, we
observe a notable performance degradation over time due to temporally drifting
variations, which requires a real-time, in-situ calibration mechanism. To
tackle this challenging reliability issues, for the first time, we propose a
lightweight dynamic on-chip remediation framework, dubbed DOCTOR, providing
adaptive, in-situ accuracy recovery against temporally drifting noises. The
DOCTOR framework intelligently monitors the chip status using adaptive probing
and performs fast in-situ training-free calibration to restore accuracy when
necessary. Recognizing nonuniform spatial variation distributions across
devices and tensor cores, we also propose a variation-aware architectural
remapping strategy to avoid executing critical tasks on noisy devices.
Extensive experiments show that our proposed framework can guarantee sustained
performance under drifting variations with 34% higher accuracy and 2-3
orders-of-magnitude lower overhead compared to state-of-the-art on-chip
training methods.
\\ ( https://arxiv.org/abs/2403.02688 ,  2266kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02701 (*cross-listing*)
Date: Tue, 5 Mar 2024 06:46:43 GMT   (914kb,D)

Title: Fighting Game Adaptive Background Music for Improved Gameplay
Authors: Ibrahim Khan, Thai Van Nguyen, Chollakorn Nimpattanavong, Ruck
  Thawonmas
Categories: cs.SD cs.AI eess.AS
Comments: This is an updated version of our IEEE CoG 2023 paper
  (https://ieeexplore.ieee.org/document/10333245). This version has revised the
  description of the association between the distance between the two players
  (PD) and the instrument's volume on page 2. arXiv admin note: substantial
  text overlap with arXiv:2303.15734
ACM-class: I.2; H.5.2; H.5
\\
  This paper presents our work to enhance the background music (BGM) in
DareFightingICE by adding adaptive features. The adaptive BGM consists of three
different categories of instruments playing the BGM of the winner sound design
from the 2022 DareFightingICE Competition. The BGM adapts by changing the
volume of each category of instruments. Each category is connected to a
different element of the game. We then run experiments to evaluate the adaptive
BGM by using a deep reinforcement learning AI agent that only uses audio as
input (Blind DL AI). The results show that the performance of the Blind DL AI
improves while playing with the adaptive BGM as compared to playing without the
adaptive BGM.
\\ ( https://arxiv.org/abs/2403.02701 ,  914kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02726 (*cross-listing*)
Date: Tue, 5 Mar 2024 07:34:41 GMT   (2087kb)

Title: Bias in Generative AI
Authors: Mi Zhou, Vibhanshu Abhishek, Timothy Derdenger, Jaymo Kim, Kannan
  Srinivasan
Categories: econ.GN cs.AI cs.CY q-fin.EC
\\
  This study analyzed images generated by three popular generative artificial
intelligence (AI) tools - Midjourney, Stable Diffusion, and DALLE 2 -
representing various occupations to investigate potential bias in AI
generators. Our analysis revealed two overarching areas of concern in these AI
generators, including (1) systematic gender and racial biases, and (2) subtle
biases in facial expressions and appearances. Firstly, we found that all three
AI generators exhibited bias against women and African Americans. Moreover, we
found that the evident gender and racial biases uncovered in our analysis were
even more pronounced than the status quo when compared to labor force
statistics or Google images, intensifying the harmful biases we are actively
striving to rectify in our society. Secondly, our study uncovered more nuanced
prejudices in the portrayal of emotions and appearances. For example, women
were depicted as younger with more smiles and happiness, while men were
depicted as older with more neutral expressions and anger, posing a risk that
generative AI models may unintentionally depict women as more submissive and
less competent than men. Such nuanced biases, by their less overt nature, might
be more problematic as they can permeate perceptions unconsciously and may be
more difficult to rectify. Although the extent of bias varied depending on the
model, the direction of bias remained consistent in both commercial and
open-source AI generators. As these tools become commonplace, our study
highlights the urgency to identify and mitigate various biases in generative
AI, reinforcing the commitment to ensuring that AI technologies benefit all of
humanity for a more inclusive future.
\\ ( https://arxiv.org/abs/2403.02726 ,  2087kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02736 (*cross-listing*)
Date: Tue, 5 Mar 2024 07:44:13 GMT   (185kb,D)

Title: Bootstrapping Rare Object Detection in High-Resolution Satellite Imagery
Authors: Akram Zaytar, Caleb Robinson, Gilles Q. Hacheme, Girmaw A. Tadesse,
  Rahul Dodhia, Juan M. Lavista Ferres, Lacey F. Hughey, Jared A. Stabach,
  Irene Amoke
Categories: cs.CV cs.AI
\\
  Rare object detection is a fundamental task in applied geospatial machine
learning, however is often challenging due to large amounts of high-resolution
satellite or aerial imagery and few or no labeled positive samples to start
with. This paper addresses the problem of bootstrapping such a rare object
detection task assuming there is no labeled data and no spatial prior over the
area of interest. We propose novel offline and online cluster-based approaches
for sampling patches that are significantly more efficient, in terms of
exposing positive samples to a human annotator, than random sampling. We apply
our methods for identifying bomas, or small enclosures for herd animals, in the
Serengeti Mara region of Kenya and Tanzania. We demonstrate a significant
enhancement in detection efficiency, achieving a positive sampling rate
increase from 2% (random) to 30%. This advancement enables effective machine
learning mapping even with minimal labeling budgets, exemplified by an F1 score
on the boma detection task of 0.51 with a budget of 300 total patches.
\\ ( https://arxiv.org/abs/2403.02736 ,  185kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02750 (*cross-listing*)
Date: Tue, 5 Mar 2024 08:08:59 GMT   (4143kb,D)

Title: Speckle Noise Reduction in Ultrasound Images using Denoising
  Auto-encoder with Skip Connection
Authors: Suraj Bhute, Subhamoy Mandal, Debashree Guha
Categories: eess.IV cs.AI physics.med-ph
Comments: Selected for presentation at 2024 IEEE South Asian Ultrasonics
  Symposium
\\
  Ultrasound is a widely used medical tool for non-invasive diagnosis, but its
images often contain speckle noise which can lower their resolution and
contrast-to-noise ratio. This can make it more difficult to extract, recognize,
and analyze features in the images, as well as impair the accuracy of
computer-assisted diagnostic techniques and the ability of doctors to interpret
the images. Reducing speckle noise, therefore, is a crucial step in the
preprocessing of ultrasound images. Researchers have proposed several speckle
reduction methods, but no single method takes all relevant factors into
account. In this paper, we compare seven such methods: Median, Gaussian,
Bilateral, Average, Weiner, Anisotropic and Denoising auto-encoder without and
with skip connections in terms of their ability to preserve features and edges
while effectively reducing noise. In an experimental study, a convolutional
noise-removing auto-encoder with skip connection, a deep learning method, was
used to improve ultrasound images of breast cancer. This method involved adding
speckle noise at various levels. The results of the deep learning method were
compared to those of traditional image enhancement methods, and it was found
that the proposed method was more effective. To assess the performance of these
algorithms, we use three established evaluation metrics and present both
filtered images and statistical data.
\\ ( https://arxiv.org/abs/2403.02750 ,  4143kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02794 (*cross-listing*)
Date: Tue, 5 Mar 2024 09:08:20 GMT   (19571kb)

Title: A Distance Metric Learning Model Based On Variational Information
  Bottleneck
Authors: YaoDan Zhang, Zidong Wang, Ru Jia and Ru Li
Categories: cs.IR cs.AI cs.LG
\\
  In recent years, personalized recommendation technology has flourished and
become one of the hot research directions. The matrix factorization model and
the metric learning model which proposed successively have been widely studied
and applied. The latter uses the Euclidean distance instead of the dot product
used by the former to measure the latent space vector. While avoiding the
shortcomings of the dot product, the assumption of Euclidean distance is
neglected, resulting in limited recommendation quality of the model. In order
to solve this problem, this paper combines the Variationl Information
Bottleneck with metric learning model for the first time, and proposes a new
metric learning model VIB-DML (Variational Information Bottleneck Distance
Metric Learning) for rating prediction, which limits the mutual information of
the latent space feature vector to improve the robustness of the model and
satisfiy the assumption of Euclidean distance by decoupling the latent space
feature vector. In this paper, the experimental results are compared with the
root mean square error (RMSE) on the three public datasets. The results show
that the generalization ability of VIB-DML is excellent. Compared with the
general metric learning model MetricF, the prediction error is reduced by
7.29%. Finally, the paper proves the strong robustness of VIBDML through
experiments.
\\ ( https://arxiv.org/abs/2403.02794 ,  19571kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02877 (*cross-listing*)
Date: Tue, 5 Mar 2024 11:39:07 GMT   (10399kb,D)

Title: ActiveAD: Planning-Oriented Active Learning for End-to-End Autonomous
  Driving
Authors: Han Lu, Xiaosong Jia, Yichen Xie, Wenlong Liao, Xiaokang Yang, Junchi
  Yan
Categories: cs.CV cs.AI cs.RO
\\
  End-to-end differentiable learning for autonomous driving (AD) has recently
become a prominent paradigm. One main bottleneck lies in its voracious appetite
for high-quality labeled data e.g. 3D bounding boxes and semantic segmentation,
which are notoriously expensive to manually annotate. The difficulty is further
pronounced due to the prominent fact that the behaviors within samples in AD
often suffer from long tailed distribution. In other words, a large part of
collected data can be trivial (e.g. simply driving forward in a straight road)
and only a few cases are safety-critical. In this paper, we explore a
practically important yet under-explored problem about how to achieve sample
and label efficiency for end-to-end AD. Specifically, we design a
planning-oriented active learning method which progressively annotates part of
collected raw data according to the proposed diversity and usefulness criteria
for planning routes. Empirically, we show that our planning-oriented approach
could outperform general active learning methods by a large margin. Notably,
our method achieves comparable performance with state-of-the-art end-to-end AD
methods - by using only 30% nuScenes data. We hope our work could inspire
future works to explore end-to-end AD from a data-centric perspective in
addition to methodology efforts.
\\ ( https://arxiv.org/abs/2403.02877 ,  10399kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02892 (*cross-listing*)
Date: Tue, 5 Mar 2024 11:57:10 GMT   (3017kb,D)

Title: Enhancing Long-Term Person Re-Identification Using Global, Local Body
  Part, and Head Streams
Authors: Duy Tran Thanh and Yeejin Lee and Byeongkeun Kang
Categories: cs.CV cs.AI
Comments: 16 pages
Journal-ref: Neurocomputing, 2024
\\
  This work addresses the task of long-term person re-identification.
Typically, person re-identification assumes that people do not change their
clothes, which limits its applications to short-term scenarios. To overcome
this limitation, we investigate long-term person re-identification, which
considers both clothes-changing and clothes-consistent scenarios. In this
paper, we propose a novel framework that effectively learns and utilizes both
global and local information. The proposed framework consists of three streams:
global, local body part, and head streams. The global and head streams encode
identity-relevant information from an entire image and a cropped image of the
head region, respectively. Both streams encode the most distinct, less
distinct, and average features using the combinations of adversarial erasing,
max pooling, and average pooling. The local body part stream extracts
identity-related information for each body part, allowing it to be compared
with the same body part from another image. Since body part annotations are not
available in re-identification datasets, pseudo-labels are generated using
clustering. These labels are then utilized to train a body part segmentation
head in the local body part stream. The proposed framework is trained by
backpropagating the weighted summation of the identity classification loss, the
pair-based loss, and the pseudo body part segmentation loss. To demonstrate the
effectiveness of the proposed method, we conducted experiments on three
publicly available datasets (Celeb-reID, PRCC, and VC-Clothes). The
experimental results demonstrate that the proposed method outperforms the
previous state-of-the-art method.
\\ ( https://arxiv.org/abs/2403.02892 ,  3017kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02910 (*cross-listing*)
Date: Tue, 5 Mar 2024 12:21:57 GMT   (9475kb,D)

Title: ImgTrojan: Jailbreaking Vision-Language Models with ONE Image
Authors: Xijia Tao, Shuai Zhong, Lei Li, Qi Liu, Lingpeng Kong
Categories: cs.CV cs.AI
\\
  There has been an increasing interest in the alignment of large language
models (LLMs) with human values. However, the safety issues of their
integration with a vision module, or vision language models (VLMs), remain
relatively underexplored. In this paper, we propose a novel jailbreaking attack
against VLMs, aiming to bypass their safety barrier when a user inputs harmful
instructions. A scenario where our poisoned (image, text) data pairs are
included in the training data is assumed. By replacing the original textual
captions with malicious jailbreak prompts, our method can perform jailbreak
attacks with the poisoned images. Moreover, we analyze the effect of poison
ratios and positions of trainable parameters on our attack's success rate. For
evaluation, we design two metrics to quantify the success rate and the
stealthiness of our attack. Together with a list of curated harmful
instructions, a benchmark for measuring attack efficacy is provided. We
demonstrate the efficacy of our attack by comparing it with baseline methods.
\\ ( https://arxiv.org/abs/2403.02910 ,  9475kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02939 (*cross-listing*)
Date: Tue, 5 Mar 2024 13:10:06 GMT   (6667kb,D)

Title: PaperWeaver: Enriching Topical Paper Alerts by Contextualizing
  Recommended Papers with User-collected Papers
Authors: Yoonjoo Lee, Hyeonsu B. Kang, Matt Latzke, Juho Kim, Jonathan Bragg,
  Joseph Chee Chang, Pao Siangliulue
Categories: cs.DL cs.AI cs.CL cs.HC
Comments: Accepted to CHI 2024
DOI: 10.1145/3613904.3642196
\\
  With the rapid growth of scholarly archives, researchers subscribe to "paper
alert" systems that periodically provide them with recommendations of recently
published papers that are similar to previously collected papers. However,
researchers sometimes struggle to make sense of nuanced connections between
recommended papers and their own research context, as existing systems only
present paper titles and abstracts. To help researchers spot these connections,
we present PaperWeaver, an enriched paper alerts system that provides
contextualized text descriptions of recommended papers based on user-collected
papers. PaperWeaver employs a computational method based on Large Language
Models (LLMs) to infer users' research interests from their collected papers,
extract context-specific aspects of papers, and compare recommended and
collected papers on these aspects. Our user study (N=15) showed that
participants using PaperWeaver were able to better understand the relevance of
recommended papers and triage them more confidently when compared to a baseline
that presented the related work sections from recommended papers.
\\ ( https://arxiv.org/abs/2403.02939 ,  6667kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02965 (*cross-listing*)
Date: Tue, 5 Mar 2024 13:41:25 GMT   (4152kb,D)

Title: ChatGPT and biometrics: an assessment of face recognition, gender
  detection, and age estimation capabilities
Authors: Ahmad Hassanpour, Yasamin Kowsari, Hatef Otroshi Shahreza, Bian Yang,
  Sebastien Marcel
Categories: cs.CV cs.AI
\\
  This paper explores the application of large language models (LLMs), like
ChatGPT, for biometric tasks. We specifically examine the capabilities of
ChatGPT in performing biometric-related tasks, with an emphasis on face
recognition, gender detection, and age estimation. Since biometrics are
considered as sensitive information, ChatGPT avoids answering direct prompts,
and thus we crafted a prompting strategy to bypass its safeguard and evaluate
the capabilities for biometrics tasks. Our study reveals that ChatGPT
recognizes facial identities and differentiates between two facial images with
considerable accuracy. Additionally, experimental results demonstrate
remarkable performance in gender detection and reasonable accuracy for the age
estimation tasks. Our findings shed light on the promising potentials in the
application of LLMs and foundation models for biometrics.
\\ ( https://arxiv.org/abs/2403.02965 ,  4152kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02983 (*cross-listing*)
Date: Tue, 5 Mar 2024 14:03:15 GMT   (2322kb,D)

Title: Federated Learning Under Attack: Exposing Vulnerabilities through Data
  Poisoning Attacks in Computer Networks
Authors: Ehsan Nowroozi, Imran Haider, Rahim Taheri, Mauro Conti
Categories: cs.CR cs.AI cs.CY cs.LG cs.NI
\\
  Federated Learning (FL) is a machine learning (ML) approach that enables
multiple decentralized devices or edge servers to collaboratively train a
shared model without exchanging raw data. During the training and sharing of
model updates between clients and servers, data and models are susceptible to
different data-poisoning attacks.
  In this study, our motivation is to explore the severity of data poisoning
attacks in the computer network domain because they are easy to implement but
difficult to detect. We considered two types of data-poisoning attacks, label
flipping (LF) and feature poisoning (FP), and applied them with a novel
approach. In LF, we randomly flipped the labels of benign data and trained the
model on the manipulated data. For FP, we randomly manipulated the highly
contributing features determined using the Random Forest algorithm. The
datasets used in this experiment were CIC and UNSW related to computer
networks. We generated adversarial samples using the two attacks mentioned
above, which were applied to a small percentage of datasets. Subsequently, we
trained and tested the accuracy of the model on adversarial datasets. We
recorded the results for both benign and manipulated datasets and observed
significant differences between the accuracy of the models on different
datasets. From the experimental results, it is evident that the LF attack
failed, whereas the FP attack showed effective results, which proved its
significance in fooling a server. With a 1% LF attack on the CIC, the accuracy
was approximately 0.0428 and the ASR was 0.9564; hence, the attack is easily
detectable, while with a 1% FP attack, the accuracy and ASR were both
approximately 0.9600, hence, FP attacks are difficult to detect. We repeated
the experiment with different poisoning percentages.
\\ ( https://arxiv.org/abs/2403.02983 ,  2322kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02995 (*cross-listing*)
Date: Tue, 5 Mar 2024 14:21:57 GMT   (659kb,D)

Title: Mitigating Label Flipping Attacks in Malicious URL Detectors Using
  Ensemble Trees
Authors: Ehsan Nowroozi, Nada Jadalla, Samaneh Ghelichkhani, Alireza Jolfaei
Categories: cs.CR cs.AI cs.CY cs.LG cs.NI
\\
  Malicious URLs provide adversarial opportunities across various industries,
including transportation, healthcare, energy, and banking which could be
detrimental to business operations. Consequently, the detection of these URLs
is of crucial importance; however, current Machine Learning (ML) models are
susceptible to backdoor attacks. These attacks involve manipulating a small
percentage of training data labels, such as Label Flipping (LF), which changes
benign labels to malicious ones and vice versa. This manipulation results in
misclassification and leads to incorrect model behavior. Therefore, integrating
defense mechanisms into the architecture of ML models becomes an imperative
consideration to fortify against potential attacks.
  The focus of this study is on backdoor attacks in the context of URL
detection using ensemble trees. By illuminating the motivations behind such
attacks, highlighting the roles of attackers, and emphasizing the critical
importance of effective defense strategies, this paper contributes to the
ongoing efforts to fortify ML models against adversarial threats within the ML
domain in network security. We propose an innovative alarm system that detects
the presence of poisoned labels and a defense mechanism designed to uncover the
original class labels with the aim of mitigating backdoor attacks on ensemble
tree classifiers. We conducted a case study using the Alexa and Phishing Site
URL datasets and showed that LF attacks can be addressed using our proposed
defense mechanism. Our experimental results prove that the LF attack achieved
an Attack Success Rate (ASR) between 50-65% within 2-5%, and the innovative
defense method successfully detected poisoned labels with an accuracy of up to
100%.
\\ ( https://arxiv.org/abs/2403.02995 ,  659kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03002 (*cross-listing*)
Date: Tue, 5 Mar 2024 14:28:40 GMT   (9080kb)

Title: Mem-elements based Neuromorphic Hardware for Neural Network Application
Authors: Ankur Singh
Categories: cs.NE cs.AI cs.ET
Comments: Master's Thesis
\\
  The thesis investigates the utilization of memristive and memcapacitive
crossbar arrays in low-power machine learning accelerators, offering a
comprehensive co-design framework for deep neural networks (DNN). The model,
implemented through a hybrid Python and PyTorch approach, accounts for various
non-idealities, achieving exceptional training accuracies of 90.02% and 91.03%
for the CIFAR-10 dataset with memristive and memcapacitive crossbar arrays on
an 8-layer VGG network. Additionally, the thesis introduces a novel approach to
emulate meminductor devices using Operational Transconductance Amplifiers (OTA)
and capacitors, showcasing adjustable behavior. Transistor-level simulations in
180 nm CMOS technology, operating at 60 MHz, demonstrate the proposed
meminductor emulator's viability with a power consumption of 0.337 mW. The
design is further validated in neuromorphic circuits and CNN accelerators,
achieving training and testing accuracies of 91.04% and 88.82%, respectively.
Notably, the exclusive use of MOS transistors ensures the feasibility of
monolithic IC fabrication. This research significantly contributes to the
exploration of advanced hardware solutions for efficient and high-performance
machine-learning applications.
\\ ( https://arxiv.org/abs/2403.03002 ,  9080kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03030 (*cross-listing*)
Date: Tue, 5 Mar 2024 15:06:16 GMT   (1262kb,D)

Title: Unifying Controller Design for Stabilizing Nonlinear Systems with
  Norm-Bounded Control Inputs
Authors: Ming Li, Zhiyong Sun, and Siep Weiland
Categories: eess.SY cs.AI cs.SY math.OC
\\
  This paper revisits a classical challenge in the design of stabilizing
controllers for nonlinear systems with a norm-bounded input constraint. By
extending Lin-Sontag's universal formula and introducing a generic
(state-dependent) scaling term, a unifying controller design method is
proposed. The incorporation of this generic scaling term gives a unified
controller and enables the derivation of alternative universal formulas with
various favorable properties, which makes it suitable for tailored control
designs to meet specific requirements and provides versatility across different
control scenarios. Additionally, we present a constructive approach to
determine the optimal scaling term, leading to an explicit solution to an
optimization problem, named optimization-based universal formula. The resulting
controller ensures asymptotic stability, satisfies a norm-bounded input
constraint, and optimizes a predefined cost function. Finally, the essential
properties of the unified controllers are analyzed, including smoothness,
continuity at the origin, stability margin, and inverse optimality. Simulations
validate the approach, showcasing its effectiveness in addressing a challenging
stabilizing control problem of a nonlinear system.
\\ ( https://arxiv.org/abs/2403.03030 ,  1262kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03053 (*cross-listing*)
Date: Tue, 5 Mar 2024 15:37:06 GMT   (2095kb,D)

Title: Neural Codebook Design for Network Beam Management
Authors: Ryan M. Dreifuerst and Robert W. Heath Jr
Categories: eess.SP cs.AI cs.IT cs.NI cs.SY eess.SY math.IT
Comments: To be submitted to IEEE Transactions on Wireless Communications
\\
  Obtaining accurate and timely channel state information (CSI) is a
fundamental challenge for large antenna systems. Mobile systems like 5G use a
beam management framework that joins the initial access, beamforming, CSI
acquisition, and data transmission. The design of codebooks for these stages,
however, is challenging due to their interrelationships, varying array sizes,
and site-specific channel and user distributions. Furthermore, beam management
is often focused on single-sector operations while ignoring the overarching
network- and system-level optimization. In this paper, we proposed an
end-to-end learned codebook design algorithm, network beamspace learning (NBL),
that captures and optimizes codebooks to mitigate interference while maximizing
the achievable performance with extremely large hybrid arrays. The proposed
algorithm requires limited shared information yet designs codebooks that
outperform traditional codebooks by over 10dB in beam alignment and achieve
more than 25% improvements in network spectral efficiency.
\\ ( https://arxiv.org/abs/2403.03053 ,  2095kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03089 (*cross-listing*)
Date: Tue, 5 Mar 2024 16:21:53 GMT   (612kb,D)

Title: VQSynery: Robust Drug Synergy Prediction With Vector Quantization
  Mechanism
Authors: Jiawei Wu, Mingyuan Yan, Dianbo Liu
Categories: q-bio.QM cs.AI cs.LG
\\
  The pursuit of optimizing cancer therapies is significantly advanced by the
accurate prediction of drug synergy. Traditional methods, such as clinical
trials, are reliable yet encumbered by extensive time and financial demands.
The emergence of high-throughput screening and computational innovations has
heralded a shift towards more efficient methodologies for exploring drug
interactions. In this study, we present VQSynergy, a novel framework that
employs the Vector Quantization (VQ) mechanism, integrated with gated residuals
and a tailored attention mechanism, to enhance the precision and
generalizability of drug synergy predictions. Our findings demonstrate that
VQSynergy surpasses existing models in terms of robustness, particularly under
Gaussian noise conditions, highlighting its superior performance and utility in
the complex and often noisy domain of drug synergy research. This study
underscores the potential of VQSynergy in revolutionizing the field through its
advanced predictive capabilities, thereby contributing to the optimization of
cancer treatment strategies.
\\ ( https://arxiv.org/abs/2403.03089 ,  612kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03100 (*cross-listing*)
Date: Tue, 5 Mar 2024 16:35:25 GMT   (892kb,D)

Title: NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and
  Diffusion Models
Authors: Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang,
  Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin,
  Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, Sheng Zhao
Categories: eess.AS cs.AI cs.CL cs.LG cs.SD
Comments: 22 pages, 15 tables, 3 figures
\\
  While recent large-scale text-to-speech (TTS) models have achieved
significant progress, they still fall short in speech quality, similarity, and
prosody. Considering speech intricately encompasses various attributes (e.g.,
content, prosody, timbre, and acoustic details) that pose significant
challenges for generation, a natural idea is to factorize speech into
individual subspaces representing different attributes and generate them
individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with
novel factorized diffusion models to generate natural speech in a zero-shot
way. Specifically, 1) we design a neural codec with factorized vector
quantization (FVQ) to disentangle speech waveform into subspaces of content,
prosody, timbre, and acoustic details; 2) we propose a factorized diffusion
model to generate attributes in each subspace following its corresponding
prompt. With this factorization design, NaturalSpeech 3 can effectively and
efficiently model the intricate speech with disentangled subspaces in a
divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the
state-of-the-art TTS systems on quality, similarity, prosody, and
intelligibility. Furthermore, we achieve better performance by scaling to 1B
parameters and 200K hours of training data.
\\ ( https://arxiv.org/abs/2403.03100 ,  892kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03111 (*cross-listing*)
Date: Tue, 5 Mar 2024 16:53:24 GMT   (7370kb,D)

Title: Improved LiDAR Odometry and Mapping using Deep Semantic Segmentation and
  Novel Outliers Detection
Authors: Mohamed Afifi, Mohamed ElHelw
Categories: cs.CV cs.AI cs.RO
\\
  Perception is a key element for enabling intelligent autonomous navigation.
Understanding the semantics of the surrounding environment and accurate vehicle
pose estimation are essential capabilities for autonomous vehicles, including
self-driving cars and mobile robots that perform complex tasks. Fast moving
platforms like self-driving cars impose a hard challenge for localization and
mapping algorithms. In this work, we propose a novel framework for real-time
LiDAR odometry and mapping based on LOAM architecture for fast moving
platforms. Our framework utilizes semantic information produced by a deep
learning model to improve point-to-line and point-to-plane matching between
LiDAR scans and build a semantic map of the environment, leading to more
accurate motion estimation using LiDAR data. We observe that including semantic
information in the matching process introduces a new type of outlier matches to
the process, where matching occur between different objects of the same
semantic class. To this end, we propose a novel algorithm that explicitly
identifies and discards potential outliers in the matching process. In our
experiments, we study the effect of improving the matching process on the
robustness of LiDAR odometry against high speed motion. Our experimental
evaluations on KITTI dataset demonstrate that utilizing semantic information
and rejecting outliers significantly enhance the robustness of LiDAR odometry
and mapping when there are large gaps between scan acquisition poses, which is
typical for fast moving platforms.
\\ ( https://arxiv.org/abs/2403.03111 ,  7370kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03114 (*cross-listing*)
Date: Tue, 5 Mar 2024 16:56:09 GMT   (38kb,D)

Title: Equilibria in Two-Stage Facility Location with Atomic Clients
Authors: Simon Krogmann, Pascal Lenzner, Alexander Skopalik, Marc Uetz and
  Marnix C. Vos
Categories: cs.GT cs.AI
\\
  We consider competitive facility location as a two-stage multi-agent system
with two types of clients. For a given host graph with weighted clients on the
vertices, first facility agents strategically select vertices for opening their
facilities. Then, the clients strategically select which of the opened
facilities in their neighborhood to patronize. Facilities want to attract as
much client weight as possible, clients want to minimize congestion on the
chosen facility.
  All recently studied versions of this model assume that clients can split
their weight strategically. We consider clients with unsplittable weights, but
allow mixed strategies. So clients may randomize over which facility to
patronize. Besides modeling a natural client behavior, this subtle change
yields drastic changes, e.g., for a given facility placement, qualitatively
different client equilibria are possible.
  As our main result, we show that pure subgame perfect equilibria always exist
if all client weights are identical. For this, we use a novel potential
function argument, employing a hierarchical classification of the clients and
sophisticated rounding in each step. In contrast, for non-identical clients, we
show that deciding the existence of even approximately stable states is
computationally intractable. On the positive side, we give a tight bound of 2
on the price of anarchy which implies high social welfare of equilibria, if
they exist.
\\ ( https://arxiv.org/abs/2403.03114 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03134 (*cross-listing*)
Date: Tue, 5 Mar 2024 17:21:31 GMT   (28899kb,D)

Title: Simplicity in Complexity
Authors: Kevin Shen, Surabhi S Nath, Aenne Brielmann, Peter Dayan
Categories: cs.CV cs.AI q-bio.NC
\\
  The complexity of visual stimuli plays an important role in many cognitive
phenomena, including attention, engagement, memorability, time perception and
aesthetic evaluation. Despite its importance, complexity is poorly understood
and ironically, previous models of image complexity have been quite
\textit{complex}. There have been many attempts to find handcrafted features
that explain complexity, but these features are usually dataset specific, and
hence fail to generalise. On the other hand, more recent work has employed deep
neural networks to predict complexity, but these models remain difficult to
interpret, and do not guide a theoretical understanding of the problem. Here we
propose to model complexity using segment-based representations of images. We
use state-of-the-art segmentation models, SAM and FC-CLIP, to quantify the
number of segments at multiple granularities, and the number of classes in an
image respectively. We find that complexity is well-explained by a simple
linear model with these two features across six diverse image-sets of
naturalistic scene and art images. This suggests that the complexity of images
can be surprisingly simple.
\\ ( https://arxiv.org/abs/2403.03134 ,  28899kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03154 (*cross-listing*)
Date: Tue, 5 Mar 2024 17:47:22 GMT   (4051kb,D)

Title: Quantum Many-Body Physics Calculations with Large Language Models
Authors: Haining Pan, Nayantara Mudur, Will Taranto, Maria Tikhanovskaya,
  Subhashini Venugopalan, Yasaman Bahri, Michael P. Brenner, Eun-Ah Kim
Categories: physics.comp-ph cond-mat.other cs.AI
Comments: 9 pages, 4 figures. Supplemental material in the source file
\\
  Large language models (LLMs) have demonstrated an unprecedented ability to
perform complex tasks in multiple domains, including mathematical and
scientific reasoning. We demonstrate that with carefully designed prompts, LLMs
can accurately carry out key calculations in research papers in theoretical
physics. We focus on a broadly used approximation method in quantum physics:
the Hartree-Fock method, requiring an analytic multi-step calculation deriving
approximate Hamiltonian and corresponding self-consistency equations. To carry
out the calculations using LLMs, we design multi-step prompt templates that
break down the analytic calculation into standardized steps with placeholders
for problem-specific information. We evaluate GPT-4's performance in executing
the calculation for 15 research papers from the past decade, demonstrating
that, with correction of intermediate steps, it can correctly derive the final
Hartree-Fock Hamiltonian in 13 cases and makes minor errors in 2 cases.
Aggregating across all research papers, we find an average score of 87.5 (out
of 100) on the execution of individual calculation steps. Overall, the
requisite skill for doing these calculations is at the graduate level in
quantum condensed matter theory. We further use LLMs to mitigate the two
primary bottlenecks in this evaluation process: (i) extracting information from
papers to fill in templates and (ii) automatic scoring of the calculation
steps, demonstrating good results in both cases. The strong performance is the
first step for developing algorithms that automatically explore theoretical
hypotheses at an unprecedented scale.
\\ ( https://arxiv.org/abs/2403.03154 ,  4051kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03168 (*cross-listing*)
Date: Tue, 5 Mar 2024 18:03:51 GMT   (433kb,D)

Title: Learning Explicitly Conditioned Sparsifying Transforms
Authors: Andrei P\u{a}tra\c{s}cu, Cristian Rusu, Paul Irofti
Categories: math.NA cs.AI cs.LG cs.NA math.OC
\\
  Sparsifying transforms became in the last decades widely known tools for
finding structured sparse representations of signals in certain transform
domains. Despite the popularity of classical transforms such as DCT and
Wavelet, learning optimal transforms that guarantee good representations of
data into the sparse domain has been recently analyzed in a series of papers.
Typically, the conditioning number and representation ability are complementary
key features of learning square transforms that may not be explicitly
controlled in a given optimization model. Unlike the existing approaches from
the literature, in our paper, we consider a new sparsifying transform model
that enforces explicit control over the data representation quality and the
condition number of the learned transforms. We confirm through numerical
experiments that our model presents better numerical behavior than the
state-of-the-art.
\\ ( https://arxiv.org/abs/2403.03168 ,  433kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03170 (*cross-listing*)
Date: Tue, 5 Mar 2024 18:04:59 GMT   (1956kb,D)

Title: SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context
  Misinformation Detection
Authors: Peng Qi, Zehong Yan, Wynne Hsu, Mong Li Lee
Categories: cs.MM cs.AI cs.CL cs.CY
Comments: To appear in CVPR 2024
\\
  Misinformation is a prevalent societal issue due to its potential high risks.
Out-of-context (OOC) misinformation, where authentic images are repurposed with
false text, is one of the easiest and most effective ways to mislead audiences.
Current methods focus on assessing image-text consistency but lack convincing
explanations for their judgments, which is essential for debunking
misinformation. While Multimodal Large Language Models (MLLMs) have rich
knowledge and innate capability for visual reasoning and explanation
generation, they still lack sophistication in understanding and discovering the
subtle crossmodal differences. In this paper, we introduce SNIFFER, a novel
multimodal large language model specifically engineered for OOC misinformation
detection and explanation. SNIFFER employs two-stage instruction tuning on
InstructBLIP. The first stage refines the model's concept alignment of generic
objects with news-domain entities and the second stage leverages language-only
GPT-4 generated OOC-specific instruction data to fine-tune the model's
discriminatory powers. Enhanced by external tools and retrieval, SNIFFER not
only detects inconsistencies between text and image but also utilizes external
knowledge for contextual verification. Our experiments show that SNIFFER
surpasses the original MLLM by over 40% and outperforms state-of-the-art
methods in detection accuracy. SNIFFER also provides accurate and persuasive
explanations as validated by quantitative and human evaluations.
\\ ( https://arxiv.org/abs/2403.03170 ,  1956kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03174 (*cross-listing*)
Date: Tue, 5 Mar 2024 18:08:45 GMT   (11673kb,D)

Title: MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual
  Prompting
Authors: Fangchen Liu, Kuan Fang, Pieter Abbeel, Sergey Levine
Categories: cs.RO cs.AI
\\
  Open-vocabulary generalization requires robotic systems to perform tasks
involving complex and diverse environments and task goals. While the recent
advances in vision language models (VLMs) present unprecedented opportunities
to solve unseen problems, how to utilize their emergent capabilities to control
robots in the physical world remains an open question. In this paper, we
present MOKA (Marking Open-vocabulary Keypoint Affordances), an approach that
employs VLMs to solve robotic manipulation tasks specified by free-form
language descriptions. At the heart of our approach is a compact point-based
representation of affordance and motion that bridges the VLM's predictions on
RGB images and the robot's motions in the physical world. By prompting a VLM
pre-trained on Internet-scale data, our approach predicts the affordances and
generates the corresponding motions by leveraging the concept understanding and
commonsense knowledge from broad sources. To scaffold the VLM's reasoning in
zero-shot, we propose a visual prompting technique that annotates marks on the
images, converting the prediction of keypoints and waypoints into a series of
visual question answering problems that are feasible for the VLM to solve.
Using the robot experiences collected in this way, we further investigate ways
to bootstrap the performance through in-context learning and policy
distillation. We evaluate and analyze MOKA's performance on a variety of
manipulation tasks specified by free-form language descriptions, such as tool
use, deformable body manipulation, and object rearrangement.
\\ ( https://arxiv.org/abs/2403.03174 ,  11673kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02563 (*cross-listing*)
Date: Tue, 5 Mar 2024 00:37:36 GMT   (100kb)

Title: Systemic Biases in Sign Language AI Research: A Deaf-Led Call to
  Reevaluate Research Agendas
Authors: Aashaka Desai, Maartje De Meulder, Julie A. Hochgesang, Annemarie
  Kocab, Alex X. Lu
Categories: cs.CV cs.CL
\\
  Growing research in sign language recognition, generation, and translation AI
has been accompanied by calls for ethical development of such technologies.
While these works are crucial to helping individual researchers do better,
there is a notable lack of discussion of systemic biases or analysis of
rhetoric that shape the research questions and methods in the field, especially
as it remains dominated by hearing non-signing researchers. Therefore, we
conduct a systematic review of 101 recent papers in sign language AI. Our
analysis identifies significant biases in the current state of sign language AI
research, including an overfocus on addressing perceived communication
barriers, a lack of use of representative datasets, use of annotations lacking
linguistic foundations, and development of methods that build on flawed models.
We take the position that the field lacks meaningful input from Deaf
stakeholders, and is instead driven by what decisions are the most convenient
or perceived as important to hearing researchers. We end with a call to action:
the field must make space for Deaf researchers to lead the conversation in sign
language AI.
\\ ( https://arxiv.org/abs/2403.02563 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02677 (*cross-listing*)
Date: Tue, 5 Mar 2024 06:05:15 GMT   (4399kb,D)

Title: Finetuned Multimodal Language Models Are High-Quality Image-Text Data
  Filters
Authors: Weizhi Wang, Khalil Mrini, Linjie Yang, Sateesh Kumar, Yu Tian, Xifeng
  Yan, Heng Wang
Categories: cs.CV cs.CL
Comments: Project Website: https://mlm-filter.github.io
\\
  We propose a novel framework for filtering image-text data by leveraging
fine-tuned Multimodal Language Models (MLMs). Our approach outperforms
predominant filtering methods (e.g., CLIPScore) via integrating the recent
advances in MLMs. We design four distinct yet complementary metrics to
holistically measure the quality of image-text data. A new pipeline is
established to construct high-quality instruction data for fine-tuning MLMs as
data filters. Comparing with CLIPScore, our MLM filters produce more precise
and comprehensive scores that directly improve the quality of filtered data and
boost the performance of pre-trained models. We achieve significant
improvements over CLIPScore on popular foundation models (i.e., CLIP and BLIP2)
and various downstream tasks. Our MLM filter can generalize to different models
and tasks, and be used as a drop-in replacement for CLIPScore. An additional
ablation study is provided to verify our design choices for the MLM filter.
\\ ( https://arxiv.org/abs/2403.02677 ,  4399kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02875 (*cross-listing*)
Date: Tue, 5 Mar 2024 11:38:48 GMT   (4371kb,D)

Title: Enhancing Conceptual Understanding in Multimodal Contrastive Learning
  through Hard Negative Samples
Authors: Philipp J. R\"osch and Norbert Oswald and Michaela Geierhos and
  Jind\v{r}ich Libovick\'y
Categories: cs.CV cs.CL cs.IR
Comments: 22 pages
\\
  Current multimodal models leveraging contrastive learning often face
limitations in developing fine-grained conceptual understanding. This is due to
random negative samples during pretraining, causing almost exclusively very
dissimilar concepts to be compared in the loss function. Consequently, the
models struggle with fine-grained semantic differences. To address this
problem, we introduce a novel pretraining method incorporating synthetic hard
negative text examples. The hard negatives permute terms corresponding to
visual concepts, leading to a more fine-grained visual and textual concept
alignment. Further, we introduce InpaintCOCO, a new challenging dataset for
assessing the fine-grained alignment of colors, objects, and sizes in
vision-language models. We created the dataset using generative inpainting from
COCO images by changing the visual concepts so that the images no longer match
their original captions. Our results show significant improvements in
fine-grained concept understanding across a wide range of vision-language
datasets, including our InpaintCOCO dataset.
\\ ( https://arxiv.org/abs/2403.02875 ,  4371kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17948 (*cross-listing*)
Date: Wed, 29 Nov 2023 05:28:05 GMT   (28631kb,D)

Title: Action-slot: Visual Action-centric Representations for Multi-label
  Atomic Activity Recognition in Traffic Scenes
Authors: Chi-Hsi Kung, Shu-Wei Lu, Yi-Hsuan Tsai, Yi-Ting Chen
Categories: cs.CV cs.LG
\\
  In this paper, we study multi-label atomic activity recognition. Despite the
notable progress in action recognition, it is still challenging to recognize
atomic activities due to a deficiency in a holistic understanding of both
multiple road users' motions and their contextual information. In this paper,
we introduce Action-slot, a slot attention-based approach that learns visual
action-centric representations, capturing both motion and contextual
information. Our key idea is to design action slots that are capable of paying
attention to regions where atomic activities occur, without the need for
explicit perception guidance. To further enhance slot attention, we introduce a
background slot that competes with action slots, aiding the training process in
avoiding unnecessary focus on background regions devoid of activities. Yet, the
imbalanced class distribution in the existing dataset hampers the assessment of
rare activities. To address the limitation, we collect a synthetic dataset
called TACO, which is four times larger than OATS and features a balanced
distribution of atomic activities. To validate the effectiveness of our method,
we conduct comprehensive experiments and ablation studies against various
action recognition baselines. We also show that the performance of multi-label
atomic activity recognition on real-world datasets can be improved by
pretraining representations on TACO. We will release our source code and
dataset. See the videos of visualization on the project page:
https://hcis-lab.github.io/Action-slot/
\\ ( https://arxiv.org/abs/2311.17948 ,  28631kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02405 (*cross-listing*)
Date: Mon, 4 Mar 2024 19:01:14 GMT   (1034kb,D)

Title: Classification of the Fashion-MNIST Dataset on a Quantum Computer
Authors: Kevin Shen, Bernhard Jobst, Elvira Shishenina, Frank Pollmann
Categories: quant-ph cs.LG
Comments: (15 pages, 11 figures)
\\
  The potential impact of quantum machine learning algorithms on industrial
applications remains an exciting open question. Conventional methods for
encoding classical data into quantum computers are not only too costly for a
potential quantum advantage in the algorithms but also severely limit the scale
of feasible experiments on current hardware. Therefore, recent works, despite
claiming the near-term suitability of their algorithms, do not provide
experimental benchmarking on standard machine learning datasets. We attempt to
solve the data encoding problem by improving a recently proposed variational
algorithm [1] that approximately prepares the encoded data, using
asymptotically shallow circuits that fit the native gate set and topology of
currently available quantum computers. We apply the improved algorithm to
encode the Fashion-MNIST dataset [2], which can be directly used in future
empirical studies of quantum machine learning algorithms. We deploy simple
quantum variational classifiers trained on the encoded dataset on a current
quantum computer ibmq-kolkata [3] and achieve moderate accuracies, providing a
proof of concept for the near-term usability of our data encoding method.
\\ ( https://arxiv.org/abs/2403.02405 ,  1034kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02411 (*cross-listing*)
Date: Mon, 4 Mar 2024 19:08:20 GMT   (248kb,D)

Title: NiNformer: A Network in Network Transformer with Token Mixing Generated
  Gating Function
Authors: Abdullah Nazhat Abdullah, Tarkan Aydin
Categories: cs.CV cs.LG
\\
  The Attention mechanism is the main component of the Transformer
architecture, and since its introduction, it has led to significant
advancements in Deep Learning that span many domains and multiple tasks. The
Attention Mechanism was utilized in Computer Vision as the Vision Transformer
ViT, and its usage has expanded into many tasks in the vision domain, such as
classification, segmentation, object detection, and image generation. While
this mechanism is very expressive and capable, it comes with the drawback of
being computationally expensive and requiring datasets of considerable size for
effective optimization. To address these shortcomings, many designs have been
proposed in the literature to reduce the computational burden and alleviate the
data size requirements. Examples of such attempts in the vision domain are the
MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper
introduces a new computational block as an alternative to the standard ViT
block that reduces the compute burdens by replacing the normal Attention layers
with a Network in Network structure that enhances the static approach of the
MLP Mixer with a dynamic system of learning an element-wise gating function by
a token mixing process. Extensive experimentation shows that the proposed
design provides better performance than the baseline architectures on multiple
datasets applied in the image classification task of the vision domain.
\\ ( https://arxiv.org/abs/2403.02411 ,  248kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02426 (*cross-listing*)
Date: Mon, 4 Mar 2024 19:18:53 GMT   (6857kb,D)

Title: Digital Twins and Civil Engineering Phases: Reorienting Adoption
  Strategies
Authors: Taiwo A. Adebiyi and Nafeezat A. Ajenifuja and Ruda Zhang
Categories: cs.CE cs.LG stat.AP
\\
  Digital twin (DT) technology has received immense attention over the years
due to the promises it presents to various stakeholders in science and
engineering. As a result, different thematic areas of DT have been explored.
This is no different in specific fields such as manufacturing, automation, oil
and gas, and civil engineering, leading to fragmented approaches for
field-specific applications. The civil engineering industry is further
disadvantaged in this regard as it relies on external techniques by other
engineering fields for its DT adoption. A rising consequence of these
extensions is a concentrated application of DT to the operations and
maintenance phase. On another spectrum, Building Information Modeling (BIM) are
pervasively utilized in the planning/design phase, and the transient nature of
the construction phase remains a challenge for its DT adoption. In this paper,
we present a phase-based development of DT in the Architecture, Engineering,
and Construction industry. We commence by presenting succinct expositions on DT
as a concept and as a service and establish a five-level scale system.
Furthermore, we present separately a systematic literature review of the
conventional techniques employed at each civil engineering phase. In this
regard, we identified enabling technologies such as computer vision for
extended sensing and the Internet of Things for reliable integration.
Ultimately, we attempt to reveal DT as an important tool across the entire life
cycle of civil engineering projects and nudge researchers to think more
holistically in their quest for the integration of DT for civil engineering
applications.
\\ ( https://arxiv.org/abs/2403.02426 ,  6857kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02432 (*cross-listing*)
Date: Mon, 4 Mar 2024 19:26:39 GMT   (230kb,D)

Title: On the impact of measure pre-conditionings on general parametric ML
  models and transfer learning via domain adaptation
Authors: Joaqu\'in S\'anchez Garc\'ia
Categories: stat.ML cs.LG math.OC
\\
  We study a new technique for understanding convergence of learning agents
under small modifications of data. We show that such convergence can be
understood via an analogue of Fatou's lemma which yields gamma-convergence. We
show it's relevance and applications in general machine learning tasks and
domain adaptation transfer learning.
\\ ( https://arxiv.org/abs/2403.02432 ,  230kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02467 (*cross-listing*)
Date: Mon, 4 Mar 2024 20:28:28 GMT   (15666kb)

Title: Applied Causal Inference Powered by ML and AI
Authors: Victor Chernozhukov, Christian Hansen, Nathan Kallus, Martin Spindler,
  Vasilis Syrgkanis
Categories: econ.EM cs.LG stat.ME stat.ML
\\
  An introduction to the emerging fusion of machine learning and causal
inference. The book presents ideas from classical structural equation models
(SEMs) and their modern AI equivalent, directed acyclical graphs (DAGs) and
structural causal models (SCMs), and covers Double/Debiased Machine Learning
methods to do inference in such models using modern predictive tools.
\\ ( https://arxiv.org/abs/2403.02467 ,  15666kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02469 (*cross-listing*)
Date: Mon, 4 Mar 2024 20:29:51 GMT   (197kb,D)

Title: Vision-Language Models for Medical Report Generation and Visual Question
  Answering: A Review
Authors: Iryna Hartsock and Ghulam Rasool
Categories: cs.CV cs.LG
Comments: 42 pages
\\
  Medical vision-language models (VLMs) combine computer vision and natural
language processing to analyze visual and textual medical data. Our paper
reviews recent advancements in developing VLMs specialized for healthcare,
focusing on models designed for medical report generation and visual question
answering. We provide background on natural language processing and computer
vision, explaining how techniques from both fields are integrated into VLMs to
enable learning from multimodal data. Key areas we address include the
exploration of medical vision-language datasets, in-depth analyses of
architectures and pre-training strategies employed in recent noteworthy medical
VLMs, and comprehensive discussion on evaluation metrics for assessing VLMs'
performance in medical report generation and visual question answering. We also
highlight current challenges and propose future directions, including enhancing
clinical validity and addressing patient privacy concerns. Overall, our review
summarizes recent progress in developing VLMs to harness multimodal medical
data for improved healthcare applications.
\\ ( https://arxiv.org/abs/2403.02469 ,  197kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02500 (*cross-listing*)
Date: Mon, 4 Mar 2024 21:48:32 GMT   (133kb)

Title: RVRAE: A Dynamic Factor Model Based on Variational Recurrent Autoencoder
  for Stock Returns Prediction
Authors: Yilun Wang, Shengjie Guo
Categories: q-fin.PM cs.LG q-fin.PR
\\
  In recent years, the dynamic factor model has emerged as a dominant tool in
economics and finance, particularly for investment strategies. This model
offers improved handling of complex, nonlinear, and noisy market conditions
compared to traditional static factor models. The advancement of machine
learning, especially in dealing with nonlinear data, has further enhanced asset
pricing methodologies. This paper introduces a groundbreaking dynamic factor
model named RVRAE. This model is a probabilistic approach that addresses the
temporal dependencies and noise in market data. RVRAE ingeniously combines the
principles of dynamic factor modeling with the variational recurrent
autoencoder (VRAE) from deep learning. A key feature of RVRAE is its use of a
prior-posterior learning method. This method fine-tunes the model's learning
process by seeking an optimal posterior factor model informed by future data.
Notably, RVRAE is adept at risk modeling in volatile stock markets, estimating
variances from latent space distributions while also predicting returns. Our
empirical tests with real stock market data underscore RVRAE's superior
performance compared to various established baseline methods.
\\ ( https://arxiv.org/abs/2403.02500 ,  133kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02506 (*cross-listing*)
Date: Mon, 4 Mar 2024 21:52:25 GMT   (4712kb,D)

Title: Differentially Private Representation Learning via Image Captioning
Authors: Tom Sander, Yaodong Yu, Maziar Sanjabi, Alain Durmus, Yi Ma, Kamalika
  Chaudhuri, Chuan Guo
Categories: cs.CV cs.LG
\\
  Differentially private (DP) machine learning is considered the gold-standard
solution for training a model from sensitive data while still preserving
privacy. However, a major barrier to achieving this ideal is its sub-optimal
privacy-accuracy trade-off, which is particularly visible in DP representation
learning. Specifically, it has been shown that under modest privacy budgets,
most models learn representations that are not significantly better than
hand-crafted features. In this work, we show that effective DP representation
learning can be done via image captioning and scaling up to internet-scale
multimodal datasets. Through a series of engineering tricks, we successfully
train a DP image captioner (DP-Cap) on a 233M subset of LAION-2B from scratch
using a reasonable amount of computation, and obtaining unprecedented
high-quality image features that can be used in a variety of downstream vision
and vision-language tasks. For example, under a privacy budget of
$\varepsilon=8$, a linear classifier trained on top of learned DP-Cap features
attains 65.8% accuracy on ImageNet-1K, considerably improving the previous SOTA
of 56.5%. Our work challenges the prevailing sentiment that high-utility DP
representation learning cannot be achieved by training from scratch.
\\ ( https://arxiv.org/abs/2403.02506 ,  4712kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02524 (*cross-listing*)
Date: Mon, 4 Mar 2024 22:28:20 GMT   (7870kb,D)

Title: Koopman operators with intrinsic observables in rigged reproducing
  kernel Hilbert spaces
Authors: Isao Ishikawa, Yuka Hashimoto, Masahiro Ikeda, Yoshinobu Kawahara
Categories: math.DS cs.LG math.FA math.SP stat.ML
MSC-class: 47B33, 37M10, 65P99, 65F99, 47A10
\\
  This paper presents a novel approach for estimating the Koopman operator
defined on a reproducing kernel Hilbert space (RKHS) and its spectra. We
propose an estimation method, what we call Jet Dynamic Mode Decomposition
(JetDMD), leveraging the intrinsic structure of RKHS and the geometric notion
known as jets to enhance the estimation of the Koopman operator. This method
refines the traditional Extended Dynamic Mode Decomposition (EDMD) in accuracy,
especially in the numerical estimation of eigenvalues. This paper proves
JetDMD's superiority through explicit error bounds and convergence rate for
special positive definite kernels, offering a solid theoretical foundation for
its performance. We also delve into the spectral analysis of the Koopman
operator, proposing the notion of extended Koopman operator within a framework
of rigged Hilbert space. This notion leads to a deeper understanding of
estimated Koopman eigenfunctions and capturing them outside the original
function space. Through the theory of rigged Hilbert space, our study provides
a principled methodology to analyze the estimated spectrum and eigenfunctions
of Koopman operators, and enables eigendecomposition within a rigged RKHS. We
also propose a new effective method for reconstructing the dynamical system
from temporally-sampled trajectory data of the dynamical system with solid
theoretical guarantee. We conduct several numerical simulations using the van
der Pol oscillator, the Duffing oscillator, the H\'enon map, and the Lorenz
attractor, and illustrate the performance of JetDMD with clear numerical
computations of eigenvalues and accurate predictions of the dynamical systems.
\\ ( https://arxiv.org/abs/2403.02524 ,  7870kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02536 (*cross-listing*)
Date: Mon, 4 Mar 2024 23:12:17 GMT   (1578kb,D)

Title: Forecasting SEP Events During Solar Cycles 23 and 24 Using Interpretable
  Machine Learning
Authors: Spiridon Kasapis, Irina N. Kitiashvili, Paul Kosovich, Alexander G.
  Kosovichev, Viacheslav M. Sadykov, Patrick O'Keefe, Vincent Wang
Categories: astro-ph.SR cs.LG physics.space-ph
Comments: Article submitted and is under revision to the AAS Astrophysical
  Journal
\\
  Prediction of the Solar Energetic Particle (SEP) events garner increasing
interest as space missions extend beyond Earth's protective magnetosphere.
These events, which are, in most cases, products of magnetic
reconnection-driven processes during solar flares or fast
coronal-mass-ejection-driven shock waves, pose significant radiation hazards to
aviation, space-based electronics, and particularly, space exploration. In this
work, we utilize the recently developed dataset that combines the Solar
Dynamics Observatory/Helioseismic and Magnetic Imager's (SDO/HMI) Space weather
HMI Active Region Patches (SHARP) and the Solar and Heliospheric
Observatory/Michelson Doppler Imager's (SoHO/MDI) Space Weather MDI Active
Region Patches (SMARP). We employ a suite of machine learning strategies,
including Support Vector Machines (SVM) and regression models, to evaluate the
predictive potential of this new data product for a forecast of post-solar
flare SEP events. Our study indicates that despite the augmented volume of
data, the prediction accuracy reaches 0.7 +- 0.1, which aligns with but does
not exceed these published benchmarks. A linear SVM model with training and
testing configurations that mimic an operational setting (positive-negative
imbalance) reveals a slight increase (+ 0.04 +- 0.05) in the accuracy of a
14-hour SEP forecast compared to previous studies. This outcome emphasizes the
imperative for more sophisticated, physics-informed models to better understand
the underlying processes leading to SEP events.
\\ ( https://arxiv.org/abs/2403.02536 ,  1578kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02544 (*cross-listing*)
Date: Mon, 4 Mar 2024 23:40:02 GMT   (10303kb,D)

Title: Coronary artery segmentation in non-contrast calcium scoring CT images
  using deep learning
Authors: Mariusz Bujny, Katarzyna Jesionek, Jakub Nalepa, Karol
  Miszalski-Jamka, Katarzyna Widawka-\.Zak, Sabina Wolny, Marcin Kostur
Categories: eess.IV cs.CV cs.LG
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\
  Precise localization of coronary arteries in Computed Tomography (CT) scans
is critical from the perspective of medical assessment of coronary artery
disease. Although various methods exist that offer high-quality segmentation of
coronary arteries in cardiac contrast-enhanced CT scans, the potential of less
invasive, non-contrast CT in this area is still not fully exploited. Since such
fine anatomical structures are hardly visible in this type of medical images,
the existing methods are characterized by high recall and low precision, and
are used mainly for filtering of atherosclerotic plaques in the context of
calcium scoring. In this paper, we address this research gap and introduce a
deep learning algorithm for segmenting coronary arteries in multi-vendor
ECG-gated non-contrast cardiac CT images which benefits from a novel framework
for semi-automatic generation of Ground Truth (GT) via image registration. We
hypothesize that the proposed GT generation process is much more efficient in
this case than manual segmentation, since it allows for a fast generation of
large volumes of diverse data, which leads to well-generalizing models. To
investigate and thoroughly evaluate the segmentation quality based on such an
approach, we propose a novel method for manual mesh-to-image registration,
which is used to create our test-GT. The experimental study shows that the
trained model has significantly higher accuracy than the GT used for training,
and leads to the Dice and clDice metrics close to the interrater variability.
\\ ( https://arxiv.org/abs/2403.02544 ,  10303kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02576 (*cross-listing*)
Date: Tue, 5 Mar 2024 01:17:56 GMT   (9666kb,D)

Title: AceMap: Knowledge Discovery through Academic Graph
Authors: Xinbing Wang, Luoyi Fu, Xiaoying Gan, Ying Wen, Guanjie Zheng, Jiaxin
  Ding, Liyao Xiang, Nanyang Ye, Meng Jin, Shiyu Liang, Bin Lu, Haiwen Wang, Yi
  Xu, Cheng Deng, Shao Zhang, Huquan Kang, Xingli Wang, Qi Li, Zhixin Guo,
  Jiexing Qi, Pan Liu, Yuyang Ren, Lyuwen Wu, Jungang Yang, Jianping Zhou,
  Chenghu Zhou
Categories: cs.DL cs.LG cs.SI
Comments: Technical Report for AceMap (https://www.acemap.info)
\\
  The exponential growth of scientific literature requires effective management
and extraction of valuable insights. While existing scientific search engines
excel at delivering search results based on relational databases, they often
neglect the analysis of collaborations between scientific entities and the
evolution of ideas, as well as the in-depth analysis of content within
scientific publications. The representation of heterogeneous graphs and the
effective measurement, analysis, and mining of such graphs pose significant
challenges. To address these challenges, we present AceMap, an academic system
designed for knowledge discovery through academic graph. We present advanced
database construction techniques to build the comprehensive AceMap database
with large-scale academic publications that contain rich visual, textual, and
numerical information. AceMap also employs innovative visualization,
quantification, and analysis methods to explore associations and logical
relationships among academic entities. AceMap introduces large-scale academic
network visualization techniques centered on nebular graphs, providing a
comprehensive view of academic networks from multiple perspectives. In
addition, AceMap proposes a unified metric based on structural entropy to
quantitatively measure the knowledge content of different academic entities.
Moreover, AceMap provides advanced analysis capabilities, including tracing the
evolution of academic ideas through citation relationships and concept
co-occurrence, and generating concise summaries informed by this evolutionary
process. In addition, AceMap uses machine reading methods to generate potential
new ideas at the intersection of different fields. Exploring the integration of
large language models and knowledge graphs is a promising direction for future
research in idea evolution. Please visit \url{https://www.acemap.info} for
further exploration.
\\ ( https://arxiv.org/abs/2403.02576 ,  9666kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02579 (*cross-listing*)
Date: Tue, 5 Mar 2024 01:30:34 GMT   (4418kb,D)

Title: Geometric Dynamics of Signal Propagation Predict Trainability of
  Transformers
Authors: Aditya Cowsik, Tamra Nebabu, Xiao-Liang Qi, Surya Ganguli
Categories: cond-mat.dis-nn cs.LG
\\
  We investigate forward signal propagation and gradient back propagation in
deep, randomly initialized transformers, yielding simple necessary and
sufficient conditions on initialization hyperparameters that ensure
trainability of deep transformers. Our approach treats the evolution of the
representations of $n$ tokens as they propagate through the transformer layers
in terms of a discrete time dynamical system of $n$ interacting particles. We
derive simple update equations for the evolving geometry of this particle
system, starting from a permutation symmetric simplex. Our update equations
show that without MLP layers, this system will collapse to a line, consistent
with prior work on rank collapse in transformers. However, unlike prior work,
our evolution equations can quantitatively track particle geometry in the
additional presence of nonlinear MLP layers, and it reveals an order-chaos
phase transition as a function of initialization hyperparameters, like the
strength of attentional and MLP residual connections and weight variances. In
the ordered phase the particles are attractive and collapse to a line, while in
the chaotic phase the particles are repulsive and converge to a regular
$n$-simplex. We analytically derive two Lyapunov exponents: an angle exponent
that governs departures from the edge of chaos in this particle system, and a
gradient exponent that governs the rate of exponential growth or decay of
backpropagated gradients. We show through experiments that, remarkably, the
final test loss at the end of training is well predicted just by these two
exponents at the beginning of training, and that the simultaneous vanishing of
these two exponents yields a simple necessary and sufficient condition to
achieve minimal test loss.
\\ ( https://arxiv.org/abs/2403.02579 ,  4418kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02580 (*cross-listing*)
Date: Tue, 5 Mar 2024 01:32:29 GMT   (22205kb,D)

Title: What do we learn from inverting CLIP models?
Authors: Hamid Kazemi, Atoosa Chegini, Jonas Geiping, Soheil Feizi, Tom
  Goldstein
Categories: cs.CV cs.LG
Comments: Warning: This paper contains sexually explicit images and language,
  offensive visuals and terminology, discussions on pornography, gender bias,
  and other potentially unsettling, distressing, and/or offensive content for
  certain readers
\\
  We employ an inversion-based approach to examine CLIP models. Our examination
reveals that inverting CLIP models results in the generation of images that
exhibit semantic alignment with the specified target prompts. We leverage these
inverted images to gain insights into various aspects of CLIP models, such as
their ability to blend concepts and inclusion of gender biases. We notably
observe instances of NSFW (Not Safe For Work) images during model inversion.
This phenomenon occurs even for semantically innocuous prompts, like "a
beautiful landscape," as well as for prompts involving the names of
celebrities.
\\ ( https://arxiv.org/abs/2403.02580 ,  22205kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02609 (*cross-listing*)
Date: Tue, 5 Mar 2024 02:53:24 GMT   (2278kb,D)

Title: Search Intenion Network for Personalized Query Auto-Completion in
  E-Commerce
Authors: Wei Bao, Mi Zhang, Tao Zhang, Chengfu Huo
Categories: cs.IR cs.LG
\\
  Query Auto-Completion(QAC), as an important part of the modern search engine,
plays a key role in complementing user queries and helping them refine their
search intentions.Today's QAC systems in real-world scenarios face two major
challenges:1)intention equivocality(IE): during the user's typing process,the
prefix often contains a combination of characters and subwords, which makes the
current intention ambiguous and difficult to model.2)intention transfer
(IT):previous works make personalized recommendations based on users'
historical sequences, but ignore the search intention transfer.However, the
current intention extracted from prefix may be contrary to the historical
preferences.
\\ ( https://arxiv.org/abs/2403.02609 ,  2278kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02626 (*cross-listing*)
Date: Tue, 5 Mar 2024 03:34:11 GMT   (1423kb,D)

Title: Modeling Collaborator: Enabling Subjective Vision Classification With
  Minimal Human Effort via LLM Tool-Use
Authors: Imad Eddine Toubal, Aditya Avinash, Neil Gordon Alldrin, Jan Dlabal,
  Wenlei Zhou, Enming Luo, Otilia Stretcu, Hao Xiong, Chun-Ta Lu, Howard Zhou,
  Ranjay Krishna, Ariel Fuxman, Tom Duerig
Categories: cs.CV cs.LG
\\
  From content moderation to wildlife conservation, the number of applications
that require models to recognize nuanced or subjective visual concepts is
growing. Traditionally, developing classifiers for such concepts requires
substantial manual effort measured in hours, days, or even months to identify
and annotate data needed for training. Even with recently proposed Agile
Modeling techniques, which enable rapid bootstrapping of image classifiers,
users are still required to spend 30 minutes or more of monotonous, repetitive
data labeling just to train a single classifier. Drawing on Fiske's Cognitive
Miser theory, we propose a new framework that alleviates manual effort by
replacing human labeling with natural language interactions, reducing the total
effort required to define a concept by an order of magnitude: from labeling
2,000 images to only 100 plus some natural language interactions. Our framework
leverages recent advances in foundation models, both large language models and
vision-language models, to carve out the concept space through conversation and
by automatically labeling training data points. Most importantly, our framework
eliminates the need for crowd-sourced annotations. Moreover, our framework
ultimately produces lightweight classification models that are deployable in
cost-sensitive scenarios. Across 15 subjective concepts and across 2 public
image classification datasets, our trained models outperform traditional Agile
Modeling as well as state-of-the-art zero-shot classification models like
ALIGN, CLIP, CuPL, and large visual question-answering models like PaLI-X.
\\ ( https://arxiv.org/abs/2403.02626 ,  1423kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02628 (*cross-listing*)
Date: Tue, 5 Mar 2024 03:37:28 GMT   (551kb,D)

Title: Interactive Continual Learning: Fast and Slow Thinking
Authors: Biqing Qi, Xingquan Chen, Junqi Gao, Jianxing Liu, Ligang Wu and Bowen
  Zhou
Categories: cs.CV cs.LG
Comments: Accepted to CVPR 2024
\\
  Advanced life forms, sustained by the synergistic interaction of neural
cognitive mechanisms, continually acquire and transfer knowledge throughout
their lifespan. In contrast, contemporary machine learning paradigms exhibit
limitations in emulating the facets of continual learning (CL). Nonetheless,
the emergence of large language models (LLMs) presents promising avenues for
realizing CL via interactions with these models. Drawing on Complementary
Learning System theory, this paper presents a novel Interactive Continual
Learning (ICL) framework, enabled by collaborative interactions among models of
various sizes. Specifically, we assign the ViT model as System1 and multimodal
LLM as System2. To enable the memory module to deduce tasks from class
information and enhance Set2Set retrieval, we propose the Class-Knowledge-Task
Multi-Head Attention (CKT-MHA). Additionally, to improve memory retrieval in
System1 through enhanced geometric representation, we introduce the CL-vMF
mechanism, based on the von Mises-Fisher (vMF) distribution. Meanwhile, we
introduce the von Mises-Fisher Outlier Detection and Interaction (vMF-ODI)
strategy to identify hard examples, thus enhancing collaboration between
System1 and System2 for complex reasoning realization. Comprehensive evaluation
of our proposed ICL demonstrates significant resistance to forgetting and
superior performance relative to existing methods.
\\ ( https://arxiv.org/abs/2403.02628 ,  551kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02639 (*cross-listing*)
Date: Tue, 5 Mar 2024 04:07:54 GMT   (2247kb,D)

Title: False Positive Sampling-based Data Augmentation for Enhanced 3D Object
  Detection Accuracy
Authors: Jiyong Oh, Junhaeng Lee, Woongchan Byun, Minsang Kong and Sang Hun Lee
Categories: cs.CV cs.LG
\\
  Recent studies have focused on enhancing the performance of 3D object
detection models. Among various approaches, ground-truth sampling has been
proposed as an augmentation technique to address the challenges posed by
limited ground-truth data. However, an inherent issue with ground-truth
sampling is its tendency to increase false positives. Therefore, this study
aims to overcome the limitations of ground-truth sampling and improve the
performance of 3D object detection models by developing a new augmentation
technique called false-positive sampling. False-positive sampling involves
retraining the model using point clouds that are identified as false positives
in the model's predictions. We propose an algorithm that utilizes both
ground-truth and false-positive sampling and an algorithm for building the
false-positive sample database. Additionally, we analyze the principles behind
the performance enhancement due to false-positive sampling and propose a
technique that applies the concept of curriculum learning to the sampling
strategy that encompasses both false-positive and ground-truth sampling
techniques. Our experiments demonstrate that models utilizing false-positive
sampling show a reduction in false positives and exhibit improved object
detection performance. On the KITTI and Waymo Open datasets, models with
false-positive sampling surpass the baseline models by a large margin.
\\ ( https://arxiv.org/abs/2403.02639 ,  2247kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02645 (*cross-listing*)
Date: Tue, 5 Mar 2024 04:29:31 GMT   (5725kb,D)

Title: Over-The-Air Double-Threshold Deep Learner for Jamming Detection in 5G
  RF domain
Authors: Ghazal Asemian, Mohammadreza Amini, Burak Kantarci, Melike
  Erol-Kantarci
Categories: eess.SP cs.CR cs.LG cs.NI
Comments: 15 pages, 16 figures
\\
  With the evolution of 5G wireless communications, the Synchronization Signal
Block (SSB) plays a critical role in the synchronization of devices and
accessibility of services. However, due to the predictable nature of SSB
transmission, including the Primary and Secondary Synchronization Signals (PSS
and SSS), jamming attacks are critical threats. By leveraging RF domain
knowledge, this work presents a novel deep learning-based technique for
detecting jammers in 5G networks. Unlike the existing jamming detection
algorithms that mostly rely on network parameters, we introduce a double
threshold deep learning jamming detector by focusing on the SSB. The detection
method is focused on RF domain features and improves the robustness of the
network without requiring integration with the pre-existing network
infrastructure. By integrating a preprocessing block that extracts PSS
correlation and energy per null resource elements (EPNRE) characteristics, our
method distinguishes between normal and jammed received signals with high
precision. Additionally, by incorporation of Discrete Wavelet Transform (DWT),
the efficacy of training and detection are optimized. A double threshold double
Deep Neural Network (DT-DDNN) is also introduced to the architecture
complemented by a deep cascade learning model to increase the sensitivity of
the model to variations of signal to jamming noise ratio (SJNR). Results show
that the proposed method achieves 96.4% detection rate in extra low jamming
power, i.e., SJNR between 15 to 30 dB which outperforms the single threshold
DNN design with 86.0% detection rate and unprocessed IQ sample DNN design with
83.2% detection rate. Ultimately, performance of DT-DDNN is validated through
the analysis of real 5G signals obtained from a practical testbed,
demonstrating a strong alignment with the simulation results.
\\ ( https://arxiv.org/abs/2403.02645 ,  5725kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02697 (*cross-listing*)
Date: Tue, 5 Mar 2024 06:25:19 GMT   (6506kb,D)

Title: Noise misleads rotation invariant algorithms on sparse targets
Authors: Manfred K. Warmuth (1), Wojciech Kot{\l}owski (2), Matt Jones (3),
  Ehsan Amid (1) ((1) Google Inc., (2) Institute of Computing Science, Poznan
  University of Technology, Poznan, Poland, (3) University of Colorado Boulder,
  Colorado, USA)
Categories: stat.ML cs.LG
\\
  It is well known that the class of rotation invariant algorithms are
suboptimal even for learning sparse linear problems when the number of examples
is below the "dimension" of the problem. This class includes any gradient
descent trained neural net with a fully-connected input layer (initialized with
a rotationally symmetric distribution). The simplest sparse problem is learning
a single feature out of $d$ features. In that case the classification error or
regression loss grows with $1-k/n$ where $k$ is the number of examples seen.
These lower bounds become vacuous when the number of examples $k$ reaches the
dimension $d$.
  We show that when noise is added to this sparse linear problem, rotation
invariant algorithms are still suboptimal after seeing $d$ or more examples. We
prove this via a lower bound for the Bayes optimal algorithm on a rotationally
symmetrized problem. We then prove much lower upper bounds on the same problem
for simple non-rotation invariant algorithms. Finally we analyze the gradient
flow trajectories of many standard optimization algorithms in some simple cases
and show how they veer toward or away from the sparse targets.
  We believe that our trajectory categorization will be useful in designing
algorithms that can exploit sparse targets and our method for proving lower
bounds will be crucial for analyzing other families of algorithms that admit
different classes of invariances.
\\ ( https://arxiv.org/abs/2403.02697 ,  6506kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02746 (*cross-listing*)
Date: Tue, 5 Mar 2024 08:02:00 GMT   (20212kb,D)

Title: Learning without Exact Guidance: Updating Large-scale High-resolution
  Land Cover Maps from Low-resolution Historical Labels
Authors: Zhuohong Li, Wei He, Jiepan Li, Fangxiao Lu, Hongyan Zhang
Categories: cs.CV cs.LG
Comments: 11 pages, 9 figures, conference paper
\\
  Large-scale high-resolution (HR) land-cover mapping is a vital task to survey
the Earth's surface and resolve many challenges facing humanity. However, it is
still a non-trivial task hindered by complex ground details, various landforms,
and the scarcity of accurate training labels over a wide-span geographic area.
In this paper, we propose an efficient, weakly supervised framework
(Paraformer), a.k.a. Low-to-High Network (L2HNet) V2, to guide large-scale HR
land-cover mapping with easy-access historical land-cover data of low
resolution (LR). Specifically, existing land-cover mapping approaches reveal
the dominance of CNNs in preserving local ground details but still suffer from
insufficient global modeling in various landforms. Therefore, we design a
parallel CNN-Transformer feature extractor in Paraformer, consisting of a
downsampling-free CNN branch and a Transformer branch, to jointly capture local
and global contextual information. Besides, facing the spatial mismatch of
training data, a pseudo-label-assisted training (PLAT) module is adopted to
reasonably refine LR labels for weakly supervised semantic segmentation of HR
images. Experiments on two large-scale datasets demonstrate the superiority of
Paraformer over other state-of-the-art methods for automatically updating HR
land-cover maps from LR historical labels.
\\ ( https://arxiv.org/abs/2403.02746 ,  20212kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02774 (*cross-listing*)
Date: Tue, 5 Mar 2024 08:41:41 GMT   (8853kb,D)

Title: Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System
  Model Fields with Generative Foundation Models
Authors: Philipp Hess, Michael Aich, Baoxiang Pan, and Niklas Boers
Categories: physics.ao-ph cs.CV cs.LG physics.geo-ph
\\
  Accurate and high-resolution Earth system model (ESM) simulations are
essential to assess the ecological and socio-economic impacts of anthropogenic
climate change, but are computationally too expensive. Recent machine learning
approaches have shown promising results in downscaling ESM simulations,
outperforming state-of-the-art statistical approaches. However, existing
methods require computationally costly retraining for each ESM and extrapolate
poorly to climates unseen during training. We address these shortcomings by
learning a consistency model (CM) that efficiently and accurately downscales
arbitrary ESM simulations without retraining in a zero-shot manner. Our
foundation model approach yields probabilistic downscaled fields at resolution
only limited by the observational reference data. We show that the CM
outperforms state-of-the-art diffusion models at a fraction of computational
cost while maintaining high controllability on the downscaling task. Further,
our method generalizes to climate states unseen during training without
explicitly formulated physical constraints.
\\ ( https://arxiv.org/abs/2403.02774 ,  8853kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02867 (*cross-listing*)
Date: Tue, 5 Mar 2024 11:21:18 GMT   (161kb,D)

Title: Scalable Continuous-time Diffusion Framework for Network Inference and
  Influence Estimation
Authors: Keke Huang, Ruize Gao, Bogdan Cautis, and Xiaokui Xiao
Categories: cs.SI cs.LG
\\
  The study of continuous-time information diffusion has been an important area
of research for many applications in recent years. When only the diffusion
traces (cascades) are accessible, cascade-based network inference and influence
estimation are two essential problems to explore. Alas, existing methods
exhibit limited capability to infer and process networks with more than a few
thousand nodes, suffering from scalability issues. In this paper, we view the
diffusion process as a continuous-time dynamical system, based on which we
establish a continuous-time diffusion model. Subsequently, we instantiate the
model to a scalable and effective framework (FIM) to approximate the diffusion
propagation from available cascades, thereby inferring the underlying network
structure. Furthermore, we undertake an analysis of the approximation error of
FIM for network inference. To achieve the desired scalability for influence
estimation, we devise an advanced sampling technique and significantly boost
the efficiency. We also quantify the effect of the approximation error on
influence estimation theoretically. Experimental results showcase the
effectiveness and superior scalability of FIM on network inference and
influence estimation.
\\ ( https://arxiv.org/abs/2403.02867 ,  161kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02871 (*cross-listing*)
Date: Tue, 5 Mar 2024 11:29:05 GMT   (468kb,D)

Title: Quantum Mixed-State Self-Attention Network
Authors: Fu Chen, Qinglin Zhao, Li Feng, Chuangtao Chen, Yangbin Lin, Jianhong
  Lin
Categories: quant-ph cs.LG
\\
  The rapid advancement of quantum computing has increasingly highlighted its
potential in the realm of machine learning, particularly in the context of
natural language processing (NLP) tasks. Quantum machine learning (QML)
leverages the unique capabilities of quantum computing to offer novel
perspectives and methodologies for complex data processing and pattern
recognition challenges. This paper introduces a novel Quantum Mixed-State
Attention Network (QMSAN), which integrates the principles of quantum computing
with classical machine learning algorithms, especially self-attention networks,
to enhance the efficiency and effectiveness in handling NLP tasks. QMSAN model
employs a quantum attention mechanism based on mixed states, enabling efficient
direct estimation of similarity between queries and keys within the quantum
domain, leading to more effective attention weight acquisition. Additionally,
we propose an innovative quantum positional encoding scheme, implemented
through fixed quantum gates within the quantum circuit, to enhance the model's
accuracy. Experimental validation on various datasets demonstrates that QMSAN
model outperforms existing quantum and classical models in text classification,
achieving significant performance improvements. QMSAN model not only
significantly reduces the number of parameters but also exceeds classical
self-attention networks in performance, showcasing its strong capability in
data representation and information extraction. Furthermore, our study
investigates the model's robustness in different quantum noise environments,
showing that QMSAN possesses commendable robustness to low noise.
\\ ( https://arxiv.org/abs/2403.02871 ,  468kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02882 (*cross-listing*)
Date: Tue, 5 Mar 2024 11:41:43 GMT   (1282kb,D)

Title: Autonomous vehicle decision and control through reinforcement learning
  with traffic flow randomization
Authors: Yuan Lin, Antai Xie, Xiao Liu
Categories: eess.SY cs.LG cs.RO cs.SY
\\
  Most of the current studies on autonomous vehicle decision-making and control
tasks based on reinforcement learning are conducted in simulated environments.
The training and testing of these studies are carried out under rule-based
microscopic traffic flow, with little consideration of migrating them to real
or near-real environments to test their performance. It may lead to a
degradation in performance when the trained model is tested in more realistic
traffic scenes. In this study, we propose a method to randomize the driving
style and behavior of surrounding vehicles by randomizing certain parameters of
the car-following model and the lane-changing model of rule-based microscopic
traffic flow in SUMO. We trained policies with deep reinforcement learning
algorithms under the domain randomized rule-based microscopic traffic flow in
freeway and merging scenes, and then tested them separately in rule-based
microscopic traffic flow and high-fidelity microscopic traffic flow. Results
indicate that the policy trained under domain randomization traffic flow has
significantly better success rate and calculative reward compared to the models
trained under other microscopic traffic flows.
\\ ( https://arxiv.org/abs/2403.02882 ,  1282kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02886 (*cross-listing*)
Date: Tue, 5 Mar 2024 11:44:14 GMT   (2923kb,D)

Title: Revisiting Confidence Estimation: Towards Reliable Failure Prediction
Authors: Fei Zhu, Xu-Yao Zhang, Zhen Cheng, Cheng-Lin Liu
Categories: cs.CV cs.LG
Comments: Accepted by IEEE TPAMI. arXiv admin note: text overlap with
  arXiv:2303.02970; text overlap with arXiv:2007.01458 by other authors
\\
  Reliable confidence estimation is a challenging yet fundamental requirement
in many risk-sensitive applications. However, modern deep neural networks are
often overconfident for their incorrect predictions, i.e., misclassified
samples from known classes, and out-of-distribution (OOD) samples from unknown
classes. In recent years, many confidence calibration and OOD detection methods
have been developed. In this paper, we find a general, widely existing but
actually-neglected phenomenon that most confidence estimation methods are
harmful for detecting misclassification errors. We investigate this problem and
reveal that popular calibration and OOD detection methods often lead to worse
confidence separation between correctly classified and misclassified examples,
making it difficult to decide whether to trust a prediction or not. Finally, we
propose to enlarge the confidence gap by finding flat minima, which yields
state-of-the-art failure prediction performance under various settings
including balanced, long-tailed, and covariate-shift classification scenarios.
Our study not only provides a strong baseline for reliable confidence
estimation but also acts as a bridge between understanding calibration, OOD
detection, and failure prediction. The code is available at
\url{https://github.com/Impression2805/FMFP}.
\\ ( https://arxiv.org/abs/2403.02886 ,  2923kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02906 (*cross-listing*)
Date: Tue, 5 Mar 2024 12:13:27 GMT   (6621kb,D)

Title: Citizen Science and Machine Learning for Research and Nature
  Conservation: The Case of Eurasian Lynx, Free-ranging Rodents and Insects
Authors: Kinga Skorupska, Rafa{\l} Stryjek, Izabela Wierzbowska, Piotr Bebas,
  Maciej Grzeszczuk, Piotr Gago, Jaros{\l}aw Kowalski, Maciej Krzywicki, Jagoda
  Lazarek, Wies{\l}aw Kope\'c
Categories: cs.HC cs.CV cs.CY cs.LG
Comments: 10 pages, 11 figures, MIDI 2023 conference
\\
  Technology is increasingly used in Nature Reserves and National Parks around
the world to support conservation efforts. Endangered species, such as the
Eurasian Lynx (Lynx lynx), are monitored by a network of automatic photo traps.
Yet, this method produces vast amounts of data, which needs to be prepared,
analyzed and interpreted. Therefore, researchers working in this area
increasingly need support to process this incoming information. One opportunity
is to seek support from volunteer Citizen Scientists who can help label the
data, however, it is challenging to retain their interest. Another way is to
automate the process with image recognition using convolutional neural
networks. During the panel, we will discuss considerations related to nature
research and conservation as well as opportunities for the use of Citizen
Science and Machine Learning to expedite the process of data preparation,
labelling and analysis.
\\ ( https://arxiv.org/abs/2403.02906 ,  6621kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02912 (*cross-listing*)
Date: Tue, 5 Mar 2024 12:28:00 GMT   (97kb)

Title: Mirror Descent Algorithms with Nearly Dimension-Independent Rates for
  Differentially-Private Stochastic Saddle-Point Problems
Authors: Tom\'as Gonz\'alez and Crist\'obal Guzm\'an and Courtney Paquette
Categories: math.OC cs.CR cs.LG
\\
  We study the problem of differentially-private (DP) stochastic
(convex-concave) saddle-points in the polyhedral setting. We propose
$(\varepsilon, \delta)$-DP algorithms based on stochastic mirror descent that
attain nearly dimension-independent convergence rates for the expected duality
gap, a type of guarantee that was known before only for bilinear objectives.
For convex-concave and first-order-smooth stochastic objectives, our algorithms
attain a rate of $\sqrt{\log(d)/n} + (\log(d)^{3/2}/[n\varepsilon])^{1/3}$,
where $d$ is the dimension of the problem and $n$ the dataset size. Under an
additional second-order-smoothness assumption, we improve the rate on the
expected gap to $\sqrt{\log(d)/n} + (\log(d)^{3/2}/[n\varepsilon])^{2/5}$.
Under this additional assumption, we also show, by using bias-reduced gradient
estimators, that the duality gap is bounded by $\log(d)/\sqrt{n} +
\log(d)/[n\varepsilon]^{1/2}$ with constant success probability. This result
provides evidence of the near-optimality of the approach. Finally, we show that
combining our methods with acceleration techniques from online learning leads
to the first algorithm for DP Stochastic Convex Optimization in the polyhedral
setting that is not based on Frank-Wolfe methods. For convex and
first-order-smooth stochastic objectives, our algorithms attain an excess risk
of $\sqrt{\log(d)/n} + \log(d)^{7/10}/[n\varepsilon]^{2/5}$, and when
additionally assuming second-order-smoothness, we improve the rate to
$\sqrt{\log(d)/n} + \log(d)/\sqrt{n\varepsilon}$. Instrumental to all of these
results are various extensions of the classical Maurey Sparsification Lemma,
which may be of independent interest.
\\ ( https://arxiv.org/abs/2403.02912 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02944 (*cross-listing*)
Date: Tue, 5 Mar 2024 13:15:01 GMT   (4376kb,D)

Title: Neural Image Compression with Text-guided Encoding for both Pixel-level
  and Perceptual Fidelity
Authors: Hagyeong Lee, Minkyu Kim, Jun-Hyuk Kim, Seungeon Kim, Dokwan Oh, Jaeho
  Lee
Categories: cs.CV cs.LG
Comments: The first two authors contributed equally
\\
  Recent advances in text-guided image compression have shown great potential
to enhance the perceptual quality of reconstructed images. These methods,
however, tend to have significantly degraded pixel-wise fidelity, limiting
their practicality. To fill this gap, we develop a new text-guided image
compression algorithm that achieves both high perceptual and pixel-wise
fidelity. In particular, we propose a compression framework that leverages text
information mainly by text-adaptive encoding and training with joint image-text
loss. By doing so, we avoid decoding based on text-guided generative models --
known for high generative diversity -- and effectively utilize the semantic
information of text at a global level. Experimental results on various datasets
show that our method can achieve high pixel-level and perceptual quality, with
either human- or machine-generated captions. In particular, our method
outperforms all baselines in terms of LPIPS, with some room for even more
improvements when we use more carefully generated captions.
\\ ( https://arxiv.org/abs/2403.02944 ,  4376kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02967 (*cross-listing*)
Date: Tue, 5 Mar 2024 13:43:58 GMT   (682kb,D)

Title: Non-Convex Stochastic Composite Optimization with Polyak Momentum
Authors: Yuan Gao and Anton Rodomanov and Sebastian U. Stich
Categories: math.OC cs.LG
\\
  The stochastic proximal gradient method is a powerful generalization of the
widely used stochastic gradient descent (SGD) method and has found numerous
applications in Machine Learning. However, it is notoriously known that this
method fails to converge in non-convex settings where the stochastic noise is
significant (i.e. when only small or bounded batch sizes are used). In this
paper, we focus on the stochastic proximal gradient method with Polyak
momentum. We prove this method attains an optimal convergence rate for
non-convex composite optimization problems, regardless of batch size.
Additionally, we rigorously analyze the variance reduction effect of the Polyak
momentum in the composite optimization setting and we show the method also
converges when the proximal step can only be solved inexactly. Finally, we
provide numerical experiments to validate our theoretical results.
\\ ( https://arxiv.org/abs/2403.02967 ,  682kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02968 (*cross-listing*)
Date: Tue, 5 Mar 2024 13:44:28 GMT   (276kb,D)

Title: Hamiltonian Property Testing
Authors: Andreas Bluhm, Matthias C. Caro, Aadil Oufkir
Categories: quant-ph cs.CC cs.DS cs.IT cs.LG math.IT
Comments: 39+18 pages, 3 figures
\\
  Locality is a fundamental feature of many physical time evolutions.
Assumptions on locality and related structural properties also underlie
recently proposed procedures for learning an unknown Hamiltonian from access to
the induced time evolution. However, no protocols to rigorously test whether an
unknown Hamiltonian is local were known. We investigate Hamiltonian locality
testing as a property testing problem, where the task is to determine whether
an unknown $n$-qubit Hamiltonian $H$ is $k$-local or $\varepsilon$-far from all
$k$-local Hamiltonians, given access to the time evolution along $H$. First, we
emphasize the importance of the chosen distance measure: With respect to the
operator norm, a worst-case distance measure, incoherent quantum locality
testers require $\tilde{\Omega}(2^n)$ many time evolution queries and an
expected total evolution time of $\tilde{\Omega}(2^n / \varepsilon)$, and even
coherent testers need $\Omega(2^{n/2})$ many queries and
$\Omega(2^{n/2}/\varepsilon)$ total evolution time. In contrast, when distances
are measured according to the normalized Frobenius norm, corresponding to an
average-case distance, we give a sample-, time-, and computationally efficient
incoherent Hamiltonian locality testing algorithm based on randomized
measurements. In fact, our procedure can be used to simultaneously test a wide
class of Hamiltonian properties beyond locality. Finally, we prove that
learning a general Hamiltonian remains exponentially hard with this
average-case distance, thereby establishing an exponential separation between
Hamiltonian testing and learning. Our work initiates the study of property
testing for quantum Hamiltonians, demonstrating that a broad class of
Hamiltonian properties is efficiently testable even with limited quantum
capabilities, and positioning Hamiltonian testing as an independent area of
research alongside Hamiltonian learning.
\\ ( https://arxiv.org/abs/2403.02968 ,  276kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02974 (*cross-listing*)
Date: Tue, 5 Mar 2024 13:53:48 GMT   (2822kb,D)

Title: Online Learning of Human Constraints from Feedback in Shared Autonomy
Authors: Shibei Zhu, Tran Nguyen Le, Samuel Kaski, Ville Kyrki
Categories: cs.RO cs.HC cs.LG
Comments: Accepted to AAAI-24 Bridge Program on Collaborative AI and Modeling
  of Humans & AAAI-24 Workshop on Ad Hoc Teamwork
\\
  Real-time collaboration with humans poses challenges due to the different
behavior patterns of humans resulting from diverse physical constraints.
Existing works typically focus on learning safety constraints for
collaboration, or how to divide and distribute the subtasks between the
participating agents to carry out the main task. In contrast, we propose to
learn a human constraints model that, in addition, considers the diverse
behaviors of different human operators. We consider a type of collaboration in
a shared-autonomy fashion, where both a human operator and an assistive robot
act simultaneously in the same task space that affects each other's actions.
The task of the assistive agent is to augment the skill of humans to perform a
shared task by supporting humans as much as possible, both in terms of reducing
the workload and minimizing the discomfort for the human operator. Therefore,
we propose an augmentative assistant agent capable of learning and adapting to
human physical constraints, aligning its actions with the ergonomic preferences
and limitations of the human operator.
\\ ( https://arxiv.org/abs/2403.02974 ,  2822kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03037 (*cross-listing*)
Date: Tue, 5 Mar 2024 15:18:02 GMT   (2716kb,D)

Title: A Backpack Full of Skills: Egocentric Video Understanding with Diverse
  Task Perspectives
Authors: Simone Alberto Peirone, Francesca Pistilli, Antonio Alliegro, Giuseppe
  Averta
Categories: cs.CV cs.LG
Comments: Accepted at IEEE/CVF Conference on Computer Vision and Pattern
  Recognition 2024. Project webpage at https://sapeirone.github.io/EgoPack
\\
  Human comprehension of a video stream is naturally broad: in a few instants,
we are able to understand what is happening, the relevance and relationship of
objects, and forecast what will follow in the near future, everything all at
once. We believe that - to effectively transfer such an holistic perception to
intelligent machines - an important role is played by learning to correlate
concepts and to abstract knowledge coming from different tasks, to
synergistically exploit them when learning novel skills. To accomplish this, we
seek for a unified approach to video understanding which combines shared
temporal modelling of human actions with minimal overhead, to support multiple
downstream tasks and enable cooperation when learning novel skills. We then
propose EgoPack, a solution that creates a collection of task perspectives that
can be carried across downstream tasks and used as a potential source of
additional insights, as a backpack of skills that a robot can carry around and
use when needed. We demonstrate the effectiveness and efficiency of our
approach on four Ego4D benchmarks, outperforming current state-of-the-art
methods.
\\ ( https://arxiv.org/abs/2403.03037 ,  2716kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03055 (*cross-listing*)
Date: Tue, 5 Mar 2024 15:38:54 GMT   (5108kb,D)

Title: Distributed Policy Gradient for Linear Quadratic Networked Control with
  Limited Communication Range
Authors: Yuzi Yan and Yuan Shen
Categories: cs.MA cs.LG cs.RO cs.SY eess.SY
Comments: 14 pages, 6 figures
\\
  This paper proposes a scalable distributed policy gradient method and proves
its convergence to near-optimal solution in multi-agent linear quadratic
networked systems. The agents engage within a specified network under local
communication constraints, implying that each agent can only exchange
information with a limited number of neighboring agents. On the underlying
graph of the network, each agent implements its control input depending on its
nearby neighbors' states in the linear quadratic control setting. We show that
it is possible to approximate the exact gradient only using local information.
Compared with the centralized optimal controller, the performance gap decreases
to zero exponentially as the communication and control ranges increase. We also
demonstrate how increasing the communication range enhances system stability in
the gradient descent process, thereby elucidating a critical trade-off. The
simulation results verify our theoretical findings.
\\ ( https://arxiv.org/abs/2403.03055 ,  5108kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03071 (*cross-listing*)
Date: Tue, 5 Mar 2024 15:59:54 GMT   (15143kb,D)

Title: On a Neural Implementation of Brenier's Polar Factorization
Authors: Nina Vesseron, Marco Cuturi
Categories: stat.ML cs.LG
\\
  In 1991, Brenier proved a theorem that generalizes the $QR$ decomposition for
square matrices -- factored as PSD $\times$ unitary -- to any vector field
$F:\mathbb{R}^d\rightarrow \mathbb{R}^d$. The theorem, known as the polar
factorization theorem, states that any field $F$ can be recovered as the
composition of the gradient of a convex function $u$ with a measure-preserving
map $M$, namely $F=\nabla u \circ M$. We propose a practical implementation of
this far-reaching theoretical result, and explore possible uses within machine
learning. The theorem is closely related to optimal transport (OT) theory, and
we borrow from recent advances in the field of neural optimal transport to
parameterize the potential $u$ as an input convex neural network. The map $M$
can be either evaluated pointwise using $u^*$, the convex conjugate of $u$,
through the identity $M=\nabla u^* \circ F$, or learned as an auxiliary
network. Because $M$ is, in general, not injective, we consider the additional
task of estimating the ill-posed inverse map that can approximate the pre-image
measure $M^{-1}$ using a stochastic generator. We illustrate possible
applications of \citeauthor{Brenier1991PolarFA}'s polar factorization to
non-convex optimization problems, as well as sampling of densities that are not
log-concave.
\\ ( https://arxiv.org/abs/2403.03071 ,  15143kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03145 (*cross-listing*)
Date: Tue, 5 Mar 2024 17:35:46 GMT   (2153kb,D)

Title: Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for
  Audio-Visual Source Localization
Authors: Yuxin Guo, Shijie Ma, Hu Su, Zhiqing Wang, Yuhao Zhao, Wei Zou, Siyang
  Sun, Yun Zheng
Categories: cs.CV cs.LG cs.MM cs.SD eess.AS
Comments: Accepted to NeurIPS2023
\\
  Audio-Visual Source Localization (AVSL) aims to locate sounding objects
within video frames given the paired audio clips. Existing methods
predominantly rely on self-supervised contrastive learning of audio-visual
correspondence. Without any bounding-box annotations, they struggle to achieve
precise localization, especially for small objects, and suffer from blurry
boundaries and false positives. Moreover, the naive semi-supervised method is
poor in fully leveraging the information of abundant unlabeled data. In this
paper, we propose a novel semi-supervised learning framework for AVSL, namely
Dual Mean-Teacher (DMT), comprising two teacher-student structures to
circumvent the confirmation bias issue. Specifically, two teachers, pre-trained
on limited labeled data, are employed to filter out noisy samples via the
consensus between their predictions, and then generate high-quality
pseudo-labels by intersecting their confidence maps. The sufficient utilization
of both labeled and unlabeled data and the proposed unbiased framework enable
DMT to outperform current state-of-the-art methods by a large margin, with CIoU
of 90.4% and 48.8% on Flickr-SoundNet and VGG-Sound Source, obtaining 8.9%,
9.6% and 4.6%, 6.4% improvements over self- and semi-supervised methods
respectively, given only 3% positional-annotations. We also extend our
framework to some existing AVSL methods and consistently boost their
performance.
\\ ( https://arxiv.org/abs/2403.03145 ,  2153kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03149 (*cross-listing*)
Date: Tue, 5 Mar 2024 17:41:35 GMT   (123kb,D)

Title: Robust Federated Learning Mitigates Client-side Training Data
  Distribution Inference Attacks
Authors: Yichang Xu, Ming Yin, Minghong Fang, Neil Zhenqiang Gong
Categories: cs.CR cs.DC cs.LG
Comments: To appear in The Web Conference 2024 (WWW '24)
\\
  Recent studies have revealed that federated learning (FL), once considered
secure due to clients not sharing their private data with the server, is
vulnerable to attacks such as client-side training data distribution inference,
where a malicious client can recreate the victim's data. While various
countermeasures exist, they are not practical, often assuming server access to
some training data or knowledge of label distribution before the attack.
  In this work, we bridge the gap by proposing InferGuard, a novel
Byzantine-robust aggregation rule aimed at defending against client-side
training data distribution inference attacks. In our proposed InferGuard, the
server first calculates the coordinate-wise median of all the model updates it
receives. A client's model update is considered malicious if it significantly
deviates from the computed median update. We conduct a thorough evaluation of
our proposed InferGuard on five benchmark datasets and perform a comparison
with ten baseline methods. The results of our experiments indicate that our
defense mechanism is highly effective in protecting against client-side
training data distribution inference attacks, even against strong adaptive
attacks. Furthermore, our method substantially outperforms the baseline methods
in various practical FL scenarios.
\\ ( https://arxiv.org/abs/2403.03149 ,  123kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03157 (*cross-listing*)
Date: Tue, 5 Mar 2024 17:49:09 GMT   (551kb,D)

Title: Rethinking Clustered Federated Learning in NOMA Enhanced Wireless
  Networks
Authors: Yushen Lin, Kaidi Wang and Zhiguo Ding
Categories: cs.NI cs.IT cs.LG math.IT
\\
  This study explores the benefits of integrating the novel clustered federated
learning (CFL) approach with non-orthogonal multiple access (NOMA) under
non-independent and identically distributed (non-IID) datasets, where multiple
devices participate in the aggregation with time limitations and a finite
number of sub-channels. A detailed theoretical analysis of the generalization
gap that measures the degree of non-IID in the data distribution is presented.
Following that, solutions to address the challenges posed by non-IID conditions
are proposed with the analysis of the properties. Specifically, users' data
distributions are parameterized as concentration parameters and grouped using
spectral clustering, with Dirichlet distribution serving as the prior. The
investigation into the generalization gap and convergence rate guides the
design of sub-channel assignments through the matching-based algorithm, and the
power allocation is achieved by Karush-Kuhn-Tucker (KKT) conditions with the
derived closed-form solution. The extensive simulation results show that the
proposed cluster-based FL framework can outperform FL baselines in terms of
both test accuracy and convergence rate. Moreover, jointly optimizing
sub-channel and power allocation in NOMA-enhanced networks can lead to a
significant improvement.
\\ ( https://arxiv.org/abs/2403.03157 ,  551kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03161 (*cross-listing*)
Date: Tue, 5 Mar 2024 17:54:22 GMT   (25661kb,D)

Title: PalmProbNet: A Probabilistic Approach to Understanding Palm
  Distributions in Ecuadorian Tropical Forest via Transfer Learning
Authors: Kangning Cui, Zishan Shao, Gregory Larsen, Victor Pauca, Sarra
  Alqahtani, David Segurado, Jo\~ao Pinheiro, Manqi Wang, David Lutz, Robert
  Plemmons, Miles Silman
Categories: cs.CV cs.LG
Comments: 11 pages, 6 figures, and 1 table, to appear in ACMSE 2024
ACM-class: I.4.9
\\
  Palms play an outsized role in tropical forests and are important resources
for humans and wildlife. A central question in tropical ecosystems is
understanding palm distribution and abundance. However, accurately identifying
and localizing palms in geospatial imagery presents significant challenges due
to dense vegetation, overlapping canopies, and variable lighting conditions in
mixed-forest landscapes. Addressing this, we introduce PalmProbNet, a
probabilistic approach utilizing transfer learning to analyze high-resolution
UAV-derived orthomosaic imagery, enabling the detection of palm trees within
the dense canopy of the Ecuadorian Rainforest. This approach represents a
substantial advancement in automated palm detection, effectively pinpointing
palm presence and locality in mixed tropical rainforests. Our process begins by
generating an orthomosaic image from UAV images, from which we extract and
label palm and non-palm image patches in two distinct sizes. These patches are
then used to train models with an identical architecture, consisting of an
unaltered pre-trained ResNet-18 and a Multilayer Perceptron (MLP) with
specifically trained parameters. Subsequently, PalmProbNet employs a sliding
window technique on the landscape orthomosaic, using both small and large
window sizes to generate a probability heatmap. This heatmap effectively
visualizes the distribution of palms, showcasing the scalability and
adaptability of our approach in various forest densities. Despite the
challenging terrain, our method demonstrated remarkable performance, achieving
an accuracy of 97.32% and a Cohen's kappa of 94.59% in testing.
\\ ( https://arxiv.org/abs/2403.03161 ,  25661kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03180 (*cross-listing*)
Date: Tue, 5 Mar 2024 18:19:02 GMT   (395kb,D)

Title: Shuffling Momentum Gradient Algorithm for Convex Optimization
Authors: Trang H. Tran, Quoc Tran-Dinh, Lam M. Nguyen
Categories: math.OC cs.LG
Comments: Vietnam Journal of Mathematics (VJOM), Special issue dedicated to Dr.
  Tam\'as Terlaky on the occasion of his 70th birthday, 2024
\\
  The Stochastic Gradient Descent method (SGD) and its stochastic variants have
become methods of choice for solving finite-sum optimization problems arising
from machine learning and data science thanks to their ability to handle
large-scale applications and big datasets. In the last decades, researchers
have made substantial effort to study the theoretical performance of SGD and
its shuffling variants. However, only limited work has investigated its
shuffling momentum variants, including shuffling heavy-ball momentum schemes
for non-convex problems and Nesterov's momentum for convex settings. In this
work, we extend the analysis of the shuffling momentum gradient method
developed in [Tran et al (2021)] to both finite-sum convex and strongly convex
optimization problems. We provide the first analysis of shuffling
momentum-based methods for the strongly convex setting, attaining a convergence
rate of $O(1/nT^2)$, where $n$ is the number of samples and $T$ is the number
of training epochs. Our analysis is a state-of-the-art, matching the best rates
of existing shuffling stochastic gradient algorithms in the literature.
\\ ( https://arxiv.org/abs/2403.03180 ,  395kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03208 (*cross-listing*)
Date: Tue, 5 Mar 2024 18:46:50 GMT   (96kb,D)

Title: Active Statistical Inference
Authors: Tijana Zrnic, Emmanuel J. Cand\`es
Categories: stat.ML cs.LG stat.ME
\\
  Inspired by the concept of active learning, we propose active
inference$\unicode{x2013}$a methodology for statistical inference with
machine-learning-assisted data collection. Assuming a budget on the number of
labels that can be collected, the methodology uses a machine learning model to
identify which data points would be most beneficial to label, thus effectively
utilizing the budget. It operates on a simple yet powerful intuition:
prioritize the collection of labels for data points where the model exhibits
uncertainty, and rely on the model's predictions where it is confident. Active
inference constructs provably valid confidence intervals and hypothesis tests
while leveraging any black-box machine learning model and handling any data
distribution. The key point is that it achieves the same level of accuracy with
far fewer samples than existing baselines relying on non-adaptively-collected
data. This means that for the same number of collected samples, active
inference enables smaller confidence intervals and more powerful p-values. We
evaluate active inference on datasets from public opinion research, census
analysis, and proteomics.
\\ ( https://arxiv.org/abs/2403.03208 ,  96kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2212.12760
replaced with revised version Tue, 5 Mar 2024 14:26:52 GMT   (1663kb)

Title: Agent-based Modeling and Simulation of Human Muscle For Development of
  Human Gait Analyzer Application
Authors: Sina Saadati, Mohammadreza Razzazi
Categories: cs.AI cs.MA
Comments: 22 pages, 15 figures, 2 tables
\\ ( https://arxiv.org/abs/2212.12760 ,  1663kb)
------------------------------------------------------------------------------
\\
arXiv:2304.12653
replaced with revised version Tue, 5 Mar 2024 06:25:06 GMT   (488kb,D)

Title: Partially Observable Mean Field Multi-Agent Reinforcement Learning Based
  on Graph-Attention
Authors: Min Yang, Guanjun Liu, Ziyuan Zhou
Categories: cs.AI
Journal-ref: Drones 2023, 7(7), 476
DOI: 10.3390/drones7070476
\\ ( https://arxiv.org/abs/2304.12653 ,  488kb)
------------------------------------------------------------------------------
\\
arXiv:2304.13269
replaced with revised version Tue, 5 Mar 2024 05:40:48 GMT   (22592kb)

Title: Game-based Platforms for Artificial Intelligence Research
Authors: Chengpeng Hu, Yunlong Zhao, Ziqi Wang, Haocheng Du, Jialin Liu
Categories: cs.AI
\\ ( https://arxiv.org/abs/2304.13269 ,  22592kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01264
replaced with revised version Mon, 4 Mar 2024 19:31:04 GMT   (1451kb)

Title: Exploring the psychology of LLMs' Moral and Legal Reasoning
Authors: Guilherme F. C. F. Almeida, Jos\'e Luiz Nunes, Neele Engelmann, Alex
  Wiegmann, Marcelo de Ara\'ujo
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2308.01264 ,  1451kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00194
replaced with revised version Tue, 5 Mar 2024 18:12:06 GMT   (505kb,D)

Title: A Prefrontal Cortex-inspired Architecture for Planning in Large Language
  Models
Authors: Taylor Webb, Shanka Subhra Mondal, Chi Wang, Brian Krabach, Ida
  Momennejad
Categories: cs.AI cs.NE
\\ ( https://arxiv.org/abs/2310.00194 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13720
replaced with revised version Tue, 5 Mar 2024 00:19:24 GMT   (1931kb,D)

Title: Can LLMs Fix Issues with Reasoning Models? Towards More Likely Models
  for AI Planning
Authors: Turgay Caglar, Sirine Belhaj, Tathagata Chakraborti, Michael Katz,
  Sarath Sreedharan
Categories: cs.AI
Comments: 24 pages
\\ ( https://arxiv.org/abs/2311.13720 ,  1931kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14058
replaced with revised version Tue, 5 Mar 2024 14:55:23 GMT   (22kb)

Title: Identification for Tree-shaped Structural Causal Models in Polynomial
  Time
Authors: Aaryan Gupta and Markus Bl\"aser
Categories: cs.AI cs.DS
\\ ( https://arxiv.org/abs/2311.14058 ,  22kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16044
replaced with revised version Tue, 5 Mar 2024 13:21:38 GMT   (10338kb,D)

Title: LLMLight: Large Language Models as Traffic Signal Control Agents
Authors: Siqi Lai, Zhao Xu, Weijia Zhang, Hao Liu and Hui Xiong
Categories: cs.AI
\\ ( https://arxiv.org/abs/2312.16044 ,  10338kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09099
replaced with revised version Tue, 5 Mar 2024 10:44:36 GMT   (25126kb,D)

Title: Exploring Neuron Interactions and Emergence in LLMs: From the
  Multifractal Analysis Perspective
Authors: Xiongye Xiao, Chenyu Zhou, Heng Ping, Defu Cao, Yaxing Li, Yizhuo
  Zhou, Shixuan Li, Paul Bogdan
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.09099 ,  25126kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00685
replaced with revised version Tue, 5 Mar 2024 16:35:43 GMT   (48kb)

Title: Know your exceptions: Towards an Ontology of Exceptions in Knowledge
  Representation
Authors: Gabriele Sacco, Loris Bozzato, Oliver Kutz
Categories: cs.AI
Comments: 18 pages, 4 pages are appendix. (v2 updates: minor revisions on
  discussions, terminology and text editing)
ACM-class: I.2.4
\\ ( https://arxiv.org/abs/2403.00685 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02164
replaced with revised version Tue, 5 Mar 2024 10:23:52 GMT   (2924kb)

Title: Cognition is All You Need -- The Next Layer of AI Above Large Language
  Models
Authors: Nova Spivack, Sam Douglas, Michelle Crames, Tim Connors
Categories: cs.AI cs.MA
Comments: 63 pages, 18 figures
ACM-class: I.2.0
\\ ( https://arxiv.org/abs/2403.02164 ,  2924kb)
------------------------------------------------------------------------------
\\
arXiv:1910.14080
replaced with revised version Tue, 5 Mar 2024 09:01:13 GMT   (35kb)

Title: Contextual Text Denoising with Masked Language Models
Authors: Yifu Sun, Haoming Jiang
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/1910.14080 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2303.16618
replaced with revised version Tue, 5 Mar 2024 08:51:30 GMT   (9401kb,D)

Title: Reference-less Analysis of Context Specificity in Translation with
  Personalised Language Models
Authors: Sebastian Vincent, Alice Dowek, Rowanne Sumner, Charlotte Blundell,
  Emily Preston, Chris Bayliss, Chris Oakley, Carolina Scarton
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to LREC-COLING 2024
\\ ( https://arxiv.org/abs/2303.16618 ,  9401kb)
------------------------------------------------------------------------------
\\
arXiv:2305.06176
replaced with revised version Tue, 5 Mar 2024 05:18:54 GMT   (2167kb)

Title: Fine-tuning Language Models with Generative Adversarial Reward Modelling
Authors: Zhang Ze Yu, Lau Jia Jaw, Zhang Hui, Bryan Kian Hsiang Low
Categories: cs.CL cs.AI cs.LG
Comments: 22 pages, 9 figures, 12 tables
\\ ( https://arxiv.org/abs/2305.06176 ,  2167kb)
------------------------------------------------------------------------------
\\
arXiv:2305.08391
replaced with revised version Tue, 5 Mar 2024 08:52:20 GMT   (1200kb,D)

Title: Uncovering the Potential of ChatGPT for Discourse Analysis in Dialogue:
  An Empirical Study
Authors: Yaxin Fan and Feng Jiang and Peifeng Li and Haizhou Li
Categories: cs.CL
Comments: Accepted by LREC-COLING'2024
\\ ( https://arxiv.org/abs/2305.08391 ,  1200kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14824
replaced with revised version Tue, 5 Mar 2024 16:32:58 GMT   (233kb,D)

Title: Mitigating Temporal Misalignment by Discarding Outdated Facts
Authors: Michael J.Q. Zhang and Eunsol Choi
Categories: cs.CL
Comments: Accepted into EMNLP 2023
\\ ( https://arxiv.org/abs/2305.14824 ,  233kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18149
replaced with revised version Tue, 5 Mar 2024 08:27:12 GMT   (90kb,D)

Title: Multiscale Positive-Unlabeled Detection of AI-Generated Texts
Authors: Yuchuan Tian, Hanting Chen, Xutao Wang, Zheyuan Bai, Qinghua Zhang,
  Ruifeng Li, Chao Xu, Yunhe Wang
Categories: cs.CL cs.AI
Comments: ICLR2024 (Spotlight)
\\ ( https://arxiv.org/abs/2305.18149 ,  90kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04735
replaced with revised version Tue, 5 Mar 2024 17:29:06 GMT   (5452kb,D)

Title: Soft-prompt Tuning for Large Language Models to Evaluate Bias
Authors: Jacob-Junqi Tian, David Emerson, Sevil Zanjani Miyandoab, Deval
  Pandya, Laleh Seyyed-Kalantari, Faiza Khan Khattak
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2306.04735 ,  5452kb)
------------------------------------------------------------------------------
\\
arXiv:2308.06354
replaced with revised version Tue, 5 Mar 2024 12:55:47 GMT   (2049kb)

Title: Large Language Models to Identify Social Determinants of Health in
  Electronic Health Records
Authors: Marco Guevara, Shan Chen, Spencer Thomas, Tafadzwa L. Chaunzwa, Idalid
  Franco, Benjamin Kann, Shalini Moningi, Jack Qian, Madeleine Goldstein, Susan
  Harper, Hugo JWL Aerts, Guergana K. Savova, Raymond H. Mak, Danielle S.
  Bitterman
Categories: cs.CL cs.AI
Comments: Peer-reviewed version published at NPJ Digital Medicine:
  https://www.nature.com/articles/s41746-023-00970-0
Journal-ref: NPJ Digit Med. 2024 Jan 11;7(1):6
DOI: 10.1038/s41746-023-00970-0.
\\ ( https://arxiv.org/abs/2308.06354 ,  2049kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04766
replaced with revised version Tue, 5 Mar 2024 03:45:51 GMT   (2753kb,D)

Title: SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment
  to Cultural Reasoning
Authors: Bin Wang and Zhengyuan Liu and Xin Huang and Fangkai Jiao and Yang
  Ding and Ai Ti Aw and Nancy F. Chen
Categories: cs.CL cs.AI
Comments: 20 pages. More datasets (2 on Cross-Lingual Consistency and 4 on
  Cultural Understanding) and more supported languages. Code:
  https://seaeval.github.io/
\\ ( https://arxiv.org/abs/2309.04766 ,  2753kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05203
replaced with revised version Tue, 5 Mar 2024 10:51:23 GMT   (2707kb,D)

Title: From Artificially Real to Real: Leveraging Pseudo Data from Large
  Language Models for Low-Resource Molecule Discovery
Authors: Yuhan Chen, Nuwa Xi, Yanrui Du, Haochun Wang, Jianyu Chen, Sendong
  Zhao, Bing Qin
Categories: cs.CL
Comments: AAAI2024
\\ ( https://arxiv.org/abs/2309.05203 ,  2707kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00648
replaced with revised version Tue, 5 Mar 2024 17:15:35 GMT   (964kb,D)

Title: PETA: Parameter-Efficient Trojan Attacks
Authors: Lauren Hong, Ting Wang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.00648 ,  964kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14540
replaced with revised version Tue, 5 Mar 2024 05:02:54 GMT   (1489kb,D)

Title: Evaluating Spatial Understanding of Large Language Models
Authors: Yutaro Yamada, Yihan Bao, Andrew K. Lampinen, Jungo Kasai, Ilker
  Yildirim
Categories: cs.CL cs.AI
Comments: Accepted to TMLR 2024. Our code and data are available at
  https://github.com/runopti/SpatialEvalLLM,
  https://huggingface.co/datasets/yyamada/SpatialEvalLLM
\\ ( https://arxiv.org/abs/2310.14540 ,  1489kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18794
replaced with revised version Mon, 4 Mar 2024 22:49:26 GMT   (118kb,D)

Title: Sequence-Level Certainty Reduces Hallucination In Knowledge-Grounded
  Dialogue Generation
Authors: Yixin Wan, Fanyou Wu, Weijie Xu, Srinivasan H. Sengamedu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2310.18794 ,  118kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05235
replaced with revised version Tue, 5 Mar 2024 05:49:24 GMT   (1934kb,D)

Title: Generative AI in Higher Education: Seeing ChatGPT Through Universities'
  Policies, Resources, and Guidelines
Authors: Hui Wang, Anh Dang, Zihao Wu, Son Mac
Categories: cs.CL cs.CY
Comments: 27 pages, 11 figures
\\ ( https://arxiv.org/abs/2312.05235 ,  1934kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09932
replaced with revised version Tue, 5 Mar 2024 17:29:48 GMT   (3477kb,D)

Title: RDR: the Recap, Deliberate, and Respond Method for Enhanced Language
  Understanding
Authors: Yuxin Zi, Hariram Veeramani, Kaushik Roy and Amit Sheth
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2312.09932 ,  3477kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09979
replaced with revised version Tue, 5 Mar 2024 13:26:56 GMT   (1899kb,D)

Title: LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models
  via MoE-Style Plugin
Authors: Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen,
  Yuhao Zhou, Zhiheng Xi, Xiao Wang, Xiaoran Fan, Shiliang Pu, Jiang Zhu, Rui
  Zheng, Tao Gui, Qi Zhang, Xuanjing Huang
Categories: cs.CL
Comments: 14 pages, 7 figures
\\ ( https://arxiv.org/abs/2312.09979 ,  1899kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06509
replaced with revised version Tue, 5 Mar 2024 12:07:04 GMT   (10742kb,D)

Title: AntEval: Evaluation of Social Interaction Competencies in LLM-Driven
  Agents
Authors: Yuanzhi Liang, Linchao Zhu, Yi Yang
Categories: cs.CL
Comments: Preliminary version of an ongoing work
\\ ( https://arxiv.org/abs/2401.06509 ,  10742kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10647
replaced with revised version Mon, 4 Mar 2024 21:10:43 GMT   (1052kb,D)

Title: Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language
  Models
Authors: Rima Hazra, Sayan Layek, Somnath Banerjee, Soujanya Poria
Categories: cs.CL
Comments: Under review.
  {https://huggingface.co/datasets/SoftMINER-Group/NicheHazardQA}
\\ ( https://arxiv.org/abs/2401.10647 ,  1052kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12671
replaced with revised version Tue, 5 Mar 2024 07:18:53 GMT   (1571kb,D)

Title: Context Matters: Pushing the Boundaries of Open-Ended Answer Generation
  with Graph-Structured Knowledge Context
Authors: Somnath Banerjee, Amruit Sahoo, Sayan Layek, Avik Dutta, Rima Hazra,
  Animesh Mukherjee
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.12671 ,  1571kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14624
replaced with revised version Tue, 5 Mar 2024 02:45:41 GMT   (8068kb,D)

Title: Query of CC: Unearthing Large Scale Domain-Specific Knowledge from
  Public Corpora
Authors: Zhaoye Fei, Yunfan Shao, Linyang Li, Zhiyuan Zeng, Conghui He, Hang
  Yan, Dahua Lin and Xipeng Qiu
Categories: cs.CL
Comments: We have released the full data (total of 735GB) in
  https://huggingface.co/datasets/Query-of-CC/knowledge_pile_full and partial
  data (about 40GB) in
  https://huggingface.co/datasets/Query-of-CC/knowledge_pile
\\ ( https://arxiv.org/abs/2401.14624 ,  8068kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00421
replaced with revised version Tue, 5 Mar 2024 03:45:22 GMT   (5726kb,D)

Title: From PARIS to LE-PARIS: Toward Patent Response Automation with
  Recommender Systems and Collaborative Large Language Models
Authors: Jung-Mei Chu, Hao-Cheng Lo, Jieh Hsiang, and Chun-Chieh Cho
Categories: cs.CL cs.HC cs.IR cs.LG
Comments: 28 pages, 5 figures, typos corrected, references added, under review
\\ ( https://arxiv.org/abs/2402.00421 ,  5726kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09353
replaced with revised version Tue, 5 Mar 2024 07:31:21 GMT   (495kb,D)

Title: DoRA: Weight-Decomposed Low-Rank Adaptation
Authors: Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang
  Frank Wang, Kwang-Ting Cheng, Min-Hung Chen
Categories: cs.CL cs.CV
Comments: Code available at https://github.com/nbasyl/DoRA
\\ ( https://arxiv.org/abs/2402.09353 ,  495kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11000
replaced with revised version Tue, 5 Mar 2024 13:57:28 GMT   (11424kb,D)

Title: ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment
Authors: Yangyifei Luo, Zhuo Chen, Lingbing Guo, Qian Li, Wenxuan Zeng, Zhixin
  Cai, Jianxin Li
Categories: cs.CL cs.AI
Comments: Ongoing work; 16 pages, 9 Tables, 8 Figures; Code:
  https://github.com/lyyf2002/ASGEA
\\ ( https://arxiv.org/abs/2402.11000 ,  11424kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13613
replaced with revised version Mon, 4 Mar 2024 21:20:57 GMT   (7905kb,D)

Title: Overview of the VLSP 2023 -- ComOM Shared Task: A Data Challenge for
  Comparative Opinion Mining from Vietnamese Product Reviews
Authors: Hoang-Quynh Le, Duy-Cat Can, Khanh-Vinh Nguyen and Mai-Vu Tran
Categories: cs.CL cs.LG
Comments: In Proceedings of VLSP 2023
\\ ( https://arxiv.org/abs/2402.13613 ,  7905kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14818
replaced with revised version Tue, 5 Mar 2024 11:22:07 GMT   (25805kb,D)

Title: PALO: A Polyglot Large Multimodal Model for 5B People
Authors: Muhammad Maaz, Hanoona Rasheed, Abdelrahman Shaker, Salman Khan,
  Hisham Cholakal, Rao M. Anwer, Tim Baldwin, Michael Felsberg, Fahad S. Khan
Categories: cs.CL cs.CV
Comments: Technical Report of PALO
\\ ( https://arxiv.org/abs/2402.14818 ,  25805kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18766
replaced with revised version Tue, 5 Mar 2024 10:44:03 GMT   (58kb,D)

Title: Advancing Generative AI for Portuguese with Open Decoder Gerv\'asio PT*
Authors: Rodrigo Santos, Jo\~ao Silva, Lu\'is Gomes, Jo\~ao Rodrigues,
  Ant\'onio Branco
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.18766 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19170
replaced with revised version Mon, 4 Mar 2024 20:54:34 GMT   (834kb)

Title: Improving Legal Judgement Prediction in Romanian with Long Text Encoders
Authors: Mihai Masala, Traian Rebedea and Horia Velicu
Categories: cs.CL cs.AI
Comments: Rejected at LREC-COLING with 4/4/3
\\ ( https://arxiv.org/abs/2402.19170 ,  834kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19282
replaced with revised version Tue, 5 Mar 2024 12:54:16 GMT   (124kb,D)

Title: WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset
Authors: Jiantao Qiu, Haijun Lv, Zhenjiang Jin, Rui Wang, Wenchang Ning, Jia
  Yu, ChaoBin Zhang, Pei Chu, Yuan Qu, Jin Shi, Lindong Lu, Runyu Peng, Zhiyuan
  Zeng, Huanze Tang, Zhikai Lei, Jiawei Hong, Keyu Chen, Zhaoye Fei, Ruiliang
  Xu, Wei Li, Zhongying Tu, Hang Yan and Conghui He
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.19282 ,  124kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19350
replaced with revised version Mon, 4 Mar 2024 19:42:48 GMT   (230kb,D)

Title: Prompting Explicit and Implicit Knowledge for Multi-hop Question
  Answering Based on Human Reading Process
Authors: Guangming Huang, Yunfei Long, Cunjin Luo, Jiaxing Shen, Xia Sun
Categories: cs.CL
Comments: This paper has been accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2402.19350 ,  230kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00818
replaced with revised version Tue, 5 Mar 2024 14:31:03 GMT   (307kb,D)

Title: DenseMamba: State Space Models with Dense Hidden Connection for
  Efficient Large Language Models
Authors: Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo,
  Yunhe Wang
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2403.00818 ,  307kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00835
replaced with revised version Tue, 5 Mar 2024 08:01:01 GMT   (1238kb,D)

Title: CLLMs: Consistency Large Language Models
Authors: Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, Hao Zhang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.00835 ,  1238kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01469
replaced with revised version Tue, 5 Mar 2024 09:58:08 GMT   (934kb,D)

Title: KorMedMCQA: Multi-Choice Question Answering Benchmark for Korean
  Healthcare Professional Licensing Examinations
Authors: Sunjun Kweon, Byungjin Choi, Minkyu Kim, Rae Woong Park, Edward Choi
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.01469 ,  934kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01528
replaced with revised version Tue, 5 Mar 2024 11:12:47 GMT   (1299kb,D)

Title: Leveraging Biomolecule and Natural Language through Multi-Modal
  Learning: A Survey
Authors: Qizhi Pei, Lijun Wu, Kaiyuan Gao, Jinhua Zhu, Yue Wang, Zun Wang, Tao
  Qin, and Rui Yan
Categories: cs.CL cs.AI q-bio.BM
Comments: Survey Paper. 25 pages, 9 figures, and 3 tables
\\ ( https://arxiv.org/abs/2403.01528 ,  1299kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01548
replaced with revised version Tue, 5 Mar 2024 18:41:07 GMT   (1339kb,D)

Title: In-Context Sharpness as Alerts: An Inner Representation Perspective for
  Hallucination Mitigation
Authors: Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang
  Gao, Junxian He
Categories: cs.CL cs.AI cs.LG
Comments: code repo is available at:
  https://github.com/hkust-nlp/Activation_decoding.git
\\ ( https://arxiv.org/abs/2403.01548 ,  1339kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01616
replaced with revised version Tue, 5 Mar 2024 18:15:49 GMT   (147kb,D)

Title: Towards Comprehensive Vietnamese Retrieval-Augmented Generation and
  Large Language Models
Authors: Nguyen Quang Duc, Le Hai Son, Nguyen Duc Nhan, Nguyen Dich Nhat Minh,
  Le Thanh Huong, Dinh Viet Sang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.01616 ,  147kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01777
replaced with revised version Tue, 5 Mar 2024 18:26:04 GMT   (7055kb,D)

Title: NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language
  Models
Authors: Lizhou Fan, Wenyue Hua, Xiang Li, Kaijie Zhu, Mingyu Jin, Lingyao Li,
  Haoyang Ling, Jinkui Chi, Jindong Wang, Xin Ma, Yongfeng Zhang
Categories: cs.CL cs.CV
Comments: 16 pages, 10 figures, 2 tables
\\ ( https://arxiv.org/abs/2403.01777 ,  7055kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01897
replaced with revised version Tue, 5 Mar 2024 10:49:17 GMT   (57kb,D)

Title: Fostering the Ecosystem of Open Neural Encoders for Portuguese with
  Albertina PT* Family
Authors: Rodrigo Santos, Jo\~ao Rodrigues, Lu\'is Gomes, Jo\~ao Silva,
  Ant\'onio Branco, Henrique Lopes Cardoso, Tom\'as Freitas Os\'orio, Bernardo
  Leite
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.01897 ,  57kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02130
replaced with revised version Tue, 5 Mar 2024 08:12:18 GMT   (633kb,D)

Title: Using LLMs for the Extraction and Normalization of Product Attribute
  Values
Authors: Nick Baumann, Alexander Brinkmann, Christian Bizer
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.02130 ,  633kb)
------------------------------------------------------------------------------
\\
arXiv:2001.05371
replaced with revised version Tue, 5 Mar 2024 12:49:00 GMT   (6554kb,D)

Title: Making deep neural networks right for the right scientific reasons by
  interacting with their explanations
Authors: Patrick Schramowski, Wolfgang Stammer, Stefano Teso, Anna Brugger,
  Xiaoting Shao, Hans-Georg Luigs, Anne-Katrin Mahlein, Kristian Kersting
Categories: cs.LG cs.AI stat.ML
Comments: arXiv admin note: text overlap with arXiv:1805.08578
\\ ( https://arxiv.org/abs/2001.05371 ,  6554kb)
------------------------------------------------------------------------------
\\
arXiv:2102.12920
replaced with revised version Tue, 5 Mar 2024 08:03:57 GMT   (70kb)

Title: Emerging Trends in Federated Learning: From Model Fusion to Federated X
  Learning
Authors: Shaoxiong Ji and Yue Tan and Teemu Saravirta and Zhiqin Yang and Yixin
  Liu and Lauri Vasankari and Shirui Pan and Guodong Long and Anwar Walid
Categories: cs.LG cs.DC
\\ ( https://arxiv.org/abs/2102.12920 ,  70kb)
------------------------------------------------------------------------------
\\
arXiv:2104.04987
replaced with revised version Tue, 5 Mar 2024 03:01:21 GMT   (170kb,D)

Title: AutoGL: A Library for Automated Graph Learning
Authors: Ziwei Zhang, Yijian Qin, Zeyang Zhang, Chaoyu Guan, Jie Cai, Heng
  Chang, Jiyan Jiang, Haoyang Li, Zixin Sun, Beini Xie, Yang Yao, Yipeng Zhang,
  Xin Wang, Wenwu Zhu
Categories: cs.LG cs.AI
Comments: Extended version; initial version published at ICLR 2021 Workshop on
  Geometrical and Topological Representation Learning
\\ ( https://arxiv.org/abs/2104.04987 ,  170kb)
------------------------------------------------------------------------------
\\
arXiv:2109.10795
replaced with revised version Tue, 5 Mar 2024 10:23:52 GMT   (1876kb,D)

Title: Neural network relief: a pruning algorithm based on neural activity
Authors: Aleksandr Dekhovich, David M.J. Tax, Marcel H.F. Sluiter, Miguel A.
  Bessa
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2109.10795 ,  1876kb)
------------------------------------------------------------------------------
\\
arXiv:2111.10657
replaced with revised version Tue, 5 Mar 2024 14:25:15 GMT   (5067kb,D)

Title: Generalizing Graph Neural Networks on Out-Of-Distribution Graphs
Authors: Shaohua Fan, Xiao Wang, Chuan Shi, Peng Cui and Bai Wang
Categories: cs.LG cs.AI
Comments: Under review
\\ ( https://arxiv.org/abs/2111.10657 ,  5067kb)
------------------------------------------------------------------------------
\\
arXiv:2205.10089
replaced with revised version Mon, 4 Mar 2024 19:35:38 GMT   (948kb,D)

Title: Kernel Normalized Convolutional Networks
Authors: Reza Nasirigerdeh, Reihaneh Torkzadehmahani, Daniel Rueckert, Georgios
  Kaissis
Categories: cs.LG cs.CV
Journal-ref: Transactions on Machine Learning Research (TMLR), 2024
\\ ( https://arxiv.org/abs/2205.10089 ,  948kb)
------------------------------------------------------------------------------
\\
arXiv:2206.05880
replaced with revised version Tue, 5 Mar 2024 07:18:44 GMT   (1028kb,D)

Title: Confident Sinkhorn Allocation for Pseudo-Labeling
Authors: Vu Nguyen and Hisham Husain and Sachin Farfade and Anton van den
  Hengel
Categories: cs.LG
Comments: Code https://github.com/amzn/confident-sinkhorn-allocation
\\ ( https://arxiv.org/abs/2206.05880 ,  1028kb)
------------------------------------------------------------------------------
\\
arXiv:2206.10540
replaced with revised version Tue, 5 Mar 2024 07:36:09 GMT   (116kb,D)

Title: Rethinking Symbolic Regression Datasets and Benchmarks for Scientific
  Discovery
Authors: Yoshitomo Matsubara, Naoya Chiba, Ryo Igarashi, Yoshitaka Ushiku
Categories: cs.LG cs.AI cs.NE cs.SC
Comments: Accepted at DMLR. Code and datasets are available at
  https://github.com/omron-sinicx/srsd-benchmark
  https://huggingface.co/datasets/yoshitomo-matsubara/srsd-feynman_easy
  https://huggingface.co/datasets/yoshitomo-matsubara/srsd-feynman_medium
  https://huggingface.co/datasets/yoshitomo-matsubara/srsd-feynman_hard and
  another three sets of SRSD datasets with dummy variables
\\ ( https://arxiv.org/abs/2206.10540 ,  116kb)
------------------------------------------------------------------------------
\\
arXiv:2211.10209
replaced with revised version Tue, 5 Mar 2024 07:51:14 GMT   (5090kb,D)

Title: On the Alignment of Group Fairness with Attribute Privacy
Authors: Jan Aalmoes and Vasisht Duddu and Antoine Boutet
Categories: cs.LG cs.CR
Comments: arXiv admin note: text overlap with arXiv:2202.02242
\\ ( https://arxiv.org/abs/2211.10209 ,  5090kb)
------------------------------------------------------------------------------
\\
arXiv:2211.15188
replaced with revised version Tue, 5 Mar 2024 04:42:21 GMT   (5614kb,D)

Title: Incremental Spatial and Spectral Learning of Neural Operators for
  Solving Large-Scale PDEs
Authors: Robert Joseph George, Jiawei Zhao, Jean Kossaifi, Zongyi Li, Anima
  Anandkumar
Categories: cs.LG
\\ ( https://arxiv.org/abs/2211.15188 ,  5614kb)
------------------------------------------------------------------------------
\\
arXiv:2211.17029
replaced with revised version Tue, 5 Mar 2024 14:57:30 GMT   (2526kb,D)

Title: Directed Acyclic Graph Structure Learning from Dynamic Graphs
Authors: Shaohua Fan, Shuyang Zhang, Xiao Wang, Chuan Shi
Categories: cs.LG cs.AI
Comments: Accepted by AAAI23
\\ ( https://arxiv.org/abs/2211.17029 ,  2526kb)
------------------------------------------------------------------------------
\\
arXiv:2301.05849
replaced with revised version Tue, 5 Mar 2024 13:57:22 GMT   (699kb,D)

Title: Knowledge Distillation in Federated Edge Learning: A Survey
Authors: Zhiyuan Wu, Sheng Sun, Yuwei Wang, Min Liu, Xuefeng Jiang, Runhan Li,
  Bo Gao
Categories: cs.LG
Comments: 13 pages, 1 figure, 2 tables
\\ ( https://arxiv.org/abs/2301.05849 ,  699kb)
------------------------------------------------------------------------------
\\
arXiv:2301.10327
replaced with revised version Mon, 4 Mar 2024 20:46:37 GMT   (12872kb,D)

Title: Generating Multidimensional Clusters With Support Lines
Authors: Nuno Fachada, Diogo de Andrade
Categories: cs.LG cs.CV cs.PL
Comments: The peer-reviewed version of this paper is published in
  Knowledge-Based Systems at https://doi.org/10.1016/j.knosys.2023.110836. This
  version is typeset by the author and differs only in pagination and
  typographical detail
ACM-class: I.5; I.2.5
Journal-ref: Knowledge-Based Systems, 277, 110836, 2023
DOI: 10.1016/j.knosys.2023.110836
\\ ( https://arxiv.org/abs/2301.10327 ,  12872kb)
------------------------------------------------------------------------------
\\
arXiv:2301.12130
replaced with revised version Tue, 5 Mar 2024 12:01:10 GMT   (910kb,D)

Title: Constrained Policy Optimization with Explicit Behavior Density for
  Offline Reinforcement Learning
Authors: Jing Zhang, Chi Zhang, Wenjia Wang, Bing-Yi Jing
Categories: cs.LG
\\ ( https://arxiv.org/abs/2301.12130 ,  910kb)
------------------------------------------------------------------------------
\\
arXiv:2304.12228
replaced with revised version Tue, 5 Mar 2024 07:11:14 GMT   (9051kb,D)

Title: Hierarchical Contrastive Learning Enhanced Heterogeneous Graph Neural
  Network
Authors: Nian Liu, Xiao Wang, Hui Han, Chuan Shi
Categories: cs.LG
Comments: This paper has been accepted by TKDE as a regular paper. arXiv admin
  note: substantial text overlap with arXiv:2105.09111
\\ ( https://arxiv.org/abs/2304.12228 ,  9051kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14342
replaced with revised version Tue, 5 Mar 2024 17:07:16 GMT   (6090kb,D)

Title: Sophia: A Scalable Stochastic Second-order Optimizer for Language Model
  Pre-training
Authors: Hong Liu, Zhiyuan Li, David Hall, Percy Liang, Tengyu Ma
Categories: cs.LG cs.CL math.OC
\\ ( https://arxiv.org/abs/2305.14342 ,  6090kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12105
replaced with revised version Fri, 1 Mar 2024 21:28:00 GMT   (19527kb,D)

Title: Mass-Producing Failures of Multimodal Systems with Language Models
Authors: Shengbang Tong, Erik Jones, Jacob Steinhardt
Categories: cs.LG cs.CL cs.SE
Comments: Under Review
\\ ( https://arxiv.org/abs/2306.12105 ,  19527kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12497
replaced with revised version Mon, 4 Mar 2024 19:29:04 GMT   (542kb,D)

Title: Density Uncertainty Layers for Reliable Uncertainty Estimation
Authors: Yookoon Park, David M. Blei
Categories: cs.LG stat.ML
Comments: Published in AISTATS 2024
\\ ( https://arxiv.org/abs/2306.12497 ,  542kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16504
replaced with revised version Tue, 5 Mar 2024 17:51:19 GMT   (218kb,D)

Title: Momentum Benefits Non-IID Federated Learning Simply and Provably
Authors: Ziheng Cheng, Xinmeng Huang, Pengfei Wu, Kun Yuan
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2306.16504 ,  218kb)
------------------------------------------------------------------------------
\\
arXiv:2307.11333
replaced with revised version Mon, 4 Mar 2024 22:56:09 GMT   (6674kb,D)

Title: Demystifying Local and Global Fairness Trade-offs in Federated Learning
  Using Partial Information Decomposition
Authors: Faisal Hamman, Sanghamitra Dutta
Categories: cs.LG cs.CY cs.IT math.IT
Comments: Published at the International Conference on Learning Representations
  (ICLR) 2024
\\ ( https://arxiv.org/abs/2307.11333 ,  6674kb)
------------------------------------------------------------------------------
\\
arXiv:2307.14940
replaced with revised version Tue, 5 Mar 2024 07:29:02 GMT   (471kb,D)

Title: A Self-Adaptive Penalty Method for Integrating Prior Knowledge
  Constraints into Neural ODEs
Authors: C. Coelho, M. Fernanda P. Costa, L. L. Ferr\'as
Categories: cs.LG math.OC
ACM-class: I.5.1; G.1.6
\\ ( https://arxiv.org/abs/2307.14940 ,  471kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07061
replaced with revised version Tue, 5 Mar 2024 01:48:36 GMT   (362kb,D)

Title: Machine Unlearning: Solutions and Challenges
Authors: Jie Xu, Zihan Wu, Cong Wang and Xiaohua Jia
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2308.07061 ,  362kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09766
replaced with revised version Tue, 5 Mar 2024 17:45:06 GMT   (1251kb,D)

Title: Time Series Predictions in Unmonitored Sites: A Survey of Machine
  Learning Techniques in Water Resources
Authors: Jared D. Willard, Charuleka Varadharajan, Xiaowei Jia, Vipin Kumar
Categories: cs.LG
Comments: 39 pages, 4 figures, 1 table, submitted to Environmental Data Science
MSC-class: 68T07
ACM-class: I.2.6; J.2
\\ ( https://arxiv.org/abs/2308.09766 ,  1251kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10664
replaced with revised version Tue, 5 Mar 2024 11:31:23 GMT   (897kb,D)

Title: A Safe Deep Reinforcement Learning Approach for Energy Efficient
  Federated Learning in Wireless Communication Networks
Authors: Nikolaos Koursioumpas, Lina Magoula, Nikolaos Petropouleas,
  Alexandros-Ioannis Thanopoulos, Theodora Panagea, Nancy Alonistioti, M. A.
  Gutierrez-Estevez, Ramin Khalili
Categories: cs.LG cs.AI
Comments: 12 Pages Double Column, 6 Figures, Accepted for publication in the
  IEEE Transactions on Green Communications and Networking (TGCN). arXiv admin
  note: text overlap with arXiv:2306.14237
DOI: 10.1109/TGCN.2024.3372695
\\ ( https://arxiv.org/abs/2308.10664 ,  897kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10402
replaced with revised version Tue, 5 Mar 2024 06:55:28 GMT   (208kb,D)

Title: Minimum width for universal approximation using ReLU networks on compact
  domain
Authors: Namjun Kim, Chanho Min, Sejun Park
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2309.10402 ,  208kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13944
replaced with revised version Tue, 5 Mar 2024 02:35:15 GMT   (2006kb,D)

Title: Provable Training for Graph Contrastive Learning
Authors: Yue Yu, Xiao Wang, Mengmei Zhang, Nian Liu, Chuan Shi
Categories: cs.LG
Comments: NeurIPS 2023 spotlight. Camera-ready version
\\ ( https://arxiv.org/abs/2309.13944 ,  2006kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16487
replaced with revised version Mon, 4 Mar 2024 19:03:44 GMT   (5447kb,D)

Title: Towards Poisoning Fair Representations
Authors: Tianci Liu, Haoyu Wang, Feijie Wu, Hengtong Zhang, Pan Li, Lu Su, Jing
  Gao
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.16487 ,  5447kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04295
replaced with revised version Tue, 5 Mar 2024 13:32:26 GMT   (4654kb,D)

Title: Identifying Representations for Intervention Extrapolation
Authors: Sorawit Saengkyongam, Elan Rosenfeld, Pradeep Ravikumar, Niklas
  Pfister, Jonas Peters
Categories: cs.LG cs.AI stat.ML
Comments: Accepted at the International Conference on Learning Representations
  (ICLR) 2024
\\ ( https://arxiv.org/abs/2310.04295 ,  4654kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09847
replaced with revised version Tue, 5 Mar 2024 13:49:52 GMT   (1043kb,D)

Title: XRMDN: An Extended Recurrent Mixture Density Network for Short-Term
  Probabilistic Rider Demand Forecasting with High Volatility
Authors: Xiaoming Li, Hubert Normandin-Taillon, Chun Wang, Xiao Huang
Categories: cs.LG
Comments: 11 pages, 14 figures, 5 tables
\\ ( https://arxiv.org/abs/2310.09847 ,  1043kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10096
replaced with revised version Tue, 5 Mar 2024 11:12:49 GMT   (989kb,D)

Title: LLP-Bench: A Large Scale Tabular Benchmark for Learning from Label
  Proportions
Authors: Anand Brahmbhatt, Mohith Pokala, Rishi Saket and Aravindan Raghuveer
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.10096 ,  989kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13061
replaced with revised version Mon, 4 Mar 2024 21:59:58 GMT   (4391kb,D)

Title: To grok or not to grok: Disentangling generalization and memorization on
  corrupted algorithmic datasets
Authors: Darshil Doshi, Aritra Das, Tianyu He, Andrey Gromov
Categories: cs.LG cond-mat.dis-nn stat.ML
Comments: 9+20 pages, 7+25 figures, 2 tables
\\ ( https://arxiv.org/abs/2310.13061 ,  4391kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13841
replaced with revised version Mon, 4 Mar 2024 20:02:03 GMT   (2406kb,D)

Title: Fast hyperboloid decision tree algorithms
Authors: Philippe Chlenski, Ethan Turok, Antonio Moretti, Itsik Pe'er
Categories: cs.LG
Journal-ref: International Conference on Learning Representations (2024)
\\ ( https://arxiv.org/abs/2310.13841 ,  2406kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17748
replaced with revised version Mon, 4 Mar 2024 20:39:19 GMT   (2627kb,D)

Title: Making the End-User a Priority in Benchmarking: OrionBench for
  Unsupervised Time Series Anomaly Detection
Authors: Sarah Alnegheimish, Laure Berti-Equille, Kalyan Veeramachaneni
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.17748 ,  2627kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03002
replaced with revised version Mon, 4 Mar 2024 17:07:24 GMT   (264kb,D)

Title: Estimating treatment effects from single-arm trials via latent-variable
  modeling
Authors: Manuel Haussmann, Tran Minh Son Le, Viivi Halla-aho, Samu Kurki, Jussi
  V. Leinonen, Miika Koskinen, Samuel Kaski, Harri L\"ahdesm\"aki
Categories: cs.LG stat.ML
Comments: Published at the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024
\\ ( https://arxiv.org/abs/2311.03002 ,  264kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04179
replaced with revised version Tue, 5 Mar 2024 09:54:27 GMT   (1243kb)

Title: On Leakage in Machine Learning Pipelines
Authors: Leonard Sasse and Eliana Nicolaisen-Sobesky and Juergen Dukart and
  Simon B. Eickhoff and Michael G\"otz and Sami Hamdan and Vera Komeyer and
  Abhijit Kulkarni and Juha Lahnakoski and Bradley C. Love and Federico
  Raimondo and Kaustubh R. Patil
Categories: cs.LG cs.AI
Comments: second draft
\\ ( https://arxiv.org/abs/2311.04179 ,  1243kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16176
replaced with revised version Tue, 5 Mar 2024 14:16:08 GMT   (13287kb,D)

Title: Mitigating Biases with Diverse Ensembles and Diffusion Models
Authors: Luca Scimeca, Alexander Rubinstein, Damien Teney, Seong Joon Oh,
  Armand Mihai Nicolicioiu, Yoshua Bengio
Categories: cs.LG cs.AI cs.CV
Comments: arXiv admin note: substantial text overlap with arXiv:2310.02230
\\ ( https://arxiv.org/abs/2311.16176 ,  13287kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18061
replaced with revised version Mon, 4 Mar 2024 23:04:21 GMT   (833kb,D)

Title: TransNAS-TSAD: Harnessing Transformers for Multi-Objective Neural
  Architecture Search in Time Series Anomaly Detection
Authors: Ijaz Ul Haq, Byung Suk Lee and Donna M. Rizzo
Categories: cs.LG cs.NE
Comments: 32 pages , 4 figures, It will submitted to a journal
\\ ( https://arxiv.org/abs/2311.18061 ,  833kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18177
replaced with revised version Tue, 5 Mar 2024 12:07:34 GMT   (101kb,D)

Title: An Effective Universal Polynomial Basis for Spectral Graph Neural
  Networks
Authors: Keke Huang, Pietro Li\`o
Categories: cs.LG cs.SI eess.SP
\\ ( https://arxiv.org/abs/2311.18177 ,  101kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16650
replaced with revised version Tue, 5 Mar 2024 00:52:06 GMT   (1307kb,D)

Title: Augmenting Replay in World Models for Continual Reinforcement Learning
Authors: Luke Yang, Levin Kuhlmann, Gideon Kowadlo
Categories: cs.LG cs.AI
ACM-class: I.2.6; I.5.0; I.5.1
\\ ( https://arxiv.org/abs/2401.16650 ,  1307kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04412
replaced with revised version Tue, 5 Mar 2024 03:52:18 GMT   (29944kb,D)

Title: The VampPrior Mixture Model
Authors: Andrew Stirn and David A. Knowles
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2402.04412 ,  29944kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06700
replaced with revised version Tue, 5 Mar 2024 05:17:21 GMT   (504kb,D)

Title: Entropy-Regularized Token-Level Policy Optimization for Large Language
  Models
Authors: Muning Wen, Cheng Deng, Jun Wang, Weinan Zhang and Ying Wen
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.06700 ,  504kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07232
replaced with revised version Tue, 5 Mar 2024 07:11:39 GMT   (4373kb,D)

Title: GTM: General Trajectory Modeling with Auto-regressive Generation of
  Feature Domains
Authors: Yan Lin, Jilin Hu, Shengnan Guo, Bin Yang, Christian S. Jensen,
  Youfang Lin, Huaiyu Wan
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.07232 ,  4373kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12177
replaced with revised version Tue, 5 Mar 2024 07:08:16 GMT   (44kb)

Title: Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-Tuning
Authors: Mingtian Zhang, Shawn Lan, Peter Hayes, David Barber
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.12177 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13852
replaced with revised version Tue, 5 Mar 2024 16:32:24 GMT   (757kb,D)

Title: Neural Control System for Continuous Glucose Monitoring and Maintenance
Authors: Azmine Toushik Wasi
Categories: cs.LG cs.AI cs.NE cs.SY eess.SY stat.ML
Comments: 9 Pages, 4 figures, ICLR 2024 Tiny Papers Track
  https://openreview.net/forum?id=Te4P3Cn54g
Journal-ref: The Second Tiny Papers Track at ICLR 2024
\\ ( https://arxiv.org/abs/2402.13852 ,  757kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14160
replaced with revised version Tue, 5 Mar 2024 06:55:26 GMT   (3133kb,D)

Title: Recursive Speculative Decoding: Accelerating LLM Inference via Sampling
  Without Replacement
Authors: Wonseok Jeon, Mukul Gagrani, Raghavv Goel, Junyoung Park, Mingu Lee,
  Christopher Lott
Categories: cs.LG cs.AI
Comments: 82 pages, 9 figures, 54 tables
\\ ( https://arxiv.org/abs/2402.14160 ,  3133kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14482
replaced with revised version Tue, 5 Mar 2024 12:02:46 GMT   (18137kb,D)

Title: SpanSeq: Similarity-based sequence data splitting method for improved
  development and assessment of deep learning projects
Authors: Alfred Ferrer Florensa, Jose Juan Almagro Armenteros, Henrik Nielsen,
  Frank M{\o}ller Aarestrup, Philip Thomas Lanken Conradsen Clausen
Categories: cs.LG q-bio.QM
\\ ( https://arxiv.org/abs/2402.14482 ,  18137kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15183
replaced with revised version Tue, 5 Mar 2024 05:22:00 GMT   (959kb,D)

Title: GraphEdit: Large Language Models for Graph Structure Learning
Authors: Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei,
  Liang Pang, Tat-Seng Chua, Chao Huang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.15183 ,  959kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15993
replaced with revised version Tue, 5 Mar 2024 05:37:21 GMT   (522kb,D)

Title: Learning Method for S4 with Diagonal State Space Layers using Balanced
  Truncation
Authors: Haruka Ezoe and Kazuhiro Sato
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.15993 ,  522kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18372
replaced with revised version Fri, 1 Mar 2024 21:53:26 GMT   (3359kb,D)

Title: FedUV: Uniformity and Variance for Heterogeneous Federated Learning
Authors: Ha Min Son, Moon-Hyun Kim, Tai-Myoung Chung, Chao Huang, Xin Liu
Categories: cs.LG cs.AI cs.DC
Comments: 11 pages, 4 figures, 5 tables, to appear at CVPR 2024
\\ ( https://arxiv.org/abs/2402.18372 ,  3359kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18946
replaced with revised version Tue, 5 Mar 2024 09:00:29 GMT   (2736kb,D)

Title: Real-Time Adaptive Safety-Critical Control with Gaussian Processes in
  High-Order Uncertain Models
Authors: Yu Zhang, Long Wen, Xiangtong Yao, Zhenshan Bing, Linghuan Kong, Wei
  He, and Alois Knoll
Categories: cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2402.18946 ,  2736kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00225
replaced with revised version Tue, 5 Mar 2024 06:23:41 GMT   (2026kb,D)

Title: Robust Policy Learning via Offline Skill Diffusion
Authors: Woo Kyung Kim, Minjong Yoo, Honguk Woo
Categories: cs.LG
Comments: Accepted for AAAI 2024
\\ ( https://arxiv.org/abs/2403.00225 ,  2026kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01053
replaced with revised version Tue, 5 Mar 2024 07:36:04 GMT   (1230kb,D)

Title: Seeing Unseen: Discover Novel Biomedical Concepts via
  Geometry-Constrained Probabilistic Modeling
Authors: Jianan Fan, Dongnan Liu, Hang Chang, Heng Huang, Mei Chen, and Weidong
  Cai
Categories: cs.LG cs.AI cs.CV
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2403.01053 ,  1230kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01128
replaced with revised version Tue, 5 Mar 2024 14:37:36 GMT   (727kb,D)

Title: Sensitivity Analysis On Loss Landscape
Authors: Salman Faroz
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.01128 ,  727kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01919
replaced with revised version Tue, 5 Mar 2024 10:12:36 GMT   (1537kb,D)

Title: Matrix Completion with Convex Optimization and Column Subset Selection
Authors: Antonina Krajewska and Ewa Niewiadomska-Szynkiewicz
Categories: cs.LG
ACM-class: G.1.3; G.1.6
\\ ( https://arxiv.org/abs/2403.01919 ,  1537kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02187
replaced with revised version Tue, 5 Mar 2024 12:43:12 GMT   (123kb,D)

Title: Mutual Information Estimation via Normalizing Flows
Authors: Ivan Butakov, Alexander Tolmachev, Sofia Malanchuk, Anna Neopryatnaya,
  Alexey Frolov
Categories: cs.LG cs.IT math.IT stat.ML
Comments: 15 pages, 5 figures
\\ ( https://arxiv.org/abs/2403.02187 ,  123kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02241
replaced with revised version Tue, 5 Mar 2024 11:43:24 GMT   (6005kb,D)

Title: Neural Redshift: Random Networks are not Random Functions
Authors: Damien Teney, Armand Nicolicioiu, Valentin Hartmann, Ehsan Abbasnejad
Categories: cs.LG cs.AI cs.CV
Journal-ref: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
  2024
\\ ( https://arxiv.org/abs/2403.02241 ,  6005kb)
------------------------------------------------------------------------------
\\
arXiv:2108.04376 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 18:28:09 GMT   (5679kb,D)

Title: Counterfactual Effect Generalization: A Combinatorial Definition
Authors: Andre F. Ribeiro
Categories: stat.ME cs.AI
\\ ( https://arxiv.org/abs/2108.04376 ,  5679kb)
------------------------------------------------------------------------------
\\
arXiv:2211.13715 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 08:33:45 GMT   (2875kb,D)

Title: Trust Your $\nabla$: Gradient-based Intervention Targeting for Causal
  Discovery
Authors: Mateusz Olko, Micha{\l} Zaj\k{a}c, Aleksandra Nowak, Nino Scherrer,
  Yashas Annadani, Stefan Bauer, {\L}ukasz Kuci\'nski, Piotr Mi{\l}o\'s
Categories: stat.ML cs.AI cs.LG stat.ME
Comments: Accepted to 37th Conference on Neural Information Processing Systems
  (NeurIPS 2023)
\\ ( https://arxiv.org/abs/2211.13715 ,  2875kb)
------------------------------------------------------------------------------
\\
arXiv:2303.15734
replaced with revised version Tue, 5 Mar 2024 06:33:40 GMT   (1078kb,D)

Title: Adaptive Background Music for a Fighting Game: A Multi-Instrument Volume
  Modulation Approach
Authors: Ibrahim Khan, Thai Van Nguyen, Chollakorn Nimpattanavong, Ruck
  Thawonmas
Categories: cs.SD cs.AI eess.AS
Comments: In the updated version, the description of the association between
  the distance between the two players (PD) and the instrument's volume on page
  3 has been revised
ACM-class: I.2; H.5.2; H.5
\\ ( https://arxiv.org/abs/2303.15734 ,  1078kb)
------------------------------------------------------------------------------
\\
arXiv:2304.07143
replaced with revised version Tue, 5 Mar 2024 16:50:50 GMT   (6816kb,D)

Title: Car-Following Models: A Multidisciplinary Review
Authors: Tianya Terry Zhang, Ph.D., Peter J. Jin, Ph.D., Sean T. McQuade,
  Ph.D., Alexandre Bayen, Ph.D., Benedetto Piccoli
Categories: eess.SY cs.AI cs.SY
\\ ( https://arxiv.org/abs/2304.07143 ,  6816kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09305
replaced with revised version Tue, 5 Mar 2024 01:10:18 GMT   (16714kb,D)

Title: Fast Training of Diffusion Models with Masked Transformers
Authors: Hongkai Zheng, Weili Nie, Arash Vahdat, Anima Anandkumar
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2306.09305 ,  16714kb)
------------------------------------------------------------------------------
\\
arXiv:2306.17256
replaced with revised version Mon, 4 Mar 2024 21:14:01 GMT   (2319kb,D)

Title: Could Small Language Models Serve as Recommenders? Towards Data-centric
  Cold-start Recommendations
Authors: Xuansheng Wu, Huachi Zhou, Yucheng Shi, Wenlin Yao, Xiao Huang,
  Ninghao Liu
Categories: cs.IR cs.AI cs.CL cs.SI
Comments: 8 pages, 4 figures
\\ ( https://arxiv.org/abs/2306.17256 ,  2319kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06924
replaced with revised version Tue, 5 Mar 2024 05:02:49 GMT   (8436kb,D)

Title: DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual
  Language Grounding
Authors: Shuijing Liu, Aamir Hasan, Kaiwen Hong, Runxuan Wang, Peixin Chang,
  Zachary Mizrachi, Justin Lin, D. Livingston McPherson, Wendy A. Rogers, and
  Katherine Driggs-Campbell
Categories: cs.RO cs.AI cs.CL cs.HC cs.LG
Comments: Published in IEEE Robotics and Automation Letters (RA-L)
DOI: 10.1109/LRA.2024.3362591
\\ ( https://arxiv.org/abs/2307.06924 ,  8436kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12754 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 17:19:25 GMT   (878kb,D)

Title: Nonparametric Linear Feature Learning in Regression Through
  Regularisation
Authors: Bertille Follain, Francis Bach
Categories: stat.ME cs.AI cs.LG math.ST stat.TH
Comments: 42 pages, 5 figures
MSC-class: 62G08, 62F10 (Primary), 65K10 (Secondary)
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2307.12754 ,  878kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16807 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 02:49:00 GMT   (1818kb,D)

Title: On the use of associative memory in Hopfield networks designed to solve
  propositional satisfiability problems
Authors: Natalya Weber, Werner Koch, Ozan Erdem, Tom Froese
Categories: nlin.AO cs.AI q-bio.NC
Comments: 7 pages, 4 figures
Journal-ref: 2023 IEEE Symposium Series on Computational Intelligence (SSCI)
  1352-1358
DOI: 10.1109/SSCI52147.2023.10371918
\\ ( https://arxiv.org/abs/2307.16807 ,  1818kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02424
replaced with revised version Tue, 5 Mar 2024 01:28:25 GMT   (22393kb,D)

Title: AXNav: Replaying Accessibility Tests from Natural Language
Authors: Maryam Taeb, Amanda Swearngin, Eldon Schoop, Ruijia Cheng, Yue Jiang,
  Jeffrey Nichols
Categories: cs.HC cs.AI
Comments: Accepted into Conference on Human Factors in Computing Systems (CHI)
  2024, 22 pages, 7 figures
ACM-class: I.2
DOI: 10.1145/3613904.3642777
\\ ( https://arxiv.org/abs/2310.02424 ,  22393kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08616
replaced with revised version Tue, 5 Mar 2024 15:31:00 GMT   (6098kb,D)

Title: A Generalized Neural Diffusion Framework on Graphs
Authors: Yibo Li, Xiao Wang, Hongrui Liu, Chuan Shi
Categories: cs.SI cs.AI
Comments: Accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2312.08616 ,  6098kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12450
replaced with revised version Tue, 5 Mar 2024 00:51:25 GMT   (660kb,D)

Title: Can It Edit? Evaluating the Ability of Large Language Models to Follow
  Code Editing Instructions
Authors: Federico Cassano, Luisa Li, Akul Sethi, Noah Shinn, Abby
  Brennan-Jones, Anton Lozhkov, Carolyn Jane Anderson, Arjun Guha
Categories: cs.SE cs.AI cs.LG cs.PL
Comments: arXiv admin note: substantial text overlap with arXiv:2312.06024
\\ ( https://arxiv.org/abs/2312.12450 ,  660kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04577
replaced with revised version Tue, 5 Mar 2024 09:12:35 GMT   (321kb,D)

Title: Masked Audio Generation using a Single Non-Autoregressive Transformer
Authors: Alon Ziv, Itai Gat, Gael Le Lan, Tal Remez, Felix Kreuk, Alexandre
  D\'efossez, Jade Copet, Gabriel Synnaeve, Yossi Adi
Categories: cs.SD cs.AI cs.LG eess.AS
\\ ( https://arxiv.org/abs/2401.04577 ,  321kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05932
replaced with revised version Tue, 5 Mar 2024 16:48:11 GMT   (2850kb,D)

Title: DiffDA: a Diffusion model for weather-scale Data Assimilation
Authors: Langwen Huang, Lukas Gianinazzi, Yuejiang Yu, Peter D. Dueben, Torsten
  Hoefler
Categories: cs.CE cs.AI
\\ ( https://arxiv.org/abs/2401.05932 ,  2850kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06401
replaced with revised version Tue, 5 Mar 2024 09:41:48 GMT   (0kb,I)

Title: DevEval: Evaluating Code Generation in Practical Software Projects
Authors: Jia Li, Ge Li, Yunfei Zhao, Yongmin Li, Zhi Jin, Hao Zhu, Huanyu Liu,
  Kaibo Liu, Lecheng Wang, Zheng Fang, Lanshen Wang, Jiazheng Ding, Xuanming
  Zhang, Yihong Dong, Yuqi Zhu, Bin Gu, Mengfei Yang
Categories: cs.SE cs.AI cs.CL
Comments: There are mistakes in the dataset. We need to re-check the dataset
  and repeat our experiments
\\ ( https://arxiv.org/abs/2401.06401 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16461
replaced with revised version Tue, 5 Mar 2024 10:58:33 GMT   (1114kb,D)

Title: Norm Enforcement with a Soft Touch: Faster Emergence, Happier Agents
Authors: Sz-Ting Tzeng, Nirav Ajmeri, Munindar P. Singh
Categories: cs.MA cs.AI cs.LG
Comments: 12 pages, 11 figures, 5 tables (and supplementary material with code
  availability and additional results), accepted at AAMAS 2024
\\ ( https://arxiv.org/abs/2401.16461 ,  1114kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12721
replaced with revised version Tue, 5 Mar 2024 04:22:32 GMT   (3676kb,D)

Title: PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for
  Recognizing Low-Quality Images
Authors: Jinsung Jeon, Hyundong Jin, Jonghyun Choi, Sanghyun Hong, Dongeun Lee,
  Kookjin Lee, Noseong Park
Categories: cs.CV cs.AI
Comments: Accepted at ICLR 2024
\\ ( https://arxiv.org/abs/2402.12721 ,  3676kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16868
replaced with revised version Tue, 5 Mar 2024 18:10:21 GMT   (2383kb,D)

Title: Codebook-enabled Generative End-to-end Semantic Communication Powered by
  Transformer
Authors: Peigen Ye, Yaping Sun, Shumin Yao, Hao Chen, Xiaodong Xu, Shuguang Cui
Categories: cs.IT cs.AI math.IT
Comments: IEEE INFOCOM PerAI6G 2024(accepted)
\\ ( https://arxiv.org/abs/2402.16868 ,  2383kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17334
replaced with revised version Tue, 5 Mar 2024 04:12:34 GMT   (4854kb,D)

Title: BiVRec: Bidirectional View-based Multimodal Sequential Recommendation
Authors: Jiaxi Hu, Jingtong Gao, Xiangyu Zhao, Yuehong Hu, Yuxuan Liang, Yiqi
  Wang, Ming He, Zitao Liu, Hongzhi Yin
Categories: cs.IR cs.AI
\\ ( https://arxiv.org/abs/2402.17334 ,  4854kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00030
replaced with revised version Tue, 5 Mar 2024 05:34:55 GMT   (1301kb,D)

Title: GraphPub: Generation of Differential Privacy Graph with High
  Availability
Authors: Wanghan Xu, Bin Shi, Ao Liu, Jiqiang Zhang, Bo Dong
Categories: cs.SI cs.AI cs.CR cs.LG
\\ ( https://arxiv.org/abs/2403.00030 ,  1301kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00867
replaced with revised version Tue, 5 Mar 2024 13:46:50 GMT   (1039kb,D)

Title: Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by
  Exploring Refusal Loss Landscapes
Authors: Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho
Categories: cs.CR cs.AI cs.CL cs.LG
Comments: Project page:
  https://huggingface.co/spaces/TrustSafeAI/GradientCuff-Jailbreak-Defense
\\ ( https://arxiv.org/abs/2403.00867 ,  1039kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00884
replaced with revised version Tue, 5 Mar 2024 13:42:06 GMT   (71kb,D)

Title: Text classification of column headers with a controlled vocabulary:
  leveraging LLMs for metadata enrichment
Authors: Margherita Martorana, Tobias Kuhn, Lise Stork, Jacco van Ossenbruggen
Categories: cs.DB cs.AI cs.IR
\\ ( https://arxiv.org/abs/2403.00884 ,  71kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00890
replaced with revised version Tue, 5 Mar 2024 14:33:33 GMT   (747kb)

Title: Improving Android Malware Detection Through Data Augmentation Using
  Wasserstein Generative Adversarial Networks
Authors: Kawana Stalin, Mikias Berhanu Mekoya
Categories: cs.CR cs.AI
Comments: 20 pages
\\ ( https://arxiv.org/abs/2403.00890 ,  747kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01131 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 11:11:41 GMT   (4006kb,D)

Title: LLaMoCo: Instruction Tuning of Large Language Models for Optimization
  Code Generation
Authors: Zeyuan Ma, Hongshu Guo, Jiacheng Chen, Guojun Peng, Zhiguang Cao,
  Yining Ma, Yue-Jiao Gong
Categories: math.OC cs.AI cs.CL cs.LG cs.NE cs.SE
\\ ( https://arxiv.org/abs/2403.01131 ,  4006kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06071
replaced with revised version Tue, 5 Mar 2024 14:36:12 GMT   (8378kb,D)

Title: GroundingGPT:Language Enhanced Multi-modal Grounding Model
Authors: Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou,
  Junting Pan, Zefeng Li, Van Tu Vu, Zhida Huang, Tao Wang
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2401.06071 ,  8378kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07383 (*cross-listing*)
replaced with revised version Mon, 4 Mar 2024 19:15:29 GMT   (1491kb,D)

Title: Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like
Authors: Naoyuki Kanda, Xiaofei Wang, Sefik Emre Eskimez, Manthan Thakker,
  Hemin Yang, Zirun Zhu, Min Tang, Canrun Li, Chung-Hsien Tsai, Zhen Xiao,
  Yufei Xia, Jinzhu Li, Yanqing Liu, Sheng Zhao, Michael Zeng
Categories: eess.AS cs.CL cs.LG cs.SD
Comments: See https://aka.ms/elate/ for demo samples, v2: subjective evaluation
  has been added
\\ ( https://arxiv.org/abs/2402.07383 ,  1491kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18240
replaced with revised version Tue, 5 Mar 2024 12:14:52 GMT   (7278kb,D)

Title: Prospect Personalized Recommendation on Large Language Model-based Agent
  Platform
Authors: Jizhi Zhang, Keqin Bao, Wenjie Wang, Yang Zhang, Wentao Shi, Wanhong
  Xu, Fuli Feng, Tat-Seng Chua
Categories: cs.IR cs.CL
\\ ( https://arxiv.org/abs/2402.18240 ,  7278kb)
------------------------------------------------------------------------------
\\
arXiv:1905.07342 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 17:45:35 GMT   (178kb,D)

Title: Pair-Matching: Links Prediction with Adaptive Queries
Authors: Christophe Giraud and Yann Issartel and Luc Leh\'ericy and Matthieu
  Lerasle
Categories: stat.ML cs.LG math.ST stat.TH
Comments: 78 pages
MSC-class: 62h30, 68T05, 05C80
\\ ( https://arxiv.org/abs/1905.07342 ,  178kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13413
replaced with revised version Tue, 5 Mar 2024 08:17:05 GMT   (2123kb,D)

Title: Fine Robotic Manipulation without Force/Torque Sensor
Authors: Shilin Shan, Quang-Cuong Pham
Categories: cs.RO cs.LG
Comments: Accepted to Robotics and Automation Letters (RA-L), 8 pages
DOI: 10.1109/LRA.2023.3341770
\\ ( https://arxiv.org/abs/2301.13413 ,  2123kb)
------------------------------------------------------------------------------
\\
arXiv:2304.04497
replaced with revised version Tue, 5 Mar 2024 07:36:02 GMT   (2364kb,D)

Title: A Unified Framework for Exploratory Learning-Aided Community Detection
  Under Topological Uncertainty
Authors: Yu Hou, Cong Tran, Ming Li, Won-Yong Shin
Categories: cs.SI cs.IR cs.LG cs.NE cs.NI
Comments: 17 pages, 9 figures, 6 tables; its conference version was presented
  at the ACM International Conference on Information and Knowledge Management
  (CIKM 2022)
\\ ( https://arxiv.org/abs/2304.04497 ,  2364kb)
------------------------------------------------------------------------------
\\
arXiv:2304.11860 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 05:25:19 GMT   (2155kb,D)

Title: On the lifting and reconstruction of nonlinear systems with multiple
  invariant sets
Authors: Shaowu Pan and Karthik Duraisamy
Categories: math.DS cs.LG
Comments: 14 pages
MSC-class: 37M10, 37M25, 47B33, 62F15
\\ ( https://arxiv.org/abs/2304.11860 ,  2155kb)
------------------------------------------------------------------------------
\\
arXiv:2304.13119
replaced with revised version Mon, 4 Mar 2024 22:42:25 GMT   (2856kb,D)

Title: Application of Transformers for Nonlinear Channel Compensation in
  Optical Systems
Authors: Behnam Behinaein Hamgini, Hossein Najafi, Ali Bakhshali, and Zhuhong
  Zhang
Categories: cs.IT cs.LG eess.SP math.IT
\\ ( https://arxiv.org/abs/2304.13119 ,  2856kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19394 (*cross-listing*)
replaced with revised version Mon, 4 Mar 2024 20:22:16 GMT   (385kb,D)

Title: Synaptic Weight Distributions Depend on the Geometry of Plasticity
Authors: Roman Pogodin, Jonathan Cornford, Arna Ghosh, Gauthier Gidel,
  Guillaume Lajoie, Blake Richards
Categories: q-bio.NC cs.LG cs.NE
Comments: ICLR 2024
Journal-ref: The Twelfth International Conference on Learning Representations,
  2024
\\ ( https://arxiv.org/abs/2305.19394 ,  385kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02775 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 16:44:43 GMT   (761kb,D)

Title: Input-gradient space particle inference for neural network ensembles
Authors: Trung Trinh, Markus Heinonen, Luigi Acerbi, Samuel Kaski
Categories: stat.ML cs.LG
Comments: Published at ICLR 2024 (spotlight presentation). Code is available at
  https://github.com/AaltoPML/FoRDE
\\ ( https://arxiv.org/abs/2306.02775 ,  761kb)
------------------------------------------------------------------------------
\\
arXiv:2306.07948
replaced with revised version Tue, 5 Mar 2024 16:09:44 GMT   (154kb,AD)

Title: Optimal Inference in Contextual Stochastic Block Models
Authors: O. Duranthon and L. Zdeborov\'a
Categories: cs.SI cs.LG
Journal-ref: TMLR 2024
\\ ( https://arxiv.org/abs/2306.07948 ,  154kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13991
replaced with revised version Tue, 5 Mar 2024 03:43:32 GMT   (4995kb,D)

Title: METAVerse: Meta-Learning Traversability Cost Map for Off-Road Navigation
Authors: Junwon Seo, Taekyung Kim, Seongyong Ahn, Kiho Kwak
Categories: cs.RO cs.CV cs.LG
Comments: Our video can be found at https://youtu.be/4rIAMM1ZKMo
\\ ( https://arxiv.org/abs/2307.13991 ,  4995kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01690 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 15:03:48 GMT   (2081kb,D)

Title: Forecasting Tropical Cyclones with Cascaded Diffusion Models
Authors: Pritthijit Nath, Pancham Shukla, Shuai Wang, C\'esar Quilodr\'an-Casas
Categories: physics.ao-ph cs.LG
Comments: Accepted for poster presentation at the ICLR 2024 workshop on
  Tackling Climate Change with Machine Learning. 7 pages, 3 figures
\\ ( https://arxiv.org/abs/2310.01690 ,  2081kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11230 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 13:59:16 GMT   (317kb,D)

Title: Zipformer: A faster and better encoder for automatic speech recognition
Authors: Zengwei Yao, Liyong Guo, Xiaoyu Yang, Wei Kang, Fangjun Kuang, Yifan
  Yang, Zengrui Jin, Long Lin, Daniel Povey
Categories: eess.AS cs.LG cs.SD
Comments: Published as a conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2310.11230 ,  317kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19794 (*cross-listing*)
replaced with revised version Mon, 4 Mar 2024 22:32:38 GMT   (1148kb,D)

Title: Robust Causal Bandits for Linear Models
Authors: Zirui Yan, Arpan Mukherjee, Burak Var{\i}c{\i}, Ali Tajer
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2310.19794 ,  1148kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03131 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 10:25:03 GMT   (5382kb,D)

Title: Decoding Neuronal Networks: A Reservoir Computing Approach for
  Predicting Connectivity and Functionality
Authors: Ilya Auslender, Giorgio Letti, Yasaman Heydari, Clara Zaccaria,
  Lorenzo Pavesi
Categories: q-bio.QM cs.LG physics.bio-ph
Comments: Submitted version
\\ ( https://arxiv.org/abs/2311.03131 ,  5382kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09354 (*cross-listing*)
replaced with revised version Mon, 4 Mar 2024 21:43:02 GMT   (10749kb)

Title: Nondestructive, quantitative viability analysis of 3D tissue cultures
  using machine learning image segmentation
Authors: Kylie J. Trettner, Jeremy Hsieh, Weikun Xiao, Jerry S.H. Lee, Andrea
  M. Armani
Categories: q-bio.QM cs.LG eess.IV
Comments: 52 total pages, Main text and SI included, 35 figures (5 main text,
  30 supplemental), 9 tables, 6 datasets (provided on linked GitHub), linked
  image files on Zenodo
\\ ( https://arxiv.org/abs/2311.09354 ,  10749kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11772
replaced with revised version Tue, 5 Mar 2024 17:56:20 GMT   (13659kb,D)

Title: A Good Feature Extractor Is All You Need for Weakly Supervised Pathology
  Slide Classification
Authors: Georg W\"olflein, Dyke Ferber, Asier Rabasco Meneghetti, Omar S. M. El
  Nahhas, Daniel Truhn, Zunamys I. Carrero, David J. Harrison, Ognjen
  Arandjelovi\'c, Jakob N. Kather
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2311.11772 ,  13659kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18377 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 10:23:33 GMT   (1404kb)

Title: Transfer Learning across Different Chemical Domains: Virtual Screening
  of Organic Materials with Deep Learning Models Pretrained on Small Molecule
  and Chemical Reaction Data
Authors: Chengwei Zhang, Yushuang Zhai, Ziyang Gong, Hongliang Duan, Yuan-Bin
  She, Yun-Fang Yang, An Su
Categories: physics.chem-ph cs.LG q-bio.BM
\\ ( https://arxiv.org/abs/2311.18377 ,  1404kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01659
replaced with revised version Tue, 5 Mar 2024 06:39:32 GMT   (40950kb,D)

Title: RiskBench: A Scenario-based Benchmark for Risk Identification
Authors: Chi-Hsi Kung, Chieh-Chi Yang, Pang-Yuan Pao, Shu-Wei Lu, Pin-Lun Chen,
  Hsin-Cheng Lu, Yi-Ting Chen
Categories: cs.CV cs.LG cs.RO
\\ ( https://arxiv.org/abs/2312.01659 ,  40950kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03968 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 04:45:14 GMT   (7296kb,D)

Title: scDiffusion: conditional generation of high-quality single-cell data
  using diffusion model
Authors: Erpai Luo, Minsheng Hao, Lei Wei, Xuegong Zhang
Categories: q-bio.QM cs.LG q-bio.GN
\\ ( https://arxiv.org/abs/2401.03968 ,  7296kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05765 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 10:48:22 GMT   (945kb,D)

Title: A new computationally efficient algorithm to solve Feature Selection for
  Functional Data Classification in high-dimensional spaces
Authors: Tobia Boschi, Francesca Bonin, Rodrigo Ordonez-Hurtado, Alessandra
  Pascale, and Jonathan Epperlein
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2401.05765 ,  945kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09356
replaced with revised version Mon, 4 Mar 2024 22:29:25 GMT   (1160kb,D)

Title: Swing: Short-cutting Rings for Higher Bandwidth Allreduce
Authors: Daniele De Sensi and Tommaso Bonato and David Saam and Torsten Hoefler
Categories: cs.DC cs.LG cs.NI cs.PF
ACM-class: C.2.4; C.2.2
Journal-ref: NSDI 2024
\\ ( https://arxiv.org/abs/2401.09356 ,  1160kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14074
replaced with revised version Tue, 5 Mar 2024 07:37:18 GMT   (2752kb,D)

Title: ProCNS: Progressive Prototype Calibration and Noise Suppression for
  Weakly-Supervised Medical Image Segmentation
Authors: Y. Liu, L. Lin, K. K. Y. Wong, X. Tang
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2401.14074 ,  2752kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01831
replaced with revised version Mon, 4 Mar 2024 23:43:42 GMT   (1119kb,D)

Title: Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and
  Dialogue Abilities
Authors: Zhifeng Kong, Arushi Goel, Rohan Badlani, Wei Ping, Rafael Valle,
  Bryan Catanzaro
Categories: cs.SD cs.LG eess.AS
\\ ( https://arxiv.org/abs/2402.01831 ,  1119kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11425 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 04:55:15 GMT   (115kb)

Title: Online Local False Discovery Rate Control: A Resource Allocation
  Approach
Authors: Ruicheng Ao, Hongyu Chen, David Simchi-Levi, Feng Zhu
Categories: stat.ME cs.LG math.OC math.PR
\\ ( https://arxiv.org/abs/2402.11425 ,  115kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12426
replaced with revised version Tue, 5 Mar 2024 16:31:53 GMT   (627kb)

Title: Attacks on Node Attributes in Graph Neural Networks
Authors: Ying Xu, Michael Lanier, Anindya Sarkar, Yevgeniy Vorobeychik
Categories: cs.SI cs.CR cs.LG
Comments: Accepted to AAAI 2024 AICS workshop
\\ ( https://arxiv.org/abs/2402.12426 ,  627kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14396 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 13:39:58 GMT   (1752kb,D)

Title: Quantum Circuit Optimization with AlphaTensor
Authors: Francisco J. R. Ruiz, Tuomas Laakkonen, Johannes Bausch, Matej Balog,
  Mohammadamin Barekatain, Francisco J. H. Heras, Alexander Novikov, Nathan
  Fitzpatrick, Bernardino Romera-Paredes, John van de Wetering, Alhussein
  Fawzi, Konstantinos Meichanetzidis, Pushmeet Kohli
Categories: quant-ph cs.LG
Comments: 25 pages main paper + 19 pages appendix
\\ ( https://arxiv.org/abs/2402.14396 ,  1752kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15919
replaced with revised version Mon, 4 Mar 2024 22:42:28 GMT   (22941kb,D)

Title: Learning to See Through Dazzle
Authors: Xiaopeng Peng, Erin F. Fleet, Abbie T. Watnik, Grover A. Swartzlander
Categories: cs.CV cs.GR cs.LG eess.IV
\\ ( https://arxiv.org/abs/2402.15919 ,  22941kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18830 (*cross-listing*)
replaced with revised version Tue, 5 Mar 2024 12:39:23 GMT   (10868kb,D)

Title: Training-set-free two-stage deep learning for spectroscopic data
  de-noising
Authors: Dongchen Huang, Junde Liu, Tian Qian, and Hongming Weng
Categories: cond-mat.mtrl-sci cs.LG physics.data-an
\\ ( https://arxiv.org/abs/2402.18830 ,  10868kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01537
replaced with revised version Tue, 5 Mar 2024 15:48:15 GMT   (8609kb,D)

Title: Mixed-Strategy Nash Equilibrium for Crowd Navigation
Authors: Muchen Sun, Francesca Baldini, Peter Trautman, Todd Murphey
Categories: cs.RO cs.GT cs.LG
\\ ( https://arxiv.org/abs/2403.01537 ,  8609kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01944
replaced with revised version Tue, 5 Mar 2024 08:43:31 GMT   (4764kb,D)

Title: Fourier-basis Functions to Bridge Augmentation Gap: Rethinking Frequency
  Augmentation in Image Classification
Authors: Puru Vaish, Shunxin Wang and Nicola Strisciuglio
Categories: cs.CV cs.LG
Comments: Accepted at CVPR 2024
\\ ( https://arxiv.org/abs/2403.01944 ,  4764kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
