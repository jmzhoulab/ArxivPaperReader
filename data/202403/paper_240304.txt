paper_240304.txt

Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月4日 20:08
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Thu 29 Feb 24 19:00:00 GMT  to  Fri  1 Mar 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.00284
Date: Fri, 1 Mar 2024 05:04:00 GMT   (12682kb,D)

Title: A Survey of Route Recommendations: Methods, Applications, and
  Opportunities
Authors: Shiming Zhang, Zhipeng Luo, Li Yang, Fei Teng, Tianrui Li
Categories: cs.AI cs.LG
Comments: 21 pages, 8 figures
\\
  Nowadays, with advanced information technologies deployed citywide, large
data volumes and powerful computational resources are intelligentizing modern
city development. As an important part of intelligent transportation, route
recommendation and its applications are widely used, directly influencing
citizens` travel habits. Developing smart and efficient travel routes based on
big data (possibly multi-modal) has become a central challenge in route
recommendation research. Our survey offers a comprehensive review of route
recommendation work based on urban computing. It is organized by the following
three parts: 1) Methodology-wise. We categorize a large volume of traditional
machine learning and modern deep learning methods. Also, we discuss their
historical relations and reveal the edge-cutting progress. 2)
Application\-wise. We present numerous novel applications related to route
commendation within urban computing scenarios. 3) We discuss current problems
and challenges and envision several promising research directions. We believe
that this survey can help relevant researchers quickly familiarize themselves
with the current state of route recommendation research and then direct them to
future research trends.
\\ ( https://arxiv.org/abs/2403.00284 ,  12682kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00315
Date: Fri, 1 Mar 2024 06:28:53 GMT   (435kb)

Title: Axe the X in XAI: A Plea for Understandable AI
Authors: Andr\'es P\'aez
Categories: cs.AI cs.LG
\\
  In a recent paper, Erasmus et al. (2021) defend the idea that the ambiguity
of the term "explanation" in explainable AI (XAI) can be solved by adopting any
of four different extant accounts of explanation in the philosophy of science:
the Deductive Nomological, Inductive Statistical, Causal Mechanical, and New
Mechanist models. In this chapter, I show that the authors' claim that these
accounts can be applied to deep neural networks as they would to any natural
phenomenon is mistaken. I also provide a more general argument as to why the
notion of explainability as it is currently used in the XAI literature bears
little resemblance to the traditional concept of scientific explanation. It
would be more fruitful to use the label "understandable AI" to avoid the
confusion that surrounds the goal and purposes of XAI. In the second half of
the chapter, I argue for a pragmatic conception of understanding that is better
suited to play the central role attributed to explanation in XAI. Following
Kuorikoski & Ylikoski (2015), the conditions of satisfaction for understanding
an ML system are fleshed out in terms of an agent's success in using the
system, in drawing correct inferences from it.
\\ ( https://arxiv.org/abs/2403.00315 ,  435kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00318
Date: Fri, 1 Mar 2024 06:40:02 GMT   (296kb,D)

Title: Deep Reinforcement Learning for Solving Management Problems: Towards A
  Large Management Mode
Authors: Jinyang Jiang, Xiaotian Liu, Tao Ren, Qinghao Wang, Yi Zheng, Yufu Du,
  Yijie Peng and Cheng Zhang
Categories: cs.AI cs.LG
\\
  We introduce a deep reinforcement learning (DRL) approach for solving
management problems including inventory management, dynamic pricing, and
recommendation. This DRL approach has the potential to lead to a large
management model based on certain transformer neural network structures,
resulting in an artificial general intelligence paradigm for various management
tasks. Traditional methods have limitations for solving complex real-world
problems, and we demonstrate how DRL can surpass existing heuristic approaches
for solving management tasks. We aim to solve the problems in a unified
framework, considering the interconnections between different tasks. Central to
our methodology is the development of a foundational decision model
coordinating decisions across the different domains through generative
decision-making. Our experimental results affirm the effectiveness of our
DRL-based framework in complex and dynamic business environments. This work
opens new pathways for the application of DRL in management problems,
highlighting its potential to revolutionize traditional business management.
\\ ( https://arxiv.org/abs/2403.00318 ,  296kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00323
Date: Fri, 1 Mar 2024 06:57:09 GMT   (3271kb,D)

Title: Softened Symbol Grounding for Neuro-symbolic Systems
Authors: Zenan Li, Yuan Yao, Taolue Chen, Jingwei Xu, Chun Cao, Xiaoxing Ma,
  Jian L\"u
Categories: cs.AI cs.LG
Comments: Published as a conference paper at ICLR 2023. Code is available at
  https://github.com/SoftWiser-group/Soften-NeSy-learning
\\
  Neuro-symbolic learning generally consists of two separated worlds, i.e.,
neural network training and symbolic constraint solving, whose success hinges
on symbol grounding, a fundamental problem in AI. This paper presents a novel,
softened symbol grounding process, bridging the gap between the two worlds, and
resulting in an effective and efficient neuro-symbolic learning framework.
Technically, the framework features (1) modeling of symbol solution states as a
Boltzmann distribution, which avoids expensive state searching and facilitates
mutually beneficial interactions between network training and symbolic
reasoning;(2) a new MCMC technique leveraging projection and SMT solvers, which
efficiently samples from disconnected symbol solution spaces; (3) an annealing
mechanism that can escape from %being trapped into sub-optimal symbol
groundings. Experiments with three representative neuro symbolic learning tasks
demonstrate that, owining to its superior symbol grounding capability, our
framework successfully solves problems well beyond the frontier of the existing
proposals.
\\ ( https://arxiv.org/abs/2403.00323 ,  3271kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00329
Date: Fri, 1 Mar 2024 07:17:20 GMT   (4283kb,D)

Title: Learning with Logical Constraints but without Shortcut Satisfaction
Authors: Zenan Li, Zehua Liu, Yuan Yao, Jingwei Xu, Taolue Chen, Xiaoxing Ma,
  Jian L\"u
Categories: cs.AI cs.LG
Comments: Published as a conference paper at ICLR 2023, and code is available
  at https://github.com/SoftWiser-group/NeSy-without-Shortcuts
\\
  Recent studies in neuro-symbolic learning have explored the integration of
logical knowledge into deep learning via encoding logical constraints as an
additional loss function. However, existing approaches tend to vacuously
satisfy logical constraints through shortcuts, failing to fully exploit the
knowledge. In this paper, we present a new framework for learning with logical
constraints. Specifically, we address the shortcut satisfaction issue by
introducing dual variables for logical connectives, encoding how the constraint
is satisfied. We further propose a variational framework where the encoded
logical constraint is expressed as a distributional loss that is compatible
with the model's original training loss. The theoretical analysis shows that
the proposed approach bears salient properties, and the experimental
evaluations demonstrate its superior performance in both model generalizability
and constraint satisfaction.
\\ ( https://arxiv.org/abs/2403.00329 ,  4283kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00685
Date: Fri, 1 Mar 2024 17:19:35 GMT   (48kb)

Title: Know your exceptions: Towards an Ontology of Exceptions in Knowledge
  Representation
Authors: Gabriele Sacco, Loris Bozzato, Oliver Kutz
Categories: cs.AI
Comments: 18 pages, 4 pages are appendix
ACM-class: I.2.4
\\
  Defeasible reasoning is a kind of reasoning where some generalisations may
not be valid in all circumstances, that is general conclusions may fail in some
cases. Various formalisms have been developed to model this kind of reasoning,
which is characteristic of common-sense contexts. However, it is not easy for a
modeller to choose among these systems the one that better fits its domain from
an ontological point of view. In this paper we first propose a framework based
on the notions of exceptionality and defeasibility in order to be able to
compare formalisms and reveal their ontological commitments. Then, we apply
this framework to compare four systems, showing the differences that may occur
from an ontological perspective.
\\ ( https://arxiv.org/abs/2403.00685 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00690
Date: Fri, 1 Mar 2024 17:22:16 GMT   (1435kb,D)

Title: Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents
Authors: Dominik Jeurissen and Diego Perez-Liebana and Jeremy Gow and Duygu
  Cakmak and James Kwan
Categories: cs.AI
\\
  Large Language Models (LLMs) have shown great success as high-level planners
for zero-shot game-playing agents. However, these agents are primarily
evaluated on Minecraft, where long-term planning is relatively straightforward.
In contrast, agents tested in dynamic robot environments face limitations due
to simplistic environments with only a few objects and interactions. To fill
this gap in the literature, we present NetPlay, the first LLM-powered zero-shot
agent for the challenging roguelike NetHack. NetHack is a particularly
challenging environment due to its diverse set of items and monsters, complex
interactions, and many ways to die.
  NetPlay uses an architecture designed for dynamic robot environments,
modified for NetHack. Like previous approaches, it prompts the LLM to choose
from predefined skills and tracks past interactions to enhance decision-making.
Given NetHack's unpredictable nature, NetPlay detects important game events to
interrupt running skills, enabling it to react to unforeseen circumstances.
While NetPlay demonstrates considerable flexibility and proficiency in
interacting with NetHack's mechanics, it struggles with ambiguous task
descriptions and a lack of explicit feedback. Our findings demonstrate that
NetPlay performs best with detailed context information, indicating the
necessity for dynamic methods in supplying context information for complex
games such as NetHack.
\\ ( https://arxiv.org/abs/2403.00690 ,  1435kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00067
Date: Thu, 29 Feb 2024 19:00:47 GMT   (7184kb,D)

Title: Query-OPT: Optimizing Inference of Large Language Models via Multi-Query
  Instructions in Meeting Summarization
Authors: Md Tahmid Rahman Laskar, Elena Khasanova, Xue-Yong Fu, Cheng Chen,
  Shashi Bhushan TN
Categories: cs.CL
\\
  This work focuses on the task of query-based meeting summarization in which
the summary of a context (meeting transcript) is generated in response to a
specific query. When using Large Language Models (LLMs) for this task, a new
call to the LLM inference endpoint/API is required for each new query even if
the context stays the same. However, repeated calls to the LLM inference
endpoints would significantly increase the costs of using them in production,
making LLMs impractical for many real-world use cases. To address this problem,
in this paper, we investigate whether combining the queries for the same input
context in a single prompt to minimize repeated calls can be successfully used
in meeting summarization. In this regard, we conduct extensive experiments by
comparing the performance of various popular LLMs: GPT-4, PaLM-2, LLaMA-2,
Mistral, and FLAN-T5 in single-query and multi-query settings. We observe that
while most LLMs tend to respond to the multi-query instructions, almost all of
them (except GPT-4), even after fine-tuning, could not properly generate the
response in the required output format. We conclude that while multi-query
prompting could be useful to optimize the inference costs by reducing calls to
the inference endpoints/APIs for the task of meeting summarization, this
capability to reliably generate the response in the expected format is only
limited to certain LLMs.
\\ ( https://arxiv.org/abs/2403.00067 ,  7184kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00071
Date: Thu, 29 Feb 2024 19:02:03 GMT   (845kb,D)

Title: Resonance RoPE: Improving Context Length Generalization of Large
  Language Models
Authors: Suyuchen Wang, Ivan Kobyzev, Peng Lu, Mehdi Rezagholizadeh, Bang Liu
Categories: cs.CL cs.AI
\\
  This paper addresses the challenge of train-short-test-long (TSTL) scenarios
in Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE),
where models pre-trained on shorter sequences face difficulty with
out-of-distribution (OOD) token positions in longer sequences. We introduce
Resonance RoPE, a novel approach designed to narrow the generalization gap in
TSTL scenarios by refining the interpolation of RoPE features for OOD
positions, significantly improving the model performance without additional
online computational costs. Furthermore, we present PosGen, a new synthetic
benchmark specifically designed for fine-grained behavior analysis in TSTL
scenarios, aiming to isolate the constantly increasing difficulty of token
generation on long contexts from the challenges of recognizing new token
positions. Our experiments on synthetic tasks show that after applying
Resonance RoPE, Transformers recognize OOD position better and more robustly.
Our extensive LLM experiments also show superior performance after applying
Resonance RoPE to the current state-of-the-art RoPE scaling method, YaRN, on
both upstream language modeling tasks and a variety of downstream long-text
applications.
\\ ( https://arxiv.org/abs/2403.00071 ,  845kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00092
Date: Thu, 29 Feb 2024 19:40:25 GMT   (874kb,D)

Title: PROC2PDDL: Open-Domain Planning Representations from Texts
Authors: Tianyi Zhang, Li Zhang, Zhaoyi Hou, Ziyu Wang, Yuling Gu, Peter Clark,
  Chris Callison-Burch, Niket Tandon
Categories: cs.CL
\\
  Planning in a text-based environment continues to be a major challenge for AI
systems. Recent approaches have used language models to predict a planning
domain definition (e.g., PDDL) but have only been evaluated in closed-domain
simulated environments. To address this, we present Proc2PDDL , the first
dataset containing open-domain procedural texts paired with expert-annotated
PDDL representations. Using this dataset, we evaluate state-of-the-art models
on defining the preconditions and effects of actions. We show that Proc2PDDL is
highly challenging, with GPT-3.5's success rate close to 0% and GPT-4's around
35%. Our analysis shows both syntactic and semantic errors, indicating LMs'
deficiency in both generating domain-specific prgorams and reasoning about
events. We hope this analysis and dataset helps future progress towards
integrating the best of LMs and formal planning.
\\ ( https://arxiv.org/abs/2403.00092 ,  874kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00126
Date: Thu, 29 Feb 2024 21:05:37 GMT   (4421kb,D)

Title: FAC$^2$E: Better Understanding Large Language Model Capabilities by
  Dissociating Language and Cognition
Authors: Xiaoqiang Wang, Bang Liu, and Lingfei Wu
Categories: cs.CL
Comments: Work in Progress
\\
  Large language models (LLMs) are primarily evaluated by overall performance
on various text understanding and generation tasks. However, such a paradigm
fails to comprehensively differentiate the fine-grained language and cognitive
skills, rendering the lack of sufficient interpretation to LLMs' capabilities.
In this paper, we present FAC$^2$E, a framework for Fine-grAined and
Cognition-grounded LLMs' Capability Evaluation. Specifically, we formulate
LLMs' evaluation in a multi-dimensional and explainable manner by dissociating
the language-related capabilities and the cognition-related ones. Besides,
through extracting the intermediate reasoning from LLMs, we further break down
the process of applying a specific capability into three sub-steps: recalling
relevant knowledge, utilizing knowledge, and solving problems. Finally,
FAC$^2$E evaluates each sub-step of each fine-grained capability, providing a
two-faceted diagnosis for LLMs. Utilizing FAC$^2$E, we identify a common
shortfall in knowledge utilization among models and propose a straightforward,
knowledge-enhanced method to mitigate this issue. Our results not only showcase
promising performance enhancements but also highlight a direction for future
LLM advancements.
\\ ( https://arxiv.org/abs/2403.00126 ,  4421kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00127
Date: Thu, 29 Feb 2024 21:05:38 GMT   (274kb)

Title: Prompting ChatGPT for Translation: A Comparative Analysis of Translation
  Brief and Persona Prompts
Authors: Sui He
Categories: cs.CL cs.CY cs.HC
\\
  Prompt engineering in LLMs has shown potential for improving translation
quality. However, the potential of incorporating translation concepts in prompt
design remains largely underexplored. Against this backdrop, this paper
discusses the effectiveness of incorporating the conceptual tool of translation
brief and the personas of translator and author into prompt design for
translation tasks in ChatGPT. Findings suggest that, although certain elements
are constructive in facilitating human to human communication for translation
tasks, their effectiveness is limited for improving translation quality in
ChatGPT. This accentuates the need for more explorative research on how
translation theorists and practitioners can develop the current set of
conceptual tools rooted in the human to human communication paradigm for
translation purposes in this emerging workflow involving human machine
interaction.
\\ ( https://arxiv.org/abs/2403.00127 ,  274kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00141
Date: Thu, 29 Feb 2024 21:44:50 GMT   (1169kb,D)

Title: EROS: Entity-Driven Controlled Policy Document Summarization
Authors: Joykirat Singh, Sehban Fazili, Rohan Jain, Md Shad Akhtar
Categories: cs.CL cs.AI
Comments: Accepted in LREC-COLING 2024
\\
  Privacy policy documents have a crucial role in educating individuals about
the collection, usage, and protection of users' personal data by organizations.
However, they are notorious for their lengthy, complex, and convoluted language
especially involving privacy-related entities. Hence, they pose a significant
challenge to users who attempt to comprehend organization's data usage policy.
In this paper, we propose to enhance the interpretability and readability of
policy documents by using controlled abstractive summarization -- we enforce
the generated summaries to include critical privacy-related entities (e.g.,
data and medium) and organization's rationale (e.g.,target and reason) in
collecting those entities. To achieve this, we develop PD-Sum, a
policy-document summarization dataset with marked privacy-related entity
labels. Our proposed model, EROS, identifies critical entities through a
span-based entity extraction model and employs them to control the information
content of the summaries using proximal policy optimization (PPO). Comparison
shows encouraging improvement over various baselines. Furthermore, we furnish
qualitative and human evaluations to establish the efficacy of EROS.
\\ ( https://arxiv.org/abs/2403.00141 ,  1169kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00143
Date: Thu, 29 Feb 2024 21:49:31 GMT   (408kb,D)

Title: Ensemble-Based Unsupervised Discontinuous Constituency Parsing by Tree
  Averaging
Authors: Behzad Shayegh, Yuqiao Wen, Lili Mou
Categories: cs.CL cs.AI cs.LG
\\
  We address unsupervised discontinuous constituency parsing, where we observe
a high variance in the performance of the only previous model. We propose to
build an ensemble of different runs of the existing discontinuous parser by
averaging the predicted trees, to stabilize and boost performance. To begin
with, we provide comprehensive computational complexity analysis (in terms of P
and NP-complete) for tree averaging under different setups of binarity and
continuity. We then develop an efficient exact algorithm to tackle the task,
which runs in a reasonable time for all samples in our experiments. Results on
three datasets show our method outperforms all baselines in all metrics; we
also provide in-depth analyses of our approach.
\\ ( https://arxiv.org/abs/2403.00143 ,  408kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00144
Date: Thu, 29 Feb 2024 21:49:31 GMT   (2794kb,D)

Title: EBBS: An Ensemble with Bi-Level Beam Search for Zero-Shot Machine
  Translation
Authors: Yuqiao Wen, Behzad Shayegh, Chenyang Huang, Yanshuai Cao, Lili Mou
Categories: cs.CL cs.AI cs.LG
ACM-class: I.2.7; I.2.6; I.2.m; I.5.1; I.7.m
\\
  The ability of zero-shot translation emerges when we train a multilingual
model with certain translation directions; the model can then directly
translate in unseen directions. Alternatively, zero-shot translation can be
accomplished by pivoting through a third language (e.g., English). In our work,
we observe that both direct and pivot translations are noisy and achieve less
satisfactory performance. We propose EBBS, an ensemble method with a novel
bi-level beam search algorithm, where each ensemble component explores its own
prediction step by step at the lower level but they are synchronized by a "soft
voting" mechanism at the upper level. Results on two popular multilingual
translation datasets show that EBBS consistently outperforms direct and pivot
translations as well as existing ensemble techniques. Further, we can distill
the ensemble's knowledge back to the multilingual model to improve inference
efficiency; profoundly, our EBBS-based distillation does not sacrifice, or even
improves, the translation quality.
\\ ( https://arxiv.org/abs/2403.00144 ,  2794kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00165
Date: Thu, 29 Feb 2024 22:26:07 GMT   (285kb,D)

Title: TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text
  Classification with Minimal Supervision
Authors: Yunyi Zhang, Ruozhen Yang, Xueqiang Xu, Jinfeng Xiao, Jiaming Shen,
  Jiawei Han
Categories: cs.CL cs.LG
\\
  Hierarchical text classification aims to categorize each document into a set
of classes in a label taxonomy. Most earlier works focus on fully or
semi-supervised methods that require a large amount of human annotated data
which is costly and time-consuming to acquire. To alleviate human efforts, in
this paper, we work on hierarchical text classification with the minimal amount
of supervision: using the sole class name of each node as the only supervision.
Recently, large language models (LLM) show competitive performance on various
tasks through zero-shot prompting, but this method performs poorly in the
hierarchical setting, because it is ineffective to include the large and
structured label space in a prompt. On the other hand, previous
weakly-supervised hierarchical text classification methods only utilize the raw
taxonomy skeleton and ignore the rich information hidden in the text corpus
that can serve as additional class-indicative features. To tackle the above
challenges, we propose TELEClass, Taxonomy Enrichment and LLM-Enhanced
weakly-supervised hierarchical text classification, which (1) automatically
enriches the label taxonomy with class-indicative topical terms mined from the
corpus to facilitate classifier training and (2) utilizes LLMs for both data
annotation and creation tailored for the hierarchical label space. Experiments
show that TELEClass can outperform previous weakly-supervised hierarchical text
classification methods and LLM-based zero-shot prompting methods on two public
datasets.
\\ ( https://arxiv.org/abs/2403.00165 ,  285kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00180
Date: Thu, 29 Feb 2024 23:11:55 GMT   (1828kb,D)

Title: "Flex Tape Can't Fix That": Bias and Misinformation in Edited Language
  Models
Authors: Karina Halevy, Anna Sotnikova, Badr AlKhamissi, Syrielle Montariol,
  Antoine Bosselut
Categories: cs.CL
Comments: 8 pages, 3 figures
\\
  Model editing has emerged as a cost-effective strategy to update knowledge
stored in language models. However, model editing can have unintended
consequences after edits are applied: information unrelated to the edits can
also be changed, and other general behaviors of the model can be wrongly
altered. In this work, we investigate how model editing methods unexpectedly
amplify model biases post-edit. We introduce a novel benchmark dataset,
Seesaw-CF, for measuring bias-related harms of model editing and conduct the
first in-depth investigation of how different weight-editing methods impact
model bias. Specifically, we focus on biases with respect to demographic
attributes such as race, geographic origin, and gender, as well as qualitative
flaws in long-form texts generated by edited language models. We find that
edited models exhibit, to various degrees, more biased behavior as they become
less confident in attributes for Asian, African, and South American subjects.
Furthermore, edited models amplify sexism and xenophobia in text generations
while remaining seemingly coherent and logical. Finally, editing facts about
place of birth, country of citizenship, or gender have particularly negative
effects on the model's knowledge about unrelated features like field of work.
\\ ( https://arxiv.org/abs/2403.00180 ,  1828kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00198
Date: Fri, 1 Mar 2024 00:02:37 GMT   (9088kb,D)

Title: AXOLOTL: Fairness through Assisted Self-Debiasing of Large Language
  Model Outputs
Authors: Sana Ebrahimi, Kaiwen Chen, Abolfazl Asudeh, Gautam Das, Nick Koudas
Categories: cs.CL cs.AI cs.CY cs.LG
\\
  Pre-trained Large Language Models (LLMs) have significantly advanced natural
language processing capabilities but are susceptible to biases present in their
training data, leading to unfair outcomes in various applications. While
numerous strategies have been proposed to mitigate bias, they often require
extensive computational resources and may compromise model performance. In this
work, we introduce AXOLOTL, a novel post-processing framework, which operates
agnostically across tasks and models, leveraging public APIs to interact with
LLMs without direct access to internal parameters. Through a three-step process
resembling zero-shot learning, AXOLOTL identifies biases, proposes resolutions,
and guides the model to self-debias its outputs. This approach minimizes
computational costs and preserves model performance, making AXOLOTL a promising
tool for debiasing LLM outputs with broad applicability and ease of use.
\\ ( https://arxiv.org/abs/2403.00198 ,  9088kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00199
Date: Fri, 1 Mar 2024 00:08:20 GMT   (406kb,D)

Title: Improving Socratic Question Generation using Data Augmentation and
  Preference Optimization
Authors: Nischal Ashok Kumar, Andrew Lan
Categories: cs.CL cs.CY cs.LG
\\
  The Socratic method is a way of guiding students toward solving a problem
independently without directly revealing the solution to the problem. Although
this method has been shown to significantly improve student learning outcomes,
it remains a complex labor-intensive task for instructors. Large language
models (LLMs) can be used to augment human effort by automatically generating
Socratic questions for students. However, existing methods that involve
prompting these LLMs sometimes produce invalid outputs, e.g., those that
directly reveal the solution to the problem or provide irrelevant or premature
questions. To alleviate this problem, inspired by reinforcement learning with
AI feedback (RLAIF), we first propose a data augmentation method to enrich
existing Socratic questioning datasets with questions that are invalid in
specific ways. Next, we propose a method to optimize open-source LLMs such as
LLama 2 to prefer ground-truth questions over generated invalid ones, using
direct preference optimization (DPO). Our experiments on a Socratic questions
dataset for student code debugging show that a DPO-optimized 7B LLama 2 model
can effectively avoid generating invalid questions, and as a result,
outperforms existing state-of-the-art prompting methods.
\\ ( https://arxiv.org/abs/2403.00199 ,  406kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00212
Date: Fri, 1 Mar 2024 01:15:45 GMT   (5903kb,D)

Title: Transcription and translation of videos using fine-tuned XLSR Wav2Vec2
  on custom dataset and mBART
Authors: Aniket Tathe, Anand Kamble, Suyash Kumbharkar, Atharva Bhandare,
  Anirban C. Mitra
Categories: cs.CL cs.CV cs.LG cs.SD eess.AS
\\
  This research addresses the challenge of training an ASR model for
personalized voices with minimal data. Utilizing just 14 minutes of custom
audio from a YouTube video, we employ Retrieval-Based Voice Conversion (RVC) to
create a custom Common Voice 16.0 corpus. Subsequently, a Cross-lingual
Self-supervised Representations (XLSR) Wav2Vec2 model is fine-tuned on this
dataset. The developed web-based GUI efficiently transcribes and translates
input Hindi videos. By integrating XLSR Wav2Vec2 and mBART, the system aligns
the translated text with the video timeline, delivering an accessible solution
for multilingual video content transcription and translation for personalized
voice.
\\ ( https://arxiv.org/abs/2403.00212 ,  5903kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00226
Date: Fri, 1 Mar 2024 02:09:25 GMT   (671kb,D)

Title: A Semantic Distance Metric Learning approach for Lexical Semantic Change
  Detection
Authors: Taichi Aida, Danushka Bollegala
Categories: cs.CL
\\
  Detecting temporal semantic changes of words is an important task for various
NLP applications that must make time-sensitive predictions. Lexical Semantic
Change Detection (SCD) task considers the problem of predicting whether a given
target word, $w$, changes its meaning between two different text corpora, $C_1$
and $C_2$. For this purpose, we propose a supervised two-staged SCD method that
uses existing Word-in-Context (WiC) datasets. In the first stage, for a target
word $w$, we learn two sense-aware encoder that represents the meaning of $w$
in a given sentence selected from a corpus. Next, in the second stage, we learn
a sense-aware distance metric that compares the semantic representations of a
target word across all of its occurrences in $C_1$ and $C_2$. Experimental
results on multiple benchmark datasets for SCD show that our proposed method
consistently outperforms all previously proposed SCD methods for multiple
languages, establishing a novel state-of-the-art for SCD. Interestingly, our
findings imply that there are specialised dimensions that carry information
related to semantic changes of words in the sense-aware embedding space. Source
code is available at https://github.com/a1da4/svp-sdml .
\\ ( https://arxiv.org/abs/2403.00226 ,  671kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00236
Date: Fri, 1 Mar 2024 02:33:26 GMT   (8739kb,D)

Title: Benchmarking zero-shot stance detection with FlanT5-XXL: Insights from
  training data, prompting, and decoding strategies into its near-SoTA
  performance
Authors: Rachith Aiyappa, Shruthi Senthilmani, Jisun An, Haewoon Kwak,
  Yong-Yeol Ahn
Categories: cs.CL cs.AI cs.LG
\\
  We investigate the performance of LLM-based zero-shot stance detection on
tweets. Using FlanT5-XXL, an instruction-tuned open-source LLM, with the
SemEval 2016 Tasks 6A, 6B, and P-Stance datasets, we study the performance and
its variations under different prompts and decoding strategies, as well as the
potential biases of the model. We show that the zero-shot approach can match or
outperform state-of-the-art benchmarks, including fine-tuned models. We provide
various insights into its performance including the sensitivity to instructions
and prompts, the decoding strategies, the perplexity of the prompts, and to
negations and oppositions present in prompts. Finally, we ensure that the LLM
has not been trained on test datasets, and identify a positivity bias which may
partially explain the performance differences across decoding strategie
\\ ( https://arxiv.org/abs/2403.00236 ,  8739kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00241
Date: Fri, 1 Mar 2024 03:07:32 GMT   (1477kb,D)

Title: CASIMIR: A Corpus of Scientific Articles enhanced with Multiple
  Author-Integrated Revisions
Authors: Leane Jourdan, Florian Boudin, Nicolas Hernandez, Richard Dufour
Categories: cs.CL
Comments: Accepted at LREC-Coling 2024
\\
  Writing a scientific article is a challenging task as it is a highly codified
and specific genre, consequently proficiency in written communication is
essential for effectively conveying research findings and ideas. In this
article, we propose an original textual resource on the revision step of the
writing process of scientific articles. This new dataset, called CASIMIR,
contains the multiple revised versions of 15,646 scientific articles from
OpenReview, along with their peer reviews. Pairs of consecutive versions of an
article are aligned at sentence-level while keeping paragraph location
information as metadata for supporting future revision studies at the discourse
level. Each pair of revised sentences is enriched with automatically extracted
edits and associated revision intention. To assess the initial quality on the
dataset, we conducted a qualitative study of several state-of-the-art text
revision approaches and compared various evaluation metrics. Our experiments
led us to question the relevance of the current evaluation methods for the text
revision task.
\\ ( https://arxiv.org/abs/2403.00241 ,  1477kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00252
Date: Fri, 1 Mar 2024 03:30:38 GMT   (8259kb,D)

Title: EUROPA: A Legal Multilingual Keyphrase Generation Dataset
Authors: Olivier Sala\"un, Fr\'ed\'eric Piedboeuf, Guillaume Le Berre, David
  Alfonso Hermelo and Philippe Langlais
Categories: cs.CL cs.AI cs.LG
Comments: 8 pages
\\
  Keyphrase generation has primarily been explored within the context of
academic research articles, with a particular focus on scientific domains and
the English language. In this work, we present EUROPA, a dataset for
multilingual keyphrase generation in the legal domain. It is derived from legal
judgments from the Court of Justice of the European Union (EU), and contains
instances in all 24 EU official languages. We run multilingual models on our
corpus and analyze the results, showing room for improvement on a
domain-specific multilingual corpus such as the one we present.
\\ ( https://arxiv.org/abs/2403.00252 ,  8259kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00260
Date: Fri, 1 Mar 2024 03:51:56 GMT   (578kb,D)

Title: Extracting Polymer Nanocomposite Samples from Full-Length Documents
Authors: Ghazal Khalighinejad, Defne Circi, L.C. Brinson, Bhuwan Dhingra
Categories: cs.CL
\\
  This paper investigates the use of large language models (LLMs) for
extracting sample lists of polymer nanocomposites (PNCs) from full-length
materials science research papers. The challenge lies in the complex nature of
PNC samples, which have numerous attributes scattered throughout the text. The
complexity of annotating detailed information on PNCs limits the availability
of data, making conventional document-level relation extraction techniques
impractical due to the challenge in creating comprehensive named entity span
annotations. To address this, we introduce a new benchmark and an evaluation
technique for this task and explore different prompting strategies in a
zero-shot manner. We also incorporate self-consistency to improve the
performance. Our findings show that even advanced LLMs struggle to extract all
of the samples from an article. Finally, we analyze the errors encountered in
this process, categorizing them into three main challenges, and discuss
potential strategies for future research to overcome them.
\\ ( https://arxiv.org/abs/2403.00260 ,  578kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00277
Date: Fri, 1 Mar 2024 04:47:16 GMT   (7192kb,D)

Title: Gender Bias in Large Language Models across Multiple Languages
Authors: Jinman Zhao, Yitian Ding, Chen Jia, Yining Wang, Zifan Qian
Categories: cs.CL
Comments: 20 pages, 27 tables, 7 figures, submitted to ACL2024
\\
  With the growing deployment of large language models (LLMs) across various
applications, assessing the influence of gender biases embedded in LLMs becomes
crucial. The topic of gender bias within the realm of natural language
processing (NLP) has gained considerable focus, particularly in the context of
English. Nonetheless, the investigation of gender bias in languages other than
English is still relatively under-explored and insufficiently analyzed. In this
work, We examine gender bias in LLMs-generated outputs for different languages.
We use three measurements: 1) gender bias in selecting descriptive words given
the gender-related context. 2) gender bias in selecting gender-related pronouns
(she/he) given the descriptive words. 3) gender bias in the topics of
LLM-generated dialogues. We investigate the outputs of the GPT series of LLMs
in various languages using our three measurement methods. Our findings revealed
significant gender biases across all the languages we examined.
\\ ( https://arxiv.org/abs/2403.00277 ,  7192kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00292
Date: Fri, 1 Mar 2024 05:28:06 GMT   (7483kb,D)

Title: DPP-Based Adversarial Prompt Searching for Lanugage Models
Authors: Xu Zhang and Xiaojun Wan
Categories: cs.CL
\\
  Language models risk generating mindless and offensive content, which hinders
their safe deployment. Therefore, it is crucial to discover and modify
potential toxic outputs of pre-trained language models before deployment. In
this work, we elicit toxic content by automatically searching for a prompt that
directs pre-trained language models towards the generation of a specific target
output. The problem is challenging due to the discrete nature of textual data
and the considerable computational resources required for a single forward pass
of the language model. To combat these challenges, we introduce Auto-regressive
Selective Replacement Ascent (ASRA), a discrete optimization algorithm that
selects prompts based on both quality and similarity with determinantal point
process (DPP). Experimental results on six different pre-trained language
models demonstrate the efficacy of ASRA for eliciting toxic content.
Furthermore, our analysis reveals a strong correlation between the success rate
of ASRA attacks and the perplexity of target outputs, while indicating limited
association with the quantity of model parameters.
\\ ( https://arxiv.org/abs/2403.00292 ,  7483kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00338
Date: Fri, 1 Mar 2024 08:05:44 GMT   (1960kb,D)

Title: Semi-Instruct: Bridging Natural-Instruct and Self-Instruct for Code
  Large Language Models
Authors: Xianzhen Luo, Qingfu Zhu, Zhiming Zhang, Xu Wang, Qing Yang, Dongliang
  Xu, Wanxiang Che
Categories: cs.CL
\\
  Instruction tuning plays a pivotal role in Code Large Language Models (Code
LLMs) for the task of program synthesis. Presently, two dominant paradigms for
collecting tuning data are natural-instruct (human-written) and self-instruct
(automatically generated). Natural-instruct includes diverse and correct codes
but lacks instruction-code pairs, and exists improper code formats like nested
single-line codes. In contrast, self-instruct automatically generates proper
paired data. However, it suffers from low diversity due to generating
duplicates and cannot ensure the correctness of codes. To bridge the both
paradigms, we propose \textbf{Semi-Instruct}. It first converts diverse but
improper codes from natural-instruct into proper instruction-code pairs through
a method similar to self-instruct. To verify the correctness of generated
codes, we design a novel way to construct test cases by generating cases'
inputs and executing correct codes from natural-instruct to get outputs.
Finally, diverse and correct instruction-code pairs are retained for
instruction tuning. Experiments show that semi-instruct is significantly better
than natural-instruct and self-instruct. Furthermore, the performance steadily
improves as data scale increases.
\\ ( https://arxiv.org/abs/2403.00338 ,  1960kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00354
Date: Fri, 1 Mar 2024 08:34:02 GMT   (2693kb,D)

Title: Self-Consistent Reasoning-based Aspect-Sentiment Quad Prediction with
  Extract-Then-Assign Strategy
Authors: Jieyong Kim, Ryang Heo, Yongsik Seo, SeongKu Kang, Jinyoung Yeo,
  Dongha Lee
Categories: cs.CL
\\
  In the task of aspect sentiment quad prediction (ASQP), generative methods
for predicting sentiment quads have shown promising results. However, they
still suffer from imprecise predictions and limited interpretability, caused by
data scarcity and inadequate modeling of the quadruplet composition process. In
this paper, we propose Self-Consistent Reasoning-based Aspect-sentiment
quadruple Prediction (SCRAP), optimizing its model to generate reasonings and
the corresponding sentiment quadruplets in sequence. SCRAP adopts the
Extract-Then-Assign reasoning strategy, which closely mimics human cognition.
In the end, SCRAP significantly improves the model's ability to handle complex
reasoning tasks and correctly predict quadruplets through consistency voting,
resulting in enhanced interpretability and accuracy in ASQP.
\\ ( https://arxiv.org/abs/2403.00354 ,  2693kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00370
Date: Fri, 1 Mar 2024 08:53:52 GMT   (1700kb,D)

Title: Post-decoder Biasing for End-to-End Speech Recognition of Multi-turn
  Medical Interview
Authors: Heyang Liu, Yu Wang and Yanfeng Wang
Categories: cs.CL cs.SD eess.AS
\\
  End-to-end (E2E) approach is gradually replacing hybrid models for automatic
speech recognition (ASR) tasks. However, the optimization of E2E models lacks
an intuitive method for handling decoding shifts, especially in scenarios with
a large number of domain-specific rare words that hold specific important
meanings. Furthermore, the absence of knowledge-intensive speech datasets in
academia has been a significant limiting factor, and the commonly used speech
corpora exhibit significant disparities with realistic conversation. To address
these challenges, we present Medical Interview (MED-IT), a multi-turn
consultation speech dataset that contains a substantial number of
knowledge-intensive named entities. We also explore methods to enhance the
recognition performance of rare words for E2E models. We propose a novel
approach, post-decoder biasing, which constructs a transform probability matrix
based on the distribution of training transcriptions. This guides the model to
prioritize recognizing words in the biasing list. In our experiments, for
subsets of rare words appearing in the training speech between 10 and 20 times,
and between 1 and 5 times, the proposed method achieves a relative improvement
of 9.3% and 5.1%, respectively.
\\ ( https://arxiv.org/abs/2403.00370 ,  1700kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00411
Date: Fri, 1 Mar 2024 09:57:46 GMT   (1195kb,D)

Title: Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with
  Fact-Checking in Turkish
Authors: Recep Firat Cekinel, Pinar Karagoz, Cagri Coltekin
Categories: cs.CL
Comments: LREC-COLING 2024
\\
  The rapid spread of misinformation through social media platforms has raised
concerns regarding its impact on public opinion. While misinformation is
prevalent in other languages, the majority of research in this field has
concentrated on the English language. Hence, there is a scarcity of datasets
for other languages, including Turkish. To address this concern, we have
introduced the FCTR dataset, consisting of 3238 real-world claims. This dataset
spans multiple domains and incorporates evidence collected from three Turkish
fact-checking organizations. Additionally, we aim to assess the effectiveness
of cross-lingual transfer learning for low-resource languages, with a
particular focus on Turkish. We demonstrate in-context learning (zero-shot and
few-shot) performance of large language models in this context. The
experimental results indicate that the dataset has the potential to advance
research in the Turkish language.
\\ ( https://arxiv.org/abs/2403.00411 ,  1195kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00417
Date: Fri, 1 Mar 2024 10:03:07 GMT   (610kb)

Title: Rethinking Tokenization: Crafting Better Tokenizers for Large Language
  Models
Authors: Jinbiao Yang
Categories: cs.CL
\\
  Tokenization significantly influences language models(LMs)' performance. This
paper traces the evolution of tokenizers from word-level to subword-level,
analyzing how they balance tokens and types to enhance model adaptability while
controlling complexity. Despite subword tokenizers like Byte Pair Encoding
(BPE) overcoming many word tokenizer limitations, they encounter difficulties
in handling non-Latin languages and depend heavily on extensive training data
and computational resources to grasp the nuances of multiword expressions
(MWEs). This article argues that tokenizers, more than mere technical tools,
should drawing inspiration from the cognitive science about human language
processing. This study then introduces the "Principle of Least Effort" from
cognitive science, that humans naturally seek to reduce cognitive effort, and
discusses the benefits of this principle for tokenizer development. Based on
this principle, the paper proposes that the Less-is-Better (LiB) model could be
a new approach for LLM tokenizer. The LiB model can autonomously learn an
integrated vocabulary consisting of subwords, words, and MWEs, which
effectively reduces both the numbers of tokens and types. Comparative
evaluations show that the LiB tokenizer outperforms existing word and BPE
tokenizers, presenting an innovative method for tokenizer development, and
hinting at the possibility of future cognitive science-based tokenizers being
more efficient.
\\ ( https://arxiv.org/abs/2403.00417 ,  610kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00418
Date: Fri, 1 Mar 2024 10:10:34 GMT   (7756kb,D)

Title: LLMs for Targeted Sentiment in News Headlines: Exploring Different
  Levels of Prompt Prescriptiveness
Authors: Jana Juro\v{s}, Laura Majer, Jan \v{S}najder
Categories: cs.CL
\\
  News headlines often evoke sentiment by intentionally portraying entities in
particular ways, making targeted sentiment analysis (TSA) of headlines a
worthwhile but difficult task. Fine-tuned encoder models show satisfactory TSA
performance, but their background knowledge is limited, and they require a
labeled dataset. LLMs offer a potentially universal solution for TSA due to
their broad linguistic and world knowledge along with in-context learning
abilities, yet their performance is heavily influenced by prompt design.
Drawing parallels with annotation paradigms for subjective tasks, we explore
the influence of prompt design on the performance of LLMs for TSA of news
headlines. We evaluate the predictive accuracy of state-of-the-art LLMs using
prompts with different levels of prescriptiveness, ranging from plain zero-shot
to elaborate few-shot prompts matching annotation guidelines. Recognizing the
subjective nature of TSA, we evaluate the ability of LLMs to quantify
predictive uncertainty via calibration error and correlation to human
inter-annotator agreement. We find that, except for few-shot prompting,
calibration and F1-score improve with increased prescriptiveness, but the
optimal level depends on the model.
\\ ( https://arxiv.org/abs/2403.00418 ,  7756kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00435
Date: Fri, 1 Mar 2024 10:38:07 GMT   (8791kb,D)

Title: Hierarchical Indexing for Retrieval-Augmented Opinion Summarization
Authors: Tom Hosking, Hao Tang, Mirella Lapata
Categories: cs.CL
Comments: Preprint
\\
  We propose a method for unsupervised abstractive opinion summarization, that
combines the attributability and scalability of extractive approaches with the
coherence and fluency of Large Language Models (LLMs). Our method, HIRO, learns
an index structure that maps sentences to a path through a semantically
organized discrete hierarchy. At inference time, we populate the index and use
it to identify and retrieve clusters of sentences containing popular opinions
from input reviews. Then, we use a pretrained LLM to generate a readable
summary that is grounded in these extracted evidential clusters. The modularity
of our approach allows us to evaluate its efficacy at each stage. We show that
HIRO learns an encoding space that is more semantically structured than prior
work, and generates summaries that are more representative of the opinions in
the input reviews. Human evaluation confirms that HIRO generates more coherent,
detailed and accurate summaries that are significantly preferred by annotators
compared to prior work.
\\ ( https://arxiv.org/abs/2403.00435 ,  8791kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00438
Date: Fri, 1 Mar 2024 10:47:02 GMT   (7810kb,D)

Title: Your Model Is Not Predicting Depression Well And That Is Why: A Case
  Study of PRIMATE Dataset
Authors: Kirill Milintsevich (1 and 2), Kairit Sirts (2), Ga\"el Dias (1) ((1)
  University of Caen Normandy, (2) University of Tartu)
Categories: cs.CL
\\
  This paper addresses the quality of annotations in mental health datasets
used for NLP-based depression level estimation from social media texts. While
previous research relies on social media-based datasets annotated with binary
categories, i.e. depressed or non-depressed, recent datasets such as D2S and
PRIMATE aim for nuanced annotations using PHQ-9 symptoms. However, most of
these datasets rely on crowd workers without the domain knowledge for
annotation. Focusing on the PRIMATE dataset, our study reveals concerns
regarding annotation validity, particularly for the lack of interest or
pleasure symptom. Through reannotation by a mental health professional, we
introduce finer labels and textual spans as evidence, identifying a notable
number of false positives. Our refined annotations, to be released under a Data
Use Agreement, offer a higher-quality test set for anhedonia detection. This
study underscores the necessity of addressing annotation quality issues in
mental health datasets, advocating for improved methodologies to enhance NLP
model reliability in mental health assessments.
\\ ( https://arxiv.org/abs/2403.00438 ,  7810kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00462
Date: Fri, 1 Mar 2024 11:33:53 GMT   (263kb,D)

Title: LUCID: LLM-Generated Utterances for Complex and Interesting Dialogues
Authors: Joe Stacey, Jianpeng Cheng, John Torr, Tristan Guigue, Joris Driesen,
  Alexandru Coca, Mark Gaynor, Anders Johannsen
Categories: cs.CL
ACM-class: I.2.7
\\
  Virtual assistants are poised to take a dramatic leap forward in terms of
their dialogue capabilities, spurred by recent advances in transformer-based
Large Language Models (LLMs). Yet a major bottleneck to achieving genuinely
transformative task-oriented dialogue capabilities remains the scarcity of high
quality and linguistically sophisticated data. Existing datasets, while
impressive in scale, have limited domain coverage and contain few genuinely
challenging conversational phenomena; those which are present are typically
unlabelled, making it difficult to assess the strengths and weaknesses of
models without time-consuming and costly human evaluation. Moreover, creating
high quality dialogue data has until now required considerable human input,
limiting both the scale of these datasets and the ability to rapidly bootstrap
data for a new target domain. We aim to overcome these issues with LUCID, a
modularised and highly automated LLM-driven data generation system that
produces realistic, diverse and challenging dialogues. We use LUCID to generate
a seed dataset of 4,277 multi-domain, multi-intent conversations across 100
intents to demonstrate its capabilities. The generated conversations include a
wide range of challenging phenomena and diverse user behaviour, conveniently
identifiable via a set of turn-level tags. Finally, we provide separate test
sets for seen and unseen intents, allowing for convenient out-of-distribution
evaluation. We release both the data generation code and the dataset itself.
\\ ( https://arxiv.org/abs/2403.00462 ,  263kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00499
Date: Fri, 1 Mar 2024 12:42:47 GMT   (8197kb,D)

Title: Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of
  Machine Cognition
Authors: Ariel Goldstein, Gabriel Stanovsky
Categories: cs.CL
\\
  Recent advances in LLMs have sparked a debate on whether they understand
text. In this position paper, we argue that opponents in this debate hold
different definitions for understanding, and particularly differ in their view
on the role of consciousness. To substantiate this claim, we propose a thought
experiment involving an open-source chatbot $Z$ which excels on every possible
benchmark, seemingly without subjective experience. We ask whether $Z$ is
capable of understanding, and show that different schools of thought within
seminal AI research seem to answer this question differently, uncovering their
terminological disagreement. Moving forward, we propose two distinct working
definitions for understanding which explicitly acknowledge the question of
consciousness, and draw connections with a rich literature in philosophy,
psychology and neuroscience.
\\ ( https://arxiv.org/abs/2403.00499 ,  8197kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00506
Date: Fri, 1 Mar 2024 13:07:39 GMT   (723kb)

Title: PoTeC: A German Naturalistic Eye-tracking-while-reading Corpus
Authors: Deborah N. Jakobi and Thomas Kern and David R. Reich and Patrick
  Haller and Lena A. J\"ager
Categories: cs.CL
\\
  The Potsdam Textbook Corpus (PoTeC) is a naturalistic
eye-tracking-while-reading corpus containing data from 75 participants reading
12 scientific texts. PoTeC is the first naturalistic eye-tracking-while-reading
corpus that contains eye-movements from domain-experts as well as novices in a
within-participant manipulation: It is based on a 2x2x2 fully-crossed factorial
design which includes the participants' level of study and the participants'
discipline of study as between-subject factors and the text domain as a
within-subject factor. The participants' reading comprehension was assessed by
a series of text comprehension questions and their domain knowledge was tested
by text-independent background questions for each of the texts. The materials
are annotated for a variety of linguistic features at different levels. We
envision PoTeC to be used for a wide range of studies including but not limited
to analyses of expert and non-expert reading strategies. The corpus and all the
accompanying data at all stages of the preprocessing pipeline and all code used
to preprocess the data are made available via GitHub:
https://github.com/DiLi-Lab/PoTeC.
\\ ( https://arxiv.org/abs/2403.00506 ,  723kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00509
Date: Fri, 1 Mar 2024 13:14:45 GMT   (7183kb,D)

Title: Surveying the Dead Minds: Historical-Psychological Text Analysis with
  Contextualized Construct Representation (CCR) for Classical Chinese
Authors: Yuqi Chen, Sixuan Li, Ying Li and Mohammad Atari
Categories: cs.CL cs.AI cs.CY
\\
  In this work, we develop a pipeline for historical-psychological text
analysis in classical Chinese. Humans have produced texts in various languages
for thousands of years; however, most of the computational literature is
focused on contemporary languages and corpora. The emerging field of historical
psychology relies on computational techniques to extract aspects of psychology
from historical corpora using new methods developed in natural language
processing (NLP). The present pipeline, called Contextualized Construct
Representations (CCR), combines expert knowledge in psychometrics (i.e.,
psychological surveys) with text representations generated via
transformer-based language models to measure psychological constructs such as
traditionalism, norm strength, and collectivism in classical Chinese corpora.
Considering the scarcity of available data, we propose an indirect supervised
contrastive learning approach and build the first Chinese historical psychology
corpus (C-HI-PSY) to fine-tune pre-trained models. We evaluate the pipeline to
demonstrate its superior performance compared with other approaches. The CCR
method outperforms word-embedding-based approaches across all of our tasks and
exceeds prompting with GPT-4 in most tasks. Finally, we benchmark the pipeline
against objective, external data to further verify its validity.
\\ ( https://arxiv.org/abs/2403.00509 ,  7183kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00510
Date: Fri, 1 Mar 2024 13:15:30 GMT   (778kb,D)

Title: ROME: Memorization Insights from Text, Probability and Hidden State in
  Large Language Models
Authors: Bo Li and Qinghua Zhao and Lijie Wen
Categories: cs.CL cs.AI
\\
  Probing the memorization of large language models holds significant
importance. Previous works have established metrics for quantifying
memorization, explored various influencing factors, such as data duplication,
model size, and prompt length, and evaluated memorization by comparing model
outputs with training corpora. However, the training corpora are of enormous
scale and its pre-processing is time-consuming. To explore memorization without
accessing training data, we propose a novel approach, named ROME, wherein
memorization is explored by comparing disparities across memorized and
non-memorized. Specifically, models firstly categorize the selected samples
into memorized and non-memorized groups, and then comparing the demonstrations
in the two groups from the insights of text, probability, and hidden state.
Experimental findings show the disparities in factors including word length,
part-of-speech, word frequency, mean and variance, just to name a few.
\\ ( https://arxiv.org/abs/2403.00510 ,  778kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00528
Date: Fri, 1 Mar 2024 13:36:04 GMT   (434kb)

Title: Large Language Models for Simultaneous Named Entity Extraction and
  Spelling Correction
Authors: Edward Whittaker and Ikuo Kitagishi
Categories: cs.CL cs.CV
Comments: 9 pages, 1 figure
ACM-class: H.3.3; H.3.4; I.2.7; I.7.1; I.7.5
\\
  Language Models (LMs) such as BERT, have been shown to perform well on the
task of identifying Named Entities (NE) in text. A BERT LM is typically used as
a classifier to classify individual tokens in the input text, or to classify
spans of tokens, as belonging to one of a set of possible NE categories.
  In this paper, we hypothesise that decoder-only Large Language Models (LLMs)
can also be used generatively to extract both the NE, as well as potentially
recover the correct surface form of the NE, where any spelling errors that were
present in the input text get automatically corrected.
  We fine-tune two BERT LMs as baselines, as well as eight open-source LLMs, on
the task of producing NEs from text that was obtained by applying Optical
Character Recognition (OCR) to images of Japanese shop receipts; in this work,
we do not attempt to find or evaluate the location of NEs in the text.
  We show that the best fine-tuned LLM performs as well as, or slightly better
than, the best fine-tuned BERT LM, although the differences are not
significant. However, the best LLM is also shown to correct OCR errors in some
cases, as initially hypothesised.
\\ ( https://arxiv.org/abs/2403.00528 ,  434kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00553
Date: Fri, 1 Mar 2024 14:23:12 GMT   (98kb,D)

Title: Standardizing the Measurement of Text Diversity: A Tool and a
  Comparative Analysis of Scores
Authors: Chantal Shaib, Joe Barrow, Jiuding Sun, Alexa F. Siu, Byron C.
  Wallace, Ani Nenkova
Categories: cs.CL
Comments: Preprint
\\
  The diversity across outputs generated by large language models shapes the
perception of their quality and utility. Prompt leaks, templated answer
structure, and canned responses across different interactions are readily
noticed by people, but there is no standard score to measure this aspect of
model behavior. In this work we empirically investigate diversity scores on
English texts. We find that computationally efficient compression algorithms
capture information similar to what is measured by slow to compute $n$-gram
overlap homogeneity scores. Further, a combination of measures -- compression
ratios, self-repetition of long $n$-grams and Self-BLEU and BERTScore -- are
sufficient to report, as they have low mutual correlation with each other. The
applicability of scores extends beyond analysis of generative models; for
example, we highlight applications on instruction-tuning datasets and
human-produced texts. We release a diversity score package to facilitate
research and invite consistency across reports.
\\ ( https://arxiv.org/abs/2403.00553 ,  98kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00662
Date: Fri, 1 Mar 2024 16:49:55 GMT   (242kb,D)

Title: Modeling the Quality of Dialogical Explanations
Authors: Milad Alshomary, Felix Lange, Meisam Booshehri, Meghdut Sengupta,
  Philipp Cimiano, Henning Wachsmuth
Categories: cs.CL
Comments: 9 pages, 3 figures, LREC-COLING 24
\\
  Explanations are pervasive in our lives. Mostly, they occur in dialogical
form where an {\em explainer} discusses a concept or phenomenon of interest
with an {\em explainee}. Leaving the explainee with a clear understanding is
not straightforward due to the knowledge gap between the two participants.
Previous research looked at the interaction of explanation moves, dialogue
acts, and topics in successful dialogues with expert explainers. However,
daily-life explanations often fail, raising the question of what makes a
dialogue successful. In this work, we study explanation dialogues in terms of
the interactions between the explainer and explainee and how they correlate
with the quality of explanations in terms of a successful understanding on the
explainee's side. In particular, we first construct a corpus of 399 dialogues
from the Reddit forum {\em Explain Like I am Five} and annotate it for
interaction flows and explanation quality. We then analyze the interaction
flows, comparing them to those appearing in expert dialogues. Finally, we
encode the interaction flows using two language models that can handle long
inputs, and we provide empirical evidence for the effectiveness boost gained
through the encoding in predicting the success of explanation dialogues.
\\ ( https://arxiv.org/abs/2403.00662 ,  242kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00686
Date: Fri, 1 Mar 2024 17:20:11 GMT   (99kb,D)

Title: A Bit of a Problem: Measurement Disparities in Dataset Sizes Across
  Languages
Authors: Catherine Arnett, Tyler A. Chang, Benjamin K. Bergen
Categories: cs.CL
\\
  How should text dataset sizes be compared across languages? Even for
content-matched (parallel) corpora, UTF-8 encoded text can require a
dramatically different number of bytes for different languages. In our work, we
define the byte premium between two languages as the ratio of bytes used to
encode content-matched text in those languages. We compute byte premiums for
1155 languages, and we use linear regressions to estimate byte premiums for
other languages. We release a tool to obtain byte premiums for any two
languages, enabling comparisons of dataset sizes across languages for more
equitable multilingual model development and data practices.
\\ ( https://arxiv.org/abs/2403.00686 ,  99kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00696
Date: Fri, 1 Mar 2024 17:31:09 GMT   (262kb,D)

Title: Self-Consistent Decoding for More Factual Open Responses
Authors: Christopher Malon and Xiaodan Zhu
Categories: cs.CL
\\
  Self-consistency has emerged as a powerful method for improving the accuracy
of short answers generated by large language models. As previously defined, it
only concerns the accuracy of a final answer parsed from generated text. In
this work, we extend the idea to open response generation, by integrating
voting into the decoding method. Each output sentence is selected from among
multiple samples, conditioning on the previous selections, based on a simple
token overlap score. We compare this "Sample & Select" method to greedy
decoding, beam search, nucleus sampling, and the recently introduced
hallucination avoiding decoders of DoLA, P-CRR, and S-CRR. We show that Sample
& Select improves factuality by a 30% relative margin against these decoders in
NLI-based evaluation on the subsets of CNN/DM and XSum used in the FRANK
benchmark, while maintaining comparable ROUGE-1 F1 scores against reference
summaries. We collect human verifications of the generated summaries,
confirming the factual superiority of our method.
\\ ( https://arxiv.org/abs/2403.00696 ,  262kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00724
Date: Fri, 1 Mar 2024 18:20:11 GMT   (6175kb,D)

Title: Few-Shot Relation Extraction with Hybrid Visual Evidence
Authors: Jiaying Gong and Hoda Eldardiry
Categories: cs.CL cs.CV
Comments: 16 pages, 5 figures
Journal-ref: LREC-COLING 2024
\\
  The goal of few-shot relation extraction is to predict relations between name
entities in a sentence when only a few labeled instances are available for
training. Existing few-shot relation extraction methods focus on uni-modal
information such as text only. This reduces performance when there are no clear
contexts between the name entities described in text. We propose a multi-modal
few-shot relation extraction model (MFS-HVE) that leverages both textual and
visual semantic information to learn a multi-modal representation jointly. The
MFS-HVE includes semantic feature extractors and multi-modal fusion components.
The MFS-HVE semantic feature extractors are developed to extract both textual
and visual features. The visual features include global image features and
local object features within the image. The MFS-HVE multi-modal fusion unit
integrates information from various modalities using image-guided attention,
object-guided attention, and hybrid feature attention to fully capture the
semantic interaction between visual regions of images and relevant texts.
Extensive experiments conducted on two public datasets demonstrate that
semantic visual information significantly improves the performance of few-shot
relation prediction.
\\ ( https://arxiv.org/abs/2403.00724 ,  6175kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00742
Date: Fri, 1 Mar 2024 18:43:09 GMT   (8521kb,D)

Title: Dialect prejudice predicts AI decisions about people's character,
  employability, and criminality
Authors: Valentin Hofmann, Pratyusha Ria Kalluri, Dan Jurafsky, Sharese King
Categories: cs.CL cs.AI cs.CY
\\
  Hundreds of millions of people now interact with language models, with uses
ranging from serving as a writing aid to informing hiring decisions. Yet these
language models are known to perpetuate systematic racial prejudices, making
their judgments biased in problematic ways about groups like African Americans.
While prior research has focused on overt racism in language models, social
scientists have argued that racism with a more subtle character has developed
over time. It is unknown whether this covert racism manifests in language
models. Here, we demonstrate that language models embody covert racism in the
form of dialect prejudice: we extend research showing that Americans hold
raciolinguistic stereotypes about speakers of African American English and find
that language models have the same prejudice, exhibiting covert stereotypes
that are more negative than any human stereotypes about African Americans ever
experimentally recorded, although closest to the ones from before the civil
rights movement. By contrast, the language models' overt stereotypes about
African Americans are much more positive. We demonstrate that dialect prejudice
has the potential for harmful consequences by asking language models to make
hypothetical decisions about people, based only on how they speak. Language
models are more likely to suggest that speakers of African American English be
assigned less prestigious jobs, be convicted of crimes, and be sentenced to
death. Finally, we show that existing methods for alleviating racial bias in
language models such as human feedback training do not mitigate the dialect
prejudice, but can exacerbate the discrepancy between covert and overt
stereotypes, by teaching language models to superficially conceal the racism
that they maintain on a deeper level. Our findings have far-reaching
implications for the fair and safe employment of language technology.
\\ ( https://arxiv.org/abs/2403.00742 ,  8521kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00758
Date: Fri, 1 Mar 2024 18:55:20 GMT   (73kb,D)

Title: Mitigating Reversal Curse via Semantic-aware Permutation Training
Authors: Qingyan Guo, Rui Wang, Junliang Guo, Xu Tan, Jiang Bian, Yujiu Yang
Categories: cs.CL cs.AI cs.LG
\\
  While large language models (LLMs) have achieved impressive performance
across diverse tasks, recent studies showcase that causal LLMs suffer from the
"reversal curse". It is a typical example that the model knows "A's father is
B", but is unable to reason "B's child is A". This limitation poses a challenge
to the advancement of artificial general intelligence (AGI), as it suggests a
gap in the models' ability to comprehend and apply bidirectional reasoning. In
this paper, we first conduct substantial evaluation and identify that the root
cause of the reversal curse lies in the different word order between the
training and inference stage, namely, the poor ability of causal language
models to predict antecedent words within the training data. Accordingly,
permutation on the training data is considered as a potential solution, since
this can make the model predict antecedent words or tokens. However, previous
permutation methods may disrupt complete phrases or entities, thereby posing
challenges for the model to comprehend and learn from training data. To address
this issue, we propose Semantic-aware Permutation Training (SPT), which
addresses this issue by segmenting the training sentences into semantic units
(i.e., entities or phrases) with an assistant language model and permuting
these units before feeding into the model. Extensive experiments demonstrate
that SPT effectively mitigates the reversal curse since the performance on
reversed questions approximates that on the forward ones, and significantly
advances the performance of existing works.
\\ ( https://arxiv.org/abs/2403.00758 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00011
Date: Mon, 26 Feb 2024 20:09:44 GMT   (2271kb,D)

Title: Introducing User Feedback-based Counterfactual Explanations (UFCE)
Authors: Muhammad Suffian, Jose M. Alonso-Moral, Alessandro Bogliolo
Categories: cs.LG cs.AI cs.HC
Comments: preprint of paper submitted to IJCIS Springer
\\
  Machine learning models are widely used in real-world applications. However,
their complexity makes it often challenging to interpret the rationale behind
their decisions. Counterfactual explanations (CEs) have emerged as a viable
solution for generating comprehensible explanations in eXplainable Artificial
Intelligence (XAI). CE provides actionable information to users on how to
achieve the desired outcome with minimal modifications to the input. However,
current CE algorithms usually operate within the entire feature space when
optimizing changes to turn over an undesired outcome, overlooking the
identification of key contributors to the outcome and disregarding the
practicality of the suggested changes. In this study, we introduce a novel
methodology, that is named as user feedback-based counterfactual explanation
(UFCE), which addresses these limitations and aims to bolster confidence in the
provided explanations. UFCE allows for the inclusion of user constraints to
determine the smallest modifications in the subset of actionable features while
considering feature dependence, and evaluates the practicality of suggested
changes using benchmark evaluation metrics. We conducted three experiments with
five datasets, demonstrating that UFCE outperforms two well-known CE methods in
terms of \textit{proximity}, \textit{sparsity}, and \textit{feasibility}.
Reported results indicate that user constraints influence the generation of
feasible CEs.
\\ ( https://arxiv.org/abs/2403.00011 ,  2271kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00012
Date: Tue, 27 Feb 2024 02:23:07 GMT   (653kb,D)

Title: PreRoutGNN for Timing Prediction with Order Preserving Partition: Global
  Circuit Pre-training, Local Delay Learning and Attentional Cell Modeling
Authors: Ruizhe Zhong, Junjie Ye, Zhentao Tang, Shixiong Kai, Mingxuan Yuan,
  Jianye Hao, Junchi Yan
Categories: cs.LG cs.AR
Comments: 13 pages, 5 figures, The 38th Annual AAAI Conference on Artificial
  Intelligence (AAAI 2024)
\\
  Pre-routing timing prediction has been recently studied for evaluating the
quality of a candidate cell placement in chip design. It involves directly
estimating the timing metrics for both pin-level (slack, slew) and edge-level
(net delay, cell delay), without time-consuming routing. However, it often
suffers from signal decay and error accumulation due to the long timing paths
in large-scale industrial circuits. To address these challenges, we propose a
two-stage approach. First, we propose global circuit training to pre-train a
graph auto-encoder that learns the global graph embedding from circuit netlist.
Second, we use a novel node updating scheme for message passing on GCN,
following the topological sorting sequence of the learned graph embedding and
circuit graph. This scheme residually models the local time delay between two
adjacent pins in the updating sequence, and extracts the lookup table
information inside each cell via a new attention mechanism. To handle
large-scale circuits efficiently, we introduce an order preserving partition
scheme that reduces memory consumption while maintaining the topological
dependencies. Experiments on 21 real world circuits achieve a new SOTA R2 of
0.93 for slack prediction, which is significantly surpasses 0.59 by previous
SOTA method. Code will be available at:
https://github.com/Thinklab-SJTU/EDA-AI.
\\ ( https://arxiv.org/abs/2403.00012 ,  653kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00013
Date: Tue, 27 Feb 2024 07:15:35 GMT   (7419kb,D)

Title: Prioritizing Informative Features and Examples for Deep Learning from
  Noisy Data
Authors: Dongmin Park
Categories: cs.LG
Comments: PhD thesis
\\
  In this dissertation, we propose a systemic framework that prioritizes
informative features and examples to enhance each stage of the development
process. Specifically, we prioritize informative features and examples and
improve the performance of feature learning, data labeling, and data selection.
We first propose an approach to extract only informative features that are
inherent to solving a target task by using auxiliary out-of-distribution data.
We deactivate the noise features in the target distribution by using that in
the out-of-distribution data. Next, we introduce an approach that prioritizes
informative examples from unlabeled noisy data in order to reduce the labeling
cost of active learning. In order to solve the purity-information dilemma,
where an attempt to select informative examples induces the selection of many
noisy examples, we propose a meta-model that finds the best balance between
purity and informativeness. Lastly, we suggest an approach that prioritizes
informative examples from labeled noisy data to preserve the performance of
data selection. For labeled image noise data, we propose a data selection
method that considers the confidence of neighboring samples to maintain the
performance of the state-of-the-art Re-labeling models. For labeled text noise
data, we present an instruction selection method that takes diversity into
account for ranking the quality of instructions with prompting, thereby
enhancing the performance of aligned large language models.
  Overall, our unified framework induces the deep learning development process
robust to noisy data, thereby effectively mitigating noisy features and
examples in real-world applications.
\\ ( https://arxiv.org/abs/2403.00013 ,  7419kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00016
Date: Wed, 28 Feb 2024 02:15:47 GMT   (3504kb,D)

Title: Deep Sensitivity Analysis for Objective-Oriented Combinatorial
  Optimization
Authors: Ganga Gireesan, Nisha Pillai, Michael J Rothrock, Bindu Nanduri,
  Zhiqian Chen, Mahalingam Ramkumar
Categories: cs.LG cs.AI
Comments: The 2023 International Conference on Computational Science &
  Computational Intelligence (CSCI'23)
\\
  Pathogen control is a critical aspect of modern poultry farming, providing
important benefits for both public health and productivity. Effective poultry
management measures to reduce pathogen levels in poultry flocks promote food
safety by lowering risks of food-borne illnesses. They also support animal
health and welfare by preventing infectious diseases that can rapidly spread
and impact flock growth, egg production, and overall health. This study frames
the search for optimal management practices that minimize the presence of
multiple pathogens as a combinatorial optimization problem. Specifically, we
model the various possible combinations of management settings as a solution
space that can be efficiently explored to identify configurations that
optimally reduce pathogen levels. This design incorporates a neural network
feedback-based method that combines feature explanations with global
sensitivity analysis to ensure combinatorial optimization in multiobjective
settings. Our preliminary experiments have promising results when applied to
two real-world agricultural datasets. While further validation is still needed,
these early experimental findings demonstrate the potential of the model to
derive targeted feature interactions that adaptively optimize pathogen control
under varying real-world constraints.
\\ ( https://arxiv.org/abs/2403.00016 ,  3504kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00017
Date: Wed, 28 Feb 2024 02:24:04 GMT   (5216kb,D)

Title: Towards Interpreting Multi-Objective Feature Associations
Authors: Nisha Pillai, Ganga Gireesan, Michael J. Rothrock Jr., Bindu Nanduri,
  Zhiqian Chen, Mahalingam Ramkumar
Categories: cs.LG cs.AI
Comments: The 18th Annual IEEE International Systems Conference 2024 (IEEE
  SYSCON 2024)
\\
  Understanding how multiple features are associated and contribute to a
specific objective is as important as understanding how each feature
contributes to a particular outcome. Interpretability of a single feature in a
prediction may be handled in multiple ways; however, in a multi-objective
prediction, it is difficult to obtain interpretability of a combination of
feature values. To address this issue, we propose an objective specific feature
interaction design using multi-labels to find the optimal combination of
features in agricultural settings. One of the novel aspects of this design is
the identification of a method that integrates feature explanations with global
sensitivity analysis in order to ensure combinatorial optimization in
multi-objective settings. We have demonstrated in our preliminary experiments
that an approximate combination of feature values can be found to achieve the
desired outcome using two agricultural datasets: one with pre-harvest poultry
farm practices for multi-drug resistance presence, and one with post-harvest
poultry farm practices for food-borne pathogens. In our combinatorial
optimization approach, all three pathogens are taken into consideration
simultaneously to account for the interaction between conditions that favor
different types of pathogen growth. These results indicate that
explanation-based approaches are capable of identifying combinations of
features that reduce pathogen presence in fewer iterations than a baseline.
\\ ( https://arxiv.org/abs/2403.00017 ,  5216kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00019
Date: Wed, 28 Feb 2024 04:30:41 GMT   (428kb,D)

Title: Transformer-based Parameter Estimation in Statistics
Authors: Xiaoxin Yin and David S. Yin
Categories: cs.LG stat.ML
\\
  Parameter estimation is one of the most important tasks in statistics, and is
key to helping people understand the distribution behind a sample of
observations. Traditionally parameter estimation is done either by closed-form
solutions (e.g., maximum likelihood estimation for Gaussian distribution), or
by iterative numerical methods such as Newton-Raphson method when closed-form
solution does not exist (e.g., for Beta distribution).
  In this paper we propose a transformer-based approach to parameter
estimation. Compared with existing solutions, our approach does not require a
closed-form solution or any mathematical derivations. It does not even require
knowing the probability density function, which is needed by numerical methods.
After the transformer model is trained, only a single inference is needed to
estimate the parameters of the underlying distribution based on a sample of
observations. In the empirical study we compared our approach with maximum
likelihood estimation on commonly used distributions such as normal
distribution, exponential distribution and beta distribution. It is shown that
our approach achieves similar or better accuracy as measured by
mean-square-errors.
\\ ( https://arxiv.org/abs/2403.00019 ,  428kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00024
Date: Wed, 28 Feb 2024 15:01:59 GMT   (852kb,D)

Title: FlowCyt: A Comparative Study of Deep Learning Approaches for Multi-Class
  Classification in Flow Cytometry Benchmarking
Authors: Lorenzo Bini, Fatemeh Nassajian Mojarrad, Margarita Liarou, Thomas
  Matthes, St\'ephane Marchand-Maillet
Categories: cs.LG q-bio.QM
Comments: arXiv admin note: text overlap with arXiv:2402.18611
\\
  This paper presents FlowCyt, the first comprehensive benchmark for
multi-class single-cell classification in flow cytometry data. The dataset
comprises bone marrow samples from 30 patients, with each cell characterized by
twelve markers. Ground truth labels identify five hematological cell types: T
lymphocytes, B lymphocytes, Monocytes, Mast cells, and Hematopoietic
Stem/Progenitor Cells (HSPCs). Experiments utilize supervised inductive
learning and semi-supervised transductive learning on up to 1 million cells per
patient. Baseline methods include Gaussian Mixture Models, XGBoost, Random
Forests, Deep Neural Networks, and Graph Neural Networks (GNNs). GNNs
demonstrate superior performance by exploiting spatial relationships in
graph-encoded data. The benchmark allows standardized evaluation of clinically
relevant classification tasks, along with exploratory analyses to gain insights
into hematological cell phenotypes. This represents the first public flow
cytometry benchmark with a richly annotated, heterogeneous dataset. It will
empower the development and rigorous assessment of novel methodologies for
single-cell analysis.
\\ ( https://arxiv.org/abs/2403.00024 ,  852kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00025
Date: Wed, 28 Feb 2024 15:19:33 GMT   (68kb)

Title: On the Challenges and Opportunities in Generative AI
Authors: Laura Manduchi, Kushagra Pandey, Robert Bamler, Ryan Cotterell, Sina
  D\"aubener, Sophie Fellenz, Asja Fischer, Thomas G\"artner, Matthias
  Kirchler, Marius Kloft, Yingzhen Li, Christoph Lippert, Gerard de Melo, Eric
  Nalisnick, Bj\"orn Ommer, Rajesh Ranganath, Maja Rudolph, Karen Ullrich, Guy
  Van den Broeck, Julia E Vogt, Yixin Wang, Florian Wenzel, Frank Wood, Stephan
  Mandt, Vincent Fortuin
Categories: cs.LG cs.AI
\\
  The field of deep generative modeling has grown rapidly and consistently over
the years. With the availability of massive amounts of training data coupled
with advances in scalable unsupervised learning paradigms, recent large-scale
generative models show tremendous promise in synthesizing high-resolution
images and text, as well as structured data such as videos and molecules.
However, we argue that current large-scale generative AI models do not
sufficiently address several fundamental issues that hinder their widespread
adoption across domains. In this work, we aim to identify key unresolved
challenges in modern generative AI paradigms that should be tackled to further
enhance their capabilities, versatility, and reliability. By identifying these
challenges, we aim to provide researchers with valuable insights for exploring
fruitful research directions, thereby fostering the development of more robust
and accessible generative AI solutions.
\\ ( https://arxiv.org/abs/2403.00025 ,  68kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00026
Date: Wed, 28 Feb 2024 16:02:29 GMT   (4441kb,D)

Title: Learning to Deliver: a Foundation Model for the Montreal Capacitated
  Vehicle Routing Problem
Authors: Samuel J. K. Chin, Matthias Winkenbach, Akash Srivastava
Categories: cs.LG cs.AI math.OC
\\
  In this paper, we present the Foundation Model for the Montreal Capacitated
Vehicle Routing Problem (FM-MCVRP), a novel Deep Learning (DL) model that
approximates high-quality solutions to a variant of the Capacitated Vehicle
Routing Problem (CVRP) that characterizes many real-world applications. The
so-called Montreal Capacitated Vehicle Routing Problem (MCVRP), first formally
described by Bengio et al. (2021), is defined on a fixed and finite graph,
which is analogous to a city. Each MCVRP instance is essentially the sub-graph
connecting a randomly sampled subset of the nodes in the fixed graph, which
represent a set of potential addresses in a real-world delivery problem on a
given day. Our work exploits this problem structure to frame the MCVRP as an
analogous Natural Language Processing (NLP) task. Specifically, we leverage a
Transformer architecture embedded in a Large Language Model (LLM) framework to
train our model in a supervised manner on computationally inexpensive,
sub-optimal MCVRP solutions obtained algorithmically. Through comprehensive
computational experiments, we show that FM-MCVRP produces better MCVRP
solutions than the training data and generalizes to larger sized problem
instances not seen during training. Even when compared to near-optimal
solutions from state-of-the-art heuristics, FM-MCVRP yields competitive results
despite being trained on inferior data. For instance, for 400-customer
problems, FM-MCVRP solutions on average fall within 2% of the benchmark. Our
results further demonstrate that unlike prior works in the literature, FM-MCVRP
is a unified model, which performs consistently and reliably on a range of
problem instance sizes and parameter values such as the vehicle capacity.
\\ ( https://arxiv.org/abs/2403.00026 ,  4441kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00036
Date: Thu, 29 Feb 2024 05:59:27 GMT   (1124kb,D)

Title: Influencing Bandits: Arm Selection for Preference Shaping
Authors: Viraj Nadkarni and D. Manjunath and Sharayu Moharir
Categories: cs.LG cs.AI cs.IR cs.SY eess.SY
Comments: 14 pages, 8 figures, 24 references, proofs in appendix
ACM-class: I.2.6
\\
  We consider a non stationary multi-armed bandit in which the population
preferences are positively and negatively reinforced by the observed rewards.
The objective of the algorithm is to shape the population preferences to
maximize the fraction of the population favouring a predetermined arm. For the
case of binary opinions, two types of opinion dynamics are considered --
decreasing elasticity (modeled as a Polya urn with increasing number of balls)
and constant elasticity (using the voter model). For the first case, we
describe an Explore-then-commit policy and a Thompson sampling policy and
analyse the regret for each of these policies. We then show that these
algorithms and their analyses carry over to the constant elasticity case. We
also describe a Thompson sampling based algorithm for the case when more than
two types of opinions are present. Finally, we discuss the case where presence
of multiple recommendation systems gives rise to a trade-off between their
popularity and opinion shaping objectives.
\\ ( https://arxiv.org/abs/2403.00036 ,  1124kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00041
Date: Thu, 29 Feb 2024 11:43:04 GMT   (3553kb,D)

Title: Global and Local Prompts Cooperation via Optimal Transport for Federated
  Learning
Authors: Hongxia Li, Wei Huang, Jingya Wang and Ye Shi
Categories: cs.LG cs.AI cs.DC
\\
  Prompt learning in pretrained visual-language models has shown remarkable
flexibility across various downstream tasks. Leveraging its inherent
lightweight nature, recent research attempted to integrate the powerful
pretrained models into federated learning frameworks to simultaneously reduce
communication costs and promote local training on insufficient data. Despite
these efforts, current federated prompt learning methods lack specialized
designs to systematically address severe data heterogeneities, e.g., data
distribution with both label and feature shifts involved. To address this
challenge, we present Federated Prompts Cooperation via Optimal Transport
(FedOTP), which introduces efficient collaborative prompt learning strategies
to capture diverse category traits on a per-client basis. Specifically, for
each client, we learn a global prompt to extract consensus knowledge among
clients, and a local prompt to capture client-specific category
characteristics. Unbalanced Optimal Transport is then employed to align local
visual features with these prompts, striking a balance between global consensus
and local personalization. Extensive experiments on datasets with various types
of heterogeneities have demonstrated that our FedOTP outperforms the
state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.00041 ,  3553kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00103
Date: Thu, 29 Feb 2024 20:11:47 GMT   (7114kb,D)

Title: On Robustness and Generalization of ML-Based Congestion Predictors to
  Valid and Imperceptible Perturbations
Authors: Chester Holtz, Yucheng Wang, Chung-Kuan Cheng, Bill Lin
Categories: cs.LG cs.AR
Comments: 7 pages, 7 figures
\\
  There is substantial interest in the use of machine learning (ML)-based
techniques throughout the electronic computer-aided design (CAD) flow,
particularly methods based on deep learning. However, while deep learning
methods have achieved state-of-the-art performance in several applications,
recent work has demonstrated that neural networks are generally vulnerable to
small, carefully chosen perturbations of their input (e.g. a single pixel
change in an image). In this work, we investigate robustness in the context of
ML-based EDA tools -- particularly for congestion prediction. As far as we are
aware, we are the first to explore this concept in the context of ML-based EDA.
  We first describe a novel notion of imperceptibility designed specifically
for VLSI layout problems defined on netlists and cell placements. Our
definition of imperceptibility is characterized by a guarantee that a
perturbation to a layout will not alter its global routing. We then demonstrate
that state-of-the-art CNN and GNN-based congestion models exhibit brittleness
to imperceptible perturbations. Namely, we show that when a small number of
cells (e.g. 1%-5% of cells) have their positions shifted such that a measure of
global congestion is guaranteed to remain unaffected (e.g. 1% of the design
adversarially shifted by 0.001% of the layout space results in a predicted
decrease in congestion of up to 90%, while no change in congestion is implied
by the perturbation). In other words, the quality of a predictor can be made
arbitrarily poor (i.e. can be made to predict that a design is
"congestion-free") for an arbitrary input layout. Next, we describe a simple
technique to train predictors that improves robustness to these perturbations.
Our work indicates that CAD engineers should be cautious when integrating
neural network-based mechanisms in EDA flows to ensure robust and high-quality
results.
\\ ( https://arxiv.org/abs/2403.00103 ,  7114kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00105
Date: Thu, 29 Feb 2024 20:17:08 GMT   (134kb,D)

Title: Longitudinal Counterfactuals: Constraints and Opportunities
Authors: Alexander Asemota and Giles Hooker
Categories: cs.LG cs.CY
\\
  Counterfactual explanations are a common approach to providing recourse to
data subjects. However, current methodology can produce counterfactuals that
cannot be achieved by the subject, making the use of counterfactuals for
recourse difficult to justify in practice. Though there is agreement that
plausibility is an important quality when using counterfactuals for algorithmic
recourse, ground truth plausibility continues to be difficult to quantify. In
this paper, we propose using longitudinal data to assess and improve
plausibility in counterfactuals. In particular, we develop a metric that
compares longitudinal differences to counterfactual differences, allowing us to
evaluate how similar a counterfactual is to prior observed changes.
Furthermore, we use this metric to generate plausible counterfactuals. Finally,
we discuss some of the inherent difficulties of using counterfactuals for
recourse.
\\ ( https://arxiv.org/abs/2403.00105 ,  134kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00116
Date: Thu, 29 Feb 2024 20:39:31 GMT   (911kb,D)

Title: Federated Linear Contextual Bandits with Heterogeneous Clients
Authors: Ethan Blaser, Chuanhao Li, Hongning Wang
Categories: cs.LG cs.AI
\\
  The demand for collaborative and private bandit learning across multiple
agents is surging due to the growing quantity of data generated from
distributed systems. Federated bandit learning has emerged as a promising
framework for private, efficient, and decentralized online learning. However,
almost all previous works rely on strong assumptions of client homogeneity,
i.e., all participating clients shall share the same bandit model; otherwise,
they all would suffer linear regret. This greatly restricts the application of
federated bandit learning in practice. In this work, we introduce a new
approach for federated bandits for heterogeneous clients, which clusters
clients for collaborative bandit learning under the federated learning setting.
Our proposed algorithm achieves non-trivial sub-linear regret and communication
cost for all clients, subject to the communication protocol under federated
learning that at anytime only one model can be shared by the server.
\\ ( https://arxiv.org/abs/2403.00116 ,  911kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00131
Date: Thu, 29 Feb 2024 21:25:58 GMT   (1184kb,D)

Title: UniTS: Building a Unified Time Series Model
Authors: Shanghua Gao, Teddy Koker, Owen Queen, Thomas Hartvigsen, Theodoros
  Tsiligkaridis, Marinka Zitnik
Categories: cs.LG cs.AI
\\
  Foundation models, especially LLMs, are profoundly transforming deep
learning. Instead of training many task-specific models, we can adapt a single
pretrained model to many tasks via fewshot prompting or fine-tuning. However,
current foundation models apply to sequence data but not to time series, which
present unique challenges due to the inherent diverse and multidomain time
series datasets, diverging task specifications across forecasting,
classification and other types of tasks, and the apparent need for
task-specialized models. We developed UNITS, a unified time series model that
supports a universal task specification, accommodating classification,
forecasting, imputation, and anomaly detection tasks. This is achieved through
a novel unified network backbone, which incorporates sequence and variable
attention along with a dynamic linear operator and is trained as a unified
model. Across 38 multi-domain datasets, UNITS demonstrates superior performance
compared to task-specific models and repurposed natural language-based LLMs.
UNITS exhibits remarkable zero-shot, few-shot, and prompt learning capabilities
when evaluated on new data domains and tasks. The source code and datasets are
available at https://github.com/mims-harvard/UniTS.
\\ ( https://arxiv.org/abs/2403.00131 ,  1184kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00155
Date: Thu, 29 Feb 2024 22:13:12 GMT   (8517kb,D)

Title: Towards Explaining Deep Neural Network Compression Through a
  Probabilistic Latent Space
Authors: Mahsa Mozafari-Nia and Salimeh Yasaei Sekeh
Categories: cs.LG
\\
  Despite the impressive performance of deep neural networks (DNNs), their
computational complexity and storage space consumption have led to the concept
of network compression. While DNN compression techniques such as pruning and
low-rank decomposition have been extensively studied, there has been
insufficient attention paid to their theoretical explanation. In this paper, we
propose a novel theoretical framework that leverages a probabilistic latent
space of DNN weights and explains the optimal network sparsity by using the
information-theoretic divergence measures. We introduce new analogous projected
patterns (AP2) and analogous-in-probability projected patterns (AP3) notions
for DNNs and prove that there exists a relationship between AP3/AP2 property of
layers in the network and its performance. Further, we provide a theoretical
analysis that explains the training process of the compressed network. The
theoretical results are empirically validated through experiments conducted on
standard pre-trained benchmarks, including AlexNet, ResNet50, and VGG16, using
CIFAR10 and CIFAR100 datasets. Through our experiments, we highlight the
relationship of AP3 and AP2 properties with fine-tuning pruned DNNs and
sparsity levels.
\\ ( https://arxiv.org/abs/2403.00155 ,  8517kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00157
Date: Thu, 29 Feb 2024 22:18:05 GMT   (246kb,D)

Title: Privacy-Preserving Distributed Optimization and Learning
Authors: Ziqin Chen and Yongqiang Wang
Categories: cs.LG cs.CR cs.GT
Comments: Accepted as a chapter in the Encyclopedia of Systems and Control
  Engineering published by Elsevier
\\
  Distributed optimization and learning has recently garnered great attention
due to its wide applications in sensor networks, smart grids, machine learning,
and so forth. Despite rapid development, existing distributed optimization and
learning algorithms require each agent to exchange messages with its neighbors,
which may expose sensitive information and raise significant privacy concerns.
In this survey paper, we overview privacy-preserving distributed optimization
and learning methods. We first discuss cryptography, differential privacy, and
other techniques that can be used for privacy preservation and indicate their
pros and cons for privacy protection in distributed optimization and learning.
We believe that among these approaches, differential privacy is most promising
due to its low computational and communication complexities, which are
extremely appealing for modern learning based applications with high dimensions
of optimization variables. We then introduce several differential-privacy
algorithms that can simultaneously ensure privacy and optimization accuracy.
Moreover, we provide example applications in several machine learning problems
to confirm the real-world effectiveness of these algorithms. Finally, we
highlight some challenges in this research domain and discuss future
directions.
\\ ( https://arxiv.org/abs/2403.00157 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00176
Date: Thu, 29 Feb 2024 23:04:01 GMT   (786kb,D)

Title: SoD$^2$: Statically Optimizing Dynamic Deep Neural Network
Authors: Wei Niu, Gagan Agrawal, Bin Ren
Categories: cs.LG cs.AI cs.PL
DOI: 10.1145/3617232.3624869
\\
  Though many compilation and runtime systems have been developed for DNNs in
recent years, the focus has largely been on static DNNs. Dynamic DNNs, where
tensor shapes and sizes and even the set of operators used are dependent upon
the input and/or execution, are becoming common. This paper presents SoD$^2$, a
comprehensive framework for optimizing Dynamic DNNs. The basis of our approach
is a classification of common operators that form DNNs, and the use of this
classification towards a Rank and Dimension Propagation (RDP) method. This
framework statically determines the shapes of operators as known constants,
symbolic constants, or operations on these. Next, using RDP we enable a series
of optimizations, like fused code generation, execution (order) planning, and
even runtime memory allocation plan generation. By evaluating the framework on
10 emerging Dynamic DNNs and comparing it against several existing systems, we
demonstrate both reductions in execution latency and memory requirements, with
RDP-enabled key optimizations responsible for much of the gains. Our evaluation
results show that SoD$^2$ runs up to $3.9\times$ faster than these systems
while saving up to $88\%$ peak memory consumption.
\\ ( https://arxiv.org/abs/2403.00176 ,  786kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00177
Date: Thu, 29 Feb 2024 23:04:42 GMT   (10120kb,D)

Title: Non-Invasive Medical Digital Twins using Physics-Informed
  Self-Supervised Learning
Authors: Keying Kuang, Frances Dean, Jack B. Jedlicki, David Ouyang, Anthony
  Philippakis, David Sontag, Ahmed M. Alaa
Categories: cs.LG q-bio.QM
\\
  A digital twin is a virtual replica of a real-world physical phenomena that
uses mathematical modeling to characterize and simulate its defining features.
By constructing digital twins for disease processes, we can perform in-silico
simulations that mimic patients' health conditions and counterfactual outcomes
under hypothetical interventions in a virtual setting. This eliminates the need
for invasive procedures or uncertain treatment decisions. In this paper, we
propose a method to identify digital twin model parameters using only
noninvasive patient health data. We approach the digital twin modeling as a
composite inverse problem, and observe that its structure resembles pretraining
and finetuning in self-supervised learning (SSL). Leveraging this, we introduce
a physics-informed SSL algorithm that initially pretrains a neural network on
the pretext task of solving the physical model equations. Subsequently, the
model is trained to reconstruct low-dimensional health measurements from
noninvasive modalities while being constrained by the physical equations
learned in pretraining. We apply our method to identify digital twins of
cardiac hemodynamics using noninvasive echocardiogram videos, and demonstrate
its utility in unsupervised disease detection and in-silico clinical trials.
\\ ( https://arxiv.org/abs/2403.00177 ,  10120kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00178
Date: Thu, 29 Feb 2024 23:07:07 GMT   (1086kb,D)

Title: Causal Graph ODE: Continuous Treatment Effect Modeling in Multi-agent
  Dynamical Systems
Authors: Zijie Huang, Jeehyun Hwang, Junkai Zhang, Jinwoo Baik, Weitong Zhang,
  Dominik Wodarz, Yizhou Sun, Quanquan Gu, Wei Wang
Categories: cs.LG cs.AI
\\
  Real-world multi-agent systems are often dynamic and continuous, where the
agents co-evolve and undergo changes in their trajectories and interactions
over time. For example, the COVID-19 transmission in the U.S. can be viewed as
a multi-agent system, where states act as agents and daily population movements
between them are interactions. Estimating the counterfactual outcomes in such
systems enables accurate future predictions and effective decision-making, such
as formulating COVID-19 policies. However, existing methods fail to model the
continuous dynamic effects of treatments on the outcome, especially when
multiple treatments (e.g., "stay-at-home" and "get-vaccine" policies) are
applied simultaneously. To tackle this challenge, we propose Causal Graph
Ordinary Differential Equations (CAG-ODE), a novel model that captures the
continuous interaction among agents using a Graph Neural Network (GNN) as the
ODE function. The key innovation of our model is to learn time-dependent
representations of treatments and incorporate them into the ODE function,
enabling precise predictions of potential outcomes. To mitigate confounding
bias, we further propose two domain adversarial learning-based objectives,
which enable our model to learn balanced continuous representations that are
not affected by treatments or interference. Experiments on two datasets (i.e.,
COVID-19 and tumor growth) demonstrate the superior performance of our proposed
model.
\\ ( https://arxiv.org/abs/2403.00178 ,  1086kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00188
Date: Thu, 29 Feb 2024 23:38:28 GMT   (66kb)

Title: Impact of Decentralized Learning on Player Utilities in Stackelberg
  Games
Authors: Kate Donahue, Nicole Immorlica, Meena Jagadeesan, Brendan Lucier, and
  Aleksandrs Slivkins
Categories: cs.LG cs.GT
\\
  When deployed in the world, a learning agent such as a recommender system or
a chatbot often repeatedly interacts with another learning agent (such as a
user) over time. In many such two-agent systems, each agent learns separately
and the rewards of the two agents are not perfectly aligned. To better
understand such cases, we examine the learning dynamics of the two-agent system
and the implications for each agent's objective. We model these systems as
Stackelberg games with decentralized learning and show that standard regret
benchmarks (such as Stackelberg equilibrium payoffs) result in worst-case
linear regret for at least one player. To better capture these systems, we
construct a relaxed regret benchmark that is tolerant to small learning errors
by agents. We show that standard learning algorithms fail to provide sublinear
regret, and we develop algorithms to achieve near-optimal $O(T^{2/3})$ regret
for both players with respect to these benchmarks. We further design relaxed
environments under which faster learning ($O(\sqrt{T})$) is possible.
Altogether, our results take a step towards assessing how two-agent
interactions in sequential and decentralized learning environments affect the
utility of both agents.
\\ ( https://arxiv.org/abs/2403.00188 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00194
Date: Thu, 29 Feb 2024 23:46:28 GMT   (11122kb,D)

Title: Ask Your Distribution Shift if Pre-Training is Right for You
Authors: Benjamin Cohen-Wang, Joshua Vendrow, Aleksander Madry
Categories: cs.LG
\\
  Pre-training is a widely used approach to develop models that are robust to
distribution shifts. However, in practice, its effectiveness varies:
fine-tuning a pre-trained model improves robustness significantly in some cases
but not at all in others (compared to training from scratch). In this work, we
seek to characterize the failure modes that pre-training can and cannot
address. In particular, we focus on two possible failure modes of models under
distribution shift: poor extrapolation (e.g., they cannot generalize to a
different domain) and biases in the training data (e.g., they rely on spurious
features). Our study suggests that, as a rule of thumb, pre-training can help
mitigate poor extrapolation but not dataset biases. After providing theoretical
motivation and empirical evidence for this finding, we explore two of its
implications for developing robust models: (1) pre-training and interventions
designed to prevent exploiting biases have complementary robustness benefits,
and (2) fine-tuning on a (very) small, non-diverse but de-biased dataset can
result in significantly more robust models than fine-tuning on a large and
diverse but biased dataset. Code is available at
https://github.com/MadryLab/pretraining-distribution-shift-robustness.
\\ ( https://arxiv.org/abs/2403.00194 ,  11122kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00222
Date: Fri, 1 Mar 2024 01:49:57 GMT   (224kb,D)

Title: Efficient Reinforcement Learning for Global Decision Making in the
  Presence of Local Agents at Scale
Authors: Emile Anand, Guannan Qu
Categories: cs.LG cs.MA
Comments: 30 pages, 6 figures
ACM-class: I.2.6
\\
  We study reinforcement learning for global decision-making in the presence of
many local agents, where the global decision-maker makes decisions affecting
all local agents, and the objective is to learn a policy that maximizes the
rewards of both the global and the local agents. Such problems find many
applications, e.g. demand response, EV charging, queueing, etc. In this
setting, scalability has been a long-standing challenge due to the size of the
state/action space which can be exponential in the number of agents. This work
proposes the SUB-SAMPLE-Q algorithm where the global agent subsamples $k\leq n$
local agents to compute an optimal policy in time that is only exponential in
$k$, providing an exponential speedup from standard methods that are
exponential in $n$. We show that the learned policy converges to the optimal
policy in the order of $\tilde{O}(1/\sqrt{k}+\epsilon_{k,m})$ as the number of
sub-sampled agents $k$ increases, where $\epsilon_{k,m}$ is the Bellman noise.
We also conduct numerical simulations in a demand-response setting and a
queueing setting.
\\ ( https://arxiv.org/abs/2403.00222 ,  224kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00225
Date: Fri, 1 Mar 2024 02:00:44 GMT   (2026kb,D)

Title: Robust Policy Learning via Offline Skill Diffusion
Authors: Woo Kyung Kim, Minjong Yoo, Honguk Woo
Categories: cs.LG
\\
  Skill-based reinforcement learning (RL) approaches have shown considerable
promise, especially in solving long-horizon tasks via hierarchical structures.
These skills, learned task-agnostically from offline datasets, can accelerate
the policy learning process for new tasks. Yet, the application of these skills
in different domains remains restricted due to their inherent dependency on the
datasets, which poses a challenge when attempting to learn a skill-based policy
via RL for a target domain different from the datasets' domains. In this paper,
we present a novel offline skill learning framework DuSkill which employs a
guided Diffusion model to generate versatile skills extended from the limited
skills in datasets, thereby enhancing the robustness of policy learning for
tasks in different domains. Specifically, we devise a guided diffusion-based
skill decoder in conjunction with the hierarchical encoding to disentangle the
skill embedding space into two distinct representations, one for encapsulating
domain-invariant behaviors and the other for delineating the factors that
induce domain variations in the behaviors. Our DuSkill framework enhances the
diversity of skills learned offline, thus enabling to accelerate the learning
procedure of high-level policies for different domains. Through experiments, we
show that DuSkill outperforms other skill-based imitation learning and RL
algorithms for several long-horizon tasks, demonstrating its benefits in
few-shot imitation and online RL.
\\ ( https://arxiv.org/abs/2403.00225 ,  2026kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00254
Date: Fri, 1 Mar 2024 03:39:17 GMT   (3169kb,D)

Title: Cloud-based Federated Learning Framework for MRI Segmentation
Authors: Rukesh Prajapati and Amr S. El-Wakeel
Categories: cs.LG cs.AI cs.CV
\\
  In contemporary rural healthcare settings, the principal challenge in
diagnosing brain images is the scarcity of available data, given that most of
the existing deep learning models demand extensive training data to optimize
their performance, necessitating centralized processing methods that
potentially compromise data privacy. This paper proposes a novel framework
tailored for brain tissue segmentation in rural healthcare facilities. The
framework employs a deep reinforcement learning (DRL) environment in tandem
with a refinement model (RM) deployed locally at rural healthcare sites. The
proposed DRL model has a reduced parameter count and practicality for
implementation across distributed rural sites. To uphold data privacy and
enhance model generalization without transgressing privacy constraints, we
employ federated learning (FL) for cooperative model training. We demonstrate
the efficacy of our approach by training the network with a limited data set
and observing a substantial performance enhancement, mitigating inaccuracies
and irregularities in segmentation across diverse sites. Remarkably, the DRL
model attains an accuracy of up to 80%, surpassing the capabilities of
conventional convolutional neural networks when confronted with data
insufficiency. Incorporating our RM results in an additional accuracy
improvement of at least 10%, while FL contributes to a further accuracy
enhancement of up to 5%. Collectively, the framework achieves an average 92%
accuracy rate within rural healthcare settings characterized by data
constraints.
\\ ( https://arxiv.org/abs/2403.00254 ,  3169kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00273
Date: Fri, 1 Mar 2024 04:25:39 GMT   (607kb,D)

Title: ARED: Argentina Real Estate Dataset
Authors: Iv\'an Belenky
Categories: cs.LG cs.DL q-fin.ST
Comments: 3 pages, 6 figures
\\
  The Argentinian real estate market presents a unique case study characterized
by its unstable and rapidly shifting macroeconomic circumstances over the past
decades. Despite the existence of a few datasets for price prediction, there is
a lack of mixed modality datasets specifically focused on Argentina. In this
paper, the first edition of ARED is introduced. A comprehensive real estate
price prediction dataset series, designed for the Argentinian market. This
edition contains information solely for Jan-Feb 2024. It was found that despite
the short time range captured by this zeroth edition (44 days), time dependent
phenomena has been occurring mostly on a market level (market as a whole).
Nevertheless future editions of this dataset, will most likely contain
historical data. Each listing in ARED comprises descriptive features, and
variable-length sets of images.
\\ ( https://arxiv.org/abs/2403.00273 ,  607kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00276
Date: Fri, 1 Mar 2024 04:38:51 GMT   (21242kb,D)

Title: Graph Construction with Flexible Nodes for Traffic Demand Prediction
Authors: Jinyan Hou, Shan Liu, Ya Zhang and Haotong Qin
Categories: cs.LG
\\
  Graph neural networks (GNNs) have been widely applied in traffic demand
prediction, and transportation modes can be divided into station-based mode and
free-floating traffic mode. Existing research in traffic graph construction
primarily relies on map matching to construct graphs based on the road network.
However, the complexity and inhomogeneity of data distribution in free-floating
traffic demand forecasting make road network matching inflexible. To tackle
these challenges, this paper introduces a novel graph construction method
tailored to free-floating traffic mode. We propose a novel density-based
clustering algorithm (HDPC-L) to determine the flexible positioning of nodes in
the graph, overcoming the computational bottlenecks of traditional clustering
algorithms and enabling effective handling of large-scale datasets.
Furthermore, we extract valuable information from ridership data to initialize
the edge weights of GNNs. Comprehensive experiments on two real-world datasets,
the Shenzhen bike-sharing dataset and the Haikou ride-hailing dataset, show
that the method significantly improves the performance of the model. On
average, our models show an improvement in accuracy of around 25\% and 19.5\%
on the two datasets. Additionally, it significantly enhances computational
efficiency, reducing training time by approximately 12% and 32.5% on the two
datasets. We make our code available at
https://github.com/houjinyan/HDPC-L-ODInit.
\\ ( https://arxiv.org/abs/2403.00276 ,  21242kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00278
Date: Fri, 1 Mar 2024 04:50:04 GMT   (186kb,D)

Title: Shifted Interpolation for Differential Privacy
Authors: Jinho Bok, Weijie Su, Jason M. Altschuler
Categories: cs.LG cs.CR math.OC math.ST stat.ML stat.TH
Comments: 42 pages
\\
  Noisy gradient descent and its variants are the predominant algorithms for
differentially private machine learning. It is a fundamental question to
quantify their privacy leakage, yet tight characterizations remain open even in
the foundational setting of convex losses. This paper improves over previous
analyses by establishing (and refining) the "privacy amplification by
iteration" phenomenon in the unifying framework of $f$-differential
privacy--which tightly captures all aspects of the privacy loss and immediately
implies tighter privacy accounting in other notions of differential privacy,
e.g., $(\varepsilon,\delta)$-DP and Renyi DP. Our key technical insight is the
construction of shifted interpolated processes that unravel the popular
shifted-divergences argument, enabling generalizations beyond divergence-based
relaxations of DP. Notably, this leads to the first exact privacy analysis in
the foundational setting of strongly convex optimization. Our techniques extend
to many settings: convex/strongly convex, constrained/unconstrained,
full/cyclic/stochastic batches, and all combinations thereof. As an immediate
corollary, we recover the $f$-DP characterization of the exponential mechanism
for strongly convex optimization in Gopi et al. (2022), and moreover extend
this result to more general settings.
\\ ( https://arxiv.org/abs/2403.00278 ,  186kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00282
Date: Fri, 1 Mar 2024 04:57:13 GMT   (1900kb,D)

Title: Scale-Invariant Gradient Aggregation for Constrained Multi-Objective
  Reinforcement Learning
Authors: Dohyeong Kim, Mineui Hong, Jeongho Park, Songhwai Oh
Categories: cs.LG
Comments: 22 pages
\\
  Multi-objective reinforcement learning (MORL) aims to find a set of Pareto
optimal policies to cover various preferences. However, to apply MORL in
real-world applications, it is important to find policies that are not only
Pareto optimal but also satisfy pre-defined constraints for safety. To this
end, we propose a constrained MORL (CMORL) algorithm called Constrained
Multi-Objective Gradient Aggregator (CoMOGA). Recognizing the difficulty of
handling multiple objectives and constraints concurrently, CoMOGA relaxes the
original CMORL problem into a constrained optimization problem by transforming
the objectives into additional constraints. This novel transformation process
ensures that the converted constraints are invariant to the objective scales
while having the same effect as the original objectives. We show that the
proposed method converges to a local Pareto optimal policy while satisfying the
predefined constraints. Empirical evaluations across various tasks show that
the proposed method outperforms other baselines by consistently meeting
constraints and demonstrating invariance to the objective scales.
\\ ( https://arxiv.org/abs/2403.00282 ,  1900kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00337
Date: Fri, 1 Mar 2024 08:01:27 GMT   (12067kb,D)

Title: Nonlinear Sheaf Diffusion in Graph Neural Networks
Authors: Olga Zaghen
Categories: cs.LG
Comments: Thesis for Master's degree in Artificial Intelligence Systems
  (University of Trento), 65 pages
\\
  This work focuses on exploring the potential benefits of introducing a
nonlinear Laplacian in Sheaf Neural Networks for graph-related tasks. The
primary aim is to understand the impact of such nonlinearity on diffusion
dynamics, signal propagation, and performance of neural network architectures
in discrete-time settings. The study primarily emphasizes experimental
analysis, using real-world and synthetic datasets to validate the practical
effectiveness of different versions of the model. This approach shifts the
focus from an initial theoretical exploration to demonstrating the practical
utility of the proposed model.
\\ ( https://arxiv.org/abs/2403.00337 ,  12067kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00403
Date: Fri, 1 Mar 2024 09:49:53 GMT   (3599kb,D)

Title: Fractal interpolation in the context of prediction accuracy optimization
Authors: Alexandra Baicoianu, Cristina Gabriela Gavril\u{a}, Cristina Maria
  Pacurar, Victor Dan Pacurar
Categories: cs.LG
\\
  This paper focuses on the hypothesis of optimizing time series predictions
using fractal interpolation techniques. In general, the accuracy of machine
learning model predictions is closely related to the quality and quantitative
aspects of the data used, following the principle of \textit{garbage-in,
garbage-out}. In order to quantitatively and qualitatively augment datasets,
one of the most prevalent concerns of data scientists is to generate synthetic
data, which should follow as closely as possible the actual pattern of the
original data.
  This study proposes three different data augmentation strategies based on
fractal interpolation, namely the \textit{Closest Hurst Strategy},
\textit{Closest Values Strategy} and \textit{Formula Strategy}. To validate the
strategies, we used four public datasets from the literature, as well as a
private dataset obtained from meteorological records in the city of Brasov,
Romania. The prediction results obtained with the LSTM model using the
presented interpolation strategies showed a significant accuracy improvement
compared to the raw datasets, thus providing a possible answer to practical
problems in the field of remote sensing and sensor sensitivity. Moreover, our
methodologies answer some optimization-related open questions for the fractal
interpolation step using \textit{Optuna} framework.
\\ ( https://arxiv.org/abs/2403.00403 ,  3599kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00409
Date: Fri, 1 Mar 2024 09:55:18 GMT   (552kb,D)

Title: Provably Robust DPO: Aligning Language Models with Noisy Feedback
Authors: Sayak Ray Chowdhury, Anush Kini, Nagarajan Natarajan
Categories: cs.LG cs.CL
\\
  Learning from preference-based feedback has recently gained traction as a
promising approach to align language models with human interests. While these
aligned generative models have demonstrated impressive capabilities across
various tasks, their dependence on high-quality human preference data poses a
bottleneck in practical applications. Specifically, noisy (incorrect and
ambiguous) preference pairs in the dataset might restrict the language models
from capturing human intent accurately. While practitioners have recently
proposed heuristics to mitigate the effect of noisy preferences, a complete
theoretical understanding of their workings remain elusive.
  In this work, we aim to bridge this gap by by introducing a general framework
for policy optimization in the presence of random preference flips. We focus on
the direct preference optimization (DPO) algorithm in particular since it
assumes that preferences adhere to the Bradley-Terry-Luce (BTL) model, raising
concerns about the impact of noisy data on the learned policy. We design a
novel loss function, which de-bias the effect of noise on average, making a
policy trained by minimizing that loss robust to the noise. Under log-linear
parameterization of the policy class and assuming good feature coverage of the
SFT policy, we prove that the sub-optimality gap of the proposed robust DPO
(rDPO) policy compared to the optimal policy is of the order
$O(\frac{1}{1-2\epsilon}\sqrt{\frac{d}{n}})$, where $\epsilon < 1/2$ is flip
rate of labels, $d$ is policy parameter dimension and $n$ is size of dataset.
Our experiments on IMDb sentiment generation and Anthropic's helpful-harmless
dataset show that rDPO is robust to noise in preference labels compared to
vanilla DPO and other heuristics proposed by practitioners.
\\ ( https://arxiv.org/abs/2403.00409 ,  552kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00420
Date: Fri, 1 Mar 2024 10:16:46 GMT   (1080kb,D)

Title: Robust Deep Reinforcement Learning Through Adversarial Attacks and
  Training : A Survey
Authors: Lucas Schott, Josephine Delas, Hatem Hajri, Elies Gherbi, Reda Yaich,
  Nora Boulahia-Cuppens, Frederic Cuppens, Sylvain Lamprier
Categories: cs.LG cs.AI
Comments: 57 pages, 16 figues, 2 tables
\\
  Deep Reinforcement Learning (DRL) is an approach for training autonomous
agents across various complex environments. Despite its significant performance
in well known environments, it remains susceptible to minor conditions
variations, raising concerns about its reliability in real-world applications.
To improve usability, DRL must demonstrate trustworthiness and robustness. A
way to improve robustness of DRL to unknown changes in the conditions is
through Adversarial Training, by training the agent against well suited
adversarial attacks on the dynamics of the environment. Addressing this
critical issue, our work presents an in-depth analysis of contemporary
adversarial attack methodologies, systematically categorizing them and
comparing their objectives and operational mechanisms. This classification
offers a detailed insight into how adversarial attacks effectively act for
evaluating the resilience of DRL agents, thereby paving the way for enhancing
their robustness.
\\ ( https://arxiv.org/abs/2403.00420 ,  1080kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00485
Date: Fri, 1 Mar 2024 12:13:04 GMT   (1430kb,D)

Title: A Survey of Geometric Graph Neural Networks: Data Structures, Models and
  Applications
Authors: Jiaqi Han, Jiacheng Cen, Liming Wu, Zongzhao Li, Xiangzhe Kong, Rui
  Jiao, Ziyang Yu, Tingyang Xu, Fandi Wu, Zihe Wang, Hongteng Xu, Zhewei Wei,
  Yang Liu, Yu Rong, Wenbing Huang
Categories: cs.LG
\\
  Geometric graph is a special kind of graph with geometric features, which is
vital to model many scientific problems. Unlike generic graphs, geometric
graphs often exhibit physical symmetries of translations, rotations, and
reflections, making them ineffectively processed by current Graph Neural
Networks (GNNs). To tackle this issue, researchers proposed a variety of
Geometric Graph Neural Networks equipped with invariant/equivariant properties
to better characterize the geometry and topology of geometric graphs. Given the
current progress in this field, it is imperative to conduct a comprehensive
survey of data structures, models, and applications related to geometric GNNs.
In this paper, based on the necessary but concise mathematical preliminaries,
we provide a unified view of existing models from the geometric message passing
perspective. Additionally, we summarize the applications as well as the related
datasets to facilitate later research for methodology development and
experimental evaluation. We also discuss the challenges and future potential
directions of Geometric GNNs at the end of this survey.
\\ ( https://arxiv.org/abs/2403.00485 ,  1430kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00514
Date: Fri, 1 Mar 2024 13:25:10 GMT   (29011kb,D)

Title: Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter
  Lesson of Reinforcement Learning
Authors: Michal Nauman, Micha{\l} Bortkiewicz, Mateusz Ostaszewski, Piotr
  Mi{\l}o\'s, Tomasz Trzci\'nski, Marek Cygan
Categories: cs.LG
\\
  Recent advancements in off-policy Reinforcement Learning (RL) have
significantly improved sample efficiency, primarily due to the incorporation of
various forms of regularization that enable more gradient update steps than
traditional agents. However, many of these techniques have been tested in
limited settings, often on tasks from single simulation benchmarks and against
well-known algorithms rather than a range of regularization approaches. This
limits our understanding of the specific mechanisms driving RL improvements. To
address this, we implemented over 60 different off-policy agents, each
integrating established regularization techniques from recent state-of-the-art
algorithms. We tested these agents across 14 diverse tasks from 2 simulation
benchmarks. Our findings reveal that while the effectiveness of a specific
regularization setup varies with the task, certain combinations consistently
demonstrate robust and superior performance. Notably, a simple Soft
Actor-Critic agent, appropriately regularized, reliably solves dog tasks, which
were previously solved mainly through model-based approaches.
\\ ( https://arxiv.org/abs/2403.00514 ,  29011kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00540
Date: Fri, 1 Mar 2024 13:53:44 GMT   (3045kb,D)

Title: Epsilon-Greedy Thompson Sampling to Bayesian Optimization
Authors: Bach Do and Ruda Zhang
Categories: cs.LG math.OC stat.ML
\\
  Thompson sampling (TS) serves as a solution for addressing the
exploitation-exploration dilemma in Bayesian optimization (BO). While it
prioritizes exploration by randomly generating and maximizing sample paths of
Gaussian process (GP) posteriors, TS weakly manages its exploitation by
gathering information about the true objective function after each exploration
is performed. In this study, we incorporate the epsilon-greedy
($\varepsilon$-greedy) policy, a well-established selection strategy in
reinforcement learning, into TS to improve its exploitation. We first delineate
two extremes of TS applied for BO, namely the generic TS and a sample-average
TS. The former and latter promote exploration and exploitation, respectively.
We then use $\varepsilon$-greedy policy to randomly switch between the two
extremes. A small value of $\varepsilon \in (0,1)$ prioritizes exploitation,
and vice versa. We empirically show that $\varepsilon$-greedy TS with an
appropriate $\varepsilon$ is better than one of its two extremes and competes
with the other.
\\ ( https://arxiv.org/abs/2403.00540 ,  3045kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00542
Date: Fri, 1 Mar 2024 13:56:36 GMT   (1205kb)

Title: Machine Learning Training Optimization using the Barycentric Correction
  Procedure
Authors: Sofia Ramos-Pulido, Neil Hernandez-Gress and Hector G.
  Ceballos-Cancino (Tecnologico de Monterrey, Mexico)
Categories: cs.LG
Journal-ref: Computer Science & Information Technology (CS & IT) ISSN : 2231 -
  5403 Volume 14, Number 04, February 2024
\\
  Machine learning (ML) algorithms are predictively competitive algorithms with
many human-impact applications. However, the issue of long execution time
remains unsolved in the literature for high-dimensional spaces. This study
proposes combining ML algorithms with an efficient methodology known as the
barycentric correction procedure (BCP) to address this issue. This study uses
synthetic data and an educational dataset from a private university to show the
benefits of the proposed method. It was found that this combination provides
significant benefits related to time in synthetic and real data without losing
accuracy when the number of instances and dimensions increases. Additionally,
for high-dimensional spaces, it was proved that BCP and linear support vector
classification (LinearSVC), after an estimated feature map for the gaussian
radial basis function (RBF) kernel, were unfeasible in terms of computational
time and accuracy.
\\ ( https://arxiv.org/abs/2403.00542 ,  1205kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00550
Date: Fri, 1 Mar 2024 14:18:46 GMT   (256kb,D)

Title: Imitation Learning Datasets: A Toolkit For Creating Datasets, Training
  Agents and Benchmarking
Authors: Nathan Gavenski, Michael Luck, Odinaldo Rodrigues
Categories: cs.LG cs.AI
Comments: his paper has been accepted in the demonstration track for the 23rd
  International Conference on Autonomous Agents and Multi-Agent Systems
\\
  Imitation learning field requires expert data to train agents in a task. Most
often, this learning approach suffers from the absence of available data, which
results in techniques being tested on its dataset. Creating datasets is a
cumbersome process requiring researchers to train expert agents from scratch,
record their interactions and test each benchmark method with newly created
data. Moreover, creating new datasets for each new technique results in a lack
of consistency in the evaluation process since each dataset can drastically
vary in state and action distribution. In response, this work aims to address
these issues by creating Imitation Learning Datasets, a toolkit that allows
for: (i) curated expert policies with multithreaded support for faster dataset
creation; (ii) readily available datasets and techniques with precise
measurements; and (iii) sharing implementations of common imitation learning
techniques. Demonstration link:
https://nathangavenski.github.io/#/il-datasets-video
\\ ( https://arxiv.org/abs/2403.00550 ,  256kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00563
Date: Fri, 1 Mar 2024 14:41:51 GMT   (951kb,D)

Title: Indirectly Parameterized Concrete Autoencoders
Authors: Alfred Nilsson, Klas Wijk, Sai bharath chandra Gutha, Erik Englesson,
  Alexandra Hotti, Carlo Saccardi, Oskar Kviman, Jens Lagergren, Ricardo
  Vinuesa, Hossein Azizpour
Categories: cs.LG stat.ML
\\
  Feature selection is a crucial task in settings where data is
high-dimensional or acquiring the full set of features is costly. Recent
developments in neural network-based embedded feature selection show promising
results across a wide range of applications. Concrete Autoencoders (CAEs),
considered state-of-the-art in embedded feature selection, may struggle to
achieve stable joint optimization, hurting their training time and
generalization. In this work, we identify that this instability is correlated
with the CAE learning duplicate selections. To remedy this, we propose a simple
and effective improvement: Indirectly Parameterized CAEs (IP-CAEs). IP-CAEs
learn an embedding and a mapping from it to the Gumbel-Softmax distributions'
parameters. Despite being simple to implement, IP-CAE exhibits significant and
consistent improvements over CAE in both generalization and training time
across several datasets for reconstruction and classification. Unlike CAE,
IP-CAE effectively leverages non-linear relationships and does not require
retraining the jointly optimized decoder. Furthermore, our approach is, in
principle, generalizable to Gumbel-Softmax distributions beyond feature
selection.
\\ ( https://arxiv.org/abs/2403.00563 ,  951kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00564
Date: Fri, 1 Mar 2024 14:42:25 GMT   (8633kb,D)

Title: EfficientZero V2: Mastering Discrete and Continuous Control with Limited
  Data
Authors: Shengjie Wang, Shaohuai Liu, Weirui Ye, Jiacheng You and Yang Gao
Categories: cs.LG cs.AI cs.RO
Comments: 21 pages,10 figures
\\
  Sample efficiency remains a crucial challenge in applying Reinforcement
Learning (RL) to real-world tasks. While recent algorithms have made
significant strides in improving sample efficiency, none have achieved
consistently superior performance across diverse domains. In this paper, we
introduce EfficientZero V2, a general framework designed for sample-efficient
RL algorithms. We have expanded the performance of EfficientZero to multiple
domains, encompassing both continuous and discrete actions, as well as visual
and low-dimensional inputs. With a series of improvements we propose,
EfficientZero V2 outperforms the current state-of-the-art (SOTA) by a
significant margin in diverse tasks under the limited data setting.
EfficientZero V2 exhibits a notable advancement over the prevailing general
algorithm, DreamerV3, achieving superior outcomes in 50 of 66 evaluated tasks
across diverse benchmarks, such as Atari 100k, Proprio Control, and Vision
Control.
\\ ( https://arxiv.org/abs/2403.00564 ,  8633kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00574
Date: Fri, 1 Mar 2024 14:55:22 GMT   (20761kb,D)

Title: Beyond Single-Model Views for Deep Learning: Optimization versus
  Generalizability of Stochastic Optimization Algorithms
Authors: Toki Tahmid Inan, Mingrui Liu, Amarda Shehu
Categories: cs.LG
\\
  Despite an extensive body of literature on deep learning optimization, our
current understanding of what makes an optimization algorithm effective is
fragmented. In particular, we do not understand well whether enhanced
optimization translates to improved generalizability. Current research
overlooks the inherent stochastic nature of stochastic gradient descent (SGD)
and its variants, resulting in a lack of comprehensive benchmarking and insight
into their statistical performance. This paper aims to address this gap by
adopting a novel approach. Rather than solely evaluating the endpoint of
individual optimization trajectories, we draw from an ensemble of trajectories
to estimate the stationary distribution of stochastic optimizers. Our
investigation encompasses a wide array of techniques, including SGD and its
variants, flat-minima optimizers, and new algorithms we propose under the Basin
Hopping framework. Through our evaluation, which encompasses synthetic
functions with known minima and real-world problems in computer vision and
natural language processing, we emphasize fair benchmarking under a statistical
framework, comparing stationary distributions and establishing statistical
significance. Our study uncovers several key findings regarding the
relationship between training loss and hold-out accuracy, as well as the
comparable performance of SGD, noise-enabled variants, and novel optimizers
utilizing the BH framework. Notably, these algorithms demonstrate performance
on par with flat-minima optimizers like SAM, albeit with half the gradient
evaluations. We anticipate that our work will catalyze further exploration in
deep learning optimization, encouraging a shift away from single-model
approaches towards methodologies that acknowledge and leverage the stochastic
nature of optimizers.
\\ ( https://arxiv.org/abs/2403.00574 ,  20761kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00625
Date: Fri, 1 Mar 2024 16:01:28 GMT   (165kb,D)

Title: Bias Mitigation in Fine-tuning Pre-trained Models for Enhanced Fairness
  and Efficiency
Authors: Yixuan Zhang and Feng Zhou
Categories: cs.LG cs.CY
\\
  Fine-tuning pre-trained models is a widely employed technique in numerous
real-world applications. However, fine-tuning these models on new tasks can
lead to unfair outcomes. This is due to the absence of generalization
guarantees for fairness properties, regardless of whether the original
pre-trained model was developed with fairness considerations. To tackle this
issue, we introduce an efficient and robust fine-tuning framework specifically
designed to mitigate biases in new tasks. Our empirical analysis shows that the
parameters in the pre-trained model that affect predictions for different
demographic groups are different, so based on this observation, we employ a
transfer learning strategy that neutralizes the importance of these influential
weights, determined using Fisher information across demographic groups.
Additionally, we integrate this weight importance neutralization strategy with
a matrix factorization technique, which provides a low-rank approximation of
the weight matrix using fewer parameters, reducing the computational demands.
Experiments on multiple pre-trained models and new tasks demonstrate the
effectiveness of our method.
\\ ( https://arxiv.org/abs/2403.00625 ,  165kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00642
Date: Fri, 1 Mar 2024 16:22:05 GMT   (765kb,D)

Title: Rethinking The Uniformity Metric in Self-Supervised Learning
Authors: Xianghong Fang, Jian Li, Qiang Sun, Benyou Wang
Categories: cs.LG cs.AI cs.CV
Journal-ref: ICLR 2024
\\
  Uniformity plays a crucial role in the assessment of learned representations,
contributing to a deeper comprehension of self-supervised learning. The seminal
work by \citet{Wang2020UnderstandingCR} introduced a uniformity metric that
quantitatively measures the collapse degree of learned representations.
Directly optimizing this metric together with alignment proves to be effective
in preventing constant collapse. However, we present both theoretical and
empirical evidence revealing that this metric lacks sensitivity to dimensional
collapse, highlighting its limitations. To address this limitation and design a
more effective uniformity metric, this paper identifies five fundamental
properties, some of which the existing uniformity metric fails to meet. We
subsequently introduce a novel uniformity metric that satisfies all of these
desiderata and exhibits sensitivity to dimensional collapse. When applied as an
auxiliary loss in various established self-supervised methods, our proposed
uniformity metric consistently enhances their performance in downstream
tasks.Our code was released at
https://github.com/sunset-clouds/WassersteinUniformityMetric.
\\ ( https://arxiv.org/abs/2403.00642 ,  765kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00646
Date: Fri, 1 Mar 2024 16:26:47 GMT   (1459kb,D)

Title: Stability-Certified Learning of Control Systems with Quadratic
  Nonlinearities
Authors: Igor Pontes Duff and Pawan Goyal and Peter Benner
Categories: cs.LG math.DS math.OC
Comments: 12 pages, 4 figures
\\
  This work primarily focuses on an operator inference methodology aimed at
constructing low-dimensional dynamical models based on a priori hypotheses
about their structure, often informed by established physics or expert
insights. Stability is a fundamental attribute of dynamical systems, yet it is
not always assured in models derived through inference. Our main objective is
to develop a method that facilitates the inference of quadratic control
dynamical systems with inherent stability guarantees. To this aim, we
investigate the stability characteristics of control systems with
energy-preserving nonlinearities, thereby identifying conditions under which
such systems are bounded-input bounded-state stable. These insights are
subsequently applied to the learning process, yielding inferred models that are
inherently stable by design. The efficacy of our proposed framework is
demonstrated through a couple of numerical examples.
\\ ( https://arxiv.org/abs/2403.00646 ,  1459kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00669
Date: Fri, 1 Mar 2024 17:01:47 GMT   (819kb,D)

Title: Advancing Additive Manufacturing through Deep Learning: A Comprehensive
  Review of Current Progress and Future Challenges
Authors: Amirul Islam Saimon, Emmanuel Yangue, Xiaowei Yue, Zhenyu (James)
  Kong, Chenang Liu
Categories: cs.LG
\\
  Additive manufacturing (AM) has already proved itself to be the potential
alternative to widely-used subtractive manufacturing due to its extraordinary
capacity of manufacturing highly customized products with minimum material
wastage. Nevertheless, it is still not being considered as the primary choice
for the industry due to some of its major inherent challenges, including
complex and dynamic process interactions, which are sometimes difficult to
fully understand even with traditional machine learning because of the
involvement of high-dimensional data such as images, point clouds, and voxels.
However, the recent emergence of deep learning (DL) is showing great promise in
overcoming many of these challenges as DL can automatically capture complex
relationships from high-dimensional data without hand-crafted feature
extraction. Therefore, the volume of research in the intersection of AM and DL
is exponentially growing each year which makes it difficult for the researchers
to keep track of the trend and future potential directions. Furthermore, to the
best of our knowledge, there is no comprehensive review paper in this research
track summarizing the recent studies. Therefore, this paper reviews the recent
studies that apply DL for making the AM process better with a high-level
summary of their contributions and limitations. Finally, it summarizes the
current challenges and recommends some of the promising opportunities in this
domain for further investigation with a special focus on generalizing DL models
for wide-range of geometry types, managing uncertainties both in AM data and DL
models, overcoming limited and noisy AM data issues by incorporating generative
models, and unveiling the potential of interpretable DL for AM.
\\ ( https://arxiv.org/abs/2403.00669 ,  819kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00673
Date: Fri, 1 Mar 2024 17:05:22 GMT   (42332kb,D)

Title: Snapshot Reinforcement Learning: Leveraging Prior Trajectories for
  Efficiency
Authors: Yanxiao Zhao, Yangge Qian, Tianyi Wang, Jingyang Shan, Xiaolin Qin
Categories: cs.LG
Comments: Under review
\\
  Deep reinforcement learning (DRL) algorithms require substantial samples and
computational resources to achieve higher performance, which restricts their
practical application and poses challenges for further development. Given the
constraint of limited resources, it is essential to leverage existing
computational work (e.g., learned policies, samples) to enhance sample
efficiency and reduce the computational resource consumption of DRL algorithms.
Previous works to leverage existing computational work require intrusive
modifications to existing algorithms and models, designed specifically for
specific algorithms, lacking flexibility and universality. In this paper, we
present the Snapshot Reinforcement Learning (SnapshotRL) framework, which
enhances sample efficiency by simply altering environments, without making any
modifications to algorithms and models. By allowing student agents to choose
states in teacher trajectories as the initial state to sample, SnapshotRL can
effectively utilize teacher trajectories to assist student agents in training,
allowing student agents to explore a larger state space at the early training
phase. We propose a simple and effective SnapshotRL baseline algorithm, S3RL,
which integrates well with existing DRL algorithms. Our experiments demonstrate
that integrating S3RL with TD3, SAC, and PPO algorithms on the MuJoCo benchmark
significantly improves sample efficiency and average return, without extra
samples and additional computational resources.
\\ ( https://arxiv.org/abs/2403.00673 ,  42332kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00675
Date: Fri, 1 Mar 2024 17:08:30 GMT   (628kb,D)

Title: Reusing Historical Trajectories in Natural Policy Gradient via
  Importance Sampling: Convergence and Convergence Rate
Authors: Yifan Lin, Yuhao Wang, Enlu Zhou
Categories: cs.LG math.OC
\\
  Reinforcement learning provides a mathematical framework for learning-based
control, whose success largely depends on the amount of data it can utilize.
The efficient utilization of historical trajectories obtained from previous
policies is essential for expediting policy optimization. Empirical evidence
has shown that policy gradient methods based on importance sampling work well.
However, existing literature often neglect the interdependence between
trajectories from different iterations, and the good empirical performance
lacks a rigorous theoretical justification. In this paper, we study a variant
of the natural policy gradient method with reusing historical trajectories via
importance sampling. We show that the bias of the proposed estimator of the
gradient is asymptotically negligible, the resultant algorithm is convergent,
and reusing past trajectories helps improve the convergence rate. We further
apply the proposed estimator to popular policy optimization algorithms such as
trust region policy optimization. Our theoretical results are verified on
classical benchmarks.
\\ ( https://arxiv.org/abs/2403.00675 ,  628kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00680
Date: Fri, 1 Mar 2024 17:12:53 GMT   (393kb,D)

Title: Scalable Learning of Item Response Theory Models
Authors: Susanne Frick and Amer Krivo\v{s}ija and Alexander Munteanu
Categories: cs.LG cs.DS stat.ML
Comments: AISTATS 2024
\\
  Item Response Theory (IRT) models aim to assess latent abilities of $n$
examinees along with latent difficulty characteristics of $m$ test items from
categorical data that indicates the quality of their corresponding answers.
Classical psychometric assessments are based on a relatively small number of
examinees and items, say a class of $200$ students solving an exam comprising
$10$ problems. More recent global large scale assessments such as PISA, or
internet studies, may lead to significantly increased numbers of participants.
Additionally, in the context of Machine Learning where algorithms take the role
of examinees and data analysis problems take the role of items, both $n$ and
$m$ may become very large, challenging the efficiency and scalability of
computations. To learn the latent variables in IRT models from large data, we
leverage the similarity of these models to logistic regression, which can be
approximated accurately using small weighted subsets called coresets. We
develop coresets for their use in alternating IRT training algorithms,
facilitating scalable learning from large data.
\\ ( https://arxiv.org/abs/2403.00680 ,  393kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00715
Date: Fri, 1 Mar 2024 18:03:49 GMT   (51kb)

Title: Adaptive Learning Rate for Follow-the-Regularized-Leader: Competitive
  Ratio Analysis and Best-of-Both-Worlds
Authors: Shinji Ito, Taira Tsuchiya, Junya Honda
Categories: cs.LG stat.ML
\\
  Follow-The-Regularized-Leader (FTRL) is known as an effective and versatile
approach in online learning, where appropriate choice of the learning rate is
crucial for smaller regret. To this end, we formulate the problem of adjusting
FTRL's learning rate as a sequential decision-making problem and introduce the
framework of competitive analysis. We establish a lower bound for the
competitive ratio and propose update rules for learning rate that achieves an
upper bound within a constant factor of this lower bound. Specifically, we
illustrate that the optimal competitive ratio is characterized by the
(approximate) monotonicity of components of the penalty term, showing that a
constant competitive ratio is achievable if the components of the penalty term
form a monotonically non-increasing sequence, and derive a tight competitive
ratio when penalty terms are $\xi$-approximately monotone non-increasing. Our
proposed update rule, referred to as \textit{stability-penalty matching}, also
facilitates constructing the Best-Of-Both-Worlds (BOBW) algorithms for
stochastic and adversarial environments. In these environments our result
contributes to achieve tighter regret bound and broaden the applicability of
algorithms for various settings such as multi-armed bandits, graph bandits,
linear bandits, and contextual bandits.
\\ ( https://arxiv.org/abs/2403.00715 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00720
Date: Fri, 1 Mar 2024 18:12:46 GMT   (202kb,D)

Title: Subhomogeneous Deep Equilibrium Models
Authors: Pietro Sittoni, Francesco Tudisco
Categories: cs.LG cs.NA math.NA math.OC
\\
  Implicit-depth neural networks have grown as powerful alternatives to
traditional networks in various applications in recent years. However, these
models often lack guarantees of existence and uniqueness, raising stability,
performance, and reproducibility issues. In this paper, we present a new
analysis of the existence and uniqueness of fixed points for implicit-depth
neural networks based on the concept of subhomogeneous operators and the
nonlinear Perron-Frobenius theory. Compared to previous similar analyses, our
theory allows for weaker assumptions on the parameter matrices, thus yielding a
more flexible framework for well-defined implicit networks. We illustrate the
performance of the resulting subhomogeneous networks on feed-forward,
convolutional, and graph neural network examples.
\\ ( https://arxiv.org/abs/2403.00720 ,  202kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00745
Date: Fri, 1 Mar 2024 18:43:51 GMT   (16911kb,D)

Title: AtP*: An efficient and scalable method for localizing LLM behaviour to
  components
Authors: J\'anos Kram\'ar, Tom Lieberum, Rohin Shah, Neel Nanda (Google
  DeepMind)
Categories: cs.LG cs.CL
\\
  Activation Patching is a method of directly computing causal attributions of
behavior to model components. However, applying it exhaustively requires a
sweep with cost scaling linearly in the number of model components, which can
be prohibitively expensive for SoTA Large Language Models (LLMs). We
investigate Attribution Patching (AtP), a fast gradient-based approximation to
Activation Patching and find two classes of failure modes of AtP which lead to
significant false negatives. We propose a variant of AtP called AtP*, with two
changes to address these failure modes while retaining scalability. We present
the first systematic study of AtP and alternative methods for faster activation
patching and show that AtP significantly outperforms all other investigated
methods, with AtP* providing further significant improvement. Finally, we
provide a method to bound the probability of remaining false negatives of AtP*
estimates.
\\ ( https://arxiv.org/abs/2403.00745 ,  16911kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2403.00014 (*cross-listing*)
Date: Tue, 27 Feb 2024 09:35:54 GMT   (4431kb,D)

Title: GIN-SD: Source Detection in Graphs with Incomplete Nodes via Positional
  Encoding and Attentive Fusion
Authors: Le Cheng, Peican Zhu, Keke Tang, Chao Gao, Zhen Wang
Categories: cs.SI cs.AI cs.LG
Comments: The paper is accepted by AAAI24
\\
  Source detection in graphs has demonstrated robust efficacy in the domain of
rumor source identification. Although recent solutions have enhanced
performance by leveraging deep neural networks, they often require complete
user data. In this paper, we address a more challenging task, rumor source
detection with incomplete user data, and propose a novel framework, i.e.,
Source Detection in Graphs with Incomplete Nodes via Positional Encoding and
Attentive Fusion (GIN-SD), to tackle this challenge. Specifically, our approach
utilizes a positional embedding module to distinguish nodes that are incomplete
and employs a self-attention mechanism to focus on nodes with greater
information transmission capacity. To mitigate the prediction bias caused by
the significant disparity between the numbers of source and non-source nodes,
we also introduce a class-balancing mechanism. Extensive experiments validate
the effectiveness of GIN-SD and its superiority to state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.00014 ,  4431kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00023 (*cross-listing*)
Date: Wed, 28 Feb 2024 14:51:18 GMT   (3043kb,D)

Title: Auditable Homomorphic-based Decentralized Collaborative AI with
  Attribute-based Differential Privacy
Authors: Lo-Yao Yeh, Sheng-Po Tseng, Chia-Hsun Lu, Chih-Ya Shen
Categories: cs.CR cs.AI cs.LG
Comments: 12 pages, 9 figures
MSC-class: 68T01
\\
  In recent years, the notion of federated learning (FL) has led to the new
paradigm of distributed artificial intelligence (AI) with privacy preservation.
However, most current FL systems suffer from data privacy issues due to the
requirement of a trusted third party. Although some previous works introduce
differential privacy to protect the data, however, it may also significantly
deteriorate the model performance. To address these issues, we propose a novel
decentralized collaborative AI framework, named Auditable Homomorphic-based
Decentralised Collaborative AI (AerisAI), to improve security with homomorphic
encryption and fine-grained differential privacy. Our proposed AerisAI directly
aggregates the encrypted parameters with a blockchain-based smart contract to
get rid of the need of a trusted third party. We also propose a brand-new
concept for eliminating the negative impacts of differential privacy for model
performance. Moreover, the proposed AerisAI also provides the broadcast-aware
group key management based on ciphertext-policy attribute-based encryption
(CPABE) to achieve fine-grained access control based on different service-level
agreements. We provide a formal theoretical analysis of the proposed AerisAI as
well as the functionality comparison with the other baselines. We also conduct
extensive experiments on real datasets to evaluate the proposed approach. The
experimental results indicate that our proposed AerisAI significantly
outperforms the other state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2403.00023 ,  3043kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00030 (*cross-listing*)
Date: Wed, 28 Feb 2024 20:02:55 GMT   (1309kb,D)

Title: GraphPub: Generation of Differential Privacy Graph with High
  Availability
Authors: Wanghan Xu, Bin Shi, Ao Liu, Jiqiang Zhang, Bo Dong
Categories: cs.SI cs.AI cs.CR cs.LG
\\
  In recent years, with the rapid development of graph neural networks (GNN),
more and more graph datasets have been published for GNN tasks. However, when
an upstream data owner publishes graph data, there are often many privacy
concerns, because many real-world graph data contain sensitive information like
person's friend list. Differential privacy (DP) is a common method to protect
privacy, but due to the complex topological structure of graph data, applying
DP on graphs often affects the message passing and aggregation of GNN models,
leading to a decrease in model accuracy. In this paper, we propose a novel
graph edge protection framework, graph publisher (GraphPub), which can protect
graph topology while ensuring that the availability of data is basically
unchanged. Through reverse learning and the encoder-decoder mechanism, we
search for some false edges that do not have a large negative impact on the
aggregation of node features, and use them to replace some real edges. The
modified graph will be published, which is difficult to distinguish between
real and false data. Sufficient experiments prove that our framework achieves
model accuracy close to the original graph with an extremely low privacy
budget.
\\ ( https://arxiv.org/abs/2403.00030 ,  1309kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00032 (*cross-listing*)
Date: Wed, 28 Feb 2024 22:59:26 GMT   (31503kb,D)

Title: Time to Cite: Modeling Citation Networks using the Dynamic Impact
  Single-Event Embedding Model
Authors: Nikolaos Nakis, Abdulkadir Celikkanat, Louis Boucherie, Sune Lehmann,
  Morten M{\o}rup
Categories: cs.SI cs.AI cs.DL
Comments: Accepted for AISTATS 2024
\\
  Understanding the structure and dynamics of scientific research, i.e., the
science of science (SciSci), has become an important area of research in order
to address imminent questions including how scholars interact to advance
science, how disciplines are related and evolve, and how research impact can be
quantified and predicted. Central to the study of SciSci has been the analysis
of citation networks. Here, two prominent modeling methodologies have been
employed: one is to assess the citation impact dynamics of papers using
parametric distributions, and the other is to embed the citation networks in a
latent space optimal for characterizing the static relations between papers in
terms of their citations. Interestingly, citation networks are a prominent
example of single-event dynamic networks, i.e., networks for which each dyad
only has a single event (i.e., the point in time of citation). We presently
propose a novel likelihood function for the characterization of such
single-event networks. Using this likelihood, we propose the Dynamic Impact
Single-Event Embedding model (DISEE). The \textsc{\modelabbrev} model
characterizes the scientific interactions in terms of a latent distance model
in which random effects account for citation heterogeneity while the
time-varying impact is characterized using existing parametric representations
for assessment of dynamic impact. We highlight the proposed approach on several
real citation networks finding that the DISEE well reconciles static latent
distance network embedding approaches with classical dynamic impact
assessments.
\\ ( https://arxiv.org/abs/2403.00032 ,  31503kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00037 (*cross-listing*)
Date: Thu, 29 Feb 2024 06:40:53 GMT   (1112kb,D)

Title: Evolving to the Future: Unseen Event Adaptive Fake News Detection on
  Social Media
Authors: Jiajun Zhang, Zhixun Li, Qiang Liu, Shu Wu, Liang Wang
Categories: cs.SI cs.AI cs.CL
\\
  With the rapid development of social media, the wide dissemination of fake
news on social media is increasingly threatening both individuals and society.
In the dynamic landscape of social media, fake news detection aims to develop a
model trained on news reporting past events. The objective is to predict and
identify fake news about future events, which often relate to subjects entirely
different from those in the past. However, existing fake detection methods
exhibit a lack of robustness and cannot generalize to unseen events. To address
this, we introduce Future ADaptive Event-based Fake news Detection (FADE)
framework. Specifically, we train a target predictor through an adaptive
augmentation strategy and graph contrastive learning to make more robust
overall predictions. Simultaneously, we independently train an event-only
predictor to obtain biased predictions. Then we further mitigate event bias by
obtaining the final prediction by subtracting the output of the event-only
predictor from the output of the target predictor. Encouraging results from
experiments designed to emulate real-world social media conditions validate the
effectiveness of our method in comparison to existing state-of-the-art
approaches.
\\ ( https://arxiv.org/abs/2403.00037 ,  1112kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00039 (*cross-listing*)
Date: Thu, 29 Feb 2024 09:43:50 GMT   (838kb,D)

Title: FhGenie: A Custom, Confidentiality-preserving Chat AI for Corporate and
  Scientific Use
Authors: Ingo Weber, Hendrik Linka, Daniel Mertens, Tamara Muryshkin, Heinrich
  Opgenoorth, Stefan Langer
Categories: cs.SE cs.AI cs.HC
\\
  Since OpenAI's release of ChatGPT, generative AI has received significant
attention across various domains. These AI-based chat systems have the
potential to enhance the productivity of knowledge workers in diverse tasks.
However, the use of free public services poses a risk of data leakage, as
service providers may exploit user input for additional training and
optimization without clear boundaries. Even subscription-based alternatives
sometimes lack transparency in handling user data. To address these concerns
and enable Fraunhofer staff to leverage this technology while ensuring
confidentiality, we have designed and developed a customized chat AI called
FhGenie (genie being a reference to a helpful spirit). Within few days of its
release, thousands of Fraunhofer employees started using this service. As
pioneers in implementing such a system, many other organizations have followed
suit. Our solution builds upon commercial large language models (LLMs), which
we have carefully integrated into our system to meet our specific requirements
and compliance constraints, including confidentiality and GDPR. In this paper,
we share detailed insights into the architectural considerations, design,
implementation, and subsequent updates of FhGenie. Additionally, we discuss
challenges, observations, and the core lessons learned from its productive
usage.
\\ ( https://arxiv.org/abs/2403.00039 ,  838kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00044 (*cross-listing*)
Date: Thu, 29 Feb 2024 15:19:35 GMT   (556kb,D)

Title: Scaling up Dynamic Edge Partition Models via Stochastic Gradient MCMC
Authors: Sikun Yang, Heinz Koeppl
Categories: cs.SI cs.AI cs.LG
\\
  The edge partition model (EPM) is a generative model for extracting an
overlapping community structure from static graph-structured data. In the EPM,
the gamma process (GaP) prior is adopted to infer the appropriate number of
latent communities, and each vertex is endowed with a gamma distributed
positive memberships vector. Despite having many attractive properties,
inference in the EPM is typically performed using Markov chain Monte Carlo
(MCMC) methods that prevent it from being applied to massive network data. In
this paper, we generalize the EPM to account for dynamic enviroment by
representing each vertex with a positive memberships vector constructed using
Dirichlet prior specification, and capturing the time-evolving behaviour of
vertices via a Dirichlet Markov chain construction. A simple-to-implement Gibbs
sampler is proposed to perform posterior computation using Negative- Binomial
augmentation technique. For large network data, we propose a stochastic
gradient Markov chain Monte Carlo (SG-MCMC) algorithm for scalable inference in
the proposed model. The experimental results show that the novel methods
achieve competitive performance in terms of link prediction, while being much
faster.
\\ ( https://arxiv.org/abs/2403.00044 ,  556kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00046 (*cross-listing*)
Date: Thu, 29 Feb 2024 16:09:02 GMT   (621kb,D)

Title: SEED: Customize Large Language Models with Sample-Efficient Adaptation
  for Code Generation
Authors: Xue Jiang, Yihong Dong, Zhi Jin, Ge Li
Categories: cs.SE cs.AI cs.CL
\\
  Although Large Language Models (LLMs) have made significant progress in code
generation, they still struggle with code generation tasks in specific
scenarios. These scenarios usually necessitate the adaptation of LLMs to
fulfill specific needs, but the limited training data available in practice
leads to poor code generation performance. How to effectively adapt LLMs to new
scenarios with fewer training samples is a major challenge for current code
generation. In this paper, we propose a novel adaptation approach named SEED,
which stands for Sample-Efficient adaptation with Error-Driven learning for
code generation. SEED leverages the errors made by LLMs as learning
opportunities, using error revision to overcome its own shortcomings, thus
achieving efficient learning. Specifically, SEED involves identifying error
code generated by LLMs, employing Self-revise for code revision, optimizing the
model with revised code, and iteratively adapting the process for continuous
improvement. Experimental results show that, compared to traditional
fine-tuning approaches, SEED achieves superior performance with fewer training
samples, showing a relative improvement of 27.2%-325.0% in Pass@1. We also
validate the effectiveness of Self-revise, which generates revised code that
optimizes the model more efficiently compared to the code samples from
datasets. Moreover, SEED consistently demonstrates strong performance across
various LLMs, underscoring its generalizability.
\\ ( https://arxiv.org/abs/2403.00046 ,  621kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00108 (*cross-listing*)
Date: Thu, 29 Feb 2024 20:25:16 GMT   (8695kb,D)

Title: LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario
Authors: Hongyi Liu, Zirui Liu, Ruixiang Tang, Jiayi Yuan, Shaochen Zhong,
  Yu-Neng Chuang, Li Li, Rui Chen, Xia Hu
Categories: cs.CR cs.AI cs.CL
\\
  Fine-tuning LLMs is crucial to enhancing their task-specific performance and
ensuring model behaviors are aligned with human preferences. Among various
fine-tuning methods, LoRA is popular for its efficiency and ease to use,
allowing end-users to easily post and adopt lightweight LoRA modules on
open-source platforms to tailor their model for different customization.
However, such a handy share-and-play setting opens up new attack surfaces, that
the attacker can render LoRA as an attacker, such as backdoor injection, and
widely distribute the adversarial LoRA to the community easily. This can result
in detrimental outcomes. Despite the huge potential risks of sharing LoRA
modules, this aspect however has not been fully explored. To fill the gap, in
this study we thoroughly investigate the attack opportunities enabled in the
growing share-and-play scenario. Specifically, we study how to inject backdoor
into the LoRA module and dive deeper into LoRA's infection mechanisms. We found
that training-free mechanism is possible in LoRA backdoor injection. We also
discover the impact of backdoor attacks with the presence of multiple LoRA
adaptions concurrently as well as LoRA based backdoor transferability. Our aim
is to raise awareness of the potential risks under the emerging share-and-play
scenario, so as to proactively prevent potential consequences caused by
LoRA-as-an-Attack. Warning: the paper contains potential offensive content
generated by models.
\\ ( https://arxiv.org/abs/2403.00108 ,  8695kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00154 (*cross-listing*)
Date: Thu, 29 Feb 2024 22:11:20 GMT   (1625kb,D)

Title: LLMs in Political Science: Heralding a New Era of Visual Analysis
Authors: Yu Wang and Mengying Xing
Categories: cs.CV cs.AI
Comments: 7 pages, 3 tables
\\
  Interest is increasing among political scientists in leveraging the extensive
information available in images. However, the challenge of interpreting these
images lies in the need for specialized knowledge in computer vision and access
to specialized hardware. As a result, image analysis has been limited to a
relatively small group within the political science community. This landscape
could potentially change thanks to the rise of large language models (LLMs).
This paper aims to raise awareness of the feasibility of using Gemini for image
content analysis. A retrospective analysis was conducted on a corpus of 688
images. Content reports were elicited from Gemini for each image and then
manually evaluated by the authors. We find that Gemini is highly accurate in
performing object detection, which is arguably the most common and fundamental
task in image analysis for political scientists. Equally important, we show
that it is easy to implement as the entire command consists of a single prompt
in natural language; it is fast to run and should meet the time budget of most
researchers; and it is free to use and does not require any specialized
hardware. In addition, we illustrate how political scientists can leverage
Gemini for other image understanding tasks, including face identification,
sentiment analysis, and caption generation. Our findings suggest that Gemini
and other similar LLMs have the potential to drastically stimulate and
accelerate image research in political science and social sciences more
broadly.
\\ ( https://arxiv.org/abs/2403.00154 ,  1625kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00172 (*cross-listing*)
Date: Thu, 29 Feb 2024 22:42:23 GMT   (649kb,D)

Title: Go Beyond Black-box Policies: Rethinking the Design of Learning Agent
  for Interpretable and Verifiable HVAC Control
Authors: Zhiyu An, Xianzhong Ding, Wan Du
Categories: eess.SY cs.AI cs.LG cs.SY
Comments: Accepted for the 61st Design Automation Conference (DAC)
\\
  Recent research has shown the potential of Model-based Reinforcement Learning
(MBRL) to enhance energy efficiency of Heating, Ventilation, and Air
Conditioning (HVAC) systems. However, existing methods rely on black-box
thermal dynamics models and stochastic optimizers, lacking reliability
guarantees and posing risks to occupant health. In this work, we overcome the
reliability bottleneck by redesigning HVAC controllers using decision trees
extracted from existing thermal dynamics models and historical data. Our
decision tree-based policies are deterministic, verifiable, interpretable, and
more energy-efficient than current MBRL methods. First, we introduce a novel
verification criterion for RL agents in HVAC control based on domain knowledge.
Second, we develop a policy extraction procedure that produces a verifiable
decision tree policy. We found that the high dimensionality of the thermal
dynamics model input hinders the efficiency of policy extraction. To tackle the
dimensionality challenge, we leverage importance sampling conditioned on
historical data distributions, significantly improving policy extraction
efficiency. Lastly, we present an offline verification algorithm that
guarantees the reliability of a control policy. Extensive experiments show that
our method saves 68.4% more energy and increases human comfort gain by 14.8%
compared to the state-of-the-art method, in addition to an 1127x reduction in
computation overhead. Our code and data are available at
https://github.com/ryeii/Veri_HVAC
\\ ( https://arxiv.org/abs/2403.00172 ,  649kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00175 (*cross-listing*)
Date: Thu, 29 Feb 2024 22:59:27 GMT   (1703kb,D)

Title: FusionVision: A comprehensive approach of 3D object reconstruction and
  segmentation from RGB-D cameras using YOLO and fast segment anything
Authors: Safouane El Ghazouali, Youssef Mhirit, Ali Oukhrid, Umberto
  Michelucci, Hichem Nouira
Categories: cs.CV cs.AI
Comments: 14 pages, 9 figures, 1 table
\\
  In the realm of computer vision, the integration of advanced techniques into
the processing of RGB-D camera inputs poses a significant challenge, given the
inherent complexities arising from diverse environmental conditions and varying
object appearances. Therefore, this paper introduces FusionVision, an
exhaustive pipeline adapted for the robust 3D segmentation of objects in RGB-D
imagery. Traditional computer vision systems face limitations in simultaneously
capturing precise object boundaries and achieving high-precision object
detection on depth map as they are mainly proposed for RGB cameras. To address
this challenge, FusionVision adopts an integrated approach by merging
state-of-the-art object detection techniques, with advanced instance
segmentation methods. The integration of these components enables a holistic
(unified analysis of information obtained from both color \textit{RGB} and
depth \textit{D} channels) interpretation of RGB-D data, facilitating the
extraction of comprehensive and accurate object information. The proposed
FusionVision pipeline employs YOLO for identifying objects within the RGB image
domain. Subsequently, FastSAM, an innovative semantic segmentation model, is
applied to delineate object boundaries, yielding refined segmentation masks.
The synergy between these components and their integration into 3D scene
understanding ensures a cohesive fusion of object detection and segmentation,
enhancing overall precision in 3D object segmentation. The code and pre-trained
models are publicly available at https://github.com/safouaneelg/FusionVision/.
\\ ( https://arxiv.org/abs/2403.00175 ,  1703kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00190 (*cross-listing*)
Date: Thu, 29 Feb 2024 23:43:08 GMT   (372kb)

Title: Identification of important nodes in the information propagation network
  based on the artificial intelligence method
Authors: Bin Yuan, Tianbo Song, Jerry Yao
Categories: cs.SI cs.AI
\\
  This study presents an integrated approach for identifying key nodes in
information propagation networks using advanced artificial intelligence
methods. We introduce a novel technique that combines the Decision-making Trial
and Evaluation Laboratory (DEMATEL) method with the Global Structure Model
(GSM), creating a synergistic model that effectively captures both local and
global influences within a network. This method is applied across various
complex networks, such as social, transportation, and communication systems,
utilizing the Global Network Influence Dataset (GNID). Our analysis highlights
the structural dynamics and resilience of these networks, revealing insights
into node connectivity and community formation. The findings demonstrate the
effectiveness of our AI-based approach in offering a comprehensive
understanding of network behavior, contributing significantly to strategic
network analysis and optimization.
\\ ( https://arxiv.org/abs/2403.00190 ,  372kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00196 (*cross-listing*)
Date: Thu, 29 Feb 2024 23:52:15 GMT   (5509kb,D)

Title: Learning to Find Missing Video Frames with Synthetic Data Augmentation:
  A General Framework and Application in Generating Thermal Images Using RGB
  Cameras
Authors: Mathias Viborg Andersen, Ross Greer, Andreas M{\o}gelmose, Mohan
  Trivedi
Categories: cs.CV cs.AI cs.LG
\\
  Advanced Driver Assistance Systems (ADAS) in intelligent vehicles rely on
accurate driver perception within the vehicle cabin, often leveraging a
combination of sensing modalities. However, these modalities operate at varying
rates, posing challenges for real-time, comprehensive driver state monitoring.
This paper addresses the issue of missing data due to sensor frame rate
mismatches, introducing a generative model approach to create synthetic yet
realistic thermal imagery. We propose using conditional generative adversarial
networks (cGANs), specifically comparing the pix2pix and CycleGAN
architectures. Experimental results demonstrate that pix2pix outperforms
CycleGAN, and utilizing multi-view input styles, especially stacked views,
enhances the accuracy of thermal image generation. Moreover, the study
evaluates the model's generalizability across different subjects, revealing the
importance of individualized training for optimal performance. The findings
suggest the potential of generative models in addressing missing frames,
advancing driver state monitoring for intelligent vehicles, and underscoring
the need for continued research in model generalization and customization.
\\ ( https://arxiv.org/abs/2403.00196 ,  5509kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00250 (*cross-listing*)
Date: Fri, 1 Mar 2024 03:27:08 GMT   (987kb,D)

Title: Rethinking Classifier Re-Training in Long-Tailed Recognition: A Simple
  Logits Retargeting Approach
Authors: Han Lu, Siyu Sun, Yichen Xie, Liqing Zhang, Xiaokang Yang, Junchi Yan
Categories: cs.CV cs.AI
\\
  In the long-tailed recognition field, the Decoupled Training paradigm has
demonstrated remarkable capabilities among various methods. This paradigm
decouples the training process into separate representation learning and
classifier re-training. Previous works have attempted to improve both stages
simultaneously, making it difficult to isolate the effect of classifier
re-training. Furthermore, recent empirical studies have demonstrated that
simple regularization can yield strong feature representations, emphasizing the
need to reassess existing classifier re-training methods. In this study, we
revisit classifier re-training methods based on a unified feature
representation and re-evaluate their performances. We propose a new metric
called Logits Magnitude as a superior measure of model performance, replacing
the commonly used Weight Norm. However, since it is hard to directly optimize
the new metric during training, we introduce a suitable approximate invariant
called Regularized Standard Deviation. Based on the two newly proposed metrics,
we prove that reducing the absolute value of Logits Magnitude when it is nearly
balanced can effectively decrease errors and disturbances during training,
leading to better model performance. Motivated by these findings, we develop a
simple logits retargeting approach (LORT) without the requirement of prior
knowledge of the number of samples per class. LORT divides the original one-hot
label into small true label probabilities and large negative label
probabilities distributed across each class. Our method achieves
state-of-the-art performance on various imbalanced datasets, including
CIFAR100-LT, ImageNet-LT, and iNaturalist2018.
\\ ( https://arxiv.org/abs/2403.00250 ,  987kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00290 (*cross-listing*)
Date: Fri, 1 Mar 2024 05:20:16 GMT   (56kb)

Title: Semantic Text Transmission via Prediction with Small Language Models:
  Cost-Similarity Trade-off
Authors: Bhavani A Madhabhavi, Gangadhar Karevvanavar, Rajshekhar V Bhat and
  Nikolaos Pappas
Categories: cs.IT cs.AI cs.LG math.IT
\\
  We consider the communication of natural language text from a source to a
destination over noiseless and character-erasure channels. We exploit
language's inherent correlations and predictability to constrain transmission
costs by allowing the destination to predict or complete words with potential
dissimilarity with the source text. Concretely, our objective is to obtain
achievable $(\bar{c}, \bar{s})$ pairs, where $\bar{c}$ is the average
transmission cost at the source and $\bar{s}$ is the average semantic
similarity measured via cosine similarity between vector embedding of words at
the source and those predicted/completed at the destination. We obtain
$(\bar{c}, \bar{s})$ pairs for neural language and first-order Markov
chain-based small language models (SLM) for prediction, using both a threshold
policy that transmits a word if its cosine similarity with that
predicted/completed at the destination is below a threshold, and a periodic
policy, which transmits words after a specific interval and predicts/completes
the words in between, at the destination. We adopt an SLM for word completion.
We demonstrate that, when communication occurs over a noiseless channel, the
threshold policy achieves a higher $\bar{s}$ for a given $\bar{c}$ than the
periodic policy and that the $\bar{s}$ achieved with the neural SLM is greater
than or equal to that of the Markov chain-based algorithm for the same
$\bar{c}$. The improved performance comes with a higher complexity in terms of
time and computing requirements. However, when communication occurs over a
character-erasure channel, all prediction algorithms and scheduling policies
perform poorly. Furthermore, if character-level Huffman coding is used, the
required $\bar{c}$ to achieve a given $\bar{s}$ is reduced, but the above
observations still apply.
\\ ( https://arxiv.org/abs/2403.00290 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00299 (*cross-listing*)
Date: Fri, 1 Mar 2024 05:57:08 GMT   (812kb)

Title: Universal Auto-encoder Framework for MIMO CSI Feedback
Authors: Jinhyun So, Hyukjoon Kwon
Categories: cs.IT cs.AI cs.LG eess.SP math.IT
Comments: 7 pages, 11 figures
\\
  Existing auto-encoder (AE)-based channel state information (CSI) frameworks
have focused on a specific configuration of user equipment (UE) and base
station (BS), and thus the input and output sizes of the AE are fixed. However,
in the real-world scenario, the input and output sizes may vary depending on
the number of antennas of the BS and UE and the allocated resource block in the
frequency dimension. A naive approach to support the different input and output
sizes is to use multiple AE models, which is impractical for the UE due to the
limited HW resources. In this paper, we propose a universal AE framework that
can support different input sizes and multiple compression ratios. The proposed
AE framework significantly reduces the HW complexity while providing comparable
performance in terms of compression ratio-distortion trade-off compared to the
naive and state-of-the-art approaches.
\\ ( https://arxiv.org/abs/2403.00299 ,  812kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00307 (*cross-listing*)
Date: Fri, 1 Mar 2024 06:18:40 GMT   (1108kb,D)

Title: Embedded Multi-label Feature Selection via Orthogonal Regression
Authors: Xueyuan Xu, Fulin Wei, Tianyuan Jia, Li Zhuo, Feiping Nie, Xia Wu
Categories: cs.CV cs.AI
\\
  In the last decade, embedded multi-label feature selection methods,
incorporating the search for feature subsets into model optimization, have
attracted considerable attention in accurately evaluating the importance of
features in multi-label classification tasks. Nevertheless, the
state-of-the-art embedded multi-label feature selection algorithms based on
least square regression usually cannot preserve sufficient discriminative
information in multi-label data. To tackle the aforementioned challenge, a
novel embedded multi-label feature selection method, termed global redundancy
and relevance optimization in orthogonal regression (GRROOR), is proposed to
facilitate the multi-label feature selection. The method employs orthogonal
regression with feature weighting to retain sufficient statistical and
structural information related to local label correlations of the multi-label
data in the feature learning process. Additionally, both global feature
redundancy and global label relevancy information have been considered in the
orthogonal regression model, which could contribute to the search for
discriminative and non-redundant feature subsets in the multi-label data. The
cost function of GRROOR is an unbalanced orthogonal Procrustes problem on the
Stiefel manifold. A simple yet effective scheme is utilized to obtain an
optimal solution. Extensive experimental results on ten multi-label data sets
demonstrate the effectiveness of GRROOR.
\\ ( https://arxiv.org/abs/2403.00307 ,  1108kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00336 (*cross-listing*)
Date: Fri, 1 Mar 2024 07:51:29 GMT   (5969kb,D)

Title: Never-Ending Embodied Robot Learning
Authors: Wenqi Liang, Gan Sun, Qian He, Yu Ren, Jiahua Dong and Yang Cong
Categories: cs.RO cs.AI
Comments: 14 pages, 5 figures, 8 tables
\\
  Relying on large language models (LLMs), embodied robots could perform
complex multimodal robot manipulation tasks from visual observations with
powerful generalization ability. However, most visual behavior-cloning agents
suffer from manipulation performance degradation and skill knowledge forgetting
when adapting into a series of challenging unseen tasks. We here investigate
the above challenge with NBCagent in embodied robots, a pioneering
language-conditioned Never-ending Behavior-Cloning agent, which can continually
learn observation knowledge of novel robot manipulation skills from
skill-specific and skill-shared attributes. Specifically, we establish a
skill-specific evolving planner to perform knowledge decoupling, which can
continually embed novel skill-specific knowledge in our NBCagent agent from
latent and low-rank space. Meanwhile, we propose a skill-shared semantics
rendering module and a skill-shared representation distillation module to
effectively transfer anti-forgetting skill-shared knowledge, further tackling
catastrophic forgetting on old skills from semantics and representation
aspects. Finally, we design a continual embodied robot manipulation benchmark,
and several expensive experiments demonstrate the significant performance of
our method. Visual results, code, and dataset are provided at:
https://neragent.github.io.
\\ ( https://arxiv.org/abs/2403.00336 ,  5969kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00353 (*cross-listing*)
Date: Fri, 1 Mar 2024 08:32:12 GMT   (5257kb,D)

Title: MS-Net: A Multi-Path Sparse Model for Motion Prediction in Multi-Scenes
Authors: Xiaqiang Tang, Weigao Sun, Siyuan Hu, Yiyang Sun, Yafeng Guo
Categories: cs.CV cs.AI cs.RO
Comments: Accepted by IEEE Robotics and Automation Letters (RAL)
DOI: 10.1109/LRA.2023.3338414
\\
  The multi-modality and stochastic characteristics of human behavior make
motion prediction a highly challenging task, which is critical for autonomous
driving. While deep learning approaches have demonstrated their great potential
in this area, it still remains unsolved to establish a connection between
multiple driving scenes (e.g., merging, roundabout, intersection) and the
design of deep learning models. Current learning-based methods typically use
one unified model to predict trajectories in different scenarios, which may
result in sub-optimal results for one individual scene. To address this issue,
we propose Multi-Scenes Network (aka. MS-Net), which is a multi-path sparse
model trained by an evolutionary process. MS-Net selectively activates a subset
of its parameters during the inference stage to produce prediction results for
each scene. In the training stage, the motion prediction task under
differentiated scenes is abstracted as a multi-task learning problem, an
evolutionary algorithm is designed to encourage the network search of the
optimal parameters for each scene while sharing common knowledge between
different scenes. Our experiment results show that with substantially reduced
parameters, MS-Net outperforms existing state-of-the-art methods on
well-established pedestrian motion prediction datasets, e.g., ETH and UCY, and
ranks the 2nd place on the INTERACTION challenge.
\\ ( https://arxiv.org/abs/2403.00353 ,  5257kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00376 (*cross-listing*)
Date: Fri, 1 Mar 2024 09:01:53 GMT   (9098kb,D)

Title: Invariant Test-Time Adaptation for Vision-Language Model Generalization
Authors: Huan Ma, Yan Zhu, Changqing Zhang, Peilin Zhao, Baoyuan Wu, Long-Kai
  Huang, Qinghua Hu, Bingzhe Wu
Categories: cs.CV cs.AI cs.LG
\\
  Vision-language foundation models have exhibited remarkable success across a
multitude of downstream tasks due to their scalability on extensive image-text
paired datasets. However, these models display significant limitations when
applied to long-tail tasks, such as fine-grained image classification, as a
result of "decision shortcuts" that hinders their generalization capabilities.
In this work, we find that the CLIP model possesses a rich set of features,
encompassing both \textit{desired invariant causal features} and
\textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP
on downstream tasks originates from its inability to effectively utilize
pre-trained features in accordance with specific task requirements. To address
this challenge, this paper introduces a test-time prompt tuning paradigm that
optimizes a learnable prompt, thereby compelling the model to exploit genuine
causal invariant features while disregarding decision shortcuts during the
inference phase. The proposed method effectively alleviates excessive
dependence on potentially misleading, task-irrelevant contextual information,
while concurrently emphasizing critical, task-related visual cues. We conduct
comparative analysis of the proposed method against various approaches which
validates its effectiveness.
\\ ( https://arxiv.org/abs/2403.00376 ,  9098kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00396 (*cross-listing*)
Date: Fri, 1 Mar 2024 09:35:03 GMT   (440kb,D)

Title: GLFNET: Global-Local (frequency) Filter Networks for efficient medical
  image segmentation
Authors: Athanasios Tragakis, Qianying Liu, Chaitanya Kaul, Swalpa Kumar Roy,
  Hang Dai, Fani Deligianni, Roderick Murray-Smith, Daniele Faccio
Categories: cs.CV cs.AI
\\
  We propose a novel transformer-style architecture called Global-Local Filter
Network (GLFNet) for medical image segmentation and demonstrate its
state-of-the-art performance. We replace the self-attention mechanism with a
combination of global-local filter blocks to optimize model efficiency. The
global filters extract features from the whole feature map whereas the local
filters are being adaptively created as 4x4 patches of the same feature map and
add restricted scale information. In particular, the feature extraction takes
place in the frequency domain rather than the commonly used spatial (image)
domain to facilitate faster computations. The fusion of information from both
spatial and frequency spaces creates an efficient model with regards to
complexity, required data and performance. We test GLFNet on three benchmark
datasets achieving state-of-the-art performance on all of them while being
almost twice as efficient in terms of GFLOP operations.
\\ ( https://arxiv.org/abs/2403.00396 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00425 (*cross-listing*)
Date: Fri, 1 Mar 2024 10:21:52 GMT   (3619kb,D)

Title: HALC: Object Hallucination Reduction via Adaptive Focal-Contrast
  Decoding
Authors: Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu Yao, Bo Li, Jiawei
  Zhou
Categories: cs.CV cs.AI cs.LG
Comments: Code is released at https://github.com/BillChan226/HALC
\\
  While large vision-language models (LVLMs) have demonstrated impressive
capabilities in interpreting multi-modal contexts, they invariably suffer from
object hallucinations (OH). We introduce HALC, a novel decoding algorithm
designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal
visual information in vision-language tasks and operates on both local and
global contexts simultaneously. Specifically, HALC integrates a robust
auto-focal grounding mechanism (locally) to correct hallucinated tokens on the
fly, and a specialized beam search algorithm (globally) to significantly reduce
OH while preserving text generation quality. Additionally, HALC can be
integrated into any LVLMs as a plug-and-play module without extra training.
Extensive experimental studies demonstrate the effectiveness of HALC in
reducing OH, outperforming state-of-the-arts across four benchmarks.
\\ ( https://arxiv.org/abs/2403.00425 ,  3619kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00436 (*cross-listing*)
Date: Fri, 1 Mar 2024 10:42:52 GMT   (3491kb,D)

Title: Abductive Ego-View Accident Video Understanding for Safe Driving
  Perception
Authors: Jianwu Fang, Lei-lei Li, Junfei Zhou, Junbin Xiao, Hongkai Yu, Chen
  Lv, Jianru Xue, and Tat-Seng Chua
Categories: cs.CV cs.AI
Comments: Accepted by CVPR2024. This is not the camera-ready version. The
  Project page: http://www.lotvsmmau.net
\\
  We present MM-AU, a novel dataset for Multi-Modal Accident video
Understanding. MM-AU contains 11,727 in-the-wild ego-view accident videos, each
with temporally aligned text descriptions. We annotate over 2.23 million object
boxes and 58,650 pairs of video-based accident reasons, covering 58 accident
categories. MM-AU supports various accident understanding tasks, particularly
multimodal video diffusion to understand accident cause-effect chains for safe
driving. With MM-AU, we present an Abductive accident Video understanding
framework for Safe Driving perception (AdVersa-SD). AdVersa-SD performs video
diffusion via an Object-Centric Video Diffusion (OAVD) method which is driven
by an abductive CLIP model. This model involves a contrastive interaction loss
to learn the pair co-occurrence of normal, near-accident, accident frames with
the corresponding text descriptions, such as accident reasons, prevention
advice, and accident categories. OAVD enforces the causal region learning while
fixing the content of the original frame background in video generation, to
find the dominant cause-effect chain for certain accidents. Extensive
experiments verify the abductive ability of AdVersa-SD and the superiority of
OAVD against the state-of-the-art diffusion models. Additionally, we provide
careful benchmark evaluations for object detection and accident reason
answering since AdVersa-SD relies on precise object and accident reason
information.
\\ ( https://arxiv.org/abs/2403.00436 ,  3491kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00437 (*cross-listing*)
Date: Fri, 1 Mar 2024 10:46:47 GMT   (33299kb,D)

Title: LoMOE: Localized Multi-Object Editing via Multi-Diffusion
Authors: Goirik Chakrabarty, Aditya Chandrasekar, Ramya Hebbalaguppe, Prathosh
  AP
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: 18 pages
\\
  Recent developments in the field of diffusion models have demonstrated an
exceptional capacity to generate high-quality prompt-conditioned image edits.
Nevertheless, previous approaches have primarily relied on textual prompts for
image editing, which tend to be less effective when making precise edits to
specific objects or fine-grained regions within a scene containing
single/multiple objects. We introduce a novel framework for zero-shot localized
multi-object editing through a multi-diffusion process to overcome this
challenge. This framework empowers users to perform various operations on
objects within an image, such as adding, replacing, or editing $\textbf{many}$
objects in a complex scene $\textbf{in one pass}$. Our approach leverages
foreground masks and corresponding simple text prompts that exert localized
influences on the target regions resulting in high-fidelity image editing. A
combination of cross-attention and background preservation losses within the
latent space ensures that the characteristics of the object being edited are
preserved while simultaneously achieving a high-quality, seamless
reconstruction of the background with fewer artifacts compared to the current
methods. We also curate and release a dataset dedicated to multi-object
editing, named $\texttt{LoMOE}$-Bench. Our experiments against existing
state-of-the-art methods demonstrate the improved effectiveness of our approach
in terms of both image editing quality and inference speed.
\\ ( https://arxiv.org/abs/2403.00437 ,  33299kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00439 (*cross-listing*)
Date: Fri, 1 Mar 2024 10:53:10 GMT   (1648kb,D)

Title: Authors' Values and Attitudes Towards AI-bridged Scalable
  Personalization of Creative Language Arts
Authors: Taewook Kim, Hyomin Han, Eytan Adar, Matthew Kay, John Joon Young
  Chung
Categories: cs.HC cs.AI
Comments: 16 pages, 6 figures, 2 tables. Accepted to ACM CHI 2024
DOI: 10.1145/3613904.3642529
\\
  Generative AI has the potential to create a new form of interactive media:
AI-bridged creative language arts (CLA), which bridge the author and audience
by personalizing the author's vision to the audience's context and taste at
scale. However, it is unclear what the authors' values and attitudes would be
regarding AI-bridged CLA. To identify these values and attitudes, we conducted
an interview study with 18 authors across eight genres (e.g., poetry, comics)
by presenting speculative but realistic AI-bridged CLA scenarios. We identified
three benefits derived from the dynamics between author, artifact, and
audience: those that 1) authors get from the process, 2) audiences get from the
artifact, and 3) authors get from the audience. We found how AI-bridged CLA
would either promote or reduce these benefits, along with authors' concerns. We
hope our investigation hints at how AI can provide intriguing experiences to
CLA audiences while promoting authors' values.
\\ ( https://arxiv.org/abs/2403.00439 ,  1648kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00450 (*cross-listing*)
Date: Fri, 1 Mar 2024 11:11:59 GMT   (1284kb,D)

Title: Parallel Hyperparameter Optimization Of Spiking Neural Network
Authors: Thomas Firmin, Pierre Boulet, El-Ghazali Talbi
Categories: cs.NE cs.AI
\\
  Spiking Neural Networks (SNN). SNNs are based on a more biologically inspired
approach than usual artificial neural networks. Such models are characterized
by complex dynamics between neurons and spikes. These are very sensitive to the
hyperparameters, making their optimization challenging. To tackle
hyperparameter optimization of SNNs, we initially extended the signal loss
issue of SNNs to what we call silent networks. These networks fail to emit
enough spikes at their outputs due to mistuned hyperparameters or architecture.
Generally, search spaces are heavily restrained, sometimes even discretized, to
prevent the sampling of such networks. By defining an early stopping criterion
detecting silent networks and by designing specific constraints, we were able
to instantiate larger and more flexible search spaces. We applied a constrained
Bayesian optimization technique, which was asynchronously parallelized, as the
evaluation time of a SNN is highly stochastic. Large-scale experiments were
carried-out on a multi-GPU Petascale architecture. By leveraging silent
networks, results show an acceleration of the search, while maintaining good
performances of both the optimization algorithm and the best solution obtained.
We were able to apply our methodology to two popular training algorithms, known
as spike timing dependent plasticity and surrogate gradient. Early detection
allowed us to prevent worthless and costly computation, directing the search
toward promising hyperparameter combinations. Our methodology could be applied
to multi-objective problems, where the spiking activity is often minimized to
reduce the energy consumption. In this scenario, it becomes essential to find
the delicate frontier between low-spiking and silent networks. Finally, our
approach may have implications for neural architecture search, particularly in
defining suitable spiking architectures.
\\ ( https://arxiv.org/abs/2403.00450 ,  1284kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00504 (*cross-listing*)
Date: Fri, 1 Mar 2024 13:05:38 GMT   (4923kb,D)

Title: Learning and Leveraging World Models in Visual Representation Learning
Authors: Quentin Garrido, Mahmoud Assran, Nicolas Ballas, Adrien Bardes,
  Laurent Najman, Yann LeCun
Categories: cs.CV cs.AI cs.LG
Comments: 23 pages, 16 figures
\\
  Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising
self-supervised approach that learns by leveraging a world model. While
previously limited to predicting missing parts of an input, we explore how to
generalize the JEPA prediction task to a broader set of corruptions. We
introduce Image World Models, an approach that goes beyond masked image
modeling and learns to predict the effect of global photometric transformations
in latent space. We study the recipe of learning performant IWMs and show that
it relies on three key aspects: conditioning, prediction difficulty, and
capacity. Additionally, we show that the predictive world model learned by IWM
can be adapted through finetuning to solve diverse tasks; a fine-tuned IWM
world model matches or surpasses the performance of previous self-supervised
methods. Finally, we show that learning with an IWM allows one to control the
abstraction level of the learned representations, learning invariant
representations such as contrastive methods, or equivariant representations
such as masked image modelling.
\\ ( https://arxiv.org/abs/2403.00504 ,  4923kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00561 (*cross-listing*)
Date: Fri, 1 Mar 2024 14:39:15 GMT   (7962kb,D)

Title: Multi-Task Learning Using Uncertainty to Weigh Losses for Heterogeneous
  Face Attribute Estimation
Authors: Huaqing Yuan and Yi He and Peng Du and Lu Song
Categories: cs.CV cs.AI
\\
  Face images contain a wide variety of attribute information. In this paper,
we propose a generalized framework for joint estimation of ordinal and nominal
attributes based on information sharing. We tackle the correlation problem
between heterogeneous attributes using hard parameter sharing of shallow
features, and trade-off multiple loss functions by considering homoskedastic
uncertainty for each attribute estimation task. This leads to optimal
estimation of multiple attributes of the face and reduces the training cost of
multitask learning. Experimental results on benchmarks with multiple face
attributes show that the proposed approach has superior performance compared to
state of the art. Finally, we discuss the bias issues arising from the proposed
approach in face attribute estimation and validate its feasibility on edge
systems.
\\ ( https://arxiv.org/abs/2403.00561 ,  7962kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00565 (*cross-listing*)
Date: Fri, 1 Mar 2024 14:43:55 GMT   (7961kb,D)

Title: Predicting UAV Type: An Exploration of Sampling and Data Augmentation
  for Time Series Classification
Authors: Tarik Crnovrsanin, Calvin Yu, Dane Hankamer, and Cody Dunne
Categories: cs.RO cs.AI
Comments: 12 pages, 3 figures, 4 tables, submitted to IEEE Transactions on
  Cybernetics
\\
  Unmanned aerial vehicles are becoming common and have many productive uses.
However, their increased prevalence raises safety concerns -- how can we
protect restricted airspace? Knowing the type of unmanned aerial vehicle can go
a long way in determining any potential risks it carries. For instance,
fixed-wing craft can carry more weight over longer distances, thus potentially
posing a more significant threat. This paper presents a machine learning model
for classifying unmanned aerial vehicles as quadrotor, hexarotor, or
fixed-wing. Our approach effectively applies a Long-Short Term Memory (LSTM)
neural network for the purpose of time series classification. We performed
experiments to test the effects of changing the timestamp sampling method and
addressing the imbalance in the class distribution. Through these experiments,
we identified the top-performing sampling and class imbalance fixing methods.
Averaging the macro f-scores across 10 folds of data, we found that the
majority quadrotor class was predicted well (98.16%), and, despite an extreme
class imbalance, the model could also predicted a majority of fixed-wing
flights correctly (73.15%). Hexarotor instances were often misclassified as
quadrotors due to the similarity of multirotors in general (42.15%). However,
results remained relatively stable across certain methods, which prompted us to
analyze and report on their tradeoffs. The supplemental material for this
paper, including the code and data for running all the experiments and
generating the results tables, is available at https://osf.io/mnsgk/.
\\ ( https://arxiv.org/abs/2403.00565 ,  7961kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00567 (*cross-listing*)
Date: Fri, 1 Mar 2024 14:44:41 GMT   (7310kb,D)

Title: Flatten Long-Range Loss Landscapes for Cross-Domain Few-Shot Learning
Authors: Yixiong Zou, Yicong Liu, Yiman Hu, Yuhua Li, Ruixuan Li
Categories: cs.CV cs.AI
\\
  Cross-domain few-shot learning (CDFSL) aims to acquire knowledge from limited
training data in the target domain by leveraging prior knowledge transferred
from source domains with abundant training samples. CDFSL faces challenges in
transferring knowledge across dissimilar domains and fine-tuning models with
limited training data. To address these challenges, we initially extend the
analysis of loss landscapes from the parameter space to the representation
space, which allows us to simultaneously interpret the transferring and
fine-tuning difficulties of CDFSL models. We observe that sharp minima in the
loss landscapes of the representation space result in representations that are
hard to transfer and fine-tune. Moreover, existing flatness-based methods have
limited generalization ability due to their short-range flatness. To enhance
the transferability and facilitate fine-tuning, we introduce a simple yet
effective approach to achieve long-range flattening of the minima in the loss
landscape. This approach considers representations that are differently
normalized as minima in the loss landscape and flattens the high-loss region in
the middle by randomly sampling interpolated representations. We implement this
method as a new normalization layer that replaces the original one in both CNNs
and ViTs. This layer is simple and lightweight, introducing only a minimal
number of additional parameters. Experimental results on 8 datasets demonstrate
that our approach outperforms state-of-the-art methods in terms of average
accuracy. Moreover, our method achieves performance improvements of up to 9\%
compared to the current best approaches on individual datasets. Our code will
be released.
\\ ( https://arxiv.org/abs/2403.00567 ,  7310kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00570 (*cross-listing*)
Date: Fri, 1 Mar 2024 14:47:46 GMT   (4494kb,D)

Title: Rethinking cluster-conditioned diffusion models
Authors: Nikolas Adaloglou and Tim Kaiser and Felix Michels and Markus Kollmann
Categories: cs.CV cs.AI cs.LG
\\
  We present a comprehensive experimental study on image-level conditioning for
diffusion models using cluster assignments. We elucidate how individual
components regarding image clustering impact image synthesis across three
datasets. By combining recent advancements from image clustering and diffusion
models, we show that, given the optimal cluster granularity with respect to
image synthesis (visual groups), cluster-conditioning can achieve
state-of-the-art FID (i.e. 1.67, 2.17 on CIFAR10 and CIFAR100 respectively),
while attaining a strong training sample efficiency. Finally, we propose a
novel method to derive an upper cluster bound that reduces the search space of
the visual groups using solely feature-based clustering. Unlike existing
approaches, we find no significant connection between clustering and
cluster-conditional image generation. The code and cluster assignments will be
released.
\\ ( https://arxiv.org/abs/2403.00570 ,  4494kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00587 (*cross-listing*)
Date: Fri, 1 Mar 2024 15:09:37 GMT   (9700kb,D)

Title: Improving Explicit Spatial Relationships in Text-to-Image Generation
  through an Automatically Derived Dataset
Authors: Ander Salaberria, Gorka Azkune, Oier Lopez de Lacalle, Aitor Soroa,
  Eneko Agirre and Frank Keller
Categories: cs.CV cs.AI
Comments: 12 pages and 5 figures
\\
  Existing work has observed that current text-to-image systems do not
accurately reflect explicit spatial relations between objects such as 'left of'
or 'below'. We hypothesize that this is because explicit spatial relations
rarely appear in the image captions used to train these models. We propose an
automatic method that, given existing images, generates synthetic captions that
contain 14 explicit spatial relations. We introduce the Spatial Relation for
Generation (SR4G) dataset, which contains 9.9 millions image-caption pairs for
training, and more than 60 thousand captions for evaluation. In order to test
generalization we also provide an 'unseen' split, where the set of objects in
the train and test captions are disjoint. SR4G is the first dataset that can be
used to spatially fine-tune text-to-image systems. We show that fine-tuning two
different Stable Diffusion models (denoted as SD$_{SR4G}$) yields up to 9
points improvements in the VISOR metric. The improvement holds in the 'unseen'
split, showing that SD$_{SR4G}$ is able to generalize to unseen objects.
SD$_{SR4G}$ improves the state-of-the-art with fewer parameters, and avoids
complex architectures. Our analysis shows that improvement is consistent for
all relations. The dataset and the code will be publicly available.
\\ ( https://arxiv.org/abs/2403.00587 ,  9700kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00691 (*cross-listing*)
Date: Fri, 1 Mar 2024 17:23:30 GMT   (2174kb,D)

Title: Tri-Modal Motion Retrieval by Learning a Joint Embedding Space
Authors: Kangning Yin, Shihao Zou, Yuxuan Ge, Zheng Tian
Categories: cs.CV cs.AI
\\
  Information retrieval is an ever-evolving and crucial research domain. The
substantial demand for high-quality human motion data especially in online
acquirement has led to a surge in human motion research works. Prior works have
mainly concentrated on dual-modality learning, such as text and motion tasks,
but three-modality learning has been rarely explored. Intuitively, an extra
introduced modality can enrich a model's application scenario, and more
importantly, an adequate choice of the extra modality can also act as an
intermediary and enhance the alignment between the other two disparate
modalities. In this work, we introduce LAVIMO (LAnguage-VIdeo-MOtion
alignment), a novel framework for three-modality learning integrating
human-centric videos as an additional modality, thereby effectively bridging
the gap between text and motion. Moreover, our approach leverages a specially
designed attention mechanism to foster enhanced alignment and synergistic
effects among text, video, and motion modalities. Empirically, our results on
the HumanML3D and KIT-ML datasets show that LAVIMO achieves state-of-the-art
performance in various motion-related cross-modal retrieval tasks, including
text-to-motion, motion-to-text, video-to-motion and motion-to-video.
\\ ( https://arxiv.org/abs/2403.00691 ,  2174kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00692 (*cross-listing*)
Date: Fri, 1 Mar 2024 17:26:02 GMT   (1006kb,D)

Title: Toward Autonomous Cooperation in Heterogeneous Nanosatellite
  Constellations Using Dynamic Graph Neural Networks
Authors: Guillem Casadesus-Vila, Joan-Adria Ruiz-de-Azua, Eduard Alarcon
Categories: eess.SP cs.AI cs.NI
Comments: 8 pages, 5 figures, conference
\\
  The upcoming landscape of Earth Observation missions will defined by
networked heterogeneous nanosatellite constellations required to meet strict
mission requirements, such as revisit times and spatial resolution. However,
scheduling satellite communications in these satellite networks through
efficiently creating a global satellite Contact Plan (CP) is a complex task,
with current solutions requiring ground-based coordination or being limited by
onboard computational resources. The paper proposes a novel approach to
overcome these challenges by modeling the constellations and CP as dynamic
networks and employing graph-based techniques. The proposed method utilizes a
state-of-the-art dynamic graph neural network to evaluate the performance of a
given CP and update it using a heuristic algorithm based on simulated
annealing. The trained neural network can predict the network delay with a mean
absolute error of 3.6 minutes. Simulation results show that the proposed method
can successfully design a contact plan for large satellite networks, improving
the delay by 29.1%, similar to a traditional approach, while performing the
objective evaluations 20x faster.
\\ ( https://arxiv.org/abs/2403.00692 ,  1006kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00694 (*cross-listing*)
Date: Fri, 1 Mar 2024 17:30:49 GMT   (106kb,D)

Title: Defining Expertise: Applications to Treatment Effect Estimation
Authors: Alihan H\"uy\"uk, Qiyao Wei, Alicia Curth, Mihaela van der Schaar
Categories: stat.ML cs.AI cs.LG stat.ME
Comments: The 12th International Conference on Learning Representations (ICLR
  2024)
\\
  Decision-makers are often experts of their domain and take actions based on
their domain knowledge. Doctors, for instance, may prescribe treatments by
predicting the likely outcome of each available treatment. Actions of an expert
thus naturally encode part of their domain knowledge, and can help make
inferences within the same domain: Knowing doctors try to prescribe the best
treatment for their patients, we can tell treatments prescribed more frequently
are likely to be more effective. Yet in machine learning, the fact that most
decision-makers are experts is often overlooked, and "expertise" is seldom
leveraged as an inductive bias. This is especially true for the literature on
treatment effect estimation, where often the only assumption made about actions
is that of overlap. In this paper, we argue that expertise - particularly the
type of expertise the decision-makers of a domain are likely to have - can be
informative in designing and selecting methods for treatment effect estimation.
We formally define two types of expertise, predictive and prognostic, and
demonstrate empirically that: (i) the prominent type of expertise in a domain
significantly influences the performance of different methods in treatment
effect estimation, and (ii) it is possible to predict the type of expertise
present in a dataset, which can provide a quantitative basis for model
selection.
\\ ( https://arxiv.org/abs/2403.00694 ,  106kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00231 (*cross-listing*)
Date: Fri, 1 Mar 2024 02:21:30 GMT   (5378kb,D)

Title: Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of
  Large Vision-Language Models
Authors: Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng
  Kong, Qi Liu
Categories: cs.CV cs.CL
Comments: Project page: https://mm-arxiv.github.io
\\
  Large vision-language models (LVLMs), exemplified by GPT-4V, excel across
diverse tasks involving concrete images from natural scenes. However, their
ability to interpret abstract figures, such as geometry shapes and scientific
plots, remains limited due to a scarcity of training datasets in scientific
domains. To fill this gap, we introduce Multimodal ArXiv, consisting of
ArXivCap and ArXivQA, for enhancing LVLMs scientific comprehension. ArXivCap is
a figure-caption dataset comprising 6.4M images and 3.9M captions sourced from
572K ArXiv papers spanning various scientific domains. Drawing from ArXivCap,
we introduce ArXivQA, a question-answering dataset generated by prompting
GPT-4V based on scientific figures. ArXivQA greatly enhances LVLMs'
mathematical reasoning capabilities, achieving a 10.4% absolute accuracy gain
on a multimodal mathematical reasoning benchmark. Furthermore, employing
ArXivCap, we devise four vision-to-text tasks for benchmarking LVLMs.
Evaluation results with state-of-the-art LVLMs underscore their struggle with
the nuanced semantics of academic figures, with domain-specific training
yielding substantial performance gains. Our error analysis uncovers
misinterpretations of visual context, recognition errors, and the production of
overly simplified captions by current LVLMs, shedding light on future
improvements.
\\ ( https://arxiv.org/abs/2403.00231 ,  5378kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00393 (*cross-listing*)
Date: Fri, 1 Mar 2024 09:28:38 GMT   (9848kb,D)

Title: Private Benchmarking to Prevent Contamination and Improve Comparative
  Evaluation of LLMs
Authors: Nishanth Chandran, Sunayana Sitaram, Divya Gupta, Rahul Sharma,
  Kashish Mittal, Manohar Swaminathan
Categories: cs.CR cs.CL
\\
  Benchmarking is the de-facto standard for evaluating LLMs, due to its speed,
replicability and low cost. However, recent work has pointed out that the
majority of the open source benchmarks available today have been contaminated
or leaked into LLMs, meaning that LLMs have access to test data during
pretraining and/or fine-tuning. This raises serious concerns about the validity
of benchmarking studies conducted so far and the future of evaluation using
benchmarks. To solve this problem, we propose Private Benchmarking, a solution
where test datasets are kept private and models are evaluated without revealing
the test data to the model. We describe various scenarios (depending on the
trust placed on model owners or dataset owners), and present solutions to avoid
data contamination using private benchmarking. For scenarios where the model
weights need to be kept private, we describe solutions from confidential
computing and cryptography that can aid in private benchmarking. Finally, we
present solutions the problem of benchmark dataset auditing, to ensure that
private benchmarks are of sufficiently high quality.
\\ ( https://arxiv.org/abs/2403.00393 ,  9848kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18354 (*cross-listing*)
Date: Wed, 28 Feb 2024 14:26:16 GMT   (11250kb,D)

Title: SuperdropNet: a Stable and Accurate Machine Learning Proxy for
  Droplet-based Cloud Microphysics
Authors: Shivani Sharma and David Greenberg
Categories: physics.ao-ph cs.LG physics.comp-ph physics.flu-dyn
\\
  Cloud microphysics has important consequences for climate and weather
phenomena, and inaccurate representations can limit forecast accuracy. While
atmospheric models increasingly resolve storms and clouds, the accuracy of the
underlying microphysics remains limited by computationally expedient bulk
moment schemes based on simplifying assumptions. Droplet-based Lagrangian
schemes are more accurate but are underutilized due to their large
computational overhead. Machine learning (ML) based schemes can bridge this gap
by learning from vast droplet-based simulation datasets, but have so far
struggled to match the accuracy and stability of bulk moment schemes. To
address this challenge, we developed SuperdropNet, an ML-based emulator of the
Lagrangian superdroplet simulations. To improve accuracy and stability, we
employ multi-step autoregressive prediction during training, impose physical
constraints, and carefully control stochasticity in the training data.
Superdropnet predicted hydrometeor states and cloud-to-rain transition times
more accurately than previous ML emulators, and matched or outperformed bulk
moment schemes in many cases. We further carried out detailed analyses to
reveal how multistep autoregressive training improves performance, and how the
performance of SuperdropNet and other microphysical schemes hydrometeors' mass,
number and size distribution. Together our results suggest that ML models can
effectively emulate cloud microphysics, in a manner consistent with
droplet-based simulations.
\\ ( https://arxiv.org/abs/2402.18354 ,  11250kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00027 (*cross-listing*)
Date: Wed, 28 Feb 2024 16:32:47 GMT   (3261kb)

Title: A Quick Framework for Evaluating Worst Robustness of Complex Networks
Authors: Wenjun Jiang, Peiyan Li, Tianlong Fan, Ting Li, Chuan-fu Zhang, Tao
  Zhang, Zong-fu Luo
Categories: cs.SI cs.LG cs.NI
Comments: 30 pages, 8figures, 4tables,journal
MSC-class: 68T07(Primary)90B25, 05C80, 05C82, 90B15, 90B18(Secondary)
ACM-class: I.2.6; G.2.2; J.4; F.2.2
\\
  Robustness is pivotal for comprehending, designing, optimizing, and
rehabilitating networks, with simulation attacks being the prevailing
evaluation method. Simulation attacks are often time-consuming or even
impractical, however, a more crucial yet persistently overlooked drawback is
that any attack strategy merely provides a potential paradigm of
disintegration. The key concern is: in the worst-case scenario or facing the
most severe attacks, what is the limit of robustness, referred to as ``Worst
Robustness'', for a given system? Understanding a system's worst robustness is
imperative for grasping its reliability limits, accurately evaluating
protective capabilities, and determining associated design and security
maintenance costs. To address these challenges, we introduce the concept of
Most Destruction Attack (MDA) based on the idea of knowledge stacking. MDA is
employed to assess the worst robustness of networks, followed by the
application of an adapted CNN algorithm for rapid worst robustness prediction.
We establish the logical validity of MDA and highlight the exceptional
performance of the adapted CNN algorithm in predicting the worst robustness
across diverse network topologies, encompassing both model and empirical
networks.
\\ ( https://arxiv.org/abs/2403.00027 ,  3261kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00028 (*cross-listing*)
Date: Wed, 28 Feb 2024 16:45:54 GMT   (38kb)

Title: Lower Bounds for Differential Privacy Under Continual Observation and
  Online Threshold Queries
Authors: Edith Cohen, Xin Lyu, Jelani Nelson, Tam\'as Sarl\'os, Uri Stemmer
Categories: cs.CR cs.LG
\\
  One of the most basic problems for studying the "price of privacy over time"
is the so called private counter problem, introduced by Dwork et al. (2010) and
Chan et al. (2010). In this problem, we aim to track the number of events that
occur over time, while hiding the existence of every single event. More
specifically, in every time step $t\in[T]$ we learn (in an online fashion) that
$\Delta_t\geq 0$ new events have occurred, and must respond with an estimate
$n_t\approx\sum_{j=1}^t \Delta_j$. The privacy requirement is that all of the
outputs together, across all time steps, satisfy event level differential
privacy. The main question here is how our error needs to depend on the total
number of time steps $T$ and the total number of events $n$. Dwork et al.
(2015) showed an upper bound of $O\left(\log(T)+\log^2(n)\right)$, and
Henzinger et al. (2023) showed a lower bound of $\Omega\left(\min\{\log n, \log
T\}\right)$. We show a new lower bound of $\Omega\left(\min\{n,\log
T\}\right)$, which is tight w.r.t. the dependence on $T$, and is tight in the
sparse case where $\log^2 n=O(\log T)$. Our lower bound has the following
implications:
  $\bullet$ We show that our lower bound extends to the "online thresholds
problem", where the goal is to privately answer many "quantile queries" when
these queries are presented one-by-one. This resolves an open question of Bun
et al. (2017).
  $\bullet$ Our lower bound implies, for the first time, a separation between
the number of mistakes obtainable by a private online learner and a non-private
online learner. This partially resolves a COLT'22 open question published by
Sanyal and Ramponi.
  $\bullet$ Our lower bound also yields the first separation between the
standard model of private online learning and a recently proposed relaxed
variant of it, called private online prediction.
\\ ( https://arxiv.org/abs/2403.00028 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00033 (*cross-listing*)
Date: Thu, 29 Feb 2024 04:01:38 GMT   (6318kb,D)

Title: Analyzing Resting-State fMRI Data in Marijuana Users via High-Order
  Attention Brain Network
Authors: Jun-En Ding, Shihao Yang, and Feng Liu
Categories: q-bio.NC cs.LG eess.SP
\\
  The sustained use of marijuana significantly impacts the lives and health of
people. In this study, we propose an interpretable novel framework called the
HOGAB (High-Order Attention Graph Attention Neural Networks) model to analyze
local abnormal brain activity in chronic marijuana users in two datasets. The
HOGAB integrates dynamic intrinsic functional networks with LSTM technology to
capture temporal patterns in fMRI time series of marijuana users. Moreover, we
use the high-order attention module in neighborhood nodes for information
fusion and message passing, enhancing community clustering analysis for
long-term marijuana users. Furthermore, we improve the overall learning ability
of the model by incorporating attention mechanisms, achieving an AUC of 85.1%
and an accuracy of 80.7% in multigraph classification. In addition, we compare
linear machine learning methods and evaluate the effectiveness of our proposed
HODAB model. Specifically, we identified the most relevant subnetworks and
cognitive regions that are negatively influenced by persistent marijuana use,
revealing that chronic marijuana use adversely affects cognitive control,
particularly within the Dorsal Attention and Frontoparietal networks, which are
essential for attentional, cognitive, and higher cognitive functions. The
results show that our proposed model is capable of accurately predicting
craving maps and identifying brain maps associated with long-term cravings, and
also pinpointing active areas that are important for analysis.
\\ ( https://arxiv.org/abs/2403.00033 ,  6318kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00043 (*cross-listing*)
Date: Thu, 29 Feb 2024 14:50:58 GMT   (14577kb,D)

Title: RiNALMo: General-Purpose RNA Language Models Can Generalize Well on
  Structure Prediction Tasks
Authors: Rafael Josip Peni\'c, Tin Vla\v{s}i\'c, Roland G. Huber, Yue Wan, Mile
  \v{S}iki\'c
Categories: q-bio.BM cs.LG
Comments: 18 pages, 7 figures
\\
  Ribonucleic acid (RNA) plays a variety of crucial roles in fundamental
biological processes. Recently, RNA has become an interesting drug target,
emphasizing the need to improve our understanding of its structures and
functions. Over the years, sequencing technologies have produced an enormous
amount of unlabeled RNA data, which hides important knowledge and potential.
Motivated by the successes of protein language models, we introduce RiboNucleic
Acid Language Model (RiNALMo) to help unveil the hidden code of RNA. RiNALMo is
the largest RNA language model to date with $650$ million parameters
pre-trained on $36$ million non-coding RNA sequences from several available
databases. RiNALMo is able to extract hidden knowledge and capture the
underlying structure information implicitly embedded within the RNA sequences.
RiNALMo achieves state-of-the-art results on several downstream tasks. Notably,
we show that its generalization capabilities can overcome the inability of
other deep learning methods for secondary structure prediction to generalize on
unseen RNA families. The code has been made publicly available on
https://github.com/lbcb-sci/RiNALMo.
\\ ( https://arxiv.org/abs/2403.00043 ,  14577kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00128 (*cross-listing*)
Date: Thu, 29 Feb 2024 21:09:08 GMT   (19562kb,D)

Title: From Flies to Robots: Inverted Landing in Small Quadcopters with Dynamic
  Perching
Authors: Bryan Habas, Bo Cheng
Categories: cs.RO cs.LG cs.SY eess.SY
Comments: 17 pages, 19 Figures, Journal paper currently under review
\\
  Inverted landing is a routine behavior among a number of animal fliers.
However, mastering this feat poses a considerable challenge for robotic fliers,
especially to perform dynamic perching with rapid body rotations (or flips) and
landing against gravity. Inverted landing in flies have suggested that optical
flow senses are closely linked to the precise triggering and control of body
flips that lead to a variety of successful landing behaviors. Building upon
this knowledge, we aimed to replicate the flies' landing behaviors in small
quadcopters by developing a control policy general to arbitrary
ceiling-approach conditions. First, we employed reinforcement learning in
simulation to optimize discrete sensory-motor pairs across a broad spectrum of
ceiling-approach velocities and directions. Next, we converted the
sensory-motor pairs to a two-stage control policy in a continuous
augmented-optical flow space. The control policy consists of a first-stage
Flip-Trigger Policy, which employs a one-class support vector machine, and a
second-stage Flip-Action Policy, implemented as a feed-forward neural network.
To transfer the inverted-landing policy to physical systems, we utilized domain
randomization and system identification techniques for a zero-shot sim-to-real
transfer. As a result, we successfully achieved a range of robust
inverted-landing behaviors in small quadcopters, emulating those observed in
flies.
\\ ( https://arxiv.org/abs/2403.00128 ,  19562kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00147 (*cross-listing*)
Date: Thu, 29 Feb 2024 21:55:17 GMT   (69kb)

Title: Analysis of Kernel Mirror Prox for Measure Optimization
Authors: Pavel Dvurechensky and Jia-Jie Zhu
Categories: math.OC cs.LG
Comments: Accepted to AISTATS 2024
Report-no: WIAS Preprint No. 3032, (2023)
MSC-class: 90C25, 90C30, 68Q25,
\\
  By choosing a suitable function space as the dual to the non-negative measure
cone, we study in a unified framework a class of functional saddle-point
optimization problems, which we term the Mixed Functional Nash Equilibrium
(MFNE), that underlies several existing machine learning algorithms, such as
implicit generative models, distributionally robust optimization (DRO), and
Wasserstein barycenters. We model the saddle-point optimization dynamics as an
interacting Fisher-Rao-RKHS gradient flow when the function space is chosen as
a reproducing kernel Hilbert space (RKHS). As a discrete time counterpart, we
propose a primal-dual kernel mirror prox (KMP) algorithm, which uses a dual
step in the RKHS, and a primal entropic mirror prox step. We then provide a
unified convergence analysis of KMP in an infinite-dimensional setting for this
class of MFNE problems, which establishes a convergence rate of $O(1/N)$ in the
deterministic case and $O(1/\sqrt{N})$ in the stochastic case, where $N$ is the
iteration counter. As a case study, we apply our analysis to DRO, providing
algorithmic guarantees for DRO robustness and convergence.
\\ ( https://arxiv.org/abs/2403.00147 ,  69kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00158 (*cross-listing*)
Date: Thu, 29 Feb 2024 22:19:46 GMT   (1076kb,D)

Title: Automated Efficient Estimation using Monte Carlo Efficient Influence
  Functions
Authors: Raj Agrawal, Sam Witty, Andy Zane, Eli Bingham
Categories: stat.CO cs.LG stat.ME
\\
  Many practical problems involve estimating low dimensional statistical
quantities with high-dimensional models and datasets. Several approaches
address these estimation tasks based on the theory of influence functions, such
as debiased/double ML or targeted minimum loss estimation. This paper
introduces \textit{Monte Carlo Efficient Influence Functions} (MC-EIF), a fully
automated technique for approximating efficient influence functions that
integrates seamlessly with existing differentiable probabilistic programming
systems. MC-EIF automates efficient statistical estimation for a broad class of
models and target functionals that would previously require rigorous custom
analysis. We prove that MC-EIF is consistent, and that estimators using MC-EIF
achieve optimal $\sqrt{N}$ convergence rates. We show empirically that
estimators using MC-EIF are at parity with estimators using analytic EIFs.
Finally, we demonstrate a novel capstone example using MC-EIF for optimal
portfolio selection.
\\ ( https://arxiv.org/abs/2403.00158 ,  1076kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00184 (*cross-listing*)
Date: Thu, 29 Feb 2024 23:24:43 GMT   (979kb,D)

Title: Entry-Specific Bounds for Low-Rank Matrix Completion under Highly
  Non-Uniform Sampling
Authors: Xumei Xi, Christina Lee Yu and Yudong Chen
Categories: stat.ML cs.LG
DOI: 10.1109/ISIT54713.2023.10206880
\\
  Low-rank matrix completion concerns the problem of estimating unobserved
entries in a matrix using a sparse set of observed entries. We consider the
non-uniform setting where the observed entries are sampled with highly varying
probabilities, potentially with different asymptotic scalings. We show that
under structured sampling probabilities, it is often better and sometimes
optimal to run estimation algorithms on a smaller submatrix rather than the
entire matrix. In particular, we prove error upper bounds customized to each
entry, which match the minimax lower bounds under certain conditions. Our
bounds characterize the hardness of estimating each entry as a function of the
localized sampling probabilities. We provide numerical experiments that confirm
our theoretical findings.
\\ ( https://arxiv.org/abs/2403.00184 ,  979kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00229 (*cross-listing*)
Date: Fri, 1 Mar 2024 02:20:01 GMT   (33141kb)

Title: Diffraction and Scattering Aware Radio Map and Environment
  Reconstruction using Geometry Model-Assisted Deep Learning
Authors: Wangqian Chen and Junting Chen
Categories: eess.SP cs.LG
Comments: 13 pages
\\
  Machine learning (ML) facilitates rapid channel modeling for 5G and beyond
wireless communication systems. Many existing ML techniques utilize a city map
to construct the radio map; however, an updated city map may not always be
available. This paper proposes to employ the received signal strength (RSS)
data to jointly construct the radio map and the virtual environment by
exploiting the geometry structure of the environment. In contrast to many
existing ML approaches that lack of an environment model, we develop a virtual
obstacle model and characterize the geometry relation between the propagation
paths and the virtual obstacles. A multi-screen knife-edge model is adopted to
extract the key diffraction features, and these features are fed into a neural
network (NN) for diffraction representation. To describe the scattering, as
oppose to most existing methods that directly input an entire city map, our
model focuses on the geometry structure from the local area surrounding the
TX-RX pair and the spatial invariance of such local geometry structure is
exploited. Numerical experiments demonstrate that, in addition to
reconstructing a 3D virtual environment, the proposed model outperforms the
state-of-the-art methods in radio map construction with 10%-18% accuracy
improvements. It can also reduce 20% data and 50% training epochs when
transferred to a new environment.
\\ ( https://arxiv.org/abs/2403.00229 ,  33141kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00233 (*cross-listing*)
Date: Fri, 1 Mar 2024 02:28:49 GMT   (1300kb,D)

Title: Causal Bandits with General Causal Models and Interventions
Authors: Zirui Yan, Dennis Wei, Dmitriy Katz-Rogozhnikov, Prasanna Sattigeri,
  Ali Tajer
Categories: stat.ML cs.LG
Comments: 37 pages, 13 figures, conference
\\
  This paper considers causal bandits (CBs) for the sequential design of
interventions in a causal system. The objective is to optimize a reward
function via minimizing a measure of cumulative regret with respect to the best
sequence of interventions in hindsight. The paper advances the results on CBs
in three directions. First, the structural causal models (SCMs) are assumed to
be unknown and drawn arbitrarily from a general class $\mathcal{F}$ of
Lipschitz-continuous functions. Existing results are often focused on
(generalized) linear SCMs. Second, the interventions are assumed to be
generalized soft with any desired level of granularity, resulting in an
infinite number of possible interventions. The existing literature, in
contrast, generally adopts atomic and hard interventions. Third, we provide
general upper and lower bounds on regret. The upper bounds subsume (and
improve) known bounds for special cases. The lower bounds are generally
hitherto unknown. These bounds are characterized as functions of the (i) graph
parameters, (ii) eluder dimension of the space of SCMs, denoted by
$\operatorname{dim}(\mathcal{F})$, and (iii) the covering number of the
function space, denoted by ${\rm cn}(\mathcal{F})$. Specifically, the
cumulative achievable regret over horizon $T$ is $\mathcal{O}(K
d^{L-1}\sqrt{T\operatorname{dim}(\mathcal{F}) \log({\rm cn}(\mathcal{F}))})$,
where $K$ is related to the Lipschitz constants, $d$ is the graph's maximum
in-degree, and $L$ is the length of the longest causal path. The upper bound is
further refined for special classes of SCMs (neural network, polynomial, and
linear), and their corresponding lower bounds are provided.
\\ ( https://arxiv.org/abs/2403.00233 ,  1300kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00257 (*cross-listing*)
Date: Fri, 1 Mar 2024 03:45:56 GMT   (654kb)

Title: Robust deep labeling of radiological emphysema subtypes using squeeze
  and excitation convolutional neural networks: The MESA Lung and SPIROMICS
  Studies
Authors: Artur Wysoczanski, Nabil Ettehadi, Soroush Arabshahi, Yifei Sun, Karen
  Hinkley Stukovsky, Karol E. Watson, MeiLan K. Han, Erin D Michos, Alejandro
  P. Comellas, Eric A. Hoffman, Andrew F. Laine, R. Graham Barr, and Elsa D.
  Angelini
Categories: cs.CV cs.LG
\\
  Pulmonary emphysema, the progressive, irreversible loss of lung tissue, is
conventionally categorized into three subtypes identifiable on pathology and on
lung computed tomography (CT) images. Recent work has led to the unsupervised
learning of ten spatially-informed lung texture patterns (sLTPs) on lung CT,
representing distinct patterns of emphysematous lung parenchyma based on both
textural appearance and spatial location within the lung, and which aggregate
into 6 robust and reproducible CT Emphysema Subtypes (CTES). Existing methods
for sLTP segmentation, however, are slow and highly sensitive to changes in CT
acquisition protocol. In this work, we present a robust 3-D
squeeze-and-excitation CNN for supervised classification of sLTPs and CTES on
lung CT. Our results demonstrate that this model achieves accurate and
reproducible sLTP segmentation on lung CTscans, across two independent cohorts
and independently of scanner manufacturer and model.
\\ ( https://arxiv.org/abs/2403.00257 ,  654kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00258 (*cross-listing*)
Date: Fri, 1 Mar 2024 03:46:28 GMT   (158kb)

Title: "Lossless" Compression of Deep Neural Networks: A High-dimensional
  Neural Tangent Kernel Approach
Authors: Lingyu Gu, Yongqi Du, Yuan Zhang, Di Xie, Shiliang Pu, Robert C. Qiu,
  Zhenyu Liao
Categories: stat.ML cs.LG
Comments: 32 pages, 4 figures, and 2 tables. Fixing typos in Theorems 1 and 2
  from NeurIPS 2022 proceeding
  (https://proceedings.neurips.cc/paper_files/paper/2022/hash/185087ea328b4f03ea8fd0c8aa96f747-Abstract-Conference.html)
\\
  Modern deep neural networks (DNNs) are extremely powerful; however, this
comes at the price of increased depth and having more parameters per layer,
making their training and inference more computationally challenging. In an
attempt to address this key limitation, efforts have been devoted to the
compression (e.g., sparsification and/or quantization) of these large-scale
machine learning models, so that they can be deployed on low-power IoT devices.
In this paper, building upon recent advances in neural tangent kernel (NTK) and
random matrix theory (RMT), we provide a novel compression approach to wide and
fully-connected \emph{deep} neural nets. Specifically, we demonstrate that in
the high-dimensional regime where the number of data points $n$ and their
dimension $p$ are both large, and under a Gaussian mixture model for the data,
there exists \emph{asymptotic spectral equivalence} between the NTK matrices
for a large family of DNN models. This theoretical result enables "lossless"
compression of a given DNN to be performed, in the sense that the compressed
network yields asymptotically the same NTK as the original (dense and
unquantized) network, with its weights and activations taking values
\emph{only} in $\{ 0, \pm 1 \}$ up to a scaling. Experiments on both synthetic
and real-world data are conducted to support the advantages of the proposed
compression scheme, with code available at
\url{https://github.com/Model-Compression/Lossless_Compression}.
\\ ( https://arxiv.org/abs/2403.00258 ,  158kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00259 (*cross-listing*)
Date: Fri, 1 Mar 2024 03:50:03 GMT   (763kb)

Title: Deciphering diffuse scattering with machine learning and the equivariant
  foundation model: The case of molten FeO
Authors: Ganesh Sivaraman and Chris J. Benmore
Categories: cond-mat.mtrl-sci cs.LG
Comments: 9 pages, 5 figures
\\
  Bridging the gap between diffuse x-ray or neutron scattering measurements and
predicted structures derived from atom-atom pair potentials in disordered
materials, has been a longstanding challenge in condensed matter physics. This
perspective gives a brief overview of the traditional approaches employed over
the past several decades. Namely, the use of approximate interatomic pair
potentials that relate 3-dimensional structural models to the measured
structure factor and its associated pair distribution function. The use of
machine learned interatomic potentials has grown in the past few years, and has
been particularly successful in the cases of ionic and oxide systems. Recent
advances in large scale sampling, along with a direct integration of scattering
measurements into the model development, has provided improved agreement
between experiments and large-scale models calculated with quantum mechanical
accuracy. However, details of local polyhedral bonding and connectivity in
meta-stable disordered systems still require improvement. Here we leverage
MACE-MP-0; a newly introduced equivariant foundation model and validate the
results against high-quality experimental scattering data for the case of
molten iron(II) oxide (FeO). These preliminary results suggest that the
emerging foundation model has the potential to surpass the traditional
limitations of classical interatomic potentials.
\\ ( https://arxiv.org/abs/2403.00259 ,  763kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00269 (*cross-listing*)
Date: Fri, 1 Mar 2024 04:16:08 GMT   (5040kb,D)

Title: Parameter-Efficient Tuning of Large Convolutional Models
Authors: Wei Chen, Zichen Miao, Qiang Qiu
Categories: cs.CV cs.LG
\\
  To address the high computational and parameter complexity associated with
fine-tuning large pre-trained models, researchers have developed
parameter-efficient methods, where only partial parameters are updated for
downstream tasks. However, these works often overlook the distinct properties
of convolutional kernels, which still remain essential elements in many large
models, such as Stable Diffusion. In this study, we first introduce filter
subspace by decomposing convolutional kernels within each network layer over a
small set of filter subspace elements, referred to as filter atoms. We then
fine-tune these models to extract task-specific representation by only adapting
the filter atoms, a few hundred parameters typically. To potentially expand the
parameter space for tuning, we further show a simple approach to generate an
overcomplete filter subspace by recursively decomposing each filter atom over
another set of filter atoms. The fine-tuning of filter atoms reshapes the
filter subspace, enabling convolutional layers to adapt to diverse downstream
tasks efficiently. Extensive experiments show that such a simple scheme
surpasses previous tuning baselines for both discriminate and generative tasks.
Our approach can potentially be complementary to many existing fine-tuning
methods.
\\ ( https://arxiv.org/abs/2403.00269 ,  5040kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00272 (*cross-listing*)
Date: Fri, 1 Mar 2024 04:20:13 GMT   (19126kb,D)

Title: Dual Pose-invariant Embeddings: Learning Category and Object-specific
  Discriminative Representations for Recognition and Retrieval
Authors: Rohan Sarkar, Avinash Kak
Categories: cs.CV cs.IR cs.LG
Comments: Accepted by IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR 2024)
\\
  In the context of pose-invariant object recognition and retrieval, we
demonstrate that it is possible to achieve significant improvements in
performance if both the category-based and the object-identity-based embeddings
are learned simultaneously during training. In hindsight, that sounds intuitive
because learning about the categories is more fundamental than learning about
the individual objects that correspond to those categories. However, to the
best of what we know, no prior work in pose-invariant learning has demonstrated
this effect. This paper presents an attention-based dual-encoder architecture
with specially designed loss functions that optimize the inter- and intra-class
distances simultaneously in two different embedding spaces, one for the
category embeddings and the other for the object-level embeddings. The loss
functions we have proposed are pose-invariant ranking losses that are designed
to minimize the intra-class distances and maximize the inter-class distances in
the dual representation spaces. We demonstrate the power of our approach with
three challenging multi-view datasets, ModelNet-40, ObjectPI, and FG3D. With
our dual approach, for single-view object recognition, we outperform the
previous best by 20.0% on ModelNet40, 2.0% on ObjectPI, and 46.5% on FG3D. On
the other hand, for single-view object retrieval, we outperform the previous
best by 33.7% on ModelNet40, 18.8% on ObjectPI, and 56.9% on FG3D.
\\ ( https://arxiv.org/abs/2403.00272 ,  19126kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00289 (*cross-listing*)
Date: Fri, 1 Mar 2024 05:19:59 GMT   (3588kb,D)

Title: Optimization of Array Encoding for Ultrasound Imaging
Authors: Jacob Spainhour, Korben Smart, Stephen Becker, Nick Bottenus
Categories: physics.med-ph cs.LG math.OC
ACM-class: J.3
\\
  Objective: The transmit encoding model for synthetic aperture imaging is a
robust and flexible framework for understanding the effect of acoustic
transmission on ultrasound image reconstruction. Our objective is to use
machine learning (ML) to construct scanning sequences, parameterized by time
delays and apodization weights, that produce high quality B-mode images.
Approach: We use an ML model in PyTorch and simulated RF data from Field II to
probe the space of possible encoding sequences for those that minimize a loss
function that describes image quality. This approach is made computationally
feasible by a novel formulation of the derivative for delay-and-sum
beamforming. We demonstrate these results experimentally on wire targets and a
tissue-mimicking phantom. Main Results: When trained according to a given set
of imaging parameters (imaging domain, hardware restrictions), our ML imaging
model produces optimized encoding sequences that improve a number of standard
quality metrics including resolution, field of view, and contrast, over
conventional sequences. Significance: This work demonstrates that the set of
encoding schemes that are commonly used represent only a narrow subset of those
available. Additionally, it demonstrates the value for ML tasks in synthetic
transmit aperture imaging to consider the beamformer within the model, instead
of as purely post-processing.
\\ ( https://arxiv.org/abs/2403.00289 ,  3588kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00293 (*cross-listing*)
Date: Fri, 1 Mar 2024 05:32:14 GMT   (389kb,D)

Title: Efficient Adapter Tuning of Pre-trained Speech Models for Automatic
  Speaker Verification
Authors: Mufan Sang, John H.L. Hansen
Categories: eess.AS cs.LG cs.SD
Comments: Accepted to ICASSP 2024
\\
  With excellent generalization ability, self-supervised speech models have
shown impressive performance on various downstream speech tasks in the
pre-training and fine-tuning paradigm. However, as the growing size of
pre-trained models, fine-tuning becomes practically unfeasible due to heavy
computation and storage overhead, as well as the risk of overfitting. Adapters
are lightweight modules inserted into pre-trained models to facilitate
parameter-efficient adaptation. In this paper, we propose an effective adapter
framework designed for adapting self-supervised speech models to the speaker
verification task. With a parallel adapter design, our proposed framework
inserts two types of adapters into the pre-trained model, allowing the
adaptation of latent features within intermediate Transformer layers and output
embeddings from all Transformer layers. We conduct comprehensive experiments to
validate the efficiency and effectiveness of the proposed framework.
Experimental results on the VoxCeleb1 dataset demonstrate that the proposed
adapters surpass fine-tuning and other parameter-efficient transfer learning
methods, achieving superior performance while updating only 5% of the
parameters.
\\ ( https://arxiv.org/abs/2403.00293 ,  389kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00321 (*cross-listing*)
Date: Fri, 1 Mar 2024 06:48:58 GMT   (1595kb,D)

Title: DEEP-IoT: Downlink-Enhanced Efficient-Power Internet of Things
Authors: Yulin Shao
Categories: cs.IT cs.LG cs.SY eess.SP eess.SY math.IT
\\
  At the heart of the Internet of Things (IoT) -- a domain witnessing explosive
growth -- the imperative for energy efficiency and the extension of device
lifespans has never been more pressing. This paper presents DEEP-IoT, a
revolutionary communication paradigm poised to redefine how IoT devices
communicate. Through a pioneering "listen more, transmit less" strategy,
DEEP-IoT challenges and transforms the traditional transmitter (IoT
devices)-centric communication model to one where the receiver (the access
point) play a pivotal role, thereby cutting down energy use and boosting device
longevity. We not only conceptualize DEEP-IoT but also actualize it by
integrating deep learning-enhanced feedback channel codes within a narrow-band
system. Simulation results show a significant enhancement in the operational
lifespan of IoT cells -- surpassing traditional systems using Turbo and Polar
codes by up to 52.71%. This leap signifies a paradigm shift in IoT
communications, setting the stage for a future where IoT devices boast
unprecedented efficiency and durability.
\\ ( https://arxiv.org/abs/2403.00321 ,  1595kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00344 (*cross-listing*)
Date: Fri, 1 Mar 2024 08:15:18 GMT   (1713kb,D)

Title: Robustifying a Policy in Multi-Agent RL with Diverse Cooperative
  Behavior and Adversarial Style Sampling for Assistive Tasks
Authors: Tayuki Osa and Tatsuya Harada
Categories: cs.RO cs.LG
Comments: 7 pages, accepted for ICRA 2024
\\
  Autonomous assistance of people with motor impairments is one of the most
promising applications of autonomous robotic systems. Recent studies have
reported encouraging results using deep reinforcement learning (RL) in the
healthcare domain. Previous studies showed that assistive tasks can be
formulated as multi-agent RL, wherein there are two agents: a caregiver and a
care-receiver. However, policies trained in multi-agent RL are often sensitive
to the policies of other agents. In such a case, a trained caregiver's policy
may not work for different care-receivers. To alleviate this issue, we propose
a framework that learns a robust caregiver's policy by training it for diverse
care-receiver responses. In our framework, diverse care-receiver responses are
autonomously learned through trials and errors. In addition, to robustify the
care-giver's policy, we propose a strategy for sampling a care-receiver's
response in an adversarial manner during the training. We evaluated the
proposed method using tasks in an Assistive Gym. We demonstrate that policies
trained with a popular deep RL method are vulnerable to changes in policies of
other agents and that the proposed framework improves the robustness against
such changes.
\\ ( https://arxiv.org/abs/2403.00344 ,  1713kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00352 (*cross-listing*)
Date: Fri, 1 Mar 2024 08:31:58 GMT   (3623kb,D)

Title: Revisiting Disentanglement in Downstream Tasks: A Study on Its Necessity
  for Abstract Visual Reasoning
Authors: Ruiqian Nai, Zixin Wen, Ji Li, Yuanzhi Li, Yang Gao
Categories: cs.CV cs.LG
Comments: Accepted to AAAI-2024
\\
  In representation learning, a disentangled representation is highly desirable
as it encodes generative factors of data in a separable and compact pattern.
Researchers have advocated leveraging disentangled representations to complete
downstream tasks with encouraging empirical evidence. This paper further
investigates the necessity of disentangled representation in downstream
applications. Specifically, we show that dimension-wise disentangled
representations are unnecessary on a fundamental downstream task, abstract
visual reasoning. We provide extensive empirical evidence against the necessity
of disentanglement, covering multiple datasets, representation learning
methods, and downstream network architectures. Furthermore, our findings
suggest that the informativeness of representations is a better indicator of
downstream performance than disentanglement. Finally, the positive correlation
between informativeness and disentanglement explains the claimed usefulness of
disentangled representations in previous works. The source code is available at
https://github.com/Richard-coder-Nai/disentanglement-lib-necessity.git.
\\ ( https://arxiv.org/abs/2403.00352 ,  3623kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00381 (*cross-listing*)
Date: Fri, 1 Mar 2024 09:09:37 GMT   (337kb,D)

Title: Structured Deep Neural Networks-Based Backstepping Trajectory Tracking
  Control for Lagrangian Systems
Authors: Jiajun Qian, Liang Xu, Xiaoqiang Ren, Xiaofan Wang
Categories: cs.RO cs.LG cs.SY eess.SY
\\
  Deep neural networks (DNN) are increasingly being used to learn controllers
due to their excellent approximation capabilities. However, their black-box
nature poses significant challenges to closed-loop stability guarantees and
performance analysis. In this paper, we introduce a structured DNN-based
controller for the trajectory tracking control of Lagrangian systems using
backing techniques. By properly designing neural network structures, the
proposed controller can ensure closed-loop stability for any compatible neural
network parameters. In addition, improved control performance can be achieved
by further optimizing neural network parameters. Besides, we provide explicit
upper bounds on tracking errors in terms of controller parameters, which allows
us to achieve the desired tracking performance by properly selecting the
controller parameters. Furthermore, when system models are unknown, we propose
an improved Lagrangian neural network (LNN) structure to learn the system
dynamics and design the controller. We show that in the presence of model
approximation errors and external disturbances, the closed-loop stability and
tracking control performance can still be guaranteed. The effectiveness of the
proposed approach is demonstrated through simulations.
\\ ( https://arxiv.org/abs/2403.00381 ,  337kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00423 (*cross-listing*)
Date: Fri, 1 Mar 2024 10:19:32 GMT   (3716kb,D)

Title: Validation of ML-UQ calibration statistics using simulated reference
  values: a sensitivity analysis
Authors: Pascal Pernot
Categories: stat.ML cs.LG physics.chem-ph
\\
  Some popular Machine Learning Uncertainty Quantification (ML-UQ) calibration
statistics do not have predefined reference values and are mostly used in
comparative studies. In consequence, calibration is almost never validated and
the diagnostic is left to the appreciation of the reader. Simulated reference
values, based on synthetic calibrated datasets derived from actual
uncertainties, have been proposed to palliate this problem. As the generative
probability distribution for the simulation of synthetic errors is often not
constrained, the sensitivity of simulated reference values to the choice of
generative distribution might be problematic, shedding a doubt on the
calibration diagnostic. This study explores various facets of this problem, and
shows that some statistics are excessively sensitive to the choice of
generative distribution to be used for validation when the generative
distribution is unknown. This is the case, for instance, of the correlation
coefficient between absolute errors and uncertainties (CC) and of the expected
normalized calibration error (ENCE). A robust validation workflow to deal with
simulated reference values is proposed.
\\ ( https://arxiv.org/abs/2403.00423 ,  3716kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00446 (*cross-listing*)
Date: Fri, 1 Mar 2024 11:03:17 GMT   (936kb,D)

Title: Safe Hybrid-Action Reinforcement Learning-Based Decision and Control for
  Discretionary Lane Change
Authors: Ruichen Xu, Xiao Liu, Jinming Xu, Yuan Lin
Categories: cs.RO cs.LG
\\
  Autonomous lane-change, a key feature of advanced driver-assistance systems,
can enhance traffic efficiency and reduce the incidence of accidents. However,
safe driving of autonomous vehicles remains challenging in complex
environments. How to perform safe and appropriate lane change is a popular
topic of research in the field of autonomous driving. Currently, few papers
consider the safety of reinforcement learning in autonomous lane-change
scenarios. We introduce safe hybrid-action reinforcement learning into
discretionary lane change for the first time and propose Parameterized Soft
Actor-Critic with PID Lagrangian (PASAC-PIDLag) algorithm. Furthermore, we
conduct a comparative analysis of the Parameterized Soft Actor-Critic (PASAC),
which is an unsafe version of PASAC-PIDLag. Both algorithms are employed to
train the lane-change strategy of autonomous vehicles to output discrete
lane-change decision and longitudinal vehicle acceleration. Our simulation
results indicate that at a traffic density of 15 vehicles per kilometer (15
veh/km), the PASAC-PIDLag algorithm exhibits superior safety with a collision
rate of 0%, outperforming the PASAC algorithm, which has a collision rate of
1%. The outcomes of the generalization assessments reveal that at low traffic
density levels, both the PASAC-PIDLag and PASAC algorithms are proficient in
attaining a 0% collision rate. Under conditions of high traffic flow density,
the PASAC-PIDLag algorithm surpasses PASAC in terms of both safety and
optimality.
\\ ( https://arxiv.org/abs/2403.00446 ,  936kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00470 (*cross-listing*)
Date: Fri, 1 Mar 2024 11:54:25 GMT   (690kb,D)

Title: Autonomous Robotic Arm Manipulation for Planetary Missions using Causal
  Machine Learning
Authors: C. McDonnell, M. Arana-Catania, S. Upadhyay
Categories: astro-ph.IM astro-ph.EP cs.LG cs.RO
Comments: 8 pages, ASTRA 2023: 17th Symposium on Advanced Space Technologies in
  Robotics and Automation, 18-20 October 2023, Leiden, The Netherlands
\\
  Autonomous robotic arm manipulators have the potential to make planetary
exploration and in-situ resource utilization missions more time efficient and
productive, as the manipulator can handle the objects itself and perform
goal-specific actions. We train a manipulator to autonomously study objects of
which it has no prior knowledge, such as planetary rocks. This is achieved
using causal machine learning in a simulated planetary environment. Here, the
manipulator interacts with objects, and classifies them based on differing
causal factors. These are parameters, such as mass or friction coefficient,
that causally determine the outcomes of its interactions. Through reinforcement
learning, the manipulator learns to interact in ways that reveal the underlying
causal factors. We show that this method works even without any prior knowledge
of the objects, or any previously-collected training data. We carry out the
training in planetary exploration conditions, with realistic manipulator
models.
\\ ( https://arxiv.org/abs/2403.00470 ,  690kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00529 (*cross-listing*)
Date: Fri, 1 Mar 2024 13:39:56 GMT   (12787kb,D)

Title: VoxGenesis: Unsupervised Discovery of Latent Speaker Manifold for Speech
  Synthesis
Authors: Weiwei Lin, Chenhang He, Man-Wai Mak, Jiachen Lian and Kong Aik Lee
Categories: cs.SD cs.LG eess.AS
Comments: preprint
\\
  Achieving nuanced and accurate emulation of human voice has been a
longstanding goal in artificial intelligence. Although significant progress has
been made in recent years, the mainstream of speech synthesis models still
relies on supervised speaker modeling and explicit reference utterances.
However, there are many aspects of human voice, such as emotion, intonation,
and speaking style, for which it is hard to obtain accurate labels. In this
paper, we propose VoxGenesis, a novel unsupervised speech synthesis framework
that can discover a latent speaker manifold and meaningful voice editing
directions without supervision. VoxGenesis is conceptually simple. Instead of
mapping speech features to waveforms deterministically, VoxGenesis transforms a
Gaussian distribution into speech distributions conditioned and aligned by
semantic tokens. This forces the model to learn a speaker distribution
disentangled from the semantic content. During the inference, sampling from the
Gaussian distribution enables the creation of novel speakers with distinct
characteristics. More importantly, the exploration of latent space uncovers
human-interpretable directions associated with specific speaker characteristics
such as gender attributes, pitch, tone, and emotion, allowing for voice editing
by manipulating the latent codes along these identified directions. We conduct
extensive experiments to evaluate the proposed VoxGenesis using both subjective
and objective metrics, finding that it produces significantly more diverse and
realistic speakers with distinct characteristics than the previous approaches.
We also show that latent space manipulation produces consistent and
human-identifiable effects that are not detrimental to the speech quality,
which was not possible with previous approaches. Audio samples of VoxGenesis
can be found at: \url{https://bit.ly/VoxGenesis}.
\\ ( https://arxiv.org/abs/2403.00529 ,  12787kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00578 (*cross-listing*)
Date: Fri, 1 Mar 2024 14:58:36 GMT   (116kb,D)

Title: SINDy vs Hard Nonlinearities and Hidden Dynamics: a Benchmarking Study
Authors: Aurelio Raffa Ugolini and Valentina Breschi and Andrea Manzoni and
  Mara Tanelli
Categories: eess.SY cs.LG cs.SY
Comments: Submitted to IFAC SYSID 2024
\\
  In this work we analyze the effectiveness of the Sparse Identification of
Nonlinear Dynamics (SINDy) technique on three benchmark datasets for nonlinear
identification, to provide a better understanding of its suitability when
tackling real dynamical systems. While SINDy can be an appealing strategy for
pursuing physics-based learning, our analysis highlights difficulties in
dealing with unobserved states and non-smooth dynamics. Due to the ubiquity of
these features in real systems in general, and control applications in
particular, we complement our analysis with hands-on approaches to tackle these
issues in order to exploit SINDy also in these challenging contexts.
\\ ( https://arxiv.org/abs/2403.00578 ,  116kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00584 (*cross-listing*)
Date: Fri, 1 Mar 2024 15:05:21 GMT   (2664kb,D)

Title: Generalized User Representations for Transfer Learning
Authors: Ghazal Fazelnia, Sanket Gupta, Claire Keum, Mark Koh, Ian Anderson,
  and Mounia Lalmas
Categories: cs.IR cs.LG
\\
  We present a novel framework for user representation in large-scale
recommender systems, aiming at effectively representing diverse user taste in a
generalized manner. Our approach employs a two-stage methodology combining
representation learning and transfer learning. The representation learning
model uses an autoencoder that compresses various user features into a
representation space. In the second stage, downstream task-specific models
leverage user representations via transfer learning instead of curating user
features individually. We further augment this methodology on the
representation's input features to increase flexibility and enable reaction to
user events, including new user experiences, in Near-Real Time. Additionally,
we propose a novel solution to manage deployment of this framework in
production models, allowing downstream models to work independently. We
validate the performance of our framework through rigorous offline and online
experiments within a large-scale system, showcasing its remarkable efficacy
across multiple evaluation tasks. Finally, we show how the proposed framework
can significantly reduce infrastructure costs compared to alternative
approaches.
\\ ( https://arxiv.org/abs/2403.00584 ,  2664kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00746 (*cross-listing*)
Date: Fri, 1 Mar 2024 18:46:26 GMT   (678kb,D)

Title: A time-stepping deep gradient flow method for option pricing in (rough)
  diffusion models
Authors: Antonis Papapantoleon and Jasper Rou
Categories: q-fin.CP cs.LG math.PR q-fin.MF
Comments: 18 pages, 10 figures
MSC-class: 91G20, 91G60, 68T07
\\
  We develop a novel deep learning approach for pricing European options in
diffusion models, that can efficiently handle high-dimensional problems
resulting from Markovian approximations of rough volatility models. The option
pricing partial differential equation is reformulated as an energy minimization
problem, which is approximated in a time-stepping fashion by deep artificial
neural networks. The proposed scheme respects the asymptotic behavior of option
prices for large levels of moneyness, and adheres to a priori known bounds for
option prices. The accuracy and efficiency of the proposed method is assessed
in a series of numerical examples, with particular focus in the lifted Heston
model.
\\ ( https://arxiv.org/abs/2403.00746 ,  678kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2302.00389
replaced with revised version Fri, 1 Mar 2024 18:44:59 GMT   (1147kb,D)

Title: Multimodality Representation Learning: A Survey on Evolution,
  Pretraining and Its Applications
Authors: Muhammad Arslan Manzoor, Sarah Albarri, Ziting Xian, Zaiqiao Meng,
  Preslav Nakov, and Shangsong Liang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2302.00389 ,  1147kb)
------------------------------------------------------------------------------
\\
arXiv:2302.11351
replaced with revised version Fri, 1 Mar 2024 16:54:07 GMT   (1316kb,D)

Title: Abrupt and spontaneous strategy switches emerge in simple regularised
  neural networks
Authors: Anika T. L\"owe, L\'eo Touzo, Paul S. Muhle-Karbe, Andrew M. Saxe,
  Christopher Summerfield, Nicolas W. Schuck
Categories: cs.AI q-bio.NC
Comments: 17 pages, 5 figures
\\ ( https://arxiv.org/abs/2302.11351 ,  1316kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04792
replaced with revised version Thu, 29 Feb 2024 20:59:17 GMT   (2955kb,D)

Title: Direct Language Model Alignment from Online AI Feedback
Authors: Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman,
  Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan
  Ferret, Mathieu Blondel
Categories: cs.AI cs.CL cs.HC
Comments: 18 pages, 9 figures, 4 tables
\\ ( https://arxiv.org/abs/2402.04792 ,  2955kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12907
replaced with revised version Fri, 1 Mar 2024 11:18:44 GMT   (395kb,D)

Title: Incentive Compatibility for AI Alignment in Sociotechnical Systems:
  Positions and Prospects
Authors: Zhaowei Zhang, Fengshuo Bai, Mingzhi Wang, Haoyang Ye, Chengdong Ma,
  Yaodong Yang
Categories: cs.AI cs.CY cs.GT cs.HC
Comments: 13 pages, 2 figures
ACM-class: I.2.m; K.4.m
\\ ( https://arxiv.org/abs/2402.12907 ,  395kb)
------------------------------------------------------------------------------
\\
arXiv:2109.07319
replaced with revised version Fri, 1 Mar 2024 06:39:26 GMT   (1892kb,D)

Title: InceptionXML: A Lightweight Framework with Synchronized Negative
  Sampling for Short Text Extreme Classification
Authors: Siddhant Kharbanda, Atmadeep Banerjee, Akash Palrecha, Devaansh Gupta,
  Rohit Babbar
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2109.07319 ,  1892kb)
------------------------------------------------------------------------------
\\
arXiv:2112.04539
replaced with revised version Fri, 1 Mar 2024 18:14:27 GMT   (382kb,D)

Title: Prompt-based Zero-shot Relation Extraction with Semantic Knowledge
  Augmentation
Authors: Jiaying Gong and Hoda Eldardiry
Categories: cs.CL
Comments: 14 pages, 5 figures
Journal-ref: LREC-COLING 2024
\\ ( https://arxiv.org/abs/2112.04539 ,  382kb)
------------------------------------------------------------------------------
\\
arXiv:2303.08774
replaced with revised version Fri, 1 Mar 2024 16:30:27 GMT   (3849kb,D)

Title: GPT-4 Technical Report
Authors: OpenAI: Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
  Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam
  Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie
  Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan
  Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny
  Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim
  Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew
  Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che
  Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark
  Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings,
  Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, et
  al. (226 additional authors not shown)
Categories: cs.CL cs.AI
Comments: 100 pages; updated authors list; fixed author names and added
  citation
\\ ( https://arxiv.org/abs/2303.08774 ,  3849kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08158
replaced with revised version Fri, 1 Mar 2024 17:40:27 GMT   (532kb,D)

Title: Sociodemographic Bias in Language Models: A Survey and Forward Path
Authors: Vipul Gupta, Pranav Narayanan Venkit, Shomir Wilson, Rebecca J.
  Passonneau
Categories: cs.CL cs.AI cs.LG
Comments: 23 pages, 3 figure
\\ ( https://arxiv.org/abs/2306.08158 ,  532kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05576
replaced with revised version Fri, 1 Mar 2024 15:38:24 GMT   (158kb,D)

Title: Do Language Models' Words Refer?
Authors: Matthew Mandelkern and Tal Linzen
Categories: cs.CL
\\ ( https://arxiv.org/abs/2308.05576 ,  158kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10261
replaced with revised version Fri, 1 Mar 2024 14:56:07 GMT   (4343kb,D)

Title: How Good Are Large Language Models at Out-of-Distribution Detection?
Authors: Bo Liu, Liming Zhan, Zexin Lu, Yujie Feng, Lei Xue, Xiao-Ming Wu
Categories: cs.CL
Comments: Accepted at COLING 2024
\\ ( https://arxiv.org/abs/2308.10261 ,  4343kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09902
replaced with revised version Fri, 1 Mar 2024 10:39:29 GMT   (20kb,D)

Title: Speaker attribution in German parliamentary debates with QLoRA-adapted
  large language models
Authors: Tobias Bornheim, Niklas Grieger, Patrick Gustav Blaneck, Stephan
  Bialonski
Categories: cs.CL
Comments: 8 pages, 3 figures; peer-reviewed and published in the Journal for
  Language Technology and Computational Linguistics, available at
  https://jlcl.org/article/view/244
Journal-ref: Journal for Language Technology and Computational Linguistics, 37,
  1-13 (2024)
\\ ( https://arxiv.org/abs/2309.09902 ,  20kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02304
replaced with revised version Fri, 1 Mar 2024 17:11:21 GMT   (217kb,D)

Title: Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation
Authors: Eric Zelikman, Eliana Lorch, Lester Mackey, Adam Tauman Kalai
Categories: cs.CL cs.AI cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.02304 ,  217kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03309
replaced with revised version Fri, 1 Mar 2024 03:47:50 GMT   (840kb,D)

Title: Concise and Organized Perception Facilitates Large Language Models for
  Deductive Reasoning
Authors: Shaotian Yan, Chen Shen, Junjie Liu and Jieping Ye
Categories: cs.CL cs.AI
Comments: 16 pages
\\ ( https://arxiv.org/abs/2310.03309 ,  840kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08107
replaced with revised version Fri, 1 Mar 2024 00:42:58 GMT   (7829kb,D)

Title: SAIE Framework: Support Alone Isn't Enough -- Advancing LLM Training
  with Adversarial Remarks
Authors: Mengsay Loem, Masahiro Kaneko, Naoaki Okazaki
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.08107 ,  7829kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09510
replaced with revised version Fri, 1 Mar 2024 16:04:28 GMT   (9654kb,D)

Title: One Size Does Not Fit All: Customizing Open-Domain Procedures
Authors: Yash Kumar Lal and Li Zhang and Faeze Brahman and Bodhisattwa Prasad
  Majumder and Peter Clark and Niket Tandon
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.09510 ,  9654kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04455
replaced with revised version Fri, 1 Mar 2024 07:56:37 GMT   (1771kb,D)

Title: Fortify the Shortest Stave in Attention: Enhancing Context Awareness of
  Large Language Models for Effective Tool Use
Authors: Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen, Yuchuan Wu, Fei Huang,
  Yongbin Li and Rui Yan
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2312.04455 ,  1771kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12343
replaced with revised version Fri, 1 Mar 2024 15:17:21 GMT   (2030kb,D)

Title: LatestEval: Addressing Data Contamination in Language Model Evaluation
  through Dynamic and Time-Sensitive Test Construction
Authors: Yucheng Li, Frank Guerin, Chenghua Lin
Categories: cs.CL cs.AI
Comments: AAAI 2024
\\ ( https://arxiv.org/abs/2312.12343 ,  2030kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01523
replaced with revised version Fri, 1 Mar 2024 05:26:39 GMT   (6017kb,D)

Title: GOAT-Bench: Safety Insights to Large Multimodal Models through
  Meme-Based Social Abuse
Authors: Hongzhan Lin, Ziyang Luo, Bo Wang, Ruichao Yang and Jing Ma
Categories: cs.CL cs.AI
Comments: The first work to benchmark Large Multimodal Models in safety insight
  on social media
\\ ( https://arxiv.org/abs/2401.01523 ,  6017kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13170
replaced with revised version Fri, 1 Mar 2024 15:12:08 GMT   (100kb,D)

Title: CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert
  Judgments For Open-Domain Question Answering
Authors: Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem, and Jordan
  Boyd-Graber
Categories: cs.CL
Comments: See arXiv:2402.11161
\\ ( https://arxiv.org/abs/2401.13170 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01719
replaced with revised version Fri, 1 Mar 2024 06:35:29 GMT   (276kb,D)

Title: Measuring Moral Inconsistencies in Large Language Models
Authors: Vamshi Krishna Bonagiri, Sreeram Vennam, Manas Gaur, Ponnurangam
  Kumaraguru
Categories: cs.CL cs.LG
Comments: Accepted at BlackBoxNLP 2023, Co-located with EMNLP 2023
\\ ( https://arxiv.org/abs/2402.01719 ,  276kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02648
replaced with revised version Fri, 1 Mar 2024 10:46:01 GMT   (229kb,D)

Title: Recursive Chain-of-Feedback Prevents Performance Degradation from
  Redundant Prompting
Authors: Jinwoo Ahn, Kyuseung Shin
Categories: cs.CL cs.AI
Comments: Still Ongoing Work; 8 Pages; 2 Figures
\\ ( https://arxiv.org/abs/2402.02648 ,  229kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03776
replaced with revised version Fri, 1 Mar 2024 04:48:41 GMT   (49kb)

Title: Large Language Models As MOOCs Graders
Authors: Shahriar Golchin, Nikhil Garuda, Christopher Impey, Matthew Wenger
Categories: cs.CL cs.AI
Comments: v1.3 preprint
\\ ( https://arxiv.org/abs/2402.03776 ,  49kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09353
replaced with revised version Fri, 1 Mar 2024 16:26:41 GMT   (495kb,D)

Title: DoRA: Weight-Decomposed Low-Rank Adaptation
Authors: Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang
  Frank Wang, Kwang-Ting Cheng, Min-Hung Chen
Categories: cs.CL cs.CV
\\ ( https://arxiv.org/abs/2402.09353 ,  495kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13693
replaced with revised version Fri, 1 Mar 2024 07:12:20 GMT   (638kb,D)

Title: CMNER: A Chinese Multimodal NER Dataset based on Social Media
Authors: Yuanze Ji, Bobo Li, Jun Zhou, Fei Li, Chong Teng, Donghong Ji
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.13693 ,  638kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13717
replaced with revised version Fri, 1 Mar 2024 08:20:48 GMT   (8852kb,D)

Title: Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character
  Role-Playing Agent
Authors: Xiaoyan Yu, Tongxu Luo, Yifan Wei, Fangyu Lei, Yiming Huang, Hao Peng,
  Liehuang Zhu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.13717 ,  8852kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13956
replaced with revised version Thu, 29 Feb 2024 22:18:03 GMT   (1146kb,D)

Title: Can You Learn Semantics Through Next-Word Prediction? The Case of
  Entailment
Authors: William Merrill and Zhaofeng Wu and Norihito Naka and Yoon Kim and Tal
  Linzen
Categories: cs.CL
Comments: Preprint
\\ ( https://arxiv.org/abs/2402.13956 ,  1146kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14875
replaced with revised version Thu, 29 Feb 2024 19:39:35 GMT   (488kb,D)

Title: What's in a Name? Auditing Large Language Models for Race and Gender
  Bias
Authors: Amit Haim, Alejandro Salinas, Julian Nyarko
Categories: cs.CL cs.AI cs.CY cs.LG
Comments: 34 pages, 9 tables, 11 figures
\\ ( https://arxiv.org/abs/2402.14875 ,  488kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15302
replaced with revised version Fri, 1 Mar 2024 04:54:44 GMT   (702kb,D)

Title: How (un)ethical are instruction-centric responses of LLMs? Unveiling the
  vulnerabilities of safety guardrails to harmful queries
Authors: Somnath Banerjee, Sayan Layek, Rima Hazra, Animesh Mukherjee
Categories: cs.CL cs.CR
Comments: Under review.
  {https://huggingface.co/datasets/SoftMINER-Group/TechHazardQA}
\\ ( https://arxiv.org/abs/2402.15302 ,  702kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15481
replaced with revised version Thu, 29 Feb 2024 22:50:10 GMT   (8558kb,D)

Title: Prejudice and Caprice: A Statistical Framework for Measuring Social
  Discrimination in Large Language Models
Authors: Yiran Liu (1 and 2), Ke Yang (1 and 3), Zehan Qi (2), Xiao Liu (2),
  Yang Yu (2), Chengxiang Zhai (3) ((1) Equal contributions, (2) Tsinghua
  University, (3) University of Illinois Urbana-Champaign)
Categories: cs.CL cs.CY
\\ ( https://arxiv.org/abs/2402.15481 ,  8558kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15861
replaced with revised version Fri, 1 Mar 2024 14:39:30 GMT   (8916kb,D)

Title: MATHWELL: Generating Educational Math Word Problems at Scale
Authors: Bryan R Christ, Jonathan Kropko, Thomas Hartvigsen
Categories: cs.CL
Comments: 21 pages, 9 figures
\\ ( https://arxiv.org/abs/2402.15861 ,  8916kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15987
replaced with revised version Fri, 1 Mar 2024 06:44:44 GMT   (7679kb,D)

Title: Likelihood-based Mitigation of Evaluation Bias in Large Language Models
Authors: Masanari Ohi, Masahiro Kaneko, Ryuto Koike, Mengsay Loem, Naoaki
  Okazaki
Categories: cs.CL cs.AI
Comments: 4 main pages
\\ ( https://arxiv.org/abs/2402.15987 ,  7679kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17944
replaced with revised version Fri, 1 Mar 2024 00:14:42 GMT   (552kb,D)

Title: Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and
  Understanding -- A Survey
Authors: Xi Fang, Weijie Xu, Fiona Anting Tan, Jiani Zhang, Ziqing Hu, Yanjun
  Qi, Scott Nickleach, Diego Socolinsky, Srinivasan Sengamedu, Christos
  Faloutsos
Categories: cs.CL
Comments: 41 pages, 4 figures, 8 tables
MSC-class: 68T50
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2402.17944 ,  552kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18045
replaced with revised version Fri, 1 Mar 2024 12:35:55 GMT   (11158kb,D)

Title: Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using
  FActScore
Authors: Sheikh Shafayat, Eunsu Kim, Juhyun Oh, Alice Oh
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.18045 ,  11158kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18678
replaced with revised version Fri, 1 Mar 2024 13:26:43 GMT   (911kb,D)

Title: RORA: Robust Free-Text Rationale Evaluation
Authors: Zhengping Jiang, Yining Lu, Hanjie Chen, Daniel Khashabi, Benjamin Van
  Durme, Anqi Liu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.18678 ,  911kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18838
replaced with revised version Fri, 1 Mar 2024 17:40:04 GMT   (8384kb,D)

Title: When does word order matter and when doesn't it?
Authors: Xuanda Chen and Timothy O'Donnell and Siva Reddy
Categories: cs.CL
Comments: 5 pages
\\ ( https://arxiv.org/abs/2402.18838 ,  8384kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19282
replaced with revised version Fri, 1 Mar 2024 15:48:32 GMT   (106kb,D)

Title: WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset
Authors: Jiantao Qiu, Haijun Lv, Zhenjiang Jin, Rui Wang, Wenchang Ning, Jia
  Yu, ChaoBin Zhang, Pei Chu, Yuan Qu, Runyu Peng, Zhiyuan Zeng, Huanze Tang,
  Zhikai Lei, Jiawei Hong, Keyu Chen, Zhaoye Fei, Ruiliang Xu, Wei Li, Hang
  Yan, and Conghui He
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.19282 ,  106kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19350
replaced with revised version Fri, 1 Mar 2024 11:48:28 GMT   (229kb,D)

Title: Prompting Explicit and Implicit Knowledge for Multi-hop Question
  Answering Based on Human Reading Process
Authors: Guangming Huang, Yunfei Long, Cunjin Luo, Jiaxing Shen, Xia Sun
Categories: cs.CL
Comments: This paper has been accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2402.19350 ,  229kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19457
replaced with revised version Fri, 1 Mar 2024 15:29:52 GMT   (17792kb,D)

Title: $\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization
  Evaluation
Authors: Maxime Darrin, Philippe Formont, Jackie Chi Kit Cheung, Pablo
  Piantanida
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.19457 ,  17792kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19467
replaced with revised version Fri, 1 Mar 2024 03:06:37 GMT   (10967kb,D)

Title: TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning
Authors: Kate Sanders, Nathaniel Weir, Benjamin Van Durme
Categories: cs.CL cs.AI cs.CV
Comments: 9 pages, preprint
ACM-class: I.2.7; I.2.10
\\ ( https://arxiv.org/abs/2402.19467 ,  10967kb)
------------------------------------------------------------------------------
\\
arXiv:1909.12077
replaced with revised version Fri, 1 Mar 2024 04:10:17 GMT   (2636kb,D)

Title: Symplectic ODE-Net: Learning Hamiltonian Dynamics with Control
Authors: Yaofeng Desmond Zhong, Biswadip Dey, Amit Chakraborty
Categories: cs.LG cs.SY eess.SY physics.comp-ph stat.ML
Comments: Published as a Conference Paper at ICLR 2020
Journal-ref: International Conference on Learning Representations (ICLR 2020);
  https://openreview.net/forum?id=ryxmb1rKDS
\\ ( https://arxiv.org/abs/1909.12077 ,  2636kb)
------------------------------------------------------------------------------
\\
arXiv:2007.12315
replaced with revised version Fri, 1 Mar 2024 04:31:22 GMT   (970kb,D)

Title: Bayesian Robust Optimization for Imitation Learning
Authors: Daniel S. Brown, Scott Niekum, Marek Petrik
Categories: cs.LG stat.ML
Comments: In proceedings NeurIPS 2020
\\ ( https://arxiv.org/abs/2007.12315 ,  970kb)
------------------------------------------------------------------------------
\\
arXiv:2203.16464
replaced with revised version Fri, 1 Mar 2024 18:40:56 GMT   (1920kb,D)

Title: Towards Interpretable Deep Reinforcement Learning Models via Inverse
  Reinforcement Learning
Authors: Sean Xie, Soroush Vosoughi, Saeed Hassanpour
Categories: cs.LG cs.AI
Comments: Paper accepted to ICPR 2022
\\ ( https://arxiv.org/abs/2203.16464 ,  1920kb)
------------------------------------------------------------------------------
\\
arXiv:2207.00288
replaced with revised version Fri, 1 Mar 2024 08:36:33 GMT   (922kb,D)

Title: Distributed Influence-Augmented Local Simulators for Parallel MARL in
  Large Networked Systems
Authors: Miguel Suau, Jinke He, Mustafa Mert \c{C}elikok, Matthijs T. J. Spaan,
  Frans A. Oliehoek
Categories: cs.LG cs.MA
\\ ( https://arxiv.org/abs/2207.00288 ,  922kb)
------------------------------------------------------------------------------
\\
arXiv:2210.00108
replaced with revised version Fri, 1 Mar 2024 13:17:56 GMT   (364kb,D)

Title: ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled
  neural networks
Authors: Tim Clifford, Ilia Shumailov, Yiren Zhao, Ross Anderson, Robert
  Mullins
Categories: cs.LG cs.CR
Comments: 10 pages, 7 figures, to be published in IEEE Secure and Trustworthy
  Machine Learning 2024. For website see https://ml.backdoors.uk . For source
  code, see https://git.sr.ht/~tim-clifford/impnet_source
\\ ( https://arxiv.org/abs/2210.00108 ,  364kb)
------------------------------------------------------------------------------
\\
arXiv:2210.01282
replaced with revised version Fri, 1 Mar 2024 18:31:18 GMT   (1509kb,D)

Title: Structural Estimation of Markov Decision Processes in High-Dimensional
  State Space with Finite-Time Guarantees
Authors: Siliang Zeng, Mingyi Hong, Alfredo Garcia
Categories: cs.LG cs.AI econ.EM stat.ML
Comments: This conference version of this paper refers to "Maximum-Likelihood
  Inverse Reinforcement Learning with Finite-Time Guarantees" in NeurIPS 2022
\\ ( https://arxiv.org/abs/2210.01282 ,  1509kb)
------------------------------------------------------------------------------
\\
arXiv:2301.06662
replaced with revised version Fri, 1 Mar 2024 06:32:40 GMT   (12943kb,D)

Title: Graph Learning Across Data Silos
Authors: Xiang Zhang and Qiao Wang
Categories: cs.LG cs.CR eess.SP
Comments: 13 pages
\\ ( https://arxiv.org/abs/2301.06662 ,  12943kb)
------------------------------------------------------------------------------
\\
arXiv:2303.04878
replaced with revised version Thu, 29 Feb 2024 23:37:29 GMT   (5685kb,D)

Title: DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep
  Neural Networks
Authors: Zohreh Aghababaeyan, Manel Abdellatif, Mahboubeh Dadkhah, Lionel
  Briand
Categories: cs.LG cs.PF cs.SE
DOI: 10.1145/3644388
\\ ( https://arxiv.org/abs/2303.04878 ,  5685kb)
------------------------------------------------------------------------------
\\
arXiv:2303.16521
replaced with revised version Fri, 1 Mar 2024 10:22:56 GMT   (131kb,D)

Title: Hard Regularization to Prevent Deep Online Clustering Collapse without
  Data Augmentation
Authors: Louis Mahon, Thomas Lukasiewicz
Categories: cs.LG
\\ ( https://arxiv.org/abs/2303.16521 ,  131kb)
------------------------------------------------------------------------------
\\
arXiv:2303.18242
replaced with revised version Fri, 1 Mar 2024 16:28:36 GMT   (15460kb,D)

Title: $\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified
  States
Authors: Sam Bond-Taylor, Chris G. Willcocks
Categories: cs.LG cs.CV
Comments: Accepted at ICLR 2024
\\ ( https://arxiv.org/abs/2303.18242 ,  15460kb)
------------------------------------------------------------------------------
\\
arXiv:2304.02780
replaced with revised version Fri, 1 Mar 2024 05:59:29 GMT   (5035kb)

Title: A Transformer-Based Deep Learning Approach for Fairly Predicting
  Post-Liver Transplant Risk Factors
Authors: Can Li, Xiaoqian Jiang, Kai Zhang
Categories: cs.LG
Comments: Published in Journal of Biomedical Informatics
Journal-ref: J Biomed Inform. 2024 Jan;149:104545
DOI: 10.1016/j.jbi.2023.104545
\\ ( https://arxiv.org/abs/2304.02780 ,  5035kb)
------------------------------------------------------------------------------
\\
arXiv:2304.03870
replaced with revised version Fri, 1 Mar 2024 00:55:08 GMT   (459kb,D)

Title: ASPEST: Bridging the Gap Between Active Learning and Selective
  Prediction
Authors: Jiefeng Chen, Jinsung Yoon, Sayna Ebrahimi, Sercan Arik, Somesh Jha,
  Tomas Pfister
Categories: cs.LG
\\ ( https://arxiv.org/abs/2304.03870 ,  459kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00497
replaced with revised version Fri, 1 Mar 2024 13:15:25 GMT   (4628kb,D)

Title: The Risks of Recourse in Binary Classification
Authors: Hidde Fokkema, Damien Garreau, Tim van Erven
Categories: cs.LG cs.CY stat.ML
Comments: 24 pages, 8 figures, 5 tables
\\ ( https://arxiv.org/abs/2306.00497 ,  4628kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01334
replaced with revised version Fri, 1 Mar 2024 14:05:48 GMT   (5002kb,D)

Title: Federated Domain Generalization: A Survey
Authors: Ying Li, Xingwei Wang, Rongfei Zeng, Praveen Kumar Donta, Ilir
  Murturi, Min Huang, and Schahram Dustdar
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2306.01334 ,  5002kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08173
replaced with revised version Fri, 1 Mar 2024 04:24:04 GMT   (772kb,D)

Title: Safeguarding Data in Multimodal AI: A Differentially Private Approach to
  CLIP Training
Authors: Alyssa Huang, Peihan Liu, Ryumei Nakada, Linjun Zhang, Wanrong Zhang
Categories: cs.LG cs.CR cs.IT math.IT stat.ML
\\ ( https://arxiv.org/abs/2306.08173 ,  772kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09547
replaced with revised version Fri, 1 Mar 2024 01:54:15 GMT   (26927kb,D)

Title: Training generative models from privatized data
Authors: Daria Reshetova, Wei-Ning Chen, Ayfer \"Ozg\"ur
Categories: cs.LG cs.CR cs.IT math.IT
\\ ( https://arxiv.org/abs/2306.09547 ,  26927kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10125
replaced with revised version Thu, 29 Feb 2024 23:23:33 GMT   (14662kb,D)

Title: Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress,
  and Prospects
Authors: Kexin Zhang, Qingsong Wen, Chaoli Zhang, Rongyao Cai, Ming Jin, Yong
  Liu, James Zhang, Yuxuan Liang, Guansong Pang, Dongjin Song, Shirui Pan
Categories: cs.LG cs.AI eess.SP stat.AP
Comments: 26 pages, 200+ references. The first work to comprehensively and
  systematically summarize self-supervised learning for time series analysis
  (SSL4TS). The GitHub repository is
  https://github.com/qingsongedu/Awesome-SSL4TS
\\ ( https://arxiv.org/abs/2306.10125 ,  14662kb)
------------------------------------------------------------------------------
\\
arXiv:2306.17366
replaced with revised version Thu, 29 Feb 2024 19:46:10 GMT   (2111kb,D)

Title: $\lambda$-models: Effective Decision-Aware Reinforcement Learning with
  Latent Models
Authors: Claas A Voelcker, Arash Ahmadian, Romina Abachi, Igor Gilitschenski,
  Amir-massoud Farahmand
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2306.17366 ,  2111kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02747
replaced with revised version Fri, 1 Mar 2024 00:33:40 GMT   (1522kb,D)

Title: SureFED: Robust Federated Learning via Uncertainty-Aware Inward and
  Outward Inspection
Authors: Nasimeh Heydaribeni, Ruisi Zhang, Tara Javidi, Cristina Nita-Rotaru,
  Farinaz Koushanfar
Categories: cs.LG cs.DC cs.MA
\\ ( https://arxiv.org/abs/2308.02747 ,  1522kb)
------------------------------------------------------------------------------
\\
arXiv:2308.06960
replaced with revised version Fri, 1 Mar 2024 11:52:33 GMT   (1484kb,D)

Title: Search to Fine-tune Pre-trained Graph Neural Networks for Graph-level
  Tasks
Authors: Zhili Wang, Shimin Di, Lei Chen, Xiaofang Zhou
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2308.06960 ,  1484kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16739
replaced with revised version Fri, 1 Mar 2024 14:48:36 GMT   (6482kb,D)

Title: Pushing Large Language Models to the 6G Edge: Vision, Challenges, and
  Opportunities
Authors: Zheng Lin, Guanqiao Qu, Qiyuan Chen, Xianhao Chen, Zhe Chen and Kaibin
  Huang
Categories: cs.LG cs.AI
Comments: 7 pages, 5 figures
\\ ( https://arxiv.org/abs/2309.16739 ,  6482kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17388
replaced with revised version Fri, 1 Mar 2024 05:15:38 GMT   (2501kb,D)

Title: Tree Cross Attention
Authors: Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Yoshua Bengio,
  Mohamed Osama Ahmed
Categories: cs.LG
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2309.17388 ,  2501kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00290
replaced with revised version Fri, 1 Mar 2024 01:35:36 GMT   (10kb)

Title: Universality of almost periodicity in bounded discrete time series
Authors: Tsuyoshi Yoneda
Categories: cs.LG cs.IT math.AP math.DS math.FA math.IT
\\ ( https://arxiv.org/abs/2310.00290 ,  10kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11046
replaced with revised version Fri, 1 Mar 2024 06:41:43 GMT   (1591kb,D)

Title: Fast Graph Condensation with Structure-based Neural Tangent Kernel
Authors: Lin Wang, Wenqi Fan, Jiatong Li, Yao Ma, Qing Li
Categories: cs.LG cs.AI
Comments: 10 pages, 6 figures, 5 tables
\\ ( https://arxiv.org/abs/2310.11046 ,  1591kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11401
replaced with revised version Fri, 1 Mar 2024 18:17:52 GMT   (2257kb,D)

Title: Enhancing Group Fairness in Online Settings Using Oblique Decision
  Forests
Authors: Somnath Basu Roy Chowdhury, Nicholas Monath, Ahmad Beirami, Rahul
  Kidambi, Avinava Dubey, Amr Ahmed, Snigdha Chaturvedi
Categories: cs.LG
Comments: ICLR 2024 (Spotlight)
\\ ( https://arxiv.org/abs/2310.11401 ,  2257kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12508
replaced with revised version Fri, 1 Mar 2024 18:40:13 GMT   (15535kb,D)

Title: SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency
  in Both Image Classification and Generation
Authors: Chongyu Fan, Jiancheng Liu, Yihua Zhang, Eric Wong, Dennis Wei, Sijia
  Liu
Categories: cs.LG cs.AI
Comments: Accepted by ICLR 2024 as a Spotlight paper
\\ ( https://arxiv.org/abs/2310.12508 ,  15535kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19103
replaced with revised version Fri, 1 Mar 2024 18:45:38 GMT   (235kb,D)

Title: Proving Linear Mode Connectivity of Neural Networks via Optimal
  Transport
Authors: Damien Ferbach, Baptiste Goujaud, Gauthier Gidel, Aymeric Dieuleveut
Categories: cs.LG
Comments: Accepted as a conference paper at AISTATS 2024
\\ ( https://arxiv.org/abs/2310.19103 ,  235kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00460
replaced with revised version Fri, 1 Mar 2024 10:56:19 GMT   (8704kb,D)

Title: Optimal Budgeted Rejection Sampling for Generative Models
Authors: Alexandre Verine and Muni Sreenivas Pydi and Benjamin Negrevergne and
  Yann Chevaleyre
Categories: cs.LG
\\ ( https://arxiv.org/abs/2311.00460 ,  8704kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02198
replaced with revised version Thu, 29 Feb 2024 19:35:41 GMT   (15030kb,D)

Title: Imitation Bootstrapped Reinforcement Learning
Authors: Hengyuan Hu, Suvir Mirchandani, Dorsa Sadigh
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2311.02198 ,  15030kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06040
replaced with revised version Fri, 1 Mar 2024 16:19:51 GMT   (477kb,D)

Title: Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for
  Traffic Forecasting
Authors: Qipeng Qian, Tanwi Mallick
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.06040 ,  477kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08909
replaced with revised version Fri, 1 Mar 2024 10:21:42 GMT   (217kb,D)

Title: Leveraging Gradients for Unsupervised Accuracy Estimation under
  Distribution Shift
Authors: Renchunzi Xie, Ambroise Odonnat, Vasilii Feofanov, Ievgen Redko,
  Jianfeng Zhang, Bo An
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.08909 ,  217kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13662
replaced with revised version Fri, 1 Mar 2024 08:58:05 GMT   (327kb,D)

Title: The Definitive Guide to Policy Gradients in Deep Reinforcement Learning:
  Theory, Algorithms and Implementations
Authors: Matthias Lehmann
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.13662 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16439
replaced with revised version Thu, 29 Feb 2024 23:18:38 GMT   (31kb)

Title: Distribution-Specific Auditing For Subgroup Fairness
Authors: Daniel Hsu, Jizhou Huang, Brendan Juba
Categories: cs.LG cs.CC cs.CY
\\ ( https://arxiv.org/abs/2401.16439 ,  31kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00957
replaced with revised version Fri, 1 Mar 2024 14:07:05 GMT   (141kb,D)

Title: Credal Learning Theory
Authors: Michele Caprio, Maryam Sultana, Eleni Elia, Fabio Cuzzolin
Categories: cs.LG cs.AI stat.ML
Comments: 14 pages, 1 figure
\\ ( https://arxiv.org/abs/2402.00957 ,  141kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03268
replaced with revised version Thu, 29 Feb 2024 22:54:09 GMT   (13522kb,D)

Title: Understanding the Reasoning Ability of Language Models From the
  Perspective of Reasoning Paths Aggregation
Authors: Xinyi Wang, Alfonso Amayuelas, Kexun Zhang, Liangming Pan, Wenhu Chen,
  William Yang Wang
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.03268 ,  13522kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05956
replaced with revised version Fri, 1 Mar 2024 06:18:33 GMT   (2496kb,D)

Title: Multi-scale Transformers with Adaptive Pathways for Time Series
  Forecasting
Authors: Peng Chen, Yingying Zhang, Yunyao Cheng, Yang Shu, Yihang Wang,
  Qingsong Wen, Bin Yang, Chenjuan Guo
Categories: cs.LG
Comments: Accepted by the 12th International Conference on Learning
  Representations (ICLR 2024)
\\ ( https://arxiv.org/abs/2402.05956 ,  2496kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14041
replaced with revised version Fri, 1 Mar 2024 04:55:02 GMT   (4317kb)

Title: E2USD: Efficient-yet-effective Unsupervised State Detection for
  Multivariate Time Series
Authors: Zhichen Lai, Huan Li, Dalin Zhang, Yan Zhao, Weizhu Qian, Christian S.
  Jensen
Categories: cs.LG cs.AI cs.DB
Comments: accepted by The Web Conference 2024 (WWW 2024)
\\ ( https://arxiv.org/abs/2402.14041 ,  4317kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14042
replaced with revised version Fri, 1 Mar 2024 11:46:26 GMT   (3565kb,D)

Title: Protect and Extend -- Using GANs for Synthetic Data Generation of
  Time-Series Medical Records
Authors: Navid Ashrafi, Vera Schmitt, Robert P. Spang, Sebastian M\"oller,
  Jan-Niklas Voigt-Antons
Categories: cs.LG cs.AI cs.CR
\\ ( https://arxiv.org/abs/2402.14042 ,  3565kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14270
replaced with revised version Fri, 1 Mar 2024 15:21:16 GMT   (320kb,D)

Title: Take the Bull by the Horns: Hard Sample-Reweighted Continual Training
  Improves LLM Generalization
Authors: Xuxi Chen, Zhendong Wang, Daouda Sow, Junjie Yang, Tianlong Chen,
  Yingbin Liang, Mingyuan Zhou, Zhangyang Wang
Categories: cs.LG
Comments: Preprint; updated reference and related works
\\ ( https://arxiv.org/abs/2402.14270 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15160
replaced with revised version Fri, 1 Mar 2024 00:58:50 GMT   (26306kb,D)

Title: Spatially-Aware Transformer for Embodied Agents
Authors: Junmo Cho, Jaesik Yoon, Sungjin Ahn
Categories: cs.LG cs.AI
Comments: ICLR 2024 Spotlight. First two authors contributed equally
\\ ( https://arxiv.org/abs/2402.15160 ,  26306kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17978
replaced with revised version Fri, 1 Mar 2024 11:08:48 GMT   (1212kb,D)

Title: Imagine, Initialize, and Explore: An Effective Exploration Method in
  Multi-Agent Reinforcement Learning
Authors: Zeyang Liu, Lipeng Wan, Xinrui Yang, Zhuoran Chen, Xingyu Chen,
  Xuguang Lan
Categories: cs.LG cs.AI cs.MA
Comments: The 38th Annual AAAI Conference on Artificial Intelligence
\\ ( https://arxiv.org/abs/2402.17978 ,  1212kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18609
replaced with revised version Fri, 1 Mar 2024 02:19:25 GMT   (3903kb,D)

Title: ICE-SEARCH: A Language Model-Driven Feature Selection Approach
Authors: Tianze Yang, Tianyi Yang, Shaoshan Liu, Fuyuan Lvu, Xue Liu
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.18609 ,  3903kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18888
replaced with revised version Fri, 1 Mar 2024 15:38:15 GMT   (2473kb,D)

Title: Uncertainty-Based Extensible Codebook for Discrete Federated Learning in
  Heterogeneous Data Silos
Authors: Tianyi Zhang, Yu Cao, Dianbo Liu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.18888 ,  2473kb)
------------------------------------------------------------------------------
\\
arXiv:2011.10577 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 11:17:17 GMT   (6356kb,D)

Title: Deep learning insights into cosmological structure formation
Authors: Luisa Lucie-Smith, Hiranya V. Peiris, Andrew Pontzen, Brian Nord,
  Jeyan Thiyagalingam
Categories: astro-ph.CO astro-ph.IM cs.AI cs.LG
Comments: 17 pages, 10 figures. Accepted in PRD
\\ ( https://arxiv.org/abs/2011.10577 ,  6356kb)
------------------------------------------------------------------------------
\\
arXiv:2202.06834
replaced with revised version Fri, 1 Mar 2024 03:35:35 GMT   (278kb,D)

Title: Memory-Efficient Sequential Pattern Mining with Hybrid Tries
Authors: Amin Hosseininasab, Willem-Jan van Hoeve, Andre A. Cire
Categories: cs.DB cs.AI cs.DS cs.LG
\\ ( https://arxiv.org/abs/2202.06834 ,  278kb)
------------------------------------------------------------------------------
\\
arXiv:2305.00188 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 05:56:59 GMT   (211kb,D)

Title: New Characterizations and Efficient Local Search for General Integer
  Linear Programming
Authors: Peng Lin, Shaowei Cai, Mengchuan Zou, Jinkun Lin
Categories: math.OC cs.AI
MSC-class: 90C10 (Primary), 90C06 (Secondary)
ACM-class: I.2.8; G.2.0
\\ ( https://arxiv.org/abs/2305.00188 ,  211kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11897
replaced with revised version Fri, 1 Mar 2024 03:41:30 GMT   (309kb)

Title: Critical Appraisal of Artificial Intelligence-Mediated Communication
Authors: Dara Tafazoli
Categories: cs.HC cs.AI
\\ ( https://arxiv.org/abs/2305.11897 ,  309kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04455
replaced with revised version Fri, 1 Mar 2024 16:52:19 GMT   (14838kb,D)

Title: Anonymizing Speech: Evaluating and Designing Speaker Anonymization
  Techniques
Authors: Pierre Champion
Categories: cs.CR cs.AI cs.SD eess.AS
Comments: PhD Thesis Pierre Champion | Universit\'e de Lorraine - INRIA Nancy |
  for associated source code, see https://github.com/deep-privacy/SA-toolkit
\\ ( https://arxiv.org/abs/2308.04455 ,  14838kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14625
replaced with revised version Fri, 1 Mar 2024 15:16:19 GMT   (886kb,D)

Title: ARIA: On the Interaction Between Architectures, Initialization and
  Aggregation Methods for Federated Visual Classification
Authors: Vasilis Siomos, Sergio Naval-Marimont, Jonathan Passerat-Palmbach,
  Giacomo Tarroni
Categories: cs.CV cs.AI cs.DC
Comments: Accepted to ISBI 2024, camera-ready version with updated information
  on hyper-parameter tuning and clearer phrasing for practical take-aways
\\ ( https://arxiv.org/abs/2311.14625 ,  886kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12462 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 09:13:41 GMT   (32712kb,D)

Title: Towards an end-to-end artificial intelligence driven global weather
  forecasting system
Authors: Kun Chen, Lei Bai, Fenghua Ling, Peng Ye, Tao Chen, Jing-Jia Luo, Hao
  Chen, Kang Chen, Tao Han, Wanli Ouyang
Categories: physics.ao-ph cs.AI cs.LG
\\ ( https://arxiv.org/abs/2312.12462 ,  32712kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00037 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 18:01:10 GMT   (3106kb,D)

Title: Messenger RNA Design via Expected Partition Function and Continuous
  Optimization
Authors: Ning Dai, Wei Yu Tang, Tianshuo Zhou, David H. Mathews, Liang Huang
Categories: q-bio.BM cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.00037 ,  3106kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10910 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 21:05:00 GMT   (4027kb,D)

Title: Metacognition is all you need? Using Introspection in Generative Agents
  to Improve Goal-directed Behavior
Authors: Jason Toy, Josh MacAdam, Phil Tabor
Categories: q-bio.NC cs.AI
Comments: 9 pages, 4 figures
\\ ( https://arxiv.org/abs/2401.10910 ,  4027kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17010
replaced with revised version Fri, 1 Mar 2024 09:53:48 GMT   (77kb)

Title: Finetuning Large Language Models for Vulnerability Detection
Authors: Alexey Shestov, Rodion Levichev, Ravil Mussabayev, Evgeny Maslov,
  Anton Cheshkov, Pavel Zadorozhny
Categories: cs.CR cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.17010 ,  77kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05493
replaced with revised version Fri, 1 Mar 2024 05:22:38 GMT   (2195kb,D)

Title: Investigating White-Box Attacks for On-Device Models
Authors: Mingyi Zhou, Xiang Gao, Jing Wu, Kui Liu, Hailong Sun, Li Li
Categories: cs.SE cs.AI cs.CR
Comments: Published in The International Conference on Software Engineering
  2024 (ICSE'24)
\\ ( https://arxiv.org/abs/2402.05493 ,  2195kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07939
replaced with revised version Fri, 1 Mar 2024 05:20:47 GMT   (20291kb,D)

Title: UFO: A UI-Focused Agent for Windows OS Interaction
Authors: Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua
  Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang
Categories: cs.HC cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.07939 ,  20291kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12810
replaced with revised version Fri, 1 Mar 2024 15:02:25 GMT   (13788kb,D)

Title: PIP-Net: Pedestrian Intention Prediction in the Wild
Authors: Mohsen Azarmi, Mahdi Rezaei, He Wang, Sebastien Glaser
Categories: cs.CV cs.AI cs.NE eess.IV stat.ML
\\ ( https://arxiv.org/abs/2402.12810 ,  13788kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14978
replaced with revised version Thu, 29 Feb 2024 22:47:21 GMT   (9613kb,D)

Title: AI-Augmented Brainwriting: Investigating the use of LLMs in group
  ideation
Authors: Orit Shaer, Angelora Cooper, Osnat Mokryn, Andrew L. Kun, Hagit Ben
  Shoshan
Categories: cs.HC cs.AI cs.CY
Comments: Conditionally Accepted to CHI24. 27 pages
ACM-class: H.5.2; J.4
\\ ( https://arxiv.org/abs/2402.14978 ,  9613kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16914
replaced with revised version Fri, 1 Mar 2024 07:26:50 GMT   (1945kb,D)

Title: DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM
  Jailbreakers
Authors: Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, Cho-Jui Hsieh
Categories: cs.CR cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.16914 ,  1945kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18826
replaced with revised version Fri, 1 Mar 2024 02:21:23 GMT   (12kb)

Title: The Machine Can't Replace the Human Heart
Authors: Baihan Lin
Categories: cs.CY cs.AI cs.HC q-bio.NC
\\ ( https://arxiv.org/abs/2402.18826 ,  12kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18929
replaced with revised version Fri, 1 Mar 2024 05:48:17 GMT   (24521kb,D)

Title: Navigating Beyond Dropout: An Intriguing Solution Towards Generalizable
  Image Super Resolution
Authors: Hongjun Wang, Jiyuan Chen, Yinqiang Zheng, Tieyong Zeng
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2402.18929 ,  24521kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19422
replaced with revised version Fri, 1 Mar 2024 09:10:38 GMT   (7221kb,D)

Title: PEM: Prototype-based Efficient MaskFormer for Image Segmentation
Authors: Niccol\`o Cavagnero, Gabriele Rosi, Claudia Cuttano, Francesca
  Pistilli, Marco Ciccone, Giuseppe Averta, Fabio Cermelli
Categories: cs.CV cs.AI
Comments: 8 pages, 3 figures, CVPR 2024
\\ ( https://arxiv.org/abs/2402.19422 ,  7221kb)
------------------------------------------------------------------------------
\\
arXiv:2303.05707
replaced with revised version Fri, 1 Mar 2024 02:32:38 GMT   (3740kb,D)

Title: MuLTI: Efficient Video-and-Language Understanding with Text-Guided
  MultiWay-Sampler and Multiple Choice Modeling
Authors: Jiaqi Xu, Bo Liu, Yunkuo Chen, Mengli Cheng, Xing Shi
Categories: cs.CV cs.CL cs.MM
\\ ( https://arxiv.org/abs/2303.05707 ,  3740kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14779
replaced with revised version Thu, 29 Feb 2024 22:31:53 GMT   (2626kb,D)

Title: Alt-Text with Context: Improving Accessibility for Images on Twitter
Authors: Nikita Srivatsan, Sofia Samaniego, Omar Florez, Taylor
  Berg-Kirkpatrick
Categories: cs.CV cs.CL cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2305.14779 ,  2626kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11971
replaced with revised version Fri, 1 Mar 2024 11:22:54 GMT   (1440kb,D)

Title: EVE: Efficient Vision-Language Pre-training with Masked Prediction and
  Modality-Aware MoE
Authors: Junyi Chen, Longteng Guo, Jia Sun, Shuai Shao, Zehuan Yuan, Liang Lin,
  Dongyu Zhang
Categories: cs.CV cs.CL cs.LG cs.MM
Comments: Accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2308.11971 ,  1440kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15021
replaced with revised version Fri, 1 Mar 2024 01:52:58 GMT   (252kb,D)

Title: CLoVe: Encoding Compositional Language in Contrastive Vision-Language
  Models
Authors: Santiago Castro, Amir Ziai, Avneesh Saluja, Zhuoning Yuan, Rada
  Mihalcea
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2402.15021 ,  252kb)
------------------------------------------------------------------------------
\\
arXiv:1907.02677
replaced with revised version Fri, 1 Mar 2024 10:28:03 GMT   (7529kb,D)

Title: Interpretable Feature Learning in Multivariate Big Data Analysis for
  Network Monitoring
Authors: Jos\'e Camacho, Katarzyna Wasielewska, Rasmus Bro, David Kotz
Categories: cs.NI cs.LG stat.ML
Journal-ref: IEEE Transactions on Network and Service Management, 2024
DOI: 10.1109/TNSM.2024.3368501
\\ ( https://arxiv.org/abs/1907.02677 ,  7529kb)
------------------------------------------------------------------------------
\\
arXiv:2205.02160 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 14:45:21 GMT   (34kb,D)

Title: Making SGD Parameter-Free
Authors: Yair Carmon and Oliver Hinder
Categories: math.OC cs.LG stat.ML
\\ ( https://arxiv.org/abs/2205.02160 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2211.08229
replaced with revised version Thu, 29 Feb 2024 21:26:45 GMT   (9806kb,D)

Title: CorruptEncoder: Data Poisoning based Backdoor Attacks to Contrastive
  Learning
Authors: Jinghuai Zhang and Hongbin Liu and Jinyuan Jia and Neil Zhenqiang Gong
Categories: cs.CR cs.CV cs.LG
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2211.08229 ,  9806kb)
------------------------------------------------------------------------------
\\
arXiv:2301.11862 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 20:00:21 GMT   (2483kb,D)

Title: Neural Additive Models for Location Scale and Shape: A Framework for
  Interpretable Neural Regression Beyond the Mean
Authors: Anton Thielmann, Ren\'e-Marcel Kruse, Thomas Kneib, Benjamin S\"afken
Categories: stat.ML cs.LG
Comments: Accepted at the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024
\\ ( https://arxiv.org/abs/2301.11862 ,  2483kb)
------------------------------------------------------------------------------
\\
arXiv:2301.12357 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 01:24:03 GMT   (1827kb,D)

Title: SPEED: Experimental Design for Policy Evaluation in Linear
  Heteroscedastic Bandits
Authors: Subhojyoti Mukherjee, Qiaomin Xie, Josiah Hanna, Robert Nowak
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2301.12357 ,  1827kb)
------------------------------------------------------------------------------
\\
arXiv:2302.05797 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 18:23:01 GMT   (46kb)

Title: Global Convergence Rate of Deep Equilibrium Models with General
  Activations
Authors: Lan V. Truong
Categories: stat.ML cs.LG
Comments: 32 pages, 4 figures
\\ ( https://arxiv.org/abs/2302.05797 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2303.16548 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 06:18:40 GMT   (3219kb)

Title: Policy Gradient Methods for Discrete Time Linear Quadratic Regulator
  With Random Parameters
Authors: Deyue Li
Categories: math.OC cs.LG
Comments: 57 pages, 4 figures
DOI: 10.1051/cocv/2024014
\\ ( https://arxiv.org/abs/2303.16548 ,  3219kb)
------------------------------------------------------------------------------
\\
arXiv:2304.09914
replaced with revised version Fri, 1 Mar 2024 14:44:49 GMT   (2386kb,D)

Title: The Face of Populism: Examining Differences in Facial Emotional
  Expressions of Political Leaders Using Machine Learning
Authors: Sara Major, Aleksandar Toma\v{s}evi\'c
Categories: cs.CY cs.CV cs.LG cs.SI physics.soc-ph
Comments: Version 3.0: Improved discussion related to the limitations of the
  study
ACM-class: J.4
\\ ( https://arxiv.org/abs/2304.09914 ,  2386kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14916 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 14:37:49 GMT   (10752kb,D)

Title: Tuning-Free Maximum Likelihood Training of Latent Variable Models via
  Coin Betting
Authors: Louis Sharrock, Daniel Dodd, Christopher Nemeth
Categories: stat.ML cs.LG stat.ME
\\ ( https://arxiv.org/abs/2305.14916 ,  10752kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15022 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 16:25:54 GMT   (278kb,D)

Title: Hierarchical clustering with dot products recovers hidden tree structure
Authors: Annie Gray, Alexander Modell, Patrick Rubin-Delanchy, Nick Whiteley
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2305.15022 ,  278kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18502 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 15:45:05 GMT   (8341kb,D)

Title: Escaping mediocrity: how two-layer networks learn hard generalized
  linear models with SGD
Authors: Luca Arnaboldi, Florent Krzakala, Bruno Loureiro, Ludovic Stephan
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2305.18502 ,  8341kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19738 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 17:45:50 GMT   (1457kb,D)

Title: Bures-Wasserstein Means of Graphs
Authors: Isabel Haasler, Pascal Frossard
Categories: stat.ML cs.LG cs.SI eess.SP
\\ ( https://arxiv.org/abs/2305.19738 ,  1457kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03303 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 02:17:43 GMT   (890kb,D)

Title: Global universal approximation of functional input maps on weighted
  spaces
Authors: Christa Cuchiero, Philipp Schmocker, Josef Teichmann
Categories: stat.ML cs.LG math.FA math.PR q-fin.MF
Comments: 60 pages, 4 figures
MSC-class: 26A16, 26E20, 41A65, 41A81, 46E40, 60L10, 68T07
\\ ( https://arxiv.org/abs/2306.03303 ,  890kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02918 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 17:24:23 GMT   (487kb,D)

Title: Spectral Ranking Inferences based on General Multiway Comparisons
Authors: Jianqing Fan, Zhipeng Lou, Weichen Wang, Mengxin Yu
Categories: stat.ME cs.IT cs.LG math.IT math.ST stat.ML stat.TH
Comments: 62 pages, 4 figures
\\ ( https://arxiv.org/abs/2308.02918 ,  487kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04466
replaced with revised version Thu, 29 Feb 2024 19:45:38 GMT   (2524kb,D)

Title: Backdoor Federated Learning by Poisoning Backdoor-Critical Layers
Authors: Haomin Zhuang, Mingxian Yu, Hao Wang, Yang Hua, Jian Li, and Xu Yuan
Categories: cs.CR cs.CV cs.LG
Comments: Accepted to ICLR'24
\\ ( https://arxiv.org/abs/2308.04466 ,  2524kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01213 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 10:54:13 GMT   (2095kb,D)

Title: Implicit regularization of deep residual networks towards neural ODEs
Authors: Pierre Marion, Yu-Han Wu, Michael E. Sander, G\'erard Biau
Categories: stat.ML cs.LG
Comments: ICLR 2024 (spotlight). 40 pages, 3 figures
\\ ( https://arxiv.org/abs/2309.01213 ,  2095kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11143 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 07:39:44 GMT   (2782kb)

Title: Exploring a new machine learning based probabilistic model for
  high-resolution indoor radon mapping, using the German indoor radon survey
  data
Authors: Eric Petermann, Peter Bossew, Joachim Kemski, Valeria Gruber, Nils
  Suhr and Bernd Hoffmann
Categories: stat.ML cs.LG physics.data-an
\\ ( https://arxiv.org/abs/2310.11143 ,  2782kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13653 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 02:23:59 GMT   (4564kb,D)

Title: Optimal Transport for Measures with Noisy Tree Metric
Authors: Tam Le, Truyen Nguyen, Kenji Fukumizu
Categories: stat.ML cs.LG
Comments: To appear in AISTATS 2024
\\ ( https://arxiv.org/abs/2310.13653 ,  4564kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17009 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 22:02:16 GMT   (3096kb,D)

Title: Simulation-based stacking
Authors: Yuling Yao, Bruno R\'egaldo-Saint Blancard, Justin Domke
Categories: stat.ME cs.LG stat.CO
Comments: Published at International Conference on Artificial Intelligence and
  Statistics (AISTATS) 2024
\\ ( https://arxiv.org/abs/2310.17009 ,  3096kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02277 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 18:26:57 GMT   (1320kb,D)

Title: ALEXR: An Optimal Single-Loop Algorithm for Convex Finite-Sum Coupled
  Compositional Stochastic Optimization
Authors: Bokun Wang and Tianbao Yang
Categories: math.OC cs.LG
Comments: Added some closed-form expressions of the dual update
\\ ( https://arxiv.org/abs/2312.02277 ,  1320kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06152 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 07:24:40 GMT   (243kb,D)

Title: Improving the performance of weak supervision searches using transfer
  and meta-learning
Authors: Hugues Beauchesne, Zong-En Chen and Cheng-Wei Chiang
Categories: hep-ph cs.LG
Comments: 20 pages, 7 figures, matches the published version
\\ ( https://arxiv.org/abs/2312.06152 ,  243kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07358 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 22:49:37 GMT   (660kb,D)

Title: Distributional Bellman Operators over Mean Embeddings
Authors: Li Kevin Wenliang, Gr\'egoire D\'eletang, Matthew Aitchison, Marcus
  Hutter, Anian Ruoss, Arthur Gretton, Mark Rowland
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2312.07358 ,  660kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14889 (*cross-listing*)
replaced with revised version Thu, 29 Feb 2024 22:41:23 GMT   (49kb)

Title: On Rate-Optimal Partitioning Classification from Observable and from
  Privatised Data
Authors: Bal\'azs Csan\'ad Cs\'aji, L\'aszl\'o Gy\"orfi, Ambrus Tam\'as, Harro
  Walk
Categories: stat.ML cs.CR cs.LG math.ST stat.TH
\\ ( https://arxiv.org/abs/2312.14889 ,  49kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10726
replaced with revised version Fri, 1 Mar 2024 17:18:30 GMT   (0kb,I)

Title: Empowering Aggregators with Practical Data-Driven Tools: Harnessing
  Aggregated and Disaggregated Flexibility for Demand Response
Authors: Costas Mylonas, Donata Boric, Leila Luttenberger Maric, Alexandros
  Tsitsanis, Eleftheria Petrianou, Magda Foti
Categories: eess.SY cs.LG cs.SY
Comments: We will perform a major update and change in the order of the name of
  the authors
\\ ( https://arxiv.org/abs/2401.10726 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01663
replaced with revised version Fri, 1 Mar 2024 12:34:36 GMT   (367kb,D)

Title: Killer Apps: Low-Speed, Large-Scale AI Weapons
Authors: Philip Feldman, Aaron Dant, James R. Foulds
Categories: cs.CY cs.CR cs.LG
Comments: 10 pages with 10 pages of appendices. 3 Figures, 2 code listings
ACM-class: I.2.7; H.4.3; J.4
\\ ( https://arxiv.org/abs/2402.01663 ,  367kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10043 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 09:34:00 GMT   (597kb,D)

Title: How to validate average calibration for machine learning regression
  tasks ?
Authors: Pascal Pernot
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2402.10043 ,  597kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13918
replaced with revised version Fri, 1 Mar 2024 13:39:07 GMT   (6425kb,D)

Title: BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for
  Cloud Detection and Segmentation in Remote Sensing Imagery
Authors: Loddo Fabio, Dario Piga, Michelucci Umberto, El Ghazouali Safouane
Categories: cs.CV cs.LG eess.IV
Comments: Submitted to Expert Systems and Applications. Under license
  CC-BY-NC-ND
\\ ( https://arxiv.org/abs/2402.13918 ,  6425kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14576
replaced with revised version Fri, 1 Mar 2024 00:21:38 GMT   (1278kb,D)

Title: Edge Caching Based on Deep Reinforcement Learning and Transfer Learning
Authors: Farnaz Niknia, Ping Wang, Zixu Wang, Aakash Agarwal and Adib S. Rezaei
Categories: cs.NI cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2402.14576 ,  1278kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15997
replaced with revised version Thu, 29 Feb 2024 22:04:24 GMT   (6201kb,D)

Title: Cieran: Designing Sequential Colormaps via In-Situ Active Preference
  Learning
Authors: Matt-Heun Hong, Zachary N. Sunberg, Danielle Albers Szafir
Categories: cs.HC cs.GR cs.LG
Comments: CHI 2024. 12 pages/9 figures
DOI: 10.1145/3613904.3642903
\\ ( https://arxiv.org/abs/2402.15997 ,  6201kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17992 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 14:09:44 GMT   (3603kb)

Title: Physics-Informed Machine Learning for Seismic Response Prediction OF
  Nonlinear Steel Moment Resisting Frame Structures
Authors: R. Bailey Bond, Pu Ren, Jerome F. Hajjar, and Hao Sun
Categories: physics.app-ph cs.LG
Comments: 34 pages, 12 figures
\\ ( https://arxiv.org/abs/2402.17992 ,  3603kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19062 (*cross-listing*)
replaced with revised version Fri, 1 Mar 2024 08:54:53 GMT   (25844kb,D)

Title: Graph Convolutional Neural Networks for Automated Echocardiography View
  Recognition: A Holistic Approach
Authors: Sarina Thomas, Cristiana Tiago, B{\o}rge Solli Andreassen, Svein Arne
  Aase, Jurica \v{S}prem, Erik Steen, Anne Solberg, Guy Ben-Yosef
Categories: eess.IV cs.CV cs.LG
Comments: Presented at ASMUS - MICCAI conference 2023, Vancouver
DOI: 10.1007/978-3-031-44521-7_5
\\ ( https://arxiv.org/abs/2402.19062 ,  25844kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
