paper_240229.txt


Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年2月29日 19:35
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Tue 27 Feb 24 19:00:00 GMT  to  Wed 28 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.17786
Date: Sat, 24 Feb 2024 08:22:39 GMT   (1828kb,D)

Title: Stepwise Self-Consistent Mathematical Reasoning with Large Language
  Models
Authors: Zilong Zhao, Yao Rong, Dongyang Guo, Emek G\"ozl\"ukl\"u, Emir
  G\"ulboy, Enkelejda Kasneci
Categories: cs.AI cs.CL cs.LG
\\
  Using Large Language Models for complex mathematical reasoning is difficult,
primarily due to the complexity of multi-step reasoning. The main challenges of
this process include (1) selecting critical intermediate results to advance the
procedure, and (2) limited exploration of potential solutions. To address these
issues, we introduce a novel algorithm, namely Stepwise Self-Consistent
Chain-of-Thought (SSC-CoT). SSC-CoT employs a strategy of selecting
intermediate steps based on the intersection of various reasoning chains.
Additionally, SSC-CoT enables the model to discover critical intermediate steps
by querying a knowledge graph comprising relevant domain knowledge. To validate
SSC-CoT, we present a new dataset, TriMaster100, tailored for complex
trigonometry problems. This dataset contains 100 questions, with each solution
broken down into scored intermediate steps, facilitating a comprehensive
evaluation of the mathematical reasoning process. On TriMaster100, SSC-CoT
triples the effectiveness of the state-of-the-art methods. Furthermore, we
benchmark SSC-CoT on the widely recognized complex mathematical question
dataset, MATH level 5, and it surpasses the second-best method by 7.2% in
accuracy. Code and the TriMaster100 dataset can be found at:
https://github.com/zhao-zilong/ssc-cot.
\\ ( https://arxiv.org/abs/2402.17786 ,  1828kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17791
Date: Mon, 26 Feb 2024 12:28:51 GMT   (1981kb,D)

Title: Label Informed Contrastive Pretraining for Node Importance Estimation on
  Knowledge Graphs
Authors: Tianyu Zhang, Chengbin Hou, Rui Jiang, Xuegong Zhang, Chenghu Zhou, Ke
  Tang, Hairong Lv
Categories: cs.AI cs.LG cs.SI
Comments: Accepted by IEEE TNNLS
DOI: 10.1109/tnnls.2024.3363695
\\
  Node Importance Estimation (NIE) is a task of inferring importance scores of
the nodes in a graph. Due to the availability of richer data and knowledge,
recent research interests of NIE have been dedicating to knowledge graphs for
predicting future or missing node importance scores. Existing state-of-the-art
NIE methods train the model by available labels, and they consider every
interested node equally before training. However, the nodes with higher
importance often require or receive more attention in real-world scenarios,
e.g., people may care more about the movies or webpages with higher importance.
To this end, we introduce Label Informed ContrAstive Pretraining (LICAP) to the
NIE problem for being better aware of the nodes with high importance scores.
Specifically, LICAP is a novel type of contrastive learning framework that aims
to fully utilize the continuous labels to generate contrastive samples for
pretraining embeddings. Considering the NIE problem, LICAP adopts a novel
sampling strategy called top nodes preferred hierarchical sampling to first
group all interested nodes into a top bin and a non-top bin based on node
importance scores, and then divide the nodes within top bin into several finer
bins also based on the scores. The contrastive samples are generated from those
bins, and are then used to pretrain node embeddings of knowledge graphs via a
newly proposed Predicate-aware Graph Attention Networks (PreGAT), so as to
better separate the top nodes from non-top nodes, and distinguish the top nodes
within top bin by keeping the relative order among finer bins. Extensive
experiments demonstrate that the LICAP pretrained embeddings can further boost
the performance of existing NIE methods and achieve the new state-of-the-art
performance regarding both regression and ranking metrics. The source code for
reproducibility is available at https://github.com/zhangtia16/LICAP
\\ ( https://arxiv.org/abs/2402.17791 ,  1981kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17793
Date: Mon, 26 Feb 2024 18:37:18 GMT   (30kb,D)

Title: A Surprising Failure? Multimodal LLMs and the NLVR Challenge
Authors: Anne Wu, Kiant\'e Brantley, Yoav Artzi
Categories: cs.AI cs.CL cs.LG
\\
  This study evaluates three state-of-the-art MLLMs -- GPT-4V, Gemini Pro, and
the open-source model IDEFICS -- on the compositional natural language vision
reasoning task NLVR. Given a human-written sentence paired with a synthetic
image, this task requires the model to determine the truth value of the
sentence with respect to the image. Despite the strong performance demonstrated
by these models, we observe they perform poorly on NLVR, which was constructed
to require compositional and spatial reasoning, and to be robust for semantic
and systematic biases.
\\ ( https://arxiv.org/abs/2402.17793 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17930
Date: Tue, 27 Feb 2024 23:06:53 GMT   (3022kb,D)

Title: Pragmatic Instruction Following and Goal Assistance via Cooperative
  Language-Guided Inverse Planning
Authors: Tan Zhi-Xuan, Lance Ying, Vikash Mansinghka, Joshua B. Tenenbaum
Categories: cs.AI cs.CL cs.LG
Comments: Accepted to AAMAS 2024. 8 pages (excl. references), 5 figures/tables.
  (Appendix: 8 pages, 8 figures/tables). Code available at:
  https://github.com/probcomp/CLIPS.jl
\\
  People often give instructions whose meaning is ambiguous without further
context, expecting that their actions or goals will disambiguate their
intentions. How can we build assistive agents that follow such instructions in
a flexible, context-sensitive manner? This paper introduces cooperative
language-guided inverse plan search (CLIPS), a Bayesian agent architecture for
pragmatic instruction following and goal assistance. Our agent assists a human
by modeling them as a cooperative planner who communicates joint plans to the
assistant, then performs multimodal Bayesian inference over the human's goal
from actions and language, using large language models (LLMs) to evaluate the
likelihood of an instruction given a hypothesized plan. Given this posterior,
our assistant acts to minimize expected goal achievement cost, enabling it to
pragmatically follow ambiguous instructions and provide effective assistance
even when uncertain about the goal. We evaluate these capabilities in two
cooperative planning domains (Doors, Keys & Gems and VirtualHome), finding that
CLIPS significantly outperforms GPT-4V, LLM-based literal instruction following
and unimodal inverse planning in both accuracy and helpfulness, while closely
matching the inferences and assistive judgments provided by human raters.
\\ ( https://arxiv.org/abs/2402.17930 ,  3022kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17975
Date: Wed, 28 Feb 2024 01:41:34 GMT   (11584kb,D)

Title: Sample-Efficient Preference-based Reinforcement Learning with Dynamics
  Aware Rewards
Authors: Katherine Metcalf, Miguel Sarabia, Natalie Mackraz, Barry-John
  Theobald
Categories: cs.AI cs.LG
Comments: CoRL 2023. arXiv admin note: substantial text overlap with
  arXiv:2211.06527
\\
  Preference-based reinforcement learning (PbRL) aligns a robot behavior with
human preferences via a reward function learned from binary feedback over agent
behaviors. We show that dynamics-aware reward functions improve the sample
efficiency of PbRL by an order of magnitude. In our experiments we iterate
between: (1) learning a dynamics-aware state-action representation (z^{sa}) via
a self-supervised temporal consistency task, and (2) bootstrapping the
preference-based reward function from (z^{sa}), which results in faster policy
learning and better final policy performance. For example, on quadruped-walk,
walker-walk, and cheetah-run, with 50 preference labels we achieve the same
performance as existing approaches with 500 preference labels, and we recover
83\% and 66\% of ground truth reward policy performance versus only 38\% and
21\%. The performance gains demonstrate the benefits of explicitly learning a
dynamics-aware reward model. Repo: \texttt{https://github.com/apple/ml-reed}.
\\ ( https://arxiv.org/abs/2402.17975 ,  11584kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18023
Date: Wed, 28 Feb 2024 03:38:20 GMT   (20670kb,D)

Title: Do Large Language Models Mirror Cognitive Language Processing?
Authors: Yuqi Ren, Renren Jin, Tongxuan Zhang, Deyi Xiong
Categories: cs.AI cs.CL
\\
  Large language models (LLMs) have demonstrated remarkable capabilities in
text comprehension and logical reasoning, achiving or even surpassing
human-level performance in numerous cognition tasks. As LLMs are trained from
massive textual outputs of human language cognition, it is natural to ask
whether LLMs mirror cognitive language processing. Or to what extend LLMs
resemble cognitive language processing? In this paper, we propose a novel
method that bridge between LLM representations and human cognition signals to
evaluate how effectively LLMs simulate cognitive language processing. We employ
Representational Similarity Analysis (RSA) to mearsure the alignment between 16
mainstream LLMs and fMRI signals of the brain. We empirically investigate the
impact of a variety of factors (e.g., model scaling, alignment training,
instruction appending) on such LLM-brain alignment. Experimental results
indicate that model scaling is positively correlated with LLM-brain similarity,
and alignment training can significantly improve LLM-brain similarity.
Additionally, the performance of a wide range of LLM evaluations (e.g., MMLU,
Chatbot Arena) is highly correlated with the LLM-brain similarity.
\\ ( https://arxiv.org/abs/2402.18023 ,  20670kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18040
Date: Wed, 28 Feb 2024 04:34:15 GMT   (581kb,D)

Title: Automated Discovery of Integral with Deep Learning
Authors: Xiaoxin Yin
Categories: cs.AI cs.LG
\\
  Recent advancements in the realm of deep learning, particularly in the
development of large language models (LLMs), have demonstrated AI's ability to
tackle complex mathematical problems or solving programming challenges.
However, the capability to solve well-defined problems based on extensive
training data differs significantly from the nuanced process of making
scientific discoveries. Trained on almost all human knowledge available,
today's sophisticated LLMs basically learn to predict sequences of tokens. They
generate mathematical derivations and write code in a similar way as writing an
essay, and do not have the ability to pioneer scientific discoveries in the
manner a human scientist would do.
  In this study we delve into the potential of using deep learning to
rediscover a fundamental mathematical concept: integrals. By defining integrals
as area under the curve, we illustrate how AI can deduce the integral of a
given function, exemplified by inferring $\int_{0}^{x} t^2 dt = \frac{x^3}{3}$
and $\int_{0}^{x} ae^{bt} dt = \frac{a}{b} e^{bx} - \frac{a}{b}$. Our
experiments show that deep learning models can approach the task of inferring
integrals either through a sequence-to-sequence model, akin to language
translation, or by uncovering the rudimentary principles of integration, such
as $\int_{0}^{x} t^n dt = \frac{x^{n+1}}{n+1}$.
\\ ( https://arxiv.org/abs/2402.18040 ,  581kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18144
Date: Wed, 28 Feb 2024 08:09:14 GMT   (1350kb)

Title: Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a
  Large Language Model Based on Group-Level Demographic Information
Authors: Seungjong Sun, Eungu Lee, Dongyan Nan, Xiangying Zhao, Wonbyung Lee,
  Bernard J. Jansen, Jang Hyun Kim
Categories: cs.AI cs.CY
Comments: 25 pages, 4 figures, 19 Tables
ACM-class: I.2.7
\\
  Large language models exhibit societal biases associated with demographic
information, including race, gender, and others. Endowing such language models
with personalities based on demographic data can enable generating opinions
that align with those of humans. Building on this idea, we propose "random
silicon sampling," a method to emulate the opinions of the human population
sub-group. Our study analyzed 1) a language model that generates the survey
responses that correspond with a human group based solely on its demographic
distribution and 2) the applicability of our methodology across various
demographic subgroups and thematic questions. Through random silicon sampling
and using only group-level demographic information, we discovered that language
models can generate response distributions that are remarkably similar to the
actual U.S. public opinion polls. Moreover, we found that the replicability of
language models varies depending on the demographic group and topic of the
question, and this can be attributed to inherent societal biases in the models.
Our findings demonstrate the feasibility of mirroring a group's opinion using
only demographic distribution and elucidate the effect of social biases in
language models on such simulations.
\\ ( https://arxiv.org/abs/2402.18144 ,  1350kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18157
Date: Wed, 28 Feb 2024 08:42:23 GMT   (1035kb,D)

Title: From Summary to Action: Enhancing Large Language Models for Complex
  Tasks with Open World APIs
Authors: Yulong Liu, Yunlong Yuan, Chunwei Wang, Jianhua Han, Yongqiang Ma, Li
  Zhang, Nanning Zheng, Hang Xu
Categories: cs.AI cs.CL cs.CV
\\
  The distinction between humans and animals lies in the unique ability of
humans to use and create tools. Tools empower humans to overcome physiological
limitations, fostering the creation of magnificent civilizations. Similarly,
enabling foundational models like Large Language Models (LLMs) with the
capacity to learn external tool usage may serve as a pivotal step toward
realizing artificial general intelligence. Previous studies in this field have
predominantly pursued two distinct approaches to augment the tool invocation
capabilities of LLMs. The first approach emphasizes the construction of
relevant datasets for model fine-tuning. The second approach, in contrast, aims
to fully exploit the inherent reasoning abilities of LLMs through in-context
learning strategies. In this work, we introduce a novel tool invocation
pipeline designed to control massive real-world APIs. This pipeline mirrors the
human task-solving process, addressing complicated real-life user queries. At
each step, we guide LLMs to summarize the achieved results and determine the
next course of action. We term this pipeline `from Summary to action', Sum2Act
for short. Empirical evaluations of our Sum2Act pipeline on the ToolBench
benchmark show significant performance improvements, outperforming established
methods like ReAct and DFSDT. This highlights Sum2Act's effectiveness in
enhancing LLMs for complex real-world tasks.
\\ ( https://arxiv.org/abs/2402.18157 ,  1035kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18381
Date: Wed, 28 Feb 2024 15:02:17 GMT   (4241kb,D)

Title: Large Language Models As Evolution Strategies
Authors: Robert Tjarko Lange, Yingtao Tian, Yujin Tang
Categories: cs.AI cs.LG cs.NE
Comments: 11 pages, 14 figures
\\
  Large Transformer models are capable of implementing a plethora of so-called
in-context learning algorithms. These include gradient descent, classification,
sequence completion, transformation, and improvement. In this work, we
investigate whether large language models (LLMs), which never explicitly
encountered the task of black-box optimization, are in principle capable of
implementing evolutionary optimization algorithms. While previous works have
solely focused on language-based task specification, we move forward and focus
on the zero-shot application of LLMs to black-box optimization. We introduce a
novel prompting strategy, consisting of least-to-most sorting of discretized
population members and querying the LLM to propose an improvement to the mean
statistic, i.e. perform a type of black-box recombination operation.
Empirically, we find that our setup allows the user to obtain an LLM-based
evolution strategy, which we call `EvoLLM', that robustly outperforms baseline
algorithms such as random search and Gaussian Hill Climbing on synthetic BBOB
functions as well as small neuroevolution tasks. Hence, LLMs can act as
`plug-in' in-context recombination operators. We provide several comparative
studies of the LLM's model size, prompt strategy, and context construction.
Finally, we show that one can flexibly improve EvoLLM's performance by
providing teacher algorithm information via instruction fine-tuning on
previously collected teacher optimization trajectories.
\\ ( https://arxiv.org/abs/2402.18381 ,  4241kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18393
Date: Wed, 28 Feb 2024 15:13:33 GMT   (468kb,D)

Title: Evaluating Decision Optimality of Autonomous Driving via Metamorphic
  Testing
Authors: Mingfei Cheng, Yuan Zhou, Xiaofei Xie, Junjie Wang, Guozhu Meng,
  Kairui Yang
Categories: cs.AI cs.NE cs.RO cs.SE
\\
  Autonomous Driving System (ADS) testing is crucial in ADS development, with
the current primary focus being on safety. However, the evaluation of
non-safety-critical performance, particularly the ADS's ability to make optimal
decisions and produce optimal paths for autonomous vehicles (AVs), is equally
vital to ensure the intelligence and reduce risks of AVs. Currently, there is
little work dedicated to assessing ADSs' optimal decision-making performance
due to the lack of corresponding oracles and the difficulty in generating
scenarios with non-optimal decisions. In this paper, we focus on evaluating the
decision-making quality of an ADS and propose the first method for detecting
non-optimal decision scenarios (NoDSs), where the ADS does not compute optimal
paths for AVs. Firstly, to deal with the oracle problem, we propose a novel
metamorphic relation (MR) aimed at exposing violations of optimal decisions.
The MR identifies the property that the ADS should retain optimal decisions
when the optimal path remains unaffected by non-invasive changes. Subsequently,
we develop a new framework, Decictor, designed to generate NoDSs efficiently.
Decictor comprises three main components: Non-invasive Mutation, MR Check, and
Feedback. The Non-invasive Mutation ensures that the original optimal path in
the mutated scenarios is not affected, while the MR Check is responsible for
determining whether non-optimal decisions are made. To enhance the
effectiveness of identifying NoDSs, we design a feedback metric that combines
both spatial and temporal aspects of the AV's movement. We evaluate Decictor on
Baidu Apollo, an open-source and production-grade ADS. The experimental results
validate the effectiveness of Decictor in detecting non-optimal decisions of
ADSs. Our work provides valuable and original insights into evaluating the
non-safety-critical performance of ADSs.
\\ ( https://arxiv.org/abs/2402.18393 ,  468kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18409
Date: Wed, 28 Feb 2024 15:28:36 GMT   (7620kb,D)

Title: A Cognitive Evaluation Benchmark of Image Reasoning and Description for
  Large Vision Language Models
Authors: Xiujie Song, Mengyue Wu, Kenny Q. Zhu, Chunhao Zhang, Yanyi Chen
Categories: cs.AI cs.CL cs.CV
\\
  Large Vision Language Models (LVLMs), despite their recent success, are
hardly comprehensively tested for their cognitive abilities. Inspired by the
prevalent use of the "Cookie Theft" task in human cognition test, we propose a
novel evaluation benchmark to evaluate high-level cognitive ability of LVLMs
using images with rich semantics. It defines eight reasoning capabilities and
consists of an image description task and a visual question answering task. Our
evaluation on well-known LVLMs shows that there is still a large gap in
cognitive ability between LVLMs and humans.
\\ ( https://arxiv.org/abs/2402.18409 ,  7620kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18426
Date: Wed, 28 Feb 2024 15:51:05 GMT   (2873kb,D)

Title: A Relational Inductive Bias for Dimensional Abstraction in Neural
  Networks
Authors: Declan Campbell, Jonathan D. Cohen
Categories: cs.AI cs.LG
\\
  The human cognitive system exhibits remarkable flexibility and generalization
capabilities, partly due to its ability to form low-dimensional, compositional
representations of the environment. In contrast, standard neural network
architectures often struggle with abstract reasoning tasks, overfitting, and
requiring extensive data for training. This paper investigates the impact of
the relational bottleneck -- a mechanism that focuses processing on relations
among inputs -- on the learning of factorized representations conducive to
compositional coding and the attendant flexibility of processing. We
demonstrate that such a bottleneck not only improves generalization and
learning efficiency, but also aligns network performance with human-like
behavioral biases. Networks trained with the relational bottleneck developed
orthogonal representations of feature dimensions latent in the dataset,
reflecting the factorized structure thought to underlie human cognitive
flexibility. Moreover, the relational network mimics human biases towards
regularity without pre-specified symbolic primitives, suggesting that the
bottleneck fosters the emergence of abstract representations that confer
flexibility akin to symbols.
\\ ( https://arxiv.org/abs/2402.18426 ,  2873kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18496
Date: Wed, 28 Feb 2024 17:25:59 GMT   (2666kb,D)

Title: Language Models Represent Beliefs of Self and Others
Authors: Wentao Zhu, Zhining Zhang, Yizhou Wang
Categories: cs.AI cs.CL
Comments: project page: https://walter0807.github.io/RepBelief/
\\
  Understanding and attributing mental states, known as Theory of Mind (ToM),
emerges as a fundamental capability for human social reasoning. While Large
Language Models (LLMs) appear to possess certain ToM abilities, the mechanisms
underlying these capabilities remain elusive. In this study, we discover that
it is possible to linearly decode the belief status from the perspectives of
various agents through neural activations of language models, indicating the
existence of internal representations of self and others' beliefs. By
manipulating these representations, we observe dramatic changes in the models'
ToM performance, underscoring their pivotal role in the social reasoning
process. Additionally, our findings extend to diverse social reasoning tasks
that involve different causal inference patterns, suggesting the potential
generalizability of these representations.
\\ ( https://arxiv.org/abs/2402.18496 ,  2666kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17811
Date: Tue, 27 Feb 2024 14:45:04 GMT   (2693kb,D)

Title: TruthX: Alleviating Hallucinations by Editing Large Language Models in
  Truthful Space
Authors: Shaolei Zhang, Tian Yu, Yang Feng
Categories: cs.CL cs.AI cs.LG
Comments: Code: https://github.com/ictnlp/TruthX, A Llama-2-7B-Chat model with
  baked-in TruthX: https:// huggingface.co/ICTNLP/Llama-2-7b-chat-TruthX
\\
  Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks. However, they sometimes suffer from producing hallucinations,
particularly in cases where they may generate untruthful responses despite
possessing the correct knowledge. In this paper, we propose TruthX, an
inference-time method to elicit the truthfulness of LLMs by editing their
internal representations in truthful space. TruthX employs an auto-encoder to
map LLM's representations into semantic and truthful latent spaces
respectively, and applies contrastive learning to identify a truthful editing
direction within the truthful space. During inference, by editing LLM's
internal representations in truthful space, TruthX effectively enhances the
truthfulness of LLMs. Experiments show that TruthX effectively improves the
truthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark.
Further analyses suggest that the truthful space acquired by TruthX plays a
pivotal role in controlling LLM to produce truthful or hallucinatory responses.
\\ ( https://arxiv.org/abs/2402.17811 ,  2693kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17834
Date: Tue, 27 Feb 2024 19:00:07 GMT   (1635kb,D)

Title: Stable LM 2 1.6B Technical Report
Authors: Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym
  Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper,
  Ashish Datta, Meng Lee, Emad Mostaque, Michael Pieler, Nikhil Pinnaparju,
  Paulo Rocha, Harry Saini, Hannah Teufel, Niccolo Zanichelli, Carlos Riquelme
Categories: cs.CL stat.ML
Comments: 23 pages, 6 figures
\\
  We introduce StableLM 2 1.6B, the first in a new generation of our language
model series. In this technical report, we present in detail the data and
training procedure leading to the base and instruction-tuned versions of
StableLM 2 1.6B. The weights for both models are available via Hugging Face for
anyone to download and use. The report contains thorough evaluations of these
models, including zero- and few-shot benchmarks, multilingual benchmarks, and
the MT benchmark focusing on multi-turn dialogues. At the time of publishing
this report, StableLM 2 1.6B was the state-of-the-art open model under 2B
parameters by a significant margin. Given its appealing small size, we also
provide throughput measurements on a number of edge devices. In addition, we
open source several quantized checkpoints and provide their performance metrics
compared to the original model.
\\ ( https://arxiv.org/abs/2402.17834 ,  1635kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17840
Date: Tue, 27 Feb 2024 19:08:05 GMT   (1066kb,D)

Title: Follow My Instruction and Spill the Beans: Scalable Data Extraction from
  Retrieval-Augmented Generation Systems
Authors: Zhenting Qi, Hanlin Zhang, Eric Xing, Sham Kakade, Himabindu Lakkaraju
Categories: cs.CL cs.AI cs.CR cs.LG
\\
  Retrieval-Augmented Generation (RAG) improves pre-trained models by
incorporating external knowledge at test time to enable customized adaptation.
We study the risk of datastore leakage in Retrieval-In-Context RAG Language
Models (LMs). We show that an adversary can exploit LMs' instruction-following
capabilities to easily extract text data verbatim from the datastore of RAG
systems built with instruction-tuned LMs via prompt injection. The
vulnerability exists for a wide range of modern LMs that span Llama2,
Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the
exploitability exacerbates as the model size scales up. Extending our study to
production RAG models GPTs, we design an attack that can cause datastore
leakage with a 100% success rate on 25 randomly selected customized GPTs with
at most 2 queries, and we extract text data verbatim at a rate of 41% from a
book of 77,000 words and 3% from a corpus of 1,569,000 words by prompting the
GPTs with only 100 queries generated by themselves.
\\ ( https://arxiv.org/abs/2402.17840 ,  1066kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17882
Date: Tue, 27 Feb 2024 20:48:24 GMT   (1834kb,D)

Title: BlendSQL: A Scalable Dialect for Unifying Hybrid Question Answering in
  Relational Algebra
Authors: Parker Glenn, Parag Pravin Dakle, Liang Wang, Preethi Raghavan
Categories: cs.CL
Comments: For associated codebase, see https://github.com/parkervg/blendsql
\\
  Many existing end-to-end systems for hybrid question answering tasks can
often be boiled down to a "prompt-and-pray" paradigm, where the user has
limited control and insight into the intermediate reasoning steps used to
achieve the final result. Additionally, due to the context size limitation of
many transformer-based LLMs, it is often not reasonable to expect that the full
structured and unstructured context will fit into a given prompt in a zero-shot
setting, let alone a few-shot setting. We introduce BlendSQL, a superset of
SQLite to act as a unified dialect for orchestrating reasoning across both
unstructured and structured data. For hybrid question answering tasks involving
multi-hop reasoning, we encode the full decomposed reasoning roadmap into a
single interpretable BlendSQL query. Notably, we show that BlendSQL can scale
to massive datasets and improve the performance of end-to-end systems while
using 35% fewer tokens. Our code is available and installable as a package at
https://github.com/parkervg/blendsql.
\\ ( https://arxiv.org/abs/2402.17882 ,  1834kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17887
Date: Tue, 27 Feb 2024 21:01:41 GMT   (1792kb,D)

Title: JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning
  and Professional Question Answering Capability
Authors: Junda Wang, Zhichao Yang, Zonghai Yao, Hong Yu
Categories: cs.CL cs.IR
\\
  With the explosive growth of medical data and the rapid development of
artificial intelligence technology, precision medicine has emerged as a key to
enhancing the quality and efficiency of healthcare services. In this context,
Large Language Models (LLMs) play an increasingly vital role in medical
knowledge acquisition and question-answering systems. To further improve the
performance of these systems in the medical domain, we introduce an innovative
method that jointly trains an Information Retrieval (IR) system and an LLM
during the fine-tuning phase. This approach, which we call Joint Medical LLM
and Retrieval Training (JMLR), is designed to overcome the challenges faced by
traditional models in handling medical question-answering tasks. By employing a
synchronized training mechanism, JMLR reduces the demand for computational
resources and enhances the model's ability to leverage medical knowledge for
reasoning and answering questions. Our experimental results demonstrate that
JMLR-13B (81.2% on Amboos, 61.3% on MedQA) outperforms models using
conventional pre-training and fine-tuning Meditron-70B (76.4% on AMBOSS, 60.3%
on MedQA). For models of the same 7B scale, JMLR-7B(68.7% on Amboos, 51.7% on
MedQA) significantly outperforms other public models (Meditron-7B: 50.1%,
47.9%), proving its superiority in terms of cost (our training time: 37 hours,
traditional method: 144 hours), efficiency, and effectiveness in medical
question-answering tasks. Through this work, we provide a new and efficient
knowledge enhancement tool for healthcare, demonstrating the great potential of
integrating IR and LLM training in precision medical information retrieval and
question-answering systems.
\\ ( https://arxiv.org/abs/2402.17887 ,  1792kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17896
Date: Tue, 27 Feb 2024 21:27:16 GMT   (7400kb,D)

Title: Researchy Questions: A Dataset of Multi-Perspective, Decompositional
  Questions for LLM Web Agents
Authors: Corby Rosset, Ho-Lam Chung, Guanghui Qin, Ethan C. Chau, Zhuo Feng,
  Ahmed Awadallah, Jennifer Neville, Nikhil Rao
Categories: cs.CL cs.AI
\\
  Existing question answering (QA) datasets are no longer challenging to most
powerful Large Language Models (LLMs). Traditional QA benchmarks like TriviaQA,
NaturalQuestions, ELI5 and HotpotQA mainly study ``known unknowns'' with clear
indications of both what information is missing, and how to find it to answer
the question. Hence, good performance on these benchmarks provides a false
sense of security. A yet unmet need of the NLP community is a bank of
non-factoid, multi-perspective questions involving a great deal of unclear
information needs, i.e. ``unknown uknowns''. We claim we can find such
questions in search engine logs, which is surprising because most
question-intent queries are indeed factoid. We present Researchy Questions, a
dataset of search engine queries tediously filtered to be non-factoid,
``decompositional'' and multi-perspective. We show that users spend a lot of
``effort'' on these questions in terms of signals like clicks and session
length, and that they are also challenging for GPT-4. We also show that ``slow
thinking'' answering techniques, like decomposition into sub-questions shows
benefit over answering directly. We release $\sim$ 100k Researchy Questions,
along with the Clueweb22 URLs that were clicked.
\\ ( https://arxiv.org/abs/2402.17896 ,  7400kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17897
Date: Tue, 27 Feb 2024 21:27:35 GMT   (300kb,D)

Title: A Language Model based Framework for New Concept Placement in Ontologies
Authors: Hang Dong, Jiaoyan Chen, Yuan He, Yongsheng Gao, Ian Horrocks
Categories: cs.CL cs.IR
Comments: 20 pages, 3 figures, accepted for ESWC 2024
ACM-class: I.2.7; I.2.4
\\
  We investigate the task of inserting new concepts extracted from texts into
an ontology using language models. We explore an approach with three steps:
edge search which is to find a set of candidate locations to insert (i.e.,
subsumptions between concepts), edge formation and enrichment which leverages
the ontological structure to produce and enhance the edge candidates, and edge
selection which eventually locates the edge to be placed into. In all steps, we
propose to leverage neural methods, where we apply embedding-based methods and
contrastive learning with Pre-trained Language Models (PLMs) such as BERT for
edge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder,
and Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for
edge selection. We evaluate the methods on recent datasets created using the
SNOMED CT ontology and the MedMentions entity linking benchmark. The best
settings in our framework use fine-tuned PLM for search and a multi-label
Cross-encoder for selection. Zero-shot prompting of LLMs is still not adequate
for the task, and we proposed explainable instruction tuning of LLMs for
improved performance. Our study shows the advantages of PLMs and highlights the
encouraging performance of LLMs that motivates future studies.
\\ ( https://arxiv.org/abs/2402.17897 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17914
Date: Tue, 27 Feb 2024 22:06:55 GMT   (2289kb,D)

Title: Extracting Lexical Features from Dialects via Interpretable Dialect
  Classifiers
Authors: Roy Xie, Orevaoghene Ahia, Yulia Tsvetkov, Antonios Anastasopoulos
Categories: cs.CL cs.AI
Comments: Code is available at
  https://github.com/ruoyuxie/interpretable_dialect_classifier
\\
  Identifying linguistic differences between dialects of a language often
requires expert knowledge and meticulous human analysis. This is largely due to
the complexity and nuance involved in studying various dialects. We present a
novel approach to extract distinguishing lexical features of dialects by
utilizing interpretable dialect classifiers, even in the absence of human
experts. We explore both post-hoc and intrinsic approaches to interpretability,
conduct experiments on Mandarin, Italian, and Low Saxon, and experimentally
demonstrate that our method successfully identifies key language-specific
lexical features that contribute to dialectal variations.
\\ ( https://arxiv.org/abs/2402.17914 ,  2289kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17916
Date: Tue, 27 Feb 2024 22:07:52 GMT   (2407kb,D)

Title: LLM-Resistant Math Word Problem Generation via Adversarial Attacks
Authors: Roy Xie, Chengxuan Huang, Junlin Wang, Bhuwan Dhingra
Categories: cs.CL cs.AI
Comments: Code is available at
  https://github.com/ruoyuxie/adversarial_mwps_generation
\\
  Large language models (LLMs) have significantly transformed the educational
landscape. As current plagiarism detection tools struggle to keep pace with
LLMs' rapid advancements, the educational community faces the challenge of
assessing students' true problem-solving abilities in the presence of LLMs. In
this work, we explore a new paradigm for ensuring fair evaluation -- generating
adversarial examples which preserve the structure and difficulty of the
original questions aimed for assessment, but are unsolvable by LLMs. Focusing
on the domain of math word problems, we leverage abstract syntax trees to
structurally generate adversarial examples that cause LLMs to produce incorrect
answers by simply editing the numeric values in the problems. We conduct
experiments on various open- and closed-source LLMs, quantitatively and
qualitatively demonstrating that our method significantly degrades their math
problem-solving ability. We identify shared vulnerabilities among LLMs and
propose a cost-effective approach to attack high-cost models. Additionally, we
conduct automatic analysis on math problems and investigate the cause of
failure to guide future research on LLM's mathematical capability.
\\ ( https://arxiv.org/abs/2402.17916 ,  2407kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17934
Date: Tue, 27 Feb 2024 23:12:45 GMT   (246kb,D)

Title: Multitask Multilingual Model Adaptation with Featurized Low-Rank
  Mixtures
Authors: Chu-Cheng Lin and Xinyi Wang and Jonathan H. Clark and Han Lu and Yun
  Zhu and Chenxi Whitehouse and Hongkun Yu
Categories: cs.CL cs.AI
\\
  Adapting pretrained large language models (LLMs) to various downstream tasks
in tens or hundreds of human languages is computationally expensive.
Parameter-efficient fine-tuning (PEFT) significantly reduces the adaptation
cost, by tuning only a small amount of parameters. However, directly applying
PEFT methods such as LoRA (Hu et al., 2022) on diverse dataset mixtures could
lead to suboptimal performance due to limited parameter capacity and negative
interference among different datasets. In this work, we propose Featurized
Low-rank Mixtures (FLix), a novel PEFT method designed for effective multitask
multilingual tuning. FLix associates each unique dataset feature, such as the
dataset's language or task, with its own low-rank weight update parameters. By
composing feature-specific parameters for each dataset, FLix can accommodate
diverse dataset mixtures and generalize better to unseen datasets. Our
experiments show that FLix leads to significant improvements over a variety of
tasks for both supervised learning and zero-shot settings using different
training data mixtures.
\\ ( https://arxiv.org/abs/2402.17934 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17936
Date: Tue, 27 Feb 2024 23:29:10 GMT   (241kb,D)

Title: Acquiring Linguistic Knowledge from Multimodal Input
Authors: Theodor Amariucai, Alex Warstadt
Categories: cs.CL
Comments: in Proceedings of the BabyLM Challenge at the 27th Conference on
  Computational Natural Language Learning
\\
  In contrast to children, language models (LMs) exhibit considerably inferior
data efficiency when acquiring language. In this submission to the BabyLM
Challenge (Warstadt et al., 2023), we test the hypothesis that this data
efficiency gap is partly caused by a lack of multimodal input and grounding in
the learning environment of typical language models. Although previous work
looking into this question found that multimodal training can even harm
language-only performance, we speculate that these findings can be attributed
to catastrophic forgetting of complex language due to fine-tuning on captions
data. To test our hypothesis, we perform an ablation study on FLAVA (Singh et
al., 2022), a multimodal vision-and-language model, independently varying the
volume of text and vision input to quantify how much text data (if any) can be
offset by vision at different data scales. We aim to limit catastrophic
forgetting through a multitask pretraining regime that includes unimodal
text-only tasks and data sampled from WiT, the relatively diverse
Wikipedia-based dataset (Srinivasan et al., 2021). Our results are largely
negative: Multimodal pretraining does not harm our models' language performance
but does not consistently help either. That said, our conclusions are limited
by our having been able to conduct only a small number of runs. While we must
leave open the possibility that multimodal input explains some of the gap in
data efficiency between LMs and humans, positive evidence for this hypothesis
will require better architectures and techniques for multimodal training.
\\ ( https://arxiv.org/abs/2402.17936 ,  241kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17944
Date: Tue, 27 Feb 2024 23:59:01 GMT   (263kb,D)

Title: Large Language Models on Tabular Data -- A Survey
Authors: Xi Fang, Weijie Xu, Fiona Anting Tan, Jiani Zhang, Ziqing Hu, Yanjun
  Qi, Scott Nickleach, Diego Socolinsky, Srinivasan Sengamedu, Christos
  Faloutsos
Categories: cs.CL
Comments: 40 pages, 3 figures, 8 tables
MSC-class: 68T50
ACM-class: I.2.7
\\
  Recent breakthroughs in large language modeling have facilitated rigorous
exploration of their application in diverse tasks related to tabular data
modeling, such as prediction, tabular data synthesis, question answering, and
table understanding. Each task presents unique challenges and opportunities.
However, there is currently a lack of comprehensive review that summarizes and
compares the key techniques, metrics, datasets, models, and optimization
approaches in this research domain. This survey aims to address this gap by
consolidating recent progress in these areas, offering a thorough survey and
taxonomy of the datasets, metrics, and methodologies utilized. It identifies
strengths, limitations, unexplored territories, and gaps in the existing
literature, while providing some insights for future research directions in
this vital and rapidly evolving field. It also provides relevant code and
datasets references. Through this comprehensive review, we hope to provide
interested readers with pertinent references and insightful perspectives,
empowering them with the necessary tools and knowledge to effectively navigate
and address the prevailing challenges in the field.
\\ ( https://arxiv.org/abs/2402.17944 ,  263kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17946
Date: Wed, 28 Feb 2024 00:09:07 GMT   (1095kb,D)

Title: Gradient-Free Adaptive Global Pruning for Pre-trained Language Models
Authors: Guangji Bai, Yijiang Li, Chen Ling, Kibaek Kim, Liang Zhao
Categories: cs.CL
Comments: Preprint
\\
  The transformative impact of large language models (LLMs) like LLaMA and GPT
on natural language processing is countered by their prohibitive computational
demands. Pruning has emerged as a pivotal compression strategy, introducing
sparsity to enhance both memory and computational efficiency. Yet, traditional
global pruning is impractical for LLMs due to scalability issues, while local
pruning, despite its efficiency, leads to suboptimal solutions. Addressing
these challenges, we propose Adaptive Global Pruning (AdaGP), a novel framework
that redefines the global pruning process into manageable, coordinated
subproblems, allowing for resource-efficient optimization with global
optimality. AdaGP's approach, which conceptualizes LLMs as a chain of modular
functions and leverages auxiliary variables for problem decomposition, not only
facilitates a pragmatic application on LLMs but also demonstrates significant
performance improvements, particularly in high-sparsity regimes where it
surpasses current state-of-the-art methods.
\\ ( https://arxiv.org/abs/2402.17946 ,  1095kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17954
Date: Wed, 28 Feb 2024 00:24:29 GMT   (9033kb,D)

Title: Multilingual Speech Models for Automatic Speech Recognition Exhibit
  Gender Performance Gaps
Authors: Giuseppe Attanasio, Beatrice Savoldi, Dennis Fucci, Dirk Hovy
Categories: cs.CL
Comments: 19 pages. Code and artifacts at
  https://github.com/g8a9/multilingual-asr-gender-gap
\\
  Current voice recognition approaches use multi-task, multilingual models for
speech tasks like Automatic Speech Recognition (ASR) to make them applicable to
many languages without substantial changes. However, broad language coverage
can still mask performance gaps within languages, for example, across genders.
We systematically evaluate multilingual ASR systems on gendered performance
gaps. Using two popular models on three datasets in 19 languages across seven
language families, we find clear gender disparities. However, the advantaged
group varies between languages. While there are no significant differences
across groups in phonetic variables (pitch, speaking rate, etc.), probing the
model's internal states reveals a negative correlation between probe
performance and the gendered performance gap. I.e., the easier to distinguish
speaker gender in a language, the more the models favor female speakers. Our
results show that group disparities remain unsolved despite great progress on
multi-tasking and multilinguality. We provide first valuable insights for
evaluating gender gaps in multilingual ASR systems. We release all code and
artifacts at https://github.com/g8a9/multilingual-asr-gender-gap.
\\ ( https://arxiv.org/abs/2402.17954 ,  9033kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17959
Date: Wed, 28 Feb 2024 00:49:06 GMT   (104kb,D)

Title: An Iterative Associative Memory Model for Empathetic Response Generation
Authors: Zhou Yang, Zhaochun Ren, Yufeng Wang, Chao Chen, Haizhou Sun, Xiaofei
  Zhu, Xiangwen Liao
Categories: cs.CL cs.HC
Comments: 12 pages, 4 figures
\\
  Empathetic response generation is to comprehend the cognitive and emotional
states in dialogue utterances and generate proper responses. Psychological
theories posit that comprehending emotional and cognitive states necessitates
iteratively capturing and understanding associated words across dialogue
utterances. However, existing approaches regard dialogue utterances as either a
long sequence or independent utterances for comprehension, which are prone to
overlook the associated words between them. To address this issue, we propose
an Iterative Associative Memory Model (IAMM) for empathetic response
generation. Specifically, we employ a novel second-order interaction attention
mechanism to iteratively capture vital associated words between dialogue
utterances and situations, dialogue history, and a memory module (for storing
associated words), thereby accurately and nuancedly comprehending the
utterances. We conduct experiments on the Empathetic-Dialogue dataset. Both
automatic and human evaluations validate the efficacy of the model. Meanwhile,
variant experiments on LLMs also demonstrate that attending to associated words
improves empathetic comprehension and expression.
\\ ( https://arxiv.org/abs/2402.17959 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17982
Date: Wed, 28 Feb 2024 01:53:37 GMT   (170kb,D)

Title: Collaborative decoding of critical tokens for boosting factuality of
  large language models
Authors: Lifeng Jin, Baolin Peng, Linfeng Song, Haitao Mi, Ye Tian and Dong Yu
Categories: cs.CL
Comments: work in progress
\\
  The most common training pipeline for large language models includes
pretraining, finetuning and aligning phases, with their respective resulting
models, such as the pretrained model and the finetuned model. Finetuned and
aligned models show improved abilities of instruction following and safe
generation, however their abilities to stay factual about the world are
impacted by the finetuning process. Furthermore, the common practice of using
sampling during generation also increases chances of hallucination. In this
work, we introduce a collaborative decoding framework to harness the high
factuality within pretrained models through the concept of critical tokens. We
first design a critical token classifier to decide which model to use for the
next token, and subsequently generates the next token using different decoding
strategies. Experiments with different models and datasets show that our
decoding framework is able to reduce model hallucination significantly,
showcasing the importance of the collaborative decoding framework.
\\ ( https://arxiv.org/abs/2402.17982 ,  170kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17983
Date: Wed, 28 Feb 2024 01:56:00 GMT   (5037kb,D)

Title: M3-VRD: Multimodal Multi-task Multi-teacher Visually-Rich Form Document
  Understanding
Authors: Yihao Ding, Lorenzo Vaiani, Caren Han, Jean Lee, Paolo Garza, Josiah
  Poon, Luca Cagliero
Categories: cs.CL cs.CV
Comments: Work in progress
\\
  This paper presents a groundbreaking multimodal, multi-task, multi-teacher
joint-grained knowledge distillation model for visually-rich form document
understanding. The model is designed to leverage insights from both
fine-grained and coarse-grained levels by facilitating a nuanced correlation
between token and entity representations, addressing the complexities inherent
in form documents. Additionally, we introduce new inter-grained and
cross-grained loss functions to further refine diverse multi-teacher knowledge
distillation transfer process, presenting distribution gaps and a harmonised
understanding of form documents. Through a comprehensive evaluation across
publicly available form document understanding datasets, our proposed model
consistently outperforms existing baselines, showcasing its efficacy in
handling the intricate structures and content of visually complex form
documents.
\\ ( https://arxiv.org/abs/2402.17983 ,  5037kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18005
Date: Wed, 28 Feb 2024 02:40:09 GMT   (1122kb,D)

Title: Exploring Multi-Document Information Consolidation for Scientific
  Sentiment Summarization
Authors: Miao Li and Jey Han Lau and Eduard Hovy
Categories: cs.CL cs.AI
Comments: 18 pages
\\
  Modern natural language generation systems with LLMs exhibit the capability
to generate a plausible summary of multiple documents; however, it is uncertain
if models truly possess the ability of information consolidation to generate
summaries, especially on those source documents with opinionated information.
To make scientific sentiment summarization more grounded, we hypothesize that
in peer review human meta-reviewers follow a three-layer framework of sentiment
consolidation to write meta-reviews and it represents the logic of summarizing
scientific sentiments in meta-review generation. The framework is validated via
human annotation. Based on the framework, we propose evaluation metrics to
assess the quality of generated meta-reviews, and we find that the hypothesis
of the sentiment consolidation framework works out empirically when we
incorporate it as prompts for LLMs to generate meta-reviews in extensive
experiments.
\\ ( https://arxiv.org/abs/2402.18005 ,  1122kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18013
Date: Wed, 28 Feb 2024 03:16:44 GMT   (4885kb,D)

Title: A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems
Authors: Zihao Yi, Jiarui Ouyang, Yuwen Liu, Tianhao Liao, Zhe Xu and Ying Shen
Categories: cs.CL cs.AI
Comments: 35 pages, 10 figures, ACM Computing Surveys
\\
  This survey provides a comprehensive review of research on multi-turn
dialogue systems, with a particular focus on multi-turn dialogue systems based
on large language models (LLMs). This paper aims to (a) give a summary of
existing LLMs and approaches for adapting LLMs to downstream tasks; (b)
elaborate recent advances in multi-turn dialogue systems, covering both
LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems,
along with datasets and evaluation metrics; (c) discuss some future emphasis
and recent research problems arising from the development of LLMs and the
increasing demands on multi-turn dialogue systems.
\\ ( https://arxiv.org/abs/2402.18013 ,  4885kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18025
Date: Wed, 28 Feb 2024 03:44:01 GMT   (279kb,D)

Title: Hire a Linguist!: Learning Endangered Languages with In-Context
  Linguistic Descriptions
Authors: Kexun Zhang, Yee Man Choi, Zhenqiao Song, Taiqi He, William Yang Wang,
  Lei Li
Categories: cs.CL
\\
  How can large language models (LLMs) process and translate endangered
languages? Many languages lack a large corpus to train a decent LLM; therefore
existing LLMs rarely perform well in unseen, endangered languages. On the
contrary, we observe that 2000 endangered languages, though without a large
corpus, have a grammar book or a dictionary. We propose LINGOLLM, a
training-free approach to enable an LLM to process unseen languages that hardly
occur in its pre-training. Our key insight is to demonstrate linguistic
knowledge of an unseen language in an LLM's prompt, including a dictionary, a
grammar book, and morphologically analyzed input text. We implement LINGOLLM on
top of two models, GPT-4 and Mixtral, and evaluate their performance on 5 tasks
across 8 endangered or low-resource languages. Our results show that LINGOLLM
elevates translation capability from GPT-4's 0 to 10.5 BLEU for 10 language
directions. Our findings demonstrate the tremendous value of linguistic
knowledge in the age of LLMs for endangered languages. Our data, code, and
model generations can be found at https://github.com/LLiLab/llm4endangeredlang.
\\ ( https://arxiv.org/abs/2402.18025 ,  279kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18039
Date: Wed, 28 Feb 2024 04:33:20 GMT   (13141kb,D)

Title: ResLoRA: Identity Residual Mapping in Low-Rank Adaption
Authors: Shuhua Shi, Shaohan Huang, Minghui Song, Zhoujun Li, Zihan Zhang,
  Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang
Categories: cs.CL cs.AI
Comments: 14 pages, 7 figures
\\
  As one of the most popular parameter-efficient fine-tuning (PEFT) methods,
low-rank adaptation (LoRA) is commonly applied to fine-tune large language
models (LLMs). However, updating the weights of LoRA blocks effectively and
expeditiously is challenging due to the long calculation path in the original
model. To address this, we propose ResLoRA, an improved framework of LoRA. By
adding residual paths during training and using merging approaches to eliminate
these extra paths during inference, our method can achieve better results in
fewer training steps without any extra trainable parameters or inference cost
compared to LoRA. The experiments on NLG, NLU, and text-to-image tasks
demonstrate the effectiveness of our method. To the best of our knowledge,
ResLoRA is the first work that combines the residual path with LoRA. The code
of our method is available at
https://github.com/microsoft/LMOps/tree/main/reslora .
\\ ( https://arxiv.org/abs/2402.18039 ,  13141kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18041
Date: Wed, 28 Feb 2024 04:35:51 GMT   (5605kb,D)

Title: Datasets for Large Language Models: A Comprehensive Survey
Authors: Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, Lianwen Jin
Categories: cs.CL cs.AI
Comments: 181 pages, 21 figures
\\
  This paper embarks on an exploration into the Large Language Model (LLM)
datasets, which play a crucial role in the remarkable advancements of LLMs. The
datasets serve as the foundational infrastructure analogous to a root system
that sustains and nurtures the development of LLMs. Consequently, examination
of these datasets emerges as a critical topic in research. In order to address
the current lack of a comprehensive overview and thorough analysis of LLM
datasets, and to gain insights into their current status and future trends,
this survey consolidates and categorizes the fundamental aspects of LLM
datasets from five perspectives: (1) Pre-training Corpora; (2) Instruction
Fine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5)
Traditional Natural Language Processing (NLP) Datasets. The survey sheds light
on the prevailing challenges and points out potential avenues for future
investigation. Additionally, a comprehensive review of the existing available
dataset resources is also provided, including statistics from 444 datasets,
covering 8 language categories and spanning 32 domains. Information from 20
dimensions is incorporated into the dataset statistics. The total data size
surveyed surpasses 774.5 TB for pre-training corpora and 700M instances for
other datasets. We aim to present the entire landscape of LLM text datasets,
serving as a comprehensive reference for researchers in this field and
contributing to future studies. Related resources are available at:
https://github.com/lmmlzn/Awesome-LLMs-Datasets.
\\ ( https://arxiv.org/abs/2402.18041 ,  5605kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18043
Date: Wed, 28 Feb 2024 04:42:59 GMT   (6599kb,D)

Title: Crisis talk: analysis of the public debate around the energy crisis and
  cost of living
Authors: Rrubaa Panchendrarajan, Geri Popova, Tony Russell-Rose
Categories: cs.CL
\\
  A prominent media topic in the UK in the early 2020s is the energy crisis
affecting the UK and most of Europe. It brings into a single public debate
issues of energy dependency and sustainability, fair distribution of economic
burdens and cost of living, as well as climate change, risk, and
sustainability. In this paper, we investigate the public discourse around the
energy crisis and cost of living to identify how these pivotal and
contradictory issues are reconciled in this debate and to identify which social
actors are involved and the role they play. We analyse a document corpus
retrieved from UK newspapers from January 2014 to March 2023. We apply a
variety of natural language processing and data visualisation techniques to
identify key topics, novel trends, critical social actors, and the role they
play in the debate, along with the sentiment associated with those actors and
topics. We combine automated techniques with manual discourse analysis to
explore and validate the insights revealed in this study. The findings verify
the utility of these techniques by providing a flexible and scalable pipeline
for discourse analysis and providing critical insights for cost of living -
energy crisis nexus research.
\\ ( https://arxiv.org/abs/2402.18043 ,  6599kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18045
Date: Wed, 28 Feb 2024 04:43:46 GMT   (11158kb,D)

Title: Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using
  FActScore
Authors: Sheikh Shafayat, Eunsu Kim, Juhyun Oh, Alice Oh
Categories: cs.CL
\\
  Large Language Models (LLMs) are prone to factuality hallucination,
generating text that contradicts established knowledge. While extensive
research has addressed this in English, little is known about multilingual
LLMs. This paper systematically evaluates multilingual LLMs' factual accuracy
across languages and geographic regions. We introduce a novel pipeline for
multilingual factuality evaluation, adapting FActScore(Min et al., 2023) for
diverse languages. Our analysis across nine languages reveals that English
consistently outperforms others in factual accuracy and quantity of generated
facts. Furthermore, multilingual models demonstrate a bias towards factual
information from Western continents. These findings highlight the need for
improved multilingual factuality assessment and underscore geographical biases
in LLMs' fact generation.
\\ ( https://arxiv.org/abs/2402.18045 ,  11158kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18048
Date: Wed, 28 Feb 2024 04:56:21 GMT   (5050kb,D)

Title: Characterizing Truthfulness in Large Language Model Generations with
  Local Intrinsic Dimension
Authors: Fan Yin, Jayanth Srinivasa, Kai-Wei Chang
Categories: cs.CL
Comments: preprint, 9 pages, 5 figures
\\
  We study how to characterize and predict the truthfulness of texts generated
from large language models (LLMs), which serves as a crucial step in building
trust between humans and LLMs. Although several approaches based on entropy or
verbalized uncertainty have been proposed to calibrate model predictions, these
methods are often intractable, sensitive to hyperparameters, and less reliable
when applied in generative tasks with LLMs. In this paper, we suggest
investigating internal activations and quantifying LLM's truthfulness using the
local intrinsic dimension (LID) of model activations. Through experiments on
four question answering (QA) datasets, we demonstrate the effectiveness
ohttps://info.arxiv.org/help/prep#abstractsf our proposed method. Additionally,
we study intrinsic dimensions in LLMs and their relations with model layers,
autoregressive language modeling, and the training of LLMs, revealing that
intrinsic dimensions can be a powerful approach to understanding LLMs.
\\ ( https://arxiv.org/abs/2402.18048 ,  5050kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18050
Date: Wed, 28 Feb 2024 04:58:07 GMT   (902kb,D)

Title: MEGAnno+: A Human-LLM Collaborative Annotation System
Authors: Hannah Kim, Kushan Mitra, Rafael Li Chen, Sajjadur Rahman, Dan Zhang
Categories: cs.CL cs.HC
Comments: EACL 2024 Demo
\\
  Large language models (LLMs) can label data faster and cheaper than humans
for various NLP tasks. Despite their prowess, LLMs may fall short in
understanding of complex, sociocultural, or domain-specific context,
potentially leading to incorrect annotations. Therefore, we advocate a
collaborative approach where humans and LLMs work together to produce reliable
and high-quality labels. We present MEGAnno+, a human-LLM collaborative
annotation system that offers effective LLM agent and annotation management,
convenient and robust LLM annotation, and exploratory verification of LLM
labels by humans.
\\ ( https://arxiv.org/abs/2402.18050 ,  902kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18054
Date: Wed, 28 Feb 2024 05:24:21 GMT   (8028kb,D)

Title: Contextualizing Generated Citation Texts
Authors: Biswadip Mandal, Xiangci Li, Jessica Ouyang
Categories: cs.CL
\\
  Abstractive citation text generation is usually framed as an infilling task,
where a sequence-to-sequence model is trained to generate a citation given a
reference paper and the context window around the target; the generated
citation should be a brief discussion of the reference paper as it relates to
the citing context. However, examining a recent LED-based citation generation
system, we find that many of the generated citations are generic summaries of
the reference papers main contribution, ignoring the citation contexts focus on
a different topic. To address this problem, we propose a simple modification to
the citation text generation task: the generation target is not only the
citation itself, but the entire context window, including the target citation.
This approach can be easily applied to any abstractive citation generation
system, and our experimental results show that training in this way is
preferred by human readers and allows the generation model to make use of
contextual clues about what topic to discuss and what stance to take.
\\ ( https://arxiv.org/abs/2402.18054 ,  8028kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18060
Date: Wed, 28 Feb 2024 05:44:41 GMT   (494kb,D)

Title: Benchmarking Large Language Models on Answering and Explaining
  Challenging Medical Questions
Authors: Hanjie Chen, Zhouxiang Fang, Yash Singla, Mark Dredze
Categories: cs.CL
\\
  LLMs have demonstrated impressive performance in answering medical questions,
such as passing medical licensing examinations. However, most existing
benchmarks rely on board exam questions or general medical questions, falling
short in capturing the complexity of realistic clinical cases. Moreover, the
lack of reference explanations for answers hampers the evaluation of model
explanations, which are crucial to supporting doctors in making complex medical
decisions. To address these challenges, we construct two new datasets: JAMA
Clinical Challenge and Medbullets. JAMA Clinical Challenge consists of
questions based on challenging clinical cases, while Medbullets comprises USMLE
Step 2&3 style clinical questions. Both datasets are structured as
multiple-choice question-answering tasks, where each question is accompanied by
an expert-written explanation. We evaluate four LLMs on the two datasets using
various prompts. Experiments demonstrate that our datasets are harder than
previous benchmarks. The inconsistency between automatic and human evaluations
of model-generated explanations highlights the need to develop new metrics to
support future research on explainable medical QA.
\\ ( https://arxiv.org/abs/2402.18060 ,  494kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18061
Date: Wed, 28 Feb 2024 05:45:37 GMT   (849kb,D)

Title: On the use of Silver Standard Data for Zero-shot Classification Tasks in
  Information Extraction
Authors: Jianwei Wang, Tianyin Wang, Ziqian Zeng
Categories: cs.CL cs.AI
Comments: accepted by coling2024. arXiv admin note: substantial text overlap
  with arXiv:2211.13883
\\
  The superior performance of supervised classification methods in the
information extraction (IE) area heavily relies on a large amount of gold
standard data. Recent zero-shot classification methods converted the task to
other NLP tasks (e.g., textual entailment) and used off-the-shelf models of
these NLP tasks to directly perform inference on the test data without using a
large amount of IE annotation data. A potentially valuable by-product of these
methods is the large-scale silver standard data, i.e., pseudo-labeled data by
the off-the-shelf models of other NLP tasks. However, there is no further
investigation into the use of these data. In this paper, we propose a new
framework, Clean-LaVe, which aims to utilize silver standard data to enhance
the zero-shot performance. Clean-LaVe includes four phases: (1) Obtaining
silver data; (2) Identifying relatively clean data from silver data; (3)
Finetuning the off-the-shelf model using clean data; (4) Inference on the test
data. The experimental results show that Clean-LaVe can outperform the baseline
by 5% and 6% on TACRED and Wiki80 dataset in the zero-shot relation
classification task, and by 3%-7% on Smile (Korean and Polish) in the zero-shot
cross-lingual relation classification task, and by 8% on ACE05-E+ in the
zero-shot event argument classification task. The code is share in
https://github.com/wjw136/Clean_LaVe.git.
\\ ( https://arxiv.org/abs/2402.18061 ,  849kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18099
Date: Wed, 28 Feb 2024 06:40:57 GMT   (944kb,D)

Title: Editing Factual Knowledge and Explanatory Ability of Medical Large
  Language Models
Authors: Derong Xu, Ziheng Zhang, Zhihong Zhu, Zhenxi Lin, Qidong Liu, Xian Wu,
  Tong Xu, Xiangyu Zhao, Yefeng Zheng, Enhong Chen
Categories: cs.CL cs.AI
\\
  Model editing aims to precisely modify the behaviours of large language
models (LLMs) on specific knowledge while keeping irrelevant knowledge
unchanged. It has been proven effective in resolving hallucination and
out-of-date issues in LLMs. As a result, it can boost the application of LLMs
in many critical domains (e.g., medical domain), where the hallucination is not
tolerable. In this paper, we propose two model editing studies and validate
them in the medical domain: (1) directly editing the factual medical knowledge
and (2) editing the explanations to facts. Meanwhile, we observed that current
model editing methods struggle with the specialization and complexity of
medical knowledge. Therefore, we propose MedLaSA, a novel Layer-wise Scalable
Adapter strategy for medical model editing. It employs causal tracing to
identify the precise location of knowledge in neurons and then introduces
scalable adapters into the dense layers of LLMs. These adapters are assigned
scaling values based on the corresponding specific knowledge. To evaluate the
editing impact, we build two benchmark datasets and introduce a series of
challenging and comprehensive metrics. Extensive experiments on medical LLMs
demonstrate the editing efficiency of MedLaSA, without affecting irrelevant
knowledge that is not edited.
\\ ( https://arxiv.org/abs/2402.18099 ,  944kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18101
Date: Wed, 28 Feb 2024 06:43:43 GMT   (1615kb)

Title: Assessing the Efficacy of Grammar Error Correction: A Human Evaluation
  Approach in the Japanese Context
Authors: Qiao Wang and Zheng Yuan
Categories: cs.CL
Comments: 2024 Joint International Conference on Computational Linguistics,
  Language Resources and Evaluation (LREC-COLING 2024)
\\
  In this study, we evaluated the performance of the state-of-the-art sequence
tagging grammar error detection and correction model (SeqTagger) using Japanese
university students' writing samples. With an automatic annotation toolkit,
ERRANT, we first evaluated SeqTagger's performance on error correction with
human expert correction as the benchmark. Then a human-annotated approach was
adopted to evaluate Seqtagger's performance in error detection using a subset
of the writing dataset. Results indicated a precision of 63.66% and a recall of
20.19% for error correction in the full dataset. For the subset, after manual
exclusion of irrelevant errors such as semantic and mechanical ones, the model
shows an adjusted precision of 97.98% and an adjusted recall of 42.98% for
error detection, indicating the model's high accuracy but also its
conservativeness. Thematic analysis on errors undetected by the model revealed
that determiners and articles, especially the latter, were predominant.
Specifically, in terms of context-independent errors, the model occasionally
overlooked basic ones and faced challenges with overly erroneous or complex
structures. Meanwhile, context-dependent errors, notably those related to tense
and noun number, as well as those possibly influenced by the students' first
language (L1), remained particularly challenging.
\\ ( https://arxiv.org/abs/2402.18101 ,  1615kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18113
Date: Wed, 28 Feb 2024 07:02:38 GMT   (9520kb,D)

Title: Small But Funny: A Feedback-Driven Approach to Humor Distillation
Authors: Sahithya Ravi, Patrick Huber, Akshat Shrivastava, Aditya Sagar, Ahmed
  Aly, Vered Shwartz, Arash Einolghozati
Categories: cs.CL cs.AI
\\
  The emergence of Large Language Models (LLMs) has brought to light promising
language generation capabilities, particularly in performing tasks like complex
reasoning and creative writing. Consequently, distillation through imitation of
teacher responses has emerged as a popular technique to transfer knowledge from
LLMs to more accessible, Small Language Models (SLMs). While this works well
for simpler tasks, there is a substantial performance gap on tasks requiring
intricate language comprehension and creativity, such as humor generation. We
hypothesize that this gap may stem from the fact that creative tasks might be
hard to learn by imitation alone and explore whether an approach, involving
supplementary guidance from the teacher, could yield higher performance. To
address this, we study the effect of assigning a dual role to the LLM - as a
"teacher" generating data, as well as a "critic" evaluating the student's
performance. Our experiments on humor generation reveal that the incorporation
of feedback significantly narrows the performance gap between SLMs and their
larger counterparts compared to merely relying on imitation. As a result, our
research highlights the potential of using feedback as an additional dimension
to data when transferring complex language abilities via distillation.
\\ ( https://arxiv.org/abs/2402.18113 ,  9520kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18120
Date: Wed, 28 Feb 2024 07:18:39 GMT   (3075kb,D)

Title: Exploring Multilingual Human Value Concepts in Large Language Models: Is
  Value Alignment Consistent, Transferable and Controllable across Languages?
Authors: Shaoyang Xu, Weilong Dong, Zishan Guo, Xinwei Wu, Deyi Xiong
Categories: cs.CL
\\
  Prior research in representation engineering has revealed that LLMs encode
concepts within their representation spaces, predominantly centered around
English. In this study, we extend this philosophy to a multilingual scenario,
delving into multilingual human value concepts in LLMs. Through our
comprehensive exploration covering 7 types of human values, 16 languages and 3
LLM series with distinct multilinguality, we empirically substantiate the
existence of multilingual human values in LLMs. Further cross-lingual analysis
on these concepts discloses 3 traits arising from language resource
disparities: cross-lingual inconsistency, distorted linguistic relationships,
and unidirectional cross-lingual transfer between high- and low-resource
languages, all in terms of human value concepts. Additionally, we validate the
feasibility of cross-lingual control over value alignment capabilities of LLMs,
leveraging the dominant language as a source language. Drawing from our
findings on multilingual value alignment, we prudently provide suggestions on
the composition of multilingual data for LLMs pre-training: including a limited
number of dominant languages for cross-lingual alignment transfer while
avoiding their excessive prevalence, and keeping a balanced distribution of
non-dominant languages. We aspire that our findings would contribute to
enhancing the safety and utility of multilingual AI.
\\ ( https://arxiv.org/abs/2402.18120 ,  3075kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18121
Date: Wed, 28 Feb 2024 07:22:13 GMT   (160kb,D)

Title: Saving the legacy of Hero Ibash: Evaluating Four Language Models for
  Aminoacian
Authors: Yunze Xiao and Yiyang Pan
Categories: cs.CL cs.AI
Comments: 5 pages, 10 figures
\\
  This study assesses four cutting-edge language models in the underexplored
Aminoacian language. Through evaluation, it scrutinizes their adaptability,
effectiveness, and limitations in text generation, semantic coherence, and
contextual understanding. Uncovering insights into these models' performance in
a low-resourced language, this research pioneers pathways to bridge linguistic
gaps. By offering benchmarks and understanding challenges, it lays groundwork
for future advancements in natural language processing, aiming to elevate the
applicability of language models in similar linguistic landscapes, marking a
significant step toward inclusivity and progress in language technology.
\\ ( https://arxiv.org/abs/2402.18121 ,  160kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18139
Date: Wed, 28 Feb 2024 08:02:14 GMT   (7919kb,D)

Title: Cause and Effect: Can Large Language Models Truly Understand Causality?
Authors: Swagata Ashwani, Kshiteesh Hegde, Nishith Reddy Mannuru, Mayank
  Jindal, Dushyant Singh Sengar, Krishna Chaitanya Rao Kathala, Dishant Banga,
  Vinija Jain and Aman Chadha
Categories: cs.CL cs.AI
\\
  With the rise of Large Language Models(LLMs), it has become crucial to
understand their capabilities and limitations in deciphering and explaining the
complex web of causal relationships that language entails. Current methods use
either explicit or implicit causal reasoning, yet there is a strong need for a
unified approach combining both to tackle a wide array of causal relationships
more effectively. This research proposes a novel architecture called Context
Aware Reasoning Enhancement with Counterfactual Analysis(CARE CA) framework to
enhance causal reasoning and explainability. The proposed framework
incorporates an explicit causal detection module with ConceptNet and
counterfactual statements, as well as implicit causal detection through LLMs.
Our framework goes one step further with a layer of counterfactual explanations
to accentuate LLMs understanding of causality. The knowledge from ConceptNet
enhances the performance of multiple causal reasoning tasks such as causal
discovery, causal identification and counterfactual reasoning. The
counterfactual sentences add explicit knowledge of the not caused by scenarios.
By combining these powerful modules, our model aims to provide a deeper
understanding of causal relationships, enabling enhanced interpretability.
Evaluation of benchmark datasets shows improved performance across all metrics,
such as accuracy, precision, recall, and F1 scores. We also introduce
CausalNet, a new dataset accompanied by our code, to facilitate further
research in this domain.
\\ ( https://arxiv.org/abs/2402.18139 ,  7919kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18145
Date: Wed, 28 Feb 2024 08:11:05 GMT   (3239kb,D)

Title: Learning Intrinsic Dimension via Information Bottleneck for Explainable
  Aspect-based Sentiment Analysis
Authors: Zhenxiao Cheng, Jie Zhou, Wen Wu, Qin Chen, Liang He
Categories: cs.CL
Comments: Accepted by COLING 2024
\\
  Gradient-based explanation methods are increasingly used to interpret neural
models in natural language processing (NLP) due to their high fidelity. Such
methods determine word-level importance using dimension-level gradient values
through a norm function, often presuming equal significance for all gradient
dimensions. However, in the context of Aspect-based Sentiment Analysis (ABSA),
our preliminary research suggests that only specific dimensions are pertinent.
To address this, we propose the Information Bottleneck-based Gradient
(\texttt{IBG}) explanation framework for ABSA. This framework leverages an
information bottleneck to refine word embeddings into a concise intrinsic
dimension, maintaining essential features and omitting unrelated information.
Comprehensive tests show that our \texttt{IBG} approach considerably improves
both the models' performance and interpretability by identifying
sentiment-aware features.
\\ ( https://arxiv.org/abs/2402.18145 ,  3239kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18150
Date: Wed, 28 Feb 2024 08:24:38 GMT   (7210kb,D)

Title: Unsupervised Information Refinement Training of Large Language Models
  for Retrieval-Augmented Generation
Authors: Shicheng Xu, Liang Pang, Mo Yu, Fandong Meng, Huawei Shen, Xueqi
  Cheng, Jie Zhou
Categories: cs.CL cs.AI cs.IR
\\
  Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating additional information from retrieval. However, studies have
shown that LLMs still face challenges in effectively using the retrieved
information, even ignoring it or being misled by it. The key reason is that the
training of LLMs does not clearly make LLMs learn how to utilize input
retrieved texts with varied quality. In this paper, we propose a novel
perspective that considers the role of LLMs in RAG as ``Information Refiner'',
which means that regardless of correctness, completeness, or usefulness of
retrieved texts, LLMs can consistently integrate knowledge within the retrieved
texts and model parameters to generate the texts that are more concise,
accurate, and complete than the retrieved texts. To this end, we propose an
information refinement training method named InFO-RAG that optimizes LLMs for
RAG in an unsupervised manner. InFO-RAG is low-cost and general across various
tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse
tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue,
and Code Generation show that InFO-RAG improves the performance of LLaMA2 by an
average of 9.39\% relative points. InFO-RAG also shows advantages in in-context
learning and robustness of RAG.
\\ ( https://arxiv.org/abs/2402.18150 ,  7210kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18154
Date: Wed, 28 Feb 2024 08:34:41 GMT   (8403kb,D)

Title: Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and
  Mitigating Knowledge Conflicts in Language Models
Authors: Zhuoran Jin, Pengfei Cao, Hongbang Yuan, Yubo Chen, Jiexin Xu, Huaijun
  Li, Xiaojian Jiang, Kang Liu, Jun Zhao
Categories: cs.CL cs.AI cs.IR
Comments: 21 pages, 42 figures, 4 tables
\\
  Recently, retrieval augmentation and tool augmentation have demonstrated a
remarkable capability to expand the internal memory boundaries of language
models (LMs) by providing external context. However, internal memory and
external context inevitably clash, leading to knowledge conflicts within LMs.
In this paper, we aim to interpret the mechanism of knowledge conflicts through
the lens of information flow, and then mitigate conflicts by precise
interventions at the pivotal point. We find there are some attention heads with
opposite effects in the later layers, where memory heads can recall knowledge
from internal memory, and context heads can retrieve knowledge from external
context. Moreover, we reveal that the pivotal point at which knowledge
conflicts emerge in LMs is the integration of inconsistent information flows by
memory heads and context heads. Inspired by the insights, we propose a novel
method called Pruning Head via PatH PatcHing (PH3), which can efficiently
mitigate knowledge conflicts by pruning conflicting attention heads without
updating model parameters. PH3 can flexibly control eight LMs to use internal
memory ($\uparrow$ 44.0%) or external context ($\uparrow$ 38.5%). Moreover, PH3
can also improve the performance of LMs on open-domain QA tasks. We also
conduct extensive experiments to demonstrate the cross-model, cross-relation,
and cross-format generalization of our method.
\\ ( https://arxiv.org/abs/2402.18154 ,  8403kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18158
Date: Wed, 28 Feb 2024 08:43:05 GMT   (1007kb,D)

Title: Evaluating Quantized Large Language Models
Authors: Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi,
  Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang
Categories: cs.CL cs.AI
\\
  Post-training quantization (PTQ) has emerged as a promising technique to
reduce the cost of large language models (LLMs). Specifically, PTQ can
effectively mitigate memory consumption and reduce computational overhead in
LLMs. To meet the requirements of both high efficiency and performance across
diverse scenarios, a comprehensive evaluation of quantized LLMs is essential to
guide the selection of quantization methods. This paper presents a thorough
evaluation of these factors by evaluating the effect of PTQ on Weight,
Activation, and KV Cache on 11 model families, including OPT, LLaMA2, Falcon,
Bloomz, Mistral, ChatGLM, Vicuna, LongChat, StableLM, Gemma, and Mamba, with
parameters ranging from 125M to 180B. The evaluation encompasses five types of
tasks: basic NLP, emergent ability, trustworthiness, dialogue, and long-context
tasks. Moreover, we also evaluate the state-of-the-art (SOTA) quantization
methods to demonstrate their applicability. Based on the extensive experiments,
we systematically summarize the effect of quantization, provide recommendations
to apply quantization techniques, and point out future directions.
\\ ( https://arxiv.org/abs/2402.18158 ,  1007kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18169
Date: Wed, 28 Feb 2024 08:57:42 GMT   (5256kb)

Title: MIKO: Multimodal Intention Knowledge Distillation from Large Language
  Models for Social-Media Commonsense Discovery
Authors: Feihong Lu, Weiqi Wang, Yangyifei Luo, Ziqin Zhu, Qingyun Sun, Baixuan
  Xu, Haochen Shi, Shiqi Gao, Qian Li, Yangqiu Song, Jianxin Li
Categories: cs.CL
Comments: 11 pages, 5 figures
\\
  Social media has become a ubiquitous tool for connecting with others, staying
updated with news, expressing opinions, and finding entertainment. However,
understanding the intention behind social media posts remains challenging due
to the implicitness of intentions in social media posts, the need for
cross-modality understanding of both text and images, and the presence of noisy
information such as hashtags, misspelled words, and complicated abbreviations.
To address these challenges, we present MIKO, a Multimodal Intention Kowledge
DistillatiOn framework that collaboratively leverages a Large Language Model
(LLM) and a Multimodal Large Language Model (MLLM) to uncover users'
intentions. Specifically, we use an MLLM to interpret the image and an LLM to
extract key information from the text and finally instruct the LLM again to
generate intentions. By applying MIKO to publicly available social media
datasets, we construct an intention knowledge base featuring 1,372K intentions
rooted in 137,287 posts. We conduct a two-stage annotation to verify the
quality of the generated knowledge and benchmark the performance of widely used
LLMs for intention generation. We further apply MIKO to a sarcasm detection
dataset and distill a student model to demonstrate the downstream benefits of
applying intention knowledge.
\\ ( https://arxiv.org/abs/2402.18169 ,  5256kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18179
Date: Wed, 28 Feb 2024 09:10:25 GMT   (147kb,D)

Title: Challenges in Pre-Training Graph Neural Networks for Context-Based Fake
  News Detection: An Evaluation of Current Strategies and Resource Limitations
Authors: Gregor Donabauer and Udo Kruschwitz
Categories: cs.CL
Comments: Preprint accepted at LREC-COLING 2024
\\
  Pre-training of neural networks has recently revolutionized the field of
Natural Language Processing (NLP) and has before demonstrated its effectiveness
in computer vision. At the same time, advances around the detection of fake
news were mainly driven by the context-based paradigm, where different types of
signals (e.g. from social media) form graph-like structures that hold
contextual information apart from the news article to classify. We propose to
merge these two developments by applying pre-training of Graph Neural Networks
(GNNs) in the domain of context-based fake news detection. Our experiments
provide an evaluation of different pre-training strategies for graph-based
misinformation detection and demonstrate that transfer learning does currently
not lead to significant improvements over training a model from scratch in the
domain. We argue that a major current issue is the lack of suitable large-scale
resources that can be used for pre-training.
\\ ( https://arxiv.org/abs/2402.18179 ,  147kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18191
Date: Wed, 28 Feb 2024 09:27:29 GMT   (4017kb,D)

Title: Clustering and Ranking: Diversity-preserved Instruction Selection
  through Expert-aligned Quality Estimation
Authors: Yuan Ge, Yilun Liu, Chi Hu, Weibin Meng, Shimin Tao, Xiaofeng Zhao,
  Hongxia Ma, Li Zhang, Hao Yang, Tong Xiao
Categories: cs.CL
\\
  With contributions from the open-source community, a vast amount of
instruction tuning (IT) data has emerged. Given the significant resource
allocation required by training and evaluating models, it is advantageous to
have an efficient method for selecting high-quality IT data. However, existing
methods for instruction data selection have limitations such as relying on
fragile external APIs, being affected by biases in GPT models, or reducing the
diversity of the selected instruction dataset. In this paper, we propose an
industrial-friendly, expert-aligned and diversity-preserved instruction data
selection method: Clustering and Ranking (CaR). CaR consists of two steps. The
first step involves ranking instruction pairs using a scoring model that is
well aligned with expert preferences (achieving an accuracy of 84.25%). The
second step involves preserving dataset diversity through a clustering
process.In our experiment, CaR selected a subset containing only 1.96% of
Alpaca's IT data, yet the underlying AlpaCaR model trained on this subset
outperforms Alpaca by an average of 32.1% in GPT-4 evaluations. Furthermore,
our method utilizes small models (355M parameters) and requires only 11.2% of
the monetary cost compared to existing methods, making it easily deployable in
industrial scenarios.
\\ ( https://arxiv.org/abs/2402.18191 ,  4017kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18209
Date: Wed, 28 Feb 2024 10:01:00 GMT   (2697kb,D)

Title: DANSK and DaCy 2.6.0: Domain Generalization of Danish Named Entity
  Recognition
Authors: Kenneth Enevoldsen, Emil Trenckner Jessen, Rebekah Baglini
Categories: cs.CL
\\
  Named entity recognition is one of the cornerstones of Danish NLP, essential
for language technology applications within both industry and research.
However, Danish NER is inhibited by a lack of available datasets. As a
consequence, no current models are capable of fine-grained named entity
recognition, nor have they been evaluated for potential generalizability issues
across datasets and domains. To alleviate these limitations, this paper
introduces: 1) DANSK: a named entity dataset providing for high-granularity
tagging as well as within-domain evaluation of models across a diverse set of
domains; 2) DaCy 2.6.0 that includes three generalizable models with
fine-grained annotation; and 3) an evaluation of current state-of-the-art
models' ability to generalize across domains. The evaluation of existing and
new models revealed notable performance discrepancies across domains, which
should be addressed within the field. Shortcomings of the annotation quality of
the dataset and its impact on model training and evaluation are also discussed.
Despite these limitations, we advocate for the use of the new dataset DANSK
alongside further work on the generalizability within Danish NER.
\\ ( https://arxiv.org/abs/2402.18209 ,  2697kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18216
Date: Wed, 28 Feb 2024 10:19:05 GMT   (2558kb,D)

Title: LLM Task Interference: An Initial Study on the Impact of Task-Switch in
  Conversational History
Authors: Akash Gupta, Ivaxi Sheth, Vyas Raina, Mark Gales, Mario Fritz
Categories: cs.CL
Comments: 16 pages, 11 figures, 10 tables
\\
  With the recent emergence of powerful instruction-tuned large language models
(LLMs), various helpful conversational Artificial Intelligence (AI) systems
have been deployed across many applications. When prompted by users, these AI
systems successfully perform a wide range of tasks as part of a conversation.
To provide some sort of memory and context, such approaches typically condition
their output on the entire conversational history. Although this sensitivity to
the conversational history can often lead to improved performance on subsequent
tasks, we find that performance can in fact also be negatively impacted, if
there is a task-switch. To the best of our knowledge, our work makes the first
attempt to formalize the study of such vulnerabilities and interference of
tasks in conversational LLMs caused by task-switches in the conversational
history. Our experiments across 5 datasets with 15 task switches using popular
LLMs reveal that many of the task-switches can lead to significant performance
degradation.
\\ ( https://arxiv.org/abs/2402.18216 ,  2558kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18223
Date: Wed, 28 Feb 2024 10:38:21 GMT   (2354kb,D)

Title: Improving Open-Ended Text Generation via Adaptive Decoding
Authors: Wenhong Zhu, Hongkun Hao, Zhiwei He, Yiming Ai and Rui Wang
Categories: cs.CL
\\
  Current language models decode text token by token according to probabilistic
distribution, and determining the appropriate candidates for the next token is
crucial to ensure generation quality. This study introduces adaptive decoding,
a mechanism that empowers the language models to ascertain a sensible candidate
set during the generation process dynamically. Specifically, we introduce an
entropy-based metric called confidence and conceptualize determining the
optimal candidate set as a confidence-increasing process. The rationality of
including a token in the candidate set is assessed by leveraging the increment
of confidence, enabling the model to determine the most suitable candidate set
adaptively. The experimental results reveal that our method achieves higher
MAUVE and diversity in story generation tasks and maintains certain coherence,
underscoring its superiority over existing algorithms. The code is available at
https://github.com/zwhong714/adaptive_decoding.
\\ ( https://arxiv.org/abs/2402.18223 ,  2354kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18225
Date: Wed, 28 Feb 2024 10:43:54 GMT   (755kb,D)

Title: CogBench: a large language model walks into a psychology lab
Authors: Julian Coda-Forno, Marcel Binz, Jane X. Wang and Eric Schulz
Categories: cs.CL cs.AI cs.LG
\\
  Large language models (LLMs) have significantly advanced the field of
artificial intelligence. Yet, evaluating them comprehensively remains
challenging. We argue that this is partly due to the predominant focus on
performance metrics in most benchmarks. This paper introduces CogBench, a
benchmark that includes ten behavioral metrics derived from seven cognitive
psychology experiments. This novel approach offers a toolkit for phenotyping
LLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse
dataset. We analyze this data using statistical multilevel modeling techniques,
accounting for the nested dependencies among fine-tuned versions of specific
LLMs. Our study highlights the crucial role of model size and reinforcement
learning from human feedback (RLHF) in improving performance and aligning with
human behavior. Interestingly, we find that open-source models are less
risk-prone than proprietary models and that fine-tuning on code does not
necessarily enhance LLMs' behavior. Finally, we explore the effects of
prompt-engineering techniques. We discover that chain-of-thought prompting
improves probabilistic reasoning, while take-a-step-back prompting fosters
model-based behaviors.
\\ ( https://arxiv.org/abs/2402.18225 ,  755kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18243
Date: Wed, 28 Feb 2024 11:16:00 GMT   (483kb,D)

Title: Learning or Self-aligning? Rethinking Instruction Fine-tuning
Authors: Mengjie Ren, Boxi Cao, Hongyu Lin, Liu Cao, Xianpei Han, Ke Zeng,
  Guanglu Wan, Xunliang Cai, Le Sun
Categories: cs.CL
\\
  Instruction Fine-tuning~(IFT) is a critical phase in building large language
models~(LLMs). Previous works mainly focus on the IFT's role in the transfer of
behavioral norms and the learning of additional world knowledge. However, the
understanding of the underlying mechanisms of IFT remains significantly
limited. In this paper, we design a knowledge intervention framework to
decouple the potential underlying factors of IFT, thereby enabling individual
analysis of different factors. Surprisingly, our experiments reveal that
attempting to learn additional world knowledge through IFT often struggles to
yield positive impacts and can even lead to markedly negative effects. Further,
we discover that maintaining internal knowledge consistency before and after
IFT is a critical factor for achieving successful IFT. Our findings reveal the
underlying mechanisms of IFT and provide robust support for some very recent
and potential future works.
\\ ( https://arxiv.org/abs/2402.18243 ,  483kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18252
Date: Wed, 28 Feb 2024 11:29:09 GMT   (507kb,D)

Title: Towards Generalist Prompting for Large Language Models by Mental Models
Authors: Haoxiang Guan, Jiyan He, Shuxin Zheng, En-Hong Chen, Weiming Zhang,
  Nenghai Yu
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) have demonstrated impressive performance on many
tasks. However, to achieve optimal performance, specially designed prompting
methods are still needed. These methods either rely on task-specific few-shot
examples that require a certain level of domain knowledge, or are designed to
be simple but only perform well on a few types of tasks. In this work, we
attempt to introduce the concept of generalist prompting, which operates on the
design principle of achieving optimal or near-optimal performance on a wide
range of tasks while eliminating the need for manual selection and
customization of prompts tailored to specific problems. Furthermore, we propose
MeMo (Mental Models), an innovative prompting method that is simple-designed
yet effectively fulfills the criteria of generalist prompting. MeMo distills
the cores of various prompting methods into individual mental models and allows
LLMs to autonomously select the most suitable mental models for the problem,
achieving or being near to the state-of-the-art results on diverse tasks such
as STEM, logical reasoning, and commonsense reasoning in zero-shot settings. We
hope that the insights presented herein will stimulate further exploration of
generalist prompting methods for LLMs.
\\ ( https://arxiv.org/abs/2402.18252 ,  507kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18258
Date: Wed, 28 Feb 2024 11:39:26 GMT   (9478kb,D)

Title: A BiRGAT Model for Multi-intent Spoken Language Understanding with
  Hierarchical Semantic Frames
Authors: Hongshen Xu, Ruisheng Cao, Su Zhu, Sheng Jiang, Hanchong Zhang, Lu
  Chen and Kai Yu
Categories: cs.CL
\\
  Previous work on spoken language understanding (SLU) mainly focuses on
single-intent settings, where each input utterance merely contains one user
intent. This configuration significantly limits the surface form of user
utterances and the capacity of output semantics. In this work, we first propose
a Multi-Intent dataset which is collected from a realistic in-Vehicle dialogue
System, called MIVS. The target semantic frame is organized in a 3-layer
hierarchical structure to tackle the alignment and assignment problems in
multi-intent cases. Accordingly, we devise a BiRGAT model to encode the
hierarchy of ontology items, the backbone of which is a dual relational graph
attention network. Coupled with the 3-way pointer-generator decoder, our method
outperforms traditional sequence labeling and classification-based schemes by a
large margin.
\\ ( https://arxiv.org/abs/2402.18258 ,  9478kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18262
Date: Wed, 28 Feb 2024 11:50:36 GMT   (7572kb,D)

Title: Hierarchical Multimodal Pre-training for Visually Rich Webpage
  Understanding
Authors: Hongshen Xu, Lu Chen, Zihan Zhao, Da Ma, Ruisheng Cao, Zichen Zhu and
  Kai Yu
Categories: cs.CL cs.CV
\\
  The growing prevalence of visually rich documents, such as webpages and
scanned/digital-born documents (images, PDFs, etc.), has led to increased
interest in automatic document understanding and information extraction across
academia and industry. Although various document modalities, including image,
text, layout, and structure, facilitate human information retrieval, the
interconnected nature of these modalities presents challenges for neural
networks. In this paper, we introduce WebLM, a multimodal pre-training network
designed to address the limitations of solely modeling text and structure
modalities of HTML in webpages. Instead of processing document images as
unified natural images, WebLM integrates the hierarchical structure of document
images to enhance the understanding of markup-language-based documents.
Additionally, we propose several pre-training tasks to model the interaction
among text, structure, and image modalities effectively. Empirical results
demonstrate that the pre-trained WebLM significantly surpasses previous
state-of-the-art pre-trained models across several webpage understanding tasks.
The pre-trained models and code are available at
https://github.com/X-LANCE/weblm.
\\ ( https://arxiv.org/abs/2402.18262 ,  7572kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18264
Date: Wed, 28 Feb 2024 11:51:56 GMT   (9517kb,D)

Title: Retrieval-based Full-length Wikipedia Generation for Emergent Events
Authors: Jiebin Zhang and Eugene J. Yu and Qinyu Chen and Chenhao Xiong and
  Dawei Zhu and Han Qian and Mingbo Song and Xiaoguang Li and Qun Liu and
  Sujian Li
Categories: cs.CL
\\
  In today's fast-paced world, the growing demand to quickly generate
comprehensive and accurate Wikipedia documents for emerging events is both
crucial and challenging. However, previous efforts in Wikipedia generation have
often fallen short of meeting real-world requirements. Some approaches focus
solely on generating segments of a complete Wikipedia document, while others
overlook the importance of faithfulness in generation or fail to consider the
influence of the pre-training corpus. In this paper, we simulate a real-world
scenario where structured full-length Wikipedia documents are generated for
emergent events using input retrieved from web sources. To ensure that Large
Language Models (LLMs) are not trained on corpora related to recently occurred
events, we select events that have taken place recently and introduce a new
benchmark Wiki-GenBen, which consists of 309 events paired with their
corresponding retrieved web pages for generating evidence. Additionally, we
design a comprehensive set of systematic evaluation metrics and baseline
methods, to evaluate the capability of LLMs in generating factual full-length
Wikipedia documents. The data and code are open-sourced at WikiGenBench.
\\ ( https://arxiv.org/abs/2402.18264 ,  9517kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18267
Date: Wed, 28 Feb 2024 11:57:12 GMT   (41kb)

Title: A Survey on Neural Question Generation: Methods, Applications, and
  Prospects
Authors: Shasha Guo, Lizi Liao, Cuiping Li, Tat-Seng Chua
Categories: cs.CL cs.AI
\\
  In this survey, we present a detailed examination of the advancements in
Neural Question Generation (NQG), a field leveraging neural network techniques
to generate relevant questions from diverse inputs like knowledge bases, texts,
and images. The survey begins with an overview of NQG's background,
encompassing the task's problem formulation, prevalent benchmark datasets,
established evaluation metrics, and notable applications. It then methodically
classifies NQG approaches into three predominant categories: structured NQG,
which utilizes organized data sources, unstructured NQG, focusing on more
loosely structured inputs like texts or visual content, and hybrid NQG, drawing
on diverse input modalities. This classification is followed by an in-depth
analysis of the distinct neural network models tailored for each category,
discussing their inherent strengths and potential limitations. The survey
culminates with a forward-looking perspective on the trajectory of NQG,
identifying emergent research trends and prospective developmental paths.
Accompanying this survey is a curated collection of related research papers,
datasets and codes, systematically organized on Github, providing an extensive
reference for those delving into NQG.
\\ ( https://arxiv.org/abs/2402.18267 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18272
Date: Wed, 28 Feb 2024 12:04:05 GMT   (2538kb,D)

Title: Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the
  Key?
Authors: Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, Yangqiu Song
Categories: cs.CL cs.AI
Comments: 22 pages, 5 figures, 10 tables
\\
  Recent progress in LLMs discussion suggests that multi-agent discussion
improves the reasoning abilities of LLMs. In this work, we reevaluate this
claim through systematic experiments, where we propose a novel group discussion
framework to enrich the set of discussion mechanisms. Interestingly, our
results show that a single-agent LLM with strong prompts can achieve almost the
same performance as the best existing discussion approach on a wide range of
reasoning tasks and backbone LLMs. We observe that the multi-agent discussion
performs better than a single agent only when there is no demonstration in the
prompt. Further study reveals the common interaction mechanisms of LLMs during
the discussion.
\\ ( https://arxiv.org/abs/2402.18272 ,  2538kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18281
Date: Wed, 28 Feb 2024 12:17:40 GMT   (13400kb,D)

Title: Towards Better Understanding of Contrastive Sentence Representation
  Learning: A Unified Paradigm for Gradient
Authors: Mingxin Li, Richong Zhang, Zhijie Nie
Categories: cs.CL
Comments: work in progress
\\
  Sentence Representation Learning (SRL) is a crucial task in Natural Language
Processing (NLP), where contrastive Self-Supervised Learning (SSL) is currently
a mainstream approach. However, the reasons behind its remarkable effectiveness
remain unclear. Specifically, in other research fields, contrastive SSL shares
similarities in both theory and practical performance with non-contrastive SSL
(e.g., alignment & uniformity, Barlow Twins, and VICReg). However, in SRL,
contrastive SSL outperforms non-contrastive SSL significantly. Therefore, two
questions arise: First, what commonalities enable various contrastive losses to
achieve superior performance in SRL? Second, how can we make non-contrastive
SSL, which is similar to contrastive SSL but ineffective in SRL, effective? To
address these questions, we start from the perspective of gradients and
discover that four effective contrastive losses can be integrated into a
unified paradigm, which depends on three components: the Gradient Dissipation,
the Weight, and the Ratio. Then, we conduct an in-depth analysis of the roles
these components play in optimization and experimentally demonstrate their
significance for model performance. Finally, by adjusting these components, we
enable non-contrastive SSL to achieve outstanding performance in SRL.
\\ ( https://arxiv.org/abs/2402.18281 ,  13400kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18284
Date: Wed, 28 Feb 2024 12:24:07 GMT   (6824kb,D)

Title: Is Crowdsourcing Breaking Your Bank? Cost-Effective Fine-Tuning of
  Pre-trained Language Models with Proximal Policy Optimization
Authors: Shuo Yang and Gjergji Kasneci
Categories: cs.CL cs.AI
Comments: 12 pages, 2 figures
\\
  Wide usage of ChatGPT has highlighted the potential of reinforcement learning
from human feedback. However, its training pipeline relies on manual ranking, a
resource-intensive process. To reduce labor costs, we propose a self-supervised
text ranking approach for applying Proximal-Policy-Optimization to fine-tune
language models while eliminating the need for human annotators. Our method
begins with probabilistic sampling to encourage a language model to generate
diverse responses for each input. We then employ TextRank and ISODATA
algorithms to rank and cluster these responses based on their semantics.
Subsequently, we construct a reward model to learn the rank and optimize our
generative policy. Our experimental results, conducted using two language
models on three tasks, demonstrate that the models trained by our method
considerably outperform baselines regarding BLEU, GLEU, and METEOR scores.
Furthermore, our manual evaluation shows that our ranking results exhibit a
remarkably high consistency with that of humans. This research significantly
reduces training costs of proximal policy-guided models and demonstrates the
potential for self-correction of language models.
\\ ( https://arxiv.org/abs/2402.18284 ,  6824kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18312
Date: Wed, 28 Feb 2024 13:14:20 GMT   (8126kb,D)

Title: How to think step-by-step: A mechanistic understanding of
  chain-of-thought reasoning
Authors: Subhabrata Dutta, Joykirat Singh, Soumen Chakrabarti, Tanmoy
  Chakraborty
Categories: cs.CL cs.LG
\\
  Despite superior reasoning prowess demonstrated by Large Language Models
(LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails
around the internal mechanisms of the models that facilitate CoT generation.
This work investigates the neural sub-structures within LLMs that manifest CoT
reasoning from a mechanistic point of view. From an analysis of LLaMA-2 7B
applied to multistep reasoning over fictional ontologies, we demonstrate that
LLMs deploy multiple parallel pathways of answer generation for step-by-step
reasoning. These parallel pathways provide sequential answers from the input
question context as well as the generated CoT. We observe a striking functional
rift in the middle layers of the LLM. Token representations in the initial half
remain strongly biased towards the pretraining prior, with the in-context
taking over abruptly in the later half. This internal phase shift manifests in
different functional components: attention heads that write the answer token
predominantly appear in the later half, attention heads that move information
along ontological relationships appear exclusively in the initial half, and so
on. To the best of our knowledge, this is the first attempt towards mechanistic
investigation of CoT reasoning in LLMs.
\\ ( https://arxiv.org/abs/2402.18312 ,  8126kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18334
Date: Wed, 28 Feb 2024 13:54:57 GMT   (342kb,D)

Title: Learning to Generate Instruction Tuning Datasets for Zero-Shot Task
  Adaptation
Authors: Nihal V. Nayak, Yiyang Nan, Avi Trost, Stephen H. Bach
Categories: cs.CL cs.LG
\\
  We introduce Bonito, an open-source model for conditional task generation:
the task of converting unannotated text into task-specific training datasets
for instruction tuning. Our goal is to enable zero-shot task adaptation of
large language models on users' specialized, private data. We train Bonito on a
new large-scale dataset with 1.65M examples created by remixing existing
instruction tuning datasets into meta-templates. The meta-templates for a
dataset produce training examples where the input is the unannotated text and
the task attribute and the output consists of the instruction and the response.
We use Bonito to generate synthetic tasks for seven datasets from specialized
domains across three task types -- yes-no question answering, extractive
question answering, and natural language inference -- and adapt language
models. We show that Bonito significantly improves the average performance of
pretrained and instruction tuned models over the de facto self supervised
baseline. For example, adapting Mistral-Instruct-v2 and instruction tuned
variants of Mistral and Llama2 with Bonito improves the strong zero-shot
performance by 22.1 F1 points whereas the next word prediction objective undoes
some of the benefits of instruction tuning and reduces the average performance
by 0.8 F1 points. We conduct additional experiments with Bonito to understand
the effects of the domain, the size of the training set, and the choice of
alternative synthetic task generators. Overall, we show that learning with
synthetic instruction tuning datasets is an effective way to adapt language
models to new domains. The model, dataset, and code are available at
https://github.com/BatsResearch/bonito.
\\ ( https://arxiv.org/abs/2402.18334 ,  342kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18344
Date: Wed, 28 Feb 2024 14:09:02 GMT   (746kb,D)

Title: Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems
  in Commonsense Reasoning
Authors: Jiachun Li, Pengfei Cao, Chenhao Wang, Zhuoran Jin, Yubo Chen, Daojian
  Zeng, Kang Liu, Jun Zhao
Categories: cs.CL
\\
  Large language models exhibit high-level commonsense reasoning abilities,
especially with enhancement methods like Chain-of-Thought (CoT). However, we
find these CoT-like methods lead to a considerable number of originally correct
answers turning wrong, which we define as the Toxic CoT problem. To interpret
and mitigate this problem, we first utilize attribution tracing and causal
tracing methods to probe the internal working mechanism of the LLM during CoT
reasoning. Through comparisons, we prove that the model exhibits information
loss from the question over the shallow attention layers when generating
rationales or answers. Based on the probing findings, we design a novel method
called RIDERS (Residual decodIng and sERial-position Swap), which compensates
for the information deficit in the model from both decoding and serial-position
perspectives. Through extensive experiments on multiple commonsense reasoning
benchmarks, we validate that this method not only significantly eliminates
Toxic CoT problems (decreased by 23.6%), but also effectively improves the
model's overall commonsense reasoning performance (increased by 5.5%).
\\ ( https://arxiv.org/abs/2402.18344 ,  746kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18374
Date: Wed, 28 Feb 2024 14:49:05 GMT   (1511kb,D)

Title: VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning
  with Large Language Models
Authors: Seoyeon Kim, Kwangwook Seo, Hyungjoo Chae, Jinyoung Yeo, Dongha Lee
Categories: cs.CL
Comments: 19 pages, 9 figures
\\
  Recent approaches in domain-specific named entity recognition (NER), such as
biomedical NER, have shown remarkable advances. However, they still lack of
faithfulness, producing erroneous predictions. We assume that knowledge of
entities can be useful in verifying the correctness of the predictions. Despite
the usefulness of knowledge, resolving such errors with knowledge is
nontrivial, since the knowledge itself does not directly indicate the
ground-truth label. To this end, we propose VerifiNER, a post-hoc verification
framework that identifies errors from existing NER methods using knowledge and
revises them into more faithful predictions. Our framework leverages the
reasoning abilities of large language models to adequately ground on knowledge
and the contextual information in the verification process. We validate
effectiveness of VerifiNER through extensive experiments on biomedical
datasets. The results suggest that VerifiNER can successfully verify errors
from existing models as a model-agnostic approach. Further analyses on
out-of-domain and low-resource settings show the usefulness of VerifiNER on
real-world applications.
\\ ( https://arxiv.org/abs/2402.18374 ,  1511kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18376
Date: Wed, 28 Feb 2024 14:52:15 GMT   (8052kb,D)

Title: Tokenization Is More Than Compression
Authors: Craig W. Schmidt, Varshini Reddy, Haoran Zhang, Alec Alameddine, Omri
  Uzan, Yuval Pinter, Chris Tanner
Categories: cs.CL cs.AI
MSC-class: 68T50
ACM-class: I.2.7
\\
  Tokenization is a foundational step in Natural Language Processing (NLP)
tasks, bridging raw text and language models. Existing tokenization approaches
like Byte-Pair Encoding (BPE) originate from the field of data compression, and
it has been suggested that the effectiveness of BPE stems from its ability to
condense text into a relatively small number of tokens. We test the hypothesis
that fewer tokens lead to better downstream performance by introducing
PathPiece, a new tokenizer that segments a document's text into the minimum
number of tokens for a given vocabulary. Through extensive experimentation we
find this hypothesis not to be the case, casting doubt on the understanding of
the reasons for effective tokenization. To examine which other factors play a
role, we evaluate design decisions across all three phases of tokenization:
pre-tokenization, vocabulary construction, and segmentation, offering new
insights into the design of effective tokenizers. Specifically, we illustrate
the importance of pre-tokenization and the benefits of using BPE to initialize
vocabulary construction. We train 64 language models with varying tokenization,
ranging in size from 350M to 2.4B parameters, all of which are made publicly
available.
\\ ( https://arxiv.org/abs/2402.18376 ,  8052kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18385
Date: Wed, 28 Feb 2024 15:05:43 GMT   (699kb,D)

Title: The First Place Solution of WSDM Cup 2024: Leveraging Large Language
  Models for Conversational Multi-Doc QA
Authors: Yiming Li and Zhao Zhang
Categories: cs.CL cs.AI
Comments: 1st solution for WSDM Cup 2024
\\
  Conversational multi-doc question answering aims to answer specific questions
based on the retrieved documents as well as the contextual conversations. In
this paper, we introduce our winning approach for the "Conversational Multi-Doc
QA" challenge in WSDM Cup 2024, which exploits the superior natural language
understanding and generation capability of Large Language Models (LLMs). We
first adapt LLMs to the task, then devise a hybrid training strategy to make
the most of in-domain unlabeled data. Moreover, an advanced text embedding
model is adopted to filter out potentially irrelevant documents and several
approaches are designed and compared for the model ensemble. Equipped with all
these techniques, our solution finally ranked 1st place in WSDM Cup 2024,
surpassing its rivals to a large extent. The source codes have been released at
https://github.com/zhangzhao219/WSDM-Cup-2024.
\\ ( https://arxiv.org/abs/2402.18385 ,  699kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18397
Date: Wed, 28 Feb 2024 15:15:39 GMT   (8271kb,D)

Title: Decomposed Prompting: Unveiling Multilingual Linguistic Structure
  Knowledge in English-Centric Large Language Models
Authors: Ercong Nie, Shuzhou Yuan, Bolei Ma, Helmut Schmid, Michael F\"arber,
  Frauke Kreuter, Hinrich Sch\"utze
Categories: cs.CL
Comments: 18 pages, 7 figures
\\
  Despite the predominance of English in their training data, English-centric
Large Language Models (LLMs) like GPT-3 and LLaMA display a remarkable ability
to perform multilingual tasks, raising questions about the depth and nature of
their cross-lingual capabilities. This paper introduces the decomposed
prompting approach to probe the linguistic structure understanding of these
LLMs in sequence labeling tasks. Diverging from the single text-to-text prompt,
our method generates for each token of the input sentence an individual prompt
which asks for its linguistic label. We assess our method on the Universal
Dependencies part-of-speech tagging dataset for 38 languages, utilizing both
English-centric and multilingual LLMs. Our findings show that decomposed
prompting surpasses the iterative prompting baseline in efficacy and efficiency
under zero- and few-shot settings. Further analysis reveals the influence of
evaluation methods and the use of instructions in prompts. Our multilingual
investigation shows that English-centric language models perform better on
average than multilingual models. Our study offers insights into the
multilingual transferability of English-centric LLMs, contributing to the
understanding of their multilingual linguistic knowledge.
\\ ( https://arxiv.org/abs/2402.18397 ,  8271kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18419
Date: Wed, 28 Feb 2024 15:39:53 GMT   (15kb)

Title: Can GPT Improve the State of Prior Authorization via Guideline Based
  Automated Question Answering?
Authors: Shubham Vatsal, Ayush Singh and Shabnam Tafreshi
Categories: cs.CL cs.AI cs.LG
\\
  Health insurance companies have a defined process called prior authorization
(PA) which is a health plan cost-control process that requires doctors and
other healthcare professionals to get clearance in advance from a health plan
before performing a particular procedure on a patient in order to be eligible
for payment coverage. For health insurance companies, approving PA requests for
patients in the medical domain is a time-consuming and challenging task. One of
those key challenges is validating if a request matches up to certain criteria
such as age, gender, etc. In this work, we evaluate whether GPT can validate
numerous key factors, in turn helping health plans reach a decision drastically
faster. We frame it as a question answering task, prompting GPT to answer a
question from patient electronic health record. We experiment with different
conventional prompting techniques as well as introduce our own novel prompting
technique. Moreover, we report qualitative assessment by humans on the natural
language generation outputs from our approach. Results show that our method
achieves superior performance with the mean weighted F1 score of 0.61 as
compared to its standard counterparts.
\\ ( https://arxiv.org/abs/2402.18419 ,  15kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18424
Date: Wed, 28 Feb 2024 15:46:09 GMT   (1166kb,D)

Title: Emotion Classification in Low and Moderate Resource Languages
Authors: Shabnam Tafreshi, Shubham Vatsal, Mona Diab
Categories: cs.CL cs.AI cs.LG
\\
  It is important to be able to analyze the emotional state of people around
the globe. There are 7100+ active languages spoken around the world and
building emotion classification for each language is labor intensive.
Particularly for low-resource and endangered languages, building emotion
classification can be quite challenging. We present a cross-lingual emotion
classifier, where we train an emotion classifier with resource-rich languages
(i.e. \textit{English} in our work) and transfer the learning to low and
moderate resource languages. We compare and contrast two approaches of transfer
learning from a high-resource language to a low or moderate-resource language.
One approach projects the annotation from a high-resource language to low and
moderate-resource language in parallel corpora and the other one uses direct
transfer from high-resource language to the other languages. We show the
efficacy of our approaches on 6 languages: Farsi, Arabic, Spanish, Ilocano,
Odia, and Azerbaijani. Our results indicate that our approaches outperform
random baselines and transfer emotions across languages successfully. For all
languages, the direct cross-lingual transfer of emotion yields better results.
We also create annotated emotion-labeled resources for four languages: Farsi,
Azerbaijani, Ilocano and Odia.
\\ ( https://arxiv.org/abs/2402.18424 ,  1166kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18428
Date: Wed, 28 Feb 2024 15:55:02 GMT   (9890kb,D)

Title: Leveraging Diverse Modeling Contexts with Collaborating Learning for
  Neural Machine Translation
Authors: Yusheng Liao and Yanfeng Wang and Yu Wang
Categories: cs.CL
Comments: 12 pages, 6 figures
\\
  Autoregressive (AR) and Non-autoregressive (NAR) models are two types of
generative models for Neural Machine Translation (NMT). AR models predict
tokens in a word-by-word manner and can effectively capture the distribution of
real translations. NAR models predict tokens by extracting bidirectional
contextual information which can improve the inference speed but they suffer
from performance degradation. Previous works utilized AR models to enhance NAR
models by reducing the training data's complexity or incorporating the global
information into AR models by virtue of NAR models. However, those investigated
methods only take advantage of the contextual information of a single type of
model while neglecting the diversity in the contextual information that can be
provided by different types of models. In this paper, we propose a novel
generic collaborative learning method, DCMCL, where AR and NAR models are
treated as collaborators instead of teachers and students. To hierarchically
leverage the bilateral contextual information, token-level mutual learning and
sequence-level contrastive learning are adopted between AR and NAR models.
Extensive experiments on four widely used benchmarks show that the proposed
DCMCL method can simultaneously improve both AR and NAR models with up to 1.38
and 2.98 BLEU scores respectively, and can also outperform the current
best-unified model with up to 0.97 BLEU scores for both AR and NAR decoding.
\\ ( https://arxiv.org/abs/2402.18428 ,  9890kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18439
Date: Wed, 28 Feb 2024 16:07:54 GMT   (1936kb,D)

Title: Beyond Natural Language: LLMs Leveraging Alternative Formats for
  Enhanced Reasoning and Communication
Authors: Weize Chen, Chenfei Yuan, Jiarui Yuan, Yusheng Su, Chen Qian, Cheng
  Yang, Ruobing Xie, Zhiyuan Liu, Maosong Sun
Categories: cs.CL cs.AI
\\
  Natural language (NL) has long been the predominant format for human
cognition and communication, and by extension, has been similarly pivotal in
the development and application of Large Language Models (LLMs). Yet, besides
NL, LLMs have seen various non-NL formats during pre-training, such as code and
logical expression. NL's status as the optimal format for LLMs, particularly in
single-LLM reasoning and multi-agent communication, has not been thoroughly
examined. In this work, we challenge the default use of NL by exploring the
utility of non-NL formats in these contexts. We show that allowing LLMs to
autonomously select the most suitable format before reasoning or communicating
leads to a 3.3 to 5.7\% improvement in reasoning efficiency for different LLMs,
and up to a 72.7\% reduction in token usage in multi-agent communication, all
while maintaining communicative effectiveness. Our comprehensive analysis
further reveals that LLMs can devise a format from limited task instructions
and that the devised format is effectively transferable across different LLMs.
Intriguingly, the structured communication format decided by LLMs exhibits
notable parallels with established agent communication languages, suggesting a
natural evolution towards efficient, structured communication in agent
communication. Our code is released at
\url{https://github.com/thunlp/AutoForm}.
\\ ( https://arxiv.org/abs/2402.18439 ,  1936kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18449
Date: Wed, 28 Feb 2024 16:21:02 GMT   (1566kb,D)

Title: HOP to the Next Tasks and Domains for Continual Learning in NLP
Authors: Umberto Michieli, Mete Ozay
Categories: cs.CL cs.AI cs.LG
Comments: AAAI 2024. Main + supplmentary
\\
  Continual Learning (CL) aims to learn a sequence of problems (i.e., tasks and
domains) by transferring knowledge acquired on previous problems, whilst
avoiding forgetting of past ones. Different from previous approaches which
focused on CL for one NLP task or domain in a specific use-case, in this paper,
we address a more general CL setting to learn from a sequence of problems in a
unique framework. Our method, HOP, permits to hop across tasks and domains by
addressing the CL problem along three directions: (i) we employ a set of
adapters to generalize a large pre-trained model to unseen problems, (ii) we
compute high-order moments over the distribution of embedded representations to
distinguish independent and correlated statistics across different tasks and
domains, (iii) we process this enriched information with auxiliary heads
specialized for each end problem. Extensive experimental campaign on 4 NLP
applications, 5 benchmarks and 2 CL setups demonstrates the effectiveness of
our HOP.
\\ ( https://arxiv.org/abs/2402.18449 ,  1566kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18458
Date: Wed, 28 Feb 2024 16:35:52 GMT   (8127kb,D)

Title: Meta-Task Prompting Elicits Embedding from Large Language Models
Authors: Yibin Lei, Di Wu, Tianyi Zhou, Tao Shen, Yu Cao, Chongyang Tao, Andrew
  Yates
Categories: cs.CL
\\
  In this work, we introduce a new unsupervised embedding method, Meta-Task
Prompting with Explicit One-Word Limitation (MetaEOL), for generating
high-quality sentence embeddings from Large Language Models (LLMs) without the
need for model fine-tuning or task-specific engineering. Leveraging meta-task
prompting, MetaEOL guides LLMs to produce embeddings through a series of
carefully designed prompts that address multiple representational aspects. Our
comprehensive experiments demonstrate that embeddings averaged from various
meta-tasks yield competitive performance on Semantic Textual Similarity (STS)
benchmarks and excel in downstream tasks, surpassing contrastive-trained
models. Our findings suggest a new scaling law for embedding generation,
offering a versatile, resource-efficient approach for embedding extraction
across diverse sentence-centric scenarios.
\\ ( https://arxiv.org/abs/2402.18458 ,  8127kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18479
Date: Wed, 28 Feb 2024 16:59:35 GMT   (7725kb,D)

Title: NewsQs: Multi-Source Question Generation for the Inquiring Mind
Authors: Alyssa Hwang, Kalpit Dixit, Miguel Ballesteros, Yassine Benajiba,
  Vittorio Castelli, Markus Dreyer, Mohit Bansal, Kathleen McKeown
Categories: cs.CL
Comments: in submission
\\
  We present NewsQs (news-cues), a dataset that provides question-answer pairs
for multiple news documents. To create NewsQs, we augment a traditional
multi-document summarization dataset with questions automatically generated by
a T5-Large model fine-tuned on FAQ-style news articles from the News On the Web
corpus. We show that fine-tuning a model with control codes produces questions
that are judged acceptable more often than the same model without them as
measured through human evaluation. We use a QNLI model with high correlation
with human annotations to filter our data. We release our final dataset of
high-quality questions, answers, and document clusters as a resource for future
work in query-based multi-document summarization.
\\ ( https://arxiv.org/abs/2402.18479 ,  7725kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18502
Date: Wed, 28 Feb 2024 17:29:27 GMT   (1119kb,D)

Title: Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware
  Classification
Authors: Garima Chhikara, Anurag Sharma, Kripabandhu Ghosh, Abhijnan
  Chakraborty
Categories: cs.CL
Comments: Under review
\\
  Employing Large Language Models (LLM) in various downstream applications such
as classification is crucial, especially for smaller companies lacking the
expertise and resources required for fine-tuning a model. Fairness in LLMs
helps ensure inclusivity, equal representation based on factors such as race,
gender and promotes responsible AI deployment. As the use of LLMs has become
increasingly prevalent, it is essential to assess whether LLMs can generate
fair outcomes when subjected to considerations of fairness. In this study, we
introduce a framework outlining fairness regulations aligned with various
fairness definitions, with each definition being modulated by varying degrees
of abstraction. We explore the configuration for in-context learning and the
procedure for selecting in-context demonstrations using RAG, while
incorporating fairness rules into the process. Experiments conducted with
different LLMs indicate that GPT-4 delivers superior results in terms of both
accuracy and fairness compared to other models. This work is one of the early
attempts to achieve fairness in prediction tasks by utilizing LLMs through
in-context learning.
\\ ( https://arxiv.org/abs/2402.18502 ,  1119kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17802
Date: Tue, 27 Feb 2024 08:34:48 GMT   (600kb,D)

Title: Time Series Analysis in Compressor-Based Machines: A Survey
Authors: Francesca Forbicini, Nicol\`o Oreste Pinciroli Vago, Piero Fraternali
Categories: cs.LG
\\
  In both industrial and residential contexts, compressor-based machines, such
as refrigerators, HVAC systems, heat pumps and chillers, are essential to
fulfil production and consumers' needs. The diffusion of sensors and IoT
connectivity supports the development of monitoring systems able to detect and
predict faults, identify behavioural shifts and forecast the operational status
of machines and of their components. The focus of this paper is to survey the
recent research on such tasks as Fault Detection, Fault Prediction, Forecasting
and Change Point Detection applied to multivariate time series characterizing
the operations of compressor-based machines. Specifically, Fault Detection
detects and diagnoses faults, Fault Prediction predicts such occurrences,
forecasting anticipates the future value of characteristic variables of
machines and Change Point Detection identifies significant variations in the
behaviour of the appliances, such as a change in the working regime. We
identify and classify the approaches to the above-mentioned tasks, compare the
algorithms employed, highlight the gaps in the current status of the art and
discuss the most promising future research directions in the field.
\\ ( https://arxiv.org/abs/2402.17802 ,  600kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17804
Date: Tue, 27 Feb 2024 09:07:59 GMT   (146kb,D)

Title: Predicting machine failures from multivariate time series: an industrial
  case study
Authors: Nicol\`o Oreste Pinciroli Vago, Francesca Forbicini, Piero Fraternali
Categories: cs.LG
\\
  Non-neural Machine Learning (ML) and Deep Learning (DL) models are often used
to predict system failures in the context of industrial maintenance. However,
only a few researches jointly assess the effect of varying the amount of past
data used to make a prediction and the extension in the future of the forecast.
This study evaluates the impact of the size of the reading window and of the
prediction window on the performances of models trained to forecast failures in
three data sets concerning the operation of (1) an industrial wrapping machine
working in discrete sessions, (2) an industrial blood refrigerator working
continuously, and (3) a nitrogen generator working continuously. The problem is
formulated as a binary classification task that assigns the positive label to
the prediction window based on the probability of a failure to occur in such an
interval. Six algorithms (logistic regression, random forest, support vector
machine, LSTM, ConvLSTM, and Transformers) are compared using multivariate
telemetry time series. The results indicate that, in the considered scenarios,
the dimension of the prediction windows plays a crucial role and highlight the
effectiveness of DL approaches at classifying data with diverse time-dependent
patterns preceding a failure and the effectiveness of ML approaches at
classifying similar and repetitive patterns preceding a failure.
\\ ( https://arxiv.org/abs/2402.17804 ,  146kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17805
Date: Tue, 27 Feb 2024 11:04:06 GMT   (56kb)

Title: Graph Neural Networks and Arithmetic Circuits
Authors: Timon Barlag, Vivian Holzapfel, Laura Strieker, Jonni Virtema,
  Heribert Vollmer
Categories: cs.LG cs.AI cs.CC
ACM-class: F.1.1; F.1.3; I.2.m
\\
  We characterize the computational power of neural networks that follow the
graph neural network (GNN) architecture, not restricted to aggregate-combine
GNNs or other particular types. We establish an exact correspondence between
the expressivity of GNNs using diverse activation functions and arithmetic
circuits over real numbers. In our results the activation function of the
network becomes a gate type in the circuit. Our result holds for families of
constant depth circuits and networks, both uniformly and non-uniformly, for all
common activation functions.
\\ ( https://arxiv.org/abs/2402.17805 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17806
Date: Tue, 27 Feb 2024 11:27:32 GMT   (7032kb,D)

Title: Material Microstructure Design Using VAE-Regression with Multimodal
  Prior
Authors: Avadhut Sardeshmukh, Sreedhar Reddy, BP Gautham, Pushpak Bhattacharyya
Categories: cs.LG cond-mat.mtrl-sci stat.ML
Comments: 12 pages main paper, 9 pages appendix. 10 tables and 11 figures.
  Accepted for publication in PAKDD 2024
\\
  We propose a variational autoencoder (VAE)-based model for building forward
and inverse structure-property linkages, a problem of paramount importance in
computational materials science. Our model systematically combines VAE with
regression, linking the two models through a two-level prior conditioned on the
regression variables. The regression loss is optimized jointly with the
reconstruction loss of the variational autoencoder, learning microstructure
features relevant for property prediction and reconstruction. The resultant
model can be used for both forward and inverse prediction i.e., for predicting
the properties of a given microstructure as well as for predicting the
microstructure required to obtain given properties. Since the inverse problem
is ill-posed (one-to-many), we derive the objective function using a
multi-modal Gaussian mixture prior enabling the model to infer multiple
microstructures for a target set of properties. We show that for forward
prediction, our model is as accurate as state-of-the-art forward-only models.
Additionally, our method enables direct inverse inference. We show that the
microstructures inferred using our model achieve desired properties reasonably
accurately, avoiding the need for expensive optimization loops.
\\ ( https://arxiv.org/abs/2402.17806 ,  7032kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17812
Date: Tue, 27 Feb 2024 14:51:11 GMT   (3920kb,D)

Title: DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping
  Backward Propagation
Authors: Sunghyeon Woo, Baeseong Park, Byeongwook Kim, Minjung Jo, Sejung Kwon,
  Dongsuk Jeon, and Dongsoo Lee
Categories: cs.LG cs.CL
\\
  Training deep neural networks typically involves substantial computational
costs during both forward and backward propagation. The conventional layer
dropping techniques drop certain layers during training for reducing the
computations burden. However, dropping layers during forward propagation
adversely affects the training process by degrading accuracy. In this paper, we
propose Dropping Backward Propagation (DropBP), a novel approach designed to
reduce computational costs while maintaining accuracy. DropBP randomly drops
layers during the backward propagation, which does not deviate forward
propagation. Moreover, DropBP calculates the sensitivity of each layer to
assign appropriate drop rate, thereby stabilizing the training process. DropBP
is designed to enhance the efficiency of the training process with
backpropagation, thereby enabling the acceleration of both full fine-tuning and
parameter-efficient fine-tuning using backpropagation. Specifically, utilizing
DropBP in QLoRA reduces training time by 44%, increases the convergence speed
to the identical loss level by 1.5$\times$, and enables training with a
6.2$\times$ larger sequence length on a single NVIDIA-A100 80GiB GPU in
LLaMA2-70B. The code is available at https://github.com/WooSunghyeon/dropbp.
\\ ( https://arxiv.org/abs/2402.17812 ,  3920kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17826
Date: Tue, 27 Feb 2024 19:00:01 GMT   (20kb)

Title: Prediction-Powered Ranking of Large Language Models
Authors: Ivi Chatzi, Eleni Straitouri, Suhas Thejaswi, Manuel Gomez Rodriguez
Categories: cs.LG cs.AI cs.CL cs.CY cs.HC stat.ML
\\
  Large language models are often ranked according to their level of alignment
with human preferences -- a model is better than other models if its outputs
are more frequently preferred by humans. One of the most popular ways to elicit
human preferences utilizes pairwise comparisons between the outputs provided by
different models to the same inputs. However, since gathering pairwise
comparisons by humans is costly and time-consuming, it has become a very common
practice to gather pairwise comparisons by a strong large language model -- a
model strongly aligned with human preferences. Surprisingly, practitioners
cannot currently measure the uncertainty that any mismatch between human and
model preferences may introduce in the constructed rankings. In this work, we
develop a statistical framework to bridge this gap. Given a small set of
pairwise comparisons by humans and a large set of pairwise comparisons by a
model, our framework provides a rank-set -- a set of possible ranking positions
-- for each of the models under comparison. Moreover, it guarantees that, with
a probability greater than or equal to a user-specified value, the rank-sets
cover the true ranking consistent with (the distribution of) human pairwise
preferences. Our framework is computationally efficient, easy to use, and does
not make any assumption about the distribution of human preferences nor about
the degree of alignment between the pairwise comparisons by the humans and the
strong large language model.
\\ ( https://arxiv.org/abs/2402.17826 ,  20kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17853
Date: Tue, 27 Feb 2024 19:36:27 GMT   (4004kb,D)

Title: Latent Neural PDE Solver: a reduced-order modelling framework for
  partial differential equations
Authors: Zijie Li, Saurabh Patil, Francis Ogoke, Dule Shu, Wilson Zhen, Michael
  Schneier, John R. Buchanan, Jr., Amir Barati Farimani
Categories: cs.LG cs.AI math.AP
\\
  Neural networks have shown promising potential in accelerating the numerical
simulation of systems governed by partial differential equations (PDEs).
Different from many existing neural network surrogates operating on
high-dimensional discretized fields, we propose to learn the dynamics of the
system in the latent space with much coarser discretizations. In our proposed
framework - Latent Neural PDE Solver (LNS), a non-linear autoencoder is first
trained to project the full-order representation of the system onto the
mesh-reduced space, then a temporal model is trained to predict the future
state in this mesh-reduced space. This reduction process simplifies the
training of the temporal model by greatly reducing the computational cost
accompanying a fine discretization. We study the capability of the proposed
framework and several other popular neural PDE solvers on various types of
systems including single-phase and multi-phase flows along with varying system
parameters. We showcase that it has competitive accuracy and efficiency
compared to the neural PDE solver that operates on full-order space.
\\ ( https://arxiv.org/abs/2402.17853 ,  4004kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17879
Date: Tue, 27 Feb 2024 20:33:22 GMT   (1058kb,D)

Title: Automated Statistical Model Discovery with Language Models
Authors: Michael Y. Li, Emily B. Fox, Noah D. Goodman
Categories: cs.LG cs.CL
\\
  Statistical model discovery involves a challenging search over a vast space
of models subject to domain-specific modeling constraints. Efficiently
searching over this space requires human expertise in modeling and the problem
domain. Motivated by the domain knowledge and programming capabilities of large
language models (LMs), we introduce a method for language model driven
automated statistical model discovery. We cast our automated procedure within
the framework of Box's Loop: the LM iterates between proposing statistical
models represented as probabilistic programs, acting as a modeler, and
critiquing those models, acting as a domain expert. By leveraging LMs, we do
not have to define a domain-specific language of models or design a handcrafted
search procedure, key restrictions of previous systems. We evaluate our method
in three common settings in probabilistic modeling: searching within a
restricted space of models, searching over an open-ended space, and improving
classic models under natural language constraints (e.g., this model should be
interpretable to an ecologist). Our method matches the performance of previous
systems, identifies models on par with human expert designed models, and
extends classic models in interpretable ways. Our results highlight the promise
of LM driven model discovery.
\\ ( https://arxiv.org/abs/2402.17879 ,  1058kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17885
Date: Tue, 27 Feb 2024 20:57:35 GMT   (370kb,D)

Title: Independent Learning in Constrained Markov Potential Games
Authors: Philip Jordan, Anas Barakat, Niao He
Categories: cs.LG cs.GT cs.MA
Comments: AISTATS 2024
\\
  Constrained Markov games offer a formal mathematical framework for modeling
multi-agent reinforcement learning problems where the behavior of the agents is
subject to constraints. In this work, we focus on the recently introduced class
of constrained Markov Potential Games. While centralized algorithms have been
proposed for solving such constrained games, the design of converging
independent learning algorithms tailored for the constrained setting remains an
open question. We propose an independent policy gradient algorithm for learning
approximate constrained Nash equilibria: Each agent observes their own actions
and rewards, along with a shared state. Inspired by the optimization
literature, our algorithm performs proximal-point-like updates augmented with a
regularized constraint set. Each proximal step is solved inexactly using a
stochastic switching gradient algorithm. Notably, our algorithm can be
implemented independently without a centralized coordination mechanism
requiring turn-based agent updates. Under some technical constraint
qualification conditions, we establish convergence guarantees towards
constrained approximate Nash equilibria. We perform simulations to illustrate
our results.
\\ ( https://arxiv.org/abs/2402.17885 ,  370kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17888
Date: Tue, 27 Feb 2024 21:02:47 GMT   (2204kb,D)

Title: ConjNorm: Tractable Density Estimation for Out-of-Distribution Detection
Authors: Bo Peng, Yadan Luo, Yonggang Zhang, Yixuan Li, Zhen Fang
Categories: cs.LG cs.AI
Comments: ICLR24 poster
\\
  Post-hoc out-of-distribution (OOD) detection has garnered intensive attention
in reliable machine learning. Many efforts have been dedicated to deriving
score functions based on logits, distances, or rigorous data distribution
assumptions to identify low-scoring OOD samples. Nevertheless, these estimate
scores may fail to accurately reflect the true data density or impose
impractical constraints. To provide a unified perspective on density-based
score design, we propose a novel theoretical framework grounded in Bregman
divergence, which extends distribution considerations to encompass an
exponential family of distributions. Leveraging the conjugation constraint
revealed in our theorem, we introduce a \textsc{ConjNorm} method, reframing
density function design as a search for the optimal norm coefficient $p$
against the given dataset. In light of the computational challenges of
normalization, we devise an unbiased and analytically tractable estimator of
the partition function using the Monte Carlo-based importance sampling
technique. Extensive experiments across OOD detection benchmarks empirically
demonstrate that our proposed \textsc{ConjNorm} has established a new
state-of-the-art in a variety of OOD detection setups, outperforming the
current best method by up to 13.25$\%$ and 28.19$\%$ (FPR95) on CIFAR-100 and
ImageNet-1K, respectively.
\\ ( https://arxiv.org/abs/2402.17888 ,  2204kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17890
Date: Tue, 27 Feb 2024 21:06:42 GMT   (1607kb,D)

Title: From Inverse Optimization to Feasibility to ERM
Authors: Saurabh Mishra, Anant Raj, Sharan Vaswani
Categories: cs.LG math.OC
\\
  Inverse optimization involves inferring unknown parameters of an optimization
problem from known solutions, and is widely used in fields such as
transportation, power systems and healthcare. We study the contextual inverse
optimization setting that utilizes additional contextual information to better
predict the unknown problem parameters. We focus on contextual inverse linear
programming (CILP), addressing the challenges posed by the non-differentiable
nature of LPs. For a linear prediction model, we reduce CILP to a convex
feasibility problem allowing the use of standard algorithms such as alternating
projections. The resulting algorithm for CILP is equipped with a linear
convergence guarantee without additional assumptions such as degeneracy or
interpolation. Next, we reduce CILP to empirical risk minimization (ERM) on a
smooth, convex loss that satisfies the Polyak-Lojasiewicz condition. This
reduction enables the use of scalable first-order optimization methods to solve
large non-convex problems, while maintaining theoretical guarantees in the
convex setting. Finally, we experimentally validate our approach on both
synthetic and real-world problems, and demonstrate improved performance
compared to existing methods.
\\ ( https://arxiv.org/abs/2402.17890 ,  1607kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17902
Date: Tue, 27 Feb 2024 21:42:18 GMT   (198kb,D)

Title: SequentialAttention++ for Block Sparsification: Differentiable Pruning
  Meets Combinatorial Optimization
Authors: Taisuke Yasuda, Kyriakos Axiotis, Gang Fu, MohammadHossein Bateni,
  Vahab Mirrokni
Categories: cs.LG
\\
  Neural network pruning is a key technique towards engineering large yet
scalable, interpretable, and generalizable models. Prior work on the subject
has developed largely along two orthogonal directions: (1) differentiable
pruning for efficiently and accurately scoring the importance of parameters,
and (2) combinatorial optimization for efficiently searching over the space of
sparse models. We unite the two approaches, both theoretically and empirically,
to produce a coherent framework for structured neural network pruning in which
differentiable pruning guides combinatorial optimization algorithms to select
the most important sparse set of parameters. Theoretically, we show how many
existing differentiable pruning techniques can be understood as nonconvex
regularization for group sparse optimization, and prove that for a wide class
of nonconvex regularizers, the global optimum is unique, group-sparse, and
provably yields an approximate solution to a sparse convex optimization
problem. The resulting algorithm that we propose, SequentialAttention++,
advances the state of the art in large-scale neural network block-wise pruning
tasks on the ImageNet and Criteo datasets.
\\ ( https://arxiv.org/abs/2402.17902 ,  198kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17905
Date: Tue, 27 Feb 2024 21:43:14 GMT   (6600kb,D)

Title: Using Graph Neural Networks to Predict Local Culture
Authors: Thiago H Silva and Daniel Silver
Categories: cs.LG cs.CY cs.SI
Comments: 14 pages, 5 figures
\\
  Urban research has long recognized that neighbourhoods are dynamic and
relational. However, lack of data, methodologies, and computer processing power
have hampered a formal quantitative examination of neighbourhood relational
dynamics. To make progress on this issue, this study proposes a graph neural
network (GNN) approach that permits combining and evaluating multiple sources
of information about internal characteristics of neighbourhoods, their past
characteristics, and flows of groups among them, potentially providing greater
expressive power in predictive models. By exploring a public large-scale
dataset from Yelp, we show the potential of our approach for considering
structural connectedness in predicting neighbourhood attributes, specifically
to predict local culture. Results are promising from a substantive and
methodologically point of view. Substantively, we find that either local area
information (e.g. area demographics) or group profiles (tastes of Yelp
reviewers) give the best results in predicting local culture, and they are
nearly equivalent in all studied cases. Methodologically, exploring group
profiles could be a helpful alternative where finding local information for
specific areas is challenging, since they can be extracted automatically from
many forms of online data. Thus, our approach could empower researchers and
policy-makers to use a range of data sources when other local area information
is lacking.
\\ ( https://arxiv.org/abs/2402.17905 ,  6600kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17906
Date: Tue, 27 Feb 2024 21:47:06 GMT   (120kb,D)

Title: Representation learning in multiplex graphs: Where and how to fuse
  information?
Authors: Piotr Bielak, Tomasz Kajdanowicz
Categories: cs.LG cs.SI
\\
  In recent years, unsupervised and self-supervised graph representation
learning has gained popularity in the research community. However, most
proposed methods are focused on homogeneous networks, whereas real-world graphs
often contain multiple node and edge types. Multiplex graphs, a special type of
heterogeneous graphs, possess richer information, provide better modeling
capabilities and integrate more detailed data from potentially different
sources. The diverse edge types in multiplex graphs provide more context and
insights into the underlying processes of representation learning. In this
paper, we tackle the problem of learning representations for nodes in multiplex
networks in an unsupervised or self-supervised manner. To that end, we explore
diverse information fusion schemes performed at different levels of the graph
processing pipeline. The detailed analysis and experimental evaluation of
various scenarios inspired us to propose improvements in how to construct GNN
architectures that deal with multiplex graphs.
\\ ( https://arxiv.org/abs/2402.17906 ,  120kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17917
Date: Tue, 27 Feb 2024 22:10:51 GMT   (1204kb,D)

Title: Collaborative learning of common latent representations in routinely
  collected multivariate ICU physiological signals
Authors: Hollan Haule, Ian Piper, Patricia Jones, Tsz-Yan Milly Lo, Javier
  Escudero
Categories: cs.LG
\\
  In Intensive Care Units (ICU), the abundance of multivariate time series
presents an opportunity for machine learning (ML) to enhance patient
phenotyping. In contrast to previous research focused on electronic health
records (EHR), here we propose an ML approach for phenotyping using routinely
collected physiological time series data. Our new algorithm integrates Long
Short-Term Memory (LSTM) networks with collaborative filtering concepts to
identify common physiological states across patients. Tested on real-world ICU
clinical data for intracranial hypertension (IH) detection in patients with
brain injury, our method achieved an area under the curve (AUC) of 0.889 and
average precision (AP) of 0.725. Moreover, our algorithm outperforms
autoencoders in learning more structured latent representations of the
physiological signals. These findings highlight the promise of our methodology
for patient phenotyping, leveraging routinely collected multivariate time
series to improve clinical care practices.
\\ ( https://arxiv.org/abs/2402.17917 ,  1204kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17966
Date: Wed, 28 Feb 2024 01:15:30 GMT   (491kb,D)

Title: Conformer: Embedding Continuous Attention in Vision Transformer for
  Weather Forecasting
Authors: Hira Saleem, Flora Salim, Cormac Purcell
Categories: cs.LG
\\
  Operational weather forecasting system relies on computationally expensive
physics-based models. Although Transformers-based models have shown remarkable
potential in weather forecasting, Transformers are discrete models which limit
their ability to learn the continuous spatio-temporal features of the dynamical
weather system. We address this issue with Conformer, a spatio-temporal
Continuous Vision Transformer for weather forecasting. Conformer is designed to
learn the continuous weather evolution over time by implementing continuity in
the multi-head attention mechanism. The attention mechanism is encoded as a
differentiable function in the transformer architecture to model the complex
weather dynamics. We evaluate Conformer against a state-of-the-art Numerical
Weather Prediction (NWP) model and several deep learning based weather
forecasting models. Conformer outperforms some of the existing data-driven
models at all lead times while only being trained at lower resolution data.
\\ ( https://arxiv.org/abs/2402.17966 ,  491kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17967
Date: Wed, 28 Feb 2024 01:19:42 GMT   (2687kb,D)

Title: Imitation-regularized Optimal Transport on Networks: Provable Robustness
  and Application to Logistics Planning
Authors: Koshi Oishi, Yota Hashizume, Tomohiko Jimbo, Hirotaka Kaji, and Kenji
  Kashima
Categories: cs.LG cs.SY eess.SY
\\
  Network systems form the foundation of modern society, playing a critical
role in various applications. However, these systems are at significant risk of
being adversely affected by unforeseen circumstances, such as disasters.
Considering this, there is a pressing need for research to enhance the
robustness of network systems. Recently, in reinforcement learning, the
relationship between acquiring robustness and regularizing entropy has been
identified. Additionally, imitation learning is used within this framework to
reflect experts' behavior. However, there are no comprehensive studies on the
use of a similar imitation framework for optimal transport on networks.
Therefore, in this study, imitation-regularized optimal transport (I-OT) on
networks was investigated. It encodes prior knowledge on the network by
imitating a given prior distribution. The I-OT solution demonstrated robustness
in terms of the cost defined on the network. Moreover, we applied the I-OT to a
logistics planning problem using real data. We also examined the imitation and
apriori risk information scenarios to demonstrate the usefulness and
implications of the proposed method.
\\ ( https://arxiv.org/abs/2402.17967 ,  2687kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17978
Date: Wed, 28 Feb 2024 01:45:01 GMT   (1205kb,D)

Title: Imagine, Initialize, and Explore: An Effective Exploration Method in
  Multi-Agent Reinforcement Learning
Authors: Zeyang Liu, Lipeng Wan, Xinrui Yang, Zhuoran Chen, Xingyu Chen,
  Xuguang Lan
Categories: cs.LG cs.AI cs.MA
Comments: The 38th Annual AAAI Conference on Artificial Intelligence
\\
  Effective exploration is crucial to discovering optimal strategies for
multi-agent reinforcement learning (MARL) in complex coordination tasks.
Existing methods mainly utilize intrinsic rewards to enable committed
exploration or use role-based learning for decomposing joint action spaces
instead of directly conducting a collective search in the entire
action-observation space. However, they often face challenges obtaining
specific joint action sequences to reach successful states in long-horizon
tasks. To address this limitation, we propose Imagine, Initialize, and Explore
(IIE), a novel method that offers a promising solution for efficient
multi-agent exploration in complex scenarios. IIE employs a transformer model
to imagine how the agents reach a critical state that can influence each
other's transition functions. Then, we initialize the environment at this state
using a simulator before the exploration phase. We formulate the imagination as
a sequence modeling problem, where the states, observations, prompts, actions,
and rewards are predicted autoregressively. The prompt consists of
timestep-to-go, return-to-go, influence value, and one-shot demonstration,
specifying the desired state and trajectory as well as guiding the action
generation. By initializing agents at the critical states, IIE significantly
increases the likelihood of discovering potentially important under-explored
regions. Despite its simplicity, empirical results demonstrate that our method
outperforms multi-agent exploration baselines on the StarCraft Multi-Agent
Challenge (SMAC) and SMACv2 environments. Particularly, IIE shows improved
performance in the sparse-reward SMAC tasks and produces more effective
curricula over the initialized states than other generative methods, such as
CVAE-GAN and diffusion models.
\\ ( https://arxiv.org/abs/2402.17978 ,  1205kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17985
Date: Wed, 28 Feb 2024 02:00:34 GMT   (9426kb,D)

Title: FlattenQuant: Breaking Through the Inference Compute-bound for Large
  Language Models with Per-tensor Quantization
Authors: Yi Zhang, Fei Yang, Shuang Peng, Fangyu Wang, Aimin Pan
Categories: cs.LG cs.AI cs.CL
\\
  Large language models (LLMs) have demonstrated state-of-the-art performance
across various tasks. However, the latency of inference and the large GPU
memory consumption of LLMs restrict their deployment performance. Recently,
there have been some efficient attempts to quantize LLMs, yet inference with
large batch size or long sequence still has the issue of being compute-bound.
Fine-grained quantization methods have showcased their proficiency in achieving
low-bit quantization for LLMs, while requiring FP16 data type for linear layer
computations, which is time-consuming when dealing with large batch size or
long sequence. In this paper, we introduce a method called FlattenQuant, which
significantly reduces the maximum value of the tensor by flattening the large
channels in the tensor, to achieve low bit per-tensor quantization with minimal
accuracy loss. Our experiments show that FlattenQuant can directly use 4 bits
to achieve 48.29% of the linear layer calculation in LLMs, with the remaining
layers using 8 bits. The 4-bit matrix multiplication introduced in the
FlattenQuant method can effectively address the compute-bound caused by large
matrix calculation. Our work achieves up to 2$\times$ speedup and 2.3$\times$
memory reduction for LLMs with negligible loss in accuracy.
\\ ( https://arxiv.org/abs/2402.17985 ,  9426kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18007
Date: Wed, 28 Feb 2024 02:45:58 GMT   (870kb,D)

Title: Mixer is more than just a model
Authors: Qingfeng Ji, Yuxin Wang, Letong Sun
Categories: cs.LG cs.AI cs.SD eess.AS
\\
  Recently, MLP structures have regained popularity, with MLP-Mixer standing
out as a prominent example. In the field of computer vision, MLP-Mixer is noted
for its ability to extract data information from both channel and token
perspectives, effectively acting as a fusion of channel and token information.
Indeed, Mixer represents a paradigm for information extraction that amalgamates
channel and token information. The essence of Mixer lies in its ability to
blend information from diverse perspectives, epitomizing the true concept of
"mixing" in the realm of neural network architectures. Beyond channel and token
considerations, it is possible to create more tailored mixers from various
perspectives to better suit specific task requirements. This study focuses on
the domain of audio recognition, introducing a novel model named Audio
Spectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH) that incorporates
insights from both time and frequency domains. Experimental results demonstrate
that ASM-RH is particularly well-suited for audio data and yields promising
outcomes across multiple classification tasks.
\\ ( https://arxiv.org/abs/2402.18007 ,  870kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18012
Date: Wed, 28 Feb 2024 03:09:12 GMT   (17603kb,D)

Title: Diffusion Models as Constrained Samplers for Optimization with Unknown
  Constraints
Authors: Lingkai Kong, Yuanqi Du, Wenhao Mu, Kirill Neklyudov, Valentin De
  Bortol, Haorui Wang, Dongxia Wu, Aaron Ferber, Yi-An Ma, Carla P. Gomes, Chao
  Zhang
Categories: cs.LG cs.AI
\\
  Addressing real-world optimization problems becomes particularly challenging
when analytic objective functions or constraints are unavailable. While
numerous studies have addressed the issue of unknown objectives, limited
research has focused on scenarios where feasibility constraints are not given
explicitly. Overlooking these constraints can lead to spurious solutions that
are unrealistic in practice. To deal with such unknown constraints, we propose
to perform optimization within the data manifold using diffusion models. To
constrain the optimization process to the data manifold, we reformulate the
original optimization problem as a sampling problem from the product of the
Boltzmann distribution defined by the objective function and the data
distribution learned by the diffusion model. To enhance sampling efficiency, we
propose a two-stage framework that begins with a guided diffusion process for
warm-up, followed by a Langevin dynamics stage for further correction.
Theoretical analysis shows that the initial stage results in a distribution
focused on feasible solutions, thereby providing a better initialization for
the later stage. Comprehensive experiments on a synthetic dataset, six
real-world black-box optimization datasets, and a multi-objective optimization
dataset show that our method achieves better or comparable performance with
previous state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2402.18012 ,  17603kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18018
Date: Wed, 28 Feb 2024 03:27:10 GMT   (862kb)

Title: Communication Efficient ConFederated Learning: An Event-Triggered SAGA
  Approach
Authors: Bin Wang and Jun Fang and Hongbin Li and Yonina C. Eldar
Categories: cs.LG cs.DC eess.SP
\\
  Federated learning (FL) is a machine learning paradigm that targets model
training without gathering the local data dispersed over various data sources.
Standard FL, which employs a single server, can only support a limited number
of users, leading to degraded learning capability. In this work, we consider a
multi-server FL framework, referred to as \emph{Confederated Learning} (CFL),
in order to accommodate a larger number of users. A CFL system is composed of
multiple networked edge servers, with each server connected to an individual
set of users. Decentralized collaboration among servers is leveraged to harness
all users' data for model training. Due to the potentially massive number of
users involved, it is crucial to reduce the communication overhead of the CFL
system. We propose a stochastic gradient method for distributed learning in the
CFL framework. The proposed method incorporates a conditionally-triggered user
selection (CTUS) mechanism as the central component to effectively reduce
communication overhead. Relying on a delicately designed triggering condition,
the CTUS mechanism allows each server to select only a small number of users to
upload their gradients, without significantly jeopardizing the convergence
performance of the algorithm. Our theoretical analysis reveals that the
proposed algorithm enjoys a linear convergence rate. Simulation results show
that it achieves substantial improvement over state-of-the-art algorithms in
terms of communication efficiency.
\\ ( https://arxiv.org/abs/2402.18018 ,  862kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18046
Date: Wed, 28 Feb 2024 04:47:32 GMT   (596kb,D)

Title: Data augmentation method for modeling health records with applications
  to clopidogrel treatment failure detection
Authors: Sunwoong Choi and Samuel Kim
Categories: cs.LG
Comments: arXiv admin note: text overlap with arXiv:2310.08757
\\
  We present a novel data augmentation method to address the challenge of data
scarcity in modeling longitudinal patterns in Electronic Health Records (EHR)
of patients using natural language processing (NLP) algorithms. The proposed
method generates augmented data by rearranging the orders of medical records
within a visit where the order of elements are not obvious, if any. Applying
the proposed method to the clopidogrel treatment failure detection task enabled
up to 5.3% absolute improvement in terms of ROC-AUC (from 0.908 without
augmentation to 0.961 with augmentation) when it was used during the
pre-training procedure. It was also shown that the augmentation helped to
improve performance during fine-tuning procedures, especially when the amount
of labeled training data is limited.
\\ ( https://arxiv.org/abs/2402.18046 ,  596kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18059
Date: Wed, 28 Feb 2024 05:43:22 GMT   (328kb,D)

Title: Token-Specific Watermarking with Enhanced Detectability and Semantic
  Coherence for Large Language Models
Authors: Mingjia Huo, Sai Ashish Somayajula, Youwei Liang, Ruisi Zhang, Farinaz
  Koushanfar, Pengtao Xie
Categories: cs.LG cs.CL cs.CR
Comments: 16 pages, 9 figures, 2 tables
\\
  Large language models generate high-quality responses with potential
misinformation, underscoring the need for regulation by distinguishing
AI-generated and human-written texts. Watermarking is pivotal in this context,
which involves embedding hidden markers in texts during the LLM inference
phase, which is imperceptible to humans. Current watermarking algorithms,
however, face the challenge of achieving both the detectability of inserted
watermarks and the semantic integrity of generated texts, where enhancing one
aspect often undermines the other. To overcome this, we introduce a novel
multi-objective optimization (MOO) approach for watermarking that utilizes
lightweight networks to generate token-specific watermarking logits and
splitting ratios. By leveraging MOO to optimize for both detection and semantic
objective functions, our method simultaneously achieves detectability and
semantic integrity. Experimental results show that our method outperforms
current watermarking techniques in enhancing the detectability of texts
generated by LLMs while maintaining their semantic coherence. Our code is
available at https://github.com/mignonjia/TS_watermark .
\\ ( https://arxiv.org/abs/2402.18059 ,  328kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18096
Date: Wed, 28 Feb 2024 06:34:54 GMT   (5828kb,D)

Title: No Token Left Behind: Reliable KV Cache Compression via Importance-Aware
  Mixed Precision Quantization
Authors: June Yong Yang, Byeongwook Kim, Jeongin Bae, Beomseok Kwon, Gunho
  Park, Eunho Yang, Se Jung Kwon, Dongsoo Lee
Categories: cs.LG cs.AI
\\
  Key-Value (KV) Caching has become an essential technique for accelerating the
inference speed and throughput of generative Large Language Models~(LLMs).
However, the memory footprint of the KV cache poses a critical bottleneck in
LLM deployment as the cache size grows with batch size and sequence length,
often surpassing even the size of the model itself. Although recent methods
were proposed to select and evict unimportant KV pairs from the cache to reduce
memory consumption, the potential ramifications of eviction on the generative
process are yet to be thoroughly examined. In this paper, we examine the
detrimental impact of cache eviction and observe that unforeseen risks arise as
the information contained in the KV pairs is exhaustively discarded, resulting
in safety breaches, hallucinations, and context loss. Surprisingly, we find
that preserving even a small amount of information contained in the evicted KV
pairs via reduced precision quantization substantially recovers the incurred
degradation. On the other hand, we observe that the important KV pairs must be
kept at a relatively higher precision to safeguard the generation quality.
Motivated by these observations, we propose \textit{Mixed-precision KV
cache}~(MiKV), a reliable cache compression method that simultaneously
preserves the context details by retaining the evicted KV pairs in
low-precision and ensure generation quality by keeping the important KV pairs
in high-precision. Experiments on diverse benchmarks and LLM backbones show
that our proposed method offers a state-of-the-art trade-off between
compression ratio and performance, compared to other baselines.
\\ ( https://arxiv.org/abs/2402.18096 ,  5828kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18127
Date: Wed, 28 Feb 2024 07:36:16 GMT   (12960kb,D)

Title: Hierarchical Multi-Relational Graph Representation Learning for
  Large-Scale Prediction of Drug-Drug Interactions
Authors: Mengying Jiang, Guizhong Liu, Yuanchao Su, Weiqiang Jin, and Biao Zhao
Categories: cs.LG
Comments: 14 pages,10 figures
\\
  Most existing methods for predicting drug-drug interactions (DDI)
predominantly concentrate on capturing the explicit relationships among drugs,
overlooking the valuable implicit correlations present between drug pairs
(DPs), which leads to weak predictions. To address this issue, this paper
introduces a hierarchical multi-relational graph representation learning
(HMGRL) approach. Within the framework of HMGRL, we leverage a wealth of
drug-related heterogeneous data sources to construct heterogeneous graphs,
where nodes represent drugs and edges denote clear and various associations.
The relational graph convolutional network (RGCN) is employed to capture
diverse explicit relationships between drugs from these heterogeneous graphs.
Additionally, a multi-view differentiable spectral clustering (MVDSC) module is
developed to capture multiple valuable implicit correlations between DPs.
Within the MVDSC, we utilize multiple DP features to construct graphs, where
nodes represent DPs and edges denote different implicit correlations.
Subsequently, multiple DP representations are generated through graph cutting,
each emphasizing distinct implicit correlations. The graph-cutting strategy
enables our HMGRL to identify strongly connected communities of graphs, thereby
reducing the fusion of irrelevant features. By combining every representation
view of a DP, we create high-level DP representations for predicting DDIs. Two
genuine datasets spanning three distinct tasks are adopted to gauge the
efficacy of our HMGRL. Experimental outcomes unequivocally indicate that HMGRL
surpasses several leading-edge methods in performance.
\\ ( https://arxiv.org/abs/2402.18127 ,  12960kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18129
Date: Wed, 28 Feb 2024 07:39:58 GMT   (2835kb,D)

Title: On the Inductive Biases of Demographic Parity-based Fair Learning
  Algorithms
Authors: Haoyu Lei, Amin Gohari, Farzan Farnia
Categories: cs.LG cs.AI cs.IT math.IT
\\
  Fair supervised learning algorithms assigning labels with little dependence
on a sensitive attribute have attracted great attention in the machine learning
community. While the demographic parity (DP) notion has been frequently used to
measure a model's fairness in training fair classifiers, several studies in the
literature suggest potential impacts of enforcing DP in fair learning
algorithms. In this work, we analytically study the effect of standard DP-based
regularization methods on the conditional distribution of the predicted label
given the sensitive attribute. Our analysis shows that an imbalanced training
dataset with a non-uniform distribution of the sensitive attribute could lead
to a classification rule biased toward the sensitive attribute outcome holding
the majority of training data. To control such inductive biases in DP-based
fair learning, we propose a sensitive attribute-based distributionally robust
optimization (SA-DRO) method improving robustness against the marginal
distribution of the sensitive attribute. Finally, we present several numerical
results on the application of DP-based learning methods to standard centralized
and distributed learning problems. The empirical findings support our
theoretical results on the inductive biases in DP-based fair learning
algorithms and the debiasing effects of the proposed SA-DRO method.
\\ ( https://arxiv.org/abs/2402.18129 ,  2835kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18133
Date: Wed, 28 Feb 2024 07:54:50 GMT   (3140kb,D)

Title: Classes Are Not Equal: An Empirical Study on Image Recognition Fairness
Authors: Jiequan Cui, Beier Zhu, Xin Wen, Xiaojuan Qi, Bei Yu, Hanwang Zhang
Categories: cs.LG cs.CV
Comments: CVPR 2024
\\
  In this paper, we present an empirical study on image recognition fairness,
i.e., extreme class accuracy disparity on balanced data like ImageNet. We
experimentally demonstrate that classes are not equal and the fairness issue is
prevalent for image classification models across various datasets, network
architectures, and model capacities. Moreover, several intriguing properties of
fairness are identified. First, the unfairness lies in problematic
representation rather than classifier bias. Second, with the proposed concept
of Model Prediction Bias, we investigate the origins of problematic
representation during optimization. Our findings reveal that models tend to
exhibit greater prediction biases for classes that are more challenging to
recognize. It means that more other classes will be confused with harder
classes. Then the False Positives (FPs) will dominate the learning in
optimization, thus leading to their poor accuracy. Further, we conclude that
data augmentation and representation learning algorithms improve overall
performance by promoting fairness to some degree in image classification.
\\ ( https://arxiv.org/abs/2402.18133 ,  3140kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18149
Date: Wed, 28 Feb 2024 08:24:06 GMT   (85kb)

Title: Provably Efficient Partially Observable Risk-Sensitive Reinforcement
  Learning with Hindsight Observation
Authors: Tonghe Zhang, Yu Chen, Longbo Huang
Categories: cs.LG stat.ML
Comments: 38 pages
\\
  This work pioneers regret analysis of risk-sensitive reinforcement learning
in partially observable environments with hindsight observation, addressing a
gap in theoretical exploration. We introduce a novel formulation that
integrates hindsight observations into a Partially Observable Markov Decision
Process (POMDP) framework, where the goal is to optimize accumulated reward
under the entropic risk measure. We develop the first provably efficient RL
algorithm tailored for this setting. We also prove by rigorous analysis that
our algorithm achieves polynomial regret
$\tilde{O}\left(\frac{e^{|{\gamma}|H}-1}{|{\gamma}|H}H^2\sqrt{KHS^2OA}\right)$,
which outperforms or matches existing upper bounds when the model degenerates
to risk-neutral or fully observable settings. We adopt the method of
change-of-measure and develop a novel analytical tool of beta vectors to
streamline mathematical derivations. These techniques are of particular
interest to the theoretical study of reinforcement learning.
\\ ( https://arxiv.org/abs/2402.18149 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18153
Date: Wed, 28 Feb 2024 08:34:23 GMT   (518kb,D)

Title: Diffusion-based Neural Network Weights Generation
Authors: Bedionita Soro, Bruno Andreis, Hayeon Lee, Song Chong, Frank Hutter,
  Sung Ju Hwang
Categories: cs.LG cs.AI
Comments: 14 pages
\\
  Transfer learning is a topic of significant interest in recent deep learning
research because it enables faster convergence and improved performance on new
tasks. While the performance of transfer learning depends on the similarity of
the source data to the target data, it is costly to train a model on a large
number of datasets. Therefore, pretrained models are generally blindly selected
with the hope that they will achieve good performance on the given task. To
tackle such suboptimality of the pretrained models, we propose an efficient and
adaptive transfer learning scheme through dataset-conditioned pretrained
weights sampling. Specifically, we use a latent diffusion model with a
variational autoencoder that can reconstruct the neural network weights, to
learn the distribution of a set of pretrained weights conditioned on each
dataset for transfer learning on unseen datasets. By learning the distribution
of a neural network on a variety pretrained models, our approach enables
adaptive sampling weights for unseen datasets achieving faster convergence and
reaching competitive performance.
\\ ( https://arxiv.org/abs/2402.18153 ,  518kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18159
Date: Wed, 28 Feb 2024 08:43:18 GMT   (239kb,D)

Title: Provable Risk-Sensitive Distributional Reinforcement Learning with
  General Function Approximation
Authors: Yu Chen, Xiangcheng Zhang, Siwei Wang, Longbo Huang
Categories: cs.LG
\\
  In the realm of reinforcement learning (RL), accounting for risk is crucial
for making decisions under uncertainty, particularly in applications where
safety and reliability are paramount. In this paper, we introduce a general
framework on Risk-Sensitive Distributional Reinforcement Learning (RS-DisRL),
with static Lipschitz Risk Measures (LRM) and general function approximation.
Our framework covers a broad class of risk-sensitive RL, and facilitates
analysis of the impact of estimation functions on the effectiveness of RSRL
strategies and evaluation of their sample complexity. We design two innovative
meta-algorithms: \texttt{RS-DisRL-M}, a model-based strategy for model-based
function approximation, and \texttt{RS-DisRL-V}, a model-free approach for
general value function approximation. With our novel estimation techniques via
Least Squares Regression (LSR) and Maximum Likelihood Estimation (MLE) in
distributional RL with augmented Markov Decision Process (MDP), we derive the
first $\widetilde{\mathcal{O}}(\sqrt{K})$ dependency of the regret upper bound
for RSRL with static LRM, marking a pioneering contribution towards
statistically efficient algorithms in this domain.
\\ ( https://arxiv.org/abs/2402.18159 ,  239kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18164
Date: Wed, 28 Feb 2024 08:53:20 GMT   (299kb,D)

Title: Autoencoder-based General Purpose Representation Learning for Customer
  Embedding
Authors: Jan Henrik Bertrand, Jacopo Pio Gargano, Laurent Mombaerts, Jonathan
  Taws
Categories: cs.LG cs.AI
Comments: 14 pages, 2 figures
MSC-class: 68T-02
\\
  In recent years, exploiting the domain-specific underlying structure of data
and its generative factors for representation learning has shown success in
various use-case agnostic applications. However, the diversity and complexity
of tabular data have made it challenging to represent these structures in a
latent space through multi-dimensional vectors. We design an autoencoder-based
framework for building general purpose embeddings, we assess the performance of
different autoencoder architectures, and show simpler models outperform complex
ones in embedding highly complex tabular data. We apply our framework to
produce plug-and-play, rich, and anonymized embeddings representing AWS
customers for usage in any model, saving up to 45% of development time, and
observe significant improvements in downstream models. Moreover, we propose a
significant improvement to the calculation of reconstruction loss for
multi-layer contractive autoencoders (CAE) by calculating the Jacobian of the
entire encoder leading to a 15% improvement in reconstruction quality when
compared to a stacked CAE.
\\ ( https://arxiv.org/abs/2402.18164 ,  299kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18167
Date: Wed, 28 Feb 2024 08:56:00 GMT   (4143kb,D)

Title: Decentralised Traffic Incident Detection via Network Lasso
Authors: Qiyuan Zhu, A. K. Qin, Prabath Abeysekara, Hussein Dia, Hanna
  Grzybowska
Categories: cs.LG
\\
  Traffic incident detection plays a key role in intelligent transportation
systems, which has gained great attention in transport engineering. In the
past, traditional machine learning (ML) based detection methods achieved good
performance under a centralised computing paradigm, where all data are
transmitted to a central server for building ML models therein. Nowadays, deep
neural networks based federated learning (FL) has become a mainstream detection
approach to enable the model training in a decentralised manner while
warranting local data governance. Such neural networks-centred techniques,
however, have overshadowed the utility of well-established ML-based detection
methods. In this work, we aim to explore the potential of potent conventional
ML-based detection models in modern traffic scenarios featured by distributed
data. We leverage an elegant but less explored distributed optimisation
framework named Network Lasso, with guaranteed global convergence for convex
problem formulations, integrate the potent convex ML model with it, and compare
it with centralised learning, local learning, and federated learning methods
atop a well-known traffic incident detection dataset. Experimental results show
that the proposed network lasso-based approach provides a promising alternative
to the FL-based approach in data-decentralised traffic scenarios, with a strong
convergence guarantee while rekindling the significance of conventional
ML-based detection methods.
\\ ( https://arxiv.org/abs/2402.18167 ,  4143kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18198
Date: Wed, 28 Feb 2024 09:40:36 GMT   (7499kb,D)

Title: Automated Machine Learning for Multi-Label Classification
Authors: Marcel Wever
Categories: cs.LG
DOI: 10.17619/UNIPB/1-1302
\\
  Automated machine learning (AutoML) aims to select and configure machine
learning algorithms and combine them into machine learning pipelines tailored
to a dataset at hand. For supervised learning tasks, most notably binary and
multinomial classification, aka single-label classification (SLC), such AutoML
approaches have shown promising results. However, the task of multi-label
classification (MLC), where data points are associated with a set of class
labels instead of a single class label, has received much less attention so
far. In the context of multi-label classification, the data-specific selection
and configuration of multi-label classifiers are challenging even for experts
in the field, as it is a high-dimensional optimization problem with multi-level
hierarchical dependencies. While for SLC, the space of machine learning
pipelines is already huge, the size of the MLC search space outnumbers the one
of SLC by several orders.
  In the first part of this thesis, we devise a novel AutoML approach for
single-label classification tasks optimizing pipelines of machine learning
algorithms, consisting of two algorithms at most. This approach is then
extended first to optimize pipelines of unlimited length and eventually
configure the complex hierarchical structures of multi-label classification
methods. Furthermore, we investigate how well AutoML approaches that form the
state of the art for single-label classification tasks scale with the increased
problem complexity of AutoML for multi-label classification.
  In the second part, we explore how methods for SLC and MLC could be
configured more flexibly to achieve better generalization performance and how
to increase the efficiency of execution-based AutoML systems.
\\ ( https://arxiv.org/abs/2402.18198 ,  7499kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18211
Date: Wed, 28 Feb 2024 10:01:44 GMT   (2060kb,D)

Title: Catastrophic Overfitting: A Potential Blessing in Disguise
Authors: Mengnan Zhao, Lihe Zhang, Yuqiu Kong, Baocai Yin
Categories: cs.LG cs.CR
\\
  Fast Adversarial Training (FAT) has gained increasing attention within the
research community owing to its efficacy in improving adversarial robustness.
Particularly noteworthy is the challenge posed by catastrophic overfitting (CO)
in this field. Although existing FAT approaches have made strides in mitigating
CO, the ascent of adversarial robustness occurs with a non-negligible decline
in classification accuracy on clean samples. To tackle this issue, we initially
employ the feature activation differences between clean and adversarial
examples to analyze the underlying causes of CO. Intriguingly, our findings
reveal that CO can be attributed to the feature coverage induced by a few
specific pathways. By intentionally manipulating feature activation differences
in these pathways with well-designed regularization terms, we can effectively
mitigate and induce CO, providing further evidence for this observation.
Notably, models trained stably with these terms exhibit superior performance
compared to prior FAT work. On this basis, we harness CO to achieve `attack
obfuscation', aiming to bolster model performance. Consequently, the models
suffering from CO can attain optimal classification accuracy on both clean and
adversarial data when adding random noise to inputs during evaluation. We also
validate their robustness against transferred adversarial examples and the
necessity of inducing CO to improve robustness. Hence, CO may not be a problem
that has to be solved.
\\ ( https://arxiv.org/abs/2402.18211 ,  2060kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18213
Date: Wed, 28 Feb 2024 10:09:04 GMT   (4948kb,D)

Title: Multi-objective Differentiable Neural Architecture Search
Authors: Rhea Sanjay Sukthanker, Arber Zela, Benedikt Staffler, Samuel Dooley,
  Josif Grabocka, Frank Hutter
Categories: cs.LG cs.CV stat.ML
Comments: 31 pages, 22 figures
\\
  Pareto front profiling in multi-objective optimization (MOO), i.e. finding a
diverse set of Pareto optimal solutions, is challenging, especially with
expensive objectives like neural network training. Typically, in MOO neural
architecture search (NAS), we aim to balance performance and hardware metrics
across devices. Prior NAS approaches simplify this task by incorporating
hardware constraints into the objective function, but profiling the Pareto
front necessitates a search for each constraint. In this work, we propose a
novel NAS algorithm that encodes user preferences for the trade-off between
performance and hardware metrics, and yields representative and diverse
architectures across multiple devices in just one search run. To this end, we
parameterize the joint architectural distribution across devices and multiple
objectives via a hypernetwork that can be conditioned on hardware features and
preference vectors, enabling zero-shot transferability to new devices.
Extensive experiments with up to 19 hardware devices and 3 objectives showcase
the effectiveness and scalability of our method. Finally, we show that, without
additional costs, our method outperforms existing MOO NAS methods across
qualitatively different search spaces and datasets, including MobileNetV3 on
ImageNet-1k and a Transformer space on machine translation.
\\ ( https://arxiv.org/abs/2402.18213 ,  4948kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18260
Date: Wed, 28 Feb 2024 11:47:15 GMT   (824kb,D)

Title: Efficiently Computable Safety Bounds for Gaussian Processes in Active
  Learning
Authors: J\"orn Tebbe, Christoph Zimmer, Ansgar Steland, Markus
  Lange-Hegermann, Fabian Mies
Categories: cs.LG
\\
  Active learning of physical systems must commonly respect practical safety
constraints, which restricts the exploration of the design space. Gaussian
Processes (GPs) and their calibrated uncertainty estimations are widely used
for this purpose. In many technical applications the design space is explored
via continuous trajectories, along which the safety needs to be assessed. This
is particularly challenging for strict safety requirements in GP methods, as it
employs computationally expensive Monte-Carlo sampling of high quantiles. We
address these challenges by providing provable safety bounds based on the
adaptively sampled median of the supremum of the posterior GP. Our method
significantly reduces the number of samples required for estimating high safety
probabilities, resulting in faster evaluation without sacrificing accuracy and
exploration speed. The effectiveness of our safe active learning approach is
demonstrated through extensive simulations and validated using a real-world
engine example.
\\ ( https://arxiv.org/abs/2402.18260 ,  824kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18285
Date: Wed, 28 Feb 2024 12:24:27 GMT   (2206kb,D)

Title: PiShield: A NeSy Framework for Learning with Requirements
Authors: Mihaela C\u{a}t\u{a}lina Stoian, Alex Tatomir, Thomas Lukasiewicz,
  Eleonora Giunchiglia
Categories: cs.LG cs.AI cs.LO
Comments: Demo paper
\\
  Deep learning models have shown their strengths in various application
domains, however, they often struggle to meet safety requirements for their
outputs. In this paper, we introduce PiShield, the first framework ever
allowing for the integration of the requirements into the neural networks'
topology. PiShield guarantees compliance with these requirements, regardless of
input. Additionally, it allows for integrating requirements both at inference
and/or training time, depending on the practitioners' needs. Given the
widespread application of deep learning, there is a growing need for frameworks
allowing for the integration of the requirements across various domains. Here,
we explore three application scenarios: functional genomics, autonomous
driving, and tabular data generation.
\\ ( https://arxiv.org/abs/2402.18285 ,  2206kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18296
Date: Wed, 28 Feb 2024 12:41:06 GMT   (827kb)

Title: Comparative Analysis of XGBoost and Minirocket Algortihms for Human
  Activity Recognition
Authors: Celal Alagoz
Categories: cs.LG
Comments: 12 pages, 1 figure, 1st Bilsel International Sur Scientific
  Researches Congress, 10-11 February, 2024, Diyarbakir, T\"urk\.Iye
MSC-class: 68-11
ACM-class: J.7
\\
  Human Activity Recognition (HAR) has been extensively studied, with recent
emphasis on the implementation of advanced Machine Learning (ML) and Deep
Learning (DL) algorithms for accurate classification. This study investigates
the efficacy of two ML algorithms, eXtreme Gradient Boosting (XGBoost) and
MiniRocket, in the realm of HAR using data collected from smartphone sensors.
The experiments are conducted on a dataset obtained from the UCI repository,
comprising accelerometer and gyroscope signals captured from 30 volunteers
performing various activities while wearing a smartphone. The dataset undergoes
preprocessing, including noise filtering and feature extraction, before being
utilized for training and testing the classifiers. Monte Carlo cross-validation
is employed to evaluate the models' robustness. The findings reveal that both
XGBoost and MiniRocket attain accuracy, F1 score, and AUC values as high as
0.99 in activity classification. XGBoost exhibits a slightly superior
performance compared to MiniRocket. Notably, both algorithms surpass the
performance of other ML and DL algorithms reported in the literature for HAR
tasks. Additionally, the study compares the computational efficiency of the two
algorithms, revealing XGBoost's advantage in terms of training time.
Furthermore, the performance of MiniRocket, which achieves accuracy and F1
values of 0.94, and an AUC value of 0.96 using raw data and utilizing only one
channel from the sensors, highlights the potential of directly leveraging
unprocessed signals. It also suggests potential advantages that could be gained
by utilizing sensor fusion or channel fusion techniques. Overall, this research
sheds light on the effectiveness and computational characteristics of XGBoost
and MiniRocket in HAR tasks, providing insights for future studies in activity
recognition using smartphone sensor data.
\\ ( https://arxiv.org/abs/2402.18296 ,  827kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18311
Date: Wed, 28 Feb 2024 13:11:06 GMT   (9444kb,D)

Title: Escaping Local Optima in Global Placement
Authors: Ke Xue, Xi Lin, Yunqi Shi, Shixiong Kai, Siyuan Xu, Chao Qian
Categories: cs.LG cs.NE
Comments: Work-in-Progress (WIP) poster of DAC 2024
\\
  Placement is crucial in the physical design, as it greatly affects power,
performance, and area metrics. Recent advancements in analytical methods, such
as DREAMPlace, have demonstrated impressive performance in global placement.
However, DREAMPlace has some limitations, e.g., may not guarantee legalizable
placements under the same settings, leading to fragile and unpredictable
results. This paper highlights the main issue as being stuck in local optima,
and proposes a hybrid optimization framework to efficiently escape the local
optima, by perturbing the placement result iteratively. The proposed framework
achieves significant improvements compared to state-of-the-art methods on two
popular benchmarks.
\\ ( https://arxiv.org/abs/2402.18311 ,  9444kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18337
Date: Wed, 28 Feb 2024 13:59:20 GMT   (4013kb,D)

Title: Probabilistic Bayesian optimal experimental design using conditional
  normalizing flows
Authors: Rafael Orozco, Felix J. Herrmann, Peng Chen
Categories: cs.LG cs.CV
\\
  Bayesian optimal experimental design (OED) seeks to conduct the most
informative experiment under budget constraints to update the prior knowledge
of a system to its posterior from the experimental data in a Bayesian
framework. Such problems are computationally challenging because of (1)
expensive and repeated evaluation of some optimality criterion that typically
involves a double integration with respect to both the system parameters and
the experimental data, (2) suffering from the curse-of-dimensionality when the
system parameters and design variables are high-dimensional, (3) the
optimization is combinatorial and highly non-convex if the design variables are
binary, often leading to non-robust designs. To make the solution of the
Bayesian OED problem efficient, scalable, and robust for practical
applications, we propose a novel joint optimization approach. This approach
performs simultaneous (1) training of a scalable conditional normalizing flow
(CNF) to efficiently maximize the expected information gain (EIG) of a jointly
learned experimental design (2) optimization of a probabilistic formulation of
the binary experimental design with a Bernoulli distribution. We demonstrate
the performance of our proposed method for a practical MRI data acquisition
problem, one of the most challenging Bayesian OED problems that has
high-dimensional (320 $\times$ 320) parameters at high image resolution,
high-dimensional (640 $\times$ 386) observations, and binary mask designs to
select the most informative observations.
\\ ( https://arxiv.org/abs/2402.18337 ,  4013kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18372
Date: Tue, 27 Feb 2024 15:53:15 GMT   (3359kb,D)

Title: FedUV: Uniformity and Variance for Heterogeneous Federated Learning
Authors: Ha Min Son, Moon Hyun Kim, Tai-Myoung Chung, Chao Huang, Xin Liu
Categories: cs.LG cs.AI cs.DC
Comments: 11 pages, 4 figures, 5 tables, to appear at CVPR 2024
\\
  Federated learning is a promising framework to train neural networks with
widely distributed data. However, performance degrades heavily with
heterogeneously distributed data. Recent work has shown this is due to the
final layer of the network being most prone to local bias, some finding success
freezing the final layer as an orthogonal classifier. We investigate the
training dynamics of the classifier by applying SVD to the weights motivated by
the observation that freezing weights results in constant singular values. We
find that there are differences when training in IID and non-IID settings.
Based on this finding, we introduce two regularization terms for local training
to continuously emulate IID settings: (1) variance in the dimension-wise
probability distribution of the classifier and (2) hyperspherical uniformity of
representations of the encoder. These regularizations promote local models to
act as if it were in an IID setting regardless of the local data distribution,
thus offsetting proneness to bias while being flexible to the data. On
extensive experiments in both label-shift and feature-shift settings, we verify
that our method achieves highest performance by a large margin especially in
highly non-IID cases in addition to being scalable to larger models and
datasets.
\\ ( https://arxiv.org/abs/2402.18372 ,  3359kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18377
Date: Wed, 28 Feb 2024 14:52:58 GMT   (9514kb,D)

Title: Out-of-Domain Generalization in Dynamical Systems Reconstruction
Authors: Niclas G\"oring, Florian Hess, Manuel Brenner, Zahra Monfared, Daniel
  Durstewitz
Categories: cs.LG
\\
  In science we are interested in finding the governing equations, the
dynamical rules, underlying empirical phenomena. While traditionally scientific
models are derived through cycles of human insight and experimentation,
recently deep learning (DL) techniques have been advanced to reconstruct
dynamical systems (DS) directly from time series data. State-of-the-art
dynamical systems reconstruction (DSR) methods show promise in capturing
invariant and long-term properties of observed DS, but their ability to
generalize to unobserved domains remains an open challenge. Yet, this is a
crucial property we would expect from any viable scientific theory. In this
work, we provide a formal framework that addresses generalization in DSR. We
explain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly
differs from OODG considered elsewhere in machine learning. We introduce
mathematical notions based on topological concepts and ergodic theory to
formalize the idea of learnability of a DSR model. We formally prove that
black-box DL techniques, without adequate structural priors, generally will not
be able to learn a generalizing DSR model. We also show this empirically,
considering major classes of DSR algorithms proposed so far, and illustrate
where and why they fail to generalize across the whole phase space. Our study
provides the first comprehensive mathematical treatment of OODG in DSR, and
gives a deeper conceptual understanding of where the fundamental problems in
OODG lie and how they could possibly be addressed in practice.
\\ ( https://arxiv.org/abs/2402.18377 ,  9514kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18392
Date: Wed, 28 Feb 2024 15:12:24 GMT   (254kb,D)

Title: Unveiling the Potential of Robustness in Evaluating Causal Inference
  Models
Authors: Yiyan Huang, Cheuk Hang Leung, Siyi Wang, Yijun Li, Qi Wu
Categories: cs.LG cs.AI econ.EM stat.ML
\\
  The growing demand for personalized decision-making has led to a surge of
interest in estimating the Conditional Average Treatment Effect (CATE). The
intersection of machine learning and causal inference has yielded various
effective CATE estimators. However, deploying these estimators in practice is
often hindered by the absence of counterfactual labels, making it challenging
to select the desirable CATE estimator using conventional model selection
procedures like cross-validation. Existing approaches for CATE estimator
selection, such as plug-in and pseudo-outcome metrics, face two inherent
challenges. Firstly, they are required to determine the metric form and the
underlying machine learning models for fitting nuisance parameters or plug-in
learners. Secondly, they lack a specific focus on selecting a robust estimator.
To address these challenges, this paper introduces a novel approach, the
Distributionally Robust Metric (DRM), for CATE estimator selection. The
proposed DRM not only eliminates the need to fit additional models but also
excels at selecting a robust CATE estimator. Experimental studies demonstrate
the efficacy of the DRM method, showcasing its consistent effectiveness in
identifying superior estimators while mitigating the risk of selecting inferior
ones.
\\ ( https://arxiv.org/abs/2402.18392 ,  254kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18434
Date: Wed, 28 Feb 2024 16:00:25 GMT   (1304kb,D)

Title: Graph Regularized Encoder Training for Extreme Classification
Authors: Anshul Mittal, Shikhar Mohan, Deepak Saini, Suchith C. Prabhu, Jain
  jiao, Sumeet Agarwal, Soumen Chakrabarti, Purushottam Kar, Manik Varma
Categories: cs.LG cs.IR
\\
  Deep extreme classification (XC) aims to train an encoder architecture and an
accompanying classifier architecture to tag a data point with the most relevant
subset of labels from a very large universe of labels. XC applications in
ranking, recommendation and tagging routinely encounter tail labels for which
the amount of training data is exceedingly small. Graph convolutional networks
(GCN) present a convenient but computationally expensive way to leverage task
metadata and enhance model accuracies in these settings. This paper formally
establishes that in several use cases, the steep computational cost of GCNs is
entirely avoidable by replacing GCNs with non-GCN architectures. The paper
notices that in these settings, it is much more effective to use graph data to
regularize encoder training than to implement a GCN. Based on these insights,
an alternative paradigm RAMEN is presented to utilize graph metadata in XC
settings that offers significant performance boosts with zero increase in
inference computational costs. RAMEN scales to datasets with up to 1M labels
and offers prediction accuracy up to 15% higher on benchmark datasets than
state of the art methods, including those that use graph metadata to train
GCNs. RAMEN also offers 10% higher accuracy over the best baseline on a
proprietary recommendation dataset sourced from click logs of a popular search
engine. Code for RAMEN will be released publicly.
\\ ( https://arxiv.org/abs/2402.18434 ,  1304kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18443
Date: Wed, 28 Feb 2024 16:13:44 GMT   (2265kb,D)

Title: LeMo-NADe: Multi-Parameter Neural Architecture Discovery with LLMs
Authors: Md Hafizur Rahman and Prabuddha Chakraborty
Categories: cs.LG cs.AI
Comments: 19 pages, 5 figures, 10 tables and 3 algorithms
\\
  Building efficient neural network architectures can be a time-consuming task
requiring extensive expert knowledge. This task becomes particularly
challenging for edge devices because one has to consider parameters such as
power consumption during inferencing, model size, inferencing speed, and CO2
emissions. In this article, we introduce a novel framework designed to
automatically discover new neural network architectures based on user-defined
parameters, an expert system, and an LLM trained on a large amount of
open-domain knowledge. The introduced framework (LeMo-NADe) is tailored to be
used by non-AI experts, does not require a predetermined neural architecture
search space, and considers a large set of edge device-specific parameters. We
implement and validate this proposed neural architecture discovery framework
using CIFAR-10, CIFAR-100, and ImageNet16-120 datasets while using GPT-4 Turbo
and Gemini as the LLM component. We observe that the proposed framework can
rapidly (within hours) discover intricate neural network models that perform
extremely well across a diverse set of application settings defined by the
user.
\\ ( https://arxiv.org/abs/2402.18443 ,  2265kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18477
Date: Wed, 28 Feb 2024 16:58:31 GMT   (1332kb,D)

Title: Signature Kernel Conditional Independence Tests in Causal Discovery for
  Stochastic Processes
Authors: Georg Manten, Cecilia Casolo, Emilio Ferrucci, S{\o}ren Wengel
  Mogensen, Cristopher Salvi, Niki Kilbertus
Categories: cs.LG cs.AI stat.ML
\\
  Inferring the causal structure underlying stochastic dynamical systems from
observational data holds great promise in domains ranging from science and
health to finance. Such processes can often be accurately modeled via
stochastic differential equations (SDEs), which naturally imply causal
relationships via "which variables enter the differential of which other
variables". In this paper, we develop a kernel-based test of conditional
independence (CI) on "path-space" -- solutions to SDEs -- by leveraging recent
advances in signature kernels. We demonstrate strictly superior performance of
our proposed CI test compared to existing approaches on path-space. Then, we
develop constraint-based causal discovery algorithms for acyclic stochastic
dynamical systems (allowing for loops) that leverage temporal information to
recover the entire directed graph. Assuming faithfulness and a CI oracle, our
algorithm is sound and complete. We empirically verify that our developed CI
test in conjunction with the causal discovery algorithm reliably outperforms
baselines across a range of settings.
\\ ( https://arxiv.org/abs/2402.18477 ,  1332kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18491
Date: Wed, 28 Feb 2024 17:19:26 GMT   (6632kb,D)

Title: Dynamical Regimes of Diffusion Models
Authors: Giulio Biroli, Tony Bonnaire, Valentin de Bortoli, Marc M\'ezard
Categories: cs.LG cond-mat.stat-mech
Comments: 22 pages, 11 figures
\\
  Using statistical physics methods, we study generative diffusion models in
the regime where the dimension of space and the number of data are large, and
the score function has been trained optimally. Our analysis reveals three
distinct dynamical regimes during the backward generative diffusion process.
The generative dynamics, starting from pure noise, encounters first a
'speciation' transition where the gross structure of data is unraveled, through
a mechanism similar to symmetry breaking in phase transitions. It is followed
at later time by a 'collapse' transition where the trajectories of the dynamics
become attracted to one of the memorized data points, through a mechanism which
is similar to the condensation in a glass phase. For any dataset, the
speciation time can be found from a spectral analysis of the correlation
matrix, and the collapse time can be found from the estimation of an 'excess
entropy' in the data. The dependence of the collapse time on the dimension and
number of data provides a thorough characterization of the curse of
dimensionality for diffusion models. Analytical solutions for simple models
like high-dimensional Gaussian mixtures substantiate these findings and provide
a theoretical framework, while extensions to more complex scenarios and
numerical validations with real datasets confirm the theoretical predictions.
\\ ( https://arxiv.org/abs/2402.18491 ,  6632kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18495
Date: Wed, 28 Feb 2024 17:25:06 GMT   (469kb,D)

Title: ROG$_{PL}$: Robust Open-Set Graph Learning via Region-Based Prototype
  Learning
Authors: Qin Zhang, Xiaowei Li, Jiexin Lu, Liping Qiu, Shirui Pan, Xiaojun
  Chen, Junyang Chen
Categories: cs.LG
Comments: 9 pages, 5 figures
\\
  Open-set graph learning is a practical task that aims to classify the known
class nodes and to identify unknown class samples as unknowns. Conventional
node classification methods usually perform unsatisfactorily in open-set
scenarios due to the complex data they encounter, such as out-of-distribution
(OOD) data and in-distribution (IND) noise. OOD data are samples that do not
belong to any known classes. They are outliers if they occur in training (OOD
noise), and open-set samples if they occur in testing. IND noise are training
samples which are assigned incorrect labels. The existence of IND noise and OOD
noise is prevalent, which usually cause the ambiguity problem, including the
intra-class variety problem and the inter-class confusion problem. Thus, to
explore robust open-set learning methods is necessary and difficult, and it
becomes even more difficult for non-IID graph data.To this end, we propose a
unified framework named ROG$_{PL}$ to achieve robust open-set learning on
complex noisy graph data, by introducing prototype learning. In specific,
ROG$_{PL}$ consists of two modules, i.e., denoising via label propagation and
open-set prototype learning via regions. The first module corrects noisy labels
through similarity-based label propagation and removes low-confidence samples,
to solve the intra-class variety problem caused by noise. The second module
learns open-set prototypes for each known class via non-overlapped regions and
remains both interior and border prototypes to remedy the inter-class confusion
problem.The two modules are iteratively updated under the constraints of
classification loss and prototype diversity loss. To the best of our knowledge,
the proposed ROG$_{PL}$ is the first robust open-set node classification method
for graph data with complex noise.
\\ ( https://arxiv.org/abs/2402.18495 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18505
Date: Wed, 28 Feb 2024 17:34:21 GMT   (1764kb)

Title: Evolving machine learning workflows through interactive AutoML
Authors: Rafael Barbudo and Aurora Ram\'irez and Jos\'e Ra\'ul Romero
Categories: cs.LG
Comments: 40 pages, 5 figures, 3 tables. Paper submitted to journal. Under
  review
MSC-class: 68T05
ACM-class: I.2.6; I.2.8
\\
  Automatic workflow composition (AWC) is a relevant problem in automated
machine learning (AutoML) that allows finding suitable sequences of
preprocessing and prediction models together with their optimal
hyperparameters. This problem can be solved using evolutionary algorithms and,
in particular, grammar-guided genetic programming (G3P). Current G3P approaches
to AWC define a fixed grammar that formally specifies how workflow elements can
be combined and which algorithms can be included. In this paper we present
\ourmethod, an interactive G3P algorithm that allows users to dynamically
modify the grammar to prune the search space and focus on their regions of
interest. Our proposal is the first to combine the advantages of a G3P method
with ideas from interactive optimisation and human-guided machine learning, an
area little explored in the context of AutoML. To evaluate our approach, we
present an experimental study in which 20 participants interact with \ourmethod
to evolve workflows according to their preferences. Our results confirm that
the collaboration between \ourmethod and humans allows us to find
high-performance workflows in terms of accuracy that require less tuning time
than those found without human intervention.
\\ ( https://arxiv.org/abs/2402.18505 ,  1764kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18508
Date: Wed, 28 Feb 2024 17:36:45 GMT   (131kb,D)

Title: Orchid: Flexible and Data-Dependent Convolution for Sequence Modeling
Authors: Mahdi Karami and Ali Ghodsi
Categories: cs.LG
\\
  In the rapidly evolving landscape of deep learning, the quest for models that
balance expressivity with computational efficiency has never been more
critical. This paper introduces Orchid, a novel architecture that reimagines
sequence modeling by incorporating a new data-dependent convolution mechanism.
Orchid is designed to address the inherent limitations of traditional attention
mechanisms, particularly their quadratic complexity, without compromising the
ability to capture long-range dependencies and in-context learning. At the core
of Orchid lies the data-dependent convolution layer, which dynamically adjusts
its kernel conditioned on input data using a dedicated conditioning neural
network. We design two simple conditioning networks that maintain shift
equivariance in the adaptive convolution operation. The dynamic nature of
data-dependent convolution kernel, coupled with gating operations, grants
Orchid high expressivity while maintaining efficiency and quasilinear
scalability for long sequences. We rigorously evaluate Orchid across multiple
domains, including language modeling and image classification, to showcase its
performance and generality. Our experiments demonstrate that Orchid
architecture not only outperforms traditional attention-based architectures
such as BERT and Vision Transformers with smaller model sizes, but also extends
the feasible sequence length beyond the limitations of the dense attention
layers. This achievement represents a significant step towards more efficient
and scalable deep learning models for sequence modeling.
\\ ( https://arxiv.org/abs/2402.18508 ,  131kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18510
Date: Wed, 28 Feb 2024 17:38:06 GMT   (333kb,D)

Title: RNNs are not Transformers (Yet): The Key Bottleneck on In-context
  Retrieval
Authors: Kaiyue Wen, Xingyu Dang, Kaifeng Lyu
Categories: cs.LG cs.CL stat.ML
Comments: 42 pages, 5 figures
\\
  This paper investigates the gap in representation powers of Recurrent Neural
Networks (RNNs) and Transformers in the context of solving algorithmic
problems. We focus on understanding whether RNNs, known for their memory
efficiency in handling long sequences, can match the performance of
Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting.
Our theoretical analysis reveals that CoT improves RNNs but is insufficient to
close the gap with Transformers. A key bottleneck lies in the inability of RNNs
to perfectly retrieve information from the context, even with CoT: for several
tasks that explicitly or implicitly require this capability, such as
associative recall and determining if a graph is a tree, we prove that RNNs are
not expressive enough to solve the tasks while Transformers can solve them with
ease. Conversely, we prove that adopting techniques to enhance the in-context
retrieval capability of RNNs, including Retrieval-Augmented Generation (RAG)
and adding a single Transformer layer, can elevate RNNs to be capable of
solving all polynomial-time solvable problems with CoT, hence closing the
representation gap with Transformers.
\\ ( https://arxiv.org/abs/2402.18510 ,  333kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18512
Date: Wed, 28 Feb 2024 17:40:05 GMT   (1928kb,D)

Title: Log Neural Controlled Differential Equations: The Lie Brackets Make a
  Difference
Authors: Benjamin Walker, Andrew D. McLeod, Tiexin Qin, Yichuan Cheng, Haoliang
  Li, Terry Lyons
Categories: cs.LG
Comments: 8 pages, 4 figures
\\
  The vector field of a controlled differential equation (CDE) describes the
relationship between a control path and the evolution of a solution path.
Neural CDEs (NCDEs) treat time series data as observations from a control path,
parameterise a CDE's vector field using a neural network, and use the solution
path as a continuously evolving hidden state. As their formulation makes them
robust to irregular sampling rates, NCDEs are a powerful approach for modelling
real-world data. Building on neural rough differential equations (NRDEs), we
introduce Log-NCDEs, a novel and effective method for training NCDEs. The core
component of Log-NCDEs is the Log-ODE method, a tool from the study of rough
paths for approximating a CDE's solution. On a range of multivariate time
series classification benchmarks, Log-NCDEs are shown to achieve a higher
average test set accuracy than NCDEs, NRDEs, and two state-of-the-art models,
S5 and the linear recurrent unit.
\\ ( https://arxiv.org/abs/2402.18512 ,  1928kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18540
Date: Wed, 28 Feb 2024 18:23:49 GMT   (921kb,D)

Title: Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt
  Templates
Authors: Kaifeng Lyu, Haoyu Zhao, Xinran Gu, Dingli Yu, Anirudh Goyal, Sanjeev
  Arora
Categories: cs.LG cs.AI cs.CL
Comments: 20 pages
\\
  Public LLMs such as the Llama 2-Chat have driven huge activity in LLM
research. These models underwent alignment training and were considered safe.
Recently Qi et al. (2023) reported that even benign fine-tuning (e.g., on
seemingly safe datasets) can give rise to unsafe behaviors in the models. The
current paper is about methods and best practices to mitigate such loss of
alignment. Through extensive experiments on several chat models (Meta's Llama
2-Chat, Mistral AI's Mistral 7B Instruct v0.2, and OpenAI's GPT-3.5 Turbo),
this paper uncovers that the prompt templates used during fine-tuning and
inference play a crucial role in preserving safety alignment, and proposes the
"Pure Tuning, Safe Testing" (PTST) principle -- fine-tune models without a
safety prompt, but include it at test time. Fine-tuning experiments on GSM8K,
ChatDoctor, and OpenOrca show that PTST significantly reduces the rise of
unsafe behaviors, and even almost eliminates them in some cases.
\\ ( https://arxiv.org/abs/2402.18540 ,  921kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18546
Date: Wed, 28 Feb 2024 18:29:25 GMT   (7502kb,D)

Title: Generalizability Under Sensor Failure: Tokenization + Transformers
  Enable More Robust Latent Spaces
Authors: Geeling Chau, Yujin An, Ahamed Raffey Iqbal, Soon-Jo Chung, Yisong
  Yue, Sabera Talukder
Categories: cs.LG
Comments: 8 pages, 8 figures
\\
  A major goal in neuroscience is to discover neural data representations that
generalize. This goal is challenged by variability along recording sessions
(e.g. environment), subjects (e.g. varying neural structures), and sensors
(e.g. sensor noise), among others. Recent work has begun to address
generalization across sessions and subjects, but few study robustness to sensor
failure which is highly prevalent in neuroscience experiments. In order to
address these generalizability dimensions we first collect our own
electroencephalography dataset with numerous sessions, subjects, and sensors,
then study two time series models: EEGNet (Lawhern et al., 2018) and TOTEM
(Talukder et al., 2024). EEGNet is a widely used convolutional neural network,
while TOTEM is a discrete time series tokenizer and transformer model. We find
that TOTEM outperforms or matches EEGNet across all generalizability cases.
Finally through analysis of TOTEM's latent codebook we observe that
tokenization enables generalization.
\\ ( https://arxiv.org/abs/2402.18546 ,  7502kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18551
Date: Wed, 28 Feb 2024 18:34:53 GMT   (1723kb,D)

Title: Implicit Bias of Next-Token Prediction
Authors: Christos Thrampoulidis
Categories: cs.LG cs.CL stat.ML
\\
  Next-token prediction (NTP), the go-to training paradigm in training large
language models, involves predicting the next token in a sequence. Departing
from traditional one-hot classification, in NTP, multiple tokens with varying
frequencies follow each given context. This work frames NTP training as
cross-entropy minimization over distinct contexts, each associated with a
sparse empirical probability vector across a finite vocabulary. It then
addresses the following question: do gradient-based optimizers exhibit a bias
towards solutions with specific structure as the NTP training loss reaches its
lower bound (entropy)? Specifically, for linear NTP models trained using
gradient descent (GD), we make the following contributions: Firstly, we
determine NTP-separability conditions on the data, under which GD can attain
its lower bound. We also demonstrate that these conditions hold under
overparameterization. Secondly, we establish that the parameters of GD
projected onto an appropriate data subspace converge to the unique solution of
a system of linear equations, which requires the logits' difference of
in-support tokens to be equal to the log-ratio of their respective
probabilities. Meanwhile, on the orthogonal subspace, the parameters diverge
and converge in the direction of the solution of a max-margin quadratic
program, minimizing the Euclidean norm of parameters satisfying the
\NTP-separability conditions. Akin to prior research on implicit bias of
one-hot classification, our work opens exciting avenues for future research
that can lead to better understanding optimization, generalization and
robustness principles of models trained with NTP.
\\ ( https://arxiv.org/abs/2402.18551 ,  1723kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18563
Date: Wed, 28 Feb 2024 18:54:18 GMT   (3212kb,D)

Title: Approaching Human-Level Forecasting with Language Models
Authors: Danny Halawi, Fred Zhang, Chen Yueh-Han, Jacob Steinhardt
Categories: cs.LG cs.AI cs.CL cs.IR
\\
  Forecasting future events is important for policy and decision making. In
this work, we study whether language models (LMs) can forecast at the level of
competitive human forecasters. Towards this goal, we develop a
retrieval-augmented LM system designed to automatically search for relevant
information, generate forecasts, and aggregate predictions. To facilitate our
study, we collect a large dataset of questions from competitive forecasting
platforms. Under a test set published after the knowledge cut-offs of our LMs,
we evaluate the end-to-end performance of our system against the aggregates of
human forecasts. On average, the system nears the crowd aggregate of
competitive forecasters, and in some settings surpasses it. Our work suggests
that using LMs to forecast the future could provide accurate predictions at
scale and help to inform institutional decision making.
\\ ( https://arxiv.org/abs/2402.18563 ,  3212kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18567
Date: Wed, 28 Feb 2024 18:57:56 GMT   (24076kb,D)

Title: Diffusion Language Models Are Versatile Protein Learners
Authors: Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang,
  Quanquan Gu
Categories: cs.LG q-bio.BM
\\
  This paper introduces diffusion protein language model (DPLM), a versatile
protein language model that demonstrates strong generative and predictive
capabilities for protein sequences. We first pre-train scalable DPLMs from
evolutionary-scale protein sequences within a generative self-supervised
discrete diffusion probabilistic framework, which generalizes language modeling
for proteins in a principled way. After pre-training, DPLM exhibits the ability
to generate structurally plausible, novel, and diverse protein sequences for
unconditional generation. We further demonstrate the proposed diffusion
generative pre-training makes DPLM possess a better understanding of proteins,
making it a superior representation learner, which can be fine-tuned for
various predictive tasks, comparing favorably to ESM2 (Lin et al., 2022).
Moreover, DPLM can be tailored for various needs, which showcases its prowess
of conditional generation in several ways: (1) conditioning on partial peptide
sequences, e.g., generating scaffolds for functional motifs with high success
rate; (2) incorporating other modalities as conditioner, e.g.,
structure-conditioned generation for inverse folding; and (3) steering sequence
generation towards desired properties, e.g., satisfying specified secondary
structures, through a plug-and-play classifier guidance.
\\ ( https://arxiv.org/abs/2402.18567 ,  24076kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18571
Date: Wed, 28 Feb 2024 18:58:25 GMT   (320kb,D)

Title: Arithmetic Control of LLMs for Diverse User Preferences: Directional
  Preference Alignment with Multi-Objective Rewards
Authors: Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu,
  Han Zhao, Tong Zhang
Categories: cs.LG cs.AI cs.CL stat.ML
Comments: The code and model are released at
  https://github.com/Haoxiang-Wang/directional-preference-alignment
\\
  Fine-grained control over large language models (LLMs) remains a significant
challenge, hindering their adaptability to diverse user needs. While
Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning
LLMs, its reliance on scalar rewards often limits its ability to capture
diverse user preferences in real-world applications. To address this
limitation, we introduce the Directional Preference Alignment (DPA) framework.
Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling
to represent diverse preference profiles. Additionally, DPA models user
preferences as directions (i.e., unit vectors) in the reward space to achieve
user-dependent preference control. Our method involves training a
multi-objective reward model and then fine-tuning the LLM with a
preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF
method adopted by Llama 2. This method enjoys a better performance trade-off
across various reward objectives. In comparison with the scalar-reward RLHF,
DPA offers users intuitive control over LLM generation: they can arithmetically
specify their desired trade-offs (e.g., more helpfulness with less verbosity).
We also validate the effectiveness of DPA with real-world alignment experiments
on Mistral-7B. Our method provides straightforward arithmetic control over the
trade-off between helpfulness and verbosity while maintaining competitive
performance with strong baselines such as Direct Preference Optimization (DPO).
\\ ( https://arxiv.org/abs/2402.18571 ,  320kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2402.16795 (*cross-listing*)
Date: Mon, 26 Feb 2024 18:08:52 GMT   (23328kb,D)

Title: If in a Crowdsourced Data Annotation Pipeline, a GPT-4
Authors: Zeyu He, Chieh-Yang Huang, Chien-Kuang Cornelia Ding, Shaurya Rohatgi,
  Ting-Hao 'Kenneth' Huang
Categories: cs.HC cs.AI cs.CL cs.LG
Comments: Accepted By CHI 2024
DOI: 10.1145/3613904.3642834
\\
  Recent studies indicated GPT-4 outperforms online crowd workers in data
labeling accuracy, notably workers from Amazon Mechanical Turk (MTurk).
However, these studies were criticized for deviating from standard
crowdsourcing practices and emphasizing individual workers' performances over
the whole data-annotation process. This paper compared GPT-4 and an ethical and
well-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments
from 200 scholarly articles using the CODA-19 scheme. Two worker interfaces
yielded 127,080 labels, which were then used to infer the final labels through
eight label-aggregation algorithms. Our evaluation showed that despite best
practices, MTurk pipeline's highest accuracy was 81.5%, whereas GPT-4 achieved
83.6%. Interestingly, when combining GPT-4's labels with crowd labels collected
via an advanced worker interface for aggregation, 2 out of the 8 algorithms
achieved an even higher accuracy (87.5%, 87.0%). Further analysis suggested
that, when the crowd's and GPT-4's labeling strengths are complementary,
aggregating them could increase labeling accuracy.
\\ ( https://arxiv.org/abs/2402.16795 ,  23328kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17775 (*cross-listing*)
Date: Tue, 20 Feb 2024 11:36:23 GMT   (457kb,D)

Title: Wavelet Scattering Transform for Bioacustics: Application to Watkins
  Marine Mammal Sound Database
Authors: Davide Carbone (1 and 2) and Alessandro Licciardi (1 and 2) ((1)
  Politecnico di Torino, (2) Istituto Nazionale di Fisica Nucleare Sezione di
  Torino)
Categories: eess.SP cs.AI cs.CV cs.LG cs.SD eess.AS
\\
  Marine mammal communication is a complex field, hindered by the diversity of
vocalizations and environmental factors. The Watkins Marine Mammal Sound
Database (WMMD) is an extensive labeled dataset used in machine learning
applications. However, the methods for data preparation, preprocessing, and
classification found in the literature are quite disparate. This study first
focuses on a brief review of the state-of-the-art benchmarks on the dataset,
with an emphasis on clarifying data preparation and preprocessing methods.
Subsequently, we propose the application of the Wavelet Scattering Transform
(WST) in place of standard methods based on the Short-Time Fourier Transform
(STFT). The study also tackles a classification task using an ad-hoc deep
architecture with residual layers. We outperform the existing classification
architecture by $6\%$ in accuracy using WST and $8\%$ using Mel spectrogram
preprocessing, effectively reducing by half the number of misclassified
samples, and reaching a top accuracy of $96\%$.
\\ ( https://arxiv.org/abs/2402.17775 ,  457kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17778 (*cross-listing*)
Date: Thu, 22 Feb 2024 08:41:49 GMT   (2531kb,D)

Title: Dynamic Anchor Selection and Real-Time Pose Prediction for
  Ultra-wideband Tagless Gate
Authors: Junyoung Choi, Sagnik Bhattacharya, Joohyun Lee
Categories: eess.SP cs.AI cs.SY eess.SY
Comments: arXiv admin note: substantial text overlap with arXiv:2402.08399
\\
  Ultra-wideband (UWB) is emerging as a promising solution that can realize
proximity services, such as UWB tagless gate (UTG), thanks to centimeter-level
localization accuracy based on two different ranging methods such as downlink
time-difference of arrival (DL-TDoA) and double-sided two-way ranging (DS-TWR).
The UTG is a UWB-based proximity service that provides a seamless gate pass
system without requiring real-time mobile device (MD) tapping. The location of
MD is calculated using DL-TDoA, and the MD communicates with the nearest UTG
using DS-TWR to open the gate. Therefore, the knowledge about the exact
location of MD is the main challenge of UTG, and hence we provide the solutions
for both DL-TDoA and DS-TWR. In this paper, we propose dynamic anchor selection
for extremely accurate DL-TDoA localization and pose prediction for DS-TWR,
called DynaPose. The pose is defined as the actual location of MD on the human
body, which affects the localization accuracy. DynaPose is based on
line-of-sight (LOS) and non-LOS (NLOS) classification using deep learning for
anchor selection and pose prediction. Deep learning models use the UWB channel
impulse response and the inertial measurement unit embedded in the smartphone.
DynaPose is implemented on Samsung Galaxy Note20 Ultra and Qorvo UWB board to
show the feasibility and applicability. DynaPose achieves a LOS/NLOS
classification accuracy of 0.984, 62% higher DL-TDoA localization accuracy, and
ultimately detects four different poses with an accuracy of 0.961 in real-time.
\\ ( https://arxiv.org/abs/2402.17778 ,  2531kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17783 (*cross-listing*)
Date: Sat, 24 Feb 2024 04:13:48 GMT   (57kb,D)

Title: BagStacking: An Integrated Ensemble Learning Approach for Freezing of
  Gait Detection in Parkinson's Disease
Authors: Seffi Cohen, Lior Rokach
Categories: eess.SP cs.AI cs.LG
\\
  This paper introduces BagStacking, a novel ensemble learning method designed
to enhance the detection of Freezing of Gait (FOG) in Parkinson's Disease (PD)
by using a lower-back sensor to track acceleration. Building on the principles
of bagging and stacking, BagStacking aims to achieve the variance reduction
benefit of bagging's bootstrap sampling while also learning sophisticated
blending through stacking. The method involves training a set of base models on
bootstrap samples from the training data, followed by a meta-learner trained on
the base model outputs and true labels to find an optimal aggregation scheme.
The experimental evaluation demonstrates significant improvements over other
state-of-the-art machine learning methods on the validation set. Specifically,
BagStacking achieved a MAP score of 0.306, outperforming LightGBM (0.234) and
classic Stacking (0.286). Additionally, the run-time of BagStacking was
measured at 3828 seconds, illustrating an efficient approach compared to
Regular Stacking's 8350 seconds. BagStacking presents a promising direction for
handling the inherent variability in FOG detection data, offering a robust and
scalable solution to improve patient care in PD.
\\ ( https://arxiv.org/abs/2402.17783 ,  57kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17785 (*cross-listing*)
Date: Sat, 24 Feb 2024 04:35:07 GMT   (3702kb,D)

Title: ByteComposer: a Human-like Melody Composition Method based on Language
  Model Agent
Authors: Xia Liang, Jiaju Lin, Xinjian Du
Categories: cs.SD cs.AI eess.AS
\\
  Large Language Models (LLM) have shown encouraging progress in multimodal
understanding and generation tasks. However, how to design a human-aligned and
interpretable melody composition system is still under-explored. To solve this
problem, we propose ByteComposer, an agent framework emulating a human's
creative pipeline in four separate steps : "Conception Analysis - Draft
Composition - Self-Evaluation and Modification - Aesthetic Selection". This
framework seamlessly blends the interactive and knowledge-understanding
features of LLMs with existing symbolic music generation models, thereby
achieving a melody composition agent comparable to human creators. We conduct
extensive experiments on GPT4 and several open-source large language models,
which substantiate our framework's effectiveness. Furthermore, professional
music composers were engaged in multi-dimensional evaluations, the final
results demonstrated that across various facets of music composition,
ByteComposer agent attains the level of a novice melody composer.
\\ ( https://arxiv.org/abs/2402.17785 ,  3702kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17792 (*cross-listing*)
Date: Mon, 26 Feb 2024 15:11:41 GMT   (674kb,D)

Title: EGNN-C+: Interpretable Evolving Granular Neural Network and Application
  in Classification of Weakly-Supervised EEG Data Streams
Authors: Daniel Leite, Alisson Silva, Gabriella Casalino, Arnab Sharma,
  Danielle Fortunato, Axel-Cyrille Ngomo
Categories: eess.SP cs.AI cs.LG cs.NE
Comments: 10 pages, IEEE International Conference on Evolving and Adaptive
  Intelligent Systems 2024 (IEEE EAIS 2024)
\\
  We introduce a modified incremental learning algorithm for evolving Granular
Neural Network Classifiers (eGNN-C+). We use double-boundary hyper-boxes to
represent granules, and customize the adaptation procedures to enhance the
robustness of outer boxes for data coverage and noise suppression, while
ensuring that inner boxes remain flexible to capture drifts. The classifier
evolves from scratch, incorporates new classes on the fly, and performs local
incremental feature weighting. As an application, we focus on the
classification of emotion-related patterns within electroencephalogram (EEG)
signals. Emotion recognition is crucial for enhancing the realism and
interactivity of computer systems. We extract features from the Fourier
spectrum of EEG signals obtained from 28 individuals engaged in playing
computer games -- a public dataset. Each game elicits a different predominant
emotion: boredom, calmness, horror, or joy. We analyze individual electrodes,
time window lengths, and frequency bands to assess the accuracy and
interpretability of resulting user-independent neural models. The findings
indicate that both brain hemispheres assist classification, especially
electrodes on the temporal (T8) and parietal (P7) areas, alongside
contributions from frontal and occipital electrodes. While patterns may
manifest in any band, the Alpha (8-13Hz), Delta (1-4Hz), and Theta (4-8Hz)
bands, in this order, exhibited higher correspondence with the emotion classes.
The eGNN-C+ demonstrates effectiveness in learning EEG data. It achieves an
accuracy of 81.7% and a 0.0029 II interpretability using 10-second time
windows, even in face of a highly-stochastic time-varying 4-class
classification problem.
\\ ( https://arxiv.org/abs/2402.17792 ,  674kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17801 (*cross-listing*)
Date: Tue, 27 Feb 2024 07:12:48 GMT   (390kb,D)

Title: Generative AI and Copyright: A Dynamic Perspective
Authors: S. Alex Yang and Angela Huyue Zhang
Categories: econ.TH cs.AI
\\
  The rapid advancement of generative AI is poised to disrupt the creative
industry. Amidst the immense excitement for this new technology, its future
development and applications in the creative industry hinge crucially upon two
copyright issues: 1) the compensation to creators whose content has been used
to train generative AI models (the fair use standard); and 2) the eligibility
of AI-generated content for copyright protection (AI-copyrightability). While
both issues have ignited heated debates among academics and practitioners, most
analysis has focused on their challenges posed to existing copyright doctrines.
In this paper, we aim to better understand the economic implications of these
two regulatory issues and their interactions. By constructing a dynamic model
with endogenous content creation and AI model development, we unravel the
impacts of the fair use standard and AI-copyrightability on AI development, AI
company profit, creators income, and consumer welfare, and how these impacts
are influenced by various economic and operational factors. For example, while
generous fair use (use data for AI training without compensating the creator)
benefits all parties when abundant training data exists, it can hurt creators
and consumers when such data is scarce. Similarly, stronger AI-copyrightability
(AI content enjoys more copyright protection) could hinder AI development and
reduce social welfare. Our analysis also highlights the complex interplay
between these two copyright issues. For instance, when existing training data
is scarce, generous fair use may be preferred only when AI-copyrightability is
weak. Our findings underscore the need for policymakers to embrace a dynamic,
context-specific approach in making regulatory decisions and provide insights
for business leaders navigating the complexities of the global regulatory
environment.
\\ ( https://arxiv.org/abs/2402.17801 ,  390kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17810 (*cross-listing*)
Date: Tue, 27 Feb 2024 12:43:09 GMT   (2246kb,D)

Title: BioT5+: Towards Generalized Biological Understanding with IUPAC
  Integration and Multi-task Tuning
Authors: Qizhi Pei, Lijun Wu, Kaiyuan Gao, Xiaozhuan Liang, Yin Fang, Jinhua
  Zhu, Shufang Xie, Tao Qin, Rui Yan
Categories: q-bio.QM cs.AI cs.CE cs.LG
Comments: 24 pages
\\
  Recent research trends in computational biology have increasingly focused on
integrating text and bio-entity modeling, especially in the context of
molecules and proteins. However, previous efforts like BioT5 faced challenges
in generalizing across diverse tasks and lacked a nuanced understanding of
molecular structures, particularly in their textual representations (e.g.,
IUPAC). This paper introduces BioT5+, an extension of the BioT5 framework,
tailored to enhance biological research and drug discovery. BioT5+ incorporates
several novel features: integration of IUPAC names for molecular understanding,
inclusion of extensive bio-text and molecule data from sources like bioRxiv and
PubChem, the multi-task instruction tuning for generality across tasks, and a
novel numerical tokenization technique for improved processing of numerical
data. These enhancements allow BioT5+ to bridge the gap between molecular
representations and their textual descriptions, providing a more holistic
understanding of biological entities, and largely improving the grounded
reasoning of bio-text and bio-sequences. The model is pre-trained and
fine-tuned with a large number of experiments, including \emph{3 types of
problems (classification, regression, generation), 15 kinds of tasks, and 21
total benchmark datasets}, demonstrating the remarkable performance and
state-of-the-art results in most cases. BioT5+ stands out for its ability to
capture intricate relationships in biological data, thereby contributing
significantly to bioinformatics and computational biology. Our code is
available at \url{https://github.com/QizhiPei/BioT5}.
\\ ( https://arxiv.org/abs/2402.17810 ,  2246kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17862 (*cross-listing*)
Date: Tue, 27 Feb 2024 19:54:30 GMT   (957kb,D)

Title: REPrune: Channel Pruning via Kernel Representative Selection
Authors: Mincheol Park, Dongjin Kim, Cheonjun Park, Yuna Park, Gyeong Eun Gong,
  Won Woo Ro, Suhyun Kim
Categories: cs.CV cs.AI
Comments: Published at AAAI2024
\\
  Channel pruning is widely accepted to accelerate modern convolutional neural
networks (CNNs). The resulting pruned model benefits from its immediate
deployment on general-purpose software and hardware resources. However, its
large pruning granularity, specifically at the unit of a convolution filter,
often leads to undesirable accuracy drops due to the inflexibility of deciding
how and where to introduce sparsity to the CNNs. In this paper, we propose
REPrune, a novel channel pruning technique that emulates kernel pruning, fully
exploiting the finer but structured granularity. REPrune identifies similar
kernels within each channel using agglomerative clustering. Then, it selects
filters that maximize the incorporation of kernel representatives while
optimizing the maximum cluster coverage problem. By integrating with a
simultaneous training-pruning paradigm, REPrune promotes efficient, progressive
pruning throughout training CNNs, avoiding the conventional
train-prune-finetune sequence. Experimental results highlight that REPrune
performs better in computer vision tasks than existing methods, effectively
achieving a balance between acceleration ratio and performance retention.
\\ ( https://arxiv.org/abs/2402.17862 ,  957kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17913 (*cross-listing*)
Date: Tue, 27 Feb 2024 22:00:50 GMT   (7814kb,D)

Title: Using AI libraries for Incompressible Computational Fluid Dynamics
Authors: Boyang Chen, Claire E. Heaney and Christopher C. Pain
Categories: physics.flu-dyn cs.AI cs.LG
Comments: 24 pages, 6 figures
\\
  Recently, there has been a huge effort focused on developing highly efficient
open source libraries to perform Artificial Intelligence (AI) related
computations on different computer architectures (for example, CPUs, GPUs and
new AI processors). This has not only made the algorithms based on these
libraries highly efficient and portable between different architectures, but
also has substantially simplified the entry barrier to develop methods using
AI. Here, we present a novel methodology to bring the power of both AI software
and hardware into the field of numerical modelling by repurposing AI methods,
such as Convolutional Neural Networks (CNNs), for the standard operations
required in the field of the numerical solution of Partial Differential
Equations (PDEs). The aim of this work is to bring the high performance,
architecture agnosticism and ease of use into the field of the numerical
solution of PDEs. We use the proposed methodology to solve the
advection-diffusion equation, the non-linear Burgers equation and
incompressible flow past a bluff body. For the latter, a convolutional neural
network is used as a multigrid solver in order to enforce the incompressibility
constraint. We show that the presented methodology can solve all these problems
using repurposed AI libraries in an efficient way, and presents a new avenue to
explore in the development of methods to solve PDEs and Computational Fluid
Dynamics problems with implicit methods.
\\ ( https://arxiv.org/abs/2402.17913 ,  7814kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17969 (*cross-listing*)
Date: Wed, 28 Feb 2024 01:29:36 GMT   (1258kb,D)

Title: Vision Language Model-based Caption Evaluation Method Leveraging Visual
  Context Extraction
Authors: Koki Maeda, Shuhei Kurita, Taiki Miyanishi, Naoaki Okazaki
Categories: cs.CV cs.AI
\\
  Given the accelerating progress of vision and language modeling, accurate
evaluation of machine-generated image captions remains critical. In order to
evaluate captions more closely to human preferences, metrics need to
discriminate between captions of varying quality and content. However,
conventional metrics fail short of comparing beyond superficial matches of
words or embedding similarities; thus, they still need improvement. This paper
presents VisCE$^2$, a vision language model-based caption evaluation method.
Our method focuses on visual context, which refers to the detailed content of
images, including objects, attributes, and relationships. By extracting and
organizing them into a structured format, we replace the human-written
references with visual contexts and help VLMs better understand the image,
enhancing evaluation performance. Through meta-evaluation on multiple datasets,
we validated that VisCE$^2$ outperforms the conventional pre-trained metrics in
capturing caption quality and demonstrates superior consistency with human
judgment.
\\ ( https://arxiv.org/abs/2402.17969 ,  1258kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17971 (*cross-listing*)
Date: Wed, 28 Feb 2024 01:32:59 GMT   (15330kb,D)

Title: All in a Single Image: Large Multimodal Models are In-Image Learners
Authors: Lei Wang, Wanyu Xu, Zhiqiang Hu, Yihuai Lan, Shan Dong, Hao Wang, Roy
  Ka-Wei Lee, Ee-Peng Lim
Categories: cs.CV cs.AI cs.CL
Comments: WIP
\\
  This paper introduces a new in-context learning (ICL) mechanism called
In-Image Learning (I$^2$L) that combines demonstration examples, visual cues,
and instructions into a single image to enhance the capabilities of GPT-4V.
Unlike previous approaches that rely on converting images to text or
incorporating visual input into language models, I$^2$L consolidates all
information into one image and primarily leverages image processing,
understanding, and reasoning abilities. This has several advantages: it avoids
inaccurate textual descriptions of complex images, provides flexibility in
positioning demonstration examples, reduces the input burden, and avoids
exceeding input limits by eliminating the need for multiple images and lengthy
text. To further combine the strengths of different ICL methods, we introduce
an automatic strategy to select the appropriate ICL method for a data example
in a given task. We conducted experiments on MathVista and Hallusionbench to
test the effectiveness of I$^2$L in complex multimodal reasoning tasks and
mitigating language hallucination and visual illusion. Additionally, we
explored the impact of image resolution, the number of demonstration examples,
and their positions on the effectiveness of I$^2$L. Our code is publicly
available at https://github.com/AGI-Edgerunners/IIL.
\\ ( https://arxiv.org/abs/2402.17971 ,  15330kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17979 (*cross-listing*)
Date: Wed, 28 Feb 2024 01:48:54 GMT   (1625kb,D)

Title: Ensemble Methodology:Innovations in Credit Default Prediction Using
  LightGBM, XGBoost, and LocalEnsemble
Authors: Mengran Zhu, Ye Zhang, Yulu Gong, Kaijuan Xing, Xu Yan, Jintong Song
Categories: cs.CE cs.AI cs.LG
\\
  In the realm of consumer lending, accurate credit default prediction stands
as a critical element in risk mitigation and lending decision optimization.
Extensive research has sought continuous improvement in existing models to
enhance customer experiences and ensure the sound economic functioning of
lending institutions. This study responds to the evolving landscape of credit
default prediction, challenging conventional models and introducing innovative
approaches. By building upon foundational research and recent innovations, our
work aims to redefine the standards of accuracy in credit default prediction,
setting a new benchmark for the industry. To overcome these challenges, we
present an Ensemble Methods framework comprising LightGBM, XGBoost, and
LocalEnsemble modules, each making unique contributions to amplify diversity
and improve generalization. By utilizing distinct feature sets, our methodology
directly tackles limitations identified in previous studies, with the
overarching goal of establishing a novel standard for credit default prediction
accuracy. Our experimental findings validate the effectiveness of the ensemble
model on the dataset, signifying substantial contributions to the field. This
innovative approach not only addresses existing obstacles but also sets a
precedent for advancing the accuracy and robustness of credit default
prediction models.
\\ ( https://arxiv.org/abs/2402.17979 ,  1625kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18002 (*cross-listing*)
Date: Wed, 28 Feb 2024 02:30:59 GMT   (6132kb,D)

Title: Symmetry-aware Reinforcement Learning for Robotic Assembly under Partial
  Observability with a Soft Wrist
Authors: Hai Nguyen, Tadashi Kozuno, Cristian C. Beltran-Hernandez, Masashi
  Hamaya
Categories: cs.RO cs.AI cs.LG
Comments: Accepted at ICRA-2024
\\
  This study tackles the representative yet challenging contact-rich
peg-in-hole task of robotic assembly, using a soft wrist that can operate more
safely and tolerate lower-frequency control signals than a rigid one. Previous
studies often use a fully observable formulation, requiring external setups or
estimators for the peg-to-hole pose. In contrast, we use a partially observable
formulation and deep reinforcement learning from demonstrations to learn a
memory-based agent that acts purely on haptic and proprioceptive signals.
Moreover, previous works do not incorporate potential domain symmetry and thus
must search for solutions in a bigger space. Instead, we propose to leverage
the symmetry for sample efficiency by augmenting the training data and
constructing auxiliary losses to force the agent to adhere to the symmetry.
Results in simulation with five different symmetric peg shapes show that our
proposed agent can be comparable to or even outperform a state-based agent. In
particular, the sample efficiency also allows us to learn directly on the real
robot within 3 hours.
\\ ( https://arxiv.org/abs/2402.18002 ,  6132kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18016 (*cross-listing*)
Date: Wed, 28 Feb 2024 03:21:25 GMT   (1234kb,D)

Title: Dynamic Explanation Selection Towards Successful User-Decision Support
  with Explainable AI
Authors: Yosuke Fukuchi and Seiji Yamada
Categories: cs.HC cs.AI
\\
  This paper addresses the problem of how to select explanations for XAI
(Explainable AI)-based Intelligent Decision Support Systems (IDSSs). IDSSs have
shown promise in improving user decisions through XAI-generated explanations
along with AI predictions. As the development of XAI made various explanations
available, we believe that IDSSs can be greatly improved if they can
strategically select explanations that guide users to better decisions. This
paper proposes X-Selector, a method for dynamically selecting explanations.
X-Selector aims to guide users to better decisions by predicting the impact of
different combinations of explanations on user decisions. We compared
X-Selector's performance with two naive strategies (all possible explanations
and explanations only for the most likely prediction) and two baselines (no
explanation and no AI support). The results suggest the potential of X-Selector
to guide users to recommended decisions and improve the performance when AI
accuracy is high and a challenge when it is low.
\\ ( https://arxiv.org/abs/2402.18016 ,  1234kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18062 (*cross-listing*)
Date: Wed, 28 Feb 2024 05:46:23 GMT   (9976kb,D)

Title: Generative AI for Unmanned Vehicle Swarms: Challenges, Applications and
  Opportunities
Authors: Guangyuan Liu, Nguyen Van Huynh, Hongyang Du, Dinh Thai Hoang, Dusit
  Niyato, Kun Zhu, Jiawen Kang, Zehui Xiong, Abbas Jamalipour, Dong In Kim
Categories: cs.RO cs.AI
Comments: 23 pages
\\
  With recent advances in artificial intelligence (AI) and robotics, unmanned
vehicle swarms have received great attention from both academia and industry
due to their potential to provide services that are difficult and dangerous to
perform by humans. However, learning and coordinating movements and actions for
a large number of unmanned vehicles in complex and dynamic environments
introduce significant challenges to conventional AI methods. Generative AI
(GAI), with its capabilities in complex data feature extraction,
transformation, and enhancement, offers great potential in solving these
challenges of unmanned vehicle swarms. For that, this paper aims to provide a
comprehensive survey on applications, challenges, and opportunities of GAI in
unmanned vehicle swarms. Specifically, we first present an overview of unmanned
vehicles and unmanned vehicle swarms as well as their use cases and existing
issues. Then, an in-depth background of various GAI techniques together with
their capabilities in enhancing unmanned vehicle swarms are provided. After
that, we present a comprehensive review on the applications and challenges of
GAI in unmanned vehicle swarms with various insights and discussions. Finally,
we highlight open issues of GAI in unmanned vehicle swarms and discuss
potential research directions.
\\ ( https://arxiv.org/abs/2402.18062 ,  9976kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18091 (*cross-listing*)
Date: Wed, 28 Feb 2024 06:24:39 GMT   (16896kb,D)

Title: Polos: Multimodal Metric Learning from Human Feedback for Image
  Captioning
Authors: Yuiga Wada, Kanta Kaneda, Daichi Saito, Komei Sugiura
Categories: cs.CV cs.AI cs.CL
Comments: CVPR 2024
\\
  Establishing an automatic evaluation metric that closely aligns with human
judgments is essential for effectively developing image captioning models.
Recent data-driven metrics have demonstrated a stronger correlation with human
judgments than classic metrics such as CIDEr; however they lack sufficient
capabilities to handle hallucinations and generalize across diverse images and
texts partially because they compute scalar similarities merely using
embeddings learned from tasks unrelated to image captioning evaluation. In this
study, we propose Polos, a supervised automatic evaluation metric for image
captioning models. Polos computes scores from multimodal inputs, using a
parallel feature extraction mechanism that leverages embeddings trained through
large-scale contrastive learning. To train Polos, we introduce Multimodal
Metric Learning from Human Feedback (M$^2$LHF), a framework for developing
metrics based on human feedback. We constructed the Polaris dataset, which
comprises 131K human judgments from 550 evaluators, which is approximately ten
times larger than standard datasets. Our approach achieved state-of-the-art
performance on Composite, Flickr8K-Expert, Flickr8K-CF, PASCAL-50S, FOIL, and
the Polaris dataset, thereby demonstrating its effectiveness and robustness.
\\ ( https://arxiv.org/abs/2402.18091 ,  16896kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18104 (*cross-listing*)
Date: Wed, 28 Feb 2024 06:50:14 GMT   (4185kb,D)

Title: Making Them Ask and Answer: Jailbreaking Large Language Models in Few
  Queries via Disguise and Reconstruction
Authors: Tong Liu, Yingjie Zhang, Zhe Zhao, Yinpeng Dong, Guozhu Meng, Kai Chen
Categories: cs.CR cs.AI
\\
  In recent years, large language models (LLMs) have demonstrated notable
success across various tasks, but the trustworthiness of LLMs is still an open
problem. One specific threat is the potential to generate toxic or harmful
responses. Attackers can craft adversarial prompts that induce harmful
responses from LLMs. In this work, we pioneer a theoretical foundation in LLMs
security by identifying bias vulnerabilities within the safety fine-tuning and
design a black-box jailbreak method named DRA (Disguise and Reconstruction
Attack), which conceals harmful instructions through disguise and prompts the
model to reconstruct the original harmful instruction within its completion. We
evaluate DRA across various open-source and close-source models, showcasing
state-of-the-art jailbreak success rates and attack efficiency. Notably, DRA
boasts a 90\% attack success rate on LLM chatbots GPT-4.
\\ ( https://arxiv.org/abs/2402.18104 ,  4185kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18137 (*cross-listing*)
Date: Wed, 28 Feb 2024 07:58:24 GMT   (13540kb,D)

Title: DecisionNCE: Embodied Multimodal Representations via Implicit Preference
  Learning
Authors: Jianxiong Li, Jinliang Zheng, Yinan Zheng, Liyuan Mao, Xiao Hu, Sijie
  Cheng, Haoyi Niu, Jihao Liu, Yu Liu, Jingjing Liu, Ya-Qin Zhang, Xianyuan
  Zhan
Categories: cs.RO cs.AI cs.CL cs.CV cs.LG
Comments: Preprint. 27 pages, 19 figures
\\
  Multimodal pretraining has emerged as an effective strategy for the trinity
of goals of representation learning in autonomous robots: 1) extracting both
local and global task progression information; 2) enforcing temporal
consistency of visual representation; 3) capturing trajectory-level language
grounding. Most existing methods approach these via separate objectives, which
often reach sub-optimal solutions. In this paper, we propose a universal
unified objective that can simultaneously extract meaningful task progression
information from image sequences and seamlessly align them with language
instructions. We discover that via implicit preferences, where a visual
trajectory inherently aligns better with its corresponding language instruction
than mismatched pairs, the popular Bradley-Terry model can transform into
representation learning through proper reward reparameterizations. The resulted
framework, DecisionNCE, mirrors an InfoNCE-style objective but is distinctively
tailored for decision-making tasks, providing an embodied representation
learning framework that elegantly extracts both local and global task
progression features, with temporal consistency enforced through implicit time
contrastive learning, while ensuring trajectory-level instruction grounding via
multimodal joint encoding. Evaluation on both simulated and real robots
demonstrates that DecisionNCE effectively facilitates diverse downstream policy
learning tasks, offering a versatile solution for unified representation and
reward learning. Project Page: https://2toinf.github.io/DecisionNCE/
\\ ( https://arxiv.org/abs/2402.18137 ,  13540kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18152 (*cross-listing*)
Date: Wed, 28 Feb 2024 08:32:19 GMT   (46581kb,D)

Title: Boosting Neural Representations for Videos with a Conditional Decoder
Authors: Xinjie Zhang, Ren Yang, Dailan He, Xingtong Ge, Tongda Xu, Yan Wang,
  Hongwei Qin, Jun Zhang
Categories: eess.IV cs.AI cs.CV
Comments: Accept by CVPR 2024
\\
  Implicit neural representations (INRs) have emerged as a promising approach
for video storage and processing, showing remarkable versatility across various
video tasks. However, existing methods often fail to fully leverage their
representation capabilities, primarily due to inadequate alignment of
intermediate features during target frame decoding. This paper introduces a
universal boosting framework for current implicit video representation
approaches. Specifically, we utilize a conditional decoder with a
temporal-aware affine transform module, which uses the frame index as a prior
condition to effectively align intermediate features with target frames.
Besides, we introduce a sinusoidal NeRV-like block to generate diverse
intermediate features and achieve a more balanced parameter distribution,
thereby enhancing the model's capacity. With a high-frequency
information-preserving reconstruction loss, our approach successfully boosts
multiple baseline INRs in the reconstruction quality and convergence speed for
video regression, and exhibits superior inpainting and interpolation results.
Further, we integrate a consistent entropy minimization technique and develop
video codecs based on these boosted INRs. Experiments on the UVG dataset
confirm that our enhanced codecs significantly outperform baseline INRs and
offer competitive rate-distortion performance compared to traditional and
learning-based codecs.
\\ ( https://arxiv.org/abs/2402.18152 ,  46581kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18205 (*cross-listing*)
Date: Wed, 28 Feb 2024 09:51:55 GMT   (868kb,D)

Title: Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging
Authors: Hongcheng Guo, Wei Zhang, Anjie Le, Jian Yang, Jiaheng Liu, Zhoujun
  Li, Tieqiao Zheng, Shi Xu, Runqiang Zang, Liangfan Zheng, Bo Zhang
Categories: cs.SE cs.AI
Comments: 7 pages
\\
  Logs produced by extensive software systems are integral to monitoring system
behaviors. Advanced log analysis facilitates the detection, alerting, and
diagnosis of system faults. Log parsing, which entails transforming raw log
messages into structured templates, constitutes a critical phase in the
automation of log analytics. Existing log parsers fail to identify the correct
templates due to reliance on human-made rules. Besides, These methods focus on
statistical features while ignoring semantic information in log messages. To
address these challenges, we introduce a cutting-edge \textbf{L}og parsing
framework with \textbf{E}ntropy sampling and Chain-of-Thought \textbf{M}erging
(Lemur). Specifically, to discard the tedious manual rules. We propose a novel
sampling method inspired by information entropy, which efficiently clusters
typical logs. Furthermore, to enhance the merging of log templates, we design a
chain-of-thought method for large language models (LLMs). LLMs exhibit
exceptional semantic comprehension, deftly distinguishing between parameters
and invariant tokens. We have conducted experiments on large-scale public
datasets. Extensive evaluation demonstrates that Lemur achieves the
state-of-the-art performance and impressive efficiency.
\\ ( https://arxiv.org/abs/2402.18205 ,  868kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18222 (*cross-listing*)
Date: Wed, 28 Feb 2024 10:37:14 GMT   (10558kb,D)

Title: HearHere: Mitigating Echo Chambers in News Consumption through an
  AI-based Web System
Authors: Youngseung Jeon, Jaehoon Kim, Sohyun Park, Yunyong Ko, Seongeun Ryu,
  Sang-Wook Kim, Kyungsik Han
Categories: cs.HC cs.AI
Comments: 30 pages, 6 figures, 6 tables, CSCW 2024
\\
  Considerable efforts are currently underway to mitigate the negative impacts
of echo chambers, such as increased susceptibility to fake news and resistance
towards accepting scientific evidence. Prior research has presented the
development of computer systems that support the consumption of news
information from diverse political perspectives to mitigate the echo chamber
effect. However, existing studies still lack the ability to effectively support
the key processes of news information consumption and quantitatively identify a
political stance towards the information. In this paper, we present HearHere,
an AI-based web system designed to help users accommodate information and
opinions from diverse perspectives. HearHere facilitates the key processes of
news information consumption through two visualizations. Visualization 1
provides political news with quantitative political stance information, derived
from our graph-based political classification model, and users can experience
diverse perspectives (Hear). Visualization 2 allows users to express their
opinions on specific political issues in a comment form and observe the
position of their own opinions relative to pro-liberal and pro-conservative
comments presented on a map interface (Here). Through a user study with 94
participants, we demonstrate the feasibility of HearHere in supporting the
consumption of information from various perspectives. Our findings highlight
the importance of providing political stance information and quantifying users'
political status as a means to mitigate political polarization. In addition, we
propose design implications for system development, including the consideration
of demographics such as political interest and providing users with
initiatives.
\\ ( https://arxiv.org/abs/2402.18222 ,  10558kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18286 (*cross-listing*)
Date: Wed, 28 Feb 2024 12:25:01 GMT   (12834kb,D)

Title: Self-Supervised Learning in Electron Microscopy: Towards a Foundation
  Model for Advanced Image Analysis
Authors: Bashir Kazimi and Karina Ruzaeva and Stefan Sandfeld
Categories: cs.CV cond-mat.mtrl-sci cs.AI cs.LG
\\
  In this work, we explore the potential of self-supervised learning from
unlabeled electron microscopy datasets, taking a step toward building a
foundation model in this field. We show how self-supervised pretraining
facilitates efficient fine-tuning for a spectrum of downstream tasks, including
semantic segmentation, denoising, noise & background removal, and
super-resolution. Experimentation with varying model complexities and receptive
field sizes reveals the remarkable phenomenon that fine-tuned models of lower
complexity consistently outperform more complex models with random weight
initialization. We demonstrate the versatility of self-supervised pretraining
across various downstream tasks in the context of electron microscopy, allowing
faster convergence and better performance. We conclude that self-supervised
pretraining serves as a powerful catalyst, being especially advantageous when
limited annotated data are available and efficient scaling of computational
cost are important.
\\ ( https://arxiv.org/abs/2402.18286 ,  12834kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18292 (*cross-listing*)
Date: Wed, 28 Feb 2024 12:37:30 GMT   (1944kb,D)

Title: FSL Model can Score Higher as It Is
Authors: Yunwei Bai, Ying Kiat Tan, Tsuhan Chen
Categories: cs.CV cs.AI cs.LG
\\
  In daily life, we tend to present the front of our faces by staring squarely
at a facial recognition machine, instead of facing it sideways, in order to
increase the chance of being correctly recognised. Few-shot-learning (FSL)
classification is challenging in itself because a model has to identify images
that belong to classes previously unseen during training. Therefore, a warped
and non-typical query or support image during testing can make it even more
challenging for a model to predict correctly. In our work, to increase the
chance of correct prediction during testing, we aim to rectify the test input
of a trained FSL model by generating new samples of the tested classes through
image-to-image translation. An FSL model is usually trained on classes with
sufficient samples, and then tested on classes with few-shot samples. Our
proposed method first captures the style or shape of the test image, and then
identifies a suitable trained class sample. It then transfers the style or
shape of the test image to the train-class images for generation of more
test-class samples, before performing classification based on a set of
generated samples instead of just one sample. Our method has potential in
empowering a trained FSL model to score higher during the testing phase without
any extra training nor dataset. According to our experiments, by augmenting the
support set with just 1 additional generated sample, we can achieve around 2%
improvement for trained FSL models on datasets consisting of either animal
faces or traffic signs. By augmenting both the support set and the queries, we
can achieve even more performance improvement. Our Github Repository is
publicly available.
\\ ( https://arxiv.org/abs/2402.18292 ,  1944kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18309 (*cross-listing*)
Date: Wed, 28 Feb 2024 13:08:46 GMT   (9987kb,D)

Title: Enhancing Roadway Safety: LiDAR-based Tree Clearance Analysis
Authors: Miriam Louise Carnot, Eric Peukert, Bogdan Franczyk
Categories: cs.CV cs.AI cs.CG
\\
  In the efforts for safer roads, ensuring adequate vertical clearance above
roadways is of great importance. Frequently, trees or other vegetation is
growing above the roads, blocking the sight of traffic signs and lights and
posing danger to traffic participants. Accurately estimating this space from
simple images proves challenging due to a lack of depth information. This is
where LiDAR technology comes into play, a laser scanning sensor that reveals a
three-dimensional perspective. Thus far, LiDAR point clouds at the street level
have mainly been used for applications in the field of autonomous driving.
These scans, however, also open up possibilities in urban management. In this
paper, we present a new point cloud algorithm that can automatically detect
those parts of the trees that grow over the street and need to be trimmed. Our
system uses semantic segmentation to filter relevant points and downstream
processing steps to create the required volume to be kept clear above the road.
Challenges include obscured stretches of road, the noisy unstructured nature of
LiDAR point clouds, and the assessment of the road shape. The identified points
of non-compliant trees can be projected from the point cloud onto images,
providing municipalities with a visual aid for dealing with such occurrences.
By automating this process, municipalities can address potential road space
constraints, enhancing safety for all. They may also save valuable time by
carrying out the inspections more systematically. Our open-source code gives
communities inspiration on how to automate the process themselves.
\\ ( https://arxiv.org/abs/2402.18309 ,  9987kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18320 (*cross-listing*)
Date: Wed, 28 Feb 2024 13:33:43 GMT   (14617kb,D)

Title: Location-guided Head Pose Estimation for Fisheye Image
Authors: Bing Li, Dong Zhang, Cheng Huang, Yun Xian, Ming Li, and Dah-Jye Lee
Categories: cs.CV cs.AI
\\
  Camera with a fisheye or ultra-wide lens covers a wide field of view that
cannot be modeled by the perspective projection. Serious fisheye
\textcolor{blue}{lens} distortion in the peripheral region of the image leads
to degraded performance of the \textcolor{blue}{existing} head pose estimation
models trained on undistorted images. This paper presents a new approach for
head pose estimation that uses the knowledge of head location in the image to
reduce the negative effect of fisheye distortion. We develop an end-to-end
convolutional neural network to estimate the head pose with the multi-task
learning of head pose and head location. Our proposed network estimates the
head pose directly from the fisheye image without the operation of
rectification or calibration. We also created \textcolor{blue}{a}
fisheye-\textcolor{blue}{distorted} version of the three popular head pose
estimation datasets, BIWI, 300W-LP, and AFLW2000 for our experiments.
Experiments results show that our network remarkably improves the accuracy of
head pose estimation compared with other state-of-the-art one-stage and
two-stage methods.
\\ ( https://arxiv.org/abs/2402.18320 ,  14617kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18326 (*cross-listing*)
Date: Wed, 28 Feb 2024 13:48:44 GMT   (719kb,D)

Title: When Should Algorithms Resign?
Authors: Umang Bhatt and Holli Sargeant
Categories: cs.CY cs.AI
\\
  This paper discusses algorithmic resignation, a strategic approach for
managing the use of AI systems within organizations. Algorithmic resignation
involves the deliberate and informed disengagement from AI assistance in
certain scenarios, by embedding governance mechanisms directly into AI systems.
Our proposal is not merely about disuse of AI but includes guiding when and how
these systems should be used or avoided. We discuss the multifaceted benefits
of algorithmic resignation, spanning economic efficiency, reputational gains,
and legal compliance. Further, we outline the operationalization of resignation
through various methods such as positive and negative nudges, stakeholder
incentive alignment, and careful consideration of the level of AI engagement.
Using techniques like barring access to AI outputs selectively or providing
explicit disclaimers on system performance, algorithmic resignation not only
mitigates risks associated with AI but also leverages its benefits, ensuring
the responsible and effective use of AI systems.
\\ ( https://arxiv.org/abs/2402.18326 ,  719kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18360 (*cross-listing*)
Date: Wed, 28 Feb 2024 14:31:34 GMT   (23kb)

Title: Similarity-based analogical proportions
Authors: Christian Anti\'c
Categories: cs.LO cs.AI math.LO
\\
  The author has recently introduced abstract algebraic frameworks of
analogical proportions and similarity within the general setting of universal
algebra. The purpose of this paper is to build a bridge from similarity to
analogical proportions by formulating the latter in terms of the former. The
benefit of this similarity-based approach is that the connection between
proportions and similarity is built into the framework and therefore evident
which is appealing since proportions and similarity are both at the center of
analogy; moreover, future results on similarity can directly be applied to
analogical proportions.
\\ ( https://arxiv.org/abs/2402.18360 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18362 (*cross-listing*)
Date: Wed, 28 Feb 2024 14:33:14 GMT   (13272kb,D)

Title: Objective and Interpretable Breast Cosmesis Evaluation with Attention
  Guided Denoising Diffusion Anomaly Detection Model
Authors: Sangjoon Park, Yong Bae Kim, Jee Suk Chang, Seo Hee Choi, Hyungjin
  Chung, Ik Jae Lee, Hwa Kyung Byun
Categories: cs.CV cs.AI
\\
  As advancements in the field of breast cancer treatment continue to progress,
the assessment of post-surgical cosmetic outcomes has gained increasing
significance due to its substantial impact on patients' quality of life.
However, evaluating breast cosmesis presents challenges due to the inherently
subjective nature of expert labeling. In this study, we present a novel
automated approach, Attention-Guided Denoising Diffusion Anomaly Detection
(AG-DDAD), designed to assess breast cosmesis following surgery, addressing the
limitations of conventional supervised learning and existing anomaly detection
models. Our approach leverages the attention mechanism of the distillation with
no label (DINO) self-supervised Vision Transformer (ViT) in combination with a
diffusion model to achieve high-quality image reconstruction and precise
transformation of discriminative regions. By training the diffusion model on
unlabeled data predominantly with normal cosmesis, we adopt an unsupervised
anomaly detection perspective to automatically score the cosmesis. Real-world
data experiments demonstrate the effectiveness of our method, providing
visually appealing representations and quantifiable scores for cosmesis
evaluation. Compared to commonly used rule-based programs, our fully automated
approach eliminates the need for manual annotations and offers objective
evaluation. Moreover, our anomaly detection model exhibits state-of-the-art
performance, surpassing existing models in accuracy. Going beyond the scope of
breast cosmesis, our research represents a significant advancement in
unsupervised anomaly detection within the medical domain, thereby paving the
way for future investigations.
\\ ( https://arxiv.org/abs/2402.18362 ,  13272kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18390 (*cross-listing*)
Date: Wed, 28 Feb 2024 15:11:02 GMT   (19592kb,D)

Title: Neuromorphic Event-Driven Semantic Communication in Microgrids
Authors: Xiaoguang Diao, Yubo Song, Subham Sahoo, Yuan Li
Categories: cs.ET cs.AI cs.NE cs.SY eess.SY
Comments: The manuscript has been accepted for publication in IEEE Transactions
  on Smart Grid
\\
  Synergies between advanced communications, computing and artificial
intelligence are unraveling new directions of coordinated operation and
resiliency in microgrids. On one hand, coordination among sources is
facilitated by distributed, privacy-minded processing at multiple locations,
whereas on the other hand, it also creates exogenous data arrival paths for
adversaries that can lead to cyber-physical attacks amongst other reliability
issues in the communication layer. This long-standing problem necessitates new
intrinsic ways of exchanging information between converters through power lines
to optimize the system's control performance. Going beyond the existing power
and data co-transfer technologies that are limited by efficiency and
scalability concerns, this paper proposes neuromorphic learning to implant
communicative features using spiking neural networks (SNNs) at each node, which
is trained collaboratively in an online manner simply using the power exchanges
between the nodes. As opposed to the conventional neuromorphic sensors that
operate with spiking signals, we employ an event-driven selective process to
collect sparse data for training of SNNs. Finally, its multi-fold effectiveness
and reliable performance is validated under simulation conditions with
different microgrid topologies and components to establish a new direction in
the sense-actuate-compute cycle for power electronic dominated grids and
microgrids.
\\ ( https://arxiv.org/abs/2402.18390 ,  19592kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18485 (*cross-listing*)
Date: Wed, 28 Feb 2024 17:06:54 GMT   (14008kb,D)

Title: FinAgent: A Multimodal Foundation Agent for Financial Trading:
  Tool-Augmented, Diversified, and Generalist
Authors: Wentao Zhang, Lingxuan Zhao, Haochong Xia, Shuo Sun, Jiaze Sun, Molei
  Qin, Xinyi Li, Yuqing Zhao, Yilei Zhao, Xinyu Cai, Longtao Zheng, Xinrun
  Wang, Bo An
Categories: q-fin.TR cs.AI
\\
  Financial trading is a crucial component of the markets, informed by a
multimodal information landscape encompassing news, prices, and Kline charts,
and encompasses diverse tasks such as quantitative trading and high-frequency
trading with various assets. While advanced AI techniques like deep learning
and reinforcement learning are extensively utilized in finance, their
application in financial trading tasks often faces challenges due to inadequate
handling of multimodal data and limited generalizability across various tasks.
To address these challenges, we present FinAgent, a multimodal foundational
agent with tool augmentation for financial trading. FinAgent's market
intelligence module processes a diverse range of data-numerical, textual, and
visual-to accurately analyze the financial market. Its unique dual-level
reflection module not only enables rapid adaptation to market dynamics but also
incorporates a diversified memory retrieval system, enhancing the agent's
ability to learn from historical data and improve decision-making processes.
The agent's emphasis on reasoning for actions fosters trust in its financial
decisions. Moreover, FinAgent integrates established trading strategies and
expert insights, ensuring that its trading approaches are both data-driven and
rooted in sound financial principles. With comprehensive experiments on 6
financial datasets, including stocks and Crypto, FinAgent significantly
outperforms 9 state-of-the-art baselines in terms of 6 financial metrics with
over 36% average improvement on profit. Specifically, a 92.27% return (a 84.39%
relative improvement) is achieved on one dataset. Notably, FinAgent is the
first advanced multimodal foundation agent designed for financial trading
tasks.
\\ ( https://arxiv.org/abs/2402.18485 ,  14008kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18487 (*cross-listing*)
Date: Wed, 28 Feb 2024 17:10:22 GMT   (590kb)

Title: Human-Centric Aware UAV Trajectory Planning in Search and Rescue
  Missions Employing Multi-Objective Reinforcement Learning with AHP and
  Similarity-Based Experience Replay
Authors: Mahya Ramezani and Jose Luis Sanchez-Lopez
Categories: cs.RO cs.AI cs.HC cs.LG
\\
  The integration of Unmanned Aerial Vehicles (UAVs) into Search and Rescue
(SAR) missions presents a promising avenue for enhancing operational efficiency
and effectiveness. However, the success of these missions is not solely
dependent on the technical capabilities of the drones but also on their
acceptance and interaction with humans on the ground. This paper explores the
effect of human-centric factor in UAV trajectory planning for SAR missions. We
introduce a novel approach based on the reinforcement learning augmented with
Analytic Hierarchy Process and novel similarity-based experience replay to
optimize UAV trajectories, balancing operational objectives with human comfort
and safety considerations. Additionally, through a comprehensive survey, we
investigate the impact of gender cues and anthropomorphism in UAV design on
public acceptance and trust, revealing significant implications for drone
interaction strategies in SAR. Our contributions include (1) a reinforcement
learning framework for UAV trajectory planning that dynamically integrates
multi-objective considerations, (2) an analysis of human perceptions towards
gendered and anthropomorphized drones in SAR contexts, and (3) the application
of similarity-based experience replay for enhanced learning efficiency in
complex SAR scenarios. The findings offer valuable insights into designing UAV
systems that are not only technically proficient but also aligned with
human-centric values.
\\ ( https://arxiv.org/abs/2402.18487 ,  590kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17938 (*cross-listing*)
Date: Tue, 27 Feb 2024 23:30:17 GMT   (1044kb,D)

Title: EmMark: Robust Watermarks for IP Protection of Embedded Quantized Large
  Language Models
Authors: Ruisi Zhang, Farinaz Koushanfar
Categories: cs.CR cs.CL
Comments: Accept to DAC 2024
\\
  This paper introduces EmMark,a novel watermarking framework for protecting
the intellectual property (IP) of embedded large language models deployed on
resource-constrained edge devices. To address the IP theft risks posed by
malicious end-users, EmMark enables proprietors to authenticate ownership by
querying the watermarked model weights and matching the inserted signatures.
EmMark's novelty lies in its strategic watermark weight parameters selection,
nsuring robustness and maintaining model quality. Extensive proof-of-concept
evaluations of models from OPT and LLaMA-2 families demonstrate EmMark's
fidelity, achieving 100% success in watermark extraction with model performance
preservation. EmMark also showcased its resilience against watermark removal
and forging attacks.
\\ ( https://arxiv.org/abs/2402.17938 ,  1044kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18031 (*cross-listing*)
Date: Wed, 28 Feb 2024 03:58:58 GMT   (7757kb,D)

Title: Corpus-Steered Query Expansion with Large Language Models
Authors: Yibin Lei, Yu Cao, Tianyi Zhou, Tao Shen, Andrew Yates
Categories: cs.IR cs.CL
Comments: EACL 2024 (Short)
\\
  Recent studies demonstrate that query expansions generated by large language
models (LLMs) can considerably enhance information retrieval systems by
generating hypothetical documents that answer the queries as expansions.
However, challenges arise from misalignments between the expansions and the
retrieval corpus, resulting in issues like hallucinations and outdated
information due to the limited intrinsic knowledge of LLMs. Inspired by Pseudo
Relevance Feedback (PRF), we introduce Corpus-Steered Query Expansion (CSQE) to
promote the incorporation of knowledge embedded within the corpus. CSQE
utilizes the relevance assessing capability of LLMs to systematically identify
pivotal sentences in the initially-retrieved documents. These corpus-originated
texts are subsequently used to expand the query together with LLM-knowledge
empowered expansions, improving the relevance prediction between the query and
the target documents. Extensive experiments reveal that CSQE exhibits strong
performance without necessitating any training, especially with queries for
which LLMs lack knowledge.
\\ ( https://arxiv.org/abs/2402.18031 ,  7757kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18115 (*cross-listing*)
Date: Wed, 28 Feb 2024 07:05:27 GMT   (16999kb,D)

Title: UniVS: Unified and Universal Video Segmentation with Prompts as Queries
Authors: Minghan Li and Shuai Li and Xindong Zhang and Lei Zhang
Categories: cs.CV cs.CL
Comments: 21 pages, 11 figures, 10 tabels, CVPR2024
Journal-ref: The IEEE/CVF Conference on Computer Vision and Pattern Recognition
  2024
\\
  Despite the recent advances in unified image segmentation (IS), developing a
unified video segmentation (VS) model remains a challenge. This is mainly
because generic category-specified VS tasks need to detect all objects and
track them across consecutive frames, while prompt-guided VS tasks require
re-identifying the target with visual/text prompts throughout the entire video,
making it hard to handle the different tasks with the same architecture. We
make an attempt to address these issues and present a novel unified VS
architecture, namely UniVS, by using prompts as queries. UniVS averages the
prompt features of the target from previous frames as its initial query to
explicitly decode masks, and introduces a target-wise prompt cross-attention
layer in the mask decoder to integrate prompt features in the memory pool. By
taking the predicted masks of entities from previous frames as their visual
prompts, UniVS converts different VS tasks into prompt-guided target
segmentation, eliminating the heuristic inter-frame matching process. Our
framework not only unifies the different VS tasks but also naturally achieves
universal training and testing, ensuring robust performance across different
scenarios. UniVS shows a commendable balance between performance and
universality on 10 challenging VS benchmarks, covering video instance,
semantic, panoptic, object, and referring segmentation tasks. Code can be found
at \url{https://github.com/MinghanLi/UniVS}.
\\ ( https://arxiv.org/abs/2402.18115 ,  16999kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18240 (*cross-listing*)
Date: Wed, 28 Feb 2024 11:12:17 GMT   (7241kb,D)

Title: Prospect Personalized Recommendation on Large Language Model-based Agent
  Platform
Authors: Jizhi Zhang, Keqin Bao, Wenjie Wang, Yang Zhang, Wentao Shi, Wanhong
  Xu, Fuli Feng, Tat-Seng Chua
Categories: cs.IR cs.CL
\\
  The new kind of Agent-oriented information system, exemplified by GPTs, urges
us to inspect the information system infrastructure to support Agent-level
information processing and to adapt to the characteristics of Large Language
Model (LLM)-based Agents, such as interactivity. In this work, we envisage the
prospect of the recommender system on LLM-based Agent platforms and introduce a
novel recommendation paradigm called Rec4Agentverse, comprised of Agent Items
and Agent Recommender. Rec4Agentverse emphasizes the collaboration between
Agent Items and Agent Recommender, thereby promoting personalized information
services and enhancing the exchange of information beyond the traditional
user-recommender feedback loop. Additionally, we prospect the evolution of
Rec4Agentverse and conceptualize it into three stages based on the enhancement
of the interaction and information exchange among Agent Items, Agent
Recommender, and the user. A preliminary study involving several cases of
Rec4Agentverse validates its significant potential for application. Lastly, we
discuss potential issues and promising directions for future research.
\\ ( https://arxiv.org/abs/2402.18240 ,  7241kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18275 (*cross-listing*)
Date: Wed, 28 Feb 2024 12:06:08 GMT   (121kb,D)

Title: Exploration of Adapter for Noise Robust Automatic Speech Recognition
Authors: Hao Shi, Tatsuya Kawahara
Categories: cs.SD cs.CL eess.AS
\\
  Adapting a robust automatic speech recognition (ASR) system to tackle unseen
noise scenarios is crucial. Integrating adapters into neural networks has
emerged as a potent technique for transfer learning. This paper thoroughly
investigates adapter-based noise-robust ASR adaptation. We conducted the
experiments using the CHiME--4 dataset. The results show that inserting the
adapter in the shallow layer yields superior effectiveness, and there is no
significant difference between adapting solely within the shallow layer and
adapting across all layers. Besides, the simulated data helps the system to
improve its performance under real noise conditions. Nonetheless, when the
amount of data is the same, the real data is more effective than the simulated
data. Multi-condition training remains valid for adapter training. Furthermore,
integrating adapters into speech enhancement-based ASR systems yields
substantial improvements.
\\ ( https://arxiv.org/abs/2402.18275 ,  121kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17771 (*cross-listing*)
Date: Thu, 15 Feb 2024 18:49:05 GMT   (21kb)

Title: Utilizing Machine Learning for Signal Classification and Noise Reduction
  in Amateur Radio
Authors: Jimi Sanchez
Categories: eess.SP cs.LG
\\
  In the realm of amateur radio, the effective classification of signals and
the mitigation of noise play crucial roles in ensuring reliable communication.
Traditional methods for signal classification and noise reduction often rely on
manual intervention and predefined thresholds, which can be labor-intensive and
less adaptable to dynamic radio environments. In this paper, we explore the
application of machine learning techniques for signal classification and noise
reduction in amateur radio operations. We investigate the feasibility and
effectiveness of employing supervised and unsupervised learning algorithms to
automatically differentiate between desired signals and unwanted interference,
as well as to reduce the impact of noise on received transmissions.
Experimental results demonstrate the potential of machine learning approaches
to enhance the efficiency and robustness of amateur radio communication
systems, paving the way for more intelligent and adaptive radio solutions in
the amateur radio community.
\\ ( https://arxiv.org/abs/2402.17771 ,  21kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17772 (*cross-listing*)
Date: Sat, 17 Feb 2024 05:22:41 GMT   (491kb,D)

Title: EEG2Rep: Enhancing Self-supervised EEG Representation Through
  Informative Masked Inputs
Authors: Navid Mohammadi Foumani, Geoffrey Mackellar, Soheila Ghane, Saad
  Irtza, Nam Nguyen, Mahsa Salehi
Categories: eess.SP cs.LG
\\
  Self-supervised approaches for electroencephalography (EEG) representation
learning face three specific challenges inherent to EEG data: (1) The low
signal-to-noise ratio which challenges the quality of the representation
learned, (2) The wide range of amplitudes from very small to relatively large
due to factors such as the inter-subject variability, risks the models to be
dominated by higher amplitude ranges, and (3) The absence of explicit
segmentation in the continuous-valued sequences which can result in less
informative representations. To address these challenges, we introduce EEG2Rep,
a self-prediction approach for self-supervised representation learning from
EEG. Two core novel components of EEG2Rep are as follows: 1) Instead of
learning to predict the masked input from raw EEG, EEG2Rep learns to predict
masked input in latent representation space, and 2) Instead of conventional
masking methods, EEG2Rep uses a new semantic subsequence preserving (SSP)
method which provides informative masked inputs to guide EEG2Rep to generate
rich semantic representations. In experiments on 6 diverse EEG tasks with
subject variability, EEG2Rep significantly outperforms state-of-the-art
methods. We show that our semantic subsequence preserving improves the existing
masking methods in self-prediction literature and find that preserving 50\% of
EEG recordings will result in the most accurate results on all 6 tasks on
average. Finally, we show that EEG2Rep is robust to noise addressing a
significant challenge that exists in EEG data. Models and code are available
at: https://github.com/Navidfoumani/EEG2Rep
\\ ( https://arxiv.org/abs/2402.17772 ,  491kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17773 (*cross-listing*)
Date: Sat, 17 Feb 2024 20:03:02 GMT   (9457kb,D)

Title: SINR-Aware Deep Reinforcement Learning for Distributed Dynamic Channel
  Allocation in Cognitive Interference Networks
Authors: Yaniv Cohen, Tomer Gafni, Ronen Greenberg, Kobi Cohen
Categories: eess.SP cs.LG
\\
  We consider the problem of dynamic channel allocation (DCA) in cognitive
communication networks with the goal of maximizing a global
signal-to-interference-plus-noise ratio (SINR) measure under a specified target
quality of service (QoS)-SINR for each network. The shared bandwidth is
partitioned into K channels with frequency separation. In contrast to the
majority of existing studies that assume perfect orthogonality or a one- to-one
user-channel allocation mapping, this paper focuses on real-world systems
experiencing inter-carrier interference (ICI) and channel reuse by multiple
large-scale networks. This realistic scenario significantly increases the
problem dimension, rendering existing algorithms inefficient. We propose a
novel multi-agent reinforcement learning (RL) framework for distributed DCA,
named Channel Allocation RL To Overlapped Networks (CARLTON). The CARLTON
framework is based on the Centralized Training with Decentralized Execution
(CTDE) paradigm, utilizing the DeepMellow value-based RL algorithm. To ensure
robust performance in the interference-laden environment we address, CARLTON
employs a low-dimensional representation of observations, generating a QoS-type
measure while maximizing a global SINR measure and ensuring the target QoS-SINR
for each network. Our results demonstrate exceptional performance and robust
generalization, showcasing superior efficiency compared to alternative
state-of-the-art methods, while achieving a marginally diminished performance
relative to a fully centralized approach.
\\ ( https://arxiv.org/abs/2402.17773 ,  9457kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17779 (*cross-listing*)
Date: Thu, 22 Feb 2024 17:39:10 GMT   (110kb,D)

Title: Assessing the importance of long-range correlations for
  deep-learning-based sleep staging
Authors: Tiezhi Wang and Nils Strodthoff
Categories: eess.SP cs.LG
Comments: 3 pages, 1 figure, Accepted at Workshop Biosignals, 28.2.-1.3.2024,
  G\"ottingen, Germany
\\
  This study aims to elucidate the significance of long-range correlations for
deep-learning-based sleep staging. It is centered around S4Sleep(TS), a
recently proposed model for automated sleep staging. This model utilizes
electroencephalography (EEG) as raw time series input and relies on structured
state space sequence (S4) models as essential model component. Although the
model already surpasses state-of-the-art methods for a moderate number of 15
input epochs, recent literature results suggest potential benefits from
incorporating very long correlations spanning hundreds of input epochs. In this
submission, we explore the possibility of achieving further enhancements by
systematically scaling up the model's input size, anticipating potential
improvements in prediction accuracy. In contrast to findings in literature, our
results demonstrate that augmenting the input size does not yield a significant
enhancement in the performance of S4Sleep(TS). These findings, coupled with the
distinctive ability of S4 models to capture long-range dependencies in time
series data, cast doubt on the diagnostic relevance of very long-range
interactions for sleep staging.
\\ ( https://arxiv.org/abs/2402.17779 ,  110kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17780 (*cross-listing*)
Date: Fri, 23 Feb 2024 02:31:35 GMT   (3750kb,D)

Title: Constraint Latent Space Matters: An Anti-anomalous Waveform
  Transformation Solution from Photoplethysmography to Arterial Blood Pressure
Authors: Cheng Bian, Xiaoyu Li, Qi Bi, Guangpu Zhu, Jiegeng Lyu, Weile Zhang,
  Yelei Li, Zijing Zeng
Categories: eess.SP cs.LG physics.med-ph
Comments: Accepted by AAAI-2024, main track
\\
  Arterial blood pressure (ABP) holds substantial promise for proactive
cardiovascular health management. Notwithstanding its potential, the invasive
nature of ABP measurements confines their utility primarily to clinical
environments, limiting their applicability for continuous monitoring beyond
medical facilities. The conversion of photoplethysmography (PPG) signals into
ABP equivalents has garnered significant attention due to its potential in
revolutionizing cardiovascular disease management. Recent strides in PPG-to-ABP
prediction encompass the integration of generative and discriminative models.
Despite these advances, the efficacy of these models is curtailed by the latent
space shift predicament, stemming from alterations in PPG data distribution
across disparate hardware and individuals, potentially leading to distorted ABP
waveforms. To tackle this problem, we present an innovative solution named the
Latent Space Constraint Transformer (LSCT), leveraging a quantized codebook to
yield robust latent spaces by employing multiple discretizing bases. To
facilitate improved reconstruction, the Correlation-boosted Attention Module
(CAM) is introduced to systematically query pertinent bases on a global scale.
Furthermore, to enhance expressive capacity, we propose the Multi-Spectrum
Enhancement Knowledge (MSEK), which fosters local information flow within the
channels of latent code and provides additional embedding for reconstruction.
Through comprehensive experimentation on both publicly available datasets and a
private downstream task dataset, the proposed approach demonstrates noteworthy
performance enhancements compared to existing methods. Extensive ablation
studies further substantiate the effectiveness of each introduced module.
\\ ( https://arxiv.org/abs/2402.17780 ,  3750kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17788 (*cross-listing*)
Date: Sat, 24 Feb 2024 16:29:36 GMT   (607kb,D)

Title: Multimodal Sleep Apnea Detection with Missing or Noisy Modalities
Authors: Hamed Fayyaz, Abigail Strang, Niharika S. D'Souza, Rahmatollah
  Beheshti
Categories: eess.SP cs.LG
\\
  Polysomnography (PSG) is a type of sleep study that records multimodal
physiological signals and is widely used for purposes such as sleep staging and
respiratory event detection. Conventional machine learning methods assume that
each sleep study is associated with a fixed set of observed modalities and that
all modalities are available for each sample. However, noisy and missing
modalities are a common issue in real-world clinical settings. In this study,
we propose a comprehensive pipeline aiming to compensate for the missing or
noisy modalities when performing sleep apnea detection. Unlike other existing
studies, our proposed model works with any combination of available modalities.
Our experiments show that the proposed model outperforms other state-of-the-art
approaches in sleep apnea detection using various subsets of available data and
different levels of noise, and maintains its high performance (AUROC>0.9) even
in the presence of high levels of noise or missingness. This is especially
relevant in settings where the level of noise and missingness is high (such as
pediatric or outside-of-clinic scenarios).
\\ ( https://arxiv.org/abs/2402.17788 ,  607kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17790 (*cross-listing*)
Date: Mon, 26 Feb 2024 11:26:38 GMT   (1511kb,D)

Title: EEG classifier cross-task transfer to avoid training sessions in
  robot-assisted rehabilitation
Authors: Niklas Kueper, Su Kyoung Kim and Elsa Andrea Kirchner
Categories: eess.SP cs.LG
Comments: 11 pages, 6 figures, 1 table
MSC-class: 68
\\
  Background: For an individualized support of patients during rehabilitation,
learning of individual machine learning models from the human
electroencephalogram (EEG) is required. Our approach allows labeled training
data to be recorded without the need for a specific training session. For this,
the planned exoskeleton-assisted rehabilitation enables bilateral mirror
therapy, in which movement intentions can be inferred from the activity of the
unaffected arm. During this therapy, labeled EEG data can be collected to
enable movement predictions of only the affected arm of a patient. Methods: A
study was conducted with 8 healthy subjects and the performance of the
classifier transfer approach was evaluated. Each subject performed 3 runs of 40
self-intended unilateral and bilateral reaching movements toward a target while
EEG data was recorded from 64 channels. A support vector machine (SVM)
classifier was trained under both movement conditions to make predictions for
the same type of movement. Furthermore, the classifier was evaluated to predict
unilateral movements by only beeing trained on the data of the bilateral
movement condition. Results: The results show that the performance of the
classifier trained on selected EEG channels evoked by bilateral movement
intentions is not significantly reduced compared to a classifier trained
directly on EEG data including unilateral movement intentions. Moreover, the
results show that our approach also works with only 8 or even 4 channels.
Conclusion: It was shown that the proposed classifier transfer approach enables
motion prediction without explicit collection of training data. Since the
approach can be applied even with a small number of EEG channels, this speaks
for the feasibility of the approach in real therapy sessions with patients and
motivates further investigations with stroke patients.
\\ ( https://arxiv.org/abs/2402.17790 ,  1511kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17807 (*cross-listing*)
Date: Tue, 27 Feb 2024 11:29:36 GMT   (42535kb,D)

Title: Exploring Gene Regulatory Interaction Networks and predicting
  therapeutic molecules for Hypopharyngeal Cancer and EGFR-mutated lung
  adenocarcinoma
Authors: Abanti Bhattacharjya, Md Manowarul Islam, Md Ashraf Uddin, Md. Alamin
  Talukder, AKM Azad, Sunil Aryal, Bikash Kumar Paul, Wahia Tasnim, Muhammad
  Ali Abdulllah Almoyad, Mohammad Ali Moni
Categories: q-bio.GN cs.LG
Comments: Accepted In The FEBS OPEN BIO (Q2, SCOPUS, SCIE, IF: 2.6, CS: 4.7),
  Wiley Journal, On FEB 25, 2024
\\
  With the advent of Information technology, the Bioinformatics research field
is becoming increasingly attractive to researchers and academicians. The recent
development of various Bioinformatics toolkits has facilitated the rapid
processing and analysis of vast quantities of biological data for human
perception. Most studies focus on locating two connected diseases and making
some observations to construct diverse gene regulatory interaction networks, a
forerunner to general drug design for curing illness. For instance,
Hypopharyngeal cancer is a disease that is associated with EGFR-mutated lung
adenocarcinoma. In this study, we select EGFR-mutated lung adenocarcinoma and
Hypopharyngeal cancer by finding the Lung metastases in hypopharyngeal cancer.
To conduct this study, we collect Mircorarray datasets from GEO (Gene
Expression Omnibus), an online database controlled by NCBI. Differentially
expressed genes, common genes, and hub genes between the selected two diseases
are detected for the succeeding move. Our research findings have suggested
common therapeutic molecules for the selected diseases based on 10 hub genes
with the highest interactions according to the degree topology method and the
maximum clique centrality (MCC). Our suggested therapeutic molecules will be
fruitful for patients with those two diseases simultaneously.
\\ ( https://arxiv.org/abs/2402.17807 ,  42535kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17808 (*cross-listing*)
Date: Tue, 27 Feb 2024 11:42:26 GMT   (364kb)

Title: AN An ica-ensemble learning approach for prediction of uwb nlos signals
  data classification
Authors: Jiya A. Enoch, Ilesanmi B. Oluwafemi, Francis A. Ibikunle and Olulope
  K. Paul
Categories: eess.SP cs.IT cs.LG math.IT
Comments: 20 pages,4 figures, 1 algorithm and 3 tables
\\
  Trapped human detection in search and rescue (SAR) scenarios poses a
significant challenge in pervasive computing. This study addresses this issue
by leveraging machine learning techniques, given their high accuracy. However,
accurate identification of trapped individuals is hindered by the curse of
dimensionality and noisy data. Particularly in non-line-of-sight (NLOS)
situations during catastrophic events, the curse of dimensionality may lead to
blind spots due to noise and uncorrelated values in detections. This research
focuses on harmonizing information through wireless communication and
identifying individuals in NLOS scenarios using ultra-wideband (UWB) radar
signals. Employing independent component analysis (ICA) for feature extraction,
the study evaluates classification performance using ensemble algorithms on
both static and dynamic datasets. The experimental results demonstrate
categorization accuracies of 88.37% for static data and 87.20% for dynamic
data, highlighting the effectiveness of the proposed approach. Finally, this
work can help scientists and engineers make instant decisions during SAR
operations.
\\ ( https://arxiv.org/abs/2402.17808 ,  364kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17870 (*cross-listing*)
Date: Tue, 27 Feb 2024 20:10:03 GMT   (149kb,D)

Title: Stochastic Approximation with Biased MCMC for Expectation Maximization
Authors: Samuel Gruffaz, Kyurae Kim, Alain Oliviero Durmus, Jacob R. Gardner
Categories: stat.CO cs.LG math.OC stat.ML
Comments: Accepted to AISTATS'24
\\
  The expectation maximization (EM) algorithm is a widespread method for
empirical Bayesian inference, but its expectation step (E-step) is often
intractable. Employing a stochastic approximation scheme with Markov chain
Monte Carlo (MCMC) can circumvent this issue, resulting in an algorithm known
as MCMC-SAEM. While theoretical guarantees for MCMC-SAEM have previously been
established, these results are restricted to the case where asymptotically
unbiased MCMC algorithms are used. In practice, MCMC-SAEM is often run with
asymptotically biased MCMC, for which the consequences are theoretically less
understood. In this work, we fill this gap by analyzing the asymptotics and
non-asymptotics of SAEM with biased MCMC steps, particularly the effect of
bias. We also provide numerical experiments comparing the Metropolis-adjusted
Langevin algorithm (MALA), which is asymptotically unbiased, and the unadjusted
Langevin algorithm (ULA), which is asymptotically biased, on synthetic and real
datasets. Experimental results show that ULA is more stable with respect to the
choice of Langevin stepsize and can sometimes result in faster convergence.
\\ ( https://arxiv.org/abs/2402.17870 ,  149kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17886 (*cross-listing*)
Date: Tue, 27 Feb 2024 21:00:00 GMT   (3337kb,D)

Title: Zeroth-Order Sampling Methods for Non-Log-Concave Distributions:
  Alleviating Metastability by Denoising Diffusion
Authors: Ye He, Kevin Rojas, Molei Tao
Categories: stat.ML cs.LG math.PR math.ST stat.ME stat.TH
\\
  This paper considers the problem of sampling from non-logconcave
distribution, based on queries of its unnormalized density. It first describes
a framework, Diffusion Monte Carlo (DMC), based on the simulation of a
denoising diffusion process with its score function approximated by a generic
Monte Carlo estimator. DMC is an oracle-based meta-algorithm, where its oracle
is the assumed access to samples that generate a Monte Carlo score estimator.
Then we provide an implementation of this oracle, based on rejection sampling,
and this turns DMC into a true algorithm, termed Zeroth-Order Diffusion Monte
Carlo (ZOD-MC). We provide convergence analyses by first constructing a general
framework, i.e. a performance guarantee for DMC, without assuming the target
distribution to be log-concave or satisfying any isoperimetric inequality. Then
we prove that ZOD-MC admits an inverse polynomial dependence on the desired
sampling accuracy, albeit still suffering from the curse of dimensionality.
Consequently, for low dimensional distributions, ZOD-MC is a very efficient
sampler, with performance exceeding latest samplers, including
also-denoising-diffusion-based RDMC and RS-DMC. Last, we experimentally
demonstrate the insensitivity of ZOD-MC to increasingly higher barriers between
modes or discontinuity in non-convex potential.
\\ ( https://arxiv.org/abs/2402.17886 ,  3337kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17898 (*cross-listing*)
Date: Tue, 27 Feb 2024 21:28:08 GMT   (36868kb,D)

Title: Exoplanets Prediction in Multi-Planetary Systems and Determining the
  Correlation Between the Parameters of Planets and Host Stars Using Artificial
  Intelligence
Authors: Mahdiyar Mousavi-Sadr
Categories: astro-ph.EP cs.LG
Comments: A Ph.D. dissertation. 154 pages, 52 figures
\\
  The number of extrasolar planets discovered is increasing, so that more than
five thousand exoplanets have been confirmed to date. Now we have an
opportunity to test the validity of the laws governing planetary systems and
take steps to discover the relationships between the physical parameters of
planets and stars. Firstly, we present the results of a search for additional
exoplanets in 229 multi-planetary systems that house at least three or more
confirmed planets, employing a logarithmic spacing between planets in our Solar
System known as the Titius-Bode (TB) relation. We find that the planets in
$\sim53\%$ of these systems adhere to a logarithmic spacing relation remarkably
better than the Solar System planets. We predict the presence of 426 additional
exoplanets, 47 of which are located within the habitable zone (HZ), and five of
the 47 planets have a maximum mass limit of 0.1-2$M_{\oplus}$ and a maximum
radius lower than 1.25$R_{\oplus}$. Secondly, we employ efficient machine
learning approaches to analyze a dataset comprising 762 confirmed exoplanets
and eight Solar System planets, aiming to characterize their fundamental
quantities. We classify the data into two main classes: 'small' and 'giant'
planets, with cut-off values at $R_{p}=8.13R_{\oplus}$ and
$M_{p}=52.48M_{\oplus}$. Giant planets have lower densities, suggesting higher
H-He mass fractions, while small planets are denser, composed mainly of heavier
elements. We highlight that planetary mass, orbital period, and stellar mass
play crucial roles in predicting exoplanet radius. Notably, our study reveals a
noteworthy result: for giant planets, we observe a strong correlation between
planetary radius and the mass of their host stars, which might provide
intriguing insights into the relationship between giant planet formation and
stellar characteristics.
\\ ( https://arxiv.org/abs/2402.17898 ,  36868kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17911 (*cross-listing*)
Date: Tue, 27 Feb 2024 21:53:32 GMT   (2795kb,D)

Title: Demonstration of Robust and Efficient Quantum Property Learning with
  Shallow Shadows
Authors: Hong-Ye Hu, Andi Gu, Swarnadeep Majumder, Hang Ren, Yipei Zhang, Derek
  S. Wang, Yi-Zhuang You, Zlatko Minev, Susanne F. Yelin, Alireza Seif
Categories: quant-ph cond-mat.stat-mech cs.IT cs.LG math.IT
Comments: 12 pages, 5 figures
\\
  Extracting information efficiently from quantum systems is a major component
of quantum information processing tasks. Randomized measurements, or classical
shadows, enable predicting many properties of arbitrary quantum states using
few measurements. While random single qubit measurements are experimentally
friendly and suitable for learning low-weight Pauli observables, they perform
poorly for nonlocal observables. Prepending a shallow random quantum circuit
before measurements maintains this experimental friendliness, but also has
favorable sample complexities for observables beyond low-weight Paulis,
including high-weight Paulis and global low-rank properties such as fidelity.
However, in realistic scenarios, quantum noise accumulated with each additional
layer of the shallow circuit biases the results. To address these challenges,
we propose the robust shallow shadows protocol. Our protocol uses Bayesian
inference to learn the experimentally relevant noise model and mitigate it in
postprocessing. This mitigation introduces a bias-variance trade-off:
correcting for noise-induced bias comes at the cost of a larger estimator
variance. Despite this increased variance, as we demonstrate on a
superconducting quantum processor, our protocol correctly recovers state
properties such as expectation values, fidelity, and entanglement entropy,
while maintaining a lower sample complexity compared to the random single qubit
measurement scheme. We also theoretically analyze the effects of noise on
sample complexity and show how the optimal choice of the shallow shadow depth
varies with noise strength. This combined theoretical and experimental analysis
positions the robust shallow shadow protocol as a scalable, robust, and
sample-efficient protocol for characterizing quantum states on current quantum
computing platforms.
\\ ( https://arxiv.org/abs/2402.17911 ,  2795kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17918 (*cross-listing*)
Date: Tue, 27 Feb 2024 22:14:01 GMT   (1810kb,D)

Title: The Seeker's Dilemma: Realistic Formulation and Benchmarking for
  Hardware Trojan Detection
Authors: Amin Sarihi, Ahmad Patooghy, Abdel-Hameed A. Badawy, Peter Jamieson
Categories: cs.CR cs.AR cs.LG
ACM-class: B.8.1
\\
  This work focuses on advancing security research in the hardware design space
by formally defining the realistic problem of Hardware Trojan (HT) detection.
The goal is to model HT detection more closely to the real world, i.e.,
describing the problem as "The Seeker's Dilemma" (an extension of Hide&Seek on
a graph), where a detecting agent is unaware of whether circuits are infected
by HTs or not. Using this theoretical problem formulation, we create a
benchmark that consists of a mixture of HT-free and HT-infected restructured
circuits while preserving their original functionalities. The restructured
circuits are randomly infected by HTs, causing a situation where the defender
is uncertain if a circuit is infected or not. We believe that our innovative
dataset will help the community better judge the detection quality of different
methods by comparing their success rates in circuit classification. We use our
developed benchmark to evaluate three state-of-the-art HT detection tools to
show baseline results for this approach. We use Principal Component Analysis to
assess the strength of our benchmark, where we observe that some restructured
HT-infected circuits are mapped closely to HT-free circuits, leading to
significant label misclassification by detectors.
\\ ( https://arxiv.org/abs/2402.17918 ,  1810kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17926 (*cross-listing*)
Date: Tue, 27 Feb 2024 22:49:33 GMT   (2081kb,D)

Title: Certain and Approximately Certain Models for Statistical Learning
Authors: Cheng Zhen, Nischal Aryal, Arash Termehchy, Amandeep Singh Chabada
Categories: stat.ML cs.DB cs.LG
Comments: A technical report for a paper to appear at SIGMOD 2024
\\
  Real-world data is often incomplete and contains missing values. To train
accurate models over real-world datasets, users need to spend a substantial
amount of time and resources imputing and finding proper values for missing
data items. In this paper, we demonstrate that it is possible to learn accurate
models directly from data with missing values for certain training data and
target models. We propose a unified approach for checking the necessity of data
imputation to learn accurate models across various widely-used machine learning
paradigms. We build efficient algorithms with theoretical guarantees to check
this necessity and return accurate models in cases where imputation is
unnecessary. Our extensive experiments indicate that our proposed algorithms
significantly reduce the amount of time and effort needed for data imputation
without imposing considerable computational overhead.
\\ ( https://arxiv.org/abs/2402.17926 ,  2081kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17943 (*cross-listing*)
Date: Tue, 27 Feb 2024 23:52:58 GMT   (1190kb,D)

Title: Sequential transport maps using SoS density estimation and
  $\alpha$-divergences
Authors: Benjamin Zanger, Tiangang Cui, Martin Schreiber, Olivier Zahm
Categories: stat.ML cs.LG
\\
  Transport-based density estimation methods are receiving growing interest
because of their ability to efficiently generate samples from the approximated
density. We further invertigate the sequential transport maps framework
proposed from arXiv:2106.04170 arXiv:2303.02554, which builds on a sequence of
composed Knothe-Rosenblatt (KR) maps. Each of those maps are built by first
estimating an intermediate density of moderate complexity, and then by
computing the exact KR map from a reference density to the precomputed
approximate density. In our work, we explore the use of Sum-of-Squares (SoS)
densities and $\alpha$-divergences for approximating the intermediate
densities. Combining SoS densities with $\alpha$-divergence interestingly
yields convex optimization problems which can be efficiently solved using
semidefinite programming. The main advantage of $\alpha$-divergences is to
enable working with unnormalized densities, which provides benefits both
numerically and theoretically. In particular, we provide two new convergence
analyses of the sequential transport maps: one based on a triangle-like
inequality and the second on information geometric properties of
$\alpha$-divergences for unnormalizied densities. The choice of intermediate
densities is also crucial for the efficiency of the method. While tempered (or
annealed) densities are the state-of-the-art, we introduce diffusion-based
intermediate densities which permits to approximate densities known from
samples only. Such intermediate densities are well-established in machine
learning for generative modeling. Finally we propose and try different
low-dimensional maps (or lazy maps) for dealing with high-dimensional problems
and numerically demonstrate our methods on several benchmarks, including
Bayesian inference problems and unsupervised learning task.
\\ ( https://arxiv.org/abs/2402.17943 ,  1190kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17987 (*cross-listing*)
Date: Wed, 28 Feb 2024 02:11:47 GMT   (4323kb,D)

Title: Multistatic-Radar RCS-Signature Recognition of Aerial Vehicles: A
  Bayesian Fusion Approach
Authors: Michael Potter, Murat Akcakaya, Marius Necsoiu, Gunar Schirner, Deniz
  Erdogmus, Tales Imbiriba
Categories: eess.SP cs.CV cs.LG math.PR stat.ML
Comments: To be submitted to IEEE Transactions on Aerospace and Electronic
  Systems
\\
  Radar Automated Target Recognition (RATR) for Unmanned Aerial Vehicles (UAVs)
involves transmitting Electromagnetic Waves (EMWs) and performing target type
recognition on the received radar echo, crucial for defense and aerospace
applications. Previous studies highlighted the advantages of multistatic radar
configurations over monostatic ones in RATR. However, fusion methods in
multistatic radar configurations often suboptimally combine classification
vectors from individual radars probabilistically. To address this, we propose a
fully Bayesian RATR framework employing Optimal Bayesian Fusion (OBF) to
aggregate classification probability vectors from multiple radars. OBF, based
on expected 0-1 loss, updates a Recursive Bayesian Classification (RBC)
posterior distribution for target UAV type, conditioned on historical
observations across multiple time steps. We evaluate the approach using
simulated random walk trajectories for seven drones, correlating target aspect
angles to Radar Cross Section (RCS) measurements in an anechoic chamber.
Comparing against single radar Automated Target Recognition (ATR) systems and
suboptimal fusion methods, our empirical results demonstrate that the OBF
method integrated with RBC significantly enhances classification accuracy
compared to other fusion methods and single radar configurations.
\\ ( https://arxiv.org/abs/2402.17987 ,  4323kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17988 (*cross-listing*)
Date: Wed, 28 Feb 2024 02:12:47 GMT   (219kb,D)

Title: Constrained Decoding for Code Language Models via Efficient Left and
  Right Quotienting of Context-Sensitive Grammars
Authors: Daniel Melcer, Nathan Fulton, Sanjay Krishna Gouda, Haifeng Qian
Categories: cs.PL cs.LG cs.SE
Comments: 20 pages, Code available at
  https://github.com/amazon-science/incremental-parsing
\\
  Large Language Models are powerful tools for program synthesis and advanced
auto-completion, but come with no guarantee that their output code is
syntactically correct. This paper contributes an incremental parser that allows
early rejection of syntactically incorrect code, as well as efficient detection
of complete programs for fill-in-the-middle (FItM) tasks. We develop
Earley-style parsers that operate over left and right quotients of arbitrary
context-free grammars, and we extend our incremental parsing and quotient
operations to several context-sensitive features present in the grammars of
many common programming languages. The result of these contributions is an
efficient, general, and well-grounded method for left and right quotient
parsing.
  To validate our theoretical contributions -- and the practical effectiveness
of certain design decisions -- we evaluate our method on the particularly
difficult case of FItM completion for Python 3. Our results demonstrate that
constrained generation can significantly reduce the incidence of syntax errors
in recommended code.
\\ ( https://arxiv.org/abs/2402.17988 ,  219kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17992 (*cross-listing*)
Date: Wed, 28 Feb 2024 02:16:03 GMT   (3604kb)

Title: Physics-Informed Machine Learning for Seismic Response Prediction OF
  Nonlinear Steel Moment Resisting Frame Structures
Authors: R. Bailey Bond, Pu Ren, Jerome F. Hajjar, and Hao Sun
Categories: physics.app-ph cs.LG
Comments: 34 pages, 12 figures
\\
  There is a growing interest in utilizing machine learning (ML) methods for
structural metamodeling due to the substantial computational cost of
traditional numerical simulations. The existing data-driven strategies show
potential limitations to the model robustness and interpretability as well as
the dependency of rich data. To address these challenges, this paper presents a
novel physics-informed machine learning (PiML) method, which incorporates
scientific principles and physical laws into deep neural networks for modeling
seismic responses of nonlinear structures. The basic concept is to constrain
the solution space of the ML model within known physical bounds. This is made
possible with three main features, namely, model order reduction, a long
short-term memory (LSTM) networks, and Newton's second law (e.g., the equation
of motion). Model order reduction is essential for handling structural systems
with inherent redundancy and enhancing model efficiency. The LSTM network
captures temporal dependencies, enabling accurate prediction of time series
responses. The equation of motion is manipulated to learn system nonlinearities
and confines the solution space within physically interpretable results. These
features enable model training with relatively sparse data and offer benefits
in terms of accuracy, interpretability, and robustness. Furthermore, a dataset
of seismically designed archetype ductile planar steel moment resistant frames
under horizontal seismic loading, available in the DesignSafe-CI Database, is
considered for evaluation of the proposed method. The resulting metamodel is
capable of handling more complex data compared to existing physics-guided LSTM
models and outperforms other non-physics data-driven neural networks.
\\ ( https://arxiv.org/abs/2402.17992 ,  3604kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18064 (*cross-listing*)
Date: Wed, 28 Feb 2024 05:49:08 GMT   (5306kb,D)

Title: Automated Testing of Spatially-Dependent Environmental Hypotheses
  through Active Transfer Learning
Authors: Nicholas Harrison, Nathan Wallace, Salah Sukkarieh
Categories: cs.RO cs.LG
\\
  The efficient collection of samples is an important factor in outdoor
information gathering applications on account of high sampling costs such as
time, energy, and potential destruction to the environment. Utilization of
available a-priori data can be a powerful tool for increasing efficiency.
However, the relationships of this data with the quantity of interest are often
not known ahead of time, limiting the ability to leverage this knowledge for
improved planning efficiency. To this end, this work combines transfer learning
and active learning through a Multi-Task Gaussian Process and an
information-based objective function. Through this combination it can explore
the space of hypothetical inter-quantity relationships and evaluate these
hypotheses in real-time, allowing this new knowledge to be immediately
exploited for future plans. The performance of the proposed method is evaluated
against synthetic data and is shown to evaluate multiple hypotheses correctly.
Its effectiveness is also demonstrated on real datasets. The technique is able
to identify and leverage hypotheses which show a medium or strong correlation
to reduce prediction error by a factor of 1.5--6 within the first 5 samples,
and poor hypotheses are quickly identified and rejected, having no adverse
effect on planning after around 3 samples.
\\ ( https://arxiv.org/abs/2402.18064 ,  5306kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18112 (*cross-listing*)
Date: Wed, 28 Feb 2024 07:02:08 GMT   (1202kb,D)

Title: Simple But Effective: Rethinking the Ability of Deep Learning in fNIRS
  to Exclude Abnormal Input
Authors: Zhihao Cao
Categories: eess.SP cs.LG
DOI: 10.36227/techrxiv.170906859.92444731/v1
\\
  Functional near-infrared spectroscopy (fNIRS) is a non-invasive technique for
monitoring brain activity. To better understand the brain, researchers often
use deep learning to address the classification challenges of fNIRS data. Our
study shows that while current networks in fNIRS are highly accurate for
predictions within their training distribution, they falter at identifying and
excluding abnormal data which is out-of-distribution, affecting their
reliability. We propose integrating metric learning and supervised methods into
fNIRS research to improve networks capability in identifying and excluding
out-of-distribution outliers. This method is simple yet effective. In our
experiments, it significantly enhances the performance of various networks in
fNIRS, particularly transformer-based one, which shows the great improvement in
reliability. We will make our experiment data available on GitHub.
\\ ( https://arxiv.org/abs/2402.18112 ,  1202kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18117 (*cross-listing*)
Date: Wed, 28 Feb 2024 07:10:37 GMT   (27958kb,D)

Title: PRCL: Probabilistic Representation Contrastive Learning for
  Semi-Supervised Semantic Segmentation
Authors: Haoyu Xie, Changqi Wang, Jian Zhao, Yang Liu, Jun Dan, Chong Fu,
  Baigui Sun
Categories: cs.CV cs.LG
Comments: 19 pages, 11 figures
\\
  Tremendous breakthroughs have been developed in Semi-Supervised Semantic
Segmentation (S4) through contrastive learning. However, due to limited
annotations, the guidance on unlabeled images is generated by the model itself,
which inevitably exists noise and disturbs the unsupervised training process.
To address this issue, we propose a robust contrastive-based S4 framework,
termed the Probabilistic Representation Contrastive Learning (PRCL) framework
to enhance the robustness of the unsupervised training process. We model the
pixel-wise representation as Probabilistic Representations (PR) via
multivariate Gaussian distribution and tune the contribution of the ambiguous
representations to tolerate the risk of inaccurate guidance in contrastive
learning. Furthermore, we introduce Global Distribution Prototypes (GDP) by
gathering all PRs throughout the whole training process. Since the GDP contains
the information of all representations with the same class, it is robust from
the instant noise in representations and bears the intra-class variance of
representations. In addition, we generate Virtual Negatives (VNs) based on GDP
to involve the contrastive learning process. Extensive experiments on two
public benchmarks demonstrate the superiority of our PRCL framework.
\\ ( https://arxiv.org/abs/2402.18117 ,  27958kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18128 (*cross-listing*)
Date: Wed, 28 Feb 2024 07:37:26 GMT   (711kb,D)

Title: Downstream Task Guided Masking Learning in Masked Autoencoders Using
  Multi-Level Optimization
Authors: Han Guo, Ramtin Hosseini, Ruiyi Zhang, Sai Ashish Somayajula, Ranak
  Roy Chowdhury, Rajesh K. Gupta, Pengtao Xie
Categories: cs.CV cs.LG
\\
  Masked Autoencoder (MAE) is a notable method for self-supervised pretraining
in visual representation learning. It operates by randomly masking image
patches and reconstructing these masked patches using the unmasked ones. A key
limitation of MAE lies in its disregard for the varying informativeness of
different patches, as it uniformly selects patches to mask. To overcome this,
some approaches propose masking based on patch informativeness. However, these
methods often do not consider the specific requirements of downstream tasks,
potentially leading to suboptimal representations for these tasks. In response,
we introduce the Multi-level Optimized Mask Autoencoder (MLO-MAE), a novel
framework that leverages end-to-end feedback from downstream tasks to learn an
optimal masking strategy during pretraining. Our experimental findings
highlight MLO-MAE's significant advancements in visual representation learning.
Compared to existing methods, it demonstrates remarkable improvements across
diverse datasets and tasks, showcasing its adaptability and efficiency. Our
code is available at: https://github.com/Alexiland/MLOMAE
\\ ( https://arxiv.org/abs/2402.18128 ,  711kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18241 (*cross-listing*)
Date: Wed, 28 Feb 2024 11:12:47 GMT   (446kb)

Title: Affective State Detection using fNIRs and Machine Learning
Authors: Ritam Ghosh
Categories: cs.HC cs.LG
\\
  Affective states regulate our day to day to function and has a tremendous
effect on mental and physical health. Detection of affective states is of
utmost importance for mental health monitoring, smart entertainment selection
and dynamic workload management. In this paper, we discussed relevant
literature on affective state detection using physiology data, the benefits and
limitations of different sensors and methods used for collecting physiology
data, and our rationale for selecting functional near-infrared spectroscopy. We
present the design of an experiment involving nine subjects to evoke the
affective states of meditation, amusement and cognitive load and the results of
the attempt to classify using machine learning. A mean accuracy of 83.04% was
achieved in three class classification with an individual model; 84.39%
accuracy was achieved for a group model and 60.57% accuracy was achieved for
subject independent model using leave one out cross validation. It was found
that prediction accuracy for cognitive load was higher (evoked using a pen and
paper task) than the other two classes (evoked using computer bases tasks). To
verify that this discrepancy was not due to motor skills involved in the pen
and paper task, a second experiment was conducted using four participants and
the results of that experiment has also been presented in the paper.
\\ ( https://arxiv.org/abs/2402.18241 ,  446kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18329 (*cross-listing*)
Date: Wed, 28 Feb 2024 13:49:23 GMT   (1161kb,D)

Title: Living-off-The-Land Reverse-Shell Detection by Informed Data
  Augmentation
Authors: Dmitrijs Trizna, Luca Demetrio, Battista Biggio, Fabio Roli
Categories: cs.CR cs.LG
\\
  The living-off-the-land (LOTL) offensive methodologies rely on the
perpetration of malicious actions through chains of commands executed by
legitimate applications, identifiable exclusively by analysis of system logs.
LOTL techniques are well hidden inside the stream of events generated by common
legitimate activities, moreover threat actors often camouflage activity through
obfuscation, making them particularly difficult to detect without incurring in
plenty of false alarms, even using machine learning. To improve the performance
of models in such an harsh environment, we propose an augmentation framework to
enhance and diversify the presence of LOTL malicious activity inside legitimate
logs. Guided by threat intelligence, we generate a dataset by injecting attack
templates known to be employed in the wild, further enriched by malleable
patterns of legitimate activities to replicate the behavior of evasive threat
actors. We conduct an extensive ablation study to understand which models
better handle our augmented dataset, also manipulated to mimic the presence of
model-agnostic evasion and poisoning attacks. Our results suggest that
augmentation is needed to maintain high-predictive capabilities, robustness to
attack is achieved through specific hardening techniques like adversarial
training, and it is possible to deploy near-real-time models with almost-zero
false alarms.
\\ ( https://arxiv.org/abs/2402.18329 ,  1161kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18396 (*cross-listing*)
Date: Wed, 28 Feb 2024 15:15:23 GMT   (7627kb,D)

Title: Deep Confident Steps to New Pockets: Strategies for Docking
  Generalization
Authors: Gabriele Corso, Arthur Deng, Benjamin Fry, Nicholas Polizzi, Regina
  Barzilay, Tommi Jaakkola
Categories: q-bio.BM cs.LG
Journal-ref: International Conference on Learning Representations 2024
\\
  Accurate blind docking has the potential to lead to new biological
breakthroughs, but for this promise to be realized, docking methods must
generalize well across the proteome. Existing benchmarks, however, fail to
rigorously assess generalizability. Therefore, we develop DockGen, a new
benchmark based on the ligand-binding domains of proteins, and we show that
existing machine learning-based docking models have very weak generalization
abilities. We carefully analyze the scaling laws of ML-based docking and show
that, by scaling data and model size, as well as integrating synthetic data
strategies, we are able to significantly increase the generalization capacity
and set new state-of-the-art performance across benchmarks. Further, we propose
Confidence Bootstrapping, a new training paradigm that solely relies on the
interaction between diffusion and confidence models and exploits the
multi-resolution generation process of diffusion models. We demonstrate that
Confidence Bootstrapping significantly improves the ability of ML-based docking
methods to dock to unseen protein classes, edging closer to accurate and
generalizable blind docking methods.
\\ ( https://arxiv.org/abs/2402.18396 ,  7627kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18484 (*cross-listing*)
Date: Wed, 28 Feb 2024 17:06:19 GMT   (14899kb,D)

Title: A non-intrusive machine learning framework for debiasing long-time
  coarse resolution climate simulations and quantifying rare events statistics
Authors: Benedikt Barthel Sorensen, Alexis Charalampopoulos, Shixuan Zhang,
  Bryce Harrop, Ruby Leung, Themistoklis Sapsis
Categories: physics.ao-ph cs.LG
\\
  Due to the rapidly changing climate, the frequency and severity of extreme
weather is expected to increase over the coming decades. As fully-resolved
climate simulations remain computationally intractable, policy makers must rely
on coarse-models to quantify risk for extremes. However, coarse models suffer
from inherent bias due to the ignored "sub-grid" scales. We propose a framework
to non-intrusively debias coarse-resolution climate predictions using
neural-network (NN) correction operators. Previous efforts have attempted to
train such operators using loss functions that match statistics. However, this
approach falls short with events that have longer return period than that of
the training data, since the reference statistics have not converged. Here, the
scope is to formulate a learning method that allows for correction of dynamics
and quantification of extreme events with longer return period than the
training data. The key obstacle is the chaotic nature of the underlying
dynamics. To overcome this challenge, we introduce a dynamical systems approach
where the correction operator is trained using reference data and a coarse
model simulation nudged towards that reference. The method is demonstrated on
debiasing an under-resolved quasi-geostrophic model and the Energy Exascale
Earth System Model (E3SM). For the former, our method enables the
quantification of events that have return period two orders longer than the
training data. For the latter, when trained on 8 years of ERA5 data, our
approach is able to correct the coarse E3SM output to closely reflect the
36-year ERA5 statistics for all prognostic variables and significantly reduce
their spatial biases.
\\ ( https://arxiv.org/abs/2402.18484 ,  14899kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18527 (*cross-listing*)
Date: Wed, 28 Feb 2024 18:07:47 GMT   (1018kb,D)

Title: Defect Detection in Tire X-Ray Images: Conventional Methods Meet Deep
  Structures
Authors: Andrei Cozma, Landon Harris, Hairong Qi, Ping Ji, Wenpeng Guo, Song
  Yuan
Categories: cs.CV cs.LG eess.IV
Comments: 7 pages, 2 figures, 3 tables, submitted to ICIP2024
ACM-class: I.4.7; I.4.9; I.4.0
\\
  This paper introduces a robust approach for automated defect detection in
tire X-ray images by harnessing traditional feature extraction methods such as
Local Binary Pattern (LBP) and Gray Level Co-Occurrence Matrix (GLCM) features,
as well as Fourier and Wavelet-based features, complemented by advanced machine
learning techniques. Recognizing the challenges inherent in the complex
patterns and textures of tire X-ray images, the study emphasizes the
significance of feature engineering to enhance the performance of defect
detection systems. By meticulously integrating combinations of these features
with a Random Forest (RF) classifier and comparing them against advanced models
like YOLOv8, the research not only benchmarks the performance of traditional
features in defect detection but also explores the synergy between classical
and modern approaches. The experimental results demonstrate that these
traditional features, when fine-tuned and combined with machine learning
models, can significantly improve the accuracy and reliability of tire defect
detection, aiming to set a new standard in automated quality assurance in tire
manufacturing.
\\ ( https://arxiv.org/abs/2402.18527 ,  1018kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2305.11624
replaced with revised version Wed, 28 Feb 2024 14:34:06 GMT   (2301kb,D)

Title: Efficient ConvBN Blocks for Transfer Learning and Beyond
Authors: Kaichao You, Guo Qin, Anchang Bao, Meng Cao, Ping Huang, Jiulong Shan,
  Mingsheng Long
Categories: cs.AI
Comments: ICLR 2024, camera ready version
\\ ( https://arxiv.org/abs/2305.11624 ,  2301kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05638
replaced with revised version Wed, 28 Feb 2024 17:02:58 GMT   (783kb,D)

Title: Errors are Robustly Tamed in Cumulative Knowledge Processes
Authors: Anna Brandenberger, Cassandra Marcussen, Elchanan Mossel, Madhu Sudan
Categories: cs.AI cs.DS cs.SI math.PR
Comments: 39 pages, 9 figures. Generalized arguments to a broader family of
  knowledge accumulation models
\\ ( https://arxiv.org/abs/2309.05638 ,  783kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07197
replaced with revised version Wed, 28 Feb 2024 02:42:35 GMT   (784kb,D)

Title: GraphTranslator: Aligning Graph Model to Large Language Model for
  Open-ended Tasks
Authors: Mengmei Zhang, Mingwei Sun, Peng Wang, Shen Fan, Yanhu Mo, Xiaoxiao
  Xu, Hong Liu, Cheng Yang, Chuan Shi
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.07197 ,  784kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16278
replaced with revised version Wed, 28 Feb 2024 09:53:56 GMT   (1253kb,D)

Title: A Self-matching Training Method with Annotation Embedding Models for
  Ontology Subsumption Prediction
Authors: Yukihiro Shiraishi, Ken Kaneiwa
Categories: cs.AI cs.CL cs.LG
Comments: 21 pages, 6 figures
\\ ( https://arxiv.org/abs/2402.16278 ,  1253kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16505
replaced with revised version Wed, 28 Feb 2024 15:40:31 GMT   (725kb,D)

Title: Memory GAPS: Would LLMs pass the Tulving Test?
Authors: Jean-Marie Chauvet
Categories: cs.AI
Comments: 15 pages, 3 figures
ACM-class: I.2.4
\\ ( https://arxiv.org/abs/2402.16505 ,  725kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17553
replaced with revised version Wed, 28 Feb 2024 17:27:39 GMT   (36745kb,D)

Title: OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist
  Autonomous Agents for Desktop and Web
Authors: Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran
  Kamble, Waseem Alshikh, Ruslan Salakhutdinov
Categories: cs.AI cs.CL cs.CV cs.HC
\\ ( https://arxiv.org/abs/2402.17553 ,  36745kb)
------------------------------------------------------------------------------
\\
arXiv:2204.05424
replaced with revised version Wed, 28 Feb 2024 08:03:28 GMT   (1235kb,D)

Title: A Call for Clarity in Beam Search: How It Works and When It Stops
Authors: Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Dragomir Radev, Yejin
  Choi, and Noah A. Smith
Categories: cs.CL
Comments: LREC-COLING 2024
\\ ( https://arxiv.org/abs/2204.05424 ,  1235kb)
------------------------------------------------------------------------------
\\
arXiv:2207.13332
replaced with revised version Wed, 28 Feb 2024 07:45:37 GMT   (2198kb,D)

Title: RealTime QA: What's the Answer Right Now?
Authors: Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari
  Asai, Xinyan Yu, Dragomir Radev, Noah A. Smith, Yejin Choi, Kentaro Inui
Categories: cs.CL
Comments: RealTime QA Website: https://realtimeqa.github.io/
\\ ( https://arxiv.org/abs/2207.13332 ,  2198kb)
------------------------------------------------------------------------------
\\
arXiv:2211.07343
replaced with revised version Wed, 28 Feb 2024 12:51:09 GMT   (2315kb,D)

Title: Replacing Language Model for Style Transfer
Authors: Pengyu Cheng, Ruineng Li
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2211.07343 ,  2315kb)
------------------------------------------------------------------------------
\\
arXiv:2305.06647
replaced with revised version Wed, 28 Feb 2024 09:12:35 GMT   (3251kb,D)

Title: PROM: A Phrase-level Copying Mechanism with Pre-training for Abstractive
  Summarization
Authors: Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan
Categories: cs.CL
Comments: Accepted by COLING2024
\\ ( https://arxiv.org/abs/2305.06647 ,  3251kb)
------------------------------------------------------------------------------
\\
arXiv:2306.07951
replaced with revised version Wed, 28 Feb 2024 12:37:53 GMT   (5667kb,D)

Title: Questioning the Survey Responses of Large Language Models
Authors: Ricardo Dominguez-Olmedo, Moritz Hardt, Celestine Mendler-D\"unner
Categories: cs.CL
\\ ( https://arxiv.org/abs/2306.07951 ,  5667kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08543
replaced with revised version Wed, 28 Feb 2024 14:48:19 GMT   (309kb,D)

Title: MiniLLM: Knowledge Distillation of Large Language Models
Authors: Yuxian Gu, Li Dong, Furu Wei, Minlie Huang
Categories: cs.CL cs.AI
Comments: Published as a conference paper in ICLR 2024
\\ ( https://arxiv.org/abs/2306.08543 ,  309kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08796
replaced with revised version Wed, 28 Feb 2024 07:12:08 GMT   (341kb,D)

Title: Chinese Spelling Correction as Rephrasing Language Model
Authors: Linfeng Liu, Hongqiu Wu, Hai Zhao
Categories: cs.CL
Comments: Accepted by AAAI'2024
\\ ( https://arxiv.org/abs/2308.08796 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02691
replaced with revised version Wed, 28 Feb 2024 09:44:46 GMT   (31164kb,D)

Title: A Joint Study of Phrase Grounding and Task Performance in Vision and
  Language Models
Authors: Noriyuki Kojima, Hadar Averbuch-Elor, Yoav Artzi
Categories: cs.CL cs.CV
Comments: This was published in TMLR in 2024, on January 24th
\\ ( https://arxiv.org/abs/2309.02691 ,  31164kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16639
replaced with revised version Wed, 28 Feb 2024 04:45:17 GMT   (5191kb,D)

Title: MindShift: Leveraging Large Language Models for Mental-States-Based
  Problematic Smartphone Use Intervention
Authors: Ruolan Wu, Chun Yu, Xiaole Pan, Yujia Liu, Ningning Zhang, Yue Fu,
  Yuhan Wang, Zhi Zheng, Li Chen, Qiaolei Jiang, Xuhai Xu, Yuanchun Shi
Categories: cs.CL cs.AI cs.HC
Comments: Published at ACM CHI'24
MSC-class: 68U35
ACM-class: H.5.2; I.2.7
DOI: 10.1145/3613904.3642790
\\ ( https://arxiv.org/abs/2309.16639 ,  5191kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02174
replaced with revised version Wed, 28 Feb 2024 03:29:56 GMT   (2268kb,D)

Title: Ask Again, Then Fail: Large Language Models' Vacillations in Judgement
Authors: Qiming Xie, Zengzhi Wang, Yi Feng, and Rui Xia
Categories: cs.CL cs.AI cs.LG
Comments: Update abstract and mitigation results of fine-tuning the model on
  synthesized high-quality preference data with DPO algorithm
\\ ( https://arxiv.org/abs/2310.02174 ,  2268kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06474
replaced with revised version Wed, 28 Feb 2024 08:28:35 GMT   (703kb,D)

Title: Multilingual Jailbreak Challenges in Large Language Models
Authors: Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing
Categories: cs.CL
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.06474 ,  703kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20329
replaced with revised version Wed, 28 Feb 2024 15:47:11 GMT   (14558kb,D)

Title: InstructCoder: Instruction Tuning Large Language Models for Code Editing
Authors: Kaixin Li, Qisheng Hu, Xu Zhao, Hui Chen, Yuxi Xie, Tiedong Liu, Qizhe
  Xie, Junxian He
Categories: cs.CL cs.SE
\\ ( https://arxiv.org/abs/2310.20329 ,  14558kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07237
replaced with revised version Tue, 27 Feb 2024 22:28:52 GMT   (12902kb,D)

Title: In Search of the Long-Tail: Systematic Generation of Long-Tail
  Inferential Knowledge via Logical Rule Guided Search
Authors: Huihan Li, Yuting Ning, Zeyi Liao, Siyuan Wang, Xiang Lorraine Li,
  Ximing Lu, Wenting Zhao, Faeze Brahman, Yejin Choi, Xiang Ren
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2311.07237 ,  12902kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08385
replaced with revised version Wed, 28 Feb 2024 04:30:53 GMT   (9653kb,D)

Title: ChOiRe: Characterizing and Predicting Human Opinions with Chain of
  Opinion Reasoning
Authors: Xuan Long Do, Kenji Kawaguchi, Min-Yen Kan, Nancy F. Chen
Categories: cs.CL
Comments: 22 pages
\\ ( https://arxiv.org/abs/2311.08385 ,  9653kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09154
replaced with revised version Wed, 28 Feb 2024 10:43:12 GMT   (3409kb,D)

Title: CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models
Authors: Wenhong Zhu, Hongkun Hao, Zhiwei He, Yunze Song, Yumeng Zhang, Hanxu
  Hu, Yiran Wei, Rui Wang, Hongyuan Lu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.09154 ,  3409kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09693
replaced with revised version Wed, 28 Feb 2024 14:46:25 GMT   (71kb,D)

Title: BLT: Can Large Language Models Handle Basic Legal Text?
Authors: Andrew Blair-Stanek, Nils Holzenberger, Benjamin Van Durme
Categories: cs.CL cs.AI
ACM-class: I.2.1; I.2.7; J.7
\\ ( https://arxiv.org/abs/2311.09693 ,  71kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01661
replaced with revised version Wed, 28 Feb 2024 04:33:33 GMT   (1481kb,D)

Title: ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating
  Pre-university Math Questions
Authors: Phuoc Pham Van Long, Duc Anh Vu, Nhat M. Hoang, Xuan Long Do, Anh Tuan
  Luu
Categories: cs.CL cs.AI
Comments: Accepted at the 39th ACM/SIGAPP Symposium On Applied Computing (SAC
  2024), Main Conference
\\ ( https://arxiv.org/abs/2312.01661 ,  1481kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10126
replaced with revised version Wed, 28 Feb 2024 11:16:29 GMT   (11483kb,D)

Title: Do Text Simplification Systems Preserve Meaning? A Human Evaluation via
  Reading Comprehension
Authors: Sweta Agrawal, Marine Carpuat
Categories: cs.CL
Comments: Accepted at TACL (a pre-MIT Press publication version)
\\ ( https://arxiv.org/abs/2312.10126 ,  11483kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17267
replaced with revised version Wed, 28 Feb 2024 01:53:28 GMT   (649kb,D)

Title: Improving Low-resource Prompt-based Relation Representation with
  Multi-view Decoupling Learning
Authors: Chenghao Fan, Wei Wei, Xiaoye Qu, Zhenyi Lu, Wenfeng Xie, Yu Cheng,
  Dangyang Chen
Categories: cs.CL cs.AI
Comments: Accepted to AAAI 2024
\\ ( https://arxiv.org/abs/2312.17267 ,  649kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02982
replaced with revised version Wed, 28 Feb 2024 09:25:03 GMT   (7123kb,D)

Title: BIBench: Benchmarking Data Analysis Knowledge of Large Language Models
Authors: Shu Liu, Shangqing Zhao, Chenghao Jia, Xinlin Zhuang, Zhaoguang Long,
  Man Lan, Qingquan Wu, Chong Yang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.02982 ,  7123kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15371
replaced with revised version Wed, 28 Feb 2024 04:38:25 GMT   (0kb,I)

Title: LegalDuet: Learning Effective Representations for Legal Judgment
  Prediction through a Dual-View Legal Clue Reasoning
Authors: Pengjie Liu, Zhenghao Liu, Xiaoyuan Yi, Liner Yang, Shuo Wang, Yu Gu,
  Ge Yu, Xing Xie, Shuang-hua Yang
Categories: cs.CL
Comments: we will update this paper and revise this paper in the near future
\\ ( https://arxiv.org/abs/2401.15371 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00838
replaced with revised version Wed, 28 Feb 2024 02:26:07 GMT   (738kb,D)

Title: OLMo: Accelerating the Science of Language Models
Authors: Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney
  Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson,
  Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi
  Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel,
  Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha
  Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha
  Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant
  Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle
  Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A.
  Smith, Hannaneh Hajishirzi
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.00838 ,  738kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05119
replaced with revised version Wed, 28 Feb 2024 14:47:08 GMT   (5782kb,D)

Title: A Closer Look at the Limitations of Instruction Tuning
Authors: Sreyan Ghosh and Chandra Kiran Reddy Evuru and Sonal Kumar and
  Ramaneswaran S and Deepali Aneja and Zeyu Jin and Ramani Duraiswami and
  Dinesh Manocha
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.05119 ,  5782kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10693
replaced with revised version Wed, 28 Feb 2024 10:12:34 GMT   (3904kb,D)

Title: Exploring Precision and Recall to assess the quality and diversity of
  LLMs
Authors: Florian Le Bronnec, Alexandre Verine, Benjamin Negrevergne, Yann
  Chevaleyre, Alexandre Allauzen
Categories: cs.CL cs.LG
Comments: 21 pages, 15 figures, Under Review
\\ ( https://arxiv.org/abs/2402.10693 ,  3904kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11894
replaced with revised version Wed, 28 Feb 2024 04:21:09 GMT   (9700kb,D)

Title: Have Seen Me Before? Automating Dataset Updates Towards Reliable and
  Timely Evaluation
Authors: Jiahao Ying, Yixin Cao, Bo Wang, Wei Tang, Yizhe Yang, Shuicheng Yan
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.11894 ,  9700kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14798
replaced with revised version Tue, 27 Feb 2024 21:53:24 GMT   (592kb,D)

Title: Enhancing Systematic Decompositional Natural Language Inference Using
  Informal Logic
Authors: Nathaniel Weir, Kate Sanders, Orion Weller, Shreya Sharma, Dongwei
  Jiang, Zhengping Jiang, Bhavana Dalvi Mishra, Oyvind Tafjord, Peter Jansen,
  Peter Clark, Benjamin Van Durme
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.14798 ,  592kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15514
replaced with revised version Wed, 28 Feb 2024 00:03:57 GMT   (3705kb)

Title: Large Scale Generative AI Text Applied to Sports and Music
Authors: Aaron Baughman, Stephen Hammer, Rahul Agarwal, Gozde Akay, Eduardo
  Morales, Tony Johnson, Leonid Karlinsky, Rogerio Feris
Categories: cs.CL cs.AI
Comments: 9 pages, 8 figures, 5 tables
\\ ( https://arxiv.org/abs/2402.15514 ,  3705kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15861
replaced with revised version Wed, 28 Feb 2024 15:19:21 GMT   (8914kb,D)

Title: MATHWELL: Generating Educational Math Word Problems at Scale
Authors: Bryan R Christ, Jonathan Kropko, Thomas Hartvigsen
Categories: cs.CL
Comments: 20 pages, 9 figures
\\ ( https://arxiv.org/abs/2402.15861 ,  8914kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16261
replaced with revised version Wed, 28 Feb 2024 06:43:48 GMT   (1062kb,D)

Title: UniRetriever: Multi-task Candidates Selection for Various
  Context-Adaptive Conversational Retrieval
Authors: Hongru Wang, Boyang Xue, Baohang Zhou, Rui Wang, Fei Mi, Weichao Wang,
  Yasheng Wang, Kam-Fai Wong
Categories: cs.CL cs.IR
\\ ( https://arxiv.org/abs/2402.16261 ,  1062kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16363
replaced with revised version Wed, 28 Feb 2024 08:36:42 GMT   (1264kb,D)

Title: LLM Inference Unveiled: Survey and Roofline Model Insights
Authors: Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Chenhao Xue,
  Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, Beidi Chen, Guangyu
  Sun, Kurt Keutzer
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.16363 ,  1264kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16567
replaced with revised version Wed, 28 Feb 2024 07:24:19 GMT   (7972kb,D)

Title: Aligning Large Language Models to a Domain-specific Graph Database
Authors: Yuanyuan Liang, Keren Tan, Tingyu Xie, Wenbiao Tao, Siyuan Wang,
  Yunshi Lan, Weining Qian
Categories: cs.CL cs.AI cs.DB
Comments: 13 pages,2 figures
\\ ( https://arxiv.org/abs/2402.16567 ,  7972kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16671
replaced with revised version Wed, 28 Feb 2024 14:49:03 GMT   (555kb,D)

Title: StructLM: Towards Building Generalist Models for Structured Knowledge
  Grounding
Authors: Alex Zhuang, Ge Zhang, Tianyu Zheng, Xinrun Du, Junjie Wang, Weiming
  Ren, Stephen W. Huang, Jie Fu, Xiang Yue, Wenhu Chen
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.16671 ,  555kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16696
replaced with revised version Wed, 28 Feb 2024 03:31:28 GMT   (9366kb,D)

Title: Look Before You Leap: Towards Decision-Aware and Generalizable
  Tool-Usage for Large Language Models
Authors: Anchun Gui, Jian Li, Yong Dai, Nan Du, Han Xiao
Categories: cs.CL
Comments: 20 pages, 18 figures
\\ ( https://arxiv.org/abs/2402.16696 ,  9366kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17433
replaced with revised version Wed, 28 Feb 2024 03:34:00 GMT   (1496kb,D)

Title: Enhancing EEG-to-Text Decoding through Transferable Representations from
  Pre-trained Contrastive EEG-Text Masked Autoencoder
Authors: Jiaqi Wang, Zhenxi Song, Zhengyu Ma, Xipeng Qiu, Min Zhang, Zhiguo
  Zhang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.17433 ,  1496kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17493
replaced with revised version Wed, 28 Feb 2024 05:51:15 GMT   (1422kb)

Title: Prescribing Large Language Models for Perioperative Care: What's The
  Right Dose for Pre-trained Models?
Authors: Bing Xue, Charles Alba, Joanna Abraham, Thomas Kannampallil, Chenyang
  Lu
Categories: cs.CL
Comments: Supplemental file available at: http://tinyurl.com/mszmjna9 models
  publicly available at:
  https://huggingface.co/cja5553/BJH-perioperative-notes-bioGPT AND
  https://huggingface.co/cja5553/BJH-perioperative-notes-bioGPT
ACM-class: J.3; I.2.7
\\ ( https://arxiv.org/abs/2402.17493 ,  1422kb)
------------------------------------------------------------------------------
\\
arXiv:2207.06154
replaced with revised version Wed, 28 Feb 2024 08:20:03 GMT   (6361kb,D)

Title: On the Robustness of Bayesian Neural Networks to Adversarial Attacks
Authors: Luca Bortolussi, Ginevra Carbone, Luca Laurenti, Andrea Patane, Guido
  Sanguinetti, Matthew Wicker
Categories: cs.LG cs.AI cs.CR
Comments: arXiv admin note: text overlap with arXiv:2002.04359
\\ ( https://arxiv.org/abs/2207.06154 ,  6361kb)
------------------------------------------------------------------------------
\\
arXiv:2210.05021
replaced with revised version Tue, 27 Feb 2024 20:55:18 GMT   (9098kb,D)

Title: The good, the bad and the ugly sides of data augmentation: An implicit
  spectral regularization perspective
Authors: Chi-Heng Lin, Chiraag Kaushik, Eva L. Dyer, Vidya Muthukumar
Categories: cs.LG stat.ML
Comments: 72 pages, 8 figures
\\ ( https://arxiv.org/abs/2210.05021 ,  9098kb)
------------------------------------------------------------------------------
\\
arXiv:2210.07484
replaced with revised version Wed, 28 Feb 2024 01:37:49 GMT   (1063kb,D)

Title: Mutual Information Regularized Offline Reinforcement Learning
Authors: Xiao Ma, Bingyi Kang, Zhongwen Xu, Min Lin, Shuicheng Yan
Categories: cs.LG cs.AI
Comments: NeurIPS 2023
\\ ( https://arxiv.org/abs/2210.07484 ,  1063kb)
------------------------------------------------------------------------------
\\
arXiv:2210.16002
replaced with revised version Wed, 28 Feb 2024 09:33:16 GMT   (3314kb,D)

Title: Online Learning Models for Vehicle Usage Prediction During COVID-19
Authors: Tobias Lindroth, Axel Svensson, Niklas {\AA}kerblom, Mitra
  Pourabdollah, Morteza Haghir Chehreghani
Categories: cs.LG
Comments: This article has been accepted for publication in IEEE Transactions
  on Intelligent Transportation Systems. This is the author's version which has
  not been fully edited and content may change prior to final publication.
  Citation information: DOI 10.1109/TITS.2024.3361676
DOI: 10.1109/TITS.2024.3361676
\\ ( https://arxiv.org/abs/2210.16002 ,  3314kb)
------------------------------------------------------------------------------
\\
arXiv:2211.06567
replaced with revised version Tue, 27 Feb 2024 21:19:08 GMT   (850kb)

Title: Online Search with Predictions: Pareto-optimal Algorithm and its
  Applications in Energy Markets
Authors: Russell Lee, Bo Sun, Mohammad Hajiesmaili, John C.S. Lui
Categories: cs.LG cs.DS
\\ ( https://arxiv.org/abs/2211.06567 ,  850kb)
------------------------------------------------------------------------------
\\
arXiv:2212.06361
replaced with revised version Wed, 28 Feb 2024 18:38:19 GMT   (901kb)

Title: Numerical Stability of DeepGOPlus Inference
Authors: In\'es Gonzalez Pepe, Yohan Chatelain, Gregory Kiar, Tristan Glatard
Categories: cs.LG q-bio.QM
Comments: 17 pages, 5 figures, 4 tables with 3 figures, 2 tables in Appendix
Journal-ref: Vol 19, no. 1 (2024): e0296725
DOI: 10.1371/journal.pone.0296725
\\ ( https://arxiv.org/abs/2212.06361 ,  901kb)
------------------------------------------------------------------------------
\\
arXiv:2302.12148
replaced with revised version Wed, 28 Feb 2024 09:32:02 GMT   (183kb,D)

Title: Streaming data recovery via Bayesian tensor train decomposition
Authors: Yunyu Huang, Yani Feng, Qifeng Liao
Categories: cs.LG math.ST stat.ML stat.TH
\\ ( https://arxiv.org/abs/2302.12148 ,  183kb)
------------------------------------------------------------------------------
\\
arXiv:2303.01687
replaced with revised version Tue, 27 Feb 2024 22:01:30 GMT   (2725kb,D)

Title: Differentially Private Neural Tangent Kernels for Privacy-Preserving
  Data Generation
Authors: Yilin Yang, Kamil Adamczewski, Danica J. Sutherland, Xiaoxiao Li,
  Mijung Park
Categories: cs.LG cs.CR cs.CV
\\ ( https://arxiv.org/abs/2303.01687 ,  2725kb)
------------------------------------------------------------------------------
\\
arXiv:2304.05055
replaced with revised version Wed, 28 Feb 2024 01:55:09 GMT   (857kb,D)

Title: A Comprehensive Survey on Deep Graph Representation Learning
Authors: Wei Ju, Zheng Fang, Yiyang Gu, Zequn Liu, Qingqing Long, Ziyue Qiao,
  Yifang Qin, Jianhao Shen, Fang Sun, Zhiping Xiao, Junwei Yang, Jingyang Yuan,
  Yusheng Zhao, Yifan Wang, Xiao Luo, Ming Zhang
Categories: cs.LG cs.AI cs.IR
Comments: Accepted by Neural Networks 2024
DOI: 10.1016/j.neunet.2024.106207
\\ ( https://arxiv.org/abs/2304.05055 ,  857kb)
------------------------------------------------------------------------------
\\
arXiv:2304.05836
replaced with revised version Wed, 28 Feb 2024 14:08:09 GMT   (723kb,D)

Title: A Game-theoretic Framework for Privacy-preserving Federated Learning
Authors: Xiaojin Zhang, Lixin Fan, Siwei Wang, Wenjie Li, Kai Chen, Qiang Yang
Categories: cs.LG cs.AI cs.CR cs.GT
\\ ( https://arxiv.org/abs/2304.05836 ,  723kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13404
replaced with revised version Wed, 28 Feb 2024 00:24:08 GMT   (2060kb,D)

Title: Improving Convergence and Generalization Using Parameter Symmetries
Authors: Bo Zhao, Robert M. Gower, Robin Walters, Rose Yu
Categories: cs.LG math.OC
Comments: 28 pages, 13 figures, ICLR 2024
\\ ( https://arxiv.org/abs/2305.13404 ,  2060kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18400
replaced with revised version Wed, 28 Feb 2024 13:45:53 GMT   (2503kb,D)

Title: A Meta-learning Framework for Tuning Parameters of Protection Mechanisms
  in Trustworthy Federated Learning
Authors: Xiaojin Zhang, Yan Kang, Lixin Fan, Kai Chen, Qiang Yang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2305.18400 ,  2503kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19044
replaced with revised version Wed, 28 Feb 2024 16:40:38 GMT   (2539kb,D)

Title: Exploring the Promise and Limits of Real-Time Recurrent Learning
Authors: Kazuki Irie, Anand Gopalakrishnan, J\"urgen Schmidhuber
Categories: cs.LG
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2305.19044 ,  2539kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19523
replaced with revised version Wed, 28 Feb 2024 09:01:41 GMT   (1279kb,D)

Title: Harnessing Explanations: LLM-to-LM Interpreter for Enhanced
  Text-Attributed Graph Representation Learning
Authors: Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun,
  Bryan Hooi
Categories: cs.LG
Comments: In Proceedings of ICLR 2024
\\ ( https://arxiv.org/abs/2305.19523 ,  1279kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05751
replaced with revised version Wed, 28 Feb 2024 04:01:47 GMT   (2605kb,D)

Title: Advancing Counterfactual Inference through Nonlinear Quantile Regression
Authors: Shaoan Xie, Biwei Huang, Bin Gu, Tongliang Liu, Kun Zhang
Categories: cs.LG stat.ME
\\ ( https://arxiv.org/abs/2306.05751 ,  2605kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10426
replaced with revised version Tue, 27 Feb 2024 20:14:47 GMT   (3485kb,D)

Title: Understanding Certified Training with Interval Bound Propagation
Authors: Yuhao Mao, Mark Niklas M\"uller, Marc Fischer, Martin Vechev
Categories: cs.LG cs.AI
Comments: ICLR'24
\\ ( https://arxiv.org/abs/2306.10426 ,  3485kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12198
replaced with revised version Wed, 28 Feb 2024 16:18:11 GMT   (227kb,D)

Title: NCART: Neural Classification and Regression Tree for Tabular Data
Authors: Jiaqi Luo, Shixin Xu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2307.12198 ,  227kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12532
replaced with revised version Wed, 28 Feb 2024 06:56:20 GMT   (4918kb,D)

Title: FedSOL: Stabilized Orthogonal Learning in Federated Learning
Authors: Gihun Lee, Minchan Jeong, Sangmook Kim, Jaehoon Oh, Se-Young Yun
Categories: cs.LG cs.AI cs.CV
Comments: The IEEE/CVF Conference on Computer Vision and Pattern Recognition
  2024 (CVPR 2024)
\\ ( https://arxiv.org/abs/2308.12532 ,  4918kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13294
replaced with revised version Wed, 28 Feb 2024 15:48:09 GMT   (1004kb,D)

Title: Training normalizing flows with computationally intensive target
  probability distributions
Authors: Piotr Bialas, Piotr Korcyl, Tomasz Stebel
Categories: cs.LG cond-mat.stat-mech hep-lat
Comments: 16 pages, 5 figures, 6 tables, 3 listings. Revised version as
  published in CPC. Added results for a other values of hoping parameter
MSC-class: cc:68T07
ACM-class: J.2; I.2.6
Journal-ref: Computer Physics Communications 2024 109094
DOI: 10.1016/j.cpc.2024.109094
\\ ( https://arxiv.org/abs/2308.13294 ,  1004kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08646
replaced with revised version Wed, 28 Feb 2024 05:56:43 GMT   (6055kb,D)

Title: CoCA: Fusing Position Embedding with Collinear Constrained Attention in
  Transformers for Long Context Window Extending
Authors: Shiyi Zhu, Jing Ye, Wei Jiang, Siqiao Xue, Qi Zhang, Yifan Wu, Jianguo
  Li
Categories: cs.LG cs.AI cs.CL
Comments: 16 pages, 7 figures
\\ ( https://arxiv.org/abs/2309.08646 ,  6055kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10688
replaced with revised version Tue, 27 Feb 2024 21:52:16 GMT   (1238kb,D)

Title: On the different regimes of Stochastic Gradient Descent
Authors: Antonio Sclocchi and Matthieu Wyart
Categories: cs.LG cond-mat.dis-nn stat.ML
Comments: Main: 8 pages, 4 figures; Appendix: 15 pages, 11 figures
Journal-ref: Proceedings of the National Academy of Sciences 121.9 (2024):
  e2316301121
DOI: 10.1073/pnas.2316301121
\\ ( https://arxiv.org/abs/2309.10688 ,  1238kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15669
replaced with revised version Wed, 28 Feb 2024 04:22:41 GMT   (9345kb,D)

Title: On the Computational Entanglement of Distant Features in Adversarial
  Machine Learning
Authors: YenLung Lai, Xingbo Dong, Zhe Jin
Categories: cs.LG cs.IT math.IT physics.comp-ph
Comments: The latest version has titled updated
\\ ( https://arxiv.org/abs/2309.15669 ,  9345kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05227
replaced with revised version Wed, 28 Feb 2024 18:04:37 GMT   (15766kb,D)

Title: Physics-aware Machine Learning Revolutionizes Scientific Paradigm for
  Machine Learning and Process-based Hydrology
Authors: Qingsong Xu, Yilei Shi, Jonathan Bamber, Ye Tuo, Ralf Ludwig, Xiao
  Xiang Zhu
Categories: cs.LG cs.AI physics.flu-dyn
Comments: 33 pages, 6 figures
\\ ( https://arxiv.org/abs/2310.05227 ,  15766kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05333
replaced with revised version Wed, 28 Feb 2024 13:48:09 GMT   (1718kb,D)

Title: DiffCPS: Diffusion Model based Constrained Policy Search for Offline
  Reinforcement Learning
Authors: Longxiang He, Li Shen, Linrui Zhang, Junbo Tan, Xueqian Wang
Categories: cs.LG
Comments: 22 pages, 9 figures, 6 tables. Submitted to ICML 2024. arXiv admin
  note: text overlap with arXiv:1910.13393 by other authors
\\ ( https://arxiv.org/abs/2310.05333 ,  1718kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11730
replaced with revised version Wed, 28 Feb 2024 05:04:34 GMT   (6611kb,D)

Title: Federated Heterogeneous Graph Neural Network for Privacy-preserving
  Recommendation
Authors: Bo Yan, Yang Cao, Haoyu Wang, Wenchuan Yang, Junping Du, Chuan Shi
Categories: cs.LG cs.AI cs.CR cs.DC
Comments: Accepted by WWW 2024
DOI: 10.1145/3589334.3645693
\\ ( https://arxiv.org/abs/2310.11730 ,  6611kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17273
replaced with revised version Wed, 28 Feb 2024 18:12:21 GMT   (12375kb,D)

Title: Looping in the Human Collaborative and Explainable Bayesian Optimization
Authors: Masaki Adachi, Brady Planden, David A. Howey, Michael A. Osborne,
  Sebastian Orbell, Natalia Ares, Krikamol Muandet, Siu Lun Chau
Categories: cs.LG cs.HC stat.ML
Comments: Accepted at AISTATS 2024, 24 pages, 11 figures
MSC-class: 62C10, 62F15
\\ ( https://arxiv.org/abs/2310.17273 ,  12375kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17491
replaced with revised version Wed, 28 Feb 2024 13:47:33 GMT   (2625kb,D)

Title: FedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine
  Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation
  Models with Mobile Edge Computing
Authors: Terence Jie Chua, Wenhan Yu, Jun Zhao, Kwok-Yan Lam
Categories: cs.LG cs.NI
\\ ( https://arxiv.org/abs/2310.17491 ,  2625kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14534
replaced with revised version Wed, 28 Feb 2024 13:58:20 GMT   (1297kb,D)

Title: Finding Foundation Models for Time Series Classification with a PreText
  Task
Authors: Ali Ismail-Fawaz, Maxime Devanne, Stefano Berretti, Jonathan Weber,
  Germain Forestier
Categories: cs.LG
\\ ( https://arxiv.org/abs/2311.14534 ,  1297kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14649
replaced with revised version Wed, 28 Feb 2024 15:56:48 GMT   (3075kb,D)

Title: Learning in Deep Factor Graphs with Gaussian Belief Propagation
Authors: Seth Nabarro, Mark van der Wilk, Andrew J Davison
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2311.14649 ,  3075kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02614
replaced with revised version Wed, 28 Feb 2024 04:42:46 GMT   (2243kb,D)

Title: Prompt Optimization via Adversarial In-Context Learning
Authors: Xuan Long Do, Yiran Zhao, Hannah Brown, Yuxi Xie, James Xu Zhao, Nancy
  F. Chen, Kenji Kawaguchi, Michael Qizhe Xie, Junxian He
Categories: cs.LG cs.CL
Comments: 20 pages
\\ ( https://arxiv.org/abs/2312.02614 ,  2243kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05440
replaced with revised version Tue, 27 Feb 2024 19:21:41 GMT   (2991kb,D)

Title: Consistency Models for Scalable and Fast Simulation-Based Inference
Authors: Marvin Schmitt, Valentin Pratz, Ullrich K\"othe, Paul-Christian
  B\"urkner, Stefan T Radev
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2312.05440 ,  2991kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03748
replaced with revised version Wed, 28 Feb 2024 09:44:16 GMT   (1512kb,D)

Title: Towards Efficient Communication and Secure Federated Recommendation
  System via Low-rank Training
Authors: Ngoc-Hieu Nguyen, Tuan-Anh Nguyen, Tuan Nguyen, Vu Tien Hoang, Dung D.
  Le, Kok-Seng Wong
Categories: cs.LG cs.CR cs.DC cs.IR
Comments: 12 pages, 6 figures, 4 tables
DOI: 10.1145/3589334.3645702
\\ ( https://arxiv.org/abs/2401.03748 ,  1512kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11618
replaced with revised version Wed, 28 Feb 2024 16:37:00 GMT   (4003kb,D)

Title: Efficient local linearity regularization to overcome catastrophic
  overfitting
Authors: Elias Abad Rocamora, Fanghui Liu, Grigorios G. Chrysos, Pablo M.
  Olmos, Volkan Cevher
Categories: cs.LG cs.AI cs.CR stat.ML
Comments: Accepted in ICLR 2024
\\ ( https://arxiv.org/abs/2401.11618 ,  4003kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01440
replaced with revised version Wed, 28 Feb 2024 11:42:01 GMT   (867kb,D)

Title: Few-Shot Learning on Graphs: from Meta-learning to Pre-training and
  Prompting
Authors: Xingtong Yu, Yuan Fang, Zemin Liu, Yuxia Wu, Zhihao Wen, Jianyuan Bo,
  Xinming Zhang and Steven C.H. Hoi
Categories: cs.LG cs.AI cs.SI
\\ ( https://arxiv.org/abs/2402.01440 ,  867kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04830
replaced with revised version Tue, 27 Feb 2024 21:26:05 GMT   (167kb,D)

Title: Closing the Gap Between SGP4 and High-Precision Propagation via
  Differentiable Programming
Authors: Giacomo Acciarini, At{\i}l{\i}m G\"une\c{s} Baydin, Dario Izzo
Categories: cs.LG astro-ph.EP
\\ ( https://arxiv.org/abs/2402.04830 ,  167kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10193
replaced with revised version Wed, 28 Feb 2024 03:42:10 GMT   (2934kb,D)

Title: BitDelta: Your Fine-Tune May Only Be Worth One Bit
Authors: James Liu, Guangxuan Xiao, Kai Li, Jason D. Lee, Song Han, Tri Dao,
  Tianle Cai
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2402.10193 ,  2934kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10228
replaced with revised version Wed, 28 Feb 2024 15:08:00 GMT   (3641kb,D)

Title: HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement
  Learning Framework for Complex Environments
Authors: Yingru Li, Jiawei Xu, Lei Han, Zhi-Quan Luo
Categories: cs.LG cs.AI stat.ML
Comments: Bridging the theory and practice!
\\ ( https://arxiv.org/abs/2402.10228 ,  3641kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10240
replaced with revised version Tue, 27 Feb 2024 22:11:42 GMT   (4312kb,D)

Title: A Dynamical View of the Question of Why
Authors: Mehdi Fatemi and Sindhu Gowda
Categories: cs.LG cs.AI cs.SY eess.SY
Comments: Accepted at the Twelfth International Conference on Learning
  Representations (ICLR'24)
\\ ( https://arxiv.org/abs/2402.10240 ,  4312kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11917
replaced with revised version Wed, 28 Feb 2024 13:31:19 GMT   (1524kb,D)

Title: A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step
  Reasoning Task
Authors: Jannik Brinkmann, Abhay Sheshadri, Victor Levoso, Paul Swoboda,
  Christian Bartelt
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.11917 ,  1524kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12921
replaced with revised version Wed, 28 Feb 2024 14:36:32 GMT   (1975kb,D)

Title: Right on Time: Revising Time Series Models by Constraining their
  Explanations
Authors: Maurice Kraus, David Steinmann, Antonia W\"ust, Andre Kokozinski,
  Kristian Kersting
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.12921 ,  1975kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15160
replaced with revised version Wed, 28 Feb 2024 13:46:47 GMT   (26306kb,D)

Title: Spatially-Aware Transformer Memory for Embodied Agents
Authors: Junmo Cho, Jaesik Yoon, Sungjin Ahn
Categories: cs.LG cs.AI
Comments: ICLR 2024 Spotlight. First two authors contributed equally
\\ ( https://arxiv.org/abs/2402.15160 ,  26306kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15179
replaced with revised version Wed, 28 Feb 2024 05:09:23 GMT   (1179kb,D)

Title: Advancing Parameter Efficiency in Fine-tuning via Representation Editing
Authors: Muling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li, Changze Lv, Zixuan
  Ling, Jianhao Zhu, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2402.15179 ,  1179kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15194
replaced with revised version Wed, 28 Feb 2024 09:21:46 GMT   (5691kb,D)

Title: Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized
  Control
Authors: Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali,
  Gabriele Scalia, Nathaniel Lee Diamant, Alex M Tseng, Tommaso Biancalani,
  Sergey Levine
Categories: cs.LG cs.AI stat.ML
Comments: Under review (codes will be released soon)
\\ ( https://arxiv.org/abs/2402.15194 ,  5691kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15490
replaced with revised version Wed, 28 Feb 2024 08:51:35 GMT   (5648kb,D)

Title: A Comprehensive Survey of Convolutions in Deep Learning: Applications,
  Challenges, and Future Trends
Authors: Abolfazl Younesi, Mohsen Ansari, MohammadAmin Fazli, Alireza Ejlali,
  Muhammad Shafique, J\"org Henkel
Categories: cs.LG cs.NE
\\ ( https://arxiv.org/abs/2402.15490 ,  5648kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15808
replaced with revised version Tue, 27 Feb 2024 20:08:16 GMT   (2656kb,D)

Title: Optimal Zero-Shot Detector for Multi-Armed Attacks
Authors: Federica Granese, Marco Romanelli, Pablo Piantanida
Categories: cs.LG cs.AI cs.CR
Comments: Accepted to appear in the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS), May 2nd - May 4th, 2024 This article
  supersedes arXiv:2302.02216
\\ ( https://arxiv.org/abs/2402.15808 ,  2656kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16897
replaced with revised version Wed, 28 Feb 2024 09:58:46 GMT   (3867kb,D)

Title: Reliable Conflictive Multi-View Learning
Authors: Cai Xu, Jiajun Si, Ziyu Guan, Wei Zhao, Yue Wu, Xiyue Gao
Categories: cs.LG cs.AI
Comments: 9 pages and to be appeared in AAAI2024
\\ ( https://arxiv.org/abs/2402.16897 ,  3867kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17570
replaced with revised version Wed, 28 Feb 2024 03:16:39 GMT   (1672kb,D)

Title: Sparse Variational Contaminated Noise Gaussian Process Regression for
  Forecasting Geomagnetic Perturbations
Authors: Daniel Iong, Matthew McAnear, Yuezhou Qu, Shasha Zou, Gabor Toth, Yang
  Chen
Categories: cs.LG stat.AP stat.ME
\\ ( https://arxiv.org/abs/2402.17570 ,  1672kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17599
replaced with revised version Wed, 28 Feb 2024 10:46:07 GMT   (127kb,D)

Title: DAGnosis: Localized Identification of Data Inconsistencies using
  Structures
Authors: Nicolas Huynh, Jeroen Berrevoets, Nabeel Seedat, Jonathan Crabb\'e,
  Zhaozhi Qian, Mihaela van der Schaar
Categories: cs.LG cs.AI stat.ML
Comments: AISTATS 2024; added correspondance email
\\ ( https://arxiv.org/abs/2402.17599 ,  127kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17690
replaced with revised version Wed, 28 Feb 2024 15:53:07 GMT   (6497kb,D)

Title: Autonomous Vehicles: Evolution of Artificial Intelligence and Learning
  Algorithms
Authors: Divya Garikapati and Sneha Sudhir Shetiya
Categories: cs.LG cs.AI
Comments: 13 pages
\\ ( https://arxiv.org/abs/2402.17690 ,  6497kb)
------------------------------------------------------------------------------
\\
arXiv:2104.03893
replaced with revised version Tue, 27 Feb 2024 22:49:26 GMT   (12281kb,D)

Title: Multimodal Fusion of EMG and Vision for Human Grasp Intent Inference in
  Prosthetic Hand Control
Authors: Mehrshad Zandigohar, Mo Han, Mohammadreza Sharif, Sezen Yagmur Gunay,
  Mariusz P. Furmanek, Mathew Yarossi, Paolo Bonato, Cagdas Onal, Taskin Padir,
  Deniz Erdogmus, Gunar Schirner
Categories: cs.RO cs.AI cs.CV cs.HC eess.SP
ACM-class: I.5.4; I.2.9
Journal-ref: Front. Robot. AI 11 (2024) Sec. Biomedical Robotics
DOI: 10.3389/frobt.2024.1312554
\\ ( https://arxiv.org/abs/2104.03893 ,  12281kb)
------------------------------------------------------------------------------
\\
arXiv:2304.06607
replaced with revised version Wed, 28 Feb 2024 04:12:37 GMT   (2520kb,D)

Title: False Claims against Model Ownership Resolution
Authors: Jian Liu, Rui Zhang, Sebastian Szyller, Kui Ren, N. Asokan
Categories: cs.CR cs.AI
Comments: 13pages,3 figures. To appear in the 33rd USENIX Security Symposium
  (USENIX Security '24)
\\ ( https://arxiv.org/abs/2304.06607 ,  2520kb)
------------------------------------------------------------------------------
\\
arXiv:2304.11821
replaced with revised version Wed, 28 Feb 2024 05:01:31 GMT   (36170kb,D)

Title: Interruption-Aware Cooperative Perception for V2X Communication-Aided
  Autonomous Driving
Authors: Shunli Ren, Zixing Lei, Zi Wang, Mehrdad Dianati, Yafei Wang, Siheng
  Chen, Wenjun Zhang
Categories: cs.RO cs.AI cs.CV
Journal-ref: IEEE Transactions on Intelligent Vehicles 2024
\\ ( https://arxiv.org/abs/2304.11821 ,  36170kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04366
replaced with revised version Wed, 28 Feb 2024 03:08:06 GMT   (2092kb,D)

Title: Eicient Recruitment Strategy for Collaborative Mobile Crowd Sensing
  Based on GCN Trustworthiness Prediction
Authors: Zhongwei Zhan, Yingjie Wang, Peiyong Duan, Akshita Maradapu Vera
  Venkata Sai, Zhaowei Liu, Chaocan Xiang, Xiangrong Tong, Weilong Wang,
  Zhipeng Cai
Categories: cs.SI cs.AI cs.HC cs.LG
\\ ( https://arxiv.org/abs/2306.04366 ,  2092kb)
------------------------------------------------------------------------------
\\
arXiv:2306.11180
replaced with revised version Wed, 28 Feb 2024 11:06:42 GMT   (38781kb,D)

Title: Hyperbolic Active Learning for Semantic Segmentation under Domain Shift
Authors: Luca Franco, Paolo Mandica, Konstantinos Kallidromitis, Devin
  Guillory, Yu-Teng Li, Trevor Darrell, Fabio Galasso
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2306.11180 ,  38781kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12545
replaced with revised version Wed, 28 Feb 2024 02:24:09 GMT   (4836kb,D)

Title: Towards Video Anomaly Retrieval from Video Anomaly Detection: New
  Benchmarks and Model
Authors: Peng Wu, Jing Liu, Xiangteng He, Yuxin Peng, Peng Wang, and Yanning
  Zhang
Categories: cs.CV cs.AI
Comments: This work has been accepted to the IEEE TIP. Copyright has been
  transferred
\\ ( https://arxiv.org/abs/2307.12545 ,  4836kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13220 (*cross-listing*)
replaced with revised version Wed, 28 Feb 2024 11:56:57 GMT   (29762kb)

Title: One for Multiple: Physics-informed Synthetic Data Boosts Generalizable
  Deep Learning for Fast MRI Reconstruction
Authors: Zi Wang, Xiaotong Yu, Chengyan Wang, Weibo Chen, Jiazheng Wang,
  Ying-Hua Chu, Hongwei Sun, Rushuai Li, Peiyong Li, Fan Yang, Haiwei Han,
  Taishan Kang, Jianzhong Lin, Chen Yang, Shufu Chang, Zhang Shi, Sha Hua, Yan
  Li, Juan Hu, Liuhong Zhu, Jianjun Zhou, Meijing Lin, Jiefeng Guo, Congbo Cai,
  Zhong Chen, Di Guo, Guang Yang, Xiaobo Qu
Categories: eess.IV cs.AI physics.med-ph
Comments: 38 pages, 19 figures, 5 tables
\\ ( https://arxiv.org/abs/2307.13220 ,  29762kb)
------------------------------------------------------------------------------
\\
arXiv:2308.16572
replaced with revised version Wed, 28 Feb 2024 08:31:17 GMT   (2658kb,D)

Title: CL-MAE: Curriculum-Learned Masked Autoencoders
Authors: Neelu Madan, Nicolae-Catalin Ristea, Kamal Nasrollahi, Thomas B.
  Moeslund, Radu Tudor Ionescu
Categories: cs.CV cs.AI cs.LG
Comments: Accepted at WACV 2024
\\ ( https://arxiv.org/abs/2308.16572 ,  2658kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15036
replaced with revised version Wed, 28 Feb 2024 02:24:28 GMT   (554kb)

Title: UWB Based Static Gesture Classification
Authors: Abhishek Sebastian
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2310.15036 ,  554kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19620
replaced with revised version Wed, 28 Feb 2024 07:37:51 GMT   (4800kb,D)

Title: Large Trajectory Models are Scalable Motion Predictors and Planners
Authors: Qiao Sun, Shiduo Zhang, Danjiao Ma, Jingzhe Shi, Derun Li, Simian Luo,
  Yu Wang, Ningyi Xu, Guangzhi Cao, Hang Zhao
Categories: cs.RO cs.AI cs.CV
\\ ( https://arxiv.org/abs/2310.19620 ,  4800kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09015 (*cross-listing*)
replaced with revised version Wed, 28 Feb 2024 18:36:15 GMT   (60kb,D)

Title: Identification and Estimation for Nonignorable Missing Data: A Data
  Fusion Approach
Authors: Zixiao Wang, AmirEmad Ghassami, Ilya Shpitser
Categories: stat.ME cs.AI
Comments: 22 pages, 3 figures
\\ ( https://arxiv.org/abs/2311.09015 ,  60kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13127
replaced with revised version Wed, 28 Feb 2024 01:05:31 GMT   (40036kb,D)

Title: Toward Robust Imperceptible Perturbation against Unauthorized
  Text-to-image Diffusion-based Synthesis
Authors: Yixin Liu, Chenrui Fan, Yutong Dai, Xun Chen, Pan Zhou, and Lichao Sun
Categories: cs.CV cs.AI cs.CR
Comments: Accepted to CVPR 2024
\\ ( https://arxiv.org/abs/2311.13127 ,  40036kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16503
replaced with revised version Wed, 28 Feb 2024 16:58:20 GMT   (27497kb,D)

Title: TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models
Authors: Yushi Huang, Ruihao Gong, Jing Liu, Tianlong Chen, Xianglong Liu
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2311.16503 ,  27497kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10797
replaced with revised version Wed, 28 Feb 2024 04:08:21 GMT   (4283kb,D)

Title: Large-Scale Multi-Robot Coverage Path Planning via Local Search
Authors: Jingtao Tang, Hang Ma
Categories: cs.RO cs.AI cs.MA
Comments: Accepted to AAAI 2024
\\ ( https://arxiv.org/abs/2312.10797 ,  4283kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12258
replaced with revised version Tue, 27 Feb 2024 19:36:43 GMT   (1402kb,D)

Title: Emergent Dominance Hierarchies in Reinforcement Learning Agents
Authors: Ram Rachum, Yonatan Nakar, Bill Tomlinson, Nitay Alon, Reuth Mirsky
Categories: cs.MA cs.AI cs.GT cs.LG
\\ ( https://arxiv.org/abs/2401.12258 ,  1402kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13301
replaced with revised version Wed, 28 Feb 2024 12:37:34 GMT   (919kb,D)

Title: Structure-informed Positional Encoding for Music Generation
Authors: Manvi Agarwal (S2A, IDS), Changhong Wang (S2A, IDS), Ga\"el Richard
  (S2A, IDS)
Categories: cs.SD cs.AI eess.AS
Journal-ref: IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), Apr 2024, Seoul, South Korea
\\ ( https://arxiv.org/abs/2402.13301 ,  919kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13573
replaced with revised version Wed, 28 Feb 2024 18:31:50 GMT   (5166kb,D)

Title: ToDo: Token Downsampling for Efficient Generation of High-Resolution
  Images
Authors: Ethan Smith, Nayan Saxena, Aninda Saha
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.13573 ,  5166kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13754 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 19:15:23 GMT   (13840kb,D)

Title: Reinforcement learning-assisted quantum architecture search for
  variational quantum algorithms
Authors: Akash Kundu
Categories: quant-ph cs.AI cs.LG
Comments: With 154 pages and 46 figures, here lies my PhD thesis. Typos
  corrected
\\ ( https://arxiv.org/abs/2402.13754 ,  13840kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14095
replaced with revised version Tue, 27 Feb 2024 20:04:43 GMT   (3906kb,D)

Title: Zero-shot generalization across architectures for visual classification
Authors: Evan Gerritz, Luciano Dyballa, Steven W. Zucker
Categories: cs.CV cs.AI cs.LG
Comments: Accepted as a Tiny Paper at ICLR 24
\\ ( https://arxiv.org/abs/2402.14095 ,  3906kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14658
replaced with revised version Wed, 28 Feb 2024 03:15:24 GMT   (2930kb,D)

Title: OpenCodeInterpreter: Integrating Code Generation with Execution and
  Refinement
Authors: Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin,
  Jie Fu, Wenhu Chen, and Xiang Yue
Categories: cs.SE cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.14658 ,  2930kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15027
replaced with revised version Wed, 28 Feb 2024 14:21:52 GMT   (751kb)

Title: Multi-stakeholder Perspective on Responsible Artificial Intelligence and
  Acceptability in Education
Authors: A.J. Karran, P. Charland, J-T. Martineau, A. Ortiz de Guinea Lopez de
  Arana, AM. Lesage, S. Senecal, P-M. Leger
Categories: cs.CY cs.AI cs.HC
Comments: 28 pages, 2 appendices, 3 figures, 5 tables, original research
ACM-class: K.3.1; I.2.0
\\ ( https://arxiv.org/abs/2402.15027 ,  751kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15276
replaced with revised version Wed, 28 Feb 2024 16:49:13 GMT   (3555kb,D)

Title: Text2Pic Swift: Enhancing Long-Text to Image Retrieval for Large-Scale
  Libraries
Authors: Zijun Long and Xuri Ge and Richard Mccreadie and Joemon Jose
Categories: cs.IR cs.AI cs.CV
\\ ( https://arxiv.org/abs/2402.15276 ,  3555kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15945
replaced with revised version Tue, 27 Feb 2024 19:27:42 GMT   (424kb)

Title: Attention-GAN for Anomaly Detection: A Cutting-Edge Approach to
  Cybersecurity Threat Management
Authors: Mohammed Abo Sen
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2402.15945 ,  424kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15990
replaced with revised version Wed, 28 Feb 2024 05:58:18 GMT   (2155kb,D)

Title: An Empirical Study of Challenges in Machine Learning Asset Management
Authors: Zhimin Zhao, Yihao Chen, Abdul Ali Bangash, Bram Adams, Ahmed E.
  Hassan
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2402.15990 ,  2155kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17177
replaced with revised version Wed, 28 Feb 2024 18:20:20 GMT   (24962kb,D)

Title: Sora: A Review on Background, Technology, Limitations, and Opportunities
  of Large Vision Models
Authors: Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen,
  Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, and Lichao
  Sun
Categories: cs.CV cs.AI cs.LG
Comments: 37 pages, 18 figures; this is not an official report; GitHub:
  https://github.com/lichao-sun/SoraReview
\\ ( https://arxiv.org/abs/2402.17177 ,  24962kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17652
replaced with revised version Wed, 28 Feb 2024 17:27:48 GMT   (652kb,D)

Title: Compass: A Decentralized Scheduler for Latency-Sensitive ML Workflows
Authors: Yuting Yang, Andrea Merlina, Weijia Song, Tiancheng Yuan, Ken Birman,
  Roman Vitenberg
Categories: cs.DC cs.AI
\\ ( https://arxiv.org/abs/2402.17652 ,  652kb)
------------------------------------------------------------------------------
\\
arXiv:2305.10734
replaced with revised version Wed, 28 Feb 2024 12:10:19 GMT   (634kb,D)

Title: Diffusion-Based Speech Enhancement with Joint Generative and Predictive
  Decoders
Authors: Hao Shi, Kazuki Shimada, Masato Hirano, Takashi Shibuya, Yuichiro
  Koyama, Zhi Zhong, Shusuke Takahashi, Tatsuya Kawahara, Yuki Mitsufuji
Categories: cs.SD cs.CL eess.AS
\\ ( https://arxiv.org/abs/2305.10734 ,  634kb)
------------------------------------------------------------------------------
\\
arXiv:2306.11943
replaced with revised version Tue, 27 Feb 2024 21:24:14 GMT   (17964kb,D)

Title: Towards Understanding What Code Language Models Learned
Authors: Toufique Ahmed, Dian Yu, Chengxuan Huang, Cathy Wang, Prem Devanbu,
  Kenji Sagae
Categories: cs.SE cs.CL cs.LG
\\ ( https://arxiv.org/abs/2306.11943 ,  17964kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14968
replaced with revised version Tue, 27 Feb 2024 21:27:53 GMT   (8918kb,D)

Title: Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment
Authors: Jiongxiao Wang, Jiazhao Li, Yiquan Li, Xiangyu Qi, Junjie Hu, Yixuan
  Li, Patrick McDaniel, Muhao Chen, Bo Li, Chaowei Xiao
Categories: cs.CR cs.CL
\\ ( https://arxiv.org/abs/2402.14968 ,  8918kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16977
replaced with revised version Wed, 28 Feb 2024 11:12:07 GMT   (341kb,D)

Title: Dealing with Data for RE: Mitigating Challenges while using NLP and
  Generative AI
Authors: Smita Ghaisas and Anmol Singhal
Categories: cs.SE cs.CL
Comments: 24 pages, 2 figures, to be published in NLP for Requirements
  Engineering Book
\\ ( https://arxiv.org/abs/2402.16977 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2107.12365 (*cross-listing*)
replaced with revised version Wed, 28 Feb 2024 17:22:09 GMT   (1078kb,D)

Title: Inference for Heteroskedastic PCA with Missing Data
Authors: Yuling Yan, Yuxin Chen, Jianqing Fan
Categories: math.ST cs.IT cs.LG math.IT stat.ME stat.ML stat.TH
Comments: accepted to Annals of Statistics
\\ ( https://arxiv.org/abs/2107.12365 ,  1078kb)
------------------------------------------------------------------------------
\\
arXiv:2205.11677 (*cross-listing*)
replaced with revised version Wed, 28 Feb 2024 02:45:24 GMT   (1787kb,D)

Title: Semi-Supervised Clustering of Sparse Graphs: Crossing the
  Information-Theoretic Threshold
Authors: Junda Sheng and Thomas Strohmer
Categories: stat.ML cs.LG math.OC math.PR
Comments: 44 pages, 8 figures
MSC-class: 60-08 (Primary) 90C35 (Secondary) 90C22
ACM-class: G.3; I.2.6
\\ ( https://arxiv.org/abs/2205.11677 ,  1787kb)
------------------------------------------------------------------------------
\\
arXiv:2208.01416
replaced with revised version Wed, 28 Feb 2024 04:06:35 GMT   (28261kb,D)

Title: Biologically Plausible Training of Deep Neural Networks Using a Top-down
  Credit Assignment Network
Authors: Jian-Hui Chen, Cheng-Lin Liu, Zuoren Wang
Categories: cs.NE cs.LG
\\ ( https://arxiv.org/abs/2208.01416 ,  28261kb)
------------------------------------------------------------------------------
\\
arXiv:2301.02227 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 23:20:19 GMT   (37kb)

Title: Optimal lower bounds for Quantum Learning via Information Theory
Authors: Shima Bab Hadiashar, Ashwin Nayak, Pulkit Sinha
Categories: quant-ph cs.CC cs.IT cs.LG math.IT
Comments: v3: 40 pages; Added references; edited extensively; simplified the
  proof of Theorem 3.2; results unchanged. A preliminary version of the results
  in Section 3 was included in the S.B.H.'s PhD thesis at University of
  Waterloo (Dec. 2020). An extended abstract of the results in Section 4 was
  included in the P.S.' bachelor's project report at Indian Institute of
  Science (Apr. 2022)
ACM-class: F.2.2
Journal-ref: IEEE Transactions on Information Theory, vol. 70, no. 3, pp.
  1876-1896, March 2024
DOI: 10.1109/TIT.2023.3324527
\\ ( https://arxiv.org/abs/2301.02227 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17224 (*cross-listing*)
replaced with revised version Wed, 28 Feb 2024 04:14:13 GMT   (8874kb,D)

Title: Fast and Accurate Estimation of Low-Rank Matrices from Noisy
  Measurements via Preconditioned Non-Convex Gradient Descent
Authors: Gavin Zhang, Hong-Ming Chiu, Richard Y. Zhang
Categories: math.OC cs.LG stat.ML
\\ ( https://arxiv.org/abs/2305.17224 ,  8874kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05292
replaced with revised version Wed, 28 Feb 2024 06:59:38 GMT   (1616kb,D)

Title: Safe Collaborative Filtering
Authors: Riku Togashi, Tatsushi Oka, Naoto Ohsaka, Tetsuro Morimura
Categories: cs.IR cs.LG
Comments: accepted at ICLR2024
\\ ( https://arxiv.org/abs/2306.05292 ,  1616kb)
------------------------------------------------------------------------------
\\
arXiv:2306.07331 (*cross-listing*)
replaced with revised version Wed, 28 Feb 2024 02:18:18 GMT   (702kb,D)

Title: Splitting and Parallelizing of Quantum Convolutional Neural Networks for
  Learning Translationally Symmetric Data
Authors: Koki Chinzei, Quoc Hoan Tran, Kazunori Maruyama, Hirotaka Oshima,
  Shintaro Sato
Categories: quant-ph cs.LG
Comments: 16 pages, 10 figures
\\ ( https://arxiv.org/abs/2306.07331 ,  702kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09055 (*cross-listing*)
replaced with revised version Wed, 28 Feb 2024 12:09:34 GMT   (1141kb)

Title: Robust Data Clustering with Outliers via Transformed Tensor Low-Rank
  Representation
Authors: Tong Wu
Categories: stat.ML cs.CV cs.LG
Comments: AISTATS 2024
\\ ( https://arxiv.org/abs/2307.09055 ,  1141kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01184
replaced with revised version Wed, 28 Feb 2024 16:09:24 GMT   (449kb,D)

Title: Partial Label Supervision for Agnostic Generative Noisy Label Learning
Authors: Fengbei Liu, Chong Wang, Yuanhong Chen, Yuyuan Liu, Gustavo Carneiro
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2308.01184 ,  449kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11015 (*cross-listing*)
replaced with revised version Wed, 28 Feb 2024 04:35:38 GMT   (0kb,I)

Title: 3D-U-SAM Network For Few-shot Tooth Segmentation in CBCT Images
Authors: Yifu Zhang and Zuozhu Liu and Yang Feng and Renjing Xu
Categories: eess.IV cs.LG
Comments: The paper needs to be updated
\\ ( https://arxiv.org/abs/2309.11015 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00656 (*cross-listing*)
replaced with revised version Wed, 28 Feb 2024 14:58:04 GMT   (1549kb,D)

Title: Online Signal Estimation on the Graph Edges via Line Graph
  Transformation
Authors: Yi Yan, Ercan Engin Kuruoglu
Categories: eess.SP cs.LG
\\ ( https://arxiv.org/abs/2311.00656 ,  1549kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02959 (*cross-listing*)
replaced with revised version Wed, 28 Feb 2024 17:40:32 GMT   (8843kb,D)

Title: Detecting algorithmic bias in medical AI-models
Authors: Jeffrey Smith, Andre Holder, Rishikesan Kamaleswaran, Yao Xie
Categories: stat.ML cs.CY cs.LG stat.AP
Comments: 26 pages, 9 figures
\\ ( https://arxiv.org/abs/2312.02959 ,  8843kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03102 (*cross-listing*)
replaced with revised version Wed, 28 Feb 2024 15:58:28 GMT   (7424kb)

Title: Fully Convolutional Slice-to-Volume Reconstruction for Single-Stack MRI
Authors: Sean I. Young, Ya\"el Balbastre, Bruce Fischl, Polina Golland, Juan
  Eugenio Iglesias
Categories: eess.IV cs.CV cs.LG
Comments: Accepted to CVPR 2024
\\ ( https://arxiv.org/abs/2312.03102 ,  7424kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14369
replaced with revised version Tue, 27 Feb 2024 19:21:46 GMT   (7626kb,D)

Title: Quality-Diversity Generative Sampling for Learning with Synthetic Data
Authors: Allen Chang, Matthew C. Fontaine, Serena Booth, Maja J. Matari\'c,
  Stefanos Nikolaidis
Categories: cs.CY cs.LG
Comments: Accepted at AAAI 2024; 7 pages main, 12 pages total, 9 figures
\\ ( https://arxiv.org/abs/2312.14369 ,  7626kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08876
replaced with revised version Wed, 28 Feb 2024 18:47:27 GMT   (5153kb,D)

Title: Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image
  Labeling
Authors: Dongping Zhang, Angelos Chatzimparmpas, Negar Kamali, and Jessica
  Hullman
Categories: cs.HC cs.CV cs.LG
Comments: 19 pages, 11 figures, 9 tables. Accepted by ACM CHI 2024
\\ ( https://arxiv.org/abs/2401.08876 ,  5153kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11585
replaced with revised version Wed, 28 Feb 2024 08:52:36 GMT   (24171kb,D)

Title: PolypNextLSTM: A lightweight and fast polyp video segmentation network
  using ConvNext and ConvLSTM
Authors: Debayan Bhattacharya, Konrad Reuter, Finn Behrendt, Lennart Maack,
  Sarah Grube, Alexander Schlaefer
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2402.11585 ,  24171kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13640
replaced with revised version Wed, 28 Feb 2024 00:48:06 GMT   (182kb,D)

Title: Green AI: A Preliminary Empirical Study on Energy Consumption in DL
  Models Across Different Runtime Infrastructures
Authors: Negar Alizadeh and Fernando Castor
Categories: cs.SE cs.LG
DOI: 10.1145/3644815.3644967
\\ ( https://arxiv.org/abs/2402.13640 ,  182kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14102 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 19:54:21 GMT   (10680kb,D)

Title: Learning dynamic representations of the functional connectome in
  neurobiological networks
Authors: Luciano Dyballa, Samuel Lang, Alexandra Haslund-Gourley, Eviatar
  Yemini, Steven W. Zucker
Categories: q-bio.NC cs.LG cs.SI
Comments: Accepted at ICLR 24
\\ ( https://arxiv.org/abs/2402.14102 ,  10680kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
