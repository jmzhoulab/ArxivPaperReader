paper_240314.txt

Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月14日 12:19
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Tue 12 Mar 24 18:00:00 GMT  to  Wed 13 Mar 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.07916
Date: Tue, 27 Feb 2024 14:08:31 GMT   (6598kb,D)

Title: Advancing Investment Frontiers: Industry-grade Deep Reinforcement
  Learning for Portfolio Optimization
Authors: Philip Ndikum, Serge Ndikum
Categories: cs.AI cs.LG
\\
  This research paper delves into the application of Deep Reinforcement
Learning (DRL) in asset-class agnostic portfolio optimization, integrating
industry-grade methodologies with quantitative finance. At the heart of this
integration is our robust framework that not only merges advanced DRL
algorithms with modern computational techniques but also emphasizes stringent
statistical analysis, software engineering and regulatory compliance. To the
best of our knowledge, this is the first study integrating financial
Reinforcement Learning with sim-to-real methodologies from robotics and
mathematical physics, thus enriching our frameworks and arguments with this
unique perspective. Our research culminates with the introduction of
AlphaOptimizerNet, a proprietary Reinforcement Learning agent (and
corresponding library). Developed from a synthesis of state-of-the-art (SOTA)
literature and our unique interdisciplinary methodology, AlphaOptimizerNet
demonstrates encouraging risk-return optimization across various asset classes
with realistic constraints. These preliminary results underscore the practical
efficacy of our frameworks. As the finance sector increasingly gravitates
towards advanced algorithmic solutions, our study bridges theoretical
advancements with real-world applicability, offering a template for ensuring
safety and robust standards in this technologically driven future.
\\ ( https://arxiv.org/abs/2403.07916 ,  6598kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07964
Date: Tue, 12 Mar 2024 11:51:30 GMT   (4602kb,D)

Title: Optimal Design and Implementation of an Open-source Emulation Platform
  for User-Centric Shared E-mobility Services
Authors: Maqsood Hussain Shah, Yue Ding, Shaoshu Zhu, Yingqi Gu and Mingming
  Liu
Categories: cs.AI
Comments: 7 pages, 3 figures, 2 tables
\\
  In response to the escalating global challenge of increasing emissions and
pollution in transportation, shared electric mobility services, encompassing
e-cars, e-bikes, and e-scooters, have emerged as a popular strategy. However,
existingshared electric mobility services exhibit critical design deficiencies,
including insufficient service integration, imprecise energy consumption
forecasting, limited scalability and geographical coverage, and a notable
absence of a user-centric perspective, particularly in the context of
multi-modal transportation. More importantly, there is no consolidated
open-source framework which could benefit the e-mobility research community.
This paper aims to bridge this gap by providing a pioneering open-source
framework for shared e-mobility. The proposed framework, with an
agent-in-the-loop approach and modular architecture, is tailored to diverse
user preferences and offers enhanced customization. We demonstrate the
viability of this framework by solving an integrated multi-modal
route-optimization problem using the modified Ant Colony Optimization (ACO)
algorithm. The primary contribution of this work is to provide a collaborative
and transparent framework to tackle the dynamic challenges in the field of
e-mobility research using a consolidated approach.
\\ ( https://arxiv.org/abs/2403.07964 ,  4602kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08386
Date: Wed, 13 Mar 2024 09:49:26 GMT   (532kb,D)

Title: Optimizing Risk-averse Human-AI Hybrid Teams
Authors: Andrew Fuchs, Andrea Passarella, and Marco Conti
Categories: cs.AI cs.LG
\\
  We anticipate increased instances of humans and AI systems working together
in what we refer to as a hybrid team. The increase in collaboration is expected
as AI systems gain proficiency and their adoption becomes more widespread.
However, their behavior is not error-free, making hybrid teams a very suitable
solution. As such, we consider methods for improving performance for these
teams of humans and AI systems. For hybrid teams, we will refer to both the
humans and AI systems as agents. To improve team performance over that seen for
agents operating individually, we propose a manager which learns, through a
standard Reinforcement Learning scheme, how to best delegate, over time, the
responsibility of taking a decision to any of the agents. We further guide the
manager's learning so they also minimize how many changes in delegation are
made resulting from undesirable team behavior. We demonstrate the optimality of
our manager's performance in several grid environments which include failure
states which terminate an episode and should be avoided. We perform our
experiments with teams of agents with varying degrees of acceptable risk, in
the form of proximity to a failure state, and measure the manager's ability to
make effective delegation decisions with respect to its own risk-based
constraints, then compare these to the optimal decisions. Our results show our
manager can successfully learn desirable delegations which result in team paths
near/exactly optimal with respect to path length and number of delegations.
\\ ( https://arxiv.org/abs/2403.08386 ,  532kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08425
Date: Wed, 13 Mar 2024 11:20:34 GMT   (194kb,D)

Title: Specification Overfitting in Artificial Intelligence
Authors: Benjamin Roth, Pedro Henrique Luz de Araujo, Yuxi Xia, Saskia
  Kaltenbrunner and Christoph Korab
Categories: cs.AI
Comments: 40 pages, 2 figures
\\
  Machine learning (ML) and artificial intelligence (AI) approaches are often
criticized for their inherent bias and for their lack of control,
accountability, and transparency. Consequently, regulatory bodies struggle with
containing this technology's potential negative side effects. High-level
requirements such as fairness and robustness need to be formalized into
concrete specification metrics, imperfect proxies that capture isolated aspects
of the underlying requirements. Given possible trade-offs between different
metrics and their vulnerability to over-optimization, integrating specification
metrics in system development processes is not trivial. This paper defines
specification overfitting, a scenario where systems focus excessively on
specified metrics to the detriment of high-level requirements and task
performance. We present an extensive literature survey to categorize how
researchers propose, measure, and optimize specification metrics in several AI
fields (e.g., natural language processing, computer vision, reinforcement
learning). Using a keyword-based search on papers from major AI conferences and
journals between 2018 and mid-2023, we identify and analyze 74 papers that
propose or optimize specification metrics. We find that although most papers
implicitly address specification overfitting (e.g., by reporting more than one
specification metric), they rarely discuss which role specification metrics
should play in system development or explicitly define the scope and
assumptions behind metric formulations.
\\ ( https://arxiv.org/abs/2403.08425 ,  194kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08002
Date: Tue, 12 Mar 2024 18:12:02 GMT   (44431kb,D)

Title: Training Small Multimodal Models to Bridge Biomedical Competency Gap: A
  Case Study in Radiology Imaging
Authors: Juan Manuel Zambrano Chaves, Shih-Cheng Huang, Yanbo Xu, Hanwen Xu,
  Naoto Usuyama, Sheng Zhang, Fei Wang, Yujia Xie, Mahmoud Khademi, Ziyi Yang,
  Hany Awadalla, Julia Gong, Houdong Hu, Jianwei Yang, Chunyuan Li, Jianfeng
  Gao, Yu Gu, Cliff Wong, Mu Wei, Tristan Naumann, Muhao Chen, Matthew P.
  Lungren, Serena Yeung-Levy, Curtis P. Langlotz, Sheng Wang, Hoifung Poon
Categories: cs.CL cs.CV
\\
  The scaling laws and extraordinary performance of large foundation models
motivate the development and utilization of such large models in biomedicine.
However, despite early promising results on some biomedical benchmarks, there
are still major challenges that need to be addressed before these models can be
used in real-world applications. Frontier models such as GPT-4V still have
major competency gaps in multimodal capabilities for biomedical applications.
Moreover, pragmatic issues such as access, cost, latency, and compliance make
it hard for clinicians to use privately-hosted state-of-the-art large models
directly on private patient data. In this paper, we explore training
open-source small multimodal models (SMMs) to bridge biomedical competency gaps
for unmet clinical needs. To maximize data efficiency, we adopt a modular
approach by incorporating state-of-the-art pre-trained models for image and
text modalities, and focusing on training a lightweight adapter to ground each
modality to the text embedding space. We conduct a comprehensive study of this
approach on radiology imaging. For training, we assemble a large dataset with
over 1 million image-text pairs. For evaluation, we propose a clinically driven
novel approach using GPT-4 and demonstrate its parity with expert evaluation.
We also study grounding qualitatively using attention. For best practice, we
conduct a systematic ablation study on various choices in data engineering and
multimodal training. The resulting LLaVA-Rad (7B) model attains
state-of-the-art results on radiology tasks such as report generation and
cross-modal retrieval, even outperforming much larger models such as GPT-4V and
Med-PaLM M (84B). LLaVA-Rad is fast and can be run on a single V100 GPU in
private settings, offering a promising state-of-the-art tool for real-world
clinical applications.
\\ ( https://arxiv.org/abs/2403.08002 ,  44431kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08004
Date: Tue, 12 Mar 2024 18:12:50 GMT   (13284kb,D)

Title: Pix2Pix-OnTheFly: Leveraging LLMs for Instruction-Guided Image Editing
Authors: Rodrigo Santos, Jo\~ao Silva, Ant\'onio Branco
Categories: cs.CL cs.AI cs.CV
\\
  The combination of language processing and image processing keeps attracting
increased interest given recent impressive advances that leverage the combined
strengths of both domains of research. Among these advances, the task of
editing an image on the basis solely of a natural language instruction stands
out as a most challenging endeavour. While recent approaches for this task
resort, in one way or other, to some form of preliminary preparation, training
or fine-tuning, this paper explores a novel approach: We propose a
preparation-free method that permits instruction-guided image editing on the
fly. This approach is organized along three steps properly orchestrated that
resort to image captioning and DDIM inversion, followed by obtaining the edit
direction embedding, followed by image editing proper. While dispensing with
preliminary preparation, our approach demonstrates to be effective and
competitive, outperforming recent, state of the art models for this task when
evaluated on the MAGICBRUSH dataset.
\\ ( https://arxiv.org/abs/2403.08004 ,  13284kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08010
Date: Tue, 12 Mar 2024 18:19:47 GMT   (887kb,D)

Title: Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological
  Analysis Based on LLM
Authors: Jingcong Liang, Rong Ye, Meng Han, Ruofei Lai, Xinyu Zhang, Xuanjing
  Huang and Zhongyu Wei
Categories: cs.CL
\\
  How can we construct an automated debate judge to evaluate an extensive,
vibrant, multi-turn debate? This task is challenging, as judging a debate
involves grappling with lengthy texts, intricate argument relationships, and
multi-dimensional assessments. At the same time, current research mainly
focuses on short dialogues, rarely touching upon the evaluation of an entire
debate. In this paper, by leveraging Large Language Models (LLMs), we propose
Debatrix, which makes the analysis and assessment of multi-turn debates more
aligned with majority preferences. Specifically, Debatrix features a vertical,
iterative chronological analysis and a horizontal, multi-dimensional evaluation
collaboration. To align with real-world debate scenarios, we introduced the
PanelBench benchmark, comparing our system's performance to actual debate
outcomes. The findings indicate a notable enhancement over directly using LLMs
for debate evaluation. Source code and benchmark data are available online at
https://github.com/ljcleo/Debatrix .
\\ ( https://arxiv.org/abs/2403.08010 ,  887kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08011
Date: Tue, 12 Mar 2024 18:21:20 GMT   (547kb,D)

Title: Gujarati-English Code-Switching Speech Recognition using ensemble
  prediction of spoken language
Authors: Yash Sharma, Basil Abraham, Preethi Jyothi
Categories: cs.CL cs.AI cs.LG
Comments: Bachelor's thesis, 28 pages, includes appendix
\\
  An important and difficult task in code-switched speech recognition is to
recognize the language, as lots of words in two languages can sound similar,
especially in some accents. We focus on improving performance of end-to-end
Automatic Speech Recognition models by conditioning transformer layers on
language ID of words and character in the output in an per layer supervised
manner. To this end, we propose two methods of introducing language specific
parameters and explainability in the multi-head attention mechanism, and
implement a Temporal Loss that helps maintain continuity in input alignment.
Despite being unable to reduce WER significantly, our method shows promise in
predicting the correct language from just spoken data. We introduce
regularization in the language prediction by dropping LID in the sequence,
which helps align long repeated output sequences.
\\ ( https://arxiv.org/abs/2403.08011 ,  547kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08035
Date: Tue, 12 Mar 2024 19:12:28 GMT   (206kb,D)

Title: Harnessing Artificial Intelligence to Combat Online Hate: Exploring the
  Challenges and Opportunities of Large Language Models in Hate Speech
  Detection
Authors: Tharindu Kumarage, Amrita Bhattacharjee, Joshua Garland
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) excel in many diverse applications beyond
language generation, e.g., translation, summarization, and sentiment analysis.
One intriguing application is in text classification. This becomes pertinent in
the realm of identifying hateful or toxic speech -- a domain fraught with
challenges and ethical dilemmas. In our study, we have two objectives: firstly,
to offer a literature review revolving around LLMs as classifiers, emphasizing
their role in detecting and classifying hateful or toxic content. Subsequently,
we explore the efficacy of several LLMs in classifying hate speech: identifying
which LLMs excel in this task as well as their underlying attributes and
training. Providing insight into the factors that contribute to an LLM
proficiency (or lack thereof) in discerning hateful content. By combining a
comprehensive literature review with an empirical analysis, our paper strives
to shed light on the capabilities and constraints of LLMs in the crucial domain
of hate speech detection.
\\ ( https://arxiv.org/abs/2403.08035 ,  206kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08043
Date: Tue, 12 Mar 2024 19:34:54 GMT   (8024kb,D)

Title: Authorship Style Transfer with Policy Optimization
Authors: Shuai Liu, Shantanu Agarwal, Jonathan May
Categories: cs.CL
\\
  Authorship style transfer aims to rewrite a given text into a specified
target while preserving the original meaning in the source. Existing approaches
rely on the availability of a large number of target style exemplars for model
training. However, these overlook cases where a limited number of target style
examples are available. The development of parameter-efficient transfer
learning techniques and policy optimization (PO) approaches suggest lightweight
PO is a feasible approach to low-resource style transfer. In this work, we
propose a simple two step tune-and-optimize technique for low-resource textual
style transfer. We apply our technique to authorship transfer as well as a
larger-data native language style task and in both cases find it outperforms
state-of-the-art baseline models.
\\ ( https://arxiv.org/abs/2403.08043 ,  8024kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08046
Date: Tue, 12 Mar 2024 19:40:18 GMT   (181kb,D)

Title: Big City Bias: Evaluating the Impact of Metropolitan Size on
  Computational Job Market Abilities of Language Models
Authors: Charlie Campanella, Rob van der Goot
Categories: cs.CL
Comments: 5 pages, 3 figures, 2 tables, NLP4HR Workshop @ EACL 2024
MSC-class: I.2.7
\\
  Large language models (LLMs) have emerged as a useful technology for job
matching, for both candidates and employers. Job matching is often based on a
particular geographic location, such as a city or region. However, LLMs have
known biases, commonly derived from their training data. In this work, we aim
to quantify the metropolitan size bias encoded within large language models,
evaluating zero-shot salary, employer presence, and commute duration
predictions in 384 of the United States' metropolitan regions. Across all
benchmarks, we observe negative correlations between the metropolitan size and
the performance of the LLMS, indicating that smaller regions are indeed
underrepresented. More concretely, the smallest 10 metropolitan regions show
upwards of 300% worse benchmark performance than the largest 10.
\\ ( https://arxiv.org/abs/2403.08046 ,  181kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08053
Date: Tue, 12 Mar 2024 19:57:39 GMT   (944kb,D)

Title: Generating Clarification Questions for Disambiguating Contracts
Authors: Anmol Singhal, Chirag Jain, Preethu Rose Anish, Arkajyoti Chakraborty,
  Smita Ghaisas
Categories: cs.CL
Comments: 9 pages, 3 figures, accepted to LREC-COLING 2024
\\
  Enterprises frequently enter into commercial contracts that can serve as
vital sources of project-specific requirements. Contractual clauses are
obligatory, and the requirements derived from contracts can detail the
downstream implementation activities that non-legal stakeholders, including
requirement analysts, engineers, and delivery personnel, need to conduct.
However, comprehending contracts is cognitively demanding and error-prone for
such stakeholders due to the extensive use of Legalese and the inherent
complexity of contract language. Furthermore, contracts often contain
ambiguously worded clauses to ensure comprehensive coverage. In contrast,
non-legal stakeholders require a detailed and unambiguous comprehension of
contractual clauses to craft actionable requirements. In this work, we
introduce a novel legal NLP task that involves generating clarification
questions for contracts. These questions aim to identify contract ambiguities
on a document level, thereby assisting non-legal stakeholders in obtaining the
necessary details for eliciting requirements. This task is challenged by three
core issues: (1) data availability, (2) the length and unstructured nature of
contracts, and (3) the complexity of legal text. To address these issues, we
propose ConRAP, a retrieval-augmented prompting framework for generating
clarification questions to disambiguate contractual text. Experiments conducted
on contracts sourced from the publicly available CUAD dataset show that ConRAP
with ChatGPT can detect ambiguities with an F2 score of 0.87. 70% of the
generated clarification questions are deemed useful by human evaluators.
\\ ( https://arxiv.org/abs/2403.08053 ,  944kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08103
Date: Tue, 12 Mar 2024 22:23:08 GMT   (9kb)

Title: Contextual Clarity: Generating Sentences with Transformer Models using
  Context-Reverso Data
Authors: Ruslan Musaev
Categories: cs.CL cs.AI
\\
  In the age of information abundance, the ability to provide users with
contextually relevant and concise information is crucial. Keyword in Context
(KIC) generation is a task that plays a vital role in and generation
applications, such as search engines, personal assistants, and content
summarization. In this paper, we present a novel approach to generating
unambiguous and brief sentence-contexts for given keywords using the T5
transformer model, leveraging data obtained from the Context-Reverso API. The
code is available at https://github.com/Rusamus/word2context/tree/main .
\\ ( https://arxiv.org/abs/2403.08103 ,  9kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08140
Date: Tue, 12 Mar 2024 23:59:15 GMT   (810kb,D)

Title: BAGEL: Bootstrapping Agents by Guiding Exploration with Language
Authors: Shikhar Murty, Christopher Manning, Peter Shaw, Mandar Joshi, Kenton
  Lee
Categories: cs.CL
\\
  Following natural language instructions by executing actions in digital
environments (e.g. web-browsers and REST APIs) is a challenging task for
language model (LM) agents. Unfortunately, LM agents often fail to generalize
to new environments without human demonstrations. This work presents BAGEL, a
method for bootstrapping LM agents without human supervision. BAGEL converts a
seed set of randomly explored trajectories or synthetic instructions, into
demonstrations, via round-trips between two noisy LM components: an LM labeler
which converts a trajectory into a synthetic instruction, and a zero-shot LM
agent which maps the synthetic instruction into a refined trajectory. By
performing these round-trips iteratively, BAGEL quickly converts the initial
distribution of trajectories towards those that are well-described by natural
language. We use BAGEL demonstrations to adapt a zero shot LM agent at test
time via in-context learning over retrieved demonstrations, and find
improvements of over 2-13% absolute on ToolQA and MiniWob++, with up to 13x
reduction in execution failures.
\\ ( https://arxiv.org/abs/2403.08140 ,  810kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08174
Date: Wed, 13 Mar 2024 01:56:32 GMT   (20kb,D)

Title: Rethinking Loss Functions for Fact Verification
Authors: Yuta Mukobara, Yutaro Shigeto, Masashi Shimbo
Categories: cs.CL cs.AI
Comments: Accepted to EACL 2024 (short paper). The souce code is available at
  https://github.com/yuta-mukobara/RLF-KGAT
\\
  We explore loss functions for fact verification in the FEVER shared task.
While the cross-entropy loss is a standard objective for training verdict
predictors, it fails to capture the heterogeneity among the FEVER verdict
classes. In this paper, we develop two task-specific objectives tailored to
FEVER. Experimental results confirm that the proposed objective functions
outperform the standard cross-entropy. Performance is further improved when
these objectives are combined with simple class weighting, which effectively
overcomes the imbalance in the training data. The souce code is available at
https://github.com/yuta-mukobara/RLF-KGAT
\\ ( https://arxiv.org/abs/2403.08174 ,  20kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08187
Date: Wed, 13 Mar 2024 02:20:05 GMT   (125kb,D)

Title: Automatic Speech Recognition (ASR) for the Diagnosis of pronunciation of
  Speech Sound Disorders in Korean children
Authors: Taekyung Ahn, Yeonjung Hong, Younggon Im, Do Hyung Kim, Dayoung Kang,
  Joo Won Jeong, Jae Won Kim, Min Jung Kim, Ah-ra Cho, Dae-Hyun Jang and Hosung
  Nam
Categories: cs.CL cs.SD eess.AS
Comments: 12 pages, 2 figures
ACM-class: I.2.7
\\
  This study presents a model of automatic speech recognition (ASR) designed to
diagnose pronunciation issues in children with speech sound disorders (SSDs) to
replace manual transcriptions in clinical procedures. Since ASR models trained
for general purposes primarily predict input speech into real words, employing
a well-known high-performance ASR model for evaluating pronunciation in
children with SSDs is impractical. We fine-tuned the wav2vec 2.0 XLS-R model to
recognize speech as pronounced rather than as existing words. The model was
fine-tuned with a speech dataset from 137 children with inadequate speech
production pronouncing 73 Korean words selected for actual clinical diagnosis.
The model's predictions of the pronunciations of the words matched the human
annotations with about 90% accuracy. While the model still requires improvement
in recognizing unclear pronunciation, this study demonstrates that ASR models
can streamline complex pronunciation error diagnostic procedures in clinical
fields.
\\ ( https://arxiv.org/abs/2403.08187 ,  125kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08189
Date: Wed, 13 Mar 2024 02:23:13 GMT   (2143kb,D)

Title: Embedded Translations for Low-resource Automated Glossing
Authors: Changbing Yang, Garrett Nicolai, Miikka Silfverberg
Categories: cs.CL
\\
  We investigate automatic interlinear glossing in low-resource settings. We
augment a hard-attentional neural model with embedded translation information
extracted from interlinear glossed text. After encoding these translations
using large language models, specifically BERT and T5, we introduce a
character-level decoder for generating glossed output. Aided by these
enhancements, our model demonstrates an average improvement of 3.97\%-points
over the previous state of the art on datasets from the SIGMORPHON 2023 Shared
Task on Interlinear Glossing. In a simulated ultra low-resource setting,
trained on as few as 100 sentences, our system achieves an average 9.78\%-point
improvement over the plain hard-attentional baseline. These results highlight
the critical role of translation information in boosting the system's
performance, especially in processing and interpreting modest data sources. Our
findings suggest a promising avenue for the documentation and preservation of
languages, with our experiments on shared task datasets indicating significant
advancements over the existing state of the art.
\\ ( https://arxiv.org/abs/2403.08189 ,  2143kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08192
Date: Wed, 13 Mar 2024 02:26:16 GMT   (1019kb,D)

Title: MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular
  Comprehension
Authors: Xingyu Lu, He Cao, Zijing Liu, Shengyuan Bai, Leqing Chen, Yuan Yao,
  Hai-Tao Zheng, Yu Li
Categories: cs.CL q-bio.BM
Comments: 19 pages, 8 figures
\\
  Large language models are playing an increasingly significant role in
molecular research, yet existing models often generate erroneous information,
posing challenges to accurate molecular comprehension. Traditional evaluation
metrics for generated content fail to assess a model's accuracy in molecular
understanding. To rectify the absence of factual evaluation, we present
MoleculeQA, a novel question answering (QA) dataset which possesses 62K QA
pairs over 23K molecules. Each QA pair, composed of a manual question, a
positive option and three negative options, has consistent semantics with a
molecular description from authoritative molecular corpus. MoleculeQA is not
only the first benchmark for molecular factual bias evaluation but also the
largest QA dataset for molecular research. A comprehensive evaluation on
MoleculeQA for existing molecular LLMs exposes their deficiencies in specific
areas and pinpoints several particularly crucial factors for molecular
understanding.
\\ ( https://arxiv.org/abs/2403.08192 ,  1019kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08196
Date: Wed, 13 Mar 2024 02:41:53 GMT   (424kb,D)

Title: SpeechColab Leaderboard: An Open-Source Platform for Automatic Speech
  Recognition Evaluation
Authors: Jiayu Du, Jinpeng Li, Guoguo Chen, and Wei-Qiang Zhang
Categories: cs.CL eess.AS
\\
  In the wake of the surging tide of deep learning over the past decade,
Automatic Speech Recognition (ASR) has garnered substantial attention, leading
to the emergence of numerous publicly accessible ASR systems that are actively
being integrated into our daily lives. Nonetheless, the impartial and
replicable evaluation of these ASR systems encounters challenges due to various
crucial subtleties. In this paper we introduce the SpeechColab Leaderboard, a
general-purpose, open-source platform designed for ASR evaluation. With this
platform: (i) We report a comprehensive benchmark, unveiling the current
state-of-the-art panorama for ASR systems, covering both open-source models and
industrial commercial services. (ii) We quantize how distinct nuances in the
scoring pipeline influence the final benchmark outcomes. These include nuances
related to capitalization, punctuation, interjection, contraction, synonym
usage, compound words, etc. These issues have gained prominence in the context
of the transition towards an End-to-End future. (iii) We propose a practical
modification to the conventional Token-Error-Rate (TER) evaluation metric, with
inspirations from Kolmogorov complexity and Normalized Information Distance
(NID). This adaptation, called modified-TER (mTER), achieves proper
normalization and symmetrical treatment of reference and hypothesis. By
leveraging this platform as a large-scale testing ground, this study
demonstrates the robustness and backward compatibility of mTER when compared to
TER. The SpeechColab Leaderboard is accessible at
https://github.com/SpeechColab/Leaderboard
\\ ( https://arxiv.org/abs/2403.08196 ,  424kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08198
Date: Wed, 13 Mar 2024 02:46:17 GMT   (1899kb,D)

Title: Validating and Exploring Large Geographic Corpora
Authors: Jonathan Dunn
Categories: cs.CL
\\
  This paper investigates the impact of corpus creation decisions on large
multi-lingual geographic web corpora. Beginning with a 427 billion word corpus
derived from the Common Crawl, three methods are used to improve the quality of
sub-corpora representing specific language-country pairs like New Zealand
English: (i) the agreement of independent language identification systems, (ii)
hash-based deduplication, and (iii) location-specific outlier detection. The
impact of each of these steps is then evaluated at the language level and the
country level by using corpus similarity measures to compare each resulting
corpus with baseline data sets. The goal is to understand the impact of
upstream data cleaning decisions on downstream corpora with a specific focus on
under-represented languages and populations. The evaluation shows that the
validity of sub-corpora is improved with each stage of cleaning but that this
improvement is unevenly distributed across languages and populations. This
result shows how standard corpus creation techniques can accidentally exclude
under-represented populations.
\\ ( https://arxiv.org/abs/2403.08198 ,  1899kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08211
Date: Wed, 13 Mar 2024 03:15:05 GMT   (749kb,D)

Title: Large Language Models are Contrastive Reasoners
Authors: Liang Yao
Categories: cs.CL cs.AI
\\
  Prompting methods play a crucial role in enhancing the capabilities of
pre-trained large language models (LLMs). We explore how contrastive prompting
(CP) significantly improves the ability of large language models to perform
complex reasoning. We demonstrate that LLMs are decent contrastive reasoners by
simply adding "Let's give a correct and a wrong answer." before LLMs provide
answers. Experiments on two large language models show that zero-shot
contrastive prompting improves performance on a range of arithmetic,
commonsense, and symbolic reasoning tasks without any hand-crafted few-shot
examples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and
AQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model. Our method
not only surpasses zero-shot CoT and few-shot CoT in most arithmetic and
commonsense reasoning tasks but also can seamlessly integrate with existing
prompting methods, resulting in improved or comparable results when compared to
state-of-the-art methods. Our code is available at
https://github.com/yao8839836/cp
\\ ( https://arxiv.org/abs/2403.08211 ,  749kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08213
Date: Wed, 13 Mar 2024 03:22:02 GMT   (4765kb,D)

Title: Can Large Language Models Identify Authorship?
Authors: Baixiang Huang, Canyu Chen, Kai Shu
Categories: cs.CL
Comments: 10 pages, 7 figures
\\
  The ability to accurately identify authorship is crucial for verifying
content authenticity and mitigating misinformation. Large Language Models
(LLMs) have demonstrated exceptional capacity for reasoning and
problem-solving. However, their potential in authorship analysis, encompassing
authorship verification and attribution, remains underexplored. This paper
conducts a comprehensive evaluation of LLMs in these critical tasks.
Traditional studies have depended on hand-crafted stylistic features, whereas
state-of-the-art approaches leverage text embeddings from pre-trained language
models. These methods, which typically require fine-tuning on labeled data,
often suffer from performance degradation in cross-domain applications and
provide limited explainability. This work seeks to address three research
questions: (1) Can LLMs perform zero-shot, end-to-end authorship verification
effectively? (2) Are LLMs capable of accurately attributing authorship among
multiple candidates authors (e.g., 10 and 20)? (3) How can LLMs provide
explainability in authorship analysis, particularly through the role of
linguistic features? Moreover, we investigate the integration of explicit
linguistic features to guide LLMs in their reasoning processes. Our extensive
assessment demonstrates LLMs' proficiency in both tasks without the need for
domain-specific fine-tuning, providing insights into their decision-making via
a detailed analysis of linguistic features. This establishes a new benchmark
for future research on LLM-based authorship analysis. The code and data are
available at https://github.com/baixianghuang/authorship-llm.
\\ ( https://arxiv.org/abs/2403.08213 ,  4765kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08217
Date: Wed, 13 Mar 2024 03:31:26 GMT   (645kb)

Title: Research on the Application of Deep Learning-based BERT Model in
  Sentiment Analysis
Authors: Yichao Wu, Zhengyu Jin, Chenxi Shi, Penghao Liang, Tong Zhan
Categories: cs.CL cs.LG
\\
  This paper explores the application of deep learning techniques, particularly
focusing on BERT models, in sentiment analysis. It begins by introducing the
fundamental concept of sentiment analysis and how deep learning methods are
utilized in this domain. Subsequently, it delves into the architecture and
characteristics of BERT models. Through detailed explanation, it elucidates the
application effects and optimization strategies of BERT models in sentiment
analysis, supported by experimental validation. The experimental findings
indicate that BERT models exhibit robust performance in sentiment analysis
tasks, with notable enhancements post fine-tuning. Lastly, the paper concludes
by summarizing the potential applications of BERT models in sentiment analysis
and suggests directions for future research and practical implementations.
\\ ( https://arxiv.org/abs/2403.08217 ,  645kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08229
Date: Wed, 13 Mar 2024 04:14:33 GMT   (443kb,D)

Title: Boosting Disfluency Detection with Large Language Model as Disfluency
  Generator
Authors: Zhenrong Cheng, Jiayan Guo, Hao Sun, Yan Zhang
Categories: cs.CL
\\
  Current disfluency detection methods heavily rely on costly and scarce
human-annotated data. To tackle this issue, some approaches employ heuristic or
statistical features to generate disfluent sentences, partially improving
detection performance. However, these sentences often deviate from real-life
scenarios, constraining overall model enhancement. In this study, we propose a
lightweight data augmentation approach for disfluency detection, utilizing the
superior generative and semantic understanding capabilities of large language
model (LLM) to generate disfluent sentences as augmentation data. We leverage
LLM to generate diverse and more realistic sentences guided by specific
prompts, without the need for fine-tuning the LLM. Subsequently, we apply an
uncertainty-aware data filtering approach to improve the quality of the
generated sentences, utilized in training a small detection model for improved
performance. Experiments using enhanced data yielded state-of-the-art results.
The results showed that using a small amount of LLM-generated enhanced data can
significantly improve performance, thereby further enhancing
cost-effectiveness.
\\ ( https://arxiv.org/abs/2403.08229 ,  443kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08258
Date: Wed, 13 Mar 2024 05:20:45 GMT   (214kb,D)

Title: Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition
Authors: Wenjing Zhu, Sining Sun, Changhao Shan, Peng Fan, Qing Yang
Categories: cs.CL cs.LG
Comments: Accepted by ICME2024
\\
  Conformer-based attention models have become the de facto backbone model for
Automatic Speech Recognition tasks. A blank symbol is usually introduced to
align the input and output sequences for CTC or RNN-T models. Unfortunately,
the long input length overloads computational budget and memory consumption
quadratically by attention mechanism. In this work, we propose a
"Skip-and-Recover" Conformer architecture, named Skipformer, to squeeze
sequence input length dynamically and inhomogeneously. Skipformer uses an
intermediate CTC output as criteria to split frames into three groups: crucial,
skipping and ignoring. The crucial group feeds into next conformer blocks and
its output joint with skipping group by original temporal order as the final
encoder output. Experiments show that our model reduces the input sequence
length by 31 times on Aishell-1 and 22 times on Librispeech corpus. Meanwhile,
the model can achieve better recognition accuracy and faster inference speed
than recent baseline models. Our code is open-sourced and available online.
\\ ( https://arxiv.org/abs/2403.08258 ,  214kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08272
Date: Wed, 13 Mar 2024 05:51:57 GMT   (9795kb,D)

Title: RECIPE4U: Student-ChatGPT Interaction Dataset in EFL Writing Education
Authors: Jieun Han, Haneul Yoo, Junho Myung, Minsun Kim, Tak Yeon Lee, So-Yeon
  Ahn and Alice Oh
Categories: cs.CL
Comments: arXiv admin note: text overlap with arXiv:2309.13243
\\
  The integration of generative AI in education is expanding, yet empirical
analyses of large-scale and real-world interactions between students and AI
systems still remain limited. Addressing this gap, we present RECIPE4U (RECIPE
for University), a dataset sourced from a semester-long experiment with 212
college students in English as Foreign Language (EFL) writing courses. During
the study, students engaged in dialogues with ChatGPT to revise their essays.
RECIPE4U includes comprehensive records of these interactions, including
conversation logs, students' intent, students' self-rated satisfaction, and
students' essay edit histories. In particular, we annotate the students'
utterances in RECIPE4U with 13 intention labels based on our coding schemes. We
establish baseline results for two subtasks in task-oriented dialogue systems
within educational contexts: intent detection and satisfaction estimation. As a
foundational step, we explore student-ChatGPT interaction patterns through
RECIPE4U and analyze them by focusing on students' dialogue, essay data
statistics, and students' essay edits. We further illustrate potential
applications of RECIPE4U dataset for enhancing the incorporation of LLMs in
educational frameworks. RECIPE4U is publicly available at
https://zeunie.github.io/RECIPE4U/.
\\ ( https://arxiv.org/abs/2403.08272 ,  9795kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08281
Date: Wed, 13 Mar 2024 06:18:48 GMT   (7384kb,D)

Title: Mastering Text, Code and Math Simultaneously via Fusing Highly
  Specialized Language Models
Authors: Ning Ding, Yulin Chen, Ganqu Cui, Xingtai Lv, Ruobing Xie, Bowen Zhou,
  Zhiyuan Liu, Maosong Sun
Categories: cs.CL cs.AI
\\
  Underlying data distributions of natural language, programming code, and
mathematical symbols vary vastly, presenting a complex challenge for large
language models (LLMs) that strive to achieve high performance across all three
domains simultaneously. Achieving a very high level of proficiency for an LLM
within a specific domain often requires extensive training with relevant
corpora, which is typically accompanied by a sacrifice in performance in other
domains. In this paper, we propose to fuse models that are already
highly-specialized directly. The proposed fusing framework, UltraFuser,
consists of three distinct specialists that are already sufficiently trained on
language, coding, and mathematics. A token-level gating mechanism is introduced
to blend the specialists' outputs. A two-stage training strategy accompanied by
balanced sampling is designed to ensure stability. To effectively train the
fused model, we further construct a high-quality supervised instruction tuning
dataset, UltraChat 2, which includes text, code, and mathematical content. This
dataset comprises approximately 300,000 instructions and covers a wide range of
topics in each domain. Experiments show that our model could simultaneously
achieve mastery of the three crucial domains.
\\ ( https://arxiv.org/abs/2403.08281 ,  7384kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08293
Date: Wed, 13 Mar 2024 06:54:47 GMT   (3542kb,D)

Title: Generative Pretrained Structured Transformers: Unsupervised Syntactic
  Language Models at Scale
Authors: Xiang Hu, Pengyu Ji, Qingyang Zhu, Wei Wu, Kewei Tu
Categories: cs.CL cs.AI
Comments: preprint
\\
  A syntactic language model (SLM) incrementally generates a sentence with its
syntactic tree in a left-to-right manner. We present Generative Pretrained
Structured Transformers (GPST), an unsupervised SLM at scale capable of being
pre-trained from scratch on raw texts with high parallelism. GPST circumvents
the limitations of previous SLMs such as relying on gold trees and sequential
training. It consists of two components, a usual SLM supervised by a
uni-directional language modeling loss, and an additional composition model,
which induces syntactic parse trees and computes constituent representations,
supervised by a bi-directional language modeling loss. We propose a
representation surrogate to enable joint parallel training of the two models in
a hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion
tokens, and demonstrate the superiority of GPST over GPT-2 with a comparable
size in numerous tasks covering both language understanding and language
generation. Meanwhile, GPST also significantly outperforms existing
unsupervised SLMs on left-to-right grammar induction, while holding a
substantial acceleration on training.
\\ ( https://arxiv.org/abs/2403.08293 ,  3542kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08295
Date: Wed, 13 Mar 2024 06:59:16 GMT   (125kb,D)

Title: Gemma: Open Models Based on Gemini Research and Technology
Authors: Gemma Team: Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya
  Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\`ere, Mihir Sanjay
  Kale, Juliette Love, Pouya Tafti, L\'eonard Hussenot, Aakanksha Chowdhery,
  Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone,
  Am\'elie H\'eliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth
  Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo,
  Cl\'ement Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya,
  Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru,
  Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko,
  Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff
  Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin
  Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa
  Lee, Lucas Dixon, Machel Reid, Maciej Miku{\l}a, Mateo Wirth, Michael
  Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar
  Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Pier Giuseppe Sessa, Rahma
  Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu,
  Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto
  Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan,
  Vlad Feinberg, Wojciech Stokowiec, Yu-hui Chen, Zafarali Ahmed, Zhitao Gong,
  Tris Warkentin, Ludovic Peran, Minh Giang, Cl\'ement Farabet, Oriol Vinyals,
  Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck,
  Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel,
  Evan Senter, Alek Andreev, Kathleen Kenealy
Categories: cs.CL cs.AI
\\
  This work introduces Gemma, a family of lightweight, state-of-the art open
models built from the research and technology used to create Gemini models.
Gemma models demonstrate strong performance across academic benchmarks for
language understanding, reasoning, and safety. We release two sizes of models
(2 billion and 7 billion parameters), and provide both pretrained and
fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out
of 18 text-based tasks, and we present comprehensive evaluations of safety and
responsibility aspects of the models, alongside a detailed description of model
development. We believe the responsible release of LLMs is critical for
improving the safety of frontier models, and for enabling the next wave of LLM
innovations.
\\ ( https://arxiv.org/abs/2403.08295 ,  125kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08305
Date: Wed, 13 Mar 2024 07:31:20 GMT   (5740kb,D)

Title: Towards Personalized Evaluation of Large Language Models with An
  Anonymous Crowd-Sourcing Platform
Authors: Mingyue Cheng, Hao Zhang, Jiqian Yang, Qi Liu, Li Li, Xin Huang, Liwei
  Song, Zhi Li, Zhenya Huang, Enhong Chen
Categories: cs.CL
DOI: 10.1145/3589335.3651243
\\
  Large language model evaluation plays a pivotal role in the enhancement of
its capacity. Previously, numerous methods for evaluating large language models
have been proposed in this area. Despite their effectiveness, these existing
works mainly focus on assessing objective questions, overlooking the capability
to evaluate subjective questions which is extremely common for large language
models. Additionally, these methods predominantly utilize centralized datasets
for evaluation, with question banks concentrated within the evaluation
platforms themselves. Moreover, the evaluation processes employed by these
platforms often overlook personalized factors, neglecting to consider the
individual characteristics of both the evaluators and the models being
evaluated. To address these limitations, we propose a novel anonymous
crowd-sourcing evaluation platform, BingJian, for large language models that
employs a competitive scoring mechanism where users participate in ranking
models based on their performance. This platform stands out not only for its
support of centralized evaluations to assess the general capabilities of models
but also for offering an open evaluation gateway. Through this gateway, users
have the opportunity to submit their questions, testing the models on a
personalized and potentially broader range of capabilities. Furthermore, our
platform introduces personalized evaluation scenarios, leveraging various forms
of human-computer interaction to assess large language models in a manner that
accounts for individual user preferences and contexts. The demonstration of
BingJian can be accessed at https://github.com/Mingyue-Cheng/Bingjian.
\\ ( https://arxiv.org/abs/2403.08305 ,  5740kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08312
Date: Wed, 13 Mar 2024 07:44:14 GMT   (8773kb,D)

Title: StreamingDialogue: Prolonged Dialogue Learning via Long Context
  Compression with Minimal Losses
Authors: Jia-Nan Li, Quan Tu, Cunli Mao, Zhengtao Yu, Ji-Rong Wen, Rui Yan
Categories: cs.CL cs.AI
\\
  Standard Large Language Models (LLMs) struggle with handling dialogues with
long contexts due to efficiency and consistency issues. According to our
observation, dialogue contexts are highly structured, and the special token of
\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate
information. We refer to the EoU tokens as ``conversational attention sinks''
(conv-attn sinks). Accordingly, we introduce StreamingDialogue, which
compresses long dialogue history into conv-attn sinks with minimal losses, and
thus reduces computational complexity quadratically with the number of sinks
(i.e., the number of utterances). Current LLMs already demonstrate the ability
to handle long context window, e.g., a window size of 200k or more. To this
end, by compressing utterances into EoUs, our method has the potential to
handle more than 200k of utterances, resulting in a prolonged dialogue
learning. In order to minimize information losses from reconstruction after
compression, we design two learning strategies of short-memory reconstruction
(SMR) and long-memory reactivation (LMR). Our method outperforms strong
baselines in dialogue tasks and achieves a 4 $\times$ speedup while reducing
memory usage by 18 $\times$ compared to dense attention recomputation.
\\ ( https://arxiv.org/abs/2403.08312 ,  8773kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08314
Date: Wed, 13 Mar 2024 07:49:50 GMT   (8714kb,D)

Title: Is Context Helpful for Chat Translation Evaluation?
Authors: Sweta Agrawal, Amin Farajian, Patrick Fernandes, Ricardo Rei, Andr\'e
  F.T. Martins
Categories: cs.CL
\\
  Despite the recent success of automatic metrics for assessing translation
quality, their application in evaluating the quality of machine-translated
chats has been limited. Unlike more structured texts like news, chat
conversations are often unstructured, short, and heavily reliant on contextual
information. This poses questions about the reliability of existing
sentence-level metrics in this domain as well as the role of context in
assessing the translation quality. Motivated by this, we conduct a
meta-evaluation of existing sentence-level automatic metrics, primarily
designed for structured domains such as news, to assess the quality of
machine-translated chats. We find that reference-free metrics lag behind
reference-based ones, especially when evaluating translation quality in
out-of-English settings. We then investigate how incorporating conversational
contextual information in these metrics affects their performance. Our findings
show that augmenting neural learned metrics with contextual information helps
improve correlation with human judgments in the reference-free scenario and
when evaluating translations in out-of-English settings. Finally, we propose a
new evaluation metric, Context-MQM, that utilizes bilingual context with a
large language model (LLM) and further validate that adding context helps even
for LLM-based evaluation metrics.
\\ ( https://arxiv.org/abs/2403.08314 ,  8714kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08319
Date: Wed, 13 Mar 2024 08:02:23 GMT   (796kb,D)

Title: Knowledge Conflicts for LLMs: A Survey
Authors: Rongwu Xu, Zehan Qi, Cunxiang Wang, Hongru Wang, Yue Zhang, Wei Xu
Categories: cs.CL cs.AI cs.IR cs.LG
\\
  This survey provides an in-depth analysis of knowledge conflicts for large
language models (LLMs), highlighting the complex challenges they encounter when
blending contextual and parametric knowledge. Our focus is on three categories
of knowledge conflicts: context-memory, inter-context, and intra-memory
conflict. These conflicts can significantly impact the trustworthiness and
performance of LLMs, especially in real-world applications where noise and
misinformation are common. By categorizing these conflicts, exploring the
causes, examining the behaviors of LLMs under such conflicts, and reviewing
available solutions, this survey aims to shed light on strategies for improving
the robustness of LLMs, thereby serving as a valuable resource for advancing
research in this evolving area.
\\ ( https://arxiv.org/abs/2403.08319 ,  796kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08332
Date: Wed, 13 Mar 2024 08:34:53 GMT   (388kb,D)

Title: Autoregressive Score Generation for Multi-trait Essay Scoring
Authors: Heejin Do, Yunsu Kim, Gary Geunbae Lee
Categories: cs.CL cs.AI
Comments: Accepted at EACL2024 Findings
\\
  Recently, encoder-only pre-trained models such as BERT have been successfully
applied in automated essay scoring (AES) to predict a single overall score.
However, studies have yet to explore these models in multi-trait AES, possibly
due to the inefficiency of replicating BERT-based models for each trait.
Breaking away from the existing sole use of encoder, we propose an
autoregressive prediction of multi-trait scores (ArTS), incorporating a
decoding process by leveraging the pre-trained T5. Unlike prior regression or
classification methods, we redefine AES as a score-generation task, allowing a
single model to predict multiple scores. During decoding, the subsequent trait
prediction can benefit by conditioning on the preceding trait scores.
Experimental results proved the efficacy of ArTS, showing over 5% average
improvements in both prompts and traits.
\\ ( https://arxiv.org/abs/2403.08332 ,  388kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08345
Date: Wed, 13 Mar 2024 08:50:15 GMT   (225kb,D)

Title: From human experts to machines: An LLM supported approach to ontology
  and knowledge graph construction
Authors: Vamsi Krishna Kommineni and Birgitta K\"onig-Ries and Sheeba Samuel
Categories: cs.CL
\\
  The conventional process of building Ontologies and Knowledge Graphs (KGs)
heavily relies on human domain experts to define entities and relationship
types, establish hierarchies, maintain relevance to the domain, fill the ABox
(or populate with instances), and ensure data quality (including amongst others
accuracy and completeness). On the other hand, Large Language Models (LLMs)
have recently gained popularity for their ability to understand and generate
human-like natural language, offering promising ways to automate aspects of
this process. This work explores the (semi-)automatic construction of KGs
facilitated by open-source LLMs. Our pipeline involves formulating competency
questions (CQs), developing an ontology (TBox) based on these CQs, constructing
KGs using the developed ontology, and evaluating the resultant KG with minimal
to no involvement of human experts. We showcase the feasibility of our
semi-automated pipeline by creating a KG on deep learning methodologies by
exploiting scholarly publications. To evaluate the answers generated via
Retrieval-Augmented-Generation (RAG) as well as the KG concepts automatically
extracted using LLMs, we design a judge LLM, which rates the generated content
based on ground truth. Our findings suggest that employing LLMs could
potentially reduce the human effort involved in the construction of KGs,
although a human-in-the-loop approach is recommended to evaluate automatically
generated KGs.
\\ ( https://arxiv.org/abs/2403.08345 ,  225kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08370
Date: Wed, 13 Mar 2024 09:31:50 GMT   (9296kb,D)

Title: SMART: Submodular Data Mixture Strategy for Instruction Tuning
Authors: H S V N S Kowndinya Renduchintala, Sumit Bhatia, Ganesh Ramakrishnan
Categories: cs.CL cs.AI cs.LG
\\
  Instruction Tuning involves finetuning a language model on a collection of
instruction-formatted datasets in order to enhance the generalizability of the
model to unseen tasks. Studies have shown the importance of balancing different
task proportions during finetuning, but finding the right balance remains
challenging. Unfortunately, there's currently no systematic method beyond
manual tuning or relying on practitioners' intuition. In this paper, we
introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a
novel data mixture strategy which makes use of a submodular function to assign
importance scores to tasks which are then used to determine the mixture
weights. Given a fine-tuning budget, SMART redistributes the budget among tasks
and selects non-redundant samples from each task. Experimental results
demonstrate that SMART significantly outperforms traditional methods such as
examples proportional mixing and equal mixing. Furthermore, SMART facilitates
the creation of data mixtures based on a few representative subsets of tasks
alone and through task pruning analysis, we reveal that in a limited budget
setting, allocating budget among a subset of representative tasks yields
superior performance compared to distributing the budget among all tasks. The
code for reproducing our results is open-sourced at
https://github.com/kowndinya-renduchintala/SMART.
\\ ( https://arxiv.org/abs/2403.08370 ,  9296kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08377
Date: Wed, 13 Mar 2024 09:42:46 GMT   (1717kb,D)

Title: Learning to Describe for Predicting Zero-shot Drug-Drug Interactions
Authors: Fangqi Zhu, Yongqi Zhang, Lei Chen, Bing Qin, Ruifeng Xu
Categories: cs.CL
DOI: 10.18653/v1/2023.emnlp-main.918
\\
  Adverse drug-drug interactions~(DDIs) can compromise the effectiveness of
concurrent drug administration, posing a significant challenge in healthcare.
As the development of new drugs continues, the potential for unknown adverse
effects resulting from DDIs becomes a growing concern. Traditional
computational methods for DDI prediction may fail to capture interactions for
new drugs due to the lack of knowledge. In this paper, we introduce a new
problem setup as zero-shot DDI prediction that deals with the case of new
drugs. Leveraging textual information from online databases like DrugBank and
PubChem, we propose an innovative approach TextDDI with a language model-based
DDI predictor and a reinforcement learning~(RL)-based information selector,
enabling the selection of concise and pertinent text for accurate DDI
prediction on new drugs. Empirical results show the benefits of the proposed
approach on several settings including zero-shot and few-shot DDI prediction,
and the selected texts are semantically relevant. Our code and data are
available at \url{https://github.com/zhufq00/DDIs-Prediction}.
\\ ( https://arxiv.org/abs/2403.08377 ,  1717kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08391
Date: Wed, 13 Mar 2024 10:10:07 GMT   (615kb,D)

Title: Misinformation is not about Bad Facts: An Analysis of the Production and
  Consumption of Fringe Content
Authors: JooYoung Lee, Emily Booth, Hany Farid, Marian-Andrei Rizoiu
Categories: cs.CL
Comments: 11 pages, 2 figures
\\
  What if misinformation is not an information problem at all? Our findings
suggest that online fringe ideologies spread through the use of content that is
consensus-based and "factually correct". We found that Australian news
publishers with both moderate and far-right political leanings contain
comparable levels of information completeness and quality; and furthermore,
that far-right Twitter users often share from moderate sources. However, a
stark difference emerges when we consider two additional factors: 1) the narrow
topic selection of articles by far-right users, suggesting that they cherrypick
only news articles that engage with specific topics of their concern, and 2)
the difference between moderate and far-right publishers when we examine the
writing style of their articles. Furthermore, we can even identify users prone
to sharing misinformation based on their communication style. These findings
have important implications for countering online misinformation, as they
highlight the powerful role that users' personal bias towards specific topics,
and publishers' writing styles, have in amplifying fringe ideologies online.
\\ ( https://arxiv.org/abs/2403.08391 ,  615kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08462
Date: Wed, 13 Mar 2024 12:25:47 GMT   (319kb,D)

Title: Authorship Verification based on the Likelihood Ratio of Grammar Models
Authors: Andrea Nini, Oren Halvani, Lukas Graner, Valerio Gherardi, Shunichi
  Ishihara
Categories: cs.CL cs.LG
\\
  Authorship Verification (AV) is the process of analyzing a set of documents
to determine whether they were written by a specific author. This problem often
arises in forensic scenarios, e.g., in cases where the documents in question
constitute evidence for a crime. Existing state-of-the-art AV methods use
computational solutions that are not supported by a plausible scientific
explanation for their functioning and that are often difficult for analysts to
interpret. To address this, we propose a method relying on calculating a
quantity we call $\lambda_G$ (LambdaG): the ratio between the likelihood of a
document given a model of the Grammar for the candidate author and the
likelihood of the same document given a model of the Grammar for a reference
population. These Grammar Models are estimated using $n$-gram language models
that are trained solely on grammatical features. Despite not needing large
amounts of data for training, LambdaG still outperforms other established AV
methods with higher computational complexity, including a fine-tuned Siamese
Transformer network. Our empirical evaluation based on four baseline methods
applied to twelve datasets shows that LambdaG leads to better results in terms
of both accuracy and AUC in eleven cases and in all twelve cases if considering
only topic-agnostic methods. The algorithm is also highly robust to important
variations in the genre of the reference population in many cross-genre
comparisons. In addition to these properties, we demonstrate how LambdaG is
easier to interpret than the current state-of-the-art. We argue that the
advantage of LambdaG over other methods is due to fact that it is compatible
with Cognitive Linguistic theories of language processing.
\\ ( https://arxiv.org/abs/2403.08462 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08484
Date: Wed, 13 Mar 2024 12:50:23 GMT   (1443kb,D)

Title: Data-oriented Dynamic Fine-tuning Parameter Selection Strategy for FISH
  Mask based Efficient Fine-tuning
Authors: Ming Dong, Kang Xue, Bolong Zheng, Tingting He
Categories: cs.CL
\\
  In view of the huge number of parameters of Large language models (LLMs) ,
tuning all parameters is very costly, and accordingly fine-tuning specific
parameters is more sensible. Most of parameter efficient fine-tuning (PEFT)
concentrate on parameter selection strategies, such as additive method,
selective method and reparametrization-based method. However, there are few
methods that consider the impact of data samples on parameter selecting, such
as Fish Mask based method. Fish Mask randomly choose a part of data samples and
treat them equally during parameter selection, which is unable to dynamically
select optimal parameters for inconstant data distributions. In this work, we
adopt a data-oriented perspective, then proposing an IRD ($\mathrm{\underline
I}$terative sample-parameter $\mathrm{\underline R}$ange $\mathrm{\underline
D}$ecreasing) algorithm to search the best setting of sample-parameter pair for
FISH Mask. In each iteration, by searching the set of samples and parameters
with larger Fish information, IRD can find better sample-parameter pair in most
scale. We demonstrate the effectiveness and rationality of proposed strategy by
conducting experiments on GLUE benchmark. Experimental results show our
strategy optimizes the parameter selection and achieves preferable performance.
\\ ( https://arxiv.org/abs/2403.08484 ,  1443kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08492
Date: Wed, 13 Mar 2024 12:55:43 GMT   (3120kb,D)

Title: Rich Semantic Knowledge Enhanced Large Language Models for Few-shot
  Chinese Spell Checking
Authors: Ming Dong, Yujing Chen, Miao Zhang, Hao Sun, Tingting He
Categories: cs.CL
\\
  Chinese Spell Checking (CSC) is a widely used technology, which plays a vital
role in speech to text (STT) and optical character recognition (OCR). Most of
the existing CSC approaches relying on BERT architecture achieve excellent
performance. However, limited by the scale of the foundation model, BERT-based
method does not work well in few-shot scenarios, showing certain limitations in
practical applications. In this paper, we explore using an in-context learning
method named RS-LLM (Rich Semantic based LLMs) to introduce large language
models (LLMs) as the foundation model. Besides, we study the impact of
introducing various Chinese rich semantic information in our framework. We
found that by introducing a small number of specific Chinese rich semantic
structures, LLMs achieve better performance than the BERT-based model on
few-shot CSC task. Furthermore, we conduct experiments on multiple datasets,
and the experimental results verified the superiority of our proposed
framework.
\\ ( https://arxiv.org/abs/2403.08492 ,  3120kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08495
Date: Wed, 13 Mar 2024 13:04:58 GMT   (5795kb,D)

Title: Automatic Interactive Evaluation for Large Language Models with State
  Aware Patient Simulator
Authors: Yusheng Liao, Yutong Meng, Yuhao Wang, Hongcheng Liu, Yanfeng Wang, Yu
  Wang
Categories: cs.CL
Comments: 23 pages, 5 figures
\\
  Large Language Models (LLMs) have demonstrated remarkable proficiency in
human interactions, yet their application within the medical field remains
insufficiently explored. Previous works mainly focus on the performance of
medical knowledge with examinations, which is far from the realistic scenarios,
falling short in assessing the abilities of LLMs on clinical tasks. In the
quest to enhance the application of Large Language Models (LLMs) in healthcare,
this paper introduces the Automated Interactive Evaluation (AIE) framework and
the State-Aware Patient Simulator (SAPS), targeting the gap between traditional
LLM evaluations and the nuanced demands of clinical practice. Unlike prior
methods that rely on static medical knowledge assessments, AIE and SAPS provide
a dynamic, realistic platform for assessing LLMs through multi-turn
doctor-patient simulations. This approach offers a closer approximation to real
clinical scenarios and allows for a detailed analysis of LLM behaviors in
response to complex patient interactions. Our extensive experimental validation
demonstrates the effectiveness of the AIE framework, with outcomes that align
well with human evaluations, underscoring its potential to revolutionize
medical LLM testing for improved healthcare delivery.
\\ ( https://arxiv.org/abs/2403.08495 ,  5795kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08540
Date: Wed, 13 Mar 2024 13:54:00 GMT   (361kb,D)

Title: Language models scale reliably with over-training and on downstream
  tasks
Authors: Samir Yitzhak Gadre and Georgios Smyrnis and Vaishaal Shankar and
  Suchin Gururangan and Mitchell Wortsman and Rulin Shao and Jean Mercat and
  Alex Fang and Jeffrey Li and Sedrick Keh and Rui Xin and Marianna Nezhurina
  and Igor Vasiljevic and Jenia Jitsev and Alexandros G. Dimakis and Gabriel
  Ilharco and Shuran Song and Thomas Kollar and Yair Carmon and Achal Dave and
  Reinhard Heckel and Niklas Muennighoff and Ludwig Schmidt
Categories: cs.CL cs.LG
\\
  Scaling laws are useful guides for developing language models, but there are
still gaps between current scaling studies and how language models are
ultimately trained and evaluated. For instance, scaling is usually studied in
the compute-optimal training regime (i.e., "Chinchilla optimal" regime);
however, in practice, models are often over-trained to reduce inference costs.
Moreover, scaling laws mostly predict loss on next-token prediction, but
ultimately models are compared based on downstream task performance. In this
paper, we address both shortcomings. To do so, we create a testbed of 104
models with 0.011B to 6.9B parameters trained with various numbers of tokens on
three data distributions. First, we investigate scaling in the over-trained
regime. We fit scaling laws that extrapolate in both the number of model
parameters and the ratio of training tokens to parameters. This enables us to
predict the validation loss of a 1.4B parameter, 900B token run (i.e.,
32$\times$ over-trained) and a 6.9B parameter, 138B token
run$\unicode{x2014}$each from experiments that take 300$\times$ less compute.
Second, we relate the perplexity of a language model to its downstream task
performance via a power law. We use this law to predict top-1 error averaged
over downstream tasks for the two aforementioned models using experiments that
take 20$\times$ less compute. Our experiments are available at
https://github.com/mlfoundations/scaling.
\\ ( https://arxiv.org/abs/2403.08540 ,  361kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08564
Date: Wed, 13 Mar 2024 14:19:08 GMT   (2800kb,D)

Title: Non-discrimination Criteria for Generative Language Models
Authors: Sara Sterlie, Nina Weng, Aasa Feragen
Categories: cs.CL cs.AI cs.HC
Comments: 14 pages, 5 figures. Submitted to ACM Conference on Fairness,
  Accountability, and Transparency (ACM FAccT 2024)
\\
  Within recent years, generative AI, such as large language models, has
undergone rapid development. As these models become increasingly available to
the public, concerns arise about perpetuating and amplifying harmful biases in
applications. Gender stereotypes can be harmful and limiting for the
individuals they target, whether they consist of misrepresentation or
discrimination. Recognizing gender bias as a pervasive societal construct, this
paper studies how to uncover and quantify the presence of gender biases in
generative language models. In particular, we derive generative AI analogues of
three well-known non-discrimination criteria from classification, namely
independence, separation and sufficiency. To demonstrate these criteria in
action, we design prompts for each of the criteria with a focus on occupational
gender stereotype, specifically utilizing the medical test to introduce the
ground truth in the generative AI context. Our results address the presence of
occupational gender bias within such conversational language models.
\\ ( https://arxiv.org/abs/2403.08564 ,  2800kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08593
Date: Wed, 13 Mar 2024 14:59:07 GMT   (4274kb,D)

Title: Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over
  Structured Environments
Authors: Sitao Cheng, Ziyuan Zhuang, Yong Xu, Fangkai Yang, Chaoyun Zhang,
  Xiaoting Qin, Xiang Huang, Ling Chen, Qingwei Lin, Dongmei Zhang, Saravan
  Rajmohan, Qi Zhang
Categories: cs.CL cs.AI
Comments: 17 pages, 8 figures, 9 tables
\\
  Large Language Models (LLMs) have shown potential in reasoning over
structured environments, e.g., knowledge graph and table. Such tasks typically
require multi-hop reasoning, i.e., match natural language utterance with
instances in the environment. Previous methods leverage LLMs to incrementally
build a reasoning path, where the LLMs either invoke tools or pick up schemas
by step-by-step interacting with the environment. We propose
Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently
and faithfully reason over structured environments. In Readi, LLMs initially
generate a reasoning path given a query, and edit the path only when necessary.
We instantiate the path on structured environments and provide feedback to edit
the path if anything goes wrong. Experimental results on three KGQA datasets
and two TableQA datasets show the effectiveness of Readi, significantly
surpassing all LLM-based methods (by 9.1% on WebQSP, 12.4% on MQA-3H and 10.9%
on WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and
74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ).
Our code will be available upon publication.
\\ ( https://arxiv.org/abs/2403.08593 ,  4274kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08604
Date: Wed, 13 Mar 2024 15:13:44 GMT   (10471kb,D)

Title: DevBench: A Comprehensive Benchmark for Software Development
Authors: Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li,
  Shunyu Yao, Chen Qian, Binyuan Hui, Qicheng Zhang, Zhiyin Yu, He Du, Ping
  Yang, Dahua Lin, Chao Peng, Kai Chen
Categories: cs.CL cs.SE
Comments: Our data and code are available at
  https://github.com/open-compass/DevBench
\\
  Recent advancements in large language models (LLMs) have significantly
enhanced their coding capabilities. However, existing benchmarks predominantly
focused on simplified or isolated aspects of programming, such as single-file
code generation or repository issue debugging, falling short of measuring the
full spectrum of challenges raised by real-world programming activities. To
this end, we propose DevBench, a comprehensive benchmark that evaluates LLMs
across various stages of the software development lifecycle, including software
design, environment setup, implementation, acceptance testing, and unit
testing. DevBench features a wide range of programming languages and domains,
high-quality data collection, and carefully designed and verified metrics for
each task. Empirical studies show that current LLMs, including GPT-4-Turbo,
fail to solve the challenges presented within DevBench. Analyses reveal that
models struggle with understanding the complex structures in the repository,
managing the compilation process, and grasping advanced programming concepts.
Our findings offer actionable insights for the future development of LLMs
toward real-world programming applications. Our benchmark is available at
https://github.com/open-compass/DevBench
\\ ( https://arxiv.org/abs/2403.08604 ,  10471kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08607
Date: Wed, 13 Mar 2024 15:20:30 GMT   (2850kb,D)

Title: MedInsight: A Multi-Source Context Augmentation Framework for Generating
  Patient-Centric Medical Responses using Large Language Models
Authors: Subash Neupane, Shaswata Mitra, Sudip Mittal, Noorbakhsh Amiri
  Golilarz, Shahram Rahimi, Amin Amirlatifi
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) have shown impressive capabilities in generating
human-like responses. However, their lack of domain-specific knowledge limits
their applicability in healthcare settings, where contextual and comprehensive
responses are vital. To address this challenge and enable the generation of
patient-centric responses that are contextually relevant and comprehensive, we
propose MedInsight:a novel retrieval augmented framework that augments LLM
inputs (prompts) with relevant background information from multiple sources.
MedInsight extracts pertinent details from the patient's medical record or
consultation transcript. It then integrates information from authoritative
medical textbooks and curated web resources based on the patient's health
history and condition. By constructing an augmented context combining the
patient's record with relevant medical knowledge, MedInsight generates
enriched, patient-specific responses tailored for healthcare applications such
as diagnosis, treatment recommendations, or patient education. Experiments on
the MTSamples dataset validate MedInsight's effectiveness in generating
contextually appropriate medical responses. Quantitative evaluation using the
Ragas metric and TruLens for answer similarity and answer correctness
demonstrates the model's efficacy. Furthermore, human evaluation studies
involving Subject Matter Expert (SMEs) confirm MedInsight's utility, with
moderate inter-rater agreement on the relevance and correctness of the
generated responses.
\\ ( https://arxiv.org/abs/2403.08607 ,  2850kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08664
Date: Wed, 13 Mar 2024 16:17:09 GMT   (1264kb,D)

Title: Zero-shot and Few-shot Generation Strategies for Artificial Clinical
  Records
Authors: Erlend Frayling, Jake Lever, Graham McDonald
Categories: cs.CL cs.LG
Comments: 4 pages
\\
  The challenge of accessing historical patient data for clinical research,
while adhering to privacy regulations, is a significant obstacle in medical
science. An innovative approach to circumvent this issue involves utilising
synthetic medical records that mirror real patient data without compromising
individual privacy. The creation of these synthetic datasets, particularly
without using actual patient data to train Large Language Models (LLMs),
presents a novel solution as gaining access to sensitive patient information to
train models is also a challenge. This study assesses the capability of the
Llama 2 LLM to create synthetic medical records that accurately reflect real
patient information, employing zero-shot and few-shot prompting strategies for
comparison against fine-tuned methodologies that do require sensitive patient
data during training. We focus on generating synthetic narratives for the
History of Present Illness section, utilising data from the MIMIC-IV dataset
for comparison. In this work introduce a novel prompting technique that
leverages a chain-of-thought approach, enhancing the model's ability to
generate more accurate and contextually relevant medical narratives without
prior fine-tuning. Our findings suggest that this chain-of-thought prompted
approach allows the zero-shot model to achieve results on par with those of
fine-tuned models, based on Rouge metrics evaluation.
\\ ( https://arxiv.org/abs/2403.08664 ,  1264kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08688
Date: Wed, 13 Mar 2024 16:44:39 GMT   (233kb,D)

Title: Token Alignment via Character Matching for Subword Completion
Authors: Ben Athiwaratkun, Shiqi Wang, Mingyue Shang, Yuchen Tian, Zijian Wang,
  Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Rob Kwiatowski, Ramesh
  Nallapati, Bing Xiang
Categories: cs.CL cs.AI
\\
  Generative models, widely utilized in various applications, can often
struggle with prompts corresponding to partial tokens. This struggle stems from
tokenization, where partial tokens fall out of distribution during inference,
leading to incorrect or nonsensical outputs. This paper examines a technique to
alleviate the tokenization artifact on text completion in generative models,
maintaining performance even in regular non-subword cases. The method, termed
token alignment, involves backtracking to the last complete tokens and ensuring
the model's generation aligns with the prompt. This approach showcases marked
improvement across many partial token scenarios, including nuanced cases like
space-prefix and partial indentation, with only a minor time increase. The
technique and analysis detailed in this paper contribute to the continuous
advancement of generative models in handling partial inputs, bearing relevance
for applications like code completion and text autocompletion.
\\ ( https://arxiv.org/abs/2403.08688 ,  233kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08693
Date: Wed, 13 Mar 2024 16:56:33 GMT   (95kb,D)

Title: Do Language Models Care About Text Quality? Evaluating Web-Crawled
  Corpora Across 11 Languages
Authors: Rik van Noord, Taja Kuzman, Peter Rupnik, Nikola Ljube\v{s}i\'c,
  Miquel Espl\`a-Gomis, Gema Ram\'irez-S\'anchez, Antonio Toral
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024 (long)
\\
  Large, curated, web-crawled corpora play a vital role in training language
models (LMs). They form the lion's share of the training data in virtually all
recent LMs, such as the well-known GPT, LLaMA and XLM-RoBERTa models. However,
despite this importance, relatively little attention has been given to the
quality of these corpora. In this paper, we compare four of the currently most
relevant large, web-crawled corpora (CC100, MaCoCu, mC4 and OSCAR) across
eleven lower-resourced European languages. Our approach is two-fold: first, we
perform an intrinsic evaluation by performing a human evaluation of the quality
of samples taken from different corpora; then, we assess the practical impact
of the qualitative differences by training specific LMs on each of the corpora
and evaluating their performance on downstream tasks. We find that there are
clear differences in quality of the corpora, with MaCoCu and OSCAR obtaining
the best results. However, during the extrinsic evaluation, we actually find
that the CC100 corpus achieves the highest scores. We conclude that, in our
experiments, the quality of the web-crawled corpora does not seem to play a
significant role when training LMs.
\\ ( https://arxiv.org/abs/2403.08693 ,  95kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08694
Date: Wed, 13 Mar 2024 16:57:57 GMT   (8618kb,D)

Title: TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via
  Reinforcement Learning
Authors: Shangding Gu, Alois Knoll, Ming Jin
Categories: cs.CL
\\
  The development of Large Language Models (LLMs) often confronts challenges
stemming from the heavy reliance on human annotators in the reinforcement
learning with human feedback (RLHF) framework, or the frequent and costly
external queries tied to the self-instruct paradigm. In this work, we pivot to
Reinforcement Learning (RL) -- but with a twist. Diverging from the typical
RLHF, which refines LLMs following instruction data training, we use RL to
directly generate the foundational instruction dataset that alone suffices for
fine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and
rules, prioritizing the diversification of training datasets. It facilitates
the generation of high-quality data without excessive reliance on external
advanced models, paving the way for a single fine-tuning step and negating the
need for subsequent RLHF stages. Our findings highlight key advantages of our
approach: reduced need for human involvement and fewer model queries (only
$5.73\%$ of WizardLM's total), along with enhanced capabilities of LLMs in
crafting and comprehending complex instructions compared to strong baselines,
and substantially improved model privacy protection.
\\ ( https://arxiv.org/abs/2403.08694 ,  8618kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08715
Date: Wed, 13 Mar 2024 17:17:48 GMT   (5227kb,D)

Title: SOTOPIA-$\pi$: Interactive Learning of Socially Intelligent Language
  Agents
Authors: Ruiyi Wang, Haofei Yu, Wenxin Zhang, Zhengyang Qi, Maarten Sap, Graham
  Neubig, Yonatan Bisk, Hao Zhu
Categories: cs.CL
\\
  Humans learn social skills through both imitation and social interaction.
This social learning process is largely understudied by existing research on
building language agents. Motivated by this gap, we propose an interactive
learning method, SOTOPIA-$\pi$, improving the social intelligence of language
agents. This method leverages behavior cloning and self-reinforcement training
on filtered social interaction data according to large language model (LLM)
ratings. We show that our training method allows a 7B LLM to reach the social
goal completion ability of an expert model (GPT-4-based agent), while improving
the safety of language agents and maintaining general QA ability on the MMLU
benchmark. We also find that this training paradigm uncovers some difficulties
in LLM-based evaluation of social intelligence: LLM-based evaluators
overestimate the abilities of the language agents trained specifically for
social interaction.
\\ ( https://arxiv.org/abs/2403.08715 ,  5227kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08730
Date: Wed, 13 Mar 2024 17:29:45 GMT   (3467kb,D)

Title: Strengthening Multimodal Large Language Model with Bootstrapped
  Preference Optimization
Authors: Renjie Pi, Tianyang Han, Wei Xiong, Jipeng Zhang, Runtao Liu, Rui Pan,
  Tong Zhang
Categories: cs.CL cs.CV
\\
  Multimodal Large Language Models (MLLMs) excel in generating responses based
on visual inputs. However, they often suffer from a bias towards generating
responses similar to their pretraining corpus, overshadowing the importance of
visual information. We treat this bias as a "preference" for pretraining
statistics, which hinders the model's grounding in visual input. To mitigate
this issue, we propose Bootstrapped Preference Optimization (BPO), which
conducts preference learning with datasets containing negative responses
bootstrapped from the model itself. Specifically, we propose the following two
strategies: 1) using distorted image inputs to the MLLM for eliciting responses
that contain signified pretraining bias; 2) leveraging text-based LLM to
explicitly inject erroneous but common elements into the original response.
Those undesirable responses are paired with original annotated responses from
the datasets to construct the preference dataset, which is subsequently
utilized to perform preference learning. Our approach effectively suppresses
pretrained LLM bias, enabling enhanced grounding in visual inputs. Extensive
experimentation demonstrates significant performance improvements across
multiple benchmarks, advancing the state-of-the-art in multimodal
conversational systems.
\\ ( https://arxiv.org/abs/2403.08730 ,  3467kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08738
Date: Wed, 13 Mar 2024 17:42:03 GMT   (8266kb,D)

Title: Improving Acoustic Word Embeddings through Correspondence Training of
  Self-supervised Speech Representations
Authors: Amit Meghanani and Thomas Hain
Categories: cs.CL cs.SD eess.AS
Comments: Accepted to EACL 2024 Main Conference, Long paper
\\
  Acoustic word embeddings (AWEs) are vector representations of spoken words.
An effective method for obtaining AWEs is the Correspondence Auto-Encoder
(CAE). In the past, the CAE method has been associated with traditional MFCC
features. Representations obtained from self-supervised learning (SSL)-based
speech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many
downstream tasks. However, they have not been well studied in the context of
learning AWEs. This work explores the effectiveness of CAE with SSL-based
speech representations to obtain improved AWEs. Additionally, the capabilities
of SSL-based speech models are explored in cross-lingual scenarios for
obtaining AWEs. Experiments are conducted on five languages: Polish,
Portuguese, Spanish, French, and English. HuBERT-based CAE model achieves the
best results for word discrimination in all languages, despite Hu-BERT being
pre-trained on English only. Also, the HuBERT-based CAE model works well in
cross-lingual settings. It outperforms MFCC-based CAE models trained on the
target languages when trained on one source language and tested on target
languages.
\\ ( https://arxiv.org/abs/2403.08738 ,  8266kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08739
Date: Wed, 13 Mar 2024 17:42:32 GMT   (3297kb,D)

Title: The Garden of Forking Paths: Observing Dynamic Parameters Distribution
  in Large Language Models
Authors: Carlo Nicolini, Jacopo Staiano, Bruno Lepri, Raffaele Marino
Categories: cs.CL cond-mat.dis-nn cond-mat.stat-mech cs.AI
Comments: 15 pages
\\
  A substantial gap persists in understanding the reasons behind the
exceptional performance of the Transformer architecture in NLP. A particularly
unexplored area involves the mechanistic description of how the distribution of
parameters evolves over time during training. In this work we suggest that
looking at the time evolution of the statistic distribution of model
parameters, and specifically at bifurcation effects, can help understanding the
model quality, potentially reducing training costs and evaluation efforts and
empirically showing the reasons behind the effectiveness of weights
sparsification.
\\ ( https://arxiv.org/abs/2403.08739 ,  3297kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08743
Date: Wed, 13 Mar 2024 17:46:28 GMT   (2999kb,D)

Title: Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing
  Framework
Authors: Jingling Li, Zeyu Tang, Xiaoyu Liu, Peter Spirtes, Kun Zhang, Liu
  Leqi, Yang Liu
Categories: cs.CL cs.AI cs.LG
Comments: 18 pages, 11 figures
\\
  Large language models (LLMs) can easily generate biased and discriminative
responses. As LLMs tap into consequential decision-making (e.g., hiring and
healthcare), it is of crucial importance to develop strategies to mitigate
these biases. This paper focuses on social bias, tackling the association
between demographic information and LLM outputs. We propose a causality-guided
debiasing framework that utilizes causal understandings of (1) the
data-generating process of the training corpus fed to LLMs, and (2) the
internal reasoning process of LLM inference, to guide the design of prompts for
debiasing LLM outputs through selection mechanisms. Our framework unifies
existing de-biasing prompting approaches such as inhibitive instructions and
in-context contrastive examples, and sheds light on new ways of debiasing by
encouraging bias-free reasoning. Our strong empirical performance on real-world
datasets demonstrates that our framework provides principled guidelines on
debiasing LLM outputs even with only the black-box access.
\\ ( https://arxiv.org/abs/2403.08743 ,  2999kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07921
Date: Wed, 28 Feb 2024 03:20:27 GMT   (981kb,D)

Title: Merino: Entropy-driven Design for Generative Language Models on IoT
  Devices
Authors: Youpeng Zhao, Ming Lin, Huadong Tang, Qiang Wu, Jun Wang
Categories: cs.LG cs.AI cs.CL
\\
  Generative Large Language Models (LLMs) stand as a revolutionary advancement
in the modern era of artificial intelligence (AI). However, directly deploying
LLMs in resource-constrained hardware, such as Internet-of-Things (IoT)
devices, is difficult due to their high computational cost. In this paper, we
propose a novel information-entropy framework for designing mobile-friendly
generative language models. Our key design paradigm is to maximize the entropy
of transformer decoders within the given computational budgets. The whole
design procedure involves solving a mathematical programming (MP) problem,
which can be done on the CPU within minutes, making it nearly zero-cost. We
evaluate our designed models, termed MeRino, across nine NLP downstream tasks,
showing their competitive performance against the state-of-the-art
autoregressive transformer models under the mobile setting. Notably, MeRino
achieves similar or better zero performance compared to the 350M parameter OPT
while being 4.9x faster on NVIDIA Jetson Nano with 5.5x reduction in model
size. Code will be made available soon.
\\ ( https://arxiv.org/abs/2403.07921 ,  981kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07929
Date: Fri, 1 Mar 2024 22:56:19 GMT   (299kb,D)

Title: Sketching the Heat Kernel: Using Gaussian Processes to Embed Data
Authors: Anna C. Gilbert, Kevin O'Neill
Categories: cs.LG cs.NA math.NA stat.ML
Comments: 28 pages
\\
  This paper introduces a novel, non-deterministic method for embedding data in
low-dimensional Euclidean space based on computing realizations of a Gaussian
process depending on the geometry of the data. This type of embedding first
appeared in (Adler et al, 2018) as a theoretical model for a generic manifold
in high dimensions.
  In particular, we take the covariance function of the Gaussian process to be
the heat kernel, and computing the embedding amounts to sketching a matrix
representing the heat kernel. The Karhunen-Lo\`eve expansion reveals that the
straight-line distances in the embedding approximate the diffusion distance in
a probabilistic sense, avoiding the need for sharp cutoffs and maintaining some
of the smaller-scale structure.
  Our method demonstrates further advantage in its robustness to outliers. We
justify the approach with both theory and experiments.
\\ ( https://arxiv.org/abs/2403.07929 ,  299kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07943
Date: Sun, 10 Mar 2024 15:50:04 GMT   (469kb,D)

Title: Revisiting Edge Perturbation for Graph Neural Network in Graph Data
  Augmentation and Attack
Authors: Xin Liu, Yuxiang Zhang, Meng Wu, Mingyu Yan, Kun He, Wei Yan, Shirui
  Pan, Xiaochun Ye, Dongrui Fan
Categories: cs.LG cs.CR
Comments: 14P
\\
  Edge perturbation is a basic method to modify graph structures. It can be
categorized into two veins based on their effects on the performance of graph
neural networks (GNNs), i.e., graph data augmentation and attack. Surprisingly,
both veins of edge perturbation methods employ the same operations, yet yield
opposite effects on GNNs' accuracy. A distinct boundary between these methods
in using edge perturbation has never been clearly defined. Consequently,
inappropriate perturbations may lead to undesirable outcomes, necessitating
precise adjustments to achieve desired effects. Therefore, questions of ``why
edge perturbation has a two-faced effect?'' and ``what makes edge perturbation
flexible and effective?'' still remain unanswered.
  In this paper, we will answer these questions by proposing a unified
formulation and establishing a clear boundary between two categories of edge
perturbation methods. Specifically, we conduct experiments to elucidate the
differences and similarities between these methods and theoretically unify the
workflow of these methods by casting it to one optimization problem. Then, we
devise Edge Priority Detector (EPD) to generate a novel priority metric,
bridging these methods up in the workflow. Experiments show that EPD can make
augmentation or attack flexibly and achieve comparable or superior performance
to other counterparts with less time overhead.
\\ ( https://arxiv.org/abs/2403.07943 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07953
Date: Tue, 12 Mar 2024 06:25:47 GMT   (3032kb,D)

Title: Abstracting Sparse DNN Acceleration via Structured Sparse Tensor
  Decomposition
Authors: Geonhwa Jeong, Po-An Tsai, Abhimanyu R. Bambhaniya, Stephen W.
  Keckler, Tushar Krishna
Categories: cs.LG cs.AI cs.AR
\\
  Exploiting sparsity in deep neural networks (DNNs) has been a promising area
to meet the growing computation need of modern DNNs. However, in practice,
sparse DNN acceleration still faces a key challenge. To minimize the overhead
of sparse acceleration, hardware designers have proposed structured sparse
hardware support recently, which provides limited flexibility and requires
extra model fine-tuning. Moreover, any sparse model fine-tuned for certain
structured sparse hardware cannot be accelerated by other structured hardware.
To bridge the gap between sparse DNN models and hardware, this paper proposes
tensor approximation via structured decomposition (TASD), which leverages the
distributive property in linear algebra to turn any sparse tensor into a series
of structured sparse tensors. Next, we develop a software framework, TASDER, to
accelerate DNNs by searching layer-wise, high-quality structured decomposition
for both weight and activation tensors so that they can be accelerated by any
systems with structured sparse hardware support. Evaluation results show that,
by exploiting prior structured sparse hardware baselines, our method can
accelerate off-the-shelf dense and sparse DNNs without fine-tuning and improves
energy-delay-product by up to 83% and 74% on average.
\\ ( https://arxiv.org/abs/2403.07953 ,  3032kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07954
Date: Tue, 12 Mar 2024 06:26:17 GMT   (102kb,D)

Title: Optimizing Polynomial Graph Filters: A Novel Adaptive Krylov Subspace
  Approach
Authors: Keke Huang, Wencai Cao, Hoang Ta, Xiaokui Xiao, and Pietro Li\`o
Categories: cs.LG eess.SP
\\
  Graph Neural Networks (GNNs), known as spectral graph filters, find a wide
range of applications in web networks. To bypass eigendecomposition, polynomial
graph filters are proposed to approximate graph filters by leveraging various
polynomial bases for filter training. However, no existing studies have
explored the diverse polynomial graph filters from a unified perspective for
optimization.
  In this paper, we first unify polynomial graph filters, as well as the
optimal filters of identical degrees into the Krylov subspace of the same
order, thus providing equivalent expressive power theoretically. Next, we
investigate the asymptotic convergence property of polynomials from the unified
Krylov subspace perspective, revealing their limited adaptability in graphs
with varying heterophily degrees. Inspired by those facts, we design a novel
adaptive Krylov subspace approach to optimize polynomial bases with provable
controllability over the graph spectrum so as to adapt various heterophily
graphs. Subsequently, we propose AdaptKry, an optimized polynomial graph filter
utilizing bases from the adaptive Krylov subspaces. Meanwhile, in light of the
diverse spectral properties of complex graphs, we extend AdaptKry by leveraging
multiple adaptive Krylov bases without incurring extra training costs. As a
consequence, extended AdaptKry is able to capture the intricate characteristics
of graphs and provide insights into their inherent complexity. We conduct
extensive experiments across a series of real-world datasets. The experimental
results demonstrate the superior filtering capability of AdaptKry, as well as
the optimized efficacy of the adaptive Krylov basis.
\\ ( https://arxiv.org/abs/2403.07954 ,  102kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07955
Date: Tue, 12 Mar 2024 07:24:17 GMT   (11505kb,D)

Title: Towards Faithful Explanations: Boosting Rationalization with Shortcuts
  Discovery
Authors: Linan Yue, Qi Liu, Yichao Du, Li Wang, Weibo Gao, Yanqing An
Categories: cs.LG cs.AI
Comments: Accepted to ICLR 2024
\\
  The remarkable success in neural networks provokes the selective
rationalization. It explains the prediction results by identifying a small
subset of the inputs sufficient to support them. Since existing methods still
suffer from adopting the shortcuts in data to compose rationales and limited
large-scale annotated rationales by human, in this paper, we propose a
Shortcuts-fused Selective Rationalization (SSR) method, which boosts the
rationalization by discovering and exploiting potential shortcuts.
Specifically, SSR first designs a shortcuts discovery approach to detect
several potential shortcuts. Then, by introducing the identified shortcuts, we
propose two strategies to mitigate the problem of utilizing shortcuts to
compose rationales. Finally, we develop two data augmentations methods to close
the gap in the number of annotated rationales. Extensive experimental results
on real-world datasets clearly validate the effectiveness of our proposed
method.
\\ ( https://arxiv.org/abs/2403.07955 ,  11505kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07956
Date: Tue, 12 Mar 2024 08:07:06 GMT   (1068kb,D)

Title: DeepCDCL: An CDCL-based Neural Network Verification Framework
Authors: Zongxin Liu, Pengfei Yang, Lijun Zhang, Xiaowei Huang
Categories: cs.LG cs.AI
\\
  Neural networks in safety-critical applications face increasing safety and
security concerns due to their susceptibility to little disturbance. In this
paper, we propose DeepCDCL, a novel neural network verification framework based
on the Conflict-Driven Clause Learning (CDCL) algorithm. We introduce an
asynchronous clause learning and management structure, reducing redundant time
consumption compared to the direct application of the CDCL framework.
Furthermore, we also provide a detailed evaluation of the performance of our
approach on the ACAS Xu and MNIST datasets, showing that a significant speed-up
is achieved in most cases.
\\ ( https://arxiv.org/abs/2403.07956 ,  1068kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07957
Date: Tue, 12 Mar 2024 08:27:53 GMT   (317kb,D)

Title: Efficient Post-Training Augmentation for Adaptive Inference in
  Heterogeneous and Distributed IoT Environments
Authors: Max Sponner and Lorenzo Servadei and Bernd Waschneck and Robert Wille
  and Akash Kumar
Categories: cs.LG cs.AI
\\
  Early Exit Neural Networks (EENNs) present a solution to enhance the
efficiency of neural network deployments. However, creating EENNs is
challenging and requires specialized domain knowledge, due to the large amount
of additional design choices. To address this issue, we propose an automated
augmentation flow that focuses on converting an existing model into an EENN. It
performs all required design decisions for the deployment to heterogeneous or
distributed hardware targets: Our framework constructs the EENN architecture,
maps its subgraphs to the hardware targets, and configures its decision
mechanism. To the best of our knowledge, it is the first framework that is able
to perform all of these steps.
  We evaluated our approach on a collection of Internet-of-Things and standard
image classification use cases. For a speech command detection task, our
solution was able to reduce the mean operations per inference by 59.67%. For an
ECG classification task, it was able to terminate all samples early, reducing
the mean inference energy by 74.9% and computations by 78.3%. On CIFAR-10, our
solution was able to achieve up to a 58.75% reduction in computations.
  The search on a ResNet-152 base model for CIFAR-10 took less than nine hours
on a laptop CPU. Our proposed approach enables the creation of EENN optimized
for IoT environments and can reduce the inference cost of Deep Learning
applications on embedded and fog platforms, while also significantly reducing
the search cost - making it more accessible for scientists and engineers in
industry and research. The low search cost improves the accessibility of EENNs,
with the potential to improve the efficiency of neural networks in a wide range
of practical applications.
\\ ( https://arxiv.org/abs/2403.07957 ,  317kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07958
Date: Tue, 12 Mar 2024 08:28:27 GMT   (286kb,D)

Title: Temporal Decisions: Leveraging Temporal Correlation for Efficient
  Decisions in Early Exit Neural Networks
Authors: Max Sponner and Lorenzo Servadei and Bernd Waschneck and Robert Wille
  and Akash Kumar
Categories: cs.LG cs.AI
\\
  Deep Learning is becoming increasingly relevant in Embedded and
Internet-of-things applications. However, deploying models on embedded devices
poses a challenge due to their resource limitations. This can impact the
model's inference accuracy and latency. One potential solution are Early Exit
Neural Networks, which adjust model depth dynamically through additional
classifiers attached between their hidden layers. However, the real-time
termination decision mechanism is critical for the system's efficiency,
latency, and sustained accuracy.
  This paper introduces Difference Detection and Temporal Patience as decision
mechanisms for Early Exit Neural Networks. They leverage the temporal
correlation present in sensor data streams to efficiently terminate the
inference. We evaluate their effectiveness in health monitoring, image
classification, and wake-word detection tasks. Our novel contributions were
able to reduce the computational footprint compared to established decision
mechanisms significantly while maintaining higher accuracy scores. We achieved
a reduction of mean operations per inference by up to 80% while maintaining
accuracy levels within 5% of the original model.
  These findings highlight the importance of considering temporal correlation
in sensor data to improve the termination decision.
\\ ( https://arxiv.org/abs/2403.07958 ,  286kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07965
Date: Tue, 12 Mar 2024 11:56:38 GMT   (1211kb,D)

Title: Conditional computation in neural networks: principles and research
  trends
Authors: Simone Scardapane, Alessandro Baiocchi, Alessio Devoto, Valerio
  Marsocci, Pasquale Minervini, Jary Pomponi
Categories: cs.LG cs.AI
Comments: Under review at Intelligenza Artificiale (IOS Press)
\\
  This article summarizes principles and ideas from the emerging area of
applying \textit{conditional computation} methods to the design of neural
networks. In particular, we focus on neural networks that can dynamically
activate or de-activate parts of their computational graph conditionally on
their input. Examples include the dynamic selection of, e.g., input tokens,
layers (or sets of layers), and sub-modules inside each layer (e.g., channels
in a convolutional filter). We first provide a general formalism to describe
these techniques in an uniform way. Then, we introduce three notable
implementations of these principles: mixture-of-experts (MoEs) networks, token
selection mechanisms, and early-exit neural networks. The paper aims to provide
a tutorial-like introduction to this growing field. To this end, we analyze the
benefits of these modular designs in terms of efficiency, explainability, and
transfer learning, with a focus on emerging applicative areas ranging from
automated scientific discovery to semantic communication.
\\ ( https://arxiv.org/abs/2403.07965 ,  1211kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07966
Date: Tue, 12 Mar 2024 12:59:00 GMT   (27161kb,D)

Title: Applying ranking techniques for estimating influence of Earth variables
  on temperature forecast error
Authors: M. Julia Flores, Melissa Ruiz-V\'asquez, Ana Bastos, Ren\'e Orth
Categories: cs.LG physics.ao-ph
Comments: 17 pages, 19 figures, 5 tables, results SIMD UCLM - BGC Jena
  collaboration, research stay
\\
  This paper describes how to analyze the influence of Earth system variables
on the errors when providing temperature forecasts. The initial framework to
get the data has been based on previous research work, which resulted in a very
interesting discovery. However, the aforementioned study only worked on
individual correlations of the variables with respect to the error. This
research work is going to re-use the main ideas but introduce three main
novelties: (1) applying a data science approach by a few representative
locations; (2) taking advantage of the rankings created by Spearman correlation
but enriching them with other metrics looking for a more robust ranking of the
variables; (3) evaluation of the methodology by learning random forest models
for regression with the distinct experimental variations. The main contribution
is the framework that shows how to convert correlations into rankings and
combine them into an aggregate ranking. We have carried out experiments on five
chosen locations to analyze the behavior of this ranking-based methodology. The
results show that the specific performance is dependent on the location and
season, which is expected, and that this selection technique works properly
with Random Forest models but can also improve simpler regression models such
as Bayesian Ridge. This work also contributes with an extensive analysis of the
results. We can conclude that this selection based on the top-k ranked
variables seems promising for this real problem, and it could also be applied
in other domains.
\\ ( https://arxiv.org/abs/2403.07966 ,  27161kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07967
Date: Tue, 12 Mar 2024 13:31:13 GMT   (4413kb,D)

Title: Feasibility of machine learning-based rice yield prediction in India at
  the district level using climate reanalysis data
Authors: Djavan De Clercq, Adam Mahdi
Categories: cs.LG
\\
  Yield forecasting, the science of predicting agricultural productivity before
the crop harvest occurs, helps a wide range of stakeholders make better
decisions around agricultural planning. This study aims to investigate whether
machine learning-based yield prediction models can capably predict Kharif
season rice yields at the district level in India several months before the
rice harvest takes place. The methodology involved training 19 machine learning
models such as CatBoost, LightGBM, Orthogonal Matching Pursuit, and Extremely
Randomized Trees on 20 years of climate, satellite, and rice yield data across
247 of Indian rice-producing districts. In addition to model-building, a
dynamic dashboard was built understand how the reliability of rice yield
predictions varies across districts. The results of the proof-of-concept
machine learning pipeline demonstrated that rice yields can be predicted with a
reasonable degree of accuracy, with out-of-sample R2, MAE, and MAPE performance
of up to 0.82, 0.29, and 0.16 respectively. These results outperformed test set
performance reported in related literature on rice yield modeling in other
contexts and countries. In addition, SHAP value analysis was conducted to infer
both the importance and directional impact of the climate and remote sensing
variables included in the model. Important features driving rice yields
included temperature, soil water volume, and leaf area index. In particular,
higher temperatures in August correlate with increased rice yields,
particularly when the leaf area index in August is also high. Building on the
results, a proof-of-concept dashboard was developed to allow users to easily
explore which districts may experience a rise or fall in yield relative to the
previous year.
\\ ( https://arxiv.org/abs/2403.07967 ,  4413kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07968
Date: Tue, 12 Mar 2024 13:59:23 GMT   (771kb,D)

Title: Do Deep Neural Network Solutions Form a Star Domain?
Authors: Ankit Sonthalia, Alexander Rubinstein, Ehsan Abbasnejad, Seong Joon Oh
Categories: cs.LG cs.AI
\\
  Entezari et al. (2022) conjectured that neural network solution sets
reachable via stochastic gradient descent (SGD) are convex, considering
permutation invariances. This means that two independent solutions can be
connected by a linear path with low loss, given one of them is appropriately
permuted. However, current methods to test this theory often fail to eliminate
loss barriers between two independent solutions (Ainsworth et al., 2022;
Benzing et al., 2022). In this work, we conjecture that a more relaxed claim
holds: the SGD solution set is a star domain that contains a star model that is
linearly connected to all the other solutions via paths with low loss values,
modulo permutations. We propose the Starlight algorithm that finds a star model
of a given learning task. We validate our claim by showing that this star model
is linearly connected with other independently found solutions. As an
additional benefit of our study, we demonstrate better uncertainty estimates on
Bayesian Model Averaging over the obtained star domain. Code is available at
https://github.com/aktsonthalia/starlight.
\\ ( https://arxiv.org/abs/2403.07968 ,  771kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07969
Date: Tue, 12 Mar 2024 14:56:34 GMT   (2124kb,D)

Title: KnowCoder: Coding Structured Knowledge into LLMs for Universal
  Information Extraction
Authors: Zixuan Li, Yutao Zeng, Yuxin Zuo, Weicheng Ren, Wenxuan Liu, Miao Su,
  Yucan Guo, Yantao Liu, Xiang Li, Zhilei Hu, Long Bai, Wei Li, Yidan Liu, Pan
  Yang, Xiaolong Jin, Jiafeng Guo and Xueqi Cheng
Categories: cs.LG cs.AI
\\
  In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct
Universal Information Extraction (UIE) via code generation. KnowCoder aims to
develop a kind of unified schema representation that LLMs can easily understand
and an effective learning framework that encourages LLMs to follow schemas and
extract structured knowledge accurately. To achieve these, KnowCoder introduces
a code-style schema representation method to uniformly transform different
schemas into Python classes, with which complex schema information, such as
constraints among tasks in UIE, can be captured in an LLM-friendly manner. We
further construct a code-style schema library covering over $\textbf{30,000}$
types of knowledge, which is the largest one for UIE, to the best of our
knowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase
learning framework that enhances its schema understanding ability via code
pretraining and its schema following ability via instruction tuning. After code
pretraining on around $1.5$B automatically constructed data, KnowCoder already
attains remarkable generalization ability and achieves relative improvements by
$\textbf{49.8\%}$ F1, compared to LLaMA2, under the few-shot setting. After
instruction tuning, KnowCoder further exhibits strong generalization ability on
unseen schemas and achieves up to $\textbf{12.5\%}$ and $\textbf{21.9\%}$,
compared to sota baselines, under the zero-shot setting and the low resource
setting, respectively. Additionally, based on our unified schema
representations, various human-annotated datasets can simultaneously be
utilized to refine KnowCoder, which achieves significant improvements up to
$\textbf{7.5\%}$ under the supervised setting.
\\ ( https://arxiv.org/abs/2403.07969 ,  2124kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07979
Date: Tue, 12 Mar 2024 18:00:02 GMT   (210kb,D)

Title: Do Agents Dream of Electric Sheep?: Improving Generalization in
  Reinforcement Learning through Generative Learning
Authors: Giorgio Franceschelli and Mirco Musolesi
Categories: cs.LG cs.AI
\\
  The Overfitted Brain hypothesis suggests dreams happen to allow
generalization in the human brain. Here, we ask if the same is true for
reinforcement learning agents as well. Given limited experience in a real
environment, we use imagination-based reinforcement learning to train a policy
on dream-like episodes, where non-imaginative, predicted trajectories are
modified through generative augmentations. Experiments on four ProcGen
environments show that, compared to classic imagination and offline training on
collected experience, our method can reach a higher level of generalization
when dealing with sparsely rewarded environments.
\\ ( https://arxiv.org/abs/2403.07979 ,  210kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08013
Date: Tue, 12 Mar 2024 18:25:10 GMT   (7447kb,D)

Title: Supervised Time Series Classification for Anomaly Detection in Subsea
  Engineering
Authors: Ergys \c{C}okaj, Halvor Snersrud Gustad, Andrea Leone, Per Thomas Moe,
  Lasse Moldestad
Categories: cs.LG math.DS
MSC-class: Primary: 62M10, Secondary: 62P30, 68T07
\\
  Time series classification is of significant importance in monitoring
structural systems. In this work, we investigate the use of supervised machine
learning classification algorithms on simulated data based on a physical system
with two states: Intact and Broken. We provide a comprehensive discussion of
the preprocessing of temporal data, using measures of statistical dispersion
and dimension reduction techniques. We present an intuitive baseline method and
discuss its efficiency. We conclude with a comparison of the various methods
based on different performance metrics, showing the advantage of using machine
learning techniques as a tool in decision making.
\\ ( https://arxiv.org/abs/2403.08013 ,  7447kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08024
Date: Tue, 12 Mar 2024 18:46:56 GMT   (1343kb,D)

Title: xMLP: Revolutionizing Private Inference with Exclusive Square Activation
Authors: Jiajie Li, Jinjun Xiong
Categories: cs.LG cs.CR
\\
  Private Inference (PI) enables deep neural networks (DNNs) to work on private
data without leaking sensitive information by exploiting cryptographic
primitives such as multi-party computation (MPC) and homomorphic encryption
(HE). However, the use of non-linear activations such as ReLU in DNNs can lead
to impractically high PI latency in existing PI systems, as ReLU requires the
use of costly MPC computations, such as Garbled Circuits. Since square
activations can be processed by Beaver's triples hundreds of times faster
compared to ReLU, they are more friendly to PI tasks, but using them leads to a
notable drop in model accuracy. This paper starts by exploring the reason for
such an accuracy drop after using square activations, and concludes that this
is due to an "information compounding" effect. Leveraging this insight, we
propose xMLP, a novel DNN architecture that uses square activations exclusively
while maintaining parity in both accuracy and efficiency with ReLU-based DNNs.
Our experiments on CIFAR-100 and ImageNet show that xMLP models consistently
achieve better performance than ResNet models with fewer activation layers and
parameters while maintaining consistent performance with its ReLU-based
variants. Remarkably, when compared to state-of-the-art PI Models, xMLP
demonstrates superior performance, achieving a 0.58% increase in accuracy with
7x faster PI speed. Moreover, it delivers a significant accuracy improvement of
4.96% while maintaining the same PI latency. When offloading PI to the GPU,
xMLP is up to 700x faster than the previous state-of-the-art PI model with
comparable accuracy.
\\ ( https://arxiv.org/abs/2403.08024 ,  1343kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08027
Date: Tue, 12 Mar 2024 18:55:23 GMT   (6215kb,D)

Title: McCatch: Scalable Microcluster Detection in Dimensional and
  Nondimensional Datasets
Authors: Braulio V. S\'anchez Vinces, Robson L. F. Cordeiro, Christos Faloutsos
Categories: cs.LG
Comments: \c{opyright} 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
\\
  How could we have an outlier detector that works even with nondimensional
data, and ranks together both singleton microclusters ('one-off' outliers) and
nonsingleton microclusters by their anomaly scores? How to obtain scores that
are principled in one scalable and 'hands-off' manner? Microclusters of
outliers indicate coalition or repetition in fraud activities, etc.; their
identification is thus highly desirable. This paper presents McCatch: a new
algorithm that detects microclusters by leveraging our proposed 'Oracle' plot
(1NN Distance versus Group 1NN Distance). We study 31 real and synthetic
datasets with up to 1M data elements to show that McCatch is the only method
that answers both of the questions above; and, it outperforms 11 other methods,
especially when the data has nonsingleton microclusters or is nondimensional.
We also showcase McCatch's ability to detect meaningful microclusters in
graphs, fingerprints, logs of network connections, text data, and satellite
imagery. For example, it found a 30-elements microcluster of confirmed 'Denial
of Service' attacks in the network logs, taking only ~3 minutes for 222K data
elements on a stock desktop.
\\ ( https://arxiv.org/abs/2403.08027 ,  6215kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08040
Date: Tue, 12 Mar 2024 19:23:13 GMT   (10198kb,D)

Title: MicroT: Low-Energy and Adaptive Models for MCUs
Authors: Yushan Huang, Ranya Aloufi, Xavier Cadet, Yuchen Zhao, Payam Barnaghi,
  Hamed Haddadi
Categories: cs.LG cs.AR
\\
  We propose MicroT, a low-energy, multi-task adaptive model framework for
resource-constrained MCUs. We divide the original model into a feature
extractor and a classifier. The feature extractor is obtained through
self-supervised knowledge distillation and further optimized into part and full
models through model splitting and joint training. These models are then
deployed on MCUs, with classifiers added and trained on local tasks, ultimately
performing stage-decision for joint inference. In this process, the part model
initially processes the sample, and if the confidence score falls below the set
threshold, the full model will resume and continue the inference. We evaluate
MicroT on two models, three datasets, and two MCU boards. Our experimental
evaluation shows that MicroT effectively improves model performance and reduces
energy consumption when dealing with multiple local tasks. Compared to the
unoptimized feature extractor, MicroT can improve accuracy by up to 9.87%. On
MCUs, compared to the standard full model inference, MicroT can save up to
about 29.13% in energy consumption. MicroT also allows users to adaptively
adjust the stage-decision ratio as needed, better balancing model performance
and energy consumption. Under the standard stage-decision ratio configuration,
MicroT can increase accuracy by 5.91% and save about 14.47% of energy
consumption.
\\ ( https://arxiv.org/abs/2403.08040 ,  10198kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08055
Date: Tue, 12 Mar 2024 20:02:39 GMT   (21182kb,D)

Title: DrivAerNet: A Parametric Car Dataset for Data-Driven Aerodynamic Design
  and Graph-Based Drag Prediction
Authors: Mohamed Elrefaie, Angela Dai, Faez Ahmed
Categories: cs.LG physics.flu-dyn
\\
  This study introduces DrivAerNet, a large-scale high-fidelity CFD dataset of
3D industry-standard car shapes, and RegDGCNN, a dynamic graph convolutional
neural network model, both aimed at aerodynamic car design through machine
learning. DrivAerNet, with its 4000 detailed 3D car meshes using 0.5 million
surface mesh faces and comprehensive aerodynamic performance data comprising of
full 3D pressure, velocity fields, and wall-shear stresses, addresses the
critical need for extensive datasets to train deep learning models in
engineering applications. It is 60\% larger than the previously available
largest public dataset of cars, and is the only open-source dataset that also
models wheels and underbody. RegDGCNN leverages this large-scale dataset to
provide high-precision drag estimates directly from 3D meshes, bypassing
traditional limitations such as the need for 2D image rendering or Signed
Distance Fields (SDF). By enabling fast drag estimation in seconds, RegDGCNN
facilitates rapid aerodynamic assessments, offering a substantial leap towards
integrating data-driven methods in automotive design. Together, DrivAerNet and
RegDGCNN promise to accelerate the car design process and contribute to the
development of more efficient vehicles. To lay the groundwork for future
innovations in the field, the dataset and code used in our study are publicly
accessible at \url{https://github.com/Mohamedelrefaie/DrivAerNet}
\\ ( https://arxiv.org/abs/2403.08055 ,  21182kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08058
Date: Tue, 12 Mar 2024 20:10:04 GMT   (2863kb,D)

Title: CHAI: Clustered Head Attention for Efficient LLM Inference
Authors: Saurabh Agarwal, Bilge Acun, Basil Homer, Mostafa Elhoushi, Yejin Lee,
  Shivaram Venkataraman, Dimitris Papailiopoulos, Carole-Jean Wu
Categories: cs.LG cs.CL
\\
  Large Language Models (LLMs) with hundreds of billions of parameters have
transformed the field of machine learning. However, serving these models at
inference time is both compute and memory intensive, where a single request can
require multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is
one of the key components of LLMs, which can account for over 50% of LLMs
memory and compute requirement. We observe that there is a high amount of
redundancy across heads on which tokens they pay attention to. Based on this
insight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a
high amount of correlation for self-attention at runtime, thus reducing both
memory and compute. In our experiments, we show that CHAI is able to reduce the
memory requirements for storing K,V cache by up to 21.4% and inference time
latency by up to 1.73x without any fine-tuning required. CHAI achieves this
with a maximum 3.2% deviation in accuracy across 3 different models (i.e.
OPT-66B, LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets.
\\ ( https://arxiv.org/abs/2403.08058 ,  2863kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08081
Date: Tue, 12 Mar 2024 21:15:38 GMT   (5721kb,D)

Title: Mechanics of Next Token Prediction with Self-Attention
Authors: Yingcong Li, Yixiao Huang, M. Emrullah Ildiz, Ankit Singh Rawat, Samet
  Oymak
Categories: cs.LG cs.AI cs.CL math.OC
Comments: Accepted to AISTATS 2024
\\
  Transformer-based language models are trained on large datasets to predict
the next token given an input sequence. Despite this simple training objective,
they have led to revolutionary advances in natural language processing.
Underlying this success is the self-attention mechanism. In this work, we ask:
$\textit{What}$ $\textit{does}$ $\textit{a}$ $\textit{single}$
$\textit{self-attention}$ $\textit{layer}$ $\textit{learn}$ $\textit{from}$
$\textit{next-token}$ $\textit{prediction?}$ We show that training
self-attention with gradient descent learns an automaton which generates the
next token in two distinct steps: $\textbf{(1)}$ $\textbf{Hard}$
$\textbf{retrieval:}$ Given input sequence, self-attention precisely selects
the $\textit{high-priority}$ $\textit{input}$ $\textit{tokens}$ associated with
the last input token. $\textbf{(2)}$ $\textbf{Soft}$ $\textbf{composition:}$ It
then creates a convex combination of the high-priority tokens from which the
next token can be sampled. Under suitable conditions, we rigorously
characterize these mechanics through a directed graph over tokens extracted
from the training data. We prove that gradient descent implicitly discovers the
strongly-connected components (SCC) of this graph and self-attention learns to
retrieve the tokens that belong to the highest-priority SCC available in the
context window. Our theory relies on decomposing the model weights into a
directional component and a finite component that correspond to hard retrieval
and soft composition steps respectively. This also formalizes a related
implicit bias formula conjectured in [Tarzanagh et al. 2023]. We hope that
these findings shed light on how self-attention processes sequential data and
pave the path toward demystifying more complex architectures.
\\ ( https://arxiv.org/abs/2403.08081 ,  5721kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08100
Date: Tue, 12 Mar 2024 22:21:48 GMT   (128kb,D)

Title: Efficient Language Model Architectures for Differentially Private
  Federated Learning
Authors: Jae Hun Ro, Srinadh Bhojanapalli, Zheng Xu, Yanxiang Zhang, Ananda
  Theertha Suresh
Categories: cs.LG cs.CR
\\
  Cross-device federated learning (FL) is a technique that trains a model on
data distributed across typically millions of edge devices without data leaving
the devices. SGD is the standard client optimizer for on device training in
cross-device FL, favored for its memory and computational efficiency. However,
in centralized training of neural language models, adaptive optimizers are
preferred as they offer improved stability and performance. In light of this,
we ask if language models can be modified such that they can be efficiently
trained with SGD client optimizers and answer this affirmatively.
  We propose a scale-invariant Coupled Input Forget Gate (SI CIFG) recurrent
network by modifying the sigmoid and tanh activations in the recurrent cell and
show that this new model converges faster and achieves better utility than the
standard CIFG recurrent model in cross-device FL in large scale experiments. We
further show that the proposed scale invariant modification also helps in
federated learning of larger transformer models. Finally, we demonstrate the
scale invariant modification is also compatible with other non-adaptive
algorithms. Particularly, our results suggest an improved privacy utility
trade-off in federated learning with differential privacy.
\\ ( https://arxiv.org/abs/2403.08100 ,  128kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08121
Date: Tue, 12 Mar 2024 23:17:32 GMT   (605kb,D)

Title: Early Directional Convergence in Deep Homogeneous Neural Networks for
  Small Initializations
Authors: Akshay Kumar and Jarvis Haupt
Categories: cs.LG math.OC stat.ML
\\
  This paper studies the gradient flow dynamics that arise when training deep
homogeneous neural networks, starting with small initializations. The present
work considers neural networks that are assumed to have locally Lipschitz
gradients and an order of homogeneity strictly greater than two. This paper
demonstrates that for sufficiently small initializations, during the early
stages of training, the weights of the neural network remain small in norm and
approximately converge in direction along the Karush-Kuhn-Tucker (KKT) points
of the neural correlation function introduced in [1]. Additionally, for square
loss and under a separability assumption on the weights of neural networks, a
similar directional convergence of gradient flow dynamics is shown near certain
saddle points of the loss function.
\\ ( https://arxiv.org/abs/2403.08121 ,  605kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08124
Date: Tue, 12 Mar 2024 23:21:09 GMT   (173kb,D)

Title: Towards Independence Criterion in Machine Unlearning of Features and
  Labels
Authors: Ling Han, Nanqing Luo, Hao Huang, Jing Chen, Mary-Anne Hartley
Categories: cs.LG cs.AI cs.CR
Comments: 10 pages, 1 figure
ACM-class: I.2.6
\\
  This work delves into the complexities of machine unlearning in the face of
distributional shifts, particularly focusing on the challenges posed by
non-uniform feature and label removal. With the advent of regulations like the
GDPR emphasizing data privacy and the right to be forgotten, machine learning
models face the daunting task of unlearning sensitive information without
compromising their integrity or performance. Our research introduces a novel
approach that leverages influence functions and principles of distributional
independence to address these challenges. By proposing a comprehensive
framework for machine unlearning, we aim to ensure privacy protection while
maintaining model performance and adaptability across varying distributions.
Our method not only facilitates efficient data removal but also dynamically
adjusts the model to preserve its generalization capabilities. Through
extensive experimentation, we demonstrate the efficacy of our approach in
scenarios characterized by significant distributional shifts, making
substantial contributions to the field of machine unlearning. This research
paves the way for developing more resilient and adaptable unlearning
techniques, ensuring models remain robust and accurate in the dynamic landscape
of data privacy and machine learning.
\\ ( https://arxiv.org/abs/2403.08124 ,  173kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08147
Date: Wed, 13 Mar 2024 00:19:06 GMT   (20023kb,D)

Title: Representing Molecules as Random Walks Over Interpretable Grammars
Authors: Michael Sun, Minghao Guo, Weize Yuan, Veronika Thost, Crystal Elaine
  Owens, Aristotle Franklin Grosz, Sharvaa Selvan, Katelyn Zhou, Hassan
  Mohiuddin, Benjamin J Pedretti, Zachary P Smith, Jie Chen and Wojciech
  Matusik
Categories: cs.LG q-bio.BM
\\
  Recent research in molecular discovery has primarily been devoted to small,
drug-like molecules, leaving many similarly important applications in material
design without adequate technology. These applications often rely on more
complex molecular structures with fewer examples that are carefully designed
using known substructures. We propose a data-efficient and interpretable model
for representing and reasoning over such molecules in terms of graph grammars
that explicitly describe the hierarchical design space featuring motifs to be
the design basis. We present a novel representation in the form of random walks
over the design space, which facilitates both molecule generation and property
prediction. We demonstrate clear advantages over existing methods in terms of
performance, efficiency, and synthesizability of predicted molecules, and we
provide detailed insights into the method's chemical interpretability.
\\ ( https://arxiv.org/abs/2403.08147 ,  20023kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08151
Date: Wed, 13 Mar 2024 00:27:19 GMT   (12895kb,D)

Title: Measuring the Energy Consumption and Efficiency of Deep Neural Networks:
  An Empirical Analysis and Design Recommendations
Authors: Charles Edison Tripp, Jordan Perr-Sauer, Jamil Gafur, Amabarish Nag,
  Avi Purkayastha, Sagi Zisman, Erik A. Bensen
Categories: cs.LG cs.AI cs.NE
Comments: 25 pages, 8 figures, for associated dataset see
  https://data.openei.org/submissions/5991
\\
  Addressing the so-called ``Red-AI'' trend of rising energy consumption by
large-scale neural networks, this study investigates the actual energy
consumption, as measured by node-level watt-meters, of training various fully
connected neural network architectures. We introduce the BUTTER-E dataset, an
augmentation to the BUTTER Empirical Deep Learning dataset, containing energy
consumption and performance data from 63,527 individual experimental runs
spanning 30,582 distinct configurations: 13 datasets, 20 sizes (number of
trainable parameters), 8 network ``shapes'', and 14 depths on both CPU and GPU
hardware collected using node-level watt-meters. This dataset reveals the
complex relationship between dataset size, network structure, and energy use,
and highlights the impact of cache effects. We propose a straightforward and
effective energy model that accounts for network size, computing, and memory
hierarchy. Our analysis also uncovers a surprising, hardware-mediated
non-linear relationship between energy efficiency and network design,
challenging the assumption that reducing the number of parameters or FLOPs is
the best way to achieve greater energy efficiency. Highlighting the need for
cache-considerate algorithm development, we suggest a combined approach to
energy efficient network, algorithm, and hardware design. This work contributes
to the fields of sustainable computing and Green AI, offering practical
guidance for creating more energy-efficient neural networks and promoting
sustainable AI.
\\ ( https://arxiv.org/abs/2403.08151 ,  12895kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08154
Date: Wed, 13 Mar 2024 00:32:30 GMT   (1955kb,D)

Title: The Effect of Different Optimization Strategies to Physics-Constrained
  Deep Learning for Soil Moisture Estimation
Authors: Jianxin Xie, Bing Yao, Zheyu Jiang
Categories: cs.LG eess.SP
\\
  Soil moisture is a key hydrological parameter that has significant importance
to human society and the environment. Accurate modeling and monitoring of soil
moisture in crop fields, especially in the root zone (top 100 cm of soil), is
essential for improving agricultural production and crop yield with the help of
precision irrigation and farming tools. Realizing the full sensor data
potential depends greatly on advanced analytical and predictive domain-aware
models. In this work, we propose a physics-constrained deep learning (P-DL)
framework to integrate physics-based principles on water transport and water
sensing signals for effective reconstruction of the soil moisture dynamics. We
adopt three different optimizers, namely Adam, RMSprop, and GD, to minimize the
loss function of P-DL during the training process. In the illustrative case
study, we demonstrate the empirical convergence of Adam optimizers outperforms
the other optimization methods in both mini-batch and full-batch training.
\\ ( https://arxiv.org/abs/2403.08154 ,  1955kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08167
Date: Wed, 13 Mar 2024 01:38:42 GMT   (2595kb,D)

Title: MolBind: Multimodal Alignment of Language, Molecules, and Proteins
Authors: Teng Xiao, Chao Cui, Huaisheng Zhu, and Vasant G. Honavar
Categories: cs.LG cs.CL q-bio.QM
\\
  Recent advancements in biology and chemistry have leveraged multi-modal
learning, integrating molecules and their natural language descriptions to
enhance drug discovery. However, current pre-training frameworks are limited to
two modalities, and designing a unified network to process different modalities
(e.g., natural language, 2D molecular graphs, 3D molecular conformations, and
3D proteins) remains challenging due to inherent gaps among them. In this work,
we propose MolBind, a framework that trains encoders for multiple modalities
through contrastive learning, mapping all modalities to a shared feature space
for multi-modal semantic alignment. To facilitate effective pre-training of
MolBind on multiple modalities, we also build and collect a high-quality
dataset with four modalities, MolBind-M4, including graph-language,
conformation-language, graph-conformation, and conformation-protein paired
data. MolBind shows superior zero-shot learning performance across a wide range
of tasks, demonstrating its strong capability of capturing the underlying
semantics of multiple modalities.
\\ ( https://arxiv.org/abs/2403.08167 ,  2595kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08193
Date: Wed, 13 Mar 2024 02:33:28 GMT   (8298kb,D)

Title: Learning-driven Physically-aware Large-scale Circuit Gate Sizing
Authors: Yuyang Ye, Peng Xu, Lizheng Ren, Tinghuan Chen, Hao Yan, Bei Yu,
  Longxing Shi
Categories: cs.LG cs.AR cs.ET
\\
  Gate sizing plays an important role in timing optimization after physical
design. Existing machine learning-based gate sizing works cannot optimize
timing on multiple timing paths simultaneously and neglect the physical
constraint on layouts. They cause sub-optimal sizing solutions and
low-efficiency issues when compared with commercial gate sizing tools. In this
work, we propose a learning-driven physically-aware gate sizing framework to
optimize timing performance on large-scale circuits efficiently. In our
gradient descent optimization-based work, for obtaining accurate gradients, a
multi-modal gate sizing-aware timing model is achieved via learning timing
information on multiple timing paths and physical information on
multiple-scaled layouts jointly. Then, gradient generation based on the
sizing-oriented estimator and adaptive back-propagation are developed to update
gate sizes. Our results demonstrate that our work achieves higher timing
performance improvements in a faster way compared with the commercial gate
sizing tool.
\\ ( https://arxiv.org/abs/2403.08193 ,  8298kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08194
Date: Wed, 13 Mar 2024 02:33:57 GMT   (12615kb,D)

Title: Unsupervised Learning of Hybrid Latent Dynamics: A Learn-to-Identify
  Framework
Authors: Yubo Ye, Sumeet Vadhavkar, Xiajun Jiang, Ryan Missel, Huafeng Liu and
  Linwei Wang
Categories: cs.LG stat.ML
Comments: Under Review
\\
  Modern applications increasingly require unsupervised learning of latent
dynamics from high-dimensional time-series. This presents a significant
challenge of identifiability: many abstract latent representations may
reconstruct observations, yet do they guarantee an adequate identification of
the governing dynamics? This paper investigates this challenge from two angles:
the use of physics inductive bias specific to the data being modeled, and a
learn-to-identify strategy that separates forecasting objectives from the data
used for the identification. We combine these two strategies in a novel
framework for unsupervised meta-learning of hybrid latent dynamics (Meta-HyLaD)
with: 1) a latent dynamic function that hybridize known mathematical
expressions of prior physics with neural functions describing its unknown
errors, and 2) a meta-learning formulation to learn to separately identify both
components of the hybrid dynamics. Through extensive experiments on five
physics and one biomedical systems, we provide strong evidence for the benefits
of Meta-HyLaD to integrate rich prior knowledge while identifying their gap to
observed data.
\\ ( https://arxiv.org/abs/2403.08194 ,  12615kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08197
Date: Wed, 13 Mar 2024 02:44:33 GMT   (2785kb,D)

Title: PAGE: Domain-Incremental Adaptation with Past-Agnostic Generative Replay
  for Smart Healthcare
Authors: Chia-Hao Li and Niraj K. Jha
Categories: cs.LG cs.AI
Comments: 30 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:2305.05738
\\
  We propose PAGE, a domain-incremental adaptation strategy with past-agnostic
generative replay for smart healthcare. PAGE enables generative replay without
the aid of any preserved data or information from prior domains. When adapting
to a new domain, it exploits real data from the new distribution and the
current model to generate synthetic data that retain the learned knowledge of
previous domains. By replaying the synthetic data with the new real data during
training, PAGE achieves a good balance between domain adaptation and knowledge
retention. In addition, we incorporate an extended inductive conformal
prediction (EICP) method into PAGE to produce a confidence score and a
credibility value for each detection result. This makes the predictions
interpretable and provides statistical guarantees for disease detection in
smart healthcare applications. We demonstrate PAGE's effectiveness in
domain-incremental disease detection with three distinct disease datasets
collected from commercially available WMSs. PAGE achieves highly competitive
performance against state-of-the-art with superior scalability, data privacy,
and feasibility. Furthermore, PAGE can enable up to 75% reduction in clinical
workload with the help of EICP.
\\ ( https://arxiv.org/abs/2403.08197 ,  2785kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08199
Date: Wed, 13 Mar 2024 02:53:52 GMT   (48599kb,D)

Title: Deep Submodular Peripteral Network
Authors: Gantavya Bhatt, Arnav Das, Jeff Bilmes
Categories: cs.LG cs.AI
Comments: Preprint
\\
  Submodular functions, crucial for various applications, often lack practical
learning methods for their acquisition. Seemingly unrelated, learning a scaling
from oracles offering graded pairwise preferences (GPC) is underexplored,
despite a rich history in psychometrics. In this paper, we introduce deep
submodular peripteral networks (DSPNs), a novel parametric family of submodular
functions, and methods for their training using a contrastive-learning inspired
GPC-ready strategy to connect and then tackle both of the above challenges. We
introduce newly devised GPC-style "peripteral" loss which leverages numerically
graded relationships between pairs of objects (sets in our case). Unlike
traditional contrastive learning, our method utilizes graded comparisons,
extracting more nuanced information than just binary-outcome comparisons, and
contrasts sets of any size (not just two). We also define a novel suite of
automatic sampling strategies for training, including active-learning inspired
submodular feedback. We demonstrate DSPNs' efficacy in learning submodularity
from a costly target submodular function showing superiority in downstream
tasks such as experimental design and streaming applications.
\\ ( https://arxiv.org/abs/2403.08199 ,  48599kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08204
Date: Wed, 13 Mar 2024 02:56:31 GMT   (10184kb,D)

Title: AutoDFP: Automatic Data-Free Pruning via Channel Similarity
  Reconstruction
Authors: Siqi Li, Jun Chen, Jingyang Xiang, Chengrui Zhu, Yong Liu
Categories: cs.LG cs.CV
Comments: 11 pages, 16 figures
\\
  Structured pruning methods are developed to bridge the gap between the
massive scale of neural networks and the limited hardware resources. Most
current structured pruning methods rely on training datasets to fine-tune the
compressed model, resulting in high computational burdens and being
inapplicable for scenarios with stringent requirements on privacy and security.
As an alternative, some data-free methods have been proposed, however, these
methods often require handcraft parameter tuning and can only achieve
inflexible reconstruction. In this paper, we propose the Automatic Data-Free
Pruning (AutoDFP) method that achieves automatic pruning and reconstruction
without fine-tuning. Our approach is based on the assumption that the loss of
information can be partially compensated by retaining focused information from
similar channels. Specifically, We formulate data-free pruning as an
optimization problem, which can be effectively addressed through reinforcement
learning. AutoDFP assesses the similarity of channels for each layer and
provides this information to the reinforcement learning agent, guiding the
pruning and reconstruction process of the network. We evaluate AutoDFP with
multiple networks on multiple datasets, achieving impressive compression
results. For instance, on the CIFAR-10 dataset, AutoDFP demonstrates a 2.87\%
reduction in accuracy loss compared to the recently proposed data-free pruning
method DFPC with fewer FLOPs on VGG-16. Furthermore, on the ImageNet dataset,
AutoDFP achieves 43.17\% higher accuracy than the SOTA method with the same
80\% preserved ratio on MobileNet-V1.
\\ ( https://arxiv.org/abs/2403.08204 ,  10184kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08207
Date: Wed, 13 Mar 2024 03:03:40 GMT   (14419kb,D)

Title: BG-HGNN: Toward Scalable and Efficient Heterogeneous Graph Neural
  Network
Authors: Junwei Su, Lingjun Mao, Chuan Wu
Categories: cs.LG
\\
  Many computer vision and machine learning problems are modelled as learning
tasks on heterogeneous graphs, featuring a wide array of relations from diverse
types of nodes and edges. Heterogeneous graph neural networks (HGNNs) stand out
as a promising neural model class designed for heterogeneous graphs. Built on
traditional GNNs, existing HGNNs employ different parameter spaces to model the
varied relationships. However, the practical effectiveness of existing HGNNs is
often limited to simple heterogeneous graphs with few relation types. This
paper first highlights and demonstrates that the standard approach employed by
existing HGNNs inevitably leads to parameter explosion and relation collapse,
making HGNNs less effective or impractical for complex heterogeneous graphs
with numerous relation types. To overcome this issue, we introduce a novel
framework, Blend&Grind-HGNN (BG-HGNN), which effectively tackles the challenges
by carefully integrating different relations into a unified feature space
manageable by a single set of parameters. This results in a refined HGNN method
that is more efficient and effective in learning from heterogeneous graphs,
especially when the number of relations grows. Our empirical studies illustrate
that BG-HGNN significantly surpasses existing HGNNs in terms of parameter
efficiency (up to 28.96 $\times$), training throughput (up to 8.12 $\times$),
and accuracy (up to 1.07 $\times$).
\\ ( https://arxiv.org/abs/2403.08207 ,  14419kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08216
Date: Wed, 13 Mar 2024 03:28:39 GMT   (7911kb,D)

Title: PaddingFlow: Improving Normalizing Flows with Padding-Dimensional Noise
Authors: Qinglong Meng, Chongkun Xia, Xueqian Wang
Categories: cs.LG cs.CV
\\
  Normalizing flow is a generative modeling approach with efficient sampling.
However, Flow-based models suffer two issues, which are manifold and discrete
data. If the target distribution is a manifold, which means the dimension of
the latent target distribution and the dimension of the data distribution are
unmatched, flow-based models might perform badly. Discrete data makes
flow-based models collapse into a degenerate mixture of point masses. In this
paper, to sidestep such two issues we propose PaddingFlow, a novel
dequantization method, which improves normalizing flows with
padding-dimensional noise. PaddingFlow is easy to implement, computationally
cheap, widely suitable for various tasks, and generates samples that are
unbiased estimations of the data. Especially, our method can overcome the
limitation of existing dequantization methods that have to change the data
distribution, which might degrade performance. We validate our method on the
main benchmarks of unconditional density estimation, including five tabular
datasets and four image datasets for VAE models, and the IK experiments which
are conditional density estimation. The results show that PaddingFlow can
provide improvement on all tasks in this paper.
\\ ( https://arxiv.org/abs/2403.08216 ,  7911kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08222
Date: Wed, 13 Mar 2024 03:47:08 GMT   (117kb,D)

Title: Robust Decision Aggregation with Adversarial Experts
Authors: Yongkang Guo, Yuqing Kong
Categories: cs.LG cs.AI
\\
  We consider a binary decision aggregation problem in the presence of both
truthful and adversarial experts. The truthful experts will report their
private signals truthfully with proper incentive, while the adversarial experts
can report arbitrarily. The decision maker needs to design a robust aggregator
to forecast the true state of the world based on the reports of experts. The
decision maker does not know the specific information structure, which is a
joint distribution of signals, states, and strategies of adversarial experts.
We want to find the optimal aggregator minimizing regret under the worst
information structure. The regret is defined by the difference in expected loss
between the aggregator and a benchmark who makes the optimal decision given the
joint distribution and reports of truthful experts.
  We prove that when the truthful experts are symmetric and adversarial experts
are not too numerous, the truncated mean is optimal, which means that we remove
some lowest reports and highest reports and take averaging among the left
reports. Moreover, for many settings, the optimal aggregators are in the family
of piecewise linear functions. The regret is independent of the total number of
experts but only depends on the ratio of adversaries. We evaluate our
aggregators by numerical experiment in an ensemble learning task. We also
obtain some negative results for the aggregation problem with adversarial
experts under some more general information structures and experts' report
space.
\\ ( https://arxiv.org/abs/2403.08222 ,  117kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08245
Date: Wed, 13 Mar 2024 05:00:23 GMT   (2016kb,D)

Title: Scattered Mixture-of-Experts Implementation
Authors: Shawn Tan, Yikang Shen, Rameswar Panda, Aaron Courville
Categories: cs.LG cs.DC
\\
  We present ScatterMoE, an implementation of Sparse Mixture-of-Experts (SMoE)
on GPUs. ScatterMoE builds upon existing implementations, and overcoming some
of the limitations to improve inference and training speed, and memory
footprint. This implementation achieves this by avoiding padding and making
excessive copies of the input.
  We introduce ParallelLinear, the main component we use to build our
implementation and the various kernels used to speed up the operation. We
benchmark our implementation against Megablocks, and show that it enables a
higher throughput and lower memory footprint. We also show how ParallelLinear
enables extension of the Mixture-of-Experts concept by demonstrating with an
implementation of Mixture of Attention.
\\ ( https://arxiv.org/abs/2403.08245 ,  2016kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08254
Date: Wed, 13 Mar 2024 05:11:24 GMT   (864kb,D)

Title: Machine Unlearning: Taxonomy, Metrics, Applications, Challenges, and
  Prospects
Authors: Na Li, Chunyi Zhou, Yansong Gao, Hui Chen, Anmin Fu, Zhi Zhang, and Yu
  Shui
Categories: cs.LG cs.CR
\\
  Personal digital data is a critical asset, and governments worldwide have
enforced laws and regulations to protect data privacy. Data users have been
endowed with the right to be forgotten of their data. In the course of machine
learning (ML), the forgotten right requires a model provider to delete user
data and its subsequent impact on ML models upon user requests. Machine
unlearning emerges to address this, which has garnered ever-increasing
attention from both industry and academia. While the area has developed
rapidly, there is a lack of comprehensive surveys to capture the latest
advancements. Recognizing this shortage, we conduct an extensive exploration to
map the landscape of machine unlearning including the (fine-grained) taxonomy
of unlearning algorithms under centralized and distributed settings, debate on
approximate unlearning, verification and evaluation metrics, challenges and
solutions for unlearning under different applications, as well as attacks
targeting machine unlearning. The survey concludes by outlining potential
directions for future research, hoping to serve as a guide for interested
scholars.
\\ ( https://arxiv.org/abs/2403.08254 ,  864kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08265
Date: Wed, 13 Mar 2024 05:32:13 GMT   (1899kb,D)

Title: Random Search as a Baseline for Sparse Neural Network Architecture
  Search
Authors: Rezsa Farahani
Categories: cs.LG cs.AI cs.NE
ACM-class: I.2.8; I.5.1
\\
  Sparse neural networks have shown similar or better generalization
performance than their dense counterparts while having higher parameter
efficiency. This has motivated a number of works to learn, induce, or search
for high performing sparse networks. While reports of quality or efficiency
gains are impressive, standard baselines are lacking, therefore hindering
having reliable comparability and reproducibility across methods. In this work,
we provide an evaluation approach and a naive Random Search baseline method for
finding good sparse configurations. We apply Random Search on the node space of
an overparameterized network with the goal of finding better initialized sparse
sub-networks that are positioned more advantageously in the loss landscape. We
record sparse network post-training performances at various levels of sparsity
and compare against both their fully connected parent networks and random
sparse configurations at the same sparsity levels. We observe that for this
architecture search task, initialized sparse networks found by Random Search
neither perform better nor converge more efficiently than their random
counterparts. Thus we conclude that Random Search may be viewed as a suitable
neutral baseline for sparsity search methods.
\\ ( https://arxiv.org/abs/2403.08265 ,  1899kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08291
Date: Wed, 13 Mar 2024 06:54:15 GMT   (1679kb,D)

Title: CleanAgent: Automating Data Standardization with LLM-based Agents
Authors: Danrui Qi, Jiannan Wang
Categories: cs.LG cs.AI cs.MA
\\
  Data standardization is a crucial part in data science life cycle. While
tools like Pandas offer robust functionalities, their complexity and the manual
effort required for customizing code to diverse column types pose significant
challenges. Although large language models (LLMs) like ChatGPT have shown
promise in automating this process through natural language understanding and
code generation, it still demands expert-level programming knowledge and
continuous interaction for prompt refinement. To solve these challenges, our
key idea is to propose a Python library with declarative, unified APIs for
standardizing column types, simplifying the code generation of LLM with concise
API calls. We first propose Dataprep.Clean which is written as a component of
the Dataprep Library, offers a significant reduction in complexity by enabling
the standardization of specific column types with a single line of code. Then
we introduce the CleanAgent framework integrating Dataprep.Clean and LLM-based
agents to automate the data standardization process. With CleanAgent, data
scientists need only provide their requirements once, allowing for a
hands-free, automatic standardization process.
\\ ( https://arxiv.org/abs/2403.08291 ,  1679kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08309
Date: Wed, 13 Mar 2024 07:38:20 GMT   (3527kb,D)

Title: HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain
  Reinforcement Learning From AI Feedback
Authors: Ang Li, Qiugen Xiao, Peng Cao, Jian Tang, Yi Yuan, Zijie Zhao,
  Xiaoyuan Chen, Liang Zhang, Xiangyang Li, Kaitong Yang, Weidong Guo, Yukang
  Gan, Daniell Wang, Ying Shan
Categories: cs.LG cs.AI
Comments: 18 pages, 7 figures
\\
  Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter
annotation cycles and lower costs over Reinforcement Learning from Human
Feedback (RLHF), making it highly efficient during the rapid strategy iteration
periods of large language model (LLM) training. Using ChatGPT as a labeler to
provide feedback on open-domain prompts in RLAIF training, we observe an
increase in human evaluators' preference win ratio for model responses, but a
decrease in evaluators' satisfaction rate. Analysis suggests that the decrease
in satisfaction rate is mainly due to some responses becoming less helpful,
particularly in terms of correctness and truthfulness, highlighting practical
limitations of basic RLAIF. In this paper, we propose Hybrid Reinforcement
Learning from AI Feedback (HRLAIF). This method enhances the accuracy of AI
annotations for responses, making the model's helpfulness more robust in
training process. Additionally, it employs AI for Red Teaming, further
improving the model's harmlessness. Human evaluation results show that HRLAIF
inherits the ability of RLAIF to enhance human preference for outcomes at a low
cost while also improving the satisfaction rate of responses. Compared to the
policy model before Reinforcement Learning (RL), it achieves an increase of
2.08\% in satisfaction rate, effectively addressing the issue of a decrease of
4.58\% in satisfaction rate after basic RLAIF.
\\ ( https://arxiv.org/abs/2403.08309 ,  3527kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08331
Date: Wed, 13 Mar 2024 08:34:40 GMT   (1266kb)

Title: Bayesian Optimization that Limits Search Region to Lower Dimensions
  Utilizing Local GPR
Authors: Yasunori Taguchi and Hiro Gangi
Categories: cs.LG stat.ML
Comments: 8 pages, 13 figures, 22nd International Conference on Machine
  Learning and Applications (ICMLA2023)
Journal-ref: 22nd International Conference on Machine Learning and Applications
  (ICMLA2023)
\\
  Optimization of product and system characteristics is required in many
fields, including design and control. Bayesian optimization (BO) is often used
when there are high observing costs, because BO theoretically guarantees an
upper bound on regret. However, computational costs increase exponentially with
the number of parameters to be optimized, decreasing search efficiency. We
propose a BO that limits the search region to lower dimensions and utilizes
local Gaussian process regression (LGPR) to scale the BO to higher dimensions.
LGPR treats the low-dimensional search region as "local," improving prediction
accuracies there. The LGPR model is trained on a local subset of data specific
to that region. This improves prediction accuracy and search efficiency and
reduces the time complexity of matrix inversion in the Gaussian process
regression. In evaluations with 20D Ackley and Rosenbrock functions, search
efficiencies are equal to or higher than those of the compared methods,
improved by about 69% and 40% from the case without LGPR. We apply our method
to an automatic design task for a power semiconductor device. We successfully
reduce the specific on-resistance to 25% better than a conventional method and
3.4% better than without LGPR.
\\ ( https://arxiv.org/abs/2403.08331 ,  1266kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08333
Date: Wed, 13 Mar 2024 08:37:31 GMT   (1598kb,D)

Title: Fast Inference of Removal-Based Node Influence
Authors: Weikai Li, Zhiping Xiao, Xiao Luo, Yizhou Sun
Categories: cs.LG cs.AI
Comments: To be published in the Web Conference 2024
\\
  Graph neural networks (GNNs) are widely utilized to capture the information
spreading patterns in graphs. While remarkable performance has been achieved,
there is a new trending topic of evaluating node influence. We propose a new
method of evaluating node influence, which measures the prediction change of a
trained GNN model caused by removing a node. A real-world application is, "In
the task of predicting Twitter accounts' polarity, had a particular account
been removed, how would others' polarity change?". We use the GNN as a
surrogate model whose prediction could simulate the change of nodes or edges
caused by node removal. To obtain the influence for every node, a
straightforward way is to alternately remove every node and apply the trained
GNN on the modified graph. It is reliable but time-consuming, so we need an
efficient method. The related lines of work, such as graph adversarial attack
and counterfactual explanation, cannot directly satisfy our needs, since they
do not focus on the global influence score for every node. We propose an
efficient and intuitive method, NOde-Removal-based fAst GNN inference (NORA),
which uses the gradient to approximate the node-removal influence. It only
costs one forward propagation and one backpropagation to approximate the
influence score for all nodes. Extensive experiments on six datasets and six
GNN models verify the effectiveness of NORA. Our code is available at
https://github.com/weikai-li/NORA.git.
\\ ( https://arxiv.org/abs/2403.08333 ,  1598kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08335
Date: Wed, 13 Mar 2024 08:40:49 GMT   (2508kb,D)

Title: A Sparsity Principle for Partially Observable Causal Representation
  Learning
Authors: Danru Xu, Dingling Yao, S\'ebastien Lachapelle, Perouz Taslakian,
  Julius von K\"ugelgen, Francesco Locatello, Sara Magliacane
Categories: cs.LG cs.AI stat.ML
Comments: 33 pages, 18 figures, 9 tables
\\
  Causal representation learning aims at identifying high-level causal
variables from perceptual data. Most methods assume that all latent causal
variables are captured in the high-dimensional observations. We instead
consider a partially observed setting, in which each measurement only provides
information about a subset of the underlying causal state. Prior work has
studied this setting with multiple domains or views, each depending on a fixed
subset of latents. Here, we focus on learning from unpaired observations from a
dataset with an instance-dependent partial observability pattern. Our main
contribution is to establish two identifiability results for this setting: one
for linear mixing functions without parametric assumptions on the underlying
causal model, and one for piecewise linear mixing functions with Gaussian
latent causal variables. Based on these insights, we propose two methods for
estimating the underlying causal variables by enforcing sparsity in the
inferred representation. Experiments on different simulated datasets and
established benchmarks highlight the effectiveness of our approach in
recovering the ground-truth latents.
\\ ( https://arxiv.org/abs/2403.08335 ,  2508kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08352
Date: Wed, 13 Mar 2024 09:00:38 GMT   (1976kb)

Title: Data augmentation with automated machine learning: approaches and
  performance comparison with classical data augmentation methods
Authors: Alhassan Mumuni and Fuseini Mumuni
Categories: cs.LG cs.AI cs.CV cs.NE
\\
  Data augmentation is arguably the most important regularization technique
commonly used to improve generalization performance of machine learning models.
It primarily involves the application of appropriate data transformation
operations to create new data samples with desired properties. Despite its
effectiveness, the process is often challenging because of the time-consuming
trial and error procedures for creating and testing different candidate
augmentations and their hyperparameters manually. Automated data augmentation
methods aim to automate the process. State-of-the-art approaches typically rely
on automated machine learning (AutoML) principles. This work presents a
comprehensive survey of AutoML-based data augmentation techniques. We discuss
various approaches for accomplishing data augmentation with AutoML, including
data manipulation, data integration and data synthesis techniques. We present
extensive discussion of techniques for realizing each of the major subtasks of
the data augmentation process: search space design, hyperparameter optimization
and model evaluation. Finally, we carried out an extensive comparison and
analysis of the performance of automated data augmentation techniques and
state-of-the-art methods based on classical augmentation approaches. The
results show that AutoML methods for data augmentation currently outperform
state-of-the-art techniques based on conventional approaches.
\\ ( https://arxiv.org/abs/2403.08352 ,  1976kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08364
Date: Wed, 13 Mar 2024 09:24:59 GMT   (798kb)

Title: Decoupled Federated Learning on Long-Tailed and Non-IID data with
  Feature Statistics
Authors: Zhuoxin Chen, Zhenyu Wu, Yang Ji
Categories: cs.LG cs.AI
\\
  Federated learning is designed to enhance data security and privacy, but
faces challenges when dealing with heterogeneous data in long-tailed and
non-IID distributions. This paper explores an overlooked scenario where tail
classes are sparsely distributed over a few clients, causing the models trained
with these classes to have a lower probability of being selected during client
aggregation, leading to slower convergence rates and poorer model performance.
To address this issue, we propose a two-stage Decoupled Federated learning
framework using Feature Statistics (DFL-FS). In the first stage, the server
estimates the client's class coverage distributions through masked local
feature statistics clustering to select models for aggregation to accelerate
convergence and enhance feature learning without privacy leakage. In the second
stage, DFL-FS employs federated feature regeneration based on global feature
statistics and utilizes resampling and weighted covariance to calibrate the
global classifier to enhance the model's adaptability to long-tailed data
distributions. We conducted experiments on CIFAR10-LT and CIFAR100-LT datasets
with various long-tailed rates. The results demonstrate that our method
outperforms state-of-the-art methods in both accuracy and convergence rate.
\\ ( https://arxiv.org/abs/2403.08364 ,  798kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08376
Date: Wed, 13 Mar 2024 09:39:15 GMT   (2625kb,D)

Title: Nonlinear Manifold Learning Determines Microgel Size from Raman
  Spectroscopy
Authors: Eleni D. Koronaki, Luise F. Kaven, Johannes M. M. Faust, Ioannis G.
  Kevrekidis, and Alexander Mitsos
Categories: cs.LG
Comments: 51 pages, 12 figures, 4 tables
\\
  Polymer particle size constitutes a crucial characteristic of product quality
in polymerization. Raman spectroscopy is an established and reliable process
analytical technology for in-line concentration monitoring. Recent approaches
and some theoretical considerations show a correlation between Raman signals
and particle sizes but do not determine polymer size from Raman spectroscopic
measurements accurately and reliably. With this in mind, we propose three
alternative machine learning workflows to perform this task, all involving
diffusion maps, a nonlinear manifold learning technique for dimensionality
reduction: (i) directly from diffusion maps, (ii) alternating diffusion maps,
and (iii) conformal autoencoder neural networks. We apply the workflows to a
data set of Raman spectra with associated size measured via dynamic light
scattering of 47 microgel (cross-linked polymer) samples in a diameter range of
208nm to 483 nm. The conformal autoencoders substantially outperform
state-of-the-art methods and results for the first time in a promising
prediction of polymer size from Raman spectra.
\\ ( https://arxiv.org/abs/2403.08376 ,  2625kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08403
Date: Wed, 13 Mar 2024 10:37:52 GMT   (5688kb,D)

Title: FSDR: A Novel Deep Learning-based Feature Selection Algorithm for Pseudo
  Time-Series Data using Discrete Relaxation
Authors: Mohammad Rahman, Manzur Murshed, Shyh Wei Teng, Manoranjan Paul
Categories: cs.LG
\\
  Conventional feature selection algorithms applied to Pseudo Time-Series (PTS)
data, which consists of observations arranged in sequential order without
adhering to a conventional temporal dimension, often exhibit impractical
computational complexities with high dimensional data. To address this
challenge, we introduce a Deep Learning (DL)-based feature selection algorithm:
Feature Selection through Discrete Relaxation (FSDR), tailored for PTS data.
Unlike the existing feature selection algorithms, FSDR learns the important
features as model parameters using discrete relaxation, which refers to the
process of approximating a discrete optimisation problem with a continuous one.
FSDR is capable of accommodating a high number of feature dimensions, a
capability beyond the reach of existing DL-based or traditional methods.
Through testing on a hyperspectral dataset (i.e., a type of PTS data), our
experimental results demonstrate that FSDR outperforms three commonly used
feature selection algorithms, taking into account a balance among execution
time, $R^2$, and $RMSE$.
\\ ( https://arxiv.org/abs/2403.08403 ,  5688kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08408
Date: Wed, 13 Mar 2024 10:51:38 GMT   (1434kb,D)

Title: Reduced Jeffries-Matusita distance: A Novel Loss Function to Improve
  Generalization Performance of Deep Classification Models
Authors: Mohammad Lashkari, Amin Gheibi
Categories: cs.LG
MSC-class: 68Q32, 68T05, 68T45, 68R10
\\
  The generalization performance of deep neural networks in classification
tasks is a major concern in machine learning research. Despite widespread
techniques used to diminish the over-fitting issue such as data augmentation,
pseudo-labeling, regularization, and ensemble learning, this performance still
needs to be enhanced with other approaches. In recent years, it has been
theoretically demonstrated that the loss function characteristics i.e. its
Lipschitzness and maximum value affect the generalization performance of deep
neural networks which can be utilized as a guidance to propose novel distance
measures. In this paper, by analyzing the aforementioned characteristics, we
introduce a distance called Reduced Jeffries-Matusita as a loss function for
training deep classification models to reduce the over-fitting issue. In our
experiments, we evaluate the new loss function in two different problems: image
classification in computer vision and node classification in the context of
graph learning. The results show that the new distance measure stabilizes the
training process significantly, enhances the generalization ability, and
improves the performance of the models in the Accuracy and F1-score metrics,
even if the training set size is small.
\\ ( https://arxiv.org/abs/2403.08408 ,  1434kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08414
Date: Wed, 13 Mar 2024 10:58:55 GMT   (4256kb,D)

Title: Causal Graph Neural Networks for Wildfire Danger Prediction
Authors: Shan Zhao, Ioannis Prapas, Ilektra Karasante, Zhitong Xiong, Ioannis
  Papoutsis, Gustau Camps-Valls, Xiao Xiang Zhu
Categories: cs.LG cs.AI
Comments: Accepted by ICLR 2024 Machine Learning for Remote Sensing (ML4RS)
  Workshop
\\
  Wildfire forecasting is notoriously hard due to the complex interplay of
different factors such as weather conditions, vegetation types and human
activities. Deep learning models show promise in dealing with this complexity
by learning directly from data. However, to inform critical decision making, we
argue that we need models that are right for the right reasons; that is, the
implicit rules learned should be grounded by the underlying processes driving
wildfires. In that direction, we propose integrating causality with Graph
Neural Networks (GNNs) that explicitly model the causal mechanism among complex
variables via graph learning. The causal adjacency matrix considers the
synergistic effect among variables and removes the spurious links from highly
correlated impacts. Our methodology's effectiveness is demonstrated through
superior performance forecasting wildfire patterns in the European boreal and
mediterranean biome. The gain is especially prominent in a highly imbalanced
dataset, showcasing an enhanced robustness of the model to adapt to regime
shifts in functional relationships. Furthermore, SHAP values from our trained
model further enhance our understanding of the model's inner workings.
\\ ( https://arxiv.org/abs/2403.08414 ,  4256kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08428
Date: Wed, 13 Mar 2024 11:26:43 GMT   (194kb,D)

Title: DeepCSHAP: Utilizing Shapley Values to Explain Deep Complex-Valued
  Neural Networks
Authors: Florian Eilers and Xiaoyi Jiang
Categories: cs.LG
Comments: 14 Pages plus 4 Pages Appendix
\\
  Deep Neural Networks are widely used in academy as well as corporate and
public applications, including safety critical applications such as health care
and autonomous driving. The ability to explain their output is critical for
safety reasons as well as acceptance among applicants. A multitude of methods
have been proposed to explain real-valued neural networks. Recently,
complex-valued neural networks have emerged as a new class of neural networks
dealing with complex-valued input data without the necessity of projecting them
onto $\mathbb{R}^2$. This brings up the need to develop explanation algorithms
for this kind of neural networks. In this paper we provide these developments.
While we focus on adapting the widely used DeepSHAP algorithm to the complex
domain, we also present versions of four gradient based explanation methods
suitable for use in complex-valued neural networks. We evaluate the explanation
quality of all presented algorithms and provide all of them as an open source
library adaptable to most recent complex-valued neural network architectures.
\\ ( https://arxiv.org/abs/2403.08428 ,  194kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08438
Date: Wed, 13 Mar 2024 11:44:30 GMT   (2020kb,D)

Title: Reproducibility and Geometric Intrinsic Dimensionality: An Investigation
  on Graph Neural Network Research
Authors: Tobias Hille and Maximilian Stubbemann and Tom Hanika
Categories: cs.LG cs.AI
Comments: 39 pages, 9 figures
MSC-class: 68T01 68T07 68T09 51F99
ACM-class: I.2.6
\\
  Difficulties in replication and reproducibility of empirical evidences in
machine learning research have become a prominent topic in recent years.
Ensuring that machine learning research results are sound and reliable requires
reproducibility, which verifies the reliability of research findings using the
same code and data. This promotes open and accessible research, robust
experimental workflows, and the rapid integration of new findings. Evaluating
the degree to which research publications support these different aspects of
reproducibility is one goal of the present work. For this we introduce an
ontology of reproducibility in machine learning and apply it to methods for
graph neural networks. Building on these efforts we turn towards another
critical challenge in machine learning, namely the curse of dimensionality,
which poses challenges in data collection, representation, and analysis, making
it harder to find representative data and impeding the training and inference
processes. Using the closely linked concept of geometric intrinsic dimension we
investigate to which extend the used machine learning models are influenced by
the intrinsic dimension of the data sets they are trained on.
\\ ( https://arxiv.org/abs/2403.08438 ,  2020kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08448
Date: Wed, 13 Mar 2024 12:03:27 GMT   (975kb,D)

Title: Actor-Critic Physics-informed Neural Lyapunov Control
Authors: Jiarui Wang and Mahyar Fazlyab
Categories: cs.LG cs.RO cs.SY eess.SY
\\
  Designing control policies for stabilization tasks with provable guarantees
is a long-standing problem in nonlinear control. A crucial performance metric
is the size of the resulting region of attraction, which essentially serves as
a robustness "margin" of the closed-loop system against uncertainties. In this
paper, we propose a new method to train a stabilizing neural network controller
along with its corresponding Lyapunov certificate, aiming to maximize the
resulting region of attraction while respecting the actuation constraints.
Crucial to our approach is the use of Zubov's Partial Differential Equation
(PDE), which precisely characterizes the true region of attraction of a given
control policy. Our framework follows an actor-critic pattern where we
alternate between improving the control policy (actor) and learning a Zubov
function (critic). Finally, we compute the largest certifiable region of
attraction by invoking an SMT solver after the training procedure. Our
numerical experiments on several design problems show consistent and
significant improvements in the size of the resulting region of attraction.
\\ ( https://arxiv.org/abs/2403.08448 ,  975kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08469
Date: Wed, 13 Mar 2024 12:31:08 GMT   (7862kb,D)

Title: An Analysis of Human Alignment of Latent Diffusion Models
Authors: Lorenz Linhardt and Marco Morik and Sidney Bender and Naima Elosegui
  Borras
Categories: cs.LG
Comments: Accepted at the ICLR 2024 Workshop on Representational Alignment
\\
  Diffusion models, trained on large amounts of data, showed remarkable
performance for image synthesis. They have high error consistency with humans
and low texture bias when used for classification. Furthermore, prior work
demonstrated the decomposability of their bottleneck layer representations into
semantic directions. In this work, we analyze how well such representations are
aligned to human responses on a triplet odd-one-out task. We find that despite
the aforementioned observations: I) The representational alignment with humans
is comparable to that of models trained only on ImageNet-1k. II) The most
aligned layers of the denoiser U-Net are intermediate layers and not the
bottleneck. III) Text conditioning greatly improves alignment at high noise
levels, hinting at the importance of abstract textual information, especially
in the early stage of generation.
\\ ( https://arxiv.org/abs/2403.08469 ,  7862kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08481
Date: Wed, 13 Mar 2024 12:46:51 GMT   (278kb,D)

Title: SoK: Reducing the Vulnerability of Fine-tuned Language Models to
  Membership Inference Attacks
Authors: Guy Amit, Abigail Goldsteen, Ariel Farkash
Categories: cs.LG
Comments: preliminary version
\\
  Natural language processing models have experienced a significant upsurge in
recent years, with numerous applications being built upon them. Many of these
applications require fine-tuning generic base models on customized, proprietary
datasets. This fine-tuning data is especially likely to contain personal or
sensitive information about individuals, resulting in increased privacy risk.
Membership inference attacks are the most commonly employed attack to assess
the privacy leakage of a machine learning model. However, limited research is
available on the factors that affect the vulnerability of language models to
this kind of attack, or on the applicability of different defense strategies in
the language domain. We provide the first systematic review of the
vulnerability of fine-tuned large language models to membership inference
attacks, the various factors that come into play, and the effectiveness of
different defense strategies. We find that some training methods provide
significantly reduced privacy risk, with the combination of differential
privacy and low-rank adaptors achieving the best privacy protection against
these attacks.
\\ ( https://arxiv.org/abs/2403.08481 ,  278kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08506
Date: Mon, 11 Mar 2024 15:58:15 GMT   (4634kb,D)

Title: DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain
  Generalization in Federated Learning
Authors: Sikai Bai, Jie Zhang, Shuaicheng Li, Song Guo, Jingcai Guo, Jun Hou,
  Tao Han, and Xiaocheng Lu
Categories: cs.LG cs.AI cs.CV
Journal-ref: The IEEE/CVF Conference on Computer Vision and Pattern Recognition
  2024
\\
  Federated learning (FL) has emerged as a powerful paradigm for learning from
decentralized data, and federated domain generalization further considers the
test dataset (target domain) is absent from the decentralized training data
(source domains). However, most existing FL methods assume that domain labels
are provided during training, and their evaluation imposes explicit constraints
on the number of domains, which must strictly match the number of clients.
Because of the underutilization of numerous edge devices and additional
cross-client domain annotations in the real world, such restrictions may be
impractical and involve potential privacy leaks. In this paper, we propose an
efficient and novel approach, called Disentangled Prompt Tuning (DiPrompT), a
method that tackles the above restrictions by learning adaptive prompts for
domain generalization in a distributed manner. Specifically, we first design
two types of prompts, i.e., global prompt to capture general knowledge across
all clients and domain prompts to capture domain-specific knowledge. They
eliminate the restriction on the one-to-one mapping between source domains and
local clients. Furthermore, a dynamic query metric is introduced to
automatically search the suitable domain label for each sample, which includes
two-substep text-image alignments based on prompt tuning without
labor-intensive annotation. Extensive experiments on multiple datasets
demonstrate that our DiPrompT achieves superior domain generalization
performance over state-of-the-art FL methods when domain labels are not
provided, and even outperforms many centralized learning methods using domain
labels.
\\ ( https://arxiv.org/abs/2403.08506 ,  4634kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08550
Date: Wed, 13 Mar 2024 14:02:42 GMT   (6138kb,D)

Title: CINA: Conditional Implicit Neural Atlas for Spatio-Temporal
  Representation of Fetal Brains
Authors: Maik Dannecker, Vanessa Kyriakopoulou, Lucilio Cordero-Grande, Anthony
  N. Price, Joseph V. Hajnal, Daniel Rueckert
Categories: cs.LG cs.CV
Comments: Submitted to MICCAI 2024
\\
  We introduce a conditional implicit neural atlas (CINA) for spatio-temporal
atlas generation from Magnetic Resonance Images (MRI) of the neurotypical and
pathological fetal brain, that is fully independent of affine or non-rigid
registration. During training, CINA learns a general representation of the
fetal brain and encodes subject specific information into latent code. After
training, CINA can construct a faithful atlas with tissue probability maps of
the fetal brain for any gestational age (GA) and anatomical variation covered
within the training domain. Thus, CINA is competent to represent both,
neurotypical and pathological brains. Furthermore, a trained CINA model can be
fit to brain MRI of unseen subjects via test-time optimization of the latent
code. CINA can then produce probabilistic tissue maps tailored to a particular
subject. We evaluate our method on a total of 198 T2 weighted MRI of normal and
abnormal fetal brains from the dHCP and FeTA datasets. We demonstrate CINA's
capability to represent a fetal brain atlas that can be flexibly conditioned on
GA and on anatomical variations like ventricular volume or degree of cortical
folding, making it a suitable tool for modeling both neurotypical and
pathological brains. We quantify the fidelity of our atlas by means of tissue
segmentation and age prediction and compare it to an established baseline. CINA
demonstrates superior accuracy for neurotypical brains and pathological brains
with ventriculomegaly. Moreover, CINA scores a mean absolute error of 0.23
weeks in fetal brain age prediction, further confirming an accurate
representation of fetal brain development.
\\ ( https://arxiv.org/abs/2403.08550 ,  6138kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08554
Date: Wed, 13 Mar 2024 14:06:51 GMT   (704kb)

Title: Federated Knowledge Graph Unlearning via Diffusion Model
Authors: Bingchen Liu and Yuanyuan Fang
Categories: cs.LG cs.AI
\\
  Federated learning (FL) promotes the development and application of
artificial intelligence technologies by enabling model sharing and
collaboration while safeguarding data privacy. Knowledge graph (KG) embedding
representation provides a foundation for knowledge reasoning and applications
by mapping entities and relations into vector space. Federated KG embedding
enables the utilization of knowledge from diverse client sources while
safeguarding the privacy of local data. However, due to demands such as privacy
protection and the need to adapt to dynamic data changes, investigations into
machine unlearning (MU) have been sparked. However, it is challenging to
maintain the performance of KG embedding models while forgetting the influence
of specific forgotten data on the model. In this paper, we propose FedDM, a
novel framework tailored for machine unlearning in federated knowledge graphs.
Leveraging diffusion models, we generate noisy data to sensibly mitigate the
influence of specific knowledge on FL models while preserving the overall
performance concerning the remaining data. We conduct experimental evaluations
on benchmark datasets to assess the efficacy of the proposed model. Extensive
experiments demonstrate that FedDM yields promising results in knowledge
forgetting.
\\ ( https://arxiv.org/abs/2403.08554 ,  704kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08562
Date: Wed, 13 Mar 2024 14:14:47 GMT   (108kb,D)

Title: Structural perspective on constraint-based learning of Markov networks
Authors: Tuukka Korhonen, Fedor V. Fomin, Pekka Parviainen
Categories: cs.LG cs.AI cs.DM
Comments: AISTATS 2024
\\
  Markov networks are probabilistic graphical models that employ undirected
graphs to depict conditional independence relationships among variables. Our
focus lies in constraint-based structure learning, which entails learning the
undirected graph from data through the execution of conditional independence
tests. We establish theoretical limits concerning two critical aspects of
constraint-based learning of Markov networks: the number of tests and the sizes
of the conditioning sets. These bounds uncover an exciting interplay between
the structural properties of the graph and the amount of tests required to
learn a Markov network. The starting point of our work is that the graph
parameter maximum pairwise connectivity, $\kappa$, that is, the maximum number
of vertex-disjoint paths connecting a pair of vertices in the graph, is
responsible for the sizes of independence tests required to learn the graph. On
one hand, we show that at least one test with the size of the conditioning set
at least $\kappa$ is always necessary. On the other hand, we prove that any
graph can be learned by performing tests of size at most $\kappa$. This
completely resolves the question of the minimum size of conditioning sets
required to learn the graph. When it comes to the number of tests, our upper
bound on the sizes of conditioning sets implies that every $n$-vertex graph can
be learned by at most $n^{\kappa}$ tests with conditioning sets of sizes at
most $\kappa$. We show that for any upper bound $q$ on the sizes of the
conditioning sets, there exist graphs with $O(n q)$ vertices that require at
least $n^{\Omega(\kappa)}$ tests to learn. This lower bound holds even when the
treewidth and the maximum degree of the graph are at most $\kappa+2$. On the
positive side, we prove that every graph of bounded treewidth can be learned by
a polynomial number of tests with conditioning sets of sizes at most $2\kappa$.
\\ ( https://arxiv.org/abs/2403.08562 ,  108kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08569
Date: Wed, 13 Mar 2024 14:25:15 GMT   (25800kb,D)

Title: A Physics-driven GraphSAGE Method for Physical Process Simulations
  Described by Partial Differential Equations
Authors: Hang Hu, Sidi Wu, Guoxiong Cai, Na Liu
Categories: cs.LG physics.comp-ph
Comments: 18 pages,11 figures, 3 tables
ACM-class: G.1.8
\\
  Physics-informed neural networks (PINNs) have successfully addressed various
computational physics problems based on partial differential equations (PDEs).
However, while tackling issues related to irregularities like singularities and
oscillations, trained solutions usually suffer low accuracy. In addition, most
current works only offer the trained solution for predetermined input
parameters. If any change occurs in input parameters, transfer learning or
retraining is required, and traditional numerical techniques also need an
independent simulation. In this work, a physics-driven GraphSAGE approach
(PD-GraphSAGE) based on the Galerkin method and piecewise polynomial nodal
basis functions is presented to solve computational problems governed by
irregular PDEs and to develop parametric PDE surrogate models. This approach
employs graph representations of physical domains, thereby reducing the demands
for evaluated points due to local refinement. A distance-related edge feature
and a feature mapping strategy are devised to help training and convergence for
singularity and oscillation situations, respectively. The merits of the
proposed method are demonstrated through a couple of cases. Moreover, the
robust PDE surrogate model for heat conduction problems parameterized by the
Gaussian random field source is successfully established, which not only
provides the solution accurately but is several times faster than the finite
element method in our experiments.
\\ ( https://arxiv.org/abs/2403.08569 ,  25800kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08572
Date: Wed, 13 Mar 2024 14:28:02 GMT   (3304kb,D)

Title: Caformer: Rethinking Time Series Analysis from Causal Perspective
Authors: Kexuan Zhang, Xiaobei Zou, Yang Tang
Categories: cs.LG
\\
  Time series analysis is a vital task with broad applications in various
domains. However, effectively capturing cross-dimension and cross-time
dependencies in non-stationary time series poses significant challenges,
particularly in the context of environmental factors. The spurious correlation
induced by the environment confounds the causal relationships between
cross-dimension and cross-time dependencies. In this paper, we introduce a
novel framework called Caformer (\underline{\textbf{Ca}}usal
Trans\underline{\textbf{former}}) for time series analysis from a causal
perspective. Specifically, our framework comprises three components: Dynamic
Learner, Environment Learner, and Dependency Learner. The Dynamic Learner
unveils dynamic interactions among dimensions, the Environment Learner
mitigates spurious correlations caused by environment with a back-door
adjustment, and the Dependency Learner aims to infer robust interactions across
both time and dimensions. Our Caformer demonstrates consistent state-of-the-art
performance across five mainstream time series analysis tasks, including long-
and short-term forecasting, imputation, classification, and anomaly detection,
with proper interpretability.
\\ ( https://arxiv.org/abs/2403.08572 ,  3304kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08579
Date: Wed, 13 Mar 2024 14:34:34 GMT   (1003kb,D)

Title: Machine Learning Optimized Orthogonal Basis Piecewise Polynomial
  Approximation
Authors: Hannes Waclawek, Stefan Huber
Categories: cs.LG
Comments: Submitted to LION18
\\
  Piecewise Polynomials (PPs) are utilized in several engineering disciplines,
like trajectory planning, to approximate position profiles given in the form of
a set of points. While the approximation target along with domain-specific
requirements, like Ck -continuity, can be formulated as a system of equations
and a result can be computed directly, such closed-form solutions posses
limited flexibility with respect to polynomial degrees, polynomial bases or
adding further domain-specific requirements. Sufficiently complex optimization
goals soon call for the use of numerical methods, like gradient descent. Since
gradient descent lies at the heart of training Artificial Neural Networks
(ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set
of gradient-based optimizers potentially suitable for a wide range of
optimization problems beyond the training task for ANNs. Our approach is to
utilize the versatility of PP models and combine it with the potential of
modern ML optimizers for the use in function approximation in 1D trajectory
planning in the context of electronic cam design. We utilize available
optimizers of the ML framework TensorFlow directly, outside of the scope of
ANNs, to optimize model parameters of our PP model. In this paper, we show how
an orthogonal polynomial basis contributes to improving approximation and
continuity optimization performance. Utilizing Chebyshev polynomials of the
first kind, we develop a novel regularization approach enabling clearly
improved convergence behavior. We show that, using this regularization
approach, Chebyshev basis performs better than power basis for all relevant
optimizers in the combined approximation and continuity optimization setting
and demonstrate usability of the presented approach within the electronic cam
domain.
\\ ( https://arxiv.org/abs/2403.08579 ,  1003kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08585
Date: Wed, 13 Mar 2024 14:42:06 GMT   (130kb,D)

Title: Improving Implicit Regularization of SGD with Preconditioning for Least
  Square Problems
Authors: Junwei Su, Difan Zou, Chuan Wu
Categories: cs.LG
\\
  Stochastic gradient descent (SGD) exhibits strong algorithmic regularization
effects in practice and plays an important role in the generalization of modern
machine learning. However, prior research has revealed instances where the
generalization performance of SGD is worse than ridge regression due to uneven
optimization along different dimensions. Preconditioning offers a natural
solution to this issue by rebalancing optimization across different directions.
Yet, the extent to which preconditioning can enhance the generalization
performance of SGD and whether it can bridge the existing gap with ridge
regression remains uncertain. In this paper, we study the generalization
performance of SGD with preconditioning for the least squared problem. We make
a comprehensive comparison between preconditioned SGD and (standard \&
preconditioned) ridge regression. Our study makes several key contributions
toward understanding and improving SGD with preconditioning. First, we
establish excess risk bounds (generalization performance) for preconditioned
SGD and ridge regression under an arbitrary preconditions matrix. Second,
leveraging the excessive risk characterization of preconditioned SGD and ridge
regression, we show that (through construction) there exists a simple
preconditioned matrix that can outperform (standard \& preconditioned) ridge
regression. Finally, we show that our proposed preconditioning matrix is
straightforward enough to allow robust estimation from finite samples while
maintaining a theoretical advantage over ridge regression. Our empirical
results align with our theoretical findings, collectively showcasing the
enhanced regularization effect of preconditioned SGD.
\\ ( https://arxiv.org/abs/2403.08585 ,  130kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08589
Date: Wed, 13 Mar 2024 14:51:16 GMT   (826kb,D)

Title: Can physical information aid the generalization ability of Neural
  Networks for hydraulic modeling?
Authors: Gianmarco Guglielmo, Andrea Montessori, Jean-Michel Tucny, Michele La
  Rocca, Pietro Prestininzi
Categories: cs.LG physics.flu-dyn
\\
  Application of Neural Networks to river hydraulics is fledgling, despite the
field suffering from data scarcity, a challenge for machine learning
techniques. Consequently, many purely data-driven Neural Networks proved to
lack predictive capabilities. In this work, we propose to mitigate such problem
by introducing physical information into the training phase. The idea is
borrowed from Physics-Informed Neural Networks which have been recently
proposed in other contexts. Physics-Informed Neural Networks embed physical
information in the form of the residual of the Partial Differential Equations
(PDEs) governing the phenomenon and, as such, are conceived as neural solvers,
i.e. an alternative to traditional numerical solvers. Such approach is seldom
suitable for environmental hydraulics, where epistemic uncertainties are large,
and computing residuals of PDEs exhibits difficulties similar to those faced by
classical numerical methods. Instead, we envisaged the employment of Neural
Networks as neural operators, featuring physical constraints formulated without
resorting to PDEs. The proposed novel methodology shares similarities with data
augmentation and regularization. We show that incorporating such soft physical
information can improve predictive capabilities.
\\ ( https://arxiv.org/abs/2403.08589 ,  826kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08592
Date: Wed, 13 Mar 2024 14:57:10 GMT   (500kb,D)

Title: Data-Efficient Sleep Staging with Synthetic Time Series Pretraining
Authors: Niklas Grieger, Siamak Mehrkanoon, Stephan Bialonski
Categories: cs.LG q-bio.QM
Comments: 10 pages, 4 figures
\\
  Analyzing electroencephalographic (EEG) time series can be challenging,
especially with deep neural networks, due to the large variability among human
subjects and often small datasets. To address these challenges, various
strategies, such as self-supervised learning, have been suggested, but they
typically rely on extensive empirical datasets. Inspired by recent advances in
computer vision, we propose a pretraining task termed "frequency pretraining"
to pretrain a neural network for sleep staging by predicting the frequency
content of randomly generated synthetic time series. Our experiments
demonstrate that our method surpasses fully supervised learning in scenarios
with limited data and few subjects, and matches its performance in regimes with
many subjects. Furthermore, our results underline the relevance of frequency
information for sleep stage scoring, while also demonstrating that deep neural
networks utilize information beyond frequencies to enhance sleep staging
performance, which is consistent with previous research. We anticipate that our
approach will be advantageous across a broad spectrum of applications where EEG
data is limited or derived from a small number of subjects, including the
domain of brain-computer interfaces.
\\ ( https://arxiv.org/abs/2403.08592 ,  500kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08609
Date: Wed, 13 Mar 2024 15:21:14 GMT   (87kb,D)

Title: On the Convergence of Locally Adaptive and Scalable Diffusion-Based
  Sampling Methods for Deep Bayesian Neural Network Posteriors
Authors: Tim Rensmeyer and Oliver Niggemann
Categories: cs.LG stat.ML
\\
  Achieving robust uncertainty quantification for deep neural networks
represents an important requirement in many real-world applications of deep
learning such as medical imaging where it is necessary to assess the
reliability of a neural network's prediction. Bayesian neural networks are a
promising approach for modeling uncertainties in deep neural networks.
Unfortunately, generating samples from the posterior distribution of neural
networks is a major challenge. One significant advance in that direction would
be the incorporation of adaptive step sizes, similar to modern neural network
optimizers, into Monte Carlo Markov chain sampling algorithms without
significantly increasing computational demand. Over the past years, several
papers have introduced sampling algorithms with claims that they achieve this
property. However, do they indeed converge to the correct distribution? In this
paper, we demonstrate that these methods can have a substantial bias in the
distribution they sample, even in the limit of vanishing step sizes and at full
batch size.
\\ ( https://arxiv.org/abs/2403.08609 ,  87kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08618
Date: Wed, 13 Mar 2024 15:32:08 GMT   (2060kb,D)

Title: Verifix: Post-Training Correction to Improve Label Noise Robustness with
  Verified Samples
Authors: Sangamesh Kodge, Deepak Ravikumar, Gobinda Saha, Kaushik Roy
Categories: cs.LG cs.AI stat.ML
\\
  Label corruption, where training samples have incorrect labels, can
significantly degrade the performance of machine learning models. This
corruption often arises from non-expert labeling or adversarial attacks.
Acquiring large, perfectly labeled datasets is costly, and retraining large
models from scratch when a clean dataset becomes available is computationally
expensive. To address this challenge, we propose Post-Training Correction, a
new paradigm that adjusts model parameters after initial training to mitigate
label noise, eliminating the need for retraining. We introduce Verifix, a novel
Singular Value Decomposition (SVD) based algorithm that leverages a small,
verified dataset to correct the model weights using a single update. Verifix
uses SVD to estimate a Clean Activation Space and then projects the model's
weights onto this space to suppress activations corresponding to corrupted
data. We demonstrate Verifix's effectiveness on both synthetic and real-world
label noise. Experiments on the CIFAR dataset with 25% synthetic corruption
show 7.36% generalization improvements on average. Additionally, we observe
generalization improvements of up to 2.63% on naturally corrupted datasets like
WebVision1.0 and Clothing1M.
\\ ( https://arxiv.org/abs/2403.08618 ,  2060kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08635
Date: Wed, 13 Mar 2024 15:47:26 GMT   (209kb,D)

Title: Human Alignment of Large Language Models through Online Preference
  Optimisation
Authors: Daniele Calandriello, Daniel Guo, Remi Munos, Mark Rowland, Yunhao
  Tang, Bernardo Avila Pires, Pierre Harvey Richemond, Charline Le Lan, Michal
  Valko, Tianqi Liu, Rishabh Joshi, Zeyu Zheng, Bilal Piot
Categories: cs.LG cs.AI stat.ML
\\
  Ensuring alignment of language models' outputs with human preferences is
critical to guarantee a useful, safe, and pleasant user experience. Thus, human
alignment has been extensively studied recently and several methods such as
Reinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation
(DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper,
our contribution is two-fold. First, we show the equivalence between two recent
alignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror
Descent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD,
that leverages the regularised sampling approach proposed by Nash-MD.
  This equivalence may seem surprising at first sight, since IPO is an offline
method whereas Nash-MD is an online method using a preference model. However,
this equivalence can be proven when we consider the online version of IPO, that
is when both generations are sampled by the online policy and annotated by a
trained preference model. Optimising the IPO loss with such a stream of data
becomes then equivalent to finding the Nash equilibrium of the preference model
through self-play. Building on this equivalence, we introduce the IPO-MD
algorithm that generates data with a mixture policy (between the online and
reference policy) similarly as the general Nash-MD algorithm. We compare
online-IPO and IPO-MD to different online versions of existing losses on
preference data such as DPO and SLiC on a summarisation task.
\\ ( https://arxiv.org/abs/2403.08635 ,  209kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08638
Date: Wed, 13 Mar 2024 15:51:03 GMT   (58kb,D)

Title: Disparate Effect Of Missing Mediators On Transportability of Causal
  Effects
Authors: Vishwali Mhasawade, Rumi Chunara
Categories: cs.LG cs.CY
\\
  Transported mediation effects provide an avenue to understand how upstream
interventions (such as improved neighborhood conditions like green spaces)
would work differently when applied to different populations as a result of
factors that mediate the effects. However, when mediators are missing in the
population where the effect is to be transported, these estimates could be
biased. We study this issue of missing mediators, motivated by challenges in
public health, wherein mediators can be missing, not at random. We propose a
sensitivity analysis framework that quantifies the impact of missing mediator
data on transported mediation effects. This framework enables us to identify
the settings under which the conditional transported mediation effect is
rendered insignificant for the subgroup with missing mediator data.
Specifically, we provide the bounds on the transported mediation effect as a
function of missingness. We then apply the framework to longitudinal data from
the Moving to Opportunity Study, a large-scale housing voucher experiment, to
quantify the effect of missing mediators on transport effect estimates of
voucher receipt, an upstream intervention on living location, in childhood on
subsequent risk of mental health or substance use disorder mediated through
parental health across sites. Our findings provide a tangible understanding of
how much missing data can be withstood for unbiased effect estimates.
\\ ( https://arxiv.org/abs/2403.08638 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08652
Date: Wed, 13 Mar 2024 16:06:26 GMT   (281kb,D)

Title: Extracting Explanations, Justification, and Uncertainty from Black-Box
  Deep Neural Networks
Authors: Paul Ardis, Arjuna Flenner
Categories: cs.LG stat.ML
Comments: 8 pages, 5 figures, SPIE DCS 2024
ACM-class: I.2.10
\\
  Deep Neural Networks (DNNs) do not inherently compute or exhibit
empirically-justified task confidence. In mission critical applications, it is
important to both understand associated DNN reasoning and its supporting
evidence. In this paper, we propose a novel Bayesian approach to extract
explanations, justifications, and uncertainty estimates from DNNs. Our approach
is efficient both in terms of memory and computation, and can be applied to any
black box DNN without any retraining, including applications to anomaly
detection and out-of-distribution detection tasks. We validate our approach on
the CIFAR-10 dataset, and show that it can significantly improve the
interpretability and reliability of DNNs.
\\ ( https://arxiv.org/abs/2403.08652 ,  281kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08673
Date: Wed, 13 Mar 2024 16:25:55 GMT   (300kb,D)

Title: When can we Approximate Wide Contrastive Models with Neural Tangent
  Kernels and Principal Component Analysis?
Authors: Gautham Govind Anil, Pascal Esser, Debarghya Ghoshdastidar
Categories: cs.LG stat.ML
\\
  Contrastive learning is a paradigm for learning representations from
unlabelled data that has been highly successful for image and text data.
Several recent works have examined contrastive losses to claim that contrastive
models effectively learn spectral embeddings, while few works show relations
between (wide) contrastive models and kernel principal component analysis
(PCA). However, it is not known if trained contrastive models indeed correspond
to kernel methods or PCA. In this work, we analyze the training dynamics of
two-layer contrastive models, with non-linear activation, and answer when these
models are close to PCA or kernel methods. It is well known in the supervised
setting that neural networks are equivalent to neural tangent kernel (NTK)
machines, and that the NTK of infinitely wide networks remains constant during
training. We provide the first convergence results of NTK for contrastive
losses, and present a nuanced picture: NTK of wide networks remains almost
constant for cosine similarity based contrastive losses, but not for losses
based on dot product similarity. We further study the training dynamics of
contrastive models with orthogonality constraints on output layer, which is
implicitly assumed in works relating contrastive learning to spectral
embedding. Our deviation bounds suggest that representations learned by
contrastive models are close to the principal components of a certain matrix
computed from random features. We empirically show that our theoretical results
possibly hold beyond two-layer networks.
\\ ( https://arxiv.org/abs/2403.08673 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08699
Date: Wed, 13 Mar 2024 17:02:27 GMT   (61kb)

Title: Implicit Regularization of Gradient Flow on One-Layer Softmax Attention
Authors: Heejune Sheen, Siyu Chen, Tianhao Wang, Harrison H. Zhou
Categories: cs.LG cs.AI math.OC stat.ML
Comments: 34 pages
\\
  We study gradient flow on the exponential loss for a classification problem
with a one-layer softmax attention model, where the key and query weight
matrices are trained separately. Under a separability assumption on the data,
we show that when gradient flow achieves the minimal loss value, it further
implicitly minimizes the nuclear norm of the product of the key and query
weight matrices. Such implicit regularization can be described by a Support
Vector Machine (SVM) problem with respect to the attention weights. This
finding contrasts with prior results showing that the gradient descent induces
an implicit regularization on the Frobenius norm on the product weight matrix
when the key and query matrices are combined into a single weight matrix for
training. For diagonal key and query matrices, our analysis builds upon the
reparameterization technique and exploits approximate KKT conditions of the SVM
associated with the classification data. Moreover, the results are extended to
general weights configurations given proper alignment of the weight matrices'
singular spaces with the data features at initialization.
\\ ( https://arxiv.org/abs/2403.08699 ,  61kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08763
Date: Wed, 13 Mar 2024 17:58:57 GMT   (10694kb,D)

Title: Simple and Scalable Strategies to Continually Pre-train Large Language
  Models
Authors: Adam Ibrahim, Benjamin Th\'erien, Kshitij Gupta, Mats L. Richter,
  Quentin Anthony, Timoth\'ee Lesort, Eugene Belilovsky, and Irina Rish
Categories: cs.LG cs.AI cs.CL
\\
  Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to start the process over again once new data becomes available. A much
more efficient solution is to continually pre-train these models, saving
significant compute compared to re-training. However, the distribution shift
induced by new data typically results in degraded performance on previous data
or poor adaptation to the new data. In this work, we show that a simple and
scalable combination of learning rate (LR) re-warming, LR re-decaying, and
replay of previous data is sufficient to match the performance of fully
re-training from scratch on all available data, as measured by final loss and
language model (LM) evaluation benchmarks. Specifically, we show this for a
weak but realistic distribution shift between two commonly used LLM
pre-training datasets (English$\rightarrow$English) and a stronger distribution
shift (English$\rightarrow$German) at the $405$M parameter model scale with
large dataset sizes (hundreds of billions of tokens). Selecting the weak but
realistic shift for larger-scale experiments, we also find that our continual
learning strategies match the re-training baseline for a 10B parameter LLM. Our
results demonstrate that LLMs can be successfully updated via simple and
scalable continual learning strategies, matching the re-training baseline using
only a fraction of the compute. Finally, inspired by previous work, we propose
alternatives to the cosine learning rate schedule that help circumvent
forgetting induced by LR re-warming and that are not bound to a fixed token
budget.
\\ ( https://arxiv.org/abs/2403.08763 ,  10694kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2309.08112 (*cross-listing*)
Date: Fri, 15 Sep 2023 02:42:03 GMT   (5375kb,D)

Title: Empowering Private Tutoring by Chaining Large Language Models
Authors: Yulin Chen, Ning Ding, Hai-Tao Zheng, Zhiyuan Liu, Maosong Sun, Bowen
  Zhou
Categories: cs.HC cs.AI cs.CL
\\
  Artificial intelligence has been applied in various aspects of online
education to facilitate teaching and learning. However, few approaches has been
made toward a complete AI-powered tutoring system. In this work, we explore the
development of a full-fledged intelligent tutoring system powered by
state-of-the-art large language models (LLMs), covering automatic course
planning and adjusting, tailored instruction, and flexible quiz evaluation. To
make the system robust to prolonged interaction and cater to individualized
education, the system is decomposed into three inter-connected core
processes-interaction, reflection, and reaction. Each process is implemented by
chaining LLM-powered tools along with dynamically updated memory modules. Tools
are LLMs prompted to execute one specific task at a time, while memories are
data storage that gets updated during education process. Statistical results
from learning logs demonstrate the effectiveness and mechanism of each tool
usage. Subjective feedback from human users reveal the usability of each
function, and comparison with ablation systems further testify the benefits of
the designed processes in long-term interaction.
\\ ( https://arxiv.org/abs/2309.08112 ,  5375kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07879 (*cross-listing*)
Date: Sun, 7 Jan 2024 11:23:13 GMT   (655kb)

Title: AI incidents and 'networked trouble': The case for a research agenda
Authors: Tommy Shaffer Shane
Categories: cs.CY cs.AI
Journal-ref: Big Data & Society, 2023, July - December 1 - 6
DOI: 10.1177/20539517231215360
\\
  Against a backdrop of widespread interest in how publics can participate in
the design of AI, I argue for a research agenda focused on AI incidents -
examples of AI going wrong and sparking controversy - and how they are
constructed in online environments. I take up the example of an AI incident
from September 2020, when a Twitter user created a 'horrible experiment' to
demonstrate the racist bias of Twitter's algorithm for cropping images. This
resulted in Twitter not only abandoning its use of that algorithm, but also
disavowing its decision to use any algorithm for the task. I argue that AI
incidents like this are a significant means for participating in AI systems
that require further research. That research agenda, I argue, should focus on
how incidents are constructed through networked online behaviours that I refer
to as 'networked trouble', where formats for participation enable individuals
and algorithms to interact in ways that others - including technology companies
- come to know and come to care about. At stake, I argue, is an important
mechanism for participating in the design and deployment of AI.
\\ ( https://arxiv.org/abs/2403.07879 ,  655kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07883 (*cross-listing*)
Date: Thu, 11 Jan 2024 14:31:30 GMT   (2444kb,D)

Title: Efficient Vision-and-Language Pre-training with Text-Relevant Image
  Patch Selection
Authors: Wei Ye, Chaoya Jiang, Haiyang Xu, Chenhao Ye, Chenliang Li, Ming Yan,
  Shikun Zhang, Songhang Huang, Fei Huang
Categories: cs.CV cs.AI
\\
  Vision Transformers (ViTs) have become increasingly popular in large-scale
Vision and Language Pre-training (VLP) models. Although previous VLP research
has demonstrated the efficacy of ViTs, these efforts still struggle with
computational inefficiencies caused by lengthy visual sequences. To address
this challenge, we introduce an efficient VLP approach called TRIPS, which
stands for Text-Relevant Image Patch Selection. TRIPS progressively reduces the
visual sequence using a text-guided patch-selection layer in the visual
backbone, thereby accelerating both training and inference processes. This
patch-selection layer dynamically computes text-dependent visual attention,
enabling it to identify attentive image tokens with text guidance and fuse
inattentive ones in an end-to-end fashion. Importantly, TRIPS does not add any
extra parameters and generalizes to most ViT-based VLP models. We incorporate
TRIPS into three representative VLP models covering single-stream, dual-stream,
and generative paradigms, and conduct extensive experiments on five widely-used
multi-modal benchmark datasets. Our experimental results reveal that TRIPS
delivers a 40% speedup, while maintaining competitive or superior performance
on downstream tasks.
\\ ( https://arxiv.org/abs/2403.07883 ,  2444kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07884 (*cross-listing*)
Date: Fri, 12 Jan 2024 16:30:54 GMT   (345kb,D)

Title: Seg-metrics: a Python package to compute segmentation metrics
Authors: Jingnan Jia, Marius Staring and Berend C. Stoel
Categories: cs.CV cs.AI
\\
  In response to a concerning trend of selectively emphasizing metrics in
medical image segmentation (MIS) studies, we introduce \texttt{seg-metrics}, an
open-source Python package for standardized MIS model evaluation. Unlike
existing packages, \texttt{seg-metrics} offers user-friendly interfaces for
various overlap-based and distance-based metrics, providing a comprehensive
solution. \texttt{seg-metrics} supports multiple file formats and is easily
installable through the Python Package Index (PyPI). With a focus on speed and
convenience, \texttt{seg-metrics} stands as a valuable tool for efficient MIS
model assessment.
\\ ( https://arxiv.org/abs/2403.07884 ,  345kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07885 (*cross-listing*)
Date: Wed, 31 Jan 2024 23:13:20 GMT   (161kb,D)

Title: MOD-CL: Multi-label Object Detection with Constrained Loss
Authors: Sota Moriyama, Koji Watanabe, Katsumi Inoue, Akihiro Takemura
Categories: cs.CV cs.AI
\\
  We introduce MOD-CL, a multi-label object detection framework that utilizes
constrained loss in the training process to produce outputs that better satisfy
the given requirements. In this paper, we use $\mathrm{MOD_{YOLO}}$, a
multi-label object detection model built upon the state-of-the-art object
detection model YOLOv8, which has been published in recent years. In Task 1, we
introduce the Corrector Model and Blender Model, two new models that follow
after the object detection process, aiming to generate a more constrained
output. For Task 2, constrained losses have been incorporated into the
$\mathrm{MOD_{YOLO}}$ architecture using Product T-Norm. The results show that
these implementations are instrumental to improving the scores for both Task 1
and Task 2.
\\ ( https://arxiv.org/abs/2403.07885 ,  161kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07886 (*cross-listing*)
Date: Thu, 1 Feb 2024 22:02:07 GMT   (9121kb,D)

Title: A Memetic Algorithm To Find a Hamiltonian Cycle in a Hamiltonian Graph
Authors: Sarwan Ali, Pablo Moscato
Categories: cs.NE cs.AI cs.DM
\\
  We present a memetic algorithm (\maa) approach for finding a Hamiltonian
cycle in a Hamiltonian graph. The \ma is based on a proven approach to the
Asymmetric Travelling Salesman Problem (\atspp) that, in this contribution, is
boosted by the introduction of more powerful local searches. Our approach also
introduces a novel technique that sparsifies the input graph under
consideration for Hamiltonicity and dynamically augments it during the search.
Such a combined heuristic approach helps to prove Hamiltonicity by finding a
Hamiltonian cycle in less time. In addition, we also employ a recently
introduced polynomial-time reduction from the \hamcyc to the Symmetric \tsp,
which is based on computing the transitive closure of the graph. Although our
approach is a metaheuristic, i.e., it does not give a theoretical guarantee for
finding a Hamiltonian cycle, we have observed that the method is successful in
practice in verifying the Hamiltonicity of a larger number of instances from
the \textit{Flinder University Hamiltonian Cycle Problem Challenge Set}
(\fhcpsc), even for the graphs that have large treewidth. The experiments on
the \fhcpscc instances and a computational comparison with five recent
state-of-the-art baseline approaches show that the proposed method outperforms
those for the majority of the instances in the \fhcpsc.
\\ ( https://arxiv.org/abs/2403.07886 ,  9121kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07887 (*cross-listing*)
Date: Fri, 2 Feb 2024 12:37:23 GMT   (27851kb,D)

Title: Neural Slot Interpreters: Grounding Object Semantics in Emergent Slot
  Representations
Authors: Bhishma Dedhia, Niraj K. Jha
Categories: cs.CV cs.AI
\\
  Object-centric methods have seen significant progress in unsupervised
decomposition of raw perception into rich object-like abstractions. However,
limited ability to ground object semantics of the real world into the learned
abstractions has hindered their adoption in downstream understanding
applications. We present the Neural Slot Interpreter (NSI) that learns to
ground and generate object semantics via slot representations. At the core of
NSI is an XML-like programming language that uses simple syntax rules to
organize the object semantics of a scene into object-centric program
primitives. Then, an alignment model learns to ground program primitives into
slots through a bi-level contrastive learning objective over a shared embedding
space. Finally, we formulate the NSI program generator model to use the dense
associations inferred from the alignment model to generate object-centric
programs from slots. Experiments on bi-modal retrieval tasks demonstrate the
efficacy of the learned alignments, surpassing set-matching-based predictors by
a significant margin. Moreover, learning the program generator from grounded
associations enhances the predictive power of slots. NSI generated programs
demonstrate improved performance of object-centric learners on property
prediction and object detection, and scale with real-world scene complexity.
\\ ( https://arxiv.org/abs/2403.07887 ,  27851kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07888 (*cross-listing*)
Date: Fri, 2 Feb 2024 18:54:48 GMT   (664kb,D)

Title: Cross-modality debiasing: using language to mitigate sub-population
  shifts in imaging
Authors: Yijiang Pang, Hoang Bao, Jiayu Zhou
Categories: cs.CV cs.AI
\\
  Sub-population shift is a specific type of domain shift that highlights
changes in data distribution within specific sub-groups or populations between
training and testing. Sub-population shift accounts for a significant source of
algorithmic bias and calls for distributional robustness. Recent studies found
inherent distributional robustness in multi-modality foundation models, such as
the vision-language model CLIP, yet this robustness is vulnerable through
parameter fine-tuning. In this paper, we propose leveraging the connection of
robustness among different modalities and reshaping the distributional
robustness of one modality with another. Specifically, in the context of the
distributional robustness of CLIP, we propose to leverage natural language
inputs to debias the image feature representations, to improve worst-case
performance on sub-populations. Our extensive empirical studies show that image
representations debiased by natural language can achieve significant
performance improvement and reduction of performance instability under
sub-population shifts.
\\ ( https://arxiv.org/abs/2403.07888 ,  664kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07890 (*cross-listing*)
Date: Fri, 2 Feb 2024 20:40:27 GMT   (727kb,D)

Title: $\widetilde{O}(T^{-1})$ Convergence to (Coarse) Correlated Equilibria in
  Full-Information General-Sum Markov Games
Authors: Weichao Mao, Haoran Qiu, Chen Wang, Hubertus Franke, Zbigniew
  Kalbarczyk, Tamer Ba\c{s}ar
Categories: cs.GT cs.AI cs.LG
\\
  No-regret learning has a long history of being closely connected to game
theory. Recent works have devised uncoupled no-regret learning dynamics that,
when adopted by all the players in normal-form games, converge to various
equilibrium solutions at a near-optimal rate of $\widetilde{O}(T^{-1})$, a
significant improvement over the $O(1/\sqrt{T})$ rate of classic no-regret
learners. However, analogous convergence results are scarce in Markov games, a
more generic setting that lays the foundation for multi-agent reinforcement
learning. In this work, we close this gap by showing that the
optimistic-follow-the-regularized-leader (OFTRL) algorithm, together with
appropriate value update procedures, can find
$\widetilde{O}(T^{-1})$-approximate (coarse) correlated equilibria in
full-information general-sum Markov games within $T$ iterations. Numerical
results are also included to corroborate our theoretical findings.
\\ ( https://arxiv.org/abs/2403.07890 ,  727kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07904 (*cross-listing*)
Date: Mon, 26 Feb 2024 11:32:42 GMT   (127kb)

Title: Addressing the Regulatory Gap: Moving Towards an EU AI Audit Ecosystem
  Beyond the AIA by Including Civil Society
Authors: David Hartmann, Jos\'e Renato Laranjeira de Pereira, Chiara
  Streitb\"orger, Bettina Berendt
Categories: cs.CY cs.AI cs.LG
\\
  The European legislature has proposed the Digital Services Act (DSA) and
Artificial Intelligence Act (AIA) to regulate platforms and Artificial
Intelligence (AI) products. We review to what extent third-party audits are
part of both laws and to what extent access to models and data is provided. By
considering the value of third-party audits and third-party data access in an
audit ecosystem, we identify a regulatory gap in that the Artificial
Intelligence Act does not provide access to data for researchers and civil
society. Our contributions to the literature include: (1) Defining an AI audit
ecosystem that incorporates compliance and oversight. (2) Highlighting a
regulatory gap within the DSA and AIA regulatory framework, preventing the
establishment of an AI audit ecosystem. (3) Emphasizing that third-party audits
by research and civil society must be part of that ecosystem and demand that
the AIA include data and model access for certain AI products. We call for the
DSA to provide NGOs and investigative journalists with data access to platforms
by delegated acts and for adaptions and amendments of the AIA to provide
third-party audits and data and model access at least for high-risk systems to
close the regulatory gap. Regulations modeled after European Union AI
regulations should enable data access and third-party audits, fostering an AI
audit ecosystem that promotes compliance and oversight mechanisms.
\\ ( https://arxiv.org/abs/2403.07904 ,  127kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07905 (*cross-listing*)
Date: Mon, 26 Feb 2024 13:12:44 GMT   (395kb)

Title: Enhancing Kubernetes Automated Scheduling with Deep Learning and
  Reinforcement Techniques for Large-Scale Cloud Computing Optimization
Authors: Zheng Xu, Yulu Gong, Yanlin Zhou, Qiaozhi Bao, Wenpin Qian
Categories: cs.DC cs.AI cs.LG
\\
  With the continuous expansion of the scale of cloud computing applications,
artificial intelligence technologies such as Deep Learning and Reinforcement
Learning have gradually become the key tools to solve the automated task
scheduling of large-scale cloud computing systems. Aiming at the complexity and
real-time requirement of task scheduling in large-scale cloud computing system,
this paper proposes an automatic task scheduling scheme based on deep learning
and reinforcement learning. Firstly, the deep learning technology is used to
monitor and predict the parameters in the cloud computing system in real time
to obtain the system status information. Then, combined with reinforcement
learning algorithm, the task scheduling strategy is dynamically adjusted
according to the real-time system state and task characteristics to achieve the
optimal utilization of system resources and the maximum of task execution
efficiency. This paper verifies the effectiveness and performance advantages of
the proposed scheme in experiments, and proves the potential and application
prospect of deep learning and reinforcement learning in automatic task
scheduling in large-scale cloud computing systems.
\\ ( https://arxiv.org/abs/2403.07905 ,  395kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07911 (*cross-listing*)
Date: Tue, 27 Feb 2024 03:33:40 GMT   (6430kb)

Title: Standing on FURM ground -- A framework for evaluating Fair, Useful, and
  Reliable AI Models in healthcare systems
Authors: Alison Callahan, Duncan McElfresh, Juan M. Banda, Gabrielle Bunney,
  Danton Char, Jonathan Chen, Conor K. Corbin, Debadutta Dash, Norman L.
  Downing, Srikar Nallan, Sneha S. Jain, Nikesh Kotecha, Jonathan Masterson,
  Michelle M. Mello, Keith Morse, Abby Pandya, Anurang Revri, Aditya Sharma,
  Christopher Sharp, Rahul Thapa, Michael Wornow, Alaa Youssef, Michael A.
  Pfeffer, Nigam H. Shah
Categories: cs.CY cs.AI
\\
  The impact of using artificial intelligence (AI) to guide patient care or
operational processes is an interplay of the AI model's output, the
decision-making protocol based on that output, and the capacity of the
stakeholders involved to take the necessary subsequent action. Estimating the
effects of this interplay before deployment, and studying it in real time
afterwards, are essential to bridge the chasm between AI model development and
achievable benefit. To accomplish this, the Data Science team at Stanford
Health Care has developed a mechanism to identify fair, useful and reliable AI
models (FURM) by conducting an ethical review to identify potential value
mismatches, simulations to estimate usefulness, financial projections to assess
sustainability, as well as analyses to determine IT feasibility, design a
deployment strategy, and recommend a prospective monitoring and evaluation
plan. We report on FURM assessments done to evaluate six AI guided solutions
for potential adoption, spanning clinical and operational settings, each with
the potential to impact from several dozen to tens of thousands of patients
each year. We describe the assessment process, summarize the six assessments,
and share our framework to enable others to conduct similar assessments. Of the
six solutions we assessed, two have moved into a planning and implementation
phase. Our novel contributions - usefulness estimates by simulation, financial
projections to quantify sustainability, and a process to do ethical assessments
- as well as their underlying methods and open source tools, are available for
other healthcare systems to conduct actionable evaluations of candidate AI
solutions.
\\ ( https://arxiv.org/abs/2403.07911 ,  6430kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07918 (*cross-listing*)
Date: Tue, 27 Feb 2024 16:49:53 GMT   (504kb,D)

Title: On the Societal Impact of Open Foundation Models
Authors: Sayash Kapoor, Rishi Bommasani, Kevin Klyman, Shayne Longpre, Ashwin
  Ramaswami, Peter Cihon, Aspen Hopkins, Kevin Bankston, Stella Biderman,
  Miranda Bogen, Rumman Chowdhury, Alex Engler, Peter Henderson, Yacine
  Jernite, Seth Lazar, Stefano Maffulli, Alondra Nelson, Joelle Pineau, Aviya
  Skowron, Dawn Song, Victor Storchan, Daniel Zhang, Daniel E. Ho, Percy Liang,
  Arvind Narayanan
Categories: cs.CY cs.AI cs.LG
\\
  Foundation models are powerful technologies: how they are released publicly
directly shapes their societal impact. In this position paper, we focus on open
foundation models, defined here as those with broadly available model weights
(e.g. Llama 2, Stable Diffusion XL). We identify five distinctive properties
(e.g. greater customizability, poor monitoring) of open foundation models that
lead to both their benefits and risks. Open foundation models present
significant benefits, with some caveats, that span innovation, competition, the
distribution of decision-making power, and transparency. To understand their
risks of misuse, we design a risk assessment framework for analyzing their
marginal risk. Across several misuse vectors (e.g. cyberattacks, bioweapons),
we find that current research is insufficient to effectively characterize the
marginal risk of open foundation models relative to pre-existing technologies.
The framework helps explain why the marginal risk is low in some cases,
clarifies disagreements about misuse risks by revealing that past work has
focused on different subsets of the framework with different assumptions, and
articulates a way forward for more constructive debate. Overall, our work helps
support a more grounded assessment of the societal impact of open foundation
models by outlining what research is needed to empirically validate their
theoretical benefits and risks.
\\ ( https://arxiv.org/abs/2403.07918 ,  504kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07920 (*cross-listing*)
Date: Wed, 28 Feb 2024 01:29:55 GMT   (1053kb,D)

Title: ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word
  Pre-Training
Authors: Le Zhuo, Zewen Chi, Minghao Xu, Heyan Huang, Heqi Zheng, Conghui He,
  Xian-Ling Mao, Wentao Zhang
Categories: q-bio.BM cs.AI cs.CL cs.LG
Comments: https://protllm.github.io/project/
\\
  We propose ProtLLM, a versatile cross-modal large language model (LLM) for
both protein-centric and protein-language tasks. ProtLLM features a unique
dynamic protein mounting mechanism, enabling it to handle complex inputs where
the natural language text is interspersed with an arbitrary number of proteins.
Besides, we propose the protein-as-word language modeling approach to train
ProtLLM. By developing a specialized protein vocabulary, we equip the model
with the capability to predict not just natural language but also proteins from
a vast pool of candidates. Additionally, we construct a large-scale interleaved
protein-text dataset, named InterPT, for pre-training. This dataset
comprehensively encompasses both (1) structured data sources like protein
annotations and (2) unstructured data sources like biological research papers,
thereby endowing ProtLLM with crucial knowledge for understanding proteins. We
evaluate ProtLLM on classic supervised protein-centric tasks and explore its
novel protein-language applications. Experimental results demonstrate that
ProtLLM not only achieves superior performance against protein-specialized
baselines on protein-centric tasks but also induces zero-shot and in-context
learning capabilities on protein-language tasks.
\\ ( https://arxiv.org/abs/2403.07920 ,  1053kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07923 (*cross-listing*)
Date: Wed, 28 Feb 2024 12:01:06 GMT   (4995kb)

Title: The Fusion of Deep Reinforcement Learning and Edge Computing for
  Real-time Monitoring and Control Optimization in IoT Environments
Authors: Jingyu Xu, Weixiang Wan, Linying Pan, Wenjian Sun, Yuxiang Liu
Categories: cs.NI cs.AI cs.LG cs.SY eess.IV eess.SY
\\
  In response to the demand for real-time performance and control quality in
industrial Internet of Things (IoT) environments, this paper proposes an
optimization control system based on deep reinforcement learning and edge
computing. The system leverages cloud-edge collaboration, deploys lightweight
policy networks at the edge, predicts system states, and outputs controls at a
high frequency, enabling monitoring and optimization of industrial objectives.
Additionally, a dynamic resource allocation mechanism is designed to ensure
rational scheduling of edge computing resources, achieving global optimization.
Results demonstrate that this approach reduces cloud-edge communication
latency, accelerates response to abnormal situations, reduces system failure
rates, extends average equipment operating time, and saves costs for manual
maintenance and replacement. This ensures real-time and stable control.
\\ ( https://arxiv.org/abs/2403.07923 ,  4995kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07924 (*cross-listing*)
Date: Thu, 29 Feb 2024 15:07:30 GMT   (185kb,D)

Title: Implications of Identity of AI: Creators, Creations, and Consequences
Authors: Sri Yash Tadimalla, Mary Lou Maher
Categories: cs.CY cs.AI cs.HC
Comments: 8 pages, 2 figures, AAAI Spring Symposium
\\
  The field of Artificial Intelligence (AI) is rapidly advancing, with
significant potential to transform society. However, it faces a notable
challenge: lack of diversity, a longstanding issue in STEM fields. In this
context, This position paper examines the intersection of AI and identity as a
pathway to understand biases, inequalities, and ethical considerations in AI
development and deployment. We present a multifaceted definition of AI
identity, which encompasses its creators, applications, and their broader
impacts. Understanding AI's identity involves analyzing the diverse individuals
involved in AI's development, the technologies produced, and the social,
ethical, and psychological implications. After exploring the AI identity
ecosystem and its societal dynamics, We propose a framework that highlights the
need for diversity in AI across three dimensions: Creators, Creations, and
Consequences through the lens of identity. This paper proposes the need for a
comprehensive approach to fostering a more inclusive and responsible AI
ecosystem through the lens of identity.
\\ ( https://arxiv.org/abs/2403.07924 ,  185kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07932 (*cross-listing*)
Date: Mon, 4 Mar 2024 03:43:45 GMT   (5006kb,D)

Title: Feint in Multi-Player Games
Authors: Junyu Liu, Wangkai Jin, Xiangjun Peng
Categories: cs.GT cs.AI
\\
  This paper introduces the first formalization, implementation and
quantitative evaluation of Feint in Multi-Player Games. Our work first
formalizes Feint from the perspective of Multi-Player Games, in terms of the
temporal, spatial, and their collective impacts. The formalization is built
upon Non-transitive Active Markov Game Model, where Feint can have a
considerable amount of impacts. Then, our work considers practical
implementation details of Feint in Multi-Player Games, under the
state-of-the-art progress of multi-agent modeling to date (namely Multi-Agent
Reinforcement Learning). Finally, our work quantitatively examines the
effectiveness of our design, and the results show that our design of Feint can
(1) greatly improve the reward gains from the game; (2) significantly improve
the diversity of Multi-Player Games; and (3) only incur negligible overheads in
terms of time consumption. We conclude that our design of Feint is effective
and practical, to make Multi-Player Games more interesting.
\\ ( https://arxiv.org/abs/2403.07932 ,  5006kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07938 (*cross-listing*)
Date: Fri, 8 Mar 2024 22:27:38 GMT   (1049kb,D)

Title: Text-to-Audio Generation Synchronized with Videos
Authors: Shentong Mo, Jing Shi, Yapeng Tian
Categories: cs.SD cs.AI cs.CV cs.LG cs.MM eess.AS
Comments: arXiv admin note: text overlap with arXiv:2305.12903
\\
  In recent times, the focus on text-to-audio (TTA) generation has intensified,
as researchers strive to synthesize audio from textual descriptions. However,
most existing methods, though leveraging latent diffusion models to learn the
correlation between audio and text embeddings, fall short when it comes to
maintaining a seamless synchronization between the produced audio and its
video. This often results in discernible audio-visual mismatches. To bridge
this gap, we introduce a groundbreaking benchmark for Text-to-Audio generation
that aligns with Videos, named T2AV-Bench. This benchmark distinguishes itself
with three novel metrics dedicated to evaluating visual alignment and temporal
consistency. To complement this, we also present a simple yet effective
video-aligned TTA generation model, namely T2AV. Moving beyond traditional
methods, T2AV refines the latent diffusion approach by integrating
visual-aligned text embeddings as its conditional foundation. It employs a
temporal multi-head attention transformer to extract and understand temporal
nuances from video data, a feat amplified by our Audio-Visual ControlNet that
adeptly merges temporal visual representations with text embeddings. Further
enhancing this integration, we weave in a contrastive learning objective,
designed to ensure that the visual-aligned text embeddings resonate closely
with the audio features. Extensive evaluations on the AudioCaps and T2AV-Bench
demonstrate that our T2AV sets a new standard for video-aligned TTA generation
in ensuring visual alignment and temporal consistency.
\\ ( https://arxiv.org/abs/2403.07938 ,  1049kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07944 (*cross-listing*)
Date: Sun, 10 Mar 2024 16:09:02 GMT   (7568kb,D)

Title: WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text
  and Image Inputs
Authors: Deshun Yang, Luhui Hu, Yu Tian, Zihao Li, Chris Kelly, Bang Yang,
  Cindy Yang, Yuexian Zou
Categories: cs.CV cs.AI
Comments: 11 pages, 2 figures, 2 tables
\\
  Several text-to-video diffusion models have demonstrated commendable
capabilities in synthesizing high-quality video content. However, it remains a
formidable challenge pertaining to maintaining temporal consistency and
ensuring action smoothness throughout the generated sequences. In this paper,
we present an innovative video generation AI agent that harnesses the power of
Sora-inspired multimodal learning to build skilled world models framework based
on textual prompts and accompanying images. The framework includes two parts:
prompt enhancer and full video translation. The first part employs the
capabilities of ChatGPT to meticulously distill and proactively construct
precise prompts for each subsequent step, thereby guaranteeing the utmost
accuracy in prompt communication and accurate execution in following model
operations. The second part employ compatible with existing advanced diffusion
techniques to expansively generate and refine the key frame at the conclusion
of a video. Then we can expertly harness the power of leading and trailing key
frames to craft videos with enhanced temporal consistency and action
smoothness. The experimental results confirm that our method has strong
effectiveness and novelty in constructing world models from text and image
inputs over the other methods.
\\ ( https://arxiv.org/abs/2403.07944 ,  7568kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07949 (*cross-listing*)
Date: Mon, 11 Mar 2024 23:03:04 GMT   (1269kb,D)

Title: Algorithmic Bayesian Epistemology
Authors: Eric Neyman
Categories: cs.GT cs.AI
Comments: 385 pages, PhD thesis, 14 figures, 4 tables
\\
  One aspect of the algorithmic lens in theoretical computer science is a view
on other scientific disciplines that focuses on satisfactory solutions that
adhere to real-world constraints, as opposed to solutions that would be optimal
ignoring such constraints. The algorithmic lens has provided a unique and
important perspective on many academic fields, including molecular biology,
ecology, neuroscience, quantum physics, economics, and social science.
  This thesis applies the algorithmic lens to Bayesian epistemology.
Traditional Bayesian epistemology provides a comprehensive framework for how an
individual's beliefs should evolve upon receiving new information. However,
these methods typically assume an exhaustive model of such information,
including the correlation structure between different pieces of evidence. In
reality, individuals might lack such an exhaustive model, while still needing
to form beliefs. Beyond such informational constraints, an individual may be
bounded by limited computation, or by limited communication with agents that
have access to information, or by the strategic behavior of such agents. Even
when these restrictions prevent the formation of a *perfectly* accurate belief,
arriving at a *reasonably* accurate belief remains crucial. In this thesis, we
establish fundamental possibility and impossibility results about belief
formation under a variety of restrictions, and lay the groundwork for further
exploration.
\\ ( https://arxiv.org/abs/2403.07949 ,  1269kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07952 (*cross-listing*)
Date: Tue, 12 Mar 2024 02:30:50 GMT   (17477kb,D)

Title: AesopAgent: Agent-driven Evolutionary System on Story-to-Video
  Production
Authors: Jiuniu Wang, Zehua Du, Yuyuan Zhao, Bo Yuan, Kexiang Wang, Jian Liang,
  Yaxi Zhao, Yihen Lu, Gengliang Li, Junlong Gao, Xin Tu, Zhenyu Guo
Categories: cs.CV cs.AI cs.MM
Comments: 22 pages, 13 figures
\\
  The Agent and AIGC (Artificial Intelligence Generated Content) technologies
have recently made significant progress. We propose AesopAgent, an Agent-driven
Evolutionary System on Story-to-Video Production. AesopAgent is a practical
application of agent technology for multimodal content generation. The system
integrates multiple generative capabilities within a unified framework, so that
individual users can leverage these modules easily. This innovative system
would convert user story proposals into scripts, images, and audio, and then
integrate these multimodal contents into videos. Additionally, the animating
units (e.g., Gen-2 and Sora) could make the videos more infectious. The
AesopAgent system could orchestrate task workflow for video generation,
ensuring that the generated video is both rich in content and coherent. This
system mainly contains two layers, i.e., the Horizontal Layer and the Utility
Layer. In the Horizontal Layer, we introduce a novel RAG-based evolutionary
system that optimizes the whole video generation workflow and the steps within
the workflow. It continuously evolves and iteratively optimizes workflow by
accumulating expert experience and professional knowledge, including optimizing
the LLM prompts and utilities usage. The Utility Layer provides multiple
utilities, leading to consistent image generation that is visually coherent in
terms of composition, characters, and style. Meanwhile, it provides audio and
special effects, integrating them into expressive and logically arranged
videos. Overall, our AesopAgent achieves state-of-the-art performance compared
with many previous works in visual storytelling. Our AesopAgent is designed for
convenient service for individual users, which is available on the following
page: https://aesopai.github.io/.
\\ ( https://arxiv.org/abs/2403.07952 ,  17477kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07959 (*cross-listing*)
Date: Tue, 12 Mar 2024 09:01:04 GMT   (668kb,D)

Title: An Interpretable Generalization Mechanism for Accurately Detecting
  Anomaly and Identifying Networking Intrusion Techniques
Authors: Hao-Ting Pai, Yu-Hsuan Kang, Wen-Cheng Chung
Categories: cs.CR cs.AI
\\
  Recent advancements in Intrusion Detection Systems (IDS), integrating
Explainable AI (XAI) methodologies, have led to notable improvements in system
performance via precise feature selection. However, a thorough understanding of
cyber-attacks requires inherently explainable decision-making processes within
IDS. In this paper, we present the Interpretable Generalization Mechanism (IG),
poised to revolutionize IDS capabilities. IG discerns coherent patterns, making
it interpretable in distinguishing between normal and anomalous network
traffic. Further, the synthesis of coherent patterns sheds light on intricate
intrusion pathways, providing essential insights for cybersecurity forensics.
By experiments with real-world datasets NSL-KDD, UNSW-NB15, and UKM-IDS20, IG
is accurate even at a low ratio of training-to-test. With 10%-to-90%, IG
achieves Precision (PRE)=0.93, Recall (REC)=0.94, and Area Under Curve
(AUC)=0.94 in NSL-KDD; PRE=0.98, REC=0.99, and AUC=0.99 in UNSW-NB15; and
PRE=0.98, REC=0.98, and AUC=0.99 in UKM-IDS20. Notably, in UNSW-NB15, IG
achieves REC=1.0 and at least PRE=0.98 since 40%-to-60%; in UKM-IDS20, IG
achieves REC=1.0 and at least PRE=0.88 since 20%-to-80%. Importantly, in
UKM-IDS20, IG successfully identifies all three anomalous instances without
prior exposure, demonstrating its generalization capabilities. These results
and inferences are reproducible. In sum, IG showcases superior generalization
by consistently performing well across diverse datasets and training-to-test
ratios (from 10%-to-90% to 90%-to-10%), and excels in identifying novel
anomalies without prior exposure. Its interpretability is enhanced by coherent
evidence that accurately distinguishes both normal and anomalous activities,
significantly improving detection accuracy and reducing false alarms, thereby
strengthening IDS reliability and trustworthiness.
\\ ( https://arxiv.org/abs/2403.07959 ,  668kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08017 (*cross-listing*)
Date: Tue, 12 Mar 2024 18:28:32 GMT   (2965kb,D)

Title: Red Teaming Models for Hyperspectral Image Analysis Using Explainable AI
Authors: Vladimir Zaigrajew, Hubert Baniecki, Lukasz Tulczyjew, Agata M.
  Wijata, Jakub Nalepa, Nicolas Long\'ep\'e, Przemyslaw Biecek
Categories: cs.CV cs.AI
Comments: 14 pages, 9 figures
\\
  Remote sensing (RS) applications in the space domain demand machine learning
(ML) models that are reliable, robust, and quality-assured, making red teaming
a vital approach for identifying and exposing potential flaws and biases. Since
both fields advance independently, there is a notable gap in integrating red
teaming strategies into RS. This paper introduces a methodology for examining
ML models operating on hyperspectral images within the HYPERVIEW challenge,
focusing on soil parameters' estimation. We use post-hoc explanation methods
from the Explainable AI (XAI) domain to critically assess the best performing
model that won the HYPERVIEW challenge and served as an inspiration for the
model deployed on board the INTUITION-1 hyperspectral mission. Our approach
effectively red teams the model by pinpointing and validating key shortcomings,
constructing a model that achieves comparable performance using just 1% of the
input features and a mere up to 5% performance loss. Additionally, we propose a
novel way of visualizing explanations that integrate domain-specific
information about hyperspectral bands (wavelengths) and data transformations to
better suit interpreting models for hyperspectral image analysis.
\\ ( https://arxiv.org/abs/2403.08017 ,  2965kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08032 (*cross-listing*)
Date: Tue, 12 Mar 2024 19:06:23 GMT   (21998kb,D)

Title: LG-Traj: LLM Guided Pedestrian Trajectory Prediction
Authors: Pranav Singh Chib, Pravendra Singh
Categories: cs.CV cs.AI
Comments: Under Review
\\
  Accurate pedestrian trajectory prediction is crucial for various
applications, and it requires a deep understanding of pedestrian motion
patterns in dynamic environments. However, existing pedestrian trajectory
prediction methods still need more exploration to fully leverage these motion
patterns. This paper investigates the possibilities of using Large Language
Models (LLMs) to improve pedestrian trajectory prediction tasks by inducing
motion cues. We introduce LG-Traj, a novel approach incorporating LLMs to
generate motion cues present in pedestrian past/observed trajectories. Our
approach also incorporates motion cues present in pedestrian future
trajectories by clustering future trajectories of training data using a mixture
of Gaussians. These motion cues, along with pedestrian coordinates, facilitate
a better understanding of the underlying representation. Furthermore, we
utilize singular value decomposition to augment the observed trajectories,
incorporating them into the model learning process to further enhance
representation learning. Our method employs a transformer-based architecture
comprising a motion encoder to model motion patterns and a social decoder to
capture social interactions among pedestrians. We demonstrate the effectiveness
of our approach on popular pedestrian trajectory prediction benchmarks, namely
ETH-UCY and SDD, and present various ablation experiments to validate our
approach.
\\ ( https://arxiv.org/abs/2403.08032 ,  21998kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08036 (*cross-listing*)
Date: Tue, 12 Mar 2024 19:15:20 GMT   (698kb,D)

Title: A Review of Cybersecurity Incidents in the Food and Agriculture Sector
Authors: Ajay Kulkarni, Yingjie Wang, Munisamy Gopinath, Dan Sobien, Abdul
  Rahman, and Feras A. Batarseh
Categories: cs.CR cs.AI cs.CY
Comments: Preprint. Submitted for journal publication
\\
  The increasing utilization of emerging technologies in the Food & Agriculture
(FA) sector has heightened the need for security to minimize cyber risks.
Considering this aspect, this manuscript reviews disclosed and documented
cybersecurity incidents in the FA sector. For this purpose, thirty
cybersecurity incidents were identified, which took place between July 2011 and
April 2023. The details of these incidents are reported from multiple sources
such as: the private industry and flash notifications generated by the Federal
Bureau of Investigation (FBI), internal reports from the affected
organizations, and available media sources. Considering the available
information, a brief description of the security threat, ransom amount, and
impact on the organization are discussed for each incident. This review reports
an increased frequency of cybersecurity threats to the FA sector. To minimize
these cyber risks, popular cybersecurity frameworks and recent
agriculture-specific cybersecurity solutions are also discussed. Further, the
need for AI assurance in the FA sector is explained, and the Farmer-Centered AI
(FCAI) framework is proposed. The main aim of the FCAI framework is to support
farmers in decision-making for agricultural production, by incorporating AI
assurance. Lastly, the effects of the reported cyber incidents on other
critical infrastructures, food security, and the economy are noted, along with
specifying the open issues for future development.
\\ ( https://arxiv.org/abs/2403.08036 ,  698kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08049 (*cross-listing*)
Date: Tue, 12 Mar 2024 19:46:59 GMT   (7243kb,D)

Title: TutoAI: A Cross-domain Framework for AI-assisted Mixed-media Tutorial
  Creation on Physical Tasks
Authors: Yuexi Chen, Vlad I. Morariu, Anh Truong, Zhicheng Liu
Categories: cs.HC cs.AI cs.LG
Comments: CHI 2024, supplementary materials:
  https://hdi.cs.umd.edu/papers/TutoAI_CHI24_Supp.pdf
\\
  Mixed-media tutorials, which integrate videos, images, text, and diagrams to
teach procedural skills, offer more browsable alternatives than timeline-based
videos. However, manually creating such tutorials is tedious, and existing
automated solutions are often restricted to a particular domain. While AI
models hold promise, it is unclear how to effectively harness their powers,
given the multi-modal data involved and the vast landscape of models. We
present TutoAI, a cross-domain framework for AI-assisted mixed-media tutorial
creation on physical tasks. First, we distill common tutorial components by
surveying existing work; then, we present an approach to identify, assemble,
and evaluate AI models for component extraction; finally, we propose guidelines
for designing user interfaces (UI) that support tutorial creation based on
AI-generated components. We show that TutoAI has achieved higher or similar
quality compared to a baseline model in preliminary user studies.
\\ ( https://arxiv.org/abs/2403.08049 ,  7243kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08059 (*cross-listing*)
Date: Tue, 12 Mar 2024 20:11:38 GMT   (6284kb,D)

Title: FluoroSAM: A Language-aligned Foundation Model for X-ray Image
  Segmentation
Authors: Benjamin D. Killeen, Liam J. Wang, Han Zhang, Mehran Armand, Russell
  H. Taylor, Greg Osgood, Mathias Unberath
Categories: cs.CV cs.AI cs.CL cs.LG
\\
  Automated X-ray image segmentation would accelerate research and development
in diagnostic and interventional precision medicine. Prior efforts have
contributed task-specific models capable of solving specific image analysis
problems, but the utility of these models is restricted to their particular
task domain, and expanding to broader use requires additional data, labels, and
retraining efforts. Recently, foundation models (FMs) -- machine learning
models trained on large amounts of highly variable data thus enabling broad
applicability -- have emerged as promising tools for automated image analysis.
Existing FMs for medical image analysis focus on scenarios and modalities where
objects are clearly defined by visually apparent boundaries, such as surgical
tool segmentation in endoscopy. X-ray imaging, by contrast, does not generally
offer such clearly delineated boundaries or structure priors. During X-ray
image formation, complex 3D structures are projected in transmission onto the
imaging plane, resulting in overlapping features of varying opacity and shape.
To pave the way toward an FM for comprehensive and automated analysis of
arbitrary medical X-ray images, we develop FluoroSAM, a language-aligned
variant of the Segment-Anything Model, trained from scratch on 1.6M synthetic
X-ray images. FluoroSAM is trained on data including masks for 128 organ types
and 464 non-anatomical objects, such as tools and implants. In real X-ray
images of cadaveric specimens, FluoroSAM is able to segment bony anatomical
structures based on text-only prompting with 0.51 and 0.79 DICE with
point-based refinement, outperforming competing SAM variants for all
structures. FluoroSAM is also capable of zero-shot generalization to segmenting
classes beyond the training set thanks to its language alignment, which we
demonstrate for full lung segmentation on real chest X-rays.
\\ ( https://arxiv.org/abs/2403.08059 ,  6284kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08077 (*cross-listing*)
Date: Tue, 12 Mar 2024 21:06:19 GMT   (1560kb,D)

Title: A Multimodal Intermediate Fusion Network with Manifold Learning for
  Stress Detection
Authors: Morteza Bodaghi, Majid Hosseini, Raju Gottumukkala
Categories: cs.CV cs.AI
Comments: This work was accepted to The 3rd International Conference on
  Computing and Machine Intelligence (ICMI 2024)
\\
  Multimodal deep learning methods capture synergistic features from multiple
modalities and have the potential to improve accuracy for stress detection
compared to unimodal methods. However, this accuracy gain typically comes from
high computational cost due to the high-dimensional feature spaces, especially
for intermediate fusion. Dimensionality reduction is one way to optimize
multimodal learning by simplifying data and making the features more amenable
to processing and analysis, thereby reducing computational complexity. This
paper introduces an intermediate multimodal fusion network with manifold
learning-based dimensionality reduction. The multimodal network generates
independent representations from biometric signals and facial landmarks through
1D-CNN and 2D-CNN. Finally, these features are fused and fed to another 1D-CNN
layer, followed by a fully connected dense layer. We compared various
dimensionality reduction techniques for different variations of unimodal and
multimodal networks. We observe that the intermediate-level fusion with the
Multi-Dimensional Scaling (MDS) manifold method showed promising results with
an accuracy of 96.00\% in a Leave-One-Subject-Out Cross-Validation (LOSO-CV)
paradigm over other dimensional reduction methods. MDS had the highest
computational cost among manifold learning methods. However, while
outperforming other networks, it managed to reduce the computational cost of
the proposed networks by 25\% when compared to six well-known conventional
feature selection methods used in the preprocessing step.
\\ ( https://arxiv.org/abs/2403.08077 ,  1560kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08111 (*cross-listing*)
Date: Tue, 12 Mar 2024 22:36:27 GMT   (3725kb,D)

Title: AI-Assisted Causal Pathway Diagram for Human-Centered Design
Authors: Ruican Zhong, Donghoon Shin, Rosemary Meza, Predrag Klasnja, Lucas
  Colusso, Gary Hsieh
Categories: cs.HC cs.AI cs.CL
ACM-class: H.5.2; I.2.7
Journal-ref: In Proceedings of the CHI Conference on Human Factors in Computing
  Systems (CHI '24), May 11-16, 2024, Honolulu, HI, USA
DOI: 10.1145/3613904.3642179
\\
  This paper explores the integration of causal pathway diagrams (CPD) into
human-centered design (HCD), investigating how these diagrams can enhance the
early stages of the design process. A dedicated CPD plugin for the online
collaborative whiteboard platform Miro was developed to streamline diagram
creation and offer real-time AI-driven guidance. Through a user study with
designers (N=20), we found that CPD's branching and its emphasis on causal
connections supported both divergent and convergent processes during design.
CPD can also facilitate communication among stakeholders. Additionally, we
found our plugin significantly reduces designers' cognitive workload and
increases their creativity during brainstorming, highlighting the implications
of AI-assisted tools in supporting creative work and evidence-based designs.
\\ ( https://arxiv.org/abs/2403.08111 ,  3725kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08115 (*cross-listing*)
Date: Tue, 12 Mar 2024 22:53:32 GMT   (37kb)

Title: Legally Binding but Unfair? Towards Assessing Fairness of Privacy
  Policies
Authors: Vincent Freiberger, Erik Buchmann
Categories: cs.CY cs.AI cs.CL
Comments: Submitted to IWSPA 2024 and under review
ACM-class: K.4.m
\\
  Privacy policies are expected to inform data subjects about their data
protection rights. They should explain the data controller's data management
practices, and make facts such as retention periods or data transfers to third
parties transparent. Privacy policies only fulfill their purpose, if they are
correctly perceived, interpreted, understood, and trusted by the data subject.
Amongst others, this requires that a privacy policy is written in a fair way,
e.g., it does not use polarizing terms, does not require a certain education,
or does not assume a particular social background. In this work-in-progress
paper, we outline our approach to assessing fairness in privacy policies. To
this end, we identify from fundamental legal sources and fairness research, how
the dimensions informational fairness, representational fairness and
ethics/morality are related to privacy policies. We propose options to
automatically assess policies in these fairness dimensions, based on text
statistics, linguistic methods and artificial intelligence. Finally, we conduct
initial experiments with German privacy policies to provide evidence that our
approach is applicable. Our experiments indicate that there are indeed issues
in all three dimensions of fairness. For example, our approach finds out if a
policy discriminates against individuals with impaired reading skills or
certain demographics, and identifies questionable ethics. This is important, as
future privacy policies may be used in a corpus for legal artificial
intelligence models.
\\ ( https://arxiv.org/abs/2403.08115 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08118 (*cross-listing*)
Date: Tue, 12 Mar 2024 22:57:53 GMT   (2017kb,D)

Title: Characterising harmful data sources when constructing multi-fidelity
  surrogate models
Authors: Nicolau Andr\'es-Thi\'o, Mario Andr\'es Mu\~noz, Kate Smith-Miles
Categories: stat.ME cs.AI cs.LG stat.ML
\\
  Surrogate modelling techniques have seen growing attention in recent years
when applied to both modelling and optimisation of industrial design problems.
These techniques are highly relevant when assessing the performance of a
particular design carries a high cost, as the overall cost can be mitigated via
the construction of a model to be queried in lieu of the available high-cost
source. The construction of these models can sometimes employ other sources of
information which are both cheaper and less accurate. The existence of these
sources however poses the question of which sources should be used when
constructing a model. Recent studies have attempted to characterise harmful
data sources to guide practitioners in choosing when to ignore a certain
source. These studies have done so in a synthetic setting, characterising
sources using a large amount of data that is not available in practice. Some of
these studies have also been shown to potentially suffer from bias in the
benchmarks used in the analysis. In this study, we present a characterisation
of harmful low-fidelity sources using only the limited data available to train
a surrogate model. We employ recently developed benchmark filtering techniques
to conduct a bias-free assessment, providing objectively varied benchmark
suites of different sizes for future research. Analysing one of these benchmark
suites with the technique known as Instance Space Analysis, we provide an
intuitive visualisation of when a low-fidelity source should be used and use
this analysis to provide guidelines that can be used in an applied industrial
setting.
\\ ( https://arxiv.org/abs/2403.08118 ,  2017kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08133 (*cross-listing*)
Date: Tue, 12 Mar 2024 23:40:51 GMT   (2367kb,D)

Title: Physics-Inspired Deep Learning Anti-Aliasing Framework in Efficient
  Channel State Feedback
Authors: Yu-Chien Lin, Yan Xin, Ta-Sung Lee, Charlie (Jianzhong) Zhang, and Zhi
  Ding
Categories: eess.SP cs.AI cs.IT math.IT
\\
  Acquiring downlink channel state information (CSI) at the base station is
vital for optimizing performance in massive Multiple input multiple output
(MIMO) Frequency-Division Duplexing (FDD) systems. While deep learning
architectures have been successful in facilitating UE-side CSI feedback and
gNB-side recovery, the undersampling issue prior to CSI feedback is often
overlooked. This issue, which arises from low density pilot placement in
current standards, results in significant aliasing effects in outdoor channels
and consequently limits CSI recovery performance. To this end, this work
introduces a new CSI upsampling framework at the gNB as a post-processing
solution to address the gaps caused by undersampling. Leveraging the physical
principles of discrete Fourier transform shifting theorem and multipath
reciprocity, our framework effectively uses uplink CSI to mitigate aliasing
effects. We further develop a learning-based method that integrates the
proposed algorithm with the Iterative Shrinkage-Thresholding Algorithm Net
(ISTA-Net) architecture, enhancing our approach for non-uniform sampling
recovery. Our numerical results show that both our rule-based and deep learning
methods significantly outperform traditional interpolation techniques and
current state-of-the-art approaches in terms of performance.
\\ ( https://arxiv.org/abs/2403.08133 ,  2367kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08136 (*cross-listing*)
Date: Tue, 12 Mar 2024 23:47:00 GMT   (1188kb,D)

Title: RoboCertProb: Property Specification for Probabilistic RoboChart Models
Authors: Kangfeng Ye and Jim Woodcock
Categories: cs.LO cs.AI
Comments: 24 pages, 10 figures, 4 tables, submitted to the International
  Journal on Software and Systems Modeling (SoSyM)
\\
  RoboChart is a core notation in the RoboStar framework which brings modern
modelling and formal verification technologies into software engineering for
robotics. It is a timed and probabilistic domain-specific language for robotics
and provides a UML-like architectural and state machine modelling. This work
presents RoboCertProb for specifying quantitative properties of probabilistic
robotic systems modelled in RoboChart. RoboCertProb's semantics is based on
PCTL*. To interpret RoboCertProb over RoboChart models, we give a Markov
semantics (DTMCs and MDPs) to RoboChart, derived from its existing
transformation semantics to the PRISM language. In addition to property
specification, RoboCertProb also entitles us to configure loose constants and
unspecified functions and operations in RoboChart models. It allows us to set
up environmental inputs to verify reactive probabilistic systems not directly
supported in probabilistic model checkers like PRISM because they employ a
closed-world assumption. We implement RoboCertProb in an accompanying tool of
RoboChart, RoboTool, for specifying properties and automatically generating
PRISM properties from them to formally verify RoboChart models using PRISM. We
have used it to analyse the behaviour of software controllers for two real
robots: an industrial painting robot and an agricultural robot for treating
plants with UV lights.
\\ ( https://arxiv.org/abs/2403.08136 ,  1188kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08137 (*cross-listing*)
Date: Tue, 12 Mar 2024 23:47:28 GMT   (1423kb,D)

Title: From Paper to Card: Transforming Design Implications with Generative AI
Authors: Donghoon Shin, Lucy Lu Wang, Gary Hsieh
Categories: cs.HC cs.AI cs.CL
ACM-class: H.5.2; I.2.7
Journal-ref: In Proceedings of the CHI Conference on Human Factors in Computing
  Systems (CHI '24), May 11-16, 2024, Honolulu, HI, USA. ACM, New York, NY, USA
DOI: 10.1145/3613904.3642266
\\
  Communicating design implications is common within the HCI community when
publishing academic papers, yet these papers are rarely read and used by
designers. One solution is to use design cards as a form of translational
resource that communicates valuable insights from papers in a more digestible
and accessible format to assist in design processes. However, creating design
cards can be time-consuming, and authors may lack the resources/know-how to
produce cards. Through an iterative design process, we built a system that
helps create design cards from academic papers using an LLM and text-to-image
model. Our evaluation with designers (N=21) and authors of selected papers
(N=12) revealed that designers perceived the design implications from our
design cards as more inspiring and generative, compared to reading original
paper texts, and the authors viewed our system as an effective way of
communicating their design implications. We also propose future enhancements
for AI-generated design cards.
\\ ( https://arxiv.org/abs/2403.08137 ,  1423kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08153 (*cross-listing*)
Date: Wed, 13 Mar 2024 00:30:47 GMT   (15kb)

Title: The Runtime of Random Local Search on the Generalized Needle Problem
Authors: Benjamin Doerr, Andrew James Kelley
Categories: cs.NE cs.AI cs.DS
Comments: 18 pages
\\
  In their recent work, C. Doerr and Krejca (Transactions on Evolutionary
Computation, 2023) proved upper bounds on the expected runtime of the
randomized local search heuristic on generalized Needle functions. Based on
these upper bounds, they deduce in a not fully rigorous manner a drastic
influence of the needle radius $k$ on the runtime.
  In this short article, we add the missing lower bound necessary to determine
the influence of parameter $k$ on the runtime. To this aim, we derive an exact
description of the expected runtime, which also significantly improves the
upper bound given by C. Doerr and Krejca. We also describe asymptotic estimates
of the expected runtime.
\\ ( https://arxiv.org/abs/2403.08153 ,  15kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08161 (*cross-listing*)
Date: Wed, 13 Mar 2024 01:07:55 GMT   (6154kb,D)

Title: LAFS: Landmark-based Facial Self-supervised Learning for Face
  Recognition
Authors: Zhonglin Sun, Chen Feng, Ioannis Patras, Georgios Tzimiropoulos
Categories: cs.CV cs.AI
Comments: accepted to CVPR 2024
\\
  In this work we focus on learning facial representations that can be adapted
to train effective face recognition models, particularly in the absence of
labels. Firstly, compared with existing labelled face datasets, a vastly larger
magnitude of unlabeled faces exists in the real world. We explore the learning
strategy of these unlabeled facial images through self-supervised pretraining
to transfer generalized face recognition performance. Moreover, motivated by
one recent finding, that is, the face saliency area is critical for face
recognition, in contrast to utilizing random cropped blocks of images for
constructing augmentations in pretraining, we utilize patches localized by
extracted facial landmarks. This enables our method - namely LAndmark-based
Facial Self-supervised learning LAFS), to learn key representation that is more
critical for face recognition. We also incorporate two landmark-specific
augmentations which introduce more diversity of landmark information to further
regularize the learning. With learned landmark-based facial representations, we
further adapt the representation for face recognition with regularization
mitigating variations in landmark positions. Our method achieves significant
improvement over the state-of-the-art on multiple face recognition benchmarks,
especially on more challenging few-shot scenarios.
\\ ( https://arxiv.org/abs/2403.08161 ,  6154kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08214 (*cross-listing*)
Date: Wed, 13 Mar 2024 03:23:50 GMT   (1497kb,D)

Title: P2LHAP:Wearable sensor-based human activity recognition, segmentation
  and forecast through Patch-to-Label Seq2Seq Transformer
Authors: Shuangjian Li, Tao Zhu, Mingxing Nie, Huansheng Ning, Zhenyu Liu and
  Liming Chen
Categories: cs.CV cs.AI
\\
  Traditional deep learning methods struggle to simultaneously segment,
recognize, and forecast human activities from sensor data. This limits their
usefulness in many fields such as healthcare and assisted living, where
real-time understanding of ongoing and upcoming activities is crucial. This
paper introduces P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles
all three tasks in a efficient single-task model. P2LHAP divides sensor data
streams into a sequence of "patches", served as input tokens, and outputs a
sequence of patch-level activity labels including the predicted future
activities. A unique smoothing technique based on surrounding patch labels, is
proposed to identify activity boundaries accurately. Additionally, P2LHAP
learns patch-level representation by sensor signal channel-independent
Transformer encoders and decoders. All channels share embedding and Transformer
weights across all sequences. Evaluated on three public datasets, P2LHAP
significantly outperforms the state-of-the-art in all three tasks,
demonstrating its effectiveness and potential for real-world applications.
\\ ( https://arxiv.org/abs/2403.08214 ,  1497kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08215 (*cross-listing*)
Date: Wed, 13 Mar 2024 03:24:36 GMT   (10816kb,D)

Title: LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual
  Semantic Segmentation for Autonomous Driving
Authors: Sicen Guo, Zhiyuan Wu, Qijun Chen, Ioannis Pitas and Rui Fan
Categories: cs.CV cs.AI cs.LG cs.RO
Comments: 13 pages, 4 figures, 5 tables
\\
  Despite the impressive performance achieved by data-fusion networks with
duplex encoders for visual semantic segmentation, they become ineffective when
spatial geometric data are not available. Implicitly infusing the spatial
geometric prior knowledge acquired by a duplex-encoder teacher model into a
single-encoder student model is a practical, albeit less explored research
avenue. This paper delves into this topic and resorts to knowledge distillation
approaches to address this problem. We introduce the Learning to Infuse "X"
(LIX) framework, with novel contributions in both logit distillation and
feature distillation aspects. We present a mathematical proof that underscores
the limitation of using a single fixed weight in decoupled knowledge
distillation and introduce a logit-wise dynamic weight controller as a solution
to this issue. Furthermore, we develop an adaptively-recalibrated feature
distillation algorithm, including two technical novelties: feature
recalibration via kernel regression and in-depth feature consistency
quantification via centered kernel alignment. Extensive experiments conducted
with intermediate-fusion and late-fusion networks across various public
datasets provide both quantitative and qualitative evaluations, demonstrating
the superior performance of our LIX framework when compared to other
state-of-the-art approaches.
\\ ( https://arxiv.org/abs/2403.08215 ,  10816kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08238 (*cross-listing*)
Date: Wed, 13 Mar 2024 04:43:10 GMT   (7241kb,D)

Title: A Novel Feature Learning-based Bio-inspired Neural Network for Real-time
  Collision-free Rescue of Multi-Robot Systems
Authors: Junfei Li, Simon X. Yang
Categories: cs.RO cs.AI cs.SY eess.SY
Comments: This paper is accepted to publish in IEEE Transactions on Industrial
  Electronics
DOI: 10.1109/TIE.2024.3370939
\\
  Natural disasters and urban accidents drive the demand for rescue robots to
provide safer, faster, and more efficient rescue trajectories. In this paper, a
feature learning-based bio-inspired neural network (FLBBINN) is proposed to
quickly generate a heuristic rescue path in complex and dynamic environments,
as traditional approaches usually cannot provide a satisfactory solution to
real-time responses to sudden environmental changes. The neurodynamic model is
incorporated into the feature learning method that can use environmental
information to improve path planning strategies. Task assignment and
collision-free rescue trajectory are generated through robot poses and the
dynamic landscape of neural activity. A dual-channel scale filter, a neural
activity channel, and a secondary distance fusion are employed to extract and
filter feature neurons. After completion of the feature learning process, a
neurodynamics-based feature matrix is established to quickly generate the new
heuristic rescue paths with parameter-driven topological adaptability. The
proposed FLBBINN aims to reduce the computational complexity of the neural
network-based approach and enable the feature learning method to achieve
real-time responses to environmental changes. Several simulations and
experiments have been conducted to evaluate the performance of the proposed
FLBBINN. The results show that the proposed FLBBINN would significantly improve
the speed, efficiency, and optimality for rescue operations.
\\ ( https://arxiv.org/abs/2403.08238 ,  7241kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08251 (*cross-listing*)
Date: Wed, 13 Mar 2024 05:08:10 GMT   (3707kb,D)

Title: Emergence of Social Norms in Large Language Model-based Agent Societies
Authors: Siyue Ren, Zhiyao Cui, Ruiqi Song, Zhen Wang, Shuyue Hu
Categories: cs.MA cs.AI cs.CY
\\
  The emergence of social norms has attracted much interest in a wide array of
disciplines, ranging from social science and cognitive science to artificial
intelligence. In this paper, we propose the first generative agent architecture
that empowers the emergence of social norms within a population of large
language model-based agents. Our architecture, named CRSEC, consists of four
modules: Creation & Representation, Spreading, Evaluation, and Compliance. Our
architecture addresses several important aspects of the emergent processes all
in one: (i) where social norms come from, (ii) how they are formally
represented, (iii) how they spread through agents' communications and
observations, (iv) how they are examined with a sanity check and synthesized in
the long term, and (v) how they are incorporated into agents' planning and
actions. Our experiments deployed in the Smallville sandbox game environment
demonstrate the capability of our architecture to establish social norms and
reduce social conflicts within large language model-based multi-agent systems.
The positive outcomes of our human evaluation, conducted with 30 evaluators,
further affirm the effectiveness of our approach.
\\ ( https://arxiv.org/abs/2403.08251 ,  3707kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08261 (*cross-listing*)
Date: Wed, 13 Mar 2024 05:24:28 GMT   (1975kb,D)

Title: CoroNetGAN: Controlled Pruning of GANs via Hypernetworks
Authors: Aman Kumar, Khushboo Anand, Shubham Mandloi, Ashutosh Mishra, Avinash
  Thakur, Neeraj Kasera, Prathosh A P
Categories: cs.CV cs.AI eess.IV
\\
  Generative Adversarial Networks (GANs) have proven to exhibit remarkable
performance and are widely used across many generative computer vision
applications. However, the unprecedented demand for the deployment of GANs on
resource-constrained edge devices still poses a challenge due to huge number of
parameters involved in the generation process. This has led to focused
attention on the area of compressing GANs. Most of the existing works use
knowledge distillation with the overhead of teacher dependency. Moreover, there
is no ability to control the degree of compression in these methods. Hence, we
propose CoroNet-GAN for compressing GAN using the combined strength of
differentiable pruning method via hypernetworks. The proposed method provides
the advantage of performing controllable compression while training along with
reducing training time by a substantial factor. Experiments have been done on
various conditional GAN architectures (Pix2Pix and CycleGAN) to signify the
effectiveness of our approach on multiple benchmark datasets such as
Edges-to-Shoes, Horse-to-Zebra and Summer-to-Winter. The results obtained
illustrate that our approach succeeds to outperform the baselines on
Zebra-to-Horse and Summer-to-Winter achieving the best FID score of 32.3 and
72.3 respectively, yielding high-fidelity images across all the datasets.
Additionally, our approach also outperforms the state-of-the-art methods in
achieving better inference time on various smart-phone chipsets and data-types
making it a feasible solution for deployment on edge devices.
\\ ( https://arxiv.org/abs/2403.08261 ,  1975kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08264 (*cross-listing*)
Date: Wed, 13 Mar 2024 05:30:30 GMT   (433kb,D)

Title: GPT, Ontology, and CAABAC: A Tripartite Personalized Access Control
  Model Anchored by Compliance, Context and Attribute
Authors: Raza Nowrozy, Khandakar Ahmed, Hua Wang
Categories: cs.CY cs.AI cs.CR
\\
  As digital healthcare evolves, the security of electronic health records
(EHR) becomes increasingly crucial. This study presents the GPT-Onto-CAABAC
framework, integrating Generative Pretrained Transformer (GPT), medical-legal
ontologies and Context-Aware Attribute-Based Access Control (CAABAC) to enhance
EHR access security. Unlike traditional models, GPT-Onto-CAABAC dynamically
interprets policies and adapts to changing healthcare and legal environments,
offering customized access control solutions. Through empirical evaluation,
this framework is shown to be effective in improving EHR security by accurately
aligning access decisions with complex regulatory and situational requirements.
The findings suggest its broader applicability in sectors where access control
must meet stringent compliance and adaptability standards.
\\ ( https://arxiv.org/abs/2403.08264 ,  433kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08271 (*cross-listing*)
Date: Wed, 13 Mar 2024 05:48:58 GMT   (9450kb,D)

Title: Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained
  Ship Classification
Authors: Long Lan, Fengxiang Wang, Shuyan Li, Xiangtao Zheng, Zengmao Wang and
  Xinwang Liu
Categories: cs.CV cs.AI
\\
  Fine-grained ship classification in remote sensing (RS-FGSC) poses a
significant challenge due to the high similarity between classes and the
limited availability of labeled data, limiting the effectiveness of traditional
supervised classification methods. Recent advancements in large pre-trained
Vision-Language Models (VLMs) have demonstrated impressive capabilities in
few-shot or zero-shot learning, particularly in understanding image content.
This study delves into harnessing the potential of VLMs to enhance
classification accuracy for unseen ship categories, which holds considerable
significance in scenarios with restricted data due to cost or privacy
constraints. Directly fine-tuning VLMs for RS-FGSC often encounters the
challenge of overfitting the seen classes, resulting in suboptimal
generalization to unseen classes, which highlights the difficulty in
differentiating complex backgrounds and capturing distinct ship features. To
address these issues, we introduce a novel prompt tuning technique that employs
a hierarchical, multi-granularity prompt design. Our approach integrates remote
sensing ship priors through bias terms, learned from a small trainable network.
This strategy enhances the model's generalization capabilities while improving
its ability to discern intricate backgrounds and learn discriminative ship
features. Furthermore, we contribute to the field by introducing a
comprehensive dataset, FGSCM-52, significantly expanding existing datasets with
more extensive data and detailed annotations for less common ship classes.
Extensive experimental evaluations demonstrate the superiority of our proposed
method over current state-of-the-art techniques. The source code will be made
publicly available.
\\ ( https://arxiv.org/abs/2403.08271 ,  9450kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08273 (*cross-listing*)
Date: Wed, 13 Mar 2024 05:53:25 GMT   (1133kb,D)

Title: LiqD: A Dynamic Liquid Level Detection Model under Tricky Small
  Containers
Authors: Yukun Ma, Zikun Mao
Categories: cs.CV cs.AI
Comments: 7pages, 7 Figures
ACM-class: I.4.6; I.5.2
\\
  In daily life and industrial production, it is crucial to accurately detect
changes in liquid level in containers. Traditional contact measurement methods
have some limitations, while emerging non-contact image processing technology
shows good application prospects. This paper proposes a container dynamic
liquid level detection model based on U^2-Net. This model uses the SAM model to
generate an initial data set, and then evaluates and filters out high-quality
pseudo-label images through the SemiReward framework to build an exclusive data
set. The model uses U^2-Net to extract mask images of containers from the data
set, and uses morphological processing to compensate for mask defects.
Subsequently, the model calculates the grayscale difference between adjacent
video frame images at the same position, segments the liquid level change area
by setting a difference threshold, and finally uses a lightweight neural
network to classify the liquid level state. This approach not only mitigates
the impact of intricate surroundings, but also reduces the demand for training
data, showing strong robustness and versatility. A large number of experimental
results show that the proposed model can effectively detect the dynamic liquid
level changes of the liquid in the container, providing a novel and efficient
solution for related fields.
\\ ( https://arxiv.org/abs/2403.08273 ,  1133kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08292 (*cross-listing*)
Date: Wed, 13 Mar 2024 06:54:38 GMT   (838kb,D)

Title: Weak Collocation Regression for Inferring Stochastic Dynamics with
  L\'{e}vy Noise
Authors: Liya Guo, Liwei Lu, Zhijun Zeng, Pipi Hu, Yi Zhu
Categories: math.NA cs.AI cs.NA math.DS
Comments: 19 pages, 5 figures, 10 tables
\\
  With the rapid increase of observational, experimental and simulated data for
stochastic systems, tremendous efforts have been devoted to identifying
governing laws underlying the evolution of these systems. Despite the broad
applications of non-Gaussian fluctuations in numerous physical phenomena, the
data-driven approaches to extracting stochastic dynamics with L\'{e}vy noise
are relatively few. In this work, we propose a Weak Collocation Regression
(WCR) to explicitly reveal unknown stochastic dynamical systems, i.e., the
Stochastic Differential Equation (SDE) with both $\alpha$-stable L\'{e}vy noise
and Gaussian noise, from discrete aggregate data. This method utilizes the
evolution equation of the probability distribution function, i.e., the
Fokker-Planck (FP) equation. With the weak form of the FP equation, the WCR
constructs a linear system of unknown parameters where all integrals are
evaluated by Monte Carlo method with the observations. Then, the unknown
parameters are obtained by a sparse linear regression. For a SDE with L\'{e}vy
noise, the corresponding FP equation is a partial integro-differential equation
(PIDE), which contains nonlocal terms, and is difficult to deal with. The weak
form can avoid complicated multiple integrals. Our approach can simultaneously
distinguish mixed noise types, even in multi-dimensional problems. Numerical
experiments demonstrate that our method is accurate and computationally
efficient.
\\ ( https://arxiv.org/abs/2403.08292 ,  838kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08299 (*cross-listing*)
Date: Wed, 13 Mar 2024 07:12:03 GMT   (273kb,D)

Title: AutoDev: Automated AI-Driven Development
Authors: Michele Tufano, Anisha Agarwal, Jinu Jang, Roshanak Zilouchian
  Moghaddam, Neel Sundaresan
Categories: cs.SE cs.AI
\\
  The landscape of software development has witnessed a paradigm shift with the
advent of AI-powered assistants, exemplified by GitHub Copilot. However,
existing solutions are not leveraging all the potential capabilities available
in an IDE such as building, testing, executing code, git operations, etc.
Therefore, they are constrained by their limited capabilities, primarily
focusing on suggesting code snippets and file manipulation within a chat-based
interface. To fill this gap, we present AutoDev, a fully automated AI-driven
software development framework, designed for autonomous planning and execution
of intricate software engineering tasks. AutoDev enables users to define
complex software engineering objectives, which are assigned to AutoDev's
autonomous AI Agents to achieve. These AI agents can perform diverse operations
on a codebase, including file editing, retrieval, build processes, execution,
testing, and git operations. They also have access to files, compiler output,
build and testing logs, static analysis tools, and more. This enables the AI
Agents to execute tasks in a fully automated manner with a comprehensive
understanding of the contextual information required. Furthermore, AutoDev
establishes a secure development environment by confining all operations within
Docker containers. This framework incorporates guardrails to ensure user
privacy and file security, allowing users to define specific permitted or
restricted commands and operations within AutoDev. In our evaluation, we tested
AutoDev on the HumanEval dataset, obtaining promising results with 91.5% and
87.8% of Pass@1 for code generation and test generation respectively,
demonstrating its effectiveness in automating software engineering tasks while
maintaining a secure and user-controlled development environment.
\\ ( https://arxiv.org/abs/2403.08299 ,  273kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08337 (*cross-listing*)
Date: Wed, 13 Mar 2024 08:41:55 GMT   (8886kb,D)

Title: LLM-Assisted Light: Leveraging Large Language Model Capabilities for
  Human-Mimetic Traffic Signal Control in Complex Urban Environments
Authors: Maonan Wang, Aoyu Pang, Yuheng Kan, Man-On Pun, Chung Shue Chen, Bo
  Huang
Categories: eess.SY cs.AI cs.LG cs.SY
Comments: 15 pages
\\
  Traffic congestion in metropolitan areas presents a formidable challenge with
far-reaching economic, environmental, and societal ramifications. Therefore,
effective congestion management is imperative, with traffic signal control
(TSC) systems being pivotal in this endeavor. Conventional TSC systems,
designed upon rule-based algorithms or reinforcement learning (RL), frequently
exhibit deficiencies in managing the complexities and variabilities of urban
traffic flows, constrained by their limited capacity for adaptation to
unfamiliar scenarios. In response to these limitations, this work introduces an
innovative approach that integrates Large Language Models (LLMs) into TSC,
harnessing their advanced reasoning and decision-making faculties.
Specifically, a hybrid framework that augments LLMs with a suite of perception
and decision-making tools is proposed, facilitating the interrogation of both
the static and dynamic traffic information. This design places the LLM at the
center of the decision-making process, combining external traffic data with
established TSC methods. Moreover, a simulation platform is developed to
corroborate the efficacy of the proposed framework. The findings from our
simulations attest to the system's adeptness in adjusting to a multiplicity of
traffic environments without the need for additional training. Notably, in
cases of Sensor Outage (SO), our approach surpasses conventional RL-based
systems by reducing the average waiting time by $20.4\%$. This research
signifies a notable advance in TSC strategies and paves the way for the
integration of LLMs into real-world, dynamic scenarios, highlighting their
potential to revolutionize traffic management. The related code is available at
\href{https://github.com/Traffic-Alpha/LLM-Assisted-Light}{https://github.com/Traffic-Alpha/LLM-Assisted-Light}.
\\ ( https://arxiv.org/abs/2403.08337 ,  8886kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08375 (*cross-listing*)
Date: Wed, 13 Mar 2024 09:38:39 GMT   (64kb)

Title: Translating between SQL Dialects for Cloud Migration
Authors: Ran Zmigrod, Salwa Alamir, Xiaomo Liu
Categories: cs.DB cs.AI cs.CL cs.SE
\\
  Migrations of systems from on-site premises to the cloud has been a
fundamental endeavor by many industrial institutions. A crucial component of
such cloud migrations is the transition of databases to be hosted online. In
this work, we consider the difficulties of this migration for SQL databases.
While SQL is one of the prominent methods for storing database procedures,
there are a plethora of different SQL dialects (e.g., MySQL, Postgres, etc.)
which can complicate migrations when the on-premise SQL dialect differs to the
dialect hosted on the cloud. Tools exist by common cloud provides such as AWS
and Azure to aid in translating between dialects in order to mitigate the
majority of the difficulties. However, these tools do not successfully
translate $100\%$ of the code. Consequently, software engineers must manually
convert the remainder of the untranslated database. For large organizations,
this task quickly becomes intractable and so more innovative solutions are
required. We consider this challenge a novel yet vital industrial research
problem for any large corporation that is considering cloud migrations.
Furthermore, we introduce potential avenues of research to tackle this
challenge that have yielded promising preliminary results.
\\ ( https://arxiv.org/abs/2403.08375 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08424 (*cross-listing*)
Date: Wed, 13 Mar 2024 11:16:43 GMT   (1879kb,D)

Title: Tastle: Distract Large Language Models for Automatic Jailbreak Attack
Authors: Zeguan Xiao, Yan Yang, Guanhua Chen, Yun Chen
Categories: cs.CR cs.AI cs.CL
\\
  Large language models (LLMs) have achieved significant advances in recent
days. Extensive efforts have been made before the public release of LLMs to
align their behaviors with human values. The primary goal of alignment is to
ensure their helpfulness, honesty and harmlessness. However, even meticulously
aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking,
leading to unintended behaviors. The jailbreak is to intentionally develop a
malicious prompt that escapes from the LLM security restrictions to produce
uncensored detrimental contents. Previous works explore different jailbreak
methods for red teaming LLMs, yet they encounter challenges regarding to
effectiveness and scalability. In this work, we propose Tastle, a novel
black-box jailbreak framework for automated red teaming of LLMs. We designed
malicious content concealing and memory reframing with an iterative
optimization algorithm to jailbreak LLMs, motivated by the research about the
distractibility and over-confidence phenomenon of LLMs. Extensive experiments
of jailbreaking both open-source and proprietary LLMs demonstrate the
superiority of our framework in terms of effectiveness, scalability and
transferability. We also evaluate the effectiveness of existing jailbreak
defense methods against our attack and highlight the crucial need to develop
more effective and practical defense strategies.
\\ ( https://arxiv.org/abs/2403.08424 ,  1879kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08426 (*cross-listing*)
Date: Wed, 13 Mar 2024 11:23:55 GMT   (6811kb,D)

Title: Language-Driven Visual Consensus for Zero-Shot Semantic Segmentation
Authors: Zicheng Zhang, Tong Zhang, Yi Zhu, Jianzhuang Liu, Xiaodan Liang,
  QiXiang Ye, Wei Ke
Categories: cs.CV cs.AI
\\
  The pre-trained vision-language model, exemplified by CLIP, advances
zero-shot semantic segmentation by aligning visual features with class
embeddings through a transformer decoder to generate semantic masks. Despite
its effectiveness, prevailing methods within this paradigm encounter
challenges, including overfitting on seen classes and small fragmentation in
masks. To mitigate these issues, we propose a Language-Driven Visual Consensus
(LDVC) approach, fostering improved alignment of semantic and visual
information.Specifically, we leverage class embeddings as anchors due to their
discrete and abstract nature, steering vision features toward class embeddings.
Moreover, to circumvent noisy alignments from the vision part due to its
redundant nature, we introduce route attention into self-attention for finding
visual consensus, thereby enhancing semantic consistency within the same
object. Equipped with a vision-language prompting strategy, our approach
significantly boosts the generalization capacity of segmentation models for
unseen classes. Experimental results underscore the effectiveness of our
approach, showcasing mIoU gains of 4.5 on the PASCAL VOC 2012 and 3.6 on the
COCO-Stuff 164k for unseen classes compared with the state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.08426 ,  6811kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08429 (*cross-listing*)
Date: Wed, 13 Mar 2024 11:29:13 GMT   (26kb)

Title: Software Vulnerability and Functionality Assessment using LLMs
Authors: Rasmus Ingemann Tuffveson Jensen, Vali Tawosi, Salwa Alamir
Categories: cs.SE cs.AI
Comments: 4 pages, accepted to NLBSE'24
\\
  While code review is central to the software development process, it can be
tedious and expensive to carry out. In this paper, we investigate whether and
how Large Language Models (LLMs) can aid with code reviews. Our investigation
focuses on two tasks that we argue are fundamental to good reviews: (i)
flagging code with security vulnerabilities and (ii) performing software
functionality validation, i.e., ensuring that code meets its intended
functionality. To test performance on both tasks, we use zero-shot and
chain-of-thought prompting to obtain final ``approve or reject''
recommendations. As data, we employ seminal code generation datasets (HumanEval
and MBPP) along with expert-written code snippets with security vulnerabilities
from the Common Weakness Enumeration (CWE). Our experiments consider a mixture
of three proprietary models from OpenAI and smaller open-source LLMs. We find
that the former outperforms the latter by a large margin. Motivated by
promising results, we finally ask our models to provide detailed descriptions
of security vulnerabilities. Results show that 36.7% of LLM-generated
descriptions can be associated with true CWE vulnerabilities.
\\ ( https://arxiv.org/abs/2403.08429 ,  26kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08430 (*cross-listing*)
Date: Wed, 13 Mar 2024 11:29:37 GMT   (44kb,D)

Title: Search-based Optimisation of LLM Learning Shots for Story Point
  Estimation
Authors: Vali Tawosi, Salwa Alamir, Xiaomo Liu
Categories: cs.SE cs.AI
Comments: 6 pages, Accepted at SSBSE'23 NIER Track
Journal-ref: Search-Based Software Engineering. SSBSE 2023. Lecture Notes in
  Computer Science, vol 14415. Springer
DOI: 10.1007/978-3-031-48796-5_9
\\
  One of the ways Large Language Models (LLMs) are used to perform machine
learning tasks is to provide them with a few examples before asking them to
produce a prediction. This is a meta-learning process known as few-shot
learning. In this paper, we use available Search-Based methods to optimise the
number and combination of examples that can improve an LLM's estimation
performance, when it is used to estimate story points for new agile tasks. Our
preliminary results show that our SBSE technique improves the estimation
performance of the LLM by 59.34% on average (in terms of mean absolute error of
the estimation) over three datasets against a zero-shot setting.
\\ ( https://arxiv.org/abs/2403.08430 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08502 (*cross-listing*)
Date: Wed, 13 Mar 2024 13:10:20 GMT   (42245kb,D)

Title: Masked Generative Story Transformer with Character Guidance and Caption
  Augmentation
Authors: Christos Papadimitriou, Giorgos Filandrianos, Maria Lymperaiou,
  Giorgos Stamou
Categories: cs.CV cs.AI
\\
  Story Visualization (SV) is a challenging generative vision task, that
requires both visual quality and consistency between different frames in
generated image sequences. Previous approaches either employ some kind of
memory mechanism to maintain context throughout an auto-regressive generation
of the image sequence, or model the generation of the characters and their
background separately, to improve the rendering of characters. On the contrary,
we embrace a completely parallel transformer-based approach, exclusively
relying on Cross-Attention with past and future captions to achieve
consistency. Additionally, we propose a Character Guidance technique to focus
on the generation of characters in an implicit manner, by forming a combination
of text-conditional and character-conditional logits in the logit space. We
also employ a caption-augmentation technique, carried out by a Large Language
Model (LLM), to enhance the robustness of our approach. The combination of
these methods culminates into state-of-the-art (SOTA) results over various
metrics in the most prominent SV benchmark (Pororo-SV), attained with
constraint resources while achieving superior computational complexity compared
to previous arts. The validity of our quantitative results is supported by a
human survey.
\\ ( https://arxiv.org/abs/2403.08502 ,  42245kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08505 (*cross-listing*)
Date: Wed, 13 Mar 2024 13:12:57 GMT   (2616kb,D)

Title: Content-aware Masked Image Modeling Transformer for Stereo Image
  Compression
Authors: Xinjie Zhang, Shenyuan Gao, Zhening Liu, Xingtong Ge, Dailan He,
  Tongda Xu, Yan Wang, Jun Zhang
Categories: eess.IV cs.AI cs.CV cs.MM
\\
  Existing learning-based stereo image codec adopt sophisticated transformation
with simple entropy models derived from single image codecs to encode latent
representations. However, those entropy models struggle to effectively capture
the spatial-disparity characteristics inherent in stereo images, which leads to
suboptimal rate-distortion results. In this paper, we propose a stereo image
compression framework, named CAMSIC. CAMSIC independently transforms each image
to latent representation and employs a powerful decoder-free Transformer
entropy model to capture both spatial and disparity dependencies, by
introducing a novel content-aware masked image modeling (MIM) technique. Our
content-aware MIM facilitates efficient bidirectional interaction between prior
information and estimated tokens, which naturally obviates the need for an
extra Transformer decoder. Experiments show that our stereo image codec
achieves state-of-the-art rate-distortion performance on two stereo image
datasets Cityscapes and InStereo2K with fast encoding and decoding speed.
\\ ( https://arxiv.org/abs/2403.08505 ,  2616kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08528 (*cross-listing*)
Date: Wed, 13 Mar 2024 13:38:58 GMT   (11743kb,D)

Title: Pig aggression classification using CNN, Transformers and Recurrent
  Networks
Authors: Junior Silva Souza, Eduardo Bedin, Gabriel Toshio Hirokawa Higa,
  Newton Loebens, Hemerson Pistori
Categories: cs.CV cs.AI
\\
  The development of techniques that can be used to analyze and detect animal
behavior is a crucial activity for the livestock sector, as it is possible to
monitor the stress and animal welfare and contributes to decision making in the
farm. Thus, the development of applications can assist breeders in making
decisions to improve production performance and reduce costs, once the animal
behavior is analyzed by humans and this can lead to susceptible errors and time
consumption. Aggressiveness in pigs is an example of behavior that is studied
to reduce its impact through animal classification and identification. However,
this process is laborious and susceptible to errors, which can be reduced
through automation by visually classifying videos captured in controlled
environment. The captured videos can be used for training and, as a result, for
classification through computer vision and artificial intelligence, employing
neural network techniques. The main techniques utilized in this study are
variants of transformers: STAM, TimeSformer, and ViViT, as well as techniques
using convolutions, such as ResNet3D2, Resnet(2+1)D, and CnnLstm. These
techniques were employed for pig video classification with the objective of
identifying aggressive and non-aggressive behaviors. In this work, various
techniques were compared to analyze the contribution of using transformers, in
addition to the effectiveness of the convolution technique in video
classification. The performance was evaluated using accuracy, precision, and
recall. The TimerSformer technique showed the best results in video
classification, with median accuracy of 0.729.
\\ ( https://arxiv.org/abs/2403.08528 ,  11743kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08536 (*cross-listing*)
Date: Wed, 13 Mar 2024 13:51:02 GMT   (26380kb,D)

Title: HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional
  Image Classifiers
Authors: Francesco Dibitonto, Fabio Garcea, Andr\'e Panisson, Alan Perotti, and
  Lia Morra
Categories: cs.CV cs.AI cs.LG
Comments: This work has been accepted to be presented to The 1st World
  Conference on eXplainable Artificial Intelligence (xAI 2023), July 26-28,
  2023 - Lisboa, Portugal
Journal-ref: Longo, L. (eds) Explainable Artificial Intelligence. xAI 2023.
  Communications in Computer and Information Science, vol 1902. Springer, Cham
DOI: 10.1007/978-3-031-44067-0_25
\\
  Convolutional Neural Networks (CNNs) are nowadays the model of choice in
Computer Vision, thanks to their ability to automatize the feature extraction
process in visual tasks. However, the knowledge acquired during training is
fully subsymbolic, and hence difficult to understand and explain to end users.
In this paper, we propose a new technique called HOLMES (HOLonym-MEronym based
Semantic inspection) that decomposes a label into a set of related concepts,
and provides component-level explanations for an image classification model.
Specifically, HOLMES leverages ontologies, web scraping and transfer learning
to automatically construct meronym (parts)-based detectors for a given holonym
(class). Then, it produces heatmaps at the meronym level and finally, by
probing the holonym CNN with occluded images, it highlights the importance of
each part on the classification output. Compared to state-of-the-art saliency
methods, HOLMES takes a step further and provides information about both where
and what the holonym CNN is looking at, without relying on densely annotated
datasets and without forcing concepts to be associated to single computational
units. Extensive experimental evaluation on different categories of objects
(animals, tools and vehicles) shows the feasibility of our approach. On
average, HOLMES explanations include at least two meronyms, and the ablation of
a single meronym roughly halves the holonym model confidence. The resulting
heatmaps were quantitatively evaluated using the
deletion/insertion/preservation curves. All metrics were comparable to those
achieved by GradCAM, while offering the advantage of further decomposing the
heatmap in human-understandable concepts, thus highlighting both the relevance
of meronyms to object classification, as well as HOLMES ability to capture it.
The code is available at https://github.com/FrancesC0de/HOLMES.
\\ ( https://arxiv.org/abs/2403.08536 ,  26380kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08551 (*cross-listing*)
Date: Wed, 13 Mar 2024 14:02:54 GMT   (1540kb,D)

Title: GaussianImage: 1000 FPS Image Representation and Compression by 2D
  Gaussian Splatting
Authors: Xinjie Zhang, Xingtong Ge, Tongda Xu, Dailan He, Yan Wang, Hongwei
  Qin, Guo Lu, Jing Geng, Jun Zhang
Categories: eess.IV cs.AI cs.CV cs.MM
\\
  Implicit neural representations (INRs) recently achieved great success in
image representation and compression, offering high visual quality and fast
rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are
available. However, this requirement often hinders their use on low-end devices
with limited memory. In response, we propose a groundbreaking paradigm of image
representation and compression by 2D Gaussian Splatting, named GaussianImage.
We first introduce 2D Gaussian to represent the image, where each Gaussian has
8 parameters including position, covariance and color. Subsequently, we unveil
a novel rendering algorithm based on accumulated summation. Remarkably, our
method with a minimum of 3$\times$ lower GPU memory usage and 5$\times$ faster
fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation
performance, but also delivers a faster rendering speed of 1500-2000 FPS
regardless of parameter size. Furthermore, we integrate existing vector
quantization technique to build an image codec. Experimental results
demonstrate that our codec attains rate-distortion performance comparable to
compression-based INRs such as COIN and COIN++, while facilitating decoding
speeds of approximately 1000 FPS. Additionally, preliminary proof of concept
shows that our codec surpasses COIN and COIN++ in performance when using
partial bits-back coding.
\\ ( https://arxiv.org/abs/2403.08551 ,  1540kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08556 (*cross-listing*)
Date: Wed, 13 Mar 2024 14:08:25 GMT   (10907kb,D)

Title: SM4Depth: Seamless Monocular Metric Depth Estimation across Multiple
  Cameras and Scenes by One Model
Authors: Yihao Liu and Feng Xue and Anlong Ming
Categories: cs.CV cs.AI
Comments: Project Page: xuefeng-cvr.github.io/SM4Depth
\\
  The generalization of monocular metric depth estimation (MMDE) has been a
longstanding challenge. Recent methods made progress by combining relative and
metric depth or aligning input image focal length. However, they are still
beset by challenges in camera, scene, and data levels: (1) Sensitivity to
different cameras; (2) Inconsistent accuracy across scenes; (3) Reliance on
massive training data. This paper proposes SM4Depth, a seamless MMDE method, to
address all the issues above within a single network. First, we reveal that a
consistent field of view (FOV) is the key to resolve ``metric ambiguity''
across cameras, which guides us to propose a more straightforward preprocessing
unit. Second, to achieve consistently high accuracy across scenes, we
explicitly model the metric scale determination as discretizing the depth
interval into bins and propose variation-based unnormalized depth bins. This
method bridges the depth gap of diverse scenes by reducing the ambiguity of the
conventional metric bin. Third, to reduce the reliance on massive training
data, we propose a ``divide and conquer" solution. Instead of estimating
directly from the vast solution space, the correct metric bins are estimated
from multiple solution sub-spaces for complexity reduction. Finally, with just
150K RGB-D pairs and a consumer-grade GPU for training, SM4Depth achieves
state-of-the-art performance on most previously unseen datasets, especially
surpassing ZoeDepth and Metric3D on mRI$_\theta$. The code can be found at
https://github.com/1hao-Liu/SM4Depth.
\\ ( https://arxiv.org/abs/2403.08556 ,  10907kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08613 (*cross-listing*)
Date: Wed, 13 Mar 2024 15:23:55 GMT   (187kb,D)

Title: Link Prediction for Social Networks using Representation Learning and
  Heuristic-based Features
Authors: Samarth Khanna, Sree Bhattacharyya, Sudipto Ghosh, Kushagra Agarwal,
  Asit Kumar Das
Categories: cs.SI cs.AI cs.LG
Comments: Accepted to the MAISoN Workshop at IJCAI 2023
\\
  The exponential growth in scale and relevance of social networks enable them
to provide expansive insights. Predicting missing links in social networks
efficiently can help in various modern-day business applications ranging from
generating recommendations to influence analysis. Several categories of
solutions exist for the same. Here, we explore various feature extraction
techniques to generate representations of nodes and edges in a social network
that allow us to predict missing links. We compare the results of using ten
feature extraction techniques categorized across Structural embeddings,
Neighborhood-based embeddings, Graph Neural Networks, and Graph Heuristics,
followed by modeling with ensemble classifiers and custom Neural Networks.
Further, we propose combining heuristic-based features and learned
representations that demonstrate improved performance for the link prediction
task on social network datasets. Using this method to generate accurate
recommendations for many applications is a matter of further study that appears
very promising. The code for all the experiments has been made public.
\\ ( https://arxiv.org/abs/2403.08613 ,  187kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08728 (*cross-listing*)
Date: Wed, 13 Mar 2024 17:28:20 GMT   (13790kb,D)

Title: Ambient Diffusion Posterior Sampling: Solving Inverse Problems with
  Diffusion Models trained on Corrupted Data
Authors: Asad Aali and Giannis Daras, Brett Levac, Sidharth Kumar, Alexandros
  G. Dimakis, Jonathan I. Tamir
Categories: cs.CV cs.AI cs.LG
Comments: Pre-print, work in progress
\\
  We provide a framework for solving inverse problems with diffusion models
learned from linearly corrupted data. Our method, Ambient Diffusion Posterior
Sampling (A-DPS), leverages a generative model pre-trained on one type of
corruption (e.g. image inpainting) to perform posterior sampling conditioned on
measurements from a potentially different forward process (e.g. image
blurring). We test the efficacy of our approach on standard natural image
datasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes
outperform models trained on clean data for several image restoration tasks in
both speed and performance. We further extend the Ambient Diffusion framework
to train MRI models with access only to Fourier subsampled multi-coil MRI
measurements at various acceleration factors (R=2, 4, 6, 8). We again observe
that models trained on highly subsampled data are better priors for solving
inverse problems in the high acceleration regime than models trained on fully
sampled data. We open-source our code and the trained Ambient Diffusion MRI
models: https://github.com/utcsilab/ambient-diffusion-mri .
\\ ( https://arxiv.org/abs/2403.08728 ,  13790kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08755 (*cross-listing*)
Date: Wed, 13 Mar 2024 17:53:47 GMT   (2668kb,D)

Title: DAM: Dynamic Adapter Merging for Continual Video QA Learning
Authors: Feng Cheng, Ziyang Wang, Yi-Lin Sung, Yan-Bo Lin, Mohit Bansal, Gedas
  Bertasius
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: The first two authors contribute equally
\\
  We present a parameter-efficient method for continual video
question-answering (VidQA) learning. Our method, named DAM, uses the proposed
Dynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable
efficient adaptation to continually arriving datasets, (iii) handle inputs from
unknown datasets during inference, and (iv) enable knowledge sharing across
similar dataset domains. Given a set of continually streaming VidQA datasets,
we sequentially train dataset-specific adapters for each dataset while freezing
the parameters of a large pretrained video-language backbone. During inference,
given a video-question sample from an unknown domain, our method first uses the
proposed non-parametric router function to compute a probability for each
adapter, reflecting how relevant that adapter is to the current video-question
input instance. Subsequently, the proposed dynamic adapter merging scheme
aggregates all the adapter weights into a new adapter instance tailored for
that particular test sample to compute the final VidQA prediction, mitigating
the impact of inaccurate router predictions and facilitating knowledge sharing
across domains. Our DAM model outperforms prior state-of-the-art continual
learning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA
datasets spanning various domains. We further extend DAM to continual image
classification and image QA and outperform prior methods by a large margin. The
code is publicly available at: https://github.com/klauscc/DAM
\\ ( https://arxiv.org/abs/2403.08755 ,  2668kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08770 (*cross-listing*)
Date: Wed, 13 Mar 2024 17:59:56 GMT   (3866kb,D)

Title: FastMAC: Stochastic Spectral Sampling of Correspondence Graph
Authors: Yifei Zhang, Hao Zhao, Hongyang Li, Siheng Chen
Categories: cs.CV cs.AI cs.RO
Comments: CVPR 2024, Code: https://github.com/Forrest-110/FastMAC
\\
  3D correspondence, i.e., a pair of 3D points, is a fundamental concept in
computer vision. A set of 3D correspondences, when equipped with compatibility
edges, forms a correspondence graph. This graph is a critical component in
several state-of-the-art 3D point cloud registration approaches, e.g., the one
based on maximal cliques (MAC). However, its properties have not been well
understood. So we present the first study that introduces graph signal
processing into the domain of correspondence graph. We exploit the generalized
degree signal on correspondence graph and pursue sampling strategies that
preserve high-frequency components of this signal. To address time-consuming
singular value decomposition in deterministic sampling, we resort to a
stochastic approximate sampling strategy. As such, the core of our method is
the stochastic spectral sampling of correspondence graph. As an application, we
build a complete 3D registration algorithm termed as FastMAC, that reaches
real-time speed while leading to little to none performance drop. Through
extensive experiments, we validate that FastMAC works for both indoor and
outdoor benchmarks. For example, FastMAC can accelerate MAC by 80 times while
maintaining high registration success rate on KITTI. Codes are publicly
available at https://github.com/Forrest-110/FastMAC.
\\ ( https://arxiv.org/abs/2403.08770 ,  3866kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07039 (*cross-listing*)
Date: Mon, 11 Mar 2024 09:57:16 GMT   (669kb)

Title: From English to ASIC: Hardware Implementation with Large Language Model
Authors: Emil Goh, Maoyang Xiang, I-Chyn Wey, T. Hui Teo
Categories: cs.AR cs.AI cs.CL cs.PL
Comments: 15 pages, 1 figure
\\
  In the realm of ASIC engineering, the landscape has been significantly
reshaped by the rapid development of LLM, paralleled by an increase in the
complexity of modern digital circuits. This complexity has escalated the
requirements for HDL coding, necessitating a higher degree of precision and
sophistication. However, challenges have been faced due to the
less-than-optimal performance of modern language models in generating hardware
description code, a situation further exacerbated by the scarcity of the
corresponding high-quality code datasets. These challenges have highlighted the
gap between the potential of LLMs to revolutionize digital circuit design and
their current capabilities in accurately interpreting and implementing hardware
specifications. To address these challenges, a strategy focusing on the
fine-tuning of the leading-edge nature language model and the reshuffling of
the HDL code dataset has been developed. The fine-tuning aims to enhance
models' proficiency in generating precise and efficient ASIC design, while the
dataset reshuffling is intended to broaden the scope and improve the quality of
training material. The model demonstrated significant improvements compared to
the base model, with approximately 10% to 20% increase in accuracy across a
wide range of temperature for the pass@1 metric. This approach is expected to
facilitate a simplified and more efficient LLM-assisted framework for complex
circuit design, leveraging their capabilities to meet the sophisticated demands
of HDL coding and thus streamlining the ASIC development process.
\\ ( https://arxiv.org/abs/2403.07039 ,  669kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07910 (*cross-listing*)
Date: Tue, 27 Feb 2024 02:00:28 GMT   (2589kb,D)

Title: Multi-Task Media-Bias Analysis Generalization for Pre-Trained
  Identification of Expressions
Authors: Tom\'a\v{s} Horych, Martin Wessel, Jan Philip Wahle, Terry Ruas,
  Jerome Wa{\ss}muth, Andr\'e Greiner-Petter, Akiko Aizawa, Bela Gipp, Timo
  Spinde
Categories: cs.CY cs.CL
\\
  Media bias detection poses a complex, multifaceted problem traditionally
tackled using single-task models and small in-domain datasets, consequently
lacking generalizability. To address this, we introduce MAGPIE, the first
large-scale multi-task pre-training approach explicitly tailored for media bias
detection. To enable pre-training at scale, we present Large Bias Mixture
(LBM), a compilation of 59 bias-related tasks. MAGPIE outperforms previous
approaches in media bias detection on the Bias Annotation By Experts (BABE)
dataset, with a relative improvement of 3.3% F1-score. MAGPIE also performs
better than previous models on 5 out of 8 tasks in the Media Bias
Identification Benchmark (MBIB). Using a RoBERTa encoder, MAGPIE needs only 15%
of finetuning steps compared to single-task approaches. Our evaluation shows,
for instance, that tasks like sentiment and emotionality boost all learning,
all tasks enhance fake news detection, and scaling tasks leads to the best
results. MAGPIE confirms that MTL is a promising approach for addressing media
bias detection, enhancing the accuracy and efficiency of existing models.
Furthermore, LBM is the first available resource collection focused on media
bias MTL.
\\ ( https://arxiv.org/abs/2403.07910 ,  2589kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07937 (*cross-listing*)
Date: Fri, 8 Mar 2024 08:10:29 GMT   (5276kb,D)

Title: Speech Robust Bench: A Robustness Benchmark For Speech Recognition
Authors: Muhammad A. Shah, David Solans Noguero, Mikko A. Heikkila and Nicolas
  Kourtellis
Categories: eess.AS cs.CL cs.LG cs.SD
\\
  As Automatic Speech Recognition (ASR) models become ever more pervasive, it
is important to ensure that they make reliable predictions under corruptions
present in the physical and digital world. We propose Speech Robust Bench
(SRB), a comprehensive benchmark for evaluating the robustness of ASR models to
diverse corruptions. SRB is composed of 69 input perturbations which are
intended to simulate various corruptions that ASR models may encounter in the
physical and digital world. We use SRB to evaluate the robustness of several
state-of-the-art ASR models and observe that model size and certain modeling
choices such as discrete representations, and self-training appear to be
conducive to robustness. We extend this analysis to measure the robustness of
ASR models on data from various demographic subgroups, namely English and
Spanish speakers, and males and females, and observed noticeable disparities in
the model's robustness across subgroups. We believe that SRB will facilitate
future research towards robust ASR models, by making it easier to conduct
comprehensive and comparable robustness evaluations.
\\ ( https://arxiv.org/abs/2403.07937 ,  5276kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07947 (*cross-listing*)
Date: Mon, 11 Mar 2024 15:11:28 GMT   (360kb)

Title: The evaluation of a code-switched Sepedi-English automatic speech
  recognition system
Authors: Amanda Phaladi and Thipe Modipa
Categories: eess.AS cs.CL cs.LG
Comments: 13 pages,2 figures,2nd International Conference on NLP & AI (NLPAI
  2024)
\\
  Speech technology is a field that encompasses various techniques and tools
used to enable machines to interact with speech, such as automatic speech
recognition (ASR), spoken dialog systems, and others, allowing a device to
capture spoken words through a microphone from a human speaker. End-to-end
approaches such as Connectionist Temporal Classification (CTC) and
attention-based methods are the most used for the development of ASR systems.
However, these techniques were commonly used for research and development for
many high-resourced languages with large amounts of speech data for training
and evaluation, leaving low-resource languages relatively underdeveloped. While
the CTC method has been successfully used for other languages, its
effectiveness for the Sepedi language remains uncertain. In this study, we
present the evaluation of the Sepedi-English code-switched automatic speech
recognition system. This end-to-end system was developed using the Sepedi
Prompted Code Switching corpus and the CTC approach. The performance of the
system was evaluated using both the NCHLT Sepedi test corpus and the Sepedi
Prompted Code Switching corpus. The model produced the lowest WER of 41.9%,
however, the model faced challenges in recognizing the Sepedi only text.
\\ ( https://arxiv.org/abs/2403.07947 ,  360kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07974 (*cross-listing*)
Date: Tue, 12 Mar 2024 17:58:04 GMT   (6890kb,D)

Title: LiveCodeBench: Holistic and Contamination Free Evaluation of Large
  Language Models for Code
Authors: Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang,
  Sida Wang, Armando Solar-Lezama, Koushik Sen, Ion Stoica
Categories: cs.SE cs.CL cs.LG
Comments: Website - https://livecodebench.github.io/
\\
  Large Language Models (LLMs) applied to code-related applications have
emerged as a prominent field, attracting significant interest from both
academia and industry. However, as new and improved LLMs are developed,
existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient
for assessing their capabilities. In this work, we propose LiveCodeBench, a
comprehensive and contamination-free evaluation of LLMs for code, which
continuously collects new problems over time from contests across three
competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our
benchmark also focuses on a broader range of code related capabilities, such as
self-repair, code execution, and test output prediction, beyond just code
generation. Currently, LiveCodeBench hosts four hundred high-quality coding
problems that were published between May 2023 and February 2024. We have
evaluated 9 base LLMs and 20 instruction-tuned LLMs on LiveCodeBench. We
present empirical findings on contamination, holistic performance comparisons,
potential overfitting in existing benchmarks as well as individual model
comparisons. We will release all prompts and model completions for further
community analysis, along with a general toolkit for adding new scenarios and
model
\\ ( https://arxiv.org/abs/2403.07974 ,  6890kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08358 (*cross-listing*)
Date: Wed, 13 Mar 2024 09:18:46 GMT   (187kb,D)

Title: Log Summarisation for Defect Evolution Analysis
Authors: Rares Dolga, Ran Zmigrod, Rui Silva, Salwa Alamir, Sameena Shah
Categories: cs.SE cs.CL
DOI: 10.1145/3617572.3617881
\\
  Log analysis and monitoring are essential aspects in software maintenance and
identifying defects. In particular, the temporal nature and vast size of log
data leads to an interesting and important research question: How can logs be
summarised and monitored over time? While this has been a fundamental topic of
research in the software engineering community, work has typically focused on
heuristic-, syntax-, or static-based methods. In this work, we suggest an
online semantic-based clustering approach to error logs that dynamically
updates the log clusters to enable monitoring code error life-cycles. We also
introduce a novel metric to evaluate the performance of temporal log clusters.
We test our system and evaluation metric with an industrial dataset and find
that our solution outperforms similar systems. We hope that our work encourages
further temporal exploration in defect datasets.
\\ ( https://arxiv.org/abs/2403.08358 ,  187kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08737 (*cross-listing*)
Date: Wed, 13 Mar 2024 17:38:05 GMT   (903kb,D)

Title: ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation
Authors: Sayar Ghosh Roy, Jiawei Han
Categories: cs.IR cs.CL
Comments: LREC-COLING 2024
\\
  Existing Machine Learning approaches for local citation recommendation
directly map or translate a query, which is typically a claim or an entity
mention, to citation-worthy research papers. Within such a formulation, it is
challenging to pinpoint why one should cite a specific research paper for a
particular query, leading to limited recommendation interpretability. To
alleviate this, we introduce the evidence-grounded local citation
recommendation task, where the target latent space comprises evidence spans for
recommending specific papers. Using a distantly-supervised evidence retrieval
and multi-step re-ranking framework, our proposed system, ILCiteR, recommends
papers to cite for a query grounded on similar evidence spans extracted from
the existing research literature. Unlike past formulations that simply output
recommendations, ILCiteR retrieves ranked lists of evidence span and
recommended paper pairs. Secondly, previously proposed neural models for
citation recommendation require expensive training on massive labeled data,
ideally after every significant update to the pool of candidate papers. In
contrast, ILCiteR relies solely on distant supervision from a dynamic evidence
database and pre-trained Transformer-based Language Models without any model
training. We contribute a novel dataset for the evidence-grounded local
citation recommendation task and demonstrate the efficacy of our proposed
conditional neural rank-ensembling approach for re-ranking evidence spans.
\\ ( https://arxiv.org/abs/2403.08737 ,  903kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07891 (*cross-listing*)
Date: Sat, 3 Feb 2024 16:05:27 GMT   (2438kb)

Title: Digital Video Manipulation Detection Technique Based on Compression
  Algorithms
Authors: Edgar Gonzalez Fernandez, Ana Lucila Sandoval Orozco, Luis Javier
  Garcia Villalba
Categories: cs.CV cs.CR cs.LG
Journal-ref: IEEE Transactions on Intelligent Transportation Systems, Vol. 23,
  No. 3, pp. 2596-2605, December 2021
DOI: 10.1109/TITS.2021.3132227
\\
  Digital images and videos play a very important role in everyday life.
Nowadays, people have access the affordable mobile devices equipped with
advanced integrated cameras and powerful image processing applications.
Technological development facilitates not only the generation of multimedia
content, but also the intentional modification of it, either with recreational
or malicious purposes. This is where forensic techniques to detect manipulation
of images and videos become essential. This paper proposes a forensic technique
by analysing compression algorithms used by the H.264 coding. The presence of
recompression uses information of macroblocks, a characteristic of the
H.264-MPEG4 standard, and motion vectors. A Vector Support Machine is used to
create the model that allows to accurately detect if a video has been
recompressed.
\\ ( https://arxiv.org/abs/2403.07891 ,  2438kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07892 (*cross-listing*)
Date: Sat, 3 Feb 2024 20:36:48 GMT   (13kb,D)

Title: Change Point Detection with Copula Entropy based Two-Sample Test
Authors: Jian Ma
Categories: stat.ME cs.LG
Comments: 9 pages, 1 figure, 4 tables
\\
  Change point detection is a typical task that aim to find changes in time
series and can be tackled with two-sample test. Copula Entropy is a
mathematical concept for measuring statistical independence and a two-sample
test based on it was introduced recently. In this paper we propose a
nonparametric multivariate method for multiple change point detection with the
copula entropy-based two-sample test. The single change point detection is
first proposed as a group of two-sample tests on every points of time series
data and the change point is considered as with the maximum of the test
statistics. The multiple change point detection is then proposed by combining
the single change point detection method with binary segmentation strategy. We
verified the effectiveness of our method and compared it with the other similar
methods on the simulated univariate and multivariate data and the Nile data.
\\ ( https://arxiv.org/abs/2403.07892 ,  13kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07901 (*cross-listing*)
Date: Mon, 26 Feb 2024 02:19:01 GMT   (4114kb,D)

Title: MIP: CLIP-based Image Reconstruction from PEFT Gradients
Authors: Peiheng Zhou, Ming Hu, Xiaofei Xie, Yihao Huang, Kangjie Chen,
  Mingsong Chen
Categories: cs.CV cs.LG
\\
  Contrastive Language-Image Pre-training (CLIP) model, as an effective
pre-trained multimodal neural network, has been widely used in distributed
machine learning tasks, especially Federated Learning (FL). Typically,
CLIP-based FL adopts Parameter-Efficient Fine-Tuning (PEFT) for model training,
which only fine-tunes adapter parameters or soft prompts rather than the full
parameters. Although PEFT is different from the traditional training mode, in
this paper, we theoretically analyze that the gradients of adapters or soft
prompts can still be used to perform image reconstruction attacks. Based on our
theoretical analysis, we propose Multm-In-Parvo (MIP), a proprietary
reconstruction attack method targeting CLIP-based distributed machine learning
architecture. Specifically, MIP can reconstruct CLIP training images according
to the gradients of soft prompts or an adapter. In addition, MIP includes a
label prediction strategy to accelerate convergence and an inverse gradient
estimation mechanism to avoid the vanishing gradient problem on the text
encoder. Experimental results show that MIP can effectively reconstruct
training images according to the gradients of soft prompts or adapters of CLIP
models.
\\ ( https://arxiv.org/abs/2403.07901 ,  4114kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07902 (*cross-listing*)
Date: Mon, 26 Feb 2024 05:21:21 GMT   (25712kb,D)

Title: DecompDiff: Diffusion Models with Decomposed Priors for Structure-Based
  Drug Design
Authors: Jiaqi Guan, Xiangxin Zhou, Yuwei Yang, Yu Bao, Jian Peng, Jianzhu Ma,
  Qiang Liu, Liang Wang, Quanquan Gu
Categories: q-bio.BM cs.LG
Comments: Accepted to ICML 2023
\\
  Designing 3D ligands within a target binding site is a fundamental task in
drug discovery. Existing structured-based drug design methods treat all ligand
atoms equally, which ignores different roles of atoms in the ligand for drug
design and can be less efficient for exploring the large drug-like molecule
space. In this paper, inspired by the convention in pharmaceutical practice, we
decompose the ligand molecule into two parts, namely arms and scaffold, and
propose a new diffusion model, DecompDiff, with decomposed priors over arms and
scaffold. In order to facilitate the decomposed generation and improve the
properties of the generated molecules, we incorporate both bond diffusion in
the model and additional validity guidance in the sampling phase. Extensive
experiments on CrossDocked2020 show that our approach achieves state-of-the-art
performance in generating high-affinity molecules while maintaining proper
molecular properties and conformational stability, with up to -8.39 Avg. Vina
Dock score and 24.5 Success Rate. The code is provided at
https://github.com/bytedance/DecompDiff
\\ ( https://arxiv.org/abs/2403.07902 ,  25712kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07903 (*cross-listing*)
Date: Mon, 26 Feb 2024 11:04:04 GMT   (15472kb,D)

Title: Multiple Access in the Era of Distributed Computing and Edge
  Intelligence
Authors: Nikos G. Evgenidis, Nikos A. Mitsiou, Vasiliki I. Koutsioumpa, Sotiris
  A. Tegos, Panagiotis D. Diamantoulakis and George K. Karagiannidis
Categories: cs.NI cs.LG eess.SP
\\
  This paper focuses on the latest research and innovations in fundamental
next-generation multiple access (NGMA) techniques and the coexistence with
other key technologies for the sixth generation (6G) of wireless networks. In
more detail, we first examine multi-access edge computing (MEC), which is
critical to meeting the growing demand for data processing and computational
capacity at the edge of the network, as well as network slicing. We then
explore over-the-air (OTA) computing, which is considered to be an approach
that provides fast and efficient computation of various functions. We also
explore semantic communications, identified as an effective way to improve
communication systems by focusing on the exchange of meaningful information,
thus minimizing unnecessary data and increasing efficiency. The
interrelationship between machine learning (ML) and multiple access
technologies is also reviewed, with an emphasis on federated learning,
federated distillation, split learning, reinforcement learning, and the
development of ML-based multiple access protocols. Finally, the concept of
digital twinning and its role in network management is discussed, highlighting
how virtual replication of physical networks can lead to improvements in
network efficiency and reliability.
\\ ( https://arxiv.org/abs/2403.07903 ,  15472kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07917 (*cross-listing*)
Date: Tue, 27 Feb 2024 15:59:15 GMT   (604kb,D)

Title: A Neural-Evolutionary Algorithm for Autonomous Transit Network Design
Authors: Andrew Holliday, Gregory Dudek
Categories: cs.NE cs.LG
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible. arXiv admin note: text overlap with arXiv:2306.00720
\\
  Planning a public transit network is a challenging optimization problem, but
essential in order to realize the benefits of autonomous buses. We propose a
novel algorithm for planning networks of routes for autonomous buses. We first
train a graph neural net model as a policy for constructing route networks, and
then use the policy as one of several mutation operators in a evolutionary
algorithm. We evaluate this algorithm on a standard set of benchmarks for
transit network design, and find that it outperforms the learned policy alone
by up to 20\% and a plain evolutionary algorithm approach by up to 53\% on
realistic benchmark instances.
\\ ( https://arxiv.org/abs/2403.07917 ,  604kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07925 (*cross-listing*)
Date: Thu, 29 Feb 2024 17:11:08 GMT   (8468kb,D)

Title: Physics-informed generative model for drug-like molecule conformers
Authors: David C. Williams and Neil Imana
Categories: q-bio.BM cs.LG physics.chem-ph
Comments: To appear in the Journal of Chemical Information and Modeling
\\
  We present a diffusion-based, generative model for conformer generation. Our
model is focused on the reproduction of bonded structure and is constructed
from the associated terms traditionally found in classical force fields to
ensure a physically relevant representation. Techniques in deep learning are
used to infer atom typing and geometric parameters from a training set.
Conformer sampling is achieved by taking advantage of recent advancements in
diffusion-based generation. By training on large, synthetic data sets of
diverse, drug-like molecules optimized with the semiempirical GFN2-xTB method,
high accuracy is achieved for bonded parameters, exceeding that of
conventional, knowledge-based methods. Results are also compared to
experimental structures from the Protein Databank (PDB) and Cambridge
Structural Database (CSD).
\\ ( https://arxiv.org/abs/2403.07925 ,  8468kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07926 (*cross-listing*)
Date: Thu, 29 Feb 2024 18:30:13 GMT   (6882kb,D)

Title: Value Prediction for Spatiotemporal Gait Data Using Deep Learning
Authors: Ryan Cavanagh, Jelena Trajkovic, Wenlu Zhang, I-Hung Khoo, and Vennila
  Krishnan
Categories: eess.SP cs.LG
Comments: 26 pages, 11 figures
ACM-class: J.3; J.7; C.3
\\
  Human gait has been commonly used for the diagnosis and evaluation of medical
conditions and for monitoring the progress during treatment and rehabilitation.
The use of wearable sensors that capture pressure or motion has yielded
techniques that analyze the gait data to aid recovery, identify activity
performed, or identify individuals. Deep learning, usually employing
classification, has been successfully utilized in a variety of applications
such as computer vision, biomedical imaging analysis, and natural language
processing. We expand the application of deep learning to value prediction of
time-series of spatiotemporal gait data. Moreover, we explore several deep
learning architectures (Recurrent Neural Networks (RNN) and RNN combined with
Convolutional Neural Networks (CNN)) to make short- and long-distance
predictions using two different experimental setups. Our results show that
short-distance prediction has an RMSE as low as 0.060675, and long-distance
prediction RMSE as low as 0.106365. Additionally, the results show that the
proposed deep learning models are capable of predicting the entire trial when
trained and validated using the trials from the same participant. The proposed,
customized models, used with value prediction open possibilities for additional
applications, such as fall prediction, in-home progress monitoring, aiding of
exoskeleton movement, and authentication.
\\ ( https://arxiv.org/abs/2403.07926 ,  6882kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07927 (*cross-listing*)
Date: Thu, 29 Feb 2024 19:40:32 GMT   (1826kb,D)

Title: Intelligent Monitoring Framework for Cloud Services: A Data-Driven
  Approach
Authors: Pooja Srinivas, Fiza Husain, Anjaly Parayil, Ayush Choure, Chetan
  Bansal, Saravan Rajmohan
Categories: cs.NI cs.LG
\\
  Cloud service owners need to continuously monitor their services to ensure
high availability and reliability. Gaps in monitoring can lead to delay in
incident detection and significant negative customer impact. Current process of
monitor creation is ad-hoc and reactive in nature. Developers create monitors
using their tribal knowledge and, primarily, a trial and error based process.
As a result, monitors often have incomplete coverage which leads to production
issues, or, redundancy which results in noise and wasted effort.
  In this work, we address this issue by proposing an intelligent monitoring
framework that recommends monitors for cloud services based on their service
properties. We start by mining the attributes of 30,000+ monitors from 791
production services at Microsoft and derive a structured ontology for monitors.
We focus on two crucial dimensions: what to monitor (resources) and which
metrics to monitor. We conduct an extensive empirical study and derive key
insights on the major classes of monitors employed by cloud services at
Microsoft, their associated dimensions, and the interrelationship between
service properties and this ontology. Using these insights, we propose a deep
learning based framework that recommends monitors based on the service
properties. Finally, we conduct a user study with engineers from Microsoft
which demonstrates the usefulness of the proposed framework. The proposed
framework along with the ontology driven projections, succeeded in creating
production quality recommendations for majority of resource classes. This was
also validated by the users from the study who rated the framework's usefulness
as 4.27 out of 5.
\\ ( https://arxiv.org/abs/2403.07927 ,  1826kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07933 (*cross-listing*)
Date: Mon, 4 Mar 2024 12:48:25 GMT   (122kb,D)

Title: Corruption-Robust Offline Two-Player Zero-Sum Markov Games
Authors: Andi Nika, Debmalya Mandal, Adish Singla and Goran Radanovi\'c
Categories: cs.GT cs.LG
\\
  We study data corruption robustness in offline two-player zero-sum Markov
games. Given a dataset of realized trajectories of two players, an adversary is
allowed to modify an $\epsilon$-fraction of it. The learner's goal is to
identify an approximate Nash Equilibrium policy pair from the corrupted data.
We consider this problem in linear Markov games under different degrees of data
coverage and corruption. We start by providing an information-theoretic lower
bound on the suboptimality gap of any learner. Next, we propose robust versions
of the Pessimistic Minimax Value Iteration algorithm, both under coverage on
the corrupted data and under coverage only on the clean data, and show that
they achieve (near)-optimal suboptimality gap bounds with respect to
$\epsilon$. We note that we are the first to provide such a characterization of
the problem of learning approximate Nash Equilibrium policies in offline
two-player zero-sum Markov games under data corruption.
\\ ( https://arxiv.org/abs/2403.07933 ,  122kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07940 (*cross-listing*)
Date: Sat, 9 Mar 2024 04:49:40 GMT   (815kb)

Title: Hair and scalp disease detection using deep learning
Authors: Kavita Sultanpure, Bhairavi Shirsath, Bhakti Bhande, Harshada Sawai,
  Srushti Gawade, Suraj Samgir
Categories: eess.IV cs.CV cs.LG
\\
  In recent years, there has been a notable advancement in the integration of
healthcare and technology, particularly evident in the field of medical image
analysis. This paper introduces a pioneering approach in dermatology,
presenting a robust method for the detection of hair and scalp diseases using
state-of-the-art deep learning techniques. Our methodology relies on
Convolutional Neural Networks (CNNs), well-known for their efficacy in image
recognition, to meticulously analyze images for various dermatological
conditions affecting the hair and scalp. Our proposed system represents a
significant advancement in dermatological diagnostics, offering a non-invasive
and highly efficient means of early detection and diagnosis. By leveraging the
capabilities of CNNs, our model holds the potential to revolutionize
dermatology, providing accessible and timely healthcare solutions. Furthermore,
the seamless integration of our trained model into a web-based platform
developed with the Django framework ensures broad accessibility and usability,
democratizing advanced medical diagnostics. The integration of machine learning
algorithms into web applications marks a pivotal moment in healthcare delivery,
promising empowerment for both healthcare providers and patients. Through the
synergy between technology and healthcare, our paper outlines the meticulous
methodology, technical intricacies, and promising future prospects of our
system. With a steadfast commitment to advancing healthcare frontiers, our goal
is to significantly contribute to leveraging technology for improved healthcare
outcomes globally. This endeavor underscores the profound impact of
technological innovation in shaping the future of healthcare delivery and
patient care, highlighting the transformative potential of our approach.
\\ ( https://arxiv.org/abs/2403.07940 ,  815kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07945 (*cross-listing*)
Date: Mon, 11 Mar 2024 03:44:18 GMT   (337kb,D)

Title: A Mathematical Framework for the Problem of Security for Cognition in
  Neurotechnology
Authors: Bryce Allen Bagley
Categories: cs.CR cs.CY cs.ET cs.LG q-bio.NC
MSC-class: 68Q99 (Primary), 68P27, 68Q07, 68Q09, 68T30, 68T05, 91E99, 92C75
  (Secondary)
ACM-class: F.m; I.2; J.2; J.3
\\
  The rapid advancement in neurotechnology in recent years has created an
emerging critical intersection between neurotechnology and security.
Implantable devices, non-invasive monitoring, and non-invasive therapies all
carry with them the prospect of violating the privacy and autonomy of
individuals' cognition. A growing number of scientists and physicians have made
calls to address this issue -- which we term Cognitive Security -- but applied
efforts have been limited. A major barrier hampering scientific and engineering
efforts to address Cognitive Security is the lack of a clear means of
describing and analyzing relevant problems. In this paper we develop Cognitive
Security, a mathematical framework which enables such description and analysis
by drawing on methods and results from multiple fields. We demonstrate certain
statistical properties which have significant implications for Cognitive
Security, and then present descriptions of the algorithmic problems faced by
attackers attempting to violate privacy and autonomy, and defenders attempting
to obstruct such attempts.
\\ ( https://arxiv.org/abs/2403.07945 ,  337kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07951 (*cross-listing*)
Date: Tue, 12 Mar 2024 02:28:29 GMT   (3345kb,D)

Title: SAMDA: Leveraging SAM on Few-Shot Domain Adaptation for Electronic
  Microscopy Segmentation
Authors: Yiran Wang, Li Xiao
Categories: eess.IV cs.CV cs.LG
\\
  It has been shown that traditional deep learning methods for electronic
microscopy segmentation usually suffer from low transferability when samples
and annotations are limited, while large-scale vision foundation models are
more robust when transferring between different domains but facing sub-optimal
improvement under fine-tuning. In this work, we present a new few-shot domain
adaptation framework SAMDA, which combines the Segment Anything Model(SAM) with
nnUNet in the embedding space to achieve high transferability and accuracy.
Specifically, we choose the Unet-based network as the "expert" component to
learn segmentation features efficiently and design a SAM-based adaptation
module as the "generic" component for domain transfer. By amalgamating the
"generic" and "expert" components, we mitigate the modality imbalance in the
complex pre-training knowledge inherent to large-scale Vision Foundation models
and the challenge of transferability inherent to traditional neural networks.
The effectiveness of our model is evaluated on two electron microscopic image
datasets with different modalities for mitochondria segmentation, which
improves the dice coefficient on the target domain by 6.7%. Also, the SAM-based
adaptor performs significantly better with only a single annotated image than
the 10-shot domain adaptation on nnUNet. We further verify our model on four
MRI datasets from different sources to prove its generalization ability.
\\ ( https://arxiv.org/abs/2403.07951 ,  3345kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07960 (*cross-listing*)
Date: Tue, 12 Mar 2024 09:37:20 GMT   (934kb,D)

Title: Unsupervised self-organising map of prostate cell Raman spectra shows
  disease-state subclustering
Authors: Daniel West, Susan Stepney, Y. Hancock
Categories: q-bio.QM cs.LG
Comments: 14 pages, 6 figures
\\
  Prostate cancer is a disease which poses an interesting clinical question:
should it be treated? A small subset of prostate cancers are aggressive and
require removal and treatment to prevent metastatic spread. However,
conventional diagnostics remain challenged to risk-stratify such patients,
hence, new methods of approach to biomolecularly subclassify the disease are
needed. Here we use an unsupervised, self-organising map approach to analyse
live-cell Raman spectroscopy data obtained from prostate cell-lines; our aim is
to test the feasibility of this method to differentiate, at the
single-cell-level, cancer from normal using high-dimensional datasets with
minimal preprocessing. The results demonstrate not only successful separation
of normal prostate and cancer cells, but also a new subclustering of the
prostate cancer cell-line into two groups. Initial analysis of the spectra from
each of the cancer subclusters demonstrates a differential expression of
lipids, which, against the normal control, may be linked to disease-related
changes in cellular signalling.
\\ ( https://arxiv.org/abs/2403.07960 ,  934kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07995 (*cross-listing*)
Date: Tue, 12 Mar 2024 18:03:08 GMT   (632kb)

Title: Motifs, Phrases, and Beyond: The Modelling of Structure in Symbolic
  Music Generation
Authors: Keshav Bhandari, Simon Colton
Categories: cs.SD cs.LG cs.SC eess.AS
Comments: Accepted to 13th International Conference on Artificial Intelligence
  in Music, Sound, Art and Design (EvoMUSART) 2024
\\
  Modelling musical structure is vital yet challenging for artificial
intelligence systems that generate symbolic music compositions. This literature
review dissects the evolution of techniques for incorporating coherent
structure, from symbolic approaches to foundational and transformative deep
learning methods that harness the power of computation and data across a wide
variety of training paradigms. In the later stages, we review an emerging
technique which we refer to as "sub-task decomposition" that involves
decomposing music generation into separate high-level structural planning and
content creation stages. Such systems incorporate some form of musical
knowledge or neuro-symbolic methods by extracting melodic skeletons or
structural templates to guide the generation. Progress is evident in capturing
motifs and repetitions across all three eras reviewed, yet modelling the
nuanced development of themes across extended compositions in the style of
human composers remains difficult. We outline several key future directions to
realize the synergistic benefits of combining approaches from all eras
examined.
\\ ( https://arxiv.org/abs/2403.07995 ,  632kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08016 (*cross-listing*)
Date: Tue, 12 Mar 2024 18:28:13 GMT   (17962kb,D)

Title: Aedes aegypti Egg Counting with Neural Networks for Object Detection
Authors: Micheli Nayara de Oliveira Vicente, Gabriel Toshio Hirokawa Higa,
  Jo\~ao Vitor de Andrade Porto, Higor Henrique, Picoli Nucci, Asser Botelho
  Santana, Karla Rejane de Andrade Porto, Antonia Railda Roel, Hemerson Pistori
Categories: eess.IV cs.CV cs.LG
\\
  Aedes aegypti is still one of the main concerns when it comes to disease
vectors. Among the many ways to deal with it, there are important protocols
that make use of egg numbers in ovitraps to calculate indices, such as the
LIRAa and the Breteau Index, which can provide information on predictable
outbursts and epidemics. Also, there are many research lines that require egg
numbers, specially when mass production of mosquitoes is needed. Egg counting
is a laborious and error-prone task that can be automated via computer
vision-based techniques, specially deep learning-based counting with object
detection. In this work, we propose a new dataset comprising field and
laboratory eggs, along with test results of three neural networks applied to
the task: Faster R-CNN, Side-Aware Boundary Localization and FoveaBox.
\\ ( https://arxiv.org/abs/2403.08016 ,  17962kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08042 (*cross-listing*)
Date: Tue, 12 Mar 2024 19:34:50 GMT   (3688kb)

Title: CT evaluation of 2D and 3D holistic deep learning methods for the
  volumetric segmentation of airway lesions
Authors: Amel Imene Hadj Bouzid, Baudouin Denis de Senneville, Fabien Baldacci,
  Pascal Desbarats, Patrick Berger, Ilyes Benlala, Ga\"el Dournes
Categories: eess.IV cs.CV cs.LG
Comments: 6 pages, 3 figures, 2 tables, IEEE International Symposium on
  Biomedical Imaging (ISBI) 2024
\\
  This research embarked on a comparative exploration of the holistic
segmentation capabilities of Convolutional Neural Networks (CNNs) in both 2D
and 3D formats, focusing on cystic fibrosis (CF) lesions. The study utilized
data from two CF reference centers, covering five major CF structural changes.
Initially, it compared the 2D and 3D models, highlighting the 3D model's
superior capability in capturing complex features like mucus plugs and
consolidations. To improve the 2D model's performance, a loss adapted to fine
structures segmentation was implemented and evaluated, significantly enhancing
its accuracy, though not surpassing the 3D model's performance. The models
underwent further validation through external evaluation against pulmonary
function tests (PFTs), confirming the robustness of the findings. Moreover,
this study went beyond comparing metrics; it also included comprehensive
assessments of the models' interpretability and reliability, providing valuable
insights for their clinical application.
\\ ( https://arxiv.org/abs/2403.08042 ,  3688kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08131 (*cross-listing*)
Date: Tue, 12 Mar 2024 23:39:43 GMT   (3199kb,D)

Title: Cost-Effective Methodology for Complex Tuning Searches in HPC:
  Navigating Interdependencies and Dimensionality
Authors: Adrian Perez Dieguez, Min Choi, Mahmut Okyay, Mauro Del Ben, Bryan M.
  Wong, Khaled Z. Ibrahim
Categories: cs.DC cs.LG
\\
  Tuning searches are pivotal in High-Performance Computing (HPC), addressing
complex optimization challenges in computational applications. The complexity
arises not only from finely tuning parameters within routines but also
potential interdependencies among them, rendering traditional optimization
methods inefficient. Instead of scrutinizing interdependencies among parameters
and routines, practitioners often face the dilemma of conducting independent
tuning searches for each routine, thereby overlooking interdependence, or
pursuing a more resource-intensive joint search for all routines. This decision
is driven by the consideration that some interdependence analysis and
high-dimensional decomposition techniques in literature may be prohibitively
expensive in HPC tuning searches. Our methodology adapts and refines these
methods to ensure computational feasibility while maximizing performance gains
in real-world scenarios. Our methodology leverages a cost-effective
interdependence analysis to decide whether to merge several tuning searches
into a joint search or conduct orthogonal searches. Tested on synthetic
functions with varying levels of parameter interdependence, our methodology
efficiently explores the search space. In comparison to
Bayesian-optimization-based full independent or fully joint searches, our
methodology suggested an optimized breakdown of independent and merged searches
that led to final configurations up to 8% more accurate, reducing the search
time by up to 95%. When applied to GPU-offloaded Real-Time Time-Dependent
Density Functional Theory (RT-TDDFT), an application in computational materials
science that challenges modern HPC autotuners, our methodology achieved an
effective tuning search. Its adaptability and efficiency extend beyond
RT-TDDFT, making it valuable for related applications in HPC.
\\ ( https://arxiv.org/abs/2403.08131 ,  3199kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08160 (*cross-listing*)
Date: Wed, 13 Mar 2024 00:59:25 GMT   (2966kb,D)

Title: Asymptotics of Random Feature Regression Beyond the Linear Scaling
  Regime
Authors: Hong Hu, Yue M. Lu, Theodor Misiakiewicz
Categories: stat.ML cs.LG math.ST stat.TH
Comments: 106 pages, 8 figures
\\
  Recent advances in machine learning have been achieved by using
overparametrized models trained until near interpolation of the training data.
It was shown, e.g., through the double descent phenomenon, that the number of
parameters is a poor proxy for the model complexity and generalization
capabilities. This leaves open the question of understanding the impact of
parametrization on the performance of these models. How does model complexity
and generalization depend on the number of parameters $p$? How should we choose
$p$ relative to the sample size $n$ to achieve optimal test error?
  In this paper, we investigate the example of random feature ridge regression
(RFRR). This model can be seen either as a finite-rank approximation to kernel
ridge regression (KRR), or as a simplified model for neural networks trained in
the so-called lazy regime. We consider covariates uniformly distributed on the
$d$-dimensional sphere and compute sharp asymptotics for the RFRR test error in
the high-dimensional polynomial scaling, where $p,n,d \to \infty$ while $p/
d^{\kappa_1}$ and $n / d^{\kappa_2}$ stay constant, for all $\kappa_1 ,
\kappa_2 \in \mathbb{R}_{>0}$. These asymptotics precisely characterize the
impact of the number of random features and regularization parameter on the
test performance. In particular, RFRR exhibits an intuitive trade-off between
approximation and generalization power. For $n = o(p)$, the sample size $n$ is
the bottleneck and RFRR achieves the same performance as KRR (which is
equivalent to taking $p = \infty$). On the other hand, if $p = o(n)$, the
number of random features $p$ is the limiting factor and RFRR test error
matches the approximation error of the random feature model class (akin to
taking $n = \infty$). Finally, a double descent appears at $n= p$, a phenomenon
that was previously only characterized in the linear scaling $\kappa_1 =
\kappa_2 = 1$.
\\ ( https://arxiv.org/abs/2403.08160 ,  2966kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08162 (*cross-listing*)
Date: Wed, 13 Mar 2024 01:18:55 GMT   (3915kb,D)

Title: Iterative Learning for Joint Image Denoising and Motion Artifact
  Correction of 3D Brain MRI
Authors: Lintao Zhang, Mengqi Wu, Lihong Wang, David C. Steffens, Guy G.
  Potter, Mingxia Liu
Categories: eess.IV cs.CV cs.LG
\\
  Image noise and motion artifacts greatly affect the quality of brain MRI and
negatively influence downstream medical image analysis. Previous studies often
focus on 2D methods that process each volumetric MR image slice-by-slice, thus
losing important 3D anatomical information. Additionally, these studies
generally treat image denoising and artifact correction as two standalone
tasks, without considering their potential relationship, especially on
low-quality images where severe noise and motion artifacts occur
simultaneously. To address these issues, we propose a Joint image Denoising and
motion Artifact Correction (JDAC) framework via iterative learning to handle
noisy MRIs with motion artifacts, consisting of an adaptive denoising model and
an anti-artifact model. In the adaptive denoising model, we first design a
novel noise level estimation strategy, and then adaptively reduce the noise
through a U-Net backbone with feature normalization conditioning on the
estimated noise variance. The anti-artifact model employs another U-Net for
eliminating motion artifacts, incorporating a novel gradient-based loss
function designed to maintain the integrity of brain anatomy during the motion
correction process. These two models are iteratively employed for joint image
denoising and artifact correction through an iterative learning framework. An
early stopping strategy depending on noise level estimation is applied to
accelerate the iteration process. The denoising model is trained with 9,544
T1-weighted MRIs with manually added Gaussian noise as supervision. The
anti-artifact model is trained on 552 T1-weighted MRIs with motion artifacts
and paired motion-free images. Experimental results on a public dataset and a
clinical study suggest the effectiveness of JDAC in both tasks of denoising and
motion artifact correction, compared with several state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.08162 ,  3915kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08164 (*cross-listing*)
Date: Wed, 13 Mar 2024 01:27:57 GMT   (2478kb,D)

Title: EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight
  Text-to-Speech
Authors: Ziqi Liang, Haoxiang Shi, Jiawei Wang, Keda Lu
Categories: cs.SD cs.LG eess.AS
Comments: Accepted by the 27th IEEE International Conference on Computer
  Supported Cooperative Work in Design (IEEE CSCWD 2024). arXiv admin note:
  substantial text overlap with arXiv:2211.01948
\\
  Recently, deep learning-based Text-to-Speech (TTS) systems have achieved
high-quality speech synthesis results. Recurrent neural networks have become a
standard modeling technique for sequential data in TTS systems and are widely
used. However, training a TTS model which includes RNN components requires
powerful GPU performance and takes a long time. In contrast, CNN-based sequence
synthesis techniques can significantly reduce the parameters and training time
of a TTS model while guaranteeing a certain performance due to their high
parallelism, which alleviate these economic costs of training. In this paper,
we propose a lightweight TTS system based on deep convolutional neural
networks, which is a two-stage training end-to-end TTS model and does not
employ any recurrent units. Our model consists of two stages: Text2Spectrum and
SSRN. The former is used to encode phonemes into a coarse mel spectrogram and
the latter is used to synthesize the complete spectrum from the coarse mel
spectrogram. Meanwhile, we improve the robustness of our model by a series of
data augmentations, such as noise suppression, time warping, frequency masking
and time masking, for solving the low resource mongolian problem. Experiments
show that our model can reduce the training time and parameters while ensuring
the quality and naturalness of the synthesized speech compared to using
mainstream TTS models. Our method uses NCMMSC2022-MTTSC Challenge dataset for
validation, which significantly reduces training time while maintaining a
certain accuracy.
\\ ( https://arxiv.org/abs/2403.08164 ,  2478kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08171 (*cross-listing*)
Date: Wed, 13 Mar 2024 01:51:30 GMT   (628kb,D)

Title: Tractable Local Equilibria in Non-Concave Games
Authors: Yang Cai, Constantinos Daskalakis, Haipeng Luo, Chen-Yu Wei, Weiqiang
  Zheng
Categories: cs.GT cs.LG
\\
  While Online Gradient Descent and other no-regret learning procedures are
known to efficiently converge to coarse correlated equilibrium in games where
each agent's utility is concave in their own strategy, this is not the case
when the utilities are non-concave, a situation that is common in machine
learning applications where the agents' strategies are parameterized by deep
neural networks, or the agents' utilities are computed by a neural network, or
both. Indeed, non-concave games present a host of game-theoretic and
optimization challenges: (i) Nash equilibria may fail to exist; (ii) local Nash
equilibria exist but are intractable; and (iii) mixed Nash, correlated, and
coarse correlated equilibria have infinite support in general, and are
intractable. To sidestep these challenges we propose a new solution concept,
termed $(\varepsilon, \Phi(\delta))$-local equilibrium, which generalizes local
Nash equilibrium in non-concave games, as well as (coarse) correlated
equilibrium in concave games. Importantly, we show that two instantiations of
this solution concept capture the convergence guarantees of Online Gradient
Descent and no-regret learning, which we show efficiently converge to this type
of equilibrium in non-concave games with smooth utilities.
\\ ( https://arxiv.org/abs/2403.08171 ,  628kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08203 (*cross-listing*)
Date: Wed, 13 Mar 2024 02:55:27 GMT   (33337kb,D)

Title: Learnable Community-Aware Transformer for Brain Connectome Analysis with
  Token Clustering
Authors: Yanting Yang, Beidi Zhao, Zhuohao Ni, Yize Zhao, Xiaoxiao Li
Categories: q-bio.NC cs.LG eess.IV
\\
  Neuroscientific research has revealed that the complex brain network can be
organized into distinct functional communities, each characterized by a
cohesive group of regions of interest (ROIs) with strong interconnections.
These communities play a crucial role in comprehending the functional
organization of the brain and its implications for neurological conditions,
including Autism Spectrum Disorder (ASD) and biological differences, such as in
gender. Traditional models have been constrained by the necessity of predefined
community clusters, limiting their flexibility and adaptability in deciphering
the brain's functional organization. Furthermore, these models were restricted
by a fixed number of communities, hindering their ability to accurately
represent the brain's dynamic nature. In this study, we present a token
clustering brain transformer-based model ($\texttt{TC-BrainTF}$) for joint
community clustering and classification. Our approach proposes a novel token
clustering (TC) module based on the transformer architecture, which utilizes
learnable prompt tokens with orthogonal loss where each ROI embedding is
projected onto the prompt embedding space, effectively clustering ROIs into
communities and reducing the dimensions of the node representation via merging
with communities. Our results demonstrate that our learnable community-aware
model $\texttt{TC-BrainTF}$ offers improved accuracy in identifying ASD and
classifying genders through rigorous testing on ABIDE and HCP datasets.
Additionally, the qualitative analysis on $\texttt{TC-BrainTF}$ has
demonstrated the effectiveness of the designed TC module and its relevance to
neuroscience interpretations.
\\ ( https://arxiv.org/abs/2403.08203 ,  33337kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08220 (*cross-listing*)
Date: Wed, 13 Mar 2024 03:45:14 GMT   (10154kb,D)

Title: Efficient geometric Markov chain Monte Carlo for nonlinear Bayesian
  inversion enabled by derivative-informed neural operators
Authors: Lianghao Cao, Thomas O'Leary-Roseberry, Omar Ghattas
Categories: math.NA cs.LG cs.NA stat.CO stat.ML
\\
  We propose an operator learning approach to accelerate geometric Markov chain
Monte Carlo (MCMC) for solving infinite-dimensional nonlinear Bayesian inverse
problems. While geometric MCMC employs high-quality proposals that adapt to
posterior local geometry, it requires computing local gradient and Hessian
information of the log-likelihood, incurring a high cost when the
parameter-to-observable (PtO) map is defined through expensive model
simulations. We consider a delayed-acceptance geometric MCMC method driven by a
neural operator surrogate of the PtO map, where the proposal is designed to
exploit fast surrogate approximations of the log-likelihood and,
simultaneously, its gradient and Hessian. To achieve a substantial speedup, the
surrogate needs to be accurate in predicting both the observable and its
parametric derivative (the derivative of the observable with respect to the
parameter). Training such a surrogate via conventional operator learning using
input--output samples often demands a prohibitively large number of model
simulations. In this work, we present an extension of derivative-informed
operator learning [O'Leary-Roseberry et al., J. Comput. Phys., 496 (2024)]
using input--output--derivative training samples. Such a learning method leads
to derivative-informed neural operator (DINO) surrogates that accurately
predict the observable and its parametric derivative at a significantly lower
training cost than the conventional method. Cost and error analysis for reduced
basis DINO surrogates are provided. Numerical studies on PDE-constrained
Bayesian inversion demonstrate that DINO-driven MCMC generates effective
posterior samples 3--9 times faster than geometric MCMC and 60--97 times faster
than prior geometry-based MCMC. Furthermore, the training cost of DINO
surrogates breaks even after collecting merely 10--25 effective posterior
samples compared to geometric MCMC.
\\ ( https://arxiv.org/abs/2403.08220 ,  10154kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08239 (*cross-listing*)
Date: Wed, 13 Mar 2024 04:45:40 GMT   (6372kb,D)

Title: Continuous Object State Recognition for Cooking Robots Using Pre-Trained
  Vision-Language Models and Black-box Optimization
Authors: Kento Kawaharazuka and Naoaki Kanazawa and Yoshiki Obinata and Kei
  Okada and Masayuki Inaba
Categories: cs.RO cs.CV cs.LG
Comments: accepted at IEEE Robotics and Automation Letters (RA-L), website -
  https://haraduka.github.io/continuous-state-recognition/
Journal-ref: 10.1109/LRA.2024.3375257
\\
  The state recognition of the environment and objects by robots is generally
based on the judgement of the current state as a classification problem. On the
other hand, state changes of food in cooking happen continuously and need to be
captured not only at a certain time point but also continuously over time. In
addition, the state changes of food are complex and cannot be easily described
by manual programming. Therefore, we propose a method to recognize the
continuous state changes of food for cooking robots through the spoken language
using pre-trained large-scale vision-language models. By using models that can
compute the similarity between images and texts continuously over time, we can
capture the state changes of food while cooking. We also show that by adjusting
the weighting of each text prompt based on fitting the similarity changes to a
sigmoid function and then performing black-box optimization, more accurate and
robust continuous state recognition can be achieved. We demonstrate the
effectiveness and limitations of this method by performing the recognition of
water boiling, butter melting, egg cooking, and onion stir-frying.
\\ ( https://arxiv.org/abs/2403.08239 ,  6372kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08246 (*cross-listing*)
Date: Wed, 13 Mar 2024 05:00:42 GMT   (239kb,D)

Title: Towards Unified Modeling for Positive and Negative Preferences in
  Sign-Aware Recommendation
Authors: Yuting Liu, Yizhou Dang, Yuliang Liang, Qiang Liu, Guibing Guo,
  Jianzhe Zhao, Xingwei Wang
Categories: cs.IR cs.LG cs.SI
\\
  Recently, sign-aware graph recommendation has drawn much attention as it will
learn users' negative preferences besides positive ones from both positive and
negative interactions (i.e., links in a graph) with items. To accommodate the
different semantics of negative and positive links, existing works utilize two
independent encoders to model users' positive and negative preferences,
respectively. However, these approaches cannot learn the negative preferences
from high-order heterogeneous interactions between users and items formed by
multiple links with different signs, resulting in inaccurate and incomplete
negative user preferences. To cope with these intractable issues, we propose a
novel \textbf{L}ight \textbf{S}igned \textbf{G}raph Convolution Network
specifically for \textbf{Rec}ommendation (\textbf{LSGRec}), which adopts a
unified modeling approach to simultaneously model high-order users' positive
and negative preferences on a signed user-item interaction graph. Specifically,
for the negative preferences within high-order heterogeneous interactions,
first-order negative preferences are captured by the negative links, while
high-order negative preferences are propagated along positive edges. Then,
recommendation results are generated based on positive preferences and
optimized with negative ones. Finally, we train representations of users and
items through different auxiliary tasks. Extensive experiments on three
real-world datasets demonstrate that our method outperforms existing baselines
regarding performance and computational efficiency. Our code is available at
\url{https://anonymous.4open.science/r/LSGRec-BB95}.
\\ ( https://arxiv.org/abs/2403.08246 ,  239kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08267 (*cross-listing*)
Date: Wed, 13 Mar 2024 05:35:55 GMT   (3175kb,D)

Title: SNOW-SCA: ML-assisted Side-Channel Attack on SNOW-V
Authors: Harshit Saurabh, Anupam Golder, Samarth Shivakumar Titti, Suparna
  Kundu, Chaoyun Li, Angshuman Karmakar, Debayan Das
Categories: cs.CR cs.LG cs.NI
ACM-class: E.3
\\
  This paper presents SNOW-SCA, the first power side-channel analysis (SCA)
attack of a 5G mobile communication security standard candidate, SNOW-V,
running on a 32-bit ARM Cortex-M4 microcontroller. First, we perform a generic
known-key correlation (KKC) analysis to identify the leakage points. Next, a
correlation power analysis (CPA) attack is performed, which reduces the attack
complexity to two key guesses for each key byte. The correct secret key is then
uniquely identified utilizing linear discriminant analysis (LDA). The profiled
SCA attack with LDA achieves 100% accuracy after training with $<200$ traces,
which means the attack succeeds with just a single trace. Overall, using the
\textit{combined CPA and LDA attack} model, the correct secret key byte is
recovered with <50 traces collected using the ChipWhisperer platform. The
entire 256-bit secret key of SNOW-V can be recovered incrementally using the
proposed SCA attack. Finally, we suggest low-overhead countermeasures that can
be used to prevent these SCA attacks.
\\ ( https://arxiv.org/abs/2403.08267 ,  3175kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08344 (*cross-listing*)
Date: Wed, 13 Mar 2024 08:49:40 GMT   (14960kb,D)

Title: STMPL: Human Soft-Tissue Simulation
Authors: Anton Agafonov and Lihi Zelnik-Manor
Categories: cs.CV cs.GR cs.LG
\\
  In various applications, such as virtual reality and gaming, simulating the
deformation of soft tissues in the human body during interactions with external
objects is essential. Traditionally, Finite Element Methods (FEM) have been
employed for this purpose, but they tend to be slow and resource-intensive. In
this paper, we propose a unified representation of human body shape and soft
tissue with a data-driven simulator of non-rigid deformations. This approach
enables rapid simulation of realistic interactions.
  Our method builds upon the SMPL model, which generates human body shapes
considering rigid transformations. We extend SMPL by incorporating a soft
tissue layer and an intuitive representation of external forces applied to the
body during object interactions. Specifically, we mapped the 3D body shape and
soft tissue and applied external forces to 2D UV maps. Leveraging a UNET
architecture designed for 2D data, our approach achieves high-accuracy
inference in real time. Our experiment shows that our method achieves plausible
deformation of the soft tissue layer, even for unseen scenarios.
\\ ( https://arxiv.org/abs/2403.08344 ,  14960kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08362 (*cross-listing*)
Date: Wed, 13 Mar 2024 09:22:30 GMT   (4779kb,D)

Title: Mean-Field Microcanonical Gradient Descent
Authors: Marcus H\"aggbom, Morten Karlsmark, Joakim and\'en
Categories: stat.ML cs.LG q-fin.ST stat.CO
\\
  Microcanonical gradient descent is a sampling procedure for energy-based
models allowing for efficient sampling of distributions in high dimension. It
works by transporting samples from a high-entropy distribution, such as
Gaussian white noise, to a low-energy region using gradient descent. We put
this model in the framework of normalizing flows, showing how it can often
overfit by losing an unnecessary amount of entropy in the descent. As a remedy,
we propose a mean-field microcanonical gradient descent that samples several
weakly coupled data points simultaneously, allowing for better control of the
entropy loss while paying little in terms of likelihood fit. We study these
models in the context of financial time series, illustrating the improvements
on both synthetic and real data.
\\ ( https://arxiv.org/abs/2403.08362 ,  4779kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08417 (*cross-listing*)
Date: Wed, 13 Mar 2024 11:05:40 GMT   (601kb)

Title: The Development and Performance of a Machine Learning Based Mobile
  Platform for Visually Determining the Etiology of Penile Pathology
Authors: Lao-Tzu Allan-Blitz, Sithira Ambepitiya, Raghavendra Tirupathi,
  Jeffrey D. Klausner, Yudara Kularathne
Categories: eess.IV cs.CV cs.LG
Comments: 12 pages, 2 figure, 2 tables
\\
  Machine-learning algorithms can facilitate low-cost, user-guided visual
diagnostic platforms for addressing disparities in access to sexual health
services. We developed a clinical image dataset using original and augmented
images for five penile diseases: herpes eruption, syphilitic chancres, penile
candidiasis, penile cancer, and genital warts. We used a U-net architecture
model for semantic pixel segmentation into background or subject image, the
Inception-ResNet version 2 neural architecture to classify each pixel as
diseased or non-diseased, and a salience map using GradCAM++. We trained the
model on a random 91% sample of the image database using 150 epochs per image,
and evaluated the model on the remaining 9% of images, assessing recall (or
sensitivity), precision, specificity, and F1-score (accuracy). Of the 239
images in the validation dataset, 45 (18.8%) were of genital warts, 43 (18.0%)
were of HSV infection, 29 (12.1%) were of penile cancer, 40 (16.7%) were of
penile candidiasis, 37 (15.5%) were of syphilitic chancres, and 45 (18.8%) were
of non-diseased penises. The overall accuracy of the model for correctly
classifying the diseased image was 0.944. Between July 1st and October 1st
2023, there were 2,640 unique users of the mobile platform. Among a random
sample of submissions (n=437), 271 (62.0%) were from the United States, 64
(14.6%) from Singapore, 41 (9.4%) from Candia, 40 (9.2%) from the United
Kingdom, and 21 (4.8%) from Vietnam. The majority (n=277 [63.4%]) were between
18 and 30 years old. We report on the development of a machine-learning model
for classifying five penile diseases, which demonstrated excellent performance
on a validation dataset. That model is currently in use globally and has the
potential to improve access to diagnostic services for penile diseases.
\\ ( https://arxiv.org/abs/2403.08417 ,  601kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08444 (*cross-listing*)
Date: Wed, 13 Mar 2024 11:56:10 GMT   (532kb,D)

Title: COSTREAM: Learned Cost Models for Operator Placement in Edge-Cloud
  Environments
Authors: Roman Heinrich, Carsten Binnig, Harald Kornmayer, Manisha Luthra
Categories: cs.DC cs.DB cs.LG
Comments: This paper has been accepted by IEEE ICDE 2024
\\
  In this work, we present COSTREAM, a novel learned cost model for Distributed
Stream Processing Systems that provides accurate predictions of the execution
costs of a streaming query in an edge-cloud environment. The cost model can be
used to find an initial placement of operators across heterogeneous hardware,
which is particularly important in these environments. In our evaluation, we
demonstrate that COSTREAM can produce highly accurate cost estimates for the
initial operator placement and even generalize to unseen placements, queries,
and hardware. When using COSTREAM to optimize the placements of streaming
operators, a median speed-up of around 21x can be achieved compared to
baselines.
\\ ( https://arxiv.org/abs/2403.08444 ,  532kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08464 (*cross-listing*)
Date: Wed, 13 Mar 2024 12:26:55 GMT   (19626kb,D)

Title: Diffusion Models with Implicit Guidance for Medical Anomaly Detection
Authors: Cosmin I. Bercea and Benedikt Wiestler and Daniel Rueckert and Julia
  A. Schnabel
Categories: eess.IV cs.CV cs.LG
\\
  Diffusion models have advanced unsupervised anomaly detection by improving
the transformation of pathological images into pseudo-healthy equivalents.
Nonetheless, standard approaches may compromise critical information during
pathology removal, leading to restorations that do not align with unaffected
regions in the original scans. Such discrepancies can inadvertently increase
false positive rates and reduce specificity, complicating radiological
evaluations. This paper introduces Temporal Harmonization for Optimal
Restoration (THOR), which refines the de-noising process by integrating
implicit guidance through temporal anomaly maps. THOR aims to preserve the
integrity of healthy tissue in areas unaffected by pathology. Comparative
evaluations show that THOR surpasses existing diffusion-based methods in
detecting and segmenting anomalies in brain MRIs and wrist X-rays. Code:
https://github.com/ci-ber/THOR_DDPM.
\\ ( https://arxiv.org/abs/2403.08464 ,  19626kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08477 (*cross-listing*)
Date: Wed, 13 Mar 2024 12:46:03 GMT   (1540kb,D)

Title: Unleashing the Power of Meta-tuning for Few-shot Generalization Through
  Sparse Interpolated Experts
Authors: Shengzhuang Chen, Jihoon Tack, Yunqiao Yang, Yee Whye Teh, Jonathan
  Richard Schwarz, Ying Wei
Categories: cs.CV cs.LG
Comments: 18 pages, preprint
\\
  Conventional wisdom suggests parameter-efficient fine-tuning of foundation
models as the state-of-the-art method for transfer learning in vision,
replacing the rich literature of alternatives such as meta-learning. In trying
to harness the best of both worlds, meta-tuning introduces a subsequent
optimization stage of foundation models but has so far only shown limited
success and crucially tends to underperform on out-of-domain (OOD) tasks. In
this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse
mixture-of-experts approaches and trained to isolate subsets of pre-trained
parameters automatically for meta-tuning on each task. SMAT successfully
overcomes OOD sensitivity and delivers on the promise of enhancing the transfer
abilities of vision foundation models beyond parameter-efficient finetuning. We
establish new state-of-the-art results on a challenging combination of
Meta-Dataset augmented with additional OOD tasks in both zero-shot and
gradient-based adaptation settings. In addition, we provide a thorough analysis
of the superiority of learned over hand-designed sparsity patterns for sparse
expert methods and the pivotal importance of the sparsity level in balancing
between in-domain and out-of-domain generalization. Our code is publicly
available.
\\ ( https://arxiv.org/abs/2403.08477 ,  1540kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08525 (*cross-listing*)
Date: Wed, 13 Mar 2024 13:33:35 GMT   (1097kb,D)

Title: From Weak to Strong Sound Event Labels using Adaptive Change-Point
  Detection and Active Learning
Authors: John Martinsson, Olof Mogren, Maria Sandsten, Tuomas Virtanen
Categories: cs.SD cs.LG
Comments: Under review at EUSIPCO 2024
\\
  In this work we propose an audio recording segmentation method based on an
adaptive change point detection (A-CPD) for machine guided weak label
annotation of audio recording segments. The goal is to maximize the amount of
information gained about the temporal activation's of the target sounds. For
each unlabeled audio recording, we use a prediction model to derive a
probability curve used to guide annotation. The prediction model is initially
pre-trained on available annotated sound event data with classes that are
disjoint from the classes in the unlabeled dataset. The prediction model then
gradually adapts to the annotations provided by the annotator in an active
learning loop. The queries used to guide the weak label annotator towards
strong labels are derived using change point detection on these probabilities.
We show that it is possible to derive strong labels of high quality even with a
limited annotation budget, and show favorable results for A-CPD when compared
to two baseline query strategies.
\\ ( https://arxiv.org/abs/2403.08525 ,  1097kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08553 (*cross-listing*)
Date: Wed, 13 Mar 2024 14:06:18 GMT   (91kb,D)

Title: Regret Analysis of Policy Optimization over Submanifolds for Linearly
  Constrained Online LQG
Authors: Ting-Jui Chang and Shahin Shahrampour
Categories: math.OC cs.LG cs.SY eess.SY
\\
  Recent advancement in online optimization and control has provided novel
tools to study online linear quadratic regulator (LQR) problems, where cost
matrices are varying adversarially over time. However, the controller
parameterization of existing works may not satisfy practical conditions like
sparsity due to physical connections. In this work, we study online linear
quadratic Gaussian problems with a given linear constraint imposed on the
controller. Inspired by the recent work of [1] which proposed, for a linearly
constrained policy optimization of an offline LQR, a second order method
equipped with a Riemannian metric that emerges naturally in the context of
optimal control problems, we propose online optimistic Newton on manifold
(OONM) which provides an online controller based on the prediction on the first
and second order information of the function sequence. To quantify the proposed
algorithm, we leverage the notion of regret defined as the sub-optimality of
its cumulative cost to that of a (locally) minimizing controller sequence and
provide the regret bound in terms of the path-length of the minimizer sequence.
Simulation results are also provided to verify the property of OONM.
\\ ( https://arxiv.org/abs/2403.08553 ,  91kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08568 (*cross-listing*)
Date: Wed, 13 Mar 2024 14:24:09 GMT   (722kb,D)

Title: Consistent Prompting for Rehearsal-Free Continual Learning
Authors: Zhanxin Gao, Jun Cen, Xiaobin Chang
Categories: cs.CV cs.LG
\\
  Continual learning empowers models to adapt autonomously to the ever-changing
environment or data streams without forgetting old knowledge. Prompt-based
approaches are built on frozen pre-trained models to learn the task-specific
prompts and classifiers efficiently. Existing prompt-based methods are
inconsistent between training and testing, limiting their effectiveness. Two
types of inconsistency are revealed. Test predictions are made from all
classifiers while training only focuses on the current task classifier without
holistic alignment, leading to Classifier inconsistency. Prompt inconsistency
indicates that the prompt selected during testing may not correspond to the one
associated with this task during training. In this paper, we propose a novel
prompt-based method, Consistent Prompting (CPrompt), for more aligned training
and testing. Specifically, all existing classifiers are exposed to prompt
training, resulting in classifier consistency learning. In addition, prompt
consistency learning is proposed to enhance prediction robustness and boost
prompt selection accuracy. Our Consistent Prompting surpasses its prompt-based
counterparts and achieves state-of-the-art performance on multiple continual
learning benchmarks. Detailed analysis shows that improvements come from more
consistent training and testing.
\\ ( https://arxiv.org/abs/2403.08568 ,  722kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08584 (*cross-listing*)
Date: Wed, 13 Mar 2024 14:37:00 GMT   (6396kb,D)

Title: Local Binary and Multiclass SVMs Trained on a Quantum Annealer
Authors: Enrico Zardini, Amer Delilbasic, Enrico Blanzieri, Gabriele Cavallaro,
  Davide Pastorello
Categories: cs.ET cs.LG quant-ph
Comments: 12 pages, 1 figure, 11 tables
\\
  Support vector machines (SVMs) are widely used machine learning models (e.g.,
in remote sensing), with formulations for both classification and regression
tasks. In the last years, with the advent of working quantum annealers, hybrid
SVM models characterised by quantum training and classical execution have been
introduced. These models have demonstrated comparable performance to their
classical counterparts. However, they are limited in the training set size due
to the restricted connectivity of the current quantum annealers. Hence, to take
advantage of large datasets (like those related to Earth observation), a
strategy is required. In the classical domain, local SVMs, namely, SVMs trained
on the data samples selected by a k-nearest neighbors model, have already
proven successful. Here, the local application of quantum-trained SVM models is
proposed and empirically assessed. In particular, this approach allows
overcoming the constraints on the training set size of the quantum-trained
models while enhancing their performance. In practice, the FaLK-SVM method,
designed for efficient local SVMs, has been combined with quantum-trained SVM
models for binary and multiclass classification. In addition, for comparison,
FaLK-SVM has been interfaced for the first time with a classical single-step
multiclass SVM model (CS SVM). Concerning the empirical evaluation, D-Wave's
quantum annealers and real-world datasets taken from the remote sensing domain
have been employed. The results have shown the effectiveness and scalability of
the proposed approach, but also its practical applicability in a real-world
large-scale scenario.
\\ ( https://arxiv.org/abs/2403.08584 ,  6396kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08627 (*cross-listing*)
Date: Wed, 13 Mar 2024 15:40:17 GMT   (1819kb,D)

Title: Multifidelity linear regression for scientific machine learning from
  scarce data
Authors: Elizabeth Qian, Anirban Chaudhuri, Dayoung Kang, Vignesh Sella
Categories: stat.ML cs.CE cs.LG
\\
  Machine learning (ML) methods, which fit to data the parameters of a given
parameterized model class, have garnered significant interest as potential
methods for learning surrogate models for complex engineering systems for which
traditional simulation is expensive. However, in many scientific and
engineering settings, generating high-fidelity data on which to train ML models
is expensive, and the available budget for generating training data is limited.
ML models trained on the resulting scarce high-fidelity data have high variance
and are sensitive to vagaries of the training data set. We propose a new
multifidelity training approach for scientific machine learning that exploits
the scientific context where data of varying fidelities and costs are
available; for example high-fidelity data may be generated by an expensive
fully resolved physics simulation whereas lower-fidelity data may arise from a
cheaper model based on simplifying assumptions. We use the multifidelity data
to define new multifidelity Monte Carlo estimators for the unknown parameters
of linear regression models, and provide theoretical analyses that guarantee
the approach's accuracy and improved robustness to small training budgets.
Numerical results verify the theoretical analysis and demonstrate that
multifidelity learned models trained on scarce high-fidelity data and
additional low-fidelity data achieve order-of-magnitude lower model variance
than standard models trained on only high-fidelity data of comparable cost.
This illustrates that in the scarce data regime, our multifidelity training
strategy yields models with lower expected error than standard training
approaches.
\\ ( https://arxiv.org/abs/2403.08627 ,  1819kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08630 (*cross-listing*)
Date: Wed, 13 Mar 2024 15:45:29 GMT   (99kb,D)

Title: Leveraging Non-Decimated Wavelet Packet Features and Transformer Models
  for Time Series Forecasting
Authors: Guy P Nason and James L. Wei
Categories: stat.ME cs.LG
MSC-class: 62M10, 62M45
\\
  This article combines wavelet analysis techniques with machine learning
methods for univariate time series forecasting, focusing on three main
contributions. Firstly, we consider the use of Daubechies wavelets with
different numbers of vanishing moments as input features to both non-temporal
and temporal forecasting methods, by selecting these numbers during the
cross-validation phase. Secondly, we compare the use of both the non-decimated
wavelet transform and the non-decimated wavelet packet transform for computing
these features, the latter providing a much larger set of potentially useful
coefficient vectors. The wavelet coefficients are computed using a shifted
version of the typical pyramidal algorithm to ensure no leakage of future
information into these inputs. Thirdly, we evaluate the use of these wavelet
features on a significantly wider set of forecasting methods than previous
studies, including both temporal and non-temporal models, and both statistical
and deep learning-based methods. The latter include state-of-the-art
transformer-based neural network architectures. Our experiments suggest
significant benefit in replacing higher-order lagged features with wavelet
features across all examined non-temporal methods for one-step-forward
forecasting, and modest benefit when used as inputs for temporal deep
learning-based models for long-horizon forecasting.
\\ ( https://arxiv.org/abs/2403.08630 ,  99kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08632 (*cross-listing*)
Date: Wed, 13 Mar 2024 15:46:37 GMT   (2198kb,D)

Title: A Decade's Battle on Dataset Bias: Are We There Yet?
Authors: Zhuang Liu, Kaiming He
Categories: cs.CV cs.LG
\\
  We revisit the "dataset classification" experiment suggested by Torralba and
Efros a decade ago, in the new era with large-scale, diverse, and hopefully
less biased datasets as well as more capable neural network architectures.
Surprisingly, we observe that modern neural networks can achieve excellent
accuracy in classifying which dataset an image is from: e.g., we report 84.7%
accuracy on held-out validation data for the three-way classification problem
consisting of the YFCC, CC, and DataComp datasets. Our further experiments show
that such a dataset classifier could learn semantic features that are
generalizable and transferable, which cannot be simply explained by
memorization. We hope our discovery will inspire the community to rethink the
issue involving dataset bias and model capabilities.
\\ ( https://arxiv.org/abs/2403.08632 ,  2198kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08662 (*cross-listing*)
Date: Wed, 13 Mar 2024 16:16:20 GMT   (211kb,D)

Title: Self-Supervised Learning for Covariance Estimation
Authors: Tzvi Diskin and Ami Wiesel
Categories: eess.SP cs.LG
\\
  We consider the use of deep learning for covariance estimation. We propose to
globally learn a neural network that will then be applied locally at inference
time. Leveraging recent advancements in self-supervised foundational models, we
train the network without any labeling by simply masking different samples and
learning to predict their covariance given their surrounding neighbors. The
architecture is based on the popular attention mechanism. Its main advantage
over classical methods is the automatic exploitation of global characteristics
without any distributional assumptions or regularization. It can be pre-trained
as a foundation model and then be repurposed for various downstream tasks,
e.g., adaptive target detection in radar or hyperspectral imagery.
\\ ( https://arxiv.org/abs/2403.08662 ,  211kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08687 (*cross-listing*)
Date: Wed, 13 Mar 2024 16:44:36 GMT   (1411kb,D)

Title: Digital Twin-assisted Reinforcement Learning for Resource-aware
  Microservice Offloading in Edge Computing
Authors: Xiangchun Chen, Jiannong Cao, Zhixuan Liang, Yuvraj Sahni, Mingjin
  Zhang
Categories: cs.NI cs.LG
Comments: 9 pages, 5 figures
Journal-ref: 2023 IEEE 20th International Conference on Mobile Ad Hoc and Smart
  Systems (MASS), Toronto, ON, Canada, 2023, pp. 28-36
DOI: 10.1109/MASS58611.2023.00012
\\
  Collaborative edge computing (CEC) has emerged as a promising paradigm,
enabling edge nodes to collaborate and execute microservices from end devices.
Microservice offloading, a fundamentally important problem, decides when and
where microservices are executed upon the arrival of services. However, the
dynamic nature of the real-world CEC environment often leads to inefficient
microservice offloading strategies, resulting in underutilized resources and
network congestion. To address this challenge, we formulate an online joint
microservice offloading and bandwidth allocation problem, JMOBA, to minimize
the average completion time of services. In this paper, we introduce a novel
microservice offloading algorithm, DTDRLMO, which leverages deep reinforcement
learning (DRL) and digital twin technology. Specifically, we employ digital
twin techniques to predict and adapt to changing edge node loads and network
conditions of CEC in real-time. Furthermore, this approach enables the
generation of an efficient offloading plan, selecting the most suitable edge
node for each microservice. Simulation results on real-world and synthetic
datasets demonstrate that DTDRLMO outperforms heuristic and learning-based
methods in average service completion time.
\\ ( https://arxiv.org/abs/2403.08687 ,  1411kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08700 (*cross-listing*)
Date: Wed, 13 Mar 2024 17:04:56 GMT   (861kb,D)

Title: Diffusion-based Iterative Counterfactual Explanations for Fetal
  Ultrasound Image Quality Assessment
Authors: Paraskevas Pegios, Manxi Lin, Nina Weng, Morten Bo S{\o}ndergaard
  Svendsen, Zahra Bashir, Siavash Bigdeli, Anders Nymark Christensen, Martin
  Tolsgaard and Aasa Feragen
Categories: eess.IV cs.CV cs.HC cs.LG
\\
  Obstetric ultrasound image quality is crucial for accurate diagnosis and
monitoring of fetal health. However, producing high-quality standard planes is
difficult, influenced by the sonographer's expertise and factors like the
maternal BMI or the fetus dynamics. In this work, we propose using
diffusion-based counterfactual explainable AI to generate realistic
high-quality standard planes from low-quality non-standard ones. Through
quantitative and qualitative evaluation, we demonstrate the effectiveness of
our method in producing plausible counterfactuals of increased quality. This
shows future promise both for enhancing training of clinicians by providing
visual feedback, as well as for improving image quality and, consequently,
downstream diagnosis and monitoring.
\\ ( https://arxiv.org/abs/2403.08700 ,  861kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08741 (*cross-listing*)
Date: Wed, 13 Mar 2024 17:44:16 GMT   (102kb)

Title: Learning How to Strategically Disclose Information
Authors: Raj Kiriti Velicheti, Melih Bastopcu, S. Rasoul Etesami, Tamer
  Ba\c{s}ar
Categories: cs.GT cs.IT cs.LG cs.SY eess.SY math.IT math.OC
\\
  Strategic information disclosure, in its simplest form, considers a game
between an information provider (sender) who has access to some private
information that an information receiver is interested in. While the receiver
takes an action that affects the utilities of both players, the sender can
design information (or modify beliefs) of the receiver through signal
commitment, hence posing a Stackelberg game. However, obtaining a Stackelberg
equilibrium for this game traditionally requires the sender to have access to
the receiver's objective. In this work, we consider an online version of
information design where a sender interacts with a receiver of an unknown type
who is adversarially chosen at each round. Restricting attention to Gaussian
prior and quadratic costs for the sender and the receiver, we show that
$\mathcal{O}(\sqrt{T})$ regret is achievable with full information feedback,
where $T$ is the total number of interactions between the sender and the
receiver. Further, we propose a novel parametrization that allows the sender to
achieve $\mathcal{O}(\sqrt{T})$ regret for a general convex utility function.
We then consider the Bayesian Persuasion problem with an additional cost term
in the objective function, which penalizes signaling policies that are more
informative and obtain $\mathcal{O}(\log(T))$ regret. Finally, we establish a
sublinear regret bound for the partial information feedback setting and provide
simulations to support our theoretical results.
\\ ( https://arxiv.org/abs/2403.08741 ,  102kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08750 (*cross-listing*)
Date: Wed, 13 Mar 2024 17:51:02 GMT   (29kb)

Title: Neural reproducing kernel Banach spaces and representer theorems for
  deep networks
Authors: Francesca Bartolucci, Ernesto De Vito, Lorenzo Rosasco, Stefano
  Vigogna
Categories: stat.ML cs.LG math.FA
\\
  Studying the function spaces defined by neural networks helps to understand
the corresponding learning models and their inductive bias. While in some
limits neural networks correspond to function spaces that are reproducing
kernel Hilbert spaces, these regimes do not capture the properties of the
networks used in practice. In contrast, in this paper we show that deep neural
networks define suitable reproducing kernel Banach spaces.
  These spaces are equipped with norms that enforce a form of sparsity,
enabling them to adapt to potential latent structures within the input data and
their representations. In particular, leveraging the theory of reproducing
kernel Banach spaces, combined with variational results, we derive representer
theorems that justify the finite architectures commonly employed in
applications. Our study extends analogous results for shallow networks and can
be seen as a step towards considering more practically plausible neural
architectures.
\\ ( https://arxiv.org/abs/2403.08750 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08757 (*cross-listing*)
Date: Wed, 13 Mar 2024 17:55:34 GMT   (866kb,D)

Title: Efficient Combinatorial Optimization via Heat Diffusion
Authors: Hengyuan Ma, Wenlian Lu, Jianfeng Feng
Categories: stat.ML cs.LG math.CO physics.app-ph
\\
  Combinatorial optimization problems are widespread but inherently challenging
due to their discrete nature.The primary limitation of existing methods is that
they can only access a small fraction of the solution space at each iteration,
resulting in limited efficiency for searching the global optimal. To overcome
this challenge, diverging from conventional efforts of expanding the solver's
search scope, we focus on enabling information to actively propagate to the
solver through heat diffusion. By transforming the target function while
preserving its optima, heat diffusion facilitates information flow from distant
regions to the solver, providing more efficient navigation. Utilizing heat
diffusion, we propose a framework for solving general combinatorial
optimization problems. The proposed methodology demonstrates superior
performance across a range of the most challenging and widely encountered
combinatorial optimizations. Echoing recent advancements in harnessing
thermodynamics for generative artificial intelligence, our study further
reveals its significant potential in advancing combinatorial optimization.
\\ ( https://arxiv.org/abs/2403.08757 ,  866kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2011.12340
replaced with revised version Wed, 13 Mar 2024 17:01:54 GMT   (13143kb,D)

Title: mForms : Multimodal Form-Filling with Question Answering
Authors: Larry Heck, Simon Heck, Anirudh Sundar
Categories: cs.AI
Comments: 5 pages, 6 figures, 4 tables
\\ ( https://arxiv.org/abs/2011.12340 ,  13143kb)
------------------------------------------------------------------------------
\\
arXiv:2303.15027
replaced with revised version Tue, 12 Mar 2024 20:14:45 GMT   (2383kb,D)

Title: A Survey on Causal Discovery Methods for I.I.D. and Time Series Data
Authors: Uzma Hasan, Emam Hossain, Md Osman Gani
Categories: cs.AI
Comments: Published (05 Sept 2023) in Transactions on Machine Learning Research
  (TMLR)
\\ ( https://arxiv.org/abs/2303.15027 ,  2383kb)
------------------------------------------------------------------------------
\\
arXiv:2304.12479
replaced with revised version Wed, 13 Mar 2024 16:47:04 GMT   (670kb,D)

Title: AGI: Artificial General Intelligence for Education
Authors: Ehsan Latif, Gengchen Mai, Matthew Nyaaba, Xuansheng Wu, Ninghao Liu,
  Guoyu Lu, Sheng Li, Tianming Liu, and Xiaoming Zhai
Categories: cs.AI
Comments: Position Paper on AGI for Education, Submitted to Technology and
  Society
\\ ( https://arxiv.org/abs/2304.12479 ,  670kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05984
replaced with revised version Wed, 13 Mar 2024 13:56:05 GMT   (1961kb,D)

Title: Contrastive Explanations of Centralized Multi-agent Optimization
  Solutions
Authors: Parisa Zehtabi, Alberto Pozanco, Ayala Bloch, Daniel Borrajo, Sarit
  Kraus
Categories: cs.AI
Comments: Paper accepted at ICAPS 2024. This is a extended version that
  includes Supplementary Material
\\ ( https://arxiv.org/abs/2308.05984 ,  1961kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13658
replaced with revised version Wed, 13 Mar 2024 02:08:34 GMT   (1163kb,D)

Title: Generating and Explaining Corner Cases Using Learnt Probabilistic Lane
  Graphs
Authors: Enrik Maci, Rhys Howard, Lars Kunze
Categories: cs.AI cs.RO
Comments: 8 Pages, 3 Figures, 1 Table, Published in the Proceedings of the 26th
  IEEE International Conference on Intelligent Transport Systems (2023), Final
  submission version with added IEEE copyright notice
ACM-class: E.1; G.1.1; G.3; I.2.6; I.2.9; I.6.0
Journal-ref: 2023 IEEE 26th International Conference on Intelligent
  Transportation Systems (ITSC), Bilbao, Spain, 2023, pp. 4201-4208
DOI: 10.1109/ITSC57777.2023.10422229
\\ ( https://arxiv.org/abs/2308.13658 ,  1163kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05657
replaced with revised version Wed, 13 Mar 2024 10:54:21 GMT   (2558kb,D)

Title: Agent Lumos: Unified and Modular Training for Open-Source Language
  Agents
Authors: Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei
  Chang, Yejin Choi, Bill Yuchen Lin
Categories: cs.AI cs.CL cs.LG
Comments: Project website: https://allenai.github.io/lumos/
\\ ( https://arxiv.org/abs/2311.05657 ,  2558kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01678
replaced with revised version Wed, 13 Mar 2024 13:02:57 GMT   (206kb,D)

Title: Jellyfish: A Large Language Model for Data Preprocessing
Authors: Haochen Zhang, Yuyang Dong, Chuan Xiao, Masafumi Oyamada
Categories: cs.AI cs.CL cs.DB cs.LG
Comments: a.k.a. "Jellyfish: Instruction-Tuning Local Large Language Models for
  Data Preprocessing''
\\ ( https://arxiv.org/abs/2312.01678 ,  206kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14106
replaced with revised version Wed, 13 Mar 2024 01:37:55 GMT   (488kb,D)

Title: Learning Human-like Representations to Enable Learning Human Values
Authors: Andrea Wynn, Ilia Sucholutsky, Thomas L. Griffiths
Categories: cs.AI cs.LG
Comments: Paper accepted in Human-Centric Representation Learning workshop at
  AAAI 2024 (https://hcrl-workshop.github.io/2024/)
\\ ( https://arxiv.org/abs/2312.14106 ,  488kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03165
replaced with revised version Wed, 13 Mar 2024 05:46:39 GMT   (487kb)

Title: Leveraging Federated Learning and Edge Computing for Recommendation
  Systems within Cloud Computing Networks
Authors: Yaqian Qi, Yuan Feng, Xiangxiang Wang, Hanzhe Li, Jingxiao Tian
Categories: cs.AI
\\ ( https://arxiv.org/abs/2403.03165 ,  487kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06483
replaced with revised version Wed, 13 Mar 2024 02:17:25 GMT   (748kb)

Title: The negation of permutation mass function
Authors: Yongchuan Tang, Rongfei Li
Categories: cs.AI cs.IT math.IT
\\ ( https://arxiv.org/abs/2403.06483 ,  748kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07548
replaced with revised version Wed, 13 Mar 2024 02:31:47 GMT   (12912kb,D)

Title: Online Continual Learning For Interactive Instruction Following Agents
Authors: Byeonghwi Kim, Minhyuk Seo, Jonghyun Choi
Categories: cs.AI cs.LG cs.RO
Comments: ICLR 2024 (Project page:
  https://bhkim94.github.io/projects/CL-ALFRED)
\\ ( https://arxiv.org/abs/2403.07548 ,  12912kb)
------------------------------------------------------------------------------
\\
arXiv:2203.11155
replaced with revised version Wed, 13 Mar 2024 14:46:05 GMT   (324kb)

Title: A New Quantum CNN Model for Image Classification
Authors: X. Q. Zhao, T. L. Chen
Categories: cs.CL cs.LG quant-ph
\\ ( https://arxiv.org/abs/2203.11155 ,  324kb)
------------------------------------------------------------------------------
\\
arXiv:2211.00635
replaced with revised version Tue, 12 Mar 2024 22:05:53 GMT   (7112kb,D)

Title: Two-stage LLM Fine-tuning with Less Specialization and More
  Generalization
Authors: Yihan Wang, Si Si, Daliang Li, Michal Lukasik, Felix Yu, Cho-Jui
  Hsieh, Inderjit S Dhillon, Sanjiv Kumar
Categories: cs.CL cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2211.00635 ,  7112kb)
------------------------------------------------------------------------------
\\
arXiv:2212.12799
replaced with revised version Wed, 13 Mar 2024 17:41:14 GMT   (192kb,D)

Title: A Comprehensive Study of Gender Bias in Chemical Named Entity
  Recognition Models
Authors: Xingmeng Zhao, Ali Niazi and Anthony Rios
Categories: cs.CL
\\ ( https://arxiv.org/abs/2212.12799 ,  192kb)
------------------------------------------------------------------------------
\\
arXiv:2305.09144
replaced with revised version Wed, 13 Mar 2024 12:34:17 GMT   (5134kb,D)

Title: Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism
  of Language Models
Authors: Boxi Cao, Qiaoyu Tang, Hongyu Lin, Shanshan Jiang, Bin Dong, Xianpei
  Han, Jiawei Chen, Tianshu Wang, Le Sun
Categories: cs.CL cs.AI
Comments: Accepted by LREC-COLING 2024
\\ ( https://arxiv.org/abs/2305.09144 ,  5134kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14336
replaced with revised version Tue, 12 Mar 2024 18:54:12 GMT   (9545kb,D)

Title: Schema-Driven Information Extraction from Heterogeneous Tables
Authors: Fan Bai, Junmo Kang, Gabriel Stanovsky, Dayne Freitag, Alan Ritter
Categories: cs.CL
\\ ( https://arxiv.org/abs/2305.14336 ,  9545kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08852
replaced with revised version Wed, 13 Mar 2024 08:49:54 GMT   (247kb)

Title: BED: Bi-Encoder-Based Detectors for Out-of-Distribution Detection
Authors: Louis Owen, Biddwan Ahmed, Abhay Kumar
Categories: cs.CL
Comments: Published in IEEE: https://ieeexplore.ieee.org/document/10389907
Journal-ref: 2023 10th International Conference on Advanced Informatics:
  Concept, Theory and Application (2023) 1-6
DOI: 10.1109/ICAICTA59291.2023.10389907
\\ ( https://arxiv.org/abs/2306.08852 ,  247kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12375
replaced with revised version Wed, 13 Mar 2024 15:00:20 GMT   (13125kb,D)

Title: In-Context Learning Learns Label Relationships but Is Not Conventional
  Learning
Authors: Jannik Kossen, Yarin Gal, Tom Rainforth
Categories: cs.CL cs.AI cs.LG
Comments: Accepted for publication at ICLR 2024
\\ ( https://arxiv.org/abs/2307.12375 ,  13125kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11767
replaced with revised version Tue, 12 Mar 2024 20:10:05 GMT   (554kb,D)

Title: Detection of ChatGPT Fake Science with the xFakeSci Learning Algorithm
Authors: Ahmed Abdeen Hamed and Xindong Wu
Categories: cs.CL cs.IR
Comments: 14 pages, 6 figures, 6 tables, 5 algorithms
ACM-class: I.2
\\ ( https://arxiv.org/abs/2308.11767 ,  554kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07759
replaced with revised version Wed, 13 Mar 2024 07:27:10 GMT   (7305kb,D)

Title: PROGrasp: Pragmatic Human-Robot Communication for Object Grasping
Authors: Gi-Cheon Kang, Junghyun Kim, Jaein Kim, Byoung-Tak Zhang
Categories: cs.CL cs.RO
Comments: ICRA 2024
\\ ( https://arxiv.org/abs/2309.07759 ,  7305kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17428
replaced with revised version Wed, 13 Mar 2024 05:39:25 GMT   (9060kb,D)

Title: CRAFT: Customizing LLMs by Creating and Retrieving from Specialized
  Toolsets
Authors: Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi R. Fung, Hao Peng, Heng Ji
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to ICLR 2024. Code is available at
  https://github.com/lifan-yuan/CRAFT
\\ ( https://arxiv.org/abs/2309.17428 ,  9060kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01188
replaced with revised version Wed, 13 Mar 2024 08:45:53 GMT   (8075kb,D)

Title: Quantifying the Plausibility of Context Reliance in Neural Machine
  Translation
Authors: Gabriele Sarti, Grzegorz Chrupa{\l}a, Malvina Nissim, Arianna Bisazza
Categories: cs.CL cs.AI cs.HC cs.LG
Comments: ICLR 2024 Camera Ready. Code: https://github.com/gsarti/pecore.
  Artifacts:
  https://huggingface.co/collections/gsarti/pecore-iclr-2024-65edab42e28439e21b612c2e
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2310.01188 ,  8075kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04475
replaced with revised version Wed, 13 Mar 2024 17:40:04 GMT   (5995kb,D)

Title: Demystifying Embedding Spaces using Large Language Models
Authors: Guy Tennenholtz, Yinlam Chow, Chih-Wei Hsu, Jihwan Jeong, Lior Shani,
  Azamat Tulepbergenov, Deepak Ramachandran, Martin Mladenov, Craig Boutilier
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2310.04475 ,  5995kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05209
replaced with revised version Wed, 13 Mar 2024 08:14:47 GMT   (32087kb,D)

Title: Scaling Laws of RoPE-based Extrapolation
Authors: Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, Dahua Lin
Categories: cs.CL cs.AI
Comments: 26 pages, 12 figures, Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2310.05209 ,  32087kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07793
replaced with revised version Wed, 13 Mar 2024 17:10:48 GMT   (3867kb,D)

Title: GenTKG: Generative Forecasting on Temporal Knowledge Graph
Authors: Ruotong Liao, Xu Jia, Yunpu Ma, Yangzhe Li, Volker Tresp
Categories: cs.CL cs.AI cs.LG
Comments: 14 pages, Findings of NAACL 2024
\\ ( https://arxiv.org/abs/2310.07793 ,  3867kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19208
replaced with revised version Wed, 13 Mar 2024 05:11:57 GMT   (930kb,D)

Title: LitCab: Lightweight Language Model Calibration over Short- and Long-form
  Responses
Authors: Xin Liu, Muhammad Khalifa, Lu Wang
Categories: cs.CL
Comments: accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2310.19208 ,  930kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08981
replaced with revised version Wed, 13 Mar 2024 17:32:50 GMT   (3276kb,D)

Title: Speculative Contrastive Decoding
Authors: Hongyi Yuan, Keming Lu, Fei Huang, Zheng Yuan, Chang Zhou
Categories: cs.CL
Comments: Revised version
\\ ( https://arxiv.org/abs/2311.08981 ,  3276kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09702
replaced with revised version Wed, 13 Mar 2024 09:11:15 GMT   (7842kb,D)

Title: Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go
  without Hallucination?
Authors: Bangzheng Li, Ben Zhou, Fei Wang, Xingyu Fu, Dan Roth, Muhao Chen
Categories: cs.CL cs.AI
Comments: Work in progress
\\ ( https://arxiv.org/abs/2311.09702 ,  7842kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09818
replaced with revised version Wed, 13 Mar 2024 17:07:02 GMT   (1042kb,D)

Title: SUQL: Conversational Search over Structured and Unstructured Data with
  Large Language Models
Authors: Shicheng Liu, Jialiang Xu, Wesley Tjangnaka, Sina J. Semnani, Chen Jie
  Yu, Monica S. Lam
Categories: cs.CL cs.PL
\\ ( https://arxiv.org/abs/2311.09818 ,  1042kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06185
replaced with revised version Wed, 13 Mar 2024 07:35:18 GMT   (1279kb,D)

Title: KnowGPT: Knowledge Injection for Large Language Models
Authors: Qinggang Zhang, Junnan Dong, Hao Chen, Daochen Zha, Zailiang Yu, Xiao
  Huang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2312.06185 ,  1279kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07950
replaced with revised version Tue, 12 Mar 2024 18:34:05 GMT   (2591kb,D)

Title: SciGLM: Training Scientific Language Models with Self-Reflective
  Instruction Annotation and Tuning
Authors: Dan Zhang and Ziniu Hu and Sining Zhoubian and Zhengxiao Du and Kaiyu
  Yang and Zihan Wang and Yisong Yue and Yuxiao Dong and Jie Tang
Categories: cs.CL
Comments: 21 pages
\\ ( https://arxiv.org/abs/2401.07950 ,  2591kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16589
replaced with revised version Wed, 13 Mar 2024 09:45:02 GMT   (8406kb,D)

Title: ToPro: Token-Level Prompt Decomposition for Cross-Lingual Sequence
  Labeling Tasks
Authors: Bolei Ma, Ercong Nie, Shuzhou Yuan, Helmut Schmid, Michael F\"arber,
  Frauke Kreuter and Hinrich Sch\"utze
Categories: cs.CL
Comments: EACL 2024
\\ ( https://arxiv.org/abs/2401.16589 ,  8406kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11522
replaced with revised version Wed, 13 Mar 2024 02:40:21 GMT   (256kb,D)

Title: Unveiling the Secrets of Engaging Conversations: Factors that Keep Users
  Hooked on Role-Playing Dialog Agents
Authors: Shuai Zhang, Yu Lu, Junwen Liu, Jia Yu, Huachuan Qiu, Yuming Yan,
  Zhenzhong Lan
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.11522 ,  256kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11550
replaced with revised version Wed, 13 Mar 2024 07:16:42 GMT   (610kb,D)

Title: LongAgent: Scaling Language Models to 128k Context through Multi-Agent
  Collaboration
Authors: Jun Zhao, Can Zu, Hao Xu, Yi Lu, Wei He, Yiwen Ding, Tao Gui, Qi
  Zhang, Xuanjing Huang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.11550 ,  610kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13405
replaced with revised version Tue, 12 Mar 2024 22:12:25 GMT   (3593kb,D)

Title: A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set
  Expansion and Taxonomy Expansion
Authors: Yanzhen Shen, Yu Zhang, Yunyi Zhang, Jiawei Han
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.13405 ,  3593kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14158
replaced with revised version Wed, 13 Mar 2024 16:38:42 GMT   (455kb,D)

Title: TOOLVERIFIER: Generalization to New Tools via Self-Verification
Authors: Dheeraj Mekala, Jason Weston, Jack Lanchantin, Roberta Raileanu, Maria
  Lomeli, Jingbo Shang, Jane Dwivedi-Yu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.14158 ,  455kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18060
replaced with revised version Wed, 13 Mar 2024 16:44:45 GMT   (494kb,D)

Title: Benchmarking Large Language Models on Answering and Explaining
  Challenging Medical Questions
Authors: Hanjie Chen, Zhouxiang Fang, Yash Singla, Mark Dredze
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.18060 ,  494kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04780
replaced with revised version Wed, 13 Mar 2024 15:52:33 GMT   (3933kb,D)

Title: MuseGraph: Graph-oriented Instruction Tuning of Large Language Models
  for Generic Graph Mining
Authors: Yanchao Tan, Hang Lv, Xinyi Huang, Jiawei Zhang, Shiping Wang, Carl
  Yang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.04780 ,  3933kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04945
replaced with revised version Wed, 13 Mar 2024 06:20:47 GMT   (4141kb,D)

Title: Electrocardiogram Instruction Tuning for Report Generation
Authors: Zhongwei Wan, Che Liu, Xin Wang, Chaofan Tao, Hui Shen, Zhenwu Peng,
  Jie Fu, Rossella Arcucci, Huaxiu Yao, Mi Zhang
Categories: cs.CL cs.LG eess.SP
Comments: Under review
\\ ( https://arxiv.org/abs/2403.04945 ,  4141kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06487
replaced with revised version Wed, 13 Mar 2024 00:41:36 GMT   (1506kb,D)

Title: Multilingual Turn-taking Prediction Using Voice Activity Projection
Authors: Koji Inoue, Bing'er Jiang, Erik Ekstedt, Tatsuya Kawahara, Gabriel
  Skantze
Categories: cs.CL cs.SD eess.AS
Comments: This paper has been accepted for presentation at The 2024 Joint
  International Conference on Computational Linguistics, Language Resources and
  Evaluation (LREC-COLING 2024) and represents the author's version of the work
\\ ( https://arxiv.org/abs/2403.06487 ,  1506kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06935
replaced with revised version Wed, 13 Mar 2024 09:26:26 GMT   (9270kb,D)

Title: Naming, Describing, and Quantifying Visual Objects in Humans and LLMs
Authors: Alberto Testoni, Juell Sprott, Sandro Pezzelle
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.06935 ,  9270kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07714
replaced with revised version Wed, 13 Mar 2024 14:08:19 GMT   (765kb,D)

Title: StableToolBench: Towards Stable Large-Scale Benchmarking on Tool
  Learning of Large Language Models
Authors: Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li,
  Zhiyuan Liu, Maosong Sun, Yang Liu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.07714 ,  765kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07805
replaced with revised version Wed, 13 Mar 2024 12:46:38 GMT   (9026kb,D)

Title: Beyond Memorization: The Challenge of Random Memory Access in Language
  Models
Authors: Tongyao Zhu, Qian Liu, Liang Pang, Zhengbao Jiang, Min-Yen Kan, Min
  Lin
Categories: cs.CL cs.AI
Comments: 8 pages, 4 figures; fixed typos
\\ ( https://arxiv.org/abs/2403.07805 ,  9026kb)
------------------------------------------------------------------------------
\\
arXiv:2111.07058
replaced with revised version Wed, 13 Mar 2024 10:20:14 GMT   (2390kb,D)

Title: Bolstering Stochastic Gradient Descent with Model Building
Authors: S. Ilker Birbil, Ozgur Martin, Gonenc Onay, Figen Oztoprak
Categories: cs.LG
\\ ( https://arxiv.org/abs/2111.07058 ,  2390kb)
------------------------------------------------------------------------------
\\
arXiv:2203.07835
replaced with revised version Tue, 12 Mar 2024 20:48:50 GMT   (2119kb,D)

Title: Better Uncertainty Calibration via Proper Scores for Classification and
  Beyond
Authors: Sebastian G. Gruber and Florian Buettner
Categories: cs.LG stat.ML
Comments: Published at NeurIPS 2022. Corrected conference version Theorem 3.1
  and Proposition 3.2 since CWCE=0 does not imply TCE=0
Journal-ref: Advances in Neural Information Processing Systems 35 (NeurIPS
  2022)
\\ ( https://arxiv.org/abs/2203.07835 ,  2119kb)
------------------------------------------------------------------------------
\\
arXiv:2209.06388
replaced with revised version Wed, 13 Mar 2024 07:50:44 GMT   (5202kb,D)

Title: TSFool: Crafting Highly-Imperceptible Adversarial Time Series through
  Multi-Objective Attack
Authors: Yanyun Wang, Dehui Du, Haibo Hu, Zi Liang and Yuanhao Liu
Categories: cs.LG cs.CR
Comments: 22 pages, 16 figures
ACM-class: I.2.0; I.5.0
\\ ( https://arxiv.org/abs/2209.06388 ,  5202kb)
------------------------------------------------------------------------------
\\
arXiv:2209.06408
replaced with revised version Wed, 13 Mar 2024 03:10:25 GMT   (4195kb,D)

Title: Meta Pattern Concern Score: A Novel Evaluation Measure with Human Values
  for Multi-classifiers
Authors: Yanyun Wang, Dehui Du, Yuanhao Liu
Categories: cs.LG
Comments: Published at the 2023 IEEE International Conference on Systems, Man,
  and Cybernetics (SMC); 9 pages, 6 figures
ACM-class: I.2.0
DOI: 10.1109/SMC53992.2023.10394380
\\ ( https://arxiv.org/abs/2209.06408 ,  4195kb)
------------------------------------------------------------------------------
\\
arXiv:2210.15050
replaced with revised version Wed, 13 Mar 2024 01:31:24 GMT   (2836kb,D)

Title: TILDE-Q: A Transformation Invariant Loss Function for Time-Series
  Forecasting
Authors: Hyunwook Lee, Chunggi Lee, Hongkyu Lim, Sungahn Ko
Categories: cs.LG
Comments: 17 pages in total, 8 pages main paper, 3 pages references, and 6
  pages appendix. Submitted as conference paper to ICML 2024 and currently
  under review
\\ ( https://arxiv.org/abs/2210.15050 ,  2836kb)
------------------------------------------------------------------------------
\\
arXiv:2212.12989
replaced with revised version Wed, 13 Mar 2024 07:07:58 GMT   (40kb)

Title: Improved Kernel Alignment Regret Bound for Online Kernel Learning
Authors: Junfan Li and Shizhong Liao
Categories: cs.LG
\\ ( https://arxiv.org/abs/2212.12989 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2212.14511
replaced with revised version Wed, 13 Mar 2024 17:44:52 GMT   (38kb,D)

Title: Can Direct Latent Model Learning Solve Linear Quadratic Gaussian
  Control?
Authors: Yi Tian, Kaiqing Zhang, Russ Tedrake, Suvrit Sra
Categories: cs.LG cs.SY eess.SY math.OC stat.ML
Comments: 37 pages; Updated structure and proofs
\\ ( https://arxiv.org/abs/2212.14511 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2302.02926
replaced with revised version Tue, 12 Mar 2024 23:54:04 GMT   (29kb,D)

Title: Curriculum Graph Machine Learning: A Survey
Authors: Haoyang Li, Xin Wang, Wenwu Zhu
Categories: cs.LG
Comments: IJCAI 2023 Survey Track
\\ ( https://arxiv.org/abs/2302.02926 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2302.04440
replaced with revised version Wed, 13 Mar 2024 00:48:30 GMT   (42706kb,D)

Title: Feature Likelihood Divergence: Evaluating the Generalization of
  Generative Models Using Samples
Authors: Marco Jiralerspong, Avishek Joey Bose, Ian Gemp, Chongli Qin, Yoram
  Bachrach, Gauthier Gidel
Categories: cs.LG cs.CV
Comments: FLD code: https://github.com/marcojira/fld
\\ ( https://arxiv.org/abs/2302.04440 ,  42706kb)
------------------------------------------------------------------------------
\\
arXiv:2302.06670
replaced with revised version Tue, 12 Mar 2024 22:23:53 GMT   (9686kb,D)

Title: Explainable Anomaly Detection in Images and Videos: A Survey
Authors: Yizhou Wang, Dongliang Guo, Sheng Li, Octavia Camps, Yun Fu
Categories: cs.LG cs.AI
Comments: Submitted to TPAMI
\\ ( https://arxiv.org/abs/2302.06670 ,  9686kb)
------------------------------------------------------------------------------
\\
arXiv:2302.09195
replaced with revised version Tue, 12 Mar 2024 19:22:20 GMT   (5183kb,D)

Title: Data-Efficient Contrastive Self-supervised Learning: Most Beneficial
  Examples for Supervised Learning Contribute the Least
Authors: Siddharth Joshi and Baharan Mirzasoleiman
Categories: cs.LG cs.AI
Comments: Accepted to ICML 2023, Code:
  https://github.com/BigML-CS-UCLA/sas-data-efficient-contrastive-learning
\\ ( https://arxiv.org/abs/2302.09195 ,  5183kb)
------------------------------------------------------------------------------
\\
arXiv:2303.16521
replaced with revised version Wed, 13 Mar 2024 12:53:31 GMT   (140kb,D)

Title: Hard Regularization to Prevent Deep Online Clustering Collapse without
  Data Augmentation
Authors: Louis Mahon, Thomas Lukasiewicz
Categories: cs.LG
\\ ( https://arxiv.org/abs/2303.16521 ,  140kb)
------------------------------------------------------------------------------
\\
arXiv:2304.10151
replaced with revised version Wed, 13 Mar 2024 08:14:02 GMT   (134kb,D)

Title: Flexible K Nearest Neighbors Classifier: Derivation and Application for
  Ion-mobility Spectrometry-based Indoor Localization
Authors: Philipp M\"uller
Categories: cs.LG eess.SP
Comments: 11 pages, 3 figures, paper presented at the 2023 International
  Conference on Indoor Positioning and Indoor Navigation (IPIN)
DOI: 10.1109/IPIN57070.2023.10332541
\\ ( https://arxiv.org/abs/2304.10151 ,  134kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18511
replaced with revised version Wed, 13 Mar 2024 05:42:44 GMT   (3874kb,D)

Title: Contextual Bandits with Budgeted Information Reveal
Authors: Kyra Gan, Esmaeil Keyvanshokooh, Xueqing Liu, Susan Murphy
Categories: cs.LG math.OC
Comments: International Conference on Artificial Intelligence and Statistics,
  2024
\\ ( https://arxiv.org/abs/2305.18511 ,  3874kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00010
replaced with revised version Tue, 12 Mar 2024 18:41:35 GMT   (1600kb,D)

Title: Trainable and Explainable Simplicial Map Neural Networks
Authors: Eduardo Paluzo-Hidalgo, Miguel A. Guti\'errez-Naranjo, Rocio
  Gonzalez-Diaz
Categories: cs.LG cs.AI math.AT
\\ ( https://arxiv.org/abs/2306.00010 ,  1600kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04621
replaced with revised version Wed, 13 Mar 2024 10:23:45 GMT   (1334kb,D)

Title: Flexible Distribution Alignment: Towards Long-tailed Semi-supervised
  Learning with Proper Calibration
Authors: Emanuel Sanchez Aimar and Hannah Helgesen and Yonghao Xu and Marco
  Kuhlmann and Michael Felsberg
Categories: cs.LG cs.CV
Comments: Under review, 24 pages
\\ ( https://arxiv.org/abs/2306.04621 ,  1334kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14306
replaced with revised version Wed, 13 Mar 2024 17:20:27 GMT   (11227kb,D)

Title: Adaptive Sharpness-Aware Pruning for Robust Sparse Networks
Authors: Anna Bair, Hongxu Yin, Maying Shen, Pavlo Molchanov, Jose Alvarez
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2306.14306 ,  11227kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16334
replaced with revised version Tue, 12 Mar 2024 20:04:04 GMT   (2940kb,D)

Title: On the Identifiability of Quantized Factors
Authors: Vit\'oria Barin-Pacela, Kartik Ahuja, Simon Lacoste-Julien, Pascal
  Vincent
Categories: cs.LG cs.AI
Comments: Appears in: 3rd Conference on Causal Learning and Reasoning (CLeaR
  2024). 39 pages
\\ ( https://arxiv.org/abs/2306.16334 ,  2940kb)
------------------------------------------------------------------------------
\\
arXiv:2306.17279
replaced with revised version Wed, 13 Mar 2024 03:58:56 GMT   (1847kb,D)

Title: Probabilistic Constraint for Safety-Critical Reinforcement Learning
Authors: Weiqin Chen, Dharmashankar Subramanian and Santiago Paternain
Categories: cs.LG cs.AI
Comments: arXiv admin note: text overlap with arXiv:2210.00596
\\ ( https://arxiv.org/abs/2306.17279 ,  1847kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02040
replaced with revised version Wed, 13 Mar 2024 08:06:37 GMT   (15260kb,D)

Title: VertiBench: Advancing Feature Distribution Diversity in Vertical
  Federated Learning Benchmarks
Authors: Zhaomin Wu, Junyi Hou, Bingsheng He
Categories: cs.LG cs.AI
Journal-ref: The Twelfth International Conference on Learning Representations
  (ICLR 2024)
\\ ( https://arxiv.org/abs/2307.02040 ,  15260kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09249
replaced with revised version Wed, 13 Mar 2024 08:20:34 GMT   (1990kb,D)

Title: UniTabE: A Universal Pretraining Protocol for Tabular Foundation Model
  in Data Science
Authors: Yazheng Yang, Yuqi Wang, Guang Liu, Ledell Wu, Qi Liu
Categories: cs.LG cs.AI cs.CL
Comments: ICLR 2024, 9 pages
\\ ( https://arxiv.org/abs/2307.09249 ,  1990kb)
------------------------------------------------------------------------------
\\
arXiv:2308.16207
replaced with revised version Wed, 13 Mar 2024 08:35:00 GMT   (2337kb,D)

Title: MASA-TCN: Multi-anchor Space-aware Temporal Convolutional Neural
  Networks for Continuous and Discrete EEG Emotion Recognition
Authors: Yi Ding, Su Zhang, Chuangao Tang, Cuntai Guan
Categories: cs.LG
Comments: 12 pages, 4 figures. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
\\ ( https://arxiv.org/abs/2308.16207 ,  2337kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15048
replaced with revised version Wed, 13 Mar 2024 14:24:28 GMT   (3250kb,D)

Title: Class Incremental Learning via Likelihood Ratio Based Task Prediction
Authors: Haowei Lin, Yijia Shao, Weinan Qian, Ningxin Pan, Yiduo Guo, Bing Liu
Categories: cs.LG cs.AI cs.CV
Journal-ref: ICLR 2024
\\ ( https://arxiv.org/abs/2309.15048 ,  3250kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16456
replaced with revised version Wed, 13 Mar 2024 14:16:54 GMT   (556kb,D)

Title: Resisting Backdoor Attacks in Federated Learning via Bidirectional
  Elections and Individual Perspective
Authors: Zhen Qin, Feiyi Chen, Chen Zhi, Xueqiang Yan, Shuiguang Deng
Categories: cs.LG
Comments: Accepted by AAAI 2024. Codes are publicly available at
  https://github.com/zhenqincn/Snowball
\\ ( https://arxiv.org/abs/2309.16456 ,  556kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00116
replaced with revised version Tue, 12 Mar 2024 18:57:37 GMT   (4726kb,D)

Title: Certified Robustness via Dynamic Margin Maximization and Improved
  Lipschitz Regularization
Authors: Mahyar Fazlyab, Taha Entesari, Aniket Roy, Rama Chellappa
Categories: cs.LG cs.AI
Comments: 37th Conference on Neural Information Processing Systems (NeurIPS
  2023)
\\ ( https://arxiv.org/abs/2310.00116 ,  4726kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00902
replaced with revised version Wed, 13 Mar 2024 14:27:46 GMT   (6231kb,D)

Title: DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and
  Diffusion Models
Authors: Yongchan Kwon, Eric Wu, Kevin Wu, James Zou
Categories: cs.LG stat.ML
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.00902 ,  6231kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01082
replaced with revised version Wed, 13 Mar 2024 16:48:27 GMT   (9559kb,D)

Title: Linear attention is (maybe) all you need (to understand transformer
  optimization)
Authors: Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie,
  Suvrit Sra
Categories: cs.LG cs.AI math.OC
Comments: Published at ICLR 2024
\\ ( https://arxiv.org/abs/2310.01082 ,  9559kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01557
replaced with revised version Wed, 13 Mar 2024 01:02:01 GMT   (12218kb,D)

Title: SmartPlay: A Benchmark for LLMs as Intelligent Agents
Authors: Yue Wu, Xuan Tang, Tom M. Mitchell, Yuanzhi Li
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.01557 ,  12218kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02279
replaced with revised version Wed, 13 Mar 2024 04:50:56 GMT   (2657kb,D)

Title: Consistency Trajectory Models: Learning Probability Flow ODE Trajectory
  of Diffusion
Authors: Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta
  Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, Stefano Ermon
Categories: cs.LG cs.AI cs.CV stat.ML
Comments: International Conference on Learning Representations
\\ ( https://arxiv.org/abs/2310.02279 ,  2657kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02895
replaced with revised version Wed, 13 Mar 2024 14:56:11 GMT   (588kb,D)

Title: CoLiDE: Concomitant Linear DAG Estimation
Authors: Seyed Saman Saboksayr, Gonzalo Mateos, Mariano Tepper
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.02895 ,  588kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04264
replaced with revised version Wed, 13 Mar 2024 13:42:40 GMT   (3081kb,D)

Title: Deep learning modelling of manufacturing and build variations on
  multi-stage axial compressors aerodynamics
Authors: Giuseppe Bruni, Sepehr Maleki, Senthil K. Krishnababu
Categories: cs.LG cs.CE physics.flu-dyn
\\ ( https://arxiv.org/abs/2310.04264 ,  3081kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08750
replaced with revised version Tue, 12 Mar 2024 22:09:41 GMT   (14706kb,D)

Title: Search-Adaptor: Embedding Customization for Information Retrieval
Authors: Jinsung Yoon, Sercan O Arik, Yanfei Chen, Tomas Pfister
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.08750 ,  14706kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11762
replaced with revised version Wed, 13 Mar 2024 04:14:52 GMT   (639kb,D)

Title: A Quasi-Wasserstein Loss for Learning Graph Neural Networks
Authors: Minjie Cheng and Hongteng Xu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.11762 ,  639kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14554
replaced with revised version Tue, 12 Mar 2024 21:49:59 GMT   (82kb)

Title: Making RL with Preference-based Feedback Efficient via Randomization
Authors: Runzhe Wu, Wen Sun
Categories: cs.LG cs.AI cs.HC
Comments: Accepted at ICLR 2024
\\ ( https://arxiv.org/abs/2310.14554 ,  82kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14864
replaced with revised version Wed, 13 Mar 2024 13:12:10 GMT   (1729kb,D)

Title: Effective Structural Encodings via Local Curvature Profiles
Authors: Lukas Fesser, Melanie Weber
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2311.14864 ,  1729kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16496
replaced with revised version Wed, 13 Mar 2024 02:32:32 GMT   (19635kb,D)

Title: DPOD: Domain-Specific Prompt Tuning for Multimodal Fake News Detection
Authors: Debarshi Brahma, Amartya Bhattacharya, Suraj Nagaje Mahadev, Anmol
  Asati, Vikas Verma, Soma Biswas
Categories: cs.LG
\\ ( https://arxiv.org/abs/2311.16496 ,  19635kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03878
replaced with revised version Tue, 12 Mar 2024 20:12:18 GMT   (920kb,D)

Title: Domain constraints improve risk prediction when outcome data is missing
Authors: Sidhika Balachandar, Nikhil Garg, Emma Pierson
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.03878 ,  920kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04142
replaced with revised version Wed, 13 Mar 2024 10:23:10 GMT   (2061kb,D)

Title: TimeDRL: Disentangled Representation Learning for Multivariate
  Time-Series
Authors: Ching Chang, Chiao-Tung Chan, Wei-Yao Wang, Wen-Chih Peng, Tien-Fu
  Chen
Categories: cs.LG cs.AI
Comments: This paper has been accepted by the International Conference on Data
  Engineering (ICDE) 2024
\\ ( https://arxiv.org/abs/2312.04142 ,  2061kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05720
replaced with revised version Wed, 13 Mar 2024 11:19:24 GMT   (723kb,D)

Title: Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer
  Inputs of Language Models in Federated Learning
Authors: Jianwei Li, Sheng Liu, Qi Lei
Categories: cs.LG cs.AI cs.CL cs.CR
\\ ( https://arxiv.org/abs/2312.05720 ,  723kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07511
replaced with revised version Wed, 13 Mar 2024 17:38:27 GMT   (18361kb,D)

Title: A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems
Authors: Alexandre Duval, Simon V. Mathis, Chaitanya K. Joshi, Victor Schmidt,
  Santiago Miret, Fragkiskos D. Malliaros, Taco Cohen, Pietro Li\`o, Yoshua
  Bengio and Michael Bronstein
Categories: cs.LG cs.AI q-bio.QM stat.ML
\\ ( https://arxiv.org/abs/2312.07511 ,  18361kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10385
replaced with revised version Wed, 13 Mar 2024 14:48:36 GMT   (30505kb,D)

Title: Imitate the Good and Avoid the Bad: An Incremental Approach to Safe
  Reinforcement Learning
Authors: Huy Hoang and Tien Mai and Pradeep Varakantham
Categories: cs.LG cs.AI
Journal-ref: AAAI 2024
\\ ( https://arxiv.org/abs/2312.10385 ,  30505kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08898
replaced with revised version Wed, 13 Mar 2024 00:24:42 GMT   (16301kb,D)

Title: Bridging State and History Representations: Understanding
  Self-Predictive RL
Authors: Tianwei Ni, Benjamin Eysenbach, Erfan Seyedsalehi, Michel Ma, Clement
  Gehring, Aditya Mahajan, Pierre-Luc Bacon
Categories: cs.LG cs.AI
Comments: ICLR 2024 (Poster). Code is available at
  https://github.com/twni2016/self-predictive-rl
\\ ( https://arxiv.org/abs/2401.08898 ,  16301kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11410
replaced with revised version Wed, 13 Mar 2024 02:03:20 GMT   (7600kb,D)

Title: Agricultural Recommendation System based on Deep Learning: A
  Multivariate Weather Forecasting Approach
Authors: Md Zubair (1), Md. Shahidul Salim (2), Mehrab Mustafy Rahman (3),
  Mohammad Jahid Ibna Basher (1), Shahin Imran (4) and Iqbal H. Sarker (5) ((1)
  Chittagong University of Engineering & Technology, Chittagong, Bangladesh,
  (2) Khulna University of Engineering & Technology, Khulna, Bangladesh, (3)
  Islamic University of Technology, Gazipur, Bangladesh, (4) Khulna
  Agricultural University, Khulna, Bangladesh, (5) Edith Cowan University,
  Perth, Australia)
Categories: cs.LG cs.AI
Comments: 18 pages, 16 figures and 13 tables. Two figures and one table have
  been added to this version
\\ ( https://arxiv.org/abs/2401.11410 ,  7600kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15604
replaced with revised version Wed, 13 Mar 2024 01:25:26 GMT   (66kb)

Title: Neural Network-Based Score Estimation in Diffusion Models: Optimization
  and Generalization
Authors: Yinbin Han, Meisam Razaviyayn, Renyuan Xu
Categories: cs.LG stat.ML
Comments: 39 pages
\\ ( https://arxiv.org/abs/2401.15604 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16537
replaced with revised version Tue, 12 Mar 2024 19:01:47 GMT   (555kb)

Title: Efficient Observation Time Window Segmentation for Administrative Data
  Machine Learning
Authors: Musa Taib, Geoffrey G. Messier
Categories: cs.LG cs.CY
\\ ( https://arxiv.org/abs/2401.16537 ,  555kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16664
replaced with revised version Wed, 13 Mar 2024 17:03:40 GMT   (307kb)

Title: Fast Dual-Regularized Autoencoder for Sparse Biological Data
Authors: Aleksandar Poleksic
Categories: cs.LG
MSC-class: 92C42
ACM-class: J.3
\\ ( https://arxiv.org/abs/2401.16664 ,  307kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09264
replaced with revised version Tue, 12 Mar 2024 23:25:10 GMT   (3667kb,D)

Title: UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers
Authors: Hong Jia, Young D. Kwon, Dong Ma, Nhat Pham, Lorena Qendro, Tam Vu and
  Cecilia Mascolo
Categories: cs.LG cs.HC
\\ ( https://arxiv.org/abs/2402.09264 ,  3667kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14989
replaced with revised version Wed, 13 Mar 2024 07:27:47 GMT   (1087kb,D)

Title: Stable Neural Stochastic Differential Equations in Analyzing Irregular
  Time Series Data
Authors: YongKyung Oh, Dongyoung Lim, Sungil Kim
Categories: cs.LG cs.AI
Comments: Accepted at ICLR 2024, Spotlight presentation (Notable Top 5%).
  https://openreview.net/forum?id=4VIgNuQ1pY
\\ ( https://arxiv.org/abs/2402.14989 ,  1087kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17453
replaced with revised version Wed, 13 Mar 2024 12:02:25 GMT   (370kb,D)

Title: DS-Agent: Automated Data Science by Empowering Large Language Models
  with Case-Based Reasoning
Authors: Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.17453 ,  370kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18133
replaced with revised version Wed, 13 Mar 2024 03:07:08 GMT   (3501kb,D)

Title: Classes Are Not Equal: An Empirical Study on Image Recognition Fairness
Authors: Jiequan Cui, Beier Zhu, Xin Wen, Xiaojuan Qi, Bei Yu, Hanwang Zhang
Categories: cs.LG cs.CV
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2402.18133 ,  3501kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02619
replaced with revised version Wed, 13 Mar 2024 07:19:06 GMT   (296kb,D)

Title: Training Machine Learning models at the Edge: A Survey
Authors: Aymen Rayane Khouas, Mohamed Reda Bouadjenek, Hakim Hacid, and Sunil
  Aryal
Categories: cs.LG cs.DC
Comments: 30 pages, 7 figures, submitted to IEEE Communications Surveys &
  Tutorials
\\ ( https://arxiv.org/abs/2403.02619 ,  296kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04161
replaced with revised version Wed, 13 Mar 2024 00:40:52 GMT   (19319kb,D)

Title: SWAP-NAS: Sample-Wise Activation Patterns for Ultra-fast NAS
Authors: Yameng Peng, Andy Song, Haytham M. Fayek, Vic Ciesielski, Xiaojun
  Chang
Categories: cs.LG cs.CV cs.NE
Comments: ICLR2024 Spotlight
\\ ( https://arxiv.org/abs/2403.04161 ,  19319kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04861
replaced with revised version Tue, 12 Mar 2024 18:35:48 GMT   (225kb,D)

Title: A Survey of Lottery Ticket Hypothesis
Authors: Bohan Liu, Zijie Zhang, Peixiong He, Zhensen Wang, Yang Xiao, Ruimeng
  Ye, Yang Zhou, Wei-Shinn Ku, Bo Hui
Categories: cs.LG cs.NE
\\ ( https://arxiv.org/abs/2403.04861 ,  225kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05529
replaced with revised version Tue, 12 Mar 2024 22:27:02 GMT   (7268kb,D)

Title: Computational-Statistical Gaps in Gaussian Single-Index Models
Authors: Alex Damian, Loucas Pillaud-Vivien, Jason D. Lee, Joan Bruna
Categories: cs.LG stat.ML
Comments: 61 pages
\\ ( https://arxiv.org/abs/2403.05529 ,  7268kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05693
replaced with revised version Wed, 13 Mar 2024 00:03:47 GMT   (508kb,D)

Title: Shielded Deep Reinforcement Learning for Complex Spacecraft Tasking
Authors: Robert Reed, Hanspeter Schaub, Morteza Lahijanian
Categories: cs.LG
Comments: 9 pages, 2 figures, 2 tables, ACC 2024
\\ ( https://arxiv.org/abs/2403.05693 ,  508kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06026
replaced with revised version Wed, 13 Mar 2024 00:09:46 GMT   (189kb,D)

Title: Towards a Generic Representation of Combinatorial Problems for
  Learning-Based Approaches
Authors: L\'eo Boisvert, H\'el\`ene Verhaeghe, Quentin Cappart
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.06026 ,  189kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06757
replaced with revised version Wed, 13 Mar 2024 13:57:42 GMT   (1079kb,D)

Title: Koopman Ensembles for Probabilistic Time Series Forecasting
Authors: Anthony Frion, Lucas Drumetz, Guillaume Tochon, Mauro Dalla Mura,
  Albdeldjalil A\"issa El Bey
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.06757 ,  1079kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06829
replaced with revised version Wed, 13 Mar 2024 17:01:57 GMT   (3541kb,D)

Title: Constructing Variables Using Classifiers as an Aid to Regression: An
  Empirical Assessment
Authors: Colin Troisemaine, Vincent Lemaire
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.06829 ,  3541kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07353
replaced with revised version Wed, 13 Mar 2024 04:43:23 GMT   (951kb,D)

Title: Graph Unlearning with Efficient Partial Retraining
Authors: Jiahao Zhang, Lin Wang, Shijie Wang, Wenqi Fan
Categories: cs.LG cs.CR
Comments: 8 pages, 3 figures, accepted by The Web Conference 2024 (PhD
  Symposium Track)
\\ ( https://arxiv.org/abs/2403.07353 ,  951kb)
------------------------------------------------------------------------------
\\
arXiv:2203.02115
replaced with revised version Wed, 13 Mar 2024 06:07:52 GMT   (5098kb,D)

Title: Towards Benchmarking and Evaluating Deepfake Detection
Authors: Chenhao Lin, Jingyi Deng, Pengbin Hu, Chao Shen, Qian Wang, Qi Li
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2203.02115 ,  5098kb)
------------------------------------------------------------------------------
\\
arXiv:2301.10164 (*cross-listing*)
replaced with revised version Wed, 13 Mar 2024 11:43:22 GMT   (3389kb,D)

Title: Lowering Detection in Sport Climbing Based on Orientation of the Sensor
  Enhanced Quickdraw
Authors: Sadaf Moaveninejad, Andrea Janes, Camillo Porcaro
Categories: eess.SP cs.AI cs.LG cs.RO
Comments: arXiv admin note: substantial text overlap with arXiv:2211.02680
Journal-ref: Proceedings of the 18th EAI International Conference on Body Area
  Networks. EAI, 2023
\\ ( https://arxiv.org/abs/2301.10164 ,  3389kb)
------------------------------------------------------------------------------
\\
arXiv:2302.08913
replaced with revised version Wed, 13 Mar 2024 16:04:03 GMT   (3491kb,D)

Title: Referential communication in heterogeneous communities of pre-trained
  visual deep networks
Authors: Mat\'eo Mahaut, Francesca Franzon, Roberto Dess\`i, Marco Baroni
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2302.08913 ,  3491kb)
------------------------------------------------------------------------------
\\
arXiv:2304.02649 (*cross-listing*)
replaced with revised version Wed, 13 Mar 2024 14:20:40 GMT   (2649kb,D)

Title: Medical Multimodal-Multitask Foundation Model for Superior Chest CT
  Performance
Authors: Chuang Niu, Qing Lyu, Christopher D. Carothers, Parisa Kaviani, Josh
  Tan, Pingkun Yan, Mannudeep K. Kalra, Christopher T. Whitlow, Ge Wang
Categories: eess.IV cs.AI cs.CV
\\ ( https://arxiv.org/abs/2304.02649 ,  2649kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12333
replaced with revised version Tue, 12 Mar 2024 21:40:53 GMT   (27824kb,D)

Title: GRACE: Loss-Resilient Real-Time Video through Neural Codecs
Authors: Yihua Cheng, Ziyi Zhang, Hanchen Li, Anton Arapin, Yue Zhang, Qizheng
  Zhang, Yuhan Liu, Xu Zhang, Francis Y. Yan, Amrita Mazumdar, Nick Feamster,
  Junchen Jiang
Categories: cs.MM cs.AI cs.NI
\\ ( https://arxiv.org/abs/2305.12333 ,  27824kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08695 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 19:14:22 GMT   (4498kb,D)

Title: A generative artificial intelligence framework based on a molecular
  diffusion model for the design of metal-organic frameworks for carbon capture
Authors: Hyun Park, Xiaoli Yan, Ruijie Zhu, E. A. Huerta, Santanu Chaudhuri,
  Donny Cooper, Ian Foster, Emad Tajkhorshid
Categories: cond-mat.mtrl-sci cs.AI
Comments: 25 pages, 17 figures, 6 tables, accepted to Nature Communications
  Chemistry. This work was awarded the HPCwire 2023 Editors' Choice Awards for
  Best Use of High Performance Data Analytics \& Artificial Intelligence see
  https://www.hpcwire.com/2023-readers-editors-choice-data-analytics-ai/
ACM-class: I.2
Journal-ref: Commun Chem 7, 21 (2024)
DOI: 10.1038/s42004-023-01090-2
\\ ( https://arxiv.org/abs/2306.08695 ,  4498kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04440
replaced with revised version Wed, 13 Mar 2024 05:31:44 GMT   (134kb)

Title: Big Bang, Low Bar -- Risk Assessment in the Public Arena
Authors: Huw Price
Categories: cs.CY cs.AI
Comments: 8 pages, no figures; revisions in response to reviewers' comment;
  forthcoming in Royal Society Open Science
\\ ( https://arxiv.org/abs/2308.04440 ,  134kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07241
replaced with revised version Wed, 13 Mar 2024 02:34:31 GMT   (15557kb,D)

Title: Context-Aware Planning and Environment-Aware Memory for Instruction
  Following Embodied Agents
Authors: Byeonghwi Kim, Jinyeon Kim, Yuyeong Kim, Cheolhong Min, Jonghyun Choi
Categories: cs.RO cs.AI
Comments: ICCV 2023 (Project page: https://bhkim94.github.io/projects/CAPEAM)
\\ ( https://arxiv.org/abs/2308.07241 ,  15557kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09387
replaced with revised version Wed, 13 Mar 2024 02:37:47 GMT   (20254kb,D)

Title: Multi-Level Compositional Reasoning for Interactive Instruction
  Following
Authors: Suvaansh Bhambri, Byeonghwi Kim, Jonghyun Choi
Categories: cs.RO cs.AI
Comments: AAAI 2023 (Oral) (Project page:
  https://bhkim94.github.io/projects/MCR-Agent)
\\ ( https://arxiv.org/abs/2308.09387 ,  20254kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10156
replaced with revised version Wed, 13 Mar 2024 12:16:20 GMT   (15278kb,D)

Title: SSMG: Spatial-Semantic Map Guided Diffusion Model for Free-form
  Layout-to-Image Generation
Authors: Chengyou Jia, Minnan Luo, Zhuohang Dang, Guang Dai, Xiaojun Chang,
  Mengmeng Wang, Jingdong Wang
Categories: cs.CV cs.AI
Comments: Accepted to AAAI 2024
Journal-ref: 38th AAAI Conference on Artificial Intelligence (AAAI2024),
  Vancouver, BC, Canada, 2024
\\ ( https://arxiv.org/abs/2308.10156 ,  15278kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10204
replaced with revised version Wed, 13 Mar 2024 03:05:52 GMT   (1057kb,D)

Title: ChatEDA: A Large Language Model Powered Autonomous Agent for EDA
Authors: Zhuolun He, Haoyuan Wu, Xinyun Zhang, Xufeng Yao, Su Zheng, Haisheng
  Zheng, Bei Yu
Categories: cs.AR cs.AI
\\ ( https://arxiv.org/abs/2308.10204 ,  1057kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00582
replaced with revised version Wed, 13 Mar 2024 03:42:31 GMT   (5974kb,D)

Title: Pink: Unveiling the Power of Referential Comprehension for Multi-modal
  LLMs
Authors: Shiyu Xuan, Qingpei Guo, Ming Yang, Shiliang Zhang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2310.00582 ,  5974kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05737
replaced with revised version Wed, 13 Mar 2024 05:34:20 GMT   (7027kb,D)

Title: Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation
Authors: Lijun Yu, Jos\'e Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk
  Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu,
  Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A.
  Ross, Lu Jiang
Categories: cs.CV cs.AI cs.MM
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.05737 ,  7027kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08873
replaced with revised version Wed, 13 Mar 2024 02:53:30 GMT   (15050kb,D)

Title: Interactive Navigation in Environments with Traversable Obstacles Using
  Large Language and Vision-Language Models
Authors: Zhen Zhang, Anran Lin, Chun Wai Wong, Xiangyu Chu, Qi Dou, and K. W.
  Samuel Au
Categories: cs.RO cs.AI
Comments: Accepted by 2024 IEEE International Conference on Robotics and
  Automation (ICRA), 7 pages, 8 figures
\\ ( https://arxiv.org/abs/2310.08873 ,  15050kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10747
replaced with revised version Tue, 12 Mar 2024 21:01:38 GMT   (4665kb,D)

Title: Safety-aware Causal Representation for Trustworthy Offline Reinforcement
  Learning in Autonomous Driving
Authors: Haohong Lin, Wenhao Ding, Zuxin Liu, Yaru Niu, Jiacheng Zhu, Yuming
  Niu, Ding Zhao
Categories: cs.RO cs.AI cs.LG
\\ ( https://arxiv.org/abs/2311.10747 ,  4665kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16480
replaced with revised version Wed, 13 Mar 2024 02:31:30 GMT   (2749kb,D)

Title: WsiCaption: Multiple Instance Generation of Pathology Reports for
  Gigapixel Whole-Slide Images
Authors: Pingyi Chen, Honglin Li, Chenglu Zhu, Sunyi Zheng, Zhongyi Shui, Lin
  Yang
Categories: cs.CV cs.AI cs.CL
\\ ( https://arxiv.org/abs/2311.16480 ,  2749kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18765
replaced with revised version Wed, 13 Mar 2024 08:47:32 GMT   (3840kb,D)

Title: MLLMs-Augmented Visual-Language Representation Learning
Authors: Yanqing Liu, Kai Wang, Wenqi Shao, Ping Luo, Yu Qiao, Mike Zheng Shou,
  Kaipeng Zhang and Yang You
Categories: cs.CV cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2311.18765 ,  3840kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01614
replaced with revised version Tue, 12 Mar 2024 23:14:33 GMT   (27857kb,D)

Title: GPT-4V(ision) is a Generalist Web Agent, if Grounded
Authors: Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, Yu Su
Categories: cs.IR cs.AI cs.CL cs.CV
\\ ( https://arxiv.org/abs/2401.01614 ,  27857kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03246
replaced with revised version Wed, 13 Mar 2024 07:55:38 GMT   (29643kb,D)

Title: SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM
Authors: Mingrui Li, Shuhong Liu, Heng Zhou, Guohao Zhu, Na Cheng, Tianchen
  Deng, Hongyu Wang
Categories: cs.CV cs.AI cs.RO
\\ ( https://arxiv.org/abs/2402.03246 ,  29643kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10659
replaced with revised version Tue, 12 Mar 2024 19:12:55 GMT   (23971kb,D)

Title: Network Formation and Dynamics Among Multi-LLMs
Authors: Marios Papachristou, Yuan Yuan
Categories: cs.SI cs.AI cs.CL cs.MA
\\ ( https://arxiv.org/abs/2402.10659 ,  23971kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18920
replaced with revised version Wed, 13 Mar 2024 14:13:04 GMT   (43301kb,D)

Title: Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation
Authors: Dongliang Cao, Marvin Eisenberger, Nafie El Amrani, Daniel Cremers,
  Florian Bernard
Categories: cs.CV cs.AI cs.CG
Comments: accepted by CVPR2024
\\ ( https://arxiv.org/abs/2402.18920 ,  43301kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03864
replaced with revised version Wed, 13 Mar 2024 00:50:05 GMT   (1737kb,D)

Title: Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious
  Challenges in Multimodal Reasoning
Authors: Deepanway Ghosal, Vernon Toh Yan Han, Chia Yew Ken, Soujanya Poria
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.03864 ,  1737kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07355 (*cross-listing*)
replaced with revised version Wed, 13 Mar 2024 02:29:29 GMT   (811kb)

Title: Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO
  Systems
Authors: Junyong Shin, Yujin Kang, Yo-Seb Jeon
Categories: eess.SP cs.AI cs.CV
\\ ( https://arxiv.org/abs/2403.07355 ,  811kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07743 (*cross-listing*)
replaced with revised version Wed, 13 Mar 2024 11:20:19 GMT   (54337kb,D)

Title: Equipping Computational Pathology Systems with Artifact Processing
  Pipelines: A Showcase for Computation and Performance Trade-offs
Authors: Neel Kanwal, Farbod Khoraminia, Umay Kiraz, Andres Mosquera-Zamudio,
  Carlos Monteagudo, Emiel A.M. Janssen, Tahlita C.M. Zuiverloon, Chunmig Rong,
  and Kjersti Engan
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: Submitted to BMC Medical Informatics and Decision Making Journal
\\ ( https://arxiv.org/abs/2403.07743 ,  54337kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02684
replaced with revised version Wed, 13 Mar 2024 12:24:06 GMT   (4836kb,D)

Title: Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE
Authors: Zeren Chen, Ziqin Wang, Zhen Wang, Huayang Liu, Zhenfei Yin, Si Liu,
  Lu Sheng, Wanli Ouyang, Yu Qiao, Jing Shao
Categories: cs.CV cs.CL
Comments: 22 pages, 12 figures. Accepted in ICLR 2024
\\ ( https://arxiv.org/abs/2311.02684 ,  4836kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03522
replaced with revised version Wed, 13 Mar 2024 09:50:40 GMT   (626kb,D)

Title: Non-verbal information in spontaneous speech -- towards a new framework
  of analysis
Authors: Tirza Biron, Moshe Barboy, Eran Ben-Artzy, Alona Golubchik, Yanir
  Marmor, Smadar Szekely, Yaron Winter, David Harel
Categories: cs.SD cs.CL cs.LG eess.AS
\\ ( https://arxiv.org/abs/2403.03522 ,  626kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06199
replaced with revised version Wed, 13 Mar 2024 01:56:18 GMT   (8354kb,D)

Title: Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small
  Language Models
Authors: Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu, Chaomin Shen,
  Yaxin Peng, Zhicai Ou, Feifei Feng, Jian Tang
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2403.06199 ,  8354kb)
------------------------------------------------------------------------------
\\
arXiv:2110.08691
replaced with revised version Wed, 13 Mar 2024 04:45:31 GMT   (359kb,D)

Title: Terminal Embeddings in Sublinear Time
Authors: Yeshwanth Cherapanamjeri, Jelani Nelson
Categories: cs.DS cs.CG cs.LG stat.ML
Journal-ref: TheoretiCS, Volume 3 (2024), Article 6, 1-52
DOI: 10.46298/theoretics.24.6
\\ ( https://arxiv.org/abs/2110.08691 ,  359kb)
------------------------------------------------------------------------------
\\
arXiv:2301.00448 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 18:48:40 GMT   (2199kb,D)

Title: Unsupervised Acoustic Scene Mapping Based on Acoustic Features and
  Dimensionality Reduction
Authors: Idan Cohen, Ofir Lindenbaum and Sharon Gannot
Categories: eess.AS cs.LG cs.SD
\\ ( https://arxiv.org/abs/2301.00448 ,  2199kb)
------------------------------------------------------------------------------
\\
arXiv:2301.04431 (*cross-listing*)
replaced with revised version Wed, 13 Mar 2024 12:01:29 GMT   (760kb,D)

Title: Adaptive proximal algorithms for convex optimization under local
  Lipschitz continuity of the gradient
Authors: Puya Latafat, Andreas Themelis, Lorenzo Stella, and Panagiotis
  Patrinos
Categories: math.OC cs.LG
MSC-class: 65K05, 90C06, 90C25, 90C30, 90C47
\\ ( https://arxiv.org/abs/2301.04431 ,  760kb)
------------------------------------------------------------------------------
\\
arXiv:2301.07628
replaced with revised version Wed, 13 Mar 2024 08:02:51 GMT   (844kb,D)

Title: Universal Neural-Cracking-Machines: Self-Configurable Password Models
  from Auxiliary Data
Authors: Dario Pasquini, Giuseppe Ateniese and Carmela Troncoso
Categories: cs.CR cs.LG
Comments: Appearing in the proceedings of the 45th IEEE Symposium on Security
  and Privacy S&P 2024
\\ ( https://arxiv.org/abs/2301.07628 ,  844kb)
------------------------------------------------------------------------------
\\
arXiv:2302.14615 (*cross-listing*)
replaced with revised version Wed, 13 Mar 2024 17:11:20 GMT   (22171kb,D)

Title: Randomized Kaczmarz in Adversarial Distributed Setting
Authors: Longxiu Huang, Xia Li, Deanna Needell
Categories: math.OC cs.CR cs.LG cs.NA math.NA
MSC-class: 65F20, 65F10, 65K10
\\ ( https://arxiv.org/abs/2302.14615 ,  22171kb)
------------------------------------------------------------------------------
\\
arXiv:2304.13764 (*cross-listing*)
replaced with revised version Wed, 13 Mar 2024 07:48:50 GMT   (34768kb,D)

Title: PhagoStat a scalable and interpretable end to end framework for
  efficient quantification of cell phagocytosis in neurodegenerative disease
  studies
Authors: Mehdi Ounissi, Morwena Latouche and Daniel Racoceanu
Categories: eess.IV cs.CV cs.LG
DOI: 10.1038/s41598-024-56081-7
\\ ( https://arxiv.org/abs/2304.13764 ,  34768kb)
------------------------------------------------------------------------------
\\
arXiv:2305.03053
replaced with revised version Wed, 13 Mar 2024 02:04:06 GMT   (7028kb,D)

Title: ZipIt! Merging Models from Different Tasks without Training
Authors: George Stoica, Daniel Bolya, Jakob Bjorner, Pratik Ramesh, Taylor
  Hearn, Judy Hoffman
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2305.03053 ,  7028kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16583 (*cross-listing*)
replaced with revised version Wed, 13 Mar 2024 03:36:44 GMT   (687kb,D)

Title: Detecting Errors in a Numerical Response via any Regression Model
Authors: Hang Zhou, Jonas Mueller, Mayank Kumar, Jane-Ling Wang and Jing Lei
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2305.16583 ,  687kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05014 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 22:55:57 GMT   (3165kb,D)

Title: Learning Closed-form Equations for Subgrid-scale Closures from
  High-fidelity Data: Promises and Challenges
Authors: Karan Jakhar, Yifei Guan, Rambod Mojgani, Ashesh Chattopadhyay, and
  Pedram Hassanzadeh
Categories: physics.flu-dyn cs.LG physics.ao-ph
Comments: 40 pages, 4 figures. The codes and data used in this work can be
  found at https://github.com/jakharkaran/EqsDiscovery_2D-FHIT_RBC and
  https://doi.org/10.5281/zenodo.7500647, respectively
MSC-class: 76F65 (Primary) 86A08, 68T01, 76F05, 76F35 (Secondary)
ACM-class: J.2; I.2.0; G.1.8
\\ ( https://arxiv.org/abs/2306.05014 ,  3165kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05093
replaced with revised version Tue, 12 Mar 2024 22:29:18 GMT   (2896kb,D)

Title: Investigating the Effect of Misalignment on Membership Privacy in the
  White-box Setting
Authors: Ana-Maria Cretu, Daniel Jones, Yves-Alexandre de Montjoye, Shruti
  Tople
Categories: cs.CR cs.LG
Comments: To appear in the Proceedings on Privacy Enhancing Technologies
  (PoPETs 2024)
\\ ( https://arxiv.org/abs/2306.05093 ,  2896kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02037 (*cross-listing*)
replaced with revised version Wed, 13 Mar 2024 08:11:18 GMT   (1709kb,D)

Title: Reverse Diffusion Monte Carlo
Authors: Xunpeng Huang, Hanze Dong, Yifan Hao, Yi-An Ma, Tong Zhang
Categories: stat.ML cs.LG math.OC
Comments: 44 pages, 16 figures, ICLR 2024
\\ ( https://arxiv.org/abs/2307.02037 ,  1709kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08509 (*cross-listing*)
replaced with revised version Wed, 13 Mar 2024 14:18:59 GMT   (1597kb,D)

Title: Kernel-Based Testing for Single-Cell Differential Analysis
Authors: Anthony Ozier-Lafontaine and Camille Fourneaux and Ghislain Durif and
  C\'eline Vallot and Olivier Gandrillon and Sandrine Giraud and Bertrand
  Michel and Franck Picard
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2307.08509 ,  1597kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02535
replaced with revised version Tue, 12 Mar 2024 21:39:33 GMT   (15463kb,D)

Title: Learning to Generate Training Datasets for Robust Semantic Segmentation
Authors: Marwane Hariat, Olivier Laurent, R\'emi Kazmierczak, Shihao Zhang,
  Andrei Bursuc, Angela Yao and Gianni Franchi
Categories: cs.CV cs.LG
Comments: Published as a conference paper at WACV 2024
Journal-ref: Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision. 2024. p. 3894-3905
\\ ( https://arxiv.org/abs/2308.02535 ,  15463kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05115 (*cross-listing*)
replaced with revised version Wed, 13 Mar 2024 05:02:32 GMT   (3603kb,D)

Title: PTransIPs: Identification of phosphorylation sites enhanced by protein
  PLM embeddings
Authors: Ziyang Xu, Haitian Zhong, Bingrui He, Xueying Wang and Tianchi Lu
Categories: q-bio.QM cs.LG
\\ ( https://arxiv.org/abs/2308.05115 ,  3603kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09262 (*cross-listing*)
replaced with revised version Wed, 13 Mar 2024 13:15:11 GMT   (288kb,D)

Title: Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality
  Assessment Model
Authors: Ryandhimas E. Zezario, Bo-Ren Brian Bai, Chiou-Shann Fuh, Hsin-Min
  Wang and Yu Tsao
Categories: eess.AS cs.LG cs.SD
Comments: Accepted to IEEE ICASSP 2024
\\ ( https://arxiv.org/abs/2308.09262 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09945 (*cross-listing*)
replaced with revised version Wed, 13 Mar 2024 11:21:11 GMT   (2124kb,D)

Title: Dual Branch Deep Learning Network for Detection and Stage Grading of
  Diabetic Retinopathy
Authors: Hossein Shakibania, Sina Raoufi, Behnam Pourafkham, Hassan Khotanlou,
  and Muharram Mansoorizadeh
Categories: eess.IV cs.CV cs.LG
Comments: Published in the Biomedical Signal Processing & Control journal, 16
  pages, 13 figures
DOI: 10.1016/j.bspc.2024.106168
\\ ( https://arxiv.org/abs/2308.09945 ,  2124kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06569
replaced with revised version Wed, 13 Mar 2024 00:15:08 GMT   (3726kb,D)

Title: Promises of Deep Kernel Learning for Control Synthesis
Authors: Robert Reed, Luca Laurenti, Morteza Lahijanian
Categories: eess.SY cs.LG cs.SY
Comments: 9 pages, 4 figures, 3 tables
\\ ( https://arxiv.org/abs/2309.06569 ,  3726kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01225 (*cross-listing*)
replaced with revised version Wed, 13 Mar 2024 12:21:06 GMT   (302kb,D)

Title: A path-norm toolkit for modern networks: consequences, promises and
  challenges
Authors: Antoine Gonon, Nicolas Brisebarre, Elisa Riccietti, R\'emi Gribonval
Categories: stat.ML cs.LG math.ST stat.TH
\\ ( https://arxiv.org/abs/2310.01225 ,  302kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07240
replaced with revised version Wed, 13 Mar 2024 05:55:39 GMT   (13491kb,D)

Title: CacheGen: Fast Context Loading for Language Model Applications via KV
  Cache Streaming
Authors: Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang,
  Qizheng Zhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan,
  Michael Maire, Henry Hoffmann, Ari Holtzman, Junchen Jiang
Categories: cs.NI cs.LG
\\ ( https://arxiv.org/abs/2310.07240 ,  13491kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09254 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 19:24:06 GMT   (26950kb,D)

Title: Entropic (Gromov) Wasserstein Flow Matching with GENOT
Authors: Dominik Klein, Th\'eo Uscidda, Fabian Theis, Marco Cuturi
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2310.09254 ,  26950kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18554 (*cross-listing*)
replaced with revised version Wed, 13 Mar 2024 02:58:07 GMT   (1889kb,D)

Title: Improved Regret Bounds of (Multinomial) Logistic Bandits via
  Regret-to-Confidence-Set Conversion
Authors: Junghyun Lee, Se-Young Yun, Kwang-Sung Jun
Categories: stat.ML cs.LG
Comments: 39 pages, 1 figure, 1 table; Accepted to the 27th International
  Conference on Artificial Intelligence and Statistics (AISTATS 2024) (ver2:
  fixed some errors and significantly expanded discussions on various parts,
  such as related work. ver3: fixed some minor typos)
\\ ( https://arxiv.org/abs/2310.18554 ,  1889kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03690 (*cross-listing*)
replaced with revised version Wed, 13 Mar 2024 12:04:36 GMT   (17982kb,D)

Title: Inverse Design of Vitrimeric Polymers by Molecular Dynamics and
  Generative Modeling
Authors: Yiwen Zheng, Prakash Thakolkaran, Jake A. Smith, Ziheng Lu, Shuxin
  Zheng, Bichlien H. Nguyen, Siddhant Kumar, Aniruddh Vashisth
Categories: cond-mat.mtrl-sci cs.LG
\\ ( https://arxiv.org/abs/2312.03690 ,  17982kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03853
replaced with revised version Wed, 13 Mar 2024 14:52:47 GMT   (11220kb,D)

Title: Dr. Jekyll and Mr. Hyde: Two Faces of LLMs
Authors: Matteo Gioele Collu, Tom Janssen-Groesbeek, Stefanos Koffas, Mauro
  Conti, Stjepan Picek
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2312.03853 ,  11220kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09481
replaced with revised version Wed, 13 Mar 2024 15:24:19 GMT   (2442kb,D)

Title: Continual Adversarial Defense
Authors: Qian Wang, Yaoyao Liu, Hefei Ling, Yingwei Li, Qihao Liu, Ping Li,
  Jiazhong Chen, Alan Yuille, Ning Yu
Categories: cs.CV cs.CR cs.LG
\\ ( https://arxiv.org/abs/2312.09481 ,  2442kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11598
replaced with revised version Wed, 13 Mar 2024 16:29:50 GMT   (3893kb,D)

Title: SkillDiffuser: Interpretable Hierarchical Planning via Skill
  Abstractions in Diffusion-Based Task Execution
Authors: Zhixuan Liang, Yao Mu, Hengbo Ma, Masayoshi Tomizuka, Mingyu Ding,
  Ping Luo
Categories: cs.RO cs.CV cs.LG
Comments: Accepted by CVPR 2024. Camera ready version. Project page:
  https://skilldiffuser.github.io/
\\ ( https://arxiv.org/abs/2312.11598 ,  3893kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01386 (*cross-listing*)
replaced with revised version Wed, 13 Mar 2024 07:14:16 GMT   (2961kb)

Title: Tissue Artifact Segmentation and Severity Analysis for Automated
  Diagnosis Using Whole Slide Images
Authors: Galib Muhammad Shahriar Himel
Categories: eess.IV cs.CV cs.LG
Comments: Master's thesis, 60 pages, 21 figures, 16 tables
\\ ( https://arxiv.org/abs/2401.01386 ,  2961kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10294
replaced with revised version Wed, 13 Mar 2024 16:08:01 GMT   (233kb,D)

Title: Tight Group-Level DP Guarantees for DP-SGD with Sampling via Mixture of
  Gaussians Mechanisms
Authors: Arun Ganesh
Categories: cs.CR cs.LG
Comments: v2: Added links to open-source implementation of PLD accounting for
  MoG mechanisms
\\ ( https://arxiv.org/abs/2401.10294 ,  233kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15281
replaced with revised version Wed, 13 Mar 2024 08:34:47 GMT   (4279kb,D)

Title: Neural Implicit Swept Volume Models for Fast Collision Detection
Authors: Dominik Joho, Jonas Schwinn, Kirill Safronov
Categories: cs.RO cs.LG
Comments: To be published at ICRA 2024. Dominik Joho and Jonas Schwinn have
  equal contribution
\\ ( https://arxiv.org/abs/2402.15281 ,  4279kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04661
replaced with revised version Tue, 12 Mar 2024 20:52:02 GMT   (214kb,D)

Title: Dynamic Cross Attention for Audio-Visual Person Verification
Authors: R. Gnana Praveen, Jahangir Alam
Categories: cs.CV cs.LG cs.SD eess.AS
Comments: Accepted to FG2024
\\ ( https://arxiv.org/abs/2403.04661 ,  214kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05452 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 18:24:32 GMT   (17619kb,D)

Title: The R2D2 deep neural network series paradigm for fast precision imaging
  in radio astronomy
Authors: Amir Aghabiglou, Chung San Chu, Arwa Dabbech, Yves Wiaux
Categories: astro-ph.IM cs.CV cs.LG
Comments: Confusing typo on the target dynamic range corrected, submitted to
  ApJS
\\ ( https://arxiv.org/abs/2403.05452 ,  17619kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06153 (*cross-listing*)
replaced with revised version Tue, 12 Mar 2024 23:10:16 GMT   (9864kb,D)

Title: The AL$\ell_0$CORE Tensor Decomposition for Sparse Count Data
Authors: John Hood, Aaron Schein
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2403.06153 ,  9864kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06888 (*cross-listing*)
replaced with revised version Wed, 13 Mar 2024 02:07:06 GMT   (1915kb)

Title: Process signature-driven high spatio-temporal resolution alignment of
  multimodal data
Authors: Abhishek Hanchate, Himanshu Balhara, Vishal S. Chindepalli, Satish
  T.S. Bukkapatnam
Categories: physics.data-an cs.LG physics.app-ph
\\ ( https://arxiv.org/abs/2403.06888 ,  1915kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
