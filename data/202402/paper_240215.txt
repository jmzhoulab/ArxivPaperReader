Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年2月15日 17:22
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Tue 13 Feb 24 19:00:00 GMT  to  Wed 14 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.08755
Date: Tue, 13 Feb 2024 19:46:39 GMT   (709kb,D)

Title: LLM-driven Imitation of Subrational Behavior : Illusion or Reality?
Authors: Andrea Coletta, Kshama Dwarakanath, Penghang Liu, Svitlana Vyetrenko,
  Tucker Balch
Categories: cs.AI econ.GN q-fin.EC
\\
  Modeling subrational agents, such as humans or economic households, is
inherently challenging due to the difficulty in calibrating reinforcement
learning models or collecting data that involves human subjects. Existing work
highlights the ability of Large Language Models (LLMs) to address complex
reasoning tasks and mimic human communication, while simulation using LLMs as
agents shows emergent social behaviors, potentially improving our comprehension
of human conduct. In this paper, we propose to investigate the use of LLMs to
generate synthetic human demonstrations, which are then used to learn
subrational agent policies though Imitation Learning. We make an assumption
that LLMs can be used as implicit computational models of humans, and propose a
framework to use synthetic demonstrations derived from LLMs to model
subrational behaviors that are characteristic of humans (e.g., myopic behavior
or preference for risk aversion). We experimentally evaluate the ability of our
framework to model sub-rationality through four simple scenarios, including the
well-researched ultimatum game and marshmallow experiment. To gain confidence
in our framework, we are able to replicate well-established findings from prior
human studies associated with the above scenarios. We conclude by discussing
the potential benefits, challenges and limitations of our framework.
\\ ( https://arxiv.org/abs/2402.08755 ,  709kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08772
Date: Tue, 13 Feb 2024 20:07:58 GMT   (459kb,D)

Title: Optimal Task Assignment and Path Planning using Conflict-Based Search
  with Precedence and Temporal Constraints
Authors: Yu Quan Chong, Jiaoyang Li, Katia Sycara
Categories: cs.AI cs.MA
ACM-class: I.2.11
\\
  The Multi-Agent Path Finding (MAPF) problem entails finding collision-free
paths for a set of agents, guiding them from their start to goal locations.
However, MAPF does not account for several practical task-related constraints.
For example, agents may need to perform actions at goal locations with specific
execution times, adhering to predetermined orders and timeframes. Moreover,
goal assignments may not be predefined for agents, and the optimization
objective may lack an explicit definition. To incorporate task assignment, path
planning, and a user-defined objective into a coherent framework, this paper
examines the Task Assignment and Path Finding with Precedence and Temporal
Constraints (TAPF-PTC) problem. We augment Conflict-Based Search (CBS) to
simultaneously generate task assignments and collision-free paths that adhere
to precedence and temporal constraints, maximizing an objective quantified by
the return from a user-defined reward function in reinforcement learning (RL).
Experimentally, we demonstrate that our algorithm, CBS-TA-PTC, can solve highly
challenging bomb-defusing tasks with precedence and temporal constraints
efficiently relative to MARL and adapted Target Assignment and Path Finding
(TAPF) methods.
\\ ( https://arxiv.org/abs/2402.08772 ,  459kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08780
Date: Tue, 13 Feb 2024 20:29:36 GMT   (2283kb,D)

Title: Enhanced Deep Q-Learning for 2D Self-Driving Cars: Implementation and
  Evaluation on a Custom Track Environment
Authors: Sagar Pathak, Bidhya Shrestha and Kritish Pahi
Categories: cs.AI
Comments: 8 pages, 8 figures
\\
  This research project presents the implementation of a Deep Q-Learning
Network (DQN) for a self-driving car on a 2-dimensional (2D) custom track, with
the objective of enhancing the DQN network's performance. It encompasses the
development of a custom driving environment using Pygame on a track surrounding
the University of Memphis map, as well as the design and implementation of the
DQN model. The algorithm utilizes data from 7 sensors installed in the car,
which measure the distance between the car and the track. These sensors are
positioned in front of the vehicle, spaced 20 degrees apart, enabling them to
sense a wide area ahead. We successfully implemented the DQN and also a
modified version of the DQN with a priority-based action selection mechanism,
which we refer to as modified DQN. The model was trained over 1000 episodes,
and the average reward received by the agent was found to be around 40, which
is approximately 60% higher than the original DQN and around 50% higher than
the vanilla neural network.
\\ ( https://arxiv.org/abs/2402.08780 ,  2283kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08806
Date: Tue, 13 Feb 2024 21:24:21 GMT   (54kb,AD)

Title: Combining Insights From Multiple Large Language Models Improves
  Diagnostic Accuracy
Authors: Gioele Barabucci, Victor Shia, Eugene Chu, Benjamin Harack, Nathan Fu
Categories: cs.AI
Comments: 5 pages, 2 figures, 1 table
ACM-class: I.2.1; J.3
\\
  Background: Large language models (LLMs) such as OpenAI's GPT-4 or Google's
PaLM 2 are proposed as viable diagnostic support tools or even spoken of as
replacements for "curbside consults". However, even LLMs specifically trained
on medical topics may lack sufficient diagnostic accuracy for real-life
applications.
  Methods: Using collective intelligence methods and a dataset of 200 clinical
vignettes of real-life cases, we assessed and compared the accuracy of
differential diagnoses obtained by asking individual commercial LLMs (OpenAI
GPT-4, Google PaLM 2, Cohere Command, Meta Llama 2) against the accuracy of
differential diagnoses synthesized by aggregating responses from combinations
of the same LLMs.
  Results: We find that aggregating responses from multiple, various LLMs leads
to more accurate differential diagnoses (average accuracy for 3 LLMs:
$75.3\%\pm 1.6pp$) compared to the differential diagnoses produced by single
LLMs (average accuracy for single LLMs: $59.0\%\pm 6.1pp$).
  Discussion: The use of collective intelligence methods to synthesize
differential diagnoses combining the responses of different LLMs achieves two
of the necessary steps towards advancing acceptance of LLMs as a diagnostic
support tool: (1) demonstrate high diagnostic accuracy and (2) eliminate
dependence on a single commercial vendor.
\\ ( https://arxiv.org/abs/2402.08806 ,  54kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08859
Date: Wed, 14 Feb 2024 00:04:33 GMT   (2096kb,D)

Title: Large Language Model with Graph Convolution for Recommendation
Authors: Yingpeng Du, Ziyan Wang, Zhu Sun, Haoyan Chua, Hongzhi Liu, Zhonghai
  Wu, Yining Ma, Jie Zhang, Youchen Sun
Categories: cs.AI
\\
  In recent years, efforts have been made to use text information for better
user profiling and item characterization in recommendations. However, text
information can sometimes be of low quality, hindering its effectiveness for
real-world applications. With knowledge and reasoning capabilities capsuled in
Large Language Models (LLMs), utilizing LLMs emerges as a promising way for
description improvement. However, existing ways of prompting LLMs with raw
texts ignore structured knowledge of user-item interactions, which may lead to
hallucination problems like inconsistent description generation. To this end,
we propose a Graph-aware Convolutional LLM method to elicit LLMs to capture
high-order relations in the user-item graph. To adapt text-based LLMs with
structured graphs, We use the LLM as an aggregator in graph processing,
allowing it to understand graph-based information step by step. Specifically,
the LLM is required for description enhancement by exploring multi-hop
neighbors layer by layer, thereby propagating information progressively in the
graph. To enable LLMs to capture large-scale graph information, we break down
the description task into smaller parts, which drastically reduces the context
length of the token input with each step. Extensive experiments on three
real-world datasets show that our method consistently outperforms
state-of-the-art methods.
\\ ( https://arxiv.org/abs/2402.08859 ,  2096kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08869
Date: Wed, 14 Feb 2024 00:30:18 GMT   (637kb)

Title: ScamSpot: Fighting Financial Fraud in Instagram Comments
Authors: Stefan Erben and Andreas Waldis
Categories: cs.AI
Comments: EACL 2024 Demo Paper, 11 pages
\\
  The long-standing problem of spam and fraudulent messages in the comment
sections of Instagram pages in the financial sector claims new victims every
day. Instagram's current spam filter proves inadequate, and existing research
approaches are primarily confined to theoretical concepts. Practical
implementations with evaluated results are missing. To solve this problem, we
propose ScamSpot, a comprehensive system that includes a browser extension, a
fine-tuned BERT model and a REST API. This approach ensures public
accessibility of our results for Instagram users using the Chrome browser.
Furthermore, we conduct a data annotation study, shedding light on the reasons
and causes of the problem and evaluate the system through user feedback and
comparison with existing models. ScamSpot is an open-source project and is
publicly available at https://scamspot.github.io/.
\\ ( https://arxiv.org/abs/2402.08869 ,  637kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08939
Date: Wed, 14 Feb 2024 04:50:18 GMT   (7955kb,D)

Title: Premise Order Matters in Reasoning with Large Language Models
Authors: Xinyun Chen, Ryan A. Chi, Xuezhi Wang, Denny Zhou
Categories: cs.AI cs.CL
Comments: Xinyun and Ryan contribute equally
\\
  Large language models (LLMs) have accomplished remarkable reasoning
performance in various domains. However, in the domain of reasoning tasks, we
discover a frailty: LLMs are surprisingly brittle to the ordering of the
premises, despite the fact that such ordering does not alter the underlying
task. In particular, we observe that LLMs achieve the best performance when the
premise order aligns with the context required in intermediate reasoning steps.
For example, in deductive reasoning tasks, presenting the premises in the same
order as the ground truth proof in the prompt (as opposed to random ordering)
drastically increases the model's accuracy. We first examine the effect of
premise ordering on deductive reasoning on a variety of LLMs, and our
evaluation shows that permuting the premise order can cause a performance drop
of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to
examine the ordering effect for mathematical problem-solving, and we again
observe a significant drop in accuracy, relative to the original GSM8K
benchmark.
\\ ( https://arxiv.org/abs/2402.08939 ,  7955kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08955
Date: Wed, 14 Feb 2024 05:52:23 GMT   (395kb,D)

Title: Using Counterfactual Tasks to Evaluate the Generality of Analogical
  Reasoning in Large Language Models
Authors: Martha Lewis and Melanie Mitchell
Categories: cs.AI cs.CL
\\
  Large language models (LLMs) have performed well on several reasoning
benchmarks, including ones that test analogical reasoning abilities. However,
it has been debated whether they are actually performing humanlike abstract
reasoning or instead employing less general processes that rely on similarity
to what has been seen in their training data. Here we investigate the
generality of analogy-making abilities previously claimed for LLMs (Webb,
Holyoak, & Lu, 2023). We take one set of analogy problems used to evaluate LLMs
and create a set of "counterfactual" variants-versions that test the same
abstract reasoning abilities but that are likely dissimilar from any
pre-training data. We test humans and three GPT models on both the original and
counterfactual problems, and show that, while the performance of humans remains
high for all the problems, the GPT models' performance declines sharply on the
counterfactual set. This work provides evidence that, despite previously
reported successes of LLMs on analogical reasoning, these models lack the
robustness and generality of human analogy-making.
\\ ( https://arxiv.org/abs/2402.08955 ,  395kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08957
Date: Wed, 14 Feb 2024 05:57:58 GMT   (6123kb,D)

Title: MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data
Authors: Yinya Huang, Xiaohan Lin, Zhengying Liu, Qingxing Cao, Huajian Xin,
  Haiming Wang, Zhenguo Li, Linqi Song, Xiaodan Liang
Categories: cs.AI cs.CL cs.FL cs.LG cs.PL
Journal-ref: ICLR 2024 spotlight
\\
  Recent large language models (LLMs) have witnessed significant advancement in
various tasks, including mathematical reasoning and theorem proving. As these
two tasks require strict and formal multi-step inference, they are appealing
domains for exploring the reasoning ability of LLMs but still face important
challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the
effectiveness of intermediate steps guidance. However, such step-wise
annotation requires heavy labor, leading to insufficient training steps for
current benchmarks. To fill this gap, this work introduces MUSTARD, a data
generation framework that masters uniform synthesis of theorem and proof data
of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It
samples a few mathematical concept seeds as the problem category. (2) Then, it
prompts a generative language model with the sampled concepts to obtain both
the problems and their step-wise formal solutions. (3) Lastly, the framework
utilizes a proof assistant (e.g., Lean Prover) to filter the valid proofs. With
the proposed MUSTARD, we present a theorem-and-proof benchmark MUSTARDSAUCE
with 5,866 valid data points. Each data point contains an informal statement,
an informal proof, and a translated formal proof that passes the prover
validation. We perform extensive analysis and demonstrate that MUSTARD
generates validated high-quality step-by-step data. We further apply the
MUSTARDSAUCE for fine-tuning smaller language models. The fine-tuned Llama 2-7B
achieves a 15.41% average relative performance gain in automated theorem
proving, and 8.18% in math word problems. Codes and data are available at
https://github.com/Eleanor-H/MUSTARD.
\\ ( https://arxiv.org/abs/2402.08957 ,  6123kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08961
Date: Wed, 14 Feb 2024 06:05:37 GMT   (568kb,D)

Title: HyCubE: Efficient Knowledge Hypergraph 3D Circular Convolutional
  Embedding
Authors: Zhao Li, Xin Wang, Jianxin Li, Wenbin Guo, Jun Zhao
Categories: cs.AI
Comments: 11 pages, 5 figures
\\
  Existing knowledge hypergraph embedding methods mainly focused on improving
model performance, but their model structures are becoming more complex and
redundant. Furthermore, due to the inherent complex semantic knowledge, the
computation of knowledge hypergraph embedding models is often very expensive,
leading to low efficiency. In this paper, we propose a feature interaction and
extraction-enhanced 3D circular convolutional embedding model, HyCubE, which
designs a novel 3D circular convolutional neural network and introduces the
alternate mask stack strategy to achieve efficient n-ary knowledge hypergraph
embedding. By adaptively adjusting the 3D circular convolution kernel size and
uniformly embedding the entity position information, HyCubE improves the model
performance with fewer parameters and reaches a better trade-off between model
performance and efficiency. In addition, we use 1-N multilinear scoring based
on the entity mask mechanism to further accelerate the model training
efficiency. Finally, extensive experimental results on all datasets demonstrate
that HyCubE consistently outperforms state-of-the-art baselines, with an
average improvement of 4.08%-10.77% and a maximum improvement of 21.16% across
all metrics. Commendably, HyCubE speeds up by an average of 7.55x and reduces
memory usage by an average of 77.02% compared to the latest state-of-the-art
baselines.
\\ ( https://arxiv.org/abs/2402.08961 ,  568kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08968
Date: Wed, 14 Feb 2024 06:25:50 GMT   (40kb,D)

Title: GrounDial: Human-norm Grounded Safe Dialog Response Generation
Authors: Siwon Kim, Shuyang Dai, Mohammad Kachuee, Shayan Ray, Tara Taghavi,
  and Sungroh Yoon
Categories: cs.AI
Comments: Accepted to findings of EACL 2024
\\
  Current conversational AI systems based on large language models (LLMs) are
known to generate unsafe responses, agreeing to offensive user input or
including toxic content. Previous research aimed to alleviate the toxicity, by
fine-tuning LLM with manually annotated safe dialogue histories. However, the
dependency on additional tuning requires substantial costs. To remove the
dependency, we propose GrounDial, where response safety is achieved by
grounding responses to commonsense social rules without requiring fine-tuning.
A hybrid approach of in-context learning and human-norm-guided decoding of
GrounDial enables the response to be quantitatively and qualitatively safer
even without additional data or tuning.
\\ ( https://arxiv.org/abs/2402.08968 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09046
Date: Wed, 14 Feb 2024 09:43:35 GMT   (2299kb,D)

Title: Inference of Abstraction for a Unified Account of Reasoning and Learning
Authors: Hiroyuki Kido
Categories: cs.AI cs.LG cs.LO
Comments: arXiv admin note: substantial text overlap with arXiv:2402.08646
\\
  Inspired by Bayesian approaches to brain function in neuroscience, we give a
simple theory of probabilistic inference for a unified account of reasoning and
learning. We simply model how data cause symbolic knowledge in terms of its
satisfiability in formal logic. The underlying idea is that reasoning is a
process of deriving symbolic knowledge from data via abstraction, i.e.,
selective ignorance. The logical consequence relation is discussed for its
proof-based theoretical correctness. The MNIST dataset is discussed for its
experiment-based empirical correctness.
\\ ( https://arxiv.org/abs/2402.09046 ,  2299kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09047
Date: Wed, 14 Feb 2024 09:44:28 GMT   (975kb,D)

Title: FGeo-TP: A Language Model-Enhanced Solver for Geometry Problems
Authors: Yiming He, Jia Zou, Xiaokai Zhang, Na Zhu, Tuo Leng
Categories: cs.AI
Comments: 16 pages
\\
  The application of contemporary artificial intelligence techniques to address
geometric problems and automated deductive proof has always been a grand
challenge to the interdiscipline field of mathematics and artificial
Intelligence. This is the fourth article in a series of our works, in our
previous work, we established of a geometric formalized system known as
FormalGeo. Moreover we annotated approximately 7000 geometric problems, forming
the FormalGeo7k dataset. Despite the FGPS (Formal Geometry Problem Solver) can
achieve interpretable algebraic equation solving and human-like deductive
reasoning, it often experiences timeouts due to the complexity of the search
strategy. In this paper, we introduced FGeo-TP (Theorem Predictor), which
utilizes the language model to predict theorem sequences for solving geometry
problems. We compared the effectiveness of various Transformer architectures,
such as BART or T5, in theorem prediction, implementing pruning in the search
process of FGPS, thereby improving its performance in solving geometry
problems. Our results demonstrate a significant increase in the problem-solving
rate of the language model-enhanced FGeo-TP on the FormalGeo7k dataset, rising
from 39.7% to 80.86%. Furthermore, FGeo-TP exhibits notable reductions in
solving time and search steps across problems of varying difficulty levels.
\\ ( https://arxiv.org/abs/2402.09047 ,  975kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09051
Date: Wed, 14 Feb 2024 09:48:39 GMT   (505kb,D)

Title: FGeo-DRL: Deductive Reasoning for Geometric Problems through Deep
  Reinforcement Learning
Authors: Jia Zou, Xiaokai Zhang, Yiming He, Na Zhu, Tuo Leng
Categories: cs.AI
Comments: 15 pages
\\
  The human-like automatic deductive reasoning has always been one of the most
challenging open problems in the interdiscipline of mathematics and artificial
intelligence. This paper is the third in a series of our works. We built a
neural-symbolic system, called FGeoDRL, to automatically perform human-like
geometric deductive reasoning. The neural part is an AI agent based on
reinforcement learning, capable of autonomously learning problem-solving
methods from the feedback of a formalized environment, without the need for
human supervision. It leverages a pre-trained natural language model to
establish a policy network for theorem selection and employ Monte Carlo Tree
Search for heuristic exploration. The symbolic part is a reinforcement learning
environment based on geometry formalization theory and
FormalGeo\cite{FormalGeo}, which models GPS as a Markov Decision
Process\cite{MDP}. In this formal symbolic system, the known conditions and
objectives of the problem form the state space, while the set of theorems forms
the action space. Leveraging FGeoDRL, we have achieved readable and verifiable
automated solutions to geometric problems. Experiments conducted on the
formalgeo7k dataset have achieved a problem-solving success rate of 86.40\%.
The project is available at https://github.com/PersonNoName/FGeoDRL.
\\ ( https://arxiv.org/abs/2402.09051 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09052
Date: Wed, 14 Feb 2024 09:51:05 GMT   (19345kb,D)

Title: L3GO: Language Agents with Chain-of-3D-Thoughts for Generating
  Unconventional Objects
Authors: Yutaro Yamada, Khyathi Chandu, Yuchen Lin, Jack Hessel, Ilker
  Yildirim, Yejin Choi
Categories: cs.AI
\\
  Diffusion-based image generation models such as DALL-E 3 and Stable
Diffusion-XL demonstrate remarkable capabilities in generating images with
realistic and unique compositions. Yet, these models are not robust in
precisely reasoning about physical and spatial configurations of objects,
especially when instructed with unconventional, thereby out-of-distribution
descriptions, such as "a chair with five legs". In this paper, we propose a
language agent with chain-of-3D-thoughts (L3GO), an inference-time approach
that can reason about part-based 3D mesh generation of unconventional objects
that current data-driven diffusion models struggle with. More concretely, we
use large language models as agents to compose a desired object via
trial-and-error within the 3D simulation environment. To facilitate our
investigation, we develop a new benchmark, Unconventionally Feasible Objects
(UFO), as well as SimpleBlenv, a wrapper environment built on top of Blender
where language agents can build and compose atomic building blocks via API
calls. Human and automatic GPT-4V evaluations show that our approach surpasses
the standard GPT-4 and other language agents (e.g., ReAct and Reflexion) for 3D
mesh generation on ShapeNet. Moreover, when tested on our UFO benchmark, our
approach outperforms other state-of-the-art text-to-2D image and text-to-3D
models based on human evaluation.
\\ ( https://arxiv.org/abs/2402.09052 ,  19345kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09056
Date: Wed, 14 Feb 2024 10:07:05 GMT   (15938kb,D)

Title: Is Epistemic Uncertainty Faithfully Represented by Evidential Deep
  Learning Methods?
Authors: Mira J\"urgens, Nis Meinert, Viktor Bengs, Eyke H\"ullermeier, Willem
  Waegeman
Categories: cs.AI cs.LG
\\
  Trustworthy ML systems should not only return accurate predictions, but also
a reliable representation of their uncertainty. Bayesian methods are commonly
used to quantify both aleatoric and epistemic uncertainty, but alternative
approaches, such as evidential deep learning methods, have become popular in
recent years. The latter group of methods in essence extends empirical risk
minimization (ERM) for predicting second-order probability distributions over
outcomes, from which measures of epistemic (and aleatoric) uncertainty can be
extracted. This paper presents novel theoretical insights of evidential deep
learning, highlighting the difficulties in optimizing second-order loss
functions and interpreting the resulting epistemic uncertainty measures. With a
systematic setup that covers a wide range of approaches for classification,
regression and counts, it provides novel insights into issues of
identifiability and convergence in second-order loss minimization, and the
relative (rather than absolute) nature of epistemic uncertainty measures.
\\ ( https://arxiv.org/abs/2402.09056 ,  15938kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09085
Date: Wed, 14 Feb 2024 11:02:04 GMT   (207kb,D)

Title: Polynomial Semantics of Tractable Probabilistic Circuits
Authors: Oliver Broadrick, Honghua Zhang, Guy Van den Broeck
Categories: cs.AI
\\
  Probabilistic circuits compute multilinear polynomials that represent
probability distributions. They are tractable models that support efficient
marginal inference. However, various polynomial semantics have been considered
in the literature (e.g., network polynomials, likelihood polynomials,
generating functions, Fourier transforms, and characteristic polynomials). The
relationships between these polynomial encodings of distributions is largely
unknown. In this paper, we prove that for binary distributions, each of these
probabilistic circuit models is equivalent in the sense that any circuit for
one of them can be transformed into a circuit for any of the others with only a
polynomial increase in size. They are therefore all tractable for marginal
inference on the same class of distributions. Finally, we explore the natural
extension of one such polynomial semantics, called probabilistic generating
circuits, to categorical random variables, and establish that marginal
inference becomes #P-hard.
\\ ( https://arxiv.org/abs/2402.09085 ,  207kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09099
Date: Wed, 14 Feb 2024 11:20:09 GMT   (24926kb,D)

Title: Exploring Neuron Interactions and Emergence in LLMs: From the
  Multifractal Analysis Perspective
Authors: Xiongye Xiao, Chenyu Zhou, Heng Ping, Defu Cao, Yaxing Li, Yizhuo
  Zhou, Shixuan Li, Paul Bogdan
Categories: cs.AI
\\
  Prior studies on the emergence in large models have primarily focused on how
the functional capabilities of large language models (LLMs) scale with model
size. Our research, however, transcends this traditional paradigm, aiming to
deepen our understanding of the emergence within LLMs by placing a special
emphasis not just on the model size but more significantly on the complex
behavior of neuron interactions during the training process. By introducing the
concepts of "self-organization" and "multifractal analysis," we explore how
neuron interactions dynamically evolve during training, leading to "emergence,"
mirroring the phenomenon in natural systems where simple micro-level
interactions give rise to complex macro-level behaviors. To quantitatively
analyze the continuously evolving interactions among neurons in large models
during training, we propose the Neuron-based Multifractal Analysis (NeuroMFA).
Utilizing NeuroMFA, we conduct a comprehensive examination of the emergent
behavior in LLMs through the lens of both model size and training process,
paving new avenues for research into the emergence in large models.
\\ ( https://arxiv.org/abs/2402.09099 ,  24926kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09132
Date: Wed, 14 Feb 2024 12:28:38 GMT   (86kb,D)

Title: Exploring the Adversarial Capabilities of Large Language Models
Authors: Lukas Struppek, Minh Hieu Le, Dominik Hintersdorf, Kristian Kersting
Categories: cs.AI cs.LG
\\
  The proliferation of large language models (LLMs) has sparked widespread and
general interest due to their strong language generation capabilities, offering
great potential for both industry and research. While previous research delved
into the security and privacy issues of LLMs, the extent to which these models
can exhibit adversarial behavior remains largely unexplored. Addressing this
gap, we investigate whether common publicly available LLMs have inherent
capabilities to perturb text samples to fool safety measures, so-called
adversarial examples resp.~attacks. More specifically, we investigate whether
LLMs are inherently able to craft adversarial examples out of benign samples to
fool existing safe rails. Our experiments, which focus on hate speech
detection, reveal that LLMs succeed in finding adversarial perturbations,
effectively undermining hate speech detection systems. Our findings carry
significant implications for (semi-)autonomous systems relying on LLMs,
highlighting potential challenges in their interaction with existing systems
and safety measures.
\\ ( https://arxiv.org/abs/2402.09132 ,  86kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09147
Date: Wed, 14 Feb 2024 12:56:58 GMT   (11106kb,D)

Title: Into the Unknown: Self-Learning Large Language Models
Authors: Teddy Ferdinan, Jan Koco\'n, Przemys{\l}aw Kazienko
Categories: cs.AI
Comments: 14 pages, 13 figures, to be submitted to ACL 2024
\\
  We address the main problem of self-learning LLM: the question of what to
learn. We propose a self-learning LLM framework that enables an LLM to
independently learn previously unknown knowledge through self-assessment of
their own hallucinations. Using the hallucination score, we introduce a new
concept of Points in The Unknown (PiUs), along with one extrinsic and three
intrinsic methods for automatic PiUs identification. It facilitates the
creation of a self-learning loop that focuses exclusively on the knowledge gap
in Points in The Unknown, resulting in a reduced hallucination score. We also
developed evaluation metrics for gauging an LLM's self-learning capability. Our
experiments revealed that 7B-Mistral models that have been finetuned or aligned
are capable of self-learning considerably well. Our self-learning concept
allows more efficient LLM updates and opens new perspectives for knowledge
exchange. It may also increase public trust in AI.
\\ ( https://arxiv.org/abs/2402.09147 ,  11106kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09161
Date: Wed, 14 Feb 2024 13:24:21 GMT   (386kb)

Title: Role-Playing Simulation Games using ChatGPT
Authors: Rita Stampfl, Igor Ivki\'c and Barbara Geyer
Categories: cs.AI cs.HC
Comments: Link to online article:
  https://ercim-news.ercim.eu/en136/special/role-playing-simulation-games-using-chatgpt
Journal-ref: ERCIM News Special Theme: Large Language Models 2024
\\
  Since the COVID-19 pandemic, educational institutions have embarked on
digital transformation projects. The success of these projects depends on
integrating new technologies and understanding the needs of digitally literate
students. The "learning by doing" approach suggests that real success in
learning new skills is achieved when students can try out and practise these
skills. In this article, we demonstrate how Large Language Models (LLMs) can
enhance the quality of teaching by using ChatGPT in a role-playing simulation
game scenario to promote active learning. Moreover, we discuss how LLMs can
boost students' interest in learning by allowing them to practice real-life
scenarios using ChatGPT.
\\ ( https://arxiv.org/abs/2402.09161 ,  386kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09221
Date: Wed, 14 Feb 2024 15:01:07 GMT   (1472kb,D)

Title: Spectral Filters, Dark Signals, and Attention Sinks
Authors: Nicola Cancedda
Categories: cs.AI cs.CL
\\
  Projecting intermediate representations onto the vocabulary is an
increasingly popular interpretation tool for transformer-based LLMs, also known
as the logit lens. We propose a quantitative extension to this approach and
define spectral filters on intermediate representations based on partitioning
the singular vectors of the vocabulary embedding and unembedding matrices into
bands. We find that the signals exchanged in the tail end of the spectrum are
responsible for attention sinking (Xiao et al. 2023), of which we provide an
explanation. We find that the loss of pretrained models can be kept low despite
suppressing sizable parts of the embedding spectrum in a layer-dependent way,
as long as attention sinking is preserved. Finally, we discover that the
representation of tokens that draw attention from many tokens have large
projections on the tail end of the spectrum.
\\ ( https://arxiv.org/abs/2402.09221 ,  1472kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09266
Date: Wed, 14 Feb 2024 15:51:58 GMT   (24412kb,D)

Title: Machine Learning in management of precautionary closures caused by
  lipophilic biotoxins
Authors: Andres Molares-Ulloa, Enrique Fernandez-Blanco, Alejandro Pazos and
  Daniel Rivero
Categories: cs.AI
Journal-ref: Computers and Electronics in Agriculture, 197, 106956. (2022)
DOI: 10.1016/j.compag.2022.106956
\\
  Mussel farming is one of the most important aquaculture industries. The main
risk to mussel farming is harmful algal blooms (HABs), which pose a risk to
human consumption. In Galicia, the Spanish main producer of cultivated mussels,
the opening and closing of the production areas is controlled by a monitoring
program. In addition to the closures resulting from the presence of toxicity
exceeding the legal threshold, in the absence of a confirmatory sampling and
the existence of risk factors, precautionary closures may be applied. These
decisions are made by experts without the support or formalisation of the
experience on which they are based. Therefore, this work proposes a predictive
model capable of supporting the application of precautionary closures.
Achieving sensitivity, accuracy and kappa index values of 97.34%, 91.83% and
0.75 respectively, the kNN algorithm has provided the best results. This allows
the creation of a system capable of helping in complex situations where
forecast errors are more common.
\\ ( https://arxiv.org/abs/2402.09266 ,  24412kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09286
Date: Wed, 14 Feb 2024 16:19:09 GMT   (354kb)

Title: Nutrition Facts, Drug Facts, and Model Facts: Putting AI Ethics into
  Practice in Gun Violence Research
Authors: Jessica Zhu, Dr. Michel Cukier, Dr. Joseph Richardson Jr
Categories: cs.AI cs.LG
\\
  Objective: Firearm injury research necessitates using data from
often-exploited vulnerable populations of Black and Brown Americans. In order
to minimize distrust, this study provides a framework for establishing AI trust
and transparency with the general population. Methods: We propose a Model Facts
template that is easily extendable and decomposes accuracy and demographics
into standardized and minimally complex values. This framework allows general
users to assess the validity and biases of a model without diving into
technical model documentation. Examples: We apply the Model Facts template on
two previously published models, a violence risk identification model and a
suicide risk prediction model. We demonstrate the ease of accessing the
appropriate information when the data is structured appropriately. Discussion:
The Model Facts template is limited in its current form to human based data and
biases. Like nutrition facts, it also will require some educational resources
for users to grasp its full utility. Human computer interaction experiments
should be conducted to ensure that the interaction between user interface and
model interface is as desired. Conclusion: The Model Facts label is the first
framework dedicated to establishing trust with end users and general population
consumers. Implementation of Model Facts into firearm injury research will
provide public health practitioners and those impacted by firearm injury
greater faith in the tools the research provides.
\\ ( https://arxiv.org/abs/2402.09286 ,  354kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09334
Date: Wed, 14 Feb 2024 17:31:04 GMT   (1121kb,D)

Title: AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe
  Approach
Authors: Maryam Amirizaniani, Tanya Roosta, Aman Chadha, Chirag Shah
Categories: cs.AI
\\
  As Large Language Models (LLMs) gain wider adoption in various contexts, it
becomes crucial to ensure they are reasonably safe, consistent, and reliable
for an application at hand. This may require probing or auditing them. Probing
LLMs with varied iterations of a single question could reveal potential
inconsistencies in their knowledge or functionality. However, a tool for
performing such audits with simple workflow and low technical threshold is
lacking. In this demo, we introduce "AuditLLM," a novel tool designed to
evaluate the performance of various LLMs in a methodical way. AuditLLM's core
functionality lies in its ability to test a given LLM by auditing it using
multiple probes generated from a single question, thereby identifying any
inconsistencies in the model's understanding or operation. A reasonably robust,
reliable, and consistent LLM should output semantically similar responses for a
question asked differently or by different people. Based on this assumption,
AuditLLM produces easily interpretable results regarding the LLM's
consistencies from a single question that the user enters. A certain level of
inconsistency has been shown to be an indicator of potential bias,
hallucinations, and other issues. One could then use the output of AuditLLM to
further investigate issues with the aforementioned LLM. To facilitate
demonstration and practical uses, AuditLLM offers two key modes: (1) Live mode
which allows instant auditing of LLMs by analyzing responses to real-time
queries; (2) Batch mode which facilitates comprehensive LLM auditing by
processing multiple queries at once for in-depth analysis. This tool is
beneficial for both researchers and general users, as it enhances our
understanding of LLMs' capabilities in generating responses, using a
standardized auditing platform.
\\ ( https://arxiv.org/abs/2402.09334 ,  1121kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09346
Date: Wed, 14 Feb 2024 17:49:31 GMT   (2093kb,D)

Title: Developing a Framework for Auditing Large Language Models Using
  Human-in-the-Loop
Authors: Maryam Amirizaniani, Jihan Yao, Adrian Lavergne, Elizabeth Snell
  Okada, Aman Chadha, Tanya Roosta, Chirag Shah
Categories: cs.AI
\\
  As LLMs become more pervasive across various users and scenarios, identifying
potential issues when using these models becomes essential. Examples include
bias, inconsistencies, and hallucination. Although auditing the LLM for these
problems is desirable, it is far from being easy or solved. An effective method
is to probe the LLM using different versions of the same question. This could
expose inconsistencies in its knowledge or operation, indicating potential for
bias or hallucination. However, to operationalize this auditing method at
scale, we need an approach to create those probes reliably and automatically.
In this paper we propose an automatic and scalable solution, where one uses a
different LLM along with human-in-the-loop. This approach offers verifiability
and transparency, while avoiding circular reliance on the same LLMs, and
increasing scientific rigor and generalizability. Specifically, we present a
novel methodology with two phases of verification using humans: standardized
evaluation criteria to verify responses, and a structured prompt template to
generate desired probes. Experiments on a set of questions from TruthfulQA
dataset show that we can generate a reliable set of probes from one LLM that
can be used to audit inconsistencies in a different LLM. The criteria for
generating and applying auditing probes is generalizable to various LLMs
regardless of the underlying structure or training mechanism.
\\ ( https://arxiv.org/abs/2402.09346 ,  2093kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09358
Date: Wed, 14 Feb 2024 18:02:24 GMT   (9921kb,D)

Title: Integrating ChatGPT into Secure Hospital Networks: A Case Study on
  Improving Radiology Report Analysis
Authors: Kyungsu Kim, Junhyun Park, Saul Langarica, Adham Mahmoud Alkhadrawi,
  Synho Do
Categories: cs.AI cs.LG
\\
  This study demonstrates the first in-hospital adaptation of a cloud-based AI,
similar to ChatGPT, into a secure model for analyzing radiology reports,
prioritizing patient data privacy. By employing a unique sentence-level
knowledge distillation method through contrastive learning, we achieve over 95%
accuracy in detecting anomalies. The model also accurately flags uncertainties
in its predictions, enhancing its reliability and interpretability for
physicians with certainty indicators. These advancements represent significant
progress in developing secure and efficient AI tools for healthcare, suggesting
a promising future for in-hospital AI applications with minimal supervision.
\\ ( https://arxiv.org/abs/2402.09358 ,  9921kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09388
Date: Wed, 14 Feb 2024 18:37:47 GMT   (107kb,D)

Title: Entropy-regularized Point-based Value Iteration
Authors: Harrison Delecki, Marcell Vazquez-Chanlatte, Esen Yel, Kyle Wray,
  Tomer Arnon, Stefan Witwicki, Mykel J. Kochenderfer
Categories: cs.AI
\\
  Model-based planners for partially observable problems must accommodate both
model uncertainty during planning and goal uncertainty during objective
inference. However, model-based planners may be brittle under these types of
uncertainty because they rely on an exact model and tend to commit to a single
optimal behavior. Inspired by results in the model-free setting, we propose an
entropy-regularized model-based planner for partially observable problems.
Entropy regularization promotes policy robustness for planning and objective
inference by encouraging policies to be no more committed to a single action
than necessary. We evaluate the robustness and objective inference performance
of entropy-regularized policies in three problem domains. Our results show that
entropy-regularized policies outperform non-entropy-regularized baselines in
terms of higher expected returns under modeling errors and higher accuracy
during objective inference.
\\ ( https://arxiv.org/abs/2402.09388 ,  107kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09390
Date: Wed, 14 Feb 2024 18:41:19 GMT   (7097kb,D)

Title: HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context
  Learning in Factuality Evaluation
Authors: Yihao Fang, Stephen W. Thomas, Xiaodan Zhu
Categories: cs.AI cs.CL
\\
  With the widespread adoption of large language models (LLMs) in numerous
applications, the challenge of factuality and the propensity for hallucinations
raises significant concerns. To address this issue, particularly in
retrieval-augmented in-context learning, we introduce the hierarchical graph of
thoughts (HGOT), a structured, multi-layered graph approach designed to enhance
the retrieval of pertinent passages during in-context learning. The framework
utilizes the emergent planning capabilities of LLMs, employing the
divide-and-conquer strategy to break down complex queries into manageable
sub-queries. It refines self-consistency majority voting for answer selection,
which incorporates the recently proposed citation recall and precision metrics
to assess the quality of thoughts, linking an answer's credibility
intrinsically to the thought's quality. This methodology introduces a weighted
system in majority voting, prioritizing answers based on the citation quality
of their thoughts. Additionally, we propose a scoring mechanism for evaluating
retrieved passages, considering factors such as citation frequency and quality,
self-consistency confidence, and the retrieval module's ranking. Experiments
reveal that HGOT outperforms other retrieval-augmented in-context learning
methods, including Demonstrate-Search-Predict (DSP), ReAct, Self-Ask, and
Retrieve-then-Read on different datasets by as much as $7\%$, demonstrating its
efficacy in enhancing the factuality of LLMs.
\\ ( https://arxiv.org/abs/2402.09390 ,  7097kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09391
Date: Wed, 14 Feb 2024 18:42:25 GMT   (875kb,D)

Title: LlaSMol: Advancing Large Language Models for Chemistry with a
  Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset
Authors: Botao Yu, Frazier N. Baker, Ziqi Chen, Xia Ning, Huan Sun
Categories: cs.AI cs.CE cs.CL
Comments: 23 pages, 5 figures
\\
  Chemistry plays a crucial role in many domains, such as drug discovery and
material science. While large language models (LLMs) such as GPT-4 exhibit
remarkable capabilities on natural language processing tasks, existing work
shows their performance on chemistry tasks is discouragingly low. In this
paper, however, we demonstrate that our developed LLMs can achieve very strong
results on a comprehensive set of chemistry tasks, outperforming the most
advanced GPT-4 across all the tasks by a substantial margin and approaching the
SoTA task-specific models. The key to our success is a large-scale,
comprehensive, high-quality dataset for instruction tuning named SMolInstruct.
It contains 14 meticulously selected chemistry tasks and over three million
high-quality samples, laying a solid foundation for training and evaluating
LLMs for chemistry. Based on SMolInstruct, we fine-tune a set of open-source
LLMs, among which, we find that Mistral serves as the best base model for
chemistry tasks. We further conduct analysis on the impact of trainable
parameters, providing insights for future research.
\\ ( https://arxiv.org/abs/2402.09391 ,  875kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08702
Date: Tue, 13 Feb 2024 16:38:01 GMT   (20848kb,D)

Title: PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human
  Feedback and Preference Alignment
Authors: Yongchao Chen, Jacob Arkin, Yilun Hao, Yang Zhang, Nicholas Roy,
  Chuchu Fan
Categories: cs.CL cs.AI cs.HC cs.RO
Comments: 39 pages, 13 figures
\\
  Prompt optimization aims to find the best prompt to a large language model
(LLM) for a given task. LLMs have been successfully used to help find and
improve prompt candidates for single-step tasks. However, realistic tasks for
agents are multi-step and introduce new challenges: (1) Prompt content is
likely to be more extensive and complex, making it more difficult for LLMs to
analyze errors, (2) the impact of an individual step is difficult to evaluate,
and (3) different people may have varied preferences about task execution.
While humans struggle to optimize prompts, they are good at providing feedback
about LLM outputs; we therefore introduce a new LLM-driven discrete prompt
optimization framework that incorporates human-designed feedback rules about
potential errors to automatically offer direct suggestions for improvement. Our
framework is stylized as a genetic algorithm in which an LLM generates new
candidate prompts from a parent prompt and its associated feedback; we use a
learned heuristic function that predicts prompt performance to efficiently
sample from these candidates. This approach significantly outperforms both
human-engineered prompts and several other prompt optimization methods across
eight representative multi-step tasks (an average 27.7% and 28.2% improvement
to current best methods on GPT-3.5 and GPT-4, respectively). We further show
that the score function for tasks can be modified to better align with
individual preferences. We believe our work can serve as a benchmark for
automatic prompt optimization for LLM-driven multi-step tasks. Datasets and
Codes are available at https://github.com/yongchao98/PROMST. Project Page is
available at https://yongchao98.github.io/MIT-REALM-PROMST.
\\ ( https://arxiv.org/abs/2402.08702 ,  20848kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08756
Date: Tue, 13 Feb 2024 19:49:17 GMT   (39300kb,D)

Title: Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal
  Foundation Models
Authors: Maurice Diesendruck, Jianzhe Lin, Shima Imani, Gayathri Mahalingam,
  Mingyang Xu, Jie Zhao
Categories: cs.CL cs.CV
\\
  When LLMs perform zero-shot inference, they typically use a prompt with a
task specification, and generate a completion. However, there is no work to
explore the possibility of the reverse - going from completion to task
specification. In this paper, we employ both directions to perform
cycle-supervised learning entirely in-context. Our goal is to create a forward
map f : X -> Y (e.g. image -> generated caption), coupled with a backward map g
: Y -> X (e.g. caption -> generated image) to construct a cycle-consistency
"loss" (formulated as an update to the prompt) to enforce g(f(X)) ~= X. The
technique, called CyclePrompt, uses cycle-consistency as a free supervisory
signal to iteratively craft the prompt. Importantly, CyclePrompt reinforces
model performance without expensive fine-tuning, without training data, and
without the complexity of external environments (e.g. compilers, APIs). We
demonstrate CyclePrompt in two domains: code generation and image captioning.
Our results on the HumanEval coding benchmark put us in first place on the
leaderboard among models that do not rely on extra training data or usage of
external environments, and third overall. Compared to the GPT4 baseline, we
improve accuracy from 80.5% to 87.2%. In the vision-language space, we generate
detailed image captions which outperform baseline zero-shot GPT4V captions,
when tested against natural (VQAv2) and diagrammatic (FigureQA) visual
question-answering benchmarks. To the best of our knowledge, this is the first
use of self-supervised learning for prompting.
\\ ( https://arxiv.org/abs/2402.08756 ,  39300kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08761
Date: Tue, 13 Feb 2024 19:54:29 GMT   (12345kb,D)

Title: JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding
  over Small Language Models
Authors: Jillian Fisher, Ximing Lu, Jaehun Jung, Liwei Jiang, Zaid Harchaoui,
  Yejin Choi
Categories: cs.CL cs.AI
Comments: Code is available at https://github.com/jfisher52/JAMDecoding
\\
  The permanence of online content combined with the enhanced authorship
identification techniques calls for stronger computational methods to protect
the identity and privacy of online authorship when needed, e.g., blind reviews
for scientific papers, anonymous online reviews, or anonymous interactions in
the mental health forums. In this paper, we propose an unsupervised
inference-time approach to authorship obfuscation to address the unique
challenges of authorship obfuscation: lack of supervision data for diverse
authorship and domains, and the need for a sufficient level of revision beyond
simple paraphrasing to obfuscate the authorship, all the while preserving the
original content and fluency.
  We introduce JAMDEC, a user-controlled, inference-time algorithm for
authorship obfuscation that can be in principle applied to any text and
authorship. Our approach builds on small language models such as GPT2-XL in
order to help avoid disclosing the original content to proprietary LLM's APIs,
while also reducing the performance gap between small and large language models
via algorithmic enhancement. The key idea behind our approach is to boost the
creative power of smaller language models through constrained decoding, while
also allowing for user-specified controls and flexibility. Experimental results
demonstrate that our approach based on GPT2-XL outperforms previous
state-of-the-art methods based on comparably small models, while performing
competitively against GPT3.5 175B, a propriety model that is two orders of
magnitudes larger.
\\ ( https://arxiv.org/abs/2402.08761 ,  12345kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08764
Date: Tue, 13 Feb 2024 19:58:24 GMT   (7755kb,D)

Title: A Dataset for the Detection of Dehumanizing Language
Authors: Paul Engelmann, Peter Brunsgaard Trolle, Christian Hardmeier
Categories: cs.CL
\\
  Dehumanization is a mental process that enables the exclusion and ill
treatment of a group of people. In this paper, we present two data sets of
dehumanizing text, a large, automatically collected corpus and a smaller,
manually annotated data set. Both data sets include a combination of political
discourse and dialogue from movie subtitles. Our methods give us a broad and
varied amount of dehumanization data to work with, enabling further exploratory
analysis and automatic classification of dehumanization patterns. Both data
sets will be publicly released.
\\ ( https://arxiv.org/abs/2402.08764 ,  7755kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08785
Date: Tue, 13 Feb 2024 20:47:17 GMT   (8967kb,D)

Title: InstructGraph: Boosting Large Language Models via Graph-centric
  Instruction Tuning and Preference Alignment
Authors: Jianing Wang, Junda Wu, Yupeng Hou, Yao Liu, Ming Gao, Julian McAuley
Categories: cs.CL
Comments: 19 pages
\\
  Do current large language models (LLMs) better solve graph reasoning and
generation tasks with parameter updates? In this paper, we propose
InstructGraph, a framework that empowers LLMs with the abilities of graph
reasoning and generation by instruction tuning and preference alignment.
Specifically, we first propose a structured format verbalizer to unify all
graph data into a universal code-like format, which can simply represent the
graph without any external graph-specific encoders. Furthermore, a graph
instruction tuning stage is introduced to guide LLMs in solving graph reasoning
and generation tasks. Finally, we identify potential hallucination problems in
graph tasks and sample negative instances for preference alignment, the target
of which is to enhance the output's reliability of the model. Extensive
experiments across multiple graph-centric tasks exhibit that InstructGraph can
achieve the best performance and outperform GPT-4 and LLaMA2 by more than 13\%
and 38\%, respectively.
\\ ( https://arxiv.org/abs/2402.08785 ,  8967kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08788
Date: Tue, 13 Feb 2024 20:54:24 GMT   (748kb)

Title: Syllable based DNN-HMM Cantonese Speech to Text System
Authors: Timothy Wong and Claire Li and Sam Lam and Billy Chiu and Qin Lu and
  Minglei Li and Dan Xiong and Roy Shing Yu and Vincent T.Y. Ng
Categories: cs.CL cs.SD eess.AS
Comments: 7 pages, 3 figures, LREC 2016
MSC-class: 94-06
ACM-class: I.2.7
\\
  This paper reports our work on building up a Cantonese Speech-to-Text (STT)
system with a syllable based acoustic model. This is a part of an effort in
building a STT system to aid dyslexic students who have cognitive deficiency in
writing skills but have no problem expressing their ideas through speech. For
Cantonese speech recognition, the basic unit of acoustic models can either be
the conventional Initial-Final (IF) syllables, or the Onset-Nucleus-Coda (ONC)
syllables where finals are further split into nucleus and coda to reflect the
intra-syllable variations in Cantonese. By using the Kaldi toolkit, our system
is trained using the stochastic gradient descent optimization model with the
aid of GPUs for the hybrid Deep Neural Network and Hidden Markov Model
(DNN-HMM) with and without I-vector based speaker adaptive training technique.
The input features of the same Gaussian Mixture Model with speaker adaptive
training (GMM-SAT) to DNN are used in all cases. Experiments show that the
ONC-based syllable acoustic modeling with I-vector based DNN-HMM achieves the
best performance with the word error rate (WER) of 9.66% and the real time
factor (RTF) of 1.38812.
\\ ( https://arxiv.org/abs/2402.08788 ,  748kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08831
Date: Tue, 13 Feb 2024 22:26:24 GMT   (1298kb,D)

Title: eCeLLM: Generalizing Large Language Models for E-commerce from
  Large-scale, High-quality Instruction Data
Authors: Bo Peng, Xinyi Ling, Ziru Chen, Huan Sun, Xia Ning
Categories: cs.CL cs.AI cs.IR
Comments: Bo Peng and Xinyi Ling contributed equally to this paper
\\
  With tremendous efforts on developing effective e-commerce models,
conventional e-commerce models show limited success in generalist e-commerce
modeling, and suffer from unsatisfactory performance on new users and new
products - a typical out-of-domain generalization challenge. Meanwhile, large
language models (LLMs) demonstrate outstanding performance in generalist
modeling and out-of-domain generalizability in many fields. Toward fully
unleashing their power for e-commerce, in this paper, we construct ECInstruct,
the first open-sourced, large-scale, and high-quality benchmark instruction
dataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of
e-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive
experiments and evaluation demonstrate that eCeLLM models substantially
outperform baseline models, including the most advanced GPT-4, and the
state-of-the-art task-specific models in in-domain evaluation. Moreover, eCeLLM
exhibits excellent generalizability to out-of-domain settings, including unseen
products and unseen instructions, highlighting its superiority as a generalist
e-commerce model. Both the ECInstruct dataset and the eCeLLM models show great
potential in empowering versatile and effective LLMs for e-commerce. ECInstruct
and eCeLLM models are publicly accessible through
https://ninglab.github.io/eCeLLM.
\\ ( https://arxiv.org/abs/2402.08831 ,  1298kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08837
Date: Tue, 13 Feb 2024 22:47:22 GMT   (25464kb,D)

Title: Learning to Generate Context-Sensitive Backchannel Smiles for Embodied
  AI Agents with Applications in Mental Health Dialogues
Authors: Maneesh Bilalpur, Mert Inan, Dorsa Zeinali, Jeffrey F. Cohn and Malihe
  Alikhani
Categories: cs.CL
Comments: Accepted to the Machine Learning for Cognitive and Mental Health
  Workshop at AAAI 2024
\\
  Addressing the critical shortage of mental health resources for effective
screening, diagnosis, and treatment remains a significant challenge. This
scarcity underscores the need for innovative solutions, particularly in
enhancing the accessibility and efficacy of therapeutic support. Embodied
agents with advanced interactive capabilities emerge as a promising and
cost-effective supplement to traditional caregiving methods. Crucial to these
agents' effectiveness is their ability to simulate non-verbal behaviors, like
backchannels, that are pivotal in establishing rapport and understanding in
therapeutic contexts but remain under-explored. To improve the rapport-building
capabilities of embodied agents we annotated backchannel smiles in videos of
intimate face-to-face conversations over topics such as mental health, illness,
and relationships. We hypothesized that both speaker and listener behaviors
affect the duration and intensity of backchannel smiles. Using cues from speech
prosody and language along with the demographics of the speaker and listener,
we found them to contain significant predictors of the intensity of backchannel
smiles. Based on our findings, we introduce backchannel smile production in
embodied agents as a generation problem. Our attention-based generative model
suggests that listener information offers performance improvements over the
baseline speaker-centric generation approach. Conditioned generation using the
significant predictors of smile intensity provides statistically significant
improvements in empirical measures of generation quality. Our user study by
transferring generated smiles to an embodied agent suggests that agent with
backchannel smiles is perceived to be more human-like and is an attractive
alternative for non-personal conversations over agent without backchannel
smiles.
\\ ( https://arxiv.org/abs/2402.08837 ,  25464kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08846
Date: Tue, 13 Feb 2024 23:25:04 GMT   (999kb,D)

Title: An Embarrassingly Simple Approach for LLM with Strong ASR Capacity
Authors: Ziyang Ma, Guanrou Yang, Yifan Yang, Zhifu Gao, Jiaming Wang, Zhihao
  Du, Fan Yu, Qian Chen, Siqi Zheng, Shiliang Zhang, Xie Chen
Categories: cs.CL cs.AI cs.MM cs.SD eess.AS
Comments: Working in progress and will open-source soon
\\
  In this paper, we focus on solving one of the most important tasks in the
field of speech processing, i.e., automatic speech recognition (ASR), with
speech foundation encoders and large language models (LLM). Recent works have
complex designs such as compressing the output temporally for the speech
encoder, tackling modal alignment for the projector, and utilizing
parameter-efficient fine-tuning for the LLM. We found that delicate designs are
not necessary, while an embarrassingly simple composition of off-the-shelf
speech encoder, LLM, and the only trainable linear projector is competent for
the ASR task. To be more specific, we benchmark and explore various
combinations of LLMs and speech encoders, leading to the optimal LLM-based ASR
system, which we call SLAM-ASR. The proposed SLAM-ASR provides a clean setup
and little task-specific design, where only the linear projector is trained. To
the best of our knowledge, SLAM-ASR achieves the best performance on the
Librispeech benchmark among LLM-based ASR models and even outperforms the
latest LLM-based audio-universal model trained on massive pair data. Finally,
we explore the capability emergence of LLM-based ASR in the process of modal
alignment. We hope that our study can facilitate the research on extending LLM
with cross-modality capacity and shed light on the LLM-based ASR community.
\\ ( https://arxiv.org/abs/2402.08846 ,  999kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08874
Date: Wed, 14 Feb 2024 00:40:51 GMT   (711kb,D)

Title: Tree-Based Hard Attention with Self-Motivation for Large Language Models
Authors: Chenxi Lin, Jiayu Ren, Guoxiu He, Zhuoren Jiang, Haiyan Yu, Xiaomin
  Zhu
Categories: cs.CL
\\
  While large language models (LLMs) excel at understanding and generating
plain text, they are not specifically tailored to handle hierarchical text
structures. Extracting the task-desired property from their natural language
responses typically necessitates additional processing steps. In fact,
selectively comprehending the hierarchical structure of large-scale text is
pivotal to understanding its substance. Aligning LLMs more closely with the
classification or regression values of specific task through prompting also
remains challenging. To this end, we propose a novel framework called
Tree-Based Hard Attention with Self-Motivation for Large Language Models
(TEAROOM). TEAROOM incorporates a tree-based hard attention mechanism for LLMs
to process hierarchically structured text inputs. By leveraging prompting, it
enables a frozen LLM to selectively focus on relevant leaves in relation to the
root, generating a tailored symbolic representation of their relationship.
Moreover, TEAROOM comprises a self-motivation strategy for another LLM equipped
with a trainable adapter and a linear layer. The selected symbolic outcomes are
integrated into another prompt, along with the predictive value of the task. We
iteratively feed output values back into the prompt, enabling the trainable LLM
to progressively approximate the golden truth. TEAROOM outperforms existing
state-of-the-art methods in experimental evaluations across three benchmark
datasets, showing its effectiveness in estimating task-specific properties.
Through comprehensive experiments and analysis, we have validated the ability
of TEAROOM to gradually approach the underlying golden truth through multiple
inferences.
\\ ( https://arxiv.org/abs/2402.08874 ,  711kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08925
Date: Wed, 14 Feb 2024 03:56:27 GMT   (4239kb,D)

Title: MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with
  Diverse Human Preferences
Authors: Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang,
  Dinesh Manocha, Amrit Singh Bedi, and Mengdi Wang
Categories: cs.CL cs.AI cs.LG cs.RO
\\
  Reinforcement Learning from Human Feedback (RLHF) aligns language models to
human preferences by employing a singular reward model derived from preference
data. However, such an approach overlooks the rich diversity of human
preferences inherent in data collected from multiple users. In this work, we
first derive an impossibility result of alignment with single reward RLHF,
thereby highlighting its insufficiency in representing diverse human
preferences. To provide an equitable solution to the problem, we learn a
mixture of preference distributions via an expectation-maximization algorithm
and propose a MaxMin alignment objective for policy learning inspired by the
Egalitarian principle in social choice theory to better represent diverse human
preferences. We elucidate the connection of our proposed approach to
distributionally robust optimization and general utility RL, thereby
highlighting the generality and robustness of our proposed solution. We present
comprehensive experimental results on small-scale (GPT-2) and large-scale
language models (with Tulu2-7B) and show the efficacy of the proposed approach
in the presence of diversity among human preferences. Our algorithm achieves an
average improvement of more than 16% in win-rates over conventional RLHF
algorithms and improves the win-rate (accuracy) for minority groups by over 33%
without compromising the performance of majority groups, showcasing the
robustness and fairness of our approach. We remark that our findings in this
work are not only limited to language models but also extend to reinforcement
learning in general.
\\ ( https://arxiv.org/abs/2402.08925 ,  4239kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08971
Date: Wed, 14 Feb 2024 06:33:22 GMT   (292kb,D)

Title: Structured Language Generation Model for Robust Structure Prediction
Authors: Minho Lee and Junghyun Min and Woochul Lee and Yeonsoo Lee
Categories: cs.CL
Comments: 8 pages, 4 figures, 5 tables, 7 pages of appendix with 8 additional
  tables
\\
  We propose Structured Language Generation Model (SLGM), a mixture of new loss
function and inference method for better generalization of structured outputs.
Previous studies on structure prediction (e.g. NER, RE) make use of explicit
dataset information, which would boost performance, yet it might pose
challenges to robust generalization in real-world situations. Instead, our
model gives generalized format information about data indirectly. With format
information, we could reduce sequence-to-sequence problem into classification
problem via loss calibration and formatted decoding. Our experimental results
showed SLGM successfully maintain performance without dataset information, and
showed much less format errors. We also showed our model can work like adapters
on individual dataset, with no additional training.
\\ ( https://arxiv.org/abs/2402.08971 ,  292kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09008
Date: Wed, 14 Feb 2024 08:22:58 GMT   (6828kb,D)

Title: Multi-Query Focused Disaster Summarization via Instruction-Based
  Prompting
Authors: Philipp Seeberger, Korbinian Riedhammer
Categories: cs.CL
Comments: CrisisFACTS (TREC 2023)
\\
  Automatic summarization of mass-emergency events plays a critical role in
disaster management. The second edition of CrisisFACTS aims to advance disaster
summarization based on multi-stream fact-finding with a focus on web sources
such as Twitter, Reddit, Facebook, and Webnews. Here, participants are asked to
develop systems that can extract key facts from several disaster-related
events, which ultimately serve as a summary. This paper describes our method to
tackle this challenging task. We follow previous work and propose to use a
combination of retrieval, reranking, and an embarrassingly simple
instruction-following summarization. The two-stage retrieval pipeline relies on
BM25 and MonoT5, while the summarizer module is based on the open-source Large
Language Model (LLM) LLaMA-13b. For summarization, we explore a Question
Answering (QA)-motivated prompting approach and find the evidence useful for
extracting query-relevant facts. The automatic metrics and human evaluation
show strong results but also highlight the gap between open-source and
proprietary systems.
\\ ( https://arxiv.org/abs/2402.09008 ,  6828kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09015
Date: Wed, 14 Feb 2024 08:46:15 GMT   (14525kb,D)

Title: Towards better Human-Agent Alignment: Assessing Task Utility in
  LLM-Powered Applications
Authors: Negar Arabzadeh and Julia Kiseleva and Qingyun Wu and Chi Wang and
  Ahmed Awadallah and Victor Dibia and Adam Fourney and Charles Clarke
Categories: cs.CL cs.AI
\\
  The rapid development in the field of Large Language Models (LLMs) has led to
a surge in applications that facilitate collaboration among multiple agents to
assist humans in their daily tasks. However, a significant gap remains in
assessing whether LLM-powered applications genuinely enhance user experience
and task execution efficiency. This highlights the pressing need for methods to
verify utility of LLM-powered applications, particularly by ensuring alignment
between the application's functionality and end-user needs. We introduce
AgentEval provides an implementation for the math problems}, a novel framework
designed to simplify the utility verification process by automatically
proposing a set of criteria tailored to the unique purpose of any given
application. This allows for a comprehensive assessment, quantifying the
utility of an application against the suggested criteria. We present a
comprehensive analysis of the robustness of quantifier's work.
\\ ( https://arxiv.org/abs/2402.09015 ,  14525kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09025
Date: Wed, 14 Feb 2024 09:01:13 GMT   (8750kb,D)

Title: SLEB: Streamlining LLMs through Redundancy Verification and Elimination
  of Transformer Blocks
Authors: Jiwon Song, Kyungseok Oh, Taesu Kim, Hyungjun Kim, Yulhwa Kim,
  Jae-Joon Kim
Categories: cs.CL cs.LG
\\
  Large language models (LLMs) have proven to be highly effective across
various natural language processing tasks. However, their large number of
parameters poses significant challenges for practical deployment. Pruning, a
technique aimed at reducing the size and complexity of LLMs, offers a potential
solution by removing redundant components from the network. Despite the promise
of pruning, existing methods often struggle to achieve substantial end-to-end
LLM inference speedup. In this paper, we introduce SLEB, a novel approach
designed to streamline LLMs by eliminating redundant transformer blocks. We
choose the transformer block as the fundamental unit for pruning, because LLMs
exhibit block-level redundancy with high similarity between the outputs of
neighboring blocks. This choice allows us to effectively enhance the processing
speed of LLMs. Our experimental results demonstrate that SLEB successfully
accelerates LLM inference without compromising the linguistic capabilities of
these models, making it a promising technique for optimizing the efficiency of
LLMs. The code is available at: https://github.com/leapingjagg-dev/SLEB
\\ ( https://arxiv.org/abs/2402.09025 ,  8750kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09136
Date: Wed, 14 Feb 2024 12:34:58 GMT   (1080kb,D)

Title: DolphCoder: Echo-Locating Code Large Language Models with Diverse and
  Multi-Objective Instruction Tuning
Authors: Yejie Wang, Keqing He, Guanting Dong, Pei Wang, Weihao Zeng, Muxi
  Diao, Yutao Mou, Mengdi Zhang, Jingang Wang, Xunliang Cai, Weiran Xu
Categories: cs.CL cs.AI
Comments: 14 pages, 6 figures
\\
  Code Large Language Models (Code LLMs) have demonstrated outstanding
performance in code-related tasks. Several instruction tuning approaches have
been proposed to boost the code generation performance of pre-trained Code
LLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with
self-evaluating for code generation. It learns diverse instruction targets and
combines a code evaluation objective to enhance its code generation ability.
Our model achieves superior performance on the HumanEval and MBPP benchmarks,
demonstrating new insights for future code instruction tuning work. Our key
findings are: (1) Augmenting more diverse responses with distinct reasoning
paths increases the code capability of LLMs. (2) Improving one's ability to
evaluate the correctness of code solutions also enhances their ability to
create it.
\\ ( https://arxiv.org/abs/2402.09136 ,  1080kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09141
Date: Wed, 14 Feb 2024 12:41:09 GMT   (1357kb,D)

Title: Advancing NLP Models with Strategic Text Augmentation: A Comprehensive
  Study of Augmentation Methods and Curriculum Strategies
Authors: Himmet Toprak Kesgin, Mehmet Fatih Amasyali
Categories: cs.CL cs.AI
\\
  This study conducts a thorough evaluation of text augmentation techniques
across a variety of datasets and natural language processing (NLP) tasks to
address the lack of reliable, generalized evidence for these methods. It
examines the effectiveness of these techniques in augmenting training sets to
improve performance in tasks such as topic classification, sentiment analysis,
and offensive language detection. The research emphasizes not only the
augmentation methods, but also the strategic order in which real and augmented
instances are introduced during training. A major contribution is the
development and evaluation of Modified Cyclical Curriculum Learning (MCCL) for
augmented datasets, which represents a novel approach in the field. Results
show that specific augmentation methods, especially when integrated with MCCL,
significantly outperform traditional training approaches in NLP model
performance. These results underscore the need for careful selection of
augmentation techniques and sequencing strategies to optimize the balance
between speed and quality improvement in various NLP tasks. The study concludes
that the use of augmentation methods, especially in conjunction with MCCL,
leads to improved results in various classification tasks, providing a
foundation for future advances in text augmentation strategies in NLP.
\\ ( https://arxiv.org/abs/2402.09141 ,  1357kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09151
Date: Wed, 14 Feb 2024 13:08:25 GMT   (2604kb,D)

Title: Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for
  Chinese Mental Health Text Analysis
Authors: Wei Zhai, Hongzhi Qi, Qing Zhao, Jianqiang Li, Ziqi Wang, Han Wang,
  Bing Xiang Yang, Guanghui Fu
Categories: cs.CL cs.LG
\\
  In the current environment, psychological issues are prevalent and
widespread, with social media serving as a key outlet for individuals to share
their feelings. This results in the generation of vast quantities of data
daily, where negative emotions have the potential to precipitate crisis
situations. There is a recognized need for models capable of efficient
analysis. While pre-trained language models have demonstrated their
effectiveness broadly, there's a noticeable gap in pre-trained models tailored
for specialized domains like psychology. To address this, we have collected a
huge dataset from Chinese social media platforms and enriched it with publicly
available datasets to create a comprehensive database encompassing 3.36 million
text entries. To enhance the model's applicability to psychological text
analysis, we integrated psychological lexicons into the pre-training masking
mechanism. Building on an existing Chinese language model, we performed
adaptive training to develop a model specialized for the psychological domain.
We assessed our model's effectiveness across four public benchmarks, where it
not only surpassed the performance of standard pre-trained models but also
showed a inclination for making psychologically relevant predictions. Due to
concerns regarding data privacy, the dataset will not be made publicly
available. However, we have made the pre-trained models and codes publicly
accessible to the community via:
https://github.com/zwzzzQAQ/Chinese-MentalBERT.
\\ ( https://arxiv.org/abs/2402.09151 ,  2604kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09193
Date: Wed, 14 Feb 2024 14:17:21 GMT   (487kb,D)

Title: (Ir)rationality and Cognitive Biases in Large Language Models
Authors: Olivia Macmillan-Scott and Mirco Musolesi
Categories: cs.CL cs.AI cs.HC
\\
  Do large language models (LLMs) display rational reasoning? LLMs have been
shown to contain human biases due to the data they have been trained on;
whether this is reflected in rational reasoning remains less clear. In this
paper, we answer this question by evaluating seven language models using tasks
from the cognitive psychology literature. We find that, like humans, LLMs
display irrationality in these tasks. However, the way this irrationality is
displayed does not reflect that shown by humans. When incorrect answers are
given by LLMs to these tasks, they are often incorrect in ways that differ from
human-like biases. On top of this, the LLMs reveal an additional layer of
irrationality in the significant inconsistency of the responses. Aside from the
experimental results, this paper seeks to make a methodological contribution by
showing how we can assess and compare different capabilities of these types of
models, in this case with respect to rational reasoning.
\\ ( https://arxiv.org/abs/2402.09193 ,  487kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09199
Date: Wed, 14 Feb 2024 14:32:16 GMT   (494kb,D)

Title: Ten Words Only Still Help: Improving Black-Box AI-Generated Text
  Detection via Proxy-Guided Efficient Re-Sampling
Authors: Yuhui Shi, Qiang Sheng, Juan Cao, Hao Mi, Beizhe Hu, Danding Wang
Categories: cs.CL cs.AI cs.LG
Comments: 13 pages, 6 figures, 7 tables
\\
  With the rapidly increasing application of large language models (LLMs),
their abuse has caused many undesirable societal problems such as fake news,
academic dishonesty, and information pollution. This makes AI-generated text
(AIGT) detection of great importance. Among existing methods, white-box methods
are generally superior to black-box methods in terms of performance and
generalizability, but they require access to LLMs' internal states and are not
applicable to black-box settings. In this paper, we propose to estimate word
generation probabilities as pseudo white-box features via multiple re-sampling
to help improve AIGT detection under the black-box setting. Specifically, we
design POGER, a proxy-guided efficient re-sampling method, which selects a
small subset of representative words (e.g., 10 words) for performing multiple
re-sampling in black-box AIGT detection. Experiments on datasets containing
texts from humans and seven LLMs show that POGER outperforms all baselines in
macro F1 under black-box, partial white-box, and out-of-distribution settings
and maintains lower re-sampling costs than its existing counterparts.
\\ ( https://arxiv.org/abs/2402.09199 ,  494kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09205
Date: Wed, 14 Feb 2024 14:36:30 GMT   (9114kb,D)

Title: Tell Me More! Towards Implicit User Intention Understanding of Language
  Model Driven Agents
Authors: Cheng Qian, Bingxiang He, Zhong Zhuang, Jia Deng, Yujia Qin, Xin Cong,
  Yankai Lin, Zhong Zhang, Zhiyuan Liu, Maosong Sun
Categories: cs.CL cs.AI cs.HC
Comments: 26 pages, 5 tables, 6 figures
\\
  Current language model-driven agents often lack mechanisms for effective user
participation, which is crucial given the vagueness commonly found in user
instructions. Although adept at devising strategies and performing tasks, these
agents struggle with seeking clarification and grasping precise user
intentions. To bridge this gap, we introduce Intention-in-Interaction (IN3), a
novel benchmark designed to inspect users' implicit intentions through explicit
queries. Next, we propose the incorporation of model experts as the upstream in
agent designs to enhance user-agent interaction. Employing IN3, we empirically
train Mistral-Interact, a powerful model that proactively assesses task
vagueness, inquires user intentions, and refines them into actionable goals
before starting downstream agent task execution. Integrating it into the XAgent
framework, we comprehensively evaluate the enhanced agent system regarding user
instruction understanding and execution, revealing that our approach notably
excels at identifying vague user tasks, recovering and summarizing critical
missing information, setting precise and necessary agent execution goals, and
minimizing redundant tool usage, thus boosting overall efficiency. All the data
and codes are released.
\\ ( https://arxiv.org/abs/2402.09205 ,  9114kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09216
Date: Wed, 14 Feb 2024 14:53:56 GMT   (1214kb,D)

Title: Scaling the Authoring of AutoTutors with Large Language Models
Authors: Sankalan Pal Chowdhury, Vil\'em Zouhar, Mrinmaya Sachan
Categories: cs.CL cs.HC
Comments: 15 pages
\\
  Large Language Models (LLMs) have found several use cases in education,
ranging from automatic question generation to essay evaluation. In this paper,
we explore the potential of using Large Language Models (LLMs) to author
Intelligent Tutoring Systems. A common pitfall of LLMs is their straying from
desired pedagogical strategies such as leaking the answer to the student, and
in general, providing no guarantees. We posit that while LLMs with certain
guardrails can take the place of subject experts, the overall pedagogical
design still needs to be handcrafted for the best learning results. Based on
this principle, we create a sample end-to-end tutoring system named MWPTutor,
which uses LLMs to fill in the state space of a pre-defined finite state
transducer. This approach retains the structure and the pedagogy of traditional
tutoring systems that has been developed over the years by learning scientists
but brings in additional flexibility of LLM-based approaches. Through a human
evaluation study on two datasets based on math word problems, we show that our
hybrid approach achieves a better overall tutoring score than an instructed,
but otherwise free-form, GPT-4. MWPTutor is completely modular and opens up the
scope for the community to improve its performance by improving individual
modules or using different teaching strategies that it can follow
\\ ( https://arxiv.org/abs/2402.09216 ,  1214kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09259
Date: Wed, 14 Feb 2024 15:45:56 GMT   (2086kb,D)

Title: SyntaxShap: Syntax-aware Explainability Method for Text Generation
Authors: Kenza Amara, Rita Sevastjanova, Mennatallah El-Assady
Categories: cs.CL cs.AI
Comments: Submitted to ACL 2024
\\
  To harness the power of large language models in safety-critical domains we
need to ensure the explainability of their predictions. However, despite the
significant attention to model interpretability, there remains an unexplored
domain in explaining sequence-to-sequence tasks using methods tailored for
textual data. This paper introduces SyntaxShap, a local, model-agnostic
explainability method for text generation that takes into consideration the
syntax in the text data. The presented work extends Shapley values to account
for parsing-based syntactic dependencies. Taking a game theoric approach,
SyntaxShap only considers coalitions constraint by the dependency tree. We
adopt a model-based evaluation to compare SyntaxShap and its weighted form to
state-of-the-art explainability methods adapted to text generation tasks, using
diverse metrics including faithfulness, complexity, coherency, and semantic
alignment of the explanations to the model. We show that our syntax-aware
method produces explanations that help build more faithful, coherent, and
interpretable explanations for predictions by autoregressive models.
\\ ( https://arxiv.org/abs/2402.09259 ,  2086kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09267
Date: Wed, 14 Feb 2024 15:52:42 GMT   (1326kb,D)

Title: Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via
  Self-Evaluation
Authors: Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin,
  Linfeng Song, Haitao Mi, Helen Meng
Categories: cs.CL cs.AI
Comments: 19 pages
\\
  Despite showing increasingly human-like abilities, large language models
(LLMs) often struggle with factual inaccuracies, i.e. "hallucinations", even
when they hold relevant knowledge. To address these hallucinations, current
approaches typically necessitate high-quality human factuality annotations. In
this work, we explore Self-Alignment for Factuality, where we leverage the
self-evaluation capability of an LLM to provide training signals that steer the
model towards factuality. Specifically, we incorporate Self-Eval, a
self-evaluation component, to prompt an LLM to validate the factuality of its
own generated responses solely based on its internal knowledge. Additionally,
we design Self-Knowledge Tuning (SK-Tuning) to augment the LLM's
self-evaluation ability by improving the model's confidence estimation and
calibration. We then utilize these self-annotated responses to fine-tune the
model via Direct Preference Optimization algorithm. We show that the proposed
self-alignment approach substantially enhances factual accuracy over Llama
family models across three key knowledge-intensive tasks on TruthfulQA and
BioGEN.
\\ ( https://arxiv.org/abs/2402.09267 ,  1326kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09269
Date: Wed, 14 Feb 2024 15:55:30 GMT   (8075kb,D)

Title: Personalized Large Language Models
Authors: Stanis{\l}aw Wo\'zniak, Bart{\l}omiej Koptyra, Arkadiusz Janz,
  Przemys{\l}aw Kazienko, Jan Koco\'n
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) have significantly advanced Natural Language
Processing (NLP) tasks in recent years. However, their universal nature poses
limitations in scenarios requiring personalized responses, such as
recommendation systems and chatbots. This paper investigates methods to
personalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on
subjective tasks. Results demonstrate that personalized fine-tuning improves
model reasoning compared to non-personalized models. Experiments on datasets
for emotion recognition and hate speech detection show consistent performance
gains with personalized methods across different LLM architectures. These
findings underscore the importance of personalization for enhancing LLM
capabilities in subjective text perception tasks.
\\ ( https://arxiv.org/abs/2402.09269 ,  8075kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09282
Date: Wed, 14 Feb 2024 16:10:45 GMT   (3003kb,D)

Title: Leveraging Large Language Models for Enhanced NLP Task Performance
  through Knowledge Distillation and Optimized Training Strategies
Authors: Yining Huang
Categories: cs.CL
Comments: 9 pages, 1 figure
\\
  The integration of Large Language Models (LLMs) like GPT-4 into traditional
Natural Language Processing (NLP) tasks has opened new avenues for enhancing
model performance while reducing the reliance on extensive human annotations.
This paper presents a novel approach that leverages the Chain of Thought (CoT)
prompting technique to distill knowledge from GPT-4, subsequently applying it
to improve the efficiency and effectiveness of a smaller model, BERT, on Named
Entity Recognition (NER) tasks. Our method involves a two-phase training
process: initially employing GPT-4 annotated data for pre-training and then
refining the model with a combination of distilled and original human-annotated
data. The results demonstrate that our mixed-training strategy significantly
outperforms models trained solely on human annotations, achieving superior
F1-scores and showcasing a cost-effective solution for resource-limited or
closed-network settings. The study also discusses the challenges encountered,
such as LLM output variability and the tendency towards hallucinations,
proposing future work directions to enhance prompt design and annotation
selection. Our findings indicate a promising synergy between LLM insights and
traditional NLP techniques, paving the way for more accessible and robust NLP
applications.
\\ ( https://arxiv.org/abs/2402.09282 ,  3003kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09283
Date: Wed, 14 Feb 2024 16:14:03 GMT   (8001kb,D)

Title: Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey
Authors: Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao
Categories: cs.CL
\\
  Large Language Models (LLMs) are now commonplace in conversation
applications. However, their risks of misuse for generating harmful responses
have raised serious societal concerns and spurred recent research on LLM
conversation safety. Therefore, in this survey, we provide a comprehensive
overview of recent studies, covering three critical aspects of LLM conversation
safety: attacks, defenses, and evaluations. Our goal is to provide a structured
summary that enhances understanding of LLM conversation safety and encourages
further investigation into this important subject. For easy reference, we have
categorized all the studies mentioned in this survey according to our taxonomy,
available at: https://github.com/niconi19/LLM-conversation-safety.
\\ ( https://arxiv.org/abs/2402.09283 ,  8001kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09320
Date: Wed, 14 Feb 2024 17:14:34 GMT   (6481kb,D)

Title: ICDPO: Effectively Borrowing Alignment Capability of Others via
  In-context Direct Preference Optimization
Authors: Feifan Song, Yuxuan Fan, Xin Zhang, Peiyi Wang, Houfeng Wang
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) rely on Human Preference Alignment (HPA) to
ensure the generation of safe content. Due to the heavy cost associated with
fine-tuning, fine-tuning-free methods have emerged, typically modifying LLM
decoding with external auxiliary methods. However, these methods do not
essentially enhance the LLM itself. In this paper, we rethink the derivation
procedures of DPO, based on which we conversely build an instant scorer using
the states of the LLM before and after In-context Learning (ICL). Accordingly,
we propose a novel approach called In-Context Direct Preference Optimization
(ICDPO). It enables LLMs to borrow the HPA capabilities from superior LLMs with
ICL, generating well-aligned responses as estimated by the aforementioned
instant scorer, thereby enhancing the final performance. ICDPO can be further
enhanced with a two-stage retriever and an upgraded scorer, both offering
benefits. Extensive experiments show its effectiveness, particularly in
outperforming two fine-tuning-free baselines, and it exhibits competitiveness
with SFT + LoRA. We also conduct detailed analyses to offer comprehensive
insights into ICDPO.
\\ ( https://arxiv.org/abs/2402.09320 ,  6481kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09344
Date: Wed, 14 Feb 2024 17:46:46 GMT   (8007kb,D)

Title: Generating Diverse Translation with Perturbed kNN-MT
Authors: Yuto Nishida, Makoto Morishita, Hidetaka Kamigaito, Taro Watanabe
Categories: cs.CL
Comments: Accepted to EACL 2024 SRW
\\
  Generating multiple translation candidates would enable users to choose the
one that satisfies their needs. Although there has been work on diversified
generation, there exists room for improving the diversity mainly because the
previous methods do not address the overcorrection problem -- the model
underestimates a prediction that is largely different from the training data,
even if that prediction is likely. This paper proposes methods that generate
more diverse translations by introducing perturbed k-nearest neighbor machine
translation (kNN-MT). Our methods expand the search space of kNN-MT and help
incorporate diverse words into candidates by addressing the overcorrection
problem. Our experiments show that the proposed methods drastically improve
candidate diversity and control the degree of diversity by tuning the
perturbation's magnitude.
\\ ( https://arxiv.org/abs/2402.09344 ,  8007kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09353
Date: Wed, 14 Feb 2024 17:59:34 GMT   (495kb,D)

Title: DoRA: Weight-Decomposed Low-Rank Adaptation
Authors: Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang
  Frank Wang, Kwang-Ting Cheng, Min-Hung Chen
Categories: cs.CL cs.CV
\\
  Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and
its variants have gained considerable popularity because of avoiding additional
inference costs. However, there still often exists an accuracy gap between
these methods and full fine-tuning (FT). In this work, we first introduce a
novel weight decomposition analysis to investigate the inherent differences
between FT and LoRA. Aiming to resemble the learning capacity of FT from the
findings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA
decomposes the pre-trained weight into two components, magnitude and direction,
for fine-tuning, specifically employing LoRA for directional updates to
efficiently minimize the number of trainable parameters. By employing DoRA, we
enhance both the learning capacity and training stability of LoRA while
avoiding any additional inference overhead. DoRA consistently outperforms LoRA
on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as
commonsense reasoning, visual instruction tuning, and image/video-text
understanding.
\\ ( https://arxiv.org/abs/2402.09353 ,  495kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09363
Date: Wed, 14 Feb 2024 18:09:53 GMT   (177kb,D)

Title: Copyright Traps for Large Language Models
Authors: Matthieu Meeus, Igor Shilov, Manuel Faysse and Yves-Alexandre de
  Montjoye
Categories: cs.CL cs.CR
\\
  Questions of fair use of copyright-protected content to train Large Language
Models (LLMs) are being very actively debated. Document-level inference has
been proposed as a new task: inferring from black-box access to the trained
model whether a piece of content has been seen during training. SOTA methods
however rely on naturally occurring memorization of (part of) the content.
While very effective against models that memorize a lot, we hypothesize--and
later confirm--that they will not work against models that do not naturally
memorize, e.g. medium-size 1B models. We here propose to use copyright traps,
the inclusion of fictitious entries in original content, to detect the use of
copyrighted materials in LLMs with a focus on models where memorization does
not naturally occur. We carefully design an experimental setup, randomly
inserting traps into original content (books) and train a 1.3B LLM. We first
validate that the use of content in our target model would be undetectable
using existing methods. We then show, contrary to intuition, that even
medium-length trap sentences repeated a significant number of times (100) are
not detectable using existing methods. However, we show that longer sequences
repeated a large number of times can be reliably detected (AUC=0.75) and used
as copyright traps. We further improve these results by studying how the number
of times a sequence is seen improves detectability, how sequences with higher
perplexity tend to be memorized more, and how taking context into account
further improves detectability.
\\ ( https://arxiv.org/abs/2402.09363 ,  177kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09369
Date: Wed, 14 Feb 2024 18:16:54 GMT   (4497kb,D)

Title: Massively Multi-Cultural Knowledge Acquisition & LM Benchmarking
Authors: Yi Fung, Ruining Zhao, Jae Doo, Chenkai Sun, Heng Ji
Categories: cs.CL
Comments: preprint
\\
  Pretrained large language models have revolutionized many applications but
still face challenges related to cultural bias and a lack of cultural
commonsense knowledge crucial for guiding cross-culture communication and
interactions. Recognizing the shortcomings of existing methods in capturing the
diverse and rich cultures across the world, this paper introduces a novel
approach for massively multicultural knowledge acquisition. Specifically, our
method strategically navigates from densely informative Wikipedia documents on
cultural topics to an extensive network of linked pages. Leveraging this
valuable source of data collection, we construct the CultureAtlas dataset,
which covers a wide range of sub-country level geographical regions and
ethnolinguistic groups, with data cleaning and preprocessing to ensure textual
assertion sentence self-containment, as well as fine-grained cultural profile
information extraction. Our dataset not only facilitates the evaluation of
language model performance in culturally diverse contexts but also serves as a
foundational tool for the development of culturally sensitive and aware
language models. Our work marks an important step towards deeper understanding
and bridging the gaps of cultural disparities in AI, to promote a more
inclusive and balanced representation of global cultures in the digital domain.
\\ ( https://arxiv.org/abs/2402.09369 ,  4497kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09394
Date: Wed, 14 Feb 2024 18:45:14 GMT   (3081kb,D)

Title: Long-form evaluation of model editing
Authors: Domenic Rosati, Robie Gonzales, Jinkun Chen, Xuemin Yu, Melis Erkan,
  Yahya Kayani, Satya Deepika Chavatapalli, Frank Rudzicz, Hassan Sajjad
Categories: cs.CL
\\
  Evaluations of model editing currently only use the `next few token'
completions after a prompt. As a result, the impact of these methods on longer
natural language generation is largely unknown. We introduce long-form
evaluation of model editing (\textbf{\textit{LEME}}) a novel evaluation
protocol that measures the efficacy and impact of model editing in long-form
generative settings. Our protocol consists of a machine-rated survey and a
classifier which correlates well with human ratings. Importantly, we find that
our protocol has very little relationship with previous short-form metrics
(despite being designed to extend efficacy, generalization, locality, and
portability into a long-form setting), indicating that our method introduces a
novel set of dimensions for understanding model editing methods. Using this
protocol, we benchmark a number of model editing techniques and present several
findings including that, while some methods (ROME and MEMIT) perform well in
making consistent edits within a limited scope, they suffer much more from
factual drift than other methods. Finally, we present a qualitative analysis
that illustrates common failure modes in long-form generative settings
including internal consistency, lexical cohesion, and locality issues.
\\ ( https://arxiv.org/abs/2402.09394 ,  3081kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09404
Date: Wed, 14 Feb 2024 18:59:33 GMT   (268kb,D)

Title: AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential
  Reasoning Ability
Authors: Siwei Yang, Bingchen Zhao, Cihang Xie
Categories: cs.CL cs.AI cs.LG
\\
  This paper introduces AQA-Bench, a novel benchmark to assess the sequential
reasoning capabilities of large language models (LLMs) in algorithmic contexts,
such as depth-first search (DFS). The key feature of our evaluation benchmark
lies in its interactive evaluation protocol -- for example, in DFS, the
availability of each node's connected edge is contingent upon the model's
traversal to that node, thereby necessitating the LLM's ability to effectively
remember visited nodes and strategize subsequent moves. We comprehensively
build AQA-Bench with three different algorithms, namely binary search,
depth-first search, and breadth-first search, and to evaluate the sequential
reasoning ability of 12 different LLMs. Our investigations reveal several
interesting findings: (1) Closed-source models like GPT-4 and Gemini generally
show strong sequential reasoning ability, significantly outperforming
open-source LLMs. (2) Naively providing interactive examples may inadvertently
hurt few-shot performance. (3) A very limited number of predecessor steps
following the optimal policy can substantially boost small models' performance.
(4) The scaling correlation between performance and model size is not always
significant, sometimes even showcasing an inverse trend. We hope our study can
catalyze future work on advancing the understanding and enhancement of LLMs'
capabilities in sequential reasoning. The code is available at
https://github.com/UCSC-VLAA/AQA-Bench.
\\ ( https://arxiv.org/abs/2402.09404 ,  268kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08712
Date: Tue, 13 Feb 2024 18:37:53 GMT   (7880kb,D)

Title: BECoTTA: Input-dependent Online Blending of Experts for Continual
  Test-time Adaptation
Authors: Daeun Lee, Jaehong Yoon, Sung Ju Hwang
Categories: cs.LG cs.CV
Comments: 9 pages, 6 figures, preprint
\\
  Continual Test Time Adaptation (CTTA) is required to adapt efficiently to
continuous unseen domains while retaining previously learned knowledge.
However, despite the progress of CTTA, forgetting-adaptation trade-offs and
efficiency are still unexplored. Moreover, current CTTA scenarios assume only
the disjoint situation, even though real-world domains are seamlessly changed.
To tackle these challenges, this paper proposes BECoTTA, an input-dependent yet
efficient framework for CTTA. We propose Mixture-of-Domain Low-rank Experts
(MoDE) that contains two core components: i) Domain-Adaptive Routing, which
aids in selectively capturing the domain-adaptive knowledge with multiple
domain routers, and (ii) Domain-Expert Synergy Loss to maximize the dependency
between each domain and expert. We validate our method outperforms multiple
CTTA scenarios including disjoint and gradual domain shits, while only
requiring ~98% fewer trainable parameters. We also provide analyses of our
method, including the construction of experts, the effect of domain-adaptive
experts, and visualizations.
\\ ( https://arxiv.org/abs/2402.08712 ,  7880kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08714
Date: Tue, 13 Feb 2024 18:58:16 GMT   (19878kb,D)

Title: PRDP: Proximal Reward Difference Prediction for Large-Scale Reward
  Finetuning of Diffusion Models
Authors: Fei Deng, Qifei Wang, Wei Wei, Matthias Grundmann, Tingbo Hou
Categories: cs.LG cs.AI
Comments: Project webpage: https://fdeng18.github.io/prdp
\\
  Reward finetuning has emerged as a promising approach to aligning foundation
models with downstream objectives. Remarkable success has been achieved in the
language domain by using reinforcement learning (RL) to maximize rewards that
reflect human preference. However, in the vision domain, existing RL-based
reward finetuning methods are limited by their instability in large-scale
training, rendering them incapable of generalizing to complex, unseen prompts.
In this paper, we propose Proximal Reward Difference Prediction (PRDP),
enabling stable black-box reward finetuning for diffusion models for the first
time on large-scale prompt datasets with over 100K prompts. Our key innovation
is the Reward Difference Prediction (RDP) objective that has the same optimal
solution as the RL objective while enjoying better training stability.
Specifically, the RDP objective is a supervised regression objective that tasks
the diffusion model with predicting the reward difference of generated image
pairs from their denoising trajectories. We theoretically prove that the
diffusion model that obtains perfect reward difference prediction is exactly
the maximizer of the RL objective. We further develop an online algorithm with
proximal updates to stably optimize the RDP objective. In experiments, we
demonstrate that PRDP can match the reward maximization ability of
well-established RL-based methods in small-scale training. Furthermore, through
large-scale training on text prompts from the Human Preference Dataset v2 and
the Pick-a-Pic v1 dataset, PRDP achieves superior generation quality on a
diverse set of complex, unseen prompts whereas RL-based methods completely
fail.
\\ ( https://arxiv.org/abs/2402.08714 ,  19878kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08733
Date: Tue, 13 Feb 2024 19:01:45 GMT   (8621kb,D)

Title: Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs
Authors: Daniel D. Johnson, Daniel Tarlow, David Duvenaud, Chris J. Maddison
Categories: cs.LG
Comments: 9 pages, 6 figures
\\
  Identifying how much a model ${\widehat{p}}_{\theta}(Y|X)$ knows about the
stochastic real-world process $p(Y|X)$ it was trained on is important to ensure
it avoids producing incorrect or "hallucinated" answers or taking unsafe
actions. But this is difficult for generative models because probabilistic
predictions do not distinguish between per-response noise (aleatoric
uncertainty) and lack of knowledge about the process (epistemic uncertainty),
and existing epistemic uncertainty quantification techniques tend to be
overconfident when the model underfits. We propose a general strategy for
teaching a model to both approximate $p(Y|X)$ and also estimate the remaining
gaps between ${\widehat{p}}_{\theta}(Y|X)$ and $p(Y|X)$: train it to predict
pairs of independent responses drawn from the true conditional distribution,
allow it to "cheat" by observing one response while predicting the other, then
measure how much it cheats. Remarkably, we prove that being good at cheating
(i.e. cheating whenever it improves your prediction) is equivalent to being
second-order calibrated, a principled extension of ordinary calibration that
allows us to construct provably-correct frequentist confidence intervals for
$p(Y|X)$ and detect incorrect responses with high probability. We demonstrate
empirically that our approach accurately estimates how much models don't know
across ambiguous image classification, (synthetic) language modeling, and
partially-observable navigation tasks, outperforming existing techniques.
\\ ( https://arxiv.org/abs/2402.08733 ,  8621kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08758
Date: Tue, 13 Feb 2024 19:51:49 GMT   (58kb)

Title: Bayesian Strategic Classification
Authors: Lee Cohen, Saeed Sharifi-Malvajerdi, Kevin Stangl, Ali Vakilian, Juba
  Ziani
Categories: cs.LG cs.GT
\\
  In strategic classification, agents modify their features, at a cost, to
ideally obtain a positive classification from the learner's classifier. The
typical response of the learner is to carefully modify their classifier to be
robust to such strategic behavior. When reasoning about agent manipulations,
most papers that study strategic classification rely on the following strong
assumption: agents fully know the exact parameters of the deployed classifier
by the learner. This often is an unrealistic assumption when using complex or
proprietary machine learning techniques in real-world prediction tasks.
  We initiate the study of partial information release by the learner in
strategic classification. We move away from the traditional assumption that
agents have full knowledge of the classifier. Instead, we consider agents that
have a common distributional prior on which classifier the learner is using.
The learner in our model can reveal truthful, yet not necessarily complete,
information about the deployed classifier to the agents. The learner's goal is
to release just enough information about the classifier to maximize accuracy.
We show how such partial information release can, counter-intuitively, benefit
the learner's accuracy, despite increasing agents' abilities to manipulate.
  We show that while it is intractable to compute the best response of an agent
in the general case, there exist oracle-efficient algorithms that can solve the
best response of the agents when the learner's hypothesis class is the class of
linear classifiers, or when the agents' cost function satisfies a natural
notion of submodularity as we define. We then turn our attention to the
learner's optimization problem and provide both positive and negative results
on the algorithmic problem of how much information the learner should release
about the classifier to maximize their expected accuracy.
\\ ( https://arxiv.org/abs/2402.08758 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08769
Date: Tue, 13 Feb 2024 20:04:39 GMT   (1253kb,D)

Title: FLASH: Federated Learning Across Simultaneous Heterogeneities
Authors: Xiangyu Chang, Sk Miraj Ahmed, Srikanth V. Krishnamurthy, Basak Guler,
  Ananthram Swami, Samet Oymak, Amit K. Roy-Chowdhury
Categories: cs.LG cs.DC
\\
  The key premise of federated learning (FL) is to train ML models across a
diverse set of data-owners (clients), without exchanging local data. An
overarching challenge to this date is client heterogeneity, which may arise not
only from variations in data distribution, but also in data quality, as well as
compute/communication latency. An integrated view of these diverse and
concurrent sources of heterogeneity is critical; for instance, low-latency
clients may have poor data quality, and vice versa. In this work, we propose
FLASH(Federated Learning Across Simultaneous Heterogeneities), a lightweight
and flexible client selection algorithm that outperforms state-of-the-art FL
frameworks under extensive sources of heterogeneity, by trading-off the
statistical information associated with the client's data quality, data
distribution, and latency. FLASH is the first method, to our knowledge, for
handling all these heterogeneities in a unified manner. To do so, FLASH models
the learning dynamics through contextual multi-armed bandits (CMAB) and
dynamically selects the most promising clients. Through extensive experiments,
we demonstrate that FLASH achieves substantial and consistent improvements over
state-of-the-art baselines -- as much as 10% in absolute accuracy -- thanks to
its unified approach. Importantly, FLASH also outperforms federated aggregation
methods that are designed to handle highly heterogeneous settings and even
enjoys a performance boost when integrated with them.
\\ ( https://arxiv.org/abs/2402.08769 ,  1253kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08787
Date: Tue, 13 Feb 2024 20:51:58 GMT   (356kb,D)

Title: Rethinking Machine Unlearning for Large Language Models
Authors: Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie
  Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R. Varshney,
  Mohit Bansal, Sanmi Koyejo, Yang Liu
Categories: cs.LG cs.CL
\\
  We explore machine unlearning (MU) in the domain of large language models
(LLMs), referred to as LLM unlearning. This initiative aims to eliminate
undesirable data influence (e.g., sensitive or illegal information) and the
associated model capabilities, while maintaining the integrity of essential
knowledge generation and not affecting causally unrelated information. We
envision LLM unlearning becoming a pivotal element in the life-cycle management
of LLMs, potentially standing as an essential foundation for developing
generative AI that is not only safe, secure, and trustworthy, but also
resource-efficient without the need of full retraining. We navigate the
unlearning landscape in LLMs from conceptual formulation, methodologies,
metrics, and applications. In particular, we highlight the often-overlooked
aspects of existing LLM unlearning research, e.g., unlearning scope, data-model
interaction, and multifaceted efficacy assessment. We also draw connections
between LLM unlearning and related areas such as model editing, influence
functions, model explanation, adversarial training, and reinforcement learning.
Furthermore, we outline an effective assessment framework for LLM unlearning
and explore its applications in copyright and privacy safeguards and
sociotechnical harm reduction.
\\ ( https://arxiv.org/abs/2402.08787 ,  356kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08790
Date: Tue, 13 Feb 2024 20:58:36 GMT   (273kb,D)

Title: Improving Molecule Generation and Drug Discovery with a
  Knowledge-enhanced Generative Model
Authors: Aditya Malusare and Vaneet Aggarwal
Categories: cs.LG q-bio.QM
Comments: 12 pages
\\
  Recent advancements in generative models have established state-of-the-art
benchmarks in generating molecules and novel drug candidates. Despite these
successes, a significant gap persists between generative models and the
utilization of extensive biomedical knowledge, often systematized within
knowledge graphs, whose potential to inform and enhance generative processes
has not been realized. In this paper, we present a novel approach that bridges
this divide by developing a framework for knowledge-enhanced generative models
called K-DReAM. We develop a scalable methodology to extend the functionality
of knowledge graphs while preserving semantic integrity and incorporate this
contextual information into a generative framework to guide a diffusion-based
model. The integration of knowledge graph embeddings with our generative model
furnishes a robust mechanism for producing novel drug candidates possessing
specific characteristics while ensuring validity and synthesizability. K-DReAM
outperforms state-of-the-art generative models on both unconditional and
targeted generation tasks.
\\ ( https://arxiv.org/abs/2402.08790 ,  273kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08799
Date: Tue, 13 Feb 2024 21:13:29 GMT   (45kb)

Title: Projection-Free Online Convex Optimization with Time-Varying Constraints
Authors: Dan Garber, Ben Kretzu
Categories: cs.LG math.OC stat.ML
\\
  We consider the setting of online convex optimization with adversarial
time-varying constraints in which actions must be feasible w.r.t. a fixed
constraint set, and are also required on average to approximately satisfy
additional time-varying constraints. Motivated by scenarios in which the fixed
feasible set (hard constraint) is difficult to project on, we consider
projection-free algorithms that access this set only through a linear
optimization oracle (LOO). We present an algorithm that, on a sequence of
length $T$ and using overall $T$ calls to the LOO, guarantees
$\tilde{O}(T^{3/4})$ regret w.r.t. the losses and $O(T^{7/8})$ constraints
violation (ignoring all quantities except for $T$) . In particular, these
bounds hold w.r.t. any interval of the sequence. We also present a more
efficient algorithm that requires only first-order oracle access to the soft
constraints and achieves similar bounds w.r.t. the entire sequence. We extend
the latter to the setting of bandit feedback and obtain similar bounds (as a
function of $T$) in expectation.
\\ ( https://arxiv.org/abs/2402.08799 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08808
Date: Tue, 13 Feb 2024 21:26:38 GMT   (146kb,D)

Title: Depth Separation in Norm-Bounded Infinite-Width Neural Networks
Authors: Suzanna Parkinson, Greg Ongie, Rebecca Willett, Ohad Shamir, Nathan
  Srebro
Categories: cs.LG stat.ML
\\
  We study depth separation in infinite-width neural networks, where complexity
is controlled by the overall squared $\ell_2$-norm of the weights (sum of
squares of all weights in the network). Whereas previous depth separation
results focused on separation in terms of width, such results do not give
insight into whether depth determines if it is possible to learn a network that
generalizes well even when the network width is unbounded. Here, we study
separation in terms of the sample complexity required for learnability.
Specifically, we show that there are functions that are learnable with sample
complexity polynomial in the input dimension by norm-controlled depth-3 ReLU
networks, yet are not learnable with sub-exponential sample complexity by
norm-controlled depth-2 ReLU networks (with any value for the norm). We also
show that a similar statement in the reverse direction is not possible: any
function learnable with polynomial sample complexity by a norm-controlled
depth-2 ReLU network with infinite width is also learnable with polynomial
sample complexity by a norm-controlled depth-3 ReLU network.
\\ ( https://arxiv.org/abs/2402.08808 ,  146kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08824
Date: Tue, 13 Feb 2024 22:07:57 GMT   (258kb,D)

Title: Disambiguated Node Classification with Graph Neural Networks
Authors: Tianxiang Zhao, Xiang Zhang, Suhang Wang
Categories: cs.LG cs.SI
Comments: Accepted by WebConf (WWW) 2024
DOI: 10.1145/3589334.3645637
\\
  Graph Neural Networks (GNNs) have demonstrated significant success in
learning from graph-structured data across various domains. Despite their great
successful, one critical challenge is often overlooked by existing works, i.e.,
the learning of message propagation that can generalize effectively to
underrepresented graph regions. These minority regions often exhibit irregular
homophily/heterophily patterns and diverse neighborhood class distributions,
resulting in ambiguity. In this work, we investigate the ambiguity problem
within GNNs, its impact on representation learning, and the development of
richer supervision signals to fight against this problem. We conduct a
fine-grained evaluation of GNN, analyzing the existence of ambiguity in
different graph regions and its relation with node positions. To disambiguate
node embeddings, we propose a novel method, {\method}, which exploits
additional optimization guidance to enhance representation learning,
particularly for nodes in ambiguous regions. {\method} identifies ambiguous
nodes based on temporal inconsistency of predictions and introduces a
disambiguation regularization by employing contrastive learning in a
topology-aware manner. {\method} promotes discriminativity of node
representations and can alleviating semantic mixing caused by message
propagation, effectively addressing the ambiguity problem. Empirical results
validate the efficiency of {\method} and highlight its potential to improve GNN
performance in underrepresented graph regions.
\\ ( https://arxiv.org/abs/2402.08824 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08832
Date: Tue, 13 Feb 2024 22:29:40 GMT   (244kb,D)

Title: Intelligent Agricultural Management Considering N$_2$O Emission and
  Climate Variability with Uncertainties
Authors: Zhaoan Wang, Shaoping Xiao, Jun Wang, Ashwin Parab, Shivam Patel
Categories: cs.LG cs.AI cs.CY
\\
  This study examines how artificial intelligence (AI), especially
Reinforcement Learning (RL), can be used in farming to boost crop yields,
fine-tune nitrogen use and watering, and reduce nitrate runoff and greenhouse
gases, focusing on Nitrous Oxide (N$_2$O) emissions from soil. Facing climate
change and limited agricultural knowledge, we use Partially Observable Markov
Decision Processes (POMDPs) with a crop simulator to model AI agents'
interactions with farming environments. We apply deep Q-learning with Recurrent
Neural Network (RNN)-based Q networks for training agents on optimal actions.
Also, we develop Machine Learning (ML) models to predict N$_2$O emissions,
integrating these predictions into the simulator. Our research tackles
uncertainties in N$_2$O emission estimates with a probabilistic ML approach and
climate variability through a stochastic weather model, offering a range of
emission outcomes to improve forecast reliability and decision-making. By
incorporating climate change effects, we enhance agents' climate adaptability,
aiming for resilient agricultural practices. Results show these agents can
align crop productivity with environmental concerns by penalizing N$_2$O
emissions, adapting effectively to climate shifts like warmer temperatures and
less rain. This strategy improves farm management under climate change,
highlighting AI's role in sustainable agriculture.
\\ ( https://arxiv.org/abs/2402.08832 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08845
Date: Tue, 13 Feb 2024 23:25:01 GMT   (2458kb,D)

Title: Feature Attribution with Necessity and Sufficiency via Dual-stage
  Perturbation Test for Causal Explanation
Authors: Xuexin Chen, Ruichu Cai, Zhengting Huang, Yuxuan Zhu, Julien Horwood,
  Zhifeng Hao, Zijian Li, Jose Miguel Hernandez-Lobato
Categories: cs.LG stat.ME
\\
  We investigate the problem of explainability in machine learning.To address
this problem, Feature Attribution Methods (FAMs) measure the contribution of
each feature through a perturbation test, where the difference in prediction is
compared under different perturbations.However, such perturbation tests may not
accurately distinguish the contributions of different features, when their
change in prediction is the same after perturbation.In order to enhance the
ability of FAMs to distinguish different features' contributions in this
challenging setting, we propose to utilize the probability (PNS) that
perturbing a feature is a necessary and sufficient cause for the prediction to
change as a measure of feature importance.Our approach, Feature Attribution
with Necessity and Sufficiency (FANS), computes the PNS via a perturbation test
involving two stages (factual and interventional).In practice, to generate
counterfactual samples, we use a resampling-based approach on the observed
samples to approximate the required conditional distribution.Finally, we
combine FANS and gradient-based optimization to extract the subset with the
largest PNS.We demonstrate that FANS outperforms existing feature attribution
methods on six benchmarks.
\\ ( https://arxiv.org/abs/2402.08845 ,  2458kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08848
Date: Tue, 13 Feb 2024 23:29:09 GMT   (1208kb,D)

Title: Hybrid Inverse Reinforcement Learning
Authors: Juntao Ren, Gokul Swamy, Zhiwei Steven Wu, J. Andrew Bagnell, Sanjiban
  Choudhury
Categories: cs.LG cs.AI
\\
  The inverse reinforcement learning approach to imitation learning is a
double-edged sword. On the one hand, it can enable learning from a smaller
number of expert demonstrations with more robustness to error compounding than
behavioral cloning approaches. On the other hand, it requires that the learner
repeatedly solve a computationally expensive reinforcement learning (RL)
problem. Often, much of this computation is wasted searching over policies very
dissimilar to the expert's. In this work, we propose using hybrid RL --
training on a mixture of online and expert data -- to curtail unnecessary
exploration. Intuitively, the expert data focuses the learner on good states
during training, which reduces the amount of exploration required to compute a
strong policy. Notably, such an approach doesn't need the ability to reset the
learner to arbitrary states in the environment, a requirement of prior work in
efficient inverse RL. More formally, we derive a reduction from inverse RL to
expert-competitive RL (rather than globally optimal RL) that allows us to
dramatically reduce interaction during the inner policy search loop while
maintaining the benefits of the IRL approach. This allows us to derive both
model-free and model-based hybrid inverse RL algorithms with strong policy
performance guarantees. Empirically, we find that our approaches are
significantly more sample efficient than standard inverse RL and several other
baselines on a suite of continuous control tasks.
\\ ( https://arxiv.org/abs/2402.08848 ,  1208kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08856
Date: Tue, 13 Feb 2024 23:53:47 GMT   (65kb,D)

Title: Approximation of relation functions and attention mechanisms
Authors: Awni Altabaa, John Lafferty
Categories: cs.LG stat.ML
Comments: 22 pages
\\
  Inner products of neural network feature maps arises in a wide variety of
machine learning frameworks as a method of modeling relations between inputs.
This work studies the approximation properties of inner products of neural
networks. It is shown that the inner product of a multi-layer perceptron with
itself is a universal approximator for symmetric positive-definite relation
functions. In the case of asymmetric relation functions, it is shown that the
inner product of two different multi-layer perceptrons is a universal
approximator. In both cases, a bound is obtained on the number of neurons
required to achieve a given accuracy of approximation. In the symmetric case,
the function class can be identified with kernels of reproducing kernel Hilbert
spaces, whereas in the asymmetric case the function class can be identified
with kernels of reproducing kernel Banach spaces. Finally, these approximation
results are applied to analyzing the attention mechanism underlying
Transformers, showing that any retrieval mechanism defined by an abstract
preorder can be approximated by attention through its inner product relations.
This result uses the Debreu representation theorem in economics to represent
preference relations in terms of utility functions.
\\ ( https://arxiv.org/abs/2402.08856 ,  65kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08871
Date: Wed, 14 Feb 2024 00:35:10 GMT   (408kb,D)

Title: Position Paper: Challenges and Opportunities in Topological Deep
  Learning
Authors: Theodore Papamarkou, Tolga Birdal, Michael Bronstein, Gunnar Carlsson,
  Justin Curry, Yue Gao, Mustafa Hajij, Roland Kwitt, Pietro Li\`o, Paolo Di
  Lorenzo, Vasileios Maroulas, Nina Miolane, Farzana Nasrin, Karthikeyan
  Natesan Ramamurthy, Bastian Rieck, Simone Scardapane, Michael T. Schaub,
  Petar Veli\v{c}kovi\'c, Bei Wang, Yusu Wang, Guo-Wei Wei, Ghada Zamzmi
Categories: cs.LG stat.ML
\\
  Topological deep learning (TDL) is a rapidly evolving field that uses
topological features to understand and design deep learning models. This paper
posits that TDL may complement graph representation learning and geometric deep
learning by incorporating topological concepts, and can thus provide a natural
choice for various machine learning settings. To this end, this paper discusses
open problems in TDL, ranging from practical benefits to theoretical
foundations. For each problem, it outlines potential solutions and future
research opportunities. At the same time, this paper serves as an invitation to
the scientific community to actively participate in TDL research to unlock the
potential of this emerging field.
\\ ( https://arxiv.org/abs/2402.08871 ,  408kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08907
Date: Wed, 14 Feb 2024 02:46:47 GMT   (508kb,D)

Title: Tackling Negative Transfer on Graphs
Authors: Zehong Wang, Zheyuan Zhang, Chuxu Zhang, Yanfang Ye
Categories: cs.LG cs.AI cs.SI
\\
  Transfer learning aims to boost the learning on the target task leveraging
knowledge learned from other relevant tasks. However, when the source and
target are not closely related, the learning performance may be adversely
affected, a phenomenon known as negative transfer. In this paper, we
investigate the negative transfer in graph transfer learning, which is
important yet underexplored. We reveal that, unlike image or text, negative
transfer commonly occurs in graph-structured data, even when source and target
graphs share semantic similarities. Specifically, we identify that structural
differences significantly amplify the dissimilarities in the node embeddings
across graphs. To mitigate this, we bring a new insight: for semantically
similar graphs, although structural differences lead to significant
distribution shift in node embeddings, their impact on subgraph embeddings
could be marginal. Building on this insight, we introduce two effective yet
elegant methods, Subgraph Pooling (SP) and Subgraph Pooling++ (SP++), that
transfer subgraph-level knowledge across graphs. We theoretically analyze the
role of SP in reducing graph discrepancy and conduct extensive experiments to
evaluate its superiority under various settings. Our code and datasets are
available at: https://github.com/Zehong-Wang/Subgraph-Pooling.
\\ ( https://arxiv.org/abs/2402.08907 ,  508kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08918
Date: Wed, 14 Feb 2024 03:16:13 GMT   (742kb,D)

Title: Graph Inference Acceleration by Learning MLPs on Graphs without
  Supervision
Authors: Zehong Wang, Zheyuan Zhang, Chuxu Zhang, Yanfang Ye
Categories: cs.LG cs.AI cs.SI
\\
  Graph Neural Networks (GNNs) have demonstrated effectiveness in various graph
learning tasks, yet their reliance on message-passing constraints their
deployment in latency-sensitive applications such as financial fraud detection.
Recent works have explored distilling knowledge from GNNs to Multi-Layer
Perceptrons (MLPs) to accelerate inference. However, this task-specific
supervised distillation limits generalization to unseen nodes, which are
prevalent in latency-sensitive applications. To this end, we present
\textbf{\textsc{SimMLP}}, a \textbf{\textsc{Sim}}ple yet effective framework
for learning \textbf{\textsc{MLP}}s on graphs without supervision, to enhance
generalization. \textsc{SimMLP} employs self-supervised alignment between GNNs
and MLPs to capture the fine-grained and generalizable correlation between node
features and graph structures, and proposes two strategies to alleviate the
risk of trivial solutions. Theoretically, we comprehensively analyze
\textsc{SimMLP} to demonstrate its equivalence to GNNs in the optimal case and
its generalization capability. Empirically, \textsc{SimMLP} outperforms
state-of-the-art baselines, especially in settings with unseen nodes. In
particular, it obtains significant performance gains {\bf (7$\sim$26\%)} over
MLPs and inference acceleration over GNNs {\bf (90$\sim$126$\times$)} on
large-scale graph datasets. Our codes are available at:
\url{https://github.com/Zehong-Wang/SimMLP}.
\\ ( https://arxiv.org/abs/2402.08918 ,  742kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08922
Date: Wed, 14 Feb 2024 03:43:05 GMT   (19011kb,D)

Title: The Mirrored Influence Hypothesis: Efficient Data Influence Estimation
  by Harnessing Forward Passes
Authors: Myeongseob Ko, Feiyang Kang, Weiyan Shi, Ming Jin, Zhou Yu, Ruoxi Jia
Categories: cs.LG stat.ML
\\
  Large-scale black-box models have become ubiquitous across numerous
applications. Understanding the influence of individual training data sources
on predictions made by these models is crucial for improving their
trustworthiness. Current influence estimation techniques involve computing
gradients for every training point or repeated training on different subsets.
These approaches face obvious computational challenges when scaled up to large
datasets and models.
  In this paper, we introduce and explore the Mirrored Influence Hypothesis,
highlighting a reciprocal nature of influence between training and test data.
Specifically, it suggests that evaluating the influence of training data on
test predictions can be reformulated as an equivalent, yet inverse problem:
assessing how the predictions for training samples would be altered if the
model were trained on specific test samples. Through both empirical and
theoretical validations, we demonstrate the wide applicability of our
hypothesis. Inspired by this, we introduce a new method for estimating the
influence of training data, which requires calculating gradients for specific
test samples, paired with a forward pass for each training point. This approach
can capitalize on the common asymmetry in scenarios where the number of test
samples under concurrent examination is much smaller than the scale of the
training dataset, thus gaining a significant improvement in efficiency compared
to existing approaches.
  We demonstrate the applicability of our method across a range of scenarios,
including data attribution in diffusion models, data leakage detection,
analysis of memorization, mislabeled data detection, and tracing behavior in
language models. Our code will be made available at
https://github.com/ruoxi-jia-group/Forward-INF.
\\ ( https://arxiv.org/abs/2402.08922 ,  19011kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08923
Date: Wed, 14 Feb 2024 03:45:26 GMT   (636kb,D)

Title: IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human
  Pose Estimation with Transformer Architecture
Authors: Varun Ramani and Hossein Khayemi and Yang Bai and Nakul Garg and
  Nirupam Roy
Categories: cs.LG
Comments: 8 pages, 16 figures
\\
  This paper presents a novel approach for predicting human poses using IMU
data, diverging from previous studies such as DIP-IMU, IMUPoser, and TransPose,
which use up to 6 IMUs in conjunction with bidirectional RNNs. We introduce two
main innovations: a data-driven strategy for optimal IMU placement and a
transformer-based model architecture for time series analysis. Our findings
indicate that our approach not only outperforms traditional 6 IMU-based biRNN
models but also that the transformer architecture significantly enhances pose
reconstruction from data obtained from 24 IMU locations, with equivalent
performance to biRNNs when using only 6 IMUs. The enhanced accuracy provided by
our optimally chosen locations, when coupled with the parallelizability and
performance of transformers, provides significant improvements to the field of
IMU-based pose estimation.
\\ ( https://arxiv.org/abs/2402.08923 ,  636kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08929
Date: Wed, 14 Feb 2024 04:03:38 GMT   (176kb,D)

Title: Second Order Methods for Bandit Optimization and Control
Authors: Arun Suggala, Y. Jennifer Sun, Praneeth Netrapalli, Elad Hazan
Categories: cs.LG stat.ML
\\
  Bandit convex optimization (BCO) is a general framework for online decision
making under uncertainty. While tight regret bounds for general convex losses
have been established, existing algorithms achieving these bounds have
prohibitive computational costs for high dimensional data.
  In this paper, we propose a simple and practical BCO algorithm inspired by
the online Newton step algorithm. We show that our algorithm achieves optimal
(in terms of horizon) regret bounds for a large class of convex functions that
we call $\kappa$-convex. This class contains a wide range of practically
relevant loss functions including linear, quadratic, and generalized linear
models. In addition to optimal regret, this method is the most efficient known
algorithm for several well-studied applications including bandit logistic
regression.
  Furthermore, we investigate the adaptation of our second-order bandit
algorithm to online convex optimization with memory. We show that for loss
functions with a certain affine structure, the extended algorithm attains
optimal regret. This leads to an algorithm with optimal regret for bandit
LQR/LQG problems under a fully adversarial noise model, thereby resolving an
open question posed in \citep{gradu2020non} and \citep{sun2023optimal}.
  Finally, we show that the more general problem of BCO with (non-affine)
memory is harder. We derive a $\tilde{\Omega}(T^{2/3})$ regret lower bound,
even under the assumption of smooth and quadratic losses.
\\ ( https://arxiv.org/abs/2402.08929 ,  176kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08943
Date: Wed, 14 Feb 2024 05:08:47 GMT   (11249kb,D)

Title: Evaluating DTW Measures via a Synthesis Framework for Time-Series Data
Authors: Kishansingh Rajput, Duong Binh Nguyen, Guoning Chen
Categories: cs.LG
\\
  Time-series data originate from various applications that describe specific
observations or quantities of interest over time. Their analysis often involves
the comparison across different time-series data sequences, which in turn
requires the alignment of these sequences. Dynamic Time Warping (DTW) is the
standard approach to achieve an optimal alignment between two temporal signals.
Different variations of DTW have been proposed to address various needs for
signal alignment or classifications. However, a comprehensive evaluation of
their performance in these time-series data processing tasks is lacking. Most
DTW measures perform well on certain types of time-series data without a clear
explanation of the reason. To address that, we propose a synthesis framework to
model the variation between two time-series data sequences for comparison. Our
synthesis framework can produce a realistic initial signal and deform it with
controllable variations that mimic real-world scenarios. With this synthesis
framework, we produce a large number of time-series sequence pairs with
different but known variations, which are used to assess the performance of a
number of well-known DTW measures for the tasks of alignment and
classification. We report their performance on different variations and suggest
the proper DTW measure to use based on the type of variations between two
time-series sequences. This is the first time such a guideline is presented for
selecting a proper DTW measure. To validate our conclusion, we apply our
findings to real-world applications, i.e., the detection of the formation top
for the oil and gas industry and the pattern search in streamlines for flow
visualization.
\\ ( https://arxiv.org/abs/2402.08943 ,  11249kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08946
Date: Wed, 14 Feb 2024 05:22:53 GMT   (189kb,D)

Title: Measuring Sharpness in Grokking
Authors: Jack Miller, Patrick Gleeson, Charles O'Neill, Thang Bui, Noam Levi
Categories: cs.LG
\\
  Neural networks sometimes exhibit grokking, a phenomenon where perfect or
near-perfect performance is achieved on a validation set well after the same
performance has been obtained on the corresponding training set. In this
workshop paper, we introduce a robust technique for measuring grokking, based
on fitting an appropriate functional form. We then use this to investigate the
sharpness of transitions in training and validation accuracy under two
settings. The first setting is the theoretical framework developed by Levi et
al. (2023) where closed form expressions are readily accessible. The second
setting is a two-layer MLP trained to predict the parity of bits, with grokking
induced by the concealment strategy of Miller et al. (2023). We find that
trends between relative grokking gap and grokking sharpness are similar in both
settings when using absolute and relative measures of sharpness. Reflecting on
this, we make progress toward explaining some trends and identify the need for
further study to untangle the various mechanisms which influence the sharpness
of grokking.
\\ ( https://arxiv.org/abs/2402.08946 ,  189kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08948
Date: Wed, 14 Feb 2024 05:34:24 GMT   (28kb)

Title: Mean-Field Analysis for Learning Subspace-Sparse Polynomials with
  Gaussian Input
Authors: Ziang Chen, Rong Ge
Categories: cs.LG
\\
  In this work, we study the mean-field flow for learning subspace-sparse
polynomials using stochastic gradient descent and two-layer neural networks,
where the input distribution is standard Gaussian and the output only depends
on the projection of the input onto a low-dimensional subspace. We propose a
basis-free generalization of the merged-staircase property in Abbe et al.
(2022) and establish a necessary condition for the SGD-learnability. In
addition, we prove that the condition is almost sufficient, in the sense that a
condition slightly stronger than the necessary condition can guarantee the
exponential decay of the loss functional to zero.
\\ ( https://arxiv.org/abs/2402.08948 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08958
Date: Wed, 14 Feb 2024 05:58:43 GMT   (342kb,D)

Title: Towards Next-Level Post-Training Quantization of Hyper-Scale
  Transformers
Authors: Junhan Kim, Kyungphil Park, Chungman Lee, Ho-young Kim, Joonyoung Kim,
  Yongkweon Jeon
Categories: cs.LG cs.AI
Comments: 17 pages, under review
\\
  With the increasing complexity of generative AI models, post-training
quantization (PTQ) has emerged as a promising solution for deploying
hyper-scale models on edge devices such as mobile devices and TVs. Existing PTQ
schemes, however, consume considerable time and resources, which could be a
bottleneck in real situations where frequent model updates and multiple
hyper-parameter tunings are required. As a cost-effective alternative, one-shot
PTQ schemes have been proposed. Still, the performance is somewhat limited
because they cannot consider the inter-layer dependency within the attention
module, which is a very important feature of Transformers. In this paper, we
thus propose a novel PTQ algorithm that balances accuracy and efficiency. The
key idea of the proposed algorithm called aespa is to perform quantization
layer-wise for efficiency while considering cross-layer dependency to preserve
the attention score. Through extensive experiments on various language models
and complexity analysis, we demonstrate that aespa is accurate and efficient in
quantizing Transformer models.
\\ ( https://arxiv.org/abs/2402.08958 ,  342kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08963
Date: Wed, 14 Feb 2024 06:09:36 GMT   (9388kb,D)

Title: DUEL: Duplicate Elimination on Active Memory for Self-Supervised
  Class-Imbalanced Learning
Authors: Won-Seok Choi, Hyundo Lee, Dong-Sig Han, Junseok Park, Heeyeon Koo and
  Byoung-Tak Zhang
Categories: cs.LG cs.AI
Comments: Accepted as a full paper at AAAI 2024: The 38th Annual AAAI
  Conference on Artificial Intelligence (Main Tech Track). 7 pages (main
  paper), 2 pages (references), 11 pages (appendix) each
\\
  Recent machine learning algorithms have been developed using well-curated
datasets, which often require substantial cost and resources. On the other
hand, the direct use of raw data often leads to overfitting towards frequently
occurring class information. To address class imbalances cost-efficiently, we
propose an active data filtering process during self-supervised pre-training in
our novel framework, Duplicate Elimination (DUEL). This framework integrates an
active memory inspired by human working memory and introduces distinctiveness
information, which measures the diversity of the data in the memory, to
optimize both the feature extractor and the memory. The DUEL policy, which
replaces the most duplicated data with new samples, aims to enhance the
distinctiveness information in the memory and thereby mitigate class
imbalances. We validate the effectiveness of the DUEL framework in
class-imbalanced environments, demonstrating its robustness and providing
reliable results in downstream tasks. We also analyze the role of the DUEL
policy in the training process through various metrics and visualizations.
\\ ( https://arxiv.org/abs/2402.08963 ,  9388kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08964
Date: Wed, 14 Feb 2024 06:10:44 GMT   (636kb,D)

Title: Predicting User Experience on Laptops from Hardware Specifications
Authors: Saswat Padhi, Sunil K. Bhasin, Udaya K. Ammu, Alex Bergman, Allan
  Knies
Categories: cs.LG cs.HC
Comments: Spotlight presentation at the ML for Systems workshop at NeurIPS 2023
  ; 9 pages with appendix ; https://openreview.net/forum?id=mHShSE7MSU
\\
  Estimating the overall user experience (UX) on a device is a common challenge
faced by manufacturers. Today, device makers primarily rely on microbenchmark
scores, such as Geekbench, that stress test specific hardware components, such
as CPU or RAM, but do not satisfactorily capture consumer workloads. System
designers often rely on domain-specific heuristics and extensive testing of
prototypes to reach a desired UX goal, and yet there is often a mismatch
between the manufacturers' performance claims and the consumers' experience.
  We present our initial results on predicting real-life experience on laptops
from their hardware specifications. We target web applications that run on
Chromebooks (ChromeOS laptops) for a simple and fair aggregation of experience
across applications and workloads. On 54 laptops, we track 9 UX metrics on
common end-user workloads: web browsing, video playback and audio/video calls.
We focus on a subset of high-level metrics exposed by the Chrome browser, that
are part of the Web Vitals initiative for judging the UX on web applications.
  With a dataset of 100K UX data points, we train gradient boosted regression
trees that predict the metric values from device specifications. Across our 9
metrics, we note a mean $R^2$ score (goodness-of-fit on our dataset) of 97.8%
and a mean MAAPE (percentage error in prediction on unseen data) of 10.1%.
\\ ( https://arxiv.org/abs/2402.08964 ,  636kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08975
Date: Wed, 14 Feb 2024 06:39:54 GMT   (1390kb,D)

Title: Research and application of Transformer based anomaly detection model: A
  literature review
Authors: Mingrui Ma, Lansheng Han, Chunjie Zhou
Categories: cs.LG cs.AI
Comments: 77 pages, 11 figures
\\
  Transformer, as one of the most advanced neural network models in Natural
Language Processing (NLP), exhibits diverse applications in the field of
anomaly detection. To inspire research on Transformer-based anomaly detection,
this review offers a fresh perspective on the concept of anomaly detection. We
explore the current challenges of anomaly detection and provide detailed
insights into the operating principles of Transformer and its variants in
anomaly detection tasks. Additionally, we delineate various application
scenarios for Transformer-based anomaly detection models and discuss the
datasets and evaluation metrics employed. Furthermore, this review highlights
the key challenges in Transformer-based anomaly detection research and conducts
a comprehensive analysis of future research trends in this domain. The review
includes an extensive compilation of over 100 core references related to
Transformer-based anomaly detection. To the best of our knowledge, this is the
first comprehensive review that focuses on the research related to Transformer
in the context of anomaly detection. We hope that this paper can provide
detailed technical information to researchers interested in Transformer-based
anomaly detection tasks.
\\ ( https://arxiv.org/abs/2402.08975 ,  1390kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08982
Date: Wed, 14 Feb 2024 06:51:49 GMT   (3166kb,D)

Title: MEL: Efficient Multi-Task Evolutionary Learning for High-Dimensional
  Feature Selection
Authors: Xubin Wang, Haojiong Shangguan, Fengyi Huang, Shangrui Wu and Weijia
  Jia
Categories: cs.LG cs.AI cs.NE
\\
  Feature selection is a crucial step in data mining to enhance model
performance by reducing data dimensionality. However, the increasing
dimensionality of collected data exacerbates the challenge known as the "curse
of dimensionality", where computation grows exponentially with the number of
dimensions. To tackle this issue, evolutionary computational (EC) approaches
have gained popularity due to their simplicity and applicability.
Unfortunately, the diverse designs of EC methods result in varying abilities to
handle different data, often underutilizing and not sharing information
effectively. In this paper, we propose a novel approach called PSO-based
Multi-task Evolutionary Learning (MEL) that leverages multi-task learning to
address these challenges. By incorporating information sharing between
different feature selection tasks, MEL achieves enhanced learning ability and
efficiency. We evaluate the effectiveness of MEL through extensive experiments
on 22 high-dimensional datasets. Comparing against 24 EC approaches, our method
exhibits strong competitiveness. Additionally, we have open-sourced our code on
GitHub at https://github.com/wangxb96/MEL.
\\ ( https://arxiv.org/abs/2402.08982 ,  3166kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08998
Date: Wed, 14 Feb 2024 07:52:00 GMT   (149kb,D)

Title: Nearly Minimax Optimal Regret for Learning Linear Mixture Stochastic
  Shortest Path
Authors: Qiwei Di, Jiafan He, Dongruo Zhou, Quanquan Gu
Categories: cs.LG stat.ML
Comments: 28 pages, 1 figure, In ICML 2023
\\
  We study the Stochastic Shortest Path (SSP) problem with a linear mixture
transition kernel, where an agent repeatedly interacts with a stochastic
environment and seeks to reach certain goal state while minimizing the
cumulative cost. Existing works often assume a strictly positive lower bound of
the cost function or an upper bound of the expected length for the optimal
policy. In this paper, we propose a new algorithm to eliminate these
restrictive assumptions. Our algorithm is based on extended value iteration
with a fine-grained variance-aware confidence set, where the variance is
estimated recursively from high-order moments. Our algorithm achieves an
$\tilde{\mathcal O}(dB_*\sqrt{K})$ regret bound, where $d$ is the dimension of
the feature mapping in the linear transition kernel, $B_*$ is the upper bound
of the total cumulative cost for the optimal policy, and $K$ is the number of
episodes. Our regret upper bound matches the $\Omega(dB_*\sqrt{K})$ lower bound
of linear mixture SSPs in Min et al. (2022), which suggests that our algorithm
is nearly minimax optimal.
\\ ( https://arxiv.org/abs/2402.08998 ,  149kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08999
Date: Wed, 14 Feb 2024 07:52:28 GMT   (649kb)

Title: Exploring Federated Deep Learning for Standardising Naming Conventions
  in Radiotherapy Data
Authors: Ali Haidar, Daniel Al Mouiee, Farhannah Aly, David Thwaites, Lois
  Holloway
Categories: cs.LG physics.med-ph
\\
  Standardising structure volume names in radiotherapy (RT) data is necessary
to enable data mining and analyses, especially across multi-institutional
centres. This process is time and resource intensive, which highlights the need
for new automated and efficient approaches to handle the task. Several machine
learning-based methods have been proposed and evaluated to standardise
nomenclature. However, no studies have considered that RT patient records are
distributed across multiple data centres. This paper introduces a method that
emulates real-world environments to establish standardised nomenclature. This
is achieved by integrating decentralised real-time data and federated learning
(FL). A multimodal deep artificial neural network was proposed to standardise
RT data in federated settings. Three types of possible attributes were
extracted from the structures to train the deep learning models: tabular,
visual, and volumetric. Simulated experiments were carried out to train the
models across several scenarios including multiple data centres, input
modalities, and aggregation strategies. The models were compared against models
developed with single modalities in federated settings, in addition to models
trained in centralised settings. Categorical classification accuracy was
calculated on hold-out samples to inform the models performance. Our results
highlight the need for fusing multiple modalities when training such models,
with better performance reported with tabular-volumetric models. In addition,
we report comparable accuracy compared to models built in centralised settings.
This demonstrates the suitability of FL for handling the standardization task.
Additional ablation analyses showed that the total number of samples in the
data centres and the number of data centres highly affects the training process
and should be carefully considered when building standardisation models.
\\ ( https://arxiv.org/abs/2402.08999 ,  649kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09034
Date: Wed, 14 Feb 2024 09:20:13 GMT   (709kb)

Title: Enhancing Sequential Model Performance with Squared Sigmoid TanH (SST)
  Activation Under Data Constraints
Authors: Barathi Subramanian, Rathinaraja Jeyaraj, Rakhmonov Akhrorjon
  Akhmadjon Ugli, and Jeonghong Kim
Categories: cs.LG cs.AI
Comments: 10 pages,9 figures, Submitted to IJCAI 2024 conference
\\
  Activation functions enable neural networks to learn complex representations
by introducing non-linearities. While feedforward models commonly use rectified
linear units, sequential models like recurrent neural networks, long short-term
memory (LSTMs) and gated recurrent units (GRUs) still rely on Sigmoid and TanH
activation functions. However, these classical activation functions often
struggle to model sparse patterns when trained on small sequential datasets to
effectively capture temporal dependencies. To address this limitation, we
propose squared Sigmoid TanH (SST) activation specifically tailored to enhance
the learning capability of sequential models under data constraints. SST
applies mathematical squaring to amplify differences between strong and weak
activations as signals propagate over time, facilitating improved gradient flow
and information filtering. We evaluate SST-powered LSTMs and GRUs for diverse
applications, such as sign language recognition, regression, and time-series
classification tasks, where the dataset is limited. Our experiments demonstrate
that SST models consistently outperform RNN-based models with baseline
activations, exhibiting improved test accuracy.
\\ ( https://arxiv.org/abs/2402.09034 ,  709kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09043
Date: Wed, 14 Feb 2024 09:38:09 GMT   (861kb)

Title: Under manipulations, are some AI models harder to audit?
Authors: Augustin Godinot, Gilles Tredan, Erwan Le Merrer, Camilla Penzo,
  Francois Ta\"iani
Categories: cs.LG
Comments: To appear in the IEEE Conference on Secure and Trustworthy Machine
  Learning, 2024
\\
  Auditors need robust methods to assess the compliance of web platforms with
the law. However, since they hardly ever have access to the algorithm,
implementation, or training data used by a platform, the problem is harder than
a simple metric estimation. Within the recent framework of manipulation-proof
auditing, we study in this paper the feasibility of robust audits in realistic
settings, in which models exhibit large capacities. We first prove a
constraining result: if a web platform uses models that may fit any data, no
audit strategy -- whether active or not -- can outperform random sampling when
estimating properties such as demographic parity. To better understand the
conditions under which state-of-the-art auditing techniques may remain
competitive, we then relate the manipulability of audits to the capacity of the
targeted models, using the Rademacher complexity. We empirically validate these
results on popular models of increasing capacities, thus confirming
experimentally that large-capacity models, which are commonly used in practice,
are particularly hard to audit robustly. These results refine the limits of the
auditing problem, and open up enticing questions on the connection between
model capacity and the ability of platforms to manipulate audit attempts.
\\ ( https://arxiv.org/abs/2402.09043 ,  861kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09050
Date: Wed, 14 Feb 2024 09:46:53 GMT   (7439kb,D)

Title: End-to-End Training Induces Information Bottleneck through Layer-Role
  Differentiation: A Comparative Analysis with Layer-wise Training
Authors: Keitaro Sakamoto, Issei Sato
Categories: cs.LG
\\
  End-to-end (E2E) training, optimizing the entire model through error
backpropagation, fundamentally supports the advancements of deep learning.
Despite its high performance, E2E training faces the problems of memory
consumption, parallel computing, and discrepancy with the functionalities of
the actual brain. Various alternative methods have been proposed to overcome
these difficulties; however, no one can yet match the performance of E2E
training, thereby falling short in practicality. Furthermore, there is no deep
understanding regarding differences in the trained model properties beyond the
performance gap. In this paper, we reconsider why E2E training demonstrates a
superior performance through a comparison with layer-wise training, a non-E2E
method that locally sets errors. On the basis of the observation that E2E
training has an advantage in propagating input information, we analyze the
information plane dynamics of intermediate representations based on the
Hilbert-Schmidt independence criterion (HSIC). The results of our normalized
HSIC value analysis reveal the E2E training ability to exhibit different
information dynamics across layers, in addition to efficient information
propagation. Furthermore, we show that this layer-role differentiation leads to
the final representation following the information bottleneck principle. It
suggests the need to consider the cooperative interactions between layers, not
just the final layer when analyzing the information bottleneck of deep
learning.
\\ ( https://arxiv.org/abs/2402.09050 ,  7439kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09059
Date: Wed, 14 Feb 2024 10:15:43 GMT   (5037kb,D)

Title: I can't see it but I can Fine-tune it: On Encrypted Fine-tuning of
  Transformers using Fully Homomorphic Encryption
Authors: Prajwal Panzade, Daniel Takabi, Zhipeng Cai
Categories: cs.LG cs.AI cs.CR
Comments: Accepted for the presentation at PPAI @The 38th Annual AAAI
  Conference on Artificial Intelligence 2024
\\
  In today's machine learning landscape, fine-tuning pretrained transformer
models has emerged as an essential technique, particularly in scenarios where
access to task-aligned training data is limited. However, challenges surface
when data sharing encounters obstacles due to stringent privacy regulations or
user apprehension regarding personal information disclosure. Earlier works
based on secure multiparty computation (SMC) and fully homomorphic encryption
(FHE) for privacy-preserving machine learning (PPML) focused more on
privacy-preserving inference than privacy-preserving training. In response, we
introduce BlindTuner, a privacy-preserving fine-tuning system that enables
transformer training exclusively on homomorphically encrypted data for image
classification. Our extensive experimentation validates BlindTuner's
effectiveness by demonstrating comparable accuracy to non-encrypted models.
Notably, our findings highlight a substantial speed enhancement of 1.5x to 600x
over previous work in this domain.
\\ ( https://arxiv.org/abs/2402.09059 ,  5037kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09063
Date: Wed, 14 Feb 2024 10:20:03 GMT   (633kb,D)

Title: Soft Prompt Threats: Attacking Safety Alignment and Unlearning in
  Open-Source LLMs through the Embedding Space
Authors: Leo Schwinn and David Dobre and Sophie Xhonneux and Gauthier Gidel and
  Stephan Gunnemann
Categories: cs.LG
Comments: Trigger Warning: the appendix contains LLM-generated text with
  violence and harassment
\\
  Current research in adversarial robustness of LLMs focuses on discrete input
manipulations in the natural language space, which can be directly transferred
to closed-source models. However, this approach neglects the steady progression
of open-source models. As open-source models advance in capability, ensuring
their safety also becomes increasingly imperative. Yet, attacks tailored to
open-source LLMs that exploit full model access remain largely unexplored. We
address this research gap and propose the embedding space attack, which
directly attacks the continuous embedding representation of input tokens. We
find that embedding space attacks circumvent model alignments and trigger
harmful behaviors more efficiently than discrete attacks or model fine-tuning.
Furthermore, we present a novel threat model in the context of unlearning and
show that embedding space attacks can extract supposedly deleted information
from unlearned LLMs across multiple datasets and models. Our findings highlight
embedding space attacks as an important threat model in open-source LLMs.
Trigger Warning: the appendix contains LLM-generated text with violence and
harassment.
\\ ( https://arxiv.org/abs/2402.09063 ,  633kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09078
Date: Wed, 14 Feb 2024 10:44:03 GMT   (2411kb,D)

Title: Exploiting Estimation Bias in Deep Double Q-Learning for Actor-Critic
  Methods
Authors: Alberto Sinigaglia, Niccol\`o Turcato, Alberto Dalla Libera, Ruggero
  Carli, Gian Antonio Susto
Categories: cs.LG cs.AI
\\
  This paper introduces innovative methods in Reinforcement Learning (RL),
focusing on addressing and exploiting estimation biases in Actor-Critic methods
for continuous control tasks, using Deep Double Q-Learning. We propose two
novel algorithms: Expectile Delayed Deep Deterministic Policy Gradient (ExpD3)
and Bias Exploiting - Twin Delayed Deep Deterministic Policy Gradient (BE-TD3).
ExpD3 aims to reduce overestimation bias with a single $Q$ estimate, offering a
balance between computational efficiency and performance, while BE-TD3 is
designed to dynamically select the most advantageous estimation bias during
training. Our extensive experiments across various continuous control tasks
demonstrate the effectiveness of our approaches. We show that these algorithms
can either match or surpass existing methods like TD3, particularly in
environments where estimation biases significantly impact learning. The results
underline the importance of bias exploitation in improving policy learning in
RL.
\\ ( https://arxiv.org/abs/2402.09078 ,  2411kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09084
Date: Wed, 14 Feb 2024 10:57:29 GMT   (886kb,D)

Title: Sobolev Training for Operator Learning
Authors: Namkyeong Cho, Junseung Ryu, Hyung Ju Hwang
Categories: cs.LG cs.AI
\\
  This study investigates the impact of Sobolev Training on operator learning
frameworks for improving model performance. Our research reveals that
integrating derivative information into the loss function enhances the training
process, and we propose a novel framework to approximate derivatives on
irregular meshes in operator learning. Our findings are supported by both
experimental evidence and theoretical analysis. This demonstrates the
effectiveness of Sobolev Training in approximating the solution operators
between infinite-dimensional spaces.
\\ ( https://arxiv.org/abs/2402.09084 ,  886kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09092
Date: Wed, 14 Feb 2024 11:13:33 GMT   (596kb,D)

Title: Three Decades of Activations: A Comprehensive Survey of 400 Activation
  Functions for Neural Networks
Authors: Vladim\'ir Kunc, Ji\v{r}\'i Kl\'ema
Categories: cs.LG cs.NE
ACM-class: I.5.1
\\
  Neural networks have proven to be a highly effective tool for solving complex
problems in many areas of life. Recently, their importance and practical
usability have further been reinforced with the advent of deep learning. One of
the important conditions for the success of neural networks is the choice of an
appropriate activation function introducing non-linearity into the model. Many
types of these functions have been proposed in the literature in the past, but
there is no single comprehensive source containing their exhaustive overview.
The absence of this overview, even in our experience, leads to redundancy and
the unintentional rediscovery of already existing activation functions. To
bridge this gap, our paper presents an extensive survey involving 400
activation functions, which is several times larger in scale than previous
surveys. Our comprehensive compilation also references these surveys; however,
its main goal is to provide the most comprehensive overview and systematization
of previously published activation functions with links to their original
sources. The secondary aim is to update the current understanding of this
family of functions.
\\ ( https://arxiv.org/abs/2402.09092 ,  596kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09095
Date: Wed, 14 Feb 2024 11:16:50 GMT   (1221kb,D)

Title: FedSiKD: Clients Similarity and Knowledge Distillation: Addressing
  Non-i.i.d. and Constraints in Federated Learning
Authors: Yousef Alsenani, Rahul Mishra, Khaled R. Ahmed, Atta Ur Rahman
Categories: cs.LG cs.CR
Comments: 11 pages, 10 figures Under Review - IEEE Transactions on Information
  Forensics & Security
\\
  In recent years, federated learning (FL) has emerged as a promising technique
for training machine learning models in a decentralized manner while also
preserving data privacy. The non-independent and identically distributed
(non-i.i.d.) nature of client data, coupled with constraints on client or edge
devices, presents significant challenges in FL. Furthermore, learning across a
high number of communication rounds can be risky and potentially unsafe for
model exploitation. Traditional FL approaches may suffer from these challenges.
Therefore, we introduce FedSiKD, which incorporates knowledge distillation (KD)
within a similarity-based federated learning framework. As clients join the
system, they securely share relevant statistics about their data distribution,
promoting intra-cluster homogeneity. This enhances optimization efficiency and
accelerates the learning process, effectively transferring knowledge between
teacher and student models and addressing device constraints. FedSiKD
outperforms state-of-the-art algorithms by achieving higher accuracy, exceeding
by 25\% and 18\% for highly skewed data at $\alpha = {0.1,0.5}$ on the HAR and
MNIST datasets, respectively. Its faster convergence is illustrated by a 17\%
and 20\% increase in accuracy within the first five rounds on the HAR and MNIST
datasets, respectively, highlighting its early-stage learning proficiency. Code
is publicly available and hosted on GitHub (https://github.com/SimuEnv/FedSiKD)
\\ ( https://arxiv.org/abs/2402.09095 ,  1221kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09113
Date: Wed, 14 Feb 2024 11:55:50 GMT   (1018kb,D)

Title: Measuring Exploration in Reinforcement Learning via Optimal Transport in
  Policy Space
Authors: Reabetswe M. Nkhumise, Debabrota Basu, Tony J. Prescott, Aditya Gilra
Categories: cs.LG
\\
  Exploration is the key ingredient of reinforcement learning (RL) that
determines the speed and success of learning. Here, we quantify and compare the
amount of exploration and learning accomplished by a Reinforcement Learning
(RL) algorithm. Specifically, we propose a novel measure, named Exploration
Index, that quantifies the relative effort of knowledge transfer
(transferability) by an RL algorithm in comparison to supervised learning (SL)
that transforms the initial data distribution of RL to the corresponding final
data distribution. The comparison is established by formulating learning in RL
as a sequence of SL tasks, and using optimal transport based metrics to compare
the total path traversed by the RL and SL algorithms in the data distribution
space. We perform extensive empirical analysis on various environments and with
multiple algorithms to demonstrate that the exploration index yields insights
about the exploration behaviour of any RL algorithm, and also allows us to
compare the exploratory behaviours of different RL algorithms.
\\ ( https://arxiv.org/abs/2402.09113 ,  1018kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09142
Date: Wed, 14 Feb 2024 12:48:17 GMT   (4610kb,D)

Title: When Representations Align: Universality in Representation Learning
  Dynamics
Authors: Loek van Rossem, Andrew M. Saxe
Categories: cs.LG q-bio.NC
Comments: 22 pages, 16 figures
\\
  Deep neural networks come in many sizes and architectures. The choice of
architecture, in conjunction with the dataset and learning algorithm, is
commonly understood to affect the learned neural representations. Yet, recent
results have shown that different architectures learn representations with
striking qualitative similarities. Here we derive an effective theory of
representation learning under the assumption that the encoding map from input
to hidden representation and the decoding map from representation to output are
arbitrary smooth functions. This theory schematizes representation learning
dynamics in the regime of complex, large architectures, where hidden
representations are not strongly constrained by the parametrization. We show
through experiments that the effective theory describes aspects of
representation learning dynamics across a range of deep networks with different
activation functions and architectures, and exhibits phenomena similar to the
"rich" and "lazy" regime. While many network behaviors depend quantitatively on
architecture, our findings point to certain behaviors that are widely conserved
once models are sufficiently flexible.
\\ ( https://arxiv.org/abs/2402.09142 ,  4610kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09146
Date: Wed, 14 Feb 2024 12:55:28 GMT   (2858kb,D)

Title: ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural
  Networks
Authors: Muhammad Kashif, Muhammad Shafique
Categories: cs.LG quant-ph
\\
  In this paper, we present a novel framework for enhancing the performance of
Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional
layers and addressing the critical challenges associated with them. Traditional
quanvolutional layers, although beneficial for feature extraction, have largely
been static, offering limited adaptability. Unlike state-of-the-art, our
research overcomes this limitation by enabling training within these layers,
significantly increasing the flexibility and potential of QuNNs. However, the
introduction of multiple trainable quanvolutional layers induces complexities
in gradient-based optimization, primarily due to the difficulty in accessing
gradients across these layers. To resolve this, we propose a novel
architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging
the concept of residual learning, which facilitates the flow of gradients by
adding skip connections between layers. By inserting residual blocks between
quanvolutional layers, we ensure enhanced gradient access throughout the
network, leading to improved training performance. Moreover, we provide
empirical evidence on the strategic placement of these residual blocks within
QuNNs. Through extensive experimentation, we identify an efficient
configuration of residual blocks, which enables gradients across all the layers
in the network that eventually results in efficient training. Our findings
suggest that the precise location of residual blocks plays a crucial role in
maximizing the performance gains in QuNNs. Our results mark a substantial step
forward in the evolution of quantum deep learning, offering new avenues for
both theoretical development and practical quantum computing applications.
\\ ( https://arxiv.org/abs/2402.09146 ,  2858kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09152
Date: Wed, 14 Feb 2024 13:08:26 GMT   (23kb,D)

Title: Improved Regret for Bandit Convex Optimization with Delayed Feedback
Authors: Yuanyu Wan and Chang Yao and Mingli Song and Lijun Zhang
Categories: cs.LG
\\
  We investigate bandit convex optimization (BCO) with delayed feedback, where
only the loss value of the action is revealed under an arbitrary delay.
Previous studies have established a regret bound of $O(T^{3/4}+d^{1/3}T^{2/3})$
for this problem, where $d$ is the maximum delay, by simply feeding delayed
loss values to the classical bandit gradient descent (BGD) algorithm. In this
paper, we develop a novel algorithm to enhance the regret, which carefully
exploits the delayed bandit feedback via a blocking update mechanism. Our
analysis first reveals that the proposed algorithm can decouple the joint
effect of the delays and bandit feedback on the regret, and improve the regret
bound to $O(T^{3/4}+\sqrt{dT})$ for convex functions. Compared with the
previous result, our regret matches the $O(T^{3/4})$ regret of BGD in the
non-delayed setting for a larger amount of delay, i.e., $d=O(\sqrt{T})$,
instead of $d=O(T^{1/4})$. Furthermore, we consider the case with strongly
convex functions, and prove that the proposed algorithm can enjoy a better
regret bound of $O(T^{2/3}\log^{1/3}T+d\log T)$. Finally, we show that in a
special case with unconstrained action sets, it can be simply extended to
achieve a regret bound of $O(\sqrt{T\log T}+d\log T)$ for strongly convex and
smooth functions.
\\ ( https://arxiv.org/abs/2402.09152 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09154
Date: Wed, 14 Feb 2024 13:13:26 GMT   (243kb,D)

Title: Attacking Large Language Models with Projected Gradient Descent
Authors: Simon Geisler, Tom Wollschl\"ager, M. H. I. Abdalla, Johannes
  Gasteiger, Stephan G\"unnemann
Categories: cs.LG
\\
  Current LLM alignment methods are readily broken through specifically crafted
adversarial prompts. While crafting adversarial prompts using discrete
optimization is highly effective, such attacks typically use more than 100,000
LLM calls. This high computational cost makes them unsuitable for, e.g.,
quantitative analyses and adversarial training. To remedy this, we revisit
Projected Gradient Descent (PGD) on the continuously relaxed input prompt.
Although previous attempts with ordinary gradient-based attacks largely failed,
we show that carefully controlling the error introduced by the continuous
relaxation tremendously boosts their efficacy. Our PGD for LLMs is up to one
order of magnitude faster than state-of-the-art discrete optimization to
achieve the same devastating attack results.
\\ ( https://arxiv.org/abs/2402.09154 ,  243kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09165
Date: Wed, 14 Feb 2024 13:31:53 GMT   (1507kb,D)

Title: Unifying Invariance and Spuriousity for Graph Out-of-Distribution via
  Probability of Necessity and Sufficiency
Authors: Xuexin Chen, Ruichu Cai, Kaitao Zheng, Zhifan Jiang, Zhengting Huang,
  Zhifeng Hao, Zijian Li
Categories: cs.LG
\\
  Graph Out-of-Distribution (OOD), requiring that models trained on biased data
generalize to the unseen test data, has a massive of real-world applications.
One of the most mainstream methods is to extract the invariant subgraph by
aligning the original and augmented data with the help of environment
augmentation. However, these solutions might lead to the loss or redundancy of
semantic subgraph and further result in suboptimal generalization. To address
this challenge, we propose a unified framework to exploit the Probability of
Necessity and Sufficiency to extract the Invariant Substructure (PNSIS). Beyond
that, this framework further leverages the spurious subgraph to boost the
generalization performance in an ensemble manner to enhance the robustness on
the noise data. Specificially, we first consider the data generation process
for graph data. Under mild conditions, we show that the invariant subgraph can
be extracted by minimizing an upper bound, which is built on the theoretical
advance of probability of necessity and sufficiency. To further bridge the
theory and algorithm, we devise the PNSIS model, which involves an invariant
subgraph extractor for invariant graph learning as well invariant and spurious
subgraph classifiers for generalization enhancement. Experimental results
demonstrate that our \textbf{PNSIS} model outperforms the state-of-the-art
techniques on graph OOD on several benchmarks, highlighting the effectiveness
in real-world scenarios.
\\ ( https://arxiv.org/abs/2402.09165 ,  1507kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09166
Date: Wed, 14 Feb 2024 13:32:23 GMT   (1519kb,D)

Title: Deinterleaving of Discrete Renewal Process Mixtures with Application to
  Electronic Support Measures
Authors: Jean Pinsolle, Olivier Goudet, Cyrille Enderli, Sylvain Lamprier and
  Jin-Kao Hao
Categories: cs.LG
\\
  In this paper, we propose a new deinterleaving method for mixtures of
discrete renewal Markov chains. This method relies on the maximization of a
penalized likelihood score. It exploits all available information about both
the sequence of the different symbols and their arrival times. A theoretical
analysis is carried out to prove that minimizing this score allows to recover
the true partition of symbols in the large sample limit, under mild conditions
on the component processes. This theoretical analysis is then validated by
experiments on synthetic data. Finally, the method is applied to deinterleave
pulse trains received from different emitters in a RESM (Radar Electronic
Support Measurements) context and we show that the proposed method competes
favorably with state-of-the-art methods on simulated warfare datasets.
\\ ( https://arxiv.org/abs/2402.09166 ,  1519kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09167
Date: Wed, 14 Feb 2024 13:36:20 GMT   (1577kb,D)

Title: Evolving Restricted Boltzmann Machine-Kohonen Network for Online
  Clustering
Authors: J. Senthilnath, Adithya Bhattiprolu, Ankur Singh, Bangjian Zhou, Min
  Wu, J\'on Atli Benediktsson, Xiaoli Li
Categories: cs.LG
Comments: 9 pages, 11 figures, 3 tables
\\
  A novel online clustering algorithm is presented where an Evolving Restricted
Boltzmann Machine (ERBM) is embedded with a Kohonen Network called ERBM-KNet.
The proposed ERBM-KNet efficiently handles streaming data in a single-pass mode
using the ERBM, employing a bias-variance strategy for neuron growing and
pruning, as well as online clustering based on a cluster update strategy for
cluster prediction and cluster center update using KNet. Initially, ERBM
evolves its architecture while processing unlabeled image data, effectively
disentangling the data distribution in the latent space. Subsequently, the KNet
utilizes the feature extracted from ERBM to predict the number of clusters and
updates the cluster centers. By overcoming the common challenges associated
with clustering algorithms, such as prior initialization of the number of
clusters and subpar clustering accuracy, the proposed ERBM-KNet offers
significant improvements. Extensive experimental evaluations on four benchmarks
and one industry dataset demonstrate the superiority of ERBM-KNet compared to
state-of-the-art approaches.
\\ ( https://arxiv.org/abs/2402.09167 ,  1577kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09173
Date: Wed, 14 Feb 2024 13:44:16 GMT   (30kb,D)

Title: Nearly Optimal Regret for Decentralized Online Convex Optimization
Authors: Yuanyu Wan and Tong Wei and Mingli Song and Lijun Zhang
Categories: cs.LG
\\
  We investigate decentralized online convex optimization (D-OCO), in which a
set of local learners are required to minimize a sequence of global loss
functions using only local computations and communications. Previous studies
have established $O(n^{5/4}\rho^{-1/2}\sqrt{T})$ and ${O}(n^{3/2}\rho^{-1}\log
T)$ regret bounds for convex and strongly convex functions respectively, where
$n$ is the number of local learners, $\rho<1$ is the spectral gap of the
communication matrix, and $T$ is the time horizon. However, there exist large
gaps from the existing lower bounds, i.e., $\Omega(n\sqrt{T})$ for convex
functions and $\Omega(n)$ for strongly convex functions. To fill these gaps, in
this paper, we first develop novel D-OCO algorithms that can respectively
reduce the regret bounds for convex and strongly convex functions to
$\tilde{O}(n\rho^{-1/4}\sqrt{T})$ and $\tilde{O}(n\rho^{-1/2}\log T)$. The
primary technique is to design an online accelerated gossip strategy that
enjoys a faster average consensus among local learners. Furthermore, by
carefully exploiting the spectral properties of a specific network topology, we
enhance the lower bounds for convex and strongly convex functions to
$\Omega(n\rho^{-1/4}\sqrt{T})$ and $\Omega(n\rho^{-1/2})$, respectively. These
lower bounds suggest that our algorithms are nearly optimal in terms of $T$,
$n$, and $\rho$.
\\ ( https://arxiv.org/abs/2402.09173 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09177
Date: Wed, 14 Feb 2024 13:45:19 GMT   (3288kb,D)

Title: Leveraging the Context through Multi-Round Interactions for Jailbreaking
  Attacks
Authors: Yixin Cheng, Markos Georgopoulos, Volkan Cevher, Grigorios G. Chrysos
Categories: cs.LG cs.AI cs.CL
Comments: 29 pages
\\
  Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which
aim to extract harmful information by subtly modifying the attack query. As
defense mechanisms evolve, directly obtaining harmful information becomes
increasingly challenging for Jailbreaking attacks. In this work, inspired by
human practices of indirect context to elicit harmful information, we focus on
a new attack form called Contextual Interaction Attack. The idea relies on the
autoregressive nature of the generation process in LLMs. We contend that the
prior context--the information preceding the attack query--plays a pivotal role
in enabling potent Jailbreaking attacks. Specifically, we propose an approach
that leverages preliminary question-answer pairs to interact with the LLM. By
doing so, we guide the responses of the model toward revealing the 'desired'
harmful information. We conduct experiments on four different LLMs and
demonstrate the efficacy of this attack, which is black-box and can also
transfer across LLMs. We believe this can lead to further developments and
understanding of the context vector in LLMs.
\\ ( https://arxiv.org/abs/2402.09177 ,  3288kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09197
Date: Wed, 14 Feb 2024 14:27:52 GMT   (2433kb,D)

Title: Implementing local-explainability in Gradient Boosting Trees: Feature
  Contribution
Authors: \'Angel Delgado-Panadero, Beatriz Hern\'andez-Lorca, Mar\'ia Teresa
  Garc\'ia-Ord\'as and Jos\'e Alberto Ben\'itez-Andrades
Categories: cs.LG cs.LO
Journal-ref: Information Sciences, Volume 589, 2022, Pages 199-212
DOI: 10.1016/j.ins.2021.12.111
\\
  Gradient Boost Decision Trees (GBDT) is a powerful additive model based on
tree ensembles. Its nature makes GBDT a black-box model even though there are
multiple explainable artificial intelligence (XAI) models obtaining information
by reinterpreting the model globally and locally. Each tree of the ensemble is
a transparent model itself but the final outcome is the result of a sum of
these trees and it is not easy to clarify.
  In this paper, a feature contribution method for GBDT is developed. The
proposed method takes advantage of the GBDT architecture to calculate the
contribution of each feature using the residue of each node. This algorithm
allows to calculate the sequence of node decisions given a prediction.
  Theoretical proofs and multiple experiments have been carried out to
demonstrate the performance of our method which is not only a local
explicability model for the GBDT algorithm but also a unique option that
reflects GBDTs internal behavior. The proposal is aligned to the contribution
of characteristics having impact in some artificial intelligence problems such
as ethical analysis of Artificial Intelligence (AI) and comply with the new
European laws such as the General Data Protection Regulation (GDPR) about the
right to explain and nondiscrimination.
\\ ( https://arxiv.org/abs/2402.09197 ,  2433kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09201
Date: Wed, 14 Feb 2024 14:33:39 GMT   (37kb)

Title: Better-than-KL PAC-Bayes Bounds
Authors: Ilja Kuzborskij, Kwang-Sung Jun, Yulian Wu, Kyoungseok Jang, Francesco
  Orabona
Categories: cs.LG stat.ML
\\
  Let $f(\theta, X_1),$ $ \dots,$ $ f(\theta, X_n)$ be a sequence of random
elements, where $f$ is a fixed scalar function, $X_1, \dots, X_n$ are
independent random variables (data), and $\theta$ is a random parameter
distributed according to some data-dependent posterior distribution $P_n$. In
this paper, we consider the problem of proving concentration inequalities to
estimate the mean of the sequence. An example of such a problem is the
estimation of the generalization error of some predictor trained by a
stochastic algorithm, such as a neural network where $f$ is a loss function.
Classically, this problem is approached through a PAC-Bayes analysis where, in
addition to the posterior, we choose a prior distribution which captures our
belief about the inductive bias of the learning problem. Then, the key quantity
in PAC-Bayes concentration bounds is a divergence that captures the complexity
of the learning problem where the de facto standard choice is the KL
divergence. However, the tightness of this choice has rarely been questioned.
  In this paper, we challenge the tightness of the KL-divergence-based bounds
by showing that it is possible to achieve a strictly tighter bound. In
particular, we demonstrate new high-probability PAC-Bayes bounds with a novel
and better-than-KL divergence that is inspired by Zhang et al. (2022). Our
proof is inspired by recent advances in regret analysis of gambling algorithms,
and its use to derive concentration inequalities. Our result is
first-of-its-kind in that existing PAC-Bayes bounds with non-KL divergences are
not known to be strictly better than KL. Thus, we believe our work marks the
first step towards identifying optimal rates of PAC-Bayes bounds.
\\ ( https://arxiv.org/abs/2402.09201 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09226
Date: Wed, 14 Feb 2024 15:10:37 GMT   (706kb,D)

Title: Directional Convergence Near Small Initializations and Saddles in
  Two-Homogeneous Neural Networks
Authors: Akshay Kumar and Jarvis Haupt
Categories: cs.LG math.OC stat.ML
\\
  This paper examines gradient flow dynamics of two-homogeneous neural networks
for small initializations, where all weights are initialized near the origin.
For both square and logistic losses, it is shown that for sufficiently small
initializations, the gradient flow dynamics spend sufficient time in the
neighborhood of the origin to allow the weights of the neural network to
approximately converge in direction to the Karush-Kuhn-Tucker (KKT) points of a
neural correlation function that quantifies the correlation between the output
of the neural network and corresponding labels in the training data set. For
square loss, it has been observed that neural networks undergo saddle-to-saddle
dynamics when initialized close to the origin. Motivated by this, this paper
also shows a similar directional convergence among weights of small magnitude
in the neighborhood of certain saddle points.
\\ ( https://arxiv.org/abs/2402.09226 ,  706kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09234
Date: Wed, 14 Feb 2024 15:22:59 GMT   (4213kb,D)

Title: Multi-Hierarchical Surrogate Learning for Structural Dynamics of
  Automotive Crashworthiness Using Graph Convolutional Neural Networks
Authors: Jonas Kneifl, J\"org Fehr, Steven L. Brunton, J. Nathan Kutz
Categories: cs.LG math.DS
\\
  Crash simulations play an essential role in improving vehicle safety, design
optimization, and injury risk estimation. Unfortunately, numerical solutions of
such problems using state-of-the-art high-fidelity models require significant
computational effort. Conventional data-driven surrogate modeling approaches
create low-dimensional embeddings for evolving the dynamics in order to
circumvent this computational effort. Most approaches directly operate on
high-resolution data obtained from numerical discretization, which is both
costly and complicated for mapping the flow of information over large spatial
distances. Furthermore, working with a fixed resolution prevents the adaptation
of surrogate models to environments with variable computing capacities,
different visualization resolutions, and different accuracy requirements. We
thus propose a multi-hierarchical framework for structurally creating a series
of surrogate models for a kart frame, which is a good proxy for
industrial-relevant crash simulations, at different levels of resolution. For
multiscale phenomena, macroscale features are captured on a coarse surrogate,
whereas microscale effects are resolved by finer ones. The learned behavior of
the individual surrogates is passed from coarse to finer levels through
transfer learning. In detail, we perform a mesh simplification on the kart
model to obtain multi-resolution representations of it. We then train a
graph-convolutional neural network-based surrogate that learns
parameter-dependent low-dimensional latent dynamics on the coarsest
representation. Subsequently, another, similarly structured surrogate is
trained on the residual of the first surrogate using a finer resolution. This
step can be repeated multiple times. By doing so, we construct multiple
surrogates for the same system with varying hardware requirements and
increasing accuracy.
\\ ( https://arxiv.org/abs/2402.09234 ,  4213kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09236
Date: Wed, 14 Feb 2024 15:23:59 GMT   (114kb,D)

Title: Learning Interpretable Concepts: Unifying Causal Representation Learning
  and Foundation Models
Authors: Goutham Rajendran, Simon Buchholz, Bryon Aragam, Bernhard Sch\"olkopf,
  Pradeep Ravikumar
Categories: cs.LG cs.AI math.ST stat.ML stat.TH
Comments: 36 pages
\\
  To build intelligent machine learning systems, there are two broad
approaches. One approach is to build inherently interpretable models, as
endeavored by the growing field of causal representation learning. The other
approach is to build highly-performant foundation models and then invest
efforts into understanding how they work. In this work, we relate these two
approaches and study how to learn human-interpretable concepts from data.
Weaving together ideas from both fields, we formally define a notion of
concepts and show that they can be provably recovered from diverse data.
Experiments on synthetic data and large language models show the utility of our
unified approach.
\\ ( https://arxiv.org/abs/2402.09236 ,  114kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09239
Date: Wed, 14 Feb 2024 15:27:53 GMT   (1808kb,D)

Title: Robust Training of Temporal GNNs using Nearest Neighbours based Hard
  Negatives
Authors: Shubham Gupta, Srikanta Bedathur
Categories: cs.LG cs.IR
Comments: 10 pages
\\
  Temporal graph neural networks Tgnn have exhibited state-of-art performance
in future-link prediction tasks. Training of these TGNNs is enumerated by
uniform random sampling based unsupervised loss. During training, in the
context of a positive example, the loss is computed over uninformative
negatives, which introduces redundancy and sub-optimal performance. In this
paper, we propose modified unsupervised learning of Tgnn, by replacing the
uniform negative sampling with importance-based negative sampling. We
theoretically motivate and define the dynamically computed distribution for a
sampling of negative examples. Finally, using empirical evaluations over three
real-world datasets, we show that Tgnn trained using loss based on proposed
negative sampling provides consistent superior performance.
\\ ( https://arxiv.org/abs/2402.09239 ,  1808kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09240
Date: Wed, 14 Feb 2024 15:28:42 GMT   (19254kb,D)

Title: Switch EMA: A Free Lunch for Better Flatness and Sharpness
Authors: Siyuan Li, Zicheng Liu, Juanxi Tian, Ge Wang, Zedong Wang, Weiyang
  Jin, Di Wu, Cheng Tan, Tao Lin, Yang Liu, Baigui Sun, and Stan Z. Li
Categories: cs.LG cs.CV
Comments: Preprint V1. Source code and models at
  https://github.com/Westlake-AI/SEMA
\\
  Exponential Moving Average (EMA) is a widely used weight averaging (WA)
regularization to learn flat optima for better generalizations without extra
cost in deep neural network (DNN) optimization. Despite achieving better
flatness, existing WA methods might fall into worse final performances or
require extra test-time computations. This work unveils the full potential of
EMA with a single line of modification, i.e., switching the EMA parameters to
the original model after each epoch, dubbed as Switch EMA (SEMA). From both
theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to
reach generalization optima that better trade-off between flatness and
sharpness. To verify the effectiveness of SEMA, we conduct comparison
experiments with discriminative, generative, and regression tasks on vision and
language datasets, including image classification, self-supervised learning,
object detection and segmentation, image generation, video prediction,
attribute regression, and language modeling. Comprehensive results with popular
optimizers and networks show that SEMA is a free lunch for DNN training by
improving performances and boosting convergence speeds.
\\ ( https://arxiv.org/abs/2402.09240 ,  19254kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09247
Date: Wed, 14 Feb 2024 15:35:53 GMT   (1156kb,D)

Title: Momentum Approximation in Asynchronous Private Federated Learning
Authors: Tao Yu, Congzheng Song, Jianyu Wang, Mona Chitnis
Categories: cs.LG
\\
  Asynchronous protocols have been shown to improve the scalability of
federated learning (FL) with a massive number of clients. Meanwhile,
momentum-based methods can achieve the best model quality in synchronous FL.
However, naively applying momentum in asynchronous FL algorithms leads to
slower convergence and degraded model performance. It is still unclear how to
effective combinie these two techniques together to achieve a win-win. In this
paper, we find that asynchrony introduces implicit bias to momentum updates. In
order to address this problem, we propose momentum approximation that minimizes
the bias by finding an optimal weighted average of all historical model
updates. Momentum approximation is compatible with secure aggregation as well
as differential privacy, and can be easily integrated in production FL systems
with a minor communication and storage cost. We empirically demonstrate that on
benchmark FL datasets, momentum approximation can achieve $1.15
\textrm{--}4\times$ speed up in convergence compared to existing asynchronous
FL optimizers with momentum.
\\ ( https://arxiv.org/abs/2402.09247 ,  1156kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09249
Date: Wed, 14 Feb 2024 15:37:58 GMT   (453kb,D)

Title: Exploring the Relationship: Transformative Adaptive Activation Functions
  in Comparison to Other Activation Functions
Authors: Vladim\'ir Kunc
Categories: cs.LG cs.NE
ACM-class: I.5.1
\\
  Neural networks are the state-of-the-art approach for many tasks and the
activation function is one of the main building blocks that allow such
performance. Recently, a novel transformative adaptive activation function
(TAAF) allowing for any vertical and horizontal translation and scaling was
proposed. This work sets the TAAF into the context of other activation
functions. It shows that the TAAFs generalize over 50 existing activation
functions and utilize similar concepts as over 70 other activation functions,
underscoring the versatility of TAAFs. This comprehensive exploration positions
TAAFs as a promising and adaptable addition to neural networks.
\\ ( https://arxiv.org/abs/2402.09249 ,  453kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09264
Date: Wed, 14 Feb 2024 15:51:28 GMT   (3528kb,D)

Title: UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers
Authors: Hong Jia, Young D. Kwon, Dong Ma, Nhat Pham, Lorena Qendro, Tam Vu and
  Cecilia Mascolo
Categories: cs.LG cs.HC
\\
  Traditional machine learning techniques are prone to generating inaccurate
predictions when confronted with shifts in the distribution of data between the
training and testing phases. This vulnerability can lead to severe
consequences, especially in applications such as mobile healthcare. Uncertainty
estimation has the potential to mitigate this issue by assessing the
reliability of a model's output. However, existing uncertainty estimation
techniques often require substantial computational resources and memory, making
them impractical for implementation on microcontrollers (MCUs). This limitation
hinders the feasibility of many important on-device wearable event detection
(WED) applications, such as heart attack detection.
  In this paper, we present UR2M, a novel Uncertainty and Resource-aware event
detection framework for MCUs. Specifically, we (i) develop an uncertainty-aware
WED based on evidential theory for accurate event detection and reliable
uncertainty estimation; (ii) introduce a cascade ML framework to achieve
efficient model inference via early exits, by sharing shallower model layers
among different event models; (iii) optimize the deployment of the model and
MCU library for system efficiency. We conducted extensive experiments and
compared UR2M to traditional uncertainty baselines using three wearable
datasets. Our results demonstrate that UR2M achieves up to 864% faster
inference speed, 857% energy-saving for uncertainty estimation, 55% memory
saving on two popular MCUs, and a 22% improvement in uncertainty quantification
performance.
  UR2M can be deployed on a wide range of MCUs, significantly expanding
real-time and reliable WED applications.
\\ ( https://arxiv.org/abs/2402.09264 ,  3528kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09268
Date: Wed, 14 Feb 2024 15:54:55 GMT   (520kb,D)

Title: Transformers, parallel computation, and logarithmic depth
Authors: Clayton Sanford, Daniel Hsu, Matus Telgarsky
Categories: cs.LG
Comments: 58 pages, 19 figures, code available at
  https://github.com/chsanford/hop-induction-heads
\\
  We show that a constant number of self-attention layers can efficiently
simulate, and be simulated by, a constant number of communication rounds of
Massively Parallel Computation. As a consequence, we show that logarithmic
depth is sufficient for transformers to solve basic computational tasks that
cannot be efficiently solved by several other neural sequence models and
sub-quadratic transformer approximations. We thus establish parallelism as a
key distinguishing property of transformers.
\\ ( https://arxiv.org/abs/2402.09268 ,  520kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09271
Date: Wed, 14 Feb 2024 15:59:22 GMT   (472kb,D)

Title: Hybrid Machine Learning techniques in the management of harmful algal
  blooms impact
Authors: Andres Molares-Ulloa, Daniel Rivero, Jesus Gil Ruiz, Enrique
  Fernandez-Blanco and Luis de-la-Fuente-Valent\'in
Categories: cs.LG q-bio.QM
Journal-ref: Computers and Electronics in Agriculture, 211, 107988. (2023)
DOI: 10.1016/j.compag.2023.107988
\\
  Harmful algal blooms (HABs) are episodes of high concentrations of algae that
are potentially toxic for human consumption. Mollusc farming can be affected by
HABs because, as filter feeders, they can accumulate high concentrations of
marine biotoxins in their tissues. To avoid the risk to human consumption,
harvesting is prohibited when toxicity is detected. At present, the closure of
production areas is based on expert knowledge and the existence of a predictive
model would help when conditions are complex and sampling is not possible.
Although the concentration of toxin in meat is the method most commonly used by
experts in the control of shellfish production areas, it is rarely used as a
target by automatic prediction models. This is largely due to the irregularity
of the data due to the established sampling programs. As an alternative, the
activity status of production areas has been proposed as a target variable
based on whether mollusc meat has a toxicity level below or above the legal
limit. This new option is the most similar to the actual functioning of the
control of shellfish production areas. For this purpose, we have made a
comparison between hybrid machine learning models like Neural-Network-Adding
Bootstrap (BAGNET) and Discriminative Nearest Neighbor Classification (SVM-KNN)
when estimating the state of production areas. The study has been carried out
in several estuaries with different levels of complexity in the episodes of
algal blooms to demonstrate the generalization capacity of the models in bloom
detection. As a result, we could observe that, with an average recall value of
93.41% and without dropping below 90% in any of the estuaries, BAGNET
outperforms the other models both in terms of results and robustness.
\\ ( https://arxiv.org/abs/2402.09271 ,  472kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09281
Date: Wed, 14 Feb 2024 16:10:42 GMT   (1100kb,D)

Title: Synergistic eigenanalysis of covariance and Hessian matrices for
  enhanced binary classification
Authors: Agus Hartoyo, Jan Argasi\'nski, Aleksandra Trenk, Kinga Przybylska,
  Anna B{\l}asiak, Alessandro Crimi
Categories: cs.LG cs.AI
Comments: 19 pages, 6 figures
\\
  Covariance and Hessian matrices have been analyzed separately in the
literature for classification problems. However, integrating these matrices has
the potential to enhance their combined power in improving classification
performance. We present a novel approach that combines the eigenanalysis of a
covariance matrix evaluated on a training set with a Hessian matrix evaluated
on a deep learning model to achieve optimal class separability in binary
classification tasks. Our approach is substantiated by formal proofs that
establish its capability to maximize between-class mean distance and minimize
within-class variances. By projecting data into the combined space of the most
relevant eigendirections from both matrices, we achieve optimal class
separability as per the linear discriminant analysis (LDA) criteria. Empirical
validation across neural and health datasets consistently supports our
theoretical framework and demonstrates that our method outperforms established
methods. Our method stands out by addressing both LDA criteria, unlike PCA and
the Hessian method, which predominantly emphasize one criterion each. This
comprehensive approach captures intricate patterns and relationships, enhancing
classification performance. Furthermore, through the utilization of both LDA
criteria, our method outperforms LDA itself by leveraging higher-dimensional
feature spaces, in accordance with Cover's theorem, which favors linear
separability in higher dimensions. Our method also surpasses kernel-based
methods and manifold learning techniques in performance. Additionally, our
approach sheds light on complex DNN decision-making, rendering them
comprehensible within a 2D space.
\\ ( https://arxiv.org/abs/2402.09281 ,  1100kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09288
Date: Wed, 14 Feb 2024 16:21:47 GMT   (7915kb,D)

Title: EcoVal: An Efficient Data Valuation Framework for Machine Learning
Authors: Ayush K Tarun, Vikram S Chundawat, Murari Mandal, Hong Ming Tan, Bowei
  Chen, Mohan Kankanhalli
Categories: cs.LG
\\
  Quantifying the value of data within a machine learning workflow can play a
pivotal role in making more strategic decisions in machine learning
initiatives. The existing Shapley value based frameworks for data valuation in
machine learning are computationally expensive as they require considerable
amount of repeated training of the model to obtain the Shapley value. In this
paper, we introduce an efficient data valuation framework EcoVal, to estimate
the value of data for machine learning models in a fast and practical manner.
Instead of directly working with individual data sample, we determine the value
of a cluster of similar data points. This value is further propagated amongst
all the member cluster points. We show that the overall data value can be
determined by estimating the intrinsic and extrinsic value of each data. This
is enabled by formulating the performance of a model as a \textit{production
function}, a concept which is popularly used to estimate the amount of output
based on factors like labor and capital in a traditional free economic market.
We provide a formal proof of our valuation technique and elucidate the
principles and mechanisms that enable its accelerated performance. We
demonstrate the real-world applicability of our method by showcasing its
effectiveness for both in-distribution and out-of-sample data. This work
addresses one of the core challenges of efficient data valuation at scale in
machine learning models.
\\ ( https://arxiv.org/abs/2402.09288 ,  7915kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09290
Date: Wed, 14 Feb 2024 16:23:23 GMT   (4673kb,D)

Title: Learning Interpretable Policies in Hindsight-Observable POMDPs through
  Partially Supervised Reinforcement Learning
Authors: Michael Lanier, Ying Xu, Nathan Jacobs, Chongjie Zhang, Yevgeniy
  Vorobeychik
Categories: cs.LG cs.AI
\\
  Deep reinforcement learning has demonstrated remarkable achievements across
diverse domains such as video games, robotic control, autonomous driving, and
drug discovery. Common methodologies in partially-observable domains largely
lean on end-to-end learning from high-dimensional observations, such as images,
without explicitly reasoning about true state. We suggest an alternative
direction, introducing the Partially Supervised Reinforcement Learning (PSRL)
framework. At the heart of PSRL is the fusion of both supervised and
unsupervised learning. The approach leverages a state estimator to distill
supervised semantic state information from high-dimensional observations which
are often fully observable at training time. This yields more interpretable
policies that compose state predictions with control. In parallel, it captures
an unsupervised latent representation. These two-the semantic state and the
latent state-are then fused and utilized as inputs to a policy network. This
juxtaposition offers practitioners a flexible and dynamic spectrum: from
emphasizing supervised state information to integrating richer, latent
insights. Extensive experimental results indicate that by merging these dual
representations, PSRL offers a potent balance, enhancing model interpretability
while preserving, and often significantly outperforming, the performance
benchmarks set by traditional methods in terms of reward and convergence speed.
\\ ( https://arxiv.org/abs/2402.09290 ,  4673kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09305
Date: Wed, 14 Feb 2024 16:49:13 GMT   (1407kb,D)

Title: Embracing the black box: Heading towards foundation models for causal
  discovery from time series data
Authors: Gideon Stein, Maha Shadaydeh, Joachim Denzler
Categories: cs.LG cs.AI
Comments: AAAI Workshop (AI4TS) 2024
MSC-class: 68T07
\\
  Causal discovery from time series data encompasses many existing solutions,
including those based on deep learning techniques. However, these methods
typically do not endorse one of the most prevalent paradigms in deep learning:
End-to-end learning. To address this gap, we explore what we call Causal
Pretraining. A methodology that aims to learn a direct mapping from
multivariate time series to the underlying causal graphs in a supervised
manner. Our empirical findings suggest that causal discovery in a supervised
manner is possible, assuming that the training and test time series samples
share most of their dynamics. More importantly, we found evidence that the
performance of Causal Pretraining can increase with data and model size, even
if the additional data do not share the same dynamics. Further, we provide
examples where causal discovery for real-world data with causally pretrained
neural networks is possible within limits. We argue that this hints at the
possibility of a foundation model for causal discovery.
\\ ( https://arxiv.org/abs/2402.09305 ,  1407kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09326
Date: Wed, 14 Feb 2024 17:17:05 GMT   (486kb,D)

Title: Stability and Multigroup Fairness in Ranking with Uncertain Predictions
Authors: Siddartha Devic, Aleksandra Korolova, David Kempe, Vatsal Sharan
Categories: cs.LG
\\
  Rankings are ubiquitous across many applications, from search engines to
hiring committees. In practice, many rankings are derived from the output of
predictors. However, when predictors trained for classification tasks have
intrinsic uncertainty, it is not obvious how this uncertainty should be
represented in the derived rankings. Our work considers ranking functions: maps
from individual predictions for a classification task to distributions over
rankings. We focus on two aspects of ranking functions: stability to
perturbations in predictions and fairness towards both individuals and
subgroups. Not only is stability an important requirement for its own sake, but
-- as we show -- it composes harmoniously with individual fairness in the sense
of Dwork et al. (2012). While deterministic ranking functions cannot be stable
aside from trivial scenarios, we show that the recently proposed uncertainty
aware (UA) ranking functions of Singh et al. (2021) are stable. Our main result
is that UA rankings also achieve multigroup fairness through successful
composition with multiaccurate or multicalibrated predictors. Our work
demonstrates that UA rankings naturally interpolate between group and
individual level fairness guarantees, while simultaneously satisfying stability
guarantees important whenever machine-learned predictions are used.
\\ ( https://arxiv.org/abs/2402.09326 ,  486kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09327
Date: Wed, 14 Feb 2024 17:17:30 GMT   (180kb,D)

Title: Information Complexity of Stochastic Convex Optimization: Applications
  to Generalization and Memorization
Authors: Idan Attias, Gintare Karolina Dziugaite, Mahdi Haghifam, Roi Livni,
  Daniel M. Roy
Categories: cs.LG
Comments: 44 Pages
\\
  In this work, we investigate the interplay between memorization and learning
in the context of \emph{stochastic convex optimization} (SCO). We define
memorization via the information a learning algorithm reveals about its
training data points. We then quantify this information using the framework of
conditional mutual information (CMI) proposed by Steinke and Zakynthinou
(2020). Our main result is a precise characterization of the tradeoff between
the accuracy of a learning algorithm and its CMI, answering an open question
posed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded setting
and under strong convexity, every learner with an excess error $\varepsilon$
has CMI bounded below by $\Omega(1/\varepsilon^2)$ and $\Omega(1/\varepsilon)$,
respectively. We further demonstrate the essential role of memorization in
learning problems in SCO by designing an adversary capable of accurately
identifying a significant fraction of the training samples in specific SCO
problems. Finally, we enumerate several implications of our results, such as a
limitation of generalization bounds based on CMI and the incompressibility of
samples in SCO problems.
\\ ( https://arxiv.org/abs/2402.09327 ,  180kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09345
Date: Wed, 14 Feb 2024 17:49:07 GMT   (41044kb,D)

Title: Mitigating Reward Hacking via Information-Theoretic Reward Modeling
Authors: Yuchun Miao, Sen Zhang, Liang Ding, Rong Bao, Lefei Zhang, Dacheng Tao
Categories: cs.LG cs.AI
Comments: 26 pages, 28 figures
\\
  Despite the success of reinforcement learning from human feedback (RLHF) in
aligning language models with human values, reward hacking, also termed reward
overoptimization, remains a critical challenge, which primarily stems from
limitations in reward modeling, i.e., generalizability of the reward model and
inconsistency in the preference dataset. In this work, we tackle this problem
from an information theoretic-perspective, and propose a generalizable and
robust framework for reward modeling, namely InfoRM, by introducing a
variational information bottleneck objective to filter out irrelevant
information and developing a mechanism for model complexity modulation.
Notably, we further identify a correlation between overoptimization and
outliers in the latent space, establishing InfoRM as a promising tool for
detecting reward overoptimization. Inspired by this finding, we propose the
Integrated Cluster Deviation Score (ICDS), which quantifies deviations in the
latent space, as an indicator of reward overoptimization to facilitate the
development of online mitigation strategies. Extensive experiments on a wide
range of settings and model scales (70M, 440M, 1.4B, and 7B) support the
effectiveness of InfoRM. Further analyses reveal that InfoRM's overoptimization
detection mechanism is effective, potentially signifying a notable advancement
in the field of RLHF. Code will be released upon acceptance.
\\ ( https://arxiv.org/abs/2402.09345 ,  41044kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09360
Date: Wed, 14 Feb 2024 18:04:36 GMT   (273kb,D)

Title: HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM
  Inference
Authors: Yashas Samaga B L and Varun Yerram and Chong You and Srinadh
  Bhojanapalli and Sanjiv Kumar and Prateek Jain and Praneeth Netrapalli
Categories: cs.LG cs.AI
\\
  Autoregressive decoding with generative Large Language Models (LLMs) on
accelerators (GPUs/TPUs) is often memory-bound where most of the time is spent
on transferring model parameters from high bandwidth memory (HBM) to cache. On
the other hand, recent works show that LLMs can maintain quality with
significant sparsity/redundancy in the feedforward (FFN) layers by
appropriately training the model to operate on a top-$k$ fraction of
rows/columns (where $k \approx 0.05$), there by suggesting a way to reduce the
transfer of model parameters, and hence latency. However, exploiting this
sparsity for improving latency is hindered by the fact that identifying top
rows/columns is data-dependent and is usually performed using full matrix
operations, severely limiting potential gains. To address these issues, we
introduce HiRE (High Recall Approximate Top-k Estimation). HiRE comprises of
two novel components: (i) a compression scheme to cheaply predict top-$k$
rows/columns with high recall, followed by full computation restricted to the
predicted subset, and (ii) DA-TOP-$k$: an efficient multi-device approximate
top-$k$ operator. We demonstrate that on a one billion parameter model, HiRE
applied to both the softmax as well as feedforward layers, achieves almost
matching pretraining and downstream accuracy, and speeds up inference latency
by $1.47\times$ on a single TPUv5e device.
\\ ( https://arxiv.org/abs/2402.09360 ,  273kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09371
Date: Wed, 14 Feb 2024 18:18:29 GMT   (767kb,D)

Title: Transformers Can Achieve Length Generalization But Not Robustly
Authors: Yongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal,
  Denny Zhou
Categories: cs.LG cs.AI cs.CL
\\
  Length generalization, defined as the ability to extrapolate from shorter
training sequences to longer test ones, is a significant challenge for language
models. This issue persists even with large-scale Transformers handling
relatively straightforward tasks. In this paper, we test the Transformer's
ability of length generalization using the task of addition of two integers. We
show that the success of length generalization is intricately linked to the
data format and the type of position encoding. Using the right combination of
data format and position encodings, we show for the first time that standard
Transformers can extrapolate to a sequence length that is 2.5x the input
length. Nevertheless, unlike in-distribution generalization, length
generalization remains fragile, significantly influenced by factors like random
weight initialization and training data order, leading to large variances
across different random seeds.
\\ ( https://arxiv.org/abs/2402.09371 ,  767kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09373
Date: Wed, 14 Feb 2024 18:20:44 GMT   (1667kb,D)

Title: Loss Shaping Constraints for Long-Term Time Series Forecasting
Authors: Ignacio Hounie, Javier Porras-Valenzuela and Alejandro Ribeiro
Categories: cs.LG stat.ML
\\
  Several applications in time series forecasting require predicting multiple
steps ahead. Despite the vast amount of literature in the topic, both classical
and recent deep learning based approaches have mostly focused on minimising
performance averaged over the predicted window. We observe that this can lead
to disparate distributions of errors across forecasting steps, especially for
recent transformer architectures trained on popular forecasting benchmarks.
That is, optimising performance on average can lead to undesirably large errors
at specific time-steps. In this work, we present a Constrained Learning
approach for long-term time series forecasting that aims to find the best model
in terms of average performance that respects a user-defined upper bound on the
loss at each time-step. We call our approach loss shaping constraints because
it imposes constraints on the loss at each time step, and leverage recent
duality results to show that despite its non-convexity, the resulting problem
has a bounded duality gap. We propose a practical Primal-Dual algorithm to
tackle it, and demonstrate that the proposed approach exhibits competitive
average performance in time series forecasting benchmarks, while shaping the
distribution of errors across the predicted window.
\\ ( https://arxiv.org/abs/2402.09373 ,  1667kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09381
Date: Wed, 14 Feb 2024 18:26:58 GMT   (1499kb,D)

Title: GraSSRep: Graph-Based Self-Supervised Learning for Repeat Detection in
  Metagenomic Assembly
Authors: Ali Azizpour, Advait Balaji, Todd J. Treangen and Santiago Segarra
Categories: cs.LG
\\
  Repetitive DNA (repeats) poses significant challenges for accurate and
efficient genome assembly and sequence alignment. This is particularly true for
metagenomic data, where genome dynamics such as horizontal gene transfer, gene
duplication, and gene loss/gain complicate accurate genome assembly from
metagenomic communities. Detecting repeats is a crucial first step in
overcoming these challenges. To address this issue, we propose GraSSRep, a
novel approach that leverages the assembly graph's structure through graph
neural networks (GNNs) within a self-supervised learning framework to classify
DNA sequences into repetitive and non-repetitive categories. Specifically, we
frame this problem as a node classification task within a metagenomic assembly
graph. In a self-supervised fashion, we rely on a high-precision (but
low-recall) heuristic to generate pseudo-labels for a small proportion of the
nodes. We then use those pseudo-labels to train a GNN embedding and a random
forest classifier to propagate the labels to the remaining nodes. In this way,
GraSSRep combines sequencing features with pre-defined and learned graph
features to achieve state-of-the-art performance in repeat detection. We
evaluate our method using simulated and synthetic metagenomic datasets. The
results on the simulated data highlight our GraSSRep's robustness to repeat
attributes, demonstrating its effectiveness in handling the complexity of
repeated sequences. Additionally, our experiments with synthetic metagenomic
datasets reveal that incorporating the graph structure and the GNN enhances our
detection performance. Finally, in comparative analyses, GraSSRep outperforms
existing repeat detection tools with respect to precision and recall.
\\ ( https://arxiv.org/abs/2402.09381 ,  1499kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09398
Date: Wed, 14 Feb 2024 18:54:56 GMT   (27238kb,D)

Title: Get More with LESS: Synthesizing Recurrence with KV Cache Compression
  for Efficient LLM Inference
Authors: Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi,
  Beidi Chen
Categories: cs.LG cs.AI
\\
  Many computational factors limit broader deployment of large language models.
In this paper, we focus on a memory bottleneck imposed by the key-value (KV)
cache, a computational shortcut that requires storing previous KV pairs during
decoding. While existing KV cache methods approach this problem by pruning or
evicting large swaths of relatively less important KV pairs to dramatically
reduce the memory footprint of the cache, they can have limited success in
tasks that require recollecting a majority of previous tokens. To alleviate
this issue, we propose LESS, a simple integration of a (nearly free) constant
sized cache with eviction-based cache methods, such that all tokens can be
queried at later decoding steps. Its ability to retain information throughout
time shows merit on a variety of tasks where we demonstrate LESS can help
reduce the performance gap from caching everything, sometimes even matching it,
all while being efficient.
\\ ( https://arxiv.org/abs/2402.09398 ,  27238kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09401
Date: Wed, 14 Feb 2024 18:58:40 GMT   (64kb,D)

Title: Reinforcement Learning from Human Feedback with Active Queries
Authors: Kaixuan Ji and Jiafan He and Quanquan Gu
Categories: cs.LG cs.AI cs.CL math.OC stat.ML
Comments: 28 pages, 1 figure, 4 table
\\
  Aligning large language models (LLM) with human preference plays a key role
in building modern generative models and can be achieved by reinforcement
learning from human feedback (RLHF). Despite their superior performance,
current RLHF approaches often require a large amount of human-labelled
preference data, which is expensive to collect. In this paper, inspired by the
success of active learning, we address this problem by proposing
query-efficient RLHF methods. We first formalize the alignment problem as a
contextual dueling bandit problem and design an active-query-based proximal
policy optimization (APPO) algorithm with an $\tilde{O}(d^2/\Delta)$ regret
bound and an $\tilde{O}(d^2/\Delta^2)$ query complexity, where $d$ is the
dimension of feature space and $\Delta$ is the sub-optimality gap over all the
contexts. We then propose ADPO, a practical version of our algorithm based on
direct preference optimization (DPO) and apply it to fine-tuning LLMs. Our
experiments show that ADPO, while only making about half of queries for human
preference, matches the performance of the state-of-the-art DPO method.
\\ ( https://arxiv.org/abs/2402.09401 ,  64kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2402.08143 (*cross-listing*)
Date: Tue, 13 Feb 2024 00:36:10 GMT   (453kb)

Title: Artificial intelligence and the transformation of higher education
  institutions
Authors: Evangelos Katsamakas, Oleg V. Pavlov, and Ryan Saklad
Categories: econ.GN cs.AI cs.CY q-fin.EC
\\
  Artificial intelligence (AI) advances and the rapid adoption of generative AI
tools like ChatGPT present new opportunities and challenges for higher
education. While substantial literature discusses AI in higher education, there
is a lack of a systemic approach that captures a holistic view of the AI
transformation of higher education institutions (HEIs). To fill this gap, this
article, taking a complex systems approach, develops a causal loop diagram
(CLD) to map the causal feedback mechanisms of AI transformation in a typical
HEI. Our model accounts for the forces that drive the AI transformation and the
consequences of the AI transformation on value creation in a typical HEI. The
article identifies and analyzes several reinforcing and balancing feedback
loops, showing how, motivated by AI technology advances, the HEI invests in AI
to improve student learning, research, and administration. The HEI must take
measures to deal with academic integrity problems and adapt to changes in
available jobs due to AI, emphasizing AI-complementary skills for its students.
However, HEIs face a competitive threat and several policy traps that may lead
to decline. HEI leaders need to become systems thinkers to manage the
complexity of the AI transformation and benefit from the AI feedback loops
while avoiding the associated pitfalls. We also discuss long-term scenarios,
the notion of HEIs influencing the direction of AI, and directions for future
research on AI transformation.
\\ ( https://arxiv.org/abs/2402.08143 ,  453kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08430 (*cross-listing*)
Date: Tue, 13 Feb 2024 12:58:53 GMT   (417kb,D)

Title: Analyzing Prompt Influence on Automated Method Generation: An Empirical
  Study with Copilot
Authors: Ionut Daniel Fagadau, Leonardo Mariani, Daniela Micucci and Oliviero
  Riganelli
Categories: cs.SE cs.AI
Journal-ref: Proceedings of the 32nd IEEE/ACM International Conference on
  Program Comprehension (ICPC 2024)
DOI: 10.1145/3643916.3644409
\\
  Generative AI is changing the way developers interact with software systems,
providing services that can produce and deliver new content, crafted to satisfy
the actual needs of developers. For instance, developers can ask for new code
directly from within their IDEs by writing natural language prompts, and
integrated services based on generative AI, such as Copilot, immediately
respond to prompts by providing ready-to-use code snippets. Formulating the
prompt appropriately, and incorporating the useful information while avoiding
any information overload, can be an important factor in obtaining the right
piece of code. The task of designing good prompts is known as prompt
engineering. In this paper, we systematically investigate the influence of
eight prompt features on the style and the content of prompts, on the level of
correctness, complexity, size, and similarity to the developers' code of the
generated code. We specifically consider the task of using Copilot with 124,800
prompts obtained by systematically combining the eight considered prompt
features to generate the implementation of 200 Java methods. Results show how
some prompt features, such as the presence of examples and the summary of the
purpose of the method, can significantly influence the quality of the result.
\\ ( https://arxiv.org/abs/2402.08430 ,  417kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08690 (*cross-listing*)
Date: Fri, 9 Feb 2024 18:43:48 GMT   (373kb,D)

Title: If Turing played piano with an artificial partner
Authors: Dobromir Dotov, Dante Camarena, Zack Harris, Joanna Spyra, Pietro
  Gagliano, Laurel Trainor
Categories: cs.SI cs.AI cs.LG cs.SD
\\
  Music is an inherently social activity that allows people to share
experiences and feel connected with one another. There has been little progress
in designing artificial partners exhibiting a similar social experience as
playing with another person. Neural network architectures that implement
generative models, such as large language models, are suited for producing
musical scores. Playing music socially, however, involves more than playing a
score; it must complement the other musicians' ideas and keep time correctly.
We addressed the question of whether a convincing social experience is made
possible by a generative model trained to produce musical scores, not
necessarily optimized for synchronization and continuation. The network, a
variational autoencoder trained on a large corpus of digital scores, was
adapted for a timed call-and-response task with a human partner. Participants
played piano with a human or artificial partner-in various configurations-and
rated the performance quality and first-person experience of self-other
integration. Overall, the artificial partners held promise but were rated lower
than human partners. The artificial partner with simplest design and highest
similarity parameter was not rated differently from the human partners on some
measures, suggesting that interactive rather than generative sophistication is
important in enabling social AI.
\\ ( https://arxiv.org/abs/2402.08690 ,  373kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08703 (*cross-listing*)
Date: Tue, 13 Feb 2024 16:56:31 GMT   (3138kb,D)

Title: A Survey of Generative AI for De Novo Drug Design: New Frontiers in
  Molecule and Protein Generation
Authors: Xiangru Tang, Howard Dai, Elizabeth Knight, Fang Wu, Yunyang Li,
  Tianxiao Li, Mark Gerstein
Categories: q-bio.BM cs.AI cs.LG
\\
  Artificial intelligence (AI)-driven methods can vastly improve the
historically costly drug design process, with various generative models already
in widespread use. Generative models for de novo drug design, in particular,
focus on the creation of novel biological compounds entirely from scratch,
representing a promising future direction. Rapid development in the field,
combined with the inherent complexity of the drug design process, creates a
difficult landscape for new researchers to enter. In this survey, we organize
de novo drug design into two overarching themes: small molecule and protein
generation. Within each theme, we identify a variety of subtasks and
applications, highlighting important datasets, benchmarks, and model
architectures and comparing the performance of top models. We take a broad
approach to AI-driven drug design, allowing for both micro-level comparisons of
various methods within each subtask and macro-level observations across
different fields. We discuss parallel challenges and approaches between the two
applications and highlight future directions for AI-driven de novo drug design
as a whole. An organized repository of all covered sources is available at
https://github.com/gersteinlab/GenAI4Drug.
\\ ( https://arxiv.org/abs/2402.08703 ,  3138kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08777 (*cross-listing*)
Date: Tue, 13 Feb 2024 20:21:29 GMT   (5620kb,D)

Title: DNABERT-S: Learning Species-Aware DNA Embedding with Genome Foundation
  Models
Authors: Zhihan Zhou, Winmin Wu, Harrison Ho, Jiayi Wang, Lizhen Shi, Ramana V
  Davuluri, Zhong Wang, Han Liu
Categories: q-bio.GN cs.AI cs.CE cs.CL
\\
  Effective DNA embedding remains crucial in genomic analysis, particularly in
scenarios lacking labeled data for model fine-tuning, despite the significant
advancements in genome foundation models. A prime example is metagenomics
binning, a critical process in microbiome research that aims to group DNA
sequences by their species from a complex mixture of DNA sequences derived from
potentially thousands of distinct, often uncharacterized species. To fill the
lack of effective DNA embedding models, we introduce DNABERT-S, a genome
foundation model that specializes in creating species-aware DNA embeddings. To
encourage effective embeddings to error-prone long-read DNA sequences, we
introduce Manifold Instance Mixup (MI-Mix), a contrastive objective that mixes
the hidden representations of DNA sequences at randomly selected layers and
trains the model to recognize and differentiate these mixed proportions at the
output layer. We further enhance it with the proposed Curriculum Contrastive
Learning (C$^2$LR) strategy. Empirical results on 18 diverse datasets showed
DNABERT-S's remarkable performance. It outperforms the top baseline's
performance in 10-shot species classification with just a 2-shot training while
doubling the Adjusted Rand Index (ARI) in species clustering and substantially
increasing the number of correctly identified species in metagenomics binning.
The code, data, and pre-trained model are publicly available at
https://github.com/Zhihan1996/DNABERT_S.
\\ ( https://arxiv.org/abs/2402.08777 ,  5620kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08789 (*cross-listing*)
Date: Tue, 13 Feb 2024 20:54:55 GMT   (451kb,D)

Title: Leveraging cough sounds to optimize chest x-ray usage in low-resource
  settings
Authors: Alexander Philip, Sanya Chawla, Lola Jover, George P. Kafentzis, Joe
  Brew, Vishakh Saraf, Shibu Vijayan, Peter Small, Carlos Chaccour
Categories: eess.AS cs.AI cs.LG q-bio.QM
\\
  Chest X-ray is a commonly used tool during triage, diagnosis and management
of respiratory diseases. In resource-constricted settings, optimizing this
resource can lead to valuable cost savings for the health care system and the
patients as well as to and improvement in consult time. We used
prospectively-collected data from 137 patients referred for chest X-ray at the
Christian Medical Center and Hospital (CMCH) in Purnia, Bihar, India. Each
patient provided at least five coughs while awaiting radiography. Collected
cough sounds were analyzed using acoustic AI methods. Cross-validation was done
on temporal and spectral features on the cough sounds of each patient. Features
were summarized using standard statistical approaches. Three models were
developed, tested and compared in their capacity to predict an abnormal result
in the chest X-ray. All three methods yielded models that could discriminate to
some extent between normal and abnormal with the logistic regression performing
best with an area under the receiver operating characteristic curves ranging
from 0.7 to 0.78. Despite limitations and its relatively small sample size,
this study shows that AI-enabled algorithms can use cough sounds to predict
which individuals presenting for chest radiographic examination will have a
normal or abnormal results. These results call for expanding this research
given the potential optimization of limited health care resources in low- and
middle-income countries.
\\ ( https://arxiv.org/abs/2402.08789 ,  451kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08801 (*cross-listing*)
Date: Tue, 13 Feb 2024 21:15:33 GMT   (1941kb)

Title: ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow
  Discussions
Authors: Leuson Da Silva and Jordan Samhi and Foutse Khomh
Categories: cs.SE cs.AI
Comments: 36 pages, 9 figures
\\
  Since its release in November 2022, ChatGPT has shaken up Stack Overflow, the
premier platform for developers' queries on programming and software
development. Demonstrating an ability to generate instant, human-like responses
to technical questions, ChatGPT has ignited debates within the developer
community about the evolving role of human-driven platforms in the age of
generative AI. Two months after ChatGPT's release, Meta released its answer
with its own Large Language Model (LLM) called LLaMA: the race was on. We
conducted an empirical study analyzing questions from Stack Overflow and using
these LLMs to address them. This way, we aim to (ii) measure user engagement
evolution with Stack Overflow over time; (ii) quantify the reliability of LLMs'
answers and their potential to replace Stack Overflow in the long term; (iii)
identify and understand why LLMs fails; and (iv) compare LLMs together. Our
empirical results are unequivocal: ChatGPT and LLaMA challenge human expertise,
yet do not outperform it for some domains, while a significant decline in user
posting activity has been observed. Furthermore, we also discuss the impact of
our findings regarding the usage and development of new LLMs.
\\ ( https://arxiv.org/abs/2402.08801 ,  1941kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08812 (*cross-listing*)
Date: Tue, 13 Feb 2024 21:33:12 GMT   (8617kb,D)

Title: Intelligent Canvas: Enabling Design-Like Exploratory Visual Data
  Analysis through Rapid Prototyping, Iteration and Curation
Authors: Zijian Ding, Joel Chan
Categories: cs.HC cs.AI
\\
  Complex data analysis inherently seeks unexpected insights through
exploratory \re{visual analysis} methods, transcending logical, step-by-step
processing. However, \re{existing interfaces such as notebooks and dashboards
have limitations in exploration and comparison for visual data analysis}.
Addressing these limitations, we introduce a "design-like" intelligent canvas
environment integrating generative AI into data analysis, offering rapid
prototyping, iteration, and comparative visualization management. Our dual
contributions include the integration of generative AI components into a canvas
interface, and empirical findings from a user study (N=10) evaluating the
effectiveness of the canvas interface.
\\ ( https://arxiv.org/abs/2402.08812 ,  8617kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08855 (*cross-listing*)
Date: Tue, 13 Feb 2024 23:48:59 GMT   (5786kb,D)

Title: GhostWriter: Augmenting Collaborative Human-AI Writing Experiences
  Through Personalization and Agency
Authors: Catherine Yeh, Gonzalo Ramos, Rachel Ng, Andy Huntington, Richard
  Banks
Categories: cs.HC cs.AI
Comments: 29 pages, 12 figures
\\
  Large language models (LLMs) are becoming more prevalent and have found a
ubiquitous use in providing different forms of writing assistance. However,
LLM-powered writing systems can frustrate users due to their limited
personalization and control, which can be exacerbated when users lack
experience with prompt engineering. We see design as one way to address these
challenges and introduce GhostWriter, an AI-enhanced writing design probe where
users can exercise enhanced agency and personalization. GhostWriter leverages
LLMs to learn the user's intended writing style implicitly as they write, while
allowing explicit teaching moments through manual style edits and annotations.
We study 18 participants who use GhostWriter on two different writing tasks,
observing that it helps users craft personalized text generations and empowers
them by providing multiple ways to control the system's writing style. From
this study, we present insights regarding people's relationship with
AI-assisted writing and offer design recommendations for future work.
\\ ( https://arxiv.org/abs/2402.08855 ,  5786kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08876 (*cross-listing*)
Date: Wed, 14 Feb 2024 00:42:19 GMT   (43197kb,D)

Title: DUDF: Differentiable Unsigned Distance Fields with Hyperbolic Scaling
Authors: Miguel Fainstein, Viviana Siless and Emmanuel Iarussi
Categories: cs.CV cs.AI cs.GR
ACM-class: I.2.10; I.4.10; I.3.7
\\
  In recent years, there has been a growing interest in training Neural
Networks to approximate Unsigned Distance Fields (UDFs) for representing open
surfaces in the context of 3D reconstruction. However, UDFs are
non-differentiable at the zero level set which leads to significant errors in
distances and gradients, generally resulting in fragmented and discontinuous
surfaces. In this paper, we propose to learn a hyperbolic scaling of the
unsigned distance field, which defines a new Eikonal problem with distinct
boundary conditions. This allows our formulation to integrate seamlessly with
state-of-the-art continuously differentiable implicit neural representation
networks, largely applied in the literature to represent signed distance
fields. Our approach not only addresses the challenge of open surface
representation but also demonstrates significant improvement in reconstruction
quality and training performance. Moreover, the unlocked field's
differentiability allows the accurate computation of essential topological
properties such as normal directions and curvatures, pervasive in downstream
tasks such as rendering. Through extensive experiments, we validate our
approach across various data sets and against competitive baselines. The
results demonstrate enhanced accuracy and up to an order of magnitude increase
in speed compared to previous methods.
\\ ( https://arxiv.org/abs/2402.08876 ,  43197kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08921 (*cross-listing*)
Date: Wed, 14 Feb 2024 03:41:50 GMT   (868kb,D)

Title: Enhancing ID and Text Fusion via Alternative Training in Session-based
  Recommendation
Authors: Juanhui Li, Haoyu Han, Zhikai Chen, Harry Shomer, Wei Jin, Amin
  Javari, Jiliang Tang
Categories: cs.IR cs.AI
\\
  Session-based recommendation has gained increasing attention in recent years,
with its aim to offer tailored suggestions based on users' historical behaviors
within sessions.
  To advance this field, a variety of methods have been developed, with
ID-based approaches typically demonstrating promising performance. However,
these methods often face challenges with long-tail items and overlook other
rich forms of information, notably valuable textual semantic information. To
integrate text information, various methods have been introduced, mostly
following a naive fusion framework. Surprisingly, we observe that fusing these
two modalities does not consistently outperform the best single modality by
following the naive fusion framework. Further investigation reveals an
potential imbalance issue in naive fusion, where the ID dominates and text
modality is undertrained. This suggests that the unexpected observation may
stem from naive fusion's failure to effectively balance the two modalities,
often over-relying on the stronger ID modality. This insight suggests that
naive fusion might not be as effective in combining ID and text as previously
expected. To address this, we propose a novel alternative training strategy
AlterRec. It separates the training of ID and text, thereby avoiding the
imbalance issue seen in naive fusion. Additionally, AlterRec designs a novel
strategy to facilitate the interaction between the two modalities, enabling
them to mutually learn from each other and integrate the text more effectively.
Comprehensive experiments demonstrate the effectiveness of AlterRec in
session-based recommendation. The implementation is available at
https://github.com/Juanhui28/AlterRec.
\\ ( https://arxiv.org/abs/2402.08921 ,  868kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08960 (*cross-listing*)
Date: Wed, 14 Feb 2024 06:01:44 GMT   (20469kb,D)

Title: Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision
Authors: Zhaoqing Wang, Xiaobo Xia, Ziye Chen, Xiao He, Yandong Guo, Mingming
  Gong, Tongliang Liu
Categories: cs.CV cs.AI
Comments: 23 pages, 17 figures, 5 tables
\\
  Contemporary cutting-edge open-vocabulary segmentation approaches commonly
rely on image-mask-text triplets, yet this restricted annotation is
labour-intensive and encounters scalability hurdles in complex real-world
scenarios. Although some methods are proposed to reduce the annotation cost
with only text supervision, the incompleteness of supervision severely limits
the versatility and performance. In this paper, we liberate the strict
correspondence between masks and texts by using independent image-mask and
image-text pairs, which can be easily collected respectively. With this
unpaired mask-text supervision, we propose a new weakly-supervised
open-vocabulary segmentation framework (Uni-OVSeg) that leverages confident
pairs of mask predictions and entities in text descriptions. Using the
independent image-mask and image-text pairs, we predict a set of binary masks
and associate them with entities by resorting to the CLIP embedding space.
However, the inherent noise in the correspondence between masks and entities
poses a significant challenge when obtaining reliable pairs. In light of this,
we advocate using the large vision-language model (LVLM) to refine text
descriptions and devise a multi-scale ensemble to stablise the matching between
masks and entities. Compared to text-only weakly-supervised methods, our
Uni-OVSeg achieves substantial improvements of 15.5% mIoU on the ADE20K
datasets, and even surpasses fully-supervised methods on the challenging PASCAL
Context-459 dataset.
\\ ( https://arxiv.org/abs/2402.08960 ,  20469kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08979 (*cross-listing*)
Date: Wed, 14 Feb 2024 06:49:23 GMT   (1318kb)

Title: Learning-enabled Flexible Job-shop Scheduling for Scalable Smart
  Manufacturing
Authors: Sihoon Moon, Sanghoon Lee, and Kyung-Joon Park
Categories: eess.SY cs.AI cs.LG cs.SY
\\
  In smart manufacturing systems (SMSs), flexible job-shop scheduling with
transportation constraints (FJSPT) is essential to optimize solutions for
maximizing productivity, considering production flexibility based on automated
guided vehicles (AGVs). Recent developments in deep reinforcement learning
(DRL)-based methods for FJSPT have encountered a scale generalization
challenge. These methods underperform when applied to environment at scales
different from their training set, resulting in low-quality solutions. To
address this, we introduce a novel graph-based DRL method, named the
Heterogeneous Graph Scheduler (HGS). Our method leverages locally extracted
relational knowledge among operations, machines, and vehicle nodes for
scheduling, with a graph-structured decision-making framework that reduces
encoding complexity and enhances scale generalization. Our performance
evaluation, conducted with benchmark datasets, reveals that the proposed method
outperforms traditional dispatching rules, meta-heuristics, and existing
DRL-based approaches in terms of makespan performance, even on large-scale
instances that have not been experienced during training.
\\ ( https://arxiv.org/abs/2402.08979 ,  1318kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08983 (*cross-listing*)
Date: Wed, 14 Feb 2024 06:54:31 GMT   (9606kb,D)

Title: SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware
  Decoding
Authors: Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill Yuchen Lin,
  Radha Poovendran
Categories: cs.CR cs.AI
\\
  As large language models (LLMs) become increasingly integrated into
real-world applications such as code generation and chatbot assistance,
extensive efforts have been made to align LLM behavior with human values,
including safety. Jailbreak attacks, aiming to provoke unintended and unsafe
behaviors from LLMs, remain a significant/leading LLM safety threat. In this
paper, we aim to defend LLMs against jailbreak attacks by introducing
SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and
harmless responses to user queries. Our insight in developing SafeDecoding is
based on the observation that, even though probabilities of tokens representing
harmful contents outweigh those representing harmless responses, safety
disclaimers still appear among the top tokens after sorting tokens by
probability in descending order. This allows us to mitigate jailbreak attacks
by identifying safety disclaimers and amplifying their token probabilities,
while simultaneously attenuating the probabilities of token sequences that are
aligned with the objectives of jailbreak attacks. We perform extensive
experiments on five LLMs using six state-of-the-art jailbreak attacks and four
benchmark datasets. Our results show that SafeDecoding significantly reduces
the attack success rate and harmfulness of jailbreak attacks without
compromising the helpfulness of responses to benign user queries. SafeDecoding
outperforms six defense methods.
\\ ( https://arxiv.org/abs/2402.08983 ,  9606kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08994 (*cross-listing*)
Date: Wed, 14 Feb 2024 07:41:48 GMT   (5669kb,D)

Title: CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic
  Decoding
Authors: Qiongyi Zhou, Changde Du, Shengpei Wang, Huiguang He
Categories: cs.CV cs.AI
Comments: Accepted by ICLR2024
\\
  The study of decoding visual neural information faces challenges in
generalizing single-subject decoding models to multiple subjects, due to
individual differences. Moreover, the limited availability of data from a
single subject has a constraining impact on model performance. Although prior
multi-subject decoding methods have made significant progress, they still
suffer from several limitations, including difficulty in extracting global
neural response features, linear scaling of model parameters with the number of
subjects, and inadequate characterization of the relationship between neural
responses of different subjects to various stimuli. To overcome these
limitations, we propose a CLIP-guided Multi-sUbject visual neural information
SEmantic Decoding (CLIP-MUSED) method. Our method consists of a
Transformer-based feature extractor to effectively model global neural
representations. It also incorporates learnable subject-specific tokens that
facilitates the aggregation of multi-subject data without a linear increase of
parameters. Additionally, we employ representational similarity analysis (RSA)
to guide token representation learning based on the topological relationship of
visual stimuli in the representation space of CLIP, enabling full
characterization of the relationship between neural responses of different
subjects under different stimuli. Finally, token representations are used for
multi-subject semantic decoding. Our proposed method outperforms single-subject
decoding methods and achieves state-of-the-art performance among the existing
multi-subject methods on two fMRI datasets. Visualization results provide
insights into the effectiveness of our proposed method. Code is available at
https://github.com/CLIP-MUSED/CLIP-MUSED.
\\ ( https://arxiv.org/abs/2402.08994 ,  5669kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08995 (*cross-listing*)
Date: Wed, 14 Feb 2024 07:48:16 GMT   (11314kb,D)

Title: AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous
  Systems
Authors: Jiaying Lu and Bo Pan and Jieyi Chen and Yingchaojie Feng and Jingyuan
  Hu and Yuchen Peng and Wei Chen
Categories: cs.HC cs.AI
\\
  Recently, Large Language Model based Autonomous system(LLMAS) has gained
great popularity for its potential to simulate complicated behaviors of human
societies. One of its main challenges is to present and analyze the dynamic
events evolution of LLMAS. In this work, we present a visualization approach to
explore detailed statuses and agents' behavior within LLMAS. We propose a
general pipeline that establishes a behavior structure from raw LLMAS execution
events, leverages a behavior summarization algorithm to construct a
hierarchical summary of the entire structure in terms of time sequence, and a
cause trace method to mine the causal relationship between agent behaviors. We
then develop AgentLens, a visual analysis system that leverages a hierarchical
temporal visualization for illustrating the evolution of LLMAS, and supports
users to interactively investigate details and causes of agents' behaviors. Two
usage scenarios and a user study demonstrate the effectiveness and usability of
our AgentLens.
\\ ( https://arxiv.org/abs/2402.08995 ,  11314kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09023 (*cross-listing*)
Date: Wed, 14 Feb 2024 08:56:41 GMT   (2116kb,D)

Title: Review-Incorporated Model-Agnostic Profile Injection Attacks on
  Recommender Systems
Authors: Shiyi Yang, Lina Yao, Chen Wang, Xiwei Xu, Liming Zhu
Categories: cs.CR cs.AI
Comments: Accepted by ICDM 2023
\\
  Recent studies have shown that recommender systems (RSs) are highly
vulnerable to data poisoning attacks. Understanding attack tactics helps
improve the robustness of RSs. We intend to develop efficient attack methods
that use limited resources to generate high-quality fake user profiles to
achieve 1) transferability among black-box RSs 2) and imperceptibility among
detectors. In order to achieve these goals, we introduce textual reviews of
products to enhance the generation quality of the profiles. Specifically, we
propose a novel attack framework named R-Trojan, which formulates the attack
objectives as an optimization problem and adopts a tailored transformer-based
generative adversarial network (GAN) to solve it so that high-quality attack
profiles can be produced. Comprehensive experiments on real-world datasets
demonstrate that R-Trojan greatly outperforms state-of-the-art attack methods
on various victim RSs under black-box settings and show its good
imperceptibility.
\\ ( https://arxiv.org/abs/2402.09023 ,  2116kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09055 (*cross-listing*)
Date: Wed, 14 Feb 2024 10:05:19 GMT   (16952kb,D)

Title: Comment-aided Video-Language Alignment via Contrastive Pre-training for
  Short-form Video Humor Detection
Authors: Yang Liu, Tongfei Shen, Dong Zhang, Qingying Sun, Shoushan Li, Guodong
  Zhou
Categories: cs.CV cs.AI
Comments: Work in progress
\\
  The growing importance of multi-modal humor detection within affective
computing correlates with the expanding influence of short-form video sharing
on social media platforms. In this paper, we propose a novel two-branch
hierarchical model for short-form video humor detection (SVHD), named
Comment-aided Video-Language Alignment (CVLA) via data-augmented multi-modal
contrastive pre-training. Notably, our CVLA not only operates on raw signals
across various modal channels but also yields an appropriate multi-modal
representation by aligning the video and language components within a
consistent semantic space. The experimental results on two humor detection
datasets, including DY11k and UR-FUNNY, demonstrate that CVLA dramatically
outperforms state-of-the-art and several competitive baseline approaches. Our
dataset, code and model release at https://github.com/yliu-cs/CVLA.
\\ ( https://arxiv.org/abs/2402.09055 ,  16952kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09066 (*cross-listing*)
Date: Wed, 14 Feb 2024 10:24:04 GMT   (491kb,D)

Title: Solid Waste Detection in Remote Sensing Images: A Survey
Authors: Piero Fraternali, Luca Morandini and Sergio Luis Herrera Gonz\'alez
Categories: cs.CV cs.AI cs.LG
\\
  The detection and characterization of illegal solid waste disposal sites are
essential for environmental protection, particularly for mitigating pollution
and health hazards. Improperly managed landfills contaminate soil and
groundwater via rainwater infiltration, posing threats to both animals and
humans. Traditional landfill identification approaches, such as on-site
inspections, are time-consuming and expensive. Remote sensing is a
cost-effective solution for the identification and monitoring of solid waste
disposal sites that enables broad coverage and repeated acquisitions over time.
Earth Observation (EO) satellites, equipped with an array of sensors and
imaging capabilities, have been providing high-resolution data for several
decades. Researchers proposed specialized techniques that leverage remote
sensing imagery to perform a range of tasks such as waste site detection,
dumping site monitoring, and assessment of suitable locations for new
landfills. This review aims to provide a detailed illustration of the most
relevant proposals for the detection and monitoring of solid waste sites by
describing and comparing the approaches, the implemented techniques, and the
employed data. Furthermore, since the data sources are of the utmost importance
for developing an effective solid waste detection model, a comprehensive
overview of the satellites and publicly available data sets is presented.
Finally, this paper identifies the open issues in the state-of-the-art and
discusses the relevant research directions for reducing the costs and improving
the effectiveness of novel solid waste detection methods.
\\ ( https://arxiv.org/abs/2402.09066 ,  491kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09091 (*cross-listing*)
Date: Wed, 14 Feb 2024 11:11:51 GMT   (8162kb,D)

Title: Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit
  Clues
Authors: Zhiyuan Chang, Mingyang Li, Yi Liu, Junjie Wang, Qing Wang, Yang Liu
Categories: cs.CR cs.AI cs.HC
Comments: 13 pages, 6 figures
\\
  With the development of LLMs, the security threats of LLMs are getting more
and more attention. Numerous jailbreak attacks have been proposed to assess the
security defense of LLMs. Current jailbreak attacks primarily utilize scenario
camouflage techniques. However their explicitly mention of malicious intent
will be easily recognized and defended by LLMs. In this paper, we propose an
indirect jailbreak attack approach, Puzzler, which can bypass the LLM's defense
strategy and obtain malicious response by implicitly providing LLMs with some
clues about the original malicious query. In addition, inspired by the wisdom
of ''When unable to attack, defend'' from Sun Tzu's Art of War, we adopt a
defensive stance to gather clues about the original malicious query through
LLMs. Extensive experimental results show that Puzzler achieves a query success
rate of 96.6% on closed-source LLMs, which is 57.9%-82.7% higher than
baselines. Furthermore, when tested against the state-of-the-art jailbreak
detection approaches, Puzzler proves to be more effective at evading detection
compared to baselines.
\\ ( https://arxiv.org/abs/2402.09091 ,  8162kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09097 (*cross-listing*)
Date: Wed, 14 Feb 2024 11:17:14 GMT   (7869kb,D)

Title: A Digital Twin prototype for traffic sign recognition of a
  learning-enabled autonomous vehicle
Authors: Mohamed AbdElSalam, Loai Ali, Saddek Bensalem, Weicheng He, Panagiotis
  Katsaros, Nikolaos Kekatos, Doron Peled, Anastasios Temperekidis, Changshun
  Wu
Categories: cs.RO cs.AI cs.SY eess.SY
\\
  In this paper, we present a novel digital twin prototype for a
learning-enabled self-driving vehicle. The primary objective of this digital
twin is to perform traffic sign recognition and lane keeping. The digital twin
architecture relies on co-simulation and uses the Functional Mock-up Interface
and SystemC Transaction Level Modeling standards. The digital twin consists of
four clients, i) a vehicle model that is designed in Amesim tool, ii) an
environment model developed in Prescan, iii) a lane-keeping controller designed
in Robot Operating System, and iv) a perception and speed control module
developed in the formal modeling language of BIP (Behavior, Interaction,
Priority). These clients interface with the digital twin platform,
PAVE360-Veloce System Interconnect (PAVE360-VSI). PAVE360-VSI acts as the
co-simulation orchestrator and is responsible for synchronization,
interconnection, and data exchange through a server. The server establishes
connections among the different clients and also ensures adherence to the
Ethernet protocol. We conclude with illustrative digital twin simulations and
recommendations for future work.
\\ ( https://arxiv.org/abs/2402.09097 ,  7869kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09109 (*cross-listing*)
Date: Wed, 14 Feb 2024 11:47:19 GMT   (784kb,D)

Title: Stochastic Spiking Attention: Accelerating Attention with Stochastic
  Computing in Spiking Networks
Authors: Zihang Song, Prabodh Katti, Osvaldo Simeone, Bipin Rajendran
Categories: cs.AR cs.AI cs.LG cs.NE eess.SP
\\
  Spiking Neural Networks (SNNs) have been recently integrated into Transformer
architectures due to their potential to reduce computational demands and to
improve power efficiency. Yet, the implementation of the attention mechanism
using spiking signals on general-purpose computing platforms remains
inefficient. In this paper, we propose a novel framework leveraging stochastic
computing (SC) to effectively execute the dot-product attention for SNN-based
Transformers. We demonstrate that our approach can achieve high classification
accuracy ($83.53\%$) on CIFAR-10 within 10 time steps, which is comparable to
the performance of a baseline artificial neural network implementation
($83.66\%$). We estimate that the proposed SC approach can lead to over
$6.3\times$ reduction in computing energy and $1.7\times$ reduction in memory
access costs for a digital CMOS-based ASIC design. We experimentally validate
our stochastic attention block design through an FPGA implementation, which is
shown to achieve $48\times$ lower latency as compared to a GPU implementation,
while consuming $15\times$ less power.
\\ ( https://arxiv.org/abs/2402.09109 ,  784kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09126 (*cross-listing*)
Date: Wed, 14 Feb 2024 12:24:21 GMT   (581kb,D)

Title: MPIrigen: MPI Code Generation through Domain-Specific Language Models
Authors: Nadav Schneider, Niranjan Hasabnis, Vy A. Vo, Tal Kadosh, Neva Krien,
  Mihai Capot\u{a}, Abdul Wasay, Guy Tamir, Ted Willke, Nesreen Ahmed, Yuval
  Pinter, Timothy Mattson, Gal Oren
Categories: cs.DC cs.AI cs.CL cs.LG cs.SE
\\
  The imperative need to scale computation across numerous nodes highlights the
significance of efficient parallel computing, particularly in the realm of
Message Passing Interface (MPI) integration. The challenging parallel
programming task of generating MPI-based parallel programs has remained
unexplored. This study first investigates the performance of state-of-the-art
language models in generating MPI-based parallel programs. Findings reveal that
widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual
code models) exhibit notable performance degradation, when generating MPI-based
programs compared to general-purpose programs. In contrast, domain-specific
models such as MonoCoder, which are pretrained on MPI-related programming
languages of C and C++, outperform larger models. Subsequently, we introduce a
dedicated downstream task of MPI-based program generation by fine-tuning
MonoCoder on HPCorpusMPI. We call the resulting model as MPIrigen. We propose
an innovative preprocessing for completion only after observing the whole code,
thus enabling better completion with a wider context. Comparative analysis
against GPT-3.5 zero-shot performance, using a novel HPC-oriented evaluation
method, demonstrates that MPIrigen excels in generating accurate MPI functions
up to 0.8 accuracy in location and function predictions, and with more than 0.9
accuracy for argument predictions. The success of this tailored solution
underscores the importance of domain-specific fine-tuning in optimizing
language models for parallel computing code generation, paving the way for a
new generation of automatic parallelization tools. The sources of this work are
available at our GitHub MPIrigen repository:
https://github.com/Scientific-Computing-Lab-NRCN/MPI-rigen
\\ ( https://arxiv.org/abs/2402.09126 ,  581kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09129 (*cross-listing*)
Date: Wed, 14 Feb 2024 12:27:54 GMT   (3553kb,D)

Title: Optimal Automated Market Makers: Differentiable Economics and Strong
  Duality
Authors: Michael J. Curry, Zhou Fan, David C. Parkes
Categories: cs.GT cs.AI econ.TH q-fin.TR
\\
  The role of a market maker is to simultaneously offer to buy and sell
quantities of goods, often a financial asset such as a share, at specified
prices. An automated market maker (AMM) is a mechanism that offers to trade
according to some predetermined schedule; the best choice of this schedule
depends on the market maker's goals. The literature on the design of AMMs has
mainly focused on prediction markets with the goal of information elicitation.
More recent work motivated by DeFi has focused instead on the goal of profit
maximization, but considering only a single type of good (traded with a
numeraire), including under adverse selection (Milionis et al. 2022). Optimal
market making in the presence of multiple goods, including the possibility of
complex bundling behavior, is not well understood. In this paper, we show that
finding an optimal market maker is dual to an optimal transport problem, with
specific geometric constraints on the transport plan in the dual. We show that
optimal mechanisms for multiple goods and under adverse selection can take
advantage of bundling, both improved prices for bundled purchases and sales as
well as sometimes accepting payment "in kind." We present conjectures of
optimal mechanisms in additional settings which show further complex behavior.
From a methodological perspective, we make essential use of the tools of
differentiable economics to generate conjectures of optimal mechanisms, and
give a proof-of-concept for the use of such tools in guiding theoretical
investigations.
\\ ( https://arxiv.org/abs/2402.09129 ,  3553kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09200 (*cross-listing*)
Date: Wed, 14 Feb 2024 14:33:17 GMT   (946kb,D)

Title: Discovering Command and Control (C2) Channels on Tor and Public Networks
  Using Reinforcement Learning
Authors: Cheng Wang, Christopher Redino, Abdul Rahman, Ryan Clark, Daniel
  Radke, Tyler Cody, Dhruv Nandakumar, Edward Bowen
Categories: cs.CR cs.AI
\\
  Command and control (C2) channels are an essential component of many types of
cyber attacks, as they enable attackers to remotely control their
malware-infected machines and execute harmful actions, such as propagating
malicious code across networks, exfiltrating confidential data, or initiating
distributed denial of service (DDoS) attacks. Identifying these C2 channels is
therefore crucial in helping to mitigate and prevent cyber attacks. However,
identifying C2 channels typically involves a manual process, requiring deep
knowledge and expertise in cyber operations. In this paper, we propose a
reinforcement learning (RL) based approach to automatically emulate C2 attack
campaigns using both the normal (public) and the Tor networks. In addition,
payload size and network firewalls are configured to simulate real-world attack
scenarios. Results on a typical network configuration show that the RL agent
can automatically discover resilient C2 attack paths utilizing both Tor-based
and conventional communication channels, while also bypassing network
firewalls.
\\ ( https://arxiv.org/abs/2402.09200 ,  946kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09211 (*cross-listing*)
Date: Wed, 14 Feb 2024 14:46:03 GMT   (12867kb,D)

Title: DivaTrack: Diverse Bodies and Motions from Acceleration-Enhanced
  Three-Point Trackers
Authors: Dongseok Yang, Jiho Kang, Lingni Ma, Joseph Greer, Yuting Ye and
  Sung-Hee Lee
Categories: cs.CV cs.AI
Comments: accepted to Eurographics 2024
\\
  Full-body avatar presence is crucial for immersive social and environmental
interactions in digital reality. However, current devices only provide three
six degrees of freedom (DOF) poses from the headset and two controllers (i.e.
three-point trackers). Because it is a highly under-constrained problem,
inferring full-body pose from these inputs is challenging, especially when
supporting the full range of body proportions and use cases represented by the
general population. In this paper, we propose a deep learning framework,
DivaTrack, which outperforms existing methods when applied to diverse body
sizes and activities. We augment the sparse three-point inputs with linear
accelerations from Inertial Measurement Units (IMU) to improve foot contact
prediction. We then condition the otherwise ambiguous lower-body pose with the
predictions of foot contact and upper-body pose in a two-stage model. We
further stabilize the inferred full-body pose in a wide range of configurations
by learning to blend predictions that are computed in two reference frames,
each of which is designed for different types of motions. We demonstrate the
effectiveness of our design on a large dataset that captures 22 subjects
performing challenging locomotion for three-point tracking, including lunges,
hula-hooping, and sitting. As shown in a live demo using the Meta VR headset
and Xsens IMUs, our method runs in real-time while accurately tracking a user's
motion when they perform a diverse set of movements.
\\ ( https://arxiv.org/abs/2402.09211 ,  12867kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09225 (*cross-listing*)
Date: Wed, 14 Feb 2024 15:09:01 GMT   (360kb,D)

Title: Is my Data in your AI Model? Membership Inference Test with Application
  to Face Images
Authors: Daniel DeAlcala, Aythami Morales, Gonzalo Mancera, Julian Fierrez,
  Ruben Tolosana, Javier Ortega-Garcia
Categories: cs.CV cs.AI
Comments: 10 pages
\\
  This paper introduces the Membership Inference Test (MINT), a novel approach
that aims to empirically assess if specific data was used during the training
of Artificial Intelligence (AI) models. Specifically, we propose two novel MINT
architectures designed to learn the distinct activation patterns that emerge
when an audited model is exposed to data used during its training process. The
first architecture is based on a Multilayer Perceptron (MLP) network and the
second one is based on Convolutional Neural Networks (CNNs). The proposed MINT
architectures are evaluated on a challenging face recognition task, considering
three state-of-the-art face recognition models. Experiments are carried out
using six publicly available databases, comprising over 22 million face images
in total. Also, different experimental scenarios are considered depending on
the context available of the AI model to test. Promising results, up to 90%
accuracy, are achieved using our proposed MINT approach, suggesting that it is
possible to recognize if an AI model has been trained with specific data.
\\ ( https://arxiv.org/abs/2402.09225 ,  360kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09233 (*cross-listing*)
Date: Wed, 14 Feb 2024 15:22:24 GMT   (1958kb,D)

Title: Design and Realization of a Benchmarking Testbed for Evaluating
  Autonomous Platooning Algorithms
Authors: Michael Shaham, Risha Ranjan, Engin Kirda, Taskin Padir
Categories: cs.RO cs.AI cs.MA cs.SY eess.SY math.OC
Comments: To be published in International Symposium on Experimental Robotics,
  2023
\\
  Autonomous vehicle platoons present near- and long-term opportunities to
enhance operational efficiencies and save lives. The past 30 years have seen
rapid development in the autonomous driving space, enabling new technologies
that will alleviate the strain placed on human drivers and reduce vehicle
emissions. This paper introduces a testbed for evaluating and benchmarking
platooning algorithms on 1/10th scale vehicles with onboard sensors. To
demonstrate the testbed's utility, we evaluate three algorithms, linear
feedback and two variations of distributed model predictive control, and
compare their results on a typical platooning scenario where the lead vehicle
tracks a reference trajectory that changes speed multiple times. We validate
our algorithms in simulation to analyze the performance as the platoon size
increases, and find that the distributed model predictive control algorithms
outperform linear feedback on hardware and in simulation.
\\ ( https://arxiv.org/abs/2402.09233 ,  1958kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09246 (*cross-listing*)
Date: Wed, 14 Feb 2024 15:34:38 GMT   (25390kb,D)

Title: Who Plays First? Optimizing the Order of Play in Stackelberg Games with
  Many Robots
Authors: Haimin Hu, Gabriele Dragotto, Zixu Zhang, Kaiqu Liang, Bartolomeo
  Stellato, Jaime F. Fisac
Categories: cs.RO cs.AI cs.SY eess.SY math.OC
\\
  We consider the multi-agent spatial navigation problem of computing the
socially optimal order of play, i.e., the sequence in which the agents commit
to their decisions, and its associated equilibrium in an N-player Stackelberg
trajectory game. We model this problem as a mixed-integer optimization problem
over the space of all possible Stackelberg games associated with the order of
play's permutations. To solve the problem, we introduce Branch and Play (B&P),
an efficient and exact algorithm that provably converges to a socially optimal
order of play and its Stackelberg equilibrium. As a subroutine for B&P, we
employ and extend sequential trajectory planning, i.e., a popular multi-agent
control approach, to scalably compute valid local Stackelberg equilibria for
any given order of play. We demonstrate the practical utility of B&P to
coordinate air traffic control, swarm formation, and delivery vehicle fleets.
We find that B&P consistently outperforms various baselines, and computes the
socially optimal equilibrium.
\\ ( https://arxiv.org/abs/2402.09246 ,  25390kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09251 (*cross-listing*)
Date: Wed, 14 Feb 2024 15:38:56 GMT   (3872kb)

Title: Universal Machine Learning Kohn-Sham Hamiltonian for Materials
Authors: Yang Zhong, Jihui Yang, Hongjun Xiang, and Xingao Gong
Categories: physics.comp-ph cond-mat.mtrl-sci cs.AI
Comments: 14 pages, 5 figures
\\
  While density functional theory (DFT) serves as a prevalent computational
approach in electronic structure calculations, its computational demands and
scalability limitations persist. Recently, leveraging neural networks to
parameterize the Kohn-Sham DFT Hamiltonian has emerged as a promising avenue
for accelerating electronic structure computations. Despite advancements,
challenges such as the necessity for computing extensive DFT training data to
explore new systems and the complexity of establishing accurate ML models for
multi-elemental materials still exist. Addressing these hurdles, this study
introduces a universal electronic Hamiltonian model trained on Hamiltonian
matrices obtained from first-principles DFT calculations of nearly all crystal
structures on the Materials Project. We demonstrate its generality in
predicting electronic structures across the whole periodic table, including
complex multi-elemental systems. By offering a reliable efficient framework for
computing electronic properties, this universal Hamiltonian model lays the
groundwork for advancements in diverse fields related to electronic structures.
\\ ( https://arxiv.org/abs/2402.09251 ,  3872kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09265 (*cross-listing*)
Date: Wed, 14 Feb 2024 15:51:55 GMT   (200kb,D)

Title: Computational Complexity of Preferred Subset Repairs on Data-Graphs
Authors: Nina Pardal and Santiago Cifuentes and Edwin Pin and Maria Vanina
  Martinez and Sergio Abriola
Categories: cs.DB cs.AI cs.LO
Comments: 16 pages, 2 figures, Appendix
MSC-class: 68P15, 68T27, 03B70, 68T37
\\
  The problem of repairing inconsistent knowledge bases has a long history
within the communities of database theory and knowledge representation and
reasoning, especially from the perspective of structured data. However, as the
data available in real-world domains becomes more complex and interconnected,
the need naturally arises for developing new types of repositories,
representation languages, and semantics, to allow for more suitable ways to
query and reason about it. Graph databases provide an effective way to
represent relationships among semi-structured data, and allow processing and
querying these connections efficiently. In this work, we focus on the problem
of computing prioritized repairs over graph databases with data values, using a
notion of consistency based on Reg-GXPath expressions as integrity constraints.
We present several preference criteria based on the standard subset repair
semantics, incorporating weights, multisets, and set-based priority levels. We
study the most common repairing tasks, showing that it is possible to maintain
the same computational complexity as in the case where no preference criterion
is available for exploitation. To complete the picture, we explore the
complexity of consistent query answering in this setting and obtain tight lower
and upper bounds for all the preference criteria introduced.
\\ ( https://arxiv.org/abs/2402.09265 ,  200kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09303 (*cross-listing*)
Date: Wed, 14 Feb 2024 16:47:20 GMT   (5199kb,D)

Title: Immediate generalisation in humans but a generalisation lag in deep
  neural networks$\unicode{x2014}$evidence for representational divergence?
Authors: Lukas S. Huber, Fred W. Mast and Felix A. Wichmann
Categories: cs.CV cs.AI cs.LG q-bio.NC
Comments: Under review at the ICLR 2024 Workshop on Representational Alignment
  (Re-Align)
\\
  Recent research has seen many behavioral comparisons between humans and deep
neural networks (DNNs) in the domain of image classification. Often, comparison
studies focus on the end-result of the learning process by measuring and
comparing the similarities in the representations of object categories once
they have been formed. However, the process of how these representations
emerge$\unicode{x2014}$that is, the behavioral changes and intermediate stages
observed during the acquisition$\unicode{x2014}$is less often directly and
empirically compared.
  Here we report a detailed investigation of how transferable representations
are acquired in human observers and various classic and state-of-the-art DNNs.
We develop a constrained supervised learning environment in which we align
learning-relevant parameters such as starting point, input modality, available
input data and the feedback provided. Across the whole learning process we
evaluate and compare how well learned representations can be generalized to
previously unseen test data.
  Our findings indicate that in terms of absolute classification performance
DNNs demonstrate a level of data efficiency comparable to$\unicode{x2014}$and
sometimes even exceeding that$\unicode{x2014}$of human learners, challenging
some prevailing assumptions in the field. However, comparisons across the
entire learning process reveal significant representational differences: while
DNNs' learning is characterized by a pronounced generalisation lag, humans
appear to immediately acquire generalizable representations without a
preliminary phase of learning training set-specific information that is only
later transferred to novel data.
\\ ( https://arxiv.org/abs/2402.09303 ,  5199kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09318 (*cross-listing*)
Date: Wed, 14 Feb 2024 17:13:36 GMT   (132kb,D)

Title: Leveraging Pre-Trained Autoencoders for Interpretable Prototype Learning
  of Music Audio
Authors: Pablo Alonso-Jim\'enez and Leonardo Pepino and Roser Batlle-Roca and
  Pablo Zinemanas and Dmitry Bogdanov and Xavier Serra and Mart\'in Rocamora
Categories: cs.SD cs.AI cs.MM eess.AS
\\
  We present PECMAE, an interpretable model for music audio classification
based on prototype learning. Our model is based on a previous method, APNet,
which jointly learns an autoencoder and a prototypical network. Instead, we
propose to decouple both training processes. This enables us to leverage
existing self-supervised autoencoders pre-trained on much larger data
(EnCodecMAE), providing representations with better generalization. APNet
allows prototypes' reconstruction to waveforms for interpretability relying on
the nearest training data samples. In contrast, we explore using a diffusion
decoder that allows reconstruction without such dependency. We evaluate our
method on datasets for music instrument classification (Medley-Solos-DB) and
genre recognition (GTZAN and a larger in-house dataset), the latter being a
more challenging task not addressed with prototypical networks before. We find
that the prototype-based models preserve most of the performance achieved with
the autoencoder embeddings, while the sonification of prototypes benefits
understanding the behavior of the classifier.
\\ ( https://arxiv.org/abs/2402.09318 ,  132kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09338 (*cross-listing*)
Date: Wed, 14 Feb 2024 17:42:24 GMT   (231kb,D)

Title: Neural Networks asymptotic behaviours suitable for the resolution of
  inverse problems
Authors: Luigi Del Debbio, Manuel Naviglio, Francesco Tarantelli
Categories: physics.comp-ph cs.AI hep-lat hep-th
\\
  In this paper, we perform a study on the effectiveness of Neural Network (NN)
techniques for deconvolution inverse problems. We consider NN's asymptotic
limits, corresponding to Gaussian Processes (GPs), where parameter
non-linearities are lost. Using these resulting GPs, we address the
deconvolution inverse problem in the case of a quantum harmonic oscillator
simulated through Monte Carlo techniques on a lattice. A scenario with a known
analytical solution. Our findings indicate that solving the deconvolution
inverse problem with a fully connected NN yields less performing results than
those obtained using the GPs derived from NN's asymptotic limits. Furthermore,
we observe the trained NN's accuracy approaching that of GPs with increasing
layer width. Notably, one of these GPs defies interpretation as a probabilistic
model, offering a novel perspective compared to established methods in the
literature. Additionally, the NNs, in their asymptotic limit, provide
cost-effective analytical solutions.
\\ ( https://arxiv.org/abs/2402.09338 ,  231kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09355 (*cross-listing*)
Date: Wed, 14 Feb 2024 17:59:47 GMT   (2885kb,D)

Title: Single-Reset Divide & Conquer Imitation Learning
Authors: Alexandre Chenu, Olivier Serris, Olivier Sigaud, Nicolas
  Perrin-Gilbert
Categories: cs.RO cs.AI
\\
  Demonstrations are commonly used to speed up the learning process of Deep
Reinforcement Learning algorithms. To cope with the difficulty of accessing
multiple demonstrations, some algorithms have been developed to learn from a
single demonstration. In particular, the Divide & Conquer Imitation Learning
algorithms leverage a sequential bias to learn a control policy for complex
robotic tasks using a single state-based demonstration. The latest version,
DCIL-II demonstrates remarkable sample efficiency. This novel method operates
within an extended Goal-Conditioned Reinforcement Learning framework, ensuring
compatibility between intermediate and subsequent goals extracted from the
demonstration. However, a fundamental limitation arises from the assumption
that the system can be reset to specific states along the demonstrated
trajectory, confining the application to simulated systems. In response, we
introduce an extension called Single-Reset DCIL (SR-DCIL), designed to overcome
this constraint by relying on a single initial state reset rather than
sequential resets. To address this more challenging setting, we integrate two
mechanisms inspired by the Learning from Demonstrations literature, including a
Demo-Buffer and Value Cloning, to guide the agent toward compatible success
states. In addition, we introduce Approximate Goal Switching to facilitate
training to reach goals distant from the reset state. Our paper makes several
contributions, highlighting the importance of the reset assumption in DCIL-II,
presenting the mechanisms of SR-DCIL variants and evaluating their performance
in challenging robotic tasks compared to DCIL-II. In summary, this work offers
insights into the significance of reset assumptions in the framework of DCIL
and proposes SR-DCIL, a first step toward a versatile algorithm capable of
learning control policies under a weaker reset assumption.
\\ ( https://arxiv.org/abs/2402.09355 ,  2885kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09368 (*cross-listing*)
Date: Wed, 14 Feb 2024 18:13:51 GMT   (22361kb,D)

Title: Magic-Me: Identity-Specific Video Customized Diffusion
Authors: Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui
  Yang, Zhen Dong, Kurt Keutzer, Jiashi Feng
Categories: cs.CV cs.AI
\\
  Creating content for a specific identity (ID) has shown significant interest
in the field of generative models. In the field of text-to-image generation
(T2I), subject-driven content generation has achieved great progress with the
ID in the images controllable. However, extending it to video generation is not
well explored. In this work, we propose a simple yet effective subject identity
controllable video generation framework, termed Video Custom Diffusion (VCD).
With a specified subject ID defined by a few images, VCD reinforces the
identity information extraction and injects frame-wise correlation at the
initialization stage for stable video outputs with identity preserved to a
large extent. To achieve this, we propose three novel components that are
essential for high-quality ID preservation: 1) an ID module trained with the
cropped identity by prompt-to-segmentation to disentangle the ID information
and the background noise for more accurate ID token learning; 2) a
text-to-video (T2V) VCD module with 3D Gaussian Noise Prior for better
inter-frame consistency and 3) video-to-video (V2V) Face VCD and Tiled VCD
modules to deblur the face and upscale the video for higher resolution.
  Despite its simplicity, we conducted extensive experiments to verify that VCD
is able to generate stable and high-quality videos with better ID over the
selected strong baselines. Besides, due to the transferability of the ID
module, VCD is also working well with finetuned text-to-image models available
publically, further improving its usability. The codes are available at
https://github.com/Zhen-Dong/Magic-Me.
\\ ( https://arxiv.org/abs/2402.09368 ,  22361kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09370 (*cross-listing*)
Date: Wed, 14 Feb 2024 18:17:45 GMT   (113kb,D)

Title: Pseudorandom Error-Correcting Codes
Authors: Miranda Christ, Sam Gunn
Categories: cs.CR cs.AI cs.LG
\\
  We construct pseudorandom error-correcting codes (or simply pseudorandom
codes), which are error-correcting codes with the property that any polynomial
number of codewords are pseudorandom to any computationally-bounded adversary.
Efficient decoding of corrupted codewords is possible with the help of a
decoding key.
  We build pseudorandom codes that are robust to substitution and deletion
errors, where pseudorandomness rests on standard cryptographic assumptions.
Specifically, pseudorandomness is based on either $2^{O(\sqrt{n})}$-hardness of
LPN, or polynomial hardness of LPN and the planted XOR problem at low density.
  As our primary application of pseudorandom codes, we present an undetectable
watermarking scheme for outputs of language models that is robust to cropping
and a constant rate of random substitutions and deletions. The watermark is
undetectable in the sense that any number of samples of watermarked text are
computationally indistinguishable from text output by the original model. This
is the first undetectable watermarking scheme that can tolerate a constant rate
of errors.
  Our second application is to steganography, where a secret message is hidden
in innocent-looking content. We present a constant-rate stateless steganography
scheme with robustness to a constant rate of substitutions. Ours is the first
stateless steganography scheme with provable steganographic security and any
robustness to errors.
\\ ( https://arxiv.org/abs/2402.09370 ,  113kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09372 (*cross-listing*)
Date: Wed, 14 Feb 2024 18:18:33 GMT   (5570kb,D)

Title: Deep Rib Fracture Instance Segmentation and Classification from CT on
  the RibFrac Challenge
Authors: Jiancheng Yang, Rui Shi, Liang Jin, Xiaoyang Huang, Kaiming Kuang,
  Donglai Wei, Shixuan Gu, Jianying Liu, Pengfei Liu, Zhizhong Chai, Yongjie
  Xiao, Hao Chen, Liming Xu, Bang Du, Xiangyi Yan, Hao Tang, Adam Alessio,
  Gregory Holste, Jiapeng Zhang, Xiaoming Wang, Jianye He, Lixuan Che,
  Hanspeter Pfister, Ming Li, Bingbing Ni
Categories: eess.IV cs.AI cs.CV
Comments: Challenge paper for MICCAI RibFrac Challenge
  (https://ribfrac.grand-challenge.org/)
\\
  Rib fractures are a common and potentially severe injury that can be
challenging and labor-intensive to detect in CT scans. While there have been
efforts to address this field, the lack of large-scale annotated datasets and
evaluation benchmarks has hindered the development and validation of deep
learning algorithms. To address this issue, the RibFrac Challenge was
introduced, providing a benchmark dataset of over 5,000 rib fractures from 660
CT scans, with voxel-level instance mask annotations and diagnosis labels for
four clinical categories (buckle, nondisplaced, displaced, or segmental). The
challenge includes two tracks: a detection (instance segmentation) track
evaluated by an FROC-style metric and a classification track evaluated by an
F1-style metric. During the MICCAI 2020 challenge period, 243 results were
evaluated, and seven teams were invited to participate in the challenge
summary. The analysis revealed that several top rib fracture detection
solutions achieved performance comparable or even better than human experts.
Nevertheless, the current rib fracture classification solutions are hardly
clinically applicable, which can be an interesting area in the future. As an
active benchmark and research resource, the data and online evaluation of the
RibFrac Challenge are available at the challenge website. As an independent
contribution, we have also extended our previous internal baseline by
incorporating recent advancements in large-scale pretrained networks and
point-based rib segmentation techniques. The resulting FracNet+ demonstrates
competitive performance in rib fracture detection, which lays a foundation for
further research and development in AI-assisted rib fracture detection and
diagnosis.
\\ ( https://arxiv.org/abs/2402.09372 ,  5570kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09384 (*cross-listing*)
Date: Wed, 14 Feb 2024 18:32:30 GMT   (51kb)

Title: Persuasion, Delegation, and Private Information in Algorithm-Assisted
  Decisions
Authors: Ruqing Xu
Categories: econ.TH cs.AI cs.CY cs.GT cs.HC
\\
  A principal designs an algorithm that generates a publicly observable
prediction of a binary state. She must decide whether to act directly based on
the prediction or to delegate the decision to an agent with private information
but potential misalignment. We study the optimal design of the prediction
algorithm and the delegation rule in such environments. Three key findings
emerge: (1) Delegation is optimal if and only if the principal would make the
same binary decision as the agent had she observed the agent's information. (2)
Providing the most informative algorithm may be suboptimal even if the
principal can act on the algorithm's prediction. Instead, the optimal algorithm
may provide more information about one state and restrict information about the
other. (3) Common restrictions on algorithms, such as keeping a
"human-in-the-loop" or requiring maximal prediction accuracy, strictly worsen
decision quality in the absence of perfectly aligned agents and state-revealing
signals. These findings predict the underperformance of human-machine
collaborations if no measures are taken to mitigate common preference
misalignment between algorithms and human decision-makers.
\\ ( https://arxiv.org/abs/2402.09384 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09392 (*cross-listing*)
Date: Wed, 14 Feb 2024 18:43:19 GMT   (18794kb,D)

Title: LL-GABR: Energy Efficient Live Video Streaming Using Reinforcement
  Learning
Authors: Adithya Raman, Bekir Turkkan and Tevfik Kosar
Categories: cs.MM cs.AI
Comments: 10 pages, 3 figures, 3 Tables
\\
  Over the recent years, research and development in adaptive bitrate (ABR)
algorithms for live video streaming have been successful in improving users'
quality of experience (QoE) by reducing latency to near real-time levels while
delivering higher bitrate videos with minimal rebuffering time. However, the
QoE models used by these ABR algorithms do not take into account that a large
portion of live video streaming clients use mobile devices where a higher
bitrate does not necessarily translate into higher perceived quality. Ignoring
perceived quality results in playing videos at higher bitrates without a
significant increase in perceptual video quality and becomes a burden for
battery-constrained mobile devices due to higher energy consumption. In this
paper, we propose LL-GABR, a deep reinforcement learning approach that models
the QoE using perceived video quality instead of bitrate and uses energy
consumption along with other metrics like latency, rebuffering events, and
smoothness. LL-GABR makes no assumptions about the underlying video,
environment, or network settings and can operate flexibly on different video
titles, each having a different bitrate encoding ladder without additional
re-training, unlike existing learning-based ABRs. Trace-driven experimental
results show that LL-GABR outperforms the state-of-the-art approaches by up to
44% in terms of perceptual QoE and a 73% increase in energy efficiency as a
result of reducing net energy consumption by 11%.
\\ ( https://arxiv.org/abs/2402.09392 ,  18794kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08830 (*cross-listing*)
Date: Tue, 13 Feb 2024 22:22:51 GMT   (71kb)

Title: Sequence graphs realizations and ambiguity in language models
Authors: Sammy Khalife, Yann Ponty, Laurent Bulteau
Categories: cs.DS cs.CC cs.CL
\\
  Several popular language models represent local contexts in an input text as
bags of words. Such representations are naturally encoded by a sequence graph
whose vertices are the distinct words occurring in x, with edges representing
the (ordered) co-occurrence of two words within a sliding window of size w.
However, this compressed representation is not generally bijective, and may
introduce some degree of ambiguity. Some sequence graphs may admit several
realizations as a sequence, while others may not admit any realization. In this
paper, we study the realizability and ambiguity of sequence graphs from a
combinatorial and computational point of view. We consider the existence and
enumeration of realizations of a sequence graph under multiple settings: window
size w, presence/absence of graph orientation, and presence/absence of weights
(multiplicities). When w = 2, we provide polynomial time algorithms for
realizability and enumeration in all cases except the undirected/weighted
setting, where we show the #P-hardness of enumeration. For a window of size at
least 3, we prove hardness of all variants, even when w is considered as a
constant, with the notable exception of the undirected/unweighted case for
which we propose an XP algorithms for both (realizability and enumeration)
problems, tight due to a corresponding W[1]-hardness result. We conclude with
an integer program formulation to solve the realizability problem, and with
dynamic programming to solve the enumeration problem. This work leaves open the
membership to NP for both problems, a non-trivial question due to the existence
of minimum realizations having exponential size on the instance encoding.
\\ ( https://arxiv.org/abs/2402.08830 ,  71kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08898 (*cross-listing*)
Date: Wed, 14 Feb 2024 02:11:04 GMT   (166kb,D)

Title: UniEnc-CASSNAT: An Encoder-only Non-autoregressive ASR for Speech SSL
  Models
Authors: Ruchao Fan, Natarajan Balaji Shanka, and Abeer Alwan
Categories: eess.AS cs.CL cs.SD
Comments: Published in IEEE Signal Processing Letters
DOI: 10.1109/LSP.2024.3365036
\\
  Non-autoregressive automatic speech recognition (NASR) models have gained
attention due to their parallelism and fast inference. The encoder-based NASR,
e.g. connectionist temporal classification (CTC), can be initialized from the
speech foundation models (SFM) but does not account for any dependencies among
intermediate tokens. The encoder-decoder-based NASR, like CTC alignment-based
single-step non-autoregressive transformer (CASS-NAT), can mitigate the
dependency problem but is not able to efficiently integrate SFM. Inspired by
the success of recent work of speech-text joint pre-training with a shared
transformer encoder, we propose a new encoder-based NASR, UniEnc-CASSNAT, to
combine the advantages of CTC and CASS-NAT. UniEnc-CASSNAT consists of only an
encoder as the major module, which can be the SFM. The encoder plays the role
of both the CASS-NAT encoder and decoder by two forward passes. The first pass
of the encoder accepts the speech signal as input, while the concatenation of
the speech signal and the token-level acoustic embedding is used as the input
for the second pass. Examined on the Librispeech 100h, MyST, and Aishell1
datasets, the proposed UniEnc-CASSNAT achieves state-of-the-art NASR results
and is better or comparable to CASS-NAT with only an encoder and hence, fewer
model parameters. Our codes are publicly available.
\\ ( https://arxiv.org/abs/2402.08898 ,  166kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08966 (*cross-listing*)
Date: Wed, 14 Feb 2024 06:20:48 GMT   (2374kb,D)

Title: Pretraining Vision-Language Model for Difference Visual Question
  Answering in Longitudinal Chest X-rays
Authors: Yeongjae Cho, Taehee Kim, Heejun Shin, Sungzoon Cho, Dongmyung Shin
Categories: cs.CV cs.CL
\\
  Difference visual question answering (diff-VQA) is a challenging task that
requires answering complex questions based on differences between a pair of
images. This task is particularly important in reading chest X-ray images
because radiologists often compare multiple images of the same patient taken at
different times to track disease progression and changes in its severity in
their clinical practice. However, previous works focused on designing specific
network architectures for the diff-VQA task, missing opportunities to enhance
the model's performance using a pretrained vision-language model (VLM). Here,
we introduce a novel VLM called PLURAL, which is pretrained on natural and
longitudinal chest X-ray data for the diff-VQA task. The model is developed
using a step-by-step approach, starting with being pretrained on natural images
and texts, followed by being trained using longitudinal chest X-ray data. The
longitudinal data consist of pairs of X-ray images, along with question-answer
sets and radiologist's reports that describe the changes in lung abnormalities
and diseases over time. Our experimental results show that the PLURAL model
outperforms state-of-the-art methods not only in diff-VQA for longitudinal
X-rays but also in conventional VQA for a single X-ray image. Through extensive
experiments, we demonstrate the effectiveness of the proposed VLM architecture
and pretraining method in improving the model's performance.
\\ ( https://arxiv.org/abs/2402.08966 ,  2374kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08687 (*cross-listing*)
Date: Fri, 26 Jan 2024 12:21:57 GMT   (661kb,D)

Title: Fuzzy clustering of circular time series based on a new dependence
  measure with applications to wind data
Authors: \'Angel L\'opez-Oriona, Ying Sun and Rosa M. Crujeiras
Categories: stat.AP cs.LG
Comments: arXiv admin note: text overlap with arXiv:2304.12249
\\
  Time series clustering is an essential machine learning task with
applications in many disciplines. While the majority of the methods focus on
time series taking values on the real line, very few works consider time series
defined on the unit circle, although the latter objects frequently arise in
many applications. In this paper, the problem of clustering circular time
series is addressed. To this aim, a distance between circular series is
introduced and used to construct a clustering procedure. The metric relies on a
new measure of serial dependence considering circular arcs, thus taking
advantage of the directional character inherent to the series range. Since the
dynamics of the series may vary over the time, we adopt a fuzzy approach, which
enables the procedure to locate each series into several clusters with
different membership degrees. The resulting clustering algorithm is able to
group series generated from similar stochastic processes, reaching accurate
results with series coming from a broad variety of models. An extensive
simulation study shows that the proposed method outperforms several alternative
techniques, besides being computationally efficient. Two interesting
applications involving time series of wind direction in Saudi Arabia highlight
the potential of the proposed approach.
\\ ( https://arxiv.org/abs/2402.08687 ,  661kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08688 (*cross-listing*)
Date: Tue, 6 Feb 2024 16:55:41 GMT   (2161kb,D)

Title: Context-Aware Automated Passenger Counting Data Denoising
Authors: No\"elie Cherrier, Baptiste R\'erolle, Martin Graive, Amir Dib,
  Eglantine Schmitt
Categories: stat.AP cs.LG
Comments: Accepted in this version to ITSC 2023
\\
  A reliable and accurate knowledge of the ridership in public transportation
networks is crucial for public transport operators and public authorities to be
aware of their network's use and optimize transport offering. Several
techniques to estimate ridership exist nowadays, some of them in an automated
manner. Among them, Automatic Passenger Counting (APC) systems detect
passengers entering and leaving the vehicle at each station of its course.
However, data resulting from these systems are often noisy or even biased,
resulting in under or overestimation of onboard occupancy. In this work, we
propose a denoising algorithm for APC data to improve their robustness and ease
their analyzes. The proposed approach consists in a constrained integer linear
optimization, taking advantage of ticketing data and historical ridership data
to further constrain and guide the optimization. The performances are assessed
and compared to other denoising methods on several public transportation
networks in France, to manual counts available on one of these networks, and on
simulated data.
\\ ( https://arxiv.org/abs/2402.08688 ,  2161kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08692 (*cross-listing*)
Date: Mon, 12 Feb 2024 12:50:10 GMT   (511kb,D)

Title: Inference Stage Denoising for Undersampled MRI Reconstruction
Authors: Yuyang Xue, Chen Qin, Sotirios A. Tsaftaris
Categories: eess.IV cs.CV cs.LG
Comments: This paper is accepted by ISBI 2024
\\
  Reconstruction of magnetic resonance imaging (MRI) data has been positively
affected by deep learning. A key challenge remains: to improve generalisation
to distribution shifts between the training and testing data. Most approaches
aim to address this via inductive design or data augmentation. However, they
can be affected by misleading data, e.g. random noise, and cases where the
inference stage data do not match assumptions in the modelled shifts. In this
work, by employing a conditional hyperparameter network, we eliminate the need
of augmentation, yet maintain robust performance under various levels of
Gaussian noise. We demonstrate that our model withstands various input noise
levels while producing high-definition reconstructions during the test stage.
Moreover, we present a hyperparameter sampling strategy that accelerates the
convergence of training. Our proposed method achieves the highest accuracy and
image quality in all settings compared to baseline methods.
\\ ( https://arxiv.org/abs/2402.08692 ,  511kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08695 (*cross-listing*)
Date: Mon, 12 Feb 2024 20:14:46 GMT   (6900kb,D)

Title: Game of Trojans: Adaptive Adversaries Against Output-based
  Trojaned-Model Detectors
Authors: Dinuka Sahabandu, Xiaojun Xu, Arezoo Rajabi, Luyao Niu, Bhaskar
  Ramasubramanian, Bo Li, Radha Poovendran
Categories: cs.CR cs.LG
\\
  We propose and analyze an adaptive adversary that can retrain a Trojaned DNN
and is also aware of SOTA output-based Trojaned model detectors. We show that
such an adversary can ensure (1) high accuracy on both trigger-embedded and
clean samples and (2) bypass detection. Our approach is based on an observation
that the high dimensionality of the DNN parameters provides sufficient degrees
of freedom to simultaneously achieve these objectives. We also enable SOTA
detectors to be adaptive by allowing retraining to recalibrate their
parameters, thus modeling a co-evolution of parameters of a Trojaned model and
detectors. We then show that this co-evolution can be modeled as an iterative
game, and prove that the resulting (optimal) solution of this interactive game
leads to the adversary successfully achieving the above objectives. In
addition, we provide a greedy algorithm for the adversary to select a minimum
number of input samples for embedding triggers. We show that for cross-entropy
or log-likelihood loss functions used by the DNNs, the greedy algorithm
provides provable guarantees on the needed number of trigger-embedded input
samples. Extensive experiments on four diverse datasets -- MNIST, CIFAR-10,
CIFAR-100, and SpeechCommand -- reveal that the adversary effectively evades
four SOTA output-based Trojaned model detectors: MNTD, NeuralCleanse, STRIP,
and TABOR.
\\ ( https://arxiv.org/abs/2402.08695 ,  6900kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08698 (*cross-listing*)
Date: Tue, 13 Feb 2024 02:43:41 GMT   (703kb,D)

Title: AMEND: A Mixture of Experts Framework for Long-tailed Trajectory
  Prediction
Authors: Ray Coden Mercurius, Ehsan Ahmadi, Soheil Mohamad Alizadeh Shabestary,
  Amir Rasouli
Categories: cs.CV cs.LG cs.RO
\\
  Accurate prediction of pedestrians' future motions is critical for
intelligent driving systems. Developing models for this task requires rich
datasets containing diverse sets of samples. However, the existing naturalistic
trajectory prediction datasets are generally imbalanced in favor of simpler
samples and lack challenging scenarios. Such a long-tail effect causes
prediction models to underperform on the tail portion of the data distribution
containing safety-critical scenarios. Previous methods tackle the long-tail
problem using methods such as contrastive learning and class-conditioned
hypernetworks. These approaches, however, are not modular and cannot be applied
to many machine learning architectures. In this work, we propose a modular
model-agnostic framework for trajectory prediction that leverages a specialized
mixture of experts. In our approach, each expert is trained with a specialized
skill with respect to a particular part of the data. To produce predictions, we
utilise a router network that selects the best expert by generating relative
confidence scores. We conduct experimentation on common pedestrian trajectory
prediction datasets and show that besides achieving state-of-the-art
performance, our method significantly performs better on long-tail scenarios.
We further conduct ablation studies to highlight the contribution of different
proposed components.
\\ ( https://arxiv.org/abs/2402.08698 ,  703kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08699 (*cross-listing*)
Date: Tue, 13 Feb 2024 11:08:08 GMT   (109kb,D)

Title: Unsupervised Evaluation of Code LLMs with Round-Trip Correctness
Authors: Miltiadis Allamanis, Sheena Panthaplackel, Pengcheng Yin
Categories: cs.SE cs.LG
\\
  To evaluate code large language models (LLMs), research has relied on a few
small manually curated benchmarks, such as HumanEval and MBPP, which represent
a narrow part of the real-world software domains. In this work, we introduce
round-trip correctness (RTC) as an alternative evaluation method. RTC allows
Code LLM evaluation on a broader spectrum of real-world software domains
without the need for costly human curation. RTC rests on the idea that we can
ask a model to make a prediction (e.g., describe some code using natural
language), feed that prediction back (e.g., synthesize code from the predicted
description), and check if this round-trip leads to code that is semantically
equivalent to the original input. We show how to employ RTC to evaluate code
synthesis and editing. We find that RTC strongly correlates with model
performance on existing narrow-domain code synthesis benchmarks while allowing
us to expand to a much broader set of domains and tasks which was not
previously possible without costly human annotations.
\\ ( https://arxiv.org/abs/2402.08699 ,  109kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08701 (*cross-listing*)
Date: Tue, 13 Feb 2024 13:02:11 GMT   (79kb,D)

Title: Primal-Dual Algorithms with Predictions for Online Bounded Allocation
  and Ad-Auctions Problems
Authors: Eniko Kevi and Nguyen Kim Thang
Categories: cs.DS cs.DM cs.GT cs.LG
\\
  Matching problems have been widely studied in the research community,
especially Ad-Auctions with many applications ranging from network design to
advertising. Following the various advancements in machine learning, one
natural question is whether classical algorithms can benefit from machine
learning and obtain better-quality solutions. Even a small percentage of
performance improvement in matching problems could result in significant gains
for the studied use cases. For example, the network throughput or the revenue
of Ad-Auctions can increase remarkably. This paper presents algorithms with
machine learning predictions for the Online Bounded Allocation and the Online
Ad-Auctions problems. We constructed primal-dual algorithms that achieve
competitive performance depending on the quality of the predictions. When the
predictions are accurate, the algorithms' performance surpasses previous
performance bounds, while when the predictions are misleading, the algorithms
maintain standard worst-case performance guarantees. We provide supporting
experiments on generated data for our theoretical findings.
\\ ( https://arxiv.org/abs/2402.08701 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08708 (*cross-listing*)
Date: Tue, 13 Feb 2024 17:53:44 GMT   (10163kb,D)

Title: Zero Shot Molecular Generation via Similarity Kernels
Authors: Rokas Elijo\v{s}ius, Fabian Zills, Ilyes Batatia, Sam Walton Norwood,
  D\'avid P\'eter Kov\'acs, Christian Holm and G\'abor Cs\'anyi
Categories: physics.chem-ph cs.LG
\\
  Generative modelling aims to accelerate the discovery of novel chemicals by
directly proposing structures with desirable properties. Recently, score-based,
or diffusion, generative models have significantly outperformed previous
approaches. Key to their success is the close relationship between the score
and physical force, allowing the use of powerful equivariant neural networks.
However, the behaviour of the learnt score is not yet well understood. Here, we
analyse the score by training an energy-based diffusion model for molecular
generation. We find that during the generation the score resembles a
restorative potential initially and a quantum-mechanical force at the end. In
between the two endpoints, it exhibits special properties that enable the
building of large molecules. Using insights from the trained model, we present
Similarity-based Molecular Generation (SiMGen), a new method for zero shot
molecular generation. SiMGen combines a time-dependent similarity kernel with
descriptors from a pretrained machine learning force field to generate
molecules without any further training. Our approach allows full control over
the molecular shape through point cloud priors and supports conditional
generation. We also release an interactive web tool that allows users to
generate structures with SiMGen online (https://zndraw.icp.uni-stuttgart.de).
\\ ( https://arxiv.org/abs/2402.08708 ,  10163kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08711 (*cross-listing*)
Date: Tue, 13 Feb 2024 18:31:55 GMT   (12kb)

Title: Correction to "Wasserstein distance estimates for the distributions of
  numerical approximations to ergodic stochastic differential equations"
Authors: Daniel Paulin, Peter A. Whalley
Categories: stat.ML cs.LG cs.NA math.NA math.PR
Comments: Comment on https://doi.org/10.1080/14685248.2020.1855352
\\
  A method for analyzing non-asymptotic guarantees of numerical discretizations
of ergodic SDEs in Wasserstein-2 distance is presented by Sanz-Serna and
Zygalakis in ``Wasserstein distance estimates for the distributions of
numerical approximations to ergodic stochastic differential equations". They
analyze the UBU integrator which is strong order two and only requires one
gradient evaluation per step, resulting in desirable non-asymptotic guarantees,
in particular $\mathcal{O}(d^{1/4}\epsilon^{-1/2})$ steps to reach a distance
of $\epsilon > 0$ in Wasserstein-2 distance away from the target distribution.
However, there is a mistake in the local error estimates in Sanz-Serna and
Zygalakis (2021), in particular, a stronger assumption is needed to achieve
these complexity estimates. This note reconciles the theory with the dimension
dependence observed in practice in many applications of interest.
\\ ( https://arxiv.org/abs/2402.08711 ,  12kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08726 (*cross-listing*)
Date: Tue, 13 Feb 2024 19:00:08 GMT   (4594kb,D)

Title: Trained quantum neural networks are Gaussian processes
Authors: Filippo Girardi, Giacomo De Palma
Categories: quant-ph cs.LG math-ph math.MP math.PR
Comments: 151 pages
\\
  We study quantum neural networks made by parametric one-qubit gates and fixed
two-qubit gates in the limit of infinite width, where the generated function is
the expectation value of the sum of single-qubit observables over all the
qubits. First, we prove that the probability distribution of the function
generated by the untrained network with randomly initialized parameters
converges in distribution to a Gaussian process whenever each measured qubit is
correlated only with few other measured qubits. Then, we analytically
characterize the training of the network via gradient descent with square loss
on supervised learning problems. We prove that, as long as the network is not
affected by barren plateaus, the trained network can perfectly fit the training
set and that the probability distribution of the function generated after
training still converges in distribution to a Gaussian process. Finally, we
consider the statistical noise of the measurement at the output of the network
and prove that a polynomial number of measurements is sufficient for all the
previous results to hold and that the network can always be trained in
polynomial time.
\\ ( https://arxiv.org/abs/2402.08726 ,  4594kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08742 (*cross-listing*)
Date: Tue, 13 Feb 2024 19:27:06 GMT   (839kb,D)

Title: Unveiling Hidden Energy Anomalies: Harnessing Deep Learning to Optimize
  Energy Management in Sports Facilities
Authors: Fodil Fadli, Yassine Himeur, Mariam Elnour and Abbes Amira
Categories: cs.CY cs.LG
Comments: 18 pages, 5 figures and 2 tables
\\
  Anomaly detection in sport facilities has gained significant attention due to
its potential to promote energy saving and optimizing operational efficiency.
In this research article, we investigate the role of machine learning,
particularly deep learning, in anomaly detection for sport facilities. We
explore the challenges and perspectives of utilizing deep learning methods for
this task, aiming to address the drawbacks and limitations of conventional
approaches. Our proposed approach involves feature extraction from the data
collected in sport facilities. We present a problem formulation using Deep
Feedforward Neural Networks (DFNN) and introduce threshold estimation
techniques to identify anomalies effectively. Furthermore, we propose methods
to reduce false alarms, ensuring the reliability and accuracy of anomaly
detection. To evaluate the effectiveness of our approach, we conduct
experiments on aquatic center dataset at Qatar University. The results
demonstrate the superiority of our deep learning-based method over conventional
techniques, highlighting its potential in real-world applications. Typically,
94.33% accuracy and 92.92% F1-score have been achieved using the proposed
scheme.
\\ ( https://arxiv.org/abs/2402.08742 ,  839kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08743 (*cross-listing*)
Date: Tue, 13 Feb 2024 19:27:34 GMT   (510kb,D)

Title: ADS: Approximate Densest Subgraph for Novel Image Discovery
Authors: Shanfeng Hu
Categories: cs.CV cs.LG eess.IV
\\
  The volume of image repositories continues to grow. Despite the availability
of content-based addressing, we still lack a lightweight tool that allows us to
discover images of distinct characteristics from a large collection. In this
paper, we propose a fast and training-free algorithm for novel image discovery.
The key of our algorithm is formulating a collection of images as a perceptual
distance-weighted graph, within which our task is to locate the K-densest
subgraph that corresponds to a subset of the most unique images. While solving
this problem is not just NP-hard but also requires a full computation of the
potentially huge distance matrix, we propose to relax it into a K-sparse
eigenvector problem that we can efficiently solve using stochastic gradient
descent (SGD) without explicitly computing the distance matrix. We compare our
algorithm against state-of-the-arts on both synthetic and real datasets,
showing that it is considerably faster to run with a smaller memory footprint
while able to mine novel images more accurately.
\\ ( https://arxiv.org/abs/2402.08743 ,  510kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08748 (*cross-listing*)
Date: Tue, 13 Feb 2024 19:33:41 GMT   (25kb)

Title: Nearest Neighbor Representations of Neurons
Authors: Kordag Mehmet Kilic, Jin Sima, and Jehoshua Bruck
Categories: cs.CC cs.DM cs.LG cs.NE
Comments: This paper is submitted to ISIT 2024
\\
  The Nearest Neighbor (NN) Representation is an emerging computational model
that is inspired by the brain. We study the complexity of representing a neuron
(threshold function) using the NN representations. It is known that two anchors
(the points to which NN is computed) are sufficient for a NN representation of
a threshold function, however, the resolution (the maximum number of bits
required for the entries of an anchor) is $O(n\log{n})$. In this work, the
trade-off between the number of anchors and the resolution of a NN
representation of threshold functions is investigated. We prove that the
well-known threshold functions EQUALITY, COMPARISON, and ODD-MAX-BIT, which
require 2 or 3 anchors and resolution of $O(n)$, can be represented by
polynomially large number of anchors in $n$ and $O(\log{n})$ resolution. We
conjecture that for all threshold functions, there are NN representations with
polynomially large size and logarithmic resolution in $n$.
\\ ( https://arxiv.org/abs/2402.08748 ,  25kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08749 (*cross-listing*)
Date: Tue, 13 Feb 2024 19:36:23 GMT   (925kb)

Title: Automated detection of motion artifacts in brain MR images using deep
  learning and explainable artificial intelligence
Authors: Marina Manso Jimeno, Keerthi Sravan Ravi, Maggie Fung, John Thomas
  Vaughan, Jr., Sairam Geethanath
Categories: cs.CV cs.LG
Comments: 25 pages, 9 figures, 1 table. Submitted to NMR in Biomedicine
\\
  Quality assessment, including inspecting the images for artifacts, is a
critical step during MRI data acquisition to ensure data quality and downstream
analysis or interpretation success. This study demonstrates a deep learning
model to detect rigid motion in T1-weighted brain images. We leveraged a 2D CNN
for three-class classification and tested it on publicly available
retrospective and prospective datasets. Grad-CAM heatmaps enabled the
identification of failure modes and provided an interpretation of the model's
results. The model achieved average precision and recall metrics of 85% and 80%
on six motion-simulated retrospective datasets. Additionally, the model's
classifications on the prospective dataset showed a strong inverse correlation
(-0.84) compared to average edge strength, an image quality metric indicative
of motion. This model is part of the ArtifactID tool, aimed at inline automatic
detection of Gibbs ringing, wrap-around, and motion artifacts. This tool
automates part of the time-consuming QA process and augments expertise on-site,
particularly relevant in low-resource settings where local MR knowledge is
scarce.
\\ ( https://arxiv.org/abs/2402.08749 ,  925kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08751 (*cross-listing*)
Date: Tue, 13 Feb 2024 19:38:01 GMT   (36kb)

Title: Nearest Neighbor Representations of Neural Circuits
Authors: Kordag Mehmet Kilic, Jin Sima, and Jehoshua Bruck
Categories: cs.CC cs.DM cs.LG cs.NE
Comments: This paper is submitted to ISIT 2024
\\
  Neural networks successfully capture the computational power of the human
brain for many tasks. Similarly inspired by the brain architecture, Nearest
Neighbor (NN) representations is a novel approach of computation. We establish
a firmer correspondence between NN representations and neural networks.
Although it was known how to represent a single neuron using NN
representations, there were no results even for small depth neural networks.
Specifically, for depth-2 threshold circuits, we provide explicit constructions
for their NN representation with an explicit bound on the number of bits to
represent it. Example functions include NN representations of convex polytopes
(AND of threshold gates), IP2, OR of threshold gates, and linear or exact
decision lists.
\\ ( https://arxiv.org/abs/2402.08751 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08753 (*cross-listing*)
Date: Tue, 13 Feb 2024 19:39:11 GMT   (421kb)

Title: Forecasting for Swap Regret for All Downstream Agents
Authors: Aaron Roth, Mirah Shi
Categories: cs.GT cs.LG
\\
  We study the problem of making predictions so that downstream agents who best
respond to them will be guaranteed diminishing swap regret, no matter what
their utility functions are. It has been known since Foster and Vohra (1997)
that agents who best-respond to calibrated forecasts have no swap regret.
Unfortunately, the best known algorithms for guaranteeing calibrated forecasts
in sequential adversarial environments do so at rates that degrade
exponentially with the dimension of the prediction space. In this work, we show
that by making predictions that are not calibrated, but are unbiased subject to
a carefully selected collection of events, we can guarantee arbitrary
downstream agents diminishing swap regret at rates that substantially improve
over the rates that result from calibrated forecasts -- while maintaining the
appealing property that our forecasts give guarantees for any downstream agent,
without our forecasting algorithm needing to know their utility function.
  We give separate results in the ``low'' (1 or 2) dimensional setting and the
``high'' ($> 2$) dimensional setting. In the low dimensional setting, we show
how to make predictions such that all agents who best respond to our
predictions have diminishing swap regret -- in 1 dimension, at the optimal
$O(\sqrt{T})$ rate. In the high dimensional setting we show how to make
forecasts that guarantee regret scaling at a rate of $O(T^{2/3})$ (crucially, a
dimension independent exponent), under the assumption that downstream agents
smoothly best respond. Our results stand in contrast to rates that derive from
agents who best respond to calibrated forecasts, which have an exponential
dependence on the dimension of the prediction space.
\\ ( https://arxiv.org/abs/2402.08753 ,  421kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08768 (*cross-listing*)
Date: Tue, 13 Feb 2024 20:02:34 GMT   (4127kb,D)

Title: Adversarially Robust Feature Learning for Breast Cancer Diagnosis
Authors: Degan Hao, Dooman Arefan, Margarita Zuley, Wendie Berg, Shandong Wu
Categories: eess.IV cs.LG
\\
  Adversarial data can lead to malfunction of deep learning applications. It is
essential to develop deep learning models that are robust to adversarial data
while accurate on standard, clean data. In this study, we proposed a novel
adversarially robust feature learning (ARFL) method for a real-world
application of breast cancer diagnosis. ARFL facilitates adversarial training
using both standard data and adversarial data, where a feature correlation
measure is incorporated as an objective function to encourage learning of
robust features and restrain spurious features. To show the effects of ARFL in
breast cancer diagnosis, we built and evaluated diagnosis models using two
independent clinically collected breast imaging datasets, comprising a total of
9,548 mammogram images. We performed extensive experiments showing that our
method outperformed several state-of-the-art methods and that our method can
enhance safer breast cancer diagnosis against adversarial attacks in clinical
settings.
\\ ( https://arxiv.org/abs/2402.08768 ,  4127kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08784 (*cross-listing*)
Date: Tue, 13 Feb 2024 20:46:37 GMT   (9880kb,D)

Title: Preconditioners for the Stochastic Training of Implicit Neural
  Representations
Authors: Shin-Fang Chng, Hemanth Saratchandran, Simon Lucey
Categories: cs.CV cs.LG
Comments: The first two authors contributed equally
\\
  Implicit neural representations have emerged as a powerful technique for
encoding complex continuous multidimensional signals as neural networks,
enabling a wide range of applications in computer vision, robotics, and
geometry. While Adam is commonly used for training due to its stochastic
proficiency, it entails lengthy training durations. To address this, we explore
alternative optimization techniques for accelerated training without
sacrificing accuracy. Traditional second-order optimizers like L-BFGS are
suboptimal in stochastic settings, making them unsuitable for large-scale data
sets. Instead, we propose stochastic training using curvature-aware diagonal
preconditioners, showcasing their effectiveness across various signal
modalities such as images, shape reconstruction, and Neural Radiance Fields
(NeRF).
\\ ( https://arxiv.org/abs/2402.08784 ,  9880kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08811 (*cross-listing*)
Date: Tue, 13 Feb 2024 21:30:44 GMT   (21kb)

Title: Deep and shallow data science for multi-scale optical neuroscience
Authors: Gal Mishne and Adam Charles
Categories: eess.IV cs.LG q-bio.QM
Comments: 6 pages
\\
  Optical imaging of the brain has expanded dramatically in the past two
decades. New optics, indicators, and experimental paradigms are now enabling
in-vivo imaging from the synaptic to the cortex-wide scales. To match the
resulting flood of data across scales, computational methods are continuously
being developed to meet the need of extracting biologically relevant
information. In this pursuit, challenges arise in some domains (e.g., SNR and
resolution limits in micron-scale data) that require specialized algorithms.
These algorithms can, for example, make use of state-of-the-art machine
learning to maximally learn the details of a given scale to optimize the
processing pipeline. In contrast, other methods, however, such as graph signal
processing, seek to abstract away from some of the details that are
scale-specific to provide solutions to specific sub-problems common across
scales of neuroimaging. Here we discuss limitations and tradeoffs in
algorithmic design with the goal of identifying how data quality and
variability can hamper algorithm use and dissemination.
\\ ( https://arxiv.org/abs/2402.08811 ,  21kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08813 (*cross-listing*)
Date: Tue, 13 Feb 2024 21:36:30 GMT   (616kb,D)

Title: Model approximation in MDPs with unbounded per-step cost
Authors: Berk Bozkurt, Aditya Mahajan, Ashutosh Nayyar, Yi Ouyang
Categories: math.OC cs.LG cs.SY eess.SY
\\
  We consider the problem of designing a control policy for an infinite-horizon
discounted cost Markov decision process $\mathcal{M}$ when we only have access
to an approximate model $\hat{\mathcal{M}}$. How well does an optimal policy
$\hat{\pi}^{\star}$ of the approximate model perform when used in the original
model $\mathcal{M}$? We answer this question by bounding a weighted norm of the
difference between the value function of $\hat{\pi}^\star $ when used in
$\mathcal{M}$ and the optimal value function of $\mathcal{M}$. We then extend
our results and obtain potentially tighter upper bounds by considering affine
transformations of the per-step cost. We further provide upper bounds that
explicitly depend on the weighted distance between cost functions and weighted
distance between transition kernels of the original and approximate models. We
present examples to illustrate our results.
\\ ( https://arxiv.org/abs/2402.08813 ,  616kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08818 (*cross-listing*)
Date: Tue, 13 Feb 2024 21:54:15 GMT   (3685kb,D)

Title: Corridor Geometry in Gradient-Based Optimization
Authors: Benoit Dherin and Mihaela Rosca
Categories: stat.ML cs.LG math.OC
\\
  We characterize regions of a loss surface as corridors when the continuous
curves of steepest descent -- the solutions of the gradient flow -- become
straight lines. We show that corridors provide insights into gradient-based
optimization, since corridors are exactly the regions where gradient descent
and the gradient flow follow the same trajectory, while the loss decreases
linearly. As a result, inside corridors there are no implicit regularization
effects or training instabilities that have been shown to occur due to the
drift between gradient descent and the gradient flow. Using the loss linear
decrease on corridors, we devise a learning rate adaptation scheme for gradient
descent; we call this scheme Corridor Learning Rate (CLR). The CLR formulation
coincides with a special case of Polyak step-size, discovered in the context of
convex optimization. The Polyak step-size has been shown recently to have also
good convergence properties for neural networks; we further confirm this here
with results on CIFAR-10 and ImageNet.
\\ ( https://arxiv.org/abs/2402.08818 ,  3685kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08823 (*cross-listing*)
Date: Tue, 13 Feb 2024 22:07:29 GMT   (835kb,D)

Title: RanDumb: A Simple Approach that Questions the Efficacy of Continual
  Representation Learning
Authors: Ameya Prabhu, Shiven Sinha, Ponnurangam Kumaraguru, Philip H.S. Torr,
  Ozan Sener, Puneet K. Dokania
Categories: cs.CV cs.LG
Comments: Tech Report
\\
  We propose RanDumb to examine the efficacy of continual representation
learning. RanDumb embeds raw pixels using a fixed random transform which
approximates an RBF-Kernel, initialized before seeing any data, and learns a
simple linear classifier on top. We present a surprising and consistent
finding: RanDumb significantly outperforms the continually learned
representations using deep networks across numerous continual learning
benchmarks, demonstrating the poor performance of representation learning in
these scenarios. RanDumb stores no exemplars and performs a single pass over
the data, processing one sample at a time. It complements GDumb, operating in a
low-exemplar regime where GDumb has especially poor performance. We reach the
same consistent conclusions when RanDumb is extended to scenarios with
pretrained models replacing the random transform with pretrained feature
extractor. Our investigation is both surprising and alarming as it questions
our understanding of how to effectively design and train models that require
efficient continual representation learning, and necessitates a principled
reinvestigation of the widely explored problem formulation itself. Our code is
available at https://github.com/drimpossible/RanDumb.
\\ ( https://arxiv.org/abs/2402.08823 ,  835kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08847 (*cross-listing*)
Date: Tue, 13 Feb 2024 23:26:11 GMT   (258kb,D)

Title: Space-Time Bridge-Diffusion
Authors: Hamidreza Behjoo, Michael Chertkov
Categories: stat.ML cs.LG
\\
  In this study, we introduce a novel method for generating new synthetic
samples that are independent and identically distributed (i.i.d.) from
high-dimensional real-valued probability distributions, as defined implicitly
by a set of Ground Truth (GT) samples. Central to our method is the integration
of space-time mixing strategies that extend across temporal and spatial
dimensions. Our methodology is underpinned by three interrelated stochastic
processes designed to enable optimal transport from an easily tractable initial
probability distribution to the target distribution represented by the GT
samples: (a) linear processes incorporating space-time mixing that yield
Gaussian conditional probability densities, (b) their bridge-diffusion analogs
that are conditioned to the initial and final state vectors, and (c) nonlinear
stochastic processes refined through score-matching techniques. The crux of our
training regime involves fine-tuning the nonlinear model, and potentially the
linear models - to align closely with the GT data. We validate the efficacy of
our space-time diffusion approach with numerical experiments, laying the
groundwork for more extensive future theory and experiments to fully
authenticate the method, particularly providing a more efficient (possibly
simulation-free) inference.
\\ ( https://arxiv.org/abs/2402.08847 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08864 (*cross-listing*)
Date: Wed, 14 Feb 2024 00:18:10 GMT   (2057kb,D)

Title: DeepPolar: Inventing Nonlinear Large-Kernel Polar Codes via Deep
  Learning
Authors: S Ashwin Hebbar, Sravan Kumar Ankireddy, Hyeji Kim, Sewoong Oh, Pramod
  Viswanath
Categories: cs.IT cs.LG math.IT
Comments: 18 pages, 18 figures
\\
  Polar codes, developed on the foundation of Arikan's polarization kernel,
represent a breakthrough in coding theory and have emerged as the
state-of-the-art error-correction-code in short-to-medium block length regimes.
Importantly, recent research has indicated that the reliability of polar codes
can be further enhanced by substituting Arikan's kernel with a larger one,
leading to a faster polarization. However, for short-to-medium block length
regimes, the development of polar codes that effectively employ large kernel
sizes has not yet been realized. In this paper, we explore a novel, non-linear
generalization of polar codes with an expanded kernel size, which we call
DeepPolar codes. Our results show that DeepPolar codes effectively utilize the
benefits of larger kernel size, resulting in enhanced reliability compared to
both the existing neural codes and conventional polar codes.
\\ ( https://arxiv.org/abs/2402.08864 ,  2057kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08879 (*cross-listing*)
Date: Wed, 14 Feb 2024 00:56:09 GMT   (1148kb,D)

Title: Inference for an Algorithmic Fairness-Accuracy Frontier
Authors: Yiqi Liu and Francesca Molinari
Categories: econ.EM cs.LG
\\
  Decision-making processes increasingly rely on the use of algorithms. Yet,
algorithms' predictive ability frequently exhibit systematic variation across
subgroups of the population. While both fairness and accuracy are desirable
properties of an algorithm, they often come at the cost of one another. What
should a fairness-minded policymaker do then, when confronted with finite data?
In this paper, we provide a consistent estimator for a theoretical
fairness-accuracy frontier put forward by Liang, Lu and Mu (2023) and propose
inference methods to test hypotheses that have received much attention in the
fairness literature, such as (i) whether fully excluding a covariate from use
in training the algorithm is optimal and (ii) whether there are less
discriminatory alternatives to an existing algorithm. We also provide an
estimator for the distance between a given algorithm and the fairest point on
the frontier, and characterize its asymptotic distribution. We leverage the
fact that the fairness-accuracy frontier is part of the boundary of a convex
set that can be fully represented by its support function. We show that the
estimated support function converges to a tight Gaussian process as the sample
size increases, and then express policy-relevant hypotheses as restrictions on
the support function to construct valid test statistics.
\\ ( https://arxiv.org/abs/2402.08879 ,  1148kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08882 (*cross-listing*)
Date: Wed, 14 Feb 2024 01:13:55 GMT   (3735kb,D)

Title: Moving Object Proposals with Deep Learned Optical Flow for Video Object
  Segmentation
Authors: Ge Shi and Zhili Yang
Categories: cs.CV cs.LG
Comments: 7 pages, 8 figures, 1 table
MSC-class: 68Txx
\\
  Dynamic scene understanding is one of the most conspicuous field of interest
among computer vision community. In order to enhance dynamic scene
understanding, pixel-wise segmentation with neural networks is widely accepted.
The latest researches on pixel-wise segmentation combined semantic and motion
information and produced good performance. In this work, we propose a state of
art architecture of neural networks to accurately and efficiently get the
moving object proposals (MOP). We first train an unsupervised convolutional
neural network (UnFlow) to generate optical flow estimation. Then we render the
output of optical flow net to a fully convolutional SegNet model. The main
contribution of our work is (1) Fine-tuning the pretrained optical flow model
on the brand new DAVIS Dataset; (2) Leveraging fully convolutional neural
networks with Encoder-Decoder architecture to segment objects. We developed the
codes with TensorFlow, and executed the training and evaluation processes on an
AWS EC2 instance.
\\ ( https://arxiv.org/abs/2402.08882 ,  3735kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08890 (*cross-listing*)
Date: Wed, 14 Feb 2024 01:41:50 GMT   (2724kb,D)

Title: Predicting the Emergence of Solar Active Regions Using Machine Learning
Authors: Spiridon Kasapis, Irina N. Kitiashvili, Alexander G. Kosovichev, John
  T. Stefan and Bhairavi Apte
Categories: astro-ph.SR cs.LG
Comments: 9 pages, 4 figures, IAU Symposium 365 Proceedings
\\
  To create early warning capabilities for upcoming Space Weather disturbances,
we have selected a dataset of 61 emerging active regions, which allows us to
identify characteristic features in the evolution of acoustic power density to
predict continuum intensity emergence. For our study, we have utilized Doppler
shift and continuum intensity observations from the Helioseismic and Magnetic
Imager (HMI) onboard the Solar Dynamics Observatory (SDO). The local tracking
of 30.66 x 30.66-degree patches in the vicinity of active regions allowed us to
trace the evolution of active regions starting from the pre-emergence state. We
have developed a machine learning model to capture the acoustic power flux
density variations associated with upcoming magnetic flux emergence. The
trained Long Short-Term Memory (LSTM) model is able to predict 5 hours ahead
whether, in a given area of the solar surface, continuum intensity values will
decrease. The performed study allows us to investigate the potential of the
machine learning approach to predict the emergence of active regions using
acoustic power maps as input.
\\ ( https://arxiv.org/abs/2402.08890 ,  2724kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08892 (*cross-listing*)
Date: Wed, 14 Feb 2024 01:56:31 GMT   (26355kb,D)

Title: Weakly Supervised Segmentation of Vertebral Bodies with Iterative
  Slice-propagation
Authors: Shiqi Peng, Bolin Lai, Guangyu Yao, Xiaoyun Zhang, Ya Zhang, Yan-Feng
  Wang, Hui Zhao
Categories: cs.CV cs.LG
Comments: arXiv admin note: text overlap with arXiv:1412.7062 by other authors
\\
  Vertebral body (VB) segmentation is an important preliminary step towards
medical visual diagnosis for spinal diseases. However, most previous works
require pixel/voxel-wise strong supervisions, which is expensive, tedious and
time-consuming for experts to annotate. In this paper, we propose a Weakly
supervised Iterative Spinal Segmentation (WISS) method leveraging only four
corner landmark weak labels on a single sagittal slice to achieve automatic
volumetric segmentation from CT images for VBs. WISS first segments VBs on an
annotated sagittal slice in an iterative self-training manner. This
self-training method alternates between training and refining labels in the
training set. Then WISS proceeds to segment the whole VBs slice by slice with a
slice-propagation method to obtain volumetric segmentations. We evaluate the
performance of WISS on a private spinal metastases CT dataset and the public
lumbar CT dataset. On the first dataset, WISS achieves distinct improvements
with regard to two different backbones. For the second dataset, WISS achieves
dice coefficients of $91.7\%$ and $83.7\%$ for mid-sagittal slices and 3D CT
volumes, respectively, saving a lot of labeling costs and only sacrificing a
little segmentation performance.
\\ ( https://arxiv.org/abs/2402.08892 ,  26355kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08902 (*cross-listing*)
Date: Wed, 14 Feb 2024 02:17:37 GMT   (1618kb,D)

Title: Auto-Encoding Bayesian Inverse Games
Authors: Xinjie Liu, Lasse Peters, Javier Alonso-Mora, Ufuk Topcu, David
  Fridovich-Keil
Categories: cs.RO cs.GT cs.LG cs.MA cs.SY eess.SY
\\
  When multiple agents interact in a common environment, each agent's actions
impact others' future decisions, and noncooperative dynamic games naturally
capture this coupling. In interactive motion planning, however, agents
typically do not have access to a complete model of the game, e.g., due to
unknown objectives of other players. Therefore, we consider the inverse game
problem, in which some properties of the game are unknown a priori and must be
inferred from observations. Existing maximum likelihood estimation (MLE)
approaches to solve inverse games provide only point estimates of unknown
parameters without quantifying uncertainty, and perform poorly when many
parameter values explain the observed behavior. To address these limitations,
we take a Bayesian perspective and construct posterior distributions of game
parameters. To render inference tractable, we employ a variational autoencoder
(VAE) with an embedded differentiable game solver. This structured VAE can be
trained from an unlabeled dataset of observed interactions, naturally handles
continuous, multi-modal distributions, and supports efficient sampling from the
inferred posteriors without computing game solutions at runtime. Extensive
evaluations in simulated driving scenarios demonstrate that the proposed
approach successfully learns the prior and posterior objective distributions,
provides more accurate objective estimates than MLE baselines, and facilitates
safer and more efficient game-theoretic motion planning.
\\ ( https://arxiv.org/abs/2402.08902 ,  1618kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08910 (*cross-listing*)
Date: Wed, 14 Feb 2024 02:53:51 GMT   (23725kb,D)

Title: Learning-based Bone Quality Classification Method for Spinal Metastasis
Authors: Shiqi Peng, Bolin Lai, Guangyu Yao, Xiaoyun Zhang, Ya Zhang, Yan-Feng
  Wang, Hui Zhao
Categories: cs.CV cs.LG
\\
  Spinal metastasis is the most common disease in bone metastasis and may cause
pain, instability and neurological injuries. Early detection of spinal
metastasis is critical for accurate staging and optimal treatment. The
diagnosis is usually facilitated with Computed Tomography (CT) scans, which
requires considerable efforts from well-trained radiologists. In this paper, we
explore a learning-based automatic bone quality classification method for
spinal metastasis based on CT images. We simultaneously take the posterolateral
spine involvement classification task into account, and employ multi-task
learning (MTL) technique to improve the performance. MTL acts as a form of
inductive bias which helps the model generalize better on each task by sharing
representations between related tasks. Based on the prior knowledge that the
mixed type can be viewed as both blastic and lytic, we model the task of bone
quality classification as two binary classification sub-tasks, i.e., whether
blastic and whether lytic, and leverage a multiple layer perceptron to combine
their predictions. In order to make the model more robust and generalize
better, self-paced learning is adopted to gradually involve from easy to more
complex samples into the training process. The proposed learning-based method
is evaluated on a proprietary spinal metastasis CT dataset. At slice level, our
method significantly outperforms an 121-layer DenseNet classifier in
sensitivities by $+12.54\%$, $+7.23\%$ and $+29.06\%$ for blastic, mixed and
lytic lesions, respectively, meanwhile $+12.33\%$, $+23.21\%$ and $+34.25\%$ at
vertebrae level.
\\ ( https://arxiv.org/abs/2402.08910 ,  23725kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08919 (*cross-listing*)
Date: Wed, 14 Feb 2024 03:31:17 GMT   (3249kb,D)

Title: Interpretable Measures of Conceptual Similarity by
  Complexity-Constrained Descriptive Auto-Encoding
Authors: Alessandro Achille, Greg Ver Steeg, Tian Yu Liu, Matthew Trager,
  Carson Klingenberg, Stefano Soatto
Categories: cs.CV cs.LG
\\
  Quantifying the degree of similarity between images is a key copyright issue
for image-based machine learning. In legal doctrine however, determining the
degree of similarity between works requires subjective analysis, and
fact-finders (judges and juries) can demonstrate considerable variability in
these subjective judgement calls. Images that are structurally similar can be
deemed dissimilar, whereas images of completely different scenes can be deemed
similar enough to support a claim of copying. We seek to define and compute a
notion of "conceptual similarity" among images that captures high-level
relations even among images that do not share repeated elements or visually
similar components. The idea is to use a base multi-modal model to generate
"explanations" (captions) of visual data at increasing levels of complexity.
Then, similarity can be measured by the length of the caption needed to
discriminate between the two images: Two highly dissimilar images can be
discriminated early in their description, whereas conceptually dissimilar ones
will need more detail to be distinguished. We operationalize this definition
and show that it correlates with subjective (averaged human evaluation)
assessment, and beats existing baselines on both image-to-image and
text-to-text similarity benchmarks. Beyond just providing a number, our method
also offers interpretability by pointing to the specific level of granularity
of the description where the source data are differentiated.
\\ ( https://arxiv.org/abs/2402.08919 ,  3249kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08978 (*cross-listing*)
Date: Wed, 14 Feb 2024 06:47:30 GMT   (5316kb,D)

Title: Prismatic: Interactive Multi-View Cluster Analysis of Concept Stocks
Authors: Wong Kam-Kwai, Yan Luo, Xuanwu Yue, Wei Chen, Huamin Qu
Categories: cs.HC cs.CE cs.LG
Comments: 14 pages. A preprint version submitted to IEEE Transactions on
  Visualization and Computer Graphics (TVCG), 2024
\\
  Financial cluster analysis allows investors to discover investment
alternatives and avoid undertaking excessive risks. However, this analytical
task faces substantial challenges arising from many pairwise comparisons, the
dynamic correlations across time spans, and the ambiguity in deriving
implications from business relational knowledge. We propose Prismatic, a visual
analytics system that integrates quantitative analysis of historical
performance and qualitative analysis of business relational knowledge to
cluster correlated businesses interactively. Prismatic features three
clustering processes: dynamic cluster generation, knowledge-based cluster
exploration, and correlation-based cluster validation. Utilizing a multi-view
clustering approach, it enriches data-driven clusters with knowledge-driven
similarity, providing a nuanced understanding of business correlations. Through
well-coordinated visual views, Prismatic facilitates a comprehensive
interpretation of intertwined quantitative and qualitative features,
demonstrating its usefulness and effectiveness via case studies on formulating
concept stocks and extensive interviews with domain experts.
\\ ( https://arxiv.org/abs/2402.08978 ,  5316kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08991 (*cross-listing*)
Date: Wed, 14 Feb 2024 07:27:30 GMT   (40kb)

Title: Towards Robust Model-Based Reinforcement Learning Against Adversarial
  Corruption
Authors: Chenlu Ye, Jiafan He, Quanquan Gu, Tong Zhang
Categories: stat.ML cs.LG
\\
  This study tackles the challenges of adversarial corruption in model-based
reinforcement learning (RL), where the transition dynamics can be corrupted by
an adversary. Existing studies on corruption-robust RL mostly focus on the
setting of model-free RL, where robust least-square regression is often
employed for value function estimation. However, these techniques cannot be
directly applied to model-based RL. In this paper, we focus on model-based RL
and take the maximum likelihood estimation (MLE) approach to learn transition
model. Our work encompasses both online and offline settings. In the online
setting, we introduce an algorithm called corruption-robust optimistic MLE
(CR-OMLE), which leverages total-variation (TV)-based information ratios as
uncertainty weights for MLE. We prove that CR-OMLE achieves a regret of
$\tilde{\mathcal{O}}(\sqrt{T} + C)$, where $C$ denotes the cumulative
corruption level after $T$ episodes. We also prove a lower bound to show that
the additive dependence on $C$ is optimal. We extend our weighting technique to
the offline setting, and propose an algorithm named corruption-robust
pessimistic MLE (CR-PMLE). Under a uniform coverage condition, CR-PMLE exhibits
suboptimality worsened by $\mathcal{O}(C/n)$, nearly matching the lower bound.
To the best of our knowledge, this is the first work on corruption-robust
model-based RL algorithms with provable guarantees.
\\ ( https://arxiv.org/abs/2402.08991 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08992 (*cross-listing*)
Date: Wed, 14 Feb 2024 07:34:22 GMT   (23kb)

Title: Variance Reduction and Low Sample Complexity in Stochastic Optimization
  via Proximal Point Method
Authors: Jiaming Liang
Categories: math.OC cs.LG stat.ML
Comments: 23 pages
\\
  This paper proposes a stochastic proximal point method to solve a stochastic
convex composite optimization problem. High probability results in stochastic
optimization typically hinge on restrictive assumptions on the stochastic
gradient noise, for example, sub-Gaussian distributions. Assuming only weak
conditions such as bounded variance of the stochastic gradient, this paper
establishes a low sample complexity to obtain a high probability guarantee on
the convergence of the proposed method. Additionally, a notable aspect of this
work is the development of a subroutine to solve the proximal subproblem, which
also serves as a novel technique for variance reduction.
\\ ( https://arxiv.org/abs/2402.08992 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09004 (*cross-listing*)
Date: Wed, 14 Feb 2024 08:17:21 GMT   (346kb,D)

Title: Gradient Alignment with Prototype Feature for Fully Test-time Adaptation
Authors: Juhyeon Shin and Jonghyun Lee and Saehyung Lee and Minjun Park and
  Dongjun Lee and Uiwon Hwang and Sungroh Yoon
Categories: cs.CV cs.LG
\\
  In context of Test-time Adaptation(TTA), we propose a regularizer, dubbed
Gradient Alignment with Prototype feature (GAP), which alleviates the
inappropriate guidance from entropy minimization loss from misclassified pseudo
label. We developed a gradient alignment loss to precisely manage the
adaptation process, ensuring that changes made for some data don't negatively
impact the model's performance on other data. We introduce a prototype feature
of a class as a proxy measure of the negative impact. To make GAP regularizer
feasible under the TTA constraints, where model can only access test data
without labels, we tailored its formula in two ways: approximating prototype
features with weight vectors of the classifier, calculating gradient without
back-propagation. We demonstrate GAP significantly improves TTA methods across
various datasets, which proves its versatility and effectiveness.
\\ ( https://arxiv.org/abs/2402.09004 ,  346kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09018 (*cross-listing*)
Date: Wed, 14 Feb 2024 08:50:14 GMT   (2156kb,D)

Title: Neural Operators Meet Energy-based Theory: Operator Learning for
  Hamiltonian and Dissipative PDEs
Authors: Yusuke Tanaka, Takaharu Yaguchi, Tomoharu Iwata, Naonori Ueda
Categories: stat.ML cs.LG
\\
  The operator learning has received significant attention in recent years,
with the aim of learning a mapping between function spaces. Prior works have
proposed deep neural networks (DNNs) for learning such a mapping, enabling the
learning of solution operators of partial differential equations (PDEs).
However, these works still struggle to learn dynamics that obeys the laws of
physics. This paper proposes Energy-consistent Neural Operators (ENOs), a
general framework for learning solution operators of PDEs that follows the
energy conservation or dissipation law from observed solution trajectories. We
introduce a novel penalty function inspired by the energy-based theory of
physics for training, in which the energy functional is modeled by another DNN,
allowing one to bias the outputs of the DNN-based solution operators to ensure
energetic consistency without explicit PDEs. Experiments on multiple physical
systems show that ENO outperforms existing DNN models in predicting solutions
from data, especially in super-resolution settings.
\\ ( https://arxiv.org/abs/2402.09018 ,  2156kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09057 (*cross-listing*)
Date: Wed, 14 Feb 2024 10:08:24 GMT   (19307kb,D)

Title: Distributed Sensing Along Fibres for Smart Clothing
Authors: Brett C. Hannigan, Tyler J. Cuthbert, Chakaveh Ahmadizadeh, Carlo
  Menon
Categories: eess.SP cs.LG
Comments: 35 pages, 7 figures, accepted version
\\
  Textile sensors transform our everyday clothing into a means to track
movement and bio-signals in a completely unobtrusive way. One major hindrance
to the adoption of "smart" clothing is the difficulty encountered with
connections and space when scaling up the number of sensors. There is a lack of
research addressing a key limitation in wearable electronics: connections
between rigid and textile elements are often unreliable and they require
interfacing sensors in a way incompatible with textile mass production methods.
We introduce a prototype garment, compact readout circuit, and algorithm to
measure localized strain along multiple regions of a fibre. We employ a helical
auxetic yarn sensor with tunable sensitivity along its length to selectively
respond to strain signals. We demonstrate distributed sensing in clothing,
monitoring arm joint angles from a single continuous fibre. Compared to optical
motion capture, we achieve around 5{\deg} error in reconstructing shoulder,
elbow, and wrist joint angles.
\\ ( https://arxiv.org/abs/2402.09057 ,  19307kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09075 (*cross-listing*)
Date: Wed, 14 Feb 2024 10:35:26 GMT   (398kb,D)

Title: Steady-State Error Compensation for Reinforcement Learning with
  Quadratic Rewards
Authors: Liyao Wang, Zishun Zheng and Yuan Lin
Categories: eess.SY cs.LG cs.SY
\\
  The selection of a reward function in Reinforcement Learning (RL) has
garnered significant attention because of its impact on system performance.
Issues of steady-state error often manifest when quadratic reward functions are
employed. Although existing solutions using absolute-value-type reward
functions partially address this problem, they tend to induce substantial
fluctuations in specific system states, leading to abrupt changes. In response
to this challenge, this study proposes an approach that introduces an integral
term. By integrating this term into quadratic-type reward functions, the RL
algorithm is adeptly tuned, augmenting the system's consideration of long-term
rewards and, consequently, alleviating concerns related to steady-state errors.
Through experiments and performance evaluations on the Adaptive Cruise Control
(ACC) model and lane change models, we validate that the proposed method not
only effectively diminishes steady-state errors but also results in smoother
variations in system states.
\\ ( https://arxiv.org/abs/2402.09075 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09077 (*cross-listing*)
Date: Wed, 14 Feb 2024 10:40:09 GMT   (14044kb,D)

Title: DisGNet: A Distance Graph Neural Network for Forward Kinematics Learning
  of Gough-Stewart Platform
Authors: Huizhi Zhu, Wenxia Xu, Jian Huang and Jiaxin Li
Categories: cs.RO cs.LG
\\
  In this paper, we propose a graph neural network, DisGNet, for learning the
graph distance matrix to address the forward kinematics problem of the
Gough-Stewart platform. DisGNet employs the k-FWL algorithm for
message-passing, providing high expressiveness with a small parameter count,
making it suitable for practical deployment. Additionally, we introduce the
GPU-friendly Newton-Raphson method, an efficient parallelized optimization
method executed on the GPU to refine DisGNet's output poses, achieving
ultra-high-precision pose. This novel two-stage approach delivers ultra-high
precision output while meeting real-time requirements. Our results indicate
that on our dataset, DisGNet can achieves error accuracys below 1mm and 1deg at
79.8\% and 98.2\%, respectively. As executed on a GPU, our two-stage method can
ensure the requirement for real-time computation. Codes are released at
https://github.com/FLAMEZZ5201/DisGNet.
\\ ( https://arxiv.org/abs/2402.09077 ,  14044kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09081 (*cross-listing*)
Date: Wed, 14 Feb 2024 10:48:00 GMT   (2405kb)

Title: Low-Rank Extragradient Methods for Scalable Semidefinite Optimization
Authors: Dan Garber. Atara Kaplan
Categories: math.OC cs.LG
\\
  We consider several classes of highly important semidefinite optimization
problems that involve both a convex objective function (smooth or nonsmooth)
and additional linear or nonlinear smooth and convex constraints, which are
ubiquitous in statistics, machine learning, combinatorial optimization, and
other domains. We focus on high-dimensional and plausible settings in which the
problem admits a low-rank solution which also satisfies a low-rank
complementarity condition. We provide several theoretical results proving that,
under these circumstances, the well-known Extragradient method, when
initialized in the proximity of an optimal primal-dual solution, converges to a
solution of the constrained optimization problem with its standard convergence
rates guarantees, using only low-rank singular value decompositions (SVD) to
project onto the positive semidefinite cone, as opposed to
computationally-prohibitive full-rank SVDs required in worst-case. Our approach
is supported by numerical experiments conducted with a dataset of Max-Cut
instances.
\\ ( https://arxiv.org/abs/2402.09081 ,  2405kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09082 (*cross-listing*)
Date: Wed, 14 Feb 2024 10:52:39 GMT   (1087kb)

Title: Detection Latencies of Anomaly Detectors: An Overlooked Perspective ?
Authors: Tommaso Puccetti, Andrea Ceccarelli
Categories: cs.CR cs.LG
\\
  The ever-evolving landscape of attacks, coupled with the growing complexity
of ICT systems, makes crafting anomaly-based intrusion detectors (ID) and error
detectors (ED) a difficult task: they must accurately detect attacks, and they
should promptly perform detections. Although improving and comparing the
detection capability is the focus of most research works, the timeliness of the
detection is less considered and often insufficiently evaluated or discussed.
In this paper, we argue the relevance of measuring the temporal latency of
attacks and errors, and we propose an evaluation approach for detectors to
ensure a pragmatic trade-off between correct and in-time detection. Briefly,
the approach relates the false positive rate with the temporal latency of
attacks and errors, and this ultimately leads to guidelines for configuring a
detector. We apply our approach by evaluating different ED and ID solutions in
two industrial cases: i) an embedded railway on-board system that optimizes
public mobility, and ii) an edge device for the Industrial Internet of Things.
Our results show that considering latency in addition to traditional metrics
like the false positive rate, precision, and coverage gives an additional
fundamental perspective on the actual performance of the detector and should be
considered when assessing and configuring anomaly detectors.
\\ ( https://arxiv.org/abs/2402.09082 ,  1087kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09105 (*cross-listing*)
Date: Wed, 14 Feb 2024 11:26:30 GMT   (1625kb,D)

Title: Scheduling for On-Board Federated Learning with Satellite Clusters
Authors: Nasrin Razmi, Bho Matthiesen, Armin Dekorsy, Petar Popovski
Categories: cs.DC cs.LG
Comments: 2023 IEEE GLOBECOM
\\
  Mega-constellations of small satellites have evolved into a source of massive
amount of valuable data. To manage this data efficiently, on-board federated
learning (FL) enables satellites to train a machine learning (ML) model
collaboratively without having to share the raw data. This paper introduces a
scheme for scheduling on-board FL for constellations connected with intra-orbit
inter-satellite links. The proposed scheme utilizes the predictable visibility
pattern between satellites and ground station (GS), both at the individual
satellite level and cumulatively within the entire orbit, to mitigate
intermittent connectivity and best use of available time. To this end, two
distinct schedulers are employed: one for coordinating the FL procedures among
orbits, and the other for controlling those within each orbit. These two
schedulers cooperatively determine the appropriate time to perform global
updates in GS and then allocate suitable duration to satellites within each
orbit for local training, proportional to usable time until next global update.
This scheme leads to improved test accuracy within a shorter time.
\\ ( https://arxiv.org/abs/2402.09105 ,  1625kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09122 (*cross-listing*)
Date: Wed, 14 Feb 2024 12:18:23 GMT   (2568kb,D)

Title: Mixed-Output Gaussian Process Latent Variable Models
Authors: James Odgers, Chrysoula Kappatou, Ruth Misener, Sarah Filippi
Categories: stat.ML cs.LG
\\
  This work develops a Bayesian non-parametric approach to signal separation
where the signals may vary according to latent variables. Our key contribution
is to augment Gaussian Process Latent Variable Models (GPLVMs) to incorporate
the case where each data point comprises the weighted sum of a known number of
pure component signals, observed across several input locations. Our framework
allows the use of a range of priors for the weights of each observation. This
flexibility enables us to represent use cases including sum-to-one constraints
for estimating fractional makeup, and binary weights for classification. Our
contributions are particularly relevant to spectroscopy, where changing
conditions may cause the underlying pure component signals to vary from sample
to sample. To demonstrate the applicability to both spectroscopy and other
domains, we consider several applications: a near-infrared spectroscopy data
set with varying temperatures, a simulated data set for identifying flow
configuration through a pipe, and a data set for determining the type of rock
from its reflectance.
\\ ( https://arxiv.org/abs/2402.09122 ,  2568kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09135 (*cross-listing*)
Date: Wed, 14 Feb 2024 12:34:38 GMT   (936kb)

Title: Unconventional Computing based on Four Wave Mixing in Highly Nonlinear
  Waveguides
Authors: Kostas Sozos, Stavros Deligiannidis, Charis Mesaritakis, Adonis Bogris
Categories: physics.optics cs.LG
Comments: 6
\\
  In this work we numerically analyze a photonic unconventional accelerator
based on the four-wave mixing effect in highly nonlinear waveguides. The
proposed scheme can act as a fully analogue system for nonlinear signal
processing directly in the optical domain. By exploiting the rich Kerr-induced
nonlinearities, multiple nonlinear transformations of an input signal can be
generated and used for solving complex nonlinear tasks. We first evaluate the
performance of our scheme in the Santa-Fe chaotic time-series prediction. The
true power of this processor is revealed in the all-optical nonlinearity
compensation in an optical communication scenario where we provide results
superior to those offered by strong machine learning algorithms with reduced
power consumption and computational complexity. Finally, we showcase how the
FWM module can be used as a reconfigurable nonlinear activation module being
capable of reproducing characteristic functions such as sigmoid or rectified
linear unit.
\\ ( https://arxiv.org/abs/2402.09135 ,  936kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09164 (*cross-listing*)
Date: Wed, 14 Feb 2024 13:30:02 GMT   (6260kb,D)

Title: Less is More: Fewer Interpretable Region via Submodular Subset Selection
Authors: Ruoyu Chen, Hua Zhang, Siyuan Liang, Jingzhi Li, Xiaochun Cao
Categories: cs.CV cs.LG
Comments: Accepted to ICLR 2024 (Oral)
\\
  Image attribution algorithms aim to identify important regions that are
highly relevant to model decisions. Although existing attribution solutions can
effectively assign importance to target elements, they still face the following
challenges: 1) existing attribution methods generate inaccurate small regions
thus misleading the direction of correct attribution, and 2) the model cannot
produce good attribution results for samples with wrong predictions. To address
the above challenges, this paper re-models the above image attribution problem
as a submodular subset selection problem, aiming to enhance model
interpretability using fewer regions. To address the lack of attention to local
regions, we construct a novel submodular function to discover more accurate
fine-grained interpretation regions. To enhance the attribution effect for all
samples, we also impose four different constraints on the selection of
sub-regions, i.e., confidence, effectiveness, consistency, and collaboration
scores, to assess the importance of various subsets. Moreover, our theoretical
analysis substantiates that the proposed function is in fact submodular.
Extensive experiments show that the proposed method outperforms SOTA methods on
two face datasets (Celeb-A and VGG-Face2) and one fine-grained dataset
(CUB-200-2011). For correctly predicted samples, the proposed method improves
the Deletion and Insertion scores with an average of 4.9% and 2.5% gain
relative to HSIC-Attribution. For incorrectly predicted samples, our method
achieves gains of 81.0% and 18.4% compared to the HSIC-Attribution algorithm in
the average highest confidence and Insertion score respectively. The code is
released at https://github.com/RuoyuChen10/SMDL-Attribution.
\\ ( https://arxiv.org/abs/2402.09164 ,  6260kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09179 (*cross-listing*)
Date: Wed, 14 Feb 2024 13:47:35 GMT   (1567kb,D)

Title: Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model
  Customization
Authors: Rui Zhang, Hongwei Li, Rui Wen, Wenbo Jiang, Yuan Zhang, Michael
  Backes, Yun Shen, Yang Zhang
Categories: cs.CR cs.LG
\\
  The increasing demand for customized Large Language Models (LLMs) has led to
the development of solutions like GPTs. These solutions facilitate tailored LLM
creation via natural language prompts without coding. However, the
trustworthiness of third-party custom versions of LLMs remains an essential
concern. In this paper, we propose the first instruction backdoor attacks
against applications integrated with untrusted customized LLMs (e.g., GPTs).
Specifically, these attacks embed the backdoor into the custom version of LLMs
by designing prompts with backdoor instructions, outputting the attacker's
desired result when inputs contain the pre-defined triggers. Our attack
includes 3 levels of attacks: word-level, syntax-level, and semantic-level,
which adopt different types of triggers with progressive stealthiness. We
stress that our attacks do not require fine-tuning or any modification to the
backend LLMs, adhering strictly to GPTs development guidelines. We conduct
extensive experiments on 4 prominent LLMs and 5 benchmark text classification
datasets. The results show that our instruction backdoor attacks achieve the
desired attack performance without compromising utility. Additionally, we
propose an instruction-ignoring defense mechanism and demonstrate its partial
effectiveness in mitigating such attacks. Our findings highlight the
vulnerability and the potential risks of LLM customization such as GPTs.
\\ ( https://arxiv.org/abs/2402.09179 ,  1567kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09230 (*cross-listing*)
Date: Wed, 14 Feb 2024 15:17:37 GMT   (11kb)

Title: Context Composing for Full Line Code Completion
Authors: Anton Semenkin, Yaroslav Sokolov, Evgeniia Vu
Categories: cs.SE cs.LG
Comments: 3 pages. Accepted for publication in the proceedings of ICSE 2024 IDE
  workshop
DOI: 10.1145/3643796.3648446
\\
  Code Completion is one of the most used Integrated Development Environment
(IDE) features, which affects the everyday life of a software developer. Modern
code completion approaches moved from the composition of several static
analysis-based contributors to pipelines that involve neural networks. This
change allows the proposal of longer code suggestions while maintaining the
relatively short time spent on generation itself. At JetBrains, we put a lot of
effort into perfecting the code completion workflow so it can be both helpful
and non-distracting for a programmer. We managed to ship the Full Line Code
Completion feature to PyCharm Pro IDE and proved its usefulness in A/B testing
on hundreds of real Python users. The paper describes our approach to context
composing for the Transformer model that is a core of the feature's
implementation. In addition to that, we share our next steps to improve the
feature and emphasize the importance of several research aspects in the area.
\\ ( https://arxiv.org/abs/2402.09230 ,  11kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09245 (*cross-listing*)
Date: Wed, 14 Feb 2024 15:34:28 GMT   (2896kb,D)

Title: Overview of the L3DAS23 Challenge on Audio-Visual Extended Reality
Authors: Christian Marinoni, Riccardo Fosco Gramaccioni, Changan Chen, Aurelio
  Uncini, Danilo Comminiello
Categories: eess.AS cs.LG eess.SP
Comments: Accepted to 2023 IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP 2023)
\\
  The primary goal of the L3DAS23 Signal Processing Grand Challenge at ICASSP
2023 is to promote and support collaborative research on machine learning for
3D audio signal processing, with a specific emphasis on 3D speech enhancement
and 3D Sound Event Localization and Detection in Extended Reality applications.
As part of our latest competition, we provide a brand-new dataset, which
maintains the same general characteristics of the L3DAS21 and L3DAS22 datasets,
but with first-order Ambisonics recordings from multiple reverberant simulated
environments. Moreover, we start exploring an audio-visual scenario by
providing images of these environments, as perceived by the different
microphone positions and orientations. We also propose updated baseline models
for both tasks that can now support audio-image couples as input and a
supporting API to replicate our results. Finally, we present the results of the
participants. Further details about the challenge are available at
https://www.l3das.com/icassp2023.
\\ ( https://arxiv.org/abs/2402.09245 ,  2896kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09299 (*cross-listing*)
Date: Wed, 14 Feb 2024 16:41:35 GMT   (3581kb,D)

Title: Trained Without My Consent: Detecting Code Inclusion In Language Models
  Trained on Code
Authors: Vahid Majdinasab, Amin Nikanjam, Foutse Khomh
Categories: cs.SE cs.LG
Comments: Submitted to TOSEM (ACM Transactions on Software Engineering and
  Methodology)
\\
  Code auditing ensures that the developed code adheres to standards,
regulations, and copyright protection by verifying that it does not contain
code from protected sources. The recent advent of Large Language Models (LLMs)
as coding assistants in the software development process poses new challenges
for code auditing. The dataset for training these models is mainly collected
from publicly available sources. This raises the issue of intellectual property
infringement as developers' codes are already included in the dataset.
Therefore, auditing code developed using LLMs is challenging, as it is
difficult to reliably assert if an LLM used during development has been trained
on specific copyrighted codes, given that we do not have access to the training
datasets of these models. Given the non-disclosure of the training datasets,
traditional approaches such as code clone detection are insufficient for
asserting copyright infringement. To address this challenge, we propose a new
approach, TraWiC; a model-agnostic and interpretable method based on membership
inference for detecting code inclusion in an LLM's training dataset. We extract
syntactic and semantic identifiers unique to each program to train a classifier
for detecting code inclusion. In our experiments, we observe that TraWiC is
capable of detecting 83.87% of codes that were used to train an LLM. In
comparison, the prevalent clone detection tool NiCad is only capable of
detecting 47.64%. In addition to its remarkable performance, TraWiC has low
resource overhead in contrast to pair-wise clone detection that is conducted
during the auditing process of tools like CodeWhisperer reference tracker,
across thousands of code snippets.
\\ ( https://arxiv.org/abs/2402.09299 ,  3581kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09316 (*cross-listing*)
Date: Wed, 14 Feb 2024 17:11:52 GMT   (15069kb,D)

Title: Only My Model On My Data: A Privacy Preserving Approach Protecting one
  Model and Deceiving Unauthorized Black-Box Models
Authors: Weiheng Chai, Brian Testa, Huantao Ren, Asif Salekin, Senem
  Velipasalar
Categories: cs.CV cs.LG
\\
  Deep neural networks are extensively applied to real-world tasks, such as
face recognition and medical image classification, where privacy and data
protection are critical. Image data, if not protected, can be exploited to
infer personal or contextual information. Existing privacy preservation
methods, like encryption, generate perturbed images that are unrecognizable to
even humans. Adversarial attack approaches prohibit automated inference even
for authorized stakeholders, limiting practical incentives for commercial and
widespread adaptation. This pioneering study tackles an unexplored practical
privacy preservation use case by generating human-perceivable images that
maintain accurate inference by an authorized model while evading other
unauthorized black-box models of similar or dissimilar objectives, and
addresses the previous research gaps. The datasets employed are ImageNet, for
image classification, Celeba-HQ dataset, for identity classification, and
AffectNet, for emotion classification. Our results show that the generated
images can successfully maintain the accuracy of a protected model and degrade
the average accuracy of the unauthorized black-box models to 11.97%, 6.63%, and
55.51% on ImageNet, Celeba-HQ, and AffectNet datasets, respectively.
\\ ( https://arxiv.org/abs/2402.09316 ,  15069kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09328 (*cross-listing*)
Date: Wed, 14 Feb 2024 17:18:03 GMT   (119kb,D)

Title: Connecting Algorithmic Fairness to Quality Dimensions in Machine
  Learning in Official Statistics and Survey Production
Authors: Patrick Oliver Schenk and Christoph Kern
Categories: stat.ML cs.LG stat.ME
\\
  National Statistical Organizations (NSOs) increasingly draw on Machine
Learning (ML) to improve the timeliness and cost-effectiveness of their
products. When introducing ML solutions, NSOs must ensure that high standards
with respect to robustness, reproducibility, and accuracy are upheld as
codified, e.g., in the Quality Framework for Statistical Algorithms (QF4SA;
Yung et al. 2022). At the same time, a growing body of research focuses on
fairness as a pre-condition of a safe deployment of ML to prevent disparate
social impacts in practice. However, fairness has not yet been explicitly
discussed as a quality aspect in the context of the application of ML at NSOs.
We employ Yung et al. (2022)'s QF4SA quality framework and present a mapping of
its quality dimensions to algorithmic fairness. We thereby extend the QF4SA
framework in several ways: we argue for fairness as its own quality dimension,
we investigate the interaction of fairness with other dimensions, and we
explicitly address data, both on its own and its interaction with applied
methodology. In parallel with empirical illustrations, we show how our mapping
can contribute to methodology in the domains of official statistics,
algorithmic fairness, and trustworthy machine learning.
\\ ( https://arxiv.org/abs/2402.09328 ,  119kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09330 (*cross-listing*)
Date: Wed, 14 Feb 2024 17:22:03 GMT   (512kb,D)

Title: 3D-based RNA function prediction tools in rnaglib
Authors: Carlos Oliver, Vincent Mallet, J\'er\^ome Waldisp\"uhl
Categories: q-bio.BM cs.LG
\\
  Understanding the connection between complex structural features of RNA and
biological function is a fundamental challenge in evolutionary studies and in
RNA design. However, building datasets of RNA 3D structures and making
appropriate modeling choices remains time-consuming and lacks standardization.
In this chapter, we describe the use of rnaglib, to train supervised and
unsupervised machine learning-based function prediction models on datasets of
RNA 3D structures.
\\ ( https://arxiv.org/abs/2402.09330 ,  512kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09387 (*cross-listing*)
Date: Wed, 14 Feb 2024 18:37:40 GMT   (3939kb,D)

Title: Active Disruption Avoidance and Trajectory Design for Tokamak Ramp-downs
  with Neural Differential Equations and Reinforcement Learning
Authors: Allen M. Wang, Oswin So, Charles Dawson, Darren T. Garnier, Cristina
  Rea, and Chuchu Fan
Categories: physics.plasm-ph cs.LG
\\
  The tokamak offers a promising path to fusion energy, but plasma disruptions
pose a major economic risk, motivating considerable advances in disruption
avoidance. This work develops a reinforcement learning approach to this problem
by training a policy to safely ramp-down the plasma current while avoiding
limits on a number of quantities correlated with disruptions. The policy
training environment is a hybrid physics and machine learning model trained on
simulations of the SPARC primary reference discharge (PRD) ramp-down, an
upcoming burning plasma scenario which we use as a testbed. To address physics
uncertainty and model inaccuracies, the simulation environment is massively
parallelized on GPU with randomized physics parameters during policy training.
The trained policy is then successfully transferred to a higher fidelity
simulator where it successfully ramps down the plasma while avoiding
user-specified disruptive limits. We also address the crucial issue of safety
criticality by demonstrating that a constraint-conditioned policy can be used
as a trajectory design assistant to design a library of feed-forward
trajectories to handle different physics conditions and user settings. As a
library of trajectories is more interpretable and verifiable offline, we argue
such an approach is a promising path for leveraging the capabilities of
reinforcement learning in the safety-critical context of burning plasma
tokamaks. Finally, we demonstrate how the training environment can be a useful
platform for other feed-forward optimization approaches by using an
evolutionary algorithm to perform optimization of feed-forward trajectories
that are robust to physics uncertainty
\\ ( https://arxiv.org/abs/2402.09387 ,  3939kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2301.13185
replaced with revised version Wed, 14 Feb 2024 01:00:18 GMT   (1401kb,D)

Title: Optimal Decision Tree Policies for Markov Decision Processes
Authors: Dani\"el Vos and Sicco Verwer
Categories: cs.AI cs.LG
Journal-ref: Proceedings of the Thirty-Second International Joint Conference on
  Artificial Intelligence Main Track, 5457-5465 (2023)
DOI: 10.24963/ijcai.2023/606
\\ ( https://arxiv.org/abs/2301.13185 ,  1401kb)
------------------------------------------------------------------------------
\\
arXiv:2302.10809
replaced with revised version Wed, 14 Feb 2024 18:28:52 GMT   (2387kb,D)

Title: Causal Explanations for Sequential Decision-Making in Multi-Agent
  Systems
Authors: Balint Gyevnar, Cheng Wang, Christopher G. Lucas, Shay B. Cohen,
  Stefano V. Albrecht
Categories: cs.AI cs.RO
Comments: Accepted in 23rd International Conference on Autonomous Agents and
  Multi-Agent Systems (AAMAS), 2024
ACM-class: I.2.9
\\ ( https://arxiv.org/abs/2302.10809 ,  2387kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17165
replaced with revised version Wed, 14 Feb 2024 14:23:46 GMT   (85kb)

Title: (Ir)rationality in AI: State of the Art, Research Challenges and Open
  Questions
Authors: Olivia Macmillan-Scott and Mirco Musolesi
Categories: cs.AI cs.CY cs.HC cs.LG cs.MA
\\ ( https://arxiv.org/abs/2311.17165 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05328
replaced with revised version Wed, 14 Feb 2024 18:22:12 GMT   (1060kb,D)

Title: Bad Students Make Great Teachers: Active Learning Accelerates
  Large-Scale Visual Understanding
Authors: Talfan Evans, Shreya Pathak, Hamza Merzic, Jonathan Schwarz, Ryutaro
  Tanno, Olivier J. Henaff
Categories: cs.AI
Comments: Technical report
\\ ( https://arxiv.org/abs/2312.05328 ,  1060kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07540
replaced with revised version Wed, 14 Feb 2024 18:59:41 GMT   (1607kb,D)

Title: diff History for Neural Language Agents
Authors: Ulyana Piterbarg, Lerrel Pinto, Rob Fergus
Categories: cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2312.07540 ,  1607kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08801
replaced with revised version Wed, 14 Feb 2024 17:23:30 GMT   (379kb,D)

Title: Automated Process Planning Based on a Semantic Capability Model and SMT
Authors: Aljosha K\"ocher, Luis Miguel Vieira da Silva, Alexander Fay
Categories: cs.AI cs.LO
Comments: Presented at CAIPI Workshop at AAAI 2024
\\ ( https://arxiv.org/abs/2312.08801 ,  379kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06072
replaced with revised version Wed, 14 Feb 2024 15:49:21 GMT   (270kb,D)

Title: Chain of History: Learning and Forecasting with LLMs for Temporal
  Knowledge Graph Completion
Authors: Ruilin Luo, Tianle Gu, Haoling Li, Junzhe Li, Zicheng Lin, Jiayi Li,
  Yujiu Yang
Categories: cs.AI cs.CL
Comments: 15 pages; typos corrected, references added
\\ ( https://arxiv.org/abs/2401.06072 ,  270kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01677
replaced with revised version Wed, 14 Feb 2024 14:02:31 GMT   (299kb,D)

Title: Embedding Ontologies via Incorporating Extensional and Intensional
  Knowledge
Authors: Keyu Wang, Guilin Qi, Jiaoyan Chen, Tianxing Wu
Categories: cs.AI cs.CL
Comments: Submitting to IJCAI2024; 9 pages and 3 figures
\\ ( https://arxiv.org/abs/2402.01677 ,  299kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05048
replaced with revised version Wed, 14 Feb 2024 12:02:45 GMT   (618kb,D)

Title: How VADER is your AI? Towards a definition of artificial intelligence
  systems appropriate for regulation
Authors: Leonardo C. T. Bezerra, Alexander E. I. Brownlee, Luana Ferraz
  Alvarenga, Renan Cipriano Moioli, Thais Vasconcelos Batista
Categories: cs.AI
ACM-class: I.2.0
\\ ( https://arxiv.org/abs/2402.05048 ,  618kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06044
replaced with revised version Wed, 14 Feb 2024 13:23:51 GMT   (4559kb,D)

Title: OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind
  Reasoning Capabilities of Large Language Models
Authors: Hainiu Xu, Runcong Zhao, Lixing Zhu, Jinhua Du, Yulan He
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.06044 ,  4559kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07477
replaced with revised version Wed, 14 Feb 2024 12:11:44 GMT   (2516kb,D)

Title: Food Recommendation as Language Processing (F-RLP): A Personalized and
  Contextual Paradigm
Authors: Ali Rostami, Ramesh Jain, Amir M. Rahmani
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.07477 ,  2516kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07744
replaced with revised version Wed, 14 Feb 2024 18:43:54 GMT   (5486kb,D)

Title: Towards Unified Alignment Between Agents, Humans, and Environment
Authors: Zonghan Yang, An Liu, Zijun Liu, Kaiming Liu, Fangzhou Xiong, Yile
  Wang, Zeyuan Yang, Qingyuan Hu, Xinrui Chen, Zhenhe Zhang, Fuwen Luo,
  Zhicheng Guo, Peng Li, Yang Liu
Categories: cs.AI cs.CL cs.LG
Comments: Project webpage:
  https://agent-force.github.io/unified-alignment-for-agents.html
\\ ( https://arxiv.org/abs/2402.07744 ,  5486kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07799
replaced with revised version Wed, 14 Feb 2024 14:01:55 GMT   (4979kb,D)

Title: Generalising Planning Environment Redesign
Authors: Alberto Pozanco, Ramon Fraga Pereira, Daniel Borrajo
Categories: cs.AI
Comments: Paper accepted at AAAI'24
\\ ( https://arxiv.org/abs/2402.07799 ,  4979kb)
------------------------------------------------------------------------------
\\
arXiv:2304.08460
replaced with revised version Wed, 14 Feb 2024 18:00:33 GMT   (690kb,D)

Title: LongForm: Effective Instruction Tuning with Reverse Instructions
Authors: Abdullatif K\"oksal, Timo Schick, Anna Korhonen, Hinrich Sch\"utze
Categories: cs.CL cs.AI cs.LG
Comments: This version extends the evaluation with new metrics and NLU tasks
\\ ( https://arxiv.org/abs/2304.08460 ,  690kb)
------------------------------------------------------------------------------
\\
arXiv:2305.03584
replaced with revised version Tue, 13 Feb 2024 22:27:36 GMT   (7048kb,D)

Title: Now It Sounds Like You: Learning Personalized Vocabulary On Device
Authors: Sid Wang, Ashish Shenoy, Pierce Chuang, John Nguyen
Categories: cs.CL cs.AI
Comments: Federated Learning, Personalization, On-device NLP, Accepted at AAAI
  Spring Symposium 2024
\\ ( https://arxiv.org/abs/2305.03584 ,  7048kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14771
replaced with revised version Wed, 14 Feb 2024 17:45:41 GMT   (3953kb,D)

Title: David helps Goliath: Inference-Time Collaboration Between Small
  Specialized and Large General Diffusion LMs
Authors: Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov, Marjan Ghazvininejad
Categories: cs.CL
\\ ( https://arxiv.org/abs/2305.14771 ,  3953kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12951
replaced with revised version Tue, 13 Feb 2024 20:53:00 GMT   (1144kb,D)

Title: Public Attitudes Toward ChatGPT on Twitter: Sentiments, Topics, and
  Occupations
Authors: Ratanond Koonchanok, Yanling Pan, Hyeju Jang
Categories: cs.CL
DOI: 10.21203/rs.3.rs-3945065/v1
\\ ( https://arxiv.org/abs/2306.12951 ,  1144kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15164
replaced with revised version Wed, 14 Feb 2024 18:56:03 GMT   (344kb,D)

Title: LINC: A Neurosymbolic Approach for Logical Reasoning by Combining
  Language Models with First-Order Logic Provers
Authors: Theo X. Olausson and Alex Gu and Benjamin Lipkin and Cedegao E. Zhang
  and Armando Solar-Lezama and Joshua B. Tenenbaum and Roger Levy
Categories: cs.CL cs.AI
Comments: EMNLP Main 2023 (Outstanding Paper Award)
Journal-ref: Proceedings of the 2023 Conference on Empirical Methods in Natural
  Language Processing, pages 5153-5176, Singapore. Association for
  Computational Linguistics
DOI: 10.18653/v1/2023.emnlp-main.313
\\ ( https://arxiv.org/abs/2310.15164 ,  344kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17438
replaced with revised version Wed, 14 Feb 2024 05:53:40 GMT   (6999kb,D)

Title: CLOMO: Counterfactual Logical Modification with Large Language Models
Authors: Yinya Huang, Ruixin Hong, Hongming Zhang, Wei Shao, Zhicheng Yang,
  Dong Yu, Changshui Zhang, Xiaodan Liang, Linqi Song
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2311.17438 ,  6999kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10323
replaced with revised version Wed, 14 Feb 2024 18:57:20 GMT   (597kb,D)

Title: Continuous Prompt Generation from Linear Combination of Discrete Prompt
  Embeddings
Authors: Pascal Passigan, Kidus Yohannes, Joshua Pereira
Categories: cs.CL
\\ ( https://arxiv.org/abs/2312.10323 ,  597kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08273
replaced with revised version Wed, 14 Feb 2024 10:34:48 GMT   (8575kb,D)

Title: Large Language Models are Null-Shot Learners
Authors: Pittawat Taveekitworachai, Febri Abdullah, Ruck Thawonmas
Categories: cs.CL cs.AI cs.LG
Comments: 28 pages; added Gemini Pro results, error analysis, and a discussion
  on confabulation
\\ ( https://arxiv.org/abs/2401.08273 ,  8575kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11624
replaced with revised version Tue, 13 Feb 2024 20:46:32 GMT   (178kb,D)

Title: In-context Learning with Retrieved Demonstrations for Language Models: A
  Survey
Authors: Man Luo, Xin Xu, Yue Liu, Panupong Pasupat, Mehran Kazemi
Categories: cs.CL cs.AI cs.IR
\\ ( https://arxiv.org/abs/2401.11624 ,  178kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01535
replaced with revised version Wed, 14 Feb 2024 10:51:08 GMT   (7589kb,D)

Title: An Empirical Analysis of Diversity in Argument Summarization
Authors: Michiel van der Meer, Piek Vossen, Catholijn M. Jonker, Pradeep K.
  Murukannaiah
Categories: cs.CL cs.AI
Comments: Accepted at EACL2024 (main proceedings)
\\ ( https://arxiv.org/abs/2402.01535 ,  7589kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04222
replaced with revised version Wed, 14 Feb 2024 14:21:59 GMT   (1597kb,D)

Title: What is 'Typological Diversity' in NLP?
Authors: Esther Ploeger, Wessel Poelman, Miryam de Lhoneux, Johannes Bjerva
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.04222 ,  1597kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04838
replaced with revised version Wed, 14 Feb 2024 12:51:56 GMT   (7984kb,D)

Title: PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity
  Recognition
Authors: Jinghui Lu, Ziwei Yang, Yanjie Wang, Xuejing Liu, Can Huang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.04838 ,  7984kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05128
replaced with revised version Wed, 14 Feb 2024 10:06:54 GMT   (1434kb,D)

Title: Enhancing Textbook Question Answering Task with Large Language Models
  and Retrieval Augmented Generation
Authors: Hessa Abdulrahman Alawwad, Areej Alhothali, Usman Naseem, Ali
  Alkhathlan, Amani Jamal
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.05128 ,  1434kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08113
replaced with revised version Wed, 14 Feb 2024 17:52:45 GMT   (20246kb,D)

Title: Addressing cognitive bias in medical language models
Authors: Samuel Schmidgall, Carl Harris, Ime Essien, Daniel Olshvang, Tawsifur
  Rahman, Ji Woong Kim, Rojin Ziaei, Jason Eshraghian, Peter Abadir, Rama
  Chellappa
Categories: cs.CL cs.HC
\\ ( https://arxiv.org/abs/2402.08113 ,  20246kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08303
replaced with revised version Wed, 14 Feb 2024 08:09:58 GMT   (4255kb,D)

Title: ChatCell: Facilitating Single-Cell Analysis with Natural Language
Authors: Yin Fang, Kangwei Liu, Ningyu Zhang, Xinle Deng, Penghui Yang, Zhuo
  Chen, Xiangru Tang, Mark Gerstein, Xiaohui Fan, Huajun Chen
Categories: cs.CL cs.AI cs.CE cs.HC cs.LG
Comments: Ongoing work; 15 pages, 6 Tables, 9 Figures; Project homepage:
  https://zjunlp.github.io/project/ChatCell Code:
  https://github.com/zjunlp/ChatCell Dataset:
  https://huggingface.co/datasets/zjunlp/ChatCell-Instructions Demo:
  https://huggingface.co/spaces/zjunlp/Chatcell
\\ ( https://arxiv.org/abs/2402.08303 ,  4255kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08479
replaced with revised version Wed, 14 Feb 2024 09:28:55 GMT   (854kb,D)

Title: Plausible Extractive Rationalization through Semi-Supervised Entailment
  Signal
Authors: Yeo Wei Jie, Ranjan Satapathy, Erik Cambria
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.08479 ,  854kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08638
replaced with revised version Wed, 14 Feb 2024 09:49:52 GMT   (7854kb,D)

Title: SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14
  Languages
Authors: Nedjma Ousidhoum, Shamsuddeen Hassan Muhammad, Mohamed Abdalla, Idris
  Abdulmumin, Ibrahim Said Ahmad, Sanchit Ahuja, Alham Fikri Aji, Vladimir
  Araujo, Abinew Ali Ayele, Pavan Baswani, Meriem Beloucif, Chris Biemann,
  Sofia Bourhim, Christine De Kock, Genet Shanko Dekebo, Oumaima Hourrane,
  Gopichand Kanumolu, Lokesh Madasu, Samuel Rutunda, Manish Shrivastava, Thamar
  Solorio, Nirmal Surange, Hailegnaw Getaneh Tilaye, Krishnapriya Vishnubhotla,
  Genta Winata, Seid Muhie Yimam, Saif M. Mohammad
Categories: cs.CL
Comments: 18 pages
\\ ( https://arxiv.org/abs/2402.08638 ,  7854kb)
------------------------------------------------------------------------------
\\
arXiv:2106.11935
replaced with revised version Wed, 14 Feb 2024 07:05:06 GMT   (353kb,D)

Title: Provably Efficient Representation Selection in Low-rank Markov Decision
  Processes: From Online to Offline RL
Authors: Weitong Zhang and Jiafan He and Dongruo Zhou and Amy Zhang and
  Quanquan Gu
Categories: cs.LG math.OC stat.ML
Comments: 32 pages, 2 figures, 7 tables, In UAI 2023
\\ ( https://arxiv.org/abs/2106.11935 ,  353kb)
------------------------------------------------------------------------------
\\
arXiv:2205.11921
replaced with revised version Wed, 14 Feb 2024 16:43:50 GMT   (1052kb,D)

Title: Compression-aware Training of Neural Networks using Frank-Wolfe
Authors: Max Zimmer and Christoph Spiegel and Sebastian Pokutta
Categories: cs.LG math.OC
Comments: 8 pages, 5 pages references, 14 pages appendix, 8 figures, and 11
  tables
\\ ( https://arxiv.org/abs/2205.11921 ,  1052kb)
------------------------------------------------------------------------------
\\
arXiv:2206.02990
replaced with revised version Wed, 14 Feb 2024 02:26:15 GMT   (5481kb,D)

Title: Enhancing Distributional Stability among Sub-populations
Authors: Jiashuo Liu, Jiayun Wu, Jie Peng, Xiaoyu Wu, Yang Zheng, Bo Li, Peng
  Cui
Categories: cs.LG
Comments: Accepted at International Conference on Artificial Intelligence and
  Statistics (AISTATS 2024)
\\ ( https://arxiv.org/abs/2206.02990 ,  5481kb)
------------------------------------------------------------------------------
\\
arXiv:2208.03915
replaced with revised version Tue, 13 Feb 2024 19:59:56 GMT   (36kb)

Title: Dynamic Maintenance of Kernel Density Estimation Data Structure: From
  Practice to Theory
Authors: Jiehao Liang, Zhao Song, Zhaozhuo Xu, Junze Yin, Danyang Zhuo
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2208.03915 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2209.14905
replaced with revised version Wed, 14 Feb 2024 15:31:56 GMT   (420kb,D)

Title: Variance Covariance Regularization Enforces Pairwise Independence in
  Self-Supervised Representations
Authors: Gr\'egoire Mialon, Randall Balestriero, and Yann LeCun
Categories: cs.LG
\\ ( https://arxiv.org/abs/2209.14905 ,  420kb)
------------------------------------------------------------------------------
\\
arXiv:2210.02042
replaced with revised version Wed, 14 Feb 2024 05:21:18 GMT   (697kb,D)

Title: FedMT: Federated Learning with Mixed-type Labels
Authors: Qiong Zhang, Aline Talhouk, Gang Niu, Xiaoxiao Li
Categories: cs.LG cs.AI cs.DC
Comments: 23 pages
\\ ( https://arxiv.org/abs/2210.02042 ,  697kb)
------------------------------------------------------------------------------
\\
arXiv:2211.10936
replaced with revised version Wed, 14 Feb 2024 08:30:39 GMT   (790kb,D)

Title: Deep Reinforcement Learning Guided Improvement Heuristic for Job Shop
  Scheduling
Authors: Cong Zhang, Zhiguang Cao, Wen Song, Yaoxin Wu, Jie Zhang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2211.10936 ,  790kb)
------------------------------------------------------------------------------
\\
arXiv:2301.07530
replaced with revised version Wed, 14 Feb 2024 15:16:47 GMT   (1744kb,D)

Title: Optimistically Tempered Online Learning
Authors: Maxime Haddouche and Olivier Wintenberger and Benjamin Guedj
Categories: cs.LG math.OC stat.ML
\\ ( https://arxiv.org/abs/2301.07530 ,  1744kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13289
replaced with revised version Wed, 14 Feb 2024 17:06:49 GMT   (88kb,D)

Title: On the Statistical Benefits of Temporal Difference Learning
Authors: David Cheikhi and Daniel Russo
Categories: cs.LG stat.ML
Comments: 26 pages, 7 figures, submitted to ICML 2023
\\ ( https://arxiv.org/abs/2301.13289 ,  88kb)
------------------------------------------------------------------------------
\\
arXiv:2303.02186
replaced with revised version Wed, 14 Feb 2024 10:30:13 GMT   (75kb)

Title: Causal Deep Learning
Authors: Jeroen Berrevoets, Krzysztof Kacprzyk, Zhaozhi Qian, Mihaela van der
  Schaar
Categories: cs.LG cs.AI
Comments: arXiv admin note: substantial text overlap with arXiv:2212.00911
\\ ( https://arxiv.org/abs/2303.02186 ,  75kb)
------------------------------------------------------------------------------
\\
arXiv:2303.10165
replaced with revised version Wed, 14 Feb 2024 17:44:08 GMT   (71kb)

Title: Optimal Horizon-Free Reward-Free Exploration for Linear Mixture MDPs
Authors: Junkai Zhang and Weitong Zhang and Quanquan Gu
Categories: cs.LG math.OC stat.ML
Comments: 37 pages, 1 figure, 2 tables. In ICML 2023
\\ ( https://arxiv.org/abs/2303.10165 ,  71kb)
------------------------------------------------------------------------------
\\
arXiv:2303.16266
replaced with revised version Wed, 14 Feb 2024 15:34:10 GMT   (1125kb,D)

Title: On-line reinforcement learning for optimization of real-life energy
  trading strategy
Authors: {\L}ukasz Lepak and Pawe{\l} Wawrzy\'nski
Categories: cs.LG q-fin.TR
\\ ( https://arxiv.org/abs/2303.16266 ,  1125kb)
------------------------------------------------------------------------------
\\
arXiv:2304.06011
replaced with revised version Tue, 13 Feb 2024 19:50:54 GMT   (2324kb)

Title: MABL: Bi-Level Latent-Variable World Model for Sample-Efficient
  Multi-Agent Reinforcement Learning
Authors: Aravind Venugopal, Stephanie Milani, Fei Fang, Balaraman Ravindran
Categories: cs.LG cs.MA
Comments: 9 pages
\\ ( https://arxiv.org/abs/2304.06011 ,  2324kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12131
replaced with revised version Wed, 14 Feb 2024 12:22:06 GMT   (27kb,D)

Title: Non-stationary Online Convex Optimization with Arbitrary Delays
Authors: Yuanyu Wan and Chang Yao and Mingli Song and Lijun Zhang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2305.12131 ,  27kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19685
replaced with revised version Wed, 14 Feb 2024 17:48:39 GMT   (4846kb,D)

Title: Deep Stochastic Mechanics
Authors: Elena Orlova, Aleksei Ustimenko, Ruoxi Jiang, Peter Y. Lu, Rebecca
  Willett
Categories: cs.LG quant-ph stat.ML
\\ ( https://arxiv.org/abs/2305.19685 ,  4846kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00740
replaced with revised version Tue, 13 Feb 2024 22:59:13 GMT   (1430kb,D)

Title: On the Limitations of Temperature Scaling for Distributions with
  Overlaps
Authors: Muthu Chidambaram and Rong Ge
Categories: cs.LG stat.ML
Comments: 27 pages, 9 Figures, published in ICLR 2024
\\ ( https://arxiv.org/abs/2306.00740 ,  1430kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02939
replaced with revised version Wed, 14 Feb 2024 10:08:56 GMT   (157kb,D)

Title: Improved Stability and Generalization Guarantees of the Decentralized
  SGD Algorithm
Authors: Batiste Le Bars, Aur\'elien Bellet, Marc Tommasi, Kevin Scaman,
  Giovanni Neglia
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2306.02939 ,  157kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04828
replaced with revised version Wed, 14 Feb 2024 16:45:47 GMT   (3555kb,D)

Title: Fast and Effective GNN Training with Linearized Random Spanning Trees
Authors: Francesco Bonchi, Claudio Gentile, Francesco Paolo Nerini, Andr\'e
  Panisson, Fabio Vitale
Categories: cs.LG
\\ ( https://arxiv.org/abs/2306.04828 ,  3555kb)
------------------------------------------------------------------------------
\\
arXiv:2306.15056
replaced with revised version Wed, 14 Feb 2024 04:36:16 GMT   (3978kb,D)

Title: Optimal Differentially Private Model Training with Public Data
Authors: Andrew Lowy, Zeman Li, Tianjian Huang, Meisam Razaviyayn
Categories: cs.LG cs.CR math.OC stat.ML
Comments: V2 changed the title and added high-dimensional approximate semi-DP
  lower bounds
\\ ( https://arxiv.org/abs/2306.15056 ,  3978kb)
------------------------------------------------------------------------------
\\
arXiv:2307.05432
replaced with revised version Wed, 14 Feb 2024 14:59:38 GMT   (10305kb,D)

Title: Self-Supervised Learning with Lie Symmetries for Partial Differential
  Equations
Authors: Gr\'egoire Mialon, Quentin Garrido, Hannah Lawrence, Danyal Rehman,
  Yann LeCun, Bobak T. Kiani
Categories: cs.LG cs.NA math.NA
Comments: NeurIPS 2023
\\ ( https://arxiv.org/abs/2307.05432 ,  10305kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01937
replaced with revised version Tue, 13 Feb 2024 19:44:59 GMT   (26646kb,D)

Title: Training Data Protection with Compositional Diffusion Models
Authors: Aditya Golatkar, Alessandro Achille, Ashwin Swaminathan, Stefano
  Soatto
Categories: cs.LG cs.AI cs.CR cs.CV
\\ ( https://arxiv.org/abs/2308.01937 ,  26646kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15732
replaced with revised version Wed, 14 Feb 2024 13:04:57 GMT   (2929kb)

Title: Deep Learning-based Analysis of Basins of Attraction
Authors: David Valle, Alexandre Wagemakers, Miguel A.F. Sanju\'an
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.15732 ,  2929kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16597
replaced with revised version Tue, 13 Feb 2024 22:44:25 GMT   (1626kb)

Title: Transfer Learning for Bayesian Optimization on Heterogeneous Search
  Spaces
Authors: Zhou Fan, Xinran Han, Zi Wang
Categories: cs.LG cs.AI stat.ML
Journal-ref: Transactions on Machine Learning Research (TMLR), February 2024
\\ ( https://arxiv.org/abs/2309.16597 ,  1626kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06514
replaced with revised version Wed, 14 Feb 2024 05:10:30 GMT   (8557kb,D)

Title: AttributionLab: Faithfulness of Feature Attribution Under Controllable
  Environments
Authors: Yang Zhang, Yawei Li, Hannah Brown, Mina Rezaei, Bernd Bischl, Philip
  Torr, Ashkan Khakzar, Kenji Kawaguchi
Categories: cs.LG
Comments: Appear at NeurIPS 2023 Workshop XAIA
\\ ( https://arxiv.org/abs/2310.06514 ,  8557kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09639
replaced with revised version Wed, 14 Feb 2024 09:42:42 GMT   (3420kb,D)

Title: DPZero: Private Fine-Tuning of Language Models without Backpropagation
Authors: Liang Zhang, Bingcong Li, Kiran Koshy Thekumparampil, Sewoong Oh, Niao
  He
Categories: cs.LG cs.CR math.OC stat.ML
\\ ( https://arxiv.org/abs/2310.09639 ,  3420kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12294
replaced with revised version Tue, 13 Feb 2024 19:33:13 GMT   (759kb,D)

Title: Open-Set Multivariate Time-Series Anomaly Detection
Authors: Thomas Lai, Thi Kieu Khanh Ho, Narges Armanfard
Categories: cs.LG
Comments: 12 pages, 7 tables, 2 figures
\\ ( https://arxiv.org/abs/2310.12294 ,  759kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14968
replaced with revised version Tue, 13 Feb 2024 21:25:21 GMT   (642kb,D)

Title: Bayesian Active Learning in the Presence of Nuisance Parameters
Authors: Sabina J. Sloman, Ayush Bharti, Julien Martinelli, and Samuel Kaski
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.14968 ,  642kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15450
replaced with revised version Wed, 14 Feb 2024 16:37:05 GMT   (63kb,D)

Title: General Identifiability and Achievability for Causal Representation
  Learning
Authors: Burak Var{\i}c{\i}, Emre Acart\"urk, Karthikeyan Shanmugam, Ali Tajer
Categories: cs.LG stat.ML
Comments: Accepted to AISTATS 2024 (oral presentation). Also appeared at CRL
  Workshop @ NeurIPS 2023 (oral presentation) titled as "Score-based Causal
  Representation Learning: Nonparametric Identifiability"
\\ ( https://arxiv.org/abs/2310.15450 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19668
replaced with revised version Wed, 14 Feb 2024 03:56:25 GMT   (3582kb,D)

Title: DrM: Mastering Visual Reinforcement Learning through Dormant Ratio
  Minimization
Authors: Guowei Xu, Ruijie Zheng, Yongyuan Liang, Xiyao Wang, Zhecheng Yuan,
  Tianying Ji, Yu Luo, Xiaoyu Liu, Jiaxin Yuan, Pu Hua, Shuzhen Li, Yanjie Ze,
  Hal Daum\'e III, Furong Huang, Huazhe Xu
Categories: cs.LG cs.CV
Comments: Accepted at The Twelfth International Conference on Learning
  Representations (ICLR 2024)
\\ ( https://arxiv.org/abs/2310.19668 ,  3582kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02333
replaced with revised version Wed, 14 Feb 2024 03:48:41 GMT   (4639kb,D)

Title: Understanding the Natural Language of DNA using Encoder-Decoder
  Foundation Models with Byte-level Precision
Authors: Aditya Malusare and Harish Kothandaraman and Dipesh Tamboli and Nadia
  A. Lanman and Vaneet Aggarwal
Categories: cs.LG q-bio.GN
Comments: 9 pages
\\ ( https://arxiv.org/abs/2311.02333 ,  4639kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02357
replaced with revised version Wed, 14 Feb 2024 09:48:39 GMT   (550kb,D)

Title: Contrastive Deep Nonnegative Matrix Factorization for Community
  Detection
Authors: Yuecheng Li, Jialong Chen, Chuan Chen, Lei Yang, Zibin Zheng
Categories: cs.LG cs.AI cs.SI
Comments: In Proceedings of the IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP) 2024. Source Code available at:
  https://github.com/6lyc/CDNMF.git
\\ ( https://arxiv.org/abs/2311.02357 ,  550kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06668
replaced with revised version Tue, 13 Feb 2024 22:37:39 GMT   (667kb,D)

Title: In-context Vectors: Making In Context Learning More Effective and
  Controllable Through Latent Space Steering
Authors: Sheng Liu, Haotian Ye, Lei Xing, James Zou
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2311.06668 ,  667kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07454
replaced with revised version Tue, 13 Feb 2024 22:42:02 GMT   (168kb,D)

Title: Discrete Nonparametric Causal Discovery Under Latent Class Confounding
Authors: Bijan Mazaheri, Spencer Gordon, Yuval Rabani, Leonard Schulman
Categories: cs.LG cs.CC math.ST stat.TH
\\ ( https://arxiv.org/abs/2311.07454 ,  168kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11342
replaced with revised version Wed, 14 Feb 2024 16:08:10 GMT   (149kb,D)

Title: On the Communication Complexity of Decentralized Bilevel Optimization
Authors: Yihan Zhang, My T. Thai, Jie Wu, Hongchang Gao
Categories: cs.LG cs.DC math.OC
\\ ( https://arxiv.org/abs/2311.11342 ,  149kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16856
replaced with revised version Wed, 14 Feb 2024 11:21:32 GMT   (583kb,D)

Title: Attentional Graph Neural Networks for Robust Massive Network
  Localization
Authors: Wenzhong Yan, Juntao Wang, Feng Yin, Yang Tian, Abdelhak M. Zoubir
Categories: cs.LG eess.SP stat.ML
\\ ( https://arxiv.org/abs/2311.16856 ,  583kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12276
replaced with revised version Wed, 14 Feb 2024 02:24:04 GMT   (2110kb,D)

Title: POND: Multi-Source Time Series Domain Adaptation with Information-Aware
  Prompt Tuning
Authors: Junxiang Wang, Guangji Bai, Wei Cheng, Zhengzhang Chen, Liang Zhao,
  and Haifeng Chen
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.12276 ,  2110kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14440
replaced with revised version Wed, 14 Feb 2024 18:09:58 GMT   (8521kb,D)

Title: Asymmetric Bias in Text-to-Image Generation with Adversarial Attacks
Authors: Haz Sameen Shahgir, Xianghao Kong, Greg Ver Steeg, Yue Dong
Categories: cs.LG cs.CR
Comments: preprint version
\\ ( https://arxiv.org/abs/2312.14440 ,  8521kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04857
replaced with revised version Wed, 14 Feb 2024 17:14:50 GMT   (622kb,D)

Title: Transportation Marketplace Rate Forecast Using Signature Transform
Authors: Haotian Gu, Xin Guo, Timothy L. Jacobs, Philip Kaminsky, Xinyu Li
Categories: cs.LG stat.AP
\\ ( https://arxiv.org/abs/2401.04857 ,  622kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00522
replaced with revised version Wed, 14 Feb 2024 05:17:31 GMT   (96kb)

Title: Understanding the Expressive Power and Mechanisms of Transformer for
  Sequence Modeling
Authors: Mingze Wang, Weinan E
Categories: cs.LG stat.ML
Comments: 65 pages
\\ ( https://arxiv.org/abs/2402.00522 ,  96kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00910
replaced with revised version Tue, 13 Feb 2024 18:54:59 GMT   (401kb,D)

Title: Addressing Bias Through Ensemble Learning and Regularized Fine-Tuning
Authors: Ahmed Radwan, Layan Zaafarani, Jetana Abudawood, Faisal AlZahrani,
  Fares Fourati
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.00910 ,  401kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01207
replaced with revised version Tue, 13 Feb 2024 22:27:00 GMT   (168kb,D)

Title: Efficient Causal Graph Discovery Using Large Language Models
Authors: Thomas Jiralerspong, Xiaoyin Chen, Yash More, Vedant Shah, Yoshua
  Bengio
Categories: cs.LG cs.AI stat.ME
\\ ( https://arxiv.org/abs/2402.01207 ,  168kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03966
replaced with revised version Wed, 14 Feb 2024 09:13:30 GMT   (2698kb,D)

Title: On dimensionality of feature vectors in MPNNs
Authors: C\'esar Bravo, Alexander Kozachinskiy, Crist\'obal Rojas
Categories: cs.LG
Comments: 15 pages, 2 figures. Changes to the previous version: added reference
  to Amir et al.~(NeurIPS'23)
\\ ( https://arxiv.org/abs/2402.03966 ,  2698kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04362
replaced with revised version Tue, 13 Feb 2024 23:54:45 GMT   (2222kb,D)

Title: Neural Networks Learn Statistics of Increasing Complexity
Authors: Nora Belrose, Quintin Pope, Lucia Quirke, Alex Mallen, Xiaoli Fern
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.04362 ,  2222kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05050
replaced with revised version Wed, 14 Feb 2024 11:22:57 GMT   (462kb,D)

Title: Federated Learning Can Find Friends That Are Beneficial
Authors: Nazarii Tupitsa and Samuel Horv\'ath and Martin Tak\'a\v{c} and Eduard
  Gorbunov
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2402.05050 ,  462kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06187
replaced with revised version Tue, 13 Feb 2024 21:19:04 GMT   (8614kb,D)

Title: Premier-TACO is a Few-Shot Policy Learner: Pretraining Multitask
  Representation via Temporal Action-Driven Contrastive Loss
Authors: Ruijie Zheng, Yongyuan Liang, Xiyao Wang, Shuang Ma, Hal Daum\'e III,
  Huazhe Xu, John Langford, Praveen Palanisamy, Kalyan Shankar Basu, Furong
  Huang
Categories: cs.LG cs.AI cs.RO
\\ ( https://arxiv.org/abs/2402.06187 ,  8614kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06819
replaced with revised version Tue, 13 Feb 2024 19:49:04 GMT   (10959kb,D)

Title: Monitored Markov Decision Processes
Authors: Simone Parisi, Montaser Mohammedalamen, Alireza Kazemipour, Matthew E.
  Taylor, Michael Bowling
Categories: cs.LG
Comments: AAMAS 2024, Main Track
\\ ( https://arxiv.org/abs/2402.06819 ,  10959kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07703
replaced with revised version Wed, 14 Feb 2024 06:16:25 GMT   (4417kb,D)

Title: Online Sequential Decision-Making with Unknown Delays
Authors: Ping Wu and Heyan Huang and Zhengyang Liu
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.07703 ,  4417kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08132
replaced with revised version Wed, 14 Feb 2024 13:04:28 GMT   (76kb)

Title: On the Resurgence of Recurrent Models for Long Sequences -- Survey and
  Research Opportunities in the Transformer Era
Authors: Matteo Tiezzi, Michele Casoni, Alessandro Betti, Tommaso Guidi, Marco
  Gori and Stefano Melacci
Categories: cs.LG
Comments: Under review
\\ ( https://arxiv.org/abs/2402.08132 ,  76kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08228
replaced with revised version Wed, 14 Feb 2024 16:26:09 GMT   (1662kb,D)

Title: Investigating Out-of-Distribution Generalization of GNNs: An
  Architecture Perspective
Authors: Kai Guo, Hongzhi Wen, Wei Jin, Yaming Guo, Jiliang Tang, Yi Chang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.08228 ,  1662kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08309
replaced with revised version Wed, 14 Feb 2024 08:10:38 GMT   (4048kb,D)

Title: Prompted Contextual Vectors for Spear-Phishing Detection
Authors: Daniel Nahmias, Gal Engelberg, Dan Klein, Asaf Shabtai
Categories: cs.LG cs.CL cs.CR
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2402.08309 ,  4048kb)
------------------------------------------------------------------------------
\\
arXiv:2212.00214
replaced with revised version Wed, 14 Feb 2024 09:32:35 GMT   (6795kb,D)

Title: Test-Time Mixup Augmentation for Data and Class-Specific Uncertainty
  Estimation in Deep Learning Image Classification
Authors: Hansang Lee, Haeil Lee, Helen Hong, and Junmo Kim
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2212.00214 ,  6795kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13763 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 20:52:00 GMT   (9180kb,D)

Title: Sobolev Space Regularised Pre Density Models
Authors: Mark Kozdoba, Binyamin Perets, Shie Mannor
Categories: stat.ML cs.AI cs.LG
\\ ( https://arxiv.org/abs/2307.13763 ,  9180kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14309
replaced with revised version Tue, 13 Feb 2024 22:38:30 GMT   (5901kb,D)

Title: Multiple Different Black Box Explanations for Image Classifiers
Authors: Hana Chockler, David A. Kelly, Daniel Kroening
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2309.14309 ,  5901kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15274
replaced with revised version Wed, 14 Feb 2024 17:11:55 GMT   (11704kb,D)

Title: Memory-Efficient Continual Learning Object Segmentation for Long Video
Authors: Amir Nazemi, Mohammad Javad Shafiee, Zahra Gharaee, Paul Fieguth
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2309.15274 ,  11704kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16779
replaced with revised version Wed, 14 Feb 2024 17:54:04 GMT   (8295kb,D)

Title: Intriguing properties of generative classifiers
Authors: Priyank Jaini and Kevin Clark and Robert Geirhos
Categories: cs.CV cs.AI cs.LG q-bio.NC stat.ML
Comments: ICLR 2024 Spotlight
\\ ( https://arxiv.org/abs/2309.16779 ,  8295kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17534
replaced with revised version Wed, 14 Feb 2024 13:56:37 GMT   (2853kb,D)

Title: SoK: Pitfalls in Evaluating Black-Box Attacks
Authors: Fnu Suya, Anshuman Suri, Tingwei Zhang, Jingtao Hong, Yuan Tian, David
  Evans
Categories: cs.CR cs.AI cs.CV cs.LG
Comments: Accepted at SaTML 2024
\\ ( https://arxiv.org/abs/2310.17534 ,  2853kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00800
replaced with revised version Wed, 14 Feb 2024 15:41:08 GMT   (737kb)

Title: Beyond still images: Temporal features and input variance resilience
Authors: Amir Hosein Fadaei, Mohammad-Reza A. Dehaqani
Categories: cs.CV cs.AI
Comments: 13 pages, 9 figures
ACM-class: I.2.10; I.5.1; I.4.8
\\ ( https://arxiv.org/abs/2311.00800 ,  737kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07601
replaced with revised version Wed, 14 Feb 2024 00:02:57 GMT   (4487kb,D)

Title: Online Advertisements with LLMs: Opportunities and Challenges
Authors: Soheil Feizi, MohammadTaghi Hajiaghayi, Keivan Rezaei, Suho Shin
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2311.07601 ,  4487kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18138
replaced with revised version Tue, 13 Feb 2024 22:31:50 GMT   (557kb,D)

Title: Algorithmic Persuasion Through Simulation
Authors: Keegan Harris, Nicole Immorlica, Brendan Lucier, Aleksandrs Slivkins
Categories: cs.GT cs.AI econ.TH
\\ ( https://arxiv.org/abs/2311.18138 ,  557kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10637
replaced with revised version Wed, 14 Feb 2024 03:49:50 GMT   (11406kb,D)

Title: An Evaluation of GPT-4V and Gemini in Online VQA
Authors: Mengchen Liu, Chongyan Chen, Danna Gurari
Categories: cs.CV cs.AI
Comments: 20 pages
\\ ( https://arxiv.org/abs/2312.10637 ,  11406kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00608
replaced with revised version Wed, 14 Feb 2024 03:43:08 GMT   (518kb,D)

Title: Bringing Back the Context: Camera Trap Species Identification as Link
  Prediction on Multimodal Knowledge Graphs
Authors: Vardaan Pahuja, Weidi Luo, Yu Gu, Cheng-Hao Tu, Hong-You Chen, Tanya
  Berger-Wolf, Charles Stewart, Song Gao, Wei-Lun Chao, Yu Su
Categories: cs.CV cs.AI
Comments: 13 pages, 5 figures
\\ ( https://arxiv.org/abs/2401.00608 ,  518kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08097
replaced with revised version Wed, 14 Feb 2024 03:33:50 GMT   (2895kb,D)

Title: A Study of Fairness Concerns in AI-based Mobile App Reviews
Authors: Ali Rezaei Nasab, Maedeh Dashti, Mojtaba Shahin, Mansooreh Zahedi,
  Hourieh Khalajzadeh, Chetan Arora, Peng Liang
Categories: cs.SE cs.AI cs.CY
Comments: 25 pages, 4 images, 2 tables, Manuscript submitted to a Journal
  (2024)
\\ ( https://arxiv.org/abs/2401.08097 ,  2895kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12513
replaced with revised version Wed, 14 Feb 2024 01:40:52 GMT   (1979kb,D)

Title: Detecting and recognizing characters in Greek papyri with YOLOv8, DeiT
  and SimCLR
Authors: Robert Turnbull and Evelyn Mannix
Categories: cs.CV cs.AI
MSC-class: 68T10
\\ ( https://arxiv.org/abs/2401.12513 ,  1979kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01721
replaced with revised version Tue, 13 Feb 2024 22:26:23 GMT   (2509kb,D)

Title: Non-Consensual Synthetic Intimate Imagery: Prevalence, Attitudes, and
  Knowledge in 10 Countries
Authors: Rebecca Umbach, Nicola Henry, Gemma Beard, Colleen Berryessa
Categories: cs.CY cs.AI cs.HC
\\ ( https://arxiv.org/abs/2402.01721 ,  2509kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03781 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 16:39:46 GMT   (7849kb,D)

Title: MolTC: Towards Molecular Relational Modeling In Language Models
Authors: Junfeng Fang, Shuai Zhang, Chang Wu, Zhengyi Yang, Zhiyuan Liu, Sihang
  Li, Kun Wang, Wenjie Du and Xiang Wang
Categories: q-bio.QM cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.03781 ,  7849kb)
------------------------------------------------------------------------------
\\
arXiv:2207.01390
replaced with revised version Wed, 14 Feb 2024 11:03:38 GMT   (21135kb,D)

Title: FakeNews: GAN-based generation of realistic 3D volumetric data -- A
  systematic review and taxonomy
Authors: Andr\'e Ferreira, Jianning Li, Kelsey L. Pomykala, Jens Kleesiek,
  Victor Alves, Jan Egger
Categories: cs.CV cs.DB cs.LG
Comments: 88 pages
Journal-ref: Medical Image Analysis, 103100 (2024)
DOI: 10.1016/j.media.2024.103100
\\ ( https://arxiv.org/abs/2207.01390 ,  21135kb)
------------------------------------------------------------------------------
\\
arXiv:2208.13504
replaced with revised version Wed, 14 Feb 2024 15:18:40 GMT   (8479kb,D)

Title: Large-scale unsupervised spatio-temporal semantic analysis of vast
  regions from satellite images sequences
Authors: Carlos Echegoyen, Aritz P\'erez, Guzm\'an Santaf\'e, Unai P\'erez-Goya
  and Mar\'ia Dolores Ugarte
Categories: cs.CV cs.LG
Journal-ref: Statistics and Computing, Volume 34, article number 71, (2024)
DOI: 10.1007/s11222-024-10383-y
\\ ( https://arxiv.org/abs/2208.13504 ,  8479kb)
------------------------------------------------------------------------------
\\
arXiv:2210.09974 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 00:03:32 GMT   (642kb,D)

Title: Theoretical Guarantees for Permutation-Equivariant Quantum Neural
  Networks
Authors: Louis Schatzki, Martin Larocca, Quynh T. Nguyen, Frederic Sauvage, M.
  Cerezo
Categories: quant-ph cs.LG stat.ML
Comments: 15+21 pages, 5 + 5 figures. Prior generalization bounds replaced with
  more general theorem. Comments added about hardness of simulation and narrow
  gorges
Report-no: LA-UR-22-29899
Journal-ref: npj Quantum Inf 10, 12 (2024)
DOI: 10.1038/s41534-024-00804-1
\\ ( https://arxiv.org/abs/2210.09974 ,  642kb)
------------------------------------------------------------------------------
\\
arXiv:2211.01595
replaced with revised version Tue, 13 Feb 2024 19:32:43 GMT   (529kb,D)

Title: Reinforcement Learning in Non-Markovian Environments
Authors: Siddharth Chandak, Pratik Shah, Vivek S Borkar, Parth Dodhia
Categories: eess.SY cs.LG cs.SY
Comments: 19 pages, accepted for publication at Systems and Control Letters
\\ ( https://arxiv.org/abs/2211.01595 ,  529kb)
------------------------------------------------------------------------------
\\
arXiv:2211.11700 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 15:03:18 GMT   (1858kb,D)

Title: High-Dimensional Undirected Graphical Models for Arbitrary Mixed Data
Authors: Konstantin G\"obler and Anne Miloschewski and Mathias Drton and Sach
  Mukherjee
Categories: stat.ML cs.LG stat.ME
Comments: 20 pages, 2 Figures
MSC-class: 62H22
\\ ( https://arxiv.org/abs/2211.11700 ,  1858kb)
------------------------------------------------------------------------------
\\
arXiv:2211.15118
replaced with revised version Tue, 13 Feb 2024 19:39:48 GMT   (125kb,D)

Title: A Faster $k$-means++ Algorithm
Authors: Jiehao Liang, Somdeb Sarkhel, Zhao Song, Chenbo Yin, Junze Yin,
  Danyang Zhuo
Categories: cs.DS cs.LG
\\ ( https://arxiv.org/abs/2211.15118 ,  125kb)
------------------------------------------------------------------------------
\\
arXiv:2303.05739
replaced with revised version Wed, 14 Feb 2024 10:22:25 GMT   (9539kb,D)

Title: LEDetection: A Simple Framework for Semi-Supervised Few-Shot Object
  Detection
Authors: Phi Vu Tran
Categories: cs.CV cs.LG
Comments: AISTATS 2024. The code is available at
  https://github.com/lexisnexis-risk-open-source/ledetection
\\ ( https://arxiv.org/abs/2303.05739 ,  9539kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11766 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 08:31:50 GMT   (1318kb,D)

Title: Transfer operators on graphs: Spectral clustering and beyond
Authors: Stefan Klus, Maia Trower
Categories: stat.ML cs.LG cs.SI math.DS
DOI: 10.1088/2632-072X/ad28fe
\\ ( https://arxiv.org/abs/2305.11766 ,  1318kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11934 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 20:09:47 GMT   (15314kb,D)

Title: Inductive Simulation of Calorimeter Showers with Normalizing Flows
Authors: Matthew R. Buckley, Claudius Krause, Ian Pang, David Shih
Categories: physics.ins-det cs.LG hep-ex hep-ph physics.data-an
Comments: 19 pages, 15 figures; v2: title changed, matches published version
Journal-ref: Phys. Rev. D 109, 033006 (2024)
DOI: 10.1103/PhysRevD.109.033006
\\ ( https://arxiv.org/abs/2305.11934 ,  15314kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12569 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 18:42:30 GMT   (18638kb,D)

Title: Conditional Generative Modeling for High-dimensional Marked Temporal
  Point Processes
Authors: Zheng Dong, Zekai Fan, Shixiang Zhu
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2305.12569 ,  18638kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15742 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 16:12:55 GMT   (9593kb,D)

Title: Counterfactual Generative Models for Time-Varying Treatments
Authors: Shenghao Wu, Wenbin Zhou, Minshuo Chen, Shixiang Zhu
Categories: stat.ML cs.LG stat.ME
\\ ( https://arxiv.org/abs/2305.15742 ,  9593kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17033 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 13:39:45 GMT   (5964kb,D)

Title: The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics
  (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs)
Authors: Anahita Fathi Kazerooni, Nastaran Khalili, Xinyang Liu, Debanjan
  Haldar, Zhifan Jiang, Syed Muhammed Anwar, Jake Albrecht, Maruf Adewole,
  Udunna Anazodo, Hannah Anderson, Sina Bagheri, Ujjwal Baid, Timothy
  Bergquist, Austin J. Borja, Evan Calabrese, Verena Chung, Gian-Marco Conte,
  Farouk Dako, James Eddy, Ivan Ezhov, Ariana Familiar, Keyvan Farahani,
  Shuvanjan Haldar, Juan Eugenio Iglesias, Anastasia Janas, Elaine Johansen,
  Blaise V Jones, Florian Kofler, Dominic LaBella, Hollie Anne Lai, Koen Van
  Leemput, Hongwei Bran Li, Nazanin Maleki, Aaron S McAllister, Zeke Meier,
  Bjoern Menze, Ahmed W Moawad, Khanak K Nandolia, Julija Pavaine, Marie
  Piraud, Tina Poussaint, Sanjay P Prabhu, Zachary Reitman, Andres Rodriguez,
  Jeffrey D Rudie, Ibraheem Salman Shaikh, Lubdha M. Shah, Nakul Sheth, Russel
  Taki Shinohara, et al. (23 additional authors not shown)
Categories: eess.IV cs.CV cs.LG q-bio.QM
\\ ( https://arxiv.org/abs/2305.17033 ,  5964kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18484 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 13:22:08 GMT   (38464kb,D)

Title: Neural Fourier Transform: A General Approach to Equivariant
  Representation Learning
Authors: Masanori Koyama and Kenji Fukumizu and Kohei Hayashi and Takeru Miyato
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2305.18484 ,  38464kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18671 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 22:44:31 GMT   (26273kb,D)

Title: Perturbation-Assisted Sample Synthesis: A Novel Approach for Uncertainty
  Quantification
Authors: Yifei Liu, Rex Shen, and Xiaotong Shen
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2305.18671 ,  26273kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02775 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 13:24:27 GMT   (761kb,D)

Title: Input-gradient space particle inference for neural network ensembles
Authors: Trung Trinh, Markus Heinonen, Luigi Acerbi, Samuel Kaski
Categories: stat.ML cs.LG
Comments: Published at ICLR 2024 (spotlight presentation)
\\ ( https://arxiv.org/abs/2306.02775 ,  761kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02895
replaced with revised version Wed, 14 Feb 2024 13:46:01 GMT   (875kb,D)

Title: Evading Black-box Classifiers Without Breaking Eggs
Authors: Edoardo Debenedetti, Nicholas Carlini and Florian Tram\`er
Categories: cs.CR cs.LG stat.ML
Comments: Code at https://github.com/ethz-privsec/realistic-adv-examples.
  Accepted at IEEE SaTML 2024
\\ ( https://arxiv.org/abs/2306.02895 ,  875kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03218 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 17:15:52 GMT   (6402kb,D)

Title: Optimal transport for automatic alignment of untargeted metabolomic data
Authors: Marie Breeur, George Stepaniants, Pekka Keski-Rahkonen, Philippe
  Rigollet, and Vivian Viallon
Categories: q-bio.QM cs.LG
Comments: 47 pages, 16 figures
MSC-class: 49Q22, 92C40
ACM-class: G.3; J.3
\\ ( https://arxiv.org/abs/2306.03218 ,  6402kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10816 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 17:45:54 GMT   (876kb,D)

Title: $\texttt{causalAssembly}$: Generating Realistic Production Data for
  Benchmarking Causal Discovery
Authors: Konstantin G\"obler, Tobias Windisch, Mathias Drton, Tim Pychynski,
  Steffen Sonntag, Martin Roth
Categories: stat.ML cs.LG stat.ME
\\ ( https://arxiv.org/abs/2306.10816 ,  876kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12214 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 08:11:21 GMT   (228kb,D)

Title: More PAC-Bayes bounds: From bounded losses, to losses with general tail
  behaviors, to anytime-validity
Authors: Borja Rodr\'iguez-G\'alvez, Ragnar Thobaben, Mikael Skoglund
Categories: stat.ML cs.LG
Comments: 31 pages: ~13.5 of main text, ~4.5 of references, and ~13 of
  appendices. Sections 2 and 3 are presented as short papers in the "PAC-Bayes
  Meets Interactive Learning" workshop at ICML 2023
\\ ( https://arxiv.org/abs/2306.12214 ,  228kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14291
replaced with revised version Wed, 14 Feb 2024 18:53:03 GMT   (11856kb,D)

Title: Hyp-OW: Exploiting Hierarchical Structure Learning with Hyperbolic
  Distance Enhances Open World Object Detection
Authors: Thang Doan, Xin Li, Sima Behpour, Wenbin He, Liang Gou, Liu Ren
Categories: cs.CV cs.LG
Comments: Accepted at AAAI 2024 || keywords: Open World Object Detection,
  Hyperbolic Distance, Unknown Detection, Deformable Transformers, Hierarchical
  Representation Learning
\\ ( https://arxiv.org/abs/2306.14291 ,  11856kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16717 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 22:46:28 GMT   (3002kb,D)

Title: Understanding Pathologies of Deep Heteroskedastic Regression
Authors: Eliot Wong-Toi, Alex Boyd, Vincent Fortuin, Stephan Mandt
Categories: stat.ML cs.LG
Comments: 20 pages, 8 figures
\\ ( https://arxiv.org/abs/2306.16717 ,  3002kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06556 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 16:25:59 GMT   (6775kb)

Title: Metal Oxide-based Gas Sensor Array for the VOCs Analysis in Complex
  Mixtures using Machine Learning
Authors: Shivam Singh, Sajana S, Poornima, Gajje Sreelekha, Chandranath Adak,
  Rajendra P. Shukla and Vinayak Kamble
Categories: physics.app-ph cond-mat.mtrl-sci cs.LG
\\ ( https://arxiv.org/abs/2307.06556 ,  6775kb)
------------------------------------------------------------------------------
\\
arXiv:2309.03780 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 15:10:57 GMT   (1275kb,D)

Title: Reduced Simulations for High-Energy Physics, a Middle Ground for
  Data-Driven Physics Research
Authors: Uraz Odyurt, Stephen Nicholas Swatman, Ana-Lucia Varbanescu, Sascha
  Caron
Categories: hep-ex cs.LG
\\ ( https://arxiv.org/abs/2309.03780 ,  1275kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02075 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 12:29:49 GMT   (251kb,D)

Title: Learning Quantum Processes with Quantum Statistical Queries
Authors: Chirag Wadhwa and Mina Doosti
Categories: quant-ph cs.CC cs.LG
Comments: 45 pages, 3 figures. v2: Added lower bounds (Section 6)
\\ ( https://arxiv.org/abs/2310.02075 ,  251kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04884
replaced with revised version Wed, 14 Feb 2024 00:07:57 GMT   (69kb)

Title: Regret Analysis of Repeated Delegated Choice
Authors: MohammadTaghi Hajiaghayi, Mohammad Mahdavi, Keivan Rezaei, Suho Shin
Categories: cs.GT cs.LG
\\ ( https://arxiv.org/abs/2310.04884 ,  69kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08748
replaced with revised version Wed, 14 Feb 2024 12:28:18 GMT   (6259kb)

Title: Evolutionary Dynamic Optimization and Machine Learning
Authors: Abdennour Boulesnane
Categories: cs.NE cs.LG
Comments: This is a preprint of the following chapter: Abdennour Boulesnane,
  Evolutionary Dynamic Optimization and Machine Learning, published in Advanced
  Machine Learning with Evolutionary and Metaheuristic Techniques,
  Computational Intelligence Methods and Applications, edited by J. Valadi et
  al. (eds.),2024, Springer Nature
DOI: 10.1007/978-981-99-9718-3_3
\\ ( https://arxiv.org/abs/2310.08748 ,  6259kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09196
replaced with revised version Wed, 14 Feb 2024 09:23:48 GMT   (426kb,D)

Title: A 4-approximation algorithm for min max correlation clustering
Authors: Holger Heidrich, Jannik Irmai, Bjoern Andres
Categories: cs.DS cs.DM cs.LG
Comments: AISTATS 2024; 10 pages
\\ ( https://arxiv.org/abs/2310.09196 ,  426kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12115 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 13:56:50 GMT   (835kb,D)

Title: MMD-based Variable Importance for Distributional Random Forest
Authors: Cl\'ement B\'enard and Jeffrey N\"af and Julie Josse
Categories: stat.ML cs.LG stat.ME
\\ ( https://arxiv.org/abs/2310.12115 ,  835kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02203 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 22:18:36 GMT   (8943kb,D)

Title: Learning High-Order Relationships of Brain Regions
Authors: Weikang Qiu, Huangrui Chu, Selena Wang, Haolan Zuo, Xiaoxiao Li, Yize
  Zhao, Rex Ying
Categories: q-bio.NC cs.LG
Comments: 20 pages, 11 figures
\\ ( https://arxiv.org/abs/2312.02203 ,  8943kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02544 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 10:13:37 GMT   (88kb)

Title: Characterization of Locality in Spin States and Forced Moves for
  Optimizations
Authors: Yoshiki Sato, Makiko Konoshima, Hirotaka Tamura, Jun Ohkubo
Categories: physics.app-ph cs.LG
Comments: 8 pages, 3 figures
\\ ( https://arxiv.org/abs/2312.02544 ,  88kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16307 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 22:45:01 GMT   (1037kb,D)

Title: Incentive-Aware Synthetic Control: Accurate Counterfactual Estimation
  via Incentivized Exploration
Authors: Daniel Ngo, Keegan Harris, Anish Agarwal, Vasilis Syrgkanis, Zhiwei
  Steven Wu
Categories: econ.EM cs.GT cs.LG stat.ME
\\ ( https://arxiv.org/abs/2312.16307 ,  1037kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09339 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 21:35:42 GMT   (609kb,D)

Title: Central Limit Theorem for Two-Timescale Stochastic Approximation with
  Markovian Noise: Theory and Applications
Authors: Jie Hu, Vishwaraj Doshi, Do Young Eun
Categories: stat.ML cs.LG math.OC
Comments: To appear in AISTATS 2024
\\ ( https://arxiv.org/abs/2401.09339 ,  609kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01274
replaced with revised version Tue, 13 Feb 2024 20:21:04 GMT   (525kb,D)

Title: On the Transferability of Large-Scale Self-Supervision to Few-Shot Audio
  Classification
Authors: Calum Heggan, Sam Budgett, Timothy Hospedales, Mehrdad Yaghoobi
Categories: cs.SD cs.LG eess.AS
Comments: Camera Ready version as submitted to ICASSP SASB Workshop 2024. 5
  pages, 2 figures, 3 tables
\\ ( https://arxiv.org/abs/2402.01274 ,  525kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02162 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 14:25:10 GMT   (1016kb,D)

Title: A Bayesian cluster validity index
Authors: Nathakhun Wiroonsri and Onthada Preedasawakul
Categories: stat.ML cs.LG math.ST stat.TH
Comments: 23 pages
MSC-class: 62H30 (Primary) 62F15, 68T10 (Secondary)
\\ ( https://arxiv.org/abs/2402.02162 ,  1016kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02304 (*cross-listing*)
replaced with revised version Tue, 13 Feb 2024 20:14:28 GMT   (1279kb,D)

Title: Efficient Numerical Wave Propagation Enhanced By An End-to-End Deep
  Learning Model
Authors: Luis Kaiser, Richard Tsai, Christian Klingenberg
Categories: math.AP cs.LG
\\ ( https://arxiv.org/abs/2402.02304 ,  1279kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07440
replaced with revised version Wed, 14 Feb 2024 04:19:16 GMT   (3265kb,D)

Title: Benchmarking and Building Long-Context Retrieval Models with LoCo and
  M2-BERT
Authors: Jon Saad-Falcon, Daniel Y. Fu, Simran Arora, Neel Guha, Christopher
  R\'e
Categories: cs.IR cs.LG
\\ ( https://arxiv.org/abs/2402.07440 ,  3265kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08082 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 17:53:29 GMT   (449kb)

Title: Score-based generative models break the curse of dimensionality in
  learning a family of sub-Gaussian probability distributions
Authors: Frank Cole, Yulong Lu
Categories: stat.ML cs.LG
Comments: 30 pages, to appear in the proceedings of 12th International
  Conference on Learning Representations
\\ ( https://arxiv.org/abs/2402.08082 ,  449kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08095 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 05:30:46 GMT   (401kb)

Title: Convergence Analysis of Discrete Diffusion Model: Exact Implementation
  through Uniformization
Authors: Hongrui Chen, Lexing Ying
Categories: stat.ML cs.LG
Comments: 19 pages
\\ ( https://arxiv.org/abs/2402.08095 ,  401kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
