Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年2月16日 17:24
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Wed 14 Feb 24 19:00:00 GMT  to  Thu 15 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.09413
Date: Sun, 31 Dec 2023 17:07:28 GMT   (17kb)

Title: Mathematical Explanations
Authors: Joseph Y. Halpern
Categories: cs.AI
\\
  A definition of what counts as an explanation of mathematical statement, and
when one explanation is better than another, is given. Since all mathematical
facts must be true in all causal models, and hence known by an agent,
mathematical facts cannot be part of an explanation (under the standard notion
of explanation). This problem is solved using impossible possible worlds.
\\ ( https://arxiv.org/abs/2402.09413 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09498
Date: Wed, 14 Feb 2024 16:45:10 GMT   (1811kb)

Title: Detection of the most influential variables for preventing postpartum
  urinary incontinence using machine learning techniques
Authors: Jos\'e Alberto Ben\'itez-Andrades, Mar\'ia Teresa Garc\'ia-Ord\'as,
  Mar\'ia \'Alvarez-Gonz\'alez, Raquel Leir\'os-Rodr\'iguez and Ana F L\'opez
  Rodr\'iguez
Categories: cs.AI
Journal-ref: Digital Health, Volume 8, 2022, 20552076221111289
DOI: 10.1177/20552076221111289
\\
  Background: Postpartum urinary incontinence (PUI) is a common issue among
postnatal women. Previous studies identified potential related variables, but
lacked analysis on certain intrinsic and extrinsic patient variables during
pregnancy.
  Objective: The study aims to evaluate the most influential variables in PUI
using machine learning, focusing on intrinsic, extrinsic, and combined variable
groups.
  Methods: Data from 93 pregnant women were analyzed using machine learning and
oversampling techniques. Four key variables were predicted: occurrence,
frequency, intensity of urinary incontinence, and stress urinary incontinence.
  Results: Models using extrinsic variables were most accurate, with 70%
accuracy for urinary incontinence, 77% for frequency, 71% for intensity, and
93% for stress urinary incontinence.
  Conclusions: The study highlights extrinsic variables as significant
predictors of PUI issues. This suggests that PUI prevention might be achievable
through healthy habits during pregnancy, although further research is needed
for confirmation.
\\ ( https://arxiv.org/abs/2402.09498 ,  1811kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09500
Date: Wed, 14 Feb 2024 18:59:37 GMT   (61kb)

Title: On Formally Undecidable Traits of Intelligent Machines
Authors: Matthew Fox
Categories: cs.AI cs.LO
Comments: 34 pages
\\
  Building on work by Alfonseca et al. (2021), we study the conditions
necessary for it to be logically possible to prove that an arbitrary
artificially intelligent machine will exhibit certain behavior. To do this, we
develop a formalism like -- but mathematically distinct from -- the theory of
formal languages and their properties. Our formalism affords a precise means
for not only talking about the traits we desire of machines (such as them being
intelligent, contained, moral, and so forth), but also for detailing the
conditions necessary for it to be logically possible to decide whether a given
arbitrary machine possesses such a trait or not. Contrary to Alfonseca et al.'s
(2021) results, we find that Rice's theorem from computability theory cannot in
general be used to determine whether an arbitrary machine possesses a given
trait or not. Therefore, it is not necessarily the case that deciding whether
an arbitrary machine is intelligent, contained, moral, and so forth is
logically impossible.
\\ ( https://arxiv.org/abs/2402.09500 ,  61kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09553
Date: Wed, 14 Feb 2024 20:10:30 GMT   (5931kb,D)

Title: Statistical and Machine Learning Models for Predicting Fire and Other
  Emergency Events
Authors: Dilli Prasad Sharma, Nasim Beigi-Mohammadi, Hongxiang Geng, Dawn
  Dixon, Rob Madro, Phil Emmenegger, Carlos Tobar, Jeff Li, Alberto Leon-Garcia
Categories: cs.AI stat.ML
\\
  Emergency events in a city cause considerable economic loss to individuals,
their families, and the community. Accurate and timely prediction of events can
help the emergency fire and rescue services in preparing for and mitigating the
consequences of emergency events. In this paper, we present a systematic
development of predictive models for various types of emergency events in the
City of Edmonton, Canada. We present methods for (i) data collection and
dataset development; (ii) descriptive analysis of each event type and its
characteristics at different spatiotemporal levels; (iii) feature analysis and
selection based on correlation coefficient analysis and feature importance
analysis; and (iv) development of prediction models for the likelihood of
occurrence of each event type at different temporal and spatial resolutions. We
analyze the association of event types with socioeconomic and demographic data
at the neighborhood level, identify a set of predictors for each event type,
and develop predictive models with negative binomial regression. We conduct
evaluations at neighborhood and fire station service area levels. Our results
show that the models perform well for most of the event types with acceptable
prediction errors for weekly and monthly periods. The evaluation shows that the
prediction accuracy is consistent at the level of the fire station, so the
predictions can be used in management by fire rescue service departments for
planning resource allocation for these time periods. We also examine the impact
of the COVID-19 pandemic on the occurrence of events and on the accuracy of
event predictor models. Our findings show that COVID-19 had a significant
impact on the performance of the event prediction models.
\\ ( https://arxiv.org/abs/2402.09553 ,  5931kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09558
Date: Wed, 14 Feb 2024 20:19:24 GMT   (5066kb,D)

Title: Bidirectional Generative Pre-training for Improving Time Series
  Representation Learning
Authors: Ziyang Song, Qincheng Lu, He Zhu, Yue Li
Categories: cs.AI cs.LG
\\
  Learning time-series representations for discriminative tasks has been a
long-standing challenge. Current pre-training methods are limited in either
unidirectional next-token prediction or randomly masked token prediction. We
propose a novel architecture called Bidirectional Timely Generative Pre-trained
Transformer (BiTimelyGPT), which pre-trains on time-series data by both
next-token and previous-token predictions in alternating transformer layers.
This pre-training task preserves original distribution and data shapes of the
time-series. Additionally, the full-rank forward and backward attention
matrices exhibit more expressive representation capabilities. Using biosignal
data, BiTimelyGPT demonstrates superior performance in predicting neurological
functionality, disease diagnosis, and physiological signs. By visualizing the
attention heatmap, we observe that the pre-trained BiTimelyGPT can identify
discriminative segments from time-series sequences, even more so after
fine-tuning on the task.
\\ ( https://arxiv.org/abs/2402.09558 ,  5066kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09565
Date: Wed, 14 Feb 2024 20:33:11 GMT   (7054kb,D)

Title: Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale
  Graph
Authors: Linfeng Cao, Haoran Deng, Chunping Wang, Lei Chen, Yang Yang
Categories: cs.AI
Comments: 21 pages, 11 figures, In Proceedings of the ACM Web Conference 2024
  (WWW'24)
DOI: 10.1145/3589334.3645452
\\
  Due to the ubiquity of graph data on the web, web graph mining has become a
hot research spot. Nonetheless, the prevalence of large-scale web graphs in
real applications poses significant challenges to storage, computational
capacity and graph model design. Despite numerous studies to enhance the
scalability of graph models, a noticeable gap remains between academic research
and practical web graph mining applications. One major cause is that in most
industrial scenarios, only a small part of nodes in a web graph are actually
required to be analyzed, where we term these nodes as target nodes, while
others as background nodes. In this paper, we argue that properly fetching and
condensing the background nodes from massive web graph data might be a more
economical shortcut to tackle the obstacles fundamentally. To this end, we make
the first attempt to study the problem of massive background nodes compression
for target nodes classification. Through extensive experiments, we reveal two
critical roles played by the background nodes in target node classification:
enhancing structural connectivity between target nodes, and feature correlation
with target nodes. Followingthis, we propose a novel Graph-Skeleton1 model,
which properly fetches the background nodes, and further condenses the semantic
and topological information of background nodes within similar
target-background local structures. Extensive experiments on various web graph
datasets demonstrate the effectiveness and efficiency of the proposed method.
In particular, for MAG240M dataset with 0.24 billion nodes, our generated
skeleton graph achieves highly comparable performance while only containing
1.8% nodes of the original graph.
\\ ( https://arxiv.org/abs/2402.09565 ,  7054kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09584
Date: Wed, 14 Feb 2024 21:19:33 GMT   (1034kb)

Title: Large Language Model-Based Interpretable Machine Learning Control in
  Building Energy Systems
Authors: Liang Zhang, Zhelun Chen
Categories: cs.AI cs.HC
\\
  The potential of Machine Learning Control (MLC) in HVAC systems is hindered
by its opaque nature and inference mechanisms, which is challenging for users
and modelers to fully comprehend, ultimately leading to a lack of trust in
MLC-based decision-making. To address this challenge, this paper investigates
and explores Interpretable Machine Learning (IML), a branch of Machine Learning
(ML) that enhances transparency and understanding of models and their
inferences, to improve the credibility of MLC and its industrial application in
HVAC systems. Specifically, we developed an innovative framework that combines
the principles of Shapley values and the in-context learning feature of Large
Language Models (LLMs). While the Shapley values are instrumental in dissecting
the contributions of various features in ML models, LLM provides an in-depth
understanding of rule-based parts in MLC; combining them, LLM further packages
these insights into a coherent, human-understandable narrative. The paper
presents a case study to demonstrate the feasibility of the developed IML
framework for model predictive control-based precooling under demand response
events in a virtual testbed. The results indicate that the developed framework
generates and explains the control signals in accordance with the rule-based
rationale.
\\ ( https://arxiv.org/abs/2402.09584 ,  1034kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09588
Date: Wed, 14 Feb 2024 21:33:13 GMT   (1250kb,D)

Title: Emerging Opportunities of Using Large Language Language Models for
  Translation Between Drug Molecules and Indications
Authors: David Oniani, Jordan Hilsman, Chengxi Zang, Junmei Wang, Lianjin Cai,
  Jan Zawala, Yanshan Wang
Categories: cs.AI cs.CL
\\
  A drug molecule is a substance that changes the organism's mental or physical
state. Every approved drug has an indication, which refers to the therapeutic
use of that drug for treating a particular medical condition. While the Large
Language Model (LLM), a generative Artificial Intelligence (AI) technique, has
recently demonstrated effectiveness in translating between molecules and their
textual descriptions, there remains a gap in research regarding their
application in facilitating the translation between drug molecules and
indications, or vice versa, which could greatly benefit the drug discovery
process. The capability of generating a drug from a given indication would
allow for the discovery of drugs targeting specific diseases or targets and
ultimately provide patients with better treatments. In this paper, we first
propose a new task, which is the translation between drug molecules and
corresponding indications, and then test existing LLMs on this new task.
Specifically, we consider nine variations of the T5 LLM and evaluate them on
two public datasets obtained from ChEMBL and DrugBank. Our experiments show the
early results of using LLMs for this task and provide a perspective on the
state-of-the-art. We also emphasize the current limitations and discuss future
work that has the potential to improve the performance on this task. The
creation of molecules from indications, or vice versa, will allow for more
efficient targeting of diseases and significantly reduce the cost of drug
discovery, with the potential to revolutionize the field of drug discovery in
the era of generative AI.
\\ ( https://arxiv.org/abs/2402.09588 ,  1250kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09592
Date: Wed, 14 Feb 2024 21:37:59 GMT   (798kb)

Title: A Web-Based Tool for Automatic Data Collection, Curation, and
  Visualization of Complex Healthcare Survey Studies including Social Network
  Analysis
Authors: Jos\'e Alberto Ben\'itez-Andrades, Jos\'e Emilio Labra, Enedina
  Quiroga, Vicente Mart\'in, Isa\'ias Garc\'ia, Pilar Marqu\'es-S\'anchez and
  Carmen Benavides
Categories: cs.AI cs.HC
Journal-ref: Computation and Mathematical Methods in Medicine, Volume 2017,
  Article ID 2579848
DOI: 10.1155/2017/2579848
\\
  There is a great concern nowadays regarding alcohol consumption and drug
abuse, especially in young people. Analyzing the social environment where these
adolescents are immersed, as well as a series of measures determining the
alcohol abuse risk or personal situation and perception using a number of
questionnaires like AUDIT, FAS, KIDSCREEN, and others, it is possible to gain
insight into the current situation of a given individual regarding his/her
consumption behavior. But this analysis, in order to be achieved, requires the
use of tools that can ease the process of questionnaire creation, data
gathering, curation and representation, and later analysis and visualization to
the user. This research presents the design and construction of a web-based
platform able to facilitate each of the mentioned processes by integrating the
different phases into an intuitive system with a graphical user interface that
hides the complexity underlying each of the questionnaires and techniques used
and presenting the results in a flexible and visual way, avoiding any manual
handling of data during the process. Advantages of this approach are shown and
compared to the previous situation where some of the tasks were accomplished by
time consuming and error prone manipulations of data.
\\ ( https://arxiv.org/abs/2402.09592 ,  798kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09617
Date: Wed, 14 Feb 2024 23:12:09 GMT   (5309kb,D)

Title: LLM-Enhanced User-Item Interactions: Leveraging Edge Information for
  Optimized Recommendations
Authors: Xinyuan Wang, Liang Wu, Liangjie Hong, Hao Liu, Yanjie Fu
Categories: cs.AI cs.IR
\\
  The extraordinary performance of large language models has not only reshaped
the research landscape in the field of NLP but has also demonstrated its
exceptional applicative potential in various domains. However, the potential of
these models in mining relationships from graph data remains under-explored.
Graph neural networks, as a popular research area in recent years, have
numerous studies on relationship mining. Yet, current cutting-edge research in
graph neural networks has not been effectively integrated with large language
models, leading to limited efficiency and capability in graph relationship
mining tasks. A primary challenge is the inability of LLMs to deeply exploit
the edge information in graphs, which is critical for understanding complex
node relationships. This gap limits the potential of LLMs to extract meaningful
insights from graph structures, limiting their applicability in more complex
graph-based analysis. We focus on how to utilize existing LLMs for mining and
understanding relationships in graph data, applying these techniques to
recommendation tasks. We propose an innovative framework that combines the
strong contextual representation capabilities of LLMs with the relationship
extraction and analysis functions of GNNs for mining relationships in graph
data. Specifically, we design a new prompt construction framework that
integrates relational information of graph data into natural language
expressions, aiding LLMs in more intuitively grasping the connectivity
information within graph data. Additionally, we introduce graph relationship
understanding and analysis functions into LLMs to enhance their focus on
connectivity information in graph data. Our evaluation on real-world datasets
demonstrates the framework's ability to understand connectivity information in
graph data.
\\ ( https://arxiv.org/abs/2402.09617 ,  5309kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09654
Date: Thu, 15 Feb 2024 01:38:50 GMT   (4861kb,D)

Title: GPT-4's assessment of its performance in a USMLE-based case study
Authors: Uttam Dhakal, Aniket Kumar Singh, Suman Devkota, Yogesh Sapkota,
  Bishal Lamichhane, Suprinsa Paudyal, Chandra Dhakal
Categories: cs.AI cs.CL cs.HC
\\
  This study investigates GPT-4's assessment of its performance in healthcare
applications. A simple prompting technique was used to prompt the LLM with
questions taken from the United States Medical Licensing Examination (USMLE)
questionnaire and it was tasked to evaluate its confidence score before posing
the question and after asking the question. The questionnaire was categorized
into two groups-questions with feedback (WF) and questions with no feedback(NF)
post-question. The model was asked to provide absolute and relative confidence
scores before and after each question. The experimental findings were analyzed
using statistical tools to study the variability of confidence in WF and NF
groups. Additionally, a sequential analysis was conducted to observe the
performance variation for the WF and NF groups. Results indicate that feedback
influences relative confidence but doesn't consistently increase or decrease
it. Understanding the performance of LLM is paramount in exploring its utility
in sensitive areas like healthcare. This study contributes to the ongoing
discourse on the reliability of AI, particularly of LLMs like GPT-4, within
healthcare, offering insights into how feedback mechanisms might be optimized
to enhance AI-assisted medical education and decision support.
\\ ( https://arxiv.org/abs/2402.09654 ,  4861kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09656
Date: Thu, 15 Feb 2024 01:50:38 GMT   (715kb,D)

Title: The Butterfly Effect of Model Editing: Few Edits Can Trigger Large
  Language Models Collapse
Authors: Wanli Yang, Fei Sun, Xinyu Ma, Xun Liu, Dawei Yin, Xueqi Cheng
Categories: cs.AI
\\
  Although model editing has shown promise in revising knowledge in Large
Language Models (LLMs), its impact on the inherent capabilities of LLMs is
often overlooked. In this work, we reveal a critical phenomenon: even a single
edit can trigger model collapse, manifesting as significant performance
degradation in various benchmark tasks. However, benchmarking LLMs after each
edit, while necessary to prevent such collapses, is impractically
time-consuming and resource-intensive. To mitigate this, we propose using
perplexity as a surrogate metric, validated by extensive experiments
demonstrating its strong correlation with downstream task performance. We
further conduct an in-depth study on sequential editing, a practical setting
for real-world scenarios, across various editing methods and LLMs, focusing on
hard cases from our previous single edit studies. The results indicate that
nearly all examined editing methods result in model collapse after only few
edits. To facilitate further research, we have utilized ChatGPT to develop a
new dataset, HardCF, based on those hard cases. This dataset aims to establish
the foundation for pioneering research in reliable model editing and the
mechanisms underlying editing-induced model collapse. We hope this work can
draw the community's attention to the potential risks inherent in model editing
practices.
\\ ( https://arxiv.org/abs/2402.09656 ,  715kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09660
Date: Thu, 15 Feb 2024 02:06:06 GMT   (199kb,D)

Title: User Modeling and User Profiling: A Comprehensive Survey
Authors: Erasmo Purificato (1), Ludovico Boratto (2), and Ernesto William De
  Luca (1) ((1) Otto von Guericke University Magdeburg, Germany, (2) University
  of Cagliari, Italy)
Categories: cs.AI cs.HC cs.IR cs.LG cs.SI
Comments: 70 pages
ACM-class: I.2
\\
  The integration of artificial intelligence (AI) into daily life, particularly
through information retrieval and recommender systems, has necessitated
advanced user modeling and profiling techniques to deliver personalized
experiences. These techniques aim to construct accurate user representations
based on the rich amounts of data generated through interactions with these
systems. This paper presents a comprehensive survey of the current state,
evolution, and future directions of user modeling and profiling research. We
provide a historical overview, tracing the development from early stereotype
models to the latest deep learning techniques, and propose a novel taxonomy
that encompasses all active topics in this research area, including recent
trends. Our survey highlights the paradigm shifts towards more sophisticated
user profiling methods, emphasizing implicit data collection, multi-behavior
modeling, and the integration of graph data structures. We also address the
critical need for privacy-preserving techniques and the push towards
explainability and fairness in user modeling approaches. By examining the
definitions of core terminology, we aim to clarify ambiguities and foster a
clearer understanding of the field by proposing two novel encyclopedic
definitions of the main terms. Furthermore, we explore the application of user
modeling in various domains, such as fake news detection, cybersecurity, and
personalized education. This survey serves as a comprehensive resource for
researchers and practitioners, offering insights into the evolution of user
modeling and profiling and guiding the development of more personalized,
ethical, and effective AI systems.
\\ ( https://arxiv.org/abs/2402.09660 ,  199kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09729
Date: Thu, 15 Feb 2024 05:56:35 GMT   (1195kb,D)

Title: Federated Prompt-based Decision Transformer for Customized VR Services
  in Mobile Edge Computing System
Authors: Tailin Zhou, Jiadong Yu, Jun Zhang, and Danny H.K. Tsang
Categories: cs.AI cs.SY eess.SY
\\
  This paper investigates resource allocation to provide heterogeneous users
with customized virtual reality (VR) services in a mobile edge computing (MEC)
system. We first introduce a quality of experience (QoE) metric to measure user
experience, which considers the MEC system's latency, user attention levels,
and preferred resolutions. Then, a QoE maximization problem is formulated for
resource allocation to ensure the highest possible user experience,which is
cast as a reinforcement learning problem, aiming to learn a generalized policy
applicable across diverse user environments for all MEC servers. To learn the
generalized policy, we propose a framework that employs federated learning (FL)
and prompt-based sequence modeling to pre-train a common decision model across
MEC servers, which is named FedPromptDT. Using FL solves the problem of
insufficient local MEC data while protecting user privacy during offline
training. The design of prompts integrating user-environment cues and
user-preferred allocation improves the model's adaptability to various user
environments during online execution.
\\ ( https://arxiv.org/abs/2402.09729 ,  1195kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09734
Date: Thu, 15 Feb 2024 06:15:46 GMT   (225kb,D)

Title: Agents Need Not Know Their Purpose
Authors: Paulo Garcia
Categories: cs.AI
\\
  Ensuring artificial intelligence behaves in such a way that is aligned with
human values is commonly referred to as the alignment challenge. Prior work has
shown that rational agents, behaving in such a way that maximizes a utility
function, will inevitably behave in such a way that is not aligned with human
values, especially as their level of intelligence goes up. Prior work has also
shown that there is no "one true utility function"; solutions must include a
more holistic approach to alignment. This paper describes oblivious agents:
agents that are architected in such a way that their effective utility function
is an aggregation of a known and hidden sub-functions. The hidden component, to
be maximized, is internally implemented as a black box, preventing the agent
from examining it. The known component, to be minimized, is knowledge of the
hidden sub-function. Architectural constraints further influence how agent
actions can evolve its internal environment model. We show that an oblivious
agent, behaving rationally, constructs an internal approximation of designers'
intentions (i.e., infers alignment), and, as a consequence of its architecture
and effective utility function, behaves in such a way that maximizes alignment;
i.e., maximizing the approximated intention function. We show that,
paradoxically, it does this for whatever utility function is used as the hidden
component and, in contrast with extant techniques, chances of alignment
actually improve as agent intelligence grows.
\\ ( https://arxiv.org/abs/2402.09734 ,  225kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09764
Date: Thu, 15 Feb 2024 07:29:43 GMT   (1795kb,D)

Title: Aligning Crowd Feedback via Distributional Preference Reward Modeling
Authors: Dexun Li, Cong Zhang, Kuicai Dong, Derrick Goh Xin Deik, Ruiming Tang,
  Yong Liu
Categories: cs.AI
\\
  Deep Reinforcement Learning is widely used for aligning Large Language Models
(LLM) with human preference. However, the conventional reward modelling has
predominantly depended on human annotations provided by a select cohort of
individuals. Such dependence may unintentionally result in models that are
skewed to reflect the inclinations of these annotators, thereby failing to
represent the expectations of the wider population adequately. In this paper,
we introduce the Distributional Preference Reward Model (DPRM), a simple yet
effective framework to align large language models with a diverse set of human
preferences. To this end, we characterize the preferences by a beta
distribution, which can dynamically adapt to fluctuations in preference trends.
On top of that, we design an optimal-transportation-based loss to calibrate
DPRM to align with the preference distribution. Finally, the expected reward is
utilized to fine-tune an LLM policy to generate responses favoured by the
population. Our experiments show that DPRM significantly enhances the alignment
of LLMs with population preference, yielding more accurate, unbiased, and
contextually appropriate responses.
\\ ( https://arxiv.org/abs/2402.09764 ,  1795kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09765
Date: Thu, 15 Feb 2024 07:35:29 GMT   (340kb,D)

Title: Reinforcement Learning for Solving Stochastic Vehicle Routing Problem
  with Time Windows
Authors: Zangir Iklassov and Ikboljon Sobirov and Ruben Solozabal and Martin
  Takac
Categories: cs.AI
\\
  This paper introduces a reinforcement learning approach to optimize the
Stochastic Vehicle Routing Problem with Time Windows (SVRP), focusing on
reducing travel costs in goods delivery. We develop a novel SVRP formulation
that accounts for uncertain travel costs and demands, alongside specific
customer time windows. An attention-based neural network trained through
reinforcement learning is employed to minimize routing costs. Our approach
addresses a gap in SVRP research, which traditionally relies on heuristic
methods, by leveraging machine learning. The model outperforms the Ant-Colony
Optimization algorithm, achieving a 1.73% reduction in travel costs. It
uniquely integrates external information, demonstrating robustness in diverse
environments, making it a valuable benchmark for future SVRP studies and
industry application.
\\ ( https://arxiv.org/abs/2402.09765 ,  340kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09769
Date: Thu, 15 Feb 2024 07:47:10 GMT   (1462kb,D)

Title: Representation Learning Using a Single Forward Pass
Authors: Aditya Somasundaram, Pushkal Mishra, Ayon Borthakur
Categories: cs.AI
Comments: Under review
\\
  We propose a neuroscience-inspired Solo Pass Embedded Learning Algorithm
(SPELA). SPELA is a prime candidate for training and inference applications in
Edge AI devices. At the same time, SPELA can optimally cater to the need for a
framework to study perceptual representation learning and formation. SPELA has
distinctive features such as neural priors (in the form of embedded vectors),
no weight transport, no update locking of weights, complete local Hebbian
learning, single forward pass with no storage of activations, and single weight
update per sample. Juxtaposed with traditional approaches, SPELA operates
without the need for backpropagation. We show that our algorithm can perform
nonlinear classification on a noisy boolean operation dataset. Additionally, we
exhibit high performance using SPELA across MNIST, KMNIST, and Fashion MNIST.
Lastly, we show the few-shot and 1-epoch learning capabilities of SPELA on
MNIST, KMNIST, and Fashion MNIST, where it consistently outperforms
backpropagation.
\\ ( https://arxiv.org/abs/2402.09769 ,  1462kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09836
Date: Thu, 15 Feb 2024 09:58:23 GMT   (1182kb,D)

Title: Beyond Imitation: Generating Human Mobility from Context-aware Reasoning
  with Large Language Models
Authors: Chenyang Shao, Fengli Xu, Bingbing Fan, Jingtao Ding, Yuan Yuan, Meng
  Wang, Yong Li
Categories: cs.AI
\\
  Human mobility behaviours are closely linked to various important societal
problems such as traffic congestion, and epidemic control. However, collecting
mobility data can be prohibitively expensive and involves serious privacy
issues, posing a pressing need for high-quality generative mobility models.
Previous efforts focus on learning the behaviour distribution from training
samples, and generate new mobility data by sampling the learned distributions.
They cannot effectively capture the coherent intentions that drive mobility
behavior, leading to low sample efficiency and semantic-awareness. Inspired by
the emergent reasoning ability in LLMs, we propose a radical perspective shift
that reformulates mobility generation as a commonsense reasoning problem. In
this paper, we design a novel Mobility Generation as Reasoning (MobiGeaR)
framework that prompts LLM to recursively generate mobility behaviour.
Specifically, we design a context-aware chain-of-thoughts prompting technique
to align LLMs with context-aware mobility behaviour by few-shot in-context
learning. Besides, MobiGeaR employ a divide-and-coordinate mechanism to exploit
the synergistic effect between LLM reasoning and mechanistic gravity model. It
leverages the step-by-step LLM reasoning to recursively generate a temporal
template of activity intentions, which are then mapped to physical locations
with a mechanistic gravity model. Experiments on two real-world datasets show
MobiGeaR achieves state-of-the-art performance across all metrics, and
substantially reduces the size of training samples at the same time. Besides,
MobiGeaR also significantly improves the semantic-awareness of mobility
generation by improving the intention accuracy by 62.23% and the generated
mobility data is proven effective in boosting the performance of downstream
applications. The implementation of our approach is available in the paper.
\\ ( https://arxiv.org/abs/2402.09836 ,  1182kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09844
Date: Thu, 15 Feb 2024 10:01:55 GMT   (6279kb,D)

Title: Jack of All Trades, Master of Some, a Multi-Purpose Transformer Agent
Authors: Quentin Gallou\'edec and Edward Beeching and Cl\'ement Romac and
  Emmanuel Dellandr\'ea
Categories: cs.AI
Comments: Under review
\\
  The search for a general model that can operate seamlessly across multiple
domains remains a key goal in machine learning research. The prevailing
methodology in Reinforcement Learning (RL) typically limits models to a single
task within a unimodal framework, a limitation that contrasts with the broader
vision of a versatile, multi-domain model. In this paper, we present Jack of
All Trades (JAT), a transformer-based model with a unique design optimized for
handling sequential decision-making tasks and multimodal data types. The JAT
model demonstrates its robust capabilities and versatility by achieving strong
performance on very different RL benchmarks, along with promising results on
Computer Vision (CV) and Natural Language Processing (NLP) tasks, all using a
single set of weights. The JAT model marks a significant step towards more
general, cross-domain AI model design, and notably, it is the first model of
its kind to be fully open-sourced (see https://huggingface.co/jat-project/jat),
including a pioneering general-purpose dataset.
\\ ( https://arxiv.org/abs/2402.09844 ,  6279kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09877
Date: Thu, 15 Feb 2024 11:00:28 GMT   (802kb,D)

Title: On Computing Plans with Uniform Action Costs
Authors: Alberto Pozanco, Daniel Borrajo, Manuela Veloso
Categories: cs.AI
\\
  In many real-world planning applications, agents might be interested in
finding plans whose actions have costs that are as uniform as possible. Such
plans provide agents with a sense of stability and predictability, which are
key features when humans are the agents executing plans suggested by planning
tools. This paper adapts three uniformity metrics to automated planning, and
introduce planning-based compilations that allow to lexicographically optimize
sum of action costs and action costs uniformity. Experimental results both in
well-known and novel planning benchmarks show that the reformulated tasks can
be effectively solved in practice to generate uniform plans.
\\ ( https://arxiv.org/abs/2402.09877 ,  802kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09880
Date: Thu, 15 Feb 2024 11:08:10 GMT   (48kb)

Title: Inadequacies of Large Language Model Benchmarks in the Era of Generative
  Artificial Intelligence
Authors: Timothy R. McIntosh, Teo Susnjak, Tong Liu, Paul Watters, and Malka N.
  Halgamuge
Categories: cs.AI cs.CY cs.HC
\\
  The rapid rise in popularity of Large Language Models (LLMs) with emerging
capabilities has spurred public curiosity to evaluate and compare different
LLMs, leading many researchers to propose their LLM benchmarks. Noticing
preliminary inadequacies in those benchmarks, we embarked on a study to
critically assess 23 state-of-the-art LLM benchmarks, using our novel unified
evaluation framework through the lenses of people, process, and technology,
under the pillars of functionality and security. Our research uncovered
significant limitations, including biases, difficulties in measuring genuine
reasoning, adaptability, implementation inconsistencies, prompt engineering
complexity, evaluator diversity, and the overlooking of cultural and
ideological norms in one comprehensive assessment. Our discussions emphasized
the urgent need for standardized methodologies, regulatory certainties, and
ethical guidelines in light of Artificial Intelligence (AI) advancements,
including advocating for an evolution from static benchmarks to dynamic
behavioral profiling to accurately capture LLMs' complex behaviors and
potential risks. Our study highlighted the necessity for a paradigm shift in
LLM evaluation methodologies, underlining the importance of collaborative
efforts for the development of universally accepted benchmarks and the
enhancement of AI systems' integration into society.
\\ ( https://arxiv.org/abs/2402.09880 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09919
Date: Thu, 15 Feb 2024 12:53:25 GMT   (1883kb,D)

Title: Road Graph Generator: Mapping roads at construction sites from GPS data
Authors: Katarzyna Micha{\l}owska, Helga Margrete Bodahl Holmestad, Signe
  Riemer-S{\o}rensen
Categories: cs.AI
Comments: 16 pages, 3 figures
\\
  We present a method for road inference from GPS trajectories to map
construction sites. This task introduces a unique challenge due to the erratic
and non-standard movement patterns of construction machinery, which diverge
significantly from typical vehicular traffic on established roads. Our method
first identifies intersections in the road network that serve as critical
decision points, and later connects them with edges, producing a graph, which
subsequently can be used for planning and task-allocation. We demonstrate the
effectiveness of our approach by mapping roads at a real-life construction site
in Norway.
\\ ( https://arxiv.org/abs/2402.09919 ,  1883kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09939
Date: Thu, 15 Feb 2024 13:39:55 GMT   (3172kb)

Title: Generative AI in the Construction Industry: A State-of-the-art Analysis
Authors: Ridwan Taiwo, Idris Temitope Bello, Sulemana Fatoama Abdulai,
  Abdul-Mugis Yussif, Babatunde Abiodun Salami, Abdullahi Saka, Tarek Zayed
Categories: cs.AI cs.CL cs.HC cs.IR cs.LG
Comments: 74 pages, 11 figures, 20 tables
\\
  The construction industry is a vital sector of the global economy, but it
faces many productivity challenges in various processes, such as design,
planning, procurement, inspection, and maintenance. Generative artificial
intelligence (AI), which can create novel and realistic data or content, such
as text, image, video, or code, based on some input or prior knowledge, offers
innovative and disruptive solutions to address these challenges. However, there
is a gap in the literature on the current state, opportunities, and challenges
of generative AI in the construction industry. This study aims to fill this gap
by providing a state-of-the-art analysis of generative AI in construction, with
three objectives: (1) to review and categorize the existing and emerging
generative AI opportunities and challenges in the construction industry; (2) to
propose a framework for construction firms to build customized generative AI
solutions using their own data, comprising steps such as data collection,
dataset curation, training custom large language model (LLM), model evaluation,
and deployment; and (3) to demonstrate the framework via a case study of
developing a generative model for querying contract documents. The results show
that retrieval augmented generation (RAG) improves the baseline LLM by 5.2,
9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study
provides academics and construction professionals with a comprehensive analysis
and practical framework to guide the adoption of generative AI techniques to
enhance productivity, quality, safety, and sustainability across the
construction industry.
\\ ( https://arxiv.org/abs/2402.09939 ,  3172kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09997
Date: Thu, 15 Feb 2024 15:02:46 GMT   (3035kb,D)

Title: LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed
  Tasks in the Wild
Authors: Ziyu Zhao, Leilei Gan, Guoyin Wang, Wangchunshu Zhou, Hongxia Yang,
  Kun Kuang, Fei Wu
Categories: cs.AI
\\
  Low-Rank Adaptation (LoRA) provides an effective yet efficient solution for
fine-tuning large language models (LLM). The modular and plug-and-play nature
of LoRA enables the integration of diverse domain-specific LoRAs to enhance the
capabilities of LLMs. Previous research on exploiting multiple LoRAs either
focuses on specific isolated downstream tasks or fixes the selection of LoRAs
during training. However, in real-world scenarios, LLMs receive diverse prompts
covering different tasks, and the pool of candidate LoRAs is often dynamically
updated. To bridge this gap, we propose LoraRetriever, a retrieve-then-compose
framework that adaptively retrieves and composes multiple LoRAs according to
the input prompts. LoraRetriever contains three main components: firstly,
identifying and retrieving LoRAs relevant to the given input; secondly,
formulating strategies for effectively integrating the retrieved LoRAs; and
thirdly, developing efficient batch inference to accommodate heterogeneous
requests. Experimental results indicate that LoraRetriever consistently
outperforms the baselines, highlighting its practical effectiveness and
versatility.
\\ ( https://arxiv.org/abs/2402.09997 ,  3035kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10011
Date: Thu, 15 Feb 2024 15:18:53 GMT   (153kb,D)

Title: Clifford Group Equivariant Simplicial Message Passing Networks
Authors: Cong Liu, David Ruhe, Floor Eijkelboom, Patrick Forr\'e
Categories: cs.AI
\\
  We introduce Clifford Group Equivariant Simplicial Message Passing Networks,
a method for steerable E(n)-equivariant message passing on simplicial
complexes. Our method integrates the expressivity of Clifford group-equivariant
layers with simplicial message passing, which is topologically more intricate
than regular graph message passing. Clifford algebras include higher-order
objects such as bivectors and trivectors, which express geometric features
(e.g., areas, volumes) derived from vectors. Using this knowledge, we represent
simplex features through geometric products of their vertices. To achieve
efficient simplicial message passing, we share the parameters of the message
network across different dimensions. Additionally, we restrict the final
message to an aggregation of the incoming messages from different dimensions,
leading to what we term shared simplicial message passing. Experimental results
show that our method is able to outperform both equivariant and simplicial
graph neural networks on a variety of geometric tasks.
\\ ( https://arxiv.org/abs/2402.10011 ,  153kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10051
Date: Thu, 15 Feb 2024 16:15:38 GMT   (7761kb,D)

Title: SwissNYF: Tool Grounded LLM Agents for Black Box Setting
Authors: Somnath Sendhil Kumar, Dhruv Jain, Eshaan Agarwal, Raunak Pandey
Categories: cs.AI cs.CL
\\
  While Large Language Models (LLMs) have demonstrated enhanced capabilities in
function-calling, these advancements primarily rely on accessing the functions'
responses. This methodology is practical for simpler APIs but faces scalability
issues with irreversible APIs that significantly impact the system, such as a
database deletion API. Similarly, processes requiring extensive time for each
API call and those necessitating forward planning, like automated action
pipelines, present complex challenges. Furthermore, scenarios often arise where
a generalized approach is needed because algorithms lack direct access to the
specific implementations of these functions or secrets to use them. Traditional
tool planning methods are inadequate in these cases, compelling the need to
operate within black-box environments. Unlike their performance in tool
manipulation, LLMs excel in black-box tasks, such as program synthesis.
Therefore, we harness the program synthesis capabilities of LLMs to strategize
tool usage in black-box settings, ensuring solutions are verified prior to
implementation. We introduce TOPGUN, an ingeniously crafted approach leveraging
program synthesis for black box tool planning. Accompanied by SwissNYF, a
comprehensive suite that integrates black-box algorithms for planning and
verification tasks, addressing the aforementioned challenges and enhancing the
versatility and effectiveness of LLMs in complex API interactions. The public
code for SwissNYF is available at https://github.com/iclr-dummy-user/SwissNYF.
\\ ( https://arxiv.org/abs/2402.10051 ,  7761kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10083
Date: Thu, 15 Feb 2024 16:43:41 GMT   (2562kb)

Title: Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots
  in Ophthalmology and LLM-based evaluation using GPT-4
Authors: Ting Fang Tan, Kabilan Elangovan, Liyuan Jin, Yao Jie, Li Yong, Joshua
  Lim, Stanley Poh, Wei Yan Ng, Daniel Lim, Yuhe Ke, Nan Liu, Daniel Shu Wei
  Ting
Categories: cs.AI
Comments: 13 Pages, 1 Figure, 8 Tables
\\
  Purpose: To assess the alignment of GPT-4-based evaluation to human clinician
experts, for the evaluation of responses to ophthalmology-related patient
queries generated by fine-tuned LLM chatbots. Methods: 400 ophthalmology
questions and paired answers were created by ophthalmologists to represent
commonly asked patient questions, divided into fine-tuning (368; 92%), and
testing (40; 8%). We find-tuned 5 different LLMs, including LLAMA2-7b,
LLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat. For the testing dataset,
additional 8 glaucoma QnA pairs were included. 200 responses to the testing
dataset were generated by 5 fine-tuned LLMs for evaluation. A customized
clinical evaluation rubric was used to guide GPT-4 evaluation, grounded on
clinical accuracy, relevance, patient safety, and ease of understanding. GPT-4
evaluation was then compared against ranking by 5 clinicians for clinical
alignment. Results: Among all fine-tuned LLMs, GPT-3.5 scored the highest
(87.1%), followed by LLAMA2-13b (80.9%), LLAMA2-13b-chat (75.5%),
LLAMA2-7b-Chat (70%) and LLAMA2-7b (68.8%) based on the GPT-4 evaluation. GPT-4
evaluation demonstrated significant agreement with human clinician rankings,
with Spearman and Kendall Tau correlation coefficients of 0.90 and 0.80
respectively; while correlation based on Cohen Kappa was more modest at 0.50.
Notably, qualitative analysis and the glaucoma sub-analysis revealed clinical
inaccuracies in the LLM-generated responses, which were appropriately
identified by the GPT-4 evaluation. Conclusion: The notable clinical alignment
of GPT-4 evaluation highlighted its potential to streamline the clinical
evaluation of LLM chatbot responses to healthcare-related queries. By
complementing the existing clinician-dependent manual grading, this efficient
and automated evaluation could assist the validation of future developments in
LLM applications for healthcare.
\\ ( https://arxiv.org/abs/2402.10083 ,  2562kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10102
Date: Thu, 15 Feb 2024 16:56:25 GMT   (1754kb,D)

Title: A privacy-preserving, distributed and cooperative FCM-based learning
  approach for Cancer Research
Authors: Jose L. Salmeron and Irina Ar\'evalo
Categories: cs.AI cs.DC
Comments: Rough Sets: International Joint Conference, IJCRS 2020
\\
  Distributed Artificial Intelligence is attracting interest day by day. In
this paper, the authors introduce an innovative methodology for distributed
learning of Particle Swarm Optimization-based Fuzzy Cognitive Maps in a
privacy-preserving way. The authors design a training scheme for collaborative
FCM learning that offers data privacy compliant with the current regulation.
This method is applied to a cancer detection problem, proving that the
performance of the model is improved by the Federated Learning process, and
obtaining similar results to the ones that can be found in the literature.
\\ ( https://arxiv.org/abs/2402.10102 ,  1754kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10104
Date: Thu, 15 Feb 2024 16:59:41 GMT   (12526kb,D)

Title: GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on
  Geometry Problem-Solving
Authors: Jiaxin Zhang, Zhongzhi Li, Mingliang Zhang, Fei Yin, Chenglin Liu,
  Yashar Moshfeghi
Categories: cs.AI cs.CL
\\
  Recent advancements in Large Language Models (LLMs) and Multi-Modal Models
(MMs) have demonstrated their remarkable capabilities in problem-solving. Yet,
their proficiency in tackling geometry math problems, which necessitates an
integrated understanding of both textual and visual information, has not been
thoroughly evaluated. To address this gap, we introduce the GeoEval benchmark,
a comprehensive collection that includes a main subset of 2000 problems, a 750
problem subset focusing on backward reasoning, an augmented subset of 2000
problems, and a hard subset of 300 problems. This benchmark facilitates a
deeper investigation into the performance of LLMs and MMs on solving geometry
math problems. Our evaluation of ten LLMs and MMs across these varied subsets
reveals that the WizardMath model excels, achieving a 55.67\% accuracy rate on
the main subset but only a 6.00\% accuracy on the challenging subset. This
highlights the critical need for testing models against datasets on which they
have not been pre-trained. Additionally, our findings indicate that GPT-series
models perform more effectively on problems they have rephrased, suggesting a
promising method for enhancing model capabilities.
\\ ( https://arxiv.org/abs/2402.10104 ,  12526kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10109
Date: Thu, 15 Feb 2024 17:05:48 GMT   (536kb,D)

Title: Towards Reducing Diagnostic Errors with Interpretable Risk Prediction
Authors: Denis Jered McInerney, William Dickinson, Lucy Flynn, Andrea Young,
  Geoffrey Young, Jan-Willem van de Meent, Byron C. Wallace
Categories: cs.AI cs.CL cs.LG
\\
  Many diagnostic errors occur because clinicians cannot easily access relevant
information in patient Electronic Health Records (EHRs). In this work we
propose a method to use LLMs to identify pieces of evidence in patient EHR data
that indicate increased or decreased risk of specific diagnoses; our ultimate
aim is to increase access to evidence and reduce diagnostic errors. In
particular, we propose a Neural Additive Model to make predictions backed by
evidence with individualized risk estimates at time-points where clinicians are
still uncertain, aiming to specifically mitigate delays in diagnosis and errors
stemming from an incomplete differential. To train such a model, it is
necessary to infer temporally fine-grained retrospective labels of eventual
"true" diagnoses. We do so with LLMs, to ensure that the input text is from
before a confident diagnosis can be made. We use an LLM to retrieve an initial
pool of evidence, but then refine this set of evidence according to
correlations learned by the model. We conduct an in-depth evaluation of the
usefulness of our approach by simulating how it might be used by a clinician to
decide between a pre-defined list of differential diagnoses.
\\ ( https://arxiv.org/abs/2402.10109 ,  536kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10115
Date: Thu, 15 Feb 2024 17:10:27 GMT   (2912kb,D)

Title: Generating Visual Stimuli from EEG Recordings using Transformer-encoder
  based EEG encoder and GAN
Authors: Rahul Mishra, Arnav Bhavsar
Categories: cs.AI cs.LG eess.SP q-bio.NC
\\
  In this study, we tackle a modern research challenge within the field of
perceptual brain decoding, which revolves around synthesizing images from EEG
signals using an adversarial deep learning framework. The specific objective is
to recreate images belonging to various object categories by leveraging EEG
recordings obtained while subjects view those images. To achieve this, we
employ a Transformer-encoder based EEG encoder to produce EEG encodings, which
serve as inputs to the generator component of the GAN network. Alongside the
adversarial loss, we also incorporate perceptual loss to enhance the quality of
the generated images.
\\ ( https://arxiv.org/abs/2402.10115 ,  2912kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10133
Date: Thu, 15 Feb 2024 17:37:25 GMT   (436kb,D)

Title: Zero-Shot Reasoning: Personalized Content Generation Without the Cold
  Start Problem
Authors: Davor Hafnar (1), Jure Dem\v{s}ar (1 and 2) ((1) Faculty of Computer
  and Information Science, University of Ljubljana (2) Department of
  Psychology, Faculty of Arts, University of Ljubljana)
Categories: cs.AI
Comments: 9 pages, 6 figures
\\
  Procedural content generation uses algorithmic techniques to create large
amounts of new content for games at much lower production costs. In newer
approaches, procedural content generation utilizes machine learning. However,
these methods usually require expensive collection of large amounts of data, as
well as the development and training of fairly complex learning models, which
can be both extremely time-consuming and expensive. The core of our research is
to explore whether we can lower the barrier to the use of personalized
procedural content generation through a more practical and generalizable
approach with large language models. Matching game content with player
preferences benefits both players, who enjoy the game more, and developers, who
increasingly depend on players enjoying the game before being able to monetize
it. Therefore, this paper presents a novel approach to achieving
personalization by using large language models to propose levels based on the
gameplay data continuously collected from individual players. We compared the
levels generated using our approach with levels generated with more traditional
procedural generation techniques. Our easily reproducible method has proven
viable in a production setting and outperformed levels generated by traditional
methods in the probability that a player will not quit the game mid-level.
\\ ( https://arxiv.org/abs/2402.10133 ,  436kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10172
Date: Thu, 15 Feb 2024 18:19:18 GMT   (1480kb,D)

Title: OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large
  Language Models
Authors: Ali AhmadiTeshnizi, Wenzhi Gao, Madeleine Udell
Categories: cs.AI cs.MA
\\
  Optimization problems are pervasive in sectors from manufacturing and
distribution to healthcare. However, most such problems are still solved
heuristically by hand rather than optimally by state-of-the-art solvers because
the expertise required to formulate and solve these problems limits the
widespread adoption of optimization tools and techniques. This paper introduces
OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and
solve (mixed integer) linear programming problems from their natural language
descriptions. OptiMUS can develop mathematical models, write and debug solver
code, evaluate the generated solutions, and improve its model and code based on
these evaluations. OptiMUS utilizes a modular structure to process problems,
allowing it to handle problems with long descriptions and complex data without
long prompts. Experiments demonstrate that OptiMUS outperforms existing
state-of-the-art methods on easy datasets by more than $20\%$ and on hard
datasets (including a new dataset, NLP4LP, released with this paper that
features long and complex problems) by more than $30\%$.
\\ ( https://arxiv.org/abs/2402.10172 ,  1480kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09552
Date: Wed, 14 Feb 2024 20:05:26 GMT   (5177kb,D)

Title: Rationality Report Cards: Assessing the Economic Rationality of Large
  Language Models
Authors: Narun Raman, Taylor Lundy, Samuel Amouyal, Yoav Levine, Kevin
  Leyton-Brown, Moshe Tennenholtz
Categories: cs.CL econ.GN q-fin.EC
\\
  There is increasing interest in using LLMs as decision-making "agents." Doing
so includes many degrees of freedom: which model should be used; how should it
be prompted; should it be asked to introspect, conduct chain-of-thought
reasoning, etc? Settling these questions -- and more broadly, determining
whether an LLM agent is reliable enough to be trusted -- requires a methodology
for assessing such an agent's economic rationality. In this paper, we provide
one. We begin by surveying the economic literature on rational decision making,
taxonomizing a large set of fine-grained "elements" that an agent should
exhibit, along with dependencies between them. We then propose a benchmark
distribution that quantitatively scores an LLMs performance on these elements
and, combined with a user-provided rubric, produces a "rationality report
card." Finally, we describe the results of a large-scale empirical experiment
with 14 different LLMs, characterizing the both current state of the art and
the impact of different model sizes on models' ability to exhibit rational
behavior.
\\ ( https://arxiv.org/abs/2402.09552 ,  5177kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09609
Date: Wed, 14 Feb 2024 22:36:07 GMT   (95kb,D)

Title: LogicPrpBank: A Corpus for Logical Implication and Equivalence
Authors: Zhexiong Liu, Jing Zhang, Jiaying Lu, Wenjing Ma, Joyce C Ho
Categories: cs.CL cs.AI
Comments: In the 5th AI4ED Workshop, held in conjunction with The 38th AAAI
  Conference on Artificial Intelligence, February 2024
\\
  Logic reasoning has been critically needed in problem-solving and
decision-making. Although Language Models (LMs) have demonstrated capabilities
of handling multiple reasoning tasks (e.g., commonsense reasoning), their
ability to reason complex mathematical problems, specifically propositional
logic, remains largely underexplored. This lack of exploration can be
attributed to the limited availability of annotated corpora. Here, we present a
well-labeled propositional logic corpus, LogicPrpBank, containing 7093
Propositional Logic Statements (PLSs) across six mathematical subjects, to
study a brand-new task of reasoning logical implication and equivalence. We
benchmark LogicPrpBank with widely-used LMs to show that our corpus offers a
useful resource for this challenging task and there is ample room for model
improvement.
\\ ( https://arxiv.org/abs/2402.09609 ,  95kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09611
Date: Wed, 14 Feb 2024 22:57:03 GMT   (491kb,D)

Title: Towards Privacy-Aware Sign Language Translation at Scale
Authors: Phillip Rust and Bowen Shi and Skyler Wang and Necati Cihan Camg\"oz
  and Jean Maillard
Categories: cs.CL cs.AI cs.CV cs.LG
\\
  A major impediment to the advancement of sign language translation (SLT) is
data scarcity. Much of the sign language data currently available on the web
cannot be used for training supervised models due to the lack of aligned
captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears
privacy risks due to the presence of biometric information, which the
responsible development of SLT technologies should account for. In this work,
we propose a two-stage framework for privacy-aware SLT at scale that addresses
both of these issues. We introduce SSVP-SLT, which leverages self-supervised
video pretraining on anonymized and unannotated videos, followed by supervised
SLT finetuning on a curated parallel dataset. SSVP-SLT achieves
state-of-the-art finetuned and zero-shot gloss-free SLT performance on the
How2Sign dataset, outperforming the strongest respective baselines by over 3
BLEU-4. Based on controlled experiments, we further discuss the advantages and
limitations of self-supervised pretraining and anonymization via facial
obfuscation for SLT.
\\ ( https://arxiv.org/abs/2402.09611 ,  491kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09614
Date: Wed, 14 Feb 2024 23:05:44 GMT   (17387kb,D)

Title: Probabilistic Reasoning in Generative Large Language Models
Authors: Aliakbar Nafar, Kristen Brent Venable, Parisa Kordjamshidi
Categories: cs.CL cs.AI
ACM-class: I.2.7
\\
  This paper considers the challenges that Large Language Models (LLMs) face
when reasoning over text that includes information involving uncertainty
explicitly quantified via probability values. This type of reasoning is
relevant to a variety of contexts ranging from everyday conversations to
medical decision-making. Despite improvements in the mathematical reasoning
capabilities of LLMs, they still exhibit significant difficulties when it comes
to probabilistic reasoning. To deal with this problem, we first introduce the
Bayesian Linguistic Inference Dataset (BLInD), a new dataset specifically
designed to test the probabilistic reasoning capabilities of LLMs. We then
leverage this new dataset to thoroughly illustrate the specific limitations of
LLMs for tasks involving probabilistic reasoning and present several strategies
that map the problem to different formal representations, including Python
code, probabilistic inference algorithms, and probabilistic logical
programming. We conclude by providing an evaluation of our methods on BLInD and
on an adaptation of a causal reasoning question-answering dataset, which
further shows their practical effectiveness.
\\ ( https://arxiv.org/abs/2402.09614 ,  17387kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09615
Date: Wed, 14 Feb 2024 23:09:15 GMT   (1561kb,D)

Title: API Pack: A Massive Multilingual Dataset for API Call Generation
Authors: Zhen Guo, Adriana Meza Soria, Wei Sun, Yikang Shen, Rameswar Panda
Categories: cs.CL cs.AI cs.LG
\\
  We introduce API Pack, a multilingual dataset featuring over one million
instruction-API call pairs aimed at advancing large language models' API call
generation capabilities. Through experiments, we demonstrate API Pack's
efficacy in enhancing models for this specialized task while maintaining their
overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000
Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4
respectively in generating unseen API calls. Scaling to 100k examples improves
generalization to new APIs not seen during training. In addition, cross-lingual
API call generation is achieved without needing extensive data per language.
The dataset, fine-tuned models, and overall code base are publicly available at
https://github.com/anonymous_url.
\\ ( https://arxiv.org/abs/2402.09615 ,  1561kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09642
Date: Thu, 15 Feb 2024 01:02:41 GMT   (1662kb,D)

Title: Answer is All You Need: Instruction-following Text Embedding via
  Answering the Question
Authors: Letian Peng, Yuwei Zhang, Zilong Wang, Jayanth Srinivasa, Gaowen Liu,
  Zihan Wang, Jingbo Shang
Categories: cs.CL
\\
  This work aims to build a text embedder that can capture characteristics of
texts specified by user instructions. Despite its tremendous potential to
deploy user-oriented embeddings, none of previous approaches provides a
concrete solution for it. This paper offers a new viewpoint, which treats the
instruction as a question about the input text and encodes the expected answers
to obtain the representation accordingly. Intuitively, texts with the same
(implicit) semantics would share similar answers following the instruction,
thus leading to more similar embeddings. Specifically, we propose InBedder that
instantiates this embed-via-answering idea by only fine-tuning language models
on abstractive question answering tasks. InBedder demonstrates significantly
improved instruction-following capabilities according to our proposed
instruction awareness tests and instruction robustness tests, when applied to
both large language models (LLMs) (e.g., llama-2-7b) and smaller encoder-based
LMs (e.g., roberta-large). Additionally, our qualitative analysis of clustering
outcomes, achieved by applying different instructions to the same corpus,
demonstrates a high degree of interpretability.
\\ ( https://arxiv.org/abs/2402.09642 ,  1662kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09666
Date: Thu, 15 Feb 2024 02:27:23 GMT   (270kb,D)

Title: EntailE: Introducing Textual Entailment in Commonsense Knowledge Graph
  Completion
Authors: Ying Su, Tianqing Fang, Huiru Xiao, Weiqi Wang, Yangqiu Song, Tong
  Zhang, Lei Chen
Categories: cs.CL
Comments: 10 pages, 5 figures, 9 tables
\\
  Commonsense knowledge graph completion is a new challenge for commonsense
knowledge graph construction and application. In contrast to factual knowledge
graphs such as Freebase and YAGO, commonsense knowledge graphs (CSKGs; e.g.,
ConceptNet) utilize free-form text to represent named entities, short phrases,
and events as their nodes. Such a loose structure results in large and sparse
CSKGs, which makes the semantic understanding of these nodes more critical for
learning rich commonsense knowledge graph embedding. While current methods
leverage semantic similarities to increase the graph density, the semantic
plausibility of the nodes and their relations are under-explored. Previous
works adopt conceptual abstraction to improve the consistency of modeling
(event) plausibility, but they are not scalable enough and still suffer from
data sparsity. In this paper, we propose to adopt textual entailment to find
implicit entailment relations between CSKG nodes, to effectively densify the
subgraph connecting nodes within the same conceptual class, which indicates a
similar level of plausibility. Each node in CSKG finds its top entailed nodes
using a finetuned transformer over natural language inference (NLI) tasks,
which sufficiently capture textual entailment signals. The entailment relation
between these nodes are further utilized to: 1) build new connections between
source triplets and entailed nodes to densify the sparse CSKGs; 2) enrich the
generalization ability of node representations by comparing the node embeddings
with a contrastive loss. Experiments on two standard CSKGs demonstrate that our
proposed framework EntailE can improve the performance of CSKG completion tasks
under both transductive and inductive settings.
\\ ( https://arxiv.org/abs/2402.09666 ,  270kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09674
Date: Thu, 15 Feb 2024 02:54:49 GMT   (2560kb,D)

Title: PAL: Proxy-Guided Black-Box Attack on Large Language Models
Authors: Chawin Sitawarin, Norman Mu, David Wagner, Alexandre Araujo
Categories: cs.CL cs.AI cs.CR cs.LG
\\
  Large Language Models (LLMs) have surged in popularity in recent months, but
they have demonstrated concerning capabilities to generate harmful content when
manipulated. While techniques like safety fine-tuning aim to minimize harmful
use, recent works have shown that LLMs remain vulnerable to attacks that elicit
toxic responses. In this work, we introduce the Proxy-Guided Attack on LLMs
(PAL), the first optimization-based attack on LLMs in a black-box query-only
setting. In particular, it relies on a surrogate model to guide the
optimization and a sophisticated loss designed for real-world LLM APIs. Our
attack achieves 84% attack success rate (ASR) on GPT-3.5-Turbo and 48% on
Llama-2-7B, compared to 4% for the current state of the art. We also propose
GCG++, an improvement to the GCG attack that reaches 94% ASR on white-box
Llama-2-7B, and the Random-Search Attack on LLMs (RAL), a strong but simple
baseline for query-based attacks. We believe the techniques proposed in this
work will enable more comprehensive safety testing of LLMs and, in the long
term, the development of better security guardrails. The code can be found at
https://github.com/chawins/pal.
\\ ( https://arxiv.org/abs/2402.09674 ,  2560kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09696
Date: Thu, 15 Feb 2024 04:10:25 GMT   (1847kb,D)

Title: An Analysis of Langauge Frequency and Error Correction for Esperanto
Authors: Junhong Liang
Categories: cs.CL
\\
  Current Grammar Error Correction (GEC) initiatives tend to focus on major
languages, with less attention given to low-resource languages like Esperanto.
In this article, we begin to bridge this gap by first conducting a
comprehensive frequency analysis using the Eo-GP dataset, created explicitly
for this purpose. We then introduce the Eo-GEC dataset, derived from authentic
user cases and annotated with fine-grained linguistic details for error
identification. Leveraging GPT-3.5 and GPT-4, our experiments show that GPT-4
outperforms GPT-3.5 in both automated and human evaluations, highlighting its
efficacy in addressing Esperanto's grammatical peculiarities and illustrating
the potential of advanced language models to enhance GEC strategies for less
commonly studied languages.
\\ ( https://arxiv.org/abs/2402.09696 ,  1847kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09725
Date: Thu, 15 Feb 2024 05:35:04 GMT   (7796kb,D)

Title: Improving Non-autoregressive Machine Translation with Error Exposure and
  Consistency Regularization
Authors: Xinran Chen, Sufeng Duan, Gongshen Liu
Categories: cs.CL cs.AI
\\
  Being one of the IR-NAT (Iterative-refinemennt-based NAT) frameworks, the
Conditional Masked Language Model (CMLM) adopts the mask-predict paradigm to
re-predict the masked low-confidence tokens. However, CMLM suffers from the
data distribution discrepancy between training and inference, where the
observed tokens are generated differently in the two cases. In this paper, we
address this problem with the training approaches of error exposure and
consistency regularization (EECR). We construct the mixed sequences based on
model prediction during training, and propose to optimize over the masked
tokens under imperfect observation conditions. We also design a consistency
learning method to constrain the data distribution for the masked tokens under
different observing situations to narrow down the gap between training and
inference. The experiments on five translation benchmarks obtains an average
improvement of 0.68 and 0.40 BLEU scores compared to the base models,
respectively, and our CMLMC-EECR achieves the best performance with a
comparable translation quality with the Transformer. The experiments results
demonstrate the effectiveness of our method.
\\ ( https://arxiv.org/abs/2402.09725 ,  7796kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09727
Date: Thu, 15 Feb 2024 05:40:21 GMT   (221kb,D)

Title: A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts
Authors: Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, Ian Fischer
Categories: cs.CL cs.AI cs.IR
Comments: Website: https://read-agent.github.io
\\
  Current Large Language Models (LLMs) are not only limited to some maximum
context length, but also are not able to robustly consume long inputs. To
address these limitations, we propose ReadAgent, an LLM agent system that
increases effective context length up to 20x in our experiments. Inspired by
how humans interactively read long documents, we implement ReadAgent as a
simple prompting system that uses the advanced language capabilities of LLMs to
(1) decide what content to store together in a memory episode, (2) compress
those memory episodes into short episodic memories called gist memories, and
(3) take actions to look up passages in the original text if ReadAgent needs to
remind itself of relevant details to complete a task. We evaluate ReadAgent
against baselines using retrieval methods, using the original long contexts,
and using the gist memories. These evaluations are performed on three
long-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum.
ReadAgent outperforms the baselines on all three tasks while extending the
effective context window by 3-20x.
\\ ( https://arxiv.org/abs/2402.09727 ,  221kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09733
Date: Thu, 15 Feb 2024 06:14:55 GMT   (6852kb,D)

Title: Do LLMs Know about Hallucination? An Empirical Investigation of LLM's
  Hidden States
Authors: Hanyu Duan, Yi Yang, Kar Yan Tam
Categories: cs.CL
Comments: 9 pages, 8 figures, 2 tables (13 pages, 12 figures, 13 tables
  including references and appendices)
\\
  Large Language Models (LLMs) can make up answers that are not real, and this
is known as hallucination. This research aims to see if, how, and to what
extent LLMs are aware of hallucination. More specifically, we check whether and
how an LLM reacts differently in its hidden states when it answers a question
right versus when it hallucinates. To do this, we introduce an experimental
framework which allows examining LLM's hidden states in different hallucination
situations. Building upon this framework, we conduct a series of experiments
with language models in the LLaMA family (Touvron et al., 2023). Our empirical
findings suggest that LLMs react differently when processing a genuine response
versus a fabricated one. We then apply various model interpretation techniques
to help understand and explain the findings better. Moreover, informed by the
empirical observations, we show great potential of using the guidance derived
from LLM's hidden representation space to mitigate hallucination. We believe
this work provides insights into how LLMs produce hallucinated answers and how
to make them occur less often.
\\ ( https://arxiv.org/abs/2402.09733 ,  6852kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09738
Date: Thu, 15 Feb 2024 06:34:15 GMT   (2908kb,D)

Title: Align before Attend: Aligning Visual and Textual Features for Multimodal
  Hateful Content Detection
Authors: Eftekhar Hossain, Omar Sharif, Mohammed Moshiul Hoque, Sarah M. Preum
Categories: cs.CL
Comments: Accepted to EACL-SRW, 2024
\\
  Multimodal hateful content detection is a challenging task that requires
complex reasoning across visual and textual modalities. Therefore, creating a
meaningful multimodal representation that effectively captures the interplay
between visual and textual features through intermediate fusion is critical.
Conventional fusion techniques are unable to attend to the modality-specific
features effectively. Moreover, most studies exclusively concentrated on
English and overlooked other low-resource languages. This paper proposes a
context-aware attention framework for multimodal hateful content detection and
assesses it for both English and non-English languages. The proposed approach
incorporates an attention layer to meaningfully align the visual and textual
features. This alignment enables selective focus on modality-specific features
before fusing them. We evaluate the proposed approach on two benchmark hateful
meme datasets, viz. MUTE (Bengali code-mixed) and MultiOFF (English).
Evaluation results demonstrate our proposed approach's effectiveness with
F1-scores of $69.7$% and $70.3$% for the MUTE and MultiOFF datasets. The scores
show approximately $2.5$% and $3.2$% performance improvement over the
state-of-the-art systems on these datasets. Our implementation is available at
https://github.com/eftekhar-hossain/Bengali-Hateful-Memes.
\\ ( https://arxiv.org/abs/2402.09738 ,  2908kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09739
Date: Thu, 15 Feb 2024 06:36:07 GMT   (8494kb,D)

Title: QuRating: Selecting High-Quality Data for Training Language Models
Authors: Alexander Wettig, Aatmik Gupta, Saumya Malik, Danqi Chen
Categories: cs.CL cs.LG
Comments: The code, models and data are available at
  https://github.com/princeton-nlp/QuRating
\\
  Selecting high-quality pre-training data is important for creating capable
language models, but existing methods rely on simple heuristics. We introduce
QuRating, a method for selecting pre-training data that captures the abstract
qualities of texts which humans intuitively perceive. In this paper, we
investigate four qualities - writing style, required expertise, facts & trivia,
and educational value. We find that LLMs are able to discern these qualities
and observe that they are better at making pairwise judgments of texts than at
rating the quality of a text directly. We train a QuRater model to learn scalar
ratings from pairwise judgments, and use it to annotate a 260B training corpus
with quality ratings for each of the four criteria. In our experiments, we
select 30B tokens according to the different quality ratings and train
1.3B-parameter language models on the selected data. We find that it is
important to balance quality and diversity, as selecting only the highest-rated
documents leads to poor results. When we sample using quality ratings as logits
over documents, our models achieve lower perplexity and stronger in-context
learning performance than baselines. Beyond data selection, we use the quality
ratings to construct a training curriculum which improves performance without
changing the training dataset. We extensively analyze the quality ratings and
discuss their characteristics, biases, and wider implications.
\\ ( https://arxiv.org/abs/2402.09739 ,  8494kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09742
Date: Thu, 15 Feb 2024 06:46:48 GMT   (9452kb,D)

Title: AI Hospital: Interactive Evaluation and Collaboration of LLMs as Intern
  Doctors for Clinical Diagnosis
Authors: Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi,
  Fei Huang, Jingren Zhou
Categories: cs.CL
\\
  The incorporation of Large Language Models (LLMs) in healthcare marks a
significant advancement. However, the application has predominantly been
limited to discriminative and question-answering tasks, which does not fully
leverage their interactive potential. To address this limitation, our paper
presents AI Hospital, a framework designed to build a real-time interactive
diagnosis environment. To simulate the procedure, we collect high-quality
medical records to create patient, examiner, and medical director agents. AI
Hospital is then utilized for the interactive evaluation and collaboration of
LLMs. Initially, we create a Multi-View Medical Evaluation (MVME) benchmark
where various LLMs serve as intern doctors for interactive diagnosis.
Subsequently, to improve diagnostic accuracy, we introduce a collaborative
mechanism that involves iterative discussions and a dispute resolution process
under the supervision of the medical director. In our experiments, we validate
the reliability of AI Hospital. The results not only explore the feasibility of
apply LLMs in clinical consultation but also confirm the effectiveness of the
dispute resolution focused collaboration method.
\\ ( https://arxiv.org/abs/2402.09742 ,  9452kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09748
Date: Thu, 15 Feb 2024 06:58:30 GMT   (1907kb,D)

Title: Model Compression and Efficient Inference for Large Language Models: A
  Survey
Authors: Wenxiao Wang, Wei Chen, Yicong Luo, Yongliu Long, Zhengkai Lin, Liye
  Zhang, Binbin Lin, Deng Cai, and Xiaofei He
Categories: cs.CL cs.AI cs.LG cs.PF
Comments: 47 pages, review 380 papers. The work is ongoing
\\
  Transformer based large language models have achieved tremendous success.
However, the significant memory and computational costs incurred during the
inference process make it challenging to deploy large models on
resource-constrained devices. In this paper, we investigate compression and
efficient inference methods for large language models from an algorithmic
perspective. Regarding taxonomy, similar to smaller models, compression and
acceleration algorithms for large language models can still be categorized into
quantization, pruning, distillation, compact architecture design, dynamic
networks. However, Large language models have two prominent characteristics
compared to smaller models: (1) Most of compression algorithms require
finetuning or even retraining the model after compression. The most notable
aspect of large models is the very high cost associated with model finetuning
or training. Therefore, many algorithms for large models, such as quantization
and pruning, start to explore tuning-free algorithms. (2) Large models
emphasize versatility and generalization rather than performance on a single
task. Hence, many algorithms, such as knowledge distillation, focus on how to
preserving their versatility and generalization after compression. Since these
two characteristics were not very pronounced in early large models, we further
distinguish large language models into medium models and ``real'' large models.
Additionally, we also provide an introduction to some mature frameworks for
efficient inference of large models, which can support basic compression or
acceleration algorithms, greatly facilitating model deployment for users.
\\ ( https://arxiv.org/abs/2402.09748 ,  1907kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09759
Date: Thu, 15 Feb 2024 07:17:10 GMT   (468kb,D)

Title: Efficient Language Adaptive Pre-training: Extending State-of-the-Art
  Large Language Models for Polish
Authors: Szymon Ruci\'nski
Categories: cs.CL cs.AI
Comments: 10 pages
\\
  This study explores the potential of fine-tuning foundational English Large
Language Models (LLMs) for generating Polish text. The first step involves
Language Adaptive Pre-training (LAPT) on a high-quality dataset of 3.11 GB,
consisting of 276 million Polish tokens. The LAPT is followed by additional
fine-tuning aimed at solving nine KLEJ challenges. Our trained model
Curie-7B-v1 not only generates Polish text with the lowest perplexity of 3.02
among decoder-based Polish models but also closely rivals the performance of
the best Polish encoder-decoder models with a less than 2% gap on 8 out of 9
tasks. Curie-7B-v1 used approximately 2-3% of a typical dataset size to learn
Polish. The LAPT was completed in less than five days using a consumer GPU,
highlighting the method's efficiency. The proficiency of the model in Polish
was significantly enhanced, demonstrating the viability of this approach for
adding new languages to existing LLMs by training just 1.2% of its parameters.
To contribute to the community's collaborative progress, the model has been
released as open-source.
\\ ( https://arxiv.org/abs/2402.09759 ,  468kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09760
Date: Thu, 15 Feb 2024 07:22:04 GMT   (7707kb,D)

Title: Grounding Language Model with Chunking-Free In-Context Retrieval
Authors: Hongjin Qian, Zheng Liu, Kelong Mao, Yujia Zhou, Zhicheng Dou
Categories: cs.CL cs.AI cs.IR
\\
  This paper presents a novel Chunking-Free In-Context (CFIC) retrieval
approach, specifically tailored for Retrieval-Augmented Generation (RAG)
systems. Traditional RAG systems often struggle with grounding responses using
precise evidence text due to the challenges of processing lengthy documents and
filtering out irrelevant content. Commonly employed solutions, such as document
chunking and adapting language models to handle longer contexts, have their
limitations. These methods either disrupt the semantic coherence of the text or
fail to effectively address the issues of noise and inaccuracy in evidence
retrieval.
  CFIC addresses these challenges by circumventing the conventional chunking
process. It utilizes the encoded hidden states of documents for in-context
retrieval, employing auto-aggressive decoding to accurately identify the
specific evidence text required for user queries, eliminating the need for
chunking. CFIC is further enhanced by incorporating two decoding strategies,
namely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies
not only improve the efficiency of the retrieval process but also ensure that
the fidelity of the generated grounding text evidence is maintained. Our
evaluations of CFIC on a range of open QA datasets demonstrate its superiority
in retrieving relevant and accurate evidence, offering a significant
improvement over traditional methods. By doing away with the need for document
chunking, CFIC presents a more streamlined, effective, and efficient retrieval
solution, making it a valuable advancement in the field of RAG systems.
\\ ( https://arxiv.org/abs/2402.09760 ,  7707kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09773
Date: Thu, 15 Feb 2024 08:03:12 GMT   (307kb,D)

Title: NutePrune: Efficient Progressive Pruning with Numerous Teachers for
  Large Language Models
Authors: Shengrui Li, Xueting Han, Jing Bai
Categories: cs.CL
\\
  The considerable size of Large Language Models (LLMs) presents notable
deployment challenges, particularly on resource-constrained hardware.
Structured pruning, offers an effective means to compress LLMs, thereby
reducing storage costs and enhancing inference speed for more efficient
utilization. In this work, we study data-efficient and resource-efficient
structure pruning methods to obtain smaller yet still powerful models.
Knowledge Distillation is well-suited for pruning, as the intact model can
serve as an excellent teacher for pruned students. However, it becomes
challenging in the context of LLMs due to memory constraints. To address this,
we propose an efficient progressive Numerous-teacher pruning method
(NutePrune). NutePrune mitigates excessive memory costs by loading only one
intact model and integrating it with various masks and LoRA modules, enabling
it to seamlessly switch between teacher and student roles. This approach allows
us to leverage numerous teachers with varying capacities to progressively guide
the pruned model, enhancing overall performance. Extensive experiments across
various tasks demonstrate the effectiveness of NutePrune. In LLaMA-7B zero-shot
experiments, NutePrune retains 97.17% of the performance of the original model
at 20% sparsity and 95.07% at 25% sparsity.
\\ ( https://arxiv.org/abs/2402.09773 ,  307kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09801
Date: Thu, 15 Feb 2024 08:58:03 GMT   (387kb,D)

Title: EFUF: Efficient Fine-grained Unlearning Framework for Mitigating
  Hallucinations in Multimodal Large Language Models
Authors: Shangyu Xing, Fei Zhao, Zhen Wu, Tuo An, Weihao Chen, Chunhui Li,
  Jianbing Zhang and Xinyu Dai
Categories: cs.CL cs.CV
\\
  Multimodal large language models (MLLMs) have attracted increasing attention
in the past few years, but they may still generate descriptions that include
objects not present in the corresponding images, a phenomenon known as object
hallucination. To eliminate hallucinations, existing methods manually annotate
paired responses with and without hallucinations, and then employ various
alignment algorithms to improve the alignment capability between images and
text. However, they not only demand considerable computation resources during
the finetuning stage but also require expensive human annotation to construct
paired data needed by the alignment algorithms. To address these issues, we
borrow the idea of unlearning and propose an efficient fine-grained unlearning
framework (EFUF), which can eliminate hallucinations without the need for
paired data. Extensive experiments show that our method consistently reduces
hallucinations while preserving the generation quality with modest
computational overhead. Our code and datasets will be publicly available.
\\ ( https://arxiv.org/abs/2402.09801 ,  387kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09808
Date: Thu, 15 Feb 2024 09:14:53 GMT   (2644kb,D)

Title: Knowledge of Pretrained Language Models on Surface Information of Tokens
Authors: Tatsuya Hiraoka, Naoaki Okazaki
Categories: cs.CL
\\
  Do pretrained language models have knowledge regarding the surface
information of tokens? We examined the surface information stored in word or
subword embeddings acquired by pretrained language models from the perspectives
of token length, substrings, and token constitution. Additionally, we evaluated
the ability of models to generate knowledge regarding token surfaces. We
focused on 12 pretrained language models that were mainly trained on English
and Japanese corpora. Experimental results demonstrate that pretrained language
models have knowledge regarding token length and substrings but not token
constitution. Additionally, the results imply that there is a bottleneck on the
decoder side in terms of effectively utilizing acquired knowledge.
\\ ( https://arxiv.org/abs/2402.09808 ,  2644kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09841
Date: Thu, 15 Feb 2024 10:00:49 GMT   (2235kb,D)

Title: LAPDoc: Layout-Aware Prompting for Documents
Authors: Marcel Lamott, Yves-Noel Weweler, Adrian Ulges, Faisal Shafait, Dirk
  Krechel, Darko Obradovic
Categories: cs.CL cs.LG
Comments: Under review at ICDAR2024
\\
  Recent advances in training large language models (LLMs) using massive
amounts of solely textual data lead to strong generalization across many
domains and tasks, including document-specific tasks. Opposed to that there is
a trend to train multi-modal transformer architectures tailored for document
understanding that are designed specifically to fuse textual inputs with the
corresponding document layout. This involves a separate fine-tuning step for
which additional training data is required. At present, no document
transformers with comparable generalization to LLMs are available That raises
the question which type of model is to be preferred for document understanding
tasks. In this paper we investigate the possibility to use purely text-based
LLMs for document-specific tasks by using layout enrichment. We explore drop-in
modifications and rule-based methods to enrich purely textual LLM prompts with
layout information. In our experiments we investigate the effects on the
commercial ChatGPT model and the open-source LLM Solar. We demonstrate that
using our approach both LLMs show improved performance on various standard
document benchmarks. In addition, we study the impact of noisy OCR and layout
errors, as well as the limitations of LLMs when it comes to utilizing document
layout. Our results indicate that layout enrichment can improve the performance
of purely text-based LLMs for document understanding by up to 15% compared to
just using plain document text. In conclusion, this approach should be
considered for the best model choice between text-based LLM or multi-modal
document transformers.
\\ ( https://arxiv.org/abs/2402.09841 ,  2235kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09874
Date: Thu, 15 Feb 2024 10:58:22 GMT   (1229kb,D)

Title: Camouflage is all you need: Evaluating and Enhancing Language Model
  Robustness Against Camouflage Adversarial Attacks
Authors: \'Alvaro Huertas-Garc\'ia, Alejandro Mart\'in, Javier Huertas-Tato,
  David Camacho
Categories: cs.CL
Comments: 19 pages, 8 figures, 5 tables
\\
  Adversarial attacks represent a substantial challenge in Natural Language
Processing (NLP). This study undertakes a systematic exploration of this
challenge in two distinct phases: vulnerability evaluation and resilience
enhancement of Transformer-based models under adversarial attacks.
  In the evaluation phase, we assess the susceptibility of three Transformer
configurations, encoder-decoder, encoder-only, and decoder-only setups, to
adversarial attacks of escalating complexity across datasets containing
offensive language and misinformation. Encoder-only models manifest a 14% and
21% performance drop in offensive language detection and misinformation
detection tasks, respectively. Decoder-only models register a 16% decrease in
both tasks, while encoder-decoder models exhibit a maximum performance drop of
14% and 26% in the respective tasks.
  The resilience-enhancement phase employs adversarial training, integrating
pre-camouflaged and dynamically altered data. This approach effectively reduces
the performance drop in encoder-only models to an average of 5% in offensive
language detection and 2% in misinformation detection tasks. Decoder-only
models, occasionally exceeding original performance, limit the performance drop
to 7% and 2% in the respective tasks. Although not surpassing the original
performance, Encoder-decoder models can reduce the drop to an average of 6% and
2% respectively.
  Results suggest a trade-off between performance and robustness, with some
models maintaining similar performance while gaining robustness. Our study and
adversarial training techniques have been incorporated into an open-source tool
for generating camouflaged datasets. However, methodology effectiveness depends
on the specific camouflage technique and data encountered, emphasizing the need
for continued exploration.
\\ ( https://arxiv.org/abs/2402.09874 ,  1229kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09906
Date: Thu, 15 Feb 2024 12:12:19 GMT   (573kb,D)

Title: Generative Representational Instruction Tuning
Authors: Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao
  Yu, Amanpreet Singh, Douwe Kiela
Categories: cs.CL cs.AI cs.LG
Comments: 65 pages (15 main), 25 figures, 33 tables
\\
  All text-based language problems can be reduced to either generation or
embedding. Current models only perform well at one or the other. We introduce
generative representational instruction tuning (GRIT) whereby a large language
model is trained to handle both generative and embedding tasks by
distinguishing between them through instructions. Compared to other open
models, our resulting GritLM 7B sets a new state of the art on the Massive Text
Embedding Benchmark (MTEB) and outperforms all models up to its size on a range
of generative tasks. By scaling up further, GritLM 8x7B outperforms all open
generative language models that we tried while still being among the best
embedding models. Notably, we find that GRIT matches training on only
generative or embedding data, thus we can unify both at no performance loss.
Among other benefits, the unification via GRIT speeds up Retrieval-Augmented
Generation (RAG) by > 60% for long documents, by no longer requiring separate
retrieval and generation models. Models, code, etc. are freely available at
https://github.com/ContextualAI/gritlm.
\\ ( https://arxiv.org/abs/2402.09906 ,  573kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09910
Date: Thu, 15 Feb 2024 12:17:15 GMT   (1017kb,D)

Title: DE-COP: Detecting Copyrighted Content in Language Models Training Data
Authors: Andr\'e V. Duarte, Xuandong Zhao, Arlindo L. Oliveira and Lei Li
Categories: cs.CL cs.LG
ACM-class: I.2
\\
  How can we detect if copyrighted content was used in the training process of
a language model, considering that the training data is typically undisclosed?
We are motivated by the premise that a language model is likely to identify
verbatim excerpts from its training text. We propose DE-COP, a method to
determine whether a piece of copyrighted content was included in training.
DE-COP's core approach is to probe an LLM with multiple-choice questions, whose
options include both verbatim text and their paraphrases. We construct
BookTection, a benchmark with excerpts from 165 books published prior and
subsequent to a model's training cutoff, along with their paraphrases. Our
experiments show that DE-COP surpasses the prior best method by 9.6% in
detection performance (AUC) on models with logits available. Moreover, DE-COP
also achieves an average accuracy of 72% for detecting suspect books on fully
black-box models where prior methods give $\approx$ 4% accuracy. Our code and
datasets are available at https://github.com/avduarte333/DE-COP_Method
\\ ( https://arxiv.org/abs/2402.09910 ,  1017kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09911
Date: Thu, 15 Feb 2024 12:20:02 GMT   (7278kb,D)

Title: Enhancing Large Language Models with Pseudo- and Multisource- Knowledge
  Graphs for Open-ended Question Answering
Authors: Jiaxiang Liu, Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao
Categories: cs.CL cs.AI
\\
  Mitigating the hallucinations of Large Language Models (LLMs) and enhancing
them is a crucial task. Although some existing methods employ model
self-enhancement techniques, they fall short of effectively addressing unknown
factual hallucinations. Using Knowledge Graph (KG) enhancement approaches fails
to address the generalization across different KG sources and the enhancement
of open-ended answer questions simultaneously. To tackle these limitations,
there is a framework that combines Pseudo-Graph Generation and Atomic Knowledge
Verification proposed. The enhancement of LLM using KG in an open-ended
question-answering setting is implemented by leveraging the Pseudo-Graph
Generation. Atomic Knowledge Verification utilizes atomic-level knowledge
querying and verification to achieve generalizability under different KG
sources. Compared to the baseline, this approach yields a minimum improvement
of 11.5 in the ROUGE-L score for open-ended questions. For precise questions,
we observe a minimum accuracy improvement of 7.5. Moreover, there is also
demonstration that this framework exhibits generalizability across different KG
sources. In summary, our results pave the way for enhancing LLMs by
incorporating Pseudo- and Multisource-KGs, particularly in the context of
open-ended questions.
\\ ( https://arxiv.org/abs/2402.09911 ,  7278kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09916
Date: Thu, 15 Feb 2024 12:39:57 GMT   (244kb,D)

Title: BUSTER: a "BUSiness Transaction Entity Recognition" dataset
Authors: Andrea Zugarini and Andrew Zamai and Marco Ernandes and Leonardo
  Rigutini
Categories: cs.CL cs.LG
Comments: The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023), Industry Track
DOI: 10.18653/v1/2023.emnlp-industry.57
\\
  Albeit Natural Language Processing has seen major breakthroughs in the last
few years, transferring such advances into real-world business cases can be
challenging. One of the reasons resides in the displacement between popular
benchmarks and actual data. Lack of supervision, unbalanced classes, noisy data
and long documents often affect real problems in vertical domains such as
finance, law and health. To support industry-oriented research, we present
BUSTER, a BUSiness Transaction Entity Recognition dataset. The dataset consists
of 3779 manually annotated documents on financial transactions. We establish
several baselines exploiting both general-purpose and domain-specific language
models. The best performing model is also used to automatically annotate 6196
documents, which we release as an additional silver corpus to BUSTER.
\\ ( https://arxiv.org/abs/2402.09916 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09923
Date: Thu, 15 Feb 2024 13:03:57 GMT   (895kb,D)

Title: A Dataset of Open-Domain Question Answering with Multiple-Span Answers
Authors: Zhiyi Luo, Yingying Zhang, Shuyun Luo, Ying Zhao, Wentao Lyu
Categories: cs.CL cs.AI
\\
  Multi-span answer extraction, also known as the task of multi-span question
answering (MSQA), is critical for real-world applications, as it requires
extracting multiple pieces of information from a text to answer complex
questions. Despite the active studies and rapid progress in English MSQA
research, there is a notable lack of publicly available MSQA benchmark in
Chinese. Previous efforts for constructing MSQA datasets predominantly
emphasized entity-centric contextualization, resulting in a bias towards
collecting factoid questions and potentially overlooking questions requiring
more detailed descriptive responses. To overcome these limitations, we present
CLEAN, a comprehensive Chinese multi-span question answering dataset that
involves a wide range of open-domain subjects with a substantial number of
instances requiring descriptive answers. Additionally, we provide established
models from relevant literature as baselines for CLEAN. Experimental results
and analysis show the characteristics and challenge of the newly proposed CLEAN
dataset for the community. Our dataset, CLEAN, will be publicly released at
zhiyiluo.site/misc/clean_v1.0_ sample.json.
\\ ( https://arxiv.org/abs/2402.09923 ,  895kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09934
Date: Thu, 15 Feb 2024 13:34:19 GMT   (8282kb,D)

Title: Paying Attention to Deflections: Mining Pragmatic Nuances for
  Whataboutism Detection in Online Discourse
Authors: Khiem Phi, Noushin Salek Faramarzi, Chenlu Wang, Ritwik Banerjee
Categories: cs.CL cs.AI
Comments: 14 pages, 5 figures
ACM-class: I.2.7
\\
  Whataboutism, a potent tool for disrupting narratives and sowing distrust,
remains under-explored in quantitative NLP research. Moreover, past work has
not distinguished its use as a strategy for misinformation and propaganda from
its use as a tool for pragmatic and semantic framing. We introduce new datasets
from Twitter and YouTube, revealing overlaps as well as distinctions between
whataboutism, propaganda, and the tu quoque fallacy. Furthermore, drawing on
recent work in linguistic semantics, we differentiate the `what about' lexical
construct from whataboutism. Our experiments bring to light unique challenges
in its accurate detection, prompting the introduction of a novel method using
attention weights for negative sample mining. We report significant
improvements of 4% and 10% over previous state-of-the-art methods in our
Twitter and YouTube collections, respectively.
\\ ( https://arxiv.org/abs/2402.09934 ,  8282kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09949
Date: Thu, 15 Feb 2024 13:52:23 GMT   (121kb,D)

Title: Multi-Word Tokenization for Sequence Compression
Authors: Leonidas Gee and Leonardo Rigutini and Marco Ernandes and Andrea
  Zugarini
Categories: cs.CL cs.LG
Comments: The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)
Journal-ref: Proceedings of the 2023 Conference on Empirical Methods in Natural
  Language Processing: Industry Track
DOI: 10.18653/v1/2023.emnlp-industry.58
\\
  Large Language Models have proven highly successful at modelling a variety of
tasks. However, this comes at a steep computational cost that hinders wider
industrial uptake. In this pa005 per, we present MWT: a Multi-Word Tokenizer
that goes beyond word boundaries by representing frequent multi-word
expressions as single tokens. MWTs produce a more compact and efficient
tokenization that yields two benefits: (1) Increase in performance due to a
greater coverage of input data given a fixed sequence length and budget; (2)
Faster and lighter inference due to the ability to reduce the sequence length
with negligible drops in performance. Our results show that MWT is more robust
across shorter sequence lengths, thus allowing for major speedups via early
sequence truncation.
\\ ( https://arxiv.org/abs/2402.09949 ,  121kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09954
Date: Thu, 15 Feb 2024 14:03:33 GMT   (32636kb,D)

Title: Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of
  In-Context Learning for Persona-based Dialogue Generation
Authors: Jiashu Pu,Yajing Wan,Yuru Zhang,Jing Chen,Ling Cheng,Qian Shao,Yongzhu
  Chang,Tangjie Lv,Rongsheng Zhang
Categories: cs.CL cs.LG
\\
  Previous in-context learning (ICL) research has focused on tasks such as
classification, machine translation, text2table, etc., while studies on whether
ICL can improve human-like dialogue generation are scarce. Our work fills this
gap by systematically investigating the ICL capabilities of large language
models (LLMs) in persona-based dialogue generation, conducting extensive
experiments on high-quality real human Chinese dialogue datasets. From
experimental results, we draw three conclusions: 1) adjusting prompt
instructions is the most direct, effective, and economical way to improve
generation quality; 2) randomly retrieving demonstrations (demos) achieves the
best results, possibly due to the greater diversity and the amount of effective
information; counter-intuitively, retrieving demos with a context identical to
the query performs the worst; 3) even when we destroy the multi-turn
associations and single-turn semantics in the demos, increasing the number of
demos still improves dialogue performance, proving that LLMs can learn from
corrupted dialogue demos. Previous explanations of the ICL mechanism, such as
$n$-gram induction head, cannot fully account for this phenomenon.
\\ ( https://arxiv.org/abs/2402.09954 ,  32636kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09967
Date: Thu, 15 Feb 2024 14:21:30 GMT   (115kb)

Title: Case Study: Testing Model Capabilities in Some Reasoning Tasks
Authors: Min Zhang, Sato Takumi, Jack Zhang, Jun Wang
Categories: cs.CL
Comments: Work in Progress
\\
  Large Language Models (LLMs) excel in generating personalized content and
facilitating interactive dialogues, showcasing their remarkable aptitude for a
myriad of applications. However, their capabilities in reasoning and providing
explainable outputs, especially within the context of reasoning abilities,
remain areas for improvement. In this study, we delve into the reasoning
abilities of LLMs, highlighting the current challenges and limitations that
hinder their effectiveness in complex reasoning scenarios.
\\ ( https://arxiv.org/abs/2402.09967 ,  115kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09977
Date: Thu, 15 Feb 2024 14:37:07 GMT   (225kb,D)

Title: Fast Vocabulary Transfer for Language Model Compression
Authors: Leonidas Gee and Andrea Zugarini and Leonardo Rigutini and Paolo
  Torroni
Categories: cs.CL cs.AI cs.LG
Comments: The 2022 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2022)
Journal-ref: Proceedings of the 2022 Conference on Empirical Methods in Natural
  Language Processing (EMNLP 2022): Industry Track
DOI: 10.18653/v1/2022.emnlp-industry.41
\\
  Real-world business applications require a trade-off between language model
performance and size. We propose a new method for model compression that relies
on vocabulary transfer. We evaluate the method on various vertical domains and
downstream tasks. Our results indicate that vocabulary transfer can be
effectively used in combination with other compression techniques, yielding a
significant reduction in model size and inference time while marginally
compromising on performance.
\\ ( https://arxiv.org/abs/2402.09977 ,  225kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10013
Date: Thu, 15 Feb 2024 15:25:30 GMT   (1766kb,D)

Title: Bridging the Empirical-Theoretical Gap in Neural Network Formal Language
  Learning Using Minimum Description Length
Authors: Nur Lan, Emmanuel Chemla, Roni Katzir
Categories: cs.CL cs.FL
Comments: 9 pages, 5 figures, 3 appendix pages
\\
  Neural networks offer good approximation to many tasks but consistently fail
to reach perfect generalization, even when theoretical work shows that such
perfect solutions can be expressed by certain architectures. Using the task of
formal language learning, we focus on one simple formal language and show that
the theoretically correct solution is in fact not an optimum of commonly used
objectives -- even with regularization techniques that according to common
wisdom should lead to simple weights and good generalization (L1, L2) or other
meta-heuristics (early-stopping, dropout). However, replacing standard targets
with the Minimum Description Length objective (MDL) results in the correct
solution being an optimum.
\\ ( https://arxiv.org/abs/2402.10013 ,  1766kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10024
Date: Thu, 15 Feb 2024 15:43:05 GMT   (8010kb,D)

Title: Self-Augmented In-Context Learning for Unsupervised Word Translation
Authors: Yaoyiran Li, Anna Korhonen, Ivan Vuli\'c
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: 10 Pages, 3 Figures, 7 Tables
\\
  Recent work has shown that, while large language models (LLMs) demonstrate
strong word translation or bilingual lexicon induction (BLI) capabilities in
few-shot setups, they still cannot match the performance of 'traditional'
mapping-based approaches in the unsupervised scenario where no seed translation
pairs are available, especially for lower-resource languages. To address this
challenge with LLMs, we propose self-augmented in-context learning (SAIL) for
unsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a
set of high-confidence word translation pairs for in-context learning (ICL)
from an LLM, which it then reapplies to the same LLM in the ICL fashion. Our
method shows substantial gains over zero-shot prompting of LLMs on two
established BLI benchmarks spanning a wide range of language pairs, also
outperforming mapping-based baselines across the board. In addition to
achieving state-of-the-art unsupervised BLI performance, we also conduct
comprehensive analyses on SAIL and discuss its limitations.
\\ ( https://arxiv.org/abs/2402.10024 ,  8010kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10038
Date: Thu, 15 Feb 2024 16:00:58 GMT   (1063kb,D)

Title: RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization
  Method for Alignment of Large Language Models
Authors: Saeed Khaki, JinJin Li, Lan Ma, Liu Yang, Prathap Ramachandra
Categories: cs.CL cs.AI cs.CV cs.LG
Comments: 16 pages, 4 figures
\\
  Reinforcement learning from human feedback (RLHF) has been extensively
employed to align large language models with user intent. However, proximal
policy optimization (PPO) based RLHF is occasionally unstable requiring
significant hyperparameter finetuning, and computationally expensive to
maximize the estimated reward during alignment. Recently, direct preference
optimization (DPO) is proposed to address those challenges. However, DPO relies
on contrastive responses generated from human annotator and alternative LLM,
instead of the policy model, limiting the effectiveness of the RLHF. In this
paper, we addresses both challenges by systematically combining rejection
sampling (RS) and DPO. Our proposed method, RS-DPO, initiates with the
development of a supervised fine-tuned policy model (SFT). A varied set of k
responses per prompt are sampled directly from the SFT model. RS-DPO identifies
pairs of contrastive samples based on their reward distribution. Finally, we
apply DPO with the contrastive samples to align the model to human preference.
Our experiments indicate that our proposed method effectively fine-tunes LLMs
with limited resource environments, leading to improved alignment with user
intent. Furthermore, it outperforms existing methods, including RS, PPO, and
DPO.
\\ ( https://arxiv.org/abs/2402.10038 ,  1063kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10052
Date: Thu, 15 Feb 2024 16:21:14 GMT   (2439kb,D)

Title: Unmemorization in Large Language Models via Self-Distillation and
  Deliberate Imagination
Authors: Yijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, Ivan
  Vuli\'c
Categories: cs.CL cs.AI
\\
  While displaying impressive generation capabilities across many tasks, Large
Language Models (LLMs) still struggle with crucial issues of privacy violation
and unwanted exposure of sensitive data. This raises an essential question: how
should we prevent such undesired behavior of LLMs while maintaining their
strong generation and natural language understanding (NLU) capabilities? In
this work, we introduce a novel approach termed deliberate imagination in the
context of LLM unlearning. Instead of trying to forget memorized data, we
employ a self-distillation framework, guiding LLMs to deliberately imagine
alternative scenarios. As demonstrated in a wide range of experiments, the
proposed method not only effectively unlearns targeted text but also preserves
the LLMs' capabilities in open-ended generation tasks as well as in NLU tasks.
Our results demonstrate the usefulness of this approach across different models
and sizes, and also with parameter-efficient fine-tuning, offering a novel
pathway to addressing the challenges with private and sensitive data in LLM
applications.
\\ ( https://arxiv.org/abs/2402.10052 ,  2439kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10058
Date: Thu, 15 Feb 2024 16:28:34 GMT   (526kb,D)

Title: Towards Safer Large Language Models through Machine Unlearning
Authors: Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, Meng Jiang
Categories: cs.CL
Comments: 13 pages in total
\\
  The rapid advancement of Large Language Models (LLMs) has demonstrated their
vast potential across various domains, attributed to their extensive
pretraining knowledge and exceptional generalizability. However, LLMs often
encounter challenges in generating harmful content when faced with problematic
prompts. To address this problem, existing work attempted to implement a
gradient ascent based approach to prevent LLMs from producing harmful output.
While these methods can be effective, they frequently impact the model utility
in responding to normal prompts. To address this gap, we introduce Selective
Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs,
designed to eliminate harmful knowledge while preserving utility on normal
prompts. Specifically, SKU is consisted of two stages: harmful knowledge
acquisition stage and knowledge negation stage. The first stage aims to
identify and acquire harmful knowledge within the model, whereas the second is
dedicated to remove this knowledge. SKU selectively isolates and removes
harmful knowledge in model parameters, ensuring the model's performance remains
robust on normal prompts. Our experiments conducted across various LLM
architectures demonstrate that SKU identifies a good balance point between
removing harmful information and preserving utility.
\\ ( https://arxiv.org/abs/2402.10058 ,  526kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10073
Date: Thu, 15 Feb 2024 16:36:04 GMT   (838kb,D)

Title: Both Matter: Enhancing the Emotional Intelligence of Large Language
  Models without Compromising the General Intelligence
Authors: Weixiang Zhao, Zhuojun Li, Shilong Wang, Yang Wang, Yulin Hu, Yanyan
  Zhao, Chen Wei, Bing Qin
Categories: cs.CL
\\
  Emotional Intelligence (EI), consisting of emotion perception, emotion
cognition and emotion expression, plays the critical roles in improving user
interaction experience for the current large language model (LLM) based
conversational general AI assistants. Previous works mainly focus on raising
the emotion perception ability of them via naive fine-tuning on EI-related
classification or regression tasks. However, this leads to the incomplete
enhancement of EI and catastrophic forgetting of the general intelligence (GI).
To this end, we first introduce \textsc{EiBench}, a large-scale collection of
EI-related tasks in the text-to-text formation with task instructions that
covers all three aspects of EI, which lays a solid foundation for the
comprehensive EI enhancement of LLMs. Then a novel \underline{\textbf{Mo}}dular
\underline{\textbf{E}}motional \underline{\textbf{I}}ntelligence enhancement
method (\textbf{MoEI}), consisting of Modular Parameter Expansion and
intra-inter modulation, is proposed to comprehensively enhance the EI of LLMs
without compromise their GI. Extensive experiments on two representative
LLM-based assistants, Flan-T5 and LLaMA-2-Chat, demonstrate the effectiveness
of MoEI to improving EI while maintain GI.
\\ ( https://arxiv.org/abs/2402.10073 ,  838kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10107
Date: Thu, 15 Feb 2024 17:02:48 GMT   (1697kb,D)

Title: Quantized Embedding Vectors for Controllable Diffusion Language Models
Authors: Cheng Kang, Xinye Chen, Yong Hu, Daniel Novak
Categories: cs.CL cs.AI
\\
  Improving the controllability, portability, and inference speed of diffusion
language models (DLMs) is a key challenge in natural language generation. While
recent research has shown significant success in complex text generation with
language models, the memory and computational power are still very demanding
and fall short of expectations, which naturally results in low portability and
instability for the models. To mitigate these issues, numerous well-established
methods were proposed for neural network quantization. To further enhance their
portability of independent deployment as well as improve their stability
evaluated by language perplexity, we propose a novel approach called the
Quantized Embedding Controllable Diffusion Language Model (QE-CDLM). QE-CDLM
builds upon the recent successful controllable DLMs by remodeling the
task-specific embedding space via quantization. This leads to a gradient-based
controller for the generation tasks, and more stable intermediate latent
variables are obtained, which naturally brings in an accelerated convergence as
well as better controllability. Additionally, the adaption fine-tuning method
is employed to reduce tunable weights. Experimental results on five challenging
fine-grained control tasks demonstrate that QE-CDLM compares favorably to
existing methods in terms of quality and feasibility, achieving better
perplexity and lightweight fine-tuning.
\\ ( https://arxiv.org/abs/2402.10107 ,  1697kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10110
Date: Thu, 15 Feb 2024 17:06:21 GMT   (1309kb,D)

Title: Selective Reflection-Tuning: Student-Selected Data Recycling for LLM
  Instruction-Tuning
Authors: Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Jiuxiang Gu, Tianyi Zhou
Categories: cs.CL cs.AI cs.LG
\\
  Instruction tuning is critical to large language models (LLMs) for achieving
better instruction following and task adaptation capabilities but its success
heavily relies on the training data quality. Many recent methods focus on
improving the data quality but often overlook the compatibility of the data
with the student model being finetuned. This paper introduces Selective
Reflection-Tuning, a novel paradigm that synergizes a teacher LLM's reflection
and introspection for improving existing data quality with the data selection
capability of the student LLM, to automatically refine existing
instruction-tuning data. This teacher-student collaboration produces
high-quality and student-compatible instruction-response pairs, resulting in
sample-efficient instruction tuning and LLMs of superior performance. Selective
Reflection-Tuning is a data augmentation and synthesis that generally improves
LLM finetuning and self-improvement without collecting brand-new data. We apply
our method to Alpaca and WizardLM data and achieve much stronger and top-tier
7B and 13B LLMs. Our codes, models, and data will be released at
https://github.com/tianyi-lab/Reflection_Tuning.
\\ ( https://arxiv.org/abs/2402.10110 ,  1309kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10137
Date: Thu, 15 Feb 2024 17:40:02 GMT   (8193kb,D)

Title: TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles
Authors: Yinhong Liu, Yimai Fang, David Vandyke and Nigel Collier
Categories: cs.CL
\\
  In light of recent advances in large language models~(LLMs), the expectations
for the next generation of virtual assistants include enhanced naturalness and
adaptability across diverse usage scenarios. However, the creation of
high-quality annotated data for Task-Oriented Dialog~(TOD) is recognized to be
slow and costly. To address these challenges, we introduce Task-Oriented
Automatic Dialogs~(TOAD), a novel and scalable TOD dataset along with its
automatic generation pipeline. The TOAD dataset simulates realistic app context
interaction and provide a variety of system response style options. Two aspects
of system response styles are considered, verbosity level and users' expression
mirroring. We benchmark TOAD on two response generation tasks and the results
show that modeling more verbose or responses without user expression mirroring
is more challenging.
\\ ( https://arxiv.org/abs/2402.10137 ,  8193kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10151
Date: Thu, 15 Feb 2024 17:58:29 GMT   (2519kb,D)

Title: ControlLM: Crafting Diverse Personalities for Language Models
Authors: Yixuan Weng, Shizhu He, Kang Liu, Shengping Liu, Jun Zhao
Categories: cs.CL
Comments: 17 pages
\\
  As language models continue to scale in size and capability, they display an
array of emerging behaviors, both beneficial and concerning. This heightens the
need to control model behaviors. We hope to be able to control the personality
traits of language models at the inference-time so as to have various character
features, on top of which the requirements of different types of tasks can be
met. Personality is a higher-level and more abstract behavioral representation
for language models. We introduce ControlLM, which leverages differential
activation patterns, derived from contrasting behavioral prompts in the model's
latent space, to influence the model's personality traits at inference. This
approach allows for the precise, real-time adjustment of model behavior. First,
we demonstrate ControlLM's capacity to elicit diverse persona behaviors without
any training, while precision control allows personality traits to closely
match average human values. Subsequently, we showcase improved reasoning and
question answering through selective amplification of beneficial attributes
like conscientiousness and friendliness. We hope that this work will inspire
research on controlling human-like behaviors of language models and provide
insights for future research. Our code is publicly available at:
https://github.com/wengsyx/ControlLM.
\\ ( https://arxiv.org/abs/2402.10151 ,  2519kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10153
Date: Thu, 15 Feb 2024 18:00:02 GMT   (253kb,D)

Title: Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study
  for Diabetes Patients
Authors: Mahyar Abbasian, Zhongqi Yang, Elahe Khatibi, Pengfei Zhang, Nitish
  Nagesh, Iman Azimi, Ramesh Jain, Amir M. Rahmani
Categories: cs.CL
Comments: 4 pages, 3 figures, and 2 tables, conference paper
\\
  Effective diabetes management is crucial for maintaining health in diabetic
patients. Large Language Models (LLMs) have opened new avenues for diabetes
management, facilitating their efficacy. However, current LLM-based approaches
are limited by their dependence on general sources and lack of integration with
domain-specific knowledge, leading to inaccurate responses. In this paper, we
propose a knowledge-infused LLM-powered conversational health agent (CHA) for
diabetic patients. We customize and leverage the open-source openCHA framework,
enhancing our CHA with external knowledge and analytical capabilities. This
integration involves two key components: 1) incorporating the American Diabetes
Association dietary guidelines and the Nutritionix information and 2) deploying
analytical tools that enable nutritional intake calculation and comparison with
the guidelines. We compare the proposed CHA with GPT4. Our evaluation includes
100 diabetes-related questions on daily meal choices and assessing the
potential risks associated with the suggested diet. Our findings show that the
proposed agent demonstrates superior performance in generating responses to
manage essential nutrients.
\\ ( https://arxiv.org/abs/2402.10153 ,  253kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10171
Date: Thu, 15 Feb 2024 18:19:16 GMT   (1657kb,D)

Title: Data Engineering for Scaling Language Models to 128K Context
Authors: Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi,
  Yoon Kim and Hao Peng
Categories: cs.CL cs.AI
Comments: Code at https://github.com/FranxYao/Long-Context-Data-Engineering
\\
  We study the continual pretraining recipe for scaling language models'
context lengths to 128K, with a focus on data engineering. We hypothesize that
long context modeling, in particular \textit{the ability to utilize information
at arbitrary input locations}, is a capability that is mostly already acquired
through large-scale pretraining, and that this capability can be readily
extended to contexts substantially longer than seen during training~(e.g., 4K
to 128K) through lightweight continual pretraining on appropriate data mixture.
We investigate the \textit{quantity} and \textit{quality} of the data for
continual pretraining: (1) for quantity, we show that 500 million to 5 billion
tokens are enough to enable the model to retrieve information anywhere within
the 128K context; (2) for quality, our results equally emphasize \textit{domain
balance} and \textit{length upsampling}. Concretely, we find that naively
upsampling longer data on certain domains like books, a common practice of
existing work, gives suboptimal performance, and that a balanced domain mixture
is important. We demonstrate that continual pretraining of the full model on
1B-5B tokens of such data is an effective and affordable strategy for scaling
the context length of language models to 128K. Our recipe outperforms strong
open-source long-context models and closes the gap to frontier models like
GPT-4 128K.
\\ ( https://arxiv.org/abs/2402.10171 ,  1657kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10175
Date: Thu, 15 Feb 2024 18:23:39 GMT   (7625kb,D)

Title: Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for
  Positional Discourse Coherence
Authors: Yinhong Liu, Yixuan Su, Ehsan Shareghi and Nigel Collier
Categories: cs.CL
\\
  Recent large language models (LLMs) have shown remarkable performance in
aligning generated text with user intentions across various tasks. When it
comes to long-form text generation, there has been a growing interest in
generation from a discourse coherence perspective. However, existing lexical or
semantic metrics such as BLEU, ROUGE, BertScore cannot effectively capture the
discourse coherence. The development of discourse-specific automatic evaluation
methods for assessing the output of LLMs warrants greater focus and
exploration. In this paper, we present a novel automatic metric designed to
quantify the discourse divergence between two long-form articles. Extensive
experiments on three datasets from representative domains demonstrate that our
metric aligns more closely with human preferences and GPT-4 coherence
evaluation, outperforming existing evaluation methods.
\\ ( https://arxiv.org/abs/2402.10175 ,  7625kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10176
Date: Thu, 15 Feb 2024 18:26:11 GMT   (388kb,D)

Title: OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset
Authors: Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei
  Jia, Igor Gitman
Categories: cs.CL cs.AI cs.LG
Comments: Data and models are available at
  https://huggingface.co/collections/nvidia/openmath-65c5619de2ba059be0775014
\\
  Recent work has shown the immense potential of synthetically generated
datasets for training large language models (LLMs), especially for acquiring
targeted skills. Current large-scale math instruction tuning datasets such as
MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed
using outputs from closed-source LLMs with commercially restrictive licenses. A
key reason limiting the use of open-source LLMs in these data generation
pipelines has been the wide gap between the mathematical skills of the best
closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on
the recent progress in open-source LLMs, our proposed prompting novelty, and
some brute-force scaling, we construct OpenMathInstruct-1, a math instruction
tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by
synthesizing code-interpreter solutions for GSM8K and MATH, two popular math
reasoning benchmarks, using the recently released and permissively licensed
Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of
OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which
is competitive with the best gpt-distilled models. We release our code, models,
and the OpenMathInstruct-1 dataset under a commercially permissive license.
\\ ( https://arxiv.org/abs/2402.10176 ,  388kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10178
Date: Thu, 15 Feb 2024 18:27:37 GMT   (256kb,D)

Title: TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and
  Agent Generation
Authors: Yaoxiang Wang, Zhiyong Wu, Junfeng Yao, Jinsong Su
Categories: cs.CL
\\
  The emergence of Large Language Models (LLMs) like ChatGPT has inspired the
development of LLM-based agents capable of addressing complex, real-world
tasks. However, these agents often struggle during task execution due to
methodological constraints, such as error propagation and limited adaptability.
To address this issue, we propose a multi-agent framework based on dynamic Task
Decomposition and Agent Generation (TDAG). This framework dynamically
decomposes complex tasks into smaller subtasks and assigns each to a
specifically generated subagent, thereby enhancing adaptability in diverse and
unpredictable real-world tasks. Simultaneously, existing benchmarks often lack
the granularity needed to evaluate incremental progress in complex, multi-step
tasks. In response, we introduce ItineraryBench in the context of travel
planning, featuring interconnected, progressively complex tasks with a
fine-grained evaluation system. ItineraryBench is designed to assess agents'
abilities in memory, planning, and tool usage across tasks of varying
complexity. Our experimental results reveal that TDAG significantly outperforms
established baselines, showcasing its superior adaptability and context
awareness in complex task scenarios.
\\ ( https://arxiv.org/abs/2402.10178 ,  256kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10189
Date: Thu, 15 Feb 2024 18:46:24 GMT   (1451kb,D)

Title: Uncertainty Decomposition and Quantification for In-Context Learning of
  Large Language Models
Authors: Chen Ling, Xujiang Zhao, Wei Cheng, Yanchi Liu, Yiyou Sun, Xuchao
  Zhang, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, Guangji Bai, Liang
  Zhao, Haifeng Chen
Categories: cs.CL cs.LG
\\
  In-context learning has emerged as a groundbreaking ability of Large Language
Models (LLMs) and revolutionized various fields by providing a few
task-relevant demonstrations in the prompt. However, trustworthy issues with
LLM's response, such as hallucination, have also been actively discussed.
Existing works have been devoted to quantifying the uncertainty in LLM's
response, but they often overlook the complex nature of LLMs and the uniqueness
of in-context learning. In this work, we delve into the predictive uncertainty
of LLMs associated with in-context learning, highlighting that such
uncertainties may stem from both the provided demonstrations (aleatoric
uncertainty) and ambiguities tied to the model's configurations (epistemic
uncertainty). We propose a novel formulation and corresponding estimation
method to quantify both types of uncertainties. The proposed method offers an
unsupervised way to understand the prediction of in-context learning in a
plug-and-play fashion. Extensive experiments are conducted to demonstrate the
effectiveness of the decomposition. The code and data are available at:
\url{https://github.com/lingchen0331/UQ_ICL}.
\\ ( https://arxiv.org/abs/2402.10189 ,  1451kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10196
Date: Thu, 15 Feb 2024 18:51:32 GMT   (9676kb,D)

Title: A Trembling House of Cards? Mapping Adversarial Attacks against Language
  Agents
Authors: Lingbo Mo, Zeyi Liao, Boyuan Zheng, Yu Su, Chaowei Xiao, Huan Sun
Categories: cs.CL cs.AI
\\
  Language agents powered by large language models (LLMs) have seen exploding
development. Their capability of using language as a vehicle for thought and
communication lends an incredible level of flexibility and versatility. People
have quickly capitalized on this capability to connect LLMs to a wide range of
external components and environments: databases, tools, the Internet, robotic
embodiment, etc. Many believe an unprecedentedly powerful automation technology
is emerging. However, new automation technologies come with new safety risks,
especially for intricate systems like language agents. There is a surprisingly
large gap between the speed and scale of their development and deployment and
our understanding of their safety risks. Are we building a house of cards? In
this position paper, we present the first systematic effort in mapping
adversarial attacks against language agents. We first present a unified
conceptual framework for agents with three major components: Perception, Brain,
and Action. Under this framework, we present a comprehensive discussion and
propose 12 potential attack scenarios against different components of an agent,
covering different attack strategies (e.g., input manipulation, adversarial
demonstrations, jailbreaking, backdoors). We also draw connections to
successful attack strategies previously applied to LLMs. We emphasize the
urgency to gain a thorough understanding of language agent risks before their
widespread deployment.
\\ ( https://arxiv.org/abs/2402.10196 ,  9676kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10200
Date: Thu, 15 Feb 2024 18:55:41 GMT   (752kb,D)

Title: Chain-of-Thought Reasoning Without Prompting
Authors: Xuezhi Wang, Denny Zhou
Categories: cs.CL
\\
  In enhancing the reasoning capabilities of large language models (LLMs),
prior research primarily focuses on specific prompting techniques such as
few-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while
effective, often involve manually intensive prompt engineering. Our study takes
a novel approach by asking: Can LLMs reason effectively without prompting? Our
findings reveal that, intriguingly, CoT reasoning paths can be elicited from
pre-trained LLMs by simply altering the \textit{decoding} process. Rather than
conventional greedy decoding, we investigate the top-$k$ alternative tokens,
uncovering that CoT paths are frequently inherent in these sequences. This
approach not only bypasses the confounders of prompting but also allows us to
assess the LLMs' \textit{intrinsic} reasoning abilities. Moreover, we observe
that the presence of a CoT in the decoding path correlates with a higher
confidence in the model's decoded answer. This confidence metric effectively
differentiates between CoT and non-CoT paths. Extensive empirical studies on
various reasoning benchmarks show that the proposed CoT-decoding substantially
outperforms the standard greedy decoding.
\\ ( https://arxiv.org/abs/2402.10200 ,  752kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09432
Date: Wed, 24 Jan 2024 22:28:14 GMT   (959kb)

Title: An Enhanced Analysis of Traffic Intelligence in Smart Cities Using
  Sustainable Deep Radial Function
Authors: Ayad Ghany Ismaeel, S.J. Jereesha Mary, C. Anitha, Jaganathan
  Logeshwaran, Sarmad Nozad Mahmood, Sameer Alani, and Akram H. Shather
Categories: cs.LG cs.NI
Comments: 25 pages, 6 figures, and 3 Tables
Journal-ref: Sustainability 2023, 15, x FOR PEER REVIEW
DOI: 10.3390/su151914441
\\
  Smart cities have revolutionized urban living by incorporating sophisticated
technologies to optimize various aspects of urban infrastructure, such as
transportation systems. Effective traffic management is a crucial component of
smart cities, as it has a direct impact on the quality of life of residents and
tourists. Utilizing deep radial basis function (RBF) networks, this paper
describes a novel strategy for enhancing traffic intelligence in smart cities.
Traditional methods of traffic analysis frequently rely on simplistic models
that are incapable of capturing the intricate patterns and dynamics of urban
traffic systems. Deep learning techniques, such as deep RBF networks, have the
potential to extract valuable insights from traffic data and enable more
precise predictions and decisions. In this paper, we propose an RBF based
method for enhancing smart city traffic intelligence. Deep RBF networks combine
the adaptability and generalization capabilities of deep learning with the
discriminative capability of radial basis functions. The proposed method can
effectively learn intricate relationships and nonlinear patterns in traffic
data by leveraging the hierarchical structure of deep neural networks. The deep
RBF model can learn to predict traffic conditions, identify congestion
patterns, and make informed recommendations for optimizing traffic management
strategies by incorporating these rich and diverse data To evaluate the
efficacy of our proposed method, extensive experiments and comparisons with
real world traffic datasets from a smart city environment were conducted. In
terms of prediction accuracy and efficiency, the results demonstrate that the
deep RBF based approach outperforms conventional traffic analysis methods.
Smart city traffic intelligence is enhanced by the model capacity to capture
nonlinear relationships and manage large scale data sets.
\\ ( https://arxiv.org/abs/2402.09432 ,  959kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09456
Date: Wed, 7 Feb 2024 06:10:47 GMT   (1766kb,D)

Title: Optimistic Thompson Sampling for No-Regret Learning in Unknown Games
Authors: Yingru Li, Liangqi Liu, Wenqiang Pi, Hao Liang, Zhi-Quan Luo
Categories: cs.LG cs.AI stat.ML
Comments: Submitted to ICML2024
\\
  Many real-world problems involving multiple decision-makers can be modeled as
an unknown game characterized by partial observations. Addressing the
challenges posed by partial information and the curse of multi-agency, we
developed Thompson sampling-type algorithms, leveraging information about
opponent's action and reward structures. Our approach significantly reduces
experimental budgets, achieving a more than tenfold reduction compared to
baseline algorithms in practical applications like traffic routing and radar
sensing. We demonstrate that, under certain assumptions about the reward
structure, the regret bound exhibits merely a logarithmic dependence on the
total action space size, effectively mitigating the curse of multi-agency.
Additionally, this research introduces the Optimism-then-NoRegret framework, a
novel contribution that integrates both our proposed methodologies and existing
algorithms in the field.
\\ ( https://arxiv.org/abs/2402.09456 ,  1766kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09469
Date: Mon, 12 Feb 2024 05:52:06 GMT   (2690kb,D)

Title: Fourier Circuits in Neural Networks: Unlocking the Potential of Large
  Language Models in Mathematical Reasoning and Modular Arithmetic
Authors: Jiuxiang Gu, Chenyang Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Tianyi
  Zhou
Categories: cs.LG stat.ML
\\
  In the evolving landscape of machine learning, a pivotal challenge lies in
deciphering the internal representations harnessed by neural networks and
Transformers. Building on recent progress toward comprehending how networks
execute distinct target functions, our study embarks on an exploration of the
underlying reasons behind networks adopting specific computational strategies.
We direct our focus to the complex algebraic learning task of modular addition
involving $k$ inputs. Our research presents a thorough analytical
characterization of the features learned by stylized one-hidden layer neural
networks and one-layer Transformers in addressing this task.
  A cornerstone of our theoretical framework is the elucidation of how the
principle of margin maximization shapes the features adopted by one-hidden
layer neural networks. Let $p$ denote the modulus, $D_p$ denote the dataset of
modular arithmetic with $k$ inputs and $m$ denote the network width. We
demonstrate that a neuron count of $ m \geq 2^{2k-2} \cdot (p-1) $, these
networks attain a maximum $ L_{2,k+1} $-margin on the dataset $ D_p $.
Furthermore, we establish that each hidden-layer neuron aligns with a specific
Fourier spectrum, integral to solving modular addition problems.
  By correlating our findings with the empirical observations of similar
studies, we contribute to a deeper comprehension of the intrinsic computational
mechanisms of neural networks. Furthermore, we observe similar computational
mechanisms in the attention matrix of the Transformer. This research stands as
a significant stride in unraveling their operation complexities, particularly
in the realm of complex algebraic tasks.
\\ ( https://arxiv.org/abs/2402.09469 ,  2690kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09470
Date: Mon, 12 Feb 2024 08:16:10 GMT   (10206kb,D)

Title: Rolling Diffusion Models
Authors: David Ruhe, Jonathan Heek, Tim Salimans, Emiel Hoogeboom
Categories: cs.LG stat.ML
\\
  Diffusion models have recently been increasingly applied to temporal data
such as video, fluid mechanics simulations, or climate data. These methods
generally treat subsequent frames equally regarding the amount of noise in the
diffusion process. This paper explores Rolling Diffusion: a new approach that
uses a sliding window denoising process. It ensures that the diffusion process
progressively corrupts through time by assigning more noise to frames that
appear later in a sequence, reflecting greater uncertainty about the future as
the generation process unfolds. Empirically, we show that when the temporal
dynamics are complex, Rolling Diffusion is superior to standard diffusion. In
particular, this result is demonstrated in a video prediction task using the
Kinetics-600 video dataset and in a chaotic fluid dynamics forecasting
experiment.
\\ ( https://arxiv.org/abs/2402.09470 ,  10206kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09471
Date: Mon, 12 Feb 2024 08:38:53 GMT   (2986kb,D)

Title: Machine Learning for Stochastic Parametrisation
Authors: Hannah M. Christensen, Salah Kouhen, Greta Miller, Raghul Parthipan
Categories: cs.LG
Comments: Submitted to Climate Informatics 2024
\\
  Atmospheric models used for weather and climate prediction are traditionally
formulated in a deterministic manner. In other words, given a particular state
of the resolved scale variables, the most likely forcing from the sub-grid
scale processes is estimated and used to predict the evolution of the
large-scale flow. However, the lack of scale-separation in the atmosphere means
that this approach is a large source of error in forecasts. Over recent years,
an alternative paradigm has developed: the use of stochastic techniques to
characterise uncertainty in small-scale processes. These techniques are now
widely used across weather, sub-seasonal, seasonal, and climate timescales. In
parallel, recent years have also seen significant progress in replacing
parametrisation schemes using machine learning (ML). This has the potential to
both speed up and improve our numerical models. However, the focus to date has
largely been on deterministic approaches. In this position paper, we bring
together these two key developments, and discuss the potential for data-driven
approaches for stochastic parametrisation. We highlight early studies in this
area, and draw attention to the novel challenges that remain.
\\ ( https://arxiv.org/abs/2402.09471 ,  2986kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09473
Date: Mon, 12 Feb 2024 10:03:31 GMT   (204kb,D)

Title: One-for-many Counterfactual Explanations by Column Generation
Authors: Andrea Lodi and Jasone Ram\'irez-Ayerbe
Categories: cs.LG stat.ML
\\
  In this paper, we consider the problem of generating a set of counterfactual
explanations for a group of instances, with the one-for-many allocation rule,
where one explanation is allocated to a subgroup of the instances. For the
first time, we solve the problem of minimizing the number of explanations
needed to explain all the instances, while considering sparsity by limiting the
number of features allowed to be changed collectively in each explanation. A
novel column generation framework is developed to efficiently search for the
explanations. Our framework can be applied to any black-box classifier, like
neural networks. Compared with a simple adaptation of a mixed-integer
programming formulation from the literature, the column generation framework
dominates in terms of scalability, computational performance and quality of the
solutions.
\\ ( https://arxiv.org/abs/2402.09473 ,  204kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09486
Date: Wed, 14 Feb 2024 08:09:46 GMT   (3367kb,D)

Title: UMOEA/D: A Multiobjective Evolutionary Algorithm for Uniform Pareto
  Objectives based on Decomposition
Authors: Xiaoyuan Zhang and Xi Lin and Yichi Zhang and Yifan Chen and Qingfu
  Zhang
Categories: cs.LG
\\
  Multiobjective optimization (MOO) is prevalent in numerous applications, in
which a Pareto front (PF) is constructed to display optima under various
preferences. Previous methods commonly utilize the set of Pareto objectives
(particles on the PF) to represent the entire PF. However, the empirical
distribution of the Pareto objectives on the PF is rarely studied, which
implicitly impedes the generation of diverse and representative Pareto
objectives in previous methods. To bridge the gap, we suggest in this paper
constructing \emph{uniformly distributed} Pareto objectives on the PF, so as to
alleviate the limited diversity found in previous MOO approaches. We are the
first to formally define the concept of ``uniformity" for an MOO problem. We
optimize the maximal minimal distances on the Pareto front using a neural
network, resulting in both asymptotically and non-asymptotically uniform Pareto
objectives. Our proposed method is validated through experiments on real-world
and synthetic problems, which demonstrates the efficacy in generating
high-quality uniform Pareto objectives and the encouraging performance
exceeding existing state-of-the-art methods.
  The detailed model implementation and the code are scheduled to be
open-sourced upon publication.
\\ ( https://arxiv.org/abs/2402.09486 ,  3367kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09492
Date: Wed, 14 Feb 2024 11:27:31 GMT   (3063kb,D)

Title: PMGDA: A Preference-based Multiple Gradient Descent Algorithm
Authors: Xiaoyuan Zhang and Xi Lin and Qingfu Zhang
Categories: cs.LG
\\
  It is desirable in many multi-objective machine learning applications, such
as multi-task learning and multi-objective reinforcement learning, to find a
Pareto optimal solution that can exactly match a given preference of
decision-makers. These problems are often large-scale with available gradient
information but cannot be handled very well by the existing algorithms. To
tackle this critical issue, this paper proposes a novel predict-and-correct
framework for locating the exact Pareto optimal solutions required by a
decision maker. In the proposed framework, a constraint function is introduced
in the search progress to align the solution with a user-specific preference,
which can be optimized simultaneously with multiple objective functions.
Experimental results show that our proposed method can efficiently find exact
Pareto optimal solutions for standard benchmarks, multi-task, and
multi-objective reinforcement learning problems with more than thousands of
decision variables.
  Code is available at: \url{https://github.com/xzhang2523/pmgda}.
\\ ( https://arxiv.org/abs/2402.09492 ,  3063kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09529
Date: Wed, 14 Feb 2024 19:09:23 GMT   (2315kb,D)

Title: The Manifold Density Function: An Intrinsic Method for the Validation of
  Manifold Learning
Authors: Benjamin Holmgren, Eli Quist, Jordan Schupbach, Brittany Terese Fasy,
  Bastian Rieck
Categories: cs.LG math.AT
Comments: 24 pages, 6 figures
MSC-class: 57Z25
ACM-class: I.5.2
\\
  We introduce the manifold density function, which is an intrinsic method to
validate manifold learning techniques. Our approach adapts and extends Ripley's
$K$-function, and categorizes in an unsupervised setting the extent to which an
output of a manifold learning algorithm captures the structure of a latent
manifold. Our manifold density function generalizes to broad classes of
Riemannian manifolds. In particular, we extend the manifold density function to
general two-manifolds using the Gauss-Bonnet theorem, and demonstrate that the
manifold density function for hypersurfaces is well approximated using the
first Laplacian eigenvalue. We prove desirable convergence and robustness
properties.
\\ ( https://arxiv.org/abs/2402.09529 ,  2315kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09542
Date: Wed, 14 Feb 2024 19:34:28 GMT   (5384kb,D)

Title: Layerwise Proximal Replay: A Proximal Point Method for Online Continual
  Learning
Authors: Jason Yoo, Yunpeng Liu, Frank Wood, Geoff Pleiss
Categories: cs.LG
\\
  In online continual learning, a neural network incrementally learns from a
non-i.i.d. data stream. Nearly all online continual learning methods employ
experience replay to simultaneously prevent catastrophic forgetting and
underfitting on past data. Our work demonstrates a limitation of this approach:
networks trained with experience replay tend to have unstable optimization
trajectories, impeding their overall accuracy. Surprisingly, these
instabilities persist even when the replay buffer stores all previous training
examples, suggesting that this issue is orthogonal to catastrophic forgetting.
We minimize these instabilities through a simple modification of the
optimization geometry. Our solution, Layerwise Proximal Replay (LPR), balances
learning from new and replay data while only allowing for gradual changes in
the hidden activation of past data. We demonstrate that LPR consistently
improves replay-based online continual learning methods across multiple problem
settings, regardless of the amount of available replay memory.
\\ ( https://arxiv.org/abs/2402.09542 ,  5384kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09550
Date: Wed, 14 Feb 2024 20:01:41 GMT   (26748kb,D)

Title: Dataset Clustering for Improved Offline Policy Learning
Authors: Qiang Wang, Yixin Deng, Francisco Roldan Sanchez, Keru Wang, Kevin
  McGuinness, Noel O'Connor, and Stephen J. Redmond
Categories: cs.LG cs.RO
\\
  Offline policy learning aims to discover decision-making policies from
previously-collected datasets without additional online interactions with the
environment. As the training dataset is fixed, its quality becomes a crucial
determining factor in the performance of the learned policy. This paper studies
a dataset characteristic that we refer to as multi-behavior, indicating that
the dataset is collected using multiple policies that exhibit distinct
behaviors. In contrast, a uni-behavior dataset would be collected solely using
one policy. We observed that policies learned from a uni-behavior dataset
typically outperform those learned from multi-behavior datasets, despite the
uni-behavior dataset having fewer examples and less diversity. Therefore, we
propose a behavior-aware deep clustering approach that partitions
multi-behavior datasets into several uni-behavior subsets, thereby benefiting
downstream policy learning. Our approach is flexible and effective; it can
adaptively estimate the number of clusters while demonstrating high clustering
accuracy, achieving an average Adjusted Rand Index of 0.987 across various
continuous control task datasets. Finally, we present improved policy learning
examples using dataset clustering and discuss several potential scenarios where
our approach might benefit the offline policy learning community.
\\ ( https://arxiv.org/abs/2402.09550 ,  26748kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09560
Date: Wed, 14 Feb 2024 20:21:43 GMT   (425kb,D)

Title: Distribution-Free Rates in Neyman-Pearson Classification
Authors: Mohammadreza M. Kalan, Samory Kpotufe
Categories: cs.LG stat.ML
\\
  We consider the problem of Neyman-Pearson classification which models
unbalanced classification settings where error w.r.t. a distribution $\mu_1$ is
to be minimized subject to low error w.r.t. a different distribution $\mu_0$.
Given a fixed VC class $\mathcal{H}$ of classifiers to be minimized over, we
provide a full characterization of possible distribution-free rates, i.e.,
minimax rates over the space of all pairs $(\mu_0, \mu_1)$. The rates involve a
dichotomy between hard and easy classes $\mathcal{H}$ as characterized by a
simple geometric condition, a three-points-separation condition, loosely
related to VC dimension.
\\ ( https://arxiv.org/abs/2402.09560 ,  425kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09573
Date: Wed, 14 Feb 2024 20:48:58 GMT   (33140kb,D)

Title: Changes by Butterflies: Farsighted Forecasting with Group Reservoir
  Transformer
Authors: Md Kowsher and Jia Xu
Categories: cs.LG cs.CL
\\
  In Chaos, a minor divergence between two initial conditions exhibits
exponential amplification over time, leading to far-away outcomes, known as the
butterfly effect. Thus, the distant future is full of uncertainty and hard to
forecast. We introduce Group Reservoir Transformer to predict long-term events
more accurately and robustly by overcoming two challenges in Chaos: (1) the
extensive historical sequences and (2) the sensitivity to initial conditions. A
reservoir is attached to a Transformer to efficiently handle arbitrarily long
historical lengths, with an extension of a group of reservoirs to reduce the
uncertainty due to the initialization variations. Our architecture consistently
outperforms state-of-the-art DNN models in multivariate time series, including
NLinear, Pyformer, Informer, Autoformer, and the baseline Transformer, with an
error reduction of up to -89.43\% in various fields such as ETTh, ETTm, and air
quality, demonstrating that an ensemble of butterfly learning, the prediction
can be improved to a more adequate and certain one, despite of the traveling
time to the unknown future.
\\ ( https://arxiv.org/abs/2402.09573 ,  33140kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09580
Date: Wed, 14 Feb 2024 21:03:08 GMT   (805kb,D)

Title: Complexity Reduction in Machine Learning-Based Wireless Positioning:
  Minimum Description Features
Authors: Myeung Suk Oh, Anindya Bijoy Das, Taejoon Kim, David J. Love, and
  Christopher G. Brinton
Categories: cs.LG eess.SP
Comments: This paper has been accepted in IEEE International Conference on
  Communications (ICC) 2024
\\
  A recent line of research has been investigating deep learning approaches to
wireless positioning (WP). Although these WP algorithms have demonstrated high
accuracy and robust performance against diverse channel conditions, they also
have a major drawback: they require processing high-dimensional features, which
can be prohibitive for mobile applications. In this work, we design a
positioning neural network (P-NN) that substantially reduces the complexity of
deep learning-based WP through carefully crafted minimum description features.
Our feature selection is based on maximum power measurements and their temporal
locations to convey information needed to conduct WP. We also develop a novel
methodology for adaptively selecting the size of feature space, which optimizes
over balancing the expected amount of useful information and classification
capability, quantified using information-theoretic measures on the signal bin
selection. Numerical results show that P-NN achieves a significant advantage in
performance-complexity tradeoff over deep learning baselines that leverage the
full power delay profile (PDP).
\\ ( https://arxiv.org/abs/2402.09580 ,  805kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09586
Date: Wed, 14 Feb 2024 21:29:28 GMT   (12102kb,D)

Title: WERank: Towards Rank Degradation Prevention for Self-Supervised Learning
  Using Weight Regularization
Authors: Ali Saheb Pasand, Reza Moravej, Mahdi Biparva, Ali Ghodsi
Categories: cs.LG
\\
  A common phenomena confining the representation quality in Self-Supervised
Learning (SSL) is dimensional collapse (also known as rank degeneration), where
the learned representations are mapped to a low dimensional subspace of the
representation space. The State-of-the-Art SSL methods have shown to suffer
from dimensional collapse and fall behind maintaining full rank. Recent
approaches to prevent this problem have proposed using contrastive losses,
regularization techniques, or architectural tricks. We propose WERank, a new
regularizer on the weight parameters of the network to prevent rank
degeneration at different layers of the network. We provide empirical evidence
and mathematical justification to demonstrate the effectiveness of the proposed
regularization method in preventing dimensional collapse. We verify the impact
of WERank on graph SSL where dimensional collapse is more pronounced due to the
lack of proper data augmentation. We empirically demonstrate that WERank is
effective in helping BYOL to achieve higher rank during SSL pre-training and
consequently downstream accuracy during evaluation probing. Ablation studies
and experimental analysis shed lights on the underlying factors behind the
performance gains of the proposed approach.
\\ ( https://arxiv.org/abs/2402.09586 ,  12102kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09591
Date: Wed, 14 Feb 2024 21:34:44 GMT   (71kb)

Title: Reconstructing the Geometry of Random Geometric Graphs
Authors: Han Huang, Pakawut Jiradilok, Elchanan Mossel
Categories: cs.LG math.PR
\\
  Random geometric graphs are random graph models defined on metric spaces.
Such a model is defined by first sampling points from a metric space and then
connecting each pair of sampled points with probability that depends on their
distance, independently among pairs. In this work, we show how to efficiently
reconstruct the geometry of the underlying space from the sampled graph under
the manifold assumption, i.e., assuming that the underlying space is a low
dimensional manifold and that the connection probability is a strictly
decreasing function of the Euclidean distance between the points in a given
embedding of the manifold in $\mathbb{R}^N$. Our work complements a large body
of work on manifold learning, where the goal is to recover a manifold from
sampled points sampled in the manifold along with their (approximate)
distances.
\\ ( https://arxiv.org/abs/2402.09591 ,  71kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09596
Date: Wed, 14 Feb 2024 22:00:57 GMT   (1616kb,D)

Title: Pulmonologists-Level lung cancer detection based on standard blood test
  results and smoking status using an explainable machine learning approach
Authors: Ricco Noel Hansen Flyckt, Louise Sjodsholm, Margrethe H{\o}stgaard
  Bang Henriksen, Claus Lohman Brasen, Ali Ebrahimi, Ole Hilberg, Torben
  Fr{\o}strup Hansen, Uffe Kock Wiil, Lars Henrik Jensen, Abdolrahman Peimankar
Categories: cs.LG
Comments: 9 pages, 4 figures
\\
  Lung cancer (LC) remains the primary cause of cancer-related mortality,
largely due to late-stage diagnoses. Effective strategies for early detection
are therefore of paramount importance. In recent years, machine learning (ML)
has demonstrated considerable potential in healthcare by facilitating the
detection of various diseases. In this retrospective development and validation
study, we developed an ML model based on dynamic ensemble selection (DES) for
LC detection. The model leverages standard blood sample analysis and smoking
history data from a large population at risk in Denmark. The study includes all
patients examined on suspicion of LC in the Region of Southern Denmark from
2009 to 2018. We validated and compared the predictions by the DES model with
diagnoses provided by five pulmonologists. Among the 38,944 patients, 9,940 had
complete data of which 2,505 (25\%) had LC. The DES model achieved an area
under the roc curve of 0.77$\pm$0.01, sensitivity of 76.2\%$\pm$2.4\%,
specificity of 63.8\%$\pm$2.3\%, positive predictive value of 41.6\%$\pm$1.2\%,
and F\textsubscript{1}-score of 53.8\%$\pm$1.1\%. The DES model outperformed
all five pulmonologists, achieving a sensitivity 9\% higher than their average.
The model identified smoking status, age, total calcium levels, neutrophil
count, and lactate dehydrogenase as the most important factors for the
detection of LC. The results highlight the successful application of the ML
approach in detecting LC, surpassing pulmonologists' performance. Incorporating
clinical and laboratory data in future risk assessment models can improve
decision-making and facilitate timely referrals.
\\ ( https://arxiv.org/abs/2402.09596 ,  1616kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09600
Date: Wed, 14 Feb 2024 22:15:37 GMT   (3637kb,D)

Title: Low-Rank Graph Contrastive Learning for Node Classification
Authors: Yancheng Wang, Yingzhen Yang
Categories: cs.LG stat.ML
Comments: arXiv admin note: text overlap with arXiv:2205.14109
\\
  Graph Neural Networks (GNNs) have been widely used to learn node
representations and with outstanding performance on various tasks such as node
classification. However, noise, which inevitably exists in real-world graph
data, would considerably degrade the performance of GNNs revealed by recent
studies. In this work, we propose a novel and robust GNN encoder, Low-Rank
Graph Contrastive Learning (LR-GCL). Our method performs transductive node
classification in two steps. First, a low-rank GCL encoder named LR-GCL is
trained by prototypical contrastive learning with low-rank regularization.
Next, using the features produced by LR-GCL, a linear transductive
classification algorithm is used to classify the unlabeled nodes in the graph.
Our LR-GCL is inspired by the low frequency property of the graph data and its
labels, and it is also theoretically motivated by our sharp generalization
bound for transductive learning. To the best of our knowledge, our theoretical
result is among the first to theoretically demonstrate the advantage of
low-rank learning in graph contrastive learning supported by strong empirical
performance. Extensive experiments on public benchmarks demonstrate the
superior performance of LR-GCL and the robustness of the learned node
representations. The code of LR-GCL is available at
\url{https://anonymous.4open.science/r/Low-Rank_Graph_Contrastive_Learning-64A6/}.
\\ ( https://arxiv.org/abs/2402.09600 ,  3637kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09603
Date: Wed, 14 Feb 2024 22:23:35 GMT   (347kb,D)

Title: Scalable Graph Self-Supervised Learning
Authors: Ali Saheb Pasand, Reza Moravej, Mahdi Biparva, Raika Karimi, Ali
  Ghodsi
Categories: cs.LG cs.AI
\\
  In regularization Self-Supervised Learning (SSL) methods for graphs,
computational complexity increases with the number of nodes in graphs and
embedding dimensions. To mitigate the scalability of non-contrastive graph SSL,
we propose a novel approach to reduce the cost of computing the covariance
matrix for the pre-training loss function with volume-maximization terms. Our
work focuses on reducing the cost associated with the loss computation via
graph node or dimension sampling. We provide theoretical insight into why
dimension sampling would result in accurate loss computations and support it
with mathematical derivation of the novel approach. We develop our experimental
setup on the node-level graph prediction tasks, where SSL pre-training has
shown to be difficult due to the large size of real world graphs. Our
experiments demonstrate that the cost associated with the loss computation can
be reduced via node or dimension sampling without lowering the downstream
performance. Our results demonstrate that sampling mostly results in improved
downstream performance. Ablation studies and experimental analysis are provided
to untangle the role of the different factors in the experimental setup.
\\ ( https://arxiv.org/abs/2402.09603 ,  347kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09608
Date: Wed, 14 Feb 2024 22:32:00 GMT   (2193kb,D)

Title: Exact, Fast and Expressive Poisson Point Processes via Squared Neural
  Families
Authors: Russell Tsuchida and Cheng Soon Ong and Dino Sejdinovic
Categories: cs.LG stat.ML
Comments: AAAI 2024 camera ready submission
\\
  We introduce squared neural Poisson point processes (SNEPPPs) by
parameterising the intensity function by the squared norm of a two layer neural
network. When the hidden layer is fixed and the second layer has a single
neuron, our approach resembles previous uses of squared Gaussian process or
kernel methods, but allowing the hidden layer to be learnt allows for
additional flexibility. In many cases of interest, the integrated intensity
function admits a closed form and can be computed in quadratic time in the
number of hidden neurons. We enumerate a far more extensive number of such
cases than has previously been discussed. Our approach is more memory and time
efficient than naive implementations of squared or exponentiated kernel methods
or Gaussian processes. Maximum likelihood and maximum a posteriori estimates in
a reparameterisation of the final layer of the intensity function can be
obtained by solving a (strongly) convex optimisation problem using projected
gradient descent. We demonstrate SNEPPPs on real, and synthetic benchmarks, and
provide a software implementation. https://github.com/RussellTsuchida/snefy
\\ ( https://arxiv.org/abs/2402.09608 ,  2193kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09629
Date: Thu, 15 Feb 2024 00:14:41 GMT   (1782kb,D)

Title: Smart Information Exchange for Unsupervised Federated Learning via
  Reinforcement Learning
Authors: Seohyun Lee, Anindya Bijoy Das, Satyavrat Wagle, Christopher G.
  Brinton
Categories: cs.LG
\\
  One of the main challenges of decentralized machine learning paradigms such
as Federated Learning (FL) is the presence of local non-i.i.d. datasets.
Device-to-device transfers (D2D) between distributed devices has been shown to
be an effective tool for dealing with this problem and robust to stragglers. In
an unsupervised case, however, it is not obvious how data exchanges should take
place due to the absence of labels. In this paper, we propose an approach to
create an optimal graph for data transfer using Reinforcement Learning. The
goal is to form links that will provide the most benefit considering the
environment's constraints and improve convergence speed in an unsupervised FL
environment. Numerical analysis shows the advantages in terms of convergence
speed and straggler resilience of the proposed method to different available FL
schemes and benchmark datasets.
\\ ( https://arxiv.org/abs/2402.09629 ,  1782kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09631
Date: Thu, 15 Feb 2024 00:20:30 GMT   (5711kb,D)

Title: MiMiC: Minimally Modified Counterfactuals in the Representation Space
Authors: Shashwat Singh, Shauli Ravfogel, Jonathan Herzig, Roee Aharoni, Ryan
  Cotterell, Ponnurangam Kumaraguru
Categories: cs.LG cs.CL
Comments: Preprint
\\
  Language models often exhibit undesirable behaviors, such as gender bias or
toxic language. Interventions in the representation space were shown effective
in mitigating such issues by altering the LM behavior. We first show that two
prominent intervention techniques, Linear Erasure and Steering Vectors, do not
enable a high degree of control and are limited in expressivity.
  We then propose a novel intervention methodology for generating expressive
counterfactuals in the representation space, aiming to make representations of
a source class (e.g., ``toxic'') resemble those of a target class (e.g.,
``non-toxic''). This approach, generalizing previous linear intervention
techniques, utilizes a closed-form solution for the Earth Mover's problem under
Gaussian assumptions and provides theoretical guarantees on the representation
space's geometric organization. We further build on this technique and derive a
nonlinear intervention that enables controlled generation. We demonstrate the
effectiveness of the proposed approaches in mitigating bias in multiclass
classification and in reducing the generation of toxic language, outperforming
strong baselines.
\\ ( https://arxiv.org/abs/2402.09631 ,  5711kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09638
Date: Thu, 15 Feb 2024 00:52:34 GMT   (35662kb,D)

Title: Multi-Fidelity Methods for Optimization: A Survey
Authors: Ke Li and Fan Li
Categories: cs.LG cs.NE
Comments: 47 pages, 9 figures
Report-no: COLALab Report #2024005
\\
  Real-world black-box optimization often involves time-consuming or costly
experiments and simulations. Multi-fidelity optimization (MFO) stands out as a
cost-effective strategy that balances high-fidelity accuracy with computational
efficiency through a hierarchical fidelity approach. This survey presents a
systematic exploration of MFO, underpinned by a novel text mining framework
based on a pre-trained language model. We delve deep into the foundational
principles and methodologies of MFO, focusing on three core components --
multi-fidelity surrogate models, fidelity management strategies, and
optimization techniques. Additionally, this survey highlights the diverse
applications of MFO across several key domains, including machine learning,
engineering design optimization, and scientific discovery, showcasing the
adaptability and effectiveness of MFO in tackling complex computational
challenges. Furthermore, we also envision several emerging challenges and
prospects in the MFO landscape, spanning scalability, the composition of lower
fidelities, and the integration of human-in-the-loop approaches at the
algorithmic level. We also address critical issues related to benchmarking and
the advancement of open science within the MFO community. Overall, this survey
aims to catalyze further research and foster collaborations in MFO, setting the
stage for future innovations and breakthroughs in the field.
\\ ( https://arxiv.org/abs/2402.09638 ,  35662kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09668
Date: Thu, 15 Feb 2024 02:27:57 GMT   (3036kb,D)

Title: How to Train Data-Efficient LLMs
Authors: Noveen Sachdeva, Benjamin Coleman, Wang-Cheng Kang, Jianmo Ni, Lichan
  Hong, Ed H. Chi, James Caverlee, Julian McAuley, Derek Zhiyuan Cheng
Categories: cs.LG cs.AI cs.CL
Comments: Under review. 44 pages, 30 figures
\\
  The training of large language models (LLMs) is expensive. In this paper, we
study data-efficient approaches for pre-training LLMs, i.e., techniques that
aim to optimize the Pareto frontier of model quality and training resource/data
consumption. We seek to understand the tradeoffs associated with data selection
routines based on (i) expensive-to-compute data-quality estimates, and (ii)
maximization of coverage and diversity-based measures in the feature space. Our
first technique, Ask-LLM, leverages the zero-shot reasoning capabilities of
instruction-tuned LLMs to directly assess the quality of a training example. To
target coverage, we propose Density sampling, which models the data
distribution to select a diverse sample. In our comparison of 19 samplers,
involving hundreds of evaluation tasks and pre-training runs, we find that
Ask-LLM and Density are the best methods in their respective categories.
Coverage sampling can recover the performance of the full data, while models
trained on Ask-LLM data consistently outperform full-data training -- even when
we reject 90% of the original dataset, while converging up to 70% faster.
\\ ( https://arxiv.org/abs/2402.09668 ,  3036kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09676
Date: Thu, 15 Feb 2024 03:05:45 GMT   (300kb,D)

Title: HyperMagNet: A Magnetic Laplacian based Hypergraph Neural Network
Authors: Tatyana Benko, Martin Buck, Ilya Amburg, Stephen J. Young, Sinan G.
  Aksoy
Categories: cs.LG
Comments: 9 pages, 1 figure
\\
  In data science, hypergraphs are natural models for data exhibiting multi-way
relations, whereas graphs only capture pairwise. Nonetheless, many proposed
hypergraph neural networks effectively reduce hypergraphs to undirected graphs
via symmetrized matrix representations, potentially losing important
information. We propose an alternative approach to hypergraph neural networks
in which the hypergraph is represented as a non-reversible Markov chain. We use
this Markov chain to construct a complex Hermitian Laplacian matrix - the
magnetic Laplacian - which serves as the input to our proposed hypergraph
neural network. We study HyperMagNet for the task of node classification, and
demonstrate its effectiveness over graph-reduction based hypergraph neural
networks.
\\ ( https://arxiv.org/abs/2402.09676 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09695
Date: Thu, 15 Feb 2024 04:08:49 GMT   (13442kb,D)

Title: Reward Poisoning Attack Against Offline Reinforcement Learning
Authors: Yinglun Xu, Rohan Gumaste, Gagandeep Singh
Categories: cs.LG cs.AI
\\
  We study the problem of reward poisoning attacks against general offline
reinforcement learning with deep neural networks for function approximation. We
consider a black-box threat model where the attacker is completely oblivious to
the learning algorithm and its budget is limited by constraining both the
amount of corruption at each data point, and the total perturbation. We propose
an attack strategy called `policy contrast attack'. The high-level idea is to
make some low-performing policies appear as high-performing while making
high-performing policies appear as low-performing. To the best of our
knowledge, we propose the first black-box reward poisoning attack in the
general offline RL setting. We provide theoretical insights on the attack
design and empirically show that our attack is efficient against current
state-of-the-art offline RL algorithms in different kinds of learning datasets.
\\ ( https://arxiv.org/abs/2402.09695 ,  13442kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09702
Date: Thu, 15 Feb 2024 04:36:52 GMT   (11046kb,D)

Title: Sparse and Faithful Explanations Without Sparse Models
Authors: Yiyang Sun, Zhi Chen, Vittorio Orlandi, Tong Wang, Cynthia Rudin
Categories: cs.LG stat.ML
Comments: Accepted in AISTATS 2024
\\
  Even if a model is not globally sparse, it is possible for decisions made
from that model to be accurately and faithfully described by a small number of
features. For instance, an application for a large loan might be denied to
someone because they have no credit history, which overwhelms any evidence
towards their creditworthiness. In this work, we introduce the Sparse
Explanation Value (SEV), a new way of measuring sparsity in machine learning
models. In the loan denial example above, the SEV is 1 because only one factor
is needed to explain why the loan was denied. SEV is a measure of decision
sparsity rather than overall model sparsity, and we are able to show that many
machine learning models -- even if they are not sparse -- actually have low
decision sparsity, as measured by SEV. SEV is defined using movements over a
hypercube, allowing SEV to be defined consistently over various model classes,
with movement restrictions reflecting real-world constraints. We proposed the
algorithms that reduce SEV without sacrificing accuracy, providing sparse and
completely faithful explanations, even without globally sparse models.
\\ ( https://arxiv.org/abs/2402.09702 ,  11046kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09711
Date: Thu, 15 Feb 2024 05:07:39 GMT   (1021kb,D)

Title: Node Duplication Improves Cold-start Link Prediction
Authors: Zhichun Guo, Tong Zhao, Yozen Liu, Kaiwen Dong, William Shiao, Neil
  Shah, Nitesh V. Chawla
Categories: cs.LG
\\
  Graph Neural Networks (GNNs) are prominent in graph machine learning and have
shown state-of-the-art performance in Link Prediction (LP) tasks. Nonetheless,
recent studies show that GNNs struggle to produce good results on low-degree
nodes despite their overall strong performance. In practical applications of
LP, like recommendation systems, improving performance on low-degree nodes is
critical, as it amounts to tackling the cold-start problem of improving the
experiences of users with few observed interactions. In this paper, we
investigate improving GNNs' LP performance on low-degree nodes while preserving
their performance on high-degree nodes and propose a simple yet surprisingly
effective augmentation technique called NodeDup. Specifically, NodeDup
duplicates low-degree nodes and creates links between nodes and their own
duplicates before following the standard supervised LP training scheme. By
leveraging a ''multi-view'' perspective for low-degree nodes, NodeDup shows
significant LP performance improvements on low-degree nodes without
compromising any performance on high-degree nodes. Additionally, as a
plug-and-play augmentation module, NodeDup can be easily applied to existing
GNNs with very light computational cost. Extensive experiments show that
NodeDup achieves 38.49%, 13.34%, and 6.76% improvements on isolated,
low-degree, and warm nodes, respectively, on average across all datasets
compared to GNNs and state-of-the-art cold-start methods.
\\ ( https://arxiv.org/abs/2402.09711 ,  1021kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09730
Date: Thu, 15 Feb 2024 05:59:21 GMT   (72kb)

Title: DOF: Accelerating High-order Differential Operators with Forward
  Propagation
Authors: Ruichen Li, Chuwei Wang, Haotian Ye, Di He, Liwei Wang
Categories: cs.LG
\\
  Solving partial differential equations (PDEs) efficiently is essential for
analyzing complex physical systems. Recent advancements in leveraging deep
learning for solving PDE have shown significant promise. However, machine
learning methods, such as Physics-Informed Neural Networks (PINN), face
challenges in handling high-order derivatives of neural network-parameterized
functions. Inspired by Forward Laplacian, a recent method of accelerating
Laplacian computation, we propose an efficient computational framework,
Differential Operator with Forward-propagation (DOF), for calculating general
second-order differential operators without losing any precision. We provide
rigorous proof of the advantages of our method over existing methods,
demonstrating two times improvement in efficiency and reduced memory
consumption on any architectures. Empirical results illustrate that our method
surpasses traditional automatic differentiation (AutoDiff) techniques,
achieving 2x improvement on the MLP structure and nearly 20x improvement on the
MLP with Jacobian sparsity.
\\ ( https://arxiv.org/abs/2402.09730 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09735
Date: Thu, 15 Feb 2024 06:22:50 GMT   (5635kb,D)

Title: DFORM: Diffeomorphic vector field alignment for assessing dynamics
  across learned models
Authors: Ruiqi Chen, Giacomo Vedovati, Todd Braver, ShiNung Ching
Categories: cs.LG cs.SY eess.SY q-bio.NC
Comments: 12 pages, 8 figures
\\
  Dynamical system models such as Recurrent Neural Networks (RNNs) have become
increasingly popular as hypothesis-generating tools in scientific research.
Evaluating the dynamics in such networks is key to understanding their learned
generative mechanisms. However, comparison of learned dynamics across models is
challenging due to their inherent nonlinearity and because a priori there is no
enforced equivalence of their coordinate systems. Here, we propose the DFORM
(Diffeomorphic vector field alignment for comparing dynamics across learned
models) framework. DFORM learns a nonlinear coordinate transformation which
provides a continuous, maximally one-to-one mapping between the trajectories of
learned models, thus approximating a diffeomorphism between them. The mismatch
between DFORM-transformed vector fields defines the orbital similarity between
two models, thus providing a generalization of the concepts of smooth orbital
and topological equivalence. As an example, we apply DFORM to models trained on
a canonical neuroscience task, showing that learned dynamics may be
functionally similar, despite overt differences in attractor landscapes.
\\ ( https://arxiv.org/abs/2402.09735 ,  5635kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09780
Date: Thu, 15 Feb 2024 08:09:17 GMT   (2926kb,D)

Title: TinyCL: An Efficient Hardware Architecture for Continual Learning on
  Autonomous Systems
Authors: Eugenio Ressa and Alberto Marchisio and Maurizio Martina and Guido
  Masera and Muhammad Shafique
Categories: cs.LG
\\
  The Continuous Learning (CL) paradigm consists of continuously evolving the
parameters of the Deep Neural Network (DNN) model to progressively learn to
perform new tasks without reducing the performance on previous tasks, i.e.,
avoiding the so-called catastrophic forgetting. However, the DNN parameter
update in CL-based autonomous systems is extremely resource-hungry. The
existing DNN accelerators cannot be directly employed in CL because they only
support the execution of the forward propagation. Only a few prior
architectures execute the backpropagation and weight update, but they lack the
control and management for CL. Towards this, we design a hardware architecture,
TinyCL, to perform CL on resource-constrained autonomous systems. It consists
of a processing unit that executes both forward and backward propagation, and a
control unit that manages memory-based CL workload. To minimize the memory
accesses, the sliding window of the convolutional layer moves in a snake-like
fashion. Moreover, the Multiply-and-Accumulate units can be reconfigured at
runtime to execute different operations. As per our knowledge, our proposed
TinyCL represents the first hardware accelerator that executes CL on autonomous
systems. We synthesize the complete TinyCL architecture in a 65 nm CMOS
technology node with the conventional ASIC design flow. It executes 1 epoch of
training on a Conv + ReLU + Dense model on the CIFAR10 dataset in 1.76 s, while
1 training epoch of the same model using an Nvidia Tesla P100 GPU takes 103 s,
thus achieving a 58 x speedup, consuming 86 mW in a 4.74 mm2 die.
\\ ( https://arxiv.org/abs/2402.09780 ,  2926kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09782
Date: Thu, 15 Feb 2024 08:21:50 GMT   (1360kb,D)

Title: MC-DBN: A Deep Belief Network-Based Model for Modality Completion
Authors: Zihong Luo, Haochen Xue, Mingyu Jin, Chengzhi Liu, Zile Huang, Chong
  Zhang, Shuliang Zhao
Categories: cs.LG cs.AI
Journal-ref: International Conference on Computer Supported Cooperative Work in
  Design 2024
\\
  Recent advancements in multi-modal artificial intelligence (AI) have
revolutionized the fields of stock market forecasting and heart rate
monitoring. Utilizing diverse data sources can substantially improve prediction
accuracy. Nonetheless, additional data may not always align with the original
dataset. Interpolation methods are commonly utilized for handling missing
values in modal data, though they may exhibit limitations in the context of
sparse information. Addressing this challenge, we propose a Modality Completion
Deep Belief Network-Based Model (MC-DBN). This approach utilizes implicit
features of complete data to compensate for gaps between itself and additional
incomplete data. It ensures that the enhanced multi-modal data closely aligns
with the dynamic nature of the real world to enhance the effectiveness of the
model. We conduct evaluations of the MC-DBN model in two datasets from the
stock market forecasting and heart rate monitoring domains. Comprehensive
experiments showcase the model's capacity to bridge the semantic divide present
in multi-modal data, subsequently enhancing its performance. The source code is
available at: https://github.com/logan-0623/DBN-generate
\\ ( https://arxiv.org/abs/2402.09782 ,  1360kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09830
Date: Thu, 15 Feb 2024 09:48:20 GMT   (745kb)

Title: Utilizing GANs for Fraud Detection: Model Training with Synthetic
  Transaction Data
Authors: Mengran Zhu, Yulu Gong, Yafei Xiang, Hanyi Yu, Shuning Huo
Categories: cs.LG cs.AI cs.CE
\\
  Anomaly detection is a critical challenge across various research domains,
aiming to identify instances that deviate from normal data distributions. This
paper explores the application of Generative Adversarial Networks (GANs) in
fraud detection, comparing their advantages with traditional methods. GANs, a
type of Artificial Neural Network (ANN), have shown promise in modeling complex
data distributions, making them effective tools for anomaly detection. The
paper systematically describes the principles of GANs and their derivative
models, emphasizing their application in fraud detection across different
datasets. And by building a collection of adversarial verification graphs, we
will effectively prevent fraud caused by bots or automated systems and ensure
that the users in the transaction are real. The objective of the experiment is
to design and implement a fake face verification code and fraud detection
system based on Generative Adversarial network (GANs) algorithm to enhance the
security of the transaction process.The study demonstrates the potential of
GANs in enhancing transaction security through deep learning techniques.
\\ ( https://arxiv.org/abs/2402.09830 ,  745kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09834
Date: Thu, 15 Feb 2024 09:55:39 GMT   (226kb,D)

Title: All in One and One for All: A Simple yet Effective Method towards
  Cross-domain Graph Pretraining
Authors: Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng, and Jia Li
Categories: cs.LG
\\
  Large Language Models (LLMs) have revolutionized the fields of computer
vision (CV) and natural language processing (NLP). One of the most notable
advancements of LLMs is that a single model is trained on vast and diverse
datasets spanning multiple domains -- a paradigm we term `All in One'. This
methodology empowers LLMs with super generalization capabilities, facilitating
an encompassing comprehension of varied data distributions. Leveraging these
capabilities, a single LLM demonstrates remarkable versatility across a variety
of domains -- a paradigm we term `One for All'. However, applying this idea to
the graph field remains a formidable challenge, with cross-domain pretraining
often resulting in negative transfer. This issue is particularly important in
few-shot learning scenarios, where the paucity of training data necessitates
the incorporation of external knowledge sources. In response to this challenge,
we propose a novel approach called Graph COordinators for PrEtraining (GCOPE),
that harnesses the underlying commonalities across diverse graph datasets to
enhance few-shot learning. Our novel methodology involves a unification
framework that amalgamates disparate graph datasets during the pretraining
phase to distill and transfer meaningful knowledge to target tasks. Extensive
experiments across multiple graph datasets demonstrate the superior efficacy of
our approach. By successfully leveraging the synergistic potential of multiple
graph datasets for pretraining, our work stands as a pioneering contribution to
the realm of graph foundational model.
\\ ( https://arxiv.org/abs/2402.09834 ,  226kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09838
Date: Thu, 15 Feb 2024 10:00:13 GMT   (2581kb,D)

Title: Performative Reinforcement Learning in Gradually Shifting Environments
Authors: Ben Rank, Stelios Triantafyllou, Debmalya Mandal, Goran Radanovic
Categories: cs.LG
\\
  When Reinforcement Learning (RL) agents are deployed in practice, they might
impact their environment and change its dynamics. Ongoing research attempts to
formally model this phenomenon and to analyze learning algorithms in these
models. To this end, we propose a framework where the current environment
depends on the deployed policy as well as its previous dynamics. This is a
generalization of Performative RL (PRL) [Mandal et al., 2023]. Unlike PRL, our
framework allows to model scenarios where the environment gradually adjusts to
a deployed policy. We adapt two algorithms from the performative prediction
literature to our setting and propose a novel algorithm called Mixed Delayed
Repeated Retraining (MDRR). We provide conditions under which these algorithms
converge and compare them using three metrics: number of retrainings,
approximation guarantee, and number of samples per deployment. Unlike previous
approaches, MDRR combines samples from multiple deployments in its training.
This makes MDRR particularly suitable for scenarios where the environment's
response strongly depends on its previous dynamics, which are common in
practice. We experimentally compare the algorithms using a simulation-based
testbed and our results show that MDRR converges significantly faster than
previous approaches.
\\ ( https://arxiv.org/abs/2402.09838 ,  2581kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09849
Date: Thu, 15 Feb 2024 10:11:28 GMT   (3387kb,D)

Title: Recommendations for Baselines and Benchmarking Approximate Gaussian
  Processes
Authors: Sebastian W. Ober, Artem Artemev, Marcel Wagenl\"ander, Rudolfs
  Grobins, Mark van der Wilk
Categories: cs.LG stat.ML
Comments: Preprint. 25 pages, 16 figures
\\
  Gaussian processes (GPs) are a mature and widely-used component of the ML
toolbox. One of their desirable qualities is automatic hyperparameter
selection, which allows for training without user intervention. However, in
many realistic settings, approximations are typically needed, which typically
do require tuning. We argue that this requirement for tuning complicates
evaluation, which has led to a lack of a clear recommendations on which method
should be used in which situation. To address this, we make recommendations for
comparing GP approximations based on a specification of what a user should
expect from a method. In addition, we develop a training procedure for the
variational method of Titsias [2009] that leaves no choices to the user, and
show that this is a strong baseline that meets our specification. We conclude
that benchmarking according to our suggestions gives a clearer view of the
current state of the field, and uncovers problems that are still open that
future papers should address.
\\ ( https://arxiv.org/abs/2402.09849 ,  3387kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09881
Date: Thu, 15 Feb 2024 11:08:23 GMT   (932kb,D)

Title: Explaining Kernel Clustering via Decision Trees
Authors: Maximilian Fleissner, Leena Chennuru Vankadara, Debarghya
  Ghoshdastidar
Categories: cs.LG
\\
  Despite the growing popularity of explainable and interpretable machine
learning, there is still surprisingly limited work on inherently interpretable
clustering methods. Recently, there has been a surge of interest in explaining
the classic k-means algorithm, leading to efficient algorithms that approximate
k-means clusters using axis-aligned decision trees. However, interpretable
variants of k-means have limited applicability in practice, where more flexible
clustering methods are often needed to obtain useful partitions of the data. In
this work, we investigate interpretable kernel clustering, and propose
algorithms that construct decision trees to approximate the partitions induced
by kernel k-means, a nonlinear extension of k-means. We further build on
previous work on explainable k-means and demonstrate how a suitable choice of
features allows preserving interpretability without sacrificing approximation
guarantees on the interpretable model.
\\ ( https://arxiv.org/abs/2402.09881 ,  932kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09891
Date: Thu, 15 Feb 2024 11:34:38 GMT   (495kb,D)

Title: Predictors from causal features do not generalize better to new domains
Authors: Vivian Y. Nastl and Moritz Hardt
Categories: cs.LG stat.ML
Comments: 13 pages, 7 figures
\\
  We study how well machine learning models trained on causal features
generalize across domains. We consider 16 prediction tasks on tabular datasets
covering applications in health, employment, education, social benefits, and
politics. Each dataset comes with multiple domains, allowing us to test how
well a model trained in one domain performs in another. For each prediction
task, we select features that have a causal influence on the target of
prediction. Our goal is to test the hypothesis that models trained on causal
features generalize better across domains. Without exception, we find that
predictors using all available features, regardless of causality, have better
in-domain and out-of-domain accuracy than predictors using causal features.
Moreover, even the absolute drop in accuracy from one domain to the other is no
better for causal predictors than for models that use all features. If the goal
is to generalize to new domains, practitioners might as well train the best
possible model on all available features.
\\ ( https://arxiv.org/abs/2402.09891 ,  495kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09897
Date: Thu, 15 Feb 2024 11:45:34 GMT   (1909kb,D)

Title: COVIDHealth: A Benchmark Twitter Dataset and Machine Learning based Web
  Application for Classifying COVID-19 Discussions
Authors: Mahathir Mohammad Bishal, Md. Rakibul Hassan Chowdory, Anik Das,
  Muhammad Ashad Kabir
Categories: cs.LG
Comments: 27 pages, 6 figures
\\
  The COVID-19 pandemic has had adverse effects on both physical and mental
health. During this pandemic, numerous studies have focused on gaining insights
into health-related perspectives from social media. In this study, our primary
objective is to develop a machine learning-based web application for
automatically classifying COVID-19-related discussions on social media. To
achieve this, we label COVID-19-related Twitter data, provide benchmark
classification results, and develop a web application. We collected data using
the Twitter API and labeled a total of 6,667 tweets into five different
classes: health risks, prevention, symptoms, transmission, and treatment. We
extracted features using various feature extraction methods and applied them to
seven different traditional machine learning algorithms, including Decision
Tree, Random Forest, Stochastic Gradient Descent, Adaboost, K-Nearest
Neighbour, Logistic Regression, and Linear SVC. Additionally, we used four deep
learning algorithms: LSTM, CNN, RNN, and BERT, for classification. Overall, we
achieved a maximum F1 score of 90.43% with the CNN algorithm in deep learning.
The Linear SVC algorithm exhibited the highest F1 score at 86.13%, surpassing
other traditional machine learning approaches. Our study not only contributes
to the field of health-related data analysis but also provides a valuable
resource in the form of a web-based tool for efficient data classification,
which can aid in addressing public health challenges and increasing awareness
during pandemics. We made the dataset and application publicly available, which
can be downloaded from this link
https://github.com/Bishal16/COVID19-Health-Related-Data-Classification-Website.
\\ ( https://arxiv.org/abs/2402.09897 ,  1909kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09900
Date: Thu, 15 Feb 2024 11:56:53 GMT   (30280kb,D)

Title: Revisiting Recurrent Reinforcement Learning with Memory Monoids
Authors: Steven Morad, Chris Lu, Ryan Kortvelesy, Stephan Liwicki, Jakob
  Foerster, Amanda Prorok
Categories: cs.LG cs.AI
\\
  In RL, memory models such as RNNs and transformers address Partially
Observable Markov Decision Processes (POMDPs) by mapping trajectories to latent
Markov states. Neither model scales particularly well to long sequences,
especially compared to an emerging class of memory models sometimes called
linear recurrent models. We discover that the recurrent update of these models
is a monoid, leading us to formally define a novel memory monoid framework. We
revisit the traditional approach to batching in recurrent RL, highlighting both
theoretical and empirical deficiencies. Leveraging the properties of memory
monoids, we propose a new batching method that improves sample efficiency,
increases the return, and simplifies the implementation of recurrent loss
functions in RL.
\\ ( https://arxiv.org/abs/2402.09900 ,  30280kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09941
Date: Thu, 15 Feb 2024 13:41:23 GMT   (5581kb,D)

Title: FedLion: Faster Adaptive Federated Optimization with Fewer Communication
Authors: Zhiwei Tang, Tsung-Hui Chang
Categories: cs.LG cs.AI stat.ML
Comments: ICASSP 2024
\\
  In Federated Learning (FL), a framework to train machine learning models
across distributed data, well-known algorithms like FedAvg tend to have slow
convergence rates, resulting in high communication costs during training. To
address this challenge, we introduce FedLion, an adaptive federated
optimization algorithm that seamlessly incorporates key elements from the
recently proposed centralized adaptive algorithm, Lion (Chen et al. 2o23), into
the FL framework. Through comprehensive evaluations on two widely adopted FL
benchmarks, we demonstrate that FedLion outperforms previous state-of-the-art
adaptive algorithms, including FAFED (Wu et al. 2023) and FedDA. Moreover,
thanks to the use of signed gradients in local training, FedLion substantially
reduces data transmission requirements during uplink communication when
compared to existing adaptive algorithms, further reducing communication costs.
Last but not least, this work also includes a novel theoretical analysis,
showcasing that FedLion attains faster convergence rate than established FL
algorithms like FedAvg.
\\ ( https://arxiv.org/abs/2402.09941 ,  5581kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09947
Date: Thu, 15 Feb 2024 13:50:00 GMT   (4729kb,D)

Title: Explaining Probabilistic Models with Distributional Values
Authors: Luca Franceschi, Michele Donini, C\'edric Archambeau and Matthias
  Seeger
Categories: cs.LG
Comments: Code coming soon
\\
  A large branch of explainable machine learning is grounded in cooperative
game theory. However, research indicates that game-theoretic explanations may
mislead or be hard to interpret. We argue that often there is a critical
mismatch between what one wishes to explain (e.g. the output of a classifier)
and what current methods such as SHAP explain (e.g. the scalar probability of a
class). This paper addresses such gap for probabilistic models by generalising
cooperative games and value operators. We introduce the distributional values,
random variables that track changes in the model output (e.g. flipping of the
predicted class) and derive their analytic expressions for games with Gaussian,
Bernoulli and Categorical payoffs. We further establish several characterising
properties, and show that our framework provides fine-grained and insightful
explanations with case studies on vision and language models.
\\ ( https://arxiv.org/abs/2402.09947 ,  4729kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09957
Date: Thu, 15 Feb 2024 14:08:08 GMT   (3999kb,D)

Title: On Designing Features for Condition Monitoring of Rotating Machines
Authors: Seetaram Maurya and Nishchal K. Verma
Categories: cs.LG
\\
  Various methods for designing input features have been proposed for fault
recognition in rotating machines using one-dimensional raw sensor data. The
available methods are complex, rely on empirical approaches, and may differ
depending on the condition monitoring data used. Therefore, this article
proposes a novel algorithm to design input features that unifies the feature
extraction process for different time-series sensor data. This new insight for
designing/extracting input features is obtained through the lens of histogram
theory. The proposed algorithm extracts discriminative input features, which
are suitable for a simple classifier to deep neural network-based classifiers.
The designed input features are given as input to the classifier with
end-to-end training in a single framework for machine conditions recognition.
The proposed scheme has been validated through three real-time datasets: a)
acoustic dataset, b) CWRU vibration dataset, and c) IMS vibration dataset. The
real-time results and comparative study show the effectiveness of the proposed
scheme for the prediction of the machine's health states.
\\ ( https://arxiv.org/abs/2402.09957 ,  3999kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09961
Date: Thu, 15 Feb 2024 14:15:51 GMT   (797kb)

Title: Enhancing Courier Scheduling in Crowdsourced Last-Mile Delivery through
  Dynamic Shift Extensions: A Deep Reinforcement Learning Approach
Authors: Zead Saleh, Ahmad Al Hanbali, and Ahmad Baubaid
Categories: cs.LG
\\
  Crowdsourced delivery platforms face complex scheduling challenges to match
couriers and customer orders. We consider two types of crowdsourced couriers,
namely, committed and occasional couriers, each with different compensation
schemes. Crowdsourced delivery platforms usually schedule committed courier
shifts based on predicted demand. Therefore, platforms may devise an offline
schedule for committed couriers before the planning period. However, due to the
unpredictability of demand, there are instances where it becomes necessary to
make online adjustments to the offline schedule. In this study, we focus on the
problem of dynamically adjusting the offline schedule through shift extensions
for committed couriers. This problem is modeled as a sequential decision
process. The objective is to maximize platform profit by determining the shift
extensions of couriers and the assignments of requests to couriers. To solve
the model, a Deep Q-Network (DQN) learning approach is developed. Comparing
this model with the baseline policy where no extensions are allowed
demonstrates the benefits that platforms can gain from allowing shift
extensions in terms of reward, reduced lost order costs, and lost requests.
Additionally, sensitivity analysis showed that the total extension compensation
increases in a nonlinear manner with the arrival rate of requests, and in a
linear manner with the arrival rate of occasional couriers. On the compensation
sensitivity, the results showed that the normal scenario exhibited the highest
average number of shift extensions and, consequently, the fewest average number
of lost requests. These findings serve as evidence of the successful learning
of such dynamics by the DQN algorithm.
\\ ( https://arxiv.org/abs/2402.09961 ,  797kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09963
Date: Thu, 15 Feb 2024 14:17:51 GMT   (1571kb,D)

Title: Why are Sensitive Functions Hard for Transformers?
Authors: Michael Hahn, Mark Rofin
Categories: cs.LG
\\
  Empirical studies have identified a range of learnability biases and
limitations of transformers, such as a persistent difficulty in learning to
compute simple formal languages such as PARITY, and a bias towards low-degree
functions. However, theoretical understanding remains limited, with existing
expressiveness theory either overpredicting or underpredicting realistic
learning abilities. We prove that, under the transformer architecture, the loss
landscape is constrained by the input-space sensitivity: Transformers whose
output is sensitive to many parts of the input string inhabit isolated points
in parameter space, leading to a low-sensitivity bias in generalization. We
show theoretically and empirically that this theory unifies a broad array of
empirical observations about the learning abilities and biases of transformers,
such as their generalization bias towards low sensitivity and low degree, and
difficulty in length generalization for PARITY. This shows that understanding
transformers' inductive biases requires studying not just their in-principle
expressivity, but also their loss landscape.
\\ ( https://arxiv.org/abs/2402.09963 ,  1571kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09965
Date: Thu, 30 Nov 2023 01:49:52 GMT   (507kb)

Title: Hierarchy Representation of Data in Machine Learnings
Authors: Han Yegang, Park Minjun, Byun Duwon, Park Inkyu
Categories: cs.LG cs.AI
\\
  When there are models with clear-cut judgment results for several data
points, it is possible that most models exhibit a relationship where if they
correctly judge one target, they also correctly judge another target.
Conversely, if most models incorrectly judge one target, they may also
incorrectly judge another target. We propose a method for visualizing this
hierarchy among targets. This information is expected to be beneficial for
model improvement.
\\ ( https://arxiv.org/abs/2402.09965 ,  507kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09970
Date: Thu, 15 Feb 2024 14:27:58 GMT   (38436kb,D)

Title: Accelerating Parallel Sampling of Diffusion Models
Authors: Zhiwei Tang, Jiasheng Tang, Hao Luo, Fan Wang, Tsung-Hui Chang
Categories: cs.LG stat.ML
\\
  Diffusion models have emerged as state-of-the-art generative models for image
generation. However, sampling from diffusion models is usually time-consuming
due to the inherent autoregressive nature of their sampling process. In this
work, we propose a novel approach that accelerates the sampling of diffusion
models by parallelizing the autoregressive process. Specifically, we
reformulate the sampling process as solving a system of triangular nonlinear
equations through fixed-point iteration. With this innovative formulation, we
explore several systematic techniques to further reduce the iteration steps
required by the solving process. Applying these techniques, we introduce
ParaTAA, a universal and training-free parallel sampling algorithm that can
leverage extra computational and memory resources to increase the sampling
speed. Our experiments demonstrate that ParaTAA can decrease the inference
steps required by common sequential sampling algorithms such as DDIM and DDPM
by a factor of 4~14 times. Notably, when applying ParaTAA with 100 steps DDIM
for Stable Diffusion, a widely-used text-to-image diffusion model, it can
produce the same images as the sequential sampling in only 7 inference steps.
\\ ( https://arxiv.org/abs/2402.09970 ,  38436kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09984
Date: Thu, 15 Feb 2024 14:49:28 GMT   (2080kb,D)

Title: Symmetry-Breaking Augmentations for Ad Hoc Teamwork
Authors: Ravi Hammond, Dustin Craggs, Mingyu Guo, Jakob Foerster, Ian Reid
Categories: cs.LG cs.AI
Comments: Currently in review for ICML 2024. 16 pages (including references and
  appendix), 9 Figures, 11 tables
\\
  In many collaborative settings, artificial intelligence (AI) agents must be
able to adapt to new teammates that use unknown or previously unobserved
strategies. While often simple for humans, this can be challenging for AI
agents. For example, if an AI agent learns to drive alongside others (a
training set) that only drive on one side of the road, it may struggle to adapt
this experience to coordinate with drivers on the opposite side, even if their
behaviours are simply flipped along the left-right symmetry. To address this we
introduce symmetry-breaking augmentations (SBA), which increases diversity in
the behaviour of training teammates by applying a symmetry-flipping operation.
By learning a best-response to the augmented set of teammates, our agent is
exposed to a wider range of behavioural conventions, improving performance when
deployed with novel teammates. We demonstrate this experimentally in two
settings, and show that our approach improves upon previous ad hoc teamwork
results in the challenging card game Hanabi. We also propose a general metric
for estimating symmetry-dependency amongst a given set of policies.
\\ ( https://arxiv.org/abs/2402.09984 ,  2080kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09992
Date: Thu, 15 Feb 2024 14:55:38 GMT   (114kb,D)

Title: Risk-Sensitive Soft Actor-Critic for Robust Deep Reinforcement Learning
  under Distribution Shifts
Authors: Tobias Enders, James Harrison, Maximilian Schiffer
Categories: cs.LG cs.SY eess.SY
Comments: 11 pages, 8 figures
\\
  We study the robustness of deep reinforcement learning algorithms against
distribution shifts within contextual multi-stage stochastic combinatorial
optimization problems from the operations research domain. In this context,
risk-sensitive algorithms promise to learn robust policies. While this field is
of general interest to the reinforcement learning community, most studies
up-to-date focus on theoretical results rather than real-world performance.
With this work, we aim to bridge this gap by formally deriving a novel
risk-sensitive deep reinforcement learning algorithm while providing numerical
evidence for its efficacy. Specifically, we introduce discrete Soft
Actor-Critic for the entropic risk measure by deriving a version of the Bellman
equation for the respective Q-values. We establish a corresponding policy
improvement result and infer a practical algorithm. We introduce an environment
that represents typical contextual multi-stage stochastic combinatorial
optimization problems and perform numerical experiments to empirically validate
our algorithm's robustness against realistic distribution shifts, without
compromising performance on the training distribution. We show that our
algorithm is superior to risk-neutral Soft Actor-Critic as well as to two
benchmark approaches for robust deep reinforcement learning. Thereby, we
provide the first structured analysis on the robustness of reinforcement
learning under distribution shifts in the realm of contextual multi-stage
stochastic combinatorial optimization problems.
\\ ( https://arxiv.org/abs/2402.09992 ,  114kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10001
Date: Thu, 15 Feb 2024 15:06:33 GMT   (1430kb,D)

Title: Privacy Attacks in Decentralized Learning
Authors: Abdellah El Mrini, Edwige Cyffers and Aur\'elien Bellet
Categories: cs.LG cs.CR
\\
  Decentralized Gradient Descent (D-GD) allows a set of users to perform
collaborative learning without sharing their data by iteratively averaging
local model updates with their neighbors in a network graph. The absence of
direct communication between non-neighbor nodes might lead to the belief that
users cannot infer precise information about the data of others. In this work,
we demonstrate the opposite, by proposing the first attack against D-GD that
enables a user (or set of users) to reconstruct the private data of other users
outside their immediate neighborhood. Our approach is based on a reconstruction
attack against the gossip averaging protocol, which we then extend to handle
the additional challenges raised by D-GD. We validate the effectiveness of our
attack on real graphs and datasets, showing that the number of users
compromised by a single or a handful of attackers is often surprisingly large.
We empirically investigate some of the factors that affect the performance of
the attack, namely the graph topology, the number of attackers, and their
position in the graph.
\\ ( https://arxiv.org/abs/2402.10001 ,  1430kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10028
Date: Thu, 15 Feb 2024 15:48:55 GMT   (3050kb,D)

Title: Diffusion Models Meet Contextual Bandits with Large Action Spaces
Authors: Imad Aouali
Categories: cs.LG cs.AI stat.ML
Comments: 26 pages, 5 figures
\\
  Efficient exploration is a key challenge in contextual bandits due to the
large size of their action space, where uninformed exploration can result in
computational and statistical inefficiencies. Fortunately, the rewards of
actions are often correlated and this can be leveraged to explore them
efficiently. In this work, we capture such correlations using pre-trained
diffusion models; upon which we design diffusion Thompson sampling (dTS). Both
theoretical and algorithmic foundations are developed for dTS, and empirical
evaluation also shows its favorable performance.
\\ ( https://arxiv.org/abs/2402.10028 ,  3050kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10046
Date: Thu, 15 Feb 2024 16:07:56 GMT   (478kb,D)

Title: How Flawed is ECE? An Analysis via Logit Smoothing
Authors: Muthu Chidambaram, Holden Lee, Colin McSwiggen, Semon Rezchikov
Categories: cs.LG math.PR
Comments: 22 pages, 4 figures
MSC-class: 68T37 (Primary) 62-08, 60E05 (Secondary)
\\
  Informally, a model is calibrated if its predictions are correct with a
probability that matches the confidence of the prediction. By far the most
common method in the literature for measuring calibration is the expected
calibration error (ECE). Recent work, however, has pointed out drawbacks of
ECE, such as the fact that it is discontinuous in the space of predictors. In
this work, we ask: how fundamental are these issues, and what are their impacts
on existing results? Towards this end, we completely characterize the
discontinuities of ECE with respect to general probability measures on Polish
spaces. We then use the nature of these discontinuities to motivate a novel
continuous, easily estimated miscalibration metric, which we term
Logit-Smoothed ECE (LS-ECE). By comparing the ECE and LS-ECE of pre-trained
image classification models, we show in initial experiments that binned ECE
closely tracks LS-ECE, indicating that the theoretical pathologies of ECE may
be avoidable in practice.
\\ ( https://arxiv.org/abs/2402.10046 ,  478kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10062
Date: Sun, 4 Feb 2024 07:31:06 GMT   (1654kb,D)

Title: Optimal Parameter and Neuron Pruning for Out-of-Distribution Detection
Authors: Chao Chen, Zhihang Fu, Kai Liu, Ze Chen, Mingyuan Tao, Jieping Ye
Categories: cs.LG stat.ML
Comments: Accepted by NeurIPS 2023. 19 pages
Journal-ref: NeurIPS 2023
\\
  For a machine learning model deployed in real world scenarios, the ability of
detecting out-of-distribution (OOD) samples is indispensable and challenging.
Most existing OOD detection methods focused on exploring advanced training
skills or training-free tricks to prevent the model from yielding overconfident
confidence score for unknown samples. The training-based methods require
expensive training cost and rely on OOD samples which are not always available,
while most training-free methods can not efficiently utilize the prior
information from the training data. In this work, we propose an
\textbf{O}ptimal \textbf{P}arameter and \textbf{N}euron \textbf{P}runing
(\textbf{OPNP}) approach, which aims to identify and remove those parameters
and neurons that lead to over-fitting. The main method is divided into two
steps. In the first step, we evaluate the sensitivity of the model parameters
and neurons by averaging gradients over all training samples. In the second
step, the parameters and neurons with exceptionally large or close to zero
sensitivities are removed for prediction. Our proposal is training-free,
compatible with other post-hoc methods, and exploring the information from all
training data. Extensive experiments are performed on multiple OOD detection
tasks and model architectures, showing that our proposed OPNP consistently
outperforms the existing methods by a large margin.
\\ ( https://arxiv.org/abs/2402.10062 ,  1654kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10063
Date: Thu, 15 Feb 2024 16:30:45 GMT   (6995kb,D)

Title: Balancing the Causal Effects in Class-Incremental Learning
Authors: Junhao Zheng, Ruiyan Wang, Chongzhi Zhang, Huawen Feng, Qianli Ma
Categories: cs.LG
\\
  Class-Incremental Learning (CIL) is a practical and challenging problem for
achieving general artificial intelligence. Recently, Pre-Trained Models (PTMs)
have led to breakthroughs in both visual and natural language processing tasks.
Despite recent studies showing PTMs' potential ability to learn sequentially, a
plethora of work indicates the necessity of alleviating the catastrophic
forgetting of PTMs. Through a pilot study and a causal analysis of CIL, we
reveal that the crux lies in the imbalanced causal effects between new and old
data. Specifically, the new data encourage models to adapt to new classes while
hindering the adaptation of old classes. Similarly, the old data encourages
models to adapt to old classes while hindering the adaptation of new classes.
In other words, the adaptation process between new and old classes conflicts
from the causal perspective. To alleviate this problem, we propose Balancing
the Causal Effects (BaCE) in CIL. Concretely, BaCE proposes two objectives for
building causal paths from both new and old data to the prediction of new and
classes, respectively. In this way, the model is encouraged to adapt to all
classes with causal effects from both new and old data and thus alleviates the
causal imbalance problem. We conduct extensive experiments on continual image
classification, continual text classification, and continual named entity
recognition. Empirical results show that BaCE outperforms a series of CIL
methods on different tasks and settings.
\\ ( https://arxiv.org/abs/2402.10063 ,  6995kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10065
Date: Thu, 15 Feb 2024 16:30:55 GMT   (175kb,D)

Title: How Much Does Each Datapoint Leak Your Privacy? Quantifying the
  Per-datum Membership Leakage
Authors: Achraf Azize, Debabrota Basu
Categories: cs.LG cs.CR math.ST stat.ML stat.TH
\\
  We study the per-datum Membership Inference Attacks (MIAs), where an attacker
aims to infer whether a fixed target datum has been included in the input
dataset of an algorithm and thus, violates privacy. First, we define the
membership leakage of a datum as the advantage of the optimal adversary
targeting to identify it. Then, we quantify the per-datum membership leakage
for the empirical mean, and show that it depends on the Mahalanobis distance
between the target datum and the data-generating distribution. We further
assess the effect of two privacy defences, i.e. adding Gaussian noise and
sub-sampling. We quantify exactly how both of them decrease the per-datum
membership leakage. Our analysis builds on a novel proof technique that
combines an Edgeworth expansion of the likelihood ratio test and a
Lindeberg-Feller central limit theorem. Our analysis connects the existing
likelihood ratio and scalar product attacks, and also justifies different
canary selection strategies used in the privacy auditing literature. Finally,
our experiments demonstrate the impacts of the leakage score, the sub-sampling
ratio and the noise scale on the per-datum membership leakage as indicated by
the theory.
\\ ( https://arxiv.org/abs/2402.10065 ,  175kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10074
Date: Thu, 15 Feb 2024 16:37:14 GMT   (516kb,D)

Title: GraphCBAL: Class-Balanced Active Learning for Graph Neural Networks via
  Reinforcement Learning
Authors: Chengcheng Yu, Jiapeng Zhu, Xiang Li
Categories: cs.LG
\\
  Graph neural networks (GNNs) have recently demonstrated significant success.
Active learning for GNNs aims to query the valuable samples from the unlabeled
data for annotation to maximize the GNNs' performance at a low cost. However,
most existing methods for reinforced active learning in GNNs may lead to a
highly imbalanced class distribution, especially in highly skewed class
scenarios. This further adversely affects the classification performance. To
tackle this issue, in this paper, we propose a novel reinforced class-balanced
active learning framework for GNNs, namely, GraphCBAL. It learns an optimal
policy to acquire class-balanced and informative nodes for annotation,
maximizing the performance of GNNs trained with selected labeled nodes.
GraphCBAL designs class-balance-aware states, as well as a reward function that
achieves trade-off between model performance and class balance. We further
upgrade GraphCBAL to GraphCBAL++ by introducing a punishment mechanism to
obtain a more class-balanced labeled set. Extensive experiments on multiple
datasets demonstrate the effectiveness of the proposed approaches, achieving
superior performance over state-of-the-art baselines. In particular, our
methods can strike the balance between classification results and class
balance.
\\ ( https://arxiv.org/abs/2402.10074 ,  516kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10076
Date: Thu, 15 Feb 2024 16:38:41 GMT   (703kb,D)

Title: QUICK: Quantization-aware Interleaving and Conflict-free Kernel for
  efficient LLM inference
Authors: Taesu Kim, Jongho Lee, Daehyun Ahn, Sarang Kim, Jiwoong Choi, Minkyu
  Kim and Hyungjun Kim
Categories: cs.LG cs.AI
Comments: 9 pages, 8 figures
\\
  We introduce QUICK, a group of novel optimized CUDA kernels for the efficient
inference of quantized Large Language Models (LLMs). QUICK addresses the shared
memory bank-conflict problem of state-of-the-art mixed precision matrix
multiplication kernels. Our method interleaves the quantized weight matrices of
LLMs offline to skip the shared memory write-back after the dequantization. We
demonstrate up to 1.91x speedup over existing kernels of AutoAWQ on larger
batches and up to 1.94x throughput gain on representative LLM models on various
NVIDIA GPU devices.
\\ ( https://arxiv.org/abs/2402.10076 ,  703kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10082
Date: Thu, 15 Feb 2024 16:42:04 GMT   (1253kb,D)

Title: FedRDF: A Robust and Dynamic Aggregation Function against Poisoning
  Attacks in Federated Learning
Authors: Enrique M\'armol Campos and Aurora Gonz\'alez Vidal and Jos\'e Luis
  Hern\'andez Ramos and Antonio Skarmeta
Categories: cs.LG cs.CR
Comments: 14 pages, 9 figures, and 6 tables
\\
  Federated Learning (FL) represents a promising approach to typical privacy
concerns associated with centralized Machine Learning (ML) deployments. Despite
its well-known advantages, FL is vulnerable to security attacks such as
Byzantine behaviors and poisoning attacks, which can significantly degrade
model performance and hinder convergence. The effectiveness of existing
approaches to mitigate complex attacks, such as median, trimmed mean, or Krum
aggregation functions, has been only partially demonstrated in the case of
specific attacks. Our study introduces a novel robust aggregation mechanism
utilizing the Fourier Transform (FT), which is able to effectively handling
sophisticated attacks without prior knowledge of the number of attackers.
Employing this data technique, weights generated by FL clients are projected
into the frequency domain to ascertain their density function, selecting the
one exhibiting the highest frequency. Consequently, malicious clients' weights
are excluded. Our proposed approach was tested against various model poisoning
attacks, demonstrating superior performance over state-of-the-art aggregation
methods.
\\ ( https://arxiv.org/abs/2402.10082 ,  1253kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10095
Date: Thu, 15 Feb 2024 16:49:42 GMT   (3041kb,D)

Title: Classification Diffusion Models
Authors: Shahar Yadin, Noam Elata, Tomer Michaeli
Categories: cs.LG
\\
  A prominent family of methods for learning data distributions relies on
density ratio estimation (DRE), where a model is trained to $\textit{classify}$
between data samples and samples from some reference distribution. These
techniques are successful in simple low-dimensional settings but fail to
achieve good results on complex high-dimensional data, like images. A different
family of methods for learning distributions is that of denoising diffusion
models (DDMs), in which a model is trained to $\textit{denoise}$ data samples.
These approaches achieve state-of-the-art results in image, video, and audio
generation. In this work, we present $\textit{Classification Diffusion Models}$
(CDMs), a generative technique that adopts the denoising-based formalism of
DDMs while making use of a classifier that predicts the amount of noise added
to a clean signal, similarly to DRE methods. Our approach is based on the
observation that an MSE-optimal denoiser for white Gaussian noise can be
expressed in terms of the gradient of a cross-entropy-optimal classifier for
predicting the noise level. As we illustrate, CDM achieves better denoising
results compared to DDM, and leads to at least comparable FID in image
generation. CDM is also capable of highly efficient one-step exact likelihood
estimation, achieving state-of-the-art results among methods that use a single
step. Code is available on the project's webpage in
https://shaharYadin.github.io/CDM/ .
\\ ( https://arxiv.org/abs/2402.10095 ,  3041kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10097
Date: Thu, 15 Feb 2024 16:51:38 GMT   (2741kb,D)

Title: Adaptive Federated Learning in Heterogeneous Wireless Networks with
  Independent Sampling
Authors: Jiaxiang Geng, Yanzhao Hou, Xiaofeng Tao, Juncheng Wang and Bing Luo
Categories: cs.LG
Comments: 6 pages, 5 figures, published to IEEE International Conference on
  Communications (ICC)
\\
  Federated Learning (FL) algorithms commonly sample a random subset of clients
to address the straggler issue and improve communication efficiency. While
recent works have proposed various client sampling methods, they have
limitations in joint system and data heterogeneity design, which may not align
with practical heterogeneous wireless networks. In this work, we advocate a new
independent client sampling strategy to minimize the wall-clock training time
of FL, while considering data heterogeneity and system heterogeneity in both
communication and computation. We first derive a new convergence bound for
non-convex loss functions with independent client sampling and then propose an
adaptive bandwidth allocation scheme. Furthermore, we propose an efficient
independent client sampling algorithm based on the upper bounds on the
convergence rounds and the expected per-round training time, to minimize the
wall-clock time of FL, while considering both the data and system
heterogeneity. Experimental results under practical wireless network settings
with real-world prototype demonstrate that the proposed independent sampling
scheme substantially outperforms the current best sampling schemes under
various training models and datasets.
\\ ( https://arxiv.org/abs/2402.10097 ,  2741kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10098
Date: Tue, 6 Feb 2024 14:04:31 GMT   (2635kb,D)

Title: Parameter-tuning-free data entry error unlearning with adaptive
  selective synaptic dampening
Authors: Stefan Schoepf, Jack Foster, Alexandra Brintrup
Categories: cs.LG
\\
  Data entry constitutes a fundamental component of the machine learning
pipeline, yet it frequently results in the introduction of labelling errors.
When a model has been trained on a dataset containing such errors its
performance is reduced. This leads to the challenge of efficiently unlearning
the influence of the erroneous data to improve the model performance without
needing to completely retrain the model. While model editing methods exist for
cases in which the correct label for a wrong entry is known, we focus on the
case of data entry errors where we do not know the correct labels for the
erroneous data. Our contribution is twofold. First, we introduce an extension
to the selective synaptic dampening unlearning method that removes the need for
parameter tuning, making unlearning accessible to practitioners. We demonstrate
the performance of this extension, adaptive selective synaptic dampening
(ASSD), on various ResNet18 and Vision Transformer unlearning tasks. Second, we
demonstrate the performance of ASSD in a supply chain delay prediction problem
with labelling errors using real-world data where we randomly introduce various
levels of labelling errors. The application of this approach is particularly
compelling in industrial settings, such as supply chain management, where a
significant portion of data entry occurs manually through Excel sheets,
rendering it error-prone. ASSD shows strong performance on general unlearning
benchmarks and on the error correction problem where it outperforms fine-tuning
for error correction.
\\ ( https://arxiv.org/abs/2402.10098 ,  2635kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10101
Date: Wed, 7 Feb 2024 14:21:21 GMT   (5706kb,D)

Title: Deep Learning Based Situation Awareness for Multiple Missiles Evasion
Authors: Edvards Scukins, Markus Klein, Lars Kroon, Petter \"Ogren
Categories: cs.LG cs.AI
\\
  As the effective range of air-to-air missiles increases, it becomes harder
for human operators to maintain the situational awareness needed to keep a UAV
safe. In this work, we propose a decision support tool to help UAV operators in
Beyond Visual Range (BVR) air combat scenarios assess the risks of different
options and make decisions based on those. Earlier work focused on the threat
posed by a single missile, and in this work, we extend the ideas to several
missile threats. The proposed method uses Deep Neural Networks (DNN) to learn
from high-fidelity simulations to provide the operator with an outcome estimate
for a set of different strategies. Our results demonstrate that the proposed
system can manage multiple incoming missiles, evaluate a family of options, and
recommend the least risky course of action.
\\ ( https://arxiv.org/abs/2402.10101 ,  5706kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10130
Date: Thu, 15 Feb 2024 17:34:56 GMT   (20743kb,D)

Title: Is Continual Learning Ready for Real-world Challenges?
Authors: Theodora Kontogianni, Yuanwen Yue, Siyu Tang, Konrad Schindler
Categories: cs.LG cs.AI cs.CV
\\
  Despite continual learning's long and well-established academic history, its
application in real-world scenarios remains rather limited. This paper contends
that this gap is attributable to a misalignment between the actual challenges
of continual learning and the evaluation protocols in use, rendering proposed
solutions ineffective for addressing the complexities of real-world setups. We
validate our hypothesis and assess progress to date, using a new 3D semantic
segmentation benchmark, OCL-3DSS. We investigate various continual learning
schemes from the literature by utilizing more realistic protocols that
necessitate online and continual learning for dynamic, real-world scenarios
(eg., in robotics and 3D vision applications). The outcomes are sobering: all
considered methods perform poorly, significantly deviating from the upper bound
of joint offline training. This raises questions about the applicability of
existing methods in realistic settings. Our paper aims to initiate a paradigm
shift, advocating for the adoption of continual learning methods through new
experimental protocols that better emulate real-world conditions to facilitate
breakthroughs in the field.
\\ ( https://arxiv.org/abs/2402.10130 ,  20743kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10135
Date: Thu, 15 Feb 2024 17:38:32 GMT   (742kb,D)

Title: Benchmarking federated strategies in Peer-to-Peer Federated learning for
  biomedical data
Authors: Jose L. Salmeron, Irina Ar\'evalo, Antonio Ruiz-Celma
Categories: cs.LG cs.AI cs.DC
Journal-ref: Heliyon 9 (2023) e16925
\\
  The increasing requirements for data protection and privacy has attracted a
huge research interest on distributed artificial intelligence and specifically
on federated learning, an emerging machine learning approach that allows the
construction of a model between several participants who hold their own private
data. In the initial proposal of federated learning the architecture was
centralised and the aggregation was done with federated averaging, meaning that
a central server will orchestrate the federation using the most straightforward
averaging strategy. This research is focused on testing different federated
strategies in a peer-to-peer environment. The authors propose various
aggregation strategies for federated learning, including weighted averaging
aggregation, using different factors and strategies based on participant
contribution. The strategies are tested with varying data sizes to identify the
most robust ones. This research tests the strategies with several biomedical
datasets and the results of the experiments show that the accuracy-based
weighted average outperforms the classical federated averaging method.
\\ ( https://arxiv.org/abs/2402.10135 ,  742kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10142
Date: Thu, 15 Feb 2024 17:48:58 GMT   (14784kb,D)

Title: Tracking Changing Probabilities via Dynamic Learners
Authors: Omid Madani
Categories: cs.LG cs.AI
Comments: 57 pages, 24 figures, 17 tables
MSC-class: 68T05
ACM-class: I.2.6
\\
  Consider a predictor, a learner, whose input is a stream of discrete items.
The predictor's task, at every time point, is probabilistic multiclass
prediction, i.e., to predict which item may occur next by outputting zero or
more candidate items, each with a probability, after which the actual item is
revealed and the predictor learns from this observation. To output
probabilities, the predictor keeps track of the proportions of the items it has
seen. The predictor has constant (limited) space and we seek efficient
prediction and update techniques: The stream is unbounded, the set of items is
unknown to the predictor and their totality can also grow unbounded. Moreover,
there is non-stationarity: the underlying frequencies of items may change,
substantially, from time to time. For instance, new items may start appearing
and a few currently frequent items may cease to occur again. The predictor,
being space-bounded, need only provide probabilities for those items with
(currently) sufficiently high frequency, i.e., the salient items. This problem
is motivated in the setting of prediction games, a self-supervised learning
regime where concepts serve as both the predictors and the predictands, and the
set of concepts grows over time, resulting in non-stationarities as new
concepts are generated and used. We develop moving average techniques designed
to respond to such non-stationarities in a timely manner, and explore their
properties. One is a simple technique based on queuing of count snapshots, and
another is a combination of queuing together with an extended version of sparse
EMA. The latter combination supports predictand-specific dynamic learning
rates. We find that this flexibility allows for a more accurate and timely
convergence.
\\ ( https://arxiv.org/abs/2402.10142 ,  14784kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10145
Date: Thu, 15 Feb 2024 17:49:50 GMT   (398kb,D)

Title: A chaotic maps-based privacy-preserving distributed deep learning for
  incomplete and Non-IID datasets
Authors: Irina Ar\'evalo and Jose L. Salmeron
Categories: cs.LG cs.CR cs.DC
Journal-ref: IEEE Transactions on Emerging Topics in Computing, 2023
\\
  Federated Learning is a machine learning approach that enables the training
of a deep learning model among several participants with sensitive data that
wish to share their own knowledge without compromising the privacy of their
data. In this research, the authors employ a secured Federated Learning method
with an additional layer of privacy and proposes a method for addressing the
non-IID challenge. Moreover, differential privacy is compared with
chaotic-based encryption as layer of privacy. The experimental approach
assesses the performance of the federated deep learning model with differential
privacy using both IID and non-IID data. In each experiment, the Federated
Learning process improves the average performance metrics of the deep neural
network, even in the case of non-IID data.
\\ ( https://arxiv.org/abs/2402.10145 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10150
Date: Thu, 15 Feb 2024 17:57:54 GMT   (35572kb,D)

Title: $f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive
  Learning
Authors: Yiwei Lu,Guojun Zhang,Sun Sun,Hongyu Guo,Yaoliang Yu
Categories: cs.LG
Comments: Accepted to TMLR in 2023
\\
  In self-supervised contrastive learning, a widely-adopted objective function
is InfoNCE, which uses the heuristic cosine similarity for the representation
comparison, and is closely related to maximizing the Kullback-Leibler
(KL)-based mutual information. In this paper, we aim at answering two
intriguing questions: (1) Can we go beyond the KL-based objective? (2) Besides
the popular cosine similarity, can we design a better similarity function? We
provide answers to both questions by generalizing the KL-based mutual
information to the $f$-Mutual Information in Contrastive Learning ($f$-MICL)
using the $f$-divergences. To answer the first question, we provide a wide
range of $f$-MICL objectives which share the nice properties of InfoNCE (e.g.,
alignment and uniformity), and meanwhile result in similar or even superior
performance. For the second question, assuming that the joint feature
distribution is proportional to the Gaussian kernel, we derive an $f$-Gaussian
similarity with better interpretability and empirical performance. Finally, we
identify close relationships between the $f$-MICL objective and several popular
InfoNCE-based objectives. Using benchmark tasks from both vision and natural
language, we empirically evaluate $f$-MICL with different $f$-divergences on
various architectures (SimCLR, MoCo, and MoCo v3) and datasets. We observe that
$f$-MICL generally outperforms the benchmarks and the best-performing
$f$-divergence is task and dataset dependent.
\\ ( https://arxiv.org/abs/2402.10150 ,  35572kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10177
Date: Thu, 15 Feb 2024 18:27:18 GMT   (1372kb,D)

Title: Large Scale Constrained Clustering With Reinforcement Learning
Authors: Benedikt Schesch, Marco Caserta
Categories: cs.LG cs.AI
Comments: LEANOPT-24 AAAI
\\
  Given a network, allocating resources at clusters level, rather than at each
node, enhances efficiency in resource allocation and usage. In this paper, we
study the problem of finding fully connected disjoint clusters to minimize the
intra-cluster distances and maximize the number of nodes assigned to the
clusters, while also ensuring that no two nodes within a cluster exceed a
threshold distance. While the problem can easily be formulated using a binary
linear model, traditional combinatorial optimization solvers struggle when
dealing with large-scale instances. We propose an approach to solve this
constrained clustering problem via reinforcement learning. Our method involves
training an agent to generate both feasible and (near) optimal solutions. The
agent learns problem-specific heuristics, tailored to the instances encountered
in this task. In the results section, we show that our algorithm finds near
optimal solutions, even for large scale instances.
\\ ( https://arxiv.org/abs/2402.10177 ,  1372kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10184
Date: Thu, 15 Feb 2024 18:39:24 GMT   (706kb,D)

Title: Rethinking Information Structures in RLHF: Reward Generalization from a
  Graph Theory Perspective
Authors: Tianyi Qiu, Fanzhi Zeng, Jiaming Ji, Dong Yan, Kaile Wang, Jiayi Zhou,
  Han Yang, Josef Dai, Xuehai Pan, Yaodong Yang
Categories: cs.LG cs.AI cs.CL cs.DM
\\
  There is a trilemma in reinforcement learning from human feedback (RLHF): the
incompatibility between highly diverse contexts, low labeling cost, and
reliable alignment performance. Here we aim to mitigate such incompatibility
through the design of dataset information structures during reward modeling.
Specifically, we first reexamine the RLHF process and propose a theoretical
framework portraying it as an autoencoding process over text distributions. Our
framework formalizes the RLHF objective of ensuring distributional consistency
between human preference and large language model (LLM) behavior. Building on
this framework, we then systematically investigate the performance impact of
information structure in the reward modeling stage of RLHF. To further
understand reward generalization in the reward modeling stage, we introduce a
new method based on random graph theory that models generalization in the
semantic space. A key insight of our analysis is the superiority of the
tree-based information structure in reward modeling, compared to chain-based
baselines adopted by conventional RLHF methods. We derive that under highly
complex contexts with limited data, the tree-based reward model (RM) induces up
to $\Theta(\log n/\log\log n)$ times less variance than chain-based RM where
$n$ is the dataset size. To validate our theoretical contribution, we
demonstrate that on three different NLP tasks, the tree-based RM achieves 65%
win rate on average against chain-based baselines. Looking forward, we hope our
framework can serve as a step towards understanding goal misgeneralization.
\\ ( https://arxiv.org/abs/2402.10184 ,  706kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10186
Date: Thu, 15 Feb 2024 18:41:35 GMT   (1129kb,D)

Title: Self-consistent Validation for Machine Learning Electronic Structure
Authors: Gengyuan Hu, Gengchen Wei, Zekun Lou, Philip H.S. Torr, Wanli Ouyang,
  Han-sen Zhong, Chen Lin
Categories: cs.LG physics.chem-ph physics.comp-ph
Comments: 6 pages, 4 figures
\\
  Machine learning has emerged as a significant approach to efficiently tackle
electronic structure problems. Despite its potential, there is less guarantee
for the model to generalize to unseen data that hinders its application in
real-world scenarios. To address this issue, a technique has been proposed to
estimate the accuracy of the predictions. This method integrates machine
learning with self-consistent field methods to achieve both low validation cost
and interpret-ability. This, in turn, enables exploration of the model's
ability with active learning and instills confidence in its integration into
real-world studies.
\\ ( https://arxiv.org/abs/2402.10186 ,  1129kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10191
Date: Thu, 15 Feb 2024 18:48:21 GMT   (2734kb,D)

Title: FedAnchor: Enhancing Federated Semi-Supervised Learning with Label
  Contrastive Loss for Unlabeled Clients
Authors: Xinchi Qiu, Yan Gao, Lorenzo Sani, Heng Pan, Wanru Zhao, Pedro P. B.
  Gusmao, Mina Alibeigi, Alex Iacob, Nicholas D. Lane
Categories: cs.LG
\\
  Federated learning (FL) is a distributed learning paradigm that facilitates
collaborative training of a shared global model across devices while keeping
data localized. The deployment of FL in numerous real-world applications faces
delays, primarily due to the prevalent reliance on supervised tasks. Generating
detailed labels at edge devices, if feasible, is demanding, given resource
constraints and the imperative for continuous data updates. In addressing these
challenges, solutions such as federated semi-supervised learning (FSSL), which
relies on unlabeled clients' data and a limited amount of labeled data on the
server, become pivotal. In this paper, we propose FedAnchor, an innovative FSSL
method that introduces a unique double-head structure, called anchor head,
paired with the classification head trained exclusively on labeled anchor data
on the server. The anchor head is empowered with a newly designed label
contrastive loss based on the cosine similarity metric. Our approach mitigates
the confirmation bias and overfitting issues associated with pseudo-labeling
techniques based on high-confidence model prediction samples. Extensive
experiments on CIFAR10/100 and SVHN datasets demonstrate that our method
outperforms the state-of-the-art method by a significant margin in terms of
convergence rate and model accuracy.
\\ ( https://arxiv.org/abs/2402.10191 ,  2734kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10192
Date: Thu, 15 Feb 2024 18:48:32 GMT   (1650kb,D)

Title: Multi-Excitation Projective Simulation with a Many-Body Physics Inspired
  Inductive Bias
Authors: Philip A. LeMaitre, Marius Krumm, and Hans J. Briegel
Categories: cs.LG cs.AI cs.DM quant-ph
Comments: 24 pages, 6 figures. Code repository at
  https://github.com/MariusKrumm/ManyBodyMEPS
\\
  With the impressive progress of deep learning, applications relying on
machine learning are increasingly being integrated into daily life. However,
most deep learning models have an opaque, oracle-like nature making it
difficult to interpret and understand their decisions. This problem led to the
development of the field known as eXplainable Artificial Intelligence (XAI).
One method in this field known as Projective Simulation (PS) models a
chain-of-thought as a random walk of a particle on a graph with vertices that
have concepts attached to them. While this description has various benefits,
including the possibility of quantization, it cannot be naturally used to model
thoughts that combine several concepts simultaneously. To overcome this
limitation, we introduce Multi-Excitation Projective Simulation (mePS), a
generalization that considers a chain-of-thought to be a random walk of several
particles on a hypergraph. A definition for a dynamic hypergraph is put forward
to describe the agent's training history along with applications to AI and
hypergraph visualization. An inductive bias inspired by the remarkably
successful few-body interaction models used in quantum many-body physics is
formalized for our classical mePS framework and employed to tackle the
exponential complexity associated with naive implementations of hypergraphs. We
prove that our inductive bias reduces the complexity from exponential to
polynomial, with the exponent representing the cutoff on how many particles can
interact. We numerically apply our method to two toy environments and a more
complex scenario modelling the diagnosis of a broken computer. These
environments demonstrate the resource savings provided by an appropriate choice
of inductive bias, as well as showcasing aspects of interpretability. A quantum
model for mePS is also briefly outlined and some future directions for it are
discussed.
\\ ( https://arxiv.org/abs/2402.10192 ,  1650kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10193
Date: Thu, 15 Feb 2024 18:50:06 GMT   (2931kb,D)

Title: BitDelta: Your Fine-Tune May Only Be Worth One Bit
Authors: James Liu, Guangxuan Xiao, Kai Li, Jason D. Lee, Song Han, Tri Dao,
  Tianle Cai
Categories: cs.LG cs.CL
\\
  Large Language Models (LLMs) are typically trained in two phases:
pre-training on large internet-scale datasets, and fine-tuning for downstream
tasks. Given the higher computational demand of pre-training, it's intuitive to
assume that fine-tuning adds less new information to the model, and is thus
more compressible. We explore this assumption by decomposing the weights of
fine-tuned models into their pre-trained components and an additional delta. We
introduce a simple method, BitDelta, which successfully quantizes this delta
down to 1 bit without compromising performance. This interesting finding not
only highlights the potential redundancy of information added during
fine-tuning, but also has significant implications for the multi-tenant serving
and multi-tenant storage of fine-tuned models. By enabling the use of a single
high-precision base model accompanied by multiple 1-bit deltas, BitDelta
dramatically reduces GPU memory requirements by more than 10x, which can also
be translated to enhanced generation latency in multi-tenant settings. We
validate BitDelta through experiments across Llama-2 and Mistral model
families, and on models up to 70B parameters, showcasing minimal performance
degradation over all tested settings.
\\ ( https://arxiv.org/abs/2402.10193 ,  2931kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10198
Date: Thu, 15 Feb 2024 18:55:05 GMT   (6115kb,D)

Title: Unlocking the Potential of Transformers in Time Series Forecasting with
  Sharpness-Aware Minimization and Channel-Wise Attention
Authors: Romain Ilbert and Ambroise Odonnat and Vasilii Feofanov and Aladin
  Virmaux and Giuseppe Paolo and Themis Palpanas and Ievgen Redko
Categories: cs.LG stat.ML
\\
  Transformer-based architectures achieved breakthrough performance in natural
language processing and computer vision, yet they remain inferior to simpler
linear baselines in multivariate long-term forecasting. To better understand
this phenomenon, we start by studying a toy linear forecasting problem for
which we show that transformers are incapable of converging to their true
solution despite their high expressive power. We further identify the attention
of transformers as being responsible for this low generalization capacity.
Building upon this insight, we propose a shallow lightweight transformer model
that successfully escapes bad local minima when optimized with sharpness-aware
optimization. We empirically demonstrate that this result extends to all
commonly used real-world multivariate time series datasets. In particular,
SAMformer surpasses the current state-of-the-art model TSMixer by 14.33% on
average, while having ~4 times fewer parameters. The code is available at
https://github.com/romilbert/samformer.
\\ ( https://arxiv.org/abs/2402.10198 ,  6115kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10202
Date: Thu, 15 Feb 2024 18:56:46 GMT   (5947kb,D)

Title: Bridging Associative Memory and Probabilistic Modeling
Authors: Rylan Schaeffer, Nika Zahedi, Mikail Khona, Dhruv Pai, Sang Truong,
  Yilun Du, Mitchell Ostrow, Sarthak Chandra, Andres Carranza, Ila Rani Fiete,
  Andrey Gromov, Sanmi Koyejo
Categories: cs.LG
\\
  Associative memory and probabilistic modeling are two fundamental topics in
artificial intelligence. The first studies recurrent neural networks designed
to denoise, complete and retrieve data, whereas the second studies learning and
sampling from probability distributions. Based on the observation that
associative memory's energy functions can be seen as probabilistic modeling's
negative log likelihoods, we build a bridge between the two that enables useful
flow of ideas in both directions. We showcase four examples: First, we propose
new energy-based models that flexibly adapt their energy functions to new
in-context datasets, an approach we term \textit{in-context learning of energy
functions}. Second, we propose two new associative memory models: one that
dynamically creates new memories as necessitated by the training data using
Bayesian nonparametrics, and another that explicitly computes proportional
memory assignments using the evidence lower bound. Third, using tools from
associative memory, we analytically and numerically characterize the memory
capacity of Gaussian kernel density estimators, a widespread tool in
probababilistic modeling. Fourth, we study a widespread implementation choice
in transformers -- normalization followed by self attention -- to show it
performs clustering on the hypersphere. Altogether, this work urges further
exchange of useful ideas between these two continents of artificial
intelligence.
\\ ( https://arxiv.org/abs/2402.10202 ,  5947kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10206
Date: Thu, 15 Feb 2024 18:58:18 GMT   (12484kb,D)

Title: Ising on the Graph: Task-specific Graph Subsampling via the Ising Model
Authors: Maria B{\aa}nkestad, Jennifer Andersson, Sebastian Mair, Jens
  Sj\"olund
Categories: cs.LG cs.AI
\\
  Reducing a graph while preserving its overall structure is an important
problem with many applications. Typically, the reduction approaches either
remove edges (sparsification) or merge nodes (coarsening) in an unsupervised
way with no specific downstream task in mind. In this paper, we present an
approach for subsampling graph structures using an Ising model defined on
either the nodes or edges and learning the external magnetic field of the Ising
model using a graph neural network. Our approach is task-specific as it can
learn how to reduce a graph for a specific downstream task in an end-to-end
fashion. The utilized loss function of the task does not even have to be
differentiable. We showcase the versatility of our approach on three distinct
applications: image segmentation, 3D shape sparsification, and sparse
approximate matrix inverse determination.
\\ ( https://arxiv.org/abs/2402.10206 ,  12484kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10207
Date: Thu, 15 Feb 2024 18:58:31 GMT   (1125kb,D)

Title: Rewards-in-Context: Multi-objective Alignment of Foundation Models with
  Dynamic Preference Adjustment
Authors: Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu,
  Jianshu Chen
Categories: cs.LG cs.AI cs.CL
\\
  We consider the problem of multi-objective alignment of foundation models
with human preferences, which is a critical step towards helpful and harmless
AI systems. However, it is generally costly and unstable to fine-tune large
foundation models using reinforcement learning (RL), and the
multi-dimensionality, heterogeneity, and conflicting nature of human
preferences further complicate the alignment process. In this paper, we
introduce Rewards-in-Context (RiC), which conditions the response of a
foundation model on multiple rewards in its prompt context and applies
supervised fine-tuning for alignment. The salient features of RiC are
simplicity and adaptivity, as it only requires supervised fine-tuning of a
single foundation model and supports dynamic adjustment for user preferences
during inference time. Inspired by the analytical solution of an abstracted
convex optimization problem, our dynamic inference-time adjustment method
approaches the Pareto-optimal solution for multiple objectives. Empirical
evidence demonstrates the efficacy of our method in aligning both Large
Language Models (LLMs) and diffusion models to accommodate diverse rewards with
only around $10\%$ GPU hours compared with multi-objective RL baseline.
\\ ( https://arxiv.org/abs/2402.10207 ,  1125kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10208
Date: Thu, 15 Feb 2024 18:59:02 GMT   (4746kb,D)

Title: Recovering the Pre-Fine-Tuning Weights of Generative Models
Authors: Eliahu Horwitz, Jonathan Kahana, Yedid Hoshen
Categories: cs.LG cs.CL cs.CR cs.CV
\\
  The dominant paradigm in generative modeling consists of two steps: i)
pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained
model with human values via fine-tuning. This practice is considered safe, as
no current method can recover the unsafe, pre-fine-tuning model weights. In
this paper, we demonstrate that this assumption is often false. Concretely, we
present Spectral DeTuning, a method that can recover the weights of the
pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In
contrast to previous attacks that attempt to recover pre-fine-tuning
capabilities, our method aims to recover the exact pre-fine-tuning weights. Our
approach exploits this new vulnerability against large-scale models such as a
personalized Stable Diffusion and an aligned Mistral.
\\ ( https://arxiv.org/abs/2402.10208 ,  4746kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10210
Date: Thu, 15 Feb 2024 18:59:18 GMT   (32965kb,D)

Title: Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation
Authors: Huizhuo Yuan and Zixiang Chen and Kaixuan Ji and Quanquan Gu
Categories: cs.LG cs.AI cs.CL cs.CV stat.ML
Comments: 28 pages, 8 figures, 10 tables
\\
  Fine-tuning Diffusion Models remains an underexplored frontier in generative
artificial intelligence (GenAI), especially when compared with the remarkable
progress made in fine-tuning Large Language Models (LLMs). While cutting-edge
diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised
fine-tuning, their performance inevitably plateaus after seeing a certain
volume of data. Recently, reinforcement learning (RL) has been employed to
fine-tune diffusion models with human preference data, but it requires at least
two images ("winner" and "loser" images) for each text prompt. In this paper,
we introduce an innovative technique called self-play fine-tuning for diffusion
models (SPIN-Diffusion), where the diffusion model engages in competition with
its earlier versions, facilitating an iterative self-improvement process. Our
approach offers an alternative to conventional supervised fine-tuning and RL
strategies, significantly improving both model performance and alignment. Our
experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms
the existing supervised fine-tuning method in aspects of human preference
alignment and visual appeal right from its first iteration. By the second
iteration, it exceeds the performance of RLHF-based methods across all metrics,
achieving these results with less data.
\\ ( https://arxiv.org/abs/2402.10210 ,  32965kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10211
Date: Thu, 15 Feb 2024 18:59:43 GMT   (19562kb,D)

Title: Hierarchical State Space Models for Continuous Sequence-to-Sequence
  Modeling
Authors: Raunaq Bhirangi, Chenyu Wang, Venkatesh Pattabiraman, Carmel Majidi,
  Abhinav Gupta, Tess Hellebrekers, Lerrel Pinto
Categories: cs.LG cs.RO
\\
  Reasoning from sequences of raw sensory data is a ubiquitous problem across
fields ranging from medical devices to robotics. These problems often involve
using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to
predict sequences of desirable physical quantities (e.g. force, inertial
measurements). While classical approaches are powerful for locally-linear
prediction problems, they often fall short when using real-world sensors. These
sensors are typically non-linear, are affected by extraneous variables (e.g.
vibration), and exhibit data-dependent drift. For many problems, the prediction
task is exacerbated by small labeled datasets since obtaining ground-truth
labels requires expensive equipment. In this work, we present Hierarchical
State-Space Models (HiSS), a conceptually simple, new technique for continuous
sequential prediction. HiSS stacks structured state-space models on top of each
other to create a temporal hierarchy. Across six real-world sensor datasets,
from tactile-based state prediction to accelerometer-based inertial
measurement, HiSS outperforms state-of-the-art sequence models such as causal
Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments
further indicate that HiSS demonstrates efficient scaling to smaller datasets
and is compatible with existing data-filtering techniques. Code, datasets and
videos can be found on https://hiss-csp.github.io.
\\ ( https://arxiv.org/abs/2402.10211 ,  19562kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2311.13380 (*cross-listing*)
Date: Wed, 22 Nov 2023 13:20:25 GMT   (2257kb,D)
Date (revised v2): Mon, 5 Feb 2024 13:37:21 GMT   (2262kb,D)

Title: Analyzing the Evolution and Maintenance of ML Models on Hugging Face
Authors: Joel Casta\~no, Silverio Mart\'inez-Fern\'andez, Xavier Franch, Justus
  Bogner
Categories: cs.SE cs.AI cs.LG
Comments: Accepted at the 2024 IEEE/ACM 21th International Conference on Mining
  Software Repositories (MSR)
\\
  Hugging Face (HF) has established itself as a crucial platform for the
development and sharing of machine learning (ML) models. This repository mining
study, which delves into more than 380,000 models using data gathered via the
HF Hub API, aims to explore the community engagement, evolution, and
maintenance around models hosted on HF, aspects that have yet to be
comprehensively explored in the literature. We first examine the overall growth
and popularity of HF, uncovering trends in ML domains, framework usage, authors
grouping and the evolution of tags and datasets used. Through text analysis of
model card descriptions, we also seek to identify prevalent themes and insights
within the developer community. Our investigation further extends to the
maintenance aspects of models, where we evaluate the maintenance status of ML
models, classify commit messages into various categories (corrective,
perfective, and adaptive), analyze the evolution across development stages of
commits metrics and introduce a new classification system that estimates the
maintenance status of models based on multiple attributes. This study aims to
provide valuable insights about ML model maintenance and evolution that could
inform future model development strategies on platforms like HF.
\\ ( https://arxiv.org/abs/2311.13380 ,  2262kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09427 (*cross-listing*)
Date: Wed, 24 Jan 2024 05:28:29 GMT   (12368kb,D)

Title: DoorINet: A Deep-Learning Inertial Framework for Door-Mounted IoT
  Applications
Authors: Aleksei Zakharchenko, Sharon Farber, Itzik Klein
Categories: eess.SP cs.AI cs.LG
Comments: 10 pages, 14 figures, 4 tables
\\
  Many Internet of Things applications utilize low-cost, micro,
electro-mechanical inertial sensors. A common task is orientation estimation.
To tackle such a task, attitude and heading reference system algorithms are
applied. Relying on the gyroscope readings, the accelerometer readings are used
to update the attitude angles, and magnetometer measurements are utilized to
update the heading angle. In indoor environments, magnetometers suffer from
interference that degrades their performance. This mainly influences
applications focused on estimating the heading angle like finding the heading
angle of a closet or fridge door. To circumvent such situations, we propose
DoorINet, an end-to-end deep-learning framework to calculate the heading angle
from door-mounted, low-cost inertial sensors without using magnetometers. To
evaluate our approach, we record a unique dataset containing 391 minutes of
accelerometer and gyroscope measurements and corresponding ground-truth heading
angle. We show that our proposed approach outperforms commonly used, model
based approaches and data-driven methods.
\\ ( https://arxiv.org/abs/2402.09427 ,  12368kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09433 (*cross-listing*)
Date: Fri, 26 Jan 2024 03:23:09 GMT   (147kb,D)

Title: Electrical Behavior Association Mining for Household ShortTerm Energy
  Consumption Forecasting
Authors: Heyang Yu, Yuxi Sun, Yintao Liu, Guangchao Geng, Quanyuan Jiang
Categories: eess.SP cs.AI cs.LG cs.SY eess.SY
Comments: 3 figures and 4 tables; This manuscript is submitted for possible
  publication
\\
  Accurate household short-term energy consumption forecasting (STECF) is
crucial for home energy management, but it is technically challenging, due to
highly random behaviors of individual residential users. To improve the
accuracy of STECF on a day-ahead scale, this paper proposes an novel STECF
methodology that leverages association mining in electrical behaviors. First, a
probabilistic association quantifying and discovering method is proposed to
model the pairwise behaviors association and generate associated clusters.
Then, a convolutional neural network-gated recurrent unit (CNN-GRU) based
forecasting is provided to explore the temporal correlation and enhance
accuracy. The testing results demonstrate that this methodology yields a
significant enhancement in the STECF.
\\ ( https://arxiv.org/abs/2402.09433 ,  147kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09442 (*cross-listing*)
Date: Tue, 30 Jan 2024 08:53:54 GMT   (574kb)

Title: Progress in artificial intelligence applications based on the
  combination of self-driven sensors and deep learning
Authors: Weixiang Wan, Wenjian Sun, Bo Liu, Linying Pan, Jingyu Xu
Categories: eess.SP cs.AI
Comments: This aticle was accepted by ieee conference
\\
  In the era of Internet of Things, how to develop a smart sensor system with
sustainable power supply, easy deployment and flexible use has become a
difficult problem to be solved. The traditional power supply has problems such
as frequent replacement or charging when in use, which limits the development
of wearable devices. The contact-to-separate friction nanogenerator (TENG) was
prepared by using polychotomy thy lene (PTFE) and aluminum (AI) foils. Human
motion energy was collected by human body arrangement, and human motion posture
was monitored according to the changes of output electrical signals. In 2012,
Academician Wang Zhong lin and his team invented the triboelectric
nanogenerator (TENG), which uses Maxwell displacement current as a driving
force to directly convert mechanical stimuli into electrical signals, so it can
be used as a self-driven sensor. Teng-based sensors have the advantages of
simple structure and high instantaneous power density, which provides an
important means for building intelligent sensor systems. At the same time,
machine learning, as a technology with low cost, short development cycle,
strong data processing ability and prediction ability, has a significant effect
on the processing of a large number of electrical signals generated by TENG,
and the combination with TENG sensors will promote the rapid development of
intelligent sensor networks in the future. Therefore, this paper is based on
the intelligent sound monitoring and recognition system of TENG, which has good
sound recognition capability, and aims to evaluate the feasibility of the sound
perception module architecture in ubiquitous sensor networks.
\\ ( https://arxiv.org/abs/2402.09442 ,  574kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09443 (*cross-listing*)
Date: Tue, 30 Jan 2024 17:32:02 GMT   (717kb)

Title: Review of algorithms for predicting fatigue using EEG
Authors: Ildar Rakhmatulin
Categories: eess.SP cs.AI cs.LG
Comments: arXiv admin note: text overlap with arXiv:2401.15766
\\
  Fatigue detection is of paramount importance in enhancing safety,
productivity, and well-being across diverse domains, including transportation,
healthcare, and industry. This scientific paper presents a comprehensive
investigation into the application of machine learning algorithms for the
detection of physiological fatigue using Electroencephalogram (EEG) signals.
The primary objective of this study was to assess the efficacy of various
algorithms in predicting an individual's level of fatigue based on EEG data.
\\ ( https://arxiv.org/abs/2402.09443 ,  717kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09444 (*cross-listing*)
Date: Wed, 31 Jan 2024 15:37:12 GMT   (3168kb,D)

Title: Multimodal Action Quality Assessment
Authors: Ling-An Zeng and Wei-Shi Zheng
Categories: eess.SP cs.AI cs.CV
Comments: IEEE Transactions on Image Processing 2024
ACM-class: I.2.10
\\
  Action quality assessment (AQA) is to assess how well an action is performed.
Previous works perform modelling by only the use of visual information,
ignoring audio information. We argue that although AQA is highly dependent on
visual information, the audio is useful complementary information for improving
the score regression accuracy, especially for sports with background music,
such as figure skating and rhythmic gymnastics. To leverage multimodal
information for AQA, i.e., RGB, optical flow and audio information, we propose
a Progressive Adaptive Multimodal Fusion Network (PAMFN) that separately models
modality-specific information and mixed-modality information. Our model
consists of with three modality-specific branches that independently explore
modality-specific information and a mixed-modality branch that progressively
aggregates the modality-specific information from the modality-specific
branches. To build the bridge between modality-specific branches and the
mixed-modality branch, three novel modules are proposed. First, a
Modality-specific Feature Decoder module is designed to selectively transfer
modality-specific information to the mixed-modality branch. Second, when
exploring the interaction between modality-specific information, we argue that
using an invariant multimodal fusion policy may lead to suboptimal results, so
as to take the potential diversity in different parts of an action into
consideration. Therefore, an Adaptive Fusion Module is proposed to learn
adaptive multimodal fusion policies in different parts of an action. This
module consists of several FusionNets for exploring different multimodal fusion
strategies and a PolicyNet for deciding which FusionNets are enabled. Third, a
module called Cross-modal Feature Decoder is designed to transfer cross-modal
features generated by Adaptive Fusion Module to the mixed-modality branch.
\\ ( https://arxiv.org/abs/2402.09444 ,  3168kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09445 (*cross-listing*)
Date: Wed, 31 Jan 2024 16:53:50 GMT   (7559kb,D)

Title: iMove: Exploring Bio-impedance Sensing for Fitness Activity Recognition
Authors: Mengxi Liu, Vitor Fortes Rey, Yu Zhang, Lala Shakti Swarup Ray, Bo
  Zhou, Paul Lukowicz
Categories: eess.SP cs.AI cs.LG cs.RO
Comments: Accepted by percom2024
\\
  Automatic and precise fitness activity recognition can be beneficial in
aspects from promoting a healthy lifestyle to personalized preventative
healthcare. While IMUs are currently the prominent fitness tracking modality,
through iMove, we show bio-impedence can help improve IMU-based fitness
tracking through sensor fusion and contrastive learning.To evaluate our
methods, we conducted an experiment including six upper body fitness activities
performed by ten subjects over five days to collect synchronized data from
bio-impedance across two wrists and IMU on the left wrist.The contrastive
learning framework uses the two modalities to train a better IMU-only
classification model, where bio-impedance is only required at the training
phase, by which the average Macro F1 score with the input of a single IMU was
improved by 3.22 \% reaching 84.71 \% compared to the 81.49 \% of the IMU
baseline model. We have also shown how bio-impedance can improve human activity
recognition (HAR) directly through sensor fusion, reaching an average Macro F1
score of 89.57 \% (two modalities required for both training and inference)
even if Bio-impedance alone has an average macro F1 score of 75.36 \%, which is
outperformed by IMU alone. In addition, similar results were obtained in an
extended study on lower body fitness activity classification, demonstrating the
generalisability of our approach.Our findings underscore the potential of
sensor fusion and contrastive learning as valuable tools for advancing fitness
activity recognition, with bio-impedance playing a pivotal role in augmenting
the capabilities of IMU-based systems.
\\ ( https://arxiv.org/abs/2402.09445 ,  7559kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09447 (*cross-listing*)
Date: Wed, 31 Jan 2024 23:13:38 GMT   (2236kb,D)

Title: Wavelet Analysis of Noninvasive EEG Signals Discriminates Complex and
  Natural Grasp Types
Authors: Ali Rabiee, Sima Ghafoori, Anna Cetera, Reza Abiri
Categories: eess.SP cs.AI cs.LG q-bio.NC
\\
  This research aims to decode hand grasps from Electroencephalograms (EEGs)
for dexterous neuroprosthetic development and Brain-Computer Interface (BCI)
applications, especially for patients with motor disorders. Particularly, it
focuses on distinguishing two complex natural power and precision grasps in
addition to a neutral condition as a no-movement condition using a new
EEG-based BCI platform and wavelet signal processing. Wavelet analysis involved
generating time-frequency and topographic maps from wavelet power coefficients.
Then, by using machine learning techniques with novel wavelet features, we
achieved high average accuracies: 85.16% for multiclass, 95.37% for No-Movement
vs Power, 95.40% for No-Movement vs Precision, and 88.07% for Power vs
Precision, demonstrating the effectiveness of these features in EEG-based grasp
differentiation. In contrast to previous studies, a critical part of our study
was permutation feature importance analysis, which highlighted key features for
grasp classification. It revealed that the most crucial brain activities during
grasping occur in the motor cortex, within the alpha and beta frequency bands.
These insights demonstrate the potential of wavelet features in real-time
neuroprosthetic technology and BCI applications.
\\ ( https://arxiv.org/abs/2402.09447 ,  2236kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09448 (*cross-listing*)
Date: Wed, 31 Jan 2024 23:35:44 GMT   (3327kb,D)

Title: A Comparative Study of Conventional and Tripolar EEG for
  High-Performance Reach-to-Grasp BCI Systems
Authors: Ali Rabiee, Sima Ghafoori, Anna Cetera, Walter Besio, Reza Abiri
Categories: eess.SP cs.AI cs.LG
\\
  This study aims to enhance BCI applications for individuals with motor
impairments by comparing the effectiveness of tripolar EEG (tEEG) with
conventional EEG. The focus is on interpreting and decoding various grasping
movements, such as power grasp and precision grasp. The goal is to determine
which EEG technology is more effective in processing and translating grasp
related neural signals. The approach involved experimenting on ten healthy
participants who performed two distinct grasp movements: power grasp and
precision grasp, with a no movement condition serving as the baseline. Our
research presents a thorough comparison between EEG and tEEG in decoding
grasping movements. This comparison spans several key parameters, including
signal to noise ratio (SNR), spatial resolution via functional connectivity,
ERPs, and wavelet time frequency analysis. Additionally, our study involved
extracting and analyzing statistical features from the wavelet coefficients,
and both binary and multiclass classification methods were employed. Four
machine learning algorithms were used to evaluate the decoding accuracies. Our
results indicated that tEEG demonstrated superior performance over conventional
EEG in various aspects. This included a higher signal to noise ratio, enhanced
spatial resolution, and more informative data in ERPs and wavelet time
frequency analysis. The use of tEEG led to notable improvements in decoding
accuracy for differentiating movement types. Specifically, tEEG achieved around
90% accuracy in binary and 75.97% for multiclass classification. These results
are markedly better than those from standard EEG, which recorded a maximum of
77.85% and 61.27% in similar tasks, respectively. These findings highlight the
superior effectiveness of tEEG over EEG in decoding grasp types and its
competitive or superior performance in complex classifications compared with
existing research.
\\ ( https://arxiv.org/abs/2402.09448 ,  3327kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09450 (*cross-listing*)
Date: Fri, 2 Feb 2024 10:04:13 GMT   (18862kb,D)

Title: Guiding Masked Representation Learning to Capture Spatio-Temporal
  Relationship of Electrocardiogram
Authors: Yeongyeon Na, Minje Park, Yunwon Tae, Sunghoon Joo
Categories: eess.SP cs.AI cs.LG
Comments: ICLR 2024.The first three authors contribute equally
\\
  Electrocardiograms (ECG) are widely employed as a diagnostic tool for
monitoring electrical signals originating from a heart. Recent machine learning
research efforts have focused on the application of screening various diseases
using ECG signals. However, adapting to the application of screening disease is
challenging in that labeled ECG data are limited. Achieving general
representation through self-supervised learning (SSL) is a well-known approach
to overcome the scarcity of labeled data; however, a naive application of SSL
to ECG data, without considering the spatial-temporal relationships inherent in
ECG signals, may yield suboptimal results. In this paper, we introduce ST-MEM
(Spatio-Temporal Masked Electrocardiogram Modeling), designed to learn
spatio-temporal features by reconstructing masked 12-lead ECG data. ST-MEM
outperforms other SSL baseline methods in various experimental settings for
arrhythmia classification tasks. Moreover, we demonstrate that ST-MEM is
adaptable to various lead combinations. Through quantitative and qualitative
analysis, we show a spatio-temporal relationship within ECG data.
\\ ( https://arxiv.org/abs/2402.09450 ,  18862kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09453 (*cross-listing*)
Date: Mon, 5 Feb 2024 03:57:30 GMT   (736kb)

Title: Improving EEG Signal Classification Accuracy Using Wasserstein
  Generative Adversarial Networks
Authors: Joshua Park, Priyanshu Mahey, Ore Adeniyi
Categories: eess.SP cs.AI cs.LG
Comments: 11 pages, 2 tables, 3 figures
\\
  Electroencephalography (EEG) plays a vital role in recording brain activities
and is integral to the development of brain-computer interface (BCI)
technologies. However, the limited availability and high variability of EEG
signals present substantial challenges in creating reliable BCIs. To address
this issue, we propose a practical solution drawing on the latest developments
in deep learning and Wasserstein Generative Adversarial Network (WGAN). The
WGAN was trained on the BCI2000 dataset, consisting of around 1500 EEG
recordings and 64 channels from 45 individuals. The generated EEG signals were
evaluated via three classifiers yielding improved average accuracies. The
quality of generated signals measured using Frechet Inception Distance (FID)
yielded scores of 1.345 and 11.565 for eyes-open and closed respectively. Even
without a spectral or spatial loss term, our WGAN model was able to emulate the
spectral and spatial properties of the EEG training data. The WGAN-generated
data mirrored the dominant alpha activity during closed-eye resting and high
delta waves in the training data in its topographic map and power spectral
density (PSD) plot. Our research testifies to the potential of WGANs in
addressing the limited EEG data issue for BCI development by enhancing a small
dataset to improve classifier generalizability.
\\ ( https://arxiv.org/abs/2402.09453 ,  736kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09459 (*cross-listing*)
Date: Sun, 4 Feb 2024 19:08:34 GMT   (3327kb)

Title: Custom IMU-Based Wearable System for Robust 2.4 GHz Wireless Human Body
  Parts Orientation Tracking and 3D Movement Visualization on an Avatar
Authors: Javier Gonz\'alez-Alonso, David Oviedo-Pastor, H\'ector J. Aguado,
  Francisco J. D\'iaz-Pernas, David Gonz\'alez-Ortega, and Mario
  Mart\'inez-Zarzuela
Categories: eess.SP cs.AI cs.CV cs.LG
Comments: 25 pages
Journal-ref: Sensors 2021, 21, 6642
DOI: 10.3390/s21196642
\\
  Recent studies confirm the applicability of Inertial Measurement Unit
(IMU)-based systems for human motion analysis. Notwithstanding, high-end
IMU-based commercial solutions are yet too expensive and complex to democratize
their use among a wide range of potential users. Less featured entry-level
commercial solutions are being introduced in the market, trying to fill this
gap, but still present some limitations that need to be overcome. At the same
time, there is a growing number of scientific papers using not commercial, but
custom do-it-yourself IMU-based systems in medical and sports applications.
Even though these solutions can help to popularize the use of this technology,
they have more limited features and the description on how to design and build
them from scratch is yet too scarce in the literature. The aim of this work is
two-fold: (1) Proving the feasibility of building an affordable custom solution
aimed at simultaneous multiple body parts orientation tracking; while providing
a detailed bottom-up description of the required hardware, tools, and
mathematical operations to estimate and represent 3D movement in real-time. (2)
Showing how the introduction of a custom 2.4 GHz communication protocol
including a channel hopping strategy can address some of the current
communication limitations of entry-level commercial solutions. The proposed
system can be used for wireless real-time human body parts orientation tracking
with up to 10 custom sensors, at least at 50 Hz. In addition, it provides a
more reliable motion data acquisition in Bluetooth and Wi-Fi crowded
environments, where the use of entry-level commercial solutions might be
unfeasible. This system can be used as a groundwork for developing affordable
human motion analysis solutions that do not require an accurate kinematic
analysis.
\\ ( https://arxiv.org/abs/2402.09459 ,  3327kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09465 (*cross-listing*)
Date: Fri, 9 Feb 2024 02:03:13 GMT   (1614kb,D)

Title: RLEEGNet: Integrating Brain-Computer Interfaces with Adaptive AI for
  Intuitive Responsiveness and High-Accuracy Motor Imagery Classification
Authors: Sriram V.C. Nallani and Gautham Ramachandran
Categories: eess.SP cs.AI
Comments: 23 pages, 1 figure, 6 tables
MSC-class: 68T05
\\
  Current approaches to prosthetic control are limited by their reliance on
traditional methods, which lack real-time adaptability and intuitive
responsiveness. These limitations are particularly pronounced in assistive
technologies designed for individuals with diverse cognitive states and motor
intentions. In this paper, we introduce a framework that leverages
Reinforcement Learning (RL) with Deep Q-Networks (DQN) for classification
tasks. Additionally, we present a preprocessing technique using the Common
Spatial Pattern (CSP) for multiclass motor imagery (MI) classification in a
One-Versus-The-Rest (OVR) manner. The subsequent 'csp space' transformation
retains the temporal dimension of EEG signals, crucial for extracting
discriminative features. The integration of DQN with a 1D-CNN-LSTM architecture
optimizes the decision-making process in real-time, thereby enhancing the
system's adaptability to the user's evolving needs and intentions. We elaborate
on the data processing methods for two EEG motor imagery datasets. Our
innovative model, RLEEGNet, incorporates a 1D-CNN-LSTM architecture as the
Online Q-Network within the DQN, facilitating continuous adaptation and
optimization of control strategies through feedback. This mechanism allows the
system to learn optimal actions through trial and error, progressively
improving its performance. RLEEGNet demonstrates high accuracy in classifying
MI-EEG signals, achieving as high as 100% accuracy in MI tasks across both the
GigaScience (3-class) and BCI-IV-2a (4-class) datasets. These results highlight
the potential of combining DQN with a 1D-CNN-LSTM architecture to significantly
enhance the adaptability and responsiveness of BCI systems.
\\ ( https://arxiv.org/abs/2402.09465 ,  1614kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09474 (*cross-listing*)
Date: Mon, 12 Feb 2024 11:04:08 GMT   (3191kb,D)

Title: Deciphering Heartbeat Signatures: A Vision Transformer Approach to
  Explainable Atrial Fibrillation Detection from ECG Signals
Authors: Aruna Mohan, Danne Elbers, Or Zilbershot, Fatemeh Afghah, David
  Vorchheimer
Categories: eess.SP cs.AI cs.CV cs.LG
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible. Submitted to IEEE EMBC 2024
\\
  Remote patient monitoring based on wearable single-lead electrocardiogram
(ECG) devices has significant potential for enabling the early detection of
heart disease, especially in combination with artificial intelligence (AI)
approaches for automated heart disease detection. There have been prior studies
applying AI approaches based on deep learning for heart disease detection.
However, these models are yet to be widely accepted as a reliable aid for
clinical diagnostics, in part due to the current black-box perception
surrounding many AI algorithms. In particular, there is a need to identify the
key features of the ECG signal that contribute toward making an accurate
diagnosis, thereby enhancing the interpretability of the model. In the present
study, we develop a vision transformer approach to identify atrial fibrillation
based on single-lead ECG data. A residual network (ResNet) approach is also
developed for comparison with the vision transformer approach. These models are
applied to the Chapman-Shaoxing dataset to classify atrial fibrillation, as
well as another common arrhythmia, sinus bradycardia, and normal sinus rhythm
heartbeats. The models enable the identification of the key regions of the
heartbeat that determine the resulting classification, and highlight the
importance of P-waves and T-waves, as well as heartbeat duration and signal
amplitude, in distinguishing normal sinus rhythm from atrial fibrillation and
sinus bradycardia.
\\ ( https://arxiv.org/abs/2402.09474 ,  3191kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09476 (*cross-listing*)
Date: Mon, 12 Feb 2024 22:09:43 GMT   (1726kb,D)

Title: AI-Enabled Lung Cancer Prognosis
Authors: Mahtab Darvish, Ryan Trask, Patrick Tallon, M\'elina Khansari, Lei
  Ren, Michelle Hershman, Bardia Yousefi
Categories: q-bio.QM cs.AI eess.IV
Comments: This is the author's version of a book chapter entitled: "Cancer
  Research: An Interdisciplinary Approach", Springer
Journal-ref: Springer book chapter "Cancer Research: An Interdisciplinary
  Approach" 2024
\\
  Lung cancer is the primary cause of cancer-related mortality, claiming
approximately 1.79 million lives globally in 2020, with an estimated 2.21
million new cases diagnosed within the same period. Among these, Non-Small Cell
Lung Cancer (NSCLC) is the predominant subtype, characterized by a notably
bleak prognosis and low overall survival rate of approximately 25% over five
years across all disease stages. However, survival outcomes vary considerably
based on the stage at diagnosis and the therapeutic interventions administered.
Recent advancements in artificial intelligence (AI) have revolutionized the
landscape of lung cancer prognosis. AI-driven methodologies, including machine
learning and deep learning algorithms, have shown promise in enhancing survival
prediction accuracy by efficiently analyzing complex multi-omics data and
integrating diverse clinical variables. By leveraging AI techniques, clinicians
can harness comprehensive prognostic insights to tailor personalized treatment
strategies, ultimately improving patient outcomes in NSCLC. Overviewing
AI-driven data processing can significantly help bolster the understanding and
provide better directions for using such systems.
\\ ( https://arxiv.org/abs/2402.09476 ,  1726kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09494 (*cross-listing*)
Date: Wed, 14 Feb 2024 13:00:40 GMT   (365kb)

Title: Can AI and humans genuinely communicate?
Authors: Constant Bonard
Categories: cs.HC cs.AI
Comments: January 2024 manuscript
\\
  Can AI and humans genuinely communicate? In this article, after giving some
background and motivating my proposal (sections 1 to 3), I explore a way to
answer this question that I call the "mental-behavioral methodology" (sections
4 and 5). This methodology follows the following three steps: First, spell out
what mental capacities are sufficient for human communication (as opposed to
communication more generally). Second, spell out the experimental paradigms
required to test whether a behavior exhibits these capacities. Third, apply or
adapt these paradigms to test whether an AI displays the relevant behaviors. If
the first two steps are successfully completed, and if the AI passes the tests
with human-like results, this constitutes evidence that this AI and humans can
genuinely communicate. This mental-behavioral methodology has the advantage
that we don't need to understand the workings of black-box algorithms, such as
standard deep neural networks. This is comparable to the fact that we don't
need to understand how human brains work to know that humans can genuinely
communicate. This methodology also has its disadvantages and I will discuss
some of them (section 6).
\\ ( https://arxiv.org/abs/2402.09494 ,  365kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09495 (*cross-listing*)
Date: Wed, 14 Feb 2024 13:20:09 GMT   (174kb,D)

Title: On the Potential of Network-Based Features for Fraud Detection
Authors: Catayoun Azarm, Erman Acar, Mickey van Zeelt
Categories: q-fin.RM cs.AI cs.LG
\\
  Online transaction fraud presents substantial challenges to businesses and
consumers, risking significant financial losses. Conventional rule-based
systems struggle to keep pace with evolving fraud tactics, leading to high
false positive rates and missed detections. Machine learning techniques offer a
promising solution by leveraging historical data to identify fraudulent
patterns. This article explores using the personalised PageRank (PPR) algorithm
to capture the social dynamics of fraud by analysing relationships between
financial accounts. The primary objective is to compare the performance of
traditional features with the addition of PPR in fraud detection models.
Results indicate that integrating PPR enhances the model's predictive power,
surpassing the baseline model. Additionally, the PPR feature provides unique
and valuable information, evidenced by its high feature importance score.
Feature stability analysis confirms consistent feature distributions across
training and test datasets.
\\ ( https://arxiv.org/abs/2402.09495 ,  174kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09497 (*cross-listing*)
Date: Wed, 14 Feb 2024 15:47:46 GMT   (58kb,D)

Title: Instruction Tuning for Secure Code Generation
Authors: Jingxuan He, Mark Vero, Gabriela Krasnopolska, Martin Vechev
Categories: cs.CR cs.AI cs.LG cs.SE
\\
  Modern language models (LMs) have gained widespread acceptance in everyday
and professional contexts, particularly in programming. An essential procedure
enabling this adoption is instruction tuning, which substantially enhances LMs'
practical utility by training them to follow user instructions and human
preferences. However, existing instruction tuning schemes overlook a crucial
aspect: the security of generated code. As a result, even the state-of-the-art
instruction-tuned LMs frequently produce unsafe code, posing significant
security risks. In this work, we introduce SafeCoder to address this gap.
SafeCoder performs security-centric fine-tuning using a diverse and
high-quality dataset that we collected using an automated pipeline. We
integrate the security fine-tuning with standard instruction tuning, to
facilitate a joint optimization of both security and utility. Despite its
simplicity, we show that SafeCoder is effective across a variety of popular LMs
and datasets. It is able to drastically improve security (by about 30%), while
preserving utility.
\\ ( https://arxiv.org/abs/2402.09497 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09508 (*cross-listing*)
Date: Wed, 14 Feb 2024 19:00:01 GMT   (7084kb,D)

Title: Arrange, Inpaint, and Refine: Steerable Long-term Music Audio Generation
  and Editing via Content-based Controls
Authors: Liwei Lin, Gus Xia, Yixiao Zhang, Junyan Jiang
Categories: cs.SD cs.AI
\\
  Controllable music generation plays a vital role in human-AI music
co-creation. While Large Language Models (LLMs) have shown promise in
generating high-quality music, their focus on autoregressive generation limits
their utility in music editing tasks. To bridge this gap, we introduce a novel
Parameter-Efficient Fine-Tuning (PEFT) method. This approach enables
autoregressive language models to seamlessly address music inpainting tasks.
Additionally, our PEFT method integrates frame-level content-based controls,
facilitating track-conditioned music refinement and score-conditioned music
arrangement. We apply this method to fine-tune MusicGen, a leading
autoregressive music generation model. Our experiments demonstrate promising
results across multiple music editing tasks, offering more flexible controls
for future AI-driven music editing tools. A demo
page\footnote{\url{https://kikyo-16.github.io/AIR/}.} showcasing our work and
source codes\footnote{\url{https://github.com/Kikyo-16/airgen}.} are available
online.
\\ ( https://arxiv.org/abs/2402.09508 ,  7084kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09540 (*cross-listing*)
Date: Wed, 14 Feb 2024 19:31:45 GMT   (66kb,D)

Title: Why Does Differential Privacy with Large Epsilon Defend Against
  Practical Membership Inference Attacks?
Authors: Andrew Lowy, Zhuohang Li, Jing Liu, Toshiaki Koike-Akino, Kieran
  Parsons, Ye Wang
Categories: cs.CR cs.AI cs.LG
Comments: Accepted at PPAI-24: AAAI Workshop on Privacy-Preserving Artificial
  Intelligence
MSC-class: 68P27
\\
  For small privacy parameter $\epsilon$, $\epsilon$-differential privacy (DP)
provides a strong worst-case guarantee that no membership inference attack
(MIA) can succeed at determining whether a person's data was used to train a
machine learning model. The guarantee of DP is worst-case because: a) it holds
even if the attacker already knows the records of all but one person in the
data set; and b) it holds uniformly over all data sets. In practical
applications, such a worst-case guarantee may be overkill: practical attackers
may lack exact knowledge of (nearly all of) the private data, and our data set
might be easier to defend, in some sense, than the worst-case data set. Such
considerations have motivated the industrial deployment of DP models with large
privacy parameter (e.g. $\epsilon \geq 7$), and it has been observed
empirically that DP with large $\epsilon$ can successfully defend against
state-of-the-art MIAs. Existing DP theory cannot explain these empirical
findings: e.g., the theoretical privacy guarantees of $\epsilon \geq 7$ are
essentially vacuous. In this paper, we aim to close this gap between theory and
practice and understand why a large DP parameter can prevent practical MIAs. To
tackle this problem, we propose a new privacy notion called practical
membership privacy (PMP). PMP models a practical attacker's uncertainty about
the contents of the private data. The PMP parameter has a natural
interpretation in terms of the success rate of a practical MIA on a given data
set. We quantitatively analyze the PMP parameter of two fundamental DP
mechanisms: the exponential mechanism and Gaussian mechanism. Our analysis
reveals that a large DP parameter often translates into a much smaller PMP
parameter, which guarantees strong privacy against practical MIAs. Using our
findings, we offer principled guidance for practitioners in choosing the DP
parameter.
\\ ( https://arxiv.org/abs/2402.09540 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09546 (*cross-listing*)
Date: Wed, 14 Feb 2024 19:45:17 GMT   (11439kb,D)

Title: How Secure Are Large Language Models (LLMs) for Navigation in Urban
  Environments?
Authors: Congcong Wen, Jiazhao Liang, Shuaihang Yuan, Hao Huang, Yi Fang
Categories: cs.RO cs.AI
\\
  In the field of robotics and automation, navigation systems based on Large
Language Models (LLMs) have recently shown impressive performance. However, the
security aspects of these systems have received relatively less attention. This
paper pioneers the exploration of vulnerabilities in LLM-based navigation
models in urban outdoor environments, a critical area given the technology's
widespread application in autonomous driving, logistics, and emergency
services. Specifically, we introduce a novel Navigational Prompt Suffix (NPS)
Attack that manipulates LLM-based navigation models by appending
gradient-derived suffixes to the original navigational prompt, leading to
incorrect actions. We conducted comprehensive experiments on an LLMs-based
navigation model that employs various LLMs for reasoning. Our results, derived
from the Touchdown and Map2Seq street-view datasets under both few-shot
learning and fine-tuning configurations, demonstrate notable performance
declines across three metrics in the face of both white-box and black-box
attacks. These results highlight the generalizability and transferability of
the NPS Attack, emphasizing the need for enhanced security in LLM-based
navigation systems. As an initial countermeasure, we propose the Navigational
Prompt Engineering (NPE) Defense strategy, concentrating on navigation-relevant
keywords to reduce the impact of adversarial suffixes. While initial findings
indicate that this strategy enhances navigational safety, there remains a
critical need for the wider research community to develop stronger defense
methods to effectively tackle the real-world challenges faced by these systems.
\\ ( https://arxiv.org/abs/2402.09546 ,  11439kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09579 (*cross-listing*)
Date: Wed, 14 Feb 2024 21:02:07 GMT   (1130kb)

Title: Advancing Building Energy Modeling with Large Language Models:
  Exploration and Case Studies
Authors: Liang Zhang, Zhelun Chen, Vitaly Ford
Categories: cs.HC cs.AI
\\
  The rapid progression in artificial intelligence has facilitated the
emergence of large language models like ChatGPT, offering potential
applications extending into specialized engineering modeling, especially
physics-based building energy modeling. This paper investigates the innovative
integration of large language models with building energy modeling software,
focusing specifically on the fusion of ChatGPT with EnergyPlus. A literature
review is first conducted to reveal a growing trend of incorporating of large
language models in engineering modeling, albeit limited research on their
application in building energy modeling. We underscore the potential of large
language models in addressing building energy modeling challenges and outline
potential applications including 1) simulation input generation, 2) simulation
output analysis and visualization, 3) conducting error analysis, 4)
co-simulation, 5) simulation knowledge extraction and training, and 6)
simulation optimization. Three case studies reveal the transformative potential
of large language models in automating and optimizing building energy modeling
tasks, underscoring the pivotal role of artificial intelligence in advancing
sustainable building practices and energy efficiency. The case studies
demonstrate that selecting the right large language model techniques is
essential to enhance performance and reduce engineering efforts. Besides direct
use of large language models, three specific techniques were utilized: 1)
prompt engineering, 2) retrieval-augmented generation, and 3) multi-agent large
language models. The findings advocate a multidisciplinary approach in future
artificial intelligence research, with implications extending beyond building
energy modeling to other specialized engineering modeling.
\\ ( https://arxiv.org/abs/2402.09579 ,  1130kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09581 (*cross-listing*)
Date: Wed, 14 Feb 2024 21:05:55 GMT   (257kb,D)

Title: Combatting deepfakes: Policies to address national security threats and
  rights violations
Authors: Andrea Miotti and Akash Wasil
Categories: cs.CR cs.AI cs.CY
\\
  This paper provides policy recommendations to address threats from deepfakes.
First, we provide background information about deepfakes and review the harms
they pose. We describe how deepfakes are currently used to proliferate sexual
abuse material, commit fraud, manipulate voter behavior, and pose threats to
national security. Second, we review previous legislative proposals designed to
address deepfakes. Third, we present a comprehensive policy proposal that
focuses on addressing multiple parts of the deepfake supply chain. The deepfake
supply chain begins with a small number of model developers, model providers,
and compute providers, and it expands to include billions of potential deepfake
creators. We describe this supply chain in greater detail and describe how
entities at each step of the supply chain ought to take reasonable measures to
prevent the creation and proliferation of deepfakes. Finally, we address
potential counterpoints of our proposal. Overall, deepfakes will present
increasingly severe threats to global security and individual liberties. To
address these threats, we call on policymakers to enact legislation that
addresses multiple parts of the deepfake supply chain.
\\ ( https://arxiv.org/abs/2402.09581 ,  257kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09604 (*cross-listing*)
Date: Wed, 14 Feb 2024 22:26:07 GMT   (12540kb,D)

Title: Medical Image Segmentation with InTEnt: Integrated Entropy Weighting for
  Single Image Test-Time Adaptation
Authors: Haoyu Dong and Nicholas Konz and Hanxue Gu and Maciej A. Mazurowski
Categories: cs.CV cs.AI
\\
  Test-time adaptation (TTA) refers to adapting a trained model to a new domain
during testing. Existing TTA techniques rely on having multiple test images
from the same domain, yet this may be impractical in real-world applications
such as medical imaging, where data acquisition is expensive and imaging
conditions vary frequently. Here, we approach such a task, of adapting a
medical image segmentation model with only a single unlabeled test image. Most
TTA approaches, which directly minimize the entropy of predictions, fail to
improve performance significantly in this setting, in which we also observe the
choice of batch normalization (BN) layer statistics to be a highly important
yet unstable factor due to only having a single test domain example. To
overcome this, we propose to instead \textit{integrate} over predictions made
with various estimates of target domain statistics between the training and
test statistics, weighted based on their entropy statistics.
\\ ( https://arxiv.org/abs/2402.09604 ,  12540kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09649 (*cross-listing*)
Date: Thu, 15 Feb 2024 01:22:30 GMT   (8657kb,D)

Title: ProtChatGPT: Towards Understanding Proteins with Large Language Models
Authors: Chao Wang, Hehe Fan, Ruijie Quan, Yi Yang
Categories: cs.CE cs.AI q-bio.BM
\\
  Protein research is crucial in various fundamental disciplines, but
understanding their intricate structure-function relationships remains
challenging. Recent Large Language Models (LLMs) have made significant strides
in comprehending task-specific knowledge, suggesting the potential for
ChatGPT-like systems specialized in protein to facilitate basic research. In
this work, we introduce ProtChatGPT, which aims at learning and understanding
protein structures via natural languages. ProtChatGPT enables users to upload
proteins, ask questions, and engage in interactive conversations to produce
comprehensive answers. The system comprises protein encoders, a
Protein-Language Pertaining Transformer (PLP-former), a projection adapter, and
an LLM. The protein first undergoes protein encoders and PLP-former to produce
protein embeddings, which are then projected by the adapter to conform with the
LLM. The LLM finally combines user questions with projected embeddings to
generate informative answers. Experiments show that ProtChatGPT can produce
promising responses to proteins and their corresponding questions. We hope that
ProtChatGPT could form the basis for further exploration and application in
protein research. Code and our pre-trained model will be publicly available.
\\ ( https://arxiv.org/abs/2402.09649 ,  8657kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09664 (*cross-listing*)
Date: Thu, 15 Feb 2024 02:24:46 GMT   (2360kb,D)

Title: CodeMind: A Framework to Challenge Large Language Models for Code
  Reasoning
Authors: Changshu Liu, Shizhuo Dylan Zhang, Reyhaneh Jabbarvand
Categories: cs.SE cs.AI cs.CL cs.PL
\\
  Solely relying on test passing to evaluate Large Language Models (LLMs) for
code synthesis may result in unfair assessment or promoting models with data
leakage. As an alternative, we introduce CodeMind, a framework designed to
gauge the code reasoning abilities of LLMs. CodeMind currently supports three
code reasoning tasks: Independent Execution Reasoning (IER), Dependent
Execution Reasoning (DER), and Specification Reasoning (SR). The first two
evaluate models to predict the execution output of an arbitrary code or code
the model could correctly synthesize. The third one evaluates the extent to
which LLMs implement the specified expected behavior. Our extensive evaluation
of nine LLMs across five benchmarks in two different programming languages
using CodeMind shows that LLMs fairly understand control flow constructs and,
in general, are capable of reasoning how inputs evolve to output, specifically
for simple programs and the ones they can correctly synthesize. However, their
performance drops for code with higher complexity, non-trivial logical and
arithmetic operators, non-primitive types, and API calls. Furthermore, we
observe that, while correlated, specification reasoning (essential for code
synthesis) does not imply execution reasoning (essential for broader
programming tasks such as testing and debugging): ranking LLMs based on test
passing can be different compared to code reasoning.
\\ ( https://arxiv.org/abs/2402.09664 ,  2360kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09683 (*cross-listing*)
Date: Thu, 15 Feb 2024 03:39:55 GMT   (2294kb,D)

Title: Exploring a Behavioral Model of "Positive Friction" in Human-AI
  Interaction
Authors: Zeya Chen, Ruth Schmidt
Categories: cs.HC cs.AI cs.CY
Comments: This preprint has not undergone peer review or any post-submission
  corrections. The Version of Record of this contribution will be published in
  Springer Nature Computer Science book series in Volume HCI International 2024
Journal-ref: DESIGN, USER EXPERIENCE AND USABILITY. HCII 2024
\\
  Designing seamless, frictionless user experiences has long been a dominant
trend in both applied behavioral science and artificial intelligence (AI), in
which the goal of making desirable actions easy and efficient informs efforts
to minimize friction in user experiences. However, in some settings, friction
can be genuinely beneficial, such as the insertion of deliberate delays to
increase reflection, preventing individuals from resorting to automatic or
biased behaviors, and enhancing opportunities for unexpected discoveries. More
recently, the popularization and availability of AI on a widespread scale has
only increased the need to examine how friction can help or hinder users of AI;
it also suggests a need to consider how positive friction can benefit AI
practitioners, both during development processes (e.g., working with diverse
teams) and to inform how AI is designed into offerings. This paper first
proposes a "positive friction" model that can help characterize how friction is
currently beneficial in user and developer experiences with AI, diagnose the
potential need for friction where it may not yet exist in these contexts, and
inform how positive friction can be used to generate solutions, especially as
advances in AI continue to be progress and new opportunities emerge. It then
explores this model in the context of AI users and developers by proposing the
value of taking a hybrid "AI+human" lens, and concludes by suggesting questions
for further exploration.
\\ ( https://arxiv.org/abs/2402.09683 ,  2294kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09712 (*cross-listing*)
Date: Thu, 15 Feb 2024 05:07:54 GMT   (12742kb,D)

Title: Diffusion Model with Cross Attention as an Inductive Bias for
  Disentanglement
Authors: Tao Yang, Cuiling Lan, Yan Lu, Nanning zheng
Categories: cs.CV cs.AI
\\
  Disentangled representation learning strives to extract the intrinsic factors
within observed data. Factorizing these representations in an unsupervised
manner is notably challenging and usually requires tailored loss functions or
specific structural designs. In this paper, we introduce a new perspective and
framework, demonstrating that diffusion models with cross-attention can serve
as a powerful inductive bias to facilitate the learning of disentangled
representations. We propose to encode an image to a set of concept tokens and
treat them as the condition of the latent diffusion for image reconstruction,
where cross-attention over the concept tokens is used to bridge the interaction
between the encoder and diffusion. Without any additional regularization, this
framework achieves superior disentanglement performance on the benchmark
datasets, surpassing all previous methods with intricate designs. We have
conducted comprehensive ablation studies and visualization analysis, shedding
light on the functioning of this model. This is the first work to reveal the
potent disentanglement capability of diffusion models with cross-attention,
requiring no complex designs. We anticipate that our findings will inspire more
investigation on exploring diffusion for disentangled representation learning
towards more sophisticated data analysis and understanding.
\\ ( https://arxiv.org/abs/2402.09712 ,  12742kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09721 (*cross-listing*)
Date: Thu, 15 Feb 2024 05:30:47 GMT   (70kb)

Title: Persuading a Learning Agent
Authors: Tao Lin, Yiling Chen
Categories: cs.GT cs.AI cs.LG econ.TH
\\
  We study a repeated Bayesian persuasion problem (and more generally, any
generalized principal-agent problem with complete information) where the
principal does not have commitment power and the agent uses algorithms to learn
to respond to the principal's signals. We reduce this problem to a one-shot
generalized principal-agent problem with an approximately-best-responding
agent. This reduction allows us to show that: if the agent uses contextual
no-regret learning algorithms, then the principal can guarantee a utility that
is arbitrarily close to the principal's optimal utility in the classic
non-learning model with commitment; if the agent uses contextual no-swap-regret
learning algorithms, then the principal cannot obtain any utility significantly
more than the optimal utility in the non-learning model with commitment. The
difference between the principal's obtainable utility in the learning model and
the non-learning model is bounded by the agent's regret (swap-regret). If the
agent uses mean-based learning algorithms (which can be no-regret but not
no-swap-regret), then the principal can do significantly better than the
non-learning model. These conclusions hold not only for Bayesian persuasion,
but also for any generalized principal-agent problem with complete information,
including Stackelberg games and contract design.
\\ ( https://arxiv.org/abs/2402.09721 ,  70kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09722 (*cross-listing*)
Date: Thu, 15 Feb 2024 05:31:03 GMT   (7887kb,D)

Title: Reg-NF: Efficient Registration of Implicit Surfaces within Neural Fields
Authors: Stephen Hausler, David Hall, Sutharsan Mahendren and Peyman Moghadam
Categories: cs.RO cs.AI cs.CV
Comments: Accepted to ICRA 2024. The first two authors contributed equally
\\
  Neural fields, coordinate-based neural networks, have recently gained
popularity for implicitly representing a scene. In contrast to classical
methods that are based on explicit representations such as point clouds, neural
fields provide a continuous scene representation able to represent 3D geometry
and appearance in a way which is compact and ideal for robotics applications.
However, limited prior methods have investigated registering multiple neural
fields by directly utilising these continuous implicit representations. In this
paper, we present Reg-NF, a neural fields-based registration that optimises for
the relative 6-DoF transformation between two arbitrary neural fields, even if
those two fields have different scale factors. Key components of Reg-NF include
a bidirectional registration loss, multi-view surface sampling, and utilisation
of volumetric signed distance functions (SDFs). We showcase our approach on a
new neural field dataset for evaluating registration problems. We provide an
exhaustive set of experiments and ablation studies to identify the performance
of our approach, while also discussing limitations to provide future direction
to the research community on open challenges in utilizing neural fields in
unconstrained environments.
\\ ( https://arxiv.org/abs/2402.09722 ,  7887kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09723 (*cross-listing*)
Date: Thu, 15 Feb 2024 05:31:13 GMT   (2609kb,D)

Title: Best Arm Identification for Prompt Learning under a Limited Budget
Authors: Chengshuai Shi, Kun Yang, Jing Yang and Cong Shen
Categories: stat.ML cs.AI cs.IT cs.LG math.IT
\\
  The remarkable instruction-following capability of large language models
(LLMs) has sparked a growing interest in automatically learning suitable
prompts. However, while many effective methods have been proposed, the cost
incurred during the learning process (e.g., accessing LLM and evaluating the
responses) has not been considered. To overcome this limitation, this work
explicitly incorporates a finite budget constraint into prompt learning.
Towards developing principled solutions, a novel connection is established
between prompt learning and fixed-budget best arm identification (BAI-FB) in
multi-armed bandits (MAB). Based on this connection, a general framework TRIPLE
(besT aRm Identification for Prompt LEarning) is proposed to harness the power
of BAI-FB in prompt learning systematically. Unique characteristics of prompt
learning further lead to two embedding-based enhancements of TRIPLE by
exploiting the ideas of clustering and function approximation. Extensive
experiments on multiple well-adopted tasks using both GPT 3.5 and Llama2
demonstrate the significant performance improvement of TRIPLE over the previous
baselines while satisfying the limited budget constraints.
\\ ( https://arxiv.org/abs/2402.09723 ,  2609kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09728 (*cross-listing*)
Date: Thu, 15 Feb 2024 05:49:22 GMT   (6539kb,D)

Title: AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns
Authors: Ashfak Md Shibli and Mir Mehedi A. Pritom and Maanak Gupta
Categories: cs.CR cs.AI
Comments: 6 pages, 12 figures, published in ISDFS 2024
\\
  SMS phishing, also known as "smishing", is a growing threat that tricks users
into disclosing private information or clicking into URLs with malicious
content through fraudulent mobile text messages. In recent past, we have also
observed a rapid advancement of conversational generative AI chatbot services
(e.g., OpenAI's ChatGPT, Google's BARD), which are powered by pre-trained large
language models (LLMs). These AI chatbots certainly have a lot of utilities but
it is not systematically understood how they can play a role in creating
threats and attacks. In this paper, we propose AbuseGPT method to show how the
existing generative AI-based chatbot services can be exploited by attackers in
real world to create smishing texts and eventually lead to craftier smishing
campaigns. To the best of our knowledge, there is no pre-existing work that
evidently shows the impacts of these generative text-based models on creating
SMS phishing. Thus, we believe this study is the first of its kind to shed
light on this emerging cybersecurity threat. We have found strong empirical
evidences to show that attackers can exploit ethical standards in the existing
generative AI-based chatbot services by crafting prompt injection attacks to
create newer smishing campaigns. We also discuss some future research
directions and guidelines to protect the abuse of generative AI-based services
and safeguard users from smishing attacks.
\\ ( https://arxiv.org/abs/2402.09728 ,  6539kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09750 (*cross-listing*)
Date: Thu, 15 Feb 2024 07:00:06 GMT   (1816kb,D)

Title: Exploring the Potential of Large Language Models in Artistic Creation:
  Collaboration and Reflection on Creative Programming
Authors: Anqi Wang, Zhizhuo Yin, Yulu Hu, Yuanyuan Mao, Pan Hui
Categories: cs.HC cs.AI
Comments: 15 pages, 4 figures
ACM-class: J.5
\\
  Recently, the potential of large language models (LLMs) has been widely used
in assisting programming. However, current research does not explore the artist
potential of LLMs in creative coding within artist and AI collaboration. Our
work probes the reflection type of artists in the creation process with such
collaboration. We compare two common collaboration approaches: invoking the
entire program and multiple subtasks. Our findings exhibit artists' different
stimulated reflections in two different methods. Our finding also shows the
correlation of reflection type with user performance, user satisfaction, and
subjective experience in two collaborations through conducting two methods,
including experimental data and qualitative interviews. In this sense, our work
reveals the artistic potential of LLM in creative coding. Meanwhile, we provide
a critical lens of human-AI collaboration from the artists' perspective and
expound design suggestions for future work of AI-assisted creative tasks.
\\ ( https://arxiv.org/abs/2402.09750 ,  1816kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09766 (*cross-listing*)
Date: Thu, 15 Feb 2024 07:35:52 GMT   (3203kb,D)

Title: From Variability to Stability: Advancing RecSys Benchmarking Practices
Authors: Valeriy Shevchenko, Nikita Belousov, Alexey Vasilev, Vladimir
  Zholobov, Artyom Sosedka, Natalia Semenova, Anna Volodkevich, Andrey
  Savchenko, Alexey Zaytsev
Categories: cs.IR cs.AI cs.LG
Comments: 8 pages with 11 figures
\\
  In the rapidly evolving domain of Recommender Systems (RecSys), new
algorithms frequently claim state-of-the-art performance based on evaluations
over a limited set of arbitrarily selected datasets. However, this approach may
fail to holistically reflect their effectiveness due to the significant impact
of dataset characteristics on algorithm performance. Addressing this
deficiency, this paper introduces a novel benchmarking methodology to
facilitate a fair and robust comparison of RecSys algorithms, thereby advancing
evaluation practices. By utilizing a diverse set of $30$ open datasets,
including two introduced in this work, and evaluating $11$ collaborative
filtering algorithms across $9$ metrics, we critically examine the influence of
dataset characteristics on algorithm performance. We further investigate the
feasibility of aggregating outcomes from multiple datasets into a unified
ranking. Through rigorous experimental analysis, we validate the reliability of
our methodology under the variability of datasets, offering a benchmarking
strategy that balances quality and computational demands. This methodology
enables a fair yet effective means of evaluating RecSys algorithms, providing
valuable guidance for future research endeavors.
\\ ( https://arxiv.org/abs/2402.09766 ,  3203kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09784 (*cross-listing*)
Date: Thu, 15 Feb 2024 08:33:16 GMT   (1011kb,D)

Title: Sequential Recommendation on Temporal Proximities with Contrastive
  Learning and Self-Attention
Authors: Hansol Jung, Hyunwoo Seo and ChieHyeon Lim
Categories: cs.IR cs.AI
Comments: 10 pages, 9 figures
\\
  Sequential recommender systems identify user preferences from their past
interactions to predict subsequent items optimally. Although traditional
deep-learning-based models and modern transformer-based models in previous
studies capture unidirectional and bidirectional patterns within user-item
interactions, the importance of temporal contexts, such as individual
behavioral and societal trend patterns, remains underexplored. Notably, recent
models often neglect similarities in users' actions that occur implicitly among
users during analogous timeframes-a concept we term vertical temporal
proximity. These models primarily adapt the self-attention mechanisms of the
transformer to consider the temporal context in individual user actions.
Meanwhile, this adaptation still remains limited in considering the horizontal
temporal proximity within item interactions, like distinguishing between
subsequent item purchases within a week versus a month. To address these gaps,
we propose a sequential recommendation model called TemProxRec, which includes
contrastive learning and self-attention methods to consider temporal
proximities both across and within user-item interactions. The proposed
contrastive learning method learns representations of items selected in close
temporal periods across different users to be close. Simultaneously, the
proposed self-attention mechanism encodes temporal and positional contexts in a
user sequence using both absolute and relative embeddings. This way, our
TemProxRec accurately predicts the relevant items based on the user-item
interactions within a specific timeframe. We validate this work through
comprehensive experiments on TemProxRec, consistently outperforming existing
models on benchmark datasets as well as showing the significance of considering
the vertical and horizontal temporal proximities into sequential
recommendation.
\\ ( https://arxiv.org/abs/2402.09784 ,  1011kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09786 (*cross-listing*)
Date: Thu, 15 Feb 2024 08:34:21 GMT   (29724kb,D)

Title: Examining Pathological Bias in a Generative Adversarial Network
  Discriminator: A Case Study on a StyleGAN3 Model
Authors: Alvin Grissom II, Ryan F. Lei, Jeova Farias Sales Rocha Neto, Bailey
  Lin, Ryan Trotter
Categories: cs.CV cs.AI cs.LG
\\
  Generative adversarial networks generate photorealistic faces that are often
indistinguishable by humans from real faces. We find that the discriminator in
the pre-trained StyleGAN3 model, a popular GAN network, systematically
stratifies scores by both image- and face-level qualities and that this
disproportionately affects images across gender, race, and other categories. We
examine the discriminator's bias for color and luminance across axes perceived
race and gender; we then examine axes common in research on stereotyping in
social psychology.
\\ ( https://arxiv.org/abs/2402.09786 ,  29724kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09792 (*cross-listing*)
Date: Thu, 15 Feb 2024 08:47:35 GMT   (2117kb)

Title: System-level Impact of Non-Ideal Program-Time of Charge Trap Flash (CTF)
  on Deep Neural Network
Authors: S. Shrivastava, A. Biswas, S. Chakrabarty, G. Dash, V. Saraswat, and
  U. Ganguly
Categories: cs.NE cs.AI cs.ET eess.IV
\\
  Learning of deep neural networks (DNN) using Resistive Processing Unit (RPU)
architecture is energy-efficient as it utilizes dedicated neuromorphic hardware
and stochastic computation of weight updates for in-memory computing. Charge
Trap Flash (CTF) devices can implement RPU-based weight updates in DNNs.
However, prior work has shown that the weight updates (V_T) in CTF-based RPU
are impacted by the non-ideal program time of CTF. The non-ideal program time
is affected by two factors of CTF. Firstly, the effects of the number of input
pulses (N) or pulse width (pw), and secondly, the gap between successive update
pulses (t_gap) used for the stochastic computation of weight updates.
Therefore, the impact of this non-ideal program time must be studied for neural
network training simulations. In this study, Firstly, we propose a pulse-train
design compensation technique to reduce the total error caused by non-ideal
program time of CTF and stochastic variance of a network. Secondly, we simulate
RPU-based DNN with non-ideal program time of CTF on MNIST and Fashion-MNIST
datasets. We find that for larger N (~1000), learning performance approaches
the ideal (software-level) training level and, therefore, is not much impacted
by the choice of t_gap used to implement RPU-based weight updates. However, for
lower N (<500), learning performance depends on T_gap of the pulses. Finally,
we also performed an ablation study to isolate the causal factor of the
improved learning performance. We conclude that the lower noise level in the
weight updates is the most likely significant factor to improve the learning
performance of DNN. Thus, our study attempts to compensate for the error caused
by non-ideal program time and standardize the pulse length (N) and pulse gap
(t_gap) specifications for CTF-based RPUs for accurate system-level on-chip
training.
\\ ( https://arxiv.org/abs/2402.09792 ,  2117kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09795 (*cross-listing*)
Date: Thu, 15 Feb 2024 08:50:36 GMT   (1594kb,D)

Title: An advanced data fabric architecture leveraging homomorphic encryption
  and federated learning
Authors: Sakib Anwar Rieyan, Md. Raisul Kabir News, A.B.M. Muntasir Rahman,
  Sadia Afrin Khan, Sultan Tasneem Jawad Zaarif, Md. Golam Rabiul Alam,
  Mohammad Mehedi Hassan, Michele Ianni, Giancarlo Fortino
Categories: cs.CR cs.AI cs.DB
Journal-ref: Information Fusion, 102, 102004 (2024)
DOI: 10.1016/j.inffus.2023.102004
\\
  Data fabric is an automated and AI-driven data fusion approach to accomplish
data management unification without moving data to a centralized location for
solving complex data problems. In a Federated learning architecture, the global
model is trained based on the learned parameters of several local models that
eliminate the necessity of moving data to a centralized repository for machine
learning. This paper introduces a secure approach for medical image analysis
using federated learning and partially homomorphic encryption within a
distributed data fabric architecture. With this method, multiple parties can
collaborate in training a machine-learning model without exchanging raw data
but using the learned or fused features. The approach complies with laws and
regulations such as HIPAA and GDPR, ensuring the privacy and security of the
data. The study demonstrates the method's effectiveness through a case study on
pituitary tumor classification, achieving a significant level of accuracy.
However, the primary focus of the study is on the development and evaluation of
federated learning and partially homomorphic encryption as tools for secure
medical image analysis. The results highlight the potential of these techniques
to be applied to other privacy-sensitive domains and contribute to the growing
body of research on secure and privacy-preserving machine learning.
\\ ( https://arxiv.org/abs/2402.09795 ,  1594kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09820 (*cross-listing*)
Date: Thu, 15 Feb 2024 09:35:57 GMT   (662kb)

Title: Enhancing Cybersecurity Resilience in Finance with Deep Learning for
  Advanced Threat Detection
Authors: Yulu Gong, Mengran Zhu, Shuning Huo, Yafei Xiang, Hanyi Yu
Categories: cs.CR cs.AI cs.LG
\\
  In the age of the Internet, people's lives are increasingly dependent on
today's network technology. However, network technology is a double-edged
sword, bringing convenience to people but also posing many security challenges.
Maintaining network security and protecting the legitimate interests of users
is at the heart of network construction. Threat detection is an important part
of a complete and effective defense system. In the field of network information
security, the technical update of network attack and network protection is
spiraling. How to effectively detect unknown threats is one of the concerns of
network protection. Currently, network threat detection is usually based on
rules and traditional machine learning methods, which create artificial rules
or extract common spatiotemporal features, which cannot be applied to
large-scale data applications, and the emergence of unknown threats causes the
detection accuracy of the original model to decline. With this in mind, this
paper uses deep learning for advanced threat detection to improve cybersecurity
resilienc e in the financial industry. Many network security researchers have
shifted their focus to exceptio n-based intrusion detection techniques. The
detection technology mainly uses statistical machine learning methods -
collecting normal program and network behavior data, extracting
multidimensional features, and training decision machine learning models on
this basis (commonly used include naive Bayes, decision trees, support vector
machines, random forests, etc.). In the detection phase, program code or
network behavior that deviates from the normal value beyond the tolerance is
considered malicious code or network attack behavior.
\\ ( https://arxiv.org/abs/2402.09820 ,  662kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09867 (*cross-listing*)
Date: Thu, 15 Feb 2024 10:50:42 GMT   (3568kb,D)

Title: Characterizing Accuracy Trade-offs of EEG Applications on Embedded HMPs
Authors: Zain Taufique, Muhammad Awais Bin Altaf, Antonio Miele, Pasi
  Liljeberg, Anil Kanduri
Categories: eess.SP cs.AI cs.CV cs.LG cs.PF
Comments: 7 pages, 10 figures
\\
  Electroencephalography (EEG) recordings are analyzed using battery-powered
wearable devices to monitor brain activities and neurological disorders. These
applications require long and continuous processing to generate feasible
results. However, wearable devices are constrained with limited energy and
computation resources, owing to their small sizes for practical use cases.
Embedded heterogeneous multi-core platforms (HMPs) can provide better
performance within limited energy budgets for EEG applications. Error
resilience of the EEG application pipeline can be exploited further to maximize
the performance and energy gains with HMPs. However, disciplined tuning of
approximation on embedded HMPs requires a thorough exploration of the
accuracy-performance-power trade-off space. In this work, we characterize the
error resilience of three EEG applications, including Epileptic Seizure
Detection, Sleep Stage Classification, and Stress Detection on the real-world
embedded HMP test-bed of the Odroid XU3 platform. We present a combinatorial
evaluation of power-performance-accuracy trade-offs of EEG applications at
different approximation, power, and performance levels to provide insights into
the disciplined tuning of approximation in EEG applications on embedded
platforms.
\\ ( https://arxiv.org/abs/2402.09867 ,  3568kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09871 (*cross-listing*)
Date: Thu, 15 Feb 2024 10:55:01 GMT   (16868kb,D)

Title: MuChin: A Chinese Colloquial Description Benchmark for Evaluating
  Language Models in the Field of Music
Authors: Zihao Wang, Shuyu Li, Tao Zhang, Qi Wang, Pengfei Yu, Jinyang Luo, Yan
  Liu, Ming Xi, Kejun Zhang
Categories: cs.SD cs.AI cs.MM eess.AS
MSC-class: 68Txx(Primary)14F05, 91Fxx(Secondary)
ACM-class: I.2.7; J.5
\\
  The rapidly evolving multimodal Large Language Models (LLMs) urgently require
new benchmarks to uniformly evaluate their performance on understanding and
textually describing music. However, due to semantic gaps between Music
Information Retrieval (MIR) algorithms and human understanding, discrepancies
between professionals and the public, and low precision of annotations,
existing music description datasets cannot serve as benchmarks. To this end, we
present MuChin, the first open-source music description benchmark in Chinese
colloquial language, designed to evaluate the performance of multimodal LLMs in
understanding and describing music. We established the Caichong Music
Annotation Platform (CaiMAP) that employs an innovative multi-person,
multi-stage assurance method, and recruited both amateurs and professionals to
ensure the precision of annotations and alignment with popular semantics.
Utilizing this method, we built a dataset with multi-dimensional,
high-precision music annotations, the Caichong Music Dataset (CaiMD), and
carefully selected 1,000 high-quality entries to serve as the test set for
MuChin. Based on MuChin, we analyzed the discrepancies between professionals
and amateurs in terms of music description, and empirically demonstrated the
effectiveness of annotated data for fine-tuning LLMs. Ultimately, we employed
MuChin to evaluate existing music understanding models on their ability to
provide colloquial descriptions of music. All data related to the benchmark and
the code for scoring have been open-sourced.
\\ ( https://arxiv.org/abs/2402.09871 ,  16868kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09883 (*cross-listing*)
Date: Thu, 15 Feb 2024 11:15:54 GMT   (10402kb,D)

Title: Lester: rotoscope animation through video object segmentation and
  tracking
Authors: Ruben Tous
Categories: cs.CV cs.AI cs.GR cs.MM
\\
  This article introduces Lester, a novel method to automatically synthetise
retro-style 2D animations from videos. The method approaches the challenge
mainly as an object segmentation and tracking problem. Video frames are
processed with the Segment Anything Model (SAM) and the resulting masks are
tracked through subsequent frames with DeAOT, a method of hierarchical
propagation for semi-supervised video object segmentation. The geometry of the
masks' contours is simplified with the Douglas-Peucker algorithm. Finally,
facial traits, pixelation and a basic shadow effect can be optionally added.
The results show that the method exhibits an excellent temporal consistency and
can correctly process videos with different poses and appearances, dynamic
shots, partial shots and diverse backgrounds. The proposed method provides a
more simple and deterministic approach than diffusion models based
video-to-video translation pipelines, which suffer from temporal consistency
problems and do not cope well with pixelated and schematic outputs. The method
is also much most practical than techniques based on 3D human pose estimation,
which require custom handcrafted 3D models and are very limited with respect to
the type of scenes they can process.
\\ ( https://arxiv.org/abs/2402.09883 ,  10402kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09894 (*cross-listing*)
Date: Thu, 15 Feb 2024 11:39:11 GMT   (8546kb,D)

Title: Not Just Novelty: A Longitudinal Study on Utility and Customization of
  AI Workflows
Authors: Tao Long, Katy Ilonka Gero, Lydia B. Chilton
Categories: cs.HC cs.AI cs.CL cs.CY
Comments: 21 pages, 11 figures
\\
  Generative AI brings novel and impressive abilities to help people in
everyday tasks. There are many AI workflows that solve real and complex
problems by chaining AI outputs together with human interaction. Although there
is an undeniable lure of AI, it's uncertain how useful generative AI workflows
are after the novelty wears off. Additionally, tools built with generative AI
have the potential to be personalized and adapted quickly and easily, but do
users take advantage of the potential to customize? We conducted a three-week
longitudinal study with 12 users to understand the familiarization and
customization of generative AI tools for science communication. Our study
revealed that the familiarization phase lasts for 4.3 sessions, where users
explore the capabilities of the workflow and which aspects they find useful.
After familiarization, the perceived utility of the system is rated higher than
before, indicating that the perceived utility of AI is not just a novelty
effect. The increase in benefits mainly comes from end-users' ability to
customize prompts, and thus appropriate the system to their own needs. This
points to a future where generative AI systems can allow us to design for
appropriation.
\\ ( https://arxiv.org/abs/2402.09894 ,  8546kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09921 (*cross-listing*)
Date: Thu, 15 Feb 2024 12:58:27 GMT   (760kb,D)

Title: Identifying and modelling cognitive biases in mobility choices
Authors: Chloe Conrad and Carole Adam
Categories: cs.CY cs.AI cs.MA
Comments: M1 internship report from Univ. Lyon 1 Claude Bernard. Internship was
  from October 2022 to June 2023
ACM-class: K.4.2
\\
  This report presents results from an M1 internship dedicated to agent-based
modelling and simulation of daily mobility choices. This simulation is intended
to be realistic enough to serve as a basis for a serious game about the
mobility transition. In order to ensure this level of realism, we conducted a
survey to measure if real mobility choices are made rationally, or how biased
they are. Results analysed here show that various biases could play a role in
decisions. We then propose an implementation in a GAMA agent-based simulation.
\\ ( https://arxiv.org/abs/2402.09921 ,  760kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09982 (*cross-listing*)
Date: Thu, 15 Feb 2024 14:46:03 GMT   (1082kb)

Title: Data Augmentation and Transfer Learning Approaches Applied to Facial
  Expressions Recognition
Authors: Enrico Randellini and Leonardo Rigutini and Claudio Sacca'
Categories: cs.CV cs.AI cs.LG
Comments: The 11th International Conference on Artificial Intelligence, Soft
  Computing and Applications (AIAA 2021)
Journal-ref: Proceeding of the 11th International Conference on Artificial
  Intelligence, Soft Computing and Applications (AIAA 2021)
DOI: 10.5121/csit.2021.111912
\\
  The face expression is the first thing we pay attention to when we want to
understand a person's state of mind. Thus, the ability to recognize facial
expressions in an automatic way is a very interesting research field. In this
paper, because the small size of available training datasets, we propose a
novel data augmentation technique that improves the performances in the
recognition task. We apply geometrical transformations and build from scratch
GAN models able to generate new synthetic images for each emotion type. Thus,
on the augmented datasets we fine tune pretrained convolutional neural networks
with different architectures. To measure the generalization ability of the
models, we apply extra-database protocol approach, namely we train models on
the augmented versions of training dataset and test them on two different
databases. The combination of these techniques allows to reach average accuracy
values of the order of 85\% for the InceptionResNetV2 model.
\\ ( https://arxiv.org/abs/2402.09982 ,  1082kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10002 (*cross-listing*)
Date: Thu, 15 Feb 2024 15:10:17 GMT   (4186kb,D)

Title: MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D
  Point Cloud Understanding
Authors: Hai-Tao Yu, Mofei Song
Categories: cs.CV cs.AI cs.MM
Comments: Accepted by AAAI 2024
\\
  In perception, multiple sensory information is integrated to map visual
information from 2D views onto 3D objects, which is beneficial for
understanding in 3D environments. But in terms of a single 2D view rendered
from different angles, only limited partial information can be provided.The
richness and value of Multi-view 2D information can provide superior
self-supervised signals for 3D objects. In this paper, we propose a novel
self-supervised point cloud representation learning method, MM-Point, which is
driven by intra-modal and inter-modal similarity objectives. The core of
MM-Point lies in the Multi-modal interaction and transmission between 3D
objects and multiple 2D views at the same time. In order to more effectively
simultaneously perform the consistent cross-modal objective of 2D multi-view
information based on contrastive learning, we further propose Multi-MLP and
Multi-level Augmentation strategies. Through carefully designed transformation
strategies, we further learn Multi-level invariance in 2D Multi-views. MM-Point
demonstrates state-of-the-art (SOTA) performance in various downstream tasks.
For instance, it achieves a peak accuracy of 92.4% on the synthetic dataset
ModelNet40, and a top accuracy of 87.8% on the real-world dataset ScanObjectNN,
comparable to fully supervised methods. Additionally, we demonstrate its
effectiveness in tasks such as few-shot classification, 3D part segmentation
and 3D semantic segmentation.
\\ ( https://arxiv.org/abs/2402.10002 ,  4186kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10005 (*cross-listing*)
Date: Mon, 18 Dec 2023 03:04:42 GMT   (495kb,D)

Title: ML-ASPA: A Contemplation of Machine Learning-based Acoustic Signal
  Processing Analysis for Sounds, & Strains Emerging Technology
Authors: Ratul Ali, Aktarul Islam, Md. Shohel Rana, Saila Nasrin, Sohel Afzal
  Shajol and Professor Dr. A.H.M. Saifullah Sadi
Categories: cs.SD cs.AI cs.LG eess.AS
Comments: 7 pages, 5 figures, Article
MSC-class: 68Qxx, 68Uxx, 68Vxx, 68Wxx, 68Txx, 68-XX
ACM-class: J.7; D.2; G.4
\\
  Acoustic data serves as a fundamental cornerstone in advancing scientific and
engineering understanding across diverse disciplines, spanning biology,
communications, and ocean and Earth science. This inquiry meticulously explores
recent advancements and transformative potential within the domain of
acoustics, specifically focusing on machine learning (ML) and deep learning.
ML, comprising an extensive array of statistical techniques, proves
indispensable for autonomously discerning and leveraging patterns within data.
In contrast to traditional acoustics and signal processing, ML adopts a
data-driven approach, unveiling intricate relationships between features and
desired labels or actions, as well as among features themselves, given ample
training data. The application of ML to expansive sets of training data
facilitates the discovery of models elucidating complex acoustic phenomena such
as human speech and reverberation. The dynamic evolution of ML in acoustics
yields compelling results and holds substantial promise for the future. The
advent of electronic stethoscopes and analogous recording and data logging
devices has expanded the application of acoustic signal processing concepts to
the analysis of bowel sounds. This paper critically reviews existing literature
on acoustic signal processing for bowel sound analysis, outlining fundamental
approaches and applicable machine learning principles. It chronicles historical
progress in signal processing techniques that have facilitated the extraction
of valuable information from bowel sounds, emphasizing advancements in noise
reduction, segmentation, signal enhancement, feature extraction, sound
localization, and machine learning techniques...
\\ ( https://arxiv.org/abs/2402.10005 ,  495kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10050 (*cross-listing*)
Date: Thu, 15 Feb 2024 16:11:47 GMT   (4431kb,D)

Title: On-Demand Myoelectric Control Using Wake Gestures to Eliminate False
  Activations During Activities of Daily Living
Authors: Ethan Eddy, Evan Campbell, Scott Bateman, and Erik Scheme
Categories: cs.HC cs.AI
\\
  While myoelectric control has recently become a focus of increased research
as a possible flexible hands-free input modality, current control approaches
are prone to inadvertent false activations in real-world conditions. In this
work, a novel myoelectric control paradigm -- on-demand myoelectric control --
is proposed, designed, and evaluated, to reduce the number of unrelated muscle
movements that are incorrectly interpreted as input gestures . By leveraging
the concept of wake gestures, users were able to switch between a dedicated
control mode and a sleep mode, effectively eliminating inadvertent activations
during activities of daily living (ADLs). The feasibility of wake gestures was
demonstrated in this work through two online ubiquitous EMG control tasks with
varying difficulty levels; dismissing an alarm and controlling a robot. The
proposed control scheme was able to appropriately ignore almost all
non-targeted muscular inputs during ADLs (>99.9%) while maintaining sufficient
sensitivity for reliable mode switching during intentional wake gesture
elicitation. These results highlight the potential of wake gestures as a
critical step towards enabling ubiquitous myoelectric control-based on-demand
input for a wide range of applications.
\\ ( https://arxiv.org/abs/2402.10050 ,  4431kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10055 (*cross-listing*)
Date: Thu, 15 Feb 2024 16:25:28 GMT   (5846kb)

Title: Robust semi-automatic vessel tracing in the human retinal image by an
  instance segmentation neural network
Authors: Siyi Chen, Amir H. Kashani, Ji Yi
Categories: eess.IV cs.AI cs.CV
\\
  The morphology and hierarchy of the vascular systems are essential for
perfusion in supporting metabolism. In human retina, one of the most
energy-demanding organs, retinal circulation nourishes the entire inner retina
by an intricate vasculature emerging and remerging at the optic nerve head
(ONH). Thus, tracing the vascular branching from ONH through the vascular tree
can illustrate vascular hierarchy and allow detailed morphological
quantification, and yet remains a challenging task. Here, we presented a novel
approach for a robust semi-automatic vessel tracing algorithm on human fundus
images by an instance segmentation neural network (InSegNN). Distinct from
semantic segmentation, InSegNN separates and labels different vascular trees
individually and therefore enable tracing each tree throughout its branching.
We have built-in three strategies to improve robustness and accuracy with
temporal learning, spatial multi-sampling, and dynamic probability map. We
achieved 83% specificity, and 50% improvement in Symmetric Best Dice (SBD)
compared to literature, and outperformed baseline U-net. We have demonstrated
tracing individual vessel trees from fundus images, and simultaneously retain
the vessel hierarchy information. InSegNN paves a way for any subsequent
morphological analysis of vascular morphology in relation to retinal diseases.
\\ ( https://arxiv.org/abs/2402.10055 ,  5846kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10067 (*cross-listing*)
Date: Mon, 22 Jan 2024 15:37:04 GMT   (942kb,D)

Title: LLM-based policy generation for intent-based management of applications
Authors: Kristina Dzeparoska, Jieyu Lin, Ali Tizghadam, Alberto Leon-Garcia
Categories: cs.DC cs.AI cs.FL cs.HC cs.LG
Comments: This article has been accepted for publication in 2023 19th
  International Conference on Network and Service Management (CNSM), 3rd
  International Workshop on Analytics for Service and Application Management
  (AnServApp 2023)
Journal-ref: 2023 19th International Conference on Network and Service
  Management (CNSM), 2023, pp. 1-7
DOI: 10.23919/CNSM59352.2023.10327837
\\
  Automated management requires decomposing high-level user requests, such as
intents, to an abstraction that the system can understand and execute. This is
challenging because even a simple intent requires performing a number of
ordered steps. And the task of identifying and adapting these steps (as
conditions change) requires a decomposition approach that cannot be exactly
pre-defined beforehand. To tackle these challenges and support automated intent
decomposition and execution, we explore the few-shot capability of Large
Language Models (LLMs). We propose a pipeline that progressively decomposes
intents by generating the required actions using a policy-based abstraction.
This allows us to automate the policy execution by creating a closed control
loop for the intent deployment. To do so, we generate and map the policies to
APIs and form application management loops that perform the necessary
monitoring, analysis, planning and execution. We evaluate our proposal with a
use-case to fulfill and assure an application service chain of virtual network
functions. Using our approach, we can generalize and generate the necessary
steps to realize intents, thereby enabling intent automation for application
management.
\\ ( https://arxiv.org/abs/2402.10067 ,  942kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10086 (*cross-listing*)
Date: Thu, 8 Feb 2024 09:08:44 GMT   (5593kb,D)

Title: Explainable AI for Safe and Trustworthy Autonomous Driving: A Systematic
  Review
Authors: Anton Kuznietsov, Balint Gyevnar, Cheng Wang, Steven Peters, Stefano
  V. Albrecht
Categories: cs.RO cs.AI cs.CV cs.HC cs.LG
\\
  Artificial Intelligence (AI) shows promising applications for the perception
and planning tasks in autonomous driving (AD) due to its superior performance
compared to conventional methods. However, inscrutable AI systems exacerbate
the existing challenge of safety assurance of AD. One way to mitigate this
challenge is to utilize explainable AI (XAI) techniques. To this end, we
present the first comprehensive systematic literature review of explainable
methods for safe and trustworthy AD. We begin by analyzing the requirements for
AI in the context of AD, focusing on three key aspects: data, model, and
agency. We find that XAI is fundamental to meeting these requirements. Based on
this, we explain the sources of explanations in AI and describe a taxonomy of
XAI. We then identify five key contributions of XAI for safe and trustworthy AI
in AD, which are interpretable design, interpretable surrogate models,
interpretable monitoring, auxiliary explanations, and interpretable validation.
Finally, we propose a modular framework called SafeX to integrate these
contributions, enabling explanation delivery to users while simultaneously
ensuring the safety of AI models.
\\ ( https://arxiv.org/abs/2402.10086 ,  5593kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10091 (*cross-listing*)
Date: Thu, 1 Feb 2024 18:52:26 GMT   (1664kb,D)

Title: Text-Based Product Matching -- Semi-Supervised Clustering Approach
Authors: Alicja Martinek, Szymon {\L}ukasik, Amir H. Gandomi
Categories: cs.DB cs.AI cs.LG
\\
  Matching identical products present in multiple product feeds constitutes a
crucial element of many tasks of e-commerce, such as comparing product
offerings, dynamic price optimization, and selecting the assortment
personalized for the client. It corresponds to the well-known machine learning
task of entity matching, with its own specificity, like omnipresent
unstructured data or inaccurate and inconsistent product descriptions. This
paper aims to present a new philosophy to product matching utilizing a
semi-supervised clustering approach. We study the properties of this method by
experimenting with the IDEC algorithm on the real-world dataset using
predominantly textual features and fuzzy string matching, with more standard
approaches as a point of reference. Encouraging results show that unsupervised
matching, enriched with a small annotated sample of product links, could be a
possible alternative to the dominant supervised strategy, requiring extensive
manual data labeling.
\\ ( https://arxiv.org/abs/2402.10091 ,  1664kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10093 (*cross-listing*)
Date: Thu, 15 Feb 2024 16:46:16 GMT   (844kb,D)

Title: MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained
  Representations
Authors: Benedikt Alkin and Lukas Miklautz and Sepp Hochreiter and Johannes
  Brandstetter
Categories: cs.CV cs.AI cs.LG
\\
  We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning
boost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted
in the insight that optimal representations within MIM models generally reside
in intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive
heads that are connected to diverse intermediate layers. In each head, a
modified nearest neighbor objective helps to construct respective semantic
clusters.
  The refinement process is short but effective. Within a few epochs, we refine
the features of MIM models from subpar to state-of-the-art, off-the-shelf
features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K,
achieves new state-of-the-art results in linear probing (84.7%) and low-shot
classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K
1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%,
outperforming larger models that were trained on up to 2000x more data such as
DINOv2-g, OpenCLIP-G and MAWS-6.5B. Project page:
https://ml-jku.github.io/MIM-Refiner
\\ ( https://arxiv.org/abs/2402.10093 ,  844kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10100 (*cross-listing*)
Date: Wed, 7 Feb 2024 16:41:11 GMT   (499kb,D)

Title: Tuning In: Analysis of Audio Classifier Performance in Clinical Settings
  with Limited Data
Authors: Hamza Mahdi, Eptehal Nashnoush, Rami Saab, Arjun Balachandar, Rishit
  Dagli, Lucas X. Perri, and Houman Khosravani
Categories: cs.SD cs.AI cs.LG eess.AS
\\
  This study assesses deep learning models for audio classification in a
clinical setting with the constraint of small datasets reflecting real-world
prospective data collection. We analyze CNNs, including DenseNet and ConvNeXt,
alongside transformer models like ViT, SWIN, and AST, and compare them against
pre-trained audio models such as YAMNet and VGGish. Our method highlights the
benefits of pre-training on large datasets before fine-tuning on specific
clinical data. We prospectively collected two first-of-their-kind patient audio
datasets from stroke patients. We investigated various preprocessing
techniques, finding that RGB and grayscale spectrogram transformations affect
model performance differently based on the priors they learn from pre-training.
Our findings indicate CNNs can match or exceed transformer models in small
dataset contexts, with DenseNet-Contrastive and AST models showing notable
performance. This study highlights the significance of incremental marginal
gains through model selection, pre-training, and preprocessing in sound
classification; this offers valuable insights for clinical diagnostics that
rely on audio classification.
\\ ( https://arxiv.org/abs/2402.10100 ,  499kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10168 (*cross-listing*)
Date: Thu, 15 Feb 2024 18:11:02 GMT   (557kb,D)

Title: DeepSRGM -- Sequence Classification and Ranking in Indian Classical
  Music with Deep Learning
Authors: Sathwik Tejaswi Madhusudhan and Girish Chowdhary
Categories: cs.SD cs.AI cs.IR cs.LG eess.AS
\\
  A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a
melodic framework for compositions and improvisations alike. Raga Recognition
is an important music information retrieval task in ICM as it can aid numerous
downstream applications ranging from music recommendations to organizing huge
music collections. In this work, we propose a deep learning based approach to
Raga recognition. Our approach employs efficient pre possessing and learns
temporal sequences in music data using Long Short Term Memory based Recurrent
Neural Networks (LSTM-RNN). We train and test the network on smaller sequences
sampled from the original audio while the final inference is performed on the
audio as a whole. Our method achieves an accuracy of 88.1% and 97 % during
inference on the Comp Music Carnatic dataset and its 10 Raga subset
respectively making it the state-of-the-art for the Raga recognition task. Our
approach also enables sequence ranking which aids us in retrieving melodic
patterns from a given music data base that are closely related to the presented
query sequence.
\\ ( https://arxiv.org/abs/2402.10168 ,  557kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10204 (*cross-listing*)
Date: Thu, 15 Feb 2024 18:57:24 GMT   (14026kb,D)

Title: Radio-astronomical Image Reconstruction with Conditional Denoising
  Diffusion Model
Authors: Mariia Drozdova, Vitaliy Kinakh, Omkar Bait, Olga Taran, Erica
  Lastufka, Miroslava Dessauges-Zavadsky, Taras Holotyak, Daniel Schaerer,
  Slava Voloshynovskiy
Categories: astro-ph.IM cs.AI cs.CV
Comments: In production in Astronomy&Astrophyics
DOI: 10.1051/0004-6361/202347948
\\
  Reconstructing sky models from dirty radio images for accurate source
localization and flux estimation is crucial for studying galaxy evolution at
high redshift, especially in deep fields using instruments like the Atacama
Large Millimetre Array (ALMA). With new projects like the Square Kilometre
Array (SKA), there's a growing need for better source extraction methods.
Current techniques, such as CLEAN and PyBDSF, often fail to detect faint
sources, highlighting the need for more accurate methods. This study proposes
using stochastic neural networks to rebuild sky models directly from dirty
images. This method can pinpoint radio sources and measure their fluxes with
related uncertainties, marking a potential improvement in radio source
characterization. We tested this approach on 10164 images simulated with the
CASA tool simalma, based on ALMA's Cycle 5.3 antenna setup. We applied
conditional Denoising Diffusion Probabilistic Models (DDPMs) for sky models
reconstruction, then used Photutils to determine source coordinates and fluxes,
assessing the model's performance across different water vapor levels. Our
method showed excellence in source localization, achieving more than 90%
completeness at a signal-to-noise ratio (SNR) as low as 2. It also surpassed
PyBDSF in flux estimation, accurately identifying fluxes for 96% of sources in
the test set, a significant improvement over CLEAN+ PyBDSF's 57%. Conditional
DDPMs is a powerful tool for image-to-image translation, yielding accurate and
robust characterisation of radio sources, and outperforming existing
methodologies. While this study underscores its significant potential for
applications in radio astronomy, we also acknowledge certain limitations that
accompany its usage, suggesting directions for further refinement and research.
\\ ( https://arxiv.org/abs/2402.10204 ,  14026kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09989 (*cross-listing*)
Date: Thu, 15 Feb 2024 14:54:33 GMT   (9777kb,D)

Title: LLMs as Bridges: Reformulating Grounded Multimodal Named Entity
  Recognition
Authors: Jinyuan Li, Han Li, Di Sun, Jiahao Wang, Wenkun Zhang, Zan Wang, Gang
  Pan
Categories: cs.CV cs.CL
\\
  Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal
task that aims to identify named entities, entity types and their corresponding
visual regions. GMNER task exhibits two challenging properties: 1) The weak
correlation between image-text pairs in social media results in a significant
portion of named entities being ungroundable. 2) There exists a distinction
between coarse-grained referring expressions commonly used in similar tasks
(e.g., phrase localization, referring expression comprehension) and
fine-grained named entities. In this paper, we propose RiVEG, a unified
framework that reformulates GMNER into a joint MNER-VE-VG task by leveraging
large language models (LLMs) as a connecting bridge. This reformulation brings
two benefits: 1) It maintains the optimal MNER performance and eliminates the
need for employing object detection methods to pre-extract regional features,
thereby naturally addressing two major limitations of existing GMNER methods.
2) The introduction of entity expansion expression and Visual Entailment (VE)
Module unifies Visual Grounding (VG) and Entity Grounding (EG). It enables
RiVEG to effortlessly inherit the Visual Entailment and Visual Grounding
capabilities of any current or prospective multimodal pretraining models.
Extensive experiments demonstrate that RiVEG outperforms state-of-the-art
methods on the existing GMNER dataset and achieves absolute leads of 10.65%,
6.21%, and 8.83% in all three subtasks.
\\ ( https://arxiv.org/abs/2402.09989 ,  9777kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09416 (*cross-listing*)
Date: Fri, 12 Jan 2024 18:38:14 GMT   (302kb,D)

Title: Deep Manifold Transformation for Protein Representation Learning
Authors: Bozhen Hu, Zelin Zang, Cheng Tan, Stan Z. Li
Categories: q-bio.BM cs.LG
Comments: This work has been accepted by ICASSP 2024
\\
  Protein representation learning is critical in various tasks in biology, such
as drug design and protein structure or function prediction, which has
primarily benefited from protein language models and graph neural networks.
These models can capture intrinsic patterns from protein sequences and
structures through masking and task-related losses. However, the learned
protein representations are usually not well optimized, leading to performance
degradation due to limited data, difficulty adapting to new tasks, etc. To
address this, we propose a new \underline{d}eep \underline{m}anifold
\underline{t}ransformation approach for universal \underline{p}rotein
\underline{r}epresentation \underline{l}earning (DMTPRL). It employs manifold
learning strategies to improve the quality and adaptability of the learned
embeddings. Specifically, we apply a novel manifold learning loss during
training based on the graph inter-node similarity. Our proposed DMTPRL method
outperforms state-of-the-art baselines on diverse downstream tasks across
popular datasets. This validates our approach for learning universal and robust
protein representations. We promise to release the code after acceptance.
\\ ( https://arxiv.org/abs/2402.09416 ,  302kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09419 (*cross-listing*)
Date: Fri, 19 Jan 2024 08:34:12 GMT   (228kb,D)

Title: Multidimensional Gabor-Like Filters Derived from Gaussian Functions on
  Logarithmic Frequency Axes
Authors: Dherik Devakumar, Ole Christian Eidheim
Categories: eess.SP cs.CV cs.LG cs.NE
\\
  A novel wavelet-like function is presented that makes it convenient to create
filter banks given mainly two parameters that influence the focus area and the
filter count. This is accomplished by computing the inverse Fourier transform
of Gaussian functions on logarithmic frequency axes in the frequency domain.
The resulting filters are similar to Gabor filters and represent oriented brief
signal oscillations of different sizes. The wavelet-like function can be
thought of as a generalized Log-Gabor filter that is multidimensional, always
uses Gaussian functions on logarithmic frequency axes, and innately includes
low-pass filters from Gaussian functions located at the frequency domain
origin.
\\ ( https://arxiv.org/abs/2402.09419 ,  228kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09421 (*cross-listing*)
Date: Fri, 19 Jan 2024 16:05:13 GMT   (2462kb,D)

Title: EEG Based Generative Depression Discriminator
Authors: Ziming Mao and Hao wu and Yongxi Tan and Yuhe Jin
Categories: eess.SP cs.LG
\\
  Depression is a very common but serious mood disorder.In this paper, We built
a generative detection network(GDN) in accordance with three physiological
laws. Our aim is that we expect the neural network to learn the relevant brain
activity based on the EEG signal and, at the same time, to regenerate the
target electrode signal based on the brain activity. We trained two generators,
the first one learns the characteristics of depressed brain activity, and the
second one learns the characteristics of control group's brain activity. In the
test, a segment of EEG signal was put into the two generators separately, if
the relationship between the EEG signal and brain activity conforms to the
characteristics of a certain category, then the signal generated by the
generator of the corresponding category is more consistent with the original
signal. Thus it is possible to determine the category corresponding to a
certain segment of EEG signal. We obtained an accuracy of 92.30\% on the MODMA
dataset and 86.73\% on the HUSM dataset. Moreover, this model is able to output
explainable information, which can be used to help the user to discover
possible misjudgments of the network.Our code will be released.
\\ ( https://arxiv.org/abs/2402.09421 ,  2462kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09424 (*cross-listing*)
Date: Sun, 21 Jan 2024 19:23:56 GMT   (5050kb,D)

Title: Epilepsy Seizure Detection and Prediction using an Approximate Spiking
  Convolutional Transformer
Authors: Qinyu Chen, Congyi Sun, Chang Gao, Shih-Chii Liu
Categories: eess.SP cs.CV cs.LG
Comments: To be published at the 2024 IEEE International Symposium on Circuits
  and Systems (ISCAS), Singapore
\\
  Epilepsy is a common disease of the nervous system. Timely prediction of
seizures and intervention treatment can significantly reduce the accidental
injury of patients and protect the life and health of patients. This paper
presents a neuromorphic Spiking Convolutional Transformer, named Spiking
Conformer, to detect and predict epileptic seizure segments from scalped
long-term electroencephalogram (EEG) recordings. We report evaluation results
from the Spiking Conformer model using the Boston Children's Hospital-MIT
(CHB-MIT) EEG dataset. By leveraging spike-based addition operations, the
Spiking Conformer significantly reduces the classification computational cost
compared to the non-spiking model. Additionally, we introduce an approximate
spiking neuron layer to further reduce spike-triggered neuron updates by nearly
38% without sacrificing accuracy. Using raw EEG data as input, the proposed
Spiking Conformer achieved an average sensitivity rate of 94.9% and a
specificity rate of 99.3% for the seizure detection task, and 96.8%, 89.5% for
the seizure prediction task, and needs >10x fewer operations compared to the
non-spiking equivalent model.
\\ ( https://arxiv.org/abs/2402.09424 ,  5050kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09426 (*cross-listing*)
Date: Tue, 23 Jan 2024 23:42:55 GMT   (4728kb,D)

Title: Graph Koopman Autoencoder for Predictive Covert Communication Against
  UAV Surveillance
Authors: Sivaram Krishnan, Jihong Park, Gregory Sherman, Benjamin Campbell,
  Jinho Choi
Categories: eess.SP cs.LG cs.SY eess.SY
\\
  Low Probability of Detection (LPD) communication aims to obscure the very
presence of radio frequency (RF) signals, going beyond just hiding the content
of the communication. However, the use of Unmanned Aerial Vehicles (UAVs)
introduces a challenge, as UAVs can detect RF signals from the ground by
hovering over specific areas of interest. With the growing utilization of UAVs
in modern surveillance, there is a crucial need for a thorough understanding of
their unknown nonlinear dynamic trajectories to effectively implement LPD
communication. Unfortunately, this critical information is often not readily
available, posing a significant hurdle in LPD communication. To address this
issue, we consider a case-study for enabling terrestrial LPD communication in
the presence of multiple UAVs that are engaged in surveillance. We introduce a
novel framework that combines graph neural networks (GNN) with Koopman theory
to predict the trajectories of multiple fixed-wing UAVs over an extended
prediction horizon. Using the predicted UAV locations, we enable LPD
communication in a terrestrial ad-hoc network by controlling nodes' transmit
powers to keep the received power at UAVs' predicted locations minimized. Our
extensive simulations validate the efficacy of the proposed framework in
accurately predicting the trajectories of multiple UAVs, thereby effectively
establishing LPD communication.
\\ ( https://arxiv.org/abs/2402.09426 ,  4728kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09434 (*cross-listing*)
Date: Fri, 26 Jan 2024 06:08:49 GMT   (5121kb,D)

Title: Disentangling Imperfect: A Wavelet-Infused Multilevel Heterogeneous
  Network for Human Activity Recognition in Flawed Wearable Sensor Data
Authors: Mengna Liu, Dong Xiang, Xu Cheng, Xiufeng Liu, Dalin Zhang, Shengyong
  Chen, Christian S. Jensen
Categories: eess.SP cs.LG
Comments: 14 pages, 7 figures
\\
  The popularity and diffusion of wearable devices provides new opportunities
for sensor-based human activity recognition that leverages deep learning-based
algorithms. Although impressive advances have been made, two major challenges
remain. First, sensor data is often incomplete or noisy due to sensor placement
and other issues as well as data transmission failure, calling for imputation
of missing values, which also introduces noise. Second, human activity has
multi-scale characteristics. Thus, different groups of people and even the same
person may behave differently under different circumstances. To address these
challenges, we propose a multilevel heterogeneous neural network, called MHNN,
for sensor data analysis. We utilize multilevel discrete wavelet decomposition
to extract multi-resolution features from sensor data. This enables
distinguishing signals with different frequencies, thereby suppressing noise.
As the components resulting from the decomposition are heterogeneous, we equip
the proposed model with heterogeneous feature extractors that enable the
learning of multi-scale features. Due to the complementarity of these features,
we also include a cross aggregation module for enhancing their interactions. An
experimental study using seven publicly available datasets offers evidence that
MHNN can outperform other cutting-edge models and offers evidence of robustness
to missing values and noise. An ablation study confirms the importance of each
module.
\\ ( https://arxiv.org/abs/2402.09434 ,  5121kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09438 (*cross-listing*)
Date: Sat, 27 Jan 2024 23:05:51 GMT   (766kb,D)

Title: Subject-Independent Deep Architecture for EEG-based Motor Imagery
  Classification
Authors: Shadi Sartipi and Mujdat Cetin
Categories: eess.SP cs.LG
\\
  Motor imagery (MI) classification based on electroencephalogram (EEG) is a
widely-used technique in non-invasive brain-computer interface (BCI) systems.
Since EEG recordings suffer from heterogeneity across subjects and labeled data
insufficiency, designing a classifier that performs the MI independently from
the subject with limited labeled samples would be desirable. To overcome these
limitations, we propose a novel subject-independent semi-supervised deep
architecture (SSDA). The proposed SSDA consists of two parts: an unsupervised
and a supervised element. The training set contains both labeled and unlabeled
data samples from multiple subjects. First, the unsupervised part, known as the
columnar spatiotemporal auto-encoder (CST-AE), extracts latent features from
all the training samples by maximizing the similarity between the original and
reconstructed data. A dimensional scaling approach is employed to reduce the
dimensionality of the representations while preserving their discriminability.
Second, a supervised part learns a classifier based on the labeled training
samples using the latent features acquired in the unsupervised part. Moreover,
we employ center loss in the supervised part to minimize the embedding space
distance of each point in a class to its center. The model optimizes both parts
of the network in an end-to-end fashion. The performance of the proposed SSDA
is evaluated on test subjects who were not seen by the model during the
training phase. To assess the performance, we use two benchmark EEG-based MI
task datasets. The results demonstrate that SSDA outperforms state-of-the-art
methods and that a small number of labeled training samples can be sufficient
for strong classification performance.
\\ ( https://arxiv.org/abs/2402.09438 ,  766kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09439 (*cross-listing*)
Date: Mon, 29 Jan 2024 14:14:39 GMT   (4961kb)

Title: Deep-Learning-Based Channel Estimation for IRS-Assisted ISAC System
Authors: Yu Liu, Ibrahim Al-Nahhal, Octavia A. Dobre, and Fanggang Wang
Categories: eess.SP cs.LG
\\
  Integrated sensing and communication (ISAC) and intelligent reflecting
surface (IRS) are viewed as promising technologies for future generations of
wireless networks. This paper investigates the channel estimation problem in an
IRS-assisted ISAC system. A deep-learning framework is proposed to estimate the
sensing and communication (S&C) channels in such a system. Considering
different propagation environments of the S&C channels, two deep neural network
(DNN) architectures are designed to realize this framework. The first DNN is
devised at the ISAC base station for estimating the sensing channel, while the
second DNN architecture is assigned to each downlink user equipment to estimate
its communication channel. Moreover, the input-output pairs to train the DNNs
are carefully designed. Simulation results show the superiority of the proposed
estimation approach compared to the benchmark scheme under various
signal-to-noise ratio conditions and system parameters.
\\ ( https://arxiv.org/abs/2402.09439 ,  4961kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09440 (*cross-listing*)
Date: Mon, 29 Jan 2024 14:15:11 GMT   (6409kb)

Title: Extreme Learning Machine-based Channel Estimation in IRS-Assisted
  Multi-User ISAC System
Authors: Yu Liu, Ibrahim Al-Nahhal, Octavia A. Dobre, Fanggang Wang, and
  Hyundong Shin
Categories: eess.SP cs.LG
\\
  Multi-user integrated sensing and communication (ISAC) assisted by
intelligent reflecting surface (IRS) has been recently investigated to provide
a high spectral and energy efficiency transmission. This paper proposes a
practical channel estimation approach for the first time to an IRS-assisted
multiuser ISAC system. The estimation problem in such a system is challenging
since the sensing and communication (SAC) signals interfere with each other,
and the passive IRS lacks signal processing ability. A two-stage approach is
proposed to transfer the overall estimation problem into sub-ones, successively
including the direct and reflected channels estimation. Based on this scheme,
the ISAC base station (BS) estimates all the SAC channels associated with the
target and uplink users, while each downlink user estimates the downlink
communication channels individually. Considering a low-cost demand of the ISAC
BS and downlink users, the proposed two-stage approach is realized by an
efficient neural network (NN) framework that contains two different extreme
learning machine (ELM) structures to estimate the above SAC channels. Moreover,
two types of input-output pairs to train the ELMs are carefully devised, which
impact the estimation accuracy and computational complexity under different
system parameters. Simulation results reveal a substantial performance
improvement achieved by the proposed ELM-based approach over the least-squares
and NN-based benchmarks, with reduced training complexity and faster training
speed.
\\ ( https://arxiv.org/abs/2402.09440 ,  6409kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09441 (*cross-listing*)
Date: Mon, 29 Jan 2024 14:15:48 GMT   (6270kb)

Title: Deep-Learning Channel Estimation for IRS-Assisted Integrated Sensing and
  Communication System
Authors: Yu Liu, Ibrahim Al-Nahhal, Octavia A. Dobre, and Fanggang Wang
Categories: eess.SP cs.LG
\\
  Integrated sensing and communication (ISAC), and intelligent reflecting
surface (IRS) are envisioned as revolutionary technologies to enhance spectral
and energy efficiencies for next wireless system generations. For the first
time, this paper focuses on the channel estimation problem in an IRS-assisted
ISAC system. This problem is challenging due to the lack of signal processing
capacity in passive IRS, as well as the presence of mutual interference between
sensing and communication (SAC) signals in ISAC systems. A three-stage approach
is proposed to decouple the estimation problem into sub-ones, including the
estimation of the direct SAC channels in the first stage, reflected
communication channel in the second stage, and reflected sensing channel in the
third stage. The proposed three-stage approach is based on a deep-learning
framework, which involves two different convolutional neural network (CNN)
architectures to estimate the channels at the full-duplex ISAC base station.
Furthermore, two types of input-output pairs to train the CNNs are carefully
designed, which affect the estimation performance under various signal-to-noise
ratio conditions and system parameters. Simulation results validate the
superiority of the proposed estimation approach compared to the least-squares
baseline scheme, and its computational complexity is also analyzed.
\\ ( https://arxiv.org/abs/2402.09441 ,  6270kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09452 (*cross-listing*)
Date: Sat, 3 Feb 2024 08:58:53 GMT   (892kb,D)

Title: Data Distribution Dynamics in Real-World WiFi-Based Patient Activity
  Monitoring for Home Healthcare
Authors: Mahathir Monjur, Jia Liu, Jingye Xu, Yuntong Zhang, Xiaomeng Wang,
  Chengdong Li, Hyejin Park, Wei Wang, Karl Shieh, Sirajum Munir, Jing Wang,
  Lixin Song, Shahriar Nirjon
Categories: eess.SP cs.LG cs.SY eess.SY
\\
  This paper examines the application of WiFi signals for real-world monitoring
of daily activities in home healthcare scenarios. While the state-of-the-art of
WiFi-based activity recognition is promising in lab environments, challenges
arise in real-world settings due to environmental, subject, and system
configuration variables, affecting accuracy and adaptability. The research
involved deploying systems in various settings and analyzing data shifts. It
aims to guide realistic development of robust, context-aware WiFi sensing
systems for elderly care. The findings suggest a shift in WiFi-based activity
sensing, bridging the gap between academic research and practical applications,
enhancing life quality through technology.
\\ ( https://arxiv.org/abs/2402.09452 ,  892kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09460 (*cross-listing*)
Date: Thu, 8 Feb 2024 06:14:12 GMT   (4870kb,D)

Title: Unsupervised learning based end-to-end delayless generative fixed-filter
  active noise control
Authors: Zhengding Luo, Dongyuan Shi, Xiaoyi Shen, Woon-Seng Gan
Categories: eess.SP cs.LG
Comments: 2024 IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP 2024)
\\
  Delayless noise control is achieved by our earlier generative fixed-filter
active noise control (GFANC) framework through efficient coordination between
the co-processor and real-time controller. However, the one-dimensional
convolutional neural network (1D CNN) in the co-processor requires initial
training using labelled noise datasets. Labelling noise data can be
resource-intensive and may introduce some biases. In this paper, we propose an
unsupervised-GFANC approach to simplify the 1D CNN training process and enhance
its practicality. During training, the co-processor and real-time controller
are integrated into an end-to-end differentiable ANC system. This enables us to
use the accumulated squared error signal as the loss for training the 1D CNN.
With this unsupervised learning paradigm, the unsupervised-GFANC method not
only omits the labelling process but also exhibits better noise reduction
performance compared to the supervised GFANC method in real noise experiments.
\\ ( https://arxiv.org/abs/2402.09460 ,  4870kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09461 (*cross-listing*)
Date: Thu, 8 Feb 2024 06:36:29 GMT   (1642kb,D)

Title: A Novel Approach to WaveNet Architecture for RF Signal Separation with
  Learnable Dilation and Data Augmentation
Authors: Yu Tian, Ahmed Alhammadi, Abdullah Quran, Abubakar Sani Ali
Categories: eess.SP cs.LG
\\
  In this paper, we address the intricate issue of RF signal separation by
presenting a novel adaptation of the WaveNet architecture that introduces
learnable dilation parameters, significantly enhancing signal separation in
dense RF spectrums. Our focused architectural refinements and innovative data
augmentation strategies have markedly improved the model's ability to discern
complex signal sources. This paper details our comprehensive methodology,
including the refined model architecture, data preparation techniques, and the
strategic training strategy that have been pivotal to our success. The efficacy
of our approach is evidenced by the substantial improvements recorded: a
58.82\% increase in SINR at a BER of $10^{-3}$ for OFDM-QPSK with EMI Signal 1,
surpassing traditional benchmarks. Notably, our model achieved first place in
the challenge \cite{datadrivenrf2024}, demonstrating its superior performance
and establishing a new standard for machine learning applications within the RF
communications domain.
\\ ( https://arxiv.org/abs/2402.09461 ,  1642kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09464 (*cross-listing*)
Date: Thu, 8 Feb 2024 19:55:07 GMT   (2898kb)

Title: Different Algorithms (Might) Uncover Different Patterns: A Brain-Age
  Prediction Case Study
Authors: Tobias Ettling, Sari Saba-Sadiya, Gemma Roig
Categories: eess.SP cs.LG q-bio.QM
Journal-ref: 2023 IEEE International Conference on Bioinformatics and
  Biomedicine (BIBM), pp. 4051-4058
DOI: 10.1109/BIBM58861.2023.10385662.
\\
  Machine learning is a rapidly evolving field with a wide range of
applications, including biological signal analysis, where novel algorithms
often improve the state-of-the-art. However, robustness to algorithmic
variability - measured by different algorithms, consistently uncovering similar
findings - is seldom explored. In this paper we investigate whether established
hypotheses in brain-age prediction from EEG research validate across
algorithms. First, we surveyed literature and identified various features known
to be informative for brain-age prediction. We employed diverse feature
extraction techniques, processing steps, and models, and utilized the
interpretative power of SHapley Additive exPlanations (SHAP) values to align
our findings with the existing research in the field. Few of our models
achieved state-of-the-art performance on the specific data-set we utilized.
Moreover, analysis demonstrated that while most models do uncover similar
patterns in the EEG signals, some variability could still be observed. Finally,
a few prominent findings could only be validated using specific models. We
conclude by suggesting remedies to the potential implications of this lack of
robustness to model variability.
\\ ( https://arxiv.org/abs/2402.09464 ,  2898kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09467 (*cross-listing*)
Date: Sun, 11 Feb 2024 21:25:14 GMT   (127kb,D)

Title: Optimal Thresholding Linear Bandit
Authors: Eduardo Ochoa Rivera and Ambuj Tewari
Categories: stat.ML cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2006.16073 by
  other authors
\\
  We study a novel pure exploration problem: the $\epsilon$-Thresholding Bandit
Problem (TBP) with fixed confidence in stochastic linear bandits. We prove a
lower bound for the sample complexity and extend an algorithm designed for Best
Arm Identification in the linear case to TBP that is asymptotically optimal.
\\ ( https://arxiv.org/abs/2402.09467 ,  127kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09477 (*cross-listing*)
Date: Mon, 12 Feb 2024 22:56:07 GMT   (7712kb,D)

Title: PANORAMIA: Privacy Auditing of Machine Learning Models without
  Retraining
Authors: Mishaal Kazmi, Hadrien Lautraite, Alireza Akbari, Mauricio Soroco,
  Qiaoyue Tang, Tao Wang, S\'ebastien Gambs, Mathias L\'ecuyer
Categories: cs.CR cs.LG
Comments: 19 pages
\\
  We introduce a privacy auditing scheme for ML models that relies on
membership inference attacks using generated data as "non-members". This
scheme, which we call PANORAMIA, quantifies the privacy leakage for large-scale
ML models without control of the training process or model re-training and only
requires access to a subset of the training data. To demonstrate its
applicability, we evaluate our auditing scheme across multiple ML domains,
ranging from image and tabular data classification to large-scale language
models.
\\ ( https://arxiv.org/abs/2402.09477 ,  7712kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09478 (*cross-listing*)
Date: Tue, 13 Feb 2024 05:06:34 GMT   (8198kb,D)

Title: Data Reconstruction Attacks and Defenses: A Systematic Evaluation
Authors: Sheng Liu, Zihan Wang, Qi Lei
Categories: cs.CR cs.LG
\\
  Reconstruction attacks and defenses are essential in understanding the data
leakage problem in machine learning. However, prior work has centered around
empirical observations of gradient inversion attacks, lacks theoretical
groundings, and was unable to disentangle the usefulness of defending methods
versus the computational limitation of attacking methods. In this work, we
propose a strong reconstruction attack in the setting of federated learning.
The attack reconstructs intermediate features and nicely integrates with and
outperforms most of the previous methods. On this stronger attack, we
thoroughly investigate both theoretically and empirically the effect of the
most common defense methods. Our findings suggest that among various defense
mechanisms, such as gradient clipping, dropout, additive noise, local
aggregation, etc., gradient pruning emerges as the most effective strategy to
defend against state-of-the-art attacks.
\\ ( https://arxiv.org/abs/2402.09478 ,  8198kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09483 (*cross-listing*)
Date: Tue, 13 Feb 2024 23:40:50 GMT   (64kb)

Title: Oracle-Efficient Differentially Private Learning with Public Data
Authors: Adam Block, Mark Bun, Rathin Desai, Abhishek Shetty, and Steven Wu
Categories: stat.ML cs.CR cs.LG
\\
  Due to statistical lower bounds on the learnability of many function classes
under privacy constraints, there has been recent interest in leveraging public
data to improve the performance of private learning algorithms. In this model,
algorithms must always guarantee differential privacy with respect to the
private samples while also ensuring learning guarantees when the private data
distribution is sufficiently close to that of the public data. Previous work
has demonstrated that when sufficient public, unlabelled data is available,
private learning can be made statistically tractable, but the resulting
algorithms have all been computationally inefficient. In this work, we present
the first computationally efficient, algorithms to provably leverage public
data to learn privately whenever a function class is learnable non-privately,
where our notion of computational efficiency is with respect to the number of
calls to an optimization oracle for the function class. In addition to this
general result, we provide specialized algorithms with improved sample
complexities in the special cases when the function class is convex or when the
task is binary classification.
\\ ( https://arxiv.org/abs/2402.09483 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09488 (*cross-listing*)
Date: Wed, 14 Feb 2024 09:07:00 GMT   (878kb,D)

Title: Intelligent Agricultural Greenhouse Control System Based on Internet of
  Things and Machine Learning
Authors: Cangqing Wang
Categories: eess.SY cs.LG cs.SY
\\
  This study endeavors to conceptualize and execute a sophisticated
agricultural greenhouse control system grounded in the amalgamation of the
Internet of Things (IoT) and machine learning. Through meticulous monitoring of
intrinsic environmental parameters within the greenhouse and the integration of
machine learning algorithms, the conditions within the greenhouse are aptly
modulated. The envisaged outcome is an enhancement in crop growth efficiency
and yield, accompanied by a reduction in resource wastage. In the backdrop of
escalating global population figures and the escalating exigencies of climate
change, agriculture confronts unprecedented challenges. Conventional
agricultural paradigms have proven inadequate in addressing the imperatives of
food safety and production efficiency. Against this backdrop, greenhouse
agriculture emerges as a viable solution, proffering a controlled milieu for
crop cultivation to augment yields, refine quality, and diminish reliance on
natural resources [b1]. Nevertheless, greenhouse agriculture contends with a
gamut of challenges. Traditional greenhouse management strategies, often
grounded in experiential knowledge and predefined rules, lack targeted
personalized regulation, thereby resulting in resource inefficiencies. The
exigencies of real-time monitoring and precise control of the greenhouse's
internal environment gain paramount importance with the burgeoning scale of
agriculture. To redress this challenge, the study introduces IoT technology and
machine learning algorithms into greenhouse agriculture, aspiring to institute
an intelligent agricultural greenhouse control system conducive to augmenting
the efficiency and sustainability of agricultural production.
\\ ( https://arxiv.org/abs/2402.09488 ,  878kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09524 (*cross-listing*)
Date: Wed, 14 Feb 2024 19:01:51 GMT   (1397kb,D)

Title: Guided Quantum Compression for Higgs Identification
Authors: Vasilis Belis, Patrick Odagiu, Michele Grossi, Florentin Reiter,
  G\"unther Dissertori, Sofia Vallecorsa
Categories: quant-ph cs.LG hep-ex
Comments: 9 pages, 3 figures
\\
  Quantum machine learning provides a fundamentally novel and promising
approach to analyzing data. However, many data sets are too complex for
currently available quantum computers. Consequently, quantum machine learning
applications conventionally resort to dimensionality reduction algorithms,
e.g., auto-encoders, before passing data through the quantum models. We show
that using a classical auto-encoder as an independent preprocessing step can
significantly decrease the classification performance of a quantum machine
learning algorithm. To ameliorate this issue, we design an architecture that
unifies the preprocessing and quantum classification algorithms into a single
trainable model: the guided quantum compression model. The utility of this
model is demonstrated by using it to identify the Higgs boson in proton-proton
collisions at the LHC, where the conventional approach proves ineffective.
Conversely, the guided quantum compression model excels at solving this
classification problem, achieving a good accuracy. Additionally, the model
developed herein shows better performance compared to the classical benchmark
when using only low-level kinematic features.
\\ ( https://arxiv.org/abs/2402.09524 ,  1397kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09589 (*cross-listing*)
Date: Wed, 14 Feb 2024 21:33:18 GMT   (2971kb,D)

Title: MLTCP: Congestion Control for DNN Training
Authors: Sudarsanan Rajasekaran, Sanjoli Narang, Anton A. Zabreyko, Manya
  Ghobadi
Categories: cs.NI cs.DC cs.LG
\\
  We present MLTCP, a technique to augment today's congestion control
algorithms to accelerate DNN training jobs in shared GPU clusters. MLTCP
enables the communication phases of jobs that compete for network bandwidth to
interleave with each other, thereby utilizing the network efficiently. At the
heart of MLTCP lies a very simple principle based on a key conceptual insight:
DNN training flows should scale their congestion window size based on the
number of bytes sent at each training iteration. We show that integrating this
principle into today's congestion control protocols is straightforward: by
adding 30-60 lines of code to Reno, CUBIC, or DCQCN, MLTCP stabilizes flows of
different jobs into an interleaved state within a few training iterations,
regardless of the number of competing flows or the start time of each flow. Our
experiments with popular DNN training jobs demonstrate that enabling MLTCP
accelerates the average and 99th percentile training iteration time by up to 2x
and 4x, respectively.
\\ ( https://arxiv.org/abs/2402.09589 ,  2971kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09598 (*cross-listing*)
Date: Wed, 14 Feb 2024 22:10:42 GMT   (8338kb,D)

Title: MCMC-driven learning
Authors: Alexandre Bouchard-C\^ot\'e, Trevor Campbell, Geoff Pleiss, Nikola
  Surjanovic
Categories: stat.ML cs.LG math.ST stat.CO stat.TH
\\
  This paper is intended to appear as a chapter for the Handbook of Markov
Chain Monte Carlo. The goal of this chapter is to unify various problems at the
intersection of Markov chain Monte Carlo (MCMC) and machine
learning$\unicode{x2014}$which includes black-box variational inference,
adaptive MCMC, normalizing flow construction and transport-assisted MCMC,
surrogate-likelihood MCMC, coreset construction for MCMC with big data, Markov
chain gradient descent, Markovian score climbing, and
more$\unicode{x2014}$within one common framework. By doing so, the theory and
methods developed for each may be translated and generalized.
\\ ( https://arxiv.org/abs/2402.09598 ,  8338kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09623 (*cross-listing*)
Date: Wed, 14 Feb 2024 23:57:19 GMT   (251kb,D)

Title: Conformalized Adaptive Forecasting of Heterogeneous Trajectories
Authors: Yanfei Zhou, Lars Lindemann, Matteo Sesia
Categories: stat.ML cs.LG
\\
  This paper presents a new conformal method for generating simultaneous
forecasting bands guaranteed to cover the entire path of a new random
trajectory with sufficiently high probability. Prompted by the need for
dependable uncertainty estimates in motion planning applications where the
behavior of diverse objects may be more or less unpredictable, we blend
different techniques from online conformal prediction of single and multiple
time series, as well as ideas for addressing heteroscedasticity in regression.
This solution is both principled, providing precise finite-sample guarantees,
and effective, often leading to more informative predictions than prior
methods.
\\ ( https://arxiv.org/abs/2402.09623 ,  251kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09650 (*cross-listing*)
Date: Thu, 15 Feb 2024 01:25:19 GMT   (10371kb,D)

Title: Foul prediction with estimated poses from soccer broadcast video
Authors: Jiale Fang, Calvin Yeung, Keisuke Fujii
Categories: cs.CV cs.LG
\\
  Recent advances in computer vision have made significant progress in tracking
and pose estimation of sports players. However, there have been fewer studies
on behavior prediction with pose estimation in sports, in particular, the
prediction of soccer fouls is challenging because of the smaller image size of
each player and of difficulty in the usage of e.g., the ball and pose
information. In our research, we introduce an innovative deep learning approach
for anticipating soccer fouls. This method integrates video data, bounding box
positions, image details, and pose information by curating a novel soccer foul
dataset. Our model utilizes a combination of convolutional and recurrent neural
networks (CNNs and RNNs) to effectively merge information from these four
modalities. The experimental results show that our full model outperformed the
ablated models, and all of the RNN modules, bounding box position and image,
and estimated pose were useful for the foul prediction. Our findings have
important implications for a deeper understanding of foul play in soccer and
provide a valuable reference for future research and practice in this area.
\\ ( https://arxiv.org/abs/2402.09650 ,  10371kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09651 (*cross-listing*)
Date: Thu, 15 Feb 2024 01:28:18 GMT   (5521kb,D)

Title: Practitioners' Challenges and Perceptions of CI Build Failure
  Predictions at Atlassian
Authors: Yang Hong, Chakkrit Tantithamthavorn, Jirat Pasuksmit, Patanamon
  Thongtanunam, Arik Friedman, Xing Zhao, Anton Krasikov
Categories: cs.SE cs.LG
\\
  Continuous Integration (CI) build failures could significantly impact the
software development process and teams, such as delaying the release of new
features and reducing developers' productivity. In this work, we report on an
empirical study that investigates CI build failures throughout product
development at Atlassian. Our quantitative analysis found that the repository
dimension is the key factor influencing CI build failures. In addition, our
qualitative survey revealed that Atlassian developers perceive CI build
failures as challenging issues in practice. Furthermore, we found that the CI
build prediction can not only provide proactive insight into CI build failures
but also facilitate the team's decision-making. Our study sheds light on the
challenges and expectations involved in integrating CI build prediction tools
into the Bitbucket environment, providing valuable insights for enhancing CI
processes.
\\ ( https://arxiv.org/abs/2402.09651 ,  5521kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09657 (*cross-listing*)
Date: Thu, 15 Feb 2024 01:50:46 GMT   (125kb)

Title: Digital versus Analog Transmissions for Federated Learning over Wireless
  Networks
Authors: Jiacheng Yao, Wei Xu, Zhaohui Yang, Xiaohu You, Mehdi Bennis, H.
  Vincent Poor
Categories: cs.IT cs.LG math.IT
Comments: Accepted by ICC 2024
\\
  In this paper, we quantitatively compare these two effective communication
schemes, i.e., digital and analog ones, for wireless federated learning (FL)
over resource-constrained networks, highlighting their essential differences as
well as their respective application scenarios. We first examine both digital
and analog transmission methods, together with a unified and fair comparison
scheme under practical constraints. A universal convergence analysis under
various imperfections is established for FL performance evaluation in wireless
networks. These analytical results reveal that the fundamental difference
between the two paradigms lies in whether communication and computation are
jointly designed or not. The digital schemes decouple the communication design
from specific FL tasks, making it difficult to support simultaneous uplink
transmission of massive devices with limited bandwidth. In contrast, the analog
communication allows over-the-air computation (AirComp), thus achieving
efficient spectrum utilization. However, computation-oriented analog
transmission reduces power efficiency, and its performance is sensitive to
computational errors. Finally, numerical simulations are conducted to verify
these theoretical observations.
\\ ( https://arxiv.org/abs/2402.09657 ,  125kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09671 (*cross-listing*)
Date: Thu, 15 Feb 2024 02:38:23 GMT   (2930kb)

Title: Exploiting Alpha Transparency In Language And Vision-Based AI Systems
Authors: David Noever and Forrest McKee
Categories: cs.CV cs.LG
\\
  This investigation reveals a novel exploit derived from PNG image file
formats, specifically their alpha transparency layer, and its potential to fool
multiple AI vision systems. Our method uses this alpha layer as a clandestine
channel invisible to human observers but fully actionable by AI image
processors. The scope tested for the vulnerability spans representative vision
systems from Apple, Microsoft, Google, Salesforce, Nvidia, and Facebook,
highlighting the attack's potential breadth. This vulnerability challenges the
security protocols of existing and fielded vision systems, from medical imaging
to autonomous driving technologies. Our experiments demonstrate that the
affected systems, which rely on convolutional neural networks or the latest
multimodal language models, cannot quickly mitigate these vulnerabilities
through simple patches or updates. Instead, they require retraining and
architectural changes, indicating a persistent hole in multimodal technologies
without some future adversarial hardening against such vision-language
exploits.
\\ ( https://arxiv.org/abs/2402.09671 ,  2930kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09687 (*cross-listing*)
Date: Thu, 15 Feb 2024 03:45:44 GMT   (13166kb,D)

Title: Robust Learning-Augmented Dictionaries
Authors: Ali Zeynali, Shahin Kamali, Mohammad Hajiesmaili
Categories: cs.DS cs.LG
Comments: 11 pages plus 4 pages appendix
\\
  We present the first learning-augmented data structure for implementing
dictionaries with optimal consistency and robustness. Our data structure, named
RobustSL, is a skip list augmented by predictions of access frequencies of
elements in a data sequence. With proper predictions, RobustSL has optimal
consistency (achieves static optimality). At the same time, it maintains a
logarithmic running time for each operation, ensuring optimal robustness, even
if predictions are generated adversarially. Therefore, RobustSL has all the
advantages of the recent learning-augmented data structures of Lin, Luo, and
Woodruff (ICML 2022) and Cao et al. (arXiv 2023), while providing robustness
guarantees that are absent in the previous work. Numerical experiments show
that RobustSL outperforms alternative data structures using both synthetic and
real datasets.
\\ ( https://arxiv.org/abs/2402.09687 ,  13166kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09698 (*cross-listing*)
Date: Thu, 15 Feb 2024 04:16:59 GMT   (631kb,D)

Title: Combining Evidence Across Filtrations
Authors: Yo Joong Choe and Aaditya Ramdas
Categories: stat.ME cs.LG math.PR math.ST stat.ML stat.TH
Comments: 29 pages, 4 figures
\\
  In anytime-valid sequential inference, it is known that any admissible
inference procedure must be based on test martingales and their composite
generalization, called e-processes, which are nonnegative processes whose
expectation at any arbitrary stopping time is upper-bounded by one. An
e-process quantifies the accumulated evidence against a composite null
hypothesis over a sequence of outcomes. This paper studies methods for
combining e-processes that are computed using different information sets, i.e.,
filtrations, for a null hypothesis. Even though e-processes constructed on the
same filtration can be combined effortlessly (e.g., by averaging), e-processes
constructed on different filtrations cannot be combined as easily because their
validity in a coarser filtration does not translate to validity in a finer
filtration. We discuss three concrete examples of such e-processes in the
literature: exchangeability tests, independence tests, and tests for evaluating
and comparing forecasts with lags. Our main result establishes that these
e-processes can be lifted into any finer filtration using adjusters, which are
functions that allow betting on the running maximum of the accumulated wealth
(thereby insuring against the loss of evidence). We also develop randomized
adjusters that can improve the power of the resulting sequential inference
procedure.
\\ ( https://arxiv.org/abs/2402.09698 ,  631kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09710 (*cross-listing*)
Date: Thu, 15 Feb 2024 05:06:53 GMT   (631kb)

Title: Preserving Data Privacy for ML-driven Applications in Open Radio Access
  Networks
Authors: Pranshav Gajjar, Azuka Chiejina, Vijay K. Shah
Categories: cs.CR cs.LG
\\
  Deep learning offers a promising solution to improve spectrum access
techniques by utilizing data-driven approaches to manage and share limited
spectrum resources for emerging applications. For several of these
applications, the sensitive wireless data (such as spectrograms) are stored in
a shared database or multistakeholder cloud environment and are therefore prone
to privacy leaks. This paper aims to address such privacy concerns by examining
the representative case study of shared database scenarios in 5G Open Radio
Access Network (O-RAN) networks where we have a shared database within the
near-real-time (near-RT) RAN intelligent controller. We focus on securing the
data that can be used by machine learning (ML) models for spectrum sharing and
interference mitigation applications without compromising the model and network
performances. The underlying idea is to leverage a (i) Shuffling-based
learnable encryption technique to encrypt the data, following which, (ii)
employ a custom Vision transformer (ViT) as the trained ML model that is
capable of performing accurate inferences on such encrypted data. The paper
offers a thorough analysis and comparisons with analogous convolutional neural
networks (CNN) as well as deeper architectures (such as ResNet-50) as
baselines. Our experiments showcase that the proposed approach significantly
outperforms the baseline CNN with an improvement of 24.5% and 23.9% for the
percent accuracy and F1-Score respectively when operated on encrypted data.
Though deeper ResNet-50 architecture is obtained as a slightly more accurate
model, with an increase of 4.4%, the proposed approach boasts a reduction of
parameters by 99.32%, and thus, offers a much-improved prediction time by
nearly 60%.
\\ ( https://arxiv.org/abs/2402.09710 ,  631kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09715 (*cross-listing*)
Date: Thu, 15 Feb 2024 05:19:53 GMT   (1596kb,D)

Title: DPBalance: Efficient and Fair Privacy Budget Scheduling for Federated
  Learning as a Service
Authors: Yu Liu, Zibo Wang, Yifei Zhu, Chen Chen
Categories: cs.DC cs.CR cs.LG
Comments: Accepted by IEEE International Conference on Computer Communications
  (INFOCOM '24)
\\
  Federated learning (FL) has emerged as a prevalent distributed machine
learning scheme that enables collaborative model training without aggregating
raw data. Cloud service providers further embrace Federated Learning as a
Service (FLaaS), allowing data analysts to execute their FL training pipelines
over differentially-protected data. Due to the intrinsic properties of
differential privacy, the enforced privacy level on data blocks can be viewed
as a privacy budget that requires careful scheduling to cater to diverse
training pipelines. Existing privacy budget scheduling studies prioritize
either efficiency or fairness individually. In this paper, we propose
DPBalance, a novel privacy budget scheduling mechanism that jointly optimizes
both efficiency and fairness. We first develop a comprehensive utility function
incorporating data analyst-level dominant shares and FL-specific performance
metrics. A sequential allocation mechanism is then designed using the Lagrange
multiplier method and effective greedy heuristics. We theoretically prove that
DPBalance satisfies Pareto Efficiency, Sharing Incentive, Envy-Freeness, and
Weak Strategy Proofness. We also theoretically prove the existence of a
fairness-efficiency tradeoff in privacy budgeting. Extensive experiments
demonstrate that DPBalance outperforms state-of-the-art solutions, achieving an
average efficiency improvement of $1.44\times \sim 3.49 \times$, and an average
fairness improvement of $1.37\times \sim 24.32 \times$.
\\ ( https://arxiv.org/abs/2402.09715 ,  1596kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09747 (*cross-listing*)
Date: Thu, 15 Feb 2024 06:58:25 GMT   (1491kb,D)

Title: Less is more: Ensemble Learning for Retinal Disease Recognition Under
  Limited Resources
Authors: Jiahao Wang, Hong Peng, Shengchao Chen, Sufen Ren
Categories: eess.IV cs.CV cs.LG
Comments: Ongoing work
\\
  Retinal optical coherence tomography (OCT) images provide crucial insights
into the health of the posterior ocular segment. Therefore, the advancement of
automated image analysis methods is imperative to equip clinicians and
researchers with quantitative data, thereby facilitating informed
decision-making. The application of deep learning (DL)-based approaches has
gained extensive traction for executing these analysis tasks, demonstrating
remarkable performance compared to labor-intensive manual analyses. However,
the acquisition of Retinal OCT images often presents challenges stemming from
privacy concerns and the resource-intensive labeling procedures, which
contradicts the prevailing notion that DL models necessitate substantial data
volumes for achieving superior performance. Moreover, limitations in available
computational resources constrain the progress of high-performance medical
artificial intelligence, particularly in less developed regions and countries.
This paper introduces a novel ensemble learning mechanism designed for
recognizing retinal diseases under limited resources (e.g., data, computation).
The mechanism leverages insights from multiple pre-trained models, facilitating
the transfer and adaptation of their knowledge to Retinal OCT images. This
approach establishes a robust model even when confronted with limited labeled
data, eliminating the need for an extensive array of parameters, as required in
learning from scratch. Comprehensive experimentation on real-world datasets
demonstrates that the proposed approach can achieve superior performance in
recognizing Retinal OCT images, even when dealing with exceedingly restricted
labeled datasets. Furthermore, this method obviates the necessity of learning
extensive-scale parameters, making it well-suited for deployment in
low-resource scenarios.
\\ ( https://arxiv.org/abs/2402.09747 ,  1491kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09754 (*cross-listing*)
Date: Thu, 15 Feb 2024 07:08:11 GMT   (706kb,D)

Title: Robust SVD Made Easy: A fast and reliable algorithm for large-scale data
  analysis
Authors: Sangil Han, Kyoowon Kim, Sungkyu Jung
Categories: stat.ML cs.LG math.ST stat.TH
\\
  The singular value decomposition (SVD) is a crucial tool in machine learning
and statistical data analysis. However, it is highly susceptible to outliers in
the data matrix. Existing robust SVD algorithms often sacrifice speed for
robustness or fail in the presence of only a few outliers. This study
introduces an efficient algorithm, called Spherically Normalized SVD, for
robust SVD approximation that is highly insensitive to outliers,
computationally scalable, and provides accurate approximations of singular
vectors. The proposed algorithm achieves remarkable speed by utilizing only two
applications of a standard reduced-rank SVD algorithm to appropriately scaled
data, significantly outperforming competing algorithms in computation times. To
assess the robustness of the approximated singular vectors and their subspaces
against data contamination, we introduce new notions of breakdown points for
matrix-valued input, including row-wise, column-wise, and block-wise breakdown
points. Theoretical and empirical analyses demonstrate that our algorithm
exhibits higher breakdown points compared to standard SVD and its
modifications. We empirically validate the effectiveness of our approach in
applications such as robust low-rank approximation and robust principal
component analysis of high-dimensional microarray datasets. Overall, our study
presents a highly efficient and robust solution for SVD approximation that
overcomes the limitations of existing algorithms in the presence of outliers.
\\ ( https://arxiv.org/abs/2402.09754 ,  706kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09761 (*cross-listing*)
Date: Thu, 15 Feb 2024 07:23:34 GMT   (856kb)

Title: A Framework For Gait-Based User Demography Estimation Using Inertial
  Sensors
Authors: Chinmay Prakash Swami
Categories: cs.HC cs.LG eess.SP
\\
  Human gait has been shown to provide crucial motion cues for various
applications. Recognizing patterns in human gait has been widely adopted in
various application areas such as security, virtual reality gaming, medical
rehabilitation, and ailment identification. Furthermore, wearable inertial
sensors have been widely used for not only recording gait but also to predict
users' demography. Machine Learning techniques such as deep learning, combined
with inertial sensor signals, have shown promising results in recognizing
patterns in human gait and estimate users' demography. However, the black-box
nature of such deep learning models hinders the researchers from uncovering the
reasons behind the model's predictions. Therefore, we propose leveraging deep
learning and Layer-Wise Relevance Propagation (LRP) to identify the important
variables that play a vital role in identifying the users' demography such as
age and gender. To assess the efficacy of this approach we train a deep neural
network model on a large sensor-based gait dataset consisting of 745 subjects
to identify users' age and gender. Using LRP we identify the variables relevant
for characterizing the gait patterns. Thus, we enable interpretation of
non-linear ML models which are experts in identifying the users' demography
based on inertial signals. We believe this approach can not only provide
clinicians information about the gait parameters relevant to age and gender but
also can be expanded to analyze and diagnose gait disorders.
\\ ( https://arxiv.org/abs/2402.09761 ,  856kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09796 (*cross-listing*)
Date: Thu, 15 Feb 2024 08:51:49 GMT   (33kb)

Title: Closed-form Filtering for Non-linear Systems
Authors: Th\'eophile Cantelobre, Carlo Ciliberto, Benjamin Guedj, Alessandro
  Rudi
Categories: stat.ML cs.LG cs.RO
Comments: 38 pages
\\
  Sequential Bayesian Filtering aims to estimate the current state distribution
of a Hidden Markov Model, given the past observations. The problem is
well-known to be intractable for most application domains, except in notable
cases such as the tabular setting or for linear dynamical systems with gaussian
noise. In this work, we propose a new class of filters based on Gaussian PSD
Models, which offer several advantages in terms of density approximation and
computational efficiency. We show that filtering can be efficiently performed
in closed form when transitions and observations are Gaussian PSD Models. When
the transition and observations are approximated by Gaussian PSD Models, we
show that our proposed estimator enjoys strong theoretical guarantees, with
estimation error that depends on the quality of the approximation and is
adaptive to the regularity of the transition probabilities. In particular, we
identify regimes in which our proposed filter attains a TV $\epsilon$-error
with memory and computational complexity of $O(\epsilon^{-1})$ and
$O(\epsilon^{-3/2})$ respectively, including the offline learning step, in
contrast to the $O(\epsilon^{-2})$ complexity of sampling methods such as
particle filtering.
\\ ( https://arxiv.org/abs/2402.09796 ,  33kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09802 (*cross-listing*)
Date: Thu, 15 Feb 2024 08:58:58 GMT   (244kb,D)

Title: Criterion collapse and loss distribution control
Authors: Matthew J. Holland
Categories: stat.ML cs.LG
\\
  In this work, we consider the notion of "criterion collapse," in which
optimization of one metric implies optimality in another, with a particular
focus on conditions for collapse into error probability minimizers under a wide
variety of learning criteria, ranging from DRO and OCE risks (CVaR, tilted ERM)
to non-monotonic criteria underlying recent ascent-descent algorithms explored
in the literature (Flooding, SoftAD). We show how collapse in the context of
losses with a Bernoulli distribution goes far beyond existing results for CVaR
and DRO, then expand our scope to include surrogate losses, showing conditions
where monotonic criteria such as tilted ERM cannot avoid collapse, whereas
non-monotonic alternatives can.
\\ ( https://arxiv.org/abs/2402.09802 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09807 (*cross-listing*)
Date: Thu, 15 Feb 2024 09:13:59 GMT   (97kb,D)

Title: Two trust region type algorithms for solving nonconvex-strongly concave
  minimax problems
Authors: Tongliang Yao and Zi Xu
Categories: math.OC cs.LG stat.ML
MSC-class: 90C47, 90C26, 90C30
\\
  In this paper, we propose a Minimax Trust Region (MINIMAX-TR) algorithm and a
Minimax Trust Region Algorithm with Contractions and Expansions(MINIMAX-TRACE)
algorithm for solving nonconvex-strongly concave minimax problems. Both
algorithms can find an $(\epsilon, \sqrt{\epsilon})$-second order stationary
point(SSP) within $\mathcal{O}(\epsilon^{-1.5})$ iterations, which matches the
best well known iteration complexity.
\\ ( https://arxiv.org/abs/2402.09807 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09821 (*cross-listing*)
Date: Thu, 15 Feb 2024 09:36:36 GMT   (7167kb,D)

Title: Diffusion Models for Audio Restoration
Authors: Jean-Marie Lemercier, Julius Richter, Simon Welker, Eloi Moliner, Vesa
  V\"alim\"aki,Timo Gerkmann
Categories: eess.AS cs.LG cs.SD
Comments: Full paper invited to the IEEE Signal Processing Magazine Special
  Issue "Model-based and Data-Driven Audio Signal Processing"
\\
  With the development of audio playback devices and fast data transmission,
the demand for high sound quality is rising, for both entertainment and
communications. In this quest for better sound quality, challenges emerge from
distortions and interferences originating at the recording side or caused by an
imperfect transmission pipeline. To address this problem, audio restoration
methods aim to recover clean sound signals from the corrupted input data. We
present here audio restoration algorithms based on diffusion models, with a
focus on speech enhancement and music restoration tasks. Traditional
approaches, often grounded in handcrafted rules and statistical heuristics,
have shaped our understanding of audio signals. In the past decades, there has
been a notable shift towards data-driven methods that exploit the modeling
capabilities of deep neural networks (DNNs). Deep generative models, and among
them diffusion models, have emerged as powerful techniques for learning complex
data distributions. However, relying solely on DNN-based learning approaches
carries the risk of reducing interpretability, particularly when employing
end-to-end models. Nonetheless, data-driven approaches allow more flexibility
in comparison to statistical model-based frameworks whose performance depends
on distributional and statistical assumptions that can be difficult to
guarantee. Here, we aim to show that diffusion models can combine the best of
both worlds and offer the opportunity to design audio restoration algorithms
with a good degree of interpretability and a remarkable performance in terms of
sound quality.
\\ ( https://arxiv.org/abs/2402.09821 ,  7167kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09846 (*cross-listing*)
Date: Thu, 15 Feb 2024 10:05:18 GMT   (2105kb)

Title: A Deep Learning Approach to Radar-based QPE
Authors: Ting-Shuo Yo, Shih-Hao Su, Jung-Lien Chu, Chiao-Wei Chang, and
  Hung-Chi Kuo
Categories: physics.ao-ph cs.LG
Comments: 22 pages, 11 figures. Published in Earth and Space Science
Journal-ref: Earth Space Sci. 2021, 8, e2020EA001340
DOI: 10.1029/2020EA001340
\\
  In this study, we propose a volume-to-point framework for quantitative
precipitation estimation (QPE) based on the Quantitative Precipitation
Estimation and Segregation Using Multiple Sensor (QPESUMS) Mosaic Radar data
set. With a data volume consisting of the time series of gridded radar
reflectivities over the Taiwan area, we used machine learning algorithms to
establish a statistical model for QPE in weather stations. The model extracts
spatial and temporal features from the input data volume and then associates
these features with the location-specific precipitations. In contrast to QPE
methods based on the Z-R relation, we leverage the machine learning algorithms
to automatically detect the evolution and movement of weather systems and
associate these patterns to a location with specific topographic attributes.
Specifically, we evaluated this framework with the hourly precipitation data of
45 weather stations in Taipei during 2013-2016. In comparison to the
operational QPE scheme used by the Central Weather Bureau, the volume-to-point
framework performed comparably well in general cases and excelled in detecting
heavy-rainfall events. By using the current results as the reference benchmark,
the proposed method can integrate the heterogeneous data sources and
potentially improve the forecast in extreme precipitation scenarios.
\\ ( https://arxiv.org/abs/2402.09846 ,  2105kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09948 (*cross-listing*)
Date: Thu, 15 Feb 2024 13:51:21 GMT   (2938kb,D)

Title: Neural 5G Indoor Localization with IMU Supervision
Authors: Aleksandr Ermolov, Shreya Kadambi, Maximilian Arnold, Mohammed
  Hirzallah, Roohollah Amiri, Deepak Singh Mahendar Singh, Srinivas Yerramalli,
  Daniel Dijkman, Fatih Porikli, Taesang Yoo, Bence Major
Categories: eess.SP cs.LG
Comments: IEEE GLOBECOM 2023
\\
  Radio signals are well suited for user localization because they are
ubiquitous, can operate in the dark and maintain privacy. Many prior works
learn mappings between channel state information (CSI) and position
fully-supervised. However, that approach relies on position labels which are
very expensive to acquire. In this work, this requirement is relaxed by using
pseudo-labels during deployment, which are calculated from an inertial
measurement unit (IMU). We propose practical algorithms for IMU double
integration and training of the localization system. We show decimeter-level
accuracy on simulated and challenging real data of 5G measurements. Our
IMU-supervised method performs similarly to fully-supervised, but requires much
less effort to deploy.
\\ ( https://arxiv.org/abs/2402.09948 ,  2938kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09978 (*cross-listing*)
Date: Thu, 15 Feb 2024 14:41:55 GMT   (1101kb)

Title: Deep learning for the design of non-Hermitian topolectrical circuits
Authors: Xi Chen, Jinyang Sun, Xiumei Wang, Hengxuan Jiang, Dandan Zhu, and
  Xingping Zhou
Categories: physics.app-ph cs.LG
\\
  Non-Hermitian topological phases can produce some remarkable properties,
compared with their Hermitian counterpart, such as the breakdown of
conventional bulk-boundary correspondence and the non-Hermitian topological
edge mode. Here, we introduce several algorithms with multi-layer perceptron
(MLP), and convolutional neural network (CNN) in the field of deep learning, to
predict the winding of eigenvalues non-Hermitian Hamiltonians. Subsequently, we
use the smallest module of the periodic circuit as one unit to construct
high-dimensional circuit data features. Further, we use the Dense Convolutional
Network (DenseNet), a type of convolutional neural network that utilizes dense
connections between layers to design a non-Hermitian topolectrical Chern
circuit, as the DenseNet algorithm is more suitable for processing
high-dimensional data. Our results demonstrate the effectiveness of the deep
learning network in capturing the global topological characteristics of a
non-Hermitian system based on training data.
\\ ( https://arxiv.org/abs/2402.09978 ,  1101kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09990 (*cross-listing*)
Date: Thu, 15 Feb 2024 14:54:46 GMT   (3720kb,D)

Title: TIAViz: A Browser-based Visualization Tool for Computational Pathology
  Models
Authors: Mark Eastwood and John Pocock and Mostafa Jahanifar and Adam Shephard
  and Skiros Habib and Ethar Alzaid and Abdullah Alsalemi and Jan Lukas
  Robertus and Nasir Rajpoot and Shan Raza and Fayyaz Minhas
Categories: cs.CV cs.HC cs.LG
Comments: Application note to be submitted to bioinformatics
\\
  Digital pathology has gained significant traction in modern healthcare
systems. This shift from optical microscopes to digital imagery brings with it
the potential for improved diagnosis, efficiency, and the integration of AI
tools into the pathologists workflow. A critical aspect of this is
visualization. Throughout the development of a machine learning (ML) model in
digital pathology, it is crucial to have flexible, openly available tools to
visualize models, from their outputs and predictions to the underlying
annotations and images used to train or test a model. We introduce TIAViz, a
Python-based visualization tool built into TIAToolbox which allows flexible,
interactive, fully zoomable overlay of a wide variety of information onto whole
slide images, including graphs, heatmaps, segmentations, annotations and other
WSIs. The UI is browser-based, allowing use either locally, on a remote
machine, or on a server to provide publicly available demos. This tool is open
source and is made available at:
https://github.com/TissueImageAnalytics/tiatoolbox and via pip installation
(pip install tiatoolbox) and conda as part of TIAToolbox.
\\ ( https://arxiv.org/abs/2402.09990 ,  3720kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10009 (*cross-listing*)
Date: Thu, 15 Feb 2024 15:17:26 GMT   (7103kb,D)

Title: Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion
Authors: Hila Manor and Tomer Michaeli
Categories: cs.SD cs.LG eess.AS
Comments: Examples and code available in
  https://hilamanor.github.io/AudioEditing/
\\
  Editing signals using large pre-trained models, in a zero-shot manner, has
recently seen rapid advancements in the image domain. However, this wave has
yet to reach the audio domain. In this paper, we explore two zero-shot editing
techniques for audio signals, which use DDPM inversion on pre-trained diffusion
models. The first, adopted from the image domain, allows text-based editing.
The second, is a novel approach for discovering semantically meaningful editing
directions without supervision. When applied to music signals, this method
exposes a range of musically interesting modifications, from controlling the
participation of specific instruments to improvisations on the melody. Samples
can be found on our examples page in https://hilamanor.github.io/AudioEditing/
and code can be found in https://github.com/hilamanor/AudioEditing/ .
\\ ( https://arxiv.org/abs/2402.10009 ,  7103kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10036 (*cross-listing*)
Date: Thu, 15 Feb 2024 15:59:59 GMT   (5543kb,D)

Title: Predictive Linear Online Tracking for Unknown Targets
Authors: Anastasios Tsiamis, Aren Karapetyan, Yueshan Li, Efe C. Balta, John
  Lygeros
Categories: eess.SY cs.LG cs.SY math.OC
\\
  In this paper, we study the problem of online tracking in linear control
systems, where the objective is to follow a moving target. Unlike classical
tracking control, the target is unknown, non-stationary, and its state is
revealed sequentially, thus, fitting the framework of online non-stochastic
control. We consider the case of quadratic costs and propose a new algorithm,
called predictive linear online tracking (PLOT). The algorithm uses recursive
least squares with exponential forgetting to learn a time-varying dynamic model
of the target. The learned model is used in the optimal policy under the
framework of receding horizon control. We show the dynamic regret of PLOT
scales with $\mathcal{O}(\sqrt{TV_T})$, where $V_T$ is the total variation of
the target dynamics and $T$ is the time horizon. Unlike prior work, our
theoretical results hold for non-stationary targets. We implement PLOT on a
real quadrotor and provide open-source software, thus, showcasing one of the
first successful applications of online control methods on real hardware.
\\ ( https://arxiv.org/abs/2402.10036 ,  5543kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10043 (*cross-listing*)
Date: Thu, 15 Feb 2024 16:05:35 GMT   (586kb,D)

Title: How to validate average calibration for machine learning regression
  tasks ?
Authors: Pascal Pernot
Categories: stat.ML cs.LG
\\
  Average calibration of the uncertainties of machine learning regression tasks
can be tested in two ways. One way is to estimate the calibration error (CE) as
the difference between the mean absolute error (MSE) and the mean variance (MV)
or mean squared uncertainty. The alternative is to compare the mean squared
z-scores or scaled errors (ZMS) to 1. Both approaches might lead to different
conclusion, as illustrated on an ensemble of datasets from the recent machine
learning uncertainty quantification literature. It is shown here that the CE is
very sensitive to the distribution of uncertainties, and notably to the
presence of outlying uncertainties, and that it cannot be used reliably for
calibration testing. By contrast, the ZMS statistic does not present this
sensitivity issue and offers the most reliable approach in this context.
Implications for the validation of conditional calibration are discussed.
\\ ( https://arxiv.org/abs/2402.10043 ,  586kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10045 (*cross-listing*)
Date: Thu, 11 Jan 2024 03:36:47 GMT   (1267kb)

Title: Short-Form Videos and Mental Health: A Knowledge-Guided Multimodal
  Neural Topic Model
Authors: Jiaheng Xie, Ruicheng Liang, Yidong Chai, Yang Liu
Categories: cs.CV cs.LG
\\
  While short-form videos head to reshape the entire social media landscape,
experts are exceedingly worried about their depressive impacts on viewers, as
evidenced by medical studies. To prevent widespread consequences, platforms are
eager to predict these videos' impact on viewers' mental health. Subsequently,
they can take intervention measures, such as revising recommendation algorithms
and displaying viewer discretion. Nevertheless, applicable predictive methods
lack relevance to well-established medical knowledge, which outlines clinically
proven external and environmental factors of depression. To account for such
medical knowledge, we resort to an emergent methodological discipline, seeded
Neural Topic Models (NTMs). However, existing seeded NTMs suffer from the
limitations of single-origin topics, unknown topic sources, unclear seed
supervision, and suboptimal convergence. To address those challenges, we
develop a novel Knowledge-guided Multimodal NTM to predict a short-form video's
depressive impact on viewers. Extensive empirical analyses using TikTok and
Douyin datasets prove that our method outperforms state-of-the-art benchmarks.
Our method also discovers medically relevant topics from videos that are linked
to depressive impact. We contribute to IS with a novel video analytics method
that is generalizable to other video classification problems. Practically, our
method can help platforms understand videos' mental impacts, thus adjusting
recommendations and video topic disclosure.
\\ ( https://arxiv.org/abs/2402.10045 ,  1267kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10064 (*cross-listing*)
Date: Mon, 22 Jan 2024 12:17:27 GMT   (600kb)

Title: Navigating the Maize: Cyclic and conditional computational graphs for
  molecular simulation
Authors: Thomas L\"ohr, Michael Dodds, Lili Cao, Mikhail Kabeshov, Michele
  Assante, Jon-Paul Janet, Marco Kl\"ahn, Ola Engkvist
Categories: cs.DC cs.LG
\\
  Many computational chemistry and molecular simulation workflows can be
expressed as graphs. This abstraction is useful to modularize and potentially
reuse existing components, as well as provide parallelization and ease
reproducibility. Existing tools represent the computation as a directed acyclic
graph (DAG), thus allowing efficient execution by parallelization of concurrent
branches. These systems can, however, generally not express cyclic and
conditional workflows. We therefore developed Maize, a workflow manager for
cyclic and conditional graphs based on the principles of flow-based
programming. By running each node of the graph concurrently in separate
processes and allowing communication at any time through dedicated inter-node
channels, arbitrary graph structures can be executed. We demonstrate the
effectiveness of the tool on a dynamic active learning task in computational
drug design, involving the use of a small molecule generative model and an
associated scoring system.
\\ ( https://arxiv.org/abs/2402.10064 ,  600kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10066 (*cross-listing*)
Date: Thu, 15 Feb 2024 16:31:54 GMT   (1353kb,D)

Title: NYCTALE: Neuro-Evidence Transformer for Adaptive and Personalized Lung
  Nodule Invasiveness Prediction
Authors: Sadaf Khademi, Anastasia Oikonomou, Konstantinos N. Plataniotis, Arash
  Mohammadi
Categories: cs.CV cs.LG eess.IV
\\
  Drawing inspiration from the primate brain's intriguing evidence accumulation
process, and guided by models from cognitive psychology and neuroscience, the
paper introduces the NYCTALE framework, a neuro-inspired and evidence
accumulation-based Transformer architecture. The proposed neuro-inspired
NYCTALE offers a novel pathway in the domain of Personalized Medicine (PM) for
lung cancer diagnosis. In nature, Nyctales are small owls known for their
nocturnal behavior, hunting primarily during the darkness of night. The NYCTALE
operates in a similarly vigilant manner, i.e., processing data in an
evidence-based fashion and making predictions dynamically/adaptively. Distinct
from conventional Computed Tomography (CT)-based Deep Learning (DL) models, the
NYCTALE performs predictions only when sufficient amount of evidence is
accumulated. In other words, instead of processing all or a pre-defined subset
of CT slices, for each person, slices are provided one at a time. The NYCTALE
framework then computes an evidence vector associated with contribution of each
new CT image. A decision is made once the total accumulated evidence surpasses
a specific threshold. Preliminary experimental analyses conducted using a
challenging in-house dataset comprising 114 subjects. The results are
noteworthy, suggesting that NYCTALE outperforms the benchmark accuracy even
with approximately 60% less training data on this demanding and small dataset.
\\ ( https://arxiv.org/abs/2402.10066 ,  1353kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10069 (*cross-listing*)
Date: Thu, 25 Jan 2024 12:03:10 GMT   (4476kb,D)

Title: Learning fast changing slow in spiking neural networks
Authors: Cristiano Capone and Paolo Muratore
Categories: cs.NE cs.LG
Comments: 12 pages, 4 figures
\\
  Reinforcement learning (RL) faces substantial challenges when applied to
real-life problems, primarily stemming from the scarcity of available data due
to limited interactions with the environment. This limitation is exacerbated by
the fact that RL often demands a considerable volume of data for effective
learning. The complexity escalates further when implementing RL in recurrent
spiking networks, where inherent noise introduced by spikes adds a layer of
difficulty. Life-long learning machines must inherently resolve the
plasticity-stability paradox. Striking a balance between acquiring new
knowledge and maintaining stability is crucial for artificial agents. In this
context, we take inspiration from machine learning technology and introduce a
biologically plausible implementation of proximal policy optimization, arguing
that it significantly alleviates this challenge. Our approach yields two
notable advancements: first, the ability to assimilate new information without
necessitating alterations to the current policy, and second, the capability to
replay experiences without succumbing to policy divergence. Furthermore, when
contrasted with other experience replay (ER) techniques, our method
demonstrates the added advantage of being computationally efficient in an
online setting. We demonstrate that the proposed methodology enhances the
efficiency of learning, showcasing its potential impact on neuromorphic and
real-world applications.
\\ ( https://arxiv.org/abs/2402.10069 ,  4476kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10072 (*cross-listing*)
Date: Fri, 26 Jan 2024 04:55:37 GMT   (326kb,D)

Title: Deep Joint Source-Channel Coding for Efficient and Reliable
  Cross-Technology Communication
Authors: Shumin Yao, Xiaodong Xu, Hao Chen, Yaping Sun, and Qinglin Zhao
Categories: cs.IT cs.LG cs.NI math.IT
\\
  Cross-technology communication (CTC) is a promising technique that enables
direct communications among incompatible wireless technologies without needing
hardware modification. However, it has not been widely adopted in real-world
applications due to its inefficiency and unreliability. To address this issue,
this paper proposes a deep joint source-channel coding (DJSCC) scheme to enable
efficient and reliable CTC. The proposed scheme builds a neural-network-based
encoder and decoder at the sender side and the receiver side, respectively, to
achieve two critical tasks simultaneously: 1) compressing the messages to the
point where only their essential semantic meanings are preserved; 2) ensuring
the robustness of the semantic meanings when they are transmitted across
incompatible technologies. The scheme incorporates existing CTC coding
algorithms as domain knowledge to guide the encoder-decoder pair to learn the
characteristics of CTC links better. Moreover, the scheme constructs shared
semantic knowledge for the encoder and decoder, allowing semantic meanings to
be converted into very few bits for cross-technology transmissions, thus
further improving the efficiency of CTC. Extensive simulations verify that the
proposed scheme can reduce the transmission overhead by up to 97.63\% and
increase the structural similarity index measure by up to 734.78%, compared
with the state-of-the-art CTC scheme.
\\ ( https://arxiv.org/abs/2402.10072 ,  326kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10077 (*cross-listing*)
Date: Sun, 28 Jan 2024 18:24:34 GMT   (17179kb)

Title: Towards a large-scale fused and labeled dataset of human pose while
  interacting with robots in shared urban areas
Authors: E. Sherafat and B. Farooq
Categories: cs.RO cs.LG
Comments: Sherafat E., and Farooq B., (2024). Towards a large-scale fused and
  labeled dataset of human pose while interacting with robots in shared urban
  areas. In the proceedings of the 103rd Annual Meeting of Transportation
  Research Board. Washington DC
\\
  Over the last decade, Autonomous Delivery Robots (ADRs) have transformed
conventional delivery methods, responding to the growing e-commerce demand.
However, the readiness of ADRs to navigate safely among pedestrians in shared
urban areas remains an open question. We contend that there are crucial
research gaps in understanding their interactions with pedestrians in such
environments. Human Pose Estimation is a vital stepping stone for various
downstream applications, including pose prediction and socially aware robot
path-planning. Yet, the absence of an enriched and pose-labeled dataset
capturing human-robot interactions in shared urban areas hinders this
objective. In this paper, we bridge this gap by repurposing, fusing, and
labeling two datasets, MOT17 and NCLT, focused on pedestrian tracking and
Simultaneous Localization and Mapping (SLAM), respectively. The resulting
unique dataset represents thousands of real-world indoor and outdoor
human-robot interaction scenarios. Leveraging YOLOv7, we obtained human pose
visual and numeric outputs and provided ground truth poses using manual
annotation. To overcome the distance bias present in the traditional MPJPE
metric, this study introduces a novel human pose estimation error metric called
Mean Scaled Joint Error (MSJE) by incorporating bounding box dimensions into
it. Findings demonstrate that YOLOv7 effectively estimates human pose in both
datasets. However, it exhibits weaker performance in specific scenarios, like
indoor, crowded scenes with a focused light source, where both MPJPE and MSJE
are recorded as 10.89 and 25.3, respectively. In contrast, YOLOv7 performs
better in single-person estimation (NCLT seq 2) and outdoor scenarios (MOT17
seq1), achieving MSJE values of 5.29 and 3.38, respectively.
\\ ( https://arxiv.org/abs/2402.10077 ,  17179kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10078 (*cross-listing*)
Date: Sun, 28 Jan 2024 19:42:05 GMT   (178kb,D)

Title: EventF2S: Asynchronous and Sparse Spiking AER Framework using
  Neuromorphic-Friendly Algorithm
Authors: Lakshmi Annamalai and Chetan Singh Thakur
Categories: cs.NE cs.LG eess.SP
\\
  Bio-inspired Address Event Representation (AER) sensors have attracted
significant popularity owing to their low power consumption, high sparsity, and
high temporal resolution. Spiking Neural Network (SNN) has become the inherent
choice for AER data processing. However, the integration of the AER-SNN
paradigm has not adequately explored asynchronous processing, neuromorphic
compatibility, and sparse spiking, which are the key requirements of
resource-constrained applications. To address this gap, we introduce a
brain-inspired AER-SNN object recognition solution, which includes a data
encoder integrated with a First-To-Spike recognition network. Being fascinated
by the functionality of neurons in the visual cortex, we designed the solution
to be asynchronous and compatible with neuromorphic hardware. Furthermore, we
have adapted the principle of denoising and First-To-Spike coding to achieve
optimal spike signaling, significantly reducing computation costs. Experimental
evaluation has demonstrated that the proposed method incurs significantly less
computation cost to achieve state-of-the-art competitive accuracy. Overall, the
proposed solution offers an asynchronous and cost-effective AER recognition
system that harnesses the full potential of AER sensors.
\\ ( https://arxiv.org/abs/2402.10078 ,  178kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10079 (*cross-listing*)
Date: Mon, 29 Jan 2024 16:56:17 GMT   (9918kb,D)

Title: Review of the Learning-based Camera and Lidar Simulation Methods for
  Autonomous Driving Systems
Authors: Hamed Haghighi, Xiaomeng Wang, Hao Jing, and Mehrdad Dianati
Categories: cs.CV cs.GR cs.LG cs.RO
\\
  Perception sensors, particularly camera and Lidar, are key elements of
Autonomous Driving Systems (ADS) that enable them to comprehend their
surroundings for informed driving and control decisions. Therefore, developing
realistic camera and Lidar simulation methods, also known as camera and Lidar
models, is of paramount importance to effectively conduct simulation-based
testing for ADS. Moreover, the rise of deep learning-based perception models
has propelled the prevalence of perception sensor models as valuable tools for
synthesising diverse training datasets. The traditional sensor simulation
methods rely on computationally expensive physics-based algorithms,
specifically in complex systems such as ADS. Hence, the current potential
resides in learning-based models, driven by the success of deep generative
models in synthesising high-dimensional data. This paper reviews the current
state-of-the-art in learning-based sensor simulation methods and validation
approaches, focusing on two main types of perception sensors: cameras and
Lidars. This review covers two categories of learning-based approaches, namely
raw-data-based and object-based models. Raw-data-based methods are explained
concerning the employed learning strategy, while object-based models are
categorised based on the type of error considered. Finally, the paper
illustrates commonly used validation techniques for evaluating perception
sensor models and highlights the existing research gaps in the area.
\\ ( https://arxiv.org/abs/2402.10079 ,  9918kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10085 (*cross-listing*)
Date: Thu, 1 Feb 2024 09:02:44 GMT   (2474kb,D)

Title: Develop End-to-End Anomaly Detection System
Authors: Emanuele Mengoli, Zhiyuan Yao, Wutao Wei
Categories: cs.NI cs.LG
\\
  Anomaly detection plays a crucial role in ensuring network robustness.
However, implementing intelligent alerting systems becomes a challenge when
considering scenarios in which anomalies can be caused by both malicious and
non-malicious events, leading to the difficulty of determining anomaly
patterns. The lack of labeled data in the computer networking domain further
exacerbates this issue, impeding the development of robust models capable of
handling real-world scenarios. To address this challenge, in this paper, we
propose an end-to-end anomaly detection model development pipeline. This
framework makes it possible to consume user feedback and enable continuous
user-centric model performance evaluation and optimization. We demonstrate the
efficacy of the framework by way of introducing and bench-marking a new
forecasting model -- named \emph{Lachesis} -- on a real-world networking
problem. Experiments have demonstrated the robustness and effectiveness of the
two proposed versions of \emph{Lachesis} compared with other models proposed in
the literature. Our findings underscore the potential for improving the
performance of data-driven products over their life cycles through a harmonized
integration of user feedback and iterative development.
\\ ( https://arxiv.org/abs/2402.10085 ,  2474kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10087 (*cross-listing*)
Date: Wed, 31 Jan 2024 23:51:14 GMT   (751kb)

Title: Decentralized Covert Routing in Heterogeneous Networks Using
  Reinforcement Learning
Authors: Justin Kong, Terrence J. Moore, and Fikadu T. Dagefu
Categories: cs.NI cs.LG eess.SP
\\
  This letter investigates covert routing communications in a heterogeneous
network where a source transmits confidential data to a destination with the
aid of relaying nodes where each transmitter judiciously chooses one modality
among multiple communication modalities. We develop a novel reinforcement
learning-based covert routing algorithm that finds a route from the source to
the destination where each node identifies its next hop and modality only based
on the local feedback information received from its neighboring nodes. We show
based on numerical simulations that the proposed covert routing strategy has
only negligible performance loss compared to the optimal centralized routing
scheme.
\\ ( https://arxiv.org/abs/2402.10087 ,  751kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10088 (*cross-listing*)
Date: Thu, 1 Feb 2024 15:15:25 GMT   (4697kb,D)

Title: Hierarchical hybrid modeling for flexible tool use
Authors: Matteo Priorelli, Ivilin Peev Stoianov
Categories: cs.RO cs.LG
\\
  In a recent computational framework called active inference, discrete models
can be linked to their continuous counterparts to perform decision-making in
changing environments. From another perspective, simple agents can be combined
to better capture the causal relationships of the world. How can we use these
two features together to achieve efficient goal-directed behavior? We present
an architecture composed of several hybrid -- continuous and discrete -- units
replicating the agent's configuration, controlled by a high-level discrete
model that achieves dynamic planning and synchronized behavior. Additional
factorizations within each level allow to represent hierarchically other agents
and objects in relation to the self. We evaluate this hierarchical hybrid model
on a non-trivial task: reaching a moving object after having picked a moving
tool. This study extends past work on control as inference and proposes an
alternative direction to deep reinforcement learning.
\\ ( https://arxiv.org/abs/2402.10088 ,  4697kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10090 (*cross-listing*)
Date: Thu, 1 Feb 2024 03:08:21 GMT   (771kb)

Title: PICS: Pipeline for Image Captioning and Search
Authors: Grant Rosario, David Noever
Categories: cs.CV cs.IR cs.LG
\\
  The growing volume of digital images necessitates advanced systems for
efficient categorization and retrieval, presenting a significant challenge in
database management and information retrieval. This paper introduces PICS
(Pipeline for Image Captioning and Search), a novel approach designed to
address the complexities inherent in organizing large-scale image repositories.
PICS leverages the advancements in Large Language Models (LLMs) to automate the
process of image captioning, offering a solution that transcends traditional
manual annotation methods. The approach is rooted in the understanding that
meaningful, AI-generated captions can significantly enhance the searchability
and accessibility of images in large databases. By integrating sentiment
analysis into the pipeline, PICS further enriches the metadata, enabling
nuanced searches that extend beyond basic descriptors. This methodology not
only simplifies the task of managing vast image collections but also sets a new
precedent for accuracy and efficiency in image retrieval. The significance of
PICS lies in its potential to transform image database systems, harnessing the
power of machine learning and natural language processing to meet the demands
of modern digital asset management.
\\ ( https://arxiv.org/abs/2402.10090 ,  771kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10092 (*cross-listing*)
Date: Thu, 1 Feb 2024 14:16:10 GMT   (1101kb,D)

Title: Workflow Optimization for Parallel Split Learning
Authors: Joana Tirana, Dimitra Tsigkari, George Iosifidis, Dimitris
  Chatzopoulos
Categories: cs.DC cs.LG cs.NI
Comments: IEEE INFOCOM 2024
\\
  Split learning (SL) has been recently proposed as a way to enable
resource-constrained devices to train multi-parameter neural networks (NNs) and
participate in federated learning (FL). In a nutshell, SL splits the NN model
into parts, and allows clients (devices) to offload the largest part as a
processing task to a computationally powerful helper. In parallel SL, multiple
helpers can process model parts of one or more clients, thus, considerably
reducing the maximum training time over all clients (makespan). In this paper,
we focus on orchestrating the workflow of this operation, which is critical in
highly heterogeneous systems, as our experiments show. In particular, we
formulate the joint problem of client-helper assignments and scheduling
decisions with the goal of minimizing the training makespan, and we prove that
it is NP-hard. We propose a solution method based on the decomposition of the
problem by leveraging its inherent symmetry, and a second one that is fully
scalable. A wealth of numerical evaluations using our testbed's measurements
allow us to build a solution strategy comprising these methods. Moreover, we
show that this strategy finds a near-optimal solution, and achieves a shorter
makespan than the baseline scheme by up to 52.3%.
\\ ( https://arxiv.org/abs/2402.10092 ,  1101kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10118 (*cross-listing*)
Date: Thu, 15 Feb 2024 17:16:33 GMT   (615kb,D)

Title: Reusing Softmax Hardware Unit for GELU Computation in Transformers
Authors: Christodoulos Peltekis, Kosmas Alexandridi, Giorgos Dimitrakopoulos
Categories: cs.AR cs.LG
Comments: AICAS 2024
\\
  Transformers have improved drastically the performance of natural language
processing (NLP) and computer vision applications. The computation of
transformers involves matrix multiplications and non-linear activation
functions such as softmax and GELU (Gaussion Error Linear Unit) that are
accelerated directly in hardware. Currently, function evaluation is done
separately for each function and rarely allows for hardware reuse. To mitigate
this problem, in this work, we map the computation of GELU to a softmax
operator. In this way, the efficient hardware units designed already for
softmax can be reused for computing GELU as well. Computation of GELU can enjoy
the inherent vectorized nature of softmax and produce in parallel multiple GELU
outcomes. Experimental results show that computing GELU via a pre-existing and
incrementally modified softmax hardware unit (a) does not reduce the accuracy
of representative NLP applications and (b) allows the reduction of the overall
hardware area and power by 6.1% and 11.9%, respectively, on average.
\\ ( https://arxiv.org/abs/2402.10118 ,  615kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10127 (*cross-listing*)
Date: Thu, 15 Feb 2024 17:31:19 GMT   (169kb,D)

Title: Nonlinear spiked covariance matrices and signal propagation in deep
  neural networks
Authors: Zhichao Wang, Denny Wu, Zhou Fan
Categories: stat.ML cs.LG math.PR math.ST stat.TH
Comments: 55 pages
\\
  Many recent works have studied the eigenvalue spectrum of the Conjugate
Kernel (CK) defined by the nonlinear feature map of a feedforward neural
network. However, existing results only establish weak convergence of the
empirical eigenvalue distribution, and fall short of providing precise
quantitative characterizations of the ''spike'' eigenvalues and eigenvectors
that often capture the low-dimensional signal structure of the learning
problem. In this work, we characterize these signal eigenvalues and
eigenvectors for a nonlinear version of the spiked covariance model, including
the CK as a special case. Using this general result, we give a quantitative
description of how spiked eigenstructure in the input data propagates through
the hidden layers of a neural network with random weights. As a second
application, we study a simple regime of representation learning where the
weight matrix develops a rank-one signal component over training and
characterize the alignment of the target function with the spike eigenvector of
the CK on test data.
\\ ( https://arxiv.org/abs/2402.10127 ,  169kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10128 (*cross-listing*)
Date: Thu, 15 Feb 2024 17:32:50 GMT   (45637kb,D)

Title: GES: Generalized Exponential Splatting for Efficient Radiance Field
  Rendering
Authors: Abdullah Hamdi, Luke Melas-Kyriazi, Guocheng Qian, Jinjie Mai, Ruoshi
  Liu, Carl Vondrick, Bernard Ghanem, Andrea Vedaldi
Categories: cs.CV cs.GR cs.LG
Comments: preprint
\\
  Advancements in 3D Gaussian Splatting have significantly accelerated 3D
reconstruction and generation. However, it may require a large number of
Gaussians, which creates a substantial memory footprint. This paper introduces
GES (Generalized Exponential Splatting), a novel representation that employs
Generalized Exponential Function (GEF) to model 3D scenes, requiring far fewer
particles to represent a scene and thus significantly outperforming Gaussian
Splatting methods in efficiency with a plug-and-play replacement ability for
Gaussian-based utilities. GES is validated theoretically and empirically in
both principled 1D setup and realistic 3D scenes.
  It is shown to represent signals with sharp edges more accurately, which are
typically challenging for Gaussians due to their inherent low-pass
characteristics. Our empirical analysis demonstrates that GEF outperforms
Gaussians in fitting natural-occurring signals (e.g. squares, triangles, and
parabolic signals), thereby reducing the need for extensive splitting
operations that increase the memory footprint of Gaussian Splatting. With the
aid of a frequency-modulated loss, GES achieves competitive performance in
novel-view synthesis benchmarks while requiring less than half the memory
storage of Gaussian Splatting and increasing the rendering speed by up to 39%.
The code is available on the project website https://abdullahamdi.com/ges .
\\ ( https://arxiv.org/abs/2402.10128 ,  45637kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10164 (*cross-listing*)
Date: Thu, 15 Feb 2024 18:09:41 GMT   (408kb,D)

Title: Random features and polynomial rules
Authors: Fabi\'an Aguirre-L\'opez, Silvio Franz, Mauro Pastore
Categories: cond-mat.dis-nn cs.LG
Comments: 11 pages + appendix, 4 figures. Comments are welcome
\\
  Random features models play a distinguished role in the theory of deep
learning, describing the behavior of neural networks close to their
infinite-width limit. In this work, we present a thorough analysis of the
generalization performance of random features models for generic supervised
learning problems with Gaussian data. Our approach, built with tools from the
statistical mechanics of disordered systems, maps the random features model to
an equivalent polynomial model, and allows us to plot average generalization
curves as functions of the two main control parameters of the problem: the
number of random features $N$ and the size $P$ of the training set, both
assumed to scale as powers in the input dimension $D$. Our results extend the
case of proportional scaling between $N$, $P$ and $D$. They are in accordance
with rigorous bounds known for certain particular learning tasks and are in
quantitative agreement with numerical experiments performed over many order of
magnitudes of $N$ and $P$. We find good agreement also far from the asymptotic
limits where $D\to \infty$ and at least one between $P/D^K$, $N/D^L$ remains
finite.
\\ ( https://arxiv.org/abs/2402.10164 ,  408kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2306.01711
replaced with revised version Thu, 15 Feb 2024 02:57:22 GMT   (24876kb,D)

Title: OMNI: Open-endedness via Models of human Notions of Interestingness
Authors: Jenny Zhang, Joel Lehman, Kenneth Stanley, Jeff Clune
Categories: cs.AI cs.LG
Comments: 47 pages, 33 figures
\\ ( https://arxiv.org/abs/2306.01711 ,  24876kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16958
replaced with revised version Thu, 15 Feb 2024 16:42:00 GMT   (47kb)

Title: Identifiability of Direct Effects from Summary Causal Graphs
Authors: Simon Ferreira and Charles K. Assaad
Categories: cs.AI
\\ ( https://arxiv.org/abs/2306.16958 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13538
replaced with revised version Sat, 27 Jan 2024 10:10:20 GMT   (1102kb,D)

Title: Speak Like a Native: Prompting Large Language Models in a Native Style
Authors: Zhicheng Yang, Yiwei Wang, Yinya Huang, Jing Xiong, Xiaodan Liang,
  Jing Tang
Categories: cs.AI cs.LG
Comments: 8 pages, 5 figures
\\ ( https://arxiv.org/abs/2311.13538 ,  1102kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14188
replaced with revised version Thu, 15 Feb 2024 13:21:44 GMT   (918kb,D)

Title: Enhancing Neural Theorem Proving through Data Augmentation and Dynamic
  Sampling Method
Authors: Rahul Vishwakarma and Subhankar Mishra
Categories: cs.AI cs.LG cs.LO
\\ ( https://arxiv.org/abs/2312.14188 ,  918kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07221
replaced with revised version Thu, 15 Feb 2024 11:45:37 GMT   (186kb,D)

Title: The Reasons that Agents Act: Intention and Instrumental Goals
Authors: Francis Rhys Ward and Matt MacDermott and Francesco Belardinelli and
  Francesca Toni and Tom Everitt
Categories: cs.AI
Comments: AAMAS24
\\ ( https://arxiv.org/abs/2402.07221 ,  186kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07456
replaced with revised version Thu, 15 Feb 2024 09:30:48 GMT   (2761kb,D)

Title: OS-Copilot: Towards Generalist Computer Agents with Self-Improvement
Authors: Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu,
  Shunyu Yao, Tao Yu and Lingpeng Kong
Categories: cs.AI
Comments: Project page: https://os-copilot.github.io
\\ ( https://arxiv.org/abs/2402.07456 ,  2761kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09051
replaced with revised version Thu, 15 Feb 2024 04:50:52 GMT   (505kb,D)

Title: FGeo-DRL: Deductive Reasoning for Geometric Problems through Deep
  Reinforcement Learning
Authors: Jia Zou, Xiaokai Zhang, Yiming He, Na Zhu, Tuo Leng
Categories: cs.AI
Comments: 15 pages
\\ ( https://arxiv.org/abs/2402.09051 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09132
replaced with revised version Thu, 15 Feb 2024 06:39:48 GMT   (86kb,D)

Title: Exploring the Adversarial Capabilities of Large Language Models
Authors: Lukas Struppek, Minh Hieu Le, Dominik Hintersdorf, Kristian Kersting
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.09132 ,  86kb)
------------------------------------------------------------------------------
\\
arXiv:2304.10813
replaced with revised version Thu, 15 Feb 2024 08:59:59 GMT   (8485kb,D)

Title: Tokenization Preference for Human and ML Model: An Annotation Study
Authors: Tatsuya Hiraoka, Tomoya Iwakura
Categories: cs.CL
\\ ( https://arxiv.org/abs/2304.10813 ,  8485kb)
------------------------------------------------------------------------------
\\
arXiv:2304.13343
replaced with revised version Thu, 15 Feb 2024 16:01:39 GMT   (3865kb,D)

Title: Enhancing Large Language Model with Self-Controlled Memory Framework
Authors: Bing Wang, Xinnian Liang, Jian Yang, Hui Huang, Shuangzhi Wu, Peihao
  Wu, Lu Lu, Zejun Ma, Zhoujun Li
Categories: cs.CL
Comments: under preview
\\ ( https://arxiv.org/abs/2304.13343 ,  3865kb)
------------------------------------------------------------------------------
\\
arXiv:2305.07358
replaced with revised version Thu, 15 Feb 2024 08:06:30 GMT   (2155kb,D)

Title: Towards Versatile and Efficient Visual Knowledge Integration into
  Pre-trained Language Models with Cross-Modal Adapters
Authors: Xinyun Zhang, Haochen Tan, Han Wu, Mingjie Zhan, Ding Liang, Bei Yu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2305.07358 ,  2155kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18354
replaced with revised version Wed, 14 Feb 2024 21:15:31 GMT   (9564kb,D)

Title: LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and
  the Importance of Object-based Representations
Authors: Yudong Xu, Wenhao Li, Pashootan Vaezipoor, Scott Sanner, Elias B.
  Khalil
Categories: cs.CL cs.AI
Comments: 26 pages, 15 figures, published in Transactions on Machine Learning
  Research (TMLR)
\\ ( https://arxiv.org/abs/2305.18354 ,  9564kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02591
replaced with revised version Thu, 15 Feb 2024 17:40:03 GMT   (164kb,D)

Title: ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant
  Behavior Detection
Authors: Sunjae Kwon, Xun Wang, Weisong Liu, Emily Druhl, Minhee L. Sung, Joel
  I. Reisman, Wenjun Li, Robert D. Kerns, William Becker, Hong Yu
Categories: cs.CL cs.AI
Comments: Under review
\\ ( https://arxiv.org/abs/2307.02591 ,  164kb)
------------------------------------------------------------------------------
\\
arXiv:2308.03958
replaced with revised version Thu, 15 Feb 2024 01:03:13 GMT   (151kb,D)

Title: Simple synthetic data reduces sycophancy in large language models
Authors: Jerry Wei and Da Huang and Yifeng Lu and Denny Zhou and Quoc V. Le
Categories: cs.CL
\\ ( https://arxiv.org/abs/2308.03958 ,  151kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06706
replaced with revised version Thu, 15 Feb 2024 06:50:00 GMT   (7983kb,D)

Title: Simultaneous Machine Translation with Large Language Models
Authors: Minghan Wang, Jinming Zhao, Thuy-Trang Vu, Fatemeh Shiri, Ehsan
  Shareghi, Gholamreza Haffari
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.06706 ,  7983kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08901
replaced with revised version Thu, 15 Feb 2024 05:42:15 GMT   (588kb,D)

Title: Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning
Authors: Xijie Huang, Li Lyna Zhang, Kwang-Ting Cheng, Fan Yang, Mao Yang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2312.08901 ,  588kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11242
replaced with revised version Thu, 15 Feb 2024 12:55:55 GMT   (8579kb,D)

Title: MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL
Authors: Bing Wang, Changyu Ren, Jian Yang, Xinnian Liang, Jiaqi Bai, Linzheng
  Chai, Zhao Yan, Qian-Wen Zhang, Di Yin, Xing Sun, Zhoujun Li
Categories: cs.CL
Comments: under preview
\\ ( https://arxiv.org/abs/2312.11242 ,  8579kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15194
replaced with revised version Thu, 15 Feb 2024 03:10:29 GMT   (293kb,D)

Title: PokeMQA: Programmable knowledge editing for Multi-hop Question Answering
Authors: Hengrui Gu, Kaixiong Zhou, Xiaotian Han, Ninghao Liu, Ruobing Wang,
  Xin Wang
Categories: cs.CL
Comments: Our code is available at https://github.com/Hengrui-Gu/PokeMQA
\\ ( https://arxiv.org/abs/2312.15194 ,  293kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01218
replaced with revised version Thu, 15 Feb 2024 08:04:13 GMT   (7039kb,D)

Title: Zero-Shot Position Debiasing for Large Language Models
Authors: Zhongkun Liu, Zheng Chen, Mengqi Zhang, Zhaochun Ren, Pengjie Ren,
  Zhumin Chen
Categories: cs.CL cs.AI cs.LG
Comments: The version for ARR submission; 20 pages, 22 figures
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2401.01218 ,  7039kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02987
replaced with revised version Wed, 14 Feb 2024 19:05:46 GMT   (6460kb,D)

Title: Has Your Pretrained Model Improved? A Multi-head Posterior Based
  Approach
Authors: Prince Aboagye, Yan Zheng, Junpeng Wang, Uday Singh Saini, Xin Dai,
  Michael Yeh, Yujie Fan, Zhongfang Zhuang, Shubham Jain, Liang Wang and Wei
  Zhang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.02987 ,  6460kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06468
replaced with revised version Thu, 15 Feb 2024 09:35:37 GMT   (7824kb,D)

Title: Adapting Large Language Models for Document-Level Machine Translation
Authors: Minghao Wu, Thuy-Trang Vu, Lizhen Qu, George Foster, Gholamreza
  Haffari
Categories: cs.CL
Comments: work in progress; 20 pages, 16 tables, 7 figures
\\ ( https://arxiv.org/abs/2401.06468 ,  7824kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06643
replaced with revised version Thu, 15 Feb 2024 11:14:10 GMT   (9390kb,D)

Title: Effects of diversity incentives on sample diversity and downstream model
  performance in LLM-based text augmentation
Authors: Jan Cegin, Branislav Pecher, Jakub Simko, Ivan Srba, Maria Bielikova,
  Peter Brusilovsky
Categories: cs.CL
Comments: 24 pages, updated with new experimets - Mistral as downstream task
  classifier and new method combination (of taboo and hints methods)
\\ ( https://arxiv.org/abs/2401.06643 ,  9390kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07927
replaced with revised version Thu, 15 Feb 2024 17:19:22 GMT   (193kb,D)

Title: Are self-explanations from Large Language Models faithful?
Authors: Andreas Madsen, Sarath Chandar, Siva Reddy
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.07927 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14681
replaced with revised version Thu, 15 Feb 2024 06:05:13 GMT   (7591kb,D)

Title: MasonTigers@LT-EDI-2024: An Ensemble Approach Towards Detecting
  Homophobia and Transphobia in Social Media Comments
Authors: Dhiman Goswami, Sadiya Sayara Chowdhury Puspo, Md Nishat Raihan, Al
  Nahian Bin Emran
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.14681 ,  7591kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17809
replaced with revised version Thu, 15 Feb 2024 15:43:55 GMT   (380kb,D)

Title: SWEA: Changing Factual Knowledge in Large Language Models via Subject
  Word Embedding Altering
Authors: Xiaopeng Li, Shasha Li, Shezheng Song, Huijun Liu, Bin Ji, Xi Wang,
  Jun Ma, Jie Yu, Xiaodong Liu, Jing Wang and Weimin Zhang
Categories: cs.CL cs.AI cs.LG
Comments: Under review; Our code will be released
\\ ( https://arxiv.org/abs/2401.17809 ,  380kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03877
replaced with revised version Wed, 14 Feb 2024 19:33:19 GMT   (18407kb,D)

Title: Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large
  Language Models
Authors: Spyridon Mouselinos, Henryk Michalewski, Mateusz Malinowski
Categories: cs.CL cs.AI
Comments: Preprint. Work in progress
\\ ( https://arxiv.org/abs/2402.03877 ,  18407kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04838
replaced with revised version Thu, 15 Feb 2024 01:52:16 GMT   (7984kb,D)

Title: PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity
  Recognition
Authors: Jinghui Lu, Ziwei Yang, Yanjie Wang, Xuejing Liu, Brian Mac Namee, Can
  Huang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.04838 ,  7984kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05515
replaced with revised version Thu, 15 Feb 2024 15:25:47 GMT   (5149kb,D)

Title: NoisyICL: A Little Noise in Model Parameters Calibrates In-context
  Learning
Authors: Yufeng Zhao, Yoshihiro Sakai, Naoya Inoue
Categories: cs.CL cs.AI
Comments: 20 pages, 28 figures, 7 tables (5 pages, 4 figures, 1 table in main
  body). ACL 2024 under review
\\ ( https://arxiv.org/abs/2402.05515 ,  5149kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06015
replaced with revised version Thu, 15 Feb 2024 10:39:19 GMT   (9225kb,D)

Title: Exploring Visual Culture Awareness in GPT-4V: A Comprehensive Probing
Authors: Yong Cao, Wenyan Li, Jiaang Li, Yifei Yuan, Antonia Karamolegkou,
  Daniel Hershcovich
Categories: cs.CL cs.CV
Comments: work in process
\\ ( https://arxiv.org/abs/2402.06015 ,  9225kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07157
replaced with revised version Wed, 14 Feb 2024 19:59:05 GMT   (480kb,D)

Title: Natural Language Reinforcement Learning
Authors: Xidong Feng, Ziyu Wan, Mengyue Yang, Ziyan Wang, Girish A. Koushik,
  Yali Du, Ying Wen, Jun Wang
Categories: cs.CL cs.AI cs.LG
Comments: Work in Progress
\\ ( https://arxiv.org/abs/2402.07157 ,  480kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08341
replaced with revised version Thu, 15 Feb 2024 09:12:40 GMT   (41375kb,D)

Title: Eliciting Personality Traits in Large Language Models
Authors: Airlie Hilliard, Cristian Munoz, Zekun Wu and Adriano Soares Koshiyama
Categories: cs.CL cs.AI
Comments: Manuscript submitted to ACM Facct. Authors One and Two contributed
  equally to this work
\\ ( https://arxiv.org/abs/2402.08341 ,  41375kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08638
replaced with revised version Thu, 15 Feb 2024 16:15:01 GMT   (6892kb,D)

Title: SemRel2024: A Collection of Semantic Textual Relatedness Datasets for 14
  Languages
Authors: Nedjma Ousidhoum, Shamsuddeen Hassan Muhammad, Mohamed Abdalla, Idris
  Abdulmumin, Ibrahim Said Ahmad, Sanchit Ahuja, Alham Fikri Aji, Vladimir
  Araujo, Abinew Ali Ayele, Pavan Baswani, Meriem Beloucif, Chris Biemann,
  Sofia Bourhim, Christine De Kock, Genet Shanko Dekebo, Oumaima Hourrane,
  Gopichand Kanumolu, Lokesh Madasu, Samuel Rutunda, Manish Shrivastava, Thamar
  Solorio, Nirmal Surange, Hailegnaw Getaneh Tilaye, Krishnapriya Vishnubhotla,
  Genta Winata, Seid Muhie Yimam, Saif M. Mohammad
Categories: cs.CL
Comments: 18 pages
\\ ( https://arxiv.org/abs/2402.08638 ,  6892kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09015
replaced with revised version Thu, 15 Feb 2024 18:24:03 GMT   (15116kb,D)

Title: Towards better Human-Agent Alignment: Assessing Task Utility in
  LLM-Powered Applications
Authors: Negar Arabzadeh and Julia Kiseleva and Qingyun Wu and Chi Wang and
  Ahmed Awadallah and Victor Dibia and Adam Fourney and Charles Clarke
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.09015 ,  15116kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09193
replaced with revised version Thu, 15 Feb 2024 11:09:09 GMT   (487kb,D)

Title: (Ir)rationality and Cognitive Biases in Large Language Models
Authors: Olivia Macmillan-Scott and Mirco Musolesi
Categories: cs.CL cs.AI cs.HC
\\ ( https://arxiv.org/abs/2402.09193 ,  487kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09205
replaced with revised version Thu, 15 Feb 2024 09:59:52 GMT   (9114kb,D)

Title: Tell Me More! Towards Implicit User Intention Understanding of Language
  Model Driven Agents
Authors: Cheng Qian, Bingxiang He, Zhong Zhuang, Jia Deng, Yujia Qin, Xin Cong,
  Zhong Zhang, Jie Zhou, Yankai Lin, Zhiyuan Liu, Maosong Sun
Categories: cs.CL cs.AI cs.HC
Comments: 26 pages, 5 tables, 6 figures
\\ ( https://arxiv.org/abs/2402.09205 ,  9114kb)
------------------------------------------------------------------------------
\\
arXiv:2006.07796
replaced with revised version Thu, 15 Feb 2024 14:34:20 GMT   (46231kb,D)

Title: Structure by Architecture: Structured Representations without
  Regularization
Authors: Felix Leeb, Guilia Lanzillotta, Yashas Annadani, Michel Besserve,
  Stefan Bauer, Bernhard Sch\"olkopf
Categories: cs.LG cs.CV stat.ML
Comments: Published at ICLR 2023
\\ ( https://arxiv.org/abs/2006.07796 ,  46231kb)
------------------------------------------------------------------------------
\\
arXiv:2112.02817
replaced with revised version Thu, 15 Feb 2024 16:05:26 GMT   (4444kb,D)

Title: ED2: Environment Dynamics Decomposition World Models for Continuous
  Control
Authors: Jianye Hao, Yifu Yuan, Cong Wang, Zhen Wang
Categories: cs.LG cs.AI
Comments: 10 pages, 13 figures
\\ ( https://arxiv.org/abs/2112.02817 ,  4444kb)
------------------------------------------------------------------------------
\\
arXiv:2202.01456
replaced with revised version Thu, 15 Feb 2024 17:02:58 GMT   (4666kb,D)

Title: Fast and explainable clustering based on sorting
Authors: Xinye Chen, Stefan G\"uttel
Categories: cs.LG cs.DS stat.CO stat.ML
\\ ( https://arxiv.org/abs/2202.01456 ,  4666kb)
------------------------------------------------------------------------------
\\
arXiv:2204.09092
replaced with revised version Thu, 15 Feb 2024 16:57:43 GMT   (9118kb,D)

Title: Indiscriminate Data Poisoning Attacks on Neural Networks
Authors: Yiwei Lu, Gautam Kamath, Yaoliang Yu
Categories: cs.LG cs.CR
Comments: Accepted to TMLR in 2022
\\ ( https://arxiv.org/abs/2204.09092 ,  9118kb)
------------------------------------------------------------------------------
\\
arXiv:2210.02042
replaced with revised version Thu, 15 Feb 2024 16:58:34 GMT   (697kb,D)

Title: FedMT: Federated Learning with Mixed-type Labels
Authors: Qiong Zhang, Jing Peng, Xin Zhang, Aline Talhouk, Gang Niu, Xiaoxiao
  Li
Categories: cs.LG cs.AI cs.DC
Comments: 23 pages
\\ ( https://arxiv.org/abs/2210.02042 ,  697kb)
------------------------------------------------------------------------------
\\
arXiv:2301.00462
replaced with revised version Thu, 15 Feb 2024 16:18:32 GMT   (4261kb,D)

Title: A Latent Space Correlation-Aware Autoencoder for Anomaly Detection in
  Skewed Data
Authors: Padmaksha Roy
Categories: cs.LG
\\ ( https://arxiv.org/abs/2301.00462 ,  4261kb)
------------------------------------------------------------------------------
\\
arXiv:2302.03811
replaced with revised version Thu, 15 Feb 2024 15:15:19 GMT   (1308kb,D)

Title: On the Convergence of Modified Policy Iteration in Risk Sensitive
  Exponential Cost Markov Decision Processes
Authors: Yashaswini Murthy, Mehrdad Moharrami and R. Srikant
Categories: cs.LG cs.AI cs.SY eess.SY
Comments: 25 pages, 3 figures, Under review at Operations Research
\\ ( https://arxiv.org/abs/2302.03811 ,  1308kb)
------------------------------------------------------------------------------
\\
arXiv:2302.07998
replaced with revised version Wed, 14 Feb 2024 20:32:16 GMT   (7515kb)

Title: cGAN-Based High Dimensional IMU Sensor Data Generation for Enhanced
  Human Activity Recognition in Therapeutic Activities
Authors: Mohammad Mohammadzadeh, Ali Ghadami, Alireza Taheri, Saeed Behzadipour
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2302.07998 ,  7515kb)
------------------------------------------------------------------------------
\\
arXiv:2303.03751
replaced with revised version Thu, 15 Feb 2024 11:25:33 GMT   (35990kb,D)

Title: Zeroth-Order Optimization Meets Human Feedback: Provable Learning via
  Ranking Oracles
Authors: Zhiwei Tang, Dmitry Rybin, Tsung-Hui Chang
Categories: cs.LG cs.AI
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2303.03751 ,  35990kb)
------------------------------------------------------------------------------
\\
arXiv:2304.00891
replaced with revised version Thu, 15 Feb 2024 15:56:37 GMT   (1205kb)

Title: Online Algorithms for Hierarchical Inference in Deep Learning
  applications at the Edge
Authors: Vishnu Narayanan Moothedath, Jaya Prakash Champati, James Gross
Categories: cs.LG cs.CV
Comments: The original version was submitted to a journal and was later
  revised. The updated version was accepted in a journal and will be published
  soon. The 'Journal reference' will be updated as and when the information is
  available
\\ ( https://arxiv.org/abs/2304.00891 ,  1205kb)
------------------------------------------------------------------------------
\\
arXiv:2304.11005
replaced with revised version Thu, 15 Feb 2024 13:52:33 GMT   (15533kb,D)

Title: Self-Correcting Bayesian Optimization through Bayesian Active Learning
Authors: Carl Hvarfner, Erik Hellsten, Frank Hutter, Luigi Nardi
Categories: cs.LG stat.ML
Journal-ref: 37th International Conference on Neural Information Processing
  Systems (NeurIPS 2023)
\\ ( https://arxiv.org/abs/2304.11005 ,  15533kb)
------------------------------------------------------------------------------
\\
arXiv:2305.10978
replaced with revised version Thu, 15 Feb 2024 13:33:22 GMT   (4325kb,D)

Title: Client Selection for Federated Policy Optimization with Environment
  Heterogeneity
Authors: Zhijie Xie, S.H. Song
Categories: cs.LG
\\ ( https://arxiv.org/abs/2305.10978 ,  4325kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13036
replaced with revised version Thu, 15 Feb 2024 01:25:29 GMT   (8878kb,D)

Title: Disentangling Structured Components: Towards Adaptive, Interpretable and
  Scalable Time Series Forecasting
Authors: Jinliang Deng, Xiusi Chen, Renhe Jiang, Du Yin, Yi Yang, Xuan Song,
  Ivor W. Tsang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2305.13036 ,  8878kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15508
replaced with revised version Thu, 15 Feb 2024 17:56:41 GMT   (4880kb,D)

Title: How to Fix a Broken Confidence Estimator: Evaluating Post-hoc Methods
  for Selective Classification with Deep Neural Networks
Authors: Lu\'is Felipe P. Cattelan and Danilo Silva
Categories: cs.LG
\\ ( https://arxiv.org/abs/2305.15508 ,  4880kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02285
replaced with revised version Thu, 15 Feb 2024 03:58:01 GMT   (10163kb,D)

Title: Clarify Confused Nodes via Separated Learning
Authors: Jiajun Zhou, Shengbo Gong, Chenxuan Xie, Shanqing Yu, Qi Xuan, Xiaoniu
  Yang
Categories: cs.LG cs.SI
Comments: 16 pages
\\ ( https://arxiv.org/abs/2306.02285 ,  10163kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02939
replaced with revised version Thu, 15 Feb 2024 15:01:31 GMT   (157kb,D)

Title: Improved Stability and Generalization Guarantees of the Decentralized
  SGD Algorithm
Authors: Batiste Le Bars, Aur\'elien Bellet, Marc Tommasi, Kevin Scaman,
  Giovanni Neglia
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2306.02939 ,  157kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09739
replaced with revised version Thu, 15 Feb 2024 16:47:31 GMT   (6439kb,D)

Title: Stabilized Neural Differential Equations for Learning Dynamics with
  Explicit Constraints
Authors: Alistair White, Niki Kilbertus, Maximilian Gelbrecht, Niklas Boers
Categories: cs.LG physics.comp-ph stat.ML
Comments: 22 pages, 8 figures. Accepted at NeurIPS 2023
\\ ( https://arxiv.org/abs/2306.09739 ,  6439kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00493
replaced with revised version Thu, 15 Feb 2024 01:29:54 GMT   (402kb)

Title: Fourier-Mixed Window Attention: Accelerating Informer for Long Sequence
  Time-Series Forecasting
Authors: Nhat Thanh Tran, Jack Xin
Categories: cs.LG cs.AI
Comments: 13 pages (main), 2 pages (appendix), 2 figures
\\ ( https://arxiv.org/abs/2307.00493 ,  402kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03577
replaced with revised version Thu, 15 Feb 2024 14:51:54 GMT   (383kb,D)

Title: CuTS: Customizable Tabular Synthetic Data Generation
Authors: Mark Vero, Mislav Balunovi\'c, Martin Vechev
Categories: cs.LG cs.DB cs.PL
\\ ( https://arxiv.org/abs/2307.03577 ,  383kb)
------------------------------------------------------------------------------
\\
arXiv:2307.15154
replaced with revised version Thu, 15 Feb 2024 07:52:59 GMT   (999kb,D)

Title: A/B Testing and Best-arm Identification for Linear Bandits with
  Robustness to Non-stationarity
Authors: Zhihan Xiong, Romain Camilleri, Maryam Fazel, Lalit Jain, Kevin
  Jamieson
Categories: cs.LG stat.ML
Comments: 25 pages, 6 figures
\\ ( https://arxiv.org/abs/2307.15154 ,  999kb)
------------------------------------------------------------------------------
\\
arXiv:2308.03306
replaced with revised version Thu, 15 Feb 2024 09:05:30 GMT   (150kb,D)

Title: Implicit Graph Neural Diffusion Networks: Convergence, Generalization,
  and Over-Smoothing
Authors: Guoji Fu, Mohammed Haroon Dupty, Yanfei Dong, Lee Wee Sun
Categories: cs.LG
Comments: 57 pages
\\ ( https://arxiv.org/abs/2308.03306 ,  150kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09565
replaced with revised version Thu, 15 Feb 2024 02:43:25 GMT   (639kb,D)

Title: Understanding the Role of Layer Normalization in Label-Skewed Federated
  Learning
Authors: Guojun Zhang, Mahdi Beitollahi, Alex Bie, Xi Chen
Categories: cs.LG stat.ML
Comments: accepted at TMLR
\\ ( https://arxiv.org/abs/2308.09565 ,  639kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13498
replaced with revised version Wed, 14 Feb 2024 19:21:59 GMT   (7503kb,D)

Title: Efficient Epistemic Uncertainty Estimation in Regression Ensemble Models
  Using Pairwise-Distance Estimators
Authors: Lucas Berry, David Meger
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2308.13498 ,  7503kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14642
replaced with revised version Thu, 15 Feb 2024 13:11:56 GMT   (33kb)

Title: Rate-Optimal Policy Optimization for Linear Markov Decision Processes
Authors: Uri Sherman, Alon Cohen, Tomer Koren, Yishay Mansour
Categories: cs.LG
\\ ( https://arxiv.org/abs/2308.14642 ,  33kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06651
replaced with revised version Thu, 15 Feb 2024 08:22:48 GMT   (4284kb,D)

Title: ConR: Contrastive Regularizer for Deep Imbalanced Regression
Authors: Mahsa Keramati, Lili Meng, R. David Evans
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.06651 ,  4284kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14053
replaced with revised version Thu, 15 Feb 2024 17:37:56 GMT   (23692kb,D)

Title: Revisiting LARS for Large Batch Training Generalization of Neural
  Networks
Authors: Khoi Do, Duong Nguyen, Hoa Nguyen, Long Tran-Thanh, Nguyen-Hoang Tran,
  and Quoc-Viet Pham
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2309.14053 ,  23692kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16391
replaced with revised version Thu, 15 Feb 2024 11:10:35 GMT   (213kb,D)

Title: 2-Cats: 2D Copula Approximating Transforms
Authors: Flavio Figueiredo, Jos\'e Geraldo Fernandes, Jackson Silva, Renato M.
  Assun\c{c}\~ao
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2309.16391 ,  213kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00301
replaced with revised version Thu, 15 Feb 2024 07:12:14 GMT   (9242kb,D)

Title: Enhancing the Hierarchical Environment Design via Generative Trajectory
  Modeling
Authors: Dexun Li, Pradeep Varakantham
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.00301 ,  9242kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05264
replaced with revised version Thu, 15 Feb 2024 18:14:53 GMT   (47464kb,D)

Title: The Emergence of Reproducibility and Consistency in Diffusion Models
Authors: Huijie Zhang, Jinfan Zhou, Yifu Lu, Minzhe Guo, Peng Wang, Liyue Shen,
  Qing Qu
Categories: cs.LG cs.CV
Comments: 49 pages, 23 figures
\\ ( https://arxiv.org/abs/2310.05264 ,  47464kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06379
replaced with revised version Thu, 15 Feb 2024 12:03:19 GMT   (6306kb,D)

Title: Initialization Bias of Fourier Neural Operator: Revisiting the Edge of
  Chaos
Authors: Takeshi Koshizuka, Masahiro Fujisawa, Yusuke Tanaka, and Issei Sato
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.06379 ,  6306kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11787
replaced with revised version Thu, 15 Feb 2024 15:31:28 GMT   (3212kb,D)

Title: NeuroCUT: A Neural Approach for Robust Graph Partitioning
Authors: Rishi Shah, Krishnanshu Jain, Sahil Manchanda, Sourav Medya and Sayan
  Ranu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.11787 ,  3212kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12595
replaced with revised version Thu, 15 Feb 2024 10:52:33 GMT   (1019kb,D)

Title: Meta-Learning With Hierarchical Models Based on Similarity of Causal
  Mechanisms
Authors: Sophie Wharrie, Samuel Kaski
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.12595 ,  1019kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13230
replaced with revised version Wed, 14 Feb 2024 19:57:40 GMT   (32419kb,D)

Title: Absolute Policy Optimization
Authors: Weiye Zhao, Feihan Li, Yifan Sun, Rui Chen, Tianhao Wei, Changliu Liu
Categories: cs.LG cs.AI cs.RO
Comments: submission to ICML 2024
\\ ( https://arxiv.org/abs/2310.13230 ,  32419kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16955
replaced with revised version Wed, 14 Feb 2024 20:01:11 GMT   (505kb,D)

Title: Break it, Imitate it, Fix it: Robustness by Generating Human-Like
  Attacks
Authors: Aradhana Sinha, Ananth Balashankar, Ahmad Beirami, Thi Avrahami, Jilin
  Chen, Alex Beutel
Categories: cs.LG
Journal-ref: Transactions on Machine Learning Research (2024)
\\ ( https://arxiv.org/abs/2310.16955 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20431
replaced with revised version Thu, 15 Feb 2024 12:24:44 GMT   (8827kb,D)

Title: Raising the ClaSS of Streaming Time Series Segmentation
Authors: Arik Ermshaus, Patrick Sch\"afer, Ulf Leser
Categories: cs.LG cs.AI cs.DB
\\ ( https://arxiv.org/abs/2310.20431 ,  8827kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01591
replaced with revised version Thu, 15 Feb 2024 17:48:33 GMT   (1601kb,D)

Title: Better Fair than Sorry: Adversarial Missing Data Imputation for Fair
  GNNs
Authors: Debolina Halder Lina and Arlei Silva
Categories: cs.LG
\\ ( https://arxiv.org/abs/2311.01591 ,  1601kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12356
replaced with revised version Thu, 15 Feb 2024 10:57:22 GMT   (5485kb,D)

Title: Random Linear Projections Loss for Hyperplane-Based Optimization in
  Neural Networks
Authors: Shyam Venkatasubramanian, Ahmed Aloui, Vahid Tarokh
Categories: cs.LG
\\ ( https://arxiv.org/abs/2311.12356 ,  5485kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14079
replaced with revised version Thu, 15 Feb 2024 16:28:57 GMT   (11247kb,D)

Title: Empirical Comparison between Cross-Validation and Mutation-Validation in
  Model Selection
Authors: Jinyang Yu, Sami Hamdan, Leonard Sasse, Abigail Morrison, Kaustubh R.
  Patil
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2311.14079 ,  11247kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15332
replaced with revised version Wed, 14 Feb 2024 19:35:22 GMT   (618kb)

Title: ASI: Accuracy-Stability Index for Evaluating Deep Learning Models
Authors: Wei Dai, Daniel Berleant
Categories: cs.LG cs.AI cs.CV cs.IT cs.PF math.IT
Comments: 6 pages, 3 figures
Journal-ref: 2023 IEEE International Conference on Big Data (BigData),
  Sorrento, Italy, 2023, pp. 4284-4289
DOI: 10.1109/BigData59044.2023.10386728
\\ ( https://arxiv.org/abs/2311.15332 ,  618kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00817
replaced with revised version Wed, 14 Feb 2024 21:03:42 GMT   (4658kb,D)

Title: Extrapolatable Transformer Pre-training for Ultra Long Time-Series
  Forecasting
Authors: Ziyang Song, Qincheng Lu, Hao Xu, David L. Buckeridge, Yue Li
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2312.00817 ,  4658kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08724
replaced with revised version Wed, 14 Feb 2024 22:13:48 GMT   (5020kb,D)

Title: Personalized Path Recourse for Reinforcement Learning Agents
Authors: Dat Hong, Tong Wang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2312.08724 ,  5020kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10913
replaced with revised version Thu, 15 Feb 2024 03:56:45 GMT   (263kb,D)

Title: GINN-LP: A Growing Interpretable Neural Network for Discovering
  Multivariate Laurent Polynomial Equations
Authors: Nisal Ranasinghe, Damith Senanayake, Sachith Seneviratne, Malin
  Premaratne, Saman Halgamuge
Categories: cs.LG cs.AI
Comments: 14 pages, 12 figures, Accepted by AAAI24
\\ ( https://arxiv.org/abs/2312.10913 ,  263kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11560
replaced with revised version Thu, 15 Feb 2024 00:49:30 GMT   (944kb,D)

Title: Learning from Emergence: A Study on Proactively Inhibiting the
  Monosemantic Neurons of Artificial Neural Networks
Authors: Jiachuan Wang, Shimin Di, Lei Chen, Charles Wang Wai Ng
Categories: cs.LG cs.AI cs.NE
Comments: 15 pages, 4 figures, conference
\\ ( https://arxiv.org/abs/2312.11560 ,  944kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06923
replaced with revised version Thu, 15 Feb 2024 18:15:35 GMT   (3655kb,D)

Title: Minimally Supervised Learning using Topological Projections in
  Self-Organizing Maps
Authors: Zimeng Lyu, Alexander Ororbia, Rui Li, Travis Desell
Categories: cs.LG cs.NE
\\ ( https://arxiv.org/abs/2401.06923 ,  3655kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15814
replaced with revised version Thu, 15 Feb 2024 01:05:18 GMT   (764kb,D)

Title: OntoMedRec: Logically-Pretrained Model-Agnostic Ontology Encoders for
  Medication Recommendation
Authors: Weicong Tan, Weiqing Wang, Xin Zhou, Wray Buntine, Gordon Bingham,
  Hongzhi Yin
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.15814 ,  764kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16785
replaced with revised version Thu, 15 Feb 2024 12:46:41 GMT   (962kb,D)

Title: HawkEye: Advancing Robust Regression with Bounded, Smooth, and
  Insensitive Loss Function
Authors: Mushir Akhtar, M. Tanveer, and Mohd. Arshad
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.16785 ,  962kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02009
replaced with revised version Wed, 14 Feb 2024 20:41:41 GMT   (251kb,D)

Title: Robust Multi-Task Learning with Excess Risks
Authors: Yifei He, Shiji Zhou, Guojun Zhang, Hyokun Yun, Yi Xu, Belinda Zeng,
  Trishul Chilimbi, Han Zhao
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.02009 ,  251kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04520
replaced with revised version Wed, 14 Feb 2024 20:35:31 GMT   (43kb)

Title: On Computational Limits of Modern Hopfield Models: A Fine-Grained
  Complexity Analysis
Authors: Jerry Yao-Chieh Hu, Thomas Lin, Zhao Song, Han Liu
Categories: cs.LG cs.AI stat.ML
Comments: 28 pages; v2: fix typos
\\ ( https://arxiv.org/abs/2402.04520 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04655
replaced with revised version Thu, 15 Feb 2024 07:26:52 GMT   (1436kb,D)

Title: Open-Vocabulary Calibration for Vision-Language Models
Authors: Shuoyuan Wang, Jindong Wang, Guoqing Wang, Bob Zhang, Kaiyang Zhou,
  Hongxin Wei
Categories: cs.LG
Comments: Preprint
\\ ( https://arxiv.org/abs/2402.04655 ,  1436kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04902
replaced with revised version Thu, 15 Feb 2024 11:30:08 GMT   (264kb,D)

Title: L4Q: Parameter Efficient Quantization-Aware Training on Large Language
  Models via LoRA-wise LSQ
Authors: Hyesung Jeon, Yulhwa Kim, Jae-joon Kim
Categories: cs.LG cs.CL
Comments: 8 pages, 2 figures
\\ ( https://arxiv.org/abs/2402.04902 ,  264kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06461
replaced with revised version Thu, 15 Feb 2024 00:44:01 GMT   (25510kb,D)

Title: Sequential Flow Straightening for Generative Modeling
Authors: Jongmin Yoon, and Juho Lee
Categories: cs.LG cs.CV stat.ML
Comments: 21 pages, 13 figures
\\ ( https://arxiv.org/abs/2402.06461 ,  25510kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06808
replaced with revised version Thu, 15 Feb 2024 17:32:38 GMT   (2890kb,D)

Title: Explain Variance of Prediction in Variational Time Series Models for
  Clinical Deterioration Prediction
Authors: Jiacheng Liu and Jaideep Srivastava
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.06808 ,  2890kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07180
replaced with revised version Wed, 14 Feb 2024 19:59:13 GMT   (5275kb,D)

Title: MAGNETO: Edge AI for Human Activity Recognition -- Privacy and
  Personalization
Authors: Jingwei Zuo, George Arvanitakis, Mthandazo Ndhlovu and Hakim Hacid
Categories: cs.LG cs.AI cs.CR
Comments: Accepted by EDBT 2024 (demo track)
\\ ( https://arxiv.org/abs/2402.07180 ,  5275kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07738
replaced with revised version Thu, 15 Feb 2024 15:19:30 GMT   (8572kb,D)

Title: Universal Link Predictor By In-Context Learning on Graphs
Authors: Kaiwen Dong, Haitao Mao, Zhichun Guo, Nitesh V. Chawla
Categories: cs.LG
Comments: Preprint
\\ ( https://arxiv.org/abs/2402.07738 ,  8572kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07834
replaced with revised version Thu, 15 Feb 2024 18:28:51 GMT   (2587kb,D)

Title: Generalizing across Temporal Domains with Koopman Operators
Authors: Qiuhao Zeng, Wei Wang, Fan Zhou, Gezheng Xu, Ruizhi Pu, Changjian
  Shui, Christian Gagne, Shichun Yang, Boyu Wang, Charles X. Ling
Categories: cs.LG
Comments: 15 pages, 7 figures, Accepted by AAAI 2024. arXiv admin note: text
  overlap with arXiv:2206.00047
\\ ( https://arxiv.org/abs/2402.07834 ,  2587kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07876
replaced with revised version Thu, 15 Feb 2024 17:20:22 GMT   (13524kb,D)

Title: Policy Improvement using Language Feedback Models
Authors: Victor Zhong, Dipendra Misra, Xingdi Yuan, Marc-Alexandre C\^ot\'e
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.07876 ,  13524kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08090
replaced with revised version Wed, 14 Feb 2024 22:19:36 GMT   (3369kb,D)

Title: Learning Neural Contracting Dynamics: Extended Linearization and Global
  Guarantees
Authors: Sean Jaffe and Alexander Davydov and Deniz Lapsekili and Ambuj Singh
  and Francesco Bullo
Categories: cs.LG math.OC
Comments: 9 pages, 3 figures. Under Review
\\ ( https://arxiv.org/abs/2402.08090 ,  3369kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08093
replaced with revised version Thu, 15 Feb 2024 18:57:26 GMT   (2424kb,D)

Title: BASE TTS: Lessons from building a billion-parameter Text-to-Speech model
  on 100K hours of data
Authors: Mateusz {\L}ajszczak, Guillermo C\'ambara, Yang Li, Fatih Beyhan,
  Arent van Korlaar, Fan Yang, Arnaud Joly, \'Alvaro Mart\'in-Cortinas, Ammar
  Abbas, Adam Michalski, Alexis Moinet, Sri Karlapati, Ewa Muszy\'nska, Haohan
  Guo, Bartosz Putrycz, Soledad L\'opez Gambino, Kayeon Yoo, Elena Sokolova,
  Thomas Drugman
Categories: cs.LG cs.CL eess.AS
Comments: v1.1 (fixed typos)
\\ ( https://arxiv.org/abs/2402.08093 ,  2424kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08563
replaced with revised version Wed, 14 Feb 2024 21:30:39 GMT   (2436kb,D)

Title: Denoising Diffusion Restoration Tackles Forward and Inverse Problems for
  the Laplace Operator
Authors: Amartya Mukherjee, Melissa M. Stadt, Lena Podina, Mohammad Kohandel,
  Jun Liu
Categories: cs.LG cs.CV math.AP
Comments: 29 pages
\\ ( https://arxiv.org/abs/2402.08563 ,  2436kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08595
replaced with revised version Thu, 15 Feb 2024 15:41:59 GMT   (143kb,D)

Title: Homomorphism Counts for Graph Neural Networks: All About That Basis
Authors: Emily Jin, Michael Bronstein, Ismail Ilkan Ceylan, Matthias Lanzinger
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.08595 ,  143kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09234
replaced with revised version Thu, 15 Feb 2024 08:10:45 GMT   (4193kb,D)

Title: Multi-Hierarchical Surrogate Learning for Structural Dynamical Crash
  Simulations Using Graph Convolutional Neural Networks
Authors: Jonas Kneifl, J\"org Fehr, Steven L. Brunton, J. Nathan Kutz
Categories: cs.LG math.DS
\\ ( https://arxiv.org/abs/2402.09234 ,  4193kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09288
replaced with revised version Thu, 15 Feb 2024 16:30:12 GMT   (7915kb,D)

Title: EcoVal: An Efficient Data Valuation Framework for Machine Learning
Authors: Ayush K Tarun, Vikram S Chundawat, Murari Mandal, Hong Ming Tan, Bowei
  Chen, Mohan Kankanhalli
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.09288 ,  7915kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09345
replaced with revised version Thu, 15 Feb 2024 09:21:26 GMT   (41044kb,D)

Title: Mitigating Reward Hacking via Information-Theoretic Reward Modeling
Authors: Yuchun Miao, Sen Zhang, Liang Ding, Rong Bao, Lefei Zhang, Dacheng Tao
Categories: cs.LG cs.AI
Comments: 26 pages, 28 figures
\\ ( https://arxiv.org/abs/2402.09345 ,  41044kb)
------------------------------------------------------------------------------
\\
arXiv:2209.03910
replaced with revised version Wed, 14 Feb 2024 09:43:01 GMT   (2501kb,D)

Title: PixTrack: Precise 6DoF Object Pose Tracking using NeRF Templates and
  Feature-metric Alignment
Authors: Prajwal Chidananda, Saurabh Nair, Douglas Lee, Adrian Kaehler
Categories: cs.CV cs.AI cs.LG cs.RO
\\ ( https://arxiv.org/abs/2209.03910 ,  2501kb)
------------------------------------------------------------------------------
\\
arXiv:2302.04944
replaced with revised version Thu, 15 Feb 2024 17:43:25 GMT   (440kb,D)

Title: Learning Complex Teamwork Tasks Using a Given Sub-task Decomposition
Authors: Elliot Fosong, Arrasy Rahman, Ignacio Carlucho, Stefano V. Albrecht
Categories: cs.MA cs.AI cs.LG
\\ ( https://arxiv.org/abs/2302.04944 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2302.08012
replaced with revised version Wed, 14 Feb 2024 21:26:09 GMT   (693kb,D)

Title: Equilibrium of Data Markets with Externality
Authors: Safwan Hossain, Yiling Chen
Categories: cs.GT cs.AI
\\ ( https://arxiv.org/abs/2302.08012 ,  693kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12520
replaced with revised version Thu, 15 Feb 2024 15:42:02 GMT   (3744kb,D)

Title: SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly
Authors: Jordi Armengol-Estap\'e, Jackson Woodruff, Chris Cummins, Michael F.P.
  O'Boyle
Categories: cs.PL cs.AI
\\ ( https://arxiv.org/abs/2305.12520 ,  3744kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02552
replaced with revised version Thu, 15 Feb 2024 11:34:29 GMT   (2583kb,D)

Title: User Behavior Simulation with Large Language Model based Agents
Authors: Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu
  Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng
  Dou, Jun Wang, Ji-Rong Wen
Categories: cs.IR cs.AI
Comments: 28 pages, 9 figures
\\ ( https://arxiv.org/abs/2306.02552 ,  2583kb)
------------------------------------------------------------------------------
\\
arXiv:2307.11114 (*cross-listing*)
replaced with revised version Thu, 15 Feb 2024 02:32:21 GMT   (501kb)

Title: The Nature of Intelligence
Authors: Barco Jie You
Categories: q-bio.NC cs.AI
\\ ( https://arxiv.org/abs/2307.11114 ,  501kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13912
replaced with revised version Thu, 15 Feb 2024 00:41:00 GMT   (17030kb,D)

Title: Embedding Democratic Values into Social Media AIs via Societal Objective
  Functions
Authors: Chenyan Jia, Michelle S. Lam, Minh Chau Mai, Jeff Hancock, Michael S.
  Bernstein
Categories: cs.HC cs.AI
Comments: This paper has been accepted to CSCW 2024 and will be published in
  Proc. ACM Hum.-Comput. Interact. 8, CSCW1, Article 163 (April 2024)
Journal-ref: Proceedings of the ACM: Human-Computer Interaction, 8, CSCW1,
  Article 163 (2024)
DOI: 10.1145/3641002
\\ ( https://arxiv.org/abs/2307.13912 ,  17030kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08598 (*cross-listing*)
replaced with revised version Thu, 15 Feb 2024 05:52:25 GMT   (3541kb,D)

Title: Domain Generalization for Medical Image Analysis: A Survey
Authors: Jee Seok Yoon, Kwanseok Oh, Yooseung Shin, Maciej A. Mazurowski,
  Heung-Il Suk
Categories: eess.IV cs.AI cs.CV
\\ ( https://arxiv.org/abs/2310.08598 ,  3541kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09298
replaced with revised version Thu, 15 Feb 2024 00:11:48 GMT   (7692kb,D)

Title: ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency
  for Grayscale Image-based Network Intrusion Detection
Authors: Irfan Khan, Yasir Ali Farrukh and Syed Wali
Categories: cs.CR cs.AI cs.LG
Comments: 6 pages, 6 figures
\\ ( https://arxiv.org/abs/2310.09298 ,  7692kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17838
replaced with revised version Thu, 15 Feb 2024 18:56:41 GMT   (4837kb,D)

Title: Real-time Animation Generation and Control on Rigged Models via Large
  Language Models
Authors: Han Huang, Fernanda De La Torre, Cathy Mengying Fang, Andrzej
  Banburski-Fahey, Judith Amores, Jaron Lanier
Categories: cs.GR cs.AI
Comments: Accepted to NeurIPS Workshop on ML for Creativity and Design 2023
\\ ( https://arxiv.org/abs/2310.17838 ,  4837kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12573
replaced with revised version Thu, 15 Feb 2024 16:19:33 GMT   (8063kb,D)

Title: Moderating Model Marketplaces: Platform Governance Puzzles for AI
  Intermediaries
Authors: Robert Gorwa and Michael Veale
Categories: cs.CY cs.AI cs.LG
Journal-ref: (2024) 16(2) Law Innovation and Technology
\\ ( https://arxiv.org/abs/2311.12573 ,  8063kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03011
replaced with revised version Thu, 15 Feb 2024 16:38:46 GMT   (34539kb,D)

Title: InstructBooth: Instruction-following Personalized Text-to-Image
  Generation
Authors: Daewon Chae, Nokyung Park, Jinkyu Kim, Kimin Lee
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2312.03011 ,  34539kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11581
replaced with revised version Thu, 15 Feb 2024 16:21:11 GMT   (250kb,D)

Title: Protect Your Score: Contact Tracing With Differential Privacy Guarantees
Authors: Rob Romijnders, Christos Louizos, Yuki M. Asano, Max Welling
Categories: cs.CR cs.AI cs.LG
Comments: Accepted to The 38th Annual AAAI Conference on Artificial
  Intelligence (AAAI 2024)
\\ ( https://arxiv.org/abs/2312.11581 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14998
replaced with revised version Thu, 15 Feb 2024 10:53:31 GMT   (20166kb,D)

Title: Synthetic images aid the recognition of human-made art forgeries
Authors: Johann Ostmeyer, Ludovica Schaerf, Pavel Buividovich, Tessa Charles,
  Eric Postma, Carina Popovici
Categories: cs.CV cs.AI
Comments: 15 + 10 pages, 4 + 5 figures, 4 + 10 tables; van Gogh dataset
  available, DOI: https://doi.org/10.5281/zenodo.10276928
DOI: 10.1371/journal.pone.0295967
\\ ( https://arxiv.org/abs/2312.14998 ,  20166kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15106
replaced with revised version Thu, 15 Feb 2024 16:51:16 GMT   (644kb)

Title: Decision Theoretic Foundations for Experiments Evaluating Human
  Decisions
Authors: Jessica Hullman, Alex Kale, Jason Hartline
Categories: cs.HC cs.AI
\\ ( https://arxiv.org/abs/2401.15106 ,  644kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05902
replaced with revised version Wed, 14 Feb 2024 23:26:06 GMT   (1510kb)

Title: ClickSAM: Fine-tuning Segment Anything Model using click prompts for
  ultrasound image segmentation
Authors: Aimee Guo, Grace Fei, Hemanth Pasupuleti, and Jing Wang
Categories: cs.CV cs.AI physics.med-ph
Comments: 5 pages, 2 figures, SPIE Medical Imaging Conference 2024
\\ ( https://arxiv.org/abs/2402.05902 ,  1510kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06664
replaced with revised version Thu, 15 Feb 2024 01:20:07 GMT   (464kb,D)

Title: LLM Agents can Autonomously Hack Websites
Authors: Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, Daniel Kang
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2402.06664 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08777 (*cross-listing*)
replaced with revised version Thu, 15 Feb 2024 04:55:23 GMT   (5620kb,D)

Title: DNABERT-S: Learning Species-Aware DNA Embedding with Genome Foundation
  Models
Authors: Zhihan Zhou, Weimin Wu, Harrison Ho, Jiayi Wang, Lizhen Shi, Ramana V
  Davuluri, Zhong Wang, Han Liu
Categories: q-bio.GN cs.AI cs.CE cs.CL
\\ ( https://arxiv.org/abs/2402.08777 ,  5620kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09338 (*cross-listing*)
replaced with revised version Thu, 15 Feb 2024 12:07:13 GMT   (230kb,D)

Title: Neural Networks Asymptotic Behaviours for the Resolution of Inverse
  Problems
Authors: Luigi Del Debbio, Manuel Naviglio, Francesco Tarantelli
Categories: physics.comp-ph cs.AI hep-lat hep-th
\\ ( https://arxiv.org/abs/2402.09338 ,  230kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13915
replaced with revised version Thu, 15 Feb 2024 14:43:16 GMT   (6866kb,D)

Title: DAPR: A Benchmark on Document-Aware Passage Retrieval
Authors: Kexin Wang, Nils Reimers, Iryna Gurevych
Categories: cs.IR cs.CL
\\ ( https://arxiv.org/abs/2305.13915 ,  6866kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02037
replaced with revised version Thu, 15 Feb 2024 15:57:06 GMT   (963kb,D)

Title: EffiBench: Benchmarking the Efficiency of Automatically Generated Code
Authors: Dong Huang, Jie M.Zhang, Yuhao Qing, Heming Cui
Categories: cs.SE cs.CL
Comments: 26 pages, 13 figures, 18 tables
\\ ( https://arxiv.org/abs/2402.02037 ,  963kb)
------------------------------------------------------------------------------
\\
arXiv:2207.06944
replaced with revised version Wed, 14 Feb 2024 19:58:13 GMT   (768kb)

Title: Differentially Private Graph Learning via Sensitivity-Bounded
  Personalized PageRank
Authors: Alessandro Epasto, Vahab Mirrokni, Bryan Perozzi, Anton Tsitsulin,
  Peilin Zhong
Categories: cs.CR cs.LG cs.SI stat.ML
\\ ( https://arxiv.org/abs/2207.06944 ,  768kb)
------------------------------------------------------------------------------
\\
arXiv:2211.05920
replaced with revised version Thu, 15 Feb 2024 18:51:53 GMT   (2480kb,D)

Title: When Less is More: On the Value of "Co-training" for Semi-Supervised
  Software Defect Predictors
Authors: Suvodeep Majumder, Joymallya Chakraborty and Tim Menzies
Categories: cs.SE cs.LG
Comments: 36 pages, 10 figures, 5 tables
\\ ( https://arxiv.org/abs/2211.05920 ,  2480kb)
------------------------------------------------------------------------------
\\
arXiv:2211.14115 (*cross-listing*)
replaced with revised version Thu, 15 Feb 2024 18:50:16 GMT   (42kb,D)

Title: Inverse Feasibility in Over-the-Air Federated Learning
Authors: Tomasz Piotrowski, Rafail Ismayilov, Matthias Frey, Renato L.G.
  Cavalcante
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2211.14115 ,  42kb)
------------------------------------------------------------------------------
\\
arXiv:2211.16234
replaced with revised version Thu, 15 Feb 2024 12:12:05 GMT   (14599kb,D)

Title: SimCS: Simulation for Domain Incremental Online Continual Segmentation
Authors: Motasem Alfarra, Zhipeng Cai, Adel Bibi, Bernard Ghanem, Matthias
  M\"uller
Categories: cs.CV cs.LG
Comments: Accepted to AAAI Conference on Artificial Intelligence (AAAI'24)
\\ ( https://arxiv.org/abs/2211.16234 ,  14599kb)
------------------------------------------------------------------------------
\\
arXiv:2303.12167
replaced with revised version Thu, 15 Feb 2024 04:00:02 GMT   (2558kb,D)

Title: Gradient-descent hardware-aware training and deployment for mixed-signal
  Neuromorphic processors
Authors: U\u{g}urcan \c{C}akal, Maryada, Chenxi Wu, Ilkay Ulusoy, Dylan R. Muir
Categories: cs.ET cs.LG cs.NE
\\ ( https://arxiv.org/abs/2303.12167 ,  2558kb)
------------------------------------------------------------------------------
\\
arXiv:2304.04315 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 21:02:10 GMT   (8502kb,D)

Title: Microseismic source imaging using physics-informed neural networks with
  hard constraints
Authors: Xinquan Huang, Tariq Alkhalifah
Categories: physics.geo-ph cs.LG physics.comp-ph
Comments: IEEE Transactions on Geoscience and Remote Sensing 2024
DOI: 10.1109/tgrs.2024.3366449
\\ ( https://arxiv.org/abs/2304.04315 ,  8502kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16794
replaced with revised version Thu, 15 Feb 2024 18:16:43 GMT   (1622kb,D)

Title: Secure Vertical Federated Learning Under Unreliable Connectivity
Authors: Xinchi Qiu, Heng Pan, Wanru Zhao, Chenyang Ma, William F. Shen, Pedro
  P.B. Gusmao, Nicholas D. Lane
Categories: cs.CR cs.LG
Comments: Generalised extension from our previous work: arXiv:2305.11236
\\ ( https://arxiv.org/abs/2305.16794 ,  1622kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17000
replaced with revised version Thu, 15 Feb 2024 16:18:29 GMT   (500kb,D)

Title: DistriBlock: Identifying adversarial audio samples by leveraging
  characteristics of the output distribution
Authors: Mat\'ias P. Pizarro B., Dorothea Kolossa and Asja Fischer
Categories: cs.SD cs.CR cs.LG eess.AS
\\ ( https://arxiv.org/abs/2305.17000 ,  500kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03117 (*cross-listing*)
replaced with revised version Thu, 15 Feb 2024 16:59:42 GMT   (16382kb,D)

Title: Str2Str: A Score-based Framework for Zero-shot Protein Conformation
  Sampling
Authors: Jiarui Lu, Bozitao Zhong, Zuobai Zhang, Jian Tang
Categories: q-bio.QM cs.LG q-bio.BM
Comments: Published as a conference paper at ICLR 2024, see
  https://openreview.net/forum?id=C4BikKsgmK
\\ ( https://arxiv.org/abs/2306.03117 ,  16382kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04689 (*cross-listing*)
replaced with revised version Thu, 15 Feb 2024 00:55:03 GMT   (5523kb,D)

Title: Multiscale Flow for Robust and Optimal Cosmological Analysis
Authors: Biwei Dai and Uros Seljak
Categories: astro-ph.CO cs.LG physics.data-an
Comments: 14 pages, 7 figures. Comments welcome
\\ ( https://arxiv.org/abs/2306.04689 ,  5523kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14291
replaced with revised version Thu, 15 Feb 2024 15:55:05 GMT   (11856kb,D)

Title: Hyp-OW: Exploiting Hierarchical Structure Learning with Hyperbolic
  Distance Enhances Open World Object Detection
Authors: Thang Doan, Xin Li, Sima Behpour, Wenbin He, Liang Gou, Liu Ren
Categories: cs.CV cs.LG
Comments: Accepted at AAAI 2024 || keywords: Open World Object Detection,
  Hyperbolic Distance, Unknown Detection, Deformable Transformers, Hierarchical
  Representation Learning
\\ ( https://arxiv.org/abs/2306.14291 ,  11856kb)
------------------------------------------------------------------------------
\\
arXiv:2307.10187
replaced with revised version Thu, 15 Feb 2024 15:42:43 GMT   (115kb,D)

Title: Personalized Privacy Amplification via Importance Sampling
Authors: Dominik Fay, Sebastian Mair, Jens Sj\"olund
Categories: cs.CR cs.LG stat.ML
Comments: Preprint. Under review
\\ ( https://arxiv.org/abs/2307.10187 ,  115kb)
------------------------------------------------------------------------------
\\
arXiv:2308.06686
replaced with revised version Wed, 14 Feb 2024 22:28:12 GMT   (19840kb,D)

Title: TorchQL: A Programming Framework for Integrity Constraints in Machine
  Learning
Authors: Aaditya Naik, Adam Stein, Yinjun Wu, Mayur Naik, Eric Wong
Categories: cs.DB cs.LG cs.SE
\\ ( https://arxiv.org/abs/2308.06686 ,  19840kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00557 (*cross-listing*)
replaced with revised version Thu, 15 Feb 2024 17:44:24 GMT   (661kb,D)

Title: Concentrated Differential Privacy for Bandits
Authors: Achraf Azize, Debabrota Basu
Categories: stat.ML cs.CR cs.IT cs.LG math.IT math.ST stat.TH
Comments: Appears in IEEE SaTML 2024
\\ ( https://arxiv.org/abs/2309.00557 ,  661kb)
------------------------------------------------------------------------------
\\
arXiv:2309.03447 (*cross-listing*)
replaced with revised version Thu, 15 Feb 2024 02:18:13 GMT   (10984kb,D)

Title: Broadband Ground Motion Synthesis via Generative Adversarial Neural
  Operators: Development and Validation
Authors: Yaozhong Shi, Grigorios Lavrentiadis, Domniki Asimaki, Zachary E.
  Ross, Kamyar Azizzadenesheli
Categories: physics.geo-ph cs.LG
\\ ( https://arxiv.org/abs/2309.03447 ,  10984kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08607
replaced with revised version Wed, 14 Feb 2024 23:10:42 GMT   (31627kb,D)

Title: Monitoring of Urban Changes with multi-modal Sentinel 1 and 2 Data in
  Mariupol, Ukraine, in 2022/23
Authors: Georg Zitzlsberger and Michal Podhoranyi
Categories: cs.CY cs.CV cs.LG
Comments: Accepted for publication in IEEE Journal of Selected Topics in
  Applied Earth Observations and Remote Sensing
DOI: 10.1109/JSTARS.2024.3362688
\\ ( https://arxiv.org/abs/2309.08607 ,  31627kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09505 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 19:39:37 GMT   (783kb,D)

Title: Outlier-Insensitive Kalman Filtering: Theory and Applications
Authors: Shunit Truzman, Guy Revach, Nir Shlezinger, Itzik Klein
Categories: eess.SP cs.LG
\\ ( https://arxiv.org/abs/2309.09505 ,  783kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09599
replaced with revised version Thu, 15 Feb 2024 14:48:08 GMT   (18799kb,D)

Title: MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential
  Deep Learning
Authors: Helbert Paat, Qing Lian, Weilong Yao, Tong Zhang
Categories: cs.CV cs.LG cs.RO
Comments: Accepted at ICRA 2024. Code: https://github.com/paathelb/MEDL-U
\\ ( https://arxiv.org/abs/2309.09599 ,  18799kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00027 (*cross-listing*)
replaced with revised version Thu, 15 Feb 2024 18:23:41 GMT   (71kb)

Title: Out-Of-Domain Unlabeled Data Improves Generalization
Authors: Amir Hossein Saberi, Amir Najafi, Alireza Heidari, Mohammad Hosein
  Movasaghinia, Abolfazl Motahari, Babak H. Khalaj
Categories: stat.ML cs.LG
Comments: Published at ICLR 2024 (Spotlight), 29 pages, no figures
\\ ( https://arxiv.org/abs/2310.00027 ,  71kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01362
replaced with revised version Thu, 15 Feb 2024 02:07:33 GMT   (19575kb,D)

Title: Fleet Learning via Policy Merging
Authors: Lirui Wang, Kaiqing Zhang, Allan Zhou, Max Simchowitz, Russ Tedrake
Categories: cs.RO cs.LG
Comments: See the code https://github.com/liruiw/Fleet-Tools for more details
\\ ( https://arxiv.org/abs/2310.01362 ,  19575kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20699 (*cross-listing*)
replaced with revised version Wed, 14 Feb 2024 19:51:21 GMT   (1831kb,D)

Title: Bayesian Multistate Bennett Acceptance Ratio Methods
Authors: Xinqiang Ding
Categories: physics.chem-ph cs.LG physics.comp-ph physics.data-an stat.AP
DOI: 10.1021/acs.jctc.3c01212
\\ ( https://arxiv.org/abs/2310.20699 ,  1831kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06095
replaced with revised version Thu, 15 Feb 2024 17:05:14 GMT   (1975kb,D)

Title: Dual input stream transformer for vertical drift correction in
  eye-tracking reading data
Authors: Thomas M. Mercier, Marcin Budka, Martin R. Vasilev, Julie A. Kirkby,
  Bernhard Angele, Timothy J. Slattery
Categories: cs.CV cs.LG
Comments: This work has been submitted to the IEEE Transactions on pattern
  analysis and machine intelligence for possible publication. Copyright may be
  transferred without notice, after which this version may no longer be
  accessible
MSC-class: 91Cxx
ACM-class: J.4
\\ ( https://arxiv.org/abs/2311.06095 ,  1975kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17978
replaced with revised version Thu, 15 Feb 2024 14:04:05 GMT   (5824kb)

Title: AutArch: An AI-assisted workflow for object detection and automated
  recording in archaeological catalogues
Authors: Kevin Klein, Alyssa Wohde, Alexander V. Gorelik, Volker Heyd, Ralf
  L\"ammel, Yoan Diekmann, Maxime Brami
Categories: cs.CV cs.GR cs.LG
\\ ( https://arxiv.org/abs/2311.17978 ,  5824kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18237
replaced with revised version Thu, 15 Feb 2024 02:26:30 GMT   (11610kb,D)

Title: Knowledge Transfer from Vision Foundation Models for Efficient Training
  of Small Task-specific Models
Authors: Raviteja Vemulapalli, Hadi Pouransari, Fartash Faghri, Sachin Mehta,
  Mehrdad Farajtabar, Mohammad Rastegari, Oncel Tuzel
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2311.18237 ,  11610kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08489
replaced with revised version Thu, 15 Feb 2024 17:40:29 GMT   (26kb)

Title: Connectivity Oracles for Predictable Vertex Failures
Authors: Bingbing Hu, Evangelos Kosinas, Adam Polak
Categories: cs.DS cs.LG
\\ ( https://arxiv.org/abs/2312.08489 ,  26kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00176 (*cross-listing*)
replaced with revised version Thu, 15 Feb 2024 13:18:04 GMT   (181kb,D)

Title: Adversarial Quantum Machine Learning: An Information-Theoretic
  Generalization Analysis
Authors: Petros Georgiou, Sharu Theresa Jose and Osvaldo Simeone
Categories: quant-ph cs.ET cs.LG
Comments: 10 pages, 2 figures. Fixed a typo (wrong inequality sign) in lemma 2
  and extended to cover the whole range of values of p. Added reference on
  inequalities in trace norms
\\ ( https://arxiv.org/abs/2402.00176 ,  181kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02816
replaced with revised version Thu, 15 Feb 2024 09:19:41 GMT   (643kb,D)

Title: Intersectional Two-sided Fairness in Recommendation
Authors: Yifan Wang, Peijie Sun, Weizhi Ma, Min Zhang, Yuan Zhang, Peng Jiang,
  Shaoping Ma
Categories: cs.IR cs.CY cs.LG
Comments: accepted by WWW2024
DOI: 10.1145/3589334.3645518
\\ ( https://arxiv.org/abs/2402.02816 ,  643kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05067 (*cross-listing*)
replaced with revised version Thu, 15 Feb 2024 06:07:01 GMT   (1542kb,D)

Title: Multiscale Modelling with Physics-informed Neural Network: from
  Large-scale Dynamics to Small-scale Predictions in Complex Systems
Authors: Jing Wang and Zheng Li and Pengyu Lai and Rui Wang and Di Yang and
  Dewu Yang and Hui Xu
Categories: physics.flu-dyn cs.LG physics.comp-ph
\\ ( https://arxiv.org/abs/2402.05067 ,  1542kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08711 (*cross-listing*)
replaced with revised version Thu, 15 Feb 2024 08:34:41 GMT   (12kb)

Title: Correction to "Wasserstein distance estimates for the distributions of
  numerical approximations to ergodic stochastic differential equations"
Authors: Daniel Paulin, Peter A. Whalley
Categories: stat.ML cs.LG cs.NA math.NA math.PR
Comments: 8 pages
\\ ( https://arxiv.org/abs/2402.08711 ,  12kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08991 (*cross-listing*)
replaced with revised version Thu, 15 Feb 2024 04:30:09 GMT   (40kb)

Title: Towards Robust Model-Based Reinforcement Learning Against Adversarial
  Corruption
Authors: Chenlu Ye, Jiafan He, Quanquan Gu, Tong Zhang
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2402.08991 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09179
replaced with revised version Thu, 15 Feb 2024 06:15:02 GMT   (1567kb,D)

Title: Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model
  Customization
Authors: Rui Zhang, Hongwei Li, Rui Wen, Wenbo Jiang, Yuan Zhang, Michael
  Backes, Yun Shen, Yang Zhang
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2402.09179 ,  1567kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
