Gmail jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org> 2024年2月2日 16:54
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Wed 31 Jan 24 19:00:00 GMT  to  Thu  1 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.00015
Date: Thu, 28 Dec 2023 14:14:31 GMT   (83kb,D)

Title: Maintaining User Trust Through Multistage Uncertainty Aware Inference
Authors: Chandan Agrawal, Ashish Papanai, Jerome White
Categories: cs.AI cs.CV
Journal-ref: AAAI 2024 workshop of Deployable AI
\\
  This paper describes and evaluates a multistage approach to AI deployment.
Each stage involves a more accurate method of inference, yet engaging each
comes with an increasing cost. In outlining the architecture, we present a
method for quantifying model uncertainty that facilitates confident deferral
decisions. The architecture is currently under active deployment to thousands
of cotton farmers across India. The broader idea however is applicable to a
growing sector of AI deployments in challenging low resources settings.
\\ ( https://arxiv.org/abs/2402.00015 ,  83kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00041
Date: Sat, 20 Jan 2024 06:06:01 GMT   (327kb,D)

Title: Spatial-temporal-demand clustering for solving large-scale vehicle
  routing problems with time windows
Authors: Christoph Kerscher and Stefan Minner
Categories: cs.AI cs.LG cs.NE math.OC
\\
  Several metaheuristics use decomposition and pruning strategies to solve
large-scale instances of the vehicle routing problem (VRP). Those complexity
reduction techniques often rely on simple, problem-specific rules. However, the
growth in available data and advances in computer hardware enable data-based
approaches that use machine learning (ML) to improve scalability of solution
algorithms. We propose a decompose-route-improve (DRI) framework that groups
customers using clustering. Its similarity metric incorporates customers'
spatial, temporal, and demand data and is formulated to reflect the problem's
objective function and constraints. The resulting sub-routing problems can
independently be solved using any suitable algorithm. We apply pruned local
search (LS) between solved subproblems to improve the overall solution. Pruning
is based on customers' similarity information obtained in the decomposition
phase. In a computational study, we parameterize and compare existing
clustering algorithms and benchmark the DRI against the Hybrid Genetic Search
(HGS) of Vidal et al. (2013). Results show that our data-based approach
outperforms classic cluster-first, route-second approaches solely based on
customers' spatial information. The newly introduced similarity metric forms
separate sub-VRPs and improves the selection of LS moves in the improvement
phase. Thus, the DRI scales existing metaheuristics to achieve high-quality
solutions faster for large-scale VRPs by efficiently reducing complexity.
Further, the DRI can be easily adapted to various solution methods and VRP
characteristics, such as distribution of customer locations and demands, depot
location, and different time window scenarios, making it a generalizable
approach to solving routing problems.
\\ ( https://arxiv.org/abs/2402.00041 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00042
Date: Sat, 20 Jan 2024 12:12:14 GMT   (779kb)

Title: Optimized Task Assignment and Predictive Maintenance for Industrial
  Machines using Markov Decision Process
Authors: Ali Nasir, Samir Mekid, Zaid Sawlan, Omar Alsawafy
Categories: cs.AI cs.SY eess.SY
Comments: 19 pages, 11 figures, 3 tables
\\
  This paper considers a distributed decision-making approach for manufacturing
task assignment and condition-based machine health maintenance. Our approach
considers information sharing between the task assignment and health management
decision-making agents. We propose the design of the decision-making agents
based on Markov decision processes. The key advantage of using a Markov
decision process-based approach is the incorporation of uncertainty involved in
the decision-making process. The paper provides detailed mathematical models
along with the associated practical execution strategy. In order to demonstrate
the effectiveness and practical applicability of our proposed approach, we have
included a detailed numerical case study that is based on open source milling
machine tool degradation data. Our case study indicates that the proposed
approach offers flexibility in terms of the selection of cost parameters and it
allows for offline computation and analysis of the decision-making policy.
These features create and opportunity for the future work on learning of the
cost parameters associated with our proposed model using artificial
intelligence.
\\ ( https://arxiv.org/abs/2402.00042 ,  779kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00043
Date: Sat, 20 Jan 2024 21:25:57 GMT   (2498kb,D)

Title: Interactive and Intelligent Root Cause Analysis in Manufacturing with
  Causal Bayesian Networks and Knowledge Graphs
Authors: Christoph Wehner, Maximilian Kertel, Judith Wewerka
Categories: cs.AI cs.CE cs.LG
DOI: 10.1109/VTC2023-Spring57618.2023.10199563
\\
  Root Cause Analysis (RCA) in the manufacturing of electric vehicles is the
process of identifying fault causes. Traditionally, the RCA is conducted
manually, relying on process expert knowledge. Meanwhile, sensor networks
collect significant amounts of data in the manufacturing process. Using this
data for RCA makes it more efficient. However, purely data-driven methods like
Causal Bayesian Networks have problems scaling to large-scale, real-world
manufacturing processes due to the vast amount of potential cause-effect
relationships (CERs). Furthermore, purely data-driven methods have the
potential to leave out already known CERs or to learn spurious CERs. The paper
contributes by proposing an interactive and intelligent RCA tool that combines
expert knowledge of an electric vehicle manufacturing process and a data-driven
machine learning method. It uses reasoning over a large-scale Knowledge Graph
of the manufacturing process while learning a Causal Bayesian Network. In
addition, an Interactive User Interface enables a process expert to give
feedback to the root cause graph by adding and removing information to the
Knowledge Graph. The interactive and intelligent RCA tool reduces the learning
time of the Causal Bayesian Network while decreasing the number of spurious
CERs. Thus, the interactive and intelligent RCA tool closes the feedback loop
between expert and machine learning method.
\\ ( https://arxiv.org/abs/2402.00043 ,  2498kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00046
Date: Tue, 23 Jan 2024 12:30:49 GMT   (358kb,D)

Title: Introducing PetriRL: An Innovative Framework for JSSP Resolution
  Integrating Petri nets and Event-based Reinforcement Learning
Authors: Sofiene Lassoued, Andreas Schwung
Categories: cs.AI cs.LG
\\
  Quality scheduling in industrial job shops is crucial. Although neural
networks excel in solving these problems, their limited explainability hinders
their widespread industrial adoption. In this research, we introduce an
innovative framework for solving job shop scheduling problems (JSSP). Our
methodology leverages Petri nets to model the job shop, not only improving
explainability but also enabling direct incorporation of raw data without the
need to preprocess JSSP instances into disjunctive graphs. The Petri net, with
its controlling capacities, also governs the automated components of the
process, allowing the agent to focus on critical decision-making, particularly
resource allocation. The integration of event-based control and action masking
in our approach yields competitive performance on public test benchmarks.
Comparative analyses across a wide spectrum of optimization solutions,
including heuristics, metaheuristics, and learning-based algorithms, highlight
the competitiveness of our approach in large instances and its superiority over
all competitors in small to medium-sized scenarios. Ultimately, our approach
not only demonstrates a robust ability to generalize across various instance
sizes but also leverages the Petri net's graph nature to dynamically add job
operations during the inference phase without the need for agent retraining,
thereby enhancing flexibility.
\\ ( https://arxiv.org/abs/2402.00046 ,  358kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00048
Date: Wed, 24 Jan 2024 15:44:16 GMT   (325kb,D)

Title: IICONGRAPH: improved Iconographic and Iconological Statements in
  Knowledge Graphs
Authors: Bruno Sartini
Categories: cs.AI
Comments: 18 pages
\\
  Iconography and iconology are fundamental domains when it comes to
understanding artifacts of cultural heritage. Iconography deals with the study
and interpretation of visual elements depicted in artifacts and their
symbolism, while iconology delves deeper, exploring the underlying cultural and
historical meanings. Despite the advances in representing cultural heritage
with Linked Open Data (LOD), recent studies show persistent gaps in the
representation of iconographic and iconological statements in current knowledge
graphs (KGs). To address them, this paper presents IICONGRAPH, a KG that was
created by refining and extending the iconographic and iconological statements
of ArCo (the Italian KG of cultural heritage) and Wikidata. The development of
IICONGRAPH was also driven by a series of requirements emerging from research
case studies that were unattainable in the non-reengineered versions of the
KGs. The evaluation results demonstrate that IICONGRAPH not only outperforms
ArCo and Wikidata through domain-specific assessments from the literature but
also serves as a robust platform for addressing the formulated research
questions. IICONGRAPH is released and documented in accordance with the FAIR
principles to guarantee the resource's reusability. The algorithms used to
create it and assess the research questions have also been made available to
ensure transparency and reproducibility. While future work focuses on ingesting
more data into the KG, and on implementing it as a backbone of LLM-based
question answering systems, the current version of IICONGRAPH still emerges as
a valuable asset, contributing to the evolving landscape of cultural heritage
representation within Knowledge Graphs, the Semantic Web, and beyond.
\\ ( https://arxiv.org/abs/2402.00048 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00052
Date: Thu, 25 Jan 2024 12:52:42 GMT   (3225kb,D)

Title: Zero-shot Sequential Neuro-symbolic Reasoning for Automatically
  Generating Architecture Schematic Designs
Authors: Milin Kodnongbua, Lawrence H. Curtis, Adriana Schulz
Categories: cs.AI cs.CV cs.GR
\\
  This paper introduces a novel automated system for generating architecture
schematic designs aimed at streamlining complex decision-making at the
multifamily real estate development project's outset. Leveraging the combined
strengths of generative AI (neuro reasoning) and mathematical program solvers
(symbolic reasoning), the method addresses both the reliance on expert insights
and technical challenges in architectural schematic design. To address the
large-scale and interconnected nature of design decisions needed for designing
a whole building, we proposed a novel sequential neuro-symbolic reasoning
approach, emulating traditional architecture design processes from initial
concept to detailed layout. To remove the need to hand-craft a cost function to
approximate the desired objectives, we propose a solution that uses neuro
reasoning to generate constraints and cost functions that the symbolic solvers
can use to solve. We also incorporate feedback loops for each design stage to
ensure a tight integration between neuro and symbolic reasoning. Developed
using GPT-4 without further training, our method's effectiveness is validated
through comparative studies with real-world buildings. Our method can generate
various building designs in accordance with the understanding of the
neighborhood, showcasing its potential to transform the realm of architectural
schematic design.
\\ ( https://arxiv.org/abs/2402.00052 ,  3225kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00053
Date: Thu, 25 Jan 2024 15:44:46 GMT   (397kb,D)

Title: Are We Wasting Time? A Fast, Accurate Performance Evaluation Framework
  for Knowledge Graph Link Predictors
Authors: Filip Cornell, Yifei Jin, Jussi Karlgren, Sarunas Girdzijauskas
Categories: cs.AI cs.LG
\\
  The standard evaluation protocol for measuring the quality of Knowledge Graph
Completion methods - the task of inferring new links to be added to a graph -
typically involves a step which ranks every entity of a Knowledge Graph to
assess their fit as a head or tail of a candidate link to be added. In
Knowledge Graphs on a larger scale, this task rapidly becomes prohibitively
heavy. Previous approaches mitigate this problem by using random sampling of
entities to assess the quality of links predicted or suggested by a method.
However, we show that this approach has serious limitations since the ranking
metrics produced do not properly reflect true outcomes. In this paper, we
present a thorough analysis of these effects along with the following findings.
First, we empirically find and theoretically motivate why sampling uniformly at
random vastly overestimates the ranking performance of a method. We show that
this can be attributed to the effect of easy versus hard negative candidates.
Second, we propose a framework that uses relational recommenders to guide the
selection of candidates for evaluation. We provide both theoretical and
empirical justification of our methodology, and find that simple and fast
methods can work extremely well, and that they match advanced neural
approaches. Even when a large portion of true candidates for a property are
missed, the estimation barely deteriorates. With our proposed framework, we can
reduce the time and computation needed similar to random sampling strategies
while vastly improving the estimation; on ogbl-wikikg2, we show that accurate
estimations of the full, filtered ranking can be obtained in 20 seconds instead
of 30 minutes. We conclude that considerable computational effort can be saved
by effective preprocessing and sampling methods and still reliably predict
performance accurately of the true performance for the entire ranking
procedure.
\\ ( https://arxiv.org/abs/2402.00053 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00060
Date: Sun, 28 Jan 2024 15:39:29 GMT   (2483kb,D)

Title: Treatment of Epistemic Uncertainty in Conjunction Analysis with
  Dempster-Shafer Theory
Authors: Luis Sanchez and Massimiliano Vasile and Silvia Sanvido and Klaus
  Mertz and Christophe Taillan
Categories: cs.AI cs.IT math.IT math.PR
Comments: 34 pages, 22 figures
\\
  The paper presents an approach to the modelling of epistemic uncertainty in
Conjunction Data Messages (CDM) and the classification of conjunction events
according to the confidence in the probability of collision. The approach
proposed in this paper is based on the Dempster-Shafer Theory (DSt) of evidence
and starts from the assumption that the observed CDMs are drawn from a family
of unknown distributions. The Dvoretzky-Kiefer-Wolfowitz (DKW) inequality is
used to construct robust bounds on such a family of unknown distributions
starting from a time series of CDMs. A DSt structure is then derived from the
probability boxes constructed with DKW inequality. The DSt structure
encapsulates the uncertainty in the CDMs at every point along the time series
and allows the computation of the belief and plausibility in the realisation of
a given probability of collision. The methodology proposed in this paper is
tested on a number of real events and compared against existing practices in
the European and French Space Agencies.
\\ ( https://arxiv.org/abs/2402.00060 ,  2483kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00064
Date: Mon, 29 Jan 2024 11:34:59 GMT   (416kb)

Title: Merging plans with incomplete knowledge about actions and goals through
  an agent-based reputation system
Authors: Javier Carbo, Jose M Molina, Miguel A Patricio
Categories: cs.AI
DOI: 10.1016/j.eswa.2018.07.062
\\
  Managing transition plans is one of the major problems of people with
cognitive disabilities. Therefore, finding an automated way to generate such
plans would be a helpful tool for this community. In this paper we have
specifically proposed and compared different alternative ways to merge plans
formed by sequences of actions of unknown similarities between goals and
actions executed by several operator agents which cooperate between them
applying such actions over some passive elements (node agents) that require
additional executions of another plan after some time of use. Such ignorance of
the similarities between plan actions and goals would justify the use of a
distributed recommendation system that would provide an useful plan to be
applied for a certain goal to a given operator agent, generated from the known
results of previous executions of different plans by other operator agents.
Here we provide the general framework of execution (agent system), and the
different merging algorithms applied to this problem. The proposed agent system
would act as an useful cognitive assistant for people with intelectual
disabilities such as autism.
\\ ( https://arxiv.org/abs/2402.00064 ,  416kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00065
Date: Mon, 29 Jan 2024 16:29:35 GMT   (1059kb)

Title: A technical note for the 91-clauses SAT resolution with Indirect QAOA
  based approach
Authors: Gerard Fleury and Philippe Lacomme
Categories: cs.AI quant-ph
\\
  This paper addresses the resolution of the 3-SAT problem using a QAOA-like
approach. The chosen principle involves modeling the solution ranks of the
3-SAT problem, which, in this particular case, directly represent a solution.
This results in a highly compact circuit with few gates, enabling the modeling
of large-sized 3-SAT problems. Numerical experimentation demonstrates that the
approach can solve instances composed of 91 clauses and 20 variables with an
implementation based on Qiskit.
\\ ( https://arxiv.org/abs/2402.00065 ,  1059kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00076
Date: Tue, 30 Jan 2024 22:13:46 GMT   (18kb)

Title: Exploitation Strategies in Conditional Markov Chain Search: A case study
  on the three-index assignment problem
Authors: Sahil Patel and Daniel Karapetyan
Categories: cs.AI
Comments: 14 pages
\\
  The Conditional Markov Chain Search (CMCS) is a framework for automated
design of metaheuristics for discrete combinatorial optimisation problems.
Given a set of algorithmic components such as hill climbers and mutations, CMCS
decides in which order to apply those components. The decisions are dictated by
the CMCS configuration that can be learnt offline. CMCS does not have an
acceptance criterion; any moves are accepted by the framework. As a result, it
is particularly good in exploration but is not as good at exploitation. In this
study, we explore several extensions of the framework to improve its
exploitation abilities. To perform a computational study, we applied the
framework to the three-index assignment problem. The results of our experiments
showed that a two-stage CMCS is indeed superior to a single-stage CMCS.
\\ ( https://arxiv.org/abs/2402.00076 ,  18kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00083
Date: Wed, 31 Jan 2024 05:25:12 GMT   (1254kb,D)

Title: Modeling Access Differences to Reduce Disparity in Resource Allocation
Authors: Kenya Andrews and Mesrob Ohannessian and Tanya Berger-Wolf
Categories: cs.AI
Comments: Association for Computing Machinery (2022)
DOI: 10.1145/3551624.3555302
\\
  Motivated by COVID-19 vaccine allocation, where vulnerable subpopulations are
simultaneously more impacted in terms of health and more disadvantaged in terms
of access to the vaccine, we formalize and study the problem of resource
allocation when there are inherent access differences that correlate with
advantage and disadvantage. We identify reducing resource disparity as a key
goal in this context and show its role as a proxy to more nuanced downstream
impacts. We develop a concrete access model that helps quantify how a given
allocation translates to resource flow for the advantaged vs. the
disadvantaged, based on the access gap between them. We then provide a
methodology for access-aware allocation. Intuitively, the resulting allocation
leverages more vaccines in locations with higher vulnerable populations to
mitigate the access gap and reduce overall disparity. Surprisingly, knowledge
of the access gap is often not needed to perform access-aware allocation. To
support this formalism, we provide empirical evidence for our access model and
show that access-aware allocation can significantly reduce resource disparity
and thus improve downstream outcomes. We demonstrate this at various scales,
including at county, state, national, and global levels.
\\ ( https://arxiv.org/abs/2402.00083 ,  1254kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00262
Date: Thu, 1 Feb 2024 01:17:46 GMT   (1955kb)

Title: Computational Experiments Meet Large Language Model Based Agents: A
  Survey and Perspective
Authors: Qun Ma, Xiao Xue, Deyu Zhou, Xiangning Yu, Donghua Liu, Xuwen Zhang,
  Zihan Zhao, Yifan Shen, Peilin Ji, Juanjuan Li, Gang Wang, Wanpeng Ma
Categories: cs.AI
\\
  Computational experiments have emerged as a valuable method for studying
complex systems, involving the algorithmization of counterfactuals. However,
accurately representing real social systems in Agent-based Modeling (ABM) is
challenging due to the diverse and intricate characteristics of humans,
including bounded rationality and heterogeneity. To address this limitation,
the integration of Large Language Models (LLMs) has been proposed, enabling
agents to possess anthropomorphic abilities such as complex reasoning and
autonomous learning. These agents, known as LLM-based Agent, offer the
potential to enhance the anthropomorphism lacking in ABM. Nonetheless, the
absence of explicit explainability in LLMs significantly hinders their
application in the social sciences. Conversely, computational experiments excel
in providing causal analysis of individual behaviors and complex phenomena.
Thus, combining computational experiments with LLM-based Agent holds
substantial research potential. This paper aims to present a comprehensive
exploration of this fusion. Primarily, it outlines the historical development
of agent structures and their evolution into artificial societies, emphasizing
their importance in computational experiments. Then it elucidates the
advantages that computational experiments and LLM-based Agents offer each
other, considering the perspectives of LLM-based Agent for computational
experiments and vice versa. Finally, this paper addresses the challenges and
future trends in this research domain, offering guidance for subsequent related
studies.
\\ ( https://arxiv.org/abs/2402.00262 ,  1955kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00468
Date: Thu, 1 Feb 2024 10:15:39 GMT   (31368kb,D)

Title: RadDQN: a Deep Q Learning-based Architecture for Finding Time-efficient
  Minimum Radiation Exposure Pathway
Authors: Biswajit Sadhu, Trijit Sadhu, S. Anand
Categories: cs.AI
Comments: 12 pages, 7 main figures, code link (GitHub)
\\
  Recent advancements in deep reinforcement learning (DRL) techniques have
sparked its multifaceted applications in the automation sector. Managing
complex decision-making problems with DRL encourages its use in the nuclear
industry for tasks such as optimizing radiation exposure to the personnel
during normal operating conditions and potential accidental scenarios. However,
the lack of efficient reward function and effective exploration strategy
thwarted its implementation in the development of radiation-aware autonomous
unmanned aerial vehicle (UAV) for achieving maximum radiation protection. Here,
in this article, we address these intriguing issues and introduce a deep
Q-learning based architecture (RadDQN) that operates on a radiation-aware
reward function to provide time-efficient minimum radiation-exposure pathway in
a radiation zone. We propose a set of unique exploration strategies that
fine-tune the extent of exploration and exploitation based on the state-wise
variation in radiation exposure during training. Further, we benchmark the
predicted path with grid-based deterministic method. We demonstrate that the
formulated reward function in conjugation with adequate exploration strategy is
effective in handling several scenarios with drastically different radiation
field distributions. When compared to vanilla DQN, our model achieves a
superior convergence rate and higher training stability.
\\ ( https://arxiv.org/abs/2402.00468 ,  31368kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00485
Date: Thu, 1 Feb 2024 10:42:05 GMT   (45090kb,D)

Title: A Personalized Framework for Consumer and Producer Group Fairness
  Optimization in Recommender Systems
Authors: Hossein A. Rahmani, Mohammadmehdi Naghiaei, Yashar Deldjoo
Categories: cs.AI cs.CY cs.IR
Comments: TORS. arXiv admin note: substantial text overlap with
  arXiv:2204.08085
\\
  In recent years, there has been an increasing recognition that when machine
learning (ML) algorithms are used to automate decisions, they may mistreat
individuals or groups, with legal, ethical, or economic implications.
Recommender systems are prominent examples of these machine learning (ML)
systems that aid users in making decisions. The majority of past literature
research on RS fairness treats user and item fairness concerns independently,
ignoring the fact that recommender systems function in a two-sided marketplace.
In this paper, we propose CP-FairRank, an optimization-based re-ranking
algorithm that seamlessly integrates fairness constraints from both the
consumer and producer side in a joint objective framework. The framework is
generalizable and may take into account varied fairness settings based on group
segmentation, recommendation model selection, and domain, which is one of its
key characteristics. For instance, we demonstrate that the system may jointly
increase consumer and producer fairness when (un)protected consumer groups are
defined on the basis of their activity level and main-streamness, while
producer groups are defined according to their popularity level. For empirical
validation, through large-scale on eight datasets and four mainstream
collaborative filtering (CF) recommendation models, we demonstrate that our
proposed strategy is able to improve both consumer and producer fairness
without compromising or very little overall recommendation quality,
demonstrating the role algorithms may play in avoiding data biases.
\\ ( https://arxiv.org/abs/2402.00485 ,  45090kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00491
Date: Thu, 1 Feb 2024 10:57:00 GMT   (3813kb,D)

Title: EXMOS: Explanatory Model Steering Through Multifaceted Explanations and
  Data Configurations
Authors: Aditya Bhattacharya, Simone Stumpf, Lucija Gosak, Gregor Stiglic,
  Katrien Verbert
Categories: cs.AI cs.HC
Comments: This is a pre-print version only for early release. Please view the
  conference published version from ACM CHI 2024 to get the latest version of
  the paper
Journal-ref: Proceedings of the CHI Conference on Human Factors in Computing
  Systems (CHI '24), May 11--16, 2024, Honolulu, HI, USA
DOI: 10.1145/3613904.3642106
\\
  Explanations in interactive machine-learning systems facilitate debugging and
improving prediction models. However, the effectiveness of various global
model-centric and data-centric explanations in aiding domain experts to detect
and resolve potential data issues for model improvement remains unexplored.
This research investigates the influence of data-centric and model-centric
global explanations in systems that support healthcare experts in optimising
models through automated and manual data configurations. We conducted
quantitative (n=70) and qualitative (n=30) studies with healthcare experts to
explore the impact of different explanations on trust, understandability and
model improvement. Our results reveal the insufficiency of global model-centric
explanations for guiding users during data configuration. Although data-centric
explanations enhanced understanding of post-configuration system changes, a
hybrid fusion of both explanation types demonstrated the highest effectiveness.
Based on our study results, we also present design implications for effective
explanation-driven interactive machine-learning systems.
\\ ( https://arxiv.org/abs/2402.00491 ,  3813kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00591
Date: Thu, 1 Feb 2024 13:37:53 GMT   (795kb,D)

Title: Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations
Authors: Nicolas Lazzari, Stefano De Giorgis, Aldo Gangemi, Valentina Presutti
Categories: cs.AI
\\
  This paper presents sandra, a neuro-symbolic reasoner combining vectorial
representations with deductive reasoning. Sandra builds a vector space
constrained by an ontology and performs reasoning over it. The geometric nature
of the reasoner allows its combination with neural networks, bridging the gap
with symbolic knowledge representations. Sandra is based on the Description and
Situation (DnS) ontology design pattern, a formalization of frame semantics.
Given a set of facts (a situation) it allows to infer all possible perspectives
(descriptions) that can provide a plausible interpretation for it, even in
presence of incomplete information. We prove that our method is correct with
respect to the DnS model. We experiment with two different tasks and their
standard benchmarks, demonstrating that, without increasing complexity, sandra
(i) outperforms all the baselines (ii) provides interpretability in the
classification process, and (iii) allows control over the vector space, which
is designed a priori.
\\ ( https://arxiv.org/abs/2402.00591 ,  795kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00658
Date: Thu, 1 Feb 2024 15:18:33 GMT   (3027kb,D)

Title: Learning Planning-based Reasoning by Trajectories Collection and Process
  Reward Synthesizing
Authors: Fangkai Jiao, Chengwei Qin, Zhengyuan Liu, Nancy F. Chen, Shafiq Joty
Categories: cs.AI cs.CL
Comments: 14 pages, 8 figures
\\
  Large Language Models (LLMs) have demonstrated significant potential in
handling complex reasoning tasks through step-by-step rationale generation.
However, recent studies have raised concerns regarding the hallucination and
flaws in their reasoning process. Substantial efforts are being made to improve
the reliability and faithfulness of the generated rationales. Some approaches
model reasoning as planning, while others focus on annotating for process
supervision. Nevertheless, the planning-based search process often results in
high latency due to the frequent assessment of intermediate reasoning states
and the extensive exploration space. Additionally, supervising the reasoning
process with human annotation is costly and challenging to scale for LLM
training. To address these issues, in this paper, we propose a framework to
learn planning-based reasoning through direct preference optimization (DPO) on
collected trajectories, which are ranked according to synthesized process
rewards. Our results on challenging logical reasoning benchmarks demonstrate
the effectiveness of our learning framework, showing that our 7B model can
surpass the strong counterparts like GPT-3.5-Turbo.
\\ ( https://arxiv.org/abs/2402.00658 ,  3027kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00715
Date: Thu, 1 Feb 2024 16:09:19 GMT   (677kb,D)

Title: Intent Assurance using LLMs guided by Intent Drift
Authors: Kristina Dzeparoska, Ali Tizghadam, Alberto Leon-Garcia
Categories: cs.AI cs.NI stat.ME
\\
  Intent-Based Networking (IBN) presents a paradigm shift for network
management, by promising to align intents and business objectives with network
operations--in an automated manner. However, its practical realization is
challenging: 1) processing intents, i.e., translate, decompose and identify the
logic to fulfill the intent, and 2) intent conformance, that is, considering
dynamic networks, the logic should be adequately adapted to assure intents. To
address the latter, intent assurance is tasked with continuous verification and
validation, including taking the necessary actions to align the operational and
target states. In this paper, we define an assurance framework that allows us
to detect and act when intent drift occurs. To do so, we leverage AI-driven
policies, generated by Large Language Models (LLMs) which can quickly learn the
necessary in-context requirements, and assist with the fulfillment and
assurance of intents.
\\ ( https://arxiv.org/abs/2402.00715 ,  677kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00738
Date: Thu, 1 Feb 2024 16:37:21 GMT   (2084kb,D)

Title: FM3Q: Factorized Multi-Agent MiniMax Q-Learning for Two-Team Zero-Sum
  Markov Game
Authors: Guangzheng Hu, Yuanheng Zhu, Haoran Li, Dongbin Zhao
Categories: cs.AI
\\
  Many real-world applications involve some agents that fall into two teams,
with payoffs that are equal within the same team but of opposite sign across
the opponent team. The so-called two-team zero-sum Markov games (2t0sMGs) can
be resolved with reinforcement learning in recent years. However, existing
methods are thus inefficient in light of insufficient consideration of
intra-team credit assignment, data utilization and computational
intractability. In this paper, we propose the individual-global-minimax (IGMM)
principle to ensure the coherence between two-team minimax behaviors and the
individual greedy behaviors through Q functions in 2t0sMGs. Based on it, we
present a novel multi-agent reinforcement learning framework, Factorized
Multi-Agent MiniMax Q-Learning (FM3Q), which can factorize the joint minimax Q
function into individual ones and iteratively solve for the IGMM-satisfied
minimax Q functions for 2t0sMGs. Moreover, an online learning algorithm with
neural networks is proposed to implement FM3Q and obtain the deterministic and
decentralized minimax policies for two-team players. A theoretical analysis is
provided to prove the convergence of FM3Q. Empirically, we use three
environments to evaluate the learning efficiency and final performance of FM3Q
and show its superiority on 2t0sMGs.
\\ ( https://arxiv.org/abs/2402.00738 ,  2084kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00075
Date: Tue, 30 Jan 2024 22:07:12 GMT   (157kb,D)

Title: D-Nikud: Enhancing Hebrew Diacritization with LSTM and Pretrained Models
Authors: Adi Rosenthal and Nadav Shaked
Categories: cs.CL
\\
  D-Nikud, a novel approach to Hebrew diacritization that integrates the
strengths of LSTM networks and BERT-based (transformer) pre-trained model.
Inspired by the methodologies employed in Nakdimon, we integrate it with the
TavBERT pre-trained model, our system incorporates advanced architectural
choices and diverse training data. Our experiments showcase state-of-the-art
results on several benchmark datasets, with a particular emphasis on modern
texts and more specified diacritization like gender.
\\ ( https://arxiv.org/abs/2402.00075 ,  157kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00123
Date: Wed, 31 Jan 2024 19:07:37 GMT   (6584kb,D)

Title: Comparing Template-based and Template-free Language Model Probing
Authors: Sagi Shaier, Kevin Bennett, Lawrence E Hunter, Katharina von der Wense
Categories: cs.CL cs.LG
Comments: Accepted to EACL 2024
\\
  The differences between cloze-task language model (LM) probing with 1)
expert-made templates and 2) naturally-occurring text have often been
overlooked. Here, we evaluate 16 different LMs on 10 probing English datasets
-- 4 template-based and 6 template-free -- in general and biomedical domains to
answer the following research questions: (RQ1) Do model rankings differ between
the two approaches? (RQ2) Do models' absolute scores differ between the two
approaches? (RQ3) Do the answers to RQ1 and RQ2 differ between general and
domain-specific models? Our findings are: 1) Template-free and template-based
approaches often rank models differently, except for the top domain-specific
models. 2) Scores decrease by up to 42% Acc@1 when comparing parallel
template-free and template-based prompts. 3) Perplexity is negatively
correlated with accuracy in the template-free approach, but,
counter-intuitively, they are positively correlated for template-based probing.
4) Models tend to predict the same answers frequently across prompts for
template-based probing, which is less common when employing template-free
techniques.
\\ ( https://arxiv.org/abs/2402.00123 ,  6584kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00143
Date: Wed, 31 Jan 2024 19:48:58 GMT   (7032kb,D)

Title: Making a Long Story Short in Conversation Modeling
Authors: Yufei Tao, Tiernan Mines, Ameeta Agrawal
Categories: cs.CL cs.HC
Comments: This paper was accepted by TEICAI workshop at EACL 2024
\\
  Conversation systems accommodate diverse users with unique personalities and
distinct writing styles. Within the domain of multi-turn dialogue modeling,
this work studies the impact of varied utterance lengths on the quality of
subsequent responses generated by conversation models. Using GPT-3 as the base
model, multiple dialogue datasets, and several metrics, we conduct a thorough
exploration of this aspect of conversational models. Our analysis sheds light
on the complex relationship between utterance lengths and the quality of
follow-up responses generated by dialogue systems. Empirical findings suggests
that, for certain types of conversations, utterance lengths can be reduced by
up to 72% without any noticeable difference in the quality of follow-up
responses.
\\ ( https://arxiv.org/abs/2402.00143 ,  7032kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00149
Date: Wed, 31 Jan 2024 20:07:43 GMT   (7370kb,D)

Title: The Impact of Language Adapters in Cross-Lingual Transfer for NLU
Authors: Jenny Kunz, Oskar Holmstr\"om
Categories: cs.CL
\\
  Modular deep learning has been proposed for the efficient adaption of
pre-trained models to new tasks, domains and languages. In particular,
combining language adapters with task adapters has shown potential where no
supervised data exists for a language. In this paper, we explore the role of
language adapters in zero-shot cross-lingual transfer for natural language
understanding (NLU) benchmarks. We study the effect of including a
target-language adapter in detailed ablation studies with two multilingual
models and three multilingual datasets. Our results show that the effect of
target-language adapters is highly inconsistent across tasks, languages and
models. Retaining the source-language adapter instead often leads to an
equivalent, and sometimes to a better, performance. Removing the language
adapter after training has only a weak negative effect, indicating that the
language adapters do not have a strong impact on the predictions.
\\ ( https://arxiv.org/abs/2402.00149 ,  7370kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00157
Date: Wed, 31 Jan 2024 20:26:32 GMT   (6944kb,D)

Title: Large Language Models for Mathematical Reasoning: Progresses and
  Challenges
Authors: Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, Wenpeng Yin
Categories: cs.CL
Comments: EACL 2024 Student Research Workshop, 8 pages
\\
  Mathematical reasoning serves as a cornerstone for assessing the fundamental
cognitive capabilities of human intelligence. In recent times, there has been a
notable surge in the development of Large Language Models (LLMs) geared towards
the automated resolution of mathematical problems. However, the landscape of
mathematical problem types is vast and varied, with LLM-oriented techniques
undergoing evaluation across diverse datasets and settings. This diversity
makes it challenging to discern the true advancements and obstacles within this
burgeoning field. This survey endeavors to address four pivotal dimensions: i)
a comprehensive exploration of the various mathematical problems and their
corresponding datasets that have been investigated; ii) an examination of the
spectrum of LLM-oriented techniques that have been proposed for mathematical
problem-solving; iii) an overview of factors and concerns affecting LLMs in
solving math; and iv) an elucidation of the persisting challenges within this
domain. To the best of our knowledge, this survey stands as one of the first
extensive examinations of the landscape of LLMs in the realm of mathematics,
providing a holistic perspective on the current state, accomplishments, and
future challenges in this rapidly evolving field.
\\ ( https://arxiv.org/abs/2402.00157 ,  6944kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00159
Date: Wed, 31 Jan 2024 20:29:50 GMT   (7715kb,D)

Title: Dolma: an Open Corpus of Three Trillion Tokens for Language Model
  Pretraining Research
Authors: Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David
  Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai
  Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu,
  Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha
  Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson,
  Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh,
  Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk
  Groeneveld, Jesse Dodge, Kyle Lo
Categories: cs.CL
Comments: Dataset available at: https://huggingface.co/datasets/allenai/dolma
\\
  Language models have become a critical technology to tackling a wide range of
natural language processing tasks, yet many details about how the
best-performing language models were developed are not reported. In particular,
information about their pretraining corpora is seldom discussed: commercial
language models rarely provide any information about their data; even open
models rarely release datasets they are trained on, or an exact recipe to
reproduce them. As a result, it is challenging to conduct certain threads of
language modeling research, such as understanding how training data impacts
model capabilities and shapes their limitations. To facilitate open research on
language model pretraining, we release Dolma, a three trillion tokens English
corpus, built from a diverse mixture of web content, scientific papers, code,
public-domain books, social media, and encyclopedic materials. In addition, we
open source our data curation toolkit to enable further experimentation and
reproduction of our work. In this report, we document Dolma, including its
design principles, details about its construction, and a summary of its
contents. We interleave this report with analyses and experimental results from
training language models on intermediate states of Dolma to share what we have
learned about important data curation practices, including the role of content
or quality filters, deduplication, and multi-source mixing. Dolma has been used
to train OLMo, a state-of-the-art, open language model and framework designed
to build and study the science of language modeling.
\\ ( https://arxiv.org/abs/2402.00159 ,  7715kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00160
Date: Wed, 31 Jan 2024 20:31:56 GMT   (250kb,D)

Title: Multimodal Clinical Pseudo-notes for Emergency Department Prediction
  Tasks using Multiple Embedding Model for EHR (MEME)
Authors: Simon A. Lee, Sujay Jain, Alex Chen, Arabdha Biswas, Jennifer Fang,
  Akos Rudas, Jeffrey N. Chiang
Categories: cs.CL
Comments: ICML Submission. However it is under review until May
\\
  In this work, we introduce Multiple Embedding Model for EHR (MEME), an
approach that views Electronic Health Records (EHR) as multimodal data. This
approach incorporates "pseudo-notes", textual representations of tabular EHR
concepts such as diagnoses and medications, and allows us to effectively employ
Large Language Models (LLMs) for EHR representation. This framework also adopts
a multimodal approach, embedding each EHR modality separately. We demonstrate
the effectiveness of MEME by applying it to several tasks within the Emergency
Department across multiple hospital systems. Our findings show that MEME
surpasses the performance of both single modality embedding methods and
traditional machine learning approaches. However, we also observe notable
limitations in generalizability across hospital institutions for all tested
models.
\\ ( https://arxiv.org/abs/2402.00160 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00179
Date: Wed, 31 Jan 2024 21:14:01 GMT   (603kb,D)

Title: De-identification is not always enough
Authors: Atiquer Rahman Sarkar, Yao-Shun Chuang, Noman Mohammed, Xiaoqian Jiang
Categories: cs.CL
\\
  For sharing privacy-sensitive data, de-identification is commonly regarded as
adequate for safeguarding privacy. Synthetic data is also being considered as a
privacy-preserving alternative. Recent successes with numerical and tabular
data generative models and the breakthroughs in large generative language
models raise the question of whether synthetically generated clinical notes
could be a viable alternative to real notes for research purposes. In this
work, we demonstrated that (i) de-identification of real clinical notes does
not protect records against a membership inference attack, (ii) proposed a
novel approach to generate synthetic clinical notes using the current
state-of-the-art large language models, (iii) evaluated the performance of the
synthetically generated notes in a clinical domain task, and (iv) proposed a
way to mount a membership inference attack where the target model is trained
with synthetic data. We observed that when synthetically generated notes
closely match the performance of real data, they also exhibit similar privacy
concerns to the real data. Whether other approaches to synthetically generated
clinical notes could offer better trade-offs and become a better alternative to
sensitive real notes warrants further investigation.
\\ ( https://arxiv.org/abs/2402.00179 ,  603kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00235
Date: Wed, 31 Jan 2024 23:29:42 GMT   (366kb,D)

Title: Exploring the limits of decoder-only models trained on public speech
  recognition corpora
Authors: Ankit Gupta, George Saon, Brian Kingsbury
Categories: cs.CL cs.SD eess.AS
\\
  The emergence of industrial-scale speech recognition (ASR) models such as
Whisper and USM, trained on 1M hours of weakly labelled and 12M hours of audio
only proprietary data respectively, has led to a stronger need for large scale
public ASR corpora and competitive open source pipelines. Unlike the said
models, large language models are typically based on Transformer decoders, and
it remains unclear if decoder-only models trained on public data alone can
deliver competitive performance. In this work, we investigate factors such as
choice of training datasets and modeling components necessary for obtaining the
best performance using public English ASR corpora alone. Our Decoder-Only
Transformer for ASR (DOTA) model comprehensively outperforms the
encoder-decoder open source replication of Whisper (OWSM) on nearly all English
ASR benchmarks and outperforms Whisper large-v3 on 7 out of 15 test sets. We
release our codebase and model checkpoints under permissive license.
\\ ( https://arxiv.org/abs/2402.00235 ,  366kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00263
Date: Thu, 1 Feb 2024 01:23:07 GMT   (407kb,D)

Title: Does \textsc{DetectGPT} Fully Utilize Perturbation? Selective
  Perturbation on Model-Based Contrastive Learning Detector would be Better
Authors: Shengchao Liu, Xiaoming Liu, Yichen Wang, Zehua Cheng, Chengzhengxu
  Li, Zhaohan Zhang, Yu Lan, Chao Shen
Categories: cs.CL
\\
  The burgeoning capabilities of large language models (LLMs) have raised
growing concerns about abuse. DetectGPT, a zero-shot metric-based unsupervised
machine-generated text detector, first introduces perturbation and shows great
performance improvement. However, DetectGPT's random perturbation strategy
might introduce noise, limiting the distinguishability and further performance
improvements. Moreover, its logit regression module relies on setting the
threshold, which harms the generalizability and applicability of individual or
small-batch inputs. Hence, we propose a novel detector, \modelname{}, which
uses selective strategy perturbation to relieve the important information loss
caused by random masking, and multi-pair contrastive learning to capture the
implicit pattern information during perturbation, facilitating few-shot
performance. The experiments show that \modelname{} outperforms the SOTA method
by 1.20\% in accuracy on average on four public datasets. We further analyze
the effectiveness, robustness, and generalization of our perturbation method.
\\ ( https://arxiv.org/abs/2402.00263 ,  407kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00271
Date: Thu, 1 Feb 2024 01:45:46 GMT   (3693kb,D)

Title: A Crucial Parameter for Rank-Frequency Relation in Natural Languages
Authors: Chenchen Ding
Categories: cs.CL
\\
  $f \propto r^{-\alpha} \cdot (r+\gamma)^{-\beta}$ has been empirically shown
more precise than a na\"ive power law $f\propto r^{-\alpha}$ to model the
rank-frequency ($r$-$f$) relation of words in natural languages. This work
shows that the only crucial parameter in the formulation is $\gamma$, which
depicts the resistance to vocabulary growth on a corpus. A method of parameter
estimation by searching an optimal $\gamma$ is proposed, where a ``zeroth
word'' is introduced technically for the calculation. The formulation and
parameters are further discussed with several case studies.
\\ ( https://arxiv.org/abs/2402.00271 ,  3693kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00322
Date: Thu, 1 Feb 2024 04:15:59 GMT   (7568kb,D)

Title: Bias in Opinion Summarisation from Pre-training to Adaptation: A Case
  Study in Political Bias
Authors: Nannan Huang, Haytham Fayek, Xiuzhen Zhang
Categories: cs.CL
Comments: 15 pages, 1 figure, 6 tables, Accepted to EACL 2024
\\
  Opinion summarisation aims to summarise the salient information and opinions
presented in documents such as product reviews, discussion forums, and social
media texts into short summaries that enable users to effectively understand
the opinions therein. Generating biased summaries has the risk of potentially
swaying public opinion. Previous studies focused on studying bias in opinion
summarisation using extractive models, but limited research has paid attention
to abstractive summarisation models. In this study, using political bias as a
case study, we first establish a methodology to quantify bias in abstractive
models, then trace it from the pre-trained models to the task of summarising
social media opinions using different models and adaptation methods. We find
that most models exhibit intrinsic bias. Using a social media text
summarisation dataset and contrasting various adaptation methods, we find that
tuning a smaller number of parameters is less biased compared to standard
fine-tuning; however, the diversity of topics in training data used for
fine-tuning is critical.
\\ ( https://arxiv.org/abs/2402.00322 ,  7568kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00345
Date: Thu, 1 Feb 2024 05:20:07 GMT   (312kb,D)

Title: IndiVec: An Exploration of Leveraging Large Language Models for Media
  Bias Detection with Fine-Grained Bias Indicators
Authors: Luyang Lin, Lingzhi Wang, Xiaoyan Zhao, Jing Li, Kam-Fai Wong
Categories: cs.CL
\\
  This study focuses on media bias detection, crucial in today's era of
influential social media platforms shaping individual attitudes and opinions.
In contrast to prior work that primarily relies on training specific models
tailored to particular datasets, resulting in limited adaptability and subpar
performance on out-of-domain data, we introduce a general bias detection
framework, IndiVec, built upon large language models. IndiVec begins by
constructing a fine-grained media bias database, leveraging the robust
instruction-following capabilities of large language models and vector database
techniques. When confronted with new input for bias detection, our framework
automatically selects the most relevant indicator from the vector database and
employs majority voting to determine the input's bias label. IndiVec excels
compared to previous methods due to its adaptability (demonstrating consistent
performance across diverse datasets from various sources) and explainability
(providing explicit top-k indicators to interpret bias predictions).
Experimental results on four political bias datasets highlight IndiVec's
significant superiority over baselines. Furthermore, additional experiments and
analysis provide profound insights into the framework's effectiveness.
\\ ( https://arxiv.org/abs/2402.00345 ,  312kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00367
Date: Thu, 1 Feb 2024 06:11:49 GMT   (5072kb,D)

Title: Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM
  Collaboration
Authors: Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha
  Balachandran, Yulia Tsvetkov
Categories: cs.CL
\\
  Despite efforts to expand the knowledge of large language models (LLMs),
knowledge gaps -- missing or outdated information in LLMs -- might always
persist given the evolving nature of knowledge. In this work, we study
approaches to identify LLM knowledge gaps and abstain from answering questions
when knowledge gaps are present. We first adapt existing approaches to model
calibration or adaptation through fine-tuning/prompting and analyze their
ability to abstain from generating low-confidence outputs. Motivated by their
failures in self-reflection and over-reliance on held-out sets, we propose two
novel approaches that are based on model collaboration, i.e., LLMs probing
other LLMs for knowledge gaps, either cooperatively or competitively. Extensive
experiments with three LLMs on four QA tasks featuring diverse knowledge
domains demonstrate that both cooperative and competitive approaches to
unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain
accuracy against the strongest baseline. Further analysis reveals that our
proposed mechanisms could help identify failure cases in retrieval augmentation
and pinpoint knowledge gaps in multi-hop reasoning.
\\ ( https://arxiv.org/abs/2402.00367 ,  5072kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00371
Date: Thu, 1 Feb 2024 06:21:19 GMT   (595kb,D)

Title: What Does the Bot Say? Opportunities and Risks of Large Language Models
  in Social Media Bot Detection
Authors: Shangbin Feng, Herun Wan, Ningnan Wang, Zhaoxuan Tan, Minnan Luo,
  Yulia Tsvetkov
Categories: cs.CL
\\
  Social media bot detection has always been an arms race between advancements
in machine learning bot detectors and adversarial bot strategies to evade
detection. In this work, we bring the arms race to the next level by
investigating the opportunities and risks of state-of-the-art large language
models (LLMs) in social bot detection. To investigate the opportunities, we
design novel LLM-based bot detectors by proposing a
mixture-of-heterogeneous-experts framework to divide and conquer diverse user
information modalities. To illuminate the risks, we explore the possibility of
LLM-guided manipulation of user textual and structured information to evade
detection. Extensive experiments with three LLMs on two datasets demonstrate
that instruction tuning on merely 1,000 annotated examples produces specialized
LLMs that outperform state-of-the-art baselines by up to 9.1% on both datasets,
while LLM-guided manipulation strategies could significantly bring down the
performance of existing bot detectors by up to 29.6% and harm the calibration
and reliability of bot detection systems.
\\ ( https://arxiv.org/abs/2402.00371 ,  595kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00385
Date: Thu, 1 Feb 2024 07:05:45 GMT   (9236kb,D)

Title: Computational Morphology and Lexicography Modeling of Modern Standard
  Arabic Nominals
Authors: Christian Khairallah, Reham Marzouk, Salam Khalifa, Mayar Nassar, and
  Nizar Habash
Categories: cs.CL
Comments: Findings of the Association for Computational Linguistics: EACL 2024
\\
  Modern Standard Arabic (MSA) nominals present many morphological and lexical
modeling challenges that have not been consistently addressed previously. This
paper attempts to define the space of such challenges, and leverage a recently
proposed morphological framework to build a comprehensive and extensible model
for MSA nominals. Our model design addresses the nominals' intricate
morphotactics, as well as their paradigmatic irregularities. Our implementation
showcases enhanced accuracy and consistency compared to a commonly used MSA
morphological analyzer and generator. We make our models publicly available.
\\ ( https://arxiv.org/abs/2402.00385 ,  9236kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00402
Date: Thu, 1 Feb 2024 07:48:50 GMT   (1894kb,D)

Title: Investigating Bias Representations in Llama 2 Chat via Activation
  Steering
Authors: Dawn Lu, Nina Rimsky
Categories: cs.CL cs.AI
\\
  We address the challenge of societal bias in Large Language Models (LLMs),
focusing on the Llama 2 7B Chat model. As LLMs are increasingly integrated into
decision-making processes with substantial societal impact, it becomes
imperative to ensure these models do not reinforce existing biases. Our
approach employs activation steering to probe for and mitigate biases related
to gender, race, and religion. This method manipulates model activations to
direct responses towards or away from biased outputs, utilizing steering
vectors derived from the StereoSet dataset and custom GPT4 generated gender
bias prompts. Our findings reveal inherent gender bias in Llama 2 7B Chat,
persisting even after Reinforcement Learning from Human Feedback (RLHF). We
also observe a predictable negative correlation between bias and the model's
tendency to refuse responses. Significantly, our study uncovers that RLHF tends
to increase the similarity in the model's representation of different forms of
societal biases, which raises questions about the model's nuanced understanding
of different forms of bias. This work also provides valuable insights into
effective red-teaming strategies for LLMs using activation steering,
particularly emphasizing the importance of integrating a refusal vector.
\\ ( https://arxiv.org/abs/2402.00402 ,  1894kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00412
Date: Thu, 1 Feb 2024 08:11:56 GMT   (67kb,D)

Title: Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated
  Student Essay Detection
Authors: Xinlin Peng, Ying Zhou, Ben He, Le Sun, Yingfei Sun
Categories: cs.CL cs.AI
Comments: Accepted by EMNLP 2023 Main conference, Oral Presentation
\\
  Large language models (LLMs) have exhibited remarkable capabilities in text
generation tasks. However, the utilization of these models carries inherent
risks, including but not limited to plagiarism, the dissemination of fake news,
and issues in educational exercises. Although several detectors have been
proposed to address these concerns, their effectiveness against adversarial
perturbations, specifically in the context of student essay writing, remains
largely unexplored. This paper aims to bridge this gap by constructing
AIG-ASAP, an AI-generated student essay dataset, employing a range of text
perturbation methods that are expected to generate high-quality essays while
evading detection. Through empirical experiments, we assess the performance of
current AIGC detectors on the AIG-ASAP dataset. The results reveal that the
existing detectors can be easily circumvented using straightforward automatic
adversarial attacks. Specifically, we explore word substitution and sentence
substitution perturbation methods that effectively evade detection while
maintaining the quality of the generated essays. This highlights the urgent
need for more accurate and robust methods to detect AI-generated student essays
in the education domain.
\\ ( https://arxiv.org/abs/2402.00412 ,  67kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00414
Date: Thu, 1 Feb 2024 08:15:28 GMT   (4303kb,D)

Title: Prompt-Time Symbolic Knowledge Capture with Large Language Models
Authors: Tolga \c{C}\"opl\"u, Arto Bendiken, Andrii Skomorokhov, Eduard
  Bateiko, Stephen Cobb, Joshua J. Bouw (Haltia, Inc.)
Categories: cs.CL cs.AI
Comments: 8 pages, 5 figures, 1 table preprint. Under review
ACM-class: I.2.7
\\
  Augmenting large language models (LLMs) with user-specific knowledge is
crucial for real-world applications, such as personal AI assistants. However,
LLMs inherently lack mechanisms for prompt-driven knowledge capture. This paper
investigates utilizing the existing LLM capabilities to enable prompt-driven
knowledge capture, with a particular emphasis on knowledge graphs. We address
this challenge by focusing on prompt-to-triple (P2T) generation. We explore
three methods: zero-shot prompting, few-shot prompting, and fine-tuning, and
then assess their performance via a specialized synthetic dataset. Our code and
datasets are publicly available at https://github.com/HaltiaAI/paper-PTSKC.
\\ ( https://arxiv.org/abs/2402.00414 ,  4303kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00421
Date: Thu, 1 Feb 2024 08:37:13 GMT   (1172kb,D)

Title: From PARIS to LE-PARIS: Toward Patent Response Automation with
  Recommender Systems and Collaborative Large Language Models
Authors: Jung-Mei Chu, Hao-Cheng Lo, Jieh Hsiang, and Chun-Chieh Cho
Categories: cs.CL cs.HC cs.IR cs.LG
Comments: 14 pages, 4 figures, summitted to a journal
\\
  In patent prosecution, timely and effective responses to Office Actions (OAs)
are crucial for acquiring patents, yet past automation and AI research have
scarcely addressed this aspect. To address this gap, our study introduces the
Patent Office Action Response Intelligence System (PARIS) and its advanced
version, the Large Language Model Enhanced PARIS (LE-PARIS). These systems are
designed to expedite the efficiency of patent attorneys in collaboratively
handling OA responses. The systems' key features include the construction of an
OA Topics Database, development of Response Templates, and implementation of
Recommender Systems and LLM-based Response Generation. Our validation involves
a multi-paradigmatic analysis using the USPTO Office Action database and
longitudinal data of attorney interactions with our systems over six years.
Through five studies, we examine the constructiveness of OA topics (studies 1
and 2) using topic modeling and the proposed Delphi process, the efficacy of
our proposed hybrid recommender system tailored for OA (both LLM-based and
non-LLM-based) (study 3), the quality of response generation (study 4), and the
practical value of the systems in real-world scenarios via user studies (study
5). Results demonstrate that both PARIS and LE-PARIS significantly meet key
metrics and positively impact attorney performance.
\\ ( https://arxiv.org/abs/2402.00421 ,  1172kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00446
Date: Thu, 1 Feb 2024 09:24:33 GMT   (8452kb,D)

Title: Improving Dialog Safety using Socially Aware Contrastive Learning
Authors: Souvik Das, Rohini K. Srihari
Categories: cs.CL
Comments: SCI-CHAT@EACL2024
\\
  State-of-the-art conversational AI systems raise concerns due to their
potential risks of generating unsafe, toxic, unethical, or dangerous content.
Previous works have developed datasets to teach conversational agents the
appropriate social paradigms to respond effectively to specifically designed
hazardous content. However, models trained on these adversarial datasets still
struggle to recognize subtle unsafe situations that appear naturally in
conversations or introduce an inappropriate response in a casual context. To
understand the extent of this problem, we study prosociality in both
adversarial and casual dialog contexts and audit the response quality of
general-purpose language models in terms of propensity to produce unsafe
content. We propose a dual-step fine-tuning process to address these issues
using a socially aware n-pair contrastive loss. Subsequently, we train a base
model that integrates prosocial behavior by leveraging datasets like Moral
Integrity Corpus (MIC) and ProsocialDialog. Experimental results on several
dialog datasets demonstrate the effectiveness of our approach in generating
socially appropriate responses.
\\ ( https://arxiv.org/abs/2402.00446 ,  8452kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00474
Date: Thu, 1 Feb 2024 10:26:27 GMT   (1008kb,D)

Title: SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection
  Framework for Large Language Models
Authors: Tianhan Xu, Zhe Hu, Ling Chen, Bin Li
Categories: cs.CL cs.AI
\\
  Recent advances in large language models (LLMs) have demonstrated exceptional
performance in various natural language processing (NLP) tasks. However, their
effective application in the medical domain is hampered by a lack of medical
domain knowledge. In this study, we present SA-MDKIF, a scalable and adaptable
framework that aims to inject medical knowledge into general-purpose LLMs
through instruction tuning, thereby enabling adaptability for various
downstream tasks. SA-MDKIF consists of two stages: skill training and skill
adaptation. In the first stage, we define 12 basic medical skills and use
AdaLoRA to train these skills based on uniformly formatted instructional
datasets that we have constructed. In the next stage, we train the skill router
using task-specific downstream data and use this router to integrate the
acquired skills with LLMs during inference. Experimental results on 9 different
medical tasks show that SA-MDKIF improves performance by 10-20% compared to the
original LLMs. Notably, this improvement is particularly pronounced for unseen
medical tasks, showing an improvement of up to 30%.
\\ ( https://arxiv.org/abs/2402.00474 ,  1008kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00530
Date: Thu, 1 Feb 2024 11:57:53 GMT   (2226kb,D)

Title: Superfiltering: Weak-to-Strong Data Filtering for Fast
  Instruction-Tuning
Authors: Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang,
  Ning Cheng, Tianyi Zhou
Categories: cs.CL
\\
  Instruction tuning is critical to improve LLMs but usually suffers from
low-quality and redundant data. Data filtering for instruction tuning has
proved important in improving both the efficiency and performance of the tuning
process. But it also leads to extra cost and computation due to the involvement
of LLMs in this process. To reduce the filtering cost, we study Superfiltering:
Can we use a smaller and weaker model to select data for finetuning a larger
and stronger model? Despite the performance gap between weak and strong
language models, we find their highly consistent capability to perceive
instruction difficulty and data selection results. This enables us to use a
much smaller and more efficient model to filter the instruction data used to
train a larger language model. Not only does it largely speed up the data
filtering, but the filtered-data-finetuned LLM achieves even better performance
on standard benchmarks. Extensive experiments validate the efficacy and
efficiency of our approach.
\\ ( https://arxiv.org/abs/2402.00530 ,  2226kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00559
Date: Thu, 1 Feb 2024 12:46:45 GMT   (8386kb,D)

Title: A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for
  Verifiers of Reasoning Chains
Authors: Alon Jacovi, Yonatan Bitton, Bernd Bohnet, Jonathan Herzig, Or
  Honovich, Michael Tseng, Michael Collins, Roee Aharoni, Mor Geva
Categories: cs.CL
Comments: https://huggingface.co/datasets/google/reveal
\\
  Prompting language models to provide step-by-step answers (e.g.,
"Chain-of-Thought") is the prominent approach for complex reasoning tasks,
where more accurate reasoning chains typically improve downstream task
performance. Recent literature discusses automatic methods to verify reasoning
steps to evaluate and improve their correctness. However, no fine-grained
step-level datasets are available to enable thorough evaluation of such
verification methods, hindering progress in this direction. We introduce
Reveal: Reasoning Verification Evaluation, a new dataset to benchmark automatic
verifiers of complex Chain-of-Thought reasoning in open-domain question
answering settings. Reveal includes comprehensive labels for the relevance,
attribution to evidence passages, and logical correctness of each reasoning
step in a language model's answer, across a wide variety of datasets and
state-of-the-art language models.
\\ ( https://arxiv.org/abs/2402.00559 ,  8386kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00620
Date: Thu, 1 Feb 2024 14:30:39 GMT   (53kb,D)

Title: Actor Identification in Discourse: A Challenge for LLMs?
Authors: Ana Bari\'c and Sean Papay and Sebastian Pad\'o
Categories: cs.CL
Comments: Proceedings of the EACL 2024 workshop on Computational Models of
  Discourse (St. Julian's, Malta)
\\
  The identification of political actors who put forward claims in public
debate is a crucial step in the construction of discourse networks, which are
helpful to analyze societal debates. Actor identification is, however, rather
challenging: Often, the locally mentioned speaker of a claim is only a pronoun
("He proposed that [claim]"), so recovering the canonical actor name requires
discourse understanding. We compare a traditional pipeline of dedicated NLP
components (similar to those applied to the related task of coreference) with a
LLM, which appears a good match for this generation task. Evaluating on a
corpus of German actors in newspaper reports, we find surprisingly that the LLM
performs worse. Further analysis reveals that the LLM is very good at
identifying the right reference, but struggles to generate the correct
canonical form. This points to an underlying issue in LLMs with controlling
generated output. Indeed, a hybrid model combining the LLM with a classifier to
normalize its output substantially outperforms both initial models.
\\ ( https://arxiv.org/abs/2402.00620 ,  53kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00632
Date: Thu, 1 Feb 2024 14:46:35 GMT   (8165kb,D)

Title: Prosody in Cascade and Direct Speech-to-Text Translation: a case study
  on Korean Wh-Phrases
Authors: Giulio Zhou, Tsz Kin Lam, Alexandra Birch, Barry Haddow
Categories: cs.CL
Comments: Accepted at Findings of EACL 2024
\\
  Speech-to-Text Translation (S2TT) has typically been addressed with cascade
systems, where speech recognition systems generate a transcription that is
subsequently passed to a translation model. While there has been a growing
interest in developing direct speech translation systems to avoid propagating
errors and losing non-verbal content, prior work in direct S2TT has struggled
to conclusively establish the advantages of integrating the acoustic signal
directly into the translation process. This work proposes using contrastive
evaluation to quantitatively measure the ability of direct S2TT systems to
disambiguate utterances where prosody plays a crucial role. Specifically, we
evaluated Korean-English translation systems on a test set containing
wh-phrases, for which prosodic features are necessary to produce translations
with the correct intent, whether it's a statement, a yes/no question, a
wh-question, and more. Our results clearly demonstrate the value of direct
translation systems over cascade translation models, with a notable 12.9%
improvement in overall accuracy in ambiguous cases, along with up to a 15.6%
increase in F1 scores for one of the major intent categories. To the best of
our knowledge, this work stands as the first to provide quantitative evidence
that direct S2TT models can effectively leverage prosody. The code for our
evaluation is openly accessible and freely available for review and
utilisation.
\\ ( https://arxiv.org/abs/2402.00632 ,  8165kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00667
Date: Thu, 1 Feb 2024 15:30:19 GMT   (4749kb,D)

Title: Improving Weak-to-Strong Generalization with Scalable Oversight and
  Ensemble Learning
Authors: Jitao Sang, Yuhang Wang, Jing Zhang, Yanxu Zhu, Chao Kong, Junhong Ye,
  Shuyu Wei and Jinlin Xiao
Categories: cs.CL
\\
  This paper presents a follow-up study to OpenAI's recent superalignment work
on Weak-to-Strong Generalization (W2SG). Superalignment focuses on ensuring
that high-level AI systems remain consistent with human values and intentions
when dealing with complex, high-risk tasks. The W2SG framework has opened new
possibilities for empirical research in this evolving field. Our study
simulates two phases of superalignment under the W2SG framework: the
development of general superhuman models and the progression towards
superintelligence. In the first phase, based on human supervision, the quality
of weak supervision is enhanced through a combination of scalable oversight and
ensemble learning, reducing the capability gap between weak teachers and strong
students. In the second phase, an automatic alignment evaluator is employed as
the weak supervisor. By recursively updating this auto aligner, the
capabilities of the weak teacher models are synchronously enhanced, achieving
weak-to-strong supervision over stronger student models.We also provide an
initial validation of the proposed approach for the first phase. Using the SciQ
task as example, we explore ensemble learning for weak teacher models through
bagging and boosting. Scalable oversight is explored through two auxiliary
settings: human-AI interaction and AI-AI debate. Additionally, the paper
discusses the impact of improved weak supervision on enhancing weak-to-strong
generalization based on in-context learning. Experiment code and dataset will
be released at https://github.com/ADaM-BJTU/W2SG.
\\ ( https://arxiv.org/abs/2402.00667 ,  4749kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00707
Date: Thu, 1 Feb 2024 16:04:04 GMT   (537kb,D)

Title: Non-Exchangeable Conformal Language Generation with Nearest Neighbors
Authors: Dennis Ulmer, Chrysoula Zerva, Andr\'e F.T. Martins
Categories: cs.CL cs.AI cs.LG
\\
  Quantifying uncertainty in automatically generated text is important for
letting humans check potential hallucinations and making systems more reliable.
Conformal prediction is an attractive framework to provide predictions imbued
with statistical guarantees, however, its application to text generation is
challenging since any i.i.d. assumptions are not realistic. In this paper, we
bridge this gap by leveraging recent results on non-exchangeable conformal
prediction, which still ensures bounds on coverage. The result,
non-exchangeable conformal nucleus sampling, is a novel extension of the
conformal prediction framework to generation based on nearest neighbors. Our
method can be used post-hoc for an arbitrary model without extra training and
supplies token-level, calibrated prediction sets equipped with statistical
guarantees. Experiments in machine translation and language modeling show
encouraging results in generation quality. By also producing tighter prediction
sets with good coverage, we thus give a more theoretically principled way to
perform sampling with conformal guarantees.
\\ ( https://arxiv.org/abs/2402.00707 ,  537kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00723
Date: Thu, 1 Feb 2024 16:14:35 GMT   (4811kb,D)

Title: Improving Semantic Control in Discrete Latent Spaces with Transformer
  Quantized Variational Autoencoders
Authors: Yingji Zhang, Danilo S. Carvalho, Marco Valentino, Ian Pratt-Hartmann,
  Andre Freitas
Categories: cs.CL
\\
  Achieving precise semantic control over the latent spaces of Variational
AutoEncoders (VAEs) holds significant value for downstream tasks in NLP as the
underlying generative mechanisms could be better localised, explained and
improved upon. Recent research, however, has struggled to achieve consistent
results, primarily due to the inevitable loss of semantic information in the
variational bottleneck and limited control over the decoding mechanism. To
overcome these challenges, we investigate discrete latent spaces in Vector
Quantized Variational AutoEncoders (VQVAEs) to improve semantic control and
generation in Transformer-based VAEs. In particular, We propose T5VQVAE, a
novel model that leverages the controllability of VQVAEs to guide the
self-attention mechanism in T5 at the token-level, exploiting its full
generalization capabilities. Experimental results indicate that T5VQVAE
outperforms existing state-of-the-art VAE models, including Optimus, in terms
of controllability and preservation of semantic information across different
tasks such as auto-encoding of sentences and mathematical expressions, text
transfer, and inference. Moreover, T5VQVAE exhibits improved inference
capabilities, suggesting potential applications for downstream natural language
and symbolic reasoning tasks.
\\ ( https://arxiv.org/abs/2402.00723 ,  4811kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00742
Date: Thu, 1 Feb 2024 16:39:28 GMT   (8936kb,D)

Title: Transforming and Combining Rewards for Aligning Large Language Models
Authors: Zihao Wang, Chirag Nagpal, Jonathan Berant, Jacob Eisenstein, Alex
  D'Amour, Sanmi Koyejo, Victor Veitch
Categories: cs.CL cs.AI
MSC-class: 68T50
ACM-class: I.2
\\
  A common approach for aligning language models to human preferences is to
first learn a reward model from preference data, and then use this reward model
to update the language model. We study two closely related problems that arise
in this approach. First, any monotone transformation of the reward model
preserves preference ranking; is there a choice that is ``better'' than others?
Second, we often wish to align language models to multiple properties: how
should we combine multiple reward models? Using a probabilistic interpretation
of the alignment procedure, we identify a natural choice for transformation for
(the common case of) rewards learned from Bradley-Terry preference models. This
derived transformation has two important properties. First, it emphasizes
improving poorly-performing outputs, rather than outputs that already score
well. This mitigates both underfitting (where some prompts are not improved)
and reward hacking (where the model learns to exploit misspecification of the
reward model). Second, it enables principled aggregation of rewards by linking
summation to logical conjunction: the sum of transformed rewards corresponds to
the probability that the output is ``good'' in all measured properties, in a
sense we make precise. Experiments aligning language models to be both helpful
and harmless using RLHF show substantial improvements over the baseline
(non-transformed) approach.
\\ ( https://arxiv.org/abs/2402.00742 ,  8936kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00745
Date: Thu, 1 Feb 2024 16:39:51 GMT   (8653kb,D)

Title: Enhancing Ethical Explanations of Large Language Models through
  Iterative Symbolic Refinement
Authors: Xin Quan, Marco Valentino, Louise A. Dennis, Andr\'e Freitas
Categories: cs.CL
Comments: Camera-ready for EACL 2024
\\
  An increasing amount of research in Natural Language Inference (NLI) focuses
on the application and evaluation of Large Language Models (LLMs) and their
reasoning capabilities. Despite their success, however, LLMs are still prone to
factual errors and inconsistencies in their explanations, offering limited
control and interpretability for inference in complex domains. In this paper,
we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can
enhance the logical validity and alignment of ethical explanations produced by
LLMs. Specifically, we present an abductive-deductive framework named
Logic-Explainer, which integrates LLMs with an external backward-chaining
solver to refine step-wise natural language explanations and jointly verify
their correctness, reduce incompleteness and minimise redundancy. An extensive
empirical analysis demonstrates that Logic-Explainer can improve explanations
generated via in-context learning methods and Chain-of-Thought (CoT) on
challenging ethical NLI tasks, while, at the same time, producing formal proofs
describing and supporting models' reasoning. As ethical NLI requires
commonsense reasoning to identify underlying moral violations, our results
suggest the effectiveness of neuro-symbolic methods for multi-step NLI more
broadly, opening new opportunities to enhance the logical consistency,
reliability, and alignment of LLMs.
\\ ( https://arxiv.org/abs/2402.00745 ,  8653kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00746
Date: Thu, 1 Feb 2024 16:40:32 GMT   (1008kb,D)

Title: Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model
Authors: Mingyu Jin, Qinkai Yu, Chong Zhang, Dong Shu, Suiyuan Zhu, Mengnan Du,
  Yongfeng Zhang, Yanda Meng
Categories: cs.CL
\\
  Artificial intelligence (AI) in healthcare has significantly advanced
intelligent medical treatment. However, traditional intelligent healthcare is
limited by static data and unified standards, preventing full integration with
individual situations and other challenges. Hence, a more professional and
detailed intelligent healthcare method is needed for development. To this end,
we propose an innovative framework named Heath-LLM, which combines large-scale
feature extraction and medical knowledge trade-off scoring. Compared to
traditional health management methods, our approach has three main advantages.
First, our method integrates health reports into a large model to provide
detailed task information. Second, professional medical expertise is used to
adjust the weighted scores of health characteristics. Third, we use a
semi-automated feature extraction framework to enhance the analytical power of
language models and incorporate expert insights to improve the accuracy of
disease prediction. We have conducted disease prediction experiments on a large
number of health reports to assess the effectiveness of Health-LLM. The results
of the experiments indicate that the proposed method surpasses traditional
methods and has the potential to revolutionize disease prediction and
personalized health management. The code is available at
https://github.com/jmyissb/HealthLLM.
\\ ( https://arxiv.org/abs/2402.00746 ,  1008kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00786
Date: Thu, 1 Feb 2024 17:17:55 GMT   (4603kb,D)

Title: CroissantLLM: A Truly Bilingual French-English Language Model
Authors: Manuel Faysse, Patrick Fernandes, Nuno Guerreiro, Ant\'onio Loison,
  Duarte Alves, Caio Corro, Nicolas Boizard, Jo\~ao Alves, Ricardo Rei, Pedro
  Martins, Antoni Bigata Casademunt, Fran\c{c}ois Yvon, Andr\'e Martins,
  Gautier Viaud, C\'eline Hudelot, Pierre Colombo
Categories: cs.CL cs.LG
\\
  We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T
English and French tokens, to bring to the research and industrial community a
high-performance, fully open-sourced bilingual model that runs swiftly on
consumer-grade local hardware. To that end, we pioneer the approach of training
an intrinsically bilingual model with a 1:1 English-to-French pretraining data
ratio, a custom tokenizer, and bilingual finetuning datasets. We release the
training dataset, notably containing a French split with manually curated,
high-quality, and varied data sources. To assess performance outside of
English, we craft a novel benchmark, FrenchBench, consisting of an array of
classification and generation tasks, covering various orthogonal aspects of
model performance in the French Language. Additionally, rooted in transparency
and to foster further Large Language Model research, we release codebases, and
dozens of checkpoints across various model sizes, training data distributions,
and training steps, as well as fine-tuned Chat models, and strong translation
models. We evaluate our model through the FMTI framework, and validate 81 % of
the transparency criteria, far beyond the scores of even most open initiatives.
This work enriches the NLP landscape, breaking away from previous
English-centric work in order to strengthen our understanding of
multilinguality in language models.
\\ ( https://arxiv.org/abs/2402.00786 ,  4603kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00794
Date: Thu, 1 Feb 2024 17:25:51 GMT   (342kb,D)

Title: ReAGent: Towards A Model-agnostic Feature Attribution Method for
  Generative Language Models
Authors: Zhixue Zhao, Boxuan Shan
Categories: cs.CL cs.AI cs.LG
\\
  Feature attribution methods (FAs), such as gradients and attention, are
widely employed approaches to derive the importance of all input features to
the model predictions. Existing work in natural language processing has mostly
focused on developing and testing FAs for encoder-only language models (LMs) in
classification tasks. However, it is unknown if it is faithful to use these FAs
for decoder-only models on text generation, due to the inherent differences
between model architectures and task settings respectively. Moreover, previous
work has demonstrated that there is no `one-wins-all' FA across models and
tasks. This makes the selection of a FA computationally expensive for large LMs
since input importance derivation often requires multiple forward and backward
passes including gradient computations that might be prohibitive even with
access to large compute. To address these issues, we present a model-agnostic
FA for generative LMs called Recursive Attribution Generator (ReAGent). Our
method updates the token importance distribution in a recursive manner. For
each update, we compute the difference in the probability distribution over the
vocabulary for predicting the next token between using the original input and
using a modified version where a part of the input is replaced with RoBERTa
predictions. Our intuition is that replacing an important token in the context
should have resulted in a larger change in the model's confidence in predicting
the token than replacing an unimportant token. Our method can be universally
applied to any generative LM without accessing internal model weights or
additional training and fine-tuning, as most other FAs require. We extensively
compare the faithfulness of ReAGent with seven popular FAs across six
decoder-only LMs of various sizes. The results show that our method
consistently provides more faithful token importance distributions.
\\ ( https://arxiv.org/abs/2402.00794 ,  342kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00835
Date: Thu, 1 Feb 2024 18:22:32 GMT   (2484kb,D)

Title: ALISON: Fast and Effective Stylometric Authorship Obfuscation
Authors: Eric Xing, Saranya Venkatraman, Thai Le, Dongwon Lee
Categories: cs.CL cs.AI cs.LG
Comments: 10 pages, 6 figures, 4 tables. To be published in the Proceedings of
  the 38th Annual AAAI Conference on Artificial Intelligence (AAAI-24)
ACM-class: I.2.7; I.2.0
\\
  Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing
tasks of increasing importance in privacy research. Modern AA leverages an
author's consistent writing style to match a text to its author using an AA
classifier. AO is the corresponding adversarial task, aiming to modify a text
in such a way that its semantics are preserved, yet an AA model cannot
correctly infer its authorship. To address privacy concerns raised by
state-of-the-art (SOTA) AA methods, new AO methods have been proposed but
remain largely impractical to use due to their prohibitively slow training and
obfuscation speed, often taking hours. To this challenge, we propose a
practical AO method, ALISON, that (1) dramatically reduces training/obfuscation
time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2)
achieves better obfuscation success through attacking three transformer-based
AA methods on two benchmark datasets, typically performing 15% better than
competing methods, (3) does not require direct signals from a target AA
classifier during obfuscation, and (4) utilizes unique stylometric features,
allowing sound model interpretation for explainable obfuscation. We also
demonstrate that ALISON can effectively prevent four SOTA AA methods from
accurately determining the authorship of ChatGPT-generated texts, all while
minimally changing the original text semantics. To ensure the reproducibility
of our findings, our code and data are available at:
https://github.com/EricX003/ALISON.
\\ ( https://arxiv.org/abs/2402.00835 ,  2484kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00838
Date: Thu, 1 Feb 2024 18:28:55 GMT   (297kb,D)

Title: OLMo: Accelerating the Science of Language Models
Authors: Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney
  Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson,
  Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi
  Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel,
  Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha
  Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha
  Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant
  Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle
  Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A.
  Smith, Hannaneh Hajishirzi
Categories: cs.CL
\\
  Language models (LMs) have become ubiquitous in both NLP research and in
commercial product offerings. As their commercial importance has surged, the
most powerful models have become closed off, gated behind proprietary
interfaces, with important details of their training data, architectures, and
development undisclosed. Given the importance of these details in
scientifically studying these models, including their biases and potential
risks, we believe it is essential for the research community to have access to
powerful, truly open LMs. To this end, this technical report details the first
release of OLMo, a state-of-the-art, truly Open Language Model and its
framework to build and study the science of language modeling. Unlike most
prior efforts that have only released model weights and inference code, we
release OLMo and the whole framework, including training data and training and
evaluation code. We hope this release will empower and strengthen the open
research community and inspire a new wave of innovation.
\\ ( https://arxiv.org/abs/2402.00838 ,  297kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00841
Date: Thu, 1 Feb 2024 18:31:34 GMT   (6996kb,D)

Title: Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight
  in the Real World for Meeting Summarization?
Authors: Xue-Yong Fu, Md Tahmid Rahman Laskar, Elena Khasanova, Cheng Chen,
  Shashi Bhushan TN
Categories: cs.CL
Comments: The first two authors contributed equally to this work
\\
  Large Language Models (LLMs) have demonstrated impressive capabilities to
solve a wide range of tasks without being explicitly fine-tuned on
task-specific datasets. However, deploying LLMs in the real world is not
trivial, as it requires substantial computing resources. In this paper, we
investigate whether smaller, compact LLMs are a good alternative to the
comparatively Larger LLMs2 to address significant costs associated with
utilizing LLMs in the real world. In this regard, we study the meeting
summarization task in a real-world industrial environment and conduct extensive
experiments by comparing the performance of fine-tuned compact LLMs (e.g.,
FLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2,
GPT-3.5, PaLM-2). We observe that most smaller LLMs, even after fine-tuning,
fail to outperform larger zero-shot LLMs in meeting summarization datasets.
However, a notable exception is FLAN-T5 (780M parameters), which performs on
par or even better than many zero-shot Larger LLMs (from 7B to above 70B
parameters), while being significantly smaller. This makes compact LLMs like
FLAN-T5 a suitable cost-efficient solution for real-world industrial
deployment.
\\ ( https://arxiv.org/abs/2402.00841 ,  6996kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00856
Date: Thu, 1 Feb 2024 18:51:54 GMT   (4816kb,D)

Title: Towards Efficient and Exact Optimization of Language Model Alignment
Authors: Haozhe Ji, Cheng Lu, Yilin Niu, Pei Ke, Hongning Wang, Jun Zhu, Jie
  Tang, Minlie Huang
Categories: cs.CL
Comments: 23 pages, 9 figures
\\
  The alignment of language models with human preferences is vital for their
application in real-world tasks. The problem is formulated as optimizing the
model's policy to maximize the expected reward that reflects human preferences
with minimal deviation from the initial policy. While considered as a
straightforward solution, reinforcement learning (RL) suffers from high
variance in policy updates, which impedes efficient policy improvement.
Recently, direct preference optimization (DPO) was proposed to directly
optimize the policy from preference data. Though simple to implement, DPO is
derived based on the optimal policy that is not assured to be achieved in
practice, which undermines its convergence to the intended solution.
  In this paper, we propose efficient exact optimization (EXO) of the alignment
objective. We prove that EXO is guaranteed to optimize in the same direction as
the RL algorithms asymptotically for arbitary parametrization of the policy,
while enables efficient optimization by circumventing the complexities
associated with RL algorithms. We compare our method to DPO with both
theoretical and empirical analyses, and further demonstrate the advantages of
our method over existing approaches on realistic human preference data.
\\ ( https://arxiv.org/abs/2402.00856 ,  4816kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00858
Date: Thu, 1 Feb 2024 18:55:29 GMT   (6827kb,D)

Title: Can Large Language Models Understand Context?
Authors: Yilun Zhu, Joel Ruben Antony Moniz, Shruti Bhargava, Jiarui Lu, Dhivya
  Piraviperumal, Site Li, Yuan Zhang, Hong Yu, Bo-Hsiang Tseng
Categories: cs.CL
Comments: Findings of EACL 2024
\\
  Understanding context is key to understanding human language, an ability
which Large Language Models (LLMs) have been increasingly seen to demonstrate
to an impressive extent. However, though the evaluation of LLMs encompasses
various domains within the realm of Natural Language Processing, limited
attention has been paid to probing their linguistic capability of understanding
contextual features. This paper introduces a context understanding benchmark by
adapting existing datasets to suit the evaluation of generative models. This
benchmark comprises of four distinct tasks and nine datasets, all featuring
prompts designed to assess the models' ability to understand context. First, we
evaluate the performance of LLMs under the in-context learning pretraining
scenario. Experimental results indicate that pre-trained dense models struggle
with understanding more nuanced contextual features when compared to
state-of-the-art fine-tuned models. Second, as LLM compression holds growing
significance in both research and real-world applications, we assess the
context understanding of quantized models under in-context-learning settings.
We find that 3-bit post-training quantization leads to varying degrees of
performance reduction on our benchmark. We conduct an extensive analysis of
these scenarios to substantiate our experimental results.
\\ ( https://arxiv.org/abs/2402.00858 ,  6827kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00861
Date: Thu, 1 Feb 2024 18:56:18 GMT   (9702kb,D)

Title: Evaluating Large Language Models for Generalization and Robustness via
  Data Compression
Authors: Yucheng Li, Yunhao Guo, Frank Guerin, Chenghua Lin
Categories: cs.CL cs.AI
\\
  Existing methods for evaluating large language models face challenges such as
data contamination, sensitivity to prompts, and the high cost of benchmark
creation. To address this, we propose a lossless data compression based
evaluation approach that tests how models' predictive abilities generalize
after their training cutoff. Specifically, we collect comprehensive test data
spanning 83 months from 2017 to 2023 and split the data into training and
testing periods according to models' training data cutoff. We measure: 1) the
compression performance on the testing period as a measure of generalization on
unseen data; and 2) the performance gap between the training and testing period
as a measure of robustness. Our experiments test 14 representative large
language models with various sizes on sources including Wikipedia, news
articles, code, arXiv papers, and multi-modal data. We find that the
compression rate of many models reduces significantly after their cutoff date,
but models such as Mistral and Llama-2 demonstrate a good balance between
performance and robustness. Results also suggest that models struggle to
generalize on news and code data, but work especially well on arXiv papers. We
also find the context size and tokenization implementation have a big impact of
on the overall compression performance.
\\ ( https://arxiv.org/abs/2402.00861 ,  9702kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00031
Date: Sat, 6 Jan 2024 23:11:50 GMT   (808kb,D)

Title: An Integrated Framework for Team Formation and Winner Prediction in the
  FIRST Robotics Competition: Model, Algorithm, and Analysis
Authors: Federico Galbiati, Ranier X. Gran, Brendan D. Jacques, Sullivan J.
  Mulhern, Chun-Kit Ngan
Categories: cs.LG cs.AI cs.RO
\\
  This research work aims to develop an analytical approach for optimizing team
formation and predicting team performance in a competitive environment based on
data on the competitors' skills prior to the team formation. There are several
approaches in scientific literature to optimize and predict a team's
performance. However, most studies employ fine-grained skill statistics of the
individual members or constraints such as teams with a set group of members.
Currently, no research tackles the highly constrained domain of the FIRST
Robotics Competition. This research effort aims to fill this gap by providing
an analytical method for optimizing and predicting team performance in a
competitive environment while allowing these constraints and only using metrics
on previous team performance, not on each individual member's performance. We
apply our method to the drafting process of the FIRST Robotics competition, a
domain in which the skills change year-over-year, team members change
throughout the season, each match only has a superficial set of statistics, and
alliance formation is key to competitive success. First, we develop a method
that could extrapolate individual members' performance based on overall team
performance. An alliance optimization algorithm is developed to optimize team
formation and a deep neural network model is trained to predict the winning
team, both using highly post-processed real-world data. Our method is able to
successfully extract individual members' metrics from overall team statistics,
form competitive teams, and predict the winning team with 84.08% accuracy.
\\ ( https://arxiv.org/abs/2402.00031 ,  808kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00059
Date: Sun, 28 Jan 2024 13:23:25 GMT   (2055kb,D)

Title: FengWu-GHR: Learning the Kilometer-scale Medium-range Global Weather
  Forecasting
Authors: Tao Han and Song Guo and Fenghua Ling and Kang Chen and Junchao Gong
  and Jingjia Luo and Junxia Gu and Kan Dai and Wanli Ouyang and Lei Bai
Categories: cs.LG cs.AI physics.ao-ph
Comments: 19 pages
\\
  Kilometer-scale modeling of global atmosphere dynamics enables fine-grained
weather forecasting and decreases the risk of disastrous weather and climate
activity. Therefore, building a kilometer-scale global forecast model is a
persistent pursuit in the meteorology domain. Active international efforts have
been made in past decades to improve the spatial resolution of numerical
weather models. Nonetheless, developing the higher resolution numerical model
remains a long-standing challenge due to the substantial consumption of
computational resources. Recent advances in data-driven global weather
forecasting models utilize reanalysis data for model training and have
demonstrated comparable or even higher forecasting skills than numerical
models. However, they are all limited by the resolution of reanalysis data and
incapable of generating higher-resolution forecasts. This work presents
FengWu-GHR, the first data-driven global weather forecasting model running at
the 0.09$^{\circ}$ horizontal resolution. FengWu-GHR introduces a novel
approach that opens the door for operating ML-based high-resolution forecasts
by inheriting prior knowledge from a pretrained low-resolution model. The
hindcast of weather prediction in 2022 indicates that FengWu-GHR is superior to
the IFS-HRES. Furthermore, evaluations on station observations and case studies
of extreme events support the competitive operational forecasting skill of
FengWu-GHR at the high resolution.
\\ ( https://arxiv.org/abs/2402.00059 ,  2055kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00066
Date: Mon, 29 Jan 2024 20:05:14 GMT   (1196kb)

Title: TrackGPT -- A generative pre-trained transformer for cross-domain entity
  trajectory forecasting
Authors: Nicholas Stroh
Categories: cs.LG cs.AI
Comments: 16 pages, 8 figures
MSC-class: 68T07
\\
  The forecasting of entity trajectories at future points in time is a critical
capability gap in applications across both Commercial and Defense sectors.
Transformers, and specifically Generative Pre-trained Transformer (GPT)
networks have recently revolutionized several fields of Artificial
Intelligence, most notably Natural Language Processing (NLP) with the advent of
Large Language Models (LLM) like OpenAI's ChatGPT. In this research paper, we
introduce TrackGPT, a GPT-based model for entity trajectory forecasting that
has shown utility across both maritime and air domains, and we expect to
perform well in others. TrackGPT stands as a pioneering GPT model capable of
producing accurate predictions across diverse entity time series datasets,
demonstrating proficiency in generating both long-term forecasts with sustained
accuracy and short-term forecasts with high precision. We present benchmarks
against state-of-the-art deep learning techniques, showing that TrackGPT's
forecasting capability excels in terms of accuracy, reliability, and
modularity. Importantly, TrackGPT achieves these results while remaining
domain-agnostic and requiring minimal data features (only location and time)
compared to models achieving similar performance. In conclusion, our findings
underscore the immense potential of applying GPT architectures to the task of
entity trajectory forecasting, exemplified by the innovative TrackGPT model.
\\ ( https://arxiv.org/abs/2402.00066 ,  1196kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00068
Date: Tue, 30 Jan 2024 14:47:15 GMT   (1701kb,D)

Title: GPT4Battery: An LLM-driven Framework for Adaptive State of Health
  Estimation of Raw Li-ion Batteries
Authors: Yuyuan Feng, Guosheng Hu, Zhihong Zhang
Categories: cs.LG cs.AI
\\
  State of health (SOH) is a crucial indicator for assessing the degradation
level of batteries that cannot be measured directly but requires estimation.
Accurate SOH estimation enhances detection, control, and feedback for Li-ion
batteries, allowing for safe and efficient energy management and guiding the
development of new-generation batteries. Despite the significant progress in
data-driven SOH estimation, the time and resource-consuming degradation
experiments for generating lifelong training data pose a challenge in
establishing one large model capable of handling diverse types of Li-ion
batteries, e.g., cross-chemistry, cross-manufacturer, and cross-capacity.
Hence, this paper utilizes the strong generalization capability of large
language model (LLM) to proposes a novel framework for adaptable SOH estimation
across diverse batteries. To match the real scenario where unlabeled data
sequentially arrives in use with distribution shifts, the proposed model is
modified by a test-time training technique to ensure estimation accuracy even
at the battery's end of life. The validation results demonstrate that the
proposed framework achieves state-of-the-art accuracy on four widely recognized
datasets collected from 62 batteries. Furthermore, we analyze the theoretical
challenges of cross-battery estimation and provide a quantitative explanation
of the effectiveness of our method.
\\ ( https://arxiv.org/abs/2402.00068 ,  1701kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00071
Date: Tue, 30 Jan 2024 20:08:15 GMT   (10550kb)

Title: Unraveling the Impact of Initial Choices and In-Loop Interventions on
  Learning Dynamics in Autonomous Scanning Probe Microscopy
Authors: Boris N. Slautin, Yongtao Liu, Hiroshi Funakubo, Sergei V. Kalinin
Categories: cs.LG cond-mat.mtrl-sci
Comments: 24 pages, 11 figures
\\
  The current focus in Autonomous Experimentation (AE) is on developing robust
workflows to conduct the AE effectively. This entails the need for well-defined
approaches to guide the AE process, including strategies for hyperparameter
tuning and high-level human interventions within the workflow loop. This paper
presents a comprehensive analysis of the influence of initial experimental
conditions and in-loop interventions on the learning dynamics of Deep Kernel
Learning (DKL) within the realm of AE in Scanning Probe Microscopy. We explore
the concept of 'seed effect', where the initial experiment setup has a
substantial impact on the subsequent learning trajectory. Additionally, we
introduce an approach of the seed point interventions in AE allowing the
operator to influence the exploration process. Using a dataset from
Piezoresponse Force Microscopy (PFM) on PbTiO3 thin films, we illustrate the
impact of the 'seed effect' and in-loop seed interventions on the effectiveness
of DKL in predicting material properties. The study highlights the importance
of initial choices and adaptive interventions in optimizing learning rates and
enhancing the efficiency of automated material characterization. This work
offers valuable insights into designing more robust and effective AE workflows
in microscopy with potential applications across various characterization
techniques. The analysis code that supports the funding is publicly available
at https://github.com/Slautin/2024_Seed_effect_DKL_BO.
\\ ( https://arxiv.org/abs/2402.00071 ,  10550kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00072
Date: Tue, 30 Jan 2024 20:47:50 GMT   (4916kb)

Title: Explainable AI for survival analysis: a median-SHAP approach
Authors: Lucile Ter-Minassian, Sahra Ghalebikesabi, Karla Diaz-Ordaz and Chris
  Holmes
Categories: cs.LG stat.ME stat.ML
Comments: Accepted to the Interpretable Machine Learning for Healthcare (IMLH)
  workshop of the ICML 2022 Conference
\\
  With the adoption of machine learning into routine clinical practice comes
the need for Explainable AI methods tailored to medical applications. Shapley
values have sparked wide interest for locally explaining models. Here, we
demonstrate their interpretation strongly depends on both the summary statistic
and the estimator for it, which in turn define what we identify as an 'anchor
point'. We show that the convention of using a mean anchor point may generate
misleading interpretations for survival analysis and introduce median-SHAP, a
method for explaining black-box models predicting individual survival times.
\\ ( https://arxiv.org/abs/2402.00072 ,  4916kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00084
Date: Wed, 31 Jan 2024 05:39:55 GMT   (6871kb,D)

Title: EPSD: Early Pruning with Self-Distillation for Efficient Model
  Compression
Authors: Dong Chen, Ning Liu, Yichen Zhu, Zhengping Che, Rui Ma, Fachao Zhang,
  Xiaofeng Mou, Yi Chang, Jian Tang
Categories: cs.LG cs.AI cs.CV
Comments: The first two authors are with equal contributions. Paper accepted by
  AAAI 2024
\\
  Neural network compression techniques, such as knowledge distillation (KD)
and network pruning, have received increasing attention. Recent work `Prune,
then Distill' reveals that a pruned student-friendly teacher network can
benefit the performance of KD. However, the conventional teacher-student
pipeline, which entails cumbersome pre-training of the teacher and complicated
compression steps, makes pruning with KD less efficient. In addition to
compressing models, recent compression techniques also emphasize the aspect of
efficiency. Early pruning demands significantly less computational cost in
comparison to the conventional pruning methods as it does not require a large
pre-trained model. Likewise, a special case of KD, known as self-distillation
(SD), is more efficient since it requires no pre-training or student-teacher
pair selection. This inspires us to collaborate early pruning with SD for
efficient model compression. In this work, we propose the framework named Early
Pruning with Self-Distillation (EPSD), which identifies and preserves
distillable weights in early pruning for a given SD task. EPSD efficiently
combines early pruning and self-distillation in a two-step process, maintaining
the pruned network's trainability for compression. Instead of a simple
combination of pruning and SD, EPSD enables the pruned network to favor SD by
keeping more distillable weights before training to ensure better distillation
of the pruned network. We demonstrated that EPSD improves the training of
pruned networks, supported by visual and quantitative analyses. Our evaluation
covered diverse benchmarks (CIFAR-10/100, Tiny-ImageNet, full ImageNet,
CUB-200-2011, and Pascal VOC), with EPSD outperforming advanced pruning and SD
techniques.
\\ ( https://arxiv.org/abs/2402.00084 ,  6871kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00085
Date: Wed, 31 Jan 2024 06:13:28 GMT   (1360kb)

Title: Scheduled Curiosity-Deep Dyna-Q: Efficient Exploration for Dialog Policy
  Learning
Authors: Xuecheng Niu, Akinori Ito, Takashi Nose
Categories: cs.LG cs.AI
Comments: 18 pages, 11 figures
\\
  Training task-oriented dialog agents based on reinforcement learning is
time-consuming and requires a large number of interactions with real users. How
to grasp dialog policy within limited dialog experiences remains an obstacle
that makes the agent training process less efficient. In addition, most
previous frameworks start training by randomly choosing training samples, which
differs from the human learning method and hurts the efficiency and stability
of training. Therefore, we propose Scheduled Curiosity-Deep Dyna-Q (SC-DDQ), a
curiosity-driven curriculum learning framework based on a state-of-the-art
model-based reinforcement learning dialog model, Deep Dyna-Q (DDQ).
Furthermore, we designed learning schedules for SC-DDQ and DDQ, respectively,
following two opposite training strategies: classic curriculum learning and its
reverse version. Our results show that by introducing scheduled learning and
curiosity, the new framework leads to a significant improvement over the DDQ
and Deep Q-learning(DQN). Surprisingly, we found that traditional curriculum
learning was not always effective. Specifically, according to the experimental
results, the easy-first and difficult-first strategies are more suitable for
SC-DDQ and DDQ. To analyze our results, we adopted the entropy of sampled
actions to depict action exploration and found that training strategies with
high entropy in the first stage and low entropy in the last stage lead to
better performance.
\\ ( https://arxiv.org/abs/2402.00085 ,  1360kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00086
Date: Wed, 31 Jan 2024 07:40:37 GMT   (1721kb,D)

Title: Retrosynthesis prediction enhanced by in-silico reaction data
  augmentation
Authors: Xu Zhang and Yiming Mo and Wenguan Wang and Yi Yang
Categories: cs.LG cs.AI
\\
  Recent advances in machine learning (ML) have expedited retrosynthesis
research by assisting chemists to design experiments more efficiently. However,
all ML-based methods consume substantial amounts of paired training data (i.e.,
chemical reaction: product-reactant(s) pair), which is costly to obtain.
Moreover, companies view reaction data as a valuable asset and restrict the
accessibility to researchers. These issues prevent the creation of more
powerful retrosynthesis models due to their data-driven nature. As a response,
we exploit easy-to-access unpaired data (i.e., one component of
product-reactant(s) pair) for generating in-silico paired data to facilitate
model training. Specifically, we present RetroWISE, a self-boosting framework
that employs a base model inferred from real paired data to perform in-silico
reaction generation and augmentation using unpaired data, ultimately leading to
a superior model. On three benchmark datasets, RetroWISE achieves the best
overall performance against state-of-the-art models (e.g., +8.6% top-1 accuracy
on the USPTO-50K test dataset). Moreover, it consistently improves the
prediction accuracy of rare transformations. These results show that Retro-
WISE overcomes the training bottleneck by in-silico reactions, thereby paving
the way toward more effective ML-based retrosynthesis models.
\\ ( https://arxiv.org/abs/2402.00086 ,  1721kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00092
Date: Wed, 31 Jan 2024 10:52:15 GMT   (500kb,D)

Title: Episodic-free Task Selection for Few-shot Learning
Authors: Tao Zhang
Categories: cs.LG cs.AI
\\
  Episodic training is a mainstream training strategy for few-shot learning. In
few-shot scenarios, however, this strategy is often inferior to some
non-episodic training strategy, e. g., Neighbourhood Component Analysis (NCA),
which challenges the principle that training conditions must match testing
conditions. Thus, a question is naturally asked: How to search for
episodic-free tasks for better few-shot learning? In this work, we propose a
novel meta-training framework beyond episodic training. In this framework,
episodic tasks are not used directly for training, but for evaluating the
effectiveness of some selected episodic-free tasks from a task set that are
performed for training the meta-learners. The selection criterion is designed
with the affinity, which measures the degree to which loss decreases when
executing the target tasks after training with the selected tasks. In
experiments, the training task set contains some promising types, e. g.,
contrastive learning and classification, and the target few-shot tasks are
achieved with the nearest centroid classifiers on the miniImageNet,
tiered-ImageNet and CIFAR-FS datasets. The experimental results demonstrate the
effectiveness of our approach.
\\ ( https://arxiv.org/abs/2402.00092 ,  500kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00137
Date: Wed, 31 Jan 2024 19:30:04 GMT   (1141kb,D)

Title: Multimodal Neurodegenerative Disease Subtyping Explained by ChatGPT
Authors: Diego Machado Reyes, Hanqing Chao, Juergen Hahn, Li Shen and Pingkun
  Yan
Categories: cs.LG cs.CV
\\
  Alzheimer's disease (AD) is the most prevalent neurodegenerative disease; yet
its currently available treatments are limited to stopping disease progression.
Moreover, effectiveness of these treatments is not guaranteed due to the
heterogenetiy of the disease. Therefore, it is essential to be able to identify
the disease subtypes at a very early stage. Current data driven approaches are
able to classify the subtypes at later stages of AD or related disorders, but
struggle when predicting at the asymptomatic or prodromal stage. Moreover, most
existing models either lack explainability behind the classification or only
use a single modality for the assessment, limiting scope of its analysis. Thus,
we propose a multimodal framework that uses early-stage indicators such as
imaging, genetics and clinical assessments to classify AD patients into
subtypes at early stages. Similarly, we build prompts and use large language
models, such as ChatGPT, to interpret the findings of our model. In our
framework, we propose a tri-modal co-attention mechanism (Tri-COAT) to
explicitly learn the cross-modal feature associations. Our proposed model
outperforms baseline models and provides insight into key cross-modal feature
associations supported by known biological mechanisms.
\\ ( https://arxiv.org/abs/2402.00137 ,  1141kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00152
Date: Wed, 31 Jan 2024 20:10:10 GMT   (182kb,D)

Title: Deeper or Wider: A Perspective from Optimal Generalization Error with
  Sobolev Loss
Authors: Yahong Yang and Juncai He
Categories: cs.LG cs.NA math.NA stat.ML
Comments: arXiv admin note: text overlap with arXiv:2310.10766,
  arXiv:2305.08466
MSC-class: 68T05
\\
  Constructing the architecture of a neural network is a challenging pursuit
for the machine learning community, and the dilemma of whether to go deeper or
wider remains a persistent question. This paper explores a comparison between
deeper neural networks (DeNNs) with a flexible number of layers and wider
neural networks (WeNNs) with limited hidden layers, focusing on their optimal
generalization error in Sobolev losses. Analytical investigations reveal that
the architecture of a neural network can be significantly influenced by various
factors, including the number of sample points, parameters within the neural
networks, and the regularity of the loss function. Specifically, a higher
number of parameters tends to favor WeNNs, while an increased number of sample
points and greater regularity in the loss function lean towards the adoption of
DeNNs. We ultimately apply this theory to address partial differential
equations using deep Ritz and physics-informed neural network (PINN) methods,
guiding the design of neural networks.
\\ ( https://arxiv.org/abs/2402.00152 ,  182kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00153
Date: Wed, 31 Jan 2024 20:15:35 GMT   (4800kb,D)

Title: Fully Data-Driven Model for Increasing Sampling Rate Frequency of
  Seismic Data using Super-Resolution Generative Adversarial Networks
Authors: Navid Gholizadeh and Javad Katebi
Categories: cs.LG eess.SP physics.geo-ph
\\
  High-quality data is one of the key requirements for any engineering
application. In earthquake engineering practice, accurate data is pivotal in
predicting the response of structure or damage detection process in an
Structural Health Monitoring (SHM) application with less uncertainty. However,
obtaining high-resolution data is fraught with challenges, such as significant
costs, extensive data channels, and substantial storage requirements. To
address these challenges, this study employs super-resolution generative
adversarial networks (SRGANs) to improve the resolution of time-history data
such as the data obtained by a sensor network in an SHM application, marking
the first application of SRGANs in earthquake engineering domain. The
time-series data are transformed into RGB values, converting raw data into
images. SRGANs are then utilized to upscale these low-resolution images,
thereby enhancing the overall sensor resolution. This methodology not only
offers potential reductions in data storage requirements but also simplifies
the sensor network, which could result in lower installation and maintenance
costs. The proposed SRGAN method is rigorously evaluated using real seismic
data, and its performance is compared with traditional enhancement techniques.
The findings of this study pave the way for cost-effective and efficient
improvements in the resolution of sensors used in SHM systems, with promising
implications for the safety and sustainability of infrastructures worldwide.
\\ ( https://arxiv.org/abs/2402.00153 ,  4800kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00162
Date: Wed, 31 Jan 2024 20:37:09 GMT   (3512kb,D)

Title: Behind the Myth of Exploration in Policy Gradients
Authors: Adrien Bolland, Gaspard Lambrechts, Damien Ernst
Categories: cs.LG stat.ML
\\
  Policy-gradient algorithms are effective reinforcement learning methods for
solving control problems with continuous state and action spaces. To compute
near-optimal policies, it is essential in practice to include exploration terms
in the learning objective. Although the effectiveness of these terms is usually
justified by an intrinsic need to explore environments, we propose a novel
analysis and distinguish two different implications of these techniques. First,
they make it possible to smooth the learning objective and to eliminate local
optima while preserving the global maximum. Second, they modify the gradient
estimates, increasing the probability that the stochastic parameter update
eventually provides an optimal policy. In light of these effects, we discuss
and illustrate empirically exploration strategies based on entropy bonuses,
highlighting their limitations and opening avenues for future works in the
design and analysis of such strategies.
\\ ( https://arxiv.org/abs/2402.00162 ,  3512kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00195
Date: Wed, 31 Jan 2024 21:48:25 GMT   (17491kb,D)

Title: Dataset Condensation Driven Machine Unlearning
Authors: Junaid Iqbal Khan
Categories: cs.LG
\\
  The current trend in data regulation requirements and privacy-preserving
machine learning has emphasized the importance of machine unlearning. The naive
approach to unlearning training data by retraining over the complement of the
forget samples is susceptible to computational challenges. These challenges
have been effectively addressed through a collection of techniques falling
under the umbrella of machine unlearning. However, there still exists a lack of
sufficiency in handling persistent computational challenges in harmony with the
utility and privacy of unlearned model. We attribute this to the lack of work
on improving the computational complexity of approximate unlearning from the
perspective of the training dataset. In this paper, we aim to fill this gap by
introducing dataset condensation as an essential component of machine
unlearning in the context of image classification. To achieve this goal, we
propose new dataset condensation techniques and an innovative unlearning scheme
that strikes a balance between machine unlearning privacy, utility, and
efficiency. Furthermore, we present a novel and effective approach to
instrumenting machine unlearning and propose its application in defending
against membership inference and model inversion attacks. Additionally, we
explore a new application of our approach, which involves removing data from
`condensed model', which can be employed to quickly train any arbitrary model
without being influenced by unlearning samples.
\\ ( https://arxiv.org/abs/2402.00195 ,  17491kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00197
Date: Wed, 31 Jan 2024 21:49:40 GMT   (3403kb,D)

Title: Determination of Trace Organic Contaminant Concentration via Machine
  Classification of Surface-Enhanced Raman Spectra
Authors: Vishnu Jayaprakash, Jae Bem You, Chiranjeevi Kanike, Jinfeng Liu,
  Christopher McCallum, and Xuehua Zhang
Categories: cs.LG eess.SP
DOI: 10.1021/acs.est.3c06447
\\
  Accurate detection and analysis of traces of persistent organic pollutants in
water is important in many areas, including environmental monitoring and food
quality control, due to their long environmental stability and potential
bioaccumulation. While conventional analysis of organic pollutants requires
expensive equipment, surface enhanced Raman spectroscopy (SERS) has
demonstrated great potential for accurate detection of these contaminants.
However, SERS analytical difficulties, such as spectral preprocessing,
denoising, and substrate-based spectral variation, have hindered widespread use
of the technique. Here, we demonstrate an approach for predicting the
concentration of sample pollutants from messy, unprocessed Raman data using
machine learning. Frequency domain transform methods, including the Fourier and
Walsh Hadamard transforms, are applied to sets of Raman spectra of three model
micropollutants in water (rhodamine 6G, chlorpyrifos, and triclosan), which are
then used to train machine learning algorithms. Using standard machine learning
models, the concentration of sample pollutants are predicted with more than 80
percent cross-validation accuracy from raw Raman data. cross-validation
accuracy of 85 percent was achieved using deep learning for a moderately sized
dataset (100 spectra), and 70 to 80 percent cross-validation accuracy was
achieved even for very small datasets (50 spectra). Additionally, standard
models were shown to accurately identify characteristic peaks via analysis of
their importance scores. The approach shown here has the potential to be
applied to facilitate accurate detection and analysis of persistent organic
pollutants by surface-enhanced Raman spectroscopy.
\\ ( https://arxiv.org/abs/2402.00197 ,  3403kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00201
Date: Wed, 31 Jan 2024 21:54:13 GMT   (5442kb,D)

Title: An Experiment on Feature Selection using Logistic Regression
Authors: Raisa Islam, Subhasish Mazumdar, Rakibul Islam
Categories: cs.LG
\\
  In supervised machine learning, feature selection plays a very important role
by potentially enhancing explainability and performance as measured by
computing time and accuracy-related metrics. In this paper, we investigate a
method for feature selection based on the well-known L1 and L2 regularization
strategies associated with logistic regression (LR). It is well known that the
learned coefficients, which serve as weights, can be used to rank the features.
Our approach is to synthesize the findings of L1 and L2 regularization. For our
experiment, we chose the CIC-IDS2018 dataset owing partly to its size and also
to the existence of two problematic classes that are hard to separate. We
report first with the exclusion of one of them and then with its inclusion. We
ranked features first with L1 and then with L2, and then compared logistic
regression with L1 (LR+L1) against that with L2 (LR+L2) by varying the sizes of
the feature sets for each of the two rankings. We found no significant
difference in accuracy between the two methods once the feature set is
selected. We chose a synthesis, i.e., only those features that were present in
both the sets obtained from L1 and that from L2, and experimented with it on
more complex models like Decision Tree and Random Forest and observed that the
accuracy was very close in spite of the small size of the feature set.
Additionally, we also report on the standard metrics: accuracy, precision,
recall, and f1-score.
\\ ( https://arxiv.org/abs/2402.00201 ,  5442kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00205
Date: Wed, 31 Jan 2024 22:06:10 GMT   (4857kb,D)

Title: Decentralised, Collaborative, and Privacy-preserving Machine Learning
  for Multi-Hospital Data
Authors: Congyu Fang, Adam Dziedzic, Lin Zhang, Laura Oliva, Amol Verma, Fahad
  Razak, Nicolas Papernot, Bo Wang
Categories: cs.LG cs.CR
\\
  Machine Learning (ML) has demonstrated its great potential on medical data
analysis. Large datasets collected from diverse sources and settings are
essential for ML models in healthcare to achieve better accuracy and
generalizability. Sharing data across different healthcare institutions is
challenging because of complex and varying privacy and regulatory requirements.
Hence, it is hard but crucial to allow multiple parties to collaboratively
train an ML model leveraging the private datasets available at each party
without the need for direct sharing of those datasets or compromising the
privacy of the datasets through collaboration. In this paper, we address this
challenge by proposing Decentralized, Collaborative, and Privacy-preserving ML
for Multi-Hospital Data (DeCaPH). It offers the following key benefits: (1) it
allows different parties to collaboratively train an ML model without
transferring their private datasets; (2) it safeguards patient privacy by
limiting the potential privacy leakage arising from any contents shared across
the parties during the training process; and (3) it facilitates the ML model
training without relying on a centralized server. We demonstrate the
generalizability and power of DeCaPH on three distinct tasks using real-world
distributed medical datasets: patient mortality prediction using electronic
health records, cell-type classification using single-cell human genomes, and
pathology identification using chest radiology images. We demonstrate that the
ML models trained with DeCaPH framework have an improved utility-privacy
trade-off, showing it enables the models to have good performance while
preserving the privacy of the training data points. In addition, the ML models
trained with DeCaPH framework in general outperform those trained solely with
the private datasets from individual parties, showing that DeCaPH enhances the
model generalizability.
\\ ( https://arxiv.org/abs/2402.00205 ,  4857kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00208
Date: Wed, 31 Jan 2024 22:09:40 GMT   (2177kb,D)

Title: MP-SL: Multihop Parallel Split Learning
Authors: Joana Tirana, Spyros Lalis, Dimitris Chatzopoulos
Categories: cs.LG cs.DC
Comments: 11 pages, 14 figures
\\
  Federated Learning (FL) stands out as a widely adopted protocol facilitating
the training of Machine Learning (ML) models while maintaining decentralized
data. However, challenges arise when dealing with a heterogeneous set of
participating devices, causing delays in the training process, particularly
among devices with limited resources. Moreover, the task of training ML models
with a vast number of parameters demands computing and memory resources beyond
the capabilities of small devices, such as mobile and Internet of Things (IoT)
devices. To address these issues, techniques like Parallel Split Learning (SL)
have been introduced, allowing multiple resource-constrained devices to
actively participate in collaborative training processes with assistance from
resourceful compute nodes. Nonetheless, a drawback of Parallel SL is the
substantial memory allocation required at the compute nodes, for instance
training VGG-19 with 100 participants needs 80 GB. In this paper, we introduce
Multihop Parallel SL (MP-SL), a modular and extensible ML as a Service (MLaaS)
framework designed to facilitate the involvement of resource-constrained
devices in collaborative and distributed ML model training. Notably, to
alleviate memory demands per compute node, MP-SL supports multihop Parallel
SL-based training. This involves splitting the model into multiple parts and
utilizing multiple compute nodes in a pipelined manner. Extensive
experimentation validates MP-SL's capability to handle system heterogeneity,
demonstrating that the multihop configuration proves more efficient than
horizontally scaled one-hop Parallel SL setups, especially in scenarios
involving more cost-effective compute nodes.
\\ ( https://arxiv.org/abs/2402.00208 ,  2177kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00232
Date: Wed, 31 Jan 2024 23:21:40 GMT   (1938kb,D)

Title: Learning Label Hierarchy with Supervised Contrastive Learning
Authors: Ruixue Lian, William A. Sethares, Junjie Hu
Categories: cs.LG cs.AI
\\
  Supervised contrastive learning (SCL) frameworks treat each class as
independent and thus consider all classes to be equally important. This
neglects the common scenario in which label hierarchy exists, where
fine-grained classes under the same category show more similarity than very
different ones. This paper introduces a family of Label-Aware SCL methods
(LASCL) that incorporates hierarchical information to SCL by leveraging
similarities between classes, resulting in creating a more well-structured and
discriminative feature space. This is achieved by first adjusting the distance
between instances based on measures of the proximity of their classes with the
scaled instance-instance-wise contrastive. An additional instance-center-wise
contrastive is introduced to move within-class examples closer to their
centers, which are represented by a set of learnable label parameters. The
learned label parameters can be directly used as a nearest neighbor classifier
without further finetuning. In this way, a better feature representation is
generated with improvements of intra-cluster compactness and inter-cluster
separation. Experiments on three datasets show that the proposed LASCL works
well on text classification of distinguishing a single label among
multi-labels, outperforming the baseline supervised approaches. Our code is
publicly available.
\\ ( https://arxiv.org/abs/2402.00232 ,  1938kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00236
Date: Wed, 31 Jan 2024 23:32:20 GMT   (2839kb)

Title: Positional Encoding Helps Recurrent Neural Networks Handle a Large
  Vocabulary
Authors: Takashi Morita
Categories: cs.LG cs.NE
\\
  This study discusses the effects of positional encoding on recurrent neural
networks (RNNs) utilizing synthetic benchmarks. Positional encoding
"time-stamps" data points in time series and complements the capabilities of
Transformer neural networks, which lack an inherent mechanism for representing
the data order. By contrast, RNNs can encode the temporal information of data
points on their own, rendering their use of positional encoding seemingly
"redundant". Nonetheless, empirical investigations reveal the effectiveness of
positional encoding even when coupled with RNNs, specifically for handling a
large vocabulary that yields diverse observations. These findings pave the way
for a new line of research on RNNs, concerning the combination of input-driven
and autonomous time representation. Additionally, biological implications of
the computational/simulational results are discussed, in the light of the
affinity between the sinusoidal implementation of positional encoding and
neural oscillations in biological brains.
\\ ( https://arxiv.org/abs/2402.00236 ,  2839kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00238
Date: Wed, 31 Jan 2024 23:40:44 GMT   (7386kb,D)

Title: CNN-FL for Biotechnology Industry Empowered by Internet-of-BioNano
  Things and Digital Twins
Authors: Mohammad (Behdad) Jamshidi, Dinh Thai Hoang, and Diep N. Nguyen
Categories: cs.LG eess.IV q-bio.QM
\\
  Digital twins (DTs) are revolutionizing the biotechnology industry by
enabling sophisticated digital representations of biological assets,
microorganisms, drug development processes, and digital health applications.
However, digital twinning at micro and nano scales, particularly in modeling
complex entities like bacteria, presents significant challenges in terms of
requiring advanced Internet of Things (IoT) infrastructure and computing
approaches to achieve enhanced accuracy and scalability. In this work, we
propose a novel framework that integrates the Internet of Bio-Nano Things
(IoBNT) with advanced machine learning techniques, specifically convolutional
neural networks (CNN) and federated learning (FL), to effectively tackle the
identified challenges. Within our framework, IoBNT devices are deployed to
gather image-based biological data across various physical environments,
leveraging the strong capabilities of CNNs for robust machine vision and
pattern recognition. Subsequently, FL is utilized to aggregate insights from
these disparate data sources, creating a refined global model that continually
enhances accuracy and predictive reliability, which is crucial for the
effective deployment of DTs in biotechnology. The primary contribution is the
development of a novel framework that synergistically combines CNN and FL,
augmented by the capabilities of the IoBNT. This novel approach is specifically
tailored to enhancing DTs in the biotechnology industry. The results showcase
enhancements in the reliability and safety of microorganism DTs, while
preserving their accuracy. Furthermore, the proposed framework excels in energy
efficiency and security, offering a user-friendly and adaptable solution. This
broadens its applicability across diverse sectors, including biotechnology and
pharmaceutical industries, as well as clinical and hospital settings.
\\ ( https://arxiv.org/abs/2402.00238 ,  7386kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00240
Date: Wed, 31 Jan 2024 23:48:48 GMT   (496kb,D)

Title: Spectral Norm of Convolutional Layers with Circular and Zero Paddings
Authors: Blaise Delattre and Quentin Barth\'elemy and Alexandre Allauzen
Categories: cs.LG cs.CV
\\
  This paper leverages the use of \emph{Gram iteration} an efficient,
deterministic, and differentiable method for computing spectral norm with an
upper bound guarantee. Designed for circular convolutional layers, we
generalize the use of the Gram iteration to zero padding convolutional layers
and prove its quadratic convergence. We also provide theorems for bridging the
gap between circular and zero padding convolution's spectral norm. We design a
\emph{spectral rescaling} that can be used as a competitive $1$-Lipschitz layer
that enhances network robustness. Demonstrated through experiments, our method
outperforms state-of-the-art techniques in precision, computational cost, and
scalability. The code of experiments is available at
https://github.com/blaisedelattre/lip4conv.
\\ ( https://arxiv.org/abs/2402.00240 ,  496kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00251
Date: Thu, 1 Feb 2024 00:23:31 GMT   (1355kb,D)

Title: Efficient Non-Parametric Uncertainty Quantification for Black-Box Large
  Language Models and Decision Planning
Authors: Yao-Hung Hubert Tsai, Walter Talbott, Jian Zhang
Categories: cs.LG cs.AI cs.CL
\\
  Step-by-step decision planning with large language models (LLMs) is gaining
attention in AI agent development. This paper focuses on decision planning with
uncertainty estimation to address the hallucination problem in language models.
Existing approaches are either white-box or computationally demanding, limiting
use of black-box proprietary LLMs within budgets. The paper's first
contribution is a non-parametric uncertainty quantification method for LLMs,
efficiently estimating point-wise dependencies between input-decision on the
fly with a single inference, without access to token logits. This estimator
informs the statistical interpretation of decision trustworthiness. The second
contribution outlines a systematic design for a decision-making agent,
generating actions like ``turn on the bathroom light'' based on user prompts
such as ``take a bath''. Users will be asked to provide preferences when more
than one action has high estimated point-wise dependencies. In conclusion, our
uncertainty estimation and decision-making agent design offer a cost-efficient
approach for AI agent development.
\\ ( https://arxiv.org/abs/2402.00251 ,  1355kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00254
Date: Thu, 1 Feb 2024 00:54:48 GMT   (5699kb,D)

Title: Vertical Symbolic Regression via Deep Policy Gradient
Authors: Nan Jiang, Md Nasim, Yexiang Xue
Categories: cs.LG cs.AI
Comments: see animated demo at: vsr-dpg.github.io
\\
  Vertical Symbolic Regression (VSR) recently has been proposed to expedite the
discovery of symbolic equations with many independent variables from
experimental data. VSR reduces the search spaces following the vertical
discovery path by building from reduced-form equations involving a subset of
independent variables to full-fledged ones. Proved successful by many symbolic
regressors, deep neural networks are expected to further scale up VSR.
Nevertheless, directly combining VSR with deep neural networks will result in
difficulty in passing gradients and other engineering issues. We propose
Vertical Symbolic Regression using Deep Policy Gradient (VSR-DPG) and
demonstrate that VSR-DPG can recover ground-truth equations involving multiple
input variables, significantly beyond both deep reinforcement learning-based
approaches and previous VSR variants. Our VSR-DPG models symbolic regression as
a sequential decision-making process, in which equations are built from
repeated applications of grammar rules. The integrated deep model is trained to
maximize a policy gradient objective. Experimental results demonstrate that our
VSR-DPG significantly outperforms popular baselines in identifying both
algebraic equations and ordinary differential equations on a series of
benchmarks.
\\ ( https://arxiv.org/abs/2402.00254 ,  5699kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00258
Date: Thu, 1 Feb 2024 01:06:32 GMT   (4820kb,D)

Title: Multi-group Learning for Hierarchical Groups
Authors: Samuel Deng and Daniel Hsu
Categories: cs.LG
\\
  The multi-group learning model formalizes the learning scenario in which a
single predictor must generalize well on multiple, possibly overlapping
subgroups of interest. We extend the study of multi-group learning to the
natural case where the groups are hierarchically structured. We design an
algorithm for this setting that outputs an interpretable and deterministic
decision tree predictor with near-optimal sample complexity. We then conduct an
empirical evaluation of our algorithm and find that it achieves attractive
generalization properties on real datasets with hierarchical group structure.
\\ ( https://arxiv.org/abs/2402.00258 ,  4820kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00306
Date: Thu, 1 Feb 2024 03:39:15 GMT   (1294kb,D)

Title: An Accurate and Low-Parameter Machine Learning Architecture for Next
  Location Prediction
Authors: Calvin Jary and Nafiseh Kahani
Categories: cs.LG cs.AI
Comments: 7 page conference paper. Paper was accepted and presented in person
  at the 2023 IEEE Future Networks World Forum, in Baltimore, Maryland, USA
\\
  Next location prediction is a discipline that involves predicting a users
next location. Its applications include resource allocation, quality of
service, energy efficiency, and traffic management. This paper proposes an
energy-efficient, small, and low parameter machine learning (ML) architecture
for accurate next location prediction, deployable on modest base stations and
edge devices. To accomplish this we ran a hundred hyperparameter experiments on
the full human mobility patterns of an entire city, to determine an exact ML
architecture that reached a plateau of accuracy with the least amount of model
parameters. We successfully achieved a reduction in the number of model
parameters within published ML architectures from 202 million down to 2
million. This reduced the total size of the model parameters from 791 MB down
to 8 MB. Additionally, this decreased the training time by a factor of four,
the amount of graphics processing unit (GPU) memory needed for training by a
factor of twenty, and the overall accuracy was increased from 80.16% to 82.54%.
This improvement allows for modest base stations and edge devices which do not
have a large amount of memory or storage, to deploy and utilize the proposed ML
architecture for next location prediction.
\\ ( https://arxiv.org/abs/2402.00306 ,  1294kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00313
Date: Thu, 1 Feb 2024 03:53:56 GMT   (10847kb,D)

Title: Control in Stochastic Environment with Delays: A Model-based
  Reinforcement Learning Approach
Authors: Zhiyuan Yao, Ionut Florescu, Chihoon Lee
Categories: cs.LG cs.SY eess.SY
Comments: Under Review
\\
  In this paper we are introducing a new reinforcement learning method for
control problems in environments with delayed feedback. Specifically, our
method employs stochastic planning, versus previous methods that used
deterministic planning. This allows us to embed risk preference in the policy
optimization problem. We show that this formulation can recover the optimal
policy for problems with deterministic transitions. We contrast our policy with
two prior methods from literature. We apply the methodology to simple tasks to
understand its features. Then, we compare the performance of the methods in
controlling multiple Atari games.
\\ ( https://arxiv.org/abs/2402.00313 ,  10847kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00315
Date: Thu, 1 Feb 2024 03:56:48 GMT   (32kb)

Title: Online Distribution Learning with Local Private Constraints
Authors: Jin Sima and Changlong Wu and Olgica Milenkovic and Wojciech
  Szpankowski
Categories: cs.LG cs.CR cs.DS cs.IT math.IT
\\
  We study the problem of online conditional distribution estimation with
\emph{unbounded} label sets under local differential privacy. Let $\mathcal{F}$
be a distribution-valued function class with unbounded label set. We aim at
estimating an \emph{unknown} function $f\in \mathcal{F}$ in an online fashion
so that at time $t$ when the context $\boldsymbol{x}_t$ is provided we can
generate an estimate of $f(\boldsymbol{x}_t)$ under KL-divergence knowing only
a privatized version of the true labels sampling from $f(\boldsymbol{x}_t)$.
The ultimate objective is to minimize the cumulative KL-risk of a finite
horizon $T$. We show that under $(\epsilon,0)$-local differential privacy of
the privatized labels, the KL-risk grows as
$\tilde{\Theta}(\frac{1}{\epsilon}\sqrt{KT})$ upto poly-logarithmic factors
where $K=|\mathcal{F}|$. This is in stark contrast to the
$\tilde{\Theta}(\sqrt{T\log K})$ bound demonstrated by Wu et al. (2023a) for
bounded label sets. As a byproduct, our results recover a nearly tight upper
bound for the hypothesis selection problem of gopi et al. (2020) established
only for the batch setting.
\\ ( https://arxiv.org/abs/2402.00315 ,  32kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00318
Date: Thu, 1 Feb 2024 04:05:24 GMT   (63kb)

Title: Analog-digital Scheduling for Federated Learning: A
  Communication-Efficient Approach
Authors: Muhammad Faraz Ul Abrar and Nicol\`o Michelusi
Categories: cs.LG cs.IT eess.SP math.IT
\\
  Over-the-air (OTA) computation has recently emerged as a
communication-efficient Federated Learning (FL) paradigm to train machine
learning models over wireless networks. However, its performance is limited by
the device with the worst SNR, resulting in fast yet noisy updates. On the
other hand, allocating orthogonal resource blocks (RB) to individual devices
via digital channels mitigates the noise problem, at the cost of increased
communication latency. In this paper, we address this discrepancy and present
ADFL, a novel Analog-Digital FL scheme: in each round, the parameter server
(PS) schedules each device to either upload its gradient via the analog OTA
scheme or transmit its quantized gradient over an orthogonal RB using the
``digital" scheme. Focusing on a single FL round, we cast the optimal
scheduling problem as the minimization of the mean squared error (MSE) on the
estimated global gradient at the PS, subject to a delay constraint, yielding
the optimal device scheduling configuration and quantization bits for the
digital devices. Our simulation results show that ADFL, by scheduling most of
the devices in the OTA scheme while also occasionally employing the digital
scheme for a few devices, consistently outperforms OTA-only and digital-only
schemes, in both i.i.d. and non-i.i.d. settings.
\\ ( https://arxiv.org/abs/2402.00318 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00324
Date: Thu, 1 Feb 2024 04:17:15 GMT   (2026kb,D)

Title: A Consistent Lebesgue Measure for Multi-label Learning
Authors: Kaan Demir, Bach Nguyen, Bing Xue, Mengjie Zhang
Categories: cs.LG
\\
  Multi-label loss functions are usually non-differentiable, requiring
surrogate loss functions for gradient-based optimisation. The consistency of
surrogate loss functions is not proven and is exacerbated by the conflicting
nature of multi-label loss functions. To directly learn from multiple related,
yet potentially conflicting multi-label loss functions, we propose a Consistent
Lebesgue Measure-based Multi-label Learner (CLML) and prove that CLML can
achieve theoretical consistency under a Bayes risk framework. Empirical
evidence supports our theory by demonstrating that: (1) CLML can consistently
achieve state-of-the-art results; (2) the primary performance factor is the
Lebesgue measure design, as CLML optimises a simpler feedforward model without
additional label graph, perturbation-based conditioning, or semantic
embeddings; and (3) an analysis of the results not only distinguishes CLML's
effectiveness but also highlights inconsistencies between the surrogate and the
desired loss functions.
\\ ( https://arxiv.org/abs/2402.00324 ,  2026kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00326
Date: Thu, 1 Feb 2024 04:17:56 GMT   (2784kb,D)

Title: PirateNets: Physics-informed Deep Learning with Residual Adaptive
  Networks
Authors: Sifan Wang, Bowen Li, Yuhan Chen, Paris Perdikaris
Categories: cs.LG cs.NA math.NA
Comments: 29 Pages, 15 Figures, 8 Tables
\\
  While physics-informed neural networks (PINNs) have become a popular deep
learning framework for tackling forward and inverse problems governed by
partial differential equations (PDEs), their performance is known to degrade
when larger and deeper neural network architectures are employed. Our study
identifies that the root of this counter-intuitive behavior lies in the use of
multi-layer perceptron (MLP) architectures with non-suitable initialization
schemes, which result in poor trainablity for the network derivatives, and
ultimately lead to an unstable minimization of the PDE residual loss. To
address this, we introduce Physics-informed Residual Adaptive Networks
(PirateNets), a novel architecture that is designed to facilitate stable and
efficient training of deep PINN models. PirateNets leverage a novel adaptive
residual connection, which allows the networks to be initialized as shallow
networks that progressively deepen during training. We also show that the
proposed initialization scheme allows us to encode appropriate inductive biases
corresponding to a given PDE system into the network architecture. We provide
comprehensive empirical evidence showing that PirateNets are easier to optimize
and can gain accuracy from considerably increased depth, ultimately achieving
state-of-the-art results across various benchmarks. All code and data
accompanying this manuscript will be made publicly available at
\url{https://github.com/PredictiveIntelligenceLab/jaxpi}.
\\ ( https://arxiv.org/abs/2402.00326 ,  2784kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00332
Date: Thu, 1 Feb 2024 04:35:37 GMT   (87kb,D)

Title: Comparing Spectral Bias and Robustness For Two-Layer Neural Networks:
  SGD vs Adaptive Random Fourier Features
Authors: Aku Kammonen and Lisi Liang and Anamika Pandey and Ra\'ul Tempone
Categories: cs.LG stat.ML
Comments: 6 Pages, 4 Figures; Accepted in the International Conference on
  Scientific Computing and Machine Learning
\\
  We present experimental results highlighting two key differences resulting
from the choice of training algorithm for two-layer neural networks. The
spectral bias of neural networks is well known, while the spectral bias
dependence on the choice of training algorithm is less studied. Our experiments
demonstrate that an adaptive random Fourier features algorithm (ARFF) can yield
a spectral bias closer to zero compared to the stochastic gradient descent
optimizer (SGD). Additionally, we train two identically structured classifiers,
employing SGD and ARFF, to the same accuracy levels and empirically assess
their robustness against adversarial noise attacks.
\\ ( https://arxiv.org/abs/2402.00332 ,  87kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00342
Date: Thu, 1 Feb 2024 05:13:14 GMT   (3144kb,D)

Title: Survey of Privacy Threats and Countermeasures in Federated Learning
Authors: Masahiro Hayashitani, Junki Mori, and Isamu Teranishi
Categories: cs.LG cs.CR
Comments: Scheduled for renewal by March 2024
\\
  Federated learning is widely considered to be as a privacy-aware learning
method because no training data is exchanged directly between clients.
Nevertheless, there are threats to privacy in federated learning, and privacy
countermeasures have been studied. However, we note that common and unique
privacy threats among typical types of federated learning have not been
categorized and described in a comprehensive and specific way. In this paper,
we describe privacy threats and countermeasures for the typical types of
federated learning; horizontal federated learning, vertical federated learning,
and transfer federated learning.
\\ ( https://arxiv.org/abs/2402.00342 ,  3144kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00347
Date: Thu, 1 Feb 2024 05:28:28 GMT   (5201kb,D)

Title: Diverse Explanations from Data-driven and Domain-driven Perspectives for
  Machine Learning Models
Authors: Sichao Li and Amanda Barnard
Categories: cs.LG
\\
  Explanations of machine learning models are important, especially in
scientific areas such as chemistry, biology, and physics, where they guide
future laboratory experiments and resource requirements. These explanations can
be derived from well-trained machine learning models (data-driven perspective)
or specific domain knowledge (domain-driven perspective). However, there exist
inconsistencies between these perspectives due to accurate yet misleading
machine learning models and various stakeholders with specific needs, wants, or
aims. This paper calls attention to these inconsistencies and suggests a way to
find an accurate model with expected explanations that reinforce physical laws
and meet stakeholders' requirements from a set of equally-good models, also
known as Rashomon sets. Our goal is to foster a comprehensive understanding of
these inconsistencies and ultimately contribute to the integration of
eXplainable Artificial Intelligence (XAI) into scientific domains.
\\ ( https://arxiv.org/abs/2402.00347 ,  5201kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00348
Date: Thu, 1 Feb 2024 05:30:51 GMT   (704kb,D)

Title: ODICE: Revealing the Mystery of Distribution Correction Estimation via
  Orthogonal-gradient Update
Authors: Liyuan Mao, Haoran Xu, Weinan Zhang, Xianyuan Zhan
Categories: cs.LG cs.AI
Comments: Spotlight @ ICLR 2024, first two authors contribute equally
\\
  In this study, we investigate the DIstribution Correction Estimation (DICE)
methods, an important line of work in offline reinforcement learning (RL) and
imitation learning (IL). DICE-based methods impose state-action-level behavior
constraint, which is an ideal choice for offline learning. However, they
typically perform much worse than current state-of-the-art (SOTA) methods that
solely use action-level behavior constraint. After revisiting DICE-based
methods, we find there exist two gradient terms when learning the value
function using true-gradient update: forward gradient (taken on the current
state) and backward gradient (taken on the next state). Using forward gradient
bears a large similarity to many offline RL methods, and thus can be regarded
as applying action-level constraint. However, directly adding the backward
gradient may degenerate or cancel out its effect if these two gradients have
conflicting directions. To resolve this issue, we propose a simple yet
effective modification that projects the backward gradient onto the normal
plane of the forward gradient, resulting in an orthogonal-gradient update, a
new learning rule for DICE-based methods. We conduct thorough theoretical
analyses and find that the projected backward gradient brings state-level
behavior regularization, which reveals the mystery of DICE-based methods: the
value learning objective does try to impose state-action-level constraint, but
needs to be used in a corrected way. Through toy examples and extensive
experiments on complex offline RL and IL tasks, we demonstrate that DICE-based
methods using orthogonal-gradient updates (O-DICE) achieve SOTA performance and
great robustness.
\\ ( https://arxiv.org/abs/2402.00348 ,  704kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00351
Date: Thu, 1 Feb 2024 05:35:25 GMT   (28130kb,D)

Title: Machine Unlearning for Image-to-Image Generative Models
Authors: Guihong Li, Hsiang Hsu, Chun-Fu (Richard) Chen, Radu Marculescu
Categories: cs.LG cs.CV
Comments: ICLR 2024
\\
  Machine unlearning has emerged as a new paradigm to deliberately forget data
samples from a given model in order to adhere to stringent regulations.
However, existing machine unlearning methods have been primarily focused on
classification models, leaving the landscape of unlearning for generative
models relatively unexplored. This paper serves as a bridge, addressing the gap
by providing a unifying framework of machine unlearning for image-to-image
generative models. Within this framework, we propose a
computationally-efficient algorithm, underpinned by rigorous theoretical
analysis, that demonstrates negligible performance degradation on the retain
samples, while effectively removing the information from the forget samples.
Empirical studies on two large-scale datasets, ImageNet-1K and Places-365,
further show that our algorithm does not rely on the availability of the retain
samples, which further complies with data retention policy. To our best
knowledge, this work is the first that represents systemic, theoretical,
empirical explorations of machine unlearning specifically tailored for
image-to-image generative models. Our code is available at
https://github.com/jpmorganchase/l2l-generator-unlearning.
\\ ( https://arxiv.org/abs/2402.00351 ,  28130kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00355
Date: Thu, 1 Feb 2024 05:53:44 GMT   (11716kb,D)

Title: Adaptive Primal-Dual Method for Safe Reinforcement Learning
Authors: Weiqin Chen, James Onyejizu, Long Vu, Lan Hoang, Dharmashankar
  Subramanian, Koushik Kar, Sandipan Mishra and Santiago Paternain
Categories: cs.LG cs.AI math.OC
\\
  Primal-dual methods have a natural application in Safe Reinforcement Learning
(SRL), posed as a constrained policy optimization problem. In practice however,
applying primal-dual methods to SRL is challenging, due to the inter-dependency
of the learning rate (LR) and Lagrangian multipliers (dual variables) each time
an embedded unconstrained RL problem is solved. In this paper, we propose,
analyze and evaluate adaptive primal-dual (APD) methods for SRL, where two
adaptive LRs are adjusted to the Lagrangian multipliers so as to optimize the
policy in each iteration. We theoretically establish the convergence,
optimality and feasibility of the APD algorithm. Finally, we conduct numerical
evaluation of the practical APD algorithm with four well-known environments in
Bullet-Safey-Gym employing two state-of-the-art SRL algorithms: PPO-Lagrangian
and DDPG-Lagrangian. All experiments show that the practical APD algorithm
outperforms (or achieves comparable performance) and attains more stable
training than the constant LR cases. Additionally, we substantiate the
robustness of selecting the two adaptive LRs by empirical evidence.
\\ ( https://arxiv.org/abs/2402.00355 ,  11716kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00388
Date: Thu, 1 Feb 2024 07:21:30 GMT   (1285kb,D)

Title: Cumulative Distribution Function based General Temporal Point Processes
Authors: Maolin Wang, Yu Pan, Zenglin Xu, Ruocheng Guo, Xiangyu Zhao, Wanyu
  Wang, Yiqi Wang, Zitao Liu, Langming Liu
Categories: cs.LG cs.AI stat.ML
\\
  Temporal Point Processes (TPPs) hold a pivotal role in modeling event
sequences across diverse domains, including social networking and e-commerce,
and have significantly contributed to the advancement of recommendation systems
and information retrieval strategies. Through the analysis of events such as
user interactions and transactions, TPPs offer valuable insights into
behavioral patterns, facilitating the prediction of future trends. However,
accurately forecasting future events remains a formidable challenge due to the
intricate nature of these patterns. The integration of Neural Networks with
TPPs has ushered in the development of advanced deep TPP models. While these
models excel at processing complex and nonlinear temporal data, they encounter
limitations in modeling intensity functions, grapple with computational
complexities in integral computations, and struggle to capture long-range
temporal dependencies effectively. In this study, we introduce the CuFun model,
representing a novel approach to TPPs that revolves around the Cumulative
Distribution Function (CDF). CuFun stands out by uniquely employing a monotonic
neural network for CDF representation, utilizing past events as a scaling
factor. This innovation significantly bolsters the model's adaptability and
precision across a wide range of data scenarios. Our approach addresses several
critical issues inherent in traditional TPP modeling: it simplifies
log-likelihood calculations, extends applicability beyond predefined density
function forms, and adeptly captures long-range temporal patterns. Our
contributions encompass the introduction of a pioneering CDF-based TPP model,
the development of a methodology for incorporating past event information into
future event prediction, and empirical validation of CuFun's effectiveness
through extensive experimentation on synthetic and real-world datasets.
\\ ( https://arxiv.org/abs/2402.00388 ,  1285kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00396
Date: Thu, 1 Feb 2024 07:32:24 GMT   (854kb,D)

Title: Efficient Exploration for LLMs
Authors: Vikranth Dwaracherla, Seyed Mohammad Asghari, Botao Hao, Benjamin Van
  Roy
Categories: cs.LG cs.AI cs.CL stat.ME stat.ML
\\
  We present evidence of substantial benefit from efficient exploration in
gathering human feedback to improve large language models. In our experiments,
an agent sequentially generates queries while fitting a reward model to the
feedback received. Our best-performing agent generates queries using double
Thompson sampling, with uncertainty represented by an epistemic neural network.
Our results demonstrate that efficient exploration enables high levels of
performance with far fewer queries. Further, both uncertainty estimation and
the choice of exploration scheme play critical roles.
\\ ( https://arxiv.org/abs/2402.00396 ,  854kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00397
Date: Thu, 1 Feb 2024 07:33:31 GMT   (3662kb,D)

Title: Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic
  Forecasting
Authors: Zhanyu Liu, Guanjie Zheng, Yanwei Yu
Categories: cs.LG cs.AI
Comments: Under review. Text overlap with arXiv:2308.09727
\\
  Traffic forecasting is crucial for intelligent transportation systems (ITS),
aiding in efficient resource allocation and effective traffic control. However,
its effectiveness often relies heavily on abundant traffic data, while many
cities lack sufficient data due to limited device support, posing a significant
challenge for traffic forecasting. Recognizing this challenge, we have made a
noteworthy observation: traffic patterns exhibit similarities across diverse
cities. Building on this key insight, we propose a solution for the cross-city
few-shot traffic forecasting problem called Multi-scale Traffic Pattern Bank
(MTPB). Primarily, MTPB initiates its learning process by leveraging data-rich
source cities, effectively acquiring comprehensive traffic knowledge through a
spatial-temporal-aware pre-training process. Subsequently, the framework
employs advanced clustering techniques to systematically generate a multi-scale
traffic pattern bank derived from the learned knowledge. Next, the traffic data
of the data-scarce target city could query the traffic pattern bank,
facilitating the aggregation of meta-knowledge. This meta-knowledge, in turn,
assumes a pivotal role as a robust guide in subsequent processes involving
graph reconstruction and forecasting. Empirical assessments conducted on
real-world traffic datasets affirm the superior performance of MTPB, surpassing
existing methods across various categories and exhibiting numerous attributes
conducive to the advancement of cross-city few-shot forecasting methodologies.
The code is available in https://github.com/zhyliu00/MTPB.
\\ ( https://arxiv.org/abs/2402.00397 ,  3662kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00433
Date: Thu, 1 Feb 2024 08:58:57 GMT   (1994kb,D)

Title: Merging Multi-Task Models via Weight-Ensembling Mixture of Experts
Authors: Anke Tang, Li Shen, Yong Luo, Nan Yin, Lefei Zhang, Dacheng Tao
Categories: cs.LG cs.CV
\\
  Merging various task-specific Transformer-based models trained on different
tasks into a single unified model can execute all the tasks concurrently.
Previous methods, exemplified by task arithmetic, have been proven to be both
effective and scalable. Existing methods have primarily focused on seeking a
static optimal solution within the original model parameter space. A notable
challenge is mitigating the interference between parameters of different
models, which can substantially deteriorate performance. In this paper, we
propose to merge most of the parameters while upscaling the MLP of the
Transformer layers to a weight-ensembling mixture of experts (MoE) module,
which can dynamically integrate shared and task-specific knowledge based on the
input, thereby providing a more flexible solution that can adapt to the
specific needs of each instance. Our key insight is that by identifying and
separating shared knowledge and task-specific knowledge, and then dynamically
integrating them, we can mitigate the parameter interference problem to a great
extent. We conduct the conventional multi-task model merging experiments and
evaluate the generalization and robustness of our method. The results
demonstrate the effectiveness of our method and provide a comprehensive
understanding of our method. The code is available at
https://anonymous.4open.science/r/weight-ensembling_MoE-67C9/
\\ ( https://arxiv.org/abs/2402.00433 ,  1994kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00447
Date: Thu, 1 Feb 2024 09:28:48 GMT   (37kb)

Title: A Survey of Data-Efficient Graph Learning
Authors: Wei Ju, Siyu Yi, Yifan Wang, Qingqing Long, Junyu Luo, Zhiping Xiao,
  Ming Zhang
Categories: cs.LG cs.AI cs.SI
\\
  Graph-structured data, prevalent in domains ranging from social networks to
biochemical analysis, serve as the foundation for diverse real-world systems.
While graph neural networks demonstrate proficiency in modeling this type of
data, their success is often reliant on significant amounts of labeled data,
posing a challenge in practical scenarios with limited annotation resources. To
tackle this problem, tremendous efforts have been devoted to enhancing graph
machine learning performance under low-resource settings by exploring various
approaches to minimal supervision. In this paper, we introduce a novel concept
of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the
first survey that summarizes the current progress of DEGL. We initiate by
highlighting the challenges inherent in training models with large labeled
data, paving the way for our exploration into DEGL. Next, we systematically
review recent advances on this topic from several key aspects, including
self-supervised graph learning, semi-supervised graph learning, and few-shot
graph learning. Also, we state promising directions for future research,
contributing to the evolution of graph machine learning.
\\ ( https://arxiv.org/abs/2402.00447 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00450
Date: Thu, 1 Feb 2024 09:36:56 GMT   (997kb,D)

Title: CPT: Competence-progressive Training Strategy for Few-shot Node
  Classification
Authors: Qilong Yan, Yufeng Zhang, Jinghao Zhang, Jingpu Duan, Jian Yin
Categories: cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2206.11972 by
  other authors
\\
  Graph Neural Networks (GNNs) have made significant advancements in node
classification, but their success relies on sufficient labeled nodes per class
in the training data. Real-world graph data often exhibits a long-tail
distribution with sparse labels, emphasizing the importance of GNNs' ability in
few-shot node classification, which entails categorizing nodes with limited
data. Traditional episodic meta-learning approaches have shown promise in this
domain, but they face an inherent limitation: it might lead the model to
converge to suboptimal solutions because of random and uniform task assignment,
ignoring task difficulty levels. This could lead the meta-learner to face
complex tasks too soon, hindering proper learning. Ideally, the meta-learner
should start with simple concepts and advance to more complex ones, like human
learning. So, we introduce CPT, a novel two-stage curriculum learning method
that aligns task difficulty with the meta-learner's progressive competence,
enhancing overall performance. Specifically, in CPT's initial stage, the focus
is on simpler tasks, fostering foundational skills for engaging with complex
tasks later. Importantly, the second stage dynamically adjusts task difficulty
based on the meta-learner's growing competence, aiming for optimal knowledge
acquisition. Extensive experiments on popular node classification datasets
demonstrate significant improvements of our strategy over existing methods.
\\ ( https://arxiv.org/abs/2402.00450 ,  997kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00518
Date: Thu, 1 Feb 2024 11:39:04 GMT   (1413kb,D)

Title: EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit
  Large Language Models
Authors: Xuchen Pan, Yanxi Chen, Yaliang Li, Bolin Ding, Jingren Zhou
Categories: cs.LG cs.AI cs.CL
\\
  This work introduces EE-Tuning, a lightweight and economical solution to
training/tuning early-exit large language models (LLMs). In contrast to the
common approach of full-parameter pre-training, EE-Tuning augments any
pre-trained (and possibly fine-tuned) standard LLM with additional early-exit
layers that are tuned in a parameter-efficient manner, which requires
significantly less computational resources and training data. Our
implementation of EE-Tuning achieves outstanding training efficiency via
extensive performance optimizations, as well as scalability due to its full
compatibility with 3D parallelism. Results of systematic experiments validate
the efficacy of EE-Tuning, confirming that effective early-exit LLM inference
can be achieved with a limited training budget. In hope of making early-exit
LLMs accessible to the community, we release the source code of our
implementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.
\\ ( https://arxiv.org/abs/2402.00518 ,  1413kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00522
Date: Thu, 1 Feb 2024 11:43:13 GMT   (95kb)

Title: Understanding the Expressive Power and Mechanisms of Transformer for
  Sequence Modeling
Authors: Mingze Wang, Weinan E
Categories: cs.LG stat.ML
Comments: 65 pages
\\
  We conduct a systematic study of the approximation properties of Transformer
for sequence modeling with long, sparse and complicated memory. We investigate
the mechanisms through which different components of Transformer, such as the
dot-product self-attention, positional encoding and feed-forward layer, affect
its expressive power, and we study their combined effects through establishing
explicit approximation rates. Our study reveals the roles of critical
parameters in the Transformer, such as the number of layers and the number of
attention heads, and these insights also provide natural suggestions for
alternative architectures.
\\ ( https://arxiv.org/abs/2402.00522 ,  95kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00531
Date: Thu, 1 Feb 2024 11:58:28 GMT   (404kb,D)

Title: Preconditioning for Physics-Informed Neural Networks
Authors: Songming Liu, Chang Su, Jiachen Yao, Zhongkai Hao, Hang Su, Youjia Wu,
  Jun Zhu
Categories: cs.LG cs.NA math.NA
\\
  Physics-informed neural networks (PINNs) have shown promise in solving
various partial differential equations (PDEs). However, training pathologies
have negatively affected the convergence and prediction accuracy of PINNs,
which further limits their practical applications. In this paper, we propose to
use condition number as a metric to diagnose and mitigate the pathologies in
PINNs. Inspired by classical numerical analysis, where the condition number
measures sensitivity and stability, we highlight its pivotal role in the
training dynamics of PINNs. We prove theorems to reveal how condition number is
related to both the error control and convergence of PINNs. Subsequently, we
present an algorithm that leverages preconditioning to improve the condition
number. Evaluations of 18 PDE problems showcase the superior performance of our
method. Significantly, in 7 of these problems, our method reduces errors by an
order of magnitude. These empirical findings verify the critical role of the
condition number in PINNs' training.
\\ ( https://arxiv.org/abs/2402.00531 ,  404kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00576
Date: Thu, 1 Feb 2024 13:14:38 GMT   (1902kb,D)

Title: Tropical Decision Boundaries for Neural Networks Are Robust Against
  Adversarial Attacks
Authors: Kurt Pasque and Christopher Teska and Ruriko Yoshida and Keiji Miura
  and Jefferson Huang
Categories: cs.LG cs.CR cs.CV math.CO
\\
  We introduce a simple, easy to implement, and computationally efficient
tropical convolutional neural network architecture that is robust against
adversarial attacks. We exploit the tropical nature of piece-wise linear neural
networks by embedding the data in the tropical projective torus in a single
hidden layer which can be added to any model. We study the geometry of its
decision boundary theoretically and show its robustness against adversarial
attacks on image datasets using computational experiments.
\\ ( https://arxiv.org/abs/2402.00576 ,  1902kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00580
Date: Wed, 31 Jan 2024 05:09:14 GMT   (4243kb,D)

Title: Continuous Unsupervised Domain Adaptation Using Stabilized
  Representations and Experience Replay
Authors: Mohammad Rostami
Categories: cs.LG cs.CV
\\
  We introduce an algorithm for tackling the problem of unsupervised domain
adaptation (UDA) in continual learning (CL) scenarios. The primary objective is
to maintain model generalization under domain shift when new domains arrive
continually through updating a base model when only unlabeled data is
accessible in subsequent tasks. While there are many existing UDA algorithms,
they typically require access to both the source and target domain datasets
simultaneously. Conversely, existing CL approaches can handle tasks that all
have labeled data. Our solution is based on stabilizing the learned internal
distribution to enhances the model generalization on new domains. The internal
distribution is modeled by network responses in hidden layer. We model this
internal distribution using a Gaussian mixture model (GMM ) and update the
model by matching the internally learned distribution of new domains to the
estimated GMM. Additionally, we leverage experience replay to overcome the
problem of catastrophic forgetting, where the model loses previously acquired
knowledge when learning new tasks. We offer theoretical analysis to explain why
our algorithm would work. We also offer extensive comparative and analytic
experiments to demonstrate that our method is effective. We perform experiments
on four benchmark datasets to demonstrate that our approach is effective.
\\ ( https://arxiv.org/abs/2402.00580 ,  4243kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00592
Date: Thu, 1 Feb 2024 13:41:44 GMT   (1931kb,D)

Title: Uncertainty-Aware Partial-Label Learning
Authors: Tobias Fuchs, Florian Kalinke, Klemens B\"ohm
Categories: cs.LG stat.ML
\\
  In real-world applications, one often encounters ambiguously labeled data,
where different annotators assign conflicting class labels. Partial-label
learning allows training classifiers in this weakly supervised setting. While
state-of-the-art methods already feature good predictive performance, they
often suffer from miscalibrated uncertainty estimates. However, having
well-calibrated uncertainty estimates is important, especially in
safety-critical domains like medicine and autonomous driving. In this article,
we propose a novel nearest-neighbor-based partial-label-learning algorithm that
leverages Dempster-Shafer theory. Extensive experiments on artificial and
real-world datasets show that the proposed method provides a well-calibrated
uncertainty estimate and achieves competitive prediction performance.
Additionally, we prove that our algorithm is risk-consistent.
\\ ( https://arxiv.org/abs/2402.00592 ,  1931kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00607
Date: Thu, 1 Feb 2024 13:59:04 GMT   (6968kb,D)

Title: Are Synthetic Time-series Data Really not as Good as Real Data?
Authors: Fanzhe Fu, Junru Chen, Jing Zhang, Carl Yang, Lvbin Ma, Yang Yang
Categories: cs.LG cs.AI
\\
  Time-series data presents limitations stemming from data quality issues, bias
and vulnerabilities, and generalization problem. Integrating universal data
synthesis methods holds promise in improving generalization. However, current
methods cannot guarantee that the generator's output covers all unseen real
data. In this paper, we introduce InfoBoost -- a highly versatile cross-domain
data synthesizing framework with time series representation learning
capability. We have developed a method based on synthetic data that enables
model training without the need for real data, surpassing the performance of
models trained with real data. Additionally, we have trained a universal
feature extractor based on our synthetic data that is applicable to all
time-series data. Our approach overcomes interference from multiple sources
rhythmic signal, noise interference, and long-period features that exceed
sampling window capabilities. Through experiments, our non-deep-learning
synthetic data enables models to achieve superior reconstruction performance
and universal explicit representation extraction without the need for real
data.
\\ ( https://arxiv.org/abs/2402.00607 ,  6968kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00608
Date: Thu, 1 Feb 2024 14:02:06 GMT   (3171kb,D)

Title: Deep Clustering Using the Soft Silhouette Score: Towards Compact and
  Well-Separated Clusters
Authors: Georgios Vardakas, Ioannis Papakostas, Aristidis Likas
Categories: cs.LG cs.CV
\\
  Unsupervised learning has gained prominence in the big data era, offering a
means to extract valuable insights from unlabeled datasets. Deep clustering has
emerged as an important unsupervised category, aiming to exploit the non-linear
mapping capabilities of neural networks in order to enhance clustering
performance. The majority of deep clustering literature focuses on minimizing
the inner-cluster variability in some embedded space while keeping the learned
representation consistent with the original high-dimensional dataset. In this
work, we propose soft silhoutte, a probabilistic formulation of the silhouette
coefficient. Soft silhouette rewards compact and distinctly separated
clustering solutions like the conventional silhouette coefficient. When
optimized within a deep clustering framework, soft silhouette guides the
learned representations towards forming compact and well-separated clusters. In
addition, we introduce an autoencoder-based deep learning architecture that is
suitable for optimizing the soft silhouette objective function. The proposed
deep clustering method has been tested and compared with several well-studied
deep clustering methods on various benchmark datasets, yielding very
satisfactory clustering results.
\\ ( https://arxiv.org/abs/2402.00608 ,  3171kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00638
Date: Thu, 1 Feb 2024 14:54:17 GMT   (836kb)

Title: Random Forest-Based Prediction of Stroke Outcome
Authors: Carlos Fernandez-Lozano, Pablo Hervella, Virginia Mato-Abad, Manuel
  Rodriguez-Yanez, Sonia Suarez-Garaboa, Iria Lopez-Dequidt, Ana Estany-Gestal,
  Tomas Sobrino, Francisco Campos, Jose Castillo, Santiago Rodriguez-Yanez and
  Ramon Iglesias-Rey
Categories: cs.LG
Comments: 12 pages, 5 figures
DOI: 10.1038/s41598-021-89434-7
\\
  We research into the clinical, biochemical and neuroimaging factors
associated with the outcome of stroke patients to generate a predictive model
using machine learning techniques for prediction of mortality and morbidity 3
months after admission. The dataset consisted of patients with ischemic stroke
(IS) and non-traumatic intracerebral hemorrhage (ICH) admitted to Stroke Unit
of a European Tertiary Hospital prospectively registered. We identified the
main variables for machine learning Random Forest (RF), generating a predictive
model that can estimate patient mortality/morbidity. In conclusion, machine
learning algorithms RF can be effectively used in stroke patients for long-term
outcome prediction of mortality and morbidity.
\\ ( https://arxiv.org/abs/2402.00638 ,  836kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00654
Date: Thu, 1 Feb 2024 15:14:16 GMT   (3754kb)

Title: Improving the accuracy of freight mode choice models: A case study using
  the 2017 CFS PUF data set and ensemble learning techniques
Authors: Diyi Liu, Hyeonsup Lim, Majbah Uddin, Yuandong Liu, Lee D. Han,
  Ho-ling Hwang, Shih-Miao Chin
Categories: cs.LG
Journal-ref: Expert Systems with Applications, 240, 122478 (2024)
DOI: 10.1016/j.eswa.2023.122478
\\
  The US Census Bureau has collected two rounds of experimental data from the
Commodity Flow Survey, providing shipment-level characteristics of nationwide
commodity movements, published in 2012 (i.e., Public Use Microdata) and in 2017
(i.e., Public Use File). With this information, data-driven methods have become
increasingly valuable for understanding detailed patterns in freight logistics.
In this study, we used the 2017 Commodity Flow Survey Public Use File data set
to explore building a high-performance freight mode choice model, considering
three main improvements: (1) constructing local models for each separate
commodity/industry category; (2) extracting useful geographical features,
particularly the derived distance of each freight mode between
origin/destination zones; and (3) applying additional ensemble learning methods
such as stacking or voting to combine results from local and unified models for
improved performance. The proposed method achieved over 92% accuracy without
incorporating external information, an over 19% increase compared to directly
fitting Random Forests models over 10,000 samples. Furthermore, SHAP (Shapely
Additive Explanations) values were computed to explain the outputs and major
patterns obtained from the proposed model. The model framework could enhance
the performance and interpretability of existing freight mode choice models.
\\ ( https://arxiv.org/abs/2402.00654 ,  3754kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00659
Date: Thu, 1 Feb 2024 15:18:48 GMT   (705kb)

Title: Modeling Freight Mode Choice Using Machine Learning Classifiers: A
  Comparative Study Using the Commodity Flow Survey (CFS) Data
Authors: Majbah Uddin, Sabreena Anowar, and Naveen Eluru
Categories: cs.LG
Journal-ref: Transportation Planning and Technology, 44(5), 543-559 (2021)
DOI: 10.1080/03081060.2021.1927306
\\
  This study explores the usefulness of machine learning classifiers for
modeling freight mode choice. We investigate eight commonly used machine
learning classifiers, namely Naive Bayes, Support Vector Machine, Artificial
Neural Network, K-Nearest Neighbors, Classification and Regression Tree, Random
Forest, Boosting and Bagging, along with the classical Multinomial Logit model.
US 2012 Commodity Flow Survey data are used as the primary data source; we
augment it with spatial attributes from secondary data sources. The performance
of the classifiers is compared based on prediction accuracy results. The
current research also examines the role of sample size and training-testing
data split ratios on the predictive ability of the various approaches. In
addition, the importance of variables is estimated to determine how the
variables influence freight mode choice. The results show that the tree-based
ensemble classifiers perform the best. Specifically, Random Forest produces the
most accurate predictions, closely followed by Boosting and Bagging. With
regard to variable importance, shipment characteristics, such as shipment
distance, industry classification of the shipper and shipment size, are the
most significant factors for freight mode choice decisions.
\\ ( https://arxiv.org/abs/2402.00659 ,  705kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00705
Date: Thu, 1 Feb 2024 16:00:21 GMT   (659kb)

Title: Combining the Strengths of Dutch Survey and Register Data in a Data
  Challenge to Predict Fertility (PreFer)
Authors: Elizaveta Sivak, Paulina Pankowska, Adrienne Mendrik, Tom Emery,
  Javier Garcia-Bernardo, Seyit Hocuk, Kasia Karpinska, Angelica Maineri, Joris
  Mulder, Malvina Nissim, Gert Stulp
Categories: cs.LG cs.DB
\\
  The social sciences have produced an impressive body of research on
determinants of fertility outcomes, or whether and when people have children.
However, the strength of these determinants and underlying theories are rarely
evaluated on their predictive ability on new data. This prevents us from
systematically comparing studies, hindering the evaluation and accumulation of
knowledge. In this paper, we present two datasets which can be used to study
the predictability of fertility outcomes in the Netherlands. One dataset is
based on the LISS panel, a longitudinal survey which includes thousands of
variables on a wide range of topics, including individual preferences and
values. The other is based on the Dutch register data which lacks attitudinal
data but includes detailed information about the life courses of millions of
Dutch residents. We provide information about the datasets and the samples, and
describe the fertility outcome of interest. We also introduce the fertility
prediction data challenge PreFer which is based on these datasets and will
start in Spring 2024. We outline the ways in which measuring the predictability
of fertility outcomes using these datasets and combining their strengths in the
data challenge can advance our understanding of fertility behaviour and
computational social science. We further provide details for participants on
how to take part in the data challenge.
\\ ( https://arxiv.org/abs/2402.00705 ,  659kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00711
Date: Thu, 1 Feb 2024 16:06:35 GMT   (753kb,D)

Title: Explaining Text Classifiers with Counterfactual Representations
Authors: Pirmin Lemberger, Antoine Saillenfest
Categories: cs.LG cs.CL
Comments: 12 pages, 4 figures
MSC-class: 62Fxx
\\
  One well motivated explanation method for classifiers leverages
counterfactuals which are hypothetical events identical to real observations in
all aspects except for one categorical feature. Constructing such
counterfactual poses specific challenges for texts, however, as some attribute
values may not necessarily align with plausible real-world events. In this
paper we propose a simple method for generating counterfactuals by intervening
in the space of text representations which bypasses this limitation. We argue
that our interventions are minimally disruptive and that they are theoretically
sound as they align with counterfactuals as defined in Pearl's causal inference
framework. To validate our method, we first conduct experiments on a synthetic
dataset of counterfactuals, allowing for a direct comparison between classifier
predictions based on ground truth counterfactuals (obtained through explicit
text interventions) and our counterfactuals, derived through interventions in
the representation space. Second, we study a real world scenario where our
counterfactuals can be leveraged both for explaining a classifier and for bias
mitigation.
\\ ( https://arxiv.org/abs/2402.00711 ,  753kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00728
Date: Thu, 1 Feb 2024 16:25:00 GMT   (24451kb,D)

Title: Dropout-Based Rashomon Set Exploration for Efficient Predictive
  Multiplicity Estimation
Authors: Hsiang Hsu, Guihong Li, Shaohan Hu, Chun-Fu (Richard) Chen
Categories: cs.LG stat.ML
Comments: ICLR 2024
\\
  Predictive multiplicity refers to the phenomenon in which classification
tasks may admit multiple competing models that achieve almost-equally-optimal
performance, yet generate conflicting outputs for individual samples. This
presents significant concerns, as it can potentially result in systemic
exclusion, inexplicable discrimination, and unfairness in practical
applications. Measuring and mitigating predictive multiplicity, however, is
computationally challenging due to the need to explore all such
almost-equally-optimal models, known as the Rashomon set, in potentially huge
hypothesis spaces. To address this challenge, we propose a novel framework that
utilizes dropout techniques for exploring models in the Rashomon set. We
provide rigorous theoretical derivations to connect the dropout parameters to
properties of the Rashomon set, and empirically evaluate our framework through
extensive experimentation. Numerical results show that our technique
consistently outperforms baselines in terms of the effectiveness of predictive
multiplicity metric estimation, with runtime speedup up to $20\times \sim
5000\times$. With efficient Rashomon set exploration and metric estimation,
mitigation of predictive multiplicity is then achieved through dropout ensemble
and model selection.
\\ ( https://arxiv.org/abs/2402.00728 ,  24451kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00732
Date: Thu, 1 Feb 2024 16:30:00 GMT   (332kb,D)

Title: MobilityDL: A Review of Deep Learning From Trajectory Data
Authors: Anita Graser, Anahid Jalali, Jasmin Lampert, Axel Wei{\ss}enfeld,
  Krzysztof Janowicz
Categories: cs.LG
Comments: Submitted to Geoinformatica
\\
  Trajectory data combines the complexities of time series, spatial data, and
(sometimes irrational) movement behavior. As data availability and computing
power have increased, so has the popularity of deep learning from trajectory
data. This review paper provides the first comprehensive overview of deep
learning approaches for trajectory data. We have identified eight specific
mobility use cases which we analyze with regards to the deep learning models
and the training data used. Besides a comprehensive quantitative review of the
literature since 2018, the main contribution of our work is the data-centric
analysis of recent work in this field, placing it along the mobility data
continuum which ranges from detailed dense trajectories of individual movers
(quasi-continuous tracking data), to sparse trajectories (such as check-in
data), and aggregated trajectories (crowd information).
\\ ( https://arxiv.org/abs/2402.00732 ,  332kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00743
Date: Thu, 1 Feb 2024 16:39:45 GMT   (4371kb,D)

Title: Benefits of Transformer: In-Context Learning in Linear Regression Tasks
  with Unstructured Data
Authors: Yue Xing, Xiaofeng Lin, Namjoon Suh, Qifan Song, Guang Cheng
Categories: cs.LG cs.CL stat.ML
\\
  In practice, it is observed that transformer-based models can learn concepts
in context in the inference stage. While existing literature, e.g.,
\citet{zhang2023trained,huang2023context}, provide theoretical explanations on
this in-context learning ability, they assume the input $x_i$ and the output
$y_i$ for each sample are embedded in the same token (i.e., structured data).
However, in reality, they are presented in two tokens (i.e., unstructured data
\cite{wibisono2023role}). In this case, this paper conducts experiments in
linear regression tasks to study the benefits of the architecture of
transformers and provides some corresponding theoretical intuitions to explain
why the transformer can learn from unstructured data. We study the exact
components in a transformer that facilitate the in-context learning. In
particular, we observe that (1) a transformer with two layers of softmax
(self-)attentions with look-ahead attention mask can learn from the prompt if
$y_i$ is in the token next to $x_i$ for each example; (2) positional encoding
can further improve the performance; and (3) multi-head attention with a high
input embedding dimension has a better prediction performance than single-head
attention.
\\ ( https://arxiv.org/abs/2402.00743 ,  4371kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00751
Date: Thu, 1 Feb 2024 16:43:04 GMT   (266kb,D)

Title: Unlearnable Algorithms for In-context Learning
Authors: Andrei Muresanu, Anvith Thudi, Michael R. Zhang, Nicolas Papernot
Categories: cs.LG cs.AI cs.CR
\\
  Machine unlearning is a desirable operation as models get increasingly
deployed on data with unknown provenance. However, achieving exact unlearning
-- obtaining a model that matches the model distribution when the data to be
forgotten was never used -- is challenging or inefficient, often requiring
significant retraining. In this paper, we focus on efficient unlearning methods
for the task adaptation phase of a pretrained large language model (LLM). We
observe that an LLM's ability to do in-context learning for task adaptation
allows for efficient exact unlearning of task adaptation training data. We
provide an algorithm for selecting few-shot training examples to prepend to the
prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation
cost is independent of model and dataset size, meaning it scales to large
models and datasets. We additionally compare our approach to fine-tuning
approaches and discuss the trade-offs between the two approaches. This leads us
to propose a new holistic measure of unlearning cost which accounts for varying
inference costs, and conclude that in-context learning can often be more
favourable than fine-tuning for deployments involving unlearning requests.
\\ ( https://arxiv.org/abs/2402.00751 ,  266kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00759
Date: Thu, 1 Feb 2024 16:49:27 GMT   (210kb)

Title: Building Expressive and Tractable Probabilistic Generative Models: A
  Review
Authors: Sahil Sidheekh, Sriraam Natarajan
Categories: cs.LG cs.AI
\\
  We present a comprehensive survey of the advancements and techniques in the
field of tractable probabilistic generative modeling, primarily focusing on
Probabilistic Circuits (PCs). We provide a unified perspective on the inherent
trade-offs between expressivity and the tractability, highlighting the design
principles and algorithmic extensions that have enabled building expressive and
efficient PCs, and provide a taxonomy of the field. We also discuss recent
efforts to build deep and hybrid PCs by fusing notions from deep neural models,
and outline the challenges and open questions that can guide future research in
this evolving field.
\\ ( https://arxiv.org/abs/2402.00759 ,  210kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00761
Date: Thu, 1 Feb 2024 16:51:11 GMT   (292kb)

Title: Control-Theoretic Techniques for Online Adaptation of Deep Neural
  Networks in Dynamical Systems
Authors: Jacob G. Elkins and Farbod Fahimi
Categories: cs.LG cs.NE cs.RO cs.SY eess.SY
Comments: Preprint version
\\
  Deep neural networks (DNNs), trained with gradient-based optimization and
backpropagation, are currently the primary tool in modern artificial
intelligence, machine learning, and data science. In many applications, DNNs
are trained offline, through supervised learning or reinforcement learning, and
deployed online for inference. However, training DNNs with standard
backpropagation and gradient-based optimization gives no intrinsic performance
guarantees or bounds on the DNN, which is essential for applications such as
controls. Additionally, many offline-training and online-inference problems,
such as sim2real transfer of reinforcement learning policies, experience domain
shift from the training distribution to the real-world distribution. To address
these stability and transfer learning issues, we propose using techniques from
control theory to update DNN parameters online. We formulate the
fully-connected feedforward DNN as a continuous-time dynamical system, and we
propose novel last-layer update laws that guarantee desirable error convergence
under various conditions on the time derivative of the DNN input vector. We
further show that training the DNN under spectral normalization controls the
upper bound of the error trajectories of the online DNN predictions, which is
desirable when numerically differentiated quantities or noisy state
measurements are input to the DNN. The proposed online DNN adaptation laws are
validated in simulation to learn the dynamics of the Van der Pol system under
domain shift, where parameters are varied in inference from the training
dataset. The simulations demonstrate the effectiveness of using
control-theoretic techniques to derive performance improvements and guarantees
in DNN-based learning systems.
\\ ( https://arxiv.org/abs/2402.00761 ,  292kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00782
Date: Thu, 1 Feb 2024 17:10:35 GMT   (4788kb,D)

Title: Dense Reward for Free in Reinforcement Learning from Human Feedback
Authors: Alex J. Chan, Hao Sun, Samuel Holt, Mihaela van der Schaar
Categories: cs.LG
\\
  Reinforcement Learning from Human Feedback (RLHF) has been credited as the
key advance that has allowed Large Language Models (LLMs) to effectively follow
instructions and produce useful assistance. Classically, this involves
generating completions from the LLM in response to a query before using a
separate reward model to assign a score to the full completion. As an
auto-regressive process, the LLM has to take many "actions" (selecting
individual tokens) and only receives a single, sparse reward at the end of an
episode, a setup that is known to be difficult to optimise in traditional
reinforcement learning. In this work we leverage the fact that the reward model
contains more information than just its scalar output, in particular, it
calculates an attention map over tokens as part of the transformer
architecture. We use these attention weights to redistribute the reward along
the whole completion, effectively densifying the signal and highlighting the
most important tokens, all without incurring extra computational cost or
requiring any additional modelling. We demonstrate that, theoretically, this
approach is equivalent to potential-based reward shaping, ensuring that the
optimal policy remains unchanged. Empirically, we show that it stabilises
training, accelerates the rate of learning, and, in practical cases, may lead
to better local optima.
\\ ( https://arxiv.org/abs/2402.00782 ,  4788kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00789
Date: Thu, 1 Feb 2024 17:21:53 GMT   (951kb,D)

Title: Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective
  State Spaces
Authors: Chloe Wang, Oleksii Tsepa, Jun Ma, Bo Wang
Categories: cs.LG cs.AI
\\
  Attention mechanisms have been widely used to capture long-range dependencies
among nodes in Graph Transformers. Bottlenecked by the quadratic computational
cost, attention mechanisms fail to scale in large graphs. Recent improvements
in computational efficiency are mainly achieved by attention sparsification
with random or heuristic-based graph subsampling, which falls short in
data-dependent context reasoning. State space models (SSMs), such as Mamba,
have gained prominence for their effectiveness and efficiency in modeling
long-range dependencies in sequential data. However, adapting SSMs to
non-sequential graph data presents a notable challenge. In this work, we
introduce Graph-Mamba, the first attempt to enhance long-range context modeling
in graph networks by integrating a Mamba block with the input-dependent node
selection mechanism. Specifically, we formulate graph-centric node
prioritization and permutation strategies to enhance context-aware reasoning,
leading to a substantial improvement in predictive performance. Extensive
experiments on ten benchmark datasets demonstrate that Graph-Mamba outperforms
state-of-the-art methods in long-range graph prediction tasks, with a fraction
of the computational cost in both FLOPs and GPU memory consumption. The code
and models are publicly available at https://github.com/bowang-lab/Graph-Mamba.
\\ ( https://arxiv.org/abs/2402.00789 ,  951kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00793
Date: Thu, 1 Feb 2024 17:23:54 GMT   (279kb,D)

Title: Distinguishing the Indistinguishable: Human Expertise in Algorithmic
  Prediction
Authors: Rohan Alur, Manish Raghavan, Devavrat Shah
Categories: cs.LG cs.AI cs.HC
\\
  We introduce a novel framework for incorporating human expertise into
algorithmic predictions. Our approach focuses on the use of human judgment to
distinguish inputs which `look the same' to any feasible predictive algorithm.
We argue that this framing clarifies the problem of human/AI collaboration in
prediction tasks, as experts often have access to information -- particularly
subjective information -- which is not encoded in the algorithm's training
data. We use this insight to develop a set of principled algorithms for
selectively incorporating human feedback only when it improves the performance
of any feasible predictor. We find empirically that although algorithms often
outperform their human counterparts on average, human judgment can
significantly improve algorithmic predictions on specific instances (which can
be identified ex-ante). In an X-ray classification task, we find that this
subset constitutes nearly 30% of the patient population. Our approach provides
a natural way of uncovering this heterogeneity and thus enabling effective
human-AI collaboration.
\\ ( https://arxiv.org/abs/2402.00793 ,  279kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00795
Date: Thu, 1 Feb 2024 17:28:10 GMT   (7108kb,D)

Title: LLMs learn governing principles of dynamical systems, revealing an
  in-context neural scaling law
Authors: Toni J.B. Liu, Nicolas Boull\'e, Rapha\"el Sarfati, Christopher J.
  Earls
Categories: cs.LG cs.AI
\\
  Pretrained large language models (LLMs) are surprisingly effective at
performing zero-shot tasks, including time-series forecasting. However,
understanding the mechanisms behind such capabilities remains highly
challenging due to the complexity of the models. In this paper, we study LLMs'
ability to extrapolate the behavior of dynamical systems whose evolution is
governed by principles of physical interest. Our results show that LLaMA 2, a
language model trained primarily on texts, achieves accurate predictions of
dynamical system time series without fine-tuning or prompt engineering.
Moreover, the accuracy of the learned physical rules increases with the length
of the input context window, revealing an in-context version of neural scaling
law. Along the way, we present a flexible and efficient algorithm for
extracting probability density functions of multi-digit numbers directly from
LLMs.
\\ ( https://arxiv.org/abs/2402.00795 ,  7108kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00798
Date: Thu, 1 Feb 2024 17:30:50 GMT   (898kb,D)

Title: Formal-LLM: Integrating Formal Language and Natural Language for
  Controllable LLM-based Agents
Authors: Zelong Li, Wenyue Hua, Hao Wang, He Zhu, Yongfeng Zhang
Categories: cs.LG cs.AI cs.CL cs.FL
Comments: 21 pages, 6 figures; working in process, suggestions are welcome
\\
  Recent advancements on Large Language Models (LLMs) enable AI Agents to
automatically generate and execute multi-step plans to solve complex tasks.
However, since LLM's content generation process is hardly controllable, current
LLM-based agents frequently generate invalid or non-executable plans, which
jeopardizes the performance of the generated plans and corrupts users' trust in
LLM-based agents. In response, this paper proposes a novel ``Formal-LLM''
framework for LLM-based agents by integrating the expressiveness of natural
language and the precision of formal language. Specifically, the framework
allows human users to express their requirements or constraints for the
planning process as an automaton. A stack-based LLM plan generation process is
then conducted under the supervision of the automaton to ensure that the
generated plan satisfies the constraints, making the planning process
controllable. We conduct experiments on both benchmark tasks and practical
real-life tasks, and our framework achieves over 50% overall performance
increase, which validates the feasibility and effectiveness of employing
Formal-LLM to guide the plan generation of agents, preventing the agents from
generating invalid and unsuccessful plans. Further, more controllable LLM-based
agents can facilitate the broader utilization of LLM in application scenarios
where high validity of planning is essential. The work is open-sourced at
https://github.com/agiresearch/Formal-LLM.
\\ ( https://arxiv.org/abs/2402.00798 ,  898kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00803
Date: Thu, 1 Feb 2024 17:40:10 GMT   (206kb,D)

Title: Signal Quality Auditing for Time-series Data
Authors: Chufan Gao, Nicholas Gisolfi, Artur Dubrawski
Categories: cs.LG eess.SP
\\
  Signal quality assessment (SQA) is required for monitoring the reliability of
data acquisition systems, especially in AI-driven Predictive Maintenance (PMx)
application contexts. SQA is vital for addressing "silent failures" of data
acquisition hardware and software, which when unnoticed, misinform the users of
data, creating the risk for incorrect decisions with unintended or even
catastrophic consequences. We have developed an open-source software
implementation of signal quality indices (SQIs) for the analysis of time-series
data. We codify a range of SQIs, demonstrate them using established benchmark
data, and show that they can be effective for signal quality assessment. We
also study alternative approaches to denoising time-series data in an attempt
to improve the quality of the already degraded signal, and evaluate them
empirically on relevant real-world data. To our knowledge, our software toolkit
is the first to provide an open source implementation of a broad range of
signal quality assessment and improvement techniques validated on publicly
available benchmark data for ease of reproducibility. The generality of our
framework can be easily extended to assessing reliability of arbitrary
time-series measurements in complex systems, especially when morphological
patterns of the waveform shapes and signal periodicity are of key interest in
downstream analyses.
\\ ( https://arxiv.org/abs/2402.00803 ,  206kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00807
Date: Thu, 1 Feb 2024 17:44:11 GMT   (907kb,D)

Title: Distilling Conditional Diffusion Models for Offline Reinforcement
  Learning through Trajectory Stitching
Authors: Shangzhe Li and Xinhua Zhang
Categories: cs.LG
\\
  Deep generative models have recently emerged as an effective approach to
offline reinforcement learning. However, their large model size poses
challenges in computation. We address this issue by proposing a knowledge
distillation method based on data augmentation. In particular, high-return
trajectories are generated from a conditional diffusion model, and they are
blended with the original trajectories through a novel stitching algorithm that
leverages a new reward generator. Applying the resulting dataset to behavioral
cloning, the learned shallow policy whose size is much smaller outperforms or
nearly matches deep generative planners on several D4RL benchmarks.
\\ ( https://arxiv.org/abs/2402.00807 ,  907kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00809
Date: Thu, 1 Feb 2024 17:45:26 GMT   (134kb,D)

Title: Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI
Authors: Theodore Papamarkou, Maria Skoularidou, Konstantina Palla, Laurence
  Aitchison, Julyan Arbel, David Dunson, Maurizio Filippone, Vincent Fortuin,
  Philipp Hennig, Aliaksandr Hubin, Alexander Immer, Theofanis Karaletsos,
  Mohammad Emtiyaz Khan, Agustinus Kristiadi, Yingzhen Li, Jose Miguel
  Hernandez Lobato, Stephan Mandt, Christopher Nemeth, Michael A. Osborne, Tim
  G. J. Rudner, David R\"ugamer, Yee Whye Teh, Max Welling, Andrew Gordon
  Wilson, Ruqi Zhang
Categories: cs.LG stat.ML
\\
  In the current landscape of deep learning research, there is a predominant
emphasis on achieving high predictive accuracy in supervised tasks involving
large image and language datasets. However, a broader perspective reveals a
multitude of overlooked metrics, tasks, and data types, such as uncertainty,
active and continual learning, and scientific data, that demand attention.
Bayesian deep learning (BDL) constitutes a promising avenue, offering
advantages across these diverse settings. This paper posits that BDL can
elevate the capabilities of deep learning. It revisits the strengths of BDL,
acknowledges existing challenges, and highlights some exciting research avenues
aimed at addressing these obstacles. Looking ahead, the discussion focuses on
possible ways to combine large-scale foundation models with BDL to unlock their
full potential.
\\ ( https://arxiv.org/abs/2402.00809 ,  134kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00816
Date: Thu, 1 Feb 2024 17:55:08 GMT   (10615kb,D)

Title: Leveraging Approximate Model-based Shielding for Probabilistic Safety
  Guarantees in Continuous Environments
Authors: Alexander W. Goodall, Francesco Belardinelli
Categories: cs.LG cs.AI
Comments: Accepted as an Extended Abstract at AAMAS 2024
\\
  Shielding is a popular technique for achieving safe reinforcement learning
(RL). However, classical shielding approaches come with quite restrictive
assumptions making them difficult to deploy in complex environments,
particularly those with continuous state or action spaces. In this paper we
extend the more versatile approximate model-based shielding (AMBS) framework to
the continuous setting. In particular we use Safety Gym as our test-bed,
allowing for a more direct comparison of AMBS with popular constrained RL
algorithms. We also provide strong probabilistic safety guarantees for the
continuous setting. In addition, we propose two novel penalty techniques that
directly modify the policy gradient, which empirically provide more stable
convergence in our experiments.
\\ ( https://arxiv.org/abs/2402.00816 ,  10615kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00823
Date: Thu, 1 Feb 2024 18:07:33 GMT   (1693kb,D)

Title: SLIM: Skill Learning with Multiple Critics
Authors: David Emukpere, Bingbing Wu, Julien Perez
Categories: cs.LG cs.AI cs.RO
Comments: IEEE ICRA 2024
\\
  Self-supervised skill learning aims to acquire useful behaviors that leverage
the underlying dynamics of the environment. Latent variable models, based on
mutual information maximization, have been particularly successful in this task
but still struggle in the context of robotic manipulation. As it requires
impacting a possibly large set of degrees of freedom composing the environment,
mutual information maximization fails alone in producing useful manipulation
behaviors. To address this limitation, we introduce SLIM, a multi-critic
learning approach for skill discovery with a particular focus on robotic
manipulation. Our main insight is that utilizing multiple critics in an
actor-critic framework to gracefully combine multiple reward functions leads to
a significant improvement in latent-variable skill discovery for robotic
manipulation while overcoming possible interference occurring among rewards
which hinders convergence to useful skills. Furthermore, in the context of
tabletop manipulation, we demonstrate the applicability of our novel skill
discovery approach to acquire safe and efficient motor primitives in a
hierarchical reinforcement learning fashion and leverage them through planning,
surpassing the state-of-the-art approaches for skill discovery by a large
margin.
\\ ( https://arxiv.org/abs/2402.00823 ,  1693kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00849
Date: Thu, 1 Feb 2024 18:40:03 GMT   (960kb,D)

Title: Score-based Causal Representation Learning: Linear and General
  Transformations
Authors: Burak Var{\i}c{\i}, Emre Acart\"urk, Karthikeyan Shanmugam, Ali Tajer
Categories: cs.LG stat.ML
Comments: Linear transformations: stronger results for hard and soft
  interventions than our previous paper Score-based Causal Representation
  Learning with Interventions (https://arxiv.org/abs/2301.08230). General
  transformations: results also appear in our paper General Identifiability and
  Achievability for Causal Representation Learning (arXiv:2310.15450) accepted
  to AISTATS 2024 (oral)
\\
  This paper addresses intervention-based causal representation learning (CRL)
under a general nonparametric latent causal model and an unknown transformation
that maps the latent variables to the observed variables. Linear and general
transformations are investigated. The paper addresses both the
\emph{identifiability} and \emph{achievability} aspects. Identifiability refers
to determining algorithm-agnostic conditions that ensure recovering the true
latent causal variables and the latent causal graph underlying them.
Achievability refers to the algorithmic aspects and addresses designing
algorithms that achieve identifiability guarantees. By drawing novel
connections between \emph{score functions} (i.e., the gradients of the
logarithm of density functions) and CRL, this paper designs a \emph{score-based
class of algorithms} that ensures both identifiability and achievability.
First, the paper focuses on \emph{linear} transformations and shows that one
stochastic hard intervention per node suffices to guarantee identifiability. It
also provides partial identifiability guarantees for soft interventions,
including identifiability up to ancestors for general causal models and perfect
latent graph recovery for sufficiently non-linear causal models. Secondly, it
focuses on \emph{general} transformations and shows that two stochastic hard
interventions per node suffice for identifiability. Notably, one does
\emph{not} need to know which pair of interventional environments have the same
node intervened.
\\ ( https://arxiv.org/abs/2402.00849 ,  960kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00851
Date: Thu, 1 Feb 2024 18:46:28 GMT   (318kb,D)

Title: Data Augmentation Scheme for Raman Spectra with Highly Correlated
  Annotations
Authors: Christoph Lange, Isabel Thiele, Lara Santolin, Sebastian L. Riedel,
  Maxim Borisyak, Peter Neubauer and M. Nicolas Cruz Bournazou
Categories: cs.LG q-bio.QM
\\
  In biotechnology Raman Spectroscopy is rapidly gaining popularity as a
process analytical technology (PAT) that measures cell densities, substrate-
and product concentrations. As it records vibrational modes of molecules it
provides that information non-invasively in a single spectrum. Typically,
partial least squares (PLS) is the model of choice to infer information about
variables of interest from the spectra. However, biological processes are known
for their complexity where convolutional neural networks (CNN) present a
powerful alternative. They can handle non-Gaussian noise and account for beam
misalignment, pixel malfunctions or the presence of additional substances.
However, they require a lot of data during model training, and they pick up
non-linear dependencies in the process variables. In this work, we exploit the
additive nature of spectra in order to generate additional data points from a
given dataset that have statistically independent labels so that a network
trained on such data exhibits low correlations between the model predictions.
We show that training a CNN on these generated data points improves the
performance on datasets where the annotations do not bear the same correlation
as the dataset that was used for model training. This data augmentation
technique enables us to reuse spectra as training data for new contexts that
exhibit different correlations. The additional data allows for building a
better and more robust model. This is of interest in scenarios where large
amounts of historical data are available but are currently not used for model
training. We demonstrate the capabilities of the proposed method using
synthetic spectra of Ralstonia eutropha batch cultivations to monitor
substrate, biomass and polyhydroxyalkanoate (PHA) biopolymer concentrations
during of the experiments.
\\ ( https://arxiv.org/abs/2402.00851 ,  318kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00853
Date: Thu, 1 Feb 2024 18:50:42 GMT   (15970kb,D)

Title: LTAU-FF: Loss Trajectory Analysis for Uncertainty in Atomistic Force
  Fields
Authors: Joshua A. Vita, Amit Samanta, Fei Zhou, Vincenzo Lordi
Categories: cs.LG cond-mat.mtrl-sci
\\
  Model ensembles are simple and effective tools for estimating the prediction
uncertainty of deep learning atomistic force fields. Despite this, widespread
adoption of ensemble-based uncertainty quantification (UQ) techniques is
limited by the high computational costs incurred by ensembles during both
training and inference. In this work we leverage the cumulative distribution
functions (CDFs) of per-sample errors obtained over the course of training to
efficiently represent the model ensemble, and couple them with a distance-based
similarity search in the model latent space. Using these tools, we develop a
simple UQ metric (which we call LTAU) that leverages the strengths of
ensemble-based techniques without requiring the evaluation of multiple models
during either training or inference. As an initial test, we apply our method
towards estimating the epistemic uncertainty in atomistic force fields
(LTAU-FF) and demonstrate that it can be easily calibrated to accurately
predict test errors on multiple datasets from the literature. We then
illustrate the utility of LTAU-FF in two practical applications: 1) tuning the
training-validation gap for an example dataset, and 2) predicting errors in
relaxation trajectories on the OC20 IS2RS task. Though in this work we focus on
the use of LTAU with deep learning atomistic force fields, we emphasize that it
can be readily applied to any regression task, or any ensemble-generation
technique, to provide a reliable and easy-to-implement UQ metric.
\\ ( https://arxiv.org/abs/2402.00853 ,  15970kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00854
Date: Thu, 1 Feb 2024 18:50:50 GMT   (4777kb,D)

Title: SymbolicAI: A framework for logic-based approaches combining generative
  models and solvers
Authors: Marius-Constantin Dinu and Claudiu Leoveanu-Condrei and Markus
  Holzleitner and Werner Zellinger and Sepp Hochreiter
Categories: cs.LG cs.AI cs.SC cs.SE
Comments: 38 pages, 12 figures, external resources: framework is available at
  https://github.com/ExtensityAI/symbolicai and benchmark at
  https://github.com/ExtensityAI/benchmark
\\
  We introduce SymbolicAI, a versatile and modular framework employing a
logic-based approach to concept learning and flow management in generative
processes. SymbolicAI enables the seamless integration of generative models
with a diverse range of solvers by treating large language models (LLMs) as
semantic parsers that execute tasks based on both natural and formal language
instructions, thus bridging the gap between symbolic reasoning and generative
AI. We leverage probabilistic programming principles to tackle complex tasks,
and utilize differentiable and classical programming paradigms with their
respective strengths. The framework introduces a set of polymorphic,
compositional, and self-referential operations for data stream manipulation,
aligning LLM outputs with user objectives. As a result, we can transition
between the capabilities of various foundation models endowed with zero- and
few-shot learning capabilities and specialized, fine-tuned models or solvers
proficient in addressing specific problems. In turn, the framework facilitates
the creation and evaluation of explainable computational graphs. We conclude by
introducing a quality measure and its empirical score for evaluating these
computational graphs, and propose a benchmark that compares various
state-of-the-art LLMs across a set of complex workflows. We refer to the
empirical score as the "Vector Embedding for Relational Trajectory Evaluation
through Cross-similarity", or VERTEX score for short. The framework codebase
and benchmark are linked below.
\\ ( https://arxiv.org/abs/2402.00854 ,  4777kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00857
Date: Thu, 1 Feb 2024 18:54:34 GMT   (4009kb,D)

Title: Early Time Classification with Accumulated Accuracy Gap Control
Authors: Liran Ringel, Regev Cohen, Daniel Freedman, Michael Elad, Yaniv Romano
Categories: cs.LG stat.ML
\\
  Early time classification algorithms aim to label a stream of features
without processing the full input stream, while maintaining accuracy comparable
to that achieved by applying the classifier to the entire input. In this paper,
we introduce a statistical framework that can be applied to any sequential
classifier, formulating a calibrated stopping rule. This data-driven rule
attains finite-sample, distribution-free control of the accuracy gap between
full and early-time classification. We start by presenting a novel method that
builds on the Learn-then-Test calibration framework to control this gap
marginally, on average over i.i.d. instances. As this algorithm tends to yield
an excessively high accuracy gap for early halt times, our main contribution is
the proposal of a framework that controls a stronger notion of error, where the
accuracy gap is controlled conditionally on the accumulated halt times.
Numerical experiments demonstrate the effectiveness, applicability, and
usefulness of our method. We show that our proposed early stopping mechanism
reduces up to 94% of timesteps used for classification while achieving rigorous
accuracy gap control.
\\ ( https://arxiv.org/abs/2402.00857 ,  4009kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2402.00017 (*cross-listing*)
Date: Sat, 30 Dec 2023 21:07:21 GMT   (14010kb,D)

Title: Deploying ADVISER: Impact and Lessons from Using Artificial Intelligence
  for Child Vaccination Uptake in Nigeria
Authors: Opadele Kehinde, Ruth Abdul, Bose Afolabi, Parminder Vir, Corinne
  Namblard, Ayan Mukhopadhyay, Abiodun Adereni
Categories: cs.CY cs.AI
Comments: Accepted for publication at the AAAI Conference on Artificial
  Intelligence (AAAI-24)
\\
  More than 5 million children under five years die from largely preventable or
treatable medical conditions every year, with an overwhelmingly large
proportion of deaths occurring in underdeveloped countries with low vaccination
uptake. One of the United Nations' sustainable development goals (SDG 3) aims
to end preventable deaths of newborns and children under five years of age. We
focus on Nigeria, where the rate of infant mortality is appalling. In
particular, low vaccination uptake in Nigeria is a major driver of more than
2,000 daily deaths of children under the age of five years. In this paper, we
describe our collaboration with government partners in Nigeria to deploy
ADVISER: AI-Driven Vaccination Intervention Optimiser. The framework, based on
an integer linear program that seeks to maximize the cumulative probability of
successful vaccination, is the first successful deployment of an AI-enabled
toolchain for optimizing the allocation of health interventions in Nigeria. In
this paper, we provide a background of the ADVISER framework and present
results, lessons, and success stories of deploying ADVISER to more than 13,000
families in the state of Oyo, Nigeria.
\\ ( https://arxiv.org/abs/2402.00017 ,  14010kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00024 (*cross-listing*)
Date: Fri, 5 Jan 2024 18:31:34 GMT   (975kb,D)

Title: Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule
  Embedding
Authors: Shaghayegh Sadeghi, Alan Bui, Ali Forooghi, Jianguo Lu, Alioune Ngom
Categories: q-bio.BM cs.AI cs.CL cs.LG
\\
  Purpose: Large Language Models (LLMs) like ChatGPT and LLaMA are increasingly
recognized for their potential in the field of cheminformatics, particularly in
interpreting Simplified Molecular Input Line Entry System (SMILES), a standard
method for representing chemical structures. These LLMs can decode SMILES
strings into vector representations, providing a novel approach to
understanding chemical graphs.
  Methods: We investigate the performance of ChatGPT and LLaMA in embedding
SMILES strings. Our evaluation focuses on two key applications: molecular
property (MP) prediction and drug-drug interaction (DDI) prediction, both
essential in drug development and healthcare.
  Results: We find that SMILES embeddings generated using LLaMA outperform
those from ChatGPT in both MP and DDI prediction tasks. Notably, LLaMA-based
SMILES embeddings show results comparable to existing methods in both
prediction tasks.
  Conclusion: The application of LLMs in cheminformatics, particularly in
utilizing SMILES embeddings, shows significant promise for advancing drug
development. This includes improving the prediction of chemical properties and
facilitating the drug discovery process. GitHub:
https://github.com/sshaghayeghs/LLaMA-VS-ChatGPT
\\ ( https://arxiv.org/abs/2402.00024 ,  975kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00025 (*cross-listing*)
Date: Fri, 5 Jan 2024 19:17:55 GMT   (1520kb,D)

Title: Accelerating a Triton Fused Kernel for W4A16 Quantized Inference with
  SplitK work decomposition
Authors: Adnan Hoque, Less Wright, Jamie Yang, Mudhakar Srivatsa and Raghu
  Ganti
Categories: cs.DC cs.AI
\\
  We propose an implementation of an efficient fused matrix multiplication
kernel for W4A16 quantized inference, where we perform dequantization and GEMM
in a fused kernel using a SplitK work decomposition. Our implementation shows
improvement for the type of skinny matrix-matrix multiplications found in
foundation model inference workloads. In particular, this paper surveys the
type of matrix multiplication between a skinny activation matrix and a square
weight matrix. Our results show an average of 65% speed improvement on A100,
and an average of 124% speed improvement on H100 (with a peak of 295%) for a
range of matrix dimensions including those found in a llama-style model, where
m < n = k.
\\ ( https://arxiv.org/abs/2402.00025 ,  1520kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00029 (*cross-listing*)
Date: Sat, 6 Jan 2024 20:57:35 GMT   (2831kb,D)

Title: Exploring Public Opinion on Responsible AI Through The Lens of Cultural
  Consensus Theory
Authors: Necdet Gurkan, Jordan W. Suchow
Categories: cs.CY cs.AI
Journal-ref: Proceedings of the 57th Hawaii International Conference on System
  Sciences, 713-723 (2024)
\\
  As the societal implications of Artificial Intelligence (AI) continue to
grow, the pursuit of responsible AI necessitates public engagement in its
development and governance processes. This involvement is crucial for capturing
diverse perspectives and promoting equitable practices and outcomes. We applied
Cultural Consensus Theory (CCT) to a nationally representative survey dataset
on various aspects of AI to discern beliefs and attitudes about responsible AI
in the United States. Our results offer valuable insights by identifying shared
and contrasting views on responsible AI. Furthermore, these findings serve as
critical reference points for developers and policymakers, enabling them to
more effectively consider individual variances and group-level cultural
perspectives when making significant decisions and addressing the public's
concerns.
\\ ( https://arxiv.org/abs/2402.00029 ,  2831kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00030 (*cross-listing*)
Date: Sat, 6 Jan 2024 21:06:58 GMT   (25kb)

Title: Evolution-Bootstrapped Simulation: Artificial or Human Intelligence:
  Which Came First?
Authors: Paul Alexander Bilokon
Categories: cs.NE cs.AI q-bio.PE
Comments: 6 pages, no figures
\\
  Humans have created artificial intelligence (AI), not the other way around.
This statement is deceptively obvious. In this note, we decided to challenge
this statement as a small, lighthearted Gedankenexperiment. We ask a simple
question: in a world driven by evolution by natural selection, would neural
networks or humans be likely to evolve first? We compare the
Solomonoff--Kolmogorov--Chaitin complexity of the two and find neural networks
(even LLMs) to be significantly simpler than humans. Further, we claim that it
is unnecessary for any complex human-made equipment to exist for there to be
neural networks. Neural networks may have evolved as naturally occurring
objects before humans did as a form of chemical reaction-based or enzyme-based
computation. Now that we know that neural networks can pass the Turing test and
suspect that they may be capable of superintelligence, we ask whether the
natural evolution of neural networks could lead from pure evolution by natural
selection to what we call evolution-bootstrapped simulation. The evolution of
neural networks does not involve irreducible complexity; would easily allow
irreducible complexity to exist in the evolution-bootstrapped simulation; is a
falsifiable scientific hypothesis; and is independent of / orthogonal to the
issue of intelligent design.
\\ ( https://arxiv.org/abs/2402.00030 ,  25kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00033 (*cross-listing*)
Date: Mon, 8 Jan 2024 01:32:49 GMT   (7541kb,D)

Title: LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient
  Image Recognition
Authors: Youbing Hu, Yun Cheng, Anqi Lu, Zhiqiang Cao, Dawei Wei, Jie Liu,
  Zhijun Li
Categories: cs.CV cs.AI
\\
  The Vision Transformer (ViT) excels in accuracy when handling high-resolution
images, yet it confronts the challenge of significant spatial redundancy,
leading to increased computational and memory requirements. To address this, we
present the Localization and Focus Vision Transformer (LF-ViT). This model
operates by strategically curtailing computational demands without impinging on
performance. In the Localization phase, a reduced-resolution image is
processed; if a definitive prediction remains elusive, our pioneering
Neighborhood Global Class Attention (NGCA) mechanism is triggered, effectively
identifying and spotlighting class-discriminative regions based on initial
findings. Subsequently, in the Focus phase, this designated region is used from
the original image to enhance recognition. Uniquely, LF-ViT employs consistent
parameters across both phases, ensuring seamless end-to-end optimization. Our
empirical tests affirm LF-ViT's prowess: it remarkably decreases Deit-S's FLOPs
by 63\% and concurrently amplifies throughput twofold. Code of this project is
at https://github.com/edgeai1/LF-ViT.git.
\\ ( https://arxiv.org/abs/2402.00033 ,  7541kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00034 (*cross-listing*)
Date: Mon, 8 Jan 2024 03:13:09 GMT   (690kb,D)

Title: Why does Prediction Accuracy Decrease over Time? Uncertain Positive
  Learning for Cloud Failure Prediction
Authors: Haozhe Li, Minghua Ma, Yudong Liu, Pu Zhao, Lingling Zheng, Ze Li,
  Yingnong Dang, Murali Chintalapati, Saravan Rajmohan, Qingwei Lin, Dongmei
  Zhang
Categories: cs.DC cs.AI
ACM-class: K.6.3; I.2.0
\\
  With the rapid growth of cloud computing, a variety of software services have
been deployed in the cloud. To ensure the reliability of cloud services, prior
studies focus on failure instance (disk, node, and switch, etc.) prediction.
Once the output of prediction is positive, mitigation actions are taken to
rapidly resolve the underlying failure. According to our real-world practice in
Microsoft Azure, we find that the prediction accuracy may decrease by about 9%
after retraining the models. Considering that the mitigation actions may result
in uncertain positive instances since they cannot be verified after mitigation,
which may introduce more noise while updating the prediction model. To the best
of our knowledge, we are the first to identify this Uncertain Positive Learning
(UPLearning) issue in the real-world cloud failure prediction scenario. To
tackle this problem, we design an Uncertain Positive Learning Risk Estimator
(Uptake) approach. Using two real-world datasets of disk failure prediction and
conducting node prediction experiments in Microsoft Azure, which is a top-tier
cloud provider that serves millions of users, we demonstrate Uptake can
significantly improve the failure prediction accuracy by 5% on average.
\\ ( https://arxiv.org/abs/2402.00034 ,  690kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00037 (*cross-listing*)
Date: Mon, 8 Jan 2024 21:10:18 GMT   (240kb)

Title: Catalyzing Equity in STEM Teams: Harnessing Generative AI for Inclusion
  and Diversity
Authors: Nia Nixon, Yiwen Lin, Lauren Snow
Categories: cs.CY cs.AI
Comments: 21 pages, 0 figure, to be published in Policy Insights from
  Behavioral and Brain Sciences
\\
  Collaboration is key to STEM, where multidisciplinary team research can solve
complex problems. However, inequality in STEM fields hinders their full
potential, due to persistent psychological barriers in underrepresented
students' experience. This paper documents teamwork in STEM and explores the
transformative potential of computational modeling and generative AI in
promoting STEM-team diversity and inclusion. Leveraging generative AI, this
paper outlines two primary areas for advancing diversity, equity, and
inclusion. First, formalizing collaboration assessment with inclusive analytics
can capture fine-grained learner behavior. Second, adaptive, personalized AI
systems can support diversity and inclusion in STEM teams. Four policy
recommendations highlight AI's capacity: formalized collaborative skill
assessment, inclusive analytics, funding for socio-cognitive research, human-AI
teaming for inclusion training. Researchers, educators, policymakers can build
an equitable STEM ecosystem. This roadmap advances AI-enhanced collaboration,
offering a vision for the future of STEM where diverse voices are actively
encouraged and heard within collaborative scientific endeavors.
\\ ( https://arxiv.org/abs/2402.00037 ,  240kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00045 (*cross-listing*)
Date: Mon, 22 Jan 2024 15:08:19 GMT   (33997kb,D)

Title: Detecting Multimedia Generated by Large AI Models: A Survey
Authors: Li Lin, Neeraj Gupta, Yue Zhang, Hainan Ren, Chun-Hao Liu, Feng Ding,
  Xin Wang, Xin Li, Luisa Verdoliva, Shu Hu
Categories: cs.MM cs.AI cs.LG
\\
  The rapid advancement of Large AI Models (LAIMs), particularly diffusion
models and large language models, has marked a new era where AI-generated
multimedia is increasingly integrated into various aspects of daily life.
Although beneficial in numerous fields, this content presents significant
risks, including potential misuse, societal disruptions, and ethical concerns.
Consequently, detecting multimedia generated by LAIMs has become crucial, with
a marked rise in related research. Despite this, there remains a notable gap in
systematic surveys that focus specifically on detecting LAIM-generated
multimedia. Addressing this, we provide the first survey to comprehensively
cover existing research on detecting multimedia (such as text, images, videos,
audio, and multimodal content) created by LAIMs. Specifically, we introduce a
novel taxonomy for detection methods, categorized by media modality, and
aligned with two perspectives: pure detection (aiming to enhance detection
performance) and beyond detection (adding attributes like generalizability,
robustness, and interpretability to detectors). Additionally, we have presented
a brief overview of generation mechanisms, public datasets, and online
detection tools to provide a valuable resource for researchers and
practitioners in this field. Furthermore, we identify current challenges in
detection and propose directions for future research that address unexplored,
ongoing, and emerging issues in detecting multimedia generated by LAIMs. Our
aim for this survey is to fill an academic gap and contribute to global AI
security efforts, helping to ensure the integrity of information in the digital
realm. The project link is
https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.
\\ ( https://arxiv.org/abs/2402.00045 ,  33997kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00069 (*cross-listing*)
Date: Tue, 30 Jan 2024 19:27:16 GMT   (273kb,D)

Title: Using the Abstract Computer Architecture Description Language to Model
  AI Hardware Accelerators
Authors: Mika Markus M\"uller, Alexander Richard Manfred Borst, Konstantin
  L\"ubeck, Alexander Louis-Ferdinand Jung, Oliver Bringmann
Categories: cs.AR cs.AI
Comments: Accepted Version for: MBMV'24
\\
  Artificial Intelligence (AI) has witnessed remarkable growth, particularly
through the proliferation of Deep Neural Networks (DNNs). These powerful models
drive technological advancements across various domains. However, to harness
their potential in real-world applications, specialized hardware accelerators
are essential. This demand has sparked a market for parameterizable AI hardware
accelerators offered by different vendors.
  Manufacturers of AI-integrated products face a critical challenge: selecting
an accelerator that aligns with their product's performance requirements. The
decision involves choosing the right hardware and configuring a suitable set of
parameters. However, comparing different accelerator design alternatives
remains a complex task. Often, engineers rely on data sheets, spreadsheet
calculations, or slow black-box simulators, which only offer a coarse
understanding of the performance characteristics.
  The Abstract Computer Architecture Description Language (ACADL) is a concise
formalization of computer architecture block diagrams, which helps to
communicate computer architecture on different abstraction levels and allows
for inferring performance characteristics. In this paper, we demonstrate how to
use the ACADL to model AI hardware accelerators, use their ACADL description to
map DNNs onto them, and explain the timing simulation semantics to gather
performance results.
\\ ( https://arxiv.org/abs/2402.00069 ,  273kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00070 (*cross-listing*)
Date: Tue, 30 Jan 2024 19:37:21 GMT   (53kb,D)

Title: EvoMerge: Neuroevolution for Large Language Models
Authors: Yushu Jiang
Categories: cs.NE cs.AI cs.CL cs.LG
Comments: The current submission is the first draft, published for the sole
  purpose of sharing an idea and encouraging community effort. A more
  consolidated version may come later
\\
  Extensive fine-tuning on Large Language Models does not always yield better
results. Oftentimes, models tend to get better at imitating one form of data
without gaining greater reasoning ability and may even end up losing some
intelligence. Here I introduce EvoMerge, a systematic approach to large
language model training and merging. Leveraging model merging for weight
crossover and fine-tuning for weight mutation, EvoMerge establishes an
evolutionary process aimed at pushing models beyond the limits of conventional
fine-tuning.
\\ ( https://arxiv.org/abs/2402.00070 ,  53kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00089 (*cross-listing*)
Date: Wed, 31 Jan 2024 10:25:45 GMT   (4038kb)

Title: SCAPE: Searching Conceptual Architecture Prompts using Evolution
Authors: Soo Ling Lim, Peter J Bentley, Fuyuki Ishikawa
Categories: cs.NE cs.AI
Comments: 8 pages
MSC-class: 68W50, 68T07
ACM-class: G.1.6; I.2.10
\\
  Conceptual architecture involves a highly creative exploration of novel
ideas, often taken from other disciplines as architects consider radical new
forms, materials, textures and colors for buildings. While today's generative
AI systems can produce remarkable results, they lack the creativity
demonstrated for decades by evolutionary algorithms. SCAPE, our proposed tool,
combines evolutionary search with generative AI, enabling users to explore
creative and good quality designs inspired by their initial input through a
simple point and click interface. SCAPE injects randomness into generative AI,
and enables memory, making use of the built-in language skills of GPT-4 to vary
prompts via text-based mutation and crossover. We demonstrate that compared to
DALL-E 3, SCAPE enables a 67% improvement in image novelty, plus improvements
in quality and effectiveness of use; we show that in just 3 iterations SCAPE
has a 24% image novelty increase enabling effective exploration, plus
optimization of images by users. We use more than 20 independent architects to
assess SCAPE, who provide markedly positive feedback.
\\ ( https://arxiv.org/abs/2402.00089 ,  4038kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00094 (*cross-listing*)
Date: Wed, 31 Jan 2024 14:49:44 GMT   (25kb)

Title: Deep Neural Networks: A Formulation Via Non-Archimedean Analysis
Authors: W. A. Z\'u\~niga-Galindo
Categories: cs.NE cs.AI cs.LG
MSC-class: Primary 68T07, 65D15, Secondary 41A30, 11S85
\\
  We introduce a new class of deep neural networks (DNNs) with multilayered
tree-like architectures. The architectures are codified using numbers from the
ring of integers of non-Archimdean local fields. These rings have a natural
hierarchical organization as infinite rooted trees. Natural morphisms on these
rings allow us to construct finite multilayered architectures. The new DNNs are
robust universal approximators of real-valued functions defined on the
mentioned rings. We also show that the DNNs are robust universal approximators
of real-valued square-integrable functions defined in the unit interval.
\\ ( https://arxiv.org/abs/2402.00094 ,  25kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00135 (*cross-listing*)
Date: Wed, 31 Jan 2024 19:20:56 GMT   (6200kb,D)

Title: A Reinforcement Learning Based Controller to Minimize Forces on the
  Crutches of a Lower-Limb Exoskeleton
Authors: Aydin Emre Utku, Suzan Ece Ada, Muhammet Hatipoglu, Mustafa Derman,
  Emre Ugur and Evren Samur
Categories: cs.RO cs.AI cs.SY eess.SY
Comments: 6 pages, 5 Figures
\\
  Metabolic energy consumption of a powered lower-limb exoskeleton user mainly
comes from the upper body effort since the lower body is considered to be
passive. However, the upper body effort of the users is largely ignored in the
literature when designing motion controllers. In this work, we use deep
reinforcement learning to develop a locomotion controller that minimizes ground
reaction forces (GRF) on crutches. The rationale for minimizing GRF is to
reduce the upper body effort of the user. Accordingly, we design a model and a
learning framework for a human-exoskeleton system with crutches. We formulate a
reward function to encourage the forward displacement of a human-exoskeleton
system while satisfying the predetermined constraints of a physical robot. We
evaluate our new framework using Proximal Policy Optimization, a
state-of-the-art deep reinforcement learning (RL) method, on the MuJoCo physics
simulator with different hyperparameters and network architectures over
multiple trials. We empirically show that our learning model can generate joint
torques based on the joint angle, velocities, and the GRF on the feet and
crutch tips. The resulting exoskeleton model can directly generate joint
torques from states in line with the RL framework. Finally, we empirically show
that policy trained using our method can generate a gait with a 35% reduction
in GRF with respect to the baseline.
\\ ( https://arxiv.org/abs/2402.00135 ,  6200kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00138 (*cross-listing*)
Date: Wed, 31 Jan 2024 19:32:33 GMT   (72kb)

Title: Decomposable Submodular Maximization in Federated Setting
Authors: Akbar Rafiey
Categories: cs.DS cs.AI cs.DC cs.LG
\\
  Submodular functions, as well as the sub-class of decomposable submodular
functions, and their optimization appear in a wide range of applications in
machine learning, recommendation systems, and welfare maximization. However,
optimization of decomposable submodular functions with millions of component
functions is computationally prohibitive. Furthermore, the component functions
may be private (they might represent user preference function, for example) and
cannot be widely shared. To address these issues, we propose a {\em federated
optimization} setting for decomposable submodular optimization. In this
setting, clients have their own preference functions, and a weighted sum of
these preferences needs to be maximized. We implement the popular {\em
continuous greedy} algorithm in this setting where clients take parallel small
local steps towards the local solution and then the local changes are
aggregated at a central server. To address the large number of clients, the
aggregation is performed only on a subsampled set. Further, the aggregation is
performed only intermittently between stretches of parallel local steps, which
reduces communication cost significantly. We show that our federated algorithm
is guaranteed to provide a good approximate solution, even in the presence of
above cost-cutting measures. Finally, we show how the federated setting can be
incorporated in solving fundamental discrete submodular optimization problems
such as Maximum Coverage and Facility Location.
\\ ( https://arxiv.org/abs/2402.00138 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00163 (*cross-listing*)
Date: Wed, 31 Jan 2024 20:37:35 GMT   (15332kb,D)

Title: Improving Object Detection Quality in Football Through Super-Resolution
  Techniques
Authors: Karolina Seweryn, Gabriel Ch\k{e}\'c, Szymon {\L}ukasik, Anna
  Wr\'oblewska
Categories: cs.CV cs.AI
\\
  This study explores the potential of super-resolution techniques in enhancing
object detection accuracy in football. Given the sport's fast-paced nature and
the critical importance of precise object (e.g. ball, player) tracking for both
analysis and broadcasting, super-resolution could offer significant
improvements. We investigate how advanced image processing through
super-resolution impacts the accuracy and reliability of object detection
algorithms in processing football match footage.
  Our methodology involved applying state-of-the-art super-resolution
techniques to a diverse set of football match videos from SoccerNet, followed
by object detection using Faster R-CNN. The performance of these algorithms,
both with and without super-resolution enhancement, was rigorously evaluated in
terms of detection accuracy.
  The results indicate a marked improvement in object detection accuracy when
super-resolution preprocessing is applied. The improvement of object detection
through the integration of super-resolution techniques yields significant
benefits, especially for low-resolution scenarios, with a notable 12\% increase
in mean Average Precision (mAP) at an IoU (Intersection over Union) range of
0.50:0.95 for 320x240 size images when increasing the resolution fourfold using
RLFN. As the dimensions increase, the magnitude of improvement becomes more
subdued; however, a discernible improvement in the quality of detection is
consistently evident. Additionally, we discuss the implications of these
findings for real-time sports analytics, player tracking, and the overall
viewing experience. The study contributes to the growing field of sports
technology by demonstrating the practical benefits and limitations of
integrating super-resolution techniques in football analytics and broadcasting.
\\ ( https://arxiv.org/abs/2402.00163 ,  15332kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00219 (*cross-listing*)
Date: Wed, 31 Jan 2024 22:40:49 GMT   (472kb,D)

Title: FedCore: Straggler-Free Federated Learning with Distributed Coresets
Authors: Hongpeng Guo, Haotian Gu, Xiaoyang Wang, Bo Chen, Eun Kyung Lee, Tamar
  Eilam, Deming Chen and Klara Nahrstedt
Categories: cs.DC cs.AI cs.LG
\\
  Federated learning (FL) is a machine learning paradigm that allows multiple
clients to collaboratively train a shared model while keeping their data
on-premise. However, the straggler issue, due to slow clients, often hinders
the efficiency and scalability of FL. This paper presents FedCore, an algorithm
that innovatively tackles the straggler problem via the decentralized selection
of coresets, representative subsets of a dataset. Contrary to existing
centralized coreset methods, FedCore creates coresets directly on each client
in a distributed manner, ensuring privacy preservation in FL. FedCore
translates the coreset optimization problem into a more tractable k-medoids
clustering problem and operates distributedly on each client. Theoretical
analysis confirms FedCore's convergence, and practical evaluations demonstrate
an 8x reduction in FL training time, without compromising model accuracy. Our
extensive evaluations also show that FedCore generalizes well to existing FL
frameworks.
\\ ( https://arxiv.org/abs/2402.00219 ,  472kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00234 (*cross-listing*)
Date: Wed, 31 Jan 2024 23:24:37 GMT   (7373kb,D)

Title: Are Generative AI systems Capable of Supporting Information Needs of
  Patients?
Authors: Shreya Rajagopal, Subhashis Hazarika, Sookyung Kim, Yan-ming Chiou,
  Jae Ho Sohn, Hari Subramonyam, Shiwali Mohan
Categories: cs.HC cs.AI cs.CL cs.LG
\\
  Patients managing a complex illness such as cancer face a complex information
challenge where they not only must learn about their illness but also how to
manage it. Close interaction with healthcare experts (radiologists,
oncologists) can improve patient learning and thereby, their disease outcome.
However, this approach is resource intensive and takes expert time away from
other critical tasks. Given the recent advancements in Generative AI models
aimed at improving the healthcare system, our work investigates whether and how
generative visual question answering systems can responsibly support patient
information needs in the context of radiology imaging data. We conducted a
formative need-finding study in which participants discussed chest computed
tomography (CT) scans and associated radiology reports of a fictitious close
relative with a cardiothoracic radiologist. Using thematic analysis of the
conversation between participants and medical experts, we identified commonly
occurring themes across interactions, including clarifying medical terminology,
locating the problems mentioned in the report in the scanned image,
understanding disease prognosis, discussing the next diagnostic steps, and
comparing treatment options. Based on these themes, we evaluated two
state-of-the-art generative visual language models against the radiologist's
responses. Our results reveal variability in the quality of responses generated
by the models across various themes. We highlight the importance of
patient-facing generative AI systems to accommodate a diverse range of
conversational themes, catering to the real-world informational needs of
patients.
\\ ( https://arxiv.org/abs/2402.00234 ,  7373kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00260 (*cross-listing*)
Date: Thu, 1 Feb 2024 01:09:00 GMT   (11068kb)

Title: Towards scalable robotic intervention of children with Autism Spectrum
  Disorder using LLMs
Authors: Ruchik Mishra and Karla Conn Welch
Categories: cs.RO cs.AI
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\
  In this paper, we propose a social robot capable of verbally interacting with
children with Autism Spectrum Disorder (ASD). This communication is meant to
teach perspective-taking using text generated using a Large Language Model
(LLM) pipeline. The social robot NAO acts as a stimulator (verbally describes a
social situation and asks a question), prompter (presents three options to
choose from), and reinforcer (praises when the answer is correct). For the role
of the stimulator, the social situation, questions, and options are generated
using our LLM pipeline. We compare two approaches: GPT-2 + BART and GPT-2 +
GPT-2, where the first GPT-2 common between the pipelines is used for
unsupervised social situation generation. We use the SOCIALIQA dataset to
fine-tune all of our LLM pipelines. We found that the GPT-2 + BART pipeline had
a better BERTscore for generating the questions and the options by combining
their individual loss functions. This observation was also consistent with the
human evaluations. Lastly, the unsupervised generation of social situations was
visualized using T-SNE plots, and the entire pipeline was evaluated for
appropriateness for children with ASD by human experts.
\\ ( https://arxiv.org/abs/2402.00260 ,  11068kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00284 (*cross-listing*)
Date: Thu, 1 Feb 2024 02:29:16 GMT   (293kb,D)

Title: PAP-REC: Personalized Automatic Prompt for Recommendation Language Model
Authors: Zelong Li, Jianchao Ji, Yingqiang Ge, Wenyue Hua, Yongfeng Zhang
Categories: cs.IR cs.AI cs.LG
\\
  Recently emerged prompt-based Recommendation Language Models (RLM) can solve
multiple recommendation tasks uniformly. The RLMs make full use of the
inherited knowledge learned from the abundant pre-training data to solve the
downstream recommendation tasks by prompts, without introducing additional
parameters or network training. However, handcrafted prompts require
significant expertise and human effort since slightly rewriting prompts may
cause massive performance changes. In this paper, we propose PAP-REC, a
framework to generate the Personalized Automatic Prompt for RECommendation
language models to mitigate the inefficiency and ineffectiveness problems
derived from manually designed prompts. Specifically, personalized automatic
prompts allow different users to have different prompt tokens for the same
task, automatically generated using a gradient-based method. One challenge for
personalized automatic prompt generation for recommendation language models is
the extremely large search space, leading to a long convergence time. To
effectively and efficiently address the problem, we develop surrogate metrics
and leverage an alternative updating schedule for prompting recommendation
language models. Experimental results show that our PAP-REC framework manages
to generate personalized prompts, and the automatically generated prompts
outperform manually constructed prompts and also outperform various baseline
recommendation models. The source code of the work is available at
https://github.com/rutgerswiselab/PAP-REC.
\\ ( https://arxiv.org/abs/2402.00284 ,  293kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00312 (*cross-listing*)
Date: Thu, 1 Feb 2024 03:53:13 GMT   (1798kb)

Title: The whack-a-mole governance challenge for AI-enabled synthetic biology:
  literature review and emerging frameworks
Authors: Trond Arne Undheim
Categories: q-bio.OT cs.AI
\\
  AI-enabled synthetic biology has tremendous potential but also significantly
increases biorisks and brings about a new set of dual use concerns. The picture
is complicated given the vast innovations envisioned to emerge by combining
emerging technologies, as AI-enabled synthetic biology potentially scales up
bioengineering into industrial biomanufacturing. However, the literature review
indicates that goals such as maintaining a reasonable scope for innovation, or
more ambitiously to foster a huge bioeconomy don't necessarily contrast with
biosafety, but need to go hand in hand. This paper presents a literature review
of the issues and describes emerging frameworks for policy and practice that
transverse the options of command-and control, stewardship, bottom-up, and
laissez-faire governance. How to achieve early warning systems that enable
prevention and mitigation of future AI-enabled biohazards from the lab, from
deliberate misuse, or from the public realm, will constantly need to evolve,
and adaptive, interactive approaches should emerge. Although biorisk is subject
to an established governance regime, and scientists generally adhere to
biosafety protocols, even experimental, but legitimate use by scientists could
lead to unexpected developments. Recent advances in chatbots enabled by
generative AI have revived fears that advanced biological insight can more
easily get into the hands of malignant individuals or organizations. Given
these sets of issues, society needs to rethink how AI-enabled synthetic biology
should be governed. The suggested way to visualize the challenge at hand is
whack-a-mole governance, although the emerging solutions are perhaps not so
different either.
\\ ( https://arxiv.org/abs/2402.00312 ,  1798kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00319 (*cross-listing*)
Date: Thu, 1 Feb 2024 04:09:17 GMT   (12472kb,D)

Title: SCO-VIST: Social Interaction Commonsense Knowledge-based Visual
  Storytelling
Authors: Eileen Wang, Soyeon Caren Han, Josiah Poon
Categories: cs.CV cs.AI
\\
  Visual storytelling aims to automatically generate a coherent story based on
a given image sequence. Unlike tasks like image captioning, visual stories
should contain factual descriptions, worldviews, and human social commonsense
to put disjointed elements together to form a coherent and engaging
human-writeable story. However, most models mainly focus on applying factual
information and using taxonomic/lexical external knowledge when attempting to
create stories. This paper introduces SCO-VIST, a framework representing the
image sequence as a graph with objects and relations that includes human action
motivation and its social interaction commonsense knowledge. SCO-VIST then
takes this graph representing plot points and creates bridges between plot
points with semantic and occurrence-based edge weights. This weighted story
graph produces the storyline in a sequence of events using Floyd-Warshall's
algorithm. Our proposed framework produces stories superior across multiple
metrics in terms of visual grounding, coherence, diversity, and humanness, per
both automatic and human evaluations.
\\ ( https://arxiv.org/abs/2402.00319 ,  12472kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00334 (*cross-listing*)
Date: Thu, 1 Feb 2024 04:39:15 GMT   (128kb,D)

Title: Multi-agent Path Finding for Cooperative Autonomous Driving
Authors: Zhongxia Yan, Han Zheng, Cathy Wu
Categories: cs.MA cs.AI cs.RO
Comments: 7 pages, 3 figures, IEEE International Conference on Robotics and
  Automation (ICRA), 2024
\\
  Anticipating possible future deployment of connected and automated vehicles
(CAVs), cooperative autonomous driving at intersections has been studied by
many works in control theory and intelligent transportation across decades.
Simultaneously, recent parallel works in robotics have devised efficient
algorithms for multi-agent path finding (MAPF), though often in environments
with simplified kinematics. In this work, we hybridize insights and algorithms
from MAPF with the structure and heuristics of optimizing the crossing order of
CAVs at signal-free intersections. We devise an optimal and complete algorithm,
Order-based Search with Kinematics Arrival Time Scheduling (OBS-KATS), which
significantly outperforms existing algorithms, fixed heuristics, and
prioritized planning with KATS. The performance is maintained under different
vehicle arrival rates, lane lengths, crossing speeds, and control horizon.
Through ablations and dissections, we offer insight on the contributing factors
to OBS-KATS's performance. Our work is directly applicable to many similarly
scaled traffic and multi-robot scenarios with directed lanes.
\\ ( https://arxiv.org/abs/2402.00334 ,  128kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00350 (*cross-listing*)
Date: Thu, 1 Feb 2024 05:34:03 GMT   (100kb,D)

Title: Large Language Models Based Fuzzing Techniques: A Survey
Authors: Linghan Huang, Peizhou Zhao, Huaming Chen, Lei Ma
Categories: cs.SE cs.AI
Comments: 9 pages submission under review
\\
  In the modern era where software plays a pivotal role, software security and
vulnerability analysis have become essential for software development. Fuzzing
test, as an efficient software testing method, are widely used in various
domains. Moreover, the rapid development of Large Language Models (LLMs) has
facilitated their application in the field of software testing, demonstrating
remarkable performance. Considering that existing fuzzing test techniques are
not entirely automated and software vulnerabilities continue to evolve, there
is a growing trend towards employing fuzzing test generated based on large
language models. This survey provides a systematic overview of the approaches
that fuse LLMs and fuzzing tests for software testing. In this paper, a
statistical analysis and discussion of the literature in three areas, namely
LLMs, fuzzing test, and fuzzing test generated based on LLMs, are conducted by
summarising the state-of-the-art methods up until 2024. Our survey also
investigates the potential for widespread deployment and application of fuzzing
test techniques generated by LLMs in the future.
\\ ( https://arxiv.org/abs/2402.00350 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00362 (*cross-listing*)
Date: Thu, 1 Feb 2024 06:02:29 GMT   (4522kb)

Title: Climate Trends of Tropical Cyclone Intensity and Energy Extremes
  Revealed by Deep Learning
Authors: Buo-Fu Chen, Boyo Chen, Chun-Min Hsiao, Hsu-Feng Teng, Cheng-Shang
  Lee, Hung-Chi Kuo
Categories: physics.ao-ph cs.AI
Comments: 41 pages
\\
  Anthropogenic influences have been linked to tropical cyclone (TC) poleward
migration, TC extreme precipitation, and an increased proportion of major
hurricanes [1, 2, 3, 4]. Understanding past TC trends and variability is
critical for projecting future TC impacts on human society considering the
changing climate [5]. However, past trends of TC structure/energy remain
uncertain due to limited observations; subjective-analyzed and
spatiotemporal-heterogeneous "best-track" datasets lead to reduced confidence
in the assessed TC repose to climate change [6, 7]. Here, we use deep learning
to reconstruct past "observations" and yield an objective global TC wind
profile dataset during 1981 to 2020, facilitating a comprehensive examination
of TC structure/energy. By training with uniquely labeled data integrating best
tracks and numerical model analysis of 2004 to 2018 TCs, our model converts
multichannel satellite imagery to a 0-750-km wind profile of axisymmetric
surface winds. The model performance is verified to be sufficient for climate
studies by comparing it to independent satellite-radar surface winds. Based on
the new homogenized dataset, the major TC proportion has increased by ~13% in
the past four decades. Moreover, the proportion of extremely high-energy TCs
has increased by ~25%, along with an increasing trend (> one standard deviation
of the 40-y variability) of the mean total energy of high-energy TCs. Although
the warming ocean favors TC intensification, the TC track migration to higher
latitudes and altered environments further affect TC structure/energy. This new
deep learning method/dataset reveals novel trends regarding TC structure
extremes and may help verify simulations/studies regarding TCs in the changing
climate.
\\ ( https://arxiv.org/abs/2402.00362 ,  4522kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00366 (*cross-listing*)
Date: Thu, 1 Feb 2024 06:06:59 GMT   (1286kb,D)

Title: Legged Robot State Estimation With Invariant Extended Kalman Filter
  Using Neural Measurement Network
Authors: Donghoon Youm, Hyunsik Oh, Suyoung Choi, Hyeongjun Kim, Jemin Hwangbo
Categories: cs.RO cs.AI
Comments: 8pages, 6paper, This work has been submitted to the IEEE for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible
\\
  This paper introduces a novel proprioceptive state estimator for legged
robots that combines model-based filters and deep neural networks. Recent
studies have shown that neural networks such as multi-layer perceptron or
recurrent neural networks can estimate the robot states, including contact
probability and linear velocity. Inspired by this, we develop a state
estimation framework that integrates a neural measurement network (NMN) with an
invariant extended Kalman filter. We show that our framework improves
estimation performance in various terrains. Existing studies that combine
model-based filters and learning-based approaches typically use real-world
data. However, our approach relies solely on simulation data, as it allows us
to easily obtain extensive data. This difference leads to a gap between the
learning and the inference domain, commonly referred to as a sim-to-real gap.
We address this challenge by adapting existing learning techniques and
regularization. To validate our proposed method, we conduct experiments using a
quadruped robot on four types of terrain: \textit{flat}, \textit{debris},
\textit{soft}, and \textit{slippery}. We observe that our approach
significantly reduces position drift compared to the existing model-based state
estimator.
\\ ( https://arxiv.org/abs/2402.00366 ,  1286kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00389 (*cross-listing*)
Date: Thu, 1 Feb 2024 07:21:32 GMT   (32kb)

Title: On the $O(\frac{\sqrt{d}}{T^{1/4}})$ Convergence Rate of RMSProp and Its
  Momentum Extension Measured by $\ell_1$ Norm: Better Dependence on the
  Dimension
Authors: Huan Li and Zhouchen Lin
Categories: math.OC cs.AI
\\
  Although adaptive gradient methods have been extensively used in deep
learning, their convergence rates have not been thoroughly studied,
particularly with respect to their dependence on the dimension. This paper
considers the classical RMSProp and its momentum extension and establishes the
convergence rate of $\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla
f(x^k)\|_1\right]\leq O(\frac{\sqrt{d}}{T^{1/4}})$ measured by $\ell_1$ norm
without the bounded gradient assumption, where $d$ is the dimension of the
optimization variable and $T$ is the iteration number. Since
$\|x\|_2\ll\|x\|_1\leq\sqrt{d}\|x\|_2$ for problems with extremely large $d$,
our convergence rate can be considered to be analogous to the
$\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_2\right]\leq
O(\frac{1}{T^{1/4}})$ one of SGD measured by $\ell_1$ norm.
\\ ( https://arxiv.org/abs/2402.00389 ,  32kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00390 (*cross-listing*)
Date: Thu, 1 Feb 2024 07:22:52 GMT   (3715kb,D)

Title: EASRec: Elastic Architecture Search for Efficient Long-term Sequential
  Recommender Systems
Authors: Sheng Zhang, Maolin Wang, Yao Zhao, Chenyi Zhuang, Jinjie Gu, Ruocheng
  Guo, Xiangyu Zhao, Zijian Zhang, Hongzhi Yin
Categories: cs.IR cs.AI
\\
  In this age where data is abundant, the ability to distill meaningful
insights from the sea of information is essential. Our research addresses the
computational and resource inefficiencies that current Sequential Recommender
Systems (SRSs) suffer from. especially those employing attention-based models
like SASRec, These systems are designed for next-item recommendations in
various applications, from e-commerce to social networks. However, such systems
suffer from substantial computational costs and resource consumption during the
inference stage. To tackle these issues, our research proposes a novel method
that combines automatic pruning techniques with advanced model architectures.
We also explore the potential of resource-constrained Neural Architecture
Search (NAS), a technique prevalent in the realm of recommendation systems, to
fine-tune models for reduced FLOPs, latency, and energy usage while retaining
or even enhancing accuracy. The main contribution of our work is developing the
Elastic Architecture Search for Efficient Long-term Sequential Recommender
Systems (EASRec). This approach aims to find optimal compact architectures for
attention-based SRSs, ensuring accuracy retention. EASRec introduces data-aware
gates that leverage historical information from input data batch to improve the
performance of the recommendation network. Additionally, it utilizes a dynamic
resource constraint approach, which standardizes the search process and results
in more appropriate architectures. The effectiveness of our methodology is
validated through exhaustive experiments on three benchmark datasets, which
demonstrates EASRec's superiority in SRSs. Our research set a new standard for
future exploration into efficient and accurate recommender systems, signifying
a substantial advancement within this swiftly advancing field.
\\ ( https://arxiv.org/abs/2402.00390 ,  3715kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00393 (*cross-listing*)
Date: Thu, 1 Feb 2024 07:28:55 GMT   (7998kb,D)

Title: Loss Function Considering Dead Zone for Neural Networks
Authors: Koki Inami, Koki Yamane, Sho Sakaino
Categories: cs.RO cs.AI cs.LG
Comments: 6 pages, 6 figures, Accepted at AMC2024
\\
  It is important to reveal the inverse dynamics of manipulators to improve
control performance of model-based control. Neural networks (NNs) are promising
techniques to represent complicated inverse dynamics while they require a large
amount of motion data. However, motion data in dead zones of actuators is not
suitable for training models decreasing the number of useful training data. In
this study, based on the fact that the manipulator joint does not work
irrespective of input torque in dead zones, we propose a new loss function that
considers only errors of joints not in dead zones. The proposed method enables
to increase in the amount of motion data available for training and the
accuracy of the inverse dynamics computation. Experiments on actual equipment
using a three-degree-of-freedom (DOF) manipulator showed higher accuracy than
conventional methods. We also confirmed and discussed the behavior of the model
of the proposed method in dead zones.
\\ ( https://arxiv.org/abs/2402.00393 ,  7998kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00411 (*cross-listing*)
Date: Thu, 1 Feb 2024 08:10:39 GMT   (196kb,D)

Title: LM-HT SNN: Enhancing the Performance of SNN to ANN Counterpart through
  Learnable Multi-hierarchical Threshold Model
Authors: Zecheng Hao, Xinyu Shi, Zhiyu Pan, Yujia Liu, Zhaofei Yu, Tiejun Huang
Categories: cs.NE cs.AI cs.CV
Comments: 15 pages, 2 figures
\\
  Compared to traditional Artificial Neural Network (ANN), Spiking Neural
Network (SNN) has garnered widespread academic interest for its intrinsic
ability to transmit information in a more biological-inspired and
energy-efficient manner. However, despite previous efforts to optimize the
learning gradients and model structure of SNNs through various methods, SNNs
still lag behind ANNs in terms of performance to some extent. The recently
proposed multi-threshold model provides more possibilities for further
enhancing the learning capability of SNNs. In this paper, we rigorously analyze
the relationship among the multi-threshold model, vanilla spiking model and
quantized ANNs from a mathematical perspective, then propose a novel LM-HT
model, which is an equidistant multi-hierarchical model that can dynamically
regulate the global input current and membrane potential leakage on the time
dimension. In addition, we note that the direct training algorithm based on the
LM-HT model can seamlessly integrate with the traditional ANN-SNN Conversion
framework. This novel hybrid learning framework can effectively improve the
relatively poor performance of converted SNNs under low time latency. Extensive
experimental results have demonstrated that our LM-HT model can significantly
outperform previous state-of-the-art works on various types of datasets, which
promote SNNs to achieve a brand-new level of performance comparable to
quantized ANNs.
\\ ( https://arxiv.org/abs/2402.00411 ,  196kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00435 (*cross-listing*)
Date: Thu, 1 Feb 2024 09:01:58 GMT   (139kb,D)

Title: A practical existence theorem for reduced order models based on
  convolutional autoencoders
Authors: Nicola Rares Franco and Simone Brugiapaglia
Categories: math.NA cs.AI cs.LG cs.NA
\\
  In recent years, deep learning has gained increasing popularity in the fields
of Partial Differential Equations (PDEs) and Reduced Order Modeling (ROM),
providing domain practitioners with new powerful data-driven techniques such as
Physics-Informed Neural Networks (PINNs), Neural Operators, Deep Operator
Networks (DeepONets) and Deep-Learning based ROMs (DL-ROMs). In this context,
deep autoencoders based on Convolutional Neural Networks (CNNs) have proven
extremely effective, outperforming established techniques, such as the reduced
basis method, when dealing with complex nonlinear problems. However, despite
the empirical success of CNN-based autoencoders, there are only a few
theoretical results supporting these architectures, usually stated in the form
of universal approximation theorems. In particular, although the existing
literature provides users with guidelines for designing convolutional
autoencoders, the subsequent challenge of learning the latent features has been
barely investigated. Furthermore, many practical questions remain unanswered,
e.g., the number of snapshots needed for convergence or the neural network
training strategy. In this work, using recent techniques from sparse
high-dimensional function approximation, we fill some of these gaps by
providing a new practical existence theorem for CNN-based autoencoders when the
parameter-to-solution map is holomorphic. This regularity assumption arises in
many relevant classes of parametric PDEs, such as the parametric diffusion
equation, for which we discuss an explicit application of our general theory.
\\ ( https://arxiv.org/abs/2402.00435 ,  139kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00448 (*cross-listing*)
Date: Thu, 1 Feb 2024 09:32:39 GMT   (4441kb,D)

Title: Dual-Student Knowledge Distillation Networks for Unsupervised Anomaly
  Detection
Authors: Liyi Yao, Shaobing Gao
Categories: cs.CV cs.AI
\\
  Due to the data imbalance and the diversity of defects, student-teacher
networks (S-T) are favored in unsupervised anomaly detection, which explores
the discrepancy in feature representation derived from the knowledge
distillation process to recognize anomalies. However, vanilla S-T network is
not stable. Employing identical structures to construct the S-T network may
weaken the representative discrepancy on anomalies. But using different
structures can increase the likelihood of divergent performance on normal data.
To address this problem, we propose a novel dual-student knowledge distillation
(DSKD) architecture. Different from other S-T networks, we use two student
networks a single pre-trained teacher network, where the students have the same
scale but inverted structures. This framework can enhance the distillation
effect to improve the consistency in recognition of normal data, and
simultaneously introduce diversity for anomaly representation. To explore
high-dimensional semantic information to capture anomaly clues, we employ two
strategies. First, a pyramid matching mode is used to perform knowledge
distillation on multi-scale feature maps in the intermediate layers of
networks. Second, an interaction is facilitated between the two student
networks through a deep feature embedding module, which is inspired by
real-world group discussions. In terms of classification, we obtain pixel-wise
anomaly segmentation maps by measuring the discrepancy between the output
feature maps of the teacher and student networks, from which an anomaly score
is computed for sample-wise determination. We evaluate DSKD on three benchmark
datasets and probe the effects of internal modules through ablation
experiments. The results demonstrate that DSKD can achieve exceptional
performance on small models like ResNet18 and effectively improve vanilla S-T
networks.
\\ ( https://arxiv.org/abs/2402.00448 ,  4441kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00459 (*cross-listing*)
Date: Thu, 1 Feb 2024 09:57:38 GMT   (1623kb,D)

Title: Genetic-based Constraint Programming for Resource Constrained Job
  Scheduling
Authors: Su Nguyen and Dhananjay Thiruvady and Yuan Sun and Mengjie Zhang
Categories: cs.NE cs.AI
\\
  Resource constrained job scheduling is a hard combinatorial optimisation
problem that originates in the mining industry. Off-the-shelf solvers cannot
solve this problem satisfactorily in reasonable timeframes, while other
solution methods such as many evolutionary computation methods and
matheuristics cannot guarantee optimality and require low-level customisation
and specialised heuristics to be effective. This paper addresses this gap by
proposing a genetic programming algorithm to discover efficient search
strategies of constraint programming for resource-constrained job scheduling.
In the proposed algorithm, evolved programs represent variable selectors to be
used in the search process of constraint programming, and their fitness is
determined by the quality of solutions obtained for training instances. The
novelties of this algorithm are (1) a new representation of variable selectors,
(2) a new fitness evaluation scheme, and (3) a pre-selection mechanism. Tests
with a large set of random and benchmark instances, the evolved variable
selectors can significantly improve the efficiency of constraining programming.
Compared to highly customised metaheuristics and hybrid algorithms, evolved
variable selectors can help constraint programming identify quality solutions
faster and proving optimality is possible if sufficiently large run-times are
allowed. The evolved variable selectors are especially helpful when solving
instances with large numbers of machines.
\\ ( https://arxiv.org/abs/2402.00459 ,  1623kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00588 (*cross-listing*)
Date: Thu, 1 Feb 2024 13:34:59 GMT   (1679kb,D)

Title: BrainSLAM: SLAM on Neural Population Activity Data
Authors: Kipp Freud, Nathan Lepora, Matt W. Jones, Cian O'Donnell
Categories: cs.RO cs.AI cs.MA
Comments: Accepted to the 23rd International Conference on Autonomous Agents
  and Multiagent Systems. 2024
\\
  Simultaneous localisation and mapping (SLAM) algorithms are commonly used in
robotic systems for learning maps of novel environments. Brains also appear to
learn maps, but the mechanisms are not known and it is unclear how to infer
these maps from neural activity data. We present BrainSLAM; a method for
performing SLAM using only population activity (local field potential, LFP)
data simultaneously recorded from three brain regions in rats: hippocampus,
prefrontal cortex, and parietal cortex. This system uses a convolutional neural
network (CNN) to decode velocity and familiarity information from wavelet
scalograms of neural local field potential data recorded from rats as they
navigate a 2D maze. The CNN's output drives a RatSLAM-inspired architecture,
powering an attractor network which performs path integration plus a separate
system which performs `loop closure' (detecting previously visited locations
and correcting map aliasing errors). Together, these three components can
construct faithful representations of the environment while simultaneously
tracking the animal's location. This is the first demonstration of inference of
a spatial map from brain recordings. Our findings expand SLAM to a new
modality, enabling a new method of mapping environments and facilitating a
better understanding of the role of cognitive maps in navigation and decision
making.
\\ ( https://arxiv.org/abs/2402.00588 ,  1679kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00627 (*cross-listing*)
Date: Thu, 1 Feb 2024 14:41:59 GMT   (32035kb,D)

Title: CapHuman: Capture Your Moments in Parallel Universes
Authors: Chao Liang, Fan Ma, Linchao Zhu, Yingying Deng, Yi Yang
Categories: cs.CV cs.AI
Comments: Project page: https://caphuman.github.io/
\\
  We concentrate on a novel human-centric image synthesis task, that is, given
only one reference facial photograph, it is expected to generate specific
individual images with diverse head positions, poses, and facial expressions in
different contexts. To accomplish this goal, we argue that our generative model
should be capable of the following favorable characteristics: (1) a strong
visual and semantic understanding of our world and human society for basic
object and human image generation. (2) generalizable identity preservation
ability. (3) flexible and fine-grained head control. Recently, large
pre-trained text-to-image diffusion models have shown remarkable results,
serving as a powerful generative foundation. As a basis, we aim to unleash the
above two capabilities of the pre-trained model. In this work, we present a new
framework named CapHuman. We embrace the ``encode then learn to align"
paradigm, which enables generalizable identity preservation for new individuals
without cumbersome tuning at inference. CapHuman encodes identity features and
then learns to align them into the latent space. Moreover, we introduce the 3D
facial prior to equip our model with control over the human head in a flexible
and 3D-consistent manner. Extensive qualitative and quantitative analyses
demonstrate our CapHuman can produce well-identity-preserved, photo-realistic,
and high-fidelity portraits with content-rich representations and various head
renditions, superior to established baselines. Code and checkpoint will be
released at https://github.com/VamosC/CapHuman.
\\ ( https://arxiv.org/abs/2402.00627 ,  32035kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00672 (*cross-listing*)
Date: Thu, 1 Feb 2024 15:33:17 GMT   (13232kb,D)

Title: Exploring Homogeneous and Heterogeneous Consistent Label Associations
  for Unsupervised Visible-Infrared Person ReID
Authors: Lingfeng He, De Cheng, Nannan Wang, Xinbo Gao
Categories: cs.CV cs.AI
\\
  Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to
retrieve pedestrian images of the same identity from different modalities
without annotations. While prior work focuses on establishing cross-modality
pseudo-label associations to bridge the modality-gap, they ignore maintaining
the instance-level homogeneous and heterogeneous consistency in pseudo-label
space, resulting in coarse associations. In response, we introduce a
Modality-Unified Label Transfer (MULT) module that simultaneously accounts for
both homogeneous and heterogeneous fine-grained instance-level structures,
yielding high-quality cross-modality label associations. It models both
homogeneous and heterogeneous affinities, leveraging them to define the
inconsistency for the pseudo-labels and then minimize it, leading to
pseudo-labels that maintain alignment across modalities and consistency within
intra-modality structures. Additionally, a straightforward plug-and-play Online
Cross-memory Label Refinement (OCLR) module is proposed to further mitigate the
impact of noisy pseudo-labels while simultaneously aligning different
modalities, coupled with a Modality-Invariant Representation Learning (MIRL)
framework. Experiments demonstrate that our proposed method outperforms
existing USL-VI-ReID methods, highlighting the superiority of our MULT in
comparison to other cross-modality association methods. The code will be
available.
\\ ( https://arxiv.org/abs/2402.00672 ,  13232kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00676 (*cross-listing*)
Date: Thu, 1 Feb 2024 15:37:23 GMT   (2184kb)

Title: Deep Robot Sketching: An application of Deep Q-Learning Networks for
  human-like sketching
Authors: Raul Fernandez-Fernandez, Juan G. Victores, Carlos Balaguer
Categories: cs.RO cs.AI cs.CV cs.LG cs.NE
Journal-ref: Cognitive Systems Research, Volume 81, September 2023, pages 57 to
  63
DOI: 10.1016/j.cogsys.2023.05.004
\\
  The current success of Reinforcement Learning algorithms for its performance
in complex environments has inspired many recent theoretical approaches to
cognitive science. Artistic environments are studied within the cognitive
science community as rich, natural, multi-sensory, multi-cultural environments.
In this work, we propose the introduction of Reinforcement Learning for
improving the control of artistic robot applications. Deep Q-learning Neural
Networks (DQN) is one of the most successful algorithms for the implementation
of Reinforcement Learning in robotics. DQN methods generate complex control
policies for the execution of complex robot applications in a wide set of
environments. Current art painting robot applications use simple control laws
that limits the adaptability of the frameworks to a set of simple environments.
In this work, the introduction of DQN within an art painting robot application
is proposed. The goal is to study how the introduction of a complex control
policy impacts the performance of a basic art painting robot application. The
main expected contribution of this work is to serve as a first baseline for
future works introducing DQN methods for complex art painting robot frameworks.
Experiments consist of real world executions of human drawn sketches using the
DQN generated policy and TEO, the humanoid robot. Results are compared in terms
of similarity and obtained reward with respect to the reference inputs
\\ ( https://arxiv.org/abs/2402.00676 ,  2184kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00677 (*cross-listing*)
Date: Thu, 1 Feb 2024 15:37:42 GMT   (803kb,D)

Title: Neural Policy Style Transfer
Authors: Raul Fernandez-Fernandez, Juan G. Victores, Jennifer J. Gago, David
  Estevez, Carlos Balaguer
Categories: cs.RO cs.AI cs.LG cs.NE
Journal-ref: Cognitive Systems Research, Volume 72, March 2022, Pages 23 to 32
DOI: 10.1016/j.cogsys.2021.11.003
\\
  Style Transfer has been proposed in a number of fields: fine arts, natural
language processing, and fixed trajectories. We scale this concept up to
control policies within a Deep Reinforcement Learning infrastructure. Each
network is trained to maximize the expected reward, which typically encodes the
goal of an action, and can be described as the content. The expressive power of
deep neural networks enables encoding a secondary task, which can be described
as the style. The Neural Policy Style Transfer (NPST) algorithm is proposed to
transfer the style of one policy to another, while maintaining the content of
the latter. Different policies are defined via Deep Q-Network architectures.
These models are trained using demonstrations through Inverse Reinforcement
Learning. Two different sets of user demonstrations are performed, one for
content and other for style. Different styles are encoded as defined by user
demonstrations. The generated policy is the result of feeding a content policy
and a style policy to the NPST algorithm. Experiments are performed in a
catch-ball game inspired by the Deep Reinforcement Learning classical Atari
games; and a real-world painting scenario with a full-sized humanoid robot,
based on previous works of the authors. The implementation of three different
Q-Network architectures (Shallow, Deep and Deep Recurrent Q-Network) to encode
the policies within the NPST framework is proposed and the results obtained in
the experiments with each of these architectures compared.
\\ ( https://arxiv.org/abs/2402.00677 ,  803kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00678 (*cross-listing*)
Date: Thu, 1 Feb 2024 15:38:21 GMT   (555kb)

Title: Real Evaluations Tractability using Continuous Goal-Directed Actions in
  Smart City Applications
Authors: Raul Fernandez-Fernandez, Juan G. Victores, David Estevez, and Carlos
  Balaguer
Categories: cs.RO cs.AI cs.LG
Journal-ref: Sensors, Volume 18, Issue 11, 2018
DOI: 10.3390/s18113818
\\
  One of the most important challenges of Smart City Applications is to adapt
the system to interact with non-expert users. Robot imitation frameworks aim to
simplify and reduce times of robot programming by allowing users to program
directly through demonstrations. In classical frameworks, actions are modeled
using joint or Cartesian space trajectories. Other features, such as visual
ones, are not always well represented with these pure geometrical approaches.
Continuous Goal-Directed Actions (CGDA) is an alternative to these methods, as
it encodes actions as changes of any feature that can be extracted from the
environment. As a consequence of this, the robot joint trajectories for
execution must be fully computed to comply with this feature-agnostic encoding.
This is achieved using Evolutionary Algorithms (EA), which usually requires too
many evaluations to perform this evolution step in the actual robot. Current
strategies involve performing evaluations in a simulation, transferring the
final joint trajectory to the actual robot. Smart City applications involve
working in highly dynamic and complex environments, where having a precise
model is not always achievable. Our goal is to study the tractability of
performing these evaluations directly in a real-world scenario. Two different
approaches to reduce the number of evaluations using EA, are proposed and
compared. In the first approach, Particle Swarm Optimization (PSO)-based
methods have been studied and compared within CGDA: naive PSO, Fitness
Inheritance PSO (FI-PSO), and Adaptive Fuzzy Fitness Granulation with PSO
(AFFG-PSO). The second approach studied the introduction of geometrical and
velocity constraints within CGDA. The effects of both approaches were analyzed
and compared in the wax and paint actions, two CGDA commonly studied use cases.
Results from this paper depict an important reduction in the number of
evaluations.
\\ ( https://arxiv.org/abs/2402.00678 ,  555kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00689 (*cross-listing*)
Date: Thu, 1 Feb 2024 15:49:47 GMT   (157kb,D)

Title: Ocassionally Secure: A Comparative Analysis of Code Generation
  Assistants
Authors: Ran Elgedawy, John Sadik, Senjuti Dutta, Anuj Gautam, Konstantinos
  Georgiou, Farzin Gholamrezae, Fujiao Ji, Kyungchan Lim, Qian Liu, and Scott
  Ruoti
Categories: cs.CR cs.AI
Comments: 12 pages, 2 figures
\\
  $ $Large Language Models (LLMs) are being increasingly utilized in various
applications, with code generations being a notable example. While previous
research has shown that LLMs have the capability to generate both secure and
insecure code, the literature does not take into account what factors help
generate secure and effective code. Therefore in this paper we focus on
identifying and understanding the conditions and contexts in which LLMs can be
effectively and safely deployed in real-world scenarios to generate quality
code. We conducted a comparative analysis of four advanced LLMs--GPT-3.5 and
GPT-4 using ChatGPT and Bard and Gemini from Google--using 9 separate tasks to
assess each model's code generation capabilities. We contextualized our study
to represent the typical use cases of a real-life developer employing LLMs for
everyday tasks as work. Additionally, we place an emphasis on security
awareness which is represented through the use of two distinct versions of our
developer persona. In total, we collected 61 code outputs and analyzed them
across several aspects: functionality, security, performance, complexity, and
reliability. These insights are crucial for understanding the models'
capabilities and limitations, guiding future development and practical
applications in the field of automated code generation.
\\ ( https://arxiv.org/abs/2402.00689 ,  157kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00699 (*cross-listing*)
Date: Thu, 1 Feb 2024 15:55:50 GMT   (14491kb,D)

Title: PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in
  Open-Source Software
Authors: Wenxin Jiang, Jerin Yasmin, Jason Jones, Nicholas Synovic, Jiashen
  Kuo, Nathaniel Bielanski, Yuan Tian, George K. Thiruvathukal, James C. Davis
Categories: cs.SE cs.AI cs.DB cs.LG
Comments: Accepted at MSR'24
\\
  The development and training of deep learning models have become increasingly
costly and complex. Consequently, software engineers are adopting pre-trained
models (PTMs) for their downstream applications. The dynamics of the PTM supply
chain remain largely unexplored, signaling a clear need for structured datasets
that document not only the metadata but also the subsequent applications of
these models. Without such data, the MSR community cannot comprehensively
understand the impact of PTM adoption and reuse. This paper presents the
PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed
snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with
28,575 open-source software repositories from GitHub that utilize these models.
Additionally, the dataset includes 44,337 mappings from 15,129 downstream
GitHub repositories to the 2,530 PTMs they use. To enhance the dataset's
comprehensiveness, we developed prompts for a large language model to
automatically extract model metadata, including the model's training datasets,
parameters, and evaluation metrics. Our analysis of this dataset provides the
first summary statistics for the PTM supply chain, showing the trend of PTM
development and common shortcomings of PTM package documentation. Our example
application reveals inconsistencies in software licenses across PTMs and their
dependent projects. PeaTMOSS lays the foundation for future research, offering
rich opportunities to investigate the PTM supply chain. We outline mining
opportunities on PTMs, their downstream usage, and cross-cutting questions.
\\ ( https://arxiv.org/abs/2402.00699 ,  14491kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00722 (*cross-listing*)
Date: Thu, 1 Feb 2024 16:14:32 GMT   (16778kb,D)

Title: Neural Style Transfer with Twin-Delayed DDPG for Shared Control of
  Robotic Manipulators
Authors: Raul Fernandez-Fernandez, Marco Aggravi, Paolo Robuffo Giordano, Juan
  G. Victores and Claudio Pacchierotti
Categories: cs.RO cs.AI cs.LG cs.NE
DOI: 10.1109/ICRA46639.2022.9812245
\\
  Neural Style Transfer (NST) refers to a class of algorithms able to
manipulate an element, most often images, to adopt the appearance or style of
another one. Each element is defined as a combination of Content and Style: the
Content can be conceptually defined as the what and the Style as the how of
said element. In this context, we propose a custom NST framework for
transferring a set of styles to the motion of a robotic manipulator, e.g., the
same robotic task can be carried out in an angry, happy, calm, or sad way. An
autoencoder architecture extracts and defines the Content and the Style of the
target robot motions. A Twin Delayed Deep Deterministic Policy Gradient (TD3)
network generates the robot control policy using the loss defined by the
autoencoder. The proposed Neural Policy Style Transfer TD3 (NPST3) alters the
robot motion by introducing the trained style. Such an approach can be
implemented either offline, for carrying out autonomous robot motions in
dynamic environments, or online, for adapting at runtime the style of a
teleoperated robot. The considered styles can be learned online from human
demonstrations. We carried out an evaluation with human subjects enrolling 73
volunteers, asking them to recognize the style behind some representative
robotic motions. Results show a good recognition rate, proving that it is
possible to convey different styles to a robot using this approach.
\\ ( https://arxiv.org/abs/2402.00722 ,  16778kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00808 (*cross-listing*)
Date: Thu, 1 Feb 2024 17:44:46 GMT   (6871kb,D)

Title: Exploring the Dynamics between Cobot's Production Rhythm, Locus of
  Control and Emotional State in a Collaborative Assembly Scenario
Authors: Marta Mondellini, Matteo Lavit Nicora, Pooja Prajod, Elisabeth
  Andr\'e, Rocco Vertechy, Alessandro Antonietti, Matteo Malosio
Categories: cs.RO cs.AI cs.HC
Comments: Accepted to 4th IEEE International Conference on Human-Machine
  Systems
\\
  In industrial scenarios, there is widespread use of collaborative robots
(cobots), and growing interest is directed at evaluating and measuring the
impact of some characteristics of the cobot on the human factor. In the present
pilot study, the effect that the production rhythm (C1 - Slow, C2 - Fast, C3 -
Adapted to the participant's pace) of a cobot has on the Experiential Locus of
Control (ELoC) and the emotional state of 31 participants has been examined.
The operators' performance, the degree of basic internal Locus of Control, and
the attitude towards the robots were also considered. No difference was found
regarding the emotional state and the ELoC in the three conditions, but
considering the other psychological variables, a more complex situation
emerges. Overall, results seem to indicate a need to consider the person's
psychological characteristics to offer a differentiated and optimal interaction
experience.
\\ ( https://arxiv.org/abs/2402.00808 ,  6871kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00822 (*cross-listing*)
Date: Thu, 1 Feb 2024 18:05:38 GMT   (8466kb,D)

Title: WiOpen: A Robust Wi-Fi-based Open-set Gesture Recognition Framework
Authors: Xiang Zhang and Jingyang Huang and Huan Yan and Peng Zhao and Guohang
  Zhuang and Zhi Liu and Bin Liu
Categories: cs.HC cs.AI
\\
  Recent years have witnessed a growing interest in Wi-Fi-based gesture
recognition. However, existing works have predominantly focused on closed-set
paradigms, where all testing gestures are predefined during training. This
poses a significant challenge in real-world applications, as unseen gestures
might be misclassified as known classes during testing. To address this issue,
we propose WiOpen, a robust Wi-Fi-based Open-Set Gesture Recognition (OSGR)
framework. Implementing OSGR requires addressing challenges caused by the
unique uncertainty in Wi-Fi sensing. This uncertainty, resulting from noise and
domains, leads to widely scattered and irregular data distributions in
collected Wi-Fi sensing data. Consequently, data ambiguity between classes and
challenges in defining appropriate decision boundaries to identify unknowns
arise. To tackle these challenges, WiOpen adopts a two-fold approach to
eliminate uncertainty and define precise decision boundaries. Initially, it
addresses uncertainty induced by noise during data preprocessing by utilizing
the CSI ratio. Next, it designs the OSGR network based on an uncertainty
quantification method. Throughout the learning process, this network
effectively mitigates uncertainty stemming from domains. Ultimately, the
network leverages relationships among samples' neighbors to dynamically define
open-set decision boundaries, successfully realizing OSGR. Comprehensive
experiments on publicly accessible datasets confirm WiOpen's effectiveness.
Notably, WiOpen also demonstrates superiority in cross-domain tasks when
compared to state-of-the-art approaches.
\\ ( https://arxiv.org/abs/2402.00822 ,  8466kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00828 (*cross-listing*)
Date: Thu, 1 Feb 2024 18:16:04 GMT   (1758kb,D)

Title: Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture
  of Adapters
Authors: Umberto Cappellazzo, Daniele Falavigna, Alessio Brutti
Categories: eess.AS cs.AI
Comments: The code will be released ad:
  \url{https://github.com/umbertocappellazzo/PETL_AST}
\\
  Mixture of Experts (MoE) architectures have recently started burgeoning due
to their ability to scale model's capacity while maintaining the computational
cost affordable. Furthermore, they can be applied to both Transformers and
State Space Models, the current state-of-the-art models in numerous fields.
While MoE has been mostly investigated for the pre-training stage, its use in
parameter-efficient transfer learning settings is under-explored. To narrow
this gap, this paper attempts to demystify the use of MoE for
parameter-efficient fine-tuning of Audio Spectrogram Transformers to audio and
speech downstream tasks. Specifically, we propose Soft Mixture of Adapters
(Soft-MoA). It exploits adapters as the experts and, leveraging the recent Soft
MoE method, it relies on a soft assignment between the input tokens and experts
to keep the computational time limited. Extensive experiments across 4
benchmarks demonstrate that Soft-MoA outperforms the single adapter method and
performs on par with the dense MoA counterpart. We finally present ablation
studies on key elements of Soft-MoA, showing for example that Soft-MoA achieves
better scaling with more experts, as well as ensuring that all experts
contribute to the computation of the output tokens, thus dispensing with the
expert imbalance issue.
\\ ( https://arxiv.org/abs/2402.00828 ,  1758kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00831 (*cross-listing*)
Date: Thu, 1 Feb 2024 18:17:37 GMT   (2426kb,D)

Title: A YANG-aided Unified Strategy for Black Hole Detection for Backbone
  Networks
Authors: Elif Ak, Kiymet Kaya, Eren Ozaltun, Sule Gunduz Oguducu, Berk Canberk
Categories: cs.NI cs.AI cs.LG
\\
  Despite the crucial importance of addressing Black Hole failures in Internet
backbone networks, effective detection strategies in backbone networks are
lacking. This is largely because previous research has been centered on Mobile
Ad-hoc Networks (MANETs), which operate under entirely different dynamics,
protocols, and topologies, making their findings not directly transferable to
backbone networks. Furthermore, detecting Black Hole failures in backbone
networks is particularly challenging. It requires a comprehensive range of
network data due to the wide variety of conditions that need to be considered,
making data collection and analysis far from straightforward. Addressing this
gap, our study introduces a novel approach for Black Hole detection in backbone
networks using specialized Yet Another Next Generation (YANG) data models with
Black Hole-sensitive Metric Matrix (BHMM) analysis. This paper details our
method of selecting and analyzing four YANG models relevant to Black Hole
detection in ISP networks, focusing on routing protocols and ISP-specific
configurations. Our BHMM approach derived from these models demonstrates a 10%
improvement in detection accuracy and a 13% increase in packet delivery rate,
highlighting the efficiency of our approach. Additionally, we evaluate the
Machine Learning approach leveraged with BHMM analysis in two different network
settings, a commercial ISP network, and a scientific research-only network
topology. This evaluation also demonstrates the practical applicability of our
method, yielding significantly improved prediction outcomes in both
environments.
\\ ( https://arxiv.org/abs/2402.00831 ,  2426kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00839 (*cross-listing*)
Date: Thu, 1 Feb 2024 18:29:16 GMT   (677kb,D)

Title: X-CBA: Explainability Aided CatBoosted Anomal-E for Intrusion Detection
  System
Authors: Kiymet Kaya, Elif Ak, Sumeyye Bas, Berk Canberk, Sule Gunduz Oguducu
Categories: cs.CR cs.AI cs.LG cs.NI
\\
  The effectiveness of Intrusion Detection Systems (IDS) is critical in an era
where cyber threats are becoming increasingly complex. Machine learning (ML)
and deep learning (DL) models provide an efficient and accurate solution for
identifying attacks and anomalies in computer networks. However, using ML and
DL models in IDS has led to a trust deficit due to their non-transparent
decision-making. This transparency gap in IDS research is significant,
affecting confidence and accountability. To address, this paper introduces a
novel Explainable IDS approach, called X-CBA, that leverages the structural
advantages of Graph Neural Networks (GNNs) to effectively process network
traffic data, while also adapting a new Explainable AI (XAI) methodology.
Unlike most GNN-based IDS that depend on labeled network traffic and node
features, thereby overlooking critical packet-level information, our approach
leverages a broader range of traffic data through network flows, including edge
attributes, to improve detection capabilities and adapt to novel threats.
Through empirical testing, we establish that our approach not only achieves
high accuracy with 99.47% in threat detection but also advances the field by
providing clear, actionable explanations of its analytical outcomes. This
research also aims to bridge the current gap and facilitate the broader
integration of ML/DL technologies in cybersecurity defenses by offering a local
and global explainability solution that is both precise and interpretable.
\\ ( https://arxiv.org/abs/2402.00839 ,  677kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00126 (*cross-listing*)
Date: Wed, 31 Jan 2024 19:11:58 GMT   (5714kb,D)

Title: Common Sense Reasoning for Deep Fake Detection
Authors: Yue Zhang, Ben Colman, Ali Shahriyari, Gaurav Bharaj
Categories: cs.CV cs.CL
\\
  State-of-the-art approaches rely on image-based features extracted via neural
networks for the deepfake detection binary classification. While these
approaches trained in the supervised sense extract likely fake features, they
may fall short in representing unnatural `non-physical' semantic facial
attributes -- blurry hairlines, double eyebrows, rigid eye pupils, or unnatural
skin shading. However, such facial attributes are generally easily perceived by
humans via common sense reasoning. Furthermore, image-based feature extraction
methods that provide visual explanation via saliency maps can be hard to be
interpreted by humans. To address these challenges, we propose the use of
common sense reasoning to model deepfake detection, and extend it to the
Deepfake Detection VQA (DD-VQA) task with the aim to model human intuition in
explaining the reason behind labeling an image as either real or fake. To this
end, we introduce a new dataset that provides answers to the questions related
to the authenticity of an image, along with its corresponding explanations. We
also propose a Vision and Language Transformer-based framework for the DD-VQA
task, incorporating text and image aware feature alignment formulations.
Finally, we evaluate our method on both the performance of deepfake detection
and the quality of the generated explanations. We hope that this task inspires
researchers to explore new avenues for enhancing language-based
interpretability and cross-modality applications in the realm of deepfake
detection.
\\ ( https://arxiv.org/abs/2402.00126 ,  5714kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00253 (*cross-listing*)
Date: Thu, 1 Feb 2024 00:33:21 GMT   (1279kb,D)

Title: A Survey on Hallucination in Large Vision-Language Models
Authors: Hanchao Liu and Wenyuan Xue and Yifei Chen and Dapeng Chen and Xiutian
  Zhao and Ke Wang and Liping Hou and Rongjun Li and Wei Peng
Categories: cs.CV cs.CL cs.LG
\\
  Recent development of Large Vision-Language Models (LVLMs) has attracted
growing attention within the AI landscape for its practical implementation
potential. However, ``hallucination'', or more specifically, the misalignment
between factual visual content and corresponding textual generation, poses a
significant challenge of utilizing LVLMs. In this comprehensive survey, we
dissect LVLM-related hallucinations in an attempt to establish an overview and
facilitate future mitigation. Our scrutiny starts with a clarification of the
concept of hallucinations in LVLMs, presenting a variety of hallucination
symptoms and highlighting the unique challenges inherent in LVLM
hallucinations. Subsequently, we outline the benchmarks and methodologies
tailored specifically for evaluating hallucinations unique to LVLMs.
Additionally, we delve into an investigation of the root causes of these
hallucinations, encompassing insights from the training data and model
components. We also critically review existing methods for mitigating
hallucinations. The open questions and future directions pertaining to
hallucinations within LVLMs are discussed to conclude this survey.
\\ ( https://arxiv.org/abs/2402.00253 ,  1279kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00453 (*cross-listing*)
Date: Thu, 1 Feb 2024 09:43:30 GMT   (4375kb,D)

Title: Instruction Makes a Difference
Authors: Tosin Adewumi, Nudrat Habib, Lama Alkhaled, and Elisa Barney
Categories: cs.CV cs.CL
Comments: 14 pages, 8 figures
\\
  We introduce Instruction Document Visual Question Answering (iDocVQA) dataset
and Large Language Document (LLaDoc) model, for training Language-Vision (LV)
models for document analysis and predictions on document images, respectively.
Usually, deep neural networks for the DocVQA task are trained on datasets
lacking instructions. We show that using instruction-following datasets
improves performance. We compare performance across document-related datasets
using the recent state-of-the-art (SotA) Large Language and Vision Assistant
(LLaVA)1.5 as the base model. We also evaluate the performance of the derived
models for object hallucination using the Polling-based Object Probing
Evaluation (POPE) dataset. The results show that instruction-tuning performance
ranges from 11X to 32X of zero-shot performance and from 0.1% to 4.2% over
non-instruction (traditional task) finetuning. Despite the gains, these still
fall short of human performance (94.36%), implying there's much room for
improvement.
\\ ( https://arxiv.org/abs/2402.00453 ,  4375kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00744 (*cross-listing*)
Date: Thu, 1 Feb 2024 16:39:47 GMT   (16441kb,D)

Title: BATON: Aligning Text-to-Audio Model with Human Preference Feedback
Authors: Huan Liao, Haonan Han, Kai Yang, Tianjiao Du, Rui Yang, Zunnan Xu,
  Qinmei Xu, Jingquan Liu, Jiasheng Lu, Xiu Li
Categories: cs.SD cs.CL eess.AS
\\
  With the development of AI-Generated Content (AIGC), text-to-audio models are
gaining widespread attention. However, it is challenging for these models to
generate audio aligned with human preference due to the inherent information
density of natural language and limited model understanding ability. To
alleviate this issue, we formulate the BATON, a framework designed to enhance
the alignment between generated audio and text prompt using human preference
feedback. Our BATON comprises three key stages: Firstly, we curated a dataset
containing both prompts and the corresponding generated audio, which was then
annotated based on human feedback. Secondly, we introduced a reward model using
the constructed dataset, which can mimic human preference by assigning rewards
to input text-audio pairs. Finally, we employed the reward model to fine-tune
an off-the-shelf text-to-audio model. The experiment results demonstrate that
our BATON can significantly improve the generation quality of the original
text-to-audio models, concerning audio integrity, temporal relationship, and
alignment with human preference.
\\ ( https://arxiv.org/abs/2402.00744 ,  16441kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00011 (*cross-listing*)
Date: Mon, 25 Dec 2023 11:58:37 GMT   (2109kb)

Title: Choosing the Right Path for AI Integration in Engineering Companies: A
  Strategic Guide
Authors: Rimma Dzhusupova, Jan Bosch, Helena Holmstrom Olsson
Categories: cs.CY cs.LG cs.SE
Report-no: JSS_111945
Journal-ref: The Journal of Systems & Software, 2023
DOI: 10.1016/j.jss.2023.111945
\\
  The Engineering, Procurement and Construction (EPC) businesses operating
within the energy sector are recognizing the increasing importance of
Artificial Intelligence (AI). Many EPC companies and their clients have
realized the benefits of applying AI to their businesses in order to reduce
manual work, drive productivity, and streamline future operations of engineered
installations in a highly competitive industry. The current AI market offers
various solutions and services to support this industry, but organizations must
understand how to acquire AI technology in the most beneficial way based on
their business strategy and available resources. This paper presents a
framework for EPC companies in their transformation towards AI. Our work is
based on examples of project execution of AI-based products development at one
of the biggest EPC contractors worldwide and on insights from EPC vendor
companies already integrating AI into their engineering solutions. The paper
covers the entire life cycle of building AI solutions, from initial business
understanding to deployment and further evolution. The framework identifies how
various factors influence the choice of approach toward AI project development
within large international engineering corporations. By presenting a practical
guide for optimal approach selection, this paper contributes to the research in
AI project management and organizational strategies for integrating AI
technology into businesses. The framework might also help engineering companies
choose the optimum AI approach to create business value.
\\ ( https://arxiv.org/abs/2402.00011 ,  2109kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00014 (*cross-listing*)
Date: Thu, 28 Dec 2023 14:10:26 GMT   (2934kb,D)

Title: Hybrid quantum cycle generative adversarial network for small molecule
  generation
Authors: Matvei Anoshin, Asel Sagingalieva, Christopher Mansell, Vishal Shete,
  Markus Pflitsch, and Alexey Melnikov
Categories: q-bio.BM cs.ET cs.LG physics.bio-ph quant-ph
Comments: 11 pages, 6 figures, 3 tables
\\
  The contemporary drug design process demands considerable time and resources
to develop each new compound entering the market. Generating small molecules is
a pivotal aspect of drug discovery, essential for developing innovative
pharmaceuticals. Uniqueness, validity, diversity, druglikeliness,
synthesizability, and solubility molecular pharmacokinetic properties, however,
are yet to be maximized. This work introduces several new generative
adversarial network models based on engineering integration of parametrized
quantum circuits into known molecular generative adversarial networks. The
introduced machine learning models incorporate a new multi-parameter reward
function grounded in reinforcement learning principles. Through extensive
experimentation on benchmark drug design datasets, QM9 and PC9, the introduced
models are shown to outperform scores achieved previously. Most prominently,
the new scores indicate an increase of up to 30% in the druglikeness
quantitative estimation. The new hybrid quantum machine learning algorithms, as
well as the achieved scores of pharmacokinetic properties, contribute to the
development of fast and accurate drug discovery processes.
\\ ( https://arxiv.org/abs/2402.00014 ,  2934kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00019 (*cross-listing*)
Date: Mon, 1 Jan 2024 13:03:35 GMT   (103kb)

Title: Diffusion MRI with Machine Learning
Authors: Davood Karimi
Categories: eess.IV cs.CV cs.LG
\\
  Diffusion-weighted magnetic resonance imaging (dMRI) offers unique
capabilities such as noninvasive assessment of brain's micro-structure and
structural connectivity. However, analyzing the dMRI data to extract useful
information for clinical and scientific purposes is challenging. The dMRI
measurements often suffer from strong noise and artifacts, there is usually
high inter-session and inter-scanner heterogeneity in the data and considerable
inter-subject variability in brain structure, and the relationship between
measurements and the phenomena of interest can be highly complex. Recent years
have witnessed increasing use of machine learning methods for dMRI analysis.
This manuscript aims to assess these efforts, with a focus on methods that have
addressed micro-structure mapping, tractography, white matter tract analysis,
as well as data preprocessing and harmonization. We summarize the main
findings, strengths, and weaknesses of the existing methods and suggest topics
for future research. We find that machine learning may be exceptionally suited
to tackle some of the difficult tasks in dMRI analysis. However, for this to
happen, several shortcomings of existing methods and critical unresolved issues
need to be addressed. These include deficient evaluation practices, lack of
rich training datasets and validation benchmarks, as well as model
generalizability, reliability, and explainability concerns.
\\ ( https://arxiv.org/abs/2402.00019 ,  103kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00023 (*cross-listing*)
Date: Fri, 5 Jan 2024 18:11:08 GMT   (8496kb,D)

Title: Using Multi-Temporal Sentinel-1 and Sentinel-2 data for water bodies
  mapping
Authors: Luigi Russo, Francesco Mauro, Babak Memar, Alessandro Sebastianelli,
  Paolo Gamba and Silvia Liberata Ullo
Categories: cs.CV cs.LG eess.IV
\\
  Climate change is intensifying extreme weather events, causing both water
scarcity and severe rainfall unpredictability, and posing threats to
sustainable development, biodiversity, and access to water and sanitation. This
paper aims to provide valuable insights for comprehensive water resource
monitoring under diverse meteorological conditions. An extension of the
SEN2DWATER dataset is proposed to enhance its capabilities for water basin
segmentation. Through the integration of temporally and spatially aligned radar
information from Sentinel-1 data with the existing multispectral Sentinel-2
data, a novel multisource and multitemporal dataset is generated. Benchmarking
the enhanced dataset involves the application of indices such as the Soil Water
Index (SWI) and Normalized Difference Water Index (NDWI), along with an
unsupervised Machine Learning (ML) classifier (k-means clustering). Promising
results are obtained and potential future developments and applications arising
from this research are also explored.
\\ ( https://arxiv.org/abs/2402.00023 ,  8496kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00035 (*cross-listing*)
Date: Mon, 8 Jan 2024 12:19:46 GMT   (1123kb,D)

Title: Robustness Assessment of a Runway Object Classifier for Safe Aircraft
  Taxiing
Authors: Yizhak Elboher, Raya Elsaleh, Omri Isac, M\'elanie Ducoffe, Audrey
  Galametz, Guillaume Pov\'eda, Ryma Boumazouza, No\'emie Cohen, Guy Katz
Categories: cs.CV cs.LG cs.LO
\\
  As deep neural networks (DNNs) are becoming the prominent solution for many
computational problems, the aviation industry seeks to explore their potential
in alleviating pilot workload and in improving operational safety. However, the
use of DNNs in this type of safety-critical applications requires a thorough
certification process. This need can be addressed through formal verification,
which provides rigorous assurances -- e.g.,~by proving the absence of certain
mispredictions. In this case-study paper, we demonstrate this process using an
image-classifier DNN currently under development at Airbus and intended for use
during the aircraft taxiing phase. We use formal methods to assess this DNN's
robustness to three common image perturbation types: noise, brightness and
contrast, and some of their combinations. This process entails multiple
invocations of the underlying verifier, which might be computationally
expensive; and we therefore propose a method that leverages the monotonicity of
these robustness properties, as well as the results of past verification
queries, in order to reduce the overall number of verification queries required
by nearly 60%. Our results provide an indication of the level of robustness
achieved by the DNN classifier under study, and indicate that it is
considerably more vulnerable to noise than to brightness or contrast
perturbations.
\\ ( https://arxiv.org/abs/2402.00035 ,  1123kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00036 (*cross-listing*)
Date: Mon, 8 Jan 2024 19:01:01 GMT   (2882kb,D)

Title: Kronecker Product Feature Fusion for Convolutional Neural Network in
  Remote Sensing Scene Classification
Authors: Yinzhu Cheng
Categories: cs.CV cs.LG
\\
  Remote Sensing Scene Classification is a challenging and valuable research
topic, in which Convolutional Neural Network (CNN) has played a crucial role.
CNN can extract hierarchical convolutional features from remote sensing
imagery, and Feature Fusion of different layers can enhance CNN's performance.
Two successful Feature Fusion methods, Add and Concat, are employed in certain
state-of-the-art CNN algorithms. In this paper, we propose a novel Feature
Fusion algorithm, which unifies the aforementioned methods using the Kronecker
Product (KPFF), and we discuss the Backpropagation procedure associated with
this algorithm. To validate the efficacy of the proposed method, a series of
experiments are designed and conducted. The results demonstrate its
effectiveness of enhancing CNN's accuracy in Remote sensing scene
classification.
\\ ( https://arxiv.org/abs/2402.00036 ,  2882kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00038 (*cross-listing*)
Date: Wed, 10 Jan 2024 13:06:52 GMT   (727kb,D)

Title: Detecting Brain Tumors through Multimodal Neural Networks
Authors: Antonio Curci and Andrea Esposito
Categories: eess.IV cs.CV cs.LG q-bio.QM
Comments: Pre-print of a manuscript submitted to NeroPRAI 2024. This version
  did not undergo peer review
ACM-class: I.5.4
\\
  Tumors can manifest in various forms and in different areas of the human
body. Brain tumors are specifically hard to diagnose and treat because of the
complexity of the organ in which they develop. Detecting them in time can lower
the chances of death and facilitate the therapy process for patients. The use
of Artificial Intelligence (AI) and, more specifically, deep learning, has the
potential to significantly reduce costs in terms of time and resources for the
discovery and identification of tumors from images obtained through imaging
techniques. This research work aims to assess the performance of a multimodal
model for the classification of Magnetic Resonance Imaging (MRI) scans
processed as grayscale images. The results are promising, and in line with
similar works, as the model reaches an accuracy of around 98\%. We also
highlight the need for explainability and transparency to ensure human control
and safety.
\\ ( https://arxiv.org/abs/2402.00038 ,  727kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00044 (*cross-listing*)
Date: Sun, 21 Jan 2024 12:18:59 GMT   (5056kb,D)

Title: Training microrobots to swim by a large language model
Authors: Zhuoqun Xu and Lailai Zhu
Categories: cs.RO cs.LG
\\
  Machine learning and artificial intelligence have recently represented a
popular paradigm for designing and optimizing robotic systems across various
scales. Recent studies have showcased the innovative application of large
language models (LLMs) in industrial control [1] and in directing legged
walking robots [2]. In this study, we utilize an LLM, GPT-4, to train two
prototypical microrobots for swimming in viscous fluids. Adopting a few-shot
learning approach, we develop a minimal, unified prompt composed of only five
sentences. The same concise prompt successfully guides two distinct articulated
microrobots -- the three-link swimmer and the three-sphere swimmer -- in
mastering their signature strokes. These strokes, initially conceptualized by
physicists, are now effectively interpreted and applied by the LLM, enabling
the microrobots to circumvent the physical constraints inherent to
micro-locomotion. Remarkably, our LLM-based decision-making strategy
substantially surpasses a traditional reinforcement learning method in terms of
training speed. We discuss the nuanced aspects of prompt design, particularly
emphasizing the reduction of monetary expenses of using GPT-4.
\\ ( https://arxiv.org/abs/2402.00044 ,  5056kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00054 (*cross-listing*)
Date: Fri, 26 Jan 2024 19:27:38 GMT   (4104kb,D)

Title: Predicting loss-of-function impact of genetic mutations: a machine
  learning approach
Authors: Arshmeet Kaur and Morteza Sarmadi
Categories: q-bio.GN cs.LG stat.AP
Comments: Index Terms: Machine Learning, Prediction Algorithms, Supervised
  Learning, Support vector machines, K-Nearest Neighbors, RANSAC, Decision
  Trees, Random Forest, Ge- netic mutations, LoFtool, Next Generation
  Sequencing
\\
  The innovation of next-generation sequencing (NGS) techniques has
significantly reduced the price of genome sequencing, lowering barriers to
future medical research; it is now feasible to apply genome sequencing to
studies where it would have previously been cost-inefficient. Identifying
damaging or pathogenic mutations in vast amounts of complex, high-dimensional
genome sequencing data may be of particular interest to researchers. Thus, this
paper's aims were to train machine learning models on the attributes of a
genetic mutation to predict LoFtool scores (which measure a gene's intolerance
to loss-of-function mutations). These attributes included, but were not limited
to, the position of a mutation on a chromosome, changes in amino acids, and
changes in codons caused by the mutation. Models were built using the
univariate feature selection technique f-regression combined with K-nearest
neighbors (KNN), Support Vector Machine (SVM), Random Sample Consensus
(RANSAC), Decision Trees, Random Forest, and Extreme Gradient Boosting
(XGBoost). These models were evaluated using five-fold cross-validated averages
of r-squared, mean squared error, root mean squared error, mean absolute error,
and explained variance. The findings of this study include the training of
multiple models with testing set r-squared values of 0.97.
\\ ( https://arxiv.org/abs/2402.00054 ,  4104kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00067 (*cross-listing*)
Date: Tue, 30 Jan 2024 09:09:22 GMT   (1744kb,D)

Title: Online speaker diarization of meetings guided by speech separation
Authors: Elio Gruttadauria (IP Paris, LTCI, IDS, S2A), Mathieu Fontaine (LTCI,
  IP Paris), Slim Essid (IDS, S2A, LTCI)
Categories: eess.AS cs.LG cs.SD eess.SP
Comments: Accepted at ICASSP 2024
Journal-ref: IEEE International Conference on Acoustics, Speech, and Signal
  Processing, Apr 2024, Seoul (Korea), South Korea
\\
  Overlapped speech is notoriously problematic for speaker diarization systems.
Consequently, the use of speech separation has recently been proposed to
improve their performance. Although promising, speech separation models
struggle with realistic data because they are trained on simulated mixtures
with a fixed number of speakers. In this work, we introduce a new speech
separation-guided diarization scheme suitable for the online speaker
diarization of long meeting recordings with a variable number of speakers, as
present in the AMI corpus. We envisage ConvTasNet and DPRNN as alternatives for
the separation networks, with two or three output sources. To obtain the
speaker diarization result, voice activity detection is applied on each
estimated source. The final model is fine-tuned end-to-end, after first
adapting the separation to real data using AMI. The system operates on short
segments, and inference is performed by stitching the local predictions using
speaker embeddings and incremental clustering. The results show that our system
improves the state-of-the-art on the AMI headset mix, using no oracle
information and under full evaluation (no collar and including overlapped
speech). Finally, we show the strength of our system particularly on overlapped
speech sections.
\\ ( https://arxiv.org/abs/2402.00067 ,  1744kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00077 (*cross-listing*)
Date: Tue, 30 Jan 2024 23:25:05 GMT   (11461kb,D)

Title: Unlocking the Power of Multi-institutional Data: Integrating and
  Harmonizing Genomic Data Across Institutions
Authors: Yuan Chen, Ronglai Shen, Xiwen Feng, Katherine Panageas
Categories: q-bio.GN cs.LG stat.ME
\\
  Cancer is a complex disease driven by genomic alterations, and tumor
sequencing is becoming a mainstay of clinical care for cancer patients. The
emergence of multi-institution sequencing data presents a powerful resource for
learning real-world evidence to enhance precision oncology. GENIE BPC, led by
the American Association for Cancer Research, establishes a unique database
linking genomic data with clinical information for patients treated at multiple
cancer centers. However, leveraging such multi-institutional sequencing data
presents significant challenges. Variations in gene panels result in loss of
information when the analysis is conducted on common gene sets. Additionally,
differences in sequencing techniques and patient heterogeneity across
institutions add complexity. High data dimensionality, sparse gene mutation
patterns, and weak signals at the individual gene level further complicate
matters. Motivated by these real-world challenges, we introduce the Bridge
model. It uses a quantile-matched latent variable approach to derive integrated
features to preserve information beyond common genes and maximize the
utilization of all available data while leveraging information sharing to
enhance both learning efficiency and the model's capacity to generalize. By
extracting harmonized and noise-reduced lower-dimensional latent variables, the
true mutation pattern unique to each individual is captured. We assess the
model's performance and parameter estimation through extensive simulation
studies. The extracted latent features from the Bridge model consistently excel
in predicting patient survival across six cancer types in GENIE BPC data.
\\ ( https://arxiv.org/abs/2402.00077 ,  11461kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00093 (*cross-listing*)
Date: Wed, 31 Jan 2024 12:41:27 GMT   (1640kb,D)

Title: ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation
Authors: Bhabesh Mali, Karthik Maddala, Sweeya Reddy, Vatsal Gupta, Chandan
  Karfa, Ramesh Karri
Categories: cs.SE cs.LG
Comments: 6 pages, 3 figures and 1 table
\\
  System Verilog Assertion (SVA) formulation, a critical yet complex task, is a
pre-requisite in the Formal Property Verification (FPV) process. Traditionally,
SVA formulation involves expert-driven interpretation of specifications. This
is time consuming and prone to human error. However, recent advances in Large
Language Models (LLM), LLM-informed automatic assertion generation is gaining
interest. We designed a novel LLM-based pipeline to generate assertions in
English Language, Linear Temporal Logic, and SVA from natural language
specifications. We developed a custom LLM-based on OpenAI GPT4 for our
experiments. Furthermore, we developed testbenches to verify/validate the
LLM-generated assertions. Only 43% of LLM-generated raw assertions had errors,
including syntax and logical errors. By iteratively prompting the LLMs using
carefully crafted prompts derived from test case failures, the pipeline could
generate correct SVAs after a maximum of nine iterations of prompting. Our
results show that LLMs can streamline the assertion generation workflow,
reshaping verification workflows.
\\ ( https://arxiv.org/abs/2402.00093 ,  1640kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00097 (*cross-listing*)
Date: Wed, 31 Jan 2024 18:21:49 GMT   (3976kb,D)

Title: Code-Aware Prompting: A study of Coverage Guided Test Generation in
  Regression Setting using LLM
Authors: Gabriel Ryan, Siddhartha Jain, Mingyue Shang, Shiqi Wang, Xiaofei Ma,
  Murali Krishna Ramanathan, Baishakhi Ray
Categories: cs.SE cs.LG
\\
  Testing plays a pivotal role in ensuring software quality, yet conventional
Search Based Software Testing (SBST) methods often struggle with complex
software units, achieving suboptimal test coverage. Recent work using large
language models (LLMs) for test generation have focused on improving generation
quality through optimizing the test generation context and correcting errors in
model outputs, but use fixed prompting strategies that prompt the model to
generate tests without additional guidance. As a result LLM-generated test
suites still suffer from low coverage. In this paper, we present SymPrompt, a
code-aware prompting strategy for LLMs in test generation. SymPrompt's approach
is based on recent work that demonstrates LLMs can solve more complex logical
problems when prompted to reason about the problem in a multi-step fashion. We
apply this methodology to test generation by deconstructing the testsuite
generation process into a multi-stage sequence, each of which is driven by a
specific prompt aligned with the execution paths of the method under test, and
exposing relevant type and dependency focal context to the model. Our approach
enables pretrained LLMs to generate more complete test cases without any
additional training. We implement SymPrompt using the TreeSitter parsing
framework and evaluate on a benchmark challenging methods from open source
Python projects. SymPrompt enhances correct test generations by a factor of 5
and bolsters relative coverage by 26% for CodeGen2. Notably, when applied to
GPT-4, symbolic path prompts improve coverage by over 2x compared to baseline
prompting strategies.
\\ ( https://arxiv.org/abs/2402.00097 ,  3976kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00168 (*cross-listing*)
Date: Wed, 31 Jan 2024 20:50:18 GMT   (75kb,D)

Title: Continuous Treatment Effects with Surrogate Outcomes
Authors: Zhenghao Zeng, David Arbour, Avi Feller, Raghavendra Addanki, Ryan
  Rossi, Ritwik Sinha, Edward H. Kennedy
Categories: stat.ML cs.LG stat.ME
Comments: 26 pages, 4 figures
\\
  In many real-world causal inference applications, the primary outcomes
(labels) are often partially missing, especially if they are expensive or
difficult to collect. If the missingness depends on covariates (i.e.,
missingness is not completely at random), analyses based on fully-observed
samples alone may be biased. Incorporating surrogates, which are fully observed
post-treatment variables related to the primary outcome, can improve estimation
in this case. In this paper, we study the role of surrogates in estimating
continuous treatment effects and propose a doubly robust method to efficiently
incorporate surrogates in the analysis, which uses both labeled and unlabeled
data and does not suffer from the above selection bias problem. Importantly, we
establish asymptotic normality of the proposed estimator and show possible
improvements on the variance compared with methods that solely use labeled
data. Extensive simulations show our methods enjoy appealing empirical
performance.
\\ ( https://arxiv.org/abs/2402.00168 ,  75kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00176 (*cross-listing*)
Date: Wed, 31 Jan 2024 21:07:43 GMT   (180kb,D)

Title: Adversarial Quantum Machine Learning: An Information-Theoretic
  Generalization Analysis
Authors: Petros Georgiou, Sharu Theresa Jose and Osvaldo Simeone
Categories: quant-ph cs.ET cs.LG
Comments: 10 pages, 2 figures
\\
  In a manner analogous to their classical counterparts, quantum classifiers
are vulnerable to adversarial attacks that perturb their inputs. A promising
countermeasure is to train the quantum classifier by adopting an attack-aware,
or adversarial, loss function. This paper studies the generalization properties
of quantum classifiers that are adversarially trained against bounded-norm
white-box attacks. Specifically, a quantum adversary maximizes the classifier's
loss by transforming an input state $\rho(x)$ into a state $\lambda$ that is
$\epsilon$-close to the original state $\rho(x)$ in $p$-Schatten distance.
Under suitable assumptions on the quantum embedding $\rho(x)$, we derive novel
information-theoretic upper bounds on the generalization error of adversarially
trained quantum classifiers for $p = 1$ and $p = \infty$. The derived upper
bounds consist of two terms: the first is an exponential function of the
2-R\'enyi mutual information between classical data and quantum embedding,
while the second term scales linearly with the adversarial perturbation size
$\epsilon$. Both terms are shown to decrease as $1/\sqrt{T}$ over the training
set size $T$ . An extension is also considered in which the adversary assumed
during training has different parameters $p$ and $\epsilon$ as compared to the
adversary affecting the test inputs. Finally, we validate our theoretical
findings with numerical experiments for a synthetic setting.
\\ ( https://arxiv.org/abs/2402.00176 ,  180kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00222 (*cross-listing*)
Date: Wed, 31 Jan 2024 22:50:49 GMT   (32548kb,D)

Title: Uncover the nature of overlapping community in cities
Authors: Peng Luo and Di Zhu
Categories: physics.soc-ph cs.LG
\\
  Urban spaces, though often perceived as discrete communities, are shared by
various functional and social groups. Our study introduces a graph-based
physics-aware deep learning framework, illuminating the intricate overlapping
nature inherent in urban communities. Through analysis of individual mobile
phone positioning data at Twin Cities metro area (TCMA) in Minnesota, USA, our
findings reveal that 95.7 % of urban functional complexity stems from the
overlapping structure of communities during weekdays. Significantly, our
research not only quantifies these overlaps but also reveals their compelling
correlations with income and racial indicators, unraveling the complex
segregation patterns in U.S. cities. As the first to elucidate the overlapping
nature of urban communities, this work offers a unique geospatial perspective
on looking at urban structures, highlighting the nuanced interplay of
socioeconomic dynamics within cities.
\\ ( https://arxiv.org/abs/2402.00222 ,  32548kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00261 (*cross-listing*)
Date: Thu, 1 Feb 2024 01:11:15 GMT   (2316kb,D)

Title: Understanding Neural Network Systems for Image Analysis using Vector
  Spaces and Inverse Maps
Authors: Rebecca Pattichis and Marios S. Pattichis
Categories: cs.CV cs.LG
\\
  There is strong interest in developing mathematical methods that can be used
to understand complex neural networks used in image analysis. In this paper, we
introduce techniques from Linear Algebra to model neural network layers as maps
between signal spaces. First, we demonstrate how signal spaces can be used to
visualize weight spaces and convolutional layer kernels. We also demonstrate
how residual vector spaces can be used to further visualize information lost at
each layer. Second, we introduce the concept of invertible networks and an
algorithm for computing input images that yield specific outputs. We
demonstrate our approach on two invertible networks and ResNet18.
\\ ( https://arxiv.org/abs/2402.00261 ,  2316kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00299 (*cross-listing*)
Date: Thu, 1 Feb 2024 03:20:53 GMT   (1829kb,D)

Title: Attention-based Dynamic Multilayer Graph Neural Networks for Loan
  Default Prediction
Authors: Sahab Zandi, Kamesh Korangi, Mar\'ia \'Oskarsd\'ottir, Christophe
  Mues, Cristi\'an Bravo
Categories: q-fin.GN cs.LG
\\
  Whereas traditional credit scoring tends to employ only individual borrower-
or loan-level predictors, it has been acknowledged for some time that
connections between borrowers may result in default risk propagating over a
network. In this paper, we present a model for credit risk assessment
leveraging a dynamic multilayer network built from a Graph Neural Network and a
Recurrent Neural Network, each layer reflecting a different source of network
connection. We test our methodology in a behavioural credit scoring context
using a dataset provided by U.S. mortgage financier Freddie Mac, in which
different types of connections arise from the geographical location of the
borrower and their choice of mortgage provider. The proposed model considers
both types of connections and the evolution of these connections over time. We
enhance the model by using a custom attention mechanism that weights the
different time snapshots according to their importance. After testing multiple
configurations, a model with GAT, LSTM, and the attention mechanism provides
the best results. Empirical results demonstrate that, when it comes to
predicting probability of default for the borrowers, our proposed model brings
both better results and novel insights for the analysis of the importance of
connections and timestamps, compared to traditional methods.
\\ ( https://arxiv.org/abs/2402.00299 ,  1829kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00300 (*cross-listing*)
Date: Thu, 1 Feb 2024 03:27:26 GMT   (10981kb,D)

Title: Self-supervised learning of video representations from a child's
  perspective
Authors: A. Emin Orhan, Wentao Wang, Alex N. Wang, Mengye Ren, Brenden M. Lake
Categories: cs.CV cs.LG cs.NE q-bio.NC
Comments: 7 pages, 6 figures; code & models available from
  https://github.com/eminorhan/video-models
\\
  Children learn powerful internal models of the world around them from a few
years of egocentric visual experience. Can such internal models be learned from
a child's visual experience with highly generic learning algorithms or do they
require strong inductive biases? Recent advances in collecting large-scale,
longitudinal, developmentally realistic video datasets and generic
self-supervised learning (SSL) algorithms are allowing us to begin to tackle
this nature vs. nurture question. However, existing work typically focuses on
image-based SSL algorithms and visual capabilities that can be learned from
static images (e.g. object recognition), thus ignoring temporal aspects of the
world. To close this gap, here we train self-supervised video models on
longitudinal, egocentric headcam recordings collected from a child over a two
year period in their early development (6-31 months). The resulting models are
highly effective at facilitating the learning of action concepts from a small
number of labeled examples; they have favorable data size scaling properties;
and they display emergent video interpolation capabilities. Video models also
learn more robust object representations than image-based models trained with
the exact same data. These results suggest that important temporal aspects of a
child's internal model of the world may be learnable from their visual
experience using highly generic learning algorithms and without strong
inductive biases.
\\ ( https://arxiv.org/abs/2402.00300 ,  10981kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00310 (*cross-listing*)
Date: Thu, 1 Feb 2024 03:49:10 GMT   (7188kb,D)

Title: Seismic Traveltime Tomography with Label-free Learning
Authors: Feng Wang, Bo Yang, Renfang Wang and Hong Qiu
Categories: physics.geo-ph cs.LG
Comments: 15 pages, 19 figures. Submitted to IEEE Transactions on Geoscience
  and Remote Sensing
\\
  Deep learning techniques have been used to build velocity models (VMs) for
seismic traveltime tomography and have shown encouraging performance in recent
years. However, they need to generate labeled samples (i.e., pairs of input and
label) to train the deep neural network (NN) with end-to-end learning, and the
real labels for field data inversion are usually missing or very expensive.
Some traditional tomographic methods can be implemented quickly, but their
effectiveness is often limited by prior assumptions. To avoid generating
labeled samples, we propose a novel method by integrating deep learning and
dictionary learning to enhance the VMs with low resolution by using the
traditional tomography-least square method (LSQR). We first design a type of
shallow and simple NN to reduce computational cost followed by proposing a
two-step strategy to enhance the VMs with low resolution: (1) Warming up. An
initial dictionary is trained from the estimation by LSQR through dictionary
learning method; (2) Dictionary optimization. The initial dictionary obtained
in the warming-up step will be optimized by the NN, and then it will be used to
reconstruct high-resolution VMs with the reference slowness and the estimation
by LSQR. Furthermore, we design a loss function to minimize traveltime misfit
to ensure that NN training is label-free, and the optimized dictionary can be
obtained after each epoch of NN training. We demonstrate the effectiveness of
the proposed method through numerical tests.
\\ ( https://arxiv.org/abs/2402.00310 ,  7188kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00376 (*cross-listing*)
Date: Thu, 1 Feb 2024 06:47:56 GMT   (915kb)

Title: Image2Points:A 3D Point-based Context Clusters GAN for High-Quality PET
  Image Reconstruction
Authors: Jiaqi Cui, Yan Wang, Lu Wen, Pinxian Zeng, Xi Wu, Jiliu Zhou, Dinggang
  Shen
Categories: eess.IV cs.CV cs.LG
Comments: Accepted by ICASSP 2024
\\
  To obtain high-quality Positron emission tomography (PET) images while
minimizing radiation exposure, numerous methods have been proposed to
reconstruct standard-dose PET (SPET) images from the corresponding low-dose PET
(LPET) images. However, these methods heavily rely on voxel-based
representations, which fall short of adequately accounting for the precise
structure and fine-grained context, leading to compromised reconstruction. In
this paper, we propose a 3D point-based context clusters GAN, namely PCC-GAN,
to reconstruct high-quality SPET images from LPET. Specifically, inspired by
the geometric representation power of points, we resort to a point-based
representation to enhance the explicit expression of the image structure, thus
facilitating the reconstruction with finer details. Moreover, a context
clustering strategy is applied to explore the contextual relationships among
points, which mitigates the ambiguities of small structures in the
reconstructed images. Experiments on both clinical and phantom datasets
demonstrate that our PCC-GAN outperforms the state-of-the-art reconstruction
methods qualitatively and quantitatively. Code is available at
https://github.com/gluucose/PCCGAN.
\\ ( https://arxiv.org/abs/2402.00376 ,  915kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00418 (*cross-listing*)
Date: Thu, 1 Feb 2024 08:36:16 GMT   (464kb)

Title: Short: Benchmarking transferable adversarial attacks
Authors: Zhibo Jin, Jiayu Zhang, Zhiyu Zhu, Huaming Chen
Categories: cs.CV cs.LG
Comments: Accepted by NDSS 2024 Workshop
\\
  The robustness of deep learning models against adversarial attacks remains a
pivotal concern. This study presents, for the first time, an exhaustive review
of the transferability aspect of adversarial attacks. It systematically
categorizes and critically evaluates various methodologies developed to augment
the transferability of adversarial attacks. This study encompasses a spectrum
of techniques, including Generative Structure, Semantic Similarity, Gradient
Editing, Target Modification, and Ensemble Approach. Concurrently, this paper
introduces a benchmark framework \textit{TAA-Bench}, integrating ten leading
methodologies for adversarial attack transferability, thereby providing a
standardized and systematic platform for comparative analysis across diverse
model architectures. Through comprehensive scrutiny, we delineate the efficacy
and constraints of each method, shedding light on their underlying operational
principles and practical utility. This review endeavors to be a quintessential
resource for both scholars and practitioners in the field, charting the complex
terrain of adversarial transferability and setting a foundation for future
explorations in this vital sector. The associated codebase is accessible at:
https://github.com/KxPlaug/TAA-Bench
\\ ( https://arxiv.org/abs/2402.00418 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00501 (*cross-listing*)
Date: Thu, 1 Feb 2024 11:12:00 GMT   (57kb)

Title: Equivalence of the Empirical Risk Minimization to Regularization on the
  Family of f-Divergences
Authors: Francisco Daunas, I\~naki Esnaola, Samir M. Perlaza, H. Vincent Poor
Categories: stat.ML cs.IT cs.LG math.IT
Comments: Submitted to the IEEE Symposium in Information Theory 2024. arXiv
  admin note: text overlap with arXiv:2306.07123
\\
  The solution to empirical risk minimization with $f$-divergence
regularization (ERM-$f$DR) is presented under mild conditions on $f$. Under
such conditions, the optimal measure is shown to be unique. Examples of the
solution for particular choices of the function $f$ are presented. Previously
known solutions to common regularization choices are obtained by leveraging the
flexibility of the family of $f$-divergences. These include the unique
solutions to empirical risk minimization with relative entropy regularization
(Type-I and Type-II). The analysis of the solution unveils the following
properties of $f$-divergences when used in the ERM-$f$DR problem: $i\bigl)$
$f$-divergence regularization forces the support of the solution to coincide
with the support of the reference measure, which introduces a strong inductive
bias that dominates the evidence provided by the training data; and $ii\bigl)$
any $f$-divergence regularization is equivalent to a different $f$-divergence
regularization with an appropriate transformation of the empirical risk
function.
\\ ( https://arxiv.org/abs/2402.00501 ,  57kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00515 (*cross-listing*)
Date: Thu, 1 Feb 2024 11:31:26 GMT   (294kb,D)

Title: Developing A Multi-Agent and Self-Adaptive Framework with Deep
  Reinforcement Learning for Dynamic Portfolio Risk Management
Authors: Zhenglong Li, Vincent Tam, Kwan L. Yeung
Categories: q-fin.PM cs.LG
Comments: Accepted by The 23rd International Conference on Autonomous Agents
  and Multi-Agent Systems
\\
  Deep or reinforcement learning (RL) approaches have been adapted as reactive
agents to quickly learn and respond with new investment strategies for
portfolio management under the highly turbulent financial market environments
in recent years. In many cases, due to the very complex correlations among
various financial sectors, and the fluctuating trends in different financial
markets, a deep or reinforcement learning based agent can be biased in
maximising the total returns of the newly formulated investment portfolio while
neglecting its potential risks under the turmoil of various market conditions
in the global or regional sectors. Accordingly, a multi-agent and self-adaptive
framework namely the MASA is proposed in which a sophisticated multi-agent
reinforcement learning (RL) approach is adopted through two cooperating and
reactive agents to carefully and dynamically balance the trade-off between the
overall portfolio returns and their potential risks. Besides, a very flexible
and proactive agent as the market observer is integrated into the MASA
framework to provide some additional information on the estimated market trends
as valuable feedbacks for multi-agent RL approach to quickly adapt to the
ever-changing market conditions. The obtained empirical results clearly reveal
the potential strengths of our proposed MASA framework based on the multi-agent
RL approach against many well-known RL-based approaches on the challenging data
sets of the CSI 300, Dow Jones Industrial Average and S&P 500 indexes over the
past 10 years. More importantly, our proposed MASA framework shed lights on
many possible directions for future investigation.
\\ ( https://arxiv.org/abs/2402.00515 ,  294kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00534 (*cross-listing*)
Date: Thu, 1 Feb 2024 12:01:43 GMT   (4572kb,D)

Title: A Manifold Representation of the Key in Vision Transformers
Authors: Li Meng, Morten Goodwin, Anis Yazidi, Paal Engelstad
Categories: cs.CV cs.LG
\\
  Vision Transformers implement multi-head self-attention (MSA) via stacking
multiple attention blocks. The query, key, and value are often intertwined and
generated within those blocks via a single, shared linear transformation. This
paper explores the concept of disentangling the key from the query and value,
and adopting a manifold representation for the key. Our experiments reveal that
decoupling and endowing the key with a manifold structure can enhance the model
performance. Specifically, ViT-B exhibits a 0.87% increase in top-1 accuracy,
while Swin-T sees a boost of 0.52% in top-1 accuracy on the ImageNet-1K
dataset, with eight charts in the manifold key. Our approach also yields
positive results in object detection and instance segmentation tasks on the
COCO dataset. Through detailed ablation studies, we establish that these
performance gains are not merely due to the simplicity of adding more
parameters and computations. Future research may investigate strategies for
cutting the budget of such representations and aim for further performance
improvements based on our findings.
\\ ( https://arxiv.org/abs/2402.00534 ,  4572kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00544 (*cross-listing*)
Date: Thu, 1 Feb 2024 12:13:35 GMT   (286kb,D)

Title: Quantum-Assisted Hilbert-Space Gaussian Process Regression
Authors: Ahmad Farooq, Cristian A. Galvis-Florez, and Simo S\"arkk\"a
Categories: stat.CO cs.LG quant-ph
Comments: 9 pages, 5 figures
\\
  Gaussian processes are probabilistic models that are commonly used as
functional priors in machine learning. Due to their probabilistic nature, they
can be used to capture the prior information on the statistics of noise,
smoothness of the functions, and training data uncertainty. However, their
computational complexity quickly becomes intractable as the size of the data
set grows. We propose a Hilbert space approximation-based quantum algorithm for
Gaussian process regression to overcome this limitation. Our method consists of
a combination of classical basis function expansion with quantum computing
techniques of quantum principal component analysis, conditional rotations, and
Hadamard and Swap tests. The quantum principal component analysis is used to
estimate the eigenvalues while the conditional rotations and the Hadamard and
Swap tests are employed to evaluate the posterior mean and variance of the
Gaussian process. Our method provides polynomial computational complexity
reduction over the classical method.
\\ ( https://arxiv.org/abs/2402.00544 ,  286kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00564 (*cross-listing*)
Date: Thu, 1 Feb 2024 12:50:48 GMT   (242kb,D)

Title: A Single Graph Convolution Is All You Need: Efficient Grayscale Image
  Classification
Authors: Jacob Fein-Ashley, Tian Ye, Sachini Wickramasinghe, Bingyi Zhang,
  Rajgopal Kannan, Viktor Prasanna
Categories: cs.CV cs.LG
Comments: 6 pages of content, 1 page of references
\\
  Image classifiers often rely on convolutional neural networks (CNN) for their
tasks, which are inherently more heavyweight than multilayer perceptrons
(MLPs), which can be problematic in real-time applications. Additionally, many
image classification models work on both RGB and grayscale datasets.
Classifiers that operate solely on grayscale images are much less common.
Grayscale image classification has diverse applications, including but not
limited to medical image classification and synthetic aperture radar (SAR)
automatic target recognition (ATR). Thus, we present a novel grayscale (single
channel) image classification approach using a vectorized view of images. We
exploit the lightweightness of MLPs by viewing images as a vector and reducing
our problem setting to the grayscale image classification setting. We find that
using a single graph convolutional layer batch-wise increases accuracy and
reduces variance in the performance of our model. Moreover, we develop a
customized accelerator on FPGA for the proposed model with several
optimizations to improve its performance. Our experimental results on benchmark
grayscale image datasets demonstrate the effectiveness of the proposed model,
achieving vastly lower latency (up to 16$\times$ less) and competitive or
leading performance compared to other state-of-the-art image classification
models on various domain-specific grayscale image classification datasets.
\\ ( https://arxiv.org/abs/2402.00564 ,  242kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00568 (*cross-listing*)
Date: Thu, 1 Feb 2024 13:01:47 GMT   (667kb)

Title: Secure Supervised Learning-Based Smart Home Authentication Framework
Authors: K. Swapna Sudha, N. Jeyanthi, and Celestine Iwendi
Categories: cs.CR cs.LG
\\
  The Smart home possesses the capability of facilitating home services to
their users with the systematic advance in The Internet of Things (IoT) and
information and communication technologies (ICT) in recent decades. The home
service offered by the smart devices helps the users in utilize maximized level
of comfort for the objective of improving life quality. As the user and smart
devices communicate through an insecure channel, the smart home environment is
prone to security and privacy problems. A secure authentication protocol needs
to be established between the smart devices and the user, such that a situation
for device authentication can be made feasible in smart home environments. Most
of the existing smart home authentication protocols were identified to fail in
facilitating a secure mutual authentication and increases the possibility of
lunching the attacks of session key disclosure, impersonation and stolen smart
device. In this paper, Secure Supervised Learning-based Smart Home
Authentication Framework (SSL-SHAF) is proposed as are liable mutual
authentication that can be contextually imposed for better security. The formal
analysis of the proposed SSL-SHAF confirmed better resistance against session
key disclosure, impersonation and stolen smart device attacks. The results of
SSL-SHAF confirmed minimized computational costs and security compared to the
baseline protocols considered for investigation.
\\ ( https://arxiv.org/abs/2402.00568 ,  667kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00623 (*cross-listing*)
Date: Thu, 1 Feb 2024 14:39:59 GMT   (4558kb,D)

Title: Bayesian Causal Inference with Gaussian Process Networks
Authors: Enrico Giudice, Jack Kuipers and Giusi Moffa
Categories: stat.ML cs.LG
\\
  Causal discovery and inference from observational data is an essential
problem in statistics posing both modeling and computational challenges. These
are typically addressed by imposing strict assumptions on the joint
distribution such as linearity. We consider the problem of the Bayesian
estimation of the effects of hypothetical interventions in the Gaussian Process
Network (GPN) model, a flexible causal framework which allows describing the
causal relationships nonparametrically. We detail how to perform causal
inference on GPNs by simulating the effect of an intervention across the whole
network and propagating the effect of the intervention on downstream variables.
We further derive a simpler computational approximation by estimating the
intervention distribution as a function of local variables only, modeling the
conditional distributions via additive Gaussian processes. We extend both
frameworks beyond the case of a known causal graph, incorporating uncertainty
about the causal structure via Markov chain Monte Carlo methods. Simulation
studies show that our approach is able to identify the effects of hypothetical
interventions with non-Gaussian, non-linear observational data and accurately
reflect the posterior uncertainty of the causal estimates. Finally we compare
the results of our GPN-based causal inference approach to existing methods on a
dataset of $A.~thaliana$ gene expressions.
\\ ( https://arxiv.org/abs/2402.00623 ,  4558kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00626 (*cross-listing*)
Date: Thu, 1 Feb 2024 14:41:20 GMT   (7922kb,D)

Title: Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks
Authors: Maan Qraitem, Nazia Tasnim, Kate Saenko, Bryan A. Plummer
Categories: cs.CV cs.CR cs.LG
\\
  Recently, significant progress has been made on Large Vision-Language Models
(LVLMs); a new class of VL models that make use of large pre-trained language
models. Yet, their vulnerability to Typographic attacks, which involve
superimposing misleading text onto an image remain unstudied. Furthermore,
prior work typographic attacks rely on sampling a random misleading class from
a predefined set of classes. However, the random chosen class might not be the
most effective attack. To address these issues, we first introduce a novel
benchmark uniquely designed to test LVLMs vulnerability to typographic attacks.
Furthermore, we introduce a new and more effective typographic attack:
Self-Generated typographic attacks. Indeed, our method, given an image, make
use of the strong language capabilities of models like GPT-4V by simply
prompting them to recommend a typographic attack. Using our novel benchmark, we
uncover that typographic attacks represent a significant threat against
LVLM(s). Furthermore, we uncover that typographic attacks recommended by GPT-4V
using our new method are not only more effective against GPT-4V itself compared
to prior work attacks, but also against a host of less capable yet popular open
source models like LLaVA, InstructBLIP, and MiniGPT4.
\\ ( https://arxiv.org/abs/2402.00626 ,  7922kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00645 (*cross-listing*)
Date: Thu, 1 Feb 2024 15:07:31 GMT   (118kb,D)

Title: Spectrally Transformed Kernel Regression
Authors: Runtian Zhai, Rattana Pukdee, Roger Jin, Maria-Florina Balcan, Pradeep
  Ravikumar
Categories: stat.ML cs.LG
Comments: ICLR 2024 spotlight. 36 pages
\\
  Unlabeled data is a key component of modern machine learning. In general, the
role of unlabeled data is to impose a form of smoothness, usually from the
similarity information encoded in a base kernel, such as the
$\epsilon$-neighbor kernel or the adjacency matrix of a graph. This work
revisits the classical idea of spectrally transformed kernel regression (STKR),
and provides a new class of general and scalable STKR estimators able to
leverage unlabeled data. Intuitively, via spectral transformation, STKR
exploits the data distribution for which unlabeled data can provide additional
information. First, we show that STKR is a principled and general approach, by
characterizing a universal type of "target smoothness", and proving that any
sufficiently smooth function can be learned by STKR. Second, we provide
scalable STKR implementations for the inductive setting and a general
transformation function, while prior work is mostly limited to the transductive
setting. Third, we derive statistical guarantees for two scenarios: STKR with a
known polynomial transformation, and STKR with kernel PCA when the
transformation is unknown. Overall, we believe that this work helps deepen our
understanding of how to work with unlabeled data, and its generality makes it
easier to inspire new methods.
\\ ( https://arxiv.org/abs/2402.00645 ,  118kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00653 (*cross-listing*)
Date: Thu, 1 Feb 2024 15:13:26 GMT   (518kb,D)

Title: Coherent Feed Forward Quantum Neural Network
Authors: Utkarsh Singh, Aaron Z. Goldberg, Khabat Heshami
Categories: quant-ph cs.LG
Comments: 11 pages, 7 figures. Comments welcome!
\\
  Quantum machine learning, focusing on quantum neural networks (QNNs), remains
a vastly uncharted field of study. Current QNN models primarily employ
variational circuits on an ansatz or a quantum feature map, often requiring
multiple entanglement layers. This methodology not only increases the
computational cost of the circuit beyond what is practical on near-term quantum
devices but also misleadingly labels these models as neural networks, given
their divergence from the structure of a typical feed-forward neural network
(FFNN). Moreover, the circuit depth and qubit needs of these models scale
poorly with the number of data features, resulting in an efficiency challenge
for real-world machine-learning tasks. We introduce a bona fide QNN model,
which seamlessly aligns with the versatility of a traditional FFNN in terms of
its adaptable intermediate layers and nodes, absent from intermediate
measurements such that our entire model is coherent. This model stands out with
its reduced circuit depth and number of requisite C-NOT gates to outperform
prevailing QNN models. Furthermore, the qubit count in our model remains
unaffected by the data's feature quantity. We test our proposed model on
various benchmarking datasets such as the diagnostic breast cancer (Wisconsin)
and credit card fraud detection datasets. We compare the outcomes of our model
with the existing QNN methods to showcase the advantageous efficacy of our
approach, even with a reduced requirement on quantum resources. Our model paves
the way for application of quantum neural networks to real relevant machine
learning problems.
\\ ( https://arxiv.org/abs/2402.00653 ,  518kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00695 (*cross-listing*)
Date: Thu, 1 Feb 2024 15:51:46 GMT   (5787kb,D)

Title: Approximating Optimal Morphing Attacks using Template Inversion
Authors: Laurent Colbois, Hatef Otroshi Shahreza, S\'ebastien Marcel
Categories: cs.CV cs.CR cs.LG
Comments: Published at the IEEE International Joint Conference on Biometrics
  (IJCB) 2023
\\
  Recent works have demonstrated the feasibility of inverting face recognition
systems, enabling to recover convincing face images using only their
embeddings. We leverage such template inversion models to develop a novel type
ofdeep morphing attack based on inverting a theoretical optimal morph
embedding, which is obtained as an average of the face embeddings of source
images. We experiment with two variants of this approach: the first one
exploits a fully self-contained embedding-to-image inversion model, while the
second leverages the synthesis network of a pretrained StyleGAN network for
increased morph realism. We generate morphing attacks from several source
datasets and study the effectiveness of those attacks against several face
recognition networks. We showcase that our method can compete with and
regularly beat the previous state of the art for deep-learning based morph
generation in terms of effectiveness, both in white-box and black-box attack
scenarios, and is additionally much faster to run. We hope this might
facilitate the development of large scale deep morph datasets for training
detection models.
\\ ( https://arxiv.org/abs/2402.00695 ,  5787kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00724 (*cross-listing*)
Date: Thu, 1 Feb 2024 16:14:54 GMT   (3911kb)

Title: Automatic Segmentation of the Spinal Cord Nerve Rootlets
Authors: Jan Valosek, Theo Mathieu, Raphaelle Schlienger, Olivia S. Kowalczyk,
  Julien Cohen-Adad
Categories: cs.CV cs.LG
\\
  Precise identification of spinal nerve rootlets is relevant to delineate
spinal levels for the study of functional activity in the spinal cord. The goal
of this study was to develop an automatic method for the semantic segmentation
of spinal nerve rootlets from T2-weighted magnetic resonance imaging (MRI)
scans. Images from two open-access MRI datasets were used to train a 3D
multi-class convolutional neural network using an active learning approach to
segment C2-C8 dorsal nerve rootlets. Each output class corresponds to a spinal
level. The method was tested on 3T T2-weighted images from datasets unseen
during training to assess inter-site, inter-session, and inter-resolution
variability. The test Dice score was 0.67 +- 0.16 (mean +- standard deviation
across rootlets levels), suggesting a good performance. The method also
demonstrated low inter-vendor and inter-site variability (coefficient of
variation <= 1.41 %), as well as low inter-session variability (coefficient of
variation <= 1.30 %) indicating stable predictions across different MRI
vendors, sites, and sessions. The proposed methodology is open-source and
readily available in the Spinal Cord Toolbox (SCT) v6.2 and higher.
\\ ( https://arxiv.org/abs/2402.00724 ,  3911kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00760 (*cross-listing*)
Date: Thu, 1 Feb 2024 16:50:41 GMT   (3896kb,D)

Title: EuroPED-NN: Uncertainty aware surrogate model
Authors: A. Panera Alvarez, A. Ho, A. Jarvinen, S. Saarelma, S. Wiesen and JET
  Contributors
Categories: physics.plasm-ph cs.LG
\\
  This work successfully generates uncertainty aware surrogate models, via the
Bayesian neural network with noise contrastive prior (BNN-NCP) technique, of
the EuroPED plasma pedestal model using data from the JET-ILW pedestal database
and subsequent model evaluations. All this conform EuroPED-NN. The BNN-NCP
technique is proven to be a good fit for uncertainty aware surrogate models,
matching the output results as a regular neural network, providing prediction's
confidence as uncertainties, and highlighting the out of distribution (OOD)
regions using surrogate model uncertainties. This provides critical insights
into model robustness and reliability. EuroPED-NN has been physically
validated, first, analyzing electron density
$n_e\!\left(\psi_{\text{pol}}=0.94\right)$ with respect to increasing plasma
current, $I_p$, and second, validating the $\Delta-\beta_{p,ped}$ relation
associated with the EuroPED model. Affirming the robustness of the underlying
physics learned by the surrogate model.
\\ ( https://arxiv.org/abs/2402.00760 ,  3896kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00769 (*cross-listing*)
Date: Thu, 1 Feb 2024 16:58:11 GMT   (13269kb,D)

Title: AnimateLCM: Accelerating the Animation of Personalized Diffusion Models
  and Adapters with Decoupled Consistency Learning
Authors: Fu-Yun Wang, Zhaoyang Huang, Xiaoyu Shi, Weikang Bian, Guanglu Song,
  Yu Liu, Hongsheng Li
Categories: cs.CV cs.LG
Comments: Project Page: https://animatelcm.github.io/
\\
  Video diffusion models has been gaining increasing attention for its ability
to produce videos that are both coherent and of high fidelity. However, the
iterative denoising process makes it computationally intensive and
time-consuming, thus limiting its applications. Inspired by the Consistency
Model (CM) that distills pretrained image diffusion models to accelerate the
sampling with minimal steps and its successful extension Latent Consistency
Model (LCM) on conditional image generation, we propose AnimateLCM, allowing
for high-fidelity video generation within minimal steps. Instead of directly
conducting consistency learning on the raw video dataset, we propose a
decoupled consistency learning strategy that decouples the distillation of
image generation priors and motion generation priors, which improves the
training efficiency and enhance the generation visual quality. Additionally, to
enable the combination of plug-and-play adapters in stable diffusion community
to achieve various functions (e.g., ControlNet for controllable generation). we
propose an efficient strategy to adapt existing adapters to our distilled
text-conditioned video consistency model or train adapters from scratch without
harming the sampling speed. We validate the proposed strategy in
image-conditioned video generation and layout-conditioned video generation, all
achieving top-performing results. Experimental results validate the
effectiveness of our proposed method. Code and weights will be made public.
More details are available at https://github.com/G-U-N/AnimateLCM.
\\ ( https://arxiv.org/abs/2402.00769 ,  13269kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00774 (*cross-listing*)
Date: Thu, 1 Feb 2024 17:04:04 GMT   (2002kb,D)

Title: Mesh motion in fluid-structure interaction with deep operator networks
Authors: Ottar Hellan
Categories: math.NA cs.LG cs.NA
Comments: 9 pages, 5 figures, submitted to proceedings of ENUMATH 2023
\\
  A mesh motion model based on deep operator networks is presented. The model
is trained on and evaluated against a biharmonic mesh motion model on a
fluid-structure interaction benchmark problem and further evaluated in a
setting where biharmonic mesh motion fails. The performance of the proposed
mesh motion model is comparable to the biharmonic mesh motion on the test
problems.
\\ ( https://arxiv.org/abs/2402.00774 ,  2002kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00776 (*cross-listing*)
Date: Thu, 1 Feb 2024 17:05:37 GMT   (1114kb,D)

Title: Hybrid Quantum Vision Transformers for Event Classification in High
  Energy Physics
Authors: Eyup B. Unlu, Mar\c{c}al Comajoan Cara, Gopal Ramesh Dahale, Zhongtian
  Dong, Roy T. Forestano, Sergei Gleyzer, Daniel Justice, Kyoungchul Kong, Tom
  Magorsch, Konstantin T. Matchev, Katia Matcheva
Categories: quant-ph cs.LG hep-ph stat.ML
Comments: 12 pages, 8 figures
\\
  Models based on vision transformer architectures are considered
state-of-the-art when it comes to image classification tasks. However, they
require extensive computational resources both for training and deployment. The
problem is exacerbated as the amount and complexity of the data increases.
Quantum-based vision transformer models could potentially alleviate this issue
by reducing the training and operating time while maintaining the same
predictive power. Although current quantum computers are not yet able to
perform high-dimensional tasks yet, they do offer one of the most efficient
solutions for the future. In this work, we construct several variations of a
quantum hybrid vision transformer for a classification problem in high energy
physics (distinguishing photons and electrons in the electromagnetic
calorimeter). We test them against classical vision transformer architectures.
Our findings indicate that the hybrid models can achieve comparable performance
to their classical analogues with a similar number of parameters.
\\ ( https://arxiv.org/abs/2402.00776 ,  1114kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00787 (*cross-listing*)
Date: Thu, 1 Feb 2024 17:21:45 GMT   (614kb,D)

Title: Learning and Calibrating Heterogeneous Bounded Rational Market Behaviour
  with Multi-Agent Reinforcement Learning
Authors: Benjamin Patrick Evans, Sumitra Ganesh
Categories: cs.MA cs.CE cs.GT cs.LG econ.GN q-fin.EC
Comments: Accepted as a full paper at AAMAS 2024
\\
  Agent-based models (ABMs) have shown promise for modelling various real world
phenomena incompatible with traditional equilibrium analysis. However, a
critical concern is the manual definition of behavioural rules in ABMs. Recent
developments in multi-agent reinforcement learning (MARL) offer a way to
address this issue from an optimisation perspective, where agents strive to
maximise their utility, eliminating the need for manual rule specification.
This learning-focused approach aligns with established economic and financial
models through the use of rational utility-maximising agents. However, this
representation departs from the fundamental motivation for ABMs: that realistic
dynamics emerging from bounded rationality and agent heterogeneity can be
modelled. To resolve this apparent disparity between the two approaches, we
propose a novel technique for representing heterogeneous processing-constrained
agents within a MARL framework. The proposed approach treats agents as
constrained optimisers with varying degrees of strategic skills, permitting
departure from strict utility maximisation. Behaviour is learnt through
repeated simulations with policy gradients to adjust action likelihoods. To
allow efficient computation, we use parameterised shared policy learning with
distributions of agent skill levels. Shared policy learning avoids the need for
agents to learn individual policies yet still enables a spectrum of bounded
rational behaviours. We validate our model's effectiveness using real-world
data on a range of canonical $n$-agent settings, demonstrating significantly
improved predictive capability.
\\ ( https://arxiv.org/abs/2402.00787 ,  614kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00811 (*cross-listing*)
Date: Thu, 1 Feb 2024 17:46:19 GMT   (557kb,D)

Title: An Analysis of the Variance of Diffusion-based Speech Enhancement
Authors: Bunlong Lay, Timo Gerkmann
Categories: eess.AS cs.LG cs.SD
Comments: 5 pages, 3 figures, 1 table
\\
  Diffusion models proved to be powerful models for generative speech
enhancement. In recent SGMSE+ approaches, training involves a stochastic
differential equation for the diffusion process, adding both Gaussian and
environmental noise to the clean speech signal gradually. The speech
enhancement performance varies depending on the choice of the stochastic
differential equation that controls the evolution of the mean and the variance
along the diffusion processes when adding environmental and Gaussian noise. In
this work, we highlight that the scale of the variance is a dominant parameter
for speech enhancement performance and show that it controls the tradeoff
between noise attenuation and speech distortions. More concretely, we show that
a larger variance increases the noise attenuation and allows for reducing the
computational footprint, as fewer function evaluations for generating the
estimate are required.
\\ ( https://arxiv.org/abs/2402.00811 ,  557kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00825 (*cross-listing*)
Date: Thu, 1 Feb 2024 18:11:22 GMT   (2971kb,D)

Title: Resolution invariant deep operator network for PDEs with complex
  geometries
Authors: Jianguo Huang and Yue Qiu
Categories: math.NA cs.LG cs.NA
\\
  Neural operators (NO) are discretization invariant deep learning methods with
functional output and can approximate any continuous operator. NO have
demonstrated the superiority of solving partial differential equations (PDEs)
over other deep learning methods. However, the spatial domain of its input
function needs to be identical to its output, which limits its applicability.
For instance, the widely used Fourier neural operator (FNO) fails to
approximate the operator that maps the boundary condition to the PDE solution.
To address this issue, we propose a novel framework called resolution-invariant
deep operator (RDO) that decouples the spatial domain of the input and output.
RDO is motivated by the Deep operator network (DeepONet) and it does not
require retraining the network when the input/output is changed compared with
DeepONet. RDO takes functional input and its output is also functional so that
it keeps the resolution invariant property of NO. It can also resolve PDEs with
complex geometries whereas NO fail. Various numerical experiments demonstrate
the advantage of our method over DeepONet and FNO.
\\ ( https://arxiv.org/abs/2402.00825 ,  2971kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00865 (*cross-listing*)
Date: Thu, 1 Feb 2024 18:59:22 GMT   (229kb,D)

Title: Towards Optimal Feature-Shaping Methods for Out-of-Distribution
  Detection
Authors: Qinyu Zhao, Ming Xu, Kartik Gupta, Akshay Asthana, Liang Zheng,
  Stephen Gould
Categories: cs.CV cs.LG
Comments: ICLR 2024. Project page: https://github.com/Qinyu-Allen-Zhao/OptFSOOD
\\
  Feature shaping refers to a family of methods that exhibit state-of-the-art
performance for out-of-distribution (OOD) detection. These approaches
manipulate the feature representation, typically from the penultimate layer of
a pre-trained deep learning model, so as to better differentiate between
in-distribution (ID) and OOD samples. However, existing feature-shaping methods
usually employ rules manually designed for specific model architectures and OOD
datasets, which consequently limit their generalization ability. To address
this gap, we first formulate an abstract optimization framework for studying
feature-shaping methods. We then propose a concrete reduction of the framework
with a simple piecewise constant shaping function and show that existing
feature-shaping methods approximate the optimal solution to the concrete
optimization problem. Further, assuming that OOD data is inaccessible, we
propose a formulation that yields a closed-form solution for the piecewise
constant shaping function, utilizing solely the ID data. Through extensive
experiments, we show that the feature-shaping function optimized by our method
improves the generalization ability of OOD detection across a large variety of
datasets and model architectures.
\\ ( https://arxiv.org/abs/2402.00865 ,  229kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2306.16958
replaced with revised version Thu, 1 Feb 2024 16:38:41 GMT   (47kb)

Title: Identifiability of Direct Effects from Summary Causal Graphs
Authors: Simon Ferreira and Charles K. Assaad
Categories: cs.AI
\\ ( https://arxiv.org/abs/2306.16958 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07515
replaced with revised version Thu, 1 Feb 2024 11:28:23 GMT   (317kb)

Title: Artificial intelligence is algorithmic mimicry: why artificial "agents"
  are not (and won't be) proper agents
Authors: Johannes Jaeger
Categories: cs.AI
\\ ( https://arxiv.org/abs/2307.07515 ,  317kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10974
replaced with revised version Wed, 31 Jan 2024 20:15:38 GMT   (381kb,D)

Title: "Guinea Pig Trials" Utilizing GPT: A Novel Smart Agent-Based Modeling
  Approach for Studying Firm Competition and Collusion
Authors: Xu Han, Zengqing Wu, Chuan Xiao
Categories: cs.AI cs.CE cs.MA econ.GN q-fin.EC
Comments: Source code is available at: https://github.com/Roihn/SABM
ACM-class: I.2.1; I.2.7; I.6.5; J.4
\\ ( https://arxiv.org/abs/2308.10974 ,  381kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00300
replaced with revised version Thu, 1 Feb 2024 14:34:31 GMT   (1732kb,D)

Title: Towards the Identifiability and Explainability for Personalized Learner
  Modeling: An Inductive Paradigm
Authors: Jiatong Li, Qi Liu, Fei Wang, Jiayu Liu, Zhenya Huang, Fangzhou Yao,
  Linbo Zhu, Yu Su
Categories: cs.AI
Comments: Accepted by Proceedings of the ACM Web Conference 2024 (WWW '24)
\\ ( https://arxiv.org/abs/2309.00300 ,  1732kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11798
replaced with revised version Wed, 31 Jan 2024 19:08:28 GMT   (119kb,D)

Title: Auction-Based Scheduling
Authors: Guy Avni, Kaushik Mallik, Suman Sadhukhan
Categories: cs.AI cs.FL cs.GT
Comments: Full version of a paper accepted at TACAS'24
\\ ( https://arxiv.org/abs/2310.11798 ,  119kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20008
replaced with revised version Thu, 1 Feb 2024 15:55:02 GMT   (1016kb,D)

Title: Evolutionary Tabletop Game Design: A Case Study in the Risk Game
Authors: Lana Bertoldo Rossato, Leonardo Boaventura Bombardelli, and Anderson
  Rocha Tavares
Categories: cs.AI
Comments: Published in the 22nd Brazilian Symposium on Games and Digital
  Entertainment (SBGames 2023)
Journal-ref: 22nd Brazilian Symposium on Computer Games and Digital
  Entertainment (SBGames 2023)
DOI: 10.1145/3631085.3631236
\\ ( https://arxiv.org/abs/2310.20008 ,  1016kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11482
replaced with revised version Thu, 1 Feb 2024 04:12:52 GMT   (700kb,D)

Title: Meta Prompting for AGI Systems
Authors: Yifan Zhang, Yang Yuan, Andrew Chi-Chih Yao
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2311.11482 ,  700kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07324
replaced with revised version Thu, 1 Feb 2024 04:34:07 GMT   (3495kb,D)

Title: Small LLMs Are Weak Tool Learners: A Multi-LLM Agent
Authors: Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan,
  Hehong Chen, Ji Zhang, Fei Huang
Categories: cs.AI cs.CL
Comments: On progress, github repo: https://github.com/X-PLUG/Multi-LLM-Agent
\\ ( https://arxiv.org/abs/2401.07324 ,  3495kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07359
replaced with revised version Wed, 31 Jan 2024 21:46:10 GMT   (39kb)

Title: Reliability and Interpretability in Science and Deep Learning
Authors: Luigi Scorzato
Categories: cs.AI cs.LG physics.hist-ph
Comments: Minor clarifications in Sec. 4.1. Misleading table removed
\\ ( https://arxiv.org/abs/2401.07359 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12557
replaced with revised version Thu, 1 Feb 2024 03:22:22 GMT   (7kb)

Title: Balancing the AI Strength of Roles in Self-Play Training with Regret
  Matching+
Authors: Xiaoxi Wang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2401.12557 ,  7kb)
------------------------------------------------------------------------------
\\
arXiv:2305.01710
replaced with revised version Thu, 1 Feb 2024 17:57:37 GMT   (663kb,D)

Title: Stars Are All You Need: A Distantly Supervised Pyramid Network for
  Unified Sentiment Analysis
Authors: Wenchang Li, Yixing Chen, Shuang Zheng, Lei Wang, John P. Lalor
Categories: cs.CL
Comments: 15 pages, 3 figures, 5 tables
Journal-ref: W-NUT 2024: The 9th Workshop on Noisy and User-generated Text (at
  EACL)
\\ ( https://arxiv.org/abs/2305.01710 ,  663kb)
------------------------------------------------------------------------------
\\
arXiv:2305.07303
replaced with revised version Thu, 1 Feb 2024 16:00:03 GMT   (408kb,D)

Title: Multi-Relational Hyperbolic Word Embeddings from Natural Language
  Definitions
Authors: Marco Valentino, Danilo S. Carvalho, Andr\'e Freitas
Categories: cs.CL cs.LG
Comments: Accepted at the 18th Conference of the European Chapter of the
  Association for Computational Linguistics (EACL 2024)
\\ ( https://arxiv.org/abs/2305.07303 ,  408kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13514
replaced with revised version Thu, 1 Feb 2024 11:47:57 GMT   (283kb,D)

Title: Small Language Models Improve Giants by Rewriting Their Outputs
Authors: Giorgos Vernikos, Arthur Bra\v{z}inskas, Jakub Adamek, Jonathan
  Mallinson, Aliaksei Severyn, Eric Malmi
Categories: cs.CL cs.LG
Comments: Accepted at EACL 2024
\\ ( https://arxiv.org/abs/2305.13514 ,  283kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14163
replaced with revised version Thu, 1 Feb 2024 17:34:22 GMT   (8456kb,D)

Title: Leveraging Open Information Extraction for More Robust Domain Transfer
  of Event Trigger Detection
Authors: David Duki\'c, Kiril Gashteovski, Goran Glava\v{s}, Jan \v{S}najder
Categories: cs.CL cs.LG
Comments: Accepted at EACL 2024 Findings
\\ ( https://arxiv.org/abs/2305.14163 ,  8456kb)
------------------------------------------------------------------------------
\\
arXiv:2307.14117
replaced with revised version Thu, 1 Feb 2024 04:30:38 GMT   (6565kb,D)

Title: Leveraging Implicit Feedback from Deployment Data in Dialogue
Authors: Richard Yuanzhe Pang, Stephen Roller, Kyunghyun Cho, He He, Jason
  Weston
Categories: cs.CL
Comments: EACL 2024
\\ ( https://arxiv.org/abs/2307.14117 ,  6565kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10263
replaced with revised version Thu, 1 Feb 2024 14:19:50 GMT   (5612kb,D)

Title: Scaling up Discovery of Latent Concepts in Deep NLP Models
Authors: Majd Hawasly, Fahim Dalvi and Nadir Durrani
Categories: cs.CL
Comments: 14 pages, accepted to The 18th Conference of the European Chapter of
  the Association for Computational Linguistics (EACL 2024)
\\ ( https://arxiv.org/abs/2308.10263 ,  5612kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12950
replaced with revised version Wed, 31 Jan 2024 19:47:26 GMT   (1988kb,D)

Title: Code Llama: Open Foundation Models for Code
Authors: Baptiste Rozi\`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai
  Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez,
  J\'er\'emy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish
  Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre
  D\'efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas
  Usunier, Thomas Scialom, Gabriel Synnaeve
Categories: cs.CL
\\ ( https://arxiv.org/abs/2308.12950 ,  1988kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00533
replaced with revised version Thu, 1 Feb 2024 06:10:00 GMT   (1020kb,D)

Title: SELF: Self-Evolution with Language Feedback
Authors: Jianqiao Lu, Wanjun Zhong, Wenyong Huang, Yufei Wang, Qi Zhu, Fei Mi,
  Baojun Wang, Weichao Wang, Xingshan Zeng, Lifeng Shang, Xin Jiang, Qun Liu
Categories: cs.CL cs.AI cs.LG
Comments: 20 pages, 4 figures, 11 tables
\\ ( https://arxiv.org/abs/2310.00533 ,  1020kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07713
replaced with revised version Wed, 31 Jan 2024 23:27:26 GMT   (2097kb,D)

Title: InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining
Authors: Boxin Wang, Wei Ping, Lawrence McAfee, Peng Xu, Bo Li, Mohammad
  Shoeybi, Bryan Catanzaro
Categories: cs.CL cs.AI cs.IR cs.LG
\\ ( https://arxiv.org/abs/2310.07713 ,  2097kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10688
replaced with revised version Wed, 31 Jan 2024 19:05:49 GMT   (1193kb,D)

Title: A decoder-only foundation model for time-series forecasting
Authors: Abhimanyu Das, Weihao Kong, Rajat Sen, Yichen Zhou
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2310.10688 ,  1193kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05085
replaced with revised version Wed, 31 Jan 2024 19:17:00 GMT   (12580kb,D)

Title: Characterizing Large Language Models as Rationalizers of
  Knowledge-intensive Tasks
Authors: Aditi Mishra and Sajjadur Rahman and Hannah Kim and Kushan Mitra and
  Estevam Hruschka
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2311.05085 ,  12580kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01472
replaced with revised version Thu, 1 Feb 2024 01:32:48 GMT   (280kb,D)

Title: A First Look at Information Highlighting in Stack Overflow Answers
Authors: Shahla Shaan Ahmed, Shaowei Wang, Yuan Tian, Tse-Hsun (Peter) Chen,
  Haoxiang Zhang
Categories: cs.CL cs.IR cs.LG cs.SE
Comments: This work is submitted to Information and Software Technology Journal
\\ ( https://arxiv.org/abs/2401.01472 ,  280kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06034
replaced with revised version Thu, 1 Feb 2024 13:10:15 GMT   (7180kb,D)

Title: LinguAlchemy: Fusing Typological and Geographical Elements for Unseen
  Language Generalization
Authors: Muhammad Farid Adilazuarda, Samuel Cahyawijaya, Alham Fikri Aji, Genta
  Indra Winata, Ayu Purwarianti
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.06034 ,  7180kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11864
replaced with revised version Thu, 1 Feb 2024 18:16:04 GMT   (1140kb,D)

Title: Distilling Mathematical Reasoning Capabilities into Small Language
  Models
Authors: Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.11864 ,  1140kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14113
replaced with revised version Thu, 1 Feb 2024 03:47:28 GMT   (301kb,D)

Title: On the Affinity, Rationality, and Diversity of Hierarchical Topic
  Modeling
Authors: Xiaobao Wu, Fengjun Pan, Thong Nguyen, Yichao Feng, Chaoqun Liu,
  Cong-Duy Nguyen, Anh Tuan Luu
Categories: cs.CL
Comments: Accepted to AAAI2024 conference. Our code is available at
  https://github.com/bobxwu/TraCo
\\ ( https://arxiv.org/abs/2401.14113 ,  301kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15316
replaced with revised version Thu, 1 Feb 2024 00:50:41 GMT   (246kb,D)

Title: UNSEE: Unsupervised Non-contrastive Sentence Embeddings
Authors: \"Omer Veysel \c{C}a\u{g}atan
Categories: cs.CL
Comments: Accepted to EACL 2024
\\ ( https://arxiv.org/abs/2401.15316 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16736
replaced with revised version Thu, 1 Feb 2024 18:24:09 GMT   (110kb,D)

Title: Engineering A Large Language Model From Scratch
Authors: Abiodun Finbarrs Oketunji
Categories: cs.CL cs.CY cs.LG cs.SE
MSC-class: I.2.7
ACM-class: I.2.7
DOI: 10.13140/RG.2.2.28532.73600/1; 10.5281/zenodo.10607236
\\ ( https://arxiv.org/abs/2401.16736 ,  110kb)
------------------------------------------------------------------------------
\\
arXiv:2201.05149
replaced with revised version Thu, 1 Feb 2024 07:38:07 GMT   (895kb,D)

Title: The curse of overparametrization in adversarial training: Precise
  analysis of robust generalization for random features regression
Authors: Hamed Hassani and Adel Javanmard
Categories: cs.LG math.ST stat.ML stat.TH
Comments: 86 pages (main file: 25 pages and supplementary: 61 pages). To appear
  in the Annals of Statistics
\\ ( https://arxiv.org/abs/2201.05149 ,  895kb)
------------------------------------------------------------------------------
\\
arXiv:2206.14397
replaced with revised version Thu, 1 Feb 2024 05:03:56 GMT   (2264kb,D)

Title: Fair Machine Learning in Healthcare: A Review
Authors: Qizhang Feng, Mengnan Du, Na Zou, Xia Hu
Categories: cs.LG cs.AI cs.CY
\\ ( https://arxiv.org/abs/2206.14397 ,  2264kb)
------------------------------------------------------------------------------
\\
arXiv:2207.07624
replaced with revised version Wed, 31 Jan 2024 19:19:45 GMT   (2090kb,D)

Title: Feed-Forward Latent Domain Adaptation
Authors: Ondrej Bohdal, Da Li, Shell Xu Hu, Timothy Hospedales
Categories: cs.LG stat.ML
Comments: Accepted at WACV 2024. Project page:
  https://ondrejbohdal.github.io/cxda
\\ ( https://arxiv.org/abs/2207.07624 ,  2090kb)
------------------------------------------------------------------------------
\\
arXiv:2211.04125
replaced with revised version Thu, 1 Feb 2024 08:37:00 GMT   (657kb)

Title: Efficacy of MRI data harmonization in the age of machine learning. A
  multicenter study across 36 datasets
Authors: Chiara Marzi, Marco Giannelli, Andrea Barucci, Carlo Tessa, Mario
  Mascalchi, Stefano Diciotti
Categories: cs.LG eess.IV q-bio.QM
ACM-class: J.3
Journal-ref: Sci Data 11, 115 (2024)
DOI: 10.1038/s41597-023-02421-7
\\ ( https://arxiv.org/abs/2211.04125 ,  657kb)
------------------------------------------------------------------------------
\\
arXiv:2303.06021
replaced with revised version Thu, 1 Feb 2024 16:45:42 GMT   (1265kb,D)

Title: Machine learning for sports betting: should model selection be based on
  accuracy or calibration?
Authors: Conor Walsh, Alok Joshi
Categories: cs.LG
Comments: 15 pages, 5 Figures. Paper submitted to Elsevier's Machine Learning
  with Applications
\\ ( https://arxiv.org/abs/2303.06021 ,  1265kb)
------------------------------------------------------------------------------
\\
arXiv:2304.00759
replaced with revised version Thu, 1 Feb 2024 10:40:09 GMT   (742kb,D)

Title: FedIN: Federated Intermediate Layers Learning for Model Heterogeneity
Authors: Yun-Hin Chan, Zhihan Jiang, Jing Deng, Edith C.-H. Ngai
Categories: cs.LG
\\ ( https://arxiv.org/abs/2304.00759 ,  742kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17665
replaced with revised version Thu, 1 Feb 2024 12:25:16 GMT   (3086kb,D)

Title: Acceleration of stochastic gradient descent with momentum by averaging:
  finite-sample rates and asymptotic normality
Authors: Kejie Tang, Weidong Liu, Yichen Zhang and Xi Chen
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2305.17665 ,  3086kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18460
replaced with revised version Thu, 1 Feb 2024 03:36:36 GMT   (227kb,D)

Title: Minimum Width of Leaky-ReLU Neural Networks for Uniform Universal
  Approximation
Authors: Li'ang Li, Yifei Duan, Guanghua Ji, Yongqiang Cai
Categories: cs.LG cs.NA math.NA
Comments: Include errata of the previous versions
MSC-class: 68T07, 65P99, 65Z05, 41A65
\\ ( https://arxiv.org/abs/2305.18460 ,  227kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01697
replaced with revised version Wed, 31 Jan 2024 19:45:38 GMT   (11709kb,D)

Title: MutateNN: Mutation Testing of Image Recognition Models Deployed on
  Hardware Accelerators
Authors: Nikolaos Louloudakis, Perry Gibson, Jos\'e Cano, and Ajitha Rajan
Categories: cs.LG cs.SE cs.SY eess.SY
Comments: 7 pages, 7 figures
\\ ( https://arxiv.org/abs/2306.01697 ,  11709kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04072
replaced with revised version Thu, 1 Feb 2024 02:31:33 GMT   (9437kb,D)

Title: Exploring Simple, High Quality Out-of-Distribution Detection with L2
  Normalization
Authors: Jarrod Haas, William Yolland, Bernhard Rabus
Categories: cs.LG
\\ ( https://arxiv.org/abs/2306.04072 ,  9437kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05587
replaced with revised version Wed, 31 Jan 2024 22:32:28 GMT   (338kb,D)

Title: MC-NN: An End-to-End Multi-Channel Neural Network Approach for
  Predicting Influenza A Virus Hosts and Antigenic Types
Authors: Yanhua Xu and Dominik Wojtczak
Categories: cs.LG q-bio.QM
Comments: Accepted version submitted to the SN Computer Science; Published in
  the SN Computer Science 2023; V1: minor updates were made to the Results
  section; V2: minor updates regarding data description
DOI: 10.1007/s42979-023-01839-5
\\ ( https://arxiv.org/abs/2306.05587 ,  338kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08670
replaced with revised version Thu, 1 Feb 2024 18:54:21 GMT   (581kb,D)

Title: The Power of Populations in Decentralized Bandits
Authors: John Lazarsfeld, Dan Alistarh
Categories: cs.LG cs.DC cs.DS
\\ ( https://arxiv.org/abs/2306.08670 ,  581kb)
------------------------------------------------------------------------------
\\
arXiv:2307.04308
replaced with revised version Thu, 1 Feb 2024 14:54:00 GMT   (591kb,D)

Title: Towards Cross-Table Masked Pretraining for Web Data Mining
Authors: Chao Ye, Guoshan Lu, Haobo Wang, Liyao Li, Sai Wu, Gang Chen, Junbo
  Zhao
Categories: cs.LG
Comments: Accepted to WWW 2024
\\ ( https://arxiv.org/abs/2307.04308 ,  591kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13831
replaced with revised version Thu, 1 Feb 2024 14:14:56 GMT   (2587kb,D)

Title: Relationship between Batch Size and Number of Steps Needed for Nonconvex
  Optimization of Stochastic Gradient Descent using Armijo Line Search
Authors: Yuki Tsukada, Hideaki Iiduka
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2307.13831 ,  2587kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13903
replaced with revised version Thu, 1 Feb 2024 08:11:55 GMT   (79kb)

Title: Corruption-Robust Lipschitz Contextual Search
Authors: Shiliang Zuo
Categories: cs.LG stat.ML
Comments: Accepted at ALT 2024
\\ ( https://arxiv.org/abs/2307.13903 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02594
replaced with revised version Wed, 31 Jan 2024 23:22:39 GMT   (8190kb,D)

Title: SMARLA: A Safety Monitoring Approach for Deep Reinforcement Learning
  Agents
Authors: Amirhossein Zolfagharian, Manel Abdellatif, Lionel C. Briand, and
  Ramesh S
Categories: cs.LG cs.AI cs.SE
\\ ( https://arxiv.org/abs/2308.02594 ,  8190kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05061
replaced with revised version Thu, 1 Feb 2024 07:49:50 GMT   (3310kb,D)

Title: Fine-Tune Language Models as Multi-Modal Differential Equation Solvers
Authors: Liu Yang, Siting Liu, Stanley J. Osher
Categories: cs.LG cs.NA math.NA stat.ML
\\ ( https://arxiv.org/abs/2308.05061 ,  3310kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12264
replaced with revised version Thu, 1 Feb 2024 17:35:09 GMT   (2428kb,D)

Title: Enhancing Energy-Awareness in Deep Learning through Fine-Grained Energy
  Measurement
Authors: Saurabhsingh Rajput, Tim Widmayer, Ziyuan Shang, Maria Kechagia,
  Federica Sarro, Tushar Sharma
Categories: cs.LG cs.AI cs.PF cs.SE
\\ ( https://arxiv.org/abs/2308.12264 ,  2428kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05915
replaced with revised version Thu, 1 Feb 2024 13:11:56 GMT   (751kb,D)

Title: ACT: Empowering Decision Transformer with Dynamic Programming via
  Advantage Conditioning
Authors: Chen-Xiao Gao, Chenyang Wu, Mingjun Cao, Rui Kong, Zongzhang Zhang,
  Yang Yu
Categories: cs.LG cs.AI
Comments: Accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2309.05915 ,  751kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00113
replaced with revised version Thu, 1 Feb 2024 18:01:02 GMT   (6106kb,D)

Title: HyperMask: Adaptive Hypernetwork-based Masks for Continual Learning
Authors: Kamil Ksi\k{a}\.zek, Przemys{\l}aw Spurek
Categories: cs.LG cs.AI
MSC-class: 68T07
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2310.00113 ,  6106kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00692
replaced with revised version Thu, 1 Feb 2024 11:15:37 GMT   (8689kb,D)

Title: A Theoretical Analysis of Noise Geometry in Stochastic Gradient Descent
Authors: Mingze Wang, Lei Wu
Categories: cs.LG stat.ML
Comments: 30 pages
\\ ( https://arxiv.org/abs/2310.00692 ,  8689kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09971
replaced with revised version Thu, 1 Feb 2024 00:42:31 GMT   (9820kb,D)

Title: AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents
Authors: Jake Grigsby, Linxi Fan, Yuke Zhu
Categories: cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.09971 ,  9820kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10899
replaced with revised version Thu, 1 Feb 2024 00:05:51 GMT   (2447kb,D)

Title: Instilling Inductive Biases with Subnetworks
Authors: Enyan Zhang, Michael A. Lepori, Ellie Pavlick
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.10899 ,  2447kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18001
replaced with revised version Thu, 1 Feb 2024 16:36:01 GMT   (450kb,D)

Title: DP-SGD with weight clipping
Authors: Antoine Barczewski and Jan Ramon
Categories: cs.LG cs.CR
\\ ( https://arxiv.org/abs/2310.18001 ,  450kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02546
replaced with revised version Thu, 1 Feb 2024 16:42:42 GMT   (61kb)

Title: On the Second-Order Convergence of Biased Policy Gradient Algorithms
Authors: Siqiao Mu and Diego Klabjan
Categories: cs.LG
\\ ( https://arxiv.org/abs/2311.02546 ,  61kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18826
replaced with revised version Thu, 1 Feb 2024 18:59:44 GMT   (2821kb,D)

Title: Geometry-Aware Normalizing Wasserstein Flows for Optimal Causal
  Inference
Authors: Kaiwen Hou
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2311.18826 ,  2821kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01057
replaced with revised version Thu, 1 Feb 2024 18:57:20 GMT   (1329kb,D)

Title: RLHF and IIA: Perverse Incentives
Authors: Wanqiao Xu, Shi Dong, Xiuyuan Lu, Grace Lam, Zheng Wen, Benjamin Van
  Roy
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2312.01057 ,  1329kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04916
replaced with revised version Thu, 1 Feb 2024 11:58:27 GMT   (1561kb,D)

Title: EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language
  Models with 3D Parallelism
Authors: Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, Jingren Zhou
Categories: cs.LG cs.AI cs.DC
Comments: arXiv v2 update: extended related works and formal analysis of
  training efficiency. We will continuously update the codebase and arXiv
  version
\\ ( https://arxiv.org/abs/2312.04916 ,  1561kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12839
replaced with revised version Wed, 31 Jan 2024 19:28:57 GMT   (402kb,D)

Title: Comparing Machine Learning Algorithms by Union-Free Generic Depth
Authors: Hannah Blocher, Georg Schollmeyer, Malte Nalenz, Christoph Jansen
Categories: cs.LG stat.ML
Comments: arXiv admin note: substantial text overlap with arXiv:2304.09872
\\ ( https://arxiv.org/abs/2312.12839 ,  402kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03140
replaced with revised version Thu, 1 Feb 2024 02:08:55 GMT   (49046kb,D)

Title: Fair Sampling in Diffusion Models through Switching Mechanism
Authors: Yujin Choi, Jinseong Park, Hoki Kim, Jaewook Lee, Saeroom Park
Categories: cs.LG cs.CY
Comments: AAAI 2024
\\ ( https://arxiv.org/abs/2401.03140 ,  49046kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05578
replaced with revised version Thu, 1 Feb 2024 16:42:05 GMT   (0kb,I)

Title: Fast Cerebral Blood Flow Analysis via Extreme Learning Machine
Authors: Xi Chen, Zhenya Zang, Xingda Li
Categories: cs.LG
Comments: Not ready to submission. Need further correction
\\ ( https://arxiv.org/abs/2401.05578 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05580
replaced with revised version Thu, 1 Feb 2024 16:36:43 GMT   (0kb,I)

Title: Enhancing Blood Flow Assessment in Diffuse Correlation Spectroscopy: A
  Transfer Learning Approach with Noise Robustness Analysis
Authors: Xi Chen, Xingda Li
Categories: cs.LG eess.SP
Comments: Not ready for submission. Need further changes
\\ ( https://arxiv.org/abs/2401.05580 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05821
replaced with revised version Thu, 1 Feb 2024 13:36:04 GMT   (3990kb,D)

Title: Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents
Authors: Quentin Delfosse, Sebastian Sztwiertnia, Mark Rothermel, Wolfgang
  Stammer, Kristian Kersting
Categories: cs.LG cs.SC
Comments: 20 pages, 8 of main text, 8 of appendix, 3 main figures
\\ ( https://arxiv.org/abs/2401.05821 ,  3990kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08426
replaced with revised version Thu, 1 Feb 2024 01:17:45 GMT   (1581kb,D)

Title: GD doesn't make the cut: Three ways that non-differentiability affects
  neural network training
Authors: Siddharth Krishna Kumar
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2401.08426 ,  1581kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10371
replaced with revised version Thu, 1 Feb 2024 02:20:36 GMT   (737kb,D)

Title: Langevin Unlearning: A New Perspective of Noisy Gradient Descent for
  Machine Unlearning
Authors: Eli Chien, Haoyu Wang, Ziang Chen, Pan Li
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.10371 ,  737kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11261
replaced with revised version Thu, 1 Feb 2024 10:44:08 GMT   (10588kb,D)

Title: Diffusion Model Conditioning on Gaussian Mixture Model and Negative
  Gaussian Mixture Gradient
Authors: Weiguo Lu, Xuan Wu, Deng Ding, Jinqiao Duan, Jirong Zhuang, Gangnan
  Yuan
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2401.11261 ,  10588kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13366
replaced with revised version Thu, 1 Feb 2024 18:26:39 GMT   (2654kb,D)

Title: Mitigating System Bias in Resource Constrained Asynchronous Federated
  Learning Systems
Authors: Jikun Gao, Ioannis Mavromatis, Peizheng Li, Pietro Carnelli, Aftab
  Khan
Categories: cs.LG
Comments: 6 pages, 5 figures. This work has been accepted by PerCom PerconAI
  workshop 2024
\\ ( https://arxiv.org/abs/2401.13366 ,  2654kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13744
replaced with revised version Thu, 1 Feb 2024 18:14:20 GMT   (2244kb,D)

Title: Conformal Prediction Sets Improve Human Decision Making
Authors: Jesse C. Cresswell, Yi Sui, Bhargava Kumar, No\"el Vouitsis
Categories: cs.LG cs.HC stat.ML
Comments: Code available at
  https://github.com/layer6ai-labs/hitl-conformal-prediction
\\ ( https://arxiv.org/abs/2401.13744 ,  2244kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15098
replaced with revised version Thu, 1 Feb 2024 11:58:07 GMT   (620kb,D)

Title: Hierarchical Continual Reinforcement Learning via Large Language Model
Authors: Chaofan Pan, Xin Yang, Hao Wang, Wei Wei, Tianrui Li
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.15098 ,  620kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15122
replaced with revised version Thu, 1 Feb 2024 07:34:53 GMT   (11872kb,D)

Title: A Multi-Grained Symmetric Differential Equation Model for Learning
  Protein-Ligand Binding Dynamics
Authors: Shengchao Liu, Weitao Du, Yanjing Li, Zhuoxinran Li, Vignesh
  Bhethanabotla, Nakul Rampal, Omar Yaghi, Christian Borgs, Anima Anandkumar,
  Hongyu Guo, Jennifer Chayes
Categories: cs.LG cs.AI q-bio.BM q-bio.QM stat.ML
\\ ( https://arxiv.org/abs/2401.15122 ,  11872kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16661
replaced with revised version Thu, 1 Feb 2024 02:08:47 GMT   (51kb)

Title: Generalization of LiNGAM that allows confounding
Authors: Joe Suzuki and Tian-Le Yang
Categories: cs.LG cs.IT math.IT math.ST stat.TH
\\ ( https://arxiv.org/abs/2401.16661 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2209.02628
replaced with revised version Thu, 1 Feb 2024 12:00:48 GMT   (3919kb,D)

Title: Weak Collocation Regression method: fast reveal hidden stochastic
  dynamics from high-dimensional aggregate data
Authors: Liwei Lu, Zhijun Zeng, Yan Jiang, Yi Zhu, and Pipi Hu
Categories: math.NA cs.AI cs.NA math.DS
DOI: 10.1016/j.jcp.2024.112799
\\ ( https://arxiv.org/abs/2209.02628 ,  3919kb)
------------------------------------------------------------------------------
\\
arXiv:2211.10819
replaced with revised version Wed, 31 Jan 2024 22:02:20 GMT   (2163kb)

Title: Block size estimation for data partitioning in HPC applications using
  machine learning techniques
Authors: Riccardo Cantini, Fabrizio Marozzo, Alessio Orsino, Domenico Talia,
  Paolo Trunfio, Rosa M. Badia, Jorge Ejarque, Fernando Vazquez
Categories: cs.DC cs.AI
Journal-ref: Journal of Big Data, vol. 11, n. 19, 2024
DOI: 10.1186/s40537-023-00862-w
\\ ( https://arxiv.org/abs/2211.10819 ,  2163kb)
------------------------------------------------------------------------------
\\
arXiv:2212.08414
replaced with revised version Thu, 1 Feb 2024 07:55:03 GMT   (2115kb,D)

Title: Deep Learning Methods for Calibrated Photometric Stereo and Beyond
Authors: Yakun Ju, Kin-Man Lam, Wuyuan Xie, Huiyu Zhou, Junyu Dong, Boxin Shi
Categories: cs.CV cs.AI
Comments: 19 pages, 11 figures, 4 tables
\\ ( https://arxiv.org/abs/2212.08414 ,  2115kb)
------------------------------------------------------------------------------
\\
arXiv:2302.13144 (*cross-listing*)
replaced with revised version Wed, 31 Jan 2024 20:58:43 GMT   (805kb,D)

Title: Revisiting LQR Control from the Perspective of Receding-Horizon Policy
  Gradient
Authors: Xiangyuan Zhang, Tamer Ba\c{s}ar
Categories: math.OC cs.AI cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2302.13144 ,  805kb)
------------------------------------------------------------------------------
\\
arXiv:2303.11098
replaced with revised version Thu, 1 Feb 2024 11:51:01 GMT   (2659kb,D)

Title: Understanding the Role of the Projector in Knowledge Distillation
Authors: Roy Miles and Krystian Mikolajczyk
Categories: cs.CV cs.AI
Comments: AAAI 2024. Code available at
  https://github.com/roymiles/Simple-Recipe-Distillation
\\ ( https://arxiv.org/abs/2303.11098 ,  2659kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01879
replaced with revised version Thu, 1 Feb 2024 18:22:25 GMT   (10206kb,D)

Title: Revisiting the Role of Language Priors in Vision-Language Models
Authors: Zhiqiu Lin, Xinyue Chen, Deepak Pathak, Pengchuan Zhang, Deva Ramanan
Categories: cs.CV cs.AI cs.CL
Comments: Website: https://linzhiqiu.github.io/papers/visual_gpt_score/
\\ ( https://arxiv.org/abs/2306.01879 ,  10206kb)
------------------------------------------------------------------------------
\\
arXiv:2306.15880
replaced with revised version Thu, 1 Feb 2024 08:31:59 GMT   (572kb,D)

Title: Towards Open Vocabulary Learning: A Survey
Authors: Jianzong Wu, Xiangtai Li, Shilin Xu, Haobo Yuan, Henghui Ding, Yibo
  Yang, Xia Li, Jiangning Zhang, Yunhai Tong, Xudong Jiang, Bernard Ghanem,
  Dacheng Tao
Categories: cs.CV cs.AI
Comments: Accepted by IEEE T-PAMI. Project page:
  https://github.com/jianzongwu/Awesome-Open-Vocabulary
\\ ( https://arxiv.org/abs/2306.15880 ,  572kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13063
replaced with revised version Thu, 1 Feb 2024 04:57:05 GMT   (369kb,D)

Title: Using Large Language Models to Generate, Validate, and Apply User Intent
  Taxonomies
Authors: Chirag Shah, Ryen W. White, Reid Andersen, Georg Buscher, Scott
  Counts, Sarkar Snigdha Sarathi Das, Ali Montazer, Sathish Manivannan,
  Jennifer Neville, Xiaochuan Ni, Nagu Rangan, Tara Safavi, Siddharth Suri,
  Mengting Wan, Leijie Wang, Longqi Yang
Categories: cs.IR cs.AI cs.CL
Report-no: MSR-TR-2023-32
\\ ( https://arxiv.org/abs/2309.13063 ,  369kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04915 (*cross-listing*)
replaced with revised version Thu, 1 Feb 2024 07:03:32 GMT   (0kb,I)

Title: On Accelerating Diffusion-based Molecular Conformation Generation in
  SE(3)-invariant Space
Authors: Zihan Zhou, Ruiying Liu and Tianshu Yu
Categories: physics.comp-ph cs.AI cs.LG
Comments: We are currently developing a new manuscript that significantly
  expands upon and integrates the research presented here. The forthcoming
  paper includes broader analyses and more comprehensive findings, rendering
  the current version obsolete. We believe this decision will contribute to a
  clearer and more consolidated presentation of our research findings
\\ ( https://arxiv.org/abs/2310.04915 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05866 (*cross-listing*)
replaced with revised version Thu, 1 Feb 2024 16:52:14 GMT   (4454kb,D)

Title: Generative quantum machine learning via denoising diffusion
  probabilistic models
Authors: Bingzhi Zhang, Peng Xu, Xiaohui Chen and Quntao Zhuang
Categories: quant-ph cs.AI cs.LG
Comments: 5+10 pages, 16 figures. PRL accepted version. Code available at:
  https://github.com/francis-hsu/quantgenmdl
\\ ( https://arxiv.org/abs/2310.05866 ,  4454kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03076
replaced with revised version Thu, 1 Feb 2024 18:47:18 GMT   (29401kb,D)

Title: SugarViT -- Multi-objective Regression of UAV Images with Vision
  Transformers and Deep Label Distribution Learning Demonstrated on Disease
  Severity Prediction in Sugar Beet
Authors: Maurice G\"under, Facundo Ram\'on Ispizua Yamati, Abel Andree Barreto
  Alc\'antara, Anne-Katrin Mahlein, Rafet Sifa, Christian Bauckhage
Categories: cs.CV cs.AI
Comments: submitted to Computers and Electronics in Agriculture
\\ ( https://arxiv.org/abs/2311.03076 ,  29401kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09680
replaced with revised version Thu, 1 Feb 2024 05:15:52 GMT   (743kb,D)

Title: Trustworthy Large Models in Vision: A Survey
Authors: Ziyan Guo and Li Xu and Jun Liu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2311.09680 ,  743kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10776
replaced with revised version Thu, 1 Feb 2024 04:19:41 GMT   (2652kb,D)

Title: Chemist-X: Large Language Model-empowered Agent for Reaction Condition
  Recommendation in Chemical Synthesis
Authors: Kexin Chen, Junyou Li, Kunyi Wang, Yuyang Du, Jiahui Yu, Jiamin Lu,
  Lanqing Li, Jiezhong Qiu, Jianzhang Pan, Yi Huang, Qun Fang, Pheng Ann Heng,
  Guangyong Chen
Categories: cs.IR cs.AI
\\ ( https://arxiv.org/abs/2311.10776 ,  2652kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16136
replaced with revised version Thu, 1 Feb 2024 02:25:13 GMT   (6514kb,D)

Title: ERASER: Machine Unlearning in MLaaS via an Inference Serving-Aware
  Approach
Authors: Yuke Hu, Jian Lou, Jiaqi Liu, Wangze Ni, Feng Lin, Zhan Qin, Kui Ren
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2311.16136 ,  6514kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06643
replaced with revised version Thu, 1 Feb 2024 18:05:36 GMT   (5528kb,D)

Title: Gaze Detection and Analysis for Initiating Joint Activity in Industrial
  Human-Robot Collaboration
Authors: Pooja Prajod, Matteo Lavit Nicora, Marta Mondellini, Giovanni Tauro,
  Rocco Vertechy, Matteo Malosio, Elisabeth Andr\'e
Categories: cs.RO cs.AI cs.CV cs.HC
Comments: First draft for a paper submitted to Frontiers in Robotics and AI
\\ ( https://arxiv.org/abs/2312.06643 ,  5528kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14232
replaced with revised version Thu, 1 Feb 2024 13:06:51 GMT   (18919kb,D)

Title: Parrot Captions Teach CLIP to Spot Text
Authors: Yiqi Lin, Conghui He, Alex Jinpeng Wang, Bin Wang, Weijia Li, Mike
  Zheng Shou
Categories: cs.CV cs.AI
Comments: project page: https://linyq17.github.io/CLIP-Parrot-Bias/. Add more
  analysis and ablation studies. Update Figure 3 with a more precise metric
\\ ( https://arxiv.org/abs/2312.14232 ,  18919kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16815 (*cross-listing*)
replaced with revised version Thu, 1 Feb 2024 17:20:51 GMT   (5711kb,D)

Title: Emergence and Causality in Complex Systems: A Survey on Causal Emergence
  and Related Quantitative Studies
Authors: Bing Yuan, Zhang Jiang, Aobo Lyu, Jiayun Wu, Zhipeng Wang, Mingzhe
  Yang, Kaiwei Liu, Muyun Mou, Peng Cui
Categories: physics.soc-ph cs.AI nlin.AO
Comments: 57 pages, 17 figures, 1 table
MSC-class: 68P30
ACM-class: K.3.2
\\ ( https://arxiv.org/abs/2312.16815 ,  5711kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17429
replaced with revised version Thu, 1 Feb 2024 01:39:39 GMT   (4994kb,D)

Title: Commonsense for Zero-Shot Natural Language Video Localization
Authors: Meghana Holla, Ismini Lourentzou
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: Accepted to AAAI 2024
\\ ( https://arxiv.org/abs/2312.17429 ,  4994kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09769
replaced with revised version Thu, 1 Feb 2024 12:12:21 GMT   (339kb,D)

Title: Learning from Graphs with Heterophily: Progress and Future
Authors: Chenghua Gong, Yao Cheng, Xiang Li, Caihua Shan, Siqiang Luo
Categories: cs.SI cs.AI cs.LG
Comments: 9 pages
\\ ( https://arxiv.org/abs/2401.09769 ,  339kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12258
replaced with revised version Thu, 1 Feb 2024 16:50:23 GMT   (1401kb,D)

Title: Emergent Dominance Hierarchies in Reinforcement Learning Agents
Authors: Ram Rachum, Yonatan Nakar, Bill Tomlinson, Nitay Alon, Reuth Mirsky
Categories: cs.MA cs.AI cs.GT cs.LG
\\ ( https://arxiv.org/abs/2401.12258 ,  1401kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13346
replaced with revised version Thu, 1 Feb 2024 18:20:34 GMT   (1278kb,D)

Title: Past, Present, Future: A Comprehensive Exploration of AI Use Cases in
  the UMBRELLA IoT Testbed
Authors: Peizheng Li, Ioannis Mavromatis, Aftab Khan
Categories: cs.NI cs.AI
Comments: 6 pgaes, 4 figures. This work has been accepted by PerCom TrustSense
  workshop 2024
\\ ( https://arxiv.org/abs/2401.13346 ,  1278kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16013
replaced with revised version Thu, 1 Feb 2024 02:25:03 GMT   (15161kb,D)

Title: SERL: A Software Suite for Sample-Efficient Robotic Reinforcement
  Learning
Authors: Jianlan Luo, Zheyuan Hu, Charles Xu, You Liang Tan, Jacob Berg, Archit
  Sharma, Stefan Schaal, Chelsea Finn, Abhishek Gupta, Sergey Levine
Categories: cs.RO cs.AI
Comments: ICRA 2024
\\ ( https://arxiv.org/abs/2401.16013 ,  15161kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17434
replaced with revised version Thu, 1 Feb 2024 16:59:14 GMT   (265kb)

Title: Integrating Generative AI in Hackathons: Opportunities, Challenges, and
  Educational Implications
Authors: Ramteja Sajja, Carlos Erazo Ramirez, Zhouyayan Li, Bekir Z. Demiray,
  Yusuf Sermet and Ibrahim Demir
Categories: cs.CY cs.AI cs.HC
Comments: 8491 words, 23 pages, 12 figures
\\ ( https://arxiv.org/abs/2401.17434 ,  265kb)
------------------------------------------------------------------------------
\\
arXiv:2307.11865
replaced with revised version Thu, 1 Feb 2024 16:32:38 GMT   (28579kb,D)

Title: CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction
  Execution for Robots
Authors: Dmitriy Rivkin, Nikhil Kakodkar, Francois Hogan, Bobak H. Baghi,
  Gregory Dudek
Categories: cs.RO cs.CL
\\ ( https://arxiv.org/abs/2307.11865 ,  28579kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06320
replaced with revised version Thu, 1 Feb 2024 02:08:28 GMT   (1290kb,D)

Title: Zero-shot Generative Large Language Models for Systematic Review
  Screening Automation
Authors: Shuai Wang, Harrisen Scells, Shengyao Zhuang, Martin Potthast, Bevan
  Koopman, Guido Zuccon
Categories: cs.IR cs.CL
Comments: Accepted to ECIR2024 full paper (findings)
\\ ( https://arxiv.org/abs/2401.06320 ,  1290kb)
------------------------------------------------------------------------------
\\
arXiv:2107.08020 (*cross-listing*)
replaced with revised version Thu, 1 Feb 2024 16:05:28 GMT   (8239kb,D)

Title: Online Graph Topology Learning from Matrix-valued Time Series
Authors: Yiye Jiang, J\'er\'emie Bigot and Sofian Maabout
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2107.08020 ,  8239kb)
------------------------------------------------------------------------------
\\
arXiv:2110.08531 (*cross-listing*)
replaced with revised version Wed, 31 Jan 2024 20:52:39 GMT   (3389kb,D)

Title: A theoretical and empirical study of new adaptive algorithms with
  additional momentum steps and shifted updates for stochastic non-convex
  optimization
Authors: Cristian Daniel Alecsa
Categories: math.OC cs.LG
Comments: 42 pages, 6 figures, 7 tables, 43 references
\\ ( https://arxiv.org/abs/2110.08531 ,  3389kb)
------------------------------------------------------------------------------
\\
arXiv:2112.00365 (*cross-listing*)
replaced with revised version Thu, 1 Feb 2024 16:58:01 GMT   (30590kb,D)

Title: Probability-Generating Function Kernels for Spherical Data
Authors: Theodore Papamarkou, Alexey Lindo
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2112.00365 ,  30590kb)
------------------------------------------------------------------------------
\\
arXiv:2205.14461 (*cross-listing*)
replaced with revised version Wed, 31 Jan 2024 19:27:55 GMT   (22247kb,D)

Title: Collaborative likelihood-ratio estimation over graphs
Authors: Alejandro de la Concha and Nicolas Vayatis and Argyris Kalogeratos
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2205.14461 ,  22247kb)
------------------------------------------------------------------------------
\\
arXiv:2209.12644
replaced with revised version Thu, 1 Feb 2024 02:14:20 GMT   (4571kb,D)

Title: FORESEE: Prediction with Expansion-Compression Unscented Transform for
  Online Policy Optimization
Authors: Hardik Parwana and Dimitra Panagou
Categories: cs.RO cs.LG cs.SY eess.SY math.OC
\\ ( https://arxiv.org/abs/2209.12644 ,  4571kb)
------------------------------------------------------------------------------
\\
arXiv:2211.01345 (*cross-listing*)
replaced with revised version Thu, 1 Feb 2024 08:29:55 GMT   (352kb,D)

Title: Generative machine learning methods for multivariate ensemble
  post-processing
Authors: Jieyu Chen, Tim Janke, Florian Steinke, Sebastian Lerch
Categories: physics.ao-ph cs.LG stat.ME
Journal-ref: Annals of Applied Statistics (2024), 18, 159-183
DOI: 10.1214/23-AOAS1784
\\ ( https://arxiv.org/abs/2211.01345 ,  352kb)
------------------------------------------------------------------------------
\\
arXiv:2302.09624
replaced with revised version Thu, 1 Feb 2024 16:04:43 GMT   (518kb)

Title: Breaking the Communication-Privacy-Accuracy Tradeoff with
  $f$-Differential Privacy
Authors: Richeng Jin, Zhonggen Su, Caijun Zhong, Zhaoyang Zhang, Tony Quek,
  Huaiyu Dai
Categories: cs.CR cs.IT cs.LG math.IT
\\ ( https://arxiv.org/abs/2302.09624 ,  518kb)
------------------------------------------------------------------------------
\\
arXiv:2305.02930 (*cross-listing*)
replaced with revised version Thu, 1 Feb 2024 12:06:30 GMT   (1612kb,D)

Title: Piecewise Normalizing Flows
Authors: Harry Bevins, Will Handley, Thomas Gessey-Jones
Categories: stat.ML cs.LG
Comments: 11 pages, 5 figures
\\ ( https://arxiv.org/abs/2305.02930 ,  1612kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04817 (*cross-listing*)
replaced with revised version Wed, 31 Jan 2024 19:58:55 GMT   (28547kb,D)

Title: SiBBlInGS: Similarity-driven Building-Block Inference using Graphs
  across States
Authors: Noga Mudrik, Gal Mishne, Adam S. Charles
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2306.04817 ,  28547kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13149
replaced with revised version Thu, 1 Feb 2024 15:24:08 GMT   (11573kb,D)

Title: Discovering interpretable elastoplasticity models via the neural
  polynomial method enabled symbolic regressions
Authors: Bahador Bahmani, Hyoung Suk Suh and WaiChing Sun
Categories: cs.CE cs.LG
\\ ( https://arxiv.org/abs/2307.13149 ,  11573kb)
------------------------------------------------------------------------------
\\
arXiv:2308.00206 (*cross-listing*)
replaced with revised version Thu, 1 Feb 2024 04:56:50 GMT   (16187kb,D)

Title: Synthetic Skull CT Generation with Generative Adversarial Networks to
  Train Deep Learning Models for Clinical Transcranial Ultrasound
Authors: Kasra Naftchi-Ardebili, Karanpartap Singh, Reza Pourabolghasem, Pejman
  Ghanouni, Gerald R. Popelka, Kim Butts Pauly
Categories: eess.IV cs.LG
Comments: The first two authors contributed equally
\\ ( https://arxiv.org/abs/2308.00206 ,  16187kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13223
replaced with revised version Thu, 1 Feb 2024 01:01:26 GMT   (11885kb,D)

Title: Causal Reasoning: Charting a Revolutionary Course for Next-Generation
  AI-Native Wireless Networks
Authors: Christo Kurisummoottil Thomas, Christina Chaccour, Walid Saad,
  Merouane Debbah and Choong Seon Hong
Categories: cs.IT cs.LG math.IT
\\ ( https://arxiv.org/abs/2309.13223 ,  11885kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17496 (*cross-listing*)
replaced with revised version Thu, 1 Feb 2024 01:43:58 GMT   (1575kb,D)

Title: Tackling Interference Induced by Data Training Loops in A/B Tests: A
  Weighted Training Approach
Authors: Nian Si
Categories: stat.ME cs.LG econ.EM
\\ ( https://arxiv.org/abs/2310.17496 ,  1575kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19390 (*cross-listing*)
replaced with revised version Thu, 1 Feb 2024 09:35:33 GMT   (5370kb,D)

Title: Implicit Manifold Gaussian Process Regression
Authors: Bernardo Fichera, Viacheslav Borovitskiy, Andreas Krause, Aude Billard
Categories: stat.ML cs.LG
Journal-ref: Advances in Neural Information Processing Systems, 2023
\\ ( https://arxiv.org/abs/2310.19390 ,  5370kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05739
replaced with revised version Thu, 1 Feb 2024 13:01:25 GMT   (1909kb,D)

Title: Adaptive Compression-Aware Split Learning and Inference for Enhanced
  Network Efficiency
Authors: Akrit Mudvari, Antero Vainio, Iason Ofeidis, Sasu Tarkoma, Leandros
  Tassiulas
Categories: cs.NI cs.LG
\\ ( https://arxiv.org/abs/2311.05739 ,  1909kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06203 (*cross-listing*)
replaced with revised version Thu, 1 Feb 2024 00:09:36 GMT   (13kb)

Title: Remixing Music for Hearing Aids Using Ensemble of Fine-Tuned Source
  Separators
Authors: Matthew Daly
Categories: eess.AS cs.LG cs.SD eess.SP
Comments: 2 pages, ICASSP 2024, Cadenza Grand Challenge
\\ ( https://arxiv.org/abs/2401.06203 ,  13kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08224 (*cross-listing*)
replaced with revised version Thu, 1 Feb 2024 15:02:55 GMT   (772kb,D)

Title: Privacy Preserving Adaptive Experiment Design
Authors: Jiachun Li, Kaining Shi and David Simchi-Levi
Categories: stat.ME cs.CR cs.LG
Comments: Update an algorithm and the title of our paper
\\ ( https://arxiv.org/abs/2401.08224 ,  772kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10306 (*cross-listing*)
replaced with revised version Thu, 1 Feb 2024 15:05:43 GMT   (3948kb,D)

Title: Physics-constrained convolutional neural networks for inverse problems
  in spatiotemporal partial differential equations
Authors: Daniel Kelshaw, Luca Magri
Categories: physics.flu-dyn cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2306.04600,
  arXiv:2306.10990
\\ ( https://arxiv.org/abs/2401.10306 ,  3948kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16719
replaced with revised version Wed, 31 Jan 2024 20:49:23 GMT   (2980kb,D)

Title: OptiState: State Estimation of Legged Robots using Gated Networks with
  Transformer-based Vision and Kalman Filtering
Authors: Alexander Schperberg, Yusuke Tanaka, Saviz Mowlavi, Feng Xu, Bharathan
  Balaji, Dennis Hong
Categories: cs.RO cs.LG cs.SY eess.SY
Comments: Accepted to the 2024 IEEE International Conference on Robotics and
  Automation (ICRA), May 13-17, in Yokohama, Japan. 7 pages, 5 figures, 1 table
\\ ( https://arxiv.org/abs/2401.16719 ,  2980kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
