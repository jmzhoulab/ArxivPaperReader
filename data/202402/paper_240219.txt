Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年2月19日 17:11
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Thu 15 Feb 24 19:00:00 GMT  to  Fri 16 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.10290
Date: Thu, 15 Feb 2024 19:45:15 GMT   (3325kb,D)

Title: Experiments with Encoding Structured Data for Neural Networks
Authors: Sujay Nagesh Koujalgi and Jonathan Dodge
Categories: cs.AI
Comments: 18 pages, 8 figures, 2 tables
ACM-class: I.2.4
\\
  The project's aim is to create an AI agent capable of selecting good actions
in a game-playing domain called Battlespace. Sequential domains like
Battlespace are important testbeds for planning problems, as such, the
Department of Defense uses such domains for wargaming exercises. The agents we
developed combine Monte Carlo Tree Search (MCTS) and Deep Q-Network (DQN)
techniques in an effort to navigate the game environment, avoid obstacles,
interact with adversaries, and capture the flag. This paper will focus on the
encoding techniques we explored to present complex structured data stored in a
Python class, a necessary precursor to an agent.
\\ ( https://arxiv.org/abs/2402.10290 ,  3325kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10416
Date: Fri, 16 Feb 2024 02:47:09 GMT   (4677kb,D)

Title: Grounding Language about Belief in a Bayesian Theory-of-Mind
Authors: Lance Ying, Tan Zhi-Xuan, Lionel Wong, Vikash Mansinghka, Joshua
  Tenenbaum
Categories: cs.AI cs.CL
Comments: Under Review, 7 pages
\\
  Despite the fact that beliefs are mental states that cannot be directly
observed, humans talk about each others' beliefs on a regular basis, often
using rich compositional language to describe what others think and know. What
explains this capacity to interpret the hidden epistemic content of other
minds? In this paper, we take a step towards an answer by grounding the
semantics of belief statements in a Bayesian theory-of-mind: By modeling how
humans jointly infer coherent sets of goals, beliefs, and plans that explain an
agent's actions, then evaluating statements about the agent's beliefs against
these inferences via epistemic logic, our framework provides a conceptual role
semantics for belief, explaining the gradedness and compositionality of human
belief attributions, as well as their intimate connection with goals and plans.
We evaluate this framework by studying how humans attribute goals and beliefs
while watching an agent solve a doors-and-keys gridworld puzzle that requires
instrumental reasoning about hidden objects. In contrast to pure logical
deduction, non-mentalizing baselines, and mentalizing that ignores the role of
instrumental plans, our model provides a much better fit to human goal and
belief attributions, demonstrating the importance of theory-of-mind for a
semantics of belief.
\\ ( https://arxiv.org/abs/2402.10416 ,  4677kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10705
Date: Fri, 16 Feb 2024 14:04:56 GMT   (2799kb,D)

Title: AutoSAT: Automatically Optimize SAT Solvers via Large Language Models
Authors: Yiwen Sun, Xianyin Zhang, Shiyu Huang, Shaowei Cai, Bing-Zhen Zhang,
  Ke Wei
Categories: cs.AI
\\
  Heuristics are crucial in SAT solvers, while no heuristic rules are suitable
for all problem instances. Therefore, it typically requires to refine specific
solvers for specific problem instances. In this context, we present AutoSAT, a
novel framework for automatically optimizing heuristics in SAT solvers. AutoSAT
is based on Large Large Models (LLMs) which is able to autonomously generate
code, conduct evaluation, then utilize the feedback to further optimize
heuristics, thereby reducing human intervention and enhancing solver
capabilities. AutoSAT operates on a plug-and-play basis, eliminating the need
for extensive preliminary setup and model training, and fosters a Chain of
Thought collaborative process with fault-tolerance, ensuring robust heuristic
optimization. Extensive experiments on a Conflict-Driven Clause Learning (CDCL)
solver demonstrates the overall superior performance of AutoSAT, especially in
solving some specific SAT problem instances.
\\ ( https://arxiv.org/abs/2402.10705 ,  2799kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10725
Date: Fri, 16 Feb 2024 14:31:33 GMT   (1535kb,D)

Title: Cloud Kitchen: Using Planning-based Composite AI to Optimize Food
  Delivery Process
Authors: Slavom\'ir \v{S}vanc\'ar, Luk\'a\v{s} Chrpa, Filip Dvo\v{r}\'ak,
  Tom\'a\v{s} Balyo
Categories: cs.AI cs.LO
\\
  The global food delivery market provides many opportunities for AI-based
services that can improve the efficiency of feeding the world. This paper
presents the Cloud Kitchen platform as a decision-making tool for restaurants
with food delivery and a simulator to evaluate the impact of the decisions. The
platform consists of a Technology-Specific Bridge (TSB) that provides an
interface for communicating with restaurants or the simulator. TSB uses a PDDL
model to represent decisions embedded in the Unified Planning Framework (UPF).
Decision-making, which concerns allocating customers' orders to vehicles and
deciding in which order the customers will be served (for each vehicle), is
done via a Vehicle Routing Problem with Time Windows (VRPTW), an efficient tool
for this problem. We show that decisions made by our platform can improve
customer satisfaction by reducing the number of delayed deliveries using a
real-world historical dataset.
\\ ( https://arxiv.org/abs/2402.10725 ,  1535kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10726
Date: Fri, 16 Feb 2024 14:36:58 GMT   (1330kb)

Title: Learning Planning Action Models from State Traces
Authors: Tom\'a\v{s} Balyo, Martin Suda, Luk\'a\v{s} Chrpa, Dominik
  \v{S}afr\'anek, Filip Dvo\v{r}\'ak, Roman Bart\'ak, G. Michael Youngblood
Categories: cs.AI
\\
  Previous STRIPS domain model acquisition approaches that learn from state
traces start with the names and parameters of the actions to be learned.
Therefore their only task is to deduce the preconditions and effects of the
given actions. In this work, we explore learning in situations when the
parameters of learned actions are not provided. We define two levels of trace
quality based on which information is provided and present an algorithm for
each. In one level (L1), the states in the traces are labeled with action
names, so we can deduce the number and names of the actions, but we still need
to work out the number and types of parameters. In the other level (L2), the
states are additionally labeled with objects that constitute the parameters of
the corresponding grounded actions. Here we still need to deduce the types of
the parameters in the learned actions. We experimentally evaluate the proposed
algorithms and compare them with the state-of-the-art learning tool FAMA on a
large collection of IPC benchmarks. The evaluation shows that our new
algorithms are faster, can handle larger inputs and provide better results in
terms of learning action models more similar to reference models.
\\ ( https://arxiv.org/abs/2402.10726 ,  1330kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10762
Date: Fri, 16 Feb 2024 15:38:00 GMT   (244kb,D)

Title: On Explaining Unfairness: An Overview
Authors: Christos Fragkathoulas, Vasiliki Papanikou, Danae Pla Karidi,
  Evaggelia Pitoura
Categories: cs.AI
\\
  Algorithmic fairness and explainability are foundational elements for
achieving responsible AI. In this paper, we focus on their interplay, a
research area that is recently receiving increasing attention. To this end, we
first present two comprehensive taxonomies, each representing one of the two
complementary fields of study: fairness and explanations. Then, we categorize
explanations for fairness into three types: (a) Explanations to enhance
fairness metrics, (b) Explanations to help us understand the causes of
(un)fairness, and (c) Explanations to assist us in designing methods for
mitigating unfairness. Finally, based on our fairness and explanation
taxonomies, we present undiscovered literature paths revealing gaps that can
serve as valuable insights for future research.
\\ ( https://arxiv.org/abs/2402.10762 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10877
Date: Fri, 16 Feb 2024 18:29:19 GMT   (2663kb,D)

Title: Robust agents learn causal world models
Authors: Jonathan Richens, Tom Everitt
Categories: cs.AI cs.LG
Comments: ICLR 2024 (oral)
\\
  It has long been hypothesised that causal reasoning plays a fundamental role
in robust and general intelligence. However, it is not known if agents must
learn causal models in order to generalise to new domains, or if other
inductive biases are sufficient. We answer this question, showing that any
agent capable of satisfying a regret bound under a large set of distributional
shifts must have learned an approximate causal model of the data generating
process, which converges to the true causal model for optimal agents. We
discuss the implications of this result for several research areas including
transfer learning and causal inference.
\\ ( https://arxiv.org/abs/2402.10877 ,  2663kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10888
Date: Fri, 16 Feb 2024 18:44:37 GMT   (16329kb,D)

Title: Explainability for Machine Learning Models: From Data Adaptability to
  User Perception
Authors: julien Delaunay
Categories: cs.AI cs.HC cs.LG
Comments: PhD Thesis
\\
  This thesis explores the generation of local explanations for already
deployed machine learning models, aiming to identify optimal conditions for
producing meaningful explanations considering both data and user requirements.
The primary goal is to develop methods for generating explanations for any
model while ensuring that these explanations remain faithful to the underlying
model and comprehensible to the users.
  The thesis is divided into two parts. The first enhances a widely used
rule-based explanation method. It then introduces a novel approach for
evaluating the suitability of linear explanations to approximate a model.
Additionally, it conducts a comparative experiment between two families of
counterfactual explanation methods to analyze the advantages of one over the
other. The second part focuses on user experiments to assess the impact of
three explanation methods and two distinct representations. These experiments
measure how users perceive their interaction with the model in terms of
understanding and trust, depending on the explanations and representations.
This research contributes to a better explanation generation, with potential
implications for enhancing the transparency, trustworthiness, and usability of
deployed AI systems.
\\ ( https://arxiv.org/abs/2402.10888 ,  16329kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10302
Date: Thu, 15 Feb 2024 20:08:07 GMT   (165kb,D)

Title: How to Discern Important Urgent News?
Authors: Oleg Vasilyev and John Bohannon
Categories: cs.CL
Comments: 12 pages, 12 figures, 12 tables
\\
  We found that a simple property of clusters in a clustered dataset of news
correlate strongly with importance and urgency of news (IUN) as assessed by
LLM. We verified our finding across different news datasets, dataset sizes,
clustering algorithms and embeddings. The found correlation should allow using
clustering (as an alternative to LLM) for identifying the most important urgent
news, or for filtering out unimportant articles.
\\ ( https://arxiv.org/abs/2402.10302 ,  165kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10311
Date: Thu, 15 Feb 2024 20:24:39 GMT   (76kb,D)

Title: The optimal placement of the head in the noun phrase. The case of
  demonstrative, numeral, adjective and noun
Authors: Ramon Ferrer-i-Cancho
Categories: cs.CL physics.soc-ph
\\
  The word order of a sentence is shaped by multiple principles. The principle
of syntactic dependency distance minimization is in conflict with the principle
of surprisal minimization (or predictability maximization) in single head
syntactic dependency structures: while the former predicts that the head should
be placed at the center of the linear arrangement, the latter predicts that the
head should be placed at one of the ends (either first or last). A critical
question is when surprisal minimization (or predictability maximization) should
surpass syntactic dependency distance minimization. In the context of single
head structures, it has been predicted that this is more likely to happen when
two conditions are met, i.e. (a) fewer words are involved and (b) words are
shorter. Here we test the prediction on the noun phrase when its composed of a
demonstrative, a numeral, an adjective and a noun. We find that, across
preferred orders in languages, the noun tends to be placed at one of the ends,
confirming the theoretical prediction. We also show evidence of anti locality
effects: syntactic dependency distances in preferred orders are longer than
expected by chance.
\\ ( https://arxiv.org/abs/2402.10311 ,  76kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10353
Date: Thu, 15 Feb 2024 22:54:24 GMT   (11114kb,D)

Title: Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of
  Language Models
Authors: Kang He, Yinghan Long, Kaushik Roy
Categories: cs.CL cs.LG
\\
  Prompt learning is susceptible to intrinsic bias present in pre-trained
language models (LMs), resulting in sub-optimal performance of prompt-based
zero/few-shot learning. In this work, we propose a null-input prompting method
to calibrate intrinsic bias encoded in pre-trained LMs. Different from prior
efforts that address intrinsic bias primarily for social fairness and often
involve excessive computational cost, our objective is to explore enhancing
LMs' performance in downstream zero/few-shot learning while emphasizing the
efficiency of intrinsic bias calibration. Specifically, we leverage a diverse
set of auto-selected null-meaning inputs generated from GPT-4 to prompt
pre-trained LMs for intrinsic bias probing. Utilizing the bias-reflected
probability distribution, we formulate a distribution disparity loss for bias
calibration, where we exclusively update bias parameters ($0.1\%$ of total
parameters) of LMs towards equal probability distribution. Experimental results
show that the calibration promotes an equitable starting point for LMs while
preserving language modeling abilities. Across a wide range of datasets,
including sentiment analysis and topic classification, our method significantly
improves zero/few-shot learning performance of LMs for both in-context learning
and prompt-based fine-tuning (on average $9\%$ and $2\%$, respectively).
\\ ( https://arxiv.org/abs/2402.10353 ,  11114kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10373
Date: Thu, 15 Feb 2024 23:39:04 GMT   (7101kb,D)

Title: BioMistral: A Collection of Open-Source Pretrained Large Language Models
  for Medical Domains
Authors: Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud,
  Mickael Rouvier, Richard Dufour
Categories: cs.CL cs.AI cs.LG
\\
  Large Language Models (LLMs) have demonstrated remarkable versatility in
recent years, offering potential applications across specialized domains such
as healthcare and medicine. Despite the availability of various open-source
LLMs tailored for health contexts, adapting general-purpose LLMs to the medical
domain presents significant challenges. In this paper, we introduce BioMistral,
an open-source LLM tailored for the biomedical domain, utilizing Mistral as its
foundation model and further pre-trained on PubMed Central. We conduct a
comprehensive evaluation of BioMistral on a benchmark comprising 10 established
medical question-answering (QA) tasks in English. We also explore lightweight
models obtained through quantization and model merging approaches. Our results
demonstrate BioMistral's superior performance compared to existing open-source
medical models and its competitive edge against proprietary counterparts.
Finally, to address the limited availability of data beyond English and to
assess the multilingual generalization of medical LLMs, we automatically
translated and evaluated this benchmark into 7 other languages. This marks the
first large-scale multilingual evaluation of LLMs in the medical domain.
Datasets, multilingual evaluation benchmarks, scripts, and all the models
obtained during our experiments are freely released.
\\ ( https://arxiv.org/abs/2402.10373 ,  7101kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10379
Date: Fri, 16 Feb 2024 00:10:26 GMT   (688kb,D)

Title: DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM
  Workflows
Authors: Ajay Patel, Colin Raffel, Chris Callison-Burch
Categories: cs.CL cs.LG
\\
  Large language models (LLMs) have become a dominant and important tool for
NLP researchers in a wide range of tasks. Today, many researchers use LLMs in
synthetic data generation, task evaluation, fine-tuning, distillation, and
other model-in-the-loop research workflows. However, challenges arise when
using these models that stem from their scale, their closed source nature, and
the lack of standardized tooling for these new and emerging workflows. The
rapid rise to prominence of these models and these unique challenges has had
immediate adverse impacts on open science and on the reproducibility of work
that uses them. In this paper, we introduce DataDreamer, an open source Python
library that allows researchers to write simple code to implement powerful LLM
workflows. DataDreamer also helps researchers adhere to best practices that we
propose to encourage open science and reproducibility. The library and
documentation are available at https://github.com/datadreamer-dev/DataDreamer .
\\ ( https://arxiv.org/abs/2402.10379 ,  688kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10400
Date: Fri, 16 Feb 2024 01:54:43 GMT   (5888kb,D)

Title: Chain of Logic: Rule-Based Reasoning with Large Language Models
Authors: Sergio Servantez, Joe Barrow, Kristian Hammond, Rajiv Jain
Categories: cs.CL
\\
  Rule-based reasoning, a fundamental type of legal reasoning, enables us to
draw conclusions by accurately applying a rule to a set of facts. We explore
causal language models as rule-based reasoners, specifically with respect to
compositional rules - rules consisting of multiple elements which form a
complex logical expression. Reasoning about compositional rules is challenging
because it requires multiple reasoning steps, and attending to the logical
relationships between elements. We introduce a new prompting method, Chain of
Logic, which elicits rule-based reasoning through decomposition (solving
elements as independent threads of logic), and recomposition (recombining these
sub-answers to resolve the underlying logical expression). This method was
inspired by the IRAC (Issue, Rule, Application, Conclusion) framework, a
sequential reasoning approach used by lawyers. We evaluate chain of logic
across eight rule-based reasoning tasks involving three distinct compositional
rules from the LegalBench benchmark and demonstrate it consistently outperforms
other prompting methods, including chain of thought and self-ask, using
open-source and commercial language models.
\\ ( https://arxiv.org/abs/2402.10400 ,  5888kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10409
Date: Fri, 16 Feb 2024 02:21:59 GMT   (2273kb,D)

Title: Understanding Survey Paper Taxonomy about Large Language Models via
  Graph Representation Learning
Authors: Jun Zhuang, Casey Kennington
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: TL;DR: We collected metadata about LLM surveys and developed a method
  for categorizing them into a taxonomy, indicating the superiority of graph
  representation learning over language models and revealing the efficacy of
  fine-tuning using weak labels
\\
  As new research on Large Language Models (LLMs) continues, it is difficult to
keep up with new research and models. To help researchers synthesize the new
research many have written survey papers, but even those have become numerous.
In this paper, we develop a method to automatically assign survey papers to a
taxonomy. We collect the metadata of 144 LLM survey papers and explore three
paradigms to classify papers within the taxonomy. Our work indicates that
leveraging graph structure information on co-category graphs can significantly
outperform the language models in two paradigms; pre-trained language models'
fine-tuning and zero-shot/few-shot classifications using LLMs. We find that our
model surpasses an average human recognition level and that fine-tuning LLMs
using weak labels generated by a smaller model, such as the GCN in this study,
can be more effective than using ground-truth labels, revealing the potential
of weak-to-strong generalization in the taxonomy classification task.
\\ ( https://arxiv.org/abs/2402.10409 ,  2273kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10412
Date: Fri, 16 Feb 2024 02:32:06 GMT   (561kb,D)

Title: Measuring and Reducing LLM Hallucination without Gold-Standard Answers
  via Expertise-Weighting
Authors: Jiaheng Wei, Yuanshun Yao, Jean-Francois Ton, Hongyi Guo, Andrew
  Estornell, Yang Liu
Categories: cs.CL cs.AI cs.LG
Comments: Paper Under Review
\\
  LLM hallucination, i.e. generating factually incorrect yet seemingly
convincing answers, is currently a major threat to the trustworthiness and
reliability of LLMs. The first step towards solving this complicated problem is
to measure it. However, existing hallucination metrics require to have a
benchmark dataset with gold-standard answers, i.e. "best" or "correct" answers
written by humans. Such requirement makes hallucination measurement costly and
prone to human errors. In this work, we propose Factualness Evaluations via
Weighting LLMs (FEWL), the first hallucination metric that is specifically
designed for the scenario when gold-standard answers are absent. FEWL leverages
the answers from off-the-shelf LLMs that serve as a proxy of gold-standard
answers. The key challenge is how to quantify the expertise of reference LLMs
resourcefully. We show FEWL has certain theoretical guarantees and demonstrate
empirically it gives more accurate hallucination measures than naively using
reference LLMs. We also show how to leverage FEWL to reduce hallucination
through both in-context learning and supervised finetuning. Last, we build a
large-scale benchmark dataset to facilitate LLM hallucination research.
\\ ( https://arxiv.org/abs/2402.10412 ,  561kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10422
Date: Fri, 16 Feb 2024 03:06:37 GMT   (444kb,D)

Title: Pushing the Limits of Zero-shot End-to-End Speech Translation
Authors: Ioannis Tsiamas, Gerard I. G\'allego, Jos\'e A. R. Fonollosa, Marta R.
  Costa-juss\`a
Categories: cs.CL
\\
  Data scarcity and the modality gap between the speech and text modalities are
two major obstacles of end-to-end Speech Translation (ST) systems, thus
hindering their performance. Prior work has attempted to mitigate these
challenges by leveraging external MT data and optimizing distance metrics that
bring closer the speech-text representations. However, achieving competitive
results typically requires some ST data. For this reason, we introduce
ZeroSwot, a method for zero-shot ST that bridges the modality gap without any
paired ST data. Leveraging a novel CTC compression and Optimal Transport, we
train a speech encoder using only ASR data, to align with the representation
space of a massively multilingual MT model. The speech encoder seamlessly
integrates with the MT model at inference, enabling direct translation from
speech to text, across all languages supported by the MT model. Our experiments
show that we can effectively close the modality gap without ST data, while our
results on MuST-C and CoVoST demonstrate our method's superiority over not only
previous zero-shot models, but also supervised ones, achieving state-of-the-art
results.
\\ ( https://arxiv.org/abs/2402.10422 ,  444kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10424
Date: Fri, 16 Feb 2024 03:20:14 GMT   (637kb,D)

Title: Understanding In-Context Learning with a Pelican Soup Framework
Authors: Ting-Rui Chiang, Dani Yogatama
Categories: cs.CL cs.AI
\\
  Many existing theoretical analyses of in-context learning for natural
language processing are based on latent variable models that leaves gaps
between theory and practice. We aim to close these gaps by proposing a
theoretical framework, the Pelican Soup Framework. In this framework, we
introduce (1) the notion of a common sense knowledge base, (2) a general
formalism for natural language classification tasks, and the notion of (3)
meaning association. Under this framework, we can establish a
$\mathcal{O}(1/T)$ loss bound for in-context learning, where $T$ is the number
of example-label pairs in the demonstration. Compared with previous works, our
bound reflects the effect of the choice of verbalizers and the effect of
instruction tuning. An additional notion of \textit{atom concepts} makes our
framework possible to explain the generalization to tasks unseen in the
language model training data. Finally, we propose a toy setup, Calcutec, and a
digit addition task that mimics types of distribution shifts a model needs to
overcome to perform in-context learning. We also experiment with GPT2-Large on
real-world NLP tasks. Our empirical results demonstrate the efficacy of our
framework to explain in-context learning.
\\ ( https://arxiv.org/abs/2402.10424 ,  637kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10426
Date: Fri, 16 Feb 2024 03:24:56 GMT   (631kb,D)

Title: DELL: Generating Reactions and Explanations for LLM-Based Misinformation
  Detection
Authors: Herun Wan, Shangbin Feng, Zhaoxuan Tan, Heng Wang, Yulia Tsvetkov,
  Minnan Luo
Categories: cs.CL
\\
  Large language models are limited by challenges in factuality and
hallucinations to be directly employed off-the-shelf for judging the veracity
of news articles, where factual accuracy is paramount. In this work, we propose
DELL that identifies three key stages in misinformation detection where LLMs
could be incorporated as part of the pipeline: 1) LLMs could \emph{generate
news reactions} to represent diverse perspectives and simulate user-news
interaction networks; 2) LLMs could \emph{generate explanations} for proxy
tasks (e.g., sentiment, stance) to enrich the contexts of news articles and
produce experts specializing in various aspects of news understanding; 3) LLMs
could \emph{merge task-specific experts} and provide an overall prediction by
incorporating the predictions and confidence scores of varying experts.
Extensive experiments on seven datasets with three LLMs demonstrate that DELL
outperforms state-of-the-art baselines by up to 16.8\% in macro f1-score.
Further analysis reveals that the generated reactions and explanations are
greatly helpful in misinformation detection, while our proposed LLM-guided
expert merging helps produce better-calibrated predictions.
\\ ( https://arxiv.org/abs/2402.10426 ,  631kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10427
Date: Fri, 16 Feb 2024 03:30:27 GMT   (452kb,D)

Title: Evaluating and Improving Continual Learning in Spoken Language
  Understanding
Authors: Muqiao Yang, Xiang Li, Umberto Cappellazzo, Shinji Watanabe, Bhiksha
  Raj
Categories: cs.CL cs.AI cs.SD
\\
  Continual learning has emerged as an increasingly important challenge across
various tasks, including Spoken Language Understanding (SLU). In SLU, its
objective is to effectively handle the emergence of new concepts and evolving
environments. The evaluation of continual learning algorithms typically
involves assessing the model's stability, plasticity, and generalizability as
fundamental aspects of standards. However, existing continual learning metrics
primarily focus on only one or two of the properties. They neglect the overall
performance across all tasks, and do not adequately disentangle the plasticity
versus stability/generalizability trade-offs within the model. In this work, we
propose an evaluation methodology that provides a unified evaluation on
stability, plasticity, and generalizability in continual learning. By employing
the proposed metric, we demonstrate how introducing various knowledge
distillations can improve different aspects of these three properties of the
SLU model. We further show that our proposed metric is more sensitive in
capturing the impact of task ordering in continual learning, making it better
suited for practical use-case scenarios.
\\ ( https://arxiv.org/abs/2402.10427 ,  452kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10430
Date: Fri, 16 Feb 2024 03:39:37 GMT   (178kb,D)

Title: Smaller Language Models are capable of selecting Instruction-Tuning
  Training Data for Larger Language Models
Authors: Dheeraj Mekala, Alex Nguyen, Jingbo Shang
Categories: cs.CL
\\
  Instruction-tuning language models has become a crucial step in aligning them
for general use. Typically, this process involves extensive training on large
datasets, incurring high training costs. In this paper, we introduce a novel
training data selection based on the learning percentage of the samples. We
assert that current language models possess the capability to autonomously
select high-quality training data, leading to comparable or improved
performance compared to training on the entire dataset. Our experiments span
different-sized models, revealing that this characteristic holds for models
ranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an
interesting finding that the data hardness transfers across model sizes, and a
smaller 350M model can effectively curate high-quality training data with hard
samples for a larger 13B model, resulting in an equally or superior
instruction-tuned model compared to training on the complete dataset. Utilizing
open-sourced OPT and Llama-2 models up to 13B in size, two publicly available
instruction-tuning training datasets and evaluated by both automatic metrics &
humans, our paper introduces a novel approach to training data selection,
showcasing a more efficient alternative.
\\ ( https://arxiv.org/abs/2402.10430 ,  178kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10436
Date: Fri, 16 Feb 2024 03:54:48 GMT   (2435kb,D)

Title: I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large
  Language Models
Authors: Wenchao Dong, Assem Zhunis, Hyojin Chin, Jiyoung Han, Meeyoung Cha
Categories: cs.CL
\\
  We explored cultural biases-individualism vs. collectivism-in ChatGPT across
three Western languages (i.e., English, German, and French) and three Eastern
languages (i.e., Chinese, Japanese, and Korean). When ChatGPT adopted an
individualistic persona in Western languages, its collectivism scores (i.e.,
out-group values) exhibited a more negative trend, surpassing their positive
orientation towards individualism (i.e., in-group values). Conversely, when a
collectivistic persona was assigned to ChatGPT in Eastern languages, a similar
pattern emerged with more negative responses toward individualism (i.e.,
out-group values) as compared to collectivism (i.e., in-group values). The
results indicate that when imbued with a particular social identity, ChatGPT
discerns in-group and out-group, embracing in-group values while eschewing
out-group values. Notably, the negativity towards the out-group, from which
prejudices and discrimination arise, exceeded the positivity towards the
in-group. The experiment was replicated in the political domain, and the
results remained consistent. Furthermore, this replication unveiled an
intrinsic Democratic bias in Large Language Models (LLMs), aligning with
earlier findings and providing integral insights into mitigating such bias
through prompt engineering. Extensive robustness checks were performed using
varying hyperparameter and persona setup methods, with or without social
identity labels, across other popular language models.
\\ ( https://arxiv.org/abs/2402.10436 ,  2435kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10447
Date: Fri, 16 Feb 2024 04:41:33 GMT   (7131kb,D)

Title: Incremental Sequence Labeling: A Tale of Two Shifts
Authors: Shengjie Qiu, Junhao Zheng, Zhen Liu, Yicheng Luo, Qianli Ma
Categories: cs.CL cs.LG
\\
  The incremental sequence labeling task involves continuously learning new
classes over time while retaining knowledge of the previous ones. Our
investigation identifies two significant semantic shifts: E2O (where the model
mislabels an old entity as a non-entity) and O2E (where the model labels a
non-entity or old entity as a new entity). Previous research has predominantly
focused on addressing the E2O problem, neglecting the O2E issue. This
negligence results in a model bias towards classifying new data samples as
belonging to the new class during the learning process. To address these
challenges, we propose a novel framework, Incremental Sequential Labeling
without Semantic Shifts (IS3). Motivated by the identified semantic shifts (E2O
and O2E), IS3 aims to mitigate catastrophic forgetting in models. As for the
E2O problem, we use knowledge distillation to maintain the model's
discriminative ability for old entities. Simultaneously, to tackle the O2E
problem, we alleviate the model's bias towards new entities through debiased
loss and optimization levels. Our experimental evaluation, conducted on three
datasets with various incremental settings, demonstrates the superior
performance of IS3 compared to the previous state-of-the-art method by a
significant margin.
\\ ( https://arxiv.org/abs/2402.10447 ,  7131kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10453
Date: Fri, 16 Feb 2024 05:03:01 GMT   (1043kb,D)

Title: Steering Conversational Large Language Models for Long Emotional Support
  Conversations
Authors: Navid Madani, Sougata Saha, Rohini Srihari
Categories: cs.CL
\\
  In this study, we address the challenge of consistently following emotional
support strategies in long conversations by large language models (LLMs). We
introduce the Strategy-Relevant Attention (SRA) metric, a model-agnostic
measure designed to evaluate the effectiveness of LLMs in adhering to strategic
prompts in emotional support contexts. By analyzing conversations within the
Emotional Support Conversations dataset (ESConv) using LLaMA models, we
demonstrate that SRA is significantly correlated with a model's ability to
sustain the outlined strategy throughout the interactions. Our findings reveal
that the application of SRA-informed prompts leads to enhanced strategic
adherence, resulting in conversations that more reliably exhibit the desired
emotional support strategies over longer conversations. Furthermore, we
contribute a comprehensive, multi-branch synthetic conversation dataset for
ESConv, featuring a variety of strategy continuations informed by our optimized
prompting method. The code and data are publicly available on our Github.
\\ ( https://arxiv.org/abs/2402.10453 ,  1043kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10466
Date: Fri, 16 Feb 2024 06:13:18 GMT   (8006kb,D)

Title: Large Language Models as Zero-shot Dialogue State Tracker through
  Function Calling
Authors: Zekun Li, Zhiyu Zoey Chen, Mike Ross, Patrick Huber, Seungwhan Moon,
  Zhaojiang Lin, Xin Luna Dong, Adithya Sagar, Xifeng Yan, Paul A. Crook
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) are increasingly prevalent in conversational
systems due to their advanced understanding and generative capabilities in
general contexts. However, their effectiveness in task-oriented dialogues
(TOD), which requires not only response generation but also effective dialogue
state tracking (DST) within specific tasks and domains, remains less
satisfying. In this work, we propose a novel approach FnCTOD for solving DST
with LLMs through function calling. This method improves zero-shot DST,
allowing adaptation to diverse domains without extensive data collection or
model tuning. Our experimental results demonstrate that our approach achieves
exceptional performance with both modestly sized open-source and also
proprietary LLMs: with in-context prompting it enables various 7B or 13B
parameter models to surpass the previous state-of-the-art (SOTA) achieved by
ChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6% Avg. JGA.
Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%,
respectively. We also show that by fine-tuning on a small collection of diverse
task-oriented dialogues, we can equip modestly sized models, specifically a 13B
parameter LLaMA2-Chat model, with function-calling capabilities and DST
performance comparable to ChatGPT while maintaining their chat capabilities. We
plan to open-source experimental code and model.
\\ ( https://arxiv.org/abs/2402.10466 ,  8006kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10496
Date: Fri, 16 Feb 2024 08:10:34 GMT   (7014kb,D)

Title: Comparing Hallucination Detection Metrics for Multilingual Generation
Authors: Haoqiang Kang, Terra Blevins, Luke Zettlemoyer
Categories: cs.CL cs.AI
\\
  While many automatic hallucination detection techniques have been proposed
for English texts, their effectiveness in multilingual contexts remains
unexplored. This paper aims to bridge the gap in understanding how these
hallucination detection metrics perform on non-English languages. We evaluate
the efficacy of various detection metrics, including lexical metrics like ROUGE
and Named Entity Overlap and Natural Language Inference (NLI)-based metrics, at
detecting hallucinations in biographical summaries in many languages; we also
evaluate how correlated these different metrics are to gauge whether they
measure the same phenomena. Our empirical analysis reveals that while lexical
metrics show limited effectiveness, NLI-based metrics perform well in
high-resource languages at the sentence level. In contrast, NLI-based metrics
often fail to detect atomic fact hallucinations. Our findings highlight
existing gaps in multilingual hallucination detection and motivate future
research to develop more robust detection methods for LLM hallucination in
other languages.
\\ ( https://arxiv.org/abs/2402.10496 ,  7014kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10527
Date: Fri, 16 Feb 2024 09:29:38 GMT   (365kb,D)

Title: Zero-shot sampling of adversarial entities in biomedical question
  answering
Authors: R. Patrick Xian, Alex J. Lee, Vincent Wang, Qiming Cui, Russell Ro,
  Reza Abbasi-Asl
Categories: cs.CL cs.CR stat.AP
Comments: 20 pages incl. appendix, under review
\\
  The increasing depth of parametric domain knowledge in large language models
(LLMs) is fueling their rapid deployment in real-world applications. In
high-stakes and knowledge-intensive tasks, understanding model vulnerabilities
is essential for quantifying the trustworthiness of model predictions and
regulating their use. The recent discovery of named entities as adversarial
examples in natural language processing tasks raises questions about their
potential guises in other settings. Here, we propose a powerscaled
distance-weighted sampling scheme in embedding space to discover diverse
adversarial entities as distractors. We demonstrate its advantage over random
sampling in adversarial question answering on biomedical topics. Our approach
enables the exploration of different regions on the attack surface, which
reveals two regimes of adversarial entities that markedly differ in their
characteristics. Moreover, we show that the attacks successfully manipulate
token-wise Shapley value explanations, which become deceptive in the
adversarial setting. Our investigations illustrate the brittleness of domain
knowledge in LLMs and reveal a shortcoming of standard evaluations for
high-capacity models.
\\ ( https://arxiv.org/abs/2402.10527 ,  365kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10528
Date: Fri, 16 Feb 2024 09:29:50 GMT   (9130kb,D)

Title: Can We Verify Step by Step for Incorrect Answer Detection?
Authors: Xin Xu, Shizhe Diao, Can Yang, Yang Wang
Categories: cs.CL cs.AI
Comments: 8 pages, 6 figures
\\
  Chain-of-Thought (CoT) prompting has marked a significant advancement in
enhancing the reasoning capabilities of large language models (LLMs). Previous
studies have developed various extensions of CoT, which focus primarily on
enhancing end-task performance. In addition, there has been research on
assessing the quality of reasoning chains in CoT. This raises an intriguing
question: Is it possible to predict the accuracy of LLM outputs by scrutinizing
the reasoning chains they generate? To answer this research question, we
introduce a benchmark, R2PE, designed specifically to explore the relationship
between reasoning chains and performance in various reasoning tasks spanning
five different domains. This benchmark aims to measure the falsehood of the
final output of LLMs based on the reasoning steps. To make full use of
information in multiple reasoning chains, we propose the process discernibility
score (PDS) framework that beats the answer-checking baseline by a large
margin. Concretely, this resulted in an average of 5.1% increase in the F1
score across all 45 subsets within R2PE. We further demonstrate our PDS's
efficacy in advancing open-domain QA accuracy. Data and code are available at
https://github.com/XinXU-USTC/R2PE.
\\ ( https://arxiv.org/abs/2402.10528 ,  9130kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10532
Date: Fri, 16 Feb 2024 09:37:54 GMT   (7611kb,D)

Title: Properties and Challenges of LLM-Generated Explanations
Authors: Jenny Kunz, Marco Kuhlmann
Categories: cs.CL cs.AI cs.HC cs.LG
\\
  The self-rationalising capabilities of large language models (LLMs) have been
explored in restricted settings, using task/specific data sets. However,
current LLMs do not (only) rely on specifically annotated data; nonetheless,
they frequently explain their outputs. The properties of the generated
explanations are influenced by the pre-training corpus and by the target data
used for instruction fine-tuning. As the pre-training corpus includes a large
amount of human-written explanations "in the wild", we hypothesise that LLMs
adopt common properties of human explanations. By analysing the outputs for a
multi-domain instruction fine-tuning data set, we find that generated
explanations show selectivity and contain illustrative elements, but less
frequently are subjective or misleading. We discuss reasons and consequences of
the properties' presence or absence. In particular, we outline positive and
negative implications depending on the goals and user groups of the
self-rationalising system.
\\ ( https://arxiv.org/abs/2402.10532 ,  7611kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10543
Date: Fri, 16 Feb 2024 10:11:20 GMT   (1270kb,D)

Title: Strong hallucinations from negation and how to fix them
Authors: Nicholas Asher and Swarnadeep Bhar
Categories: cs.CL cs.AI
ACM-class: I.2.7
\\
  Despite great performance on many tasks, language models (LMs) still struggle
with reasoning, sometimes providing responses that cannot possibly be true
because they stem from logical incoherence. We call such responses
\textit{strong hallucinations} and prove that they follow from an LM's
computation of its internal representations for logical operators and outputs
from those representations. Focusing on negation, we provide a novel solution
in which negation is treated not as another element of a latent representation,
but as \textit{an operation over an LM's latent representations that constrains
how they may evolve}. We show that our approach improves model performance in
cloze prompting and natural language inference tasks with negation without
requiring training on sparse negative data.
\\ ( https://arxiv.org/abs/2402.10543 ,  1270kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10552
Date: Fri, 16 Feb 2024 10:32:16 GMT   (7921kb,D)

Title: Conversational SimulMT: Efficient Simultaneous Translation with Large
  Language Models
Authors: Minghan Wang, Thuy-Trang Vu, Ehsan Shareghi, Gholamreza Haffari
Categories: cs.CL
\\
  Simultaneous machine translation (SimulMT) presents a challenging trade-off
between translation quality and latency. Recent studies have shown that LLMs
can achieve good performance in SimulMT tasks. However, this often comes at the
expense of high inference cost and latency. In this paper, we propose a
conversational SimulMT framework to enhance the inference efficiency of
LLM-based SimulMT through multi-turn-dialogue-based decoding. Our experiments
with Llama2-7b-chat on two SimulMT benchmarks demonstrate the superiority of
LLM in translation quality while achieving comparable computational latency to
specialized SimulMT models.
\\ ( https://arxiv.org/abs/2402.10552 ,  7921kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10554
Date: Fri, 16 Feb 2024 10:35:18 GMT   (1926kb,D)

Title: Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in
  Disordered Texts
Authors: Xiaobo Guo and Soroush Vosoughi
Categories: cs.CL
\\
  Aspect-based summarization has seen significant advancements, especially in
structured text. Yet, summarizing disordered, large-scale texts, like those
found in social media and customer feedback, remains a significant challenge.
Current research largely targets predefined aspects within structured texts,
neglecting the complexities of dynamic and disordered environments. Addressing
this gap, we introduce Disordered-DABS, a novel benchmark for dynamic
aspect-based summarization tailored to unstructured text. Developed by adapting
existing datasets for cost-efficiency and scalability, our comprehensive
experiments and detailed human evaluations reveal that Disordered-DABS poses
unique challenges to contemporary summarization models, including
state-of-the-art language models such as GPT-3.5.
\\ ( https://arxiv.org/abs/2402.10554 ,  1926kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10558
Date: Fri, 16 Feb 2024 10:40:38 GMT   (346kb)

Title: Neural paraphrasing by automatically crawled and aligned sentence pairs
Authors: Achille Globo and Antonio Trevisi and Andrea Zugarini and Leonardo
  Rigutini and Marco Maggini and Stefano Melacci
Categories: cs.CL
Comments: The 6th International Conference on Social Networks Analysis,
  Management and Security (SNAMS 2019)
Journal-ref: Proceedings of The 6th International Conference on Social Networks
  Analysis, Management and Security (SNAMS 2019)
DOI: 10.1109/SNAMS.2019.8931824
\\
  Paraphrasing is the task of re-writing an input text using other words,
without altering the meaning of the original content. Conversational systems
can exploit automatic paraphrasing to make the conversation more natural, e.g.,
talking about a certain topic using different paraphrases in different time
instants. Recently, the task of automatically generating paraphrases has been
approached in the context of Natural Language Generation (NLG). While many
existing systems simply consist in rule-based models, the recent success of the
Deep Neural Networks in several NLG tasks naturally suggests the possibility of
exploiting such networks for generating paraphrases. However, the main obstacle
toward neural-network-based paraphrasing is the lack of large datasets with
aligned pairs of sentences and paraphrases, that are needed to efficiently
train the neural models. In this paper we present a method for the automatic
generation of large aligned corpora, that is based on the assumption that news
and blog websites talk about the same events using different narrative styles.
We propose a similarity search procedure with linguistic constraints that,
given a reference sentence, is able to locate the most similar candidate
paraphrases out from millions of indexed sentences. The data generation process
is evaluated in the case of the Italian language, performing experiments using
pointer-based deep neural architectures.
\\ ( https://arxiv.org/abs/2402.10558 ,  346kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10567
Date: Fri, 16 Feb 2024 10:54:10 GMT   (2276kb,D)

Title: InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs
  ready for the Indian Legal Domain?
Authors: Yogesh Tripathi, Raghav Donakanti, Sahil Girhepuje, Ishan Kavathekar,
  Bhaskara Hanuma Vedula, Gokul S Krishnan, Shreya Goyal, Anmol Goel, Balaraman
  Ravindran, Ponnurangam Kumaraguru
Categories: cs.CL cs.AI
\\
  Recent advancements in language technology and Artificial Intelligence have
resulted in numerous Language Models being proposed to perform various tasks in
the legal domain ranging from predicting judgments to generating summaries.
Despite their immense potential, these models have been proven to learn and
exhibit societal biases and make unfair predictions. In this study, we explore
the ability of Large Language Models (LLMs) to perform legal tasks in the
Indian landscape when social factors are involved. We present a novel metric,
$\beta$-weighted $\textit{Legal Safety Score ($LSS_{\beta}$)}$, which
encapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs'
safety by considering its performance in the $\textit{Binary Statutory
Reasoning}$ task and its fairness exhibition with respect to various axes of
disparities in the Indian society. Task performance and fairness scores of
LLaMA and LLaMA--2 models indicate that the proposed $LSS_{\beta}$ metric can
effectively determine the readiness of a model for safe usage in the legal
sector. We also propose finetuning pipelines, utilising specialised legal
datasets, as a potential method to mitigate bias and improve model safety. The
finetuning procedures on LLaMA and LLaMA--2 models increase the $LSS_{\beta}$,
improving their usability in the Indian legal domain. Our code is publicly
released.
\\ ( https://arxiv.org/abs/2402.10567 ,  2276kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10571
Date: Fri, 16 Feb 2024 10:55:38 GMT   (878kb,D)

Title: Direct Preference Optimization with an Offset
Authors: Afra Amini, Tim Vieira, Ryan Cotterell
Categories: cs.CL cs.AI cs.LG
\\
  Direct preference optimization (DPO) is a successful fine-tuning strategy for
aligning large language models with human preferences without the need to train
a reward model or employ reinforcement learning. DPO, as originally formulated,
relies on binary preference data and fine-tunes a language model to increase
the likelihood of a preferred response over a dispreferred response. However,
not all preference pairs are equal: while in some cases the preferred response
is only slightly better than the dispreferred response, there can be a stronger
preference for one response when, for example, the other response includes
harmful or toxic content. In this paper, we propose a generalization of DPO,
termed DPO with an offset (ODPO), that does not treat every preference pair
equally during fine-tuning. Intuitively, ODPO requires the difference between
the likelihood of the preferred and dispreferred response to be greater than an
offset value. The offset is determined based on the extent to which one
response is preferred over another. Our experiments on various tasks suggest
that ODPO significantly outperforms DPO in aligning language models, especially
when the number of preference pairs is limited.
\\ ( https://arxiv.org/abs/2402.10571 ,  878kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10573
Date: Fri, 16 Feb 2024 11:02:29 GMT   (4905kb,D)

Title: LinkNER: Linking Local Named Entity Recognition Models to Large Language
  Models using Uncertainty
Authors: Zhen Zhang, Yuhua Zhao, Hang Gao, and Mengting Hu
Categories: cs.CL
Comments: Accepted by WebConf (WWW'2024)
ACM-class: I.2.7
\\
  Named Entity Recognition (NER) serves as a fundamental task in natural
language understanding, bearing direct implications for web content analysis,
search engines, and information retrieval systems. Fine-tuned NER models
exhibit satisfactory performance on standard NER benchmarks. However, due to
limited fine-tuning data and lack of knowledge, it performs poorly on unseen
entity recognition. As a result, the usability and reliability of NER models in
web-related applications are compromised. Instead, Large Language Models (LLMs)
like GPT-4 possess extensive external knowledge, but research indicates that
they lack specialty for NER tasks. Furthermore, non-public and large-scale
weights make tuning LLMs difficult. To address these challenges, we propose a
framework that combines small fine-tuned models with LLMs (LinkNER) and an
uncertainty-based linking strategy called RDC that enables fine-tuned models to
complement black-box LLMs, achieving better performance. We experiment with
both standard NER test sets and noisy social media datasets. LinkNER enhances
NER task performance, notably surpassing SOTA models in robustness tests. We
also quantitatively analyze the influence of key components like uncertainty
estimation methods, LLMs, and in-context learning on diverse NER tasks,
offering specific web-related recommendations.
\\ ( https://arxiv.org/abs/2402.10573 ,  4905kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10586
Date: Fri, 16 Feb 2024 11:20:30 GMT   (10155kb,D)

Title: Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse
  Motifs
Authors: Zae Myung Kim and Kwang Hee Lee and Preston Zhu and Vipul Raheja and
  Dongyeop Kang
Categories: cs.CL
Comments: 25 pages
\\
  With the advent of large language models (LLM), the line between
human-crafted and machine-generated texts has become increasingly blurred. This
paper delves into the inquiry of identifying discernible and unique linguistic
properties in texts that were written by humans, particularly uncovering the
underlying discourse structures of texts beyond their surface structures.
Introducing a novel methodology, we leverage hierarchical parse trees and
recursive hypergraphs to unveil distinctive discourse patterns in texts
produced by both LLMs and humans. Empirical findings demonstrate that, although
both LLMs and humans generate distinct discourse patterns influenced by
specific domains, human-written texts exhibit more structural variability,
reflecting the nuanced nature of human writing in different domains. Notably,
incorporating hierarchical discourse features enhances binary classifiers'
overall performance in distinguishing between human-written and
machine-generated texts, even on out-of-distribution and paraphrased samples.
This underscores the significance of incorporating hierarchical discourse
features in the analysis of text patterns. The code and dataset will be
available at [TBA].
\\ ( https://arxiv.org/abs/2402.10586 ,  10155kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10588
Date: Fri, 16 Feb 2024 11:21:28 GMT   (10443kb,D)

Title: Do Llamas Work in English? On the Latent Language of Multilingual
  Transformers
Authors: Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West
Categories: cs.CL cs.CY
Comments: 12 pages. 28 with appendix
\\
  We ask whether multilingual language models trained on unbalanced,
English-dominated corpora use English as an internal pivot language -- a
question of key importance for understanding how language models function and
the origins of linguistic bias. Focusing on the Llama-2 family of transformer
models, our study uses carefully constructed non-English prompts with a unique
correct single-token continuation. From layer to layer, transformers gradually
map an input embedding of the final prompt token to an output embedding from
which next-token probabilities are computed. Tracking intermediate embeddings
through their high-dimensional space reveals three distinct phases, whereby
intermediate embeddings (1) start far away from output token embeddings; (2)
already allow for decoding a semantically correct next token in the middle
layers, but give higher probability to its version in English than in the input
language; (3) finally move into an input-language-specific region of the
embedding space. We cast these results into a conceptual model where the three
phases operate in "input space", "concept space", and "output space",
respectively. Crucially, our evidence suggests that the abstract "concept
space" lies closer to English than to other languages, which may have important
consequences regarding the biases held by multilingual language models.
\\ ( https://arxiv.org/abs/2402.10588 ,  10443kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10597
Date: Fri, 16 Feb 2024 11:30:11 GMT   (3051kb,D)

Title: Efficiency at Scale: Investigating the Performance of Diminutive
  Language Models in Clinical Tasks
Authors: Niall Taylor, Upamanyu Ghose, Omid Rohanian, Mohammadmahdi Nouriborji,
  Andrey Kormilitzin, David Clifton, Alejo Nevado-Holgado
Categories: cs.CL cs.AI
\\
  The entry of large language models (LLMs) into research and commercial spaces
has led to a trend of ever-larger models, with initial promises of
generalisability, followed by a widespread desire to downsize and create
specialised models without the need for complete fine-tuning, using Parameter
Efficient Fine-tuning (PEFT) methods. We present an investigation into the
suitability of different PEFT methods to clinical decision-making tasks, across
a range of model sizes, including extremely small models with as few as $25$
million parameters.
  Our analysis shows that the performance of most PEFT approaches varies
significantly from one task to another, with the exception of LoRA, which
maintains relatively high performance across all model sizes and tasks,
typically approaching or matching full fine-tuned performance. The
effectiveness of PEFT methods in the clinical domain is evident, particularly
for specialised models which can operate on low-cost, in-house computing
infrastructure. The advantages of these models, in terms of speed and reduced
training costs, dramatically outweighs any performance gain from large
foundation LLMs. Furthermore, we highlight how domain-specific pre-training
interacts with PEFT methods and model size, and discuss how these factors
interplay to provide the best efficiency-performance trade-off. Full code
available at: tbd.
\\ ( https://arxiv.org/abs/2402.10597 ,  3051kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10601
Date: Fri, 16 Feb 2024 11:37:05 GMT   (7269kb,D)

Title: Jailbreaking Proprietary Large Language Models using Word Substitution
  Cipher
Authors: Divij Handa, Advait Chirmule, Bimal Gajera, Chitta Baral
Categories: cs.CL cs.AI
Comments: 15 pages
\\
  Large Language Models (LLMs) are aligned to moral and ethical guidelines but
remain susceptible to creative prompts called Jailbreak that can bypass the
alignment process. However, most jailbreaking prompts contain harmful questions
in the natural language (mainly English), which can be detected by the LLM
themselves. In this paper, we present jailbreaking prompts encoded using
cryptographic techniques. We first present a pilot study on the
state-of-the-art LLM, GPT-4, in decoding several safe sentences that have been
encrypted using various cryptographic techniques and find that a
straightforward word substitution cipher can be decoded most effectively.
Motivated by this result, we use this encoding technique for writing
jailbreaking prompts. We present a mapping of unsafe words with safe words and
ask the unsafe question using these mapped words. Experimental results show an
attack success rate (up to 59.42%) of our proposed jailbreaking approach on
state-of-the-art proprietary models including ChatGPT, GPT-4, and Gemini-Pro.
Additionally, we discuss the over-defensiveness of these models. We believe
that our work will encourage further research in making these LLMs more robust
while maintaining their decoding capabilities.
\\ ( https://arxiv.org/abs/2402.10601 ,  7269kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10612
Date: Fri, 16 Feb 2024 11:55:40 GMT   (7886kb,D)

Title: Retrieve Only When It Needs: Adaptive Retrieval Augmentation for
  Hallucination Mitigation in Large Language Models
Authors: Hanxing Ding, Liang Pang, Zihao Wei, Huawei Shen, Xueqi Cheng
Categories: cs.CL
\\
  Hallucinations pose a significant challenge for the practical implementation
of large language models (LLMs). The utilization of parametric knowledge in
generating factual content is constrained by the limited knowledge of LLMs,
potentially resulting in internal hallucinations. While incorporating external
information can help fill knowledge gaps, it also introduces the risk of
irrelevant information, thereby increasing the likelihood of external
hallucinations. A careful and balanced integration of the parametric knowledge
within LLMs with external information is crucial to alleviate hallucinations.
In this study, we present Rowen, a novel approach that enhances LLMs with a
selective retrieval augmentation process tailored to address hallucinated
outputs. This process is governed by a multilingual semantic-aware detection
module, which evaluates the consistency of the perturbed responses across
various languages for the same queries. Upon detecting inconsistencies
indicative of hallucinations, Rowen activates the retrieval of external
information to rectify the model outputs. Rowen adeptly harmonizes the
intrinsic parameters in LLMs with external knowledge sources, effectively
mitigating hallucinations by ensuring a balanced integration of internal
reasoning and external evidence. Through a comprehensive empirical analysis, we
demonstrate that Rowen surpasses the current state-of-the-art in both detecting
and mitigating hallucinated content within the outputs of LLMs.
\\ ( https://arxiv.org/abs/2402.10612 ,  7886kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10614
Date: Fri, 16 Feb 2024 12:00:34 GMT   (2032kb,D)

Title: Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate
  Controllable Controversial Statements
Authors: Ming Li, Jiuhai Chen, Lichang Chen, Tianyi Zhou
Categories: cs.CL cs.AI cs.LG
\\
  Making LLMs speak for different, especially minority groups of people, and
generate statements supporting their diverse or even controversial perspectives
is critical to creating an inclusive environment. However, existing LLMs lack
sufficient controllability to the stance of their generated content, which
often contains inconsistent, neutral, or biased statements. In this paper, we
improve the controllability of LLMs in generating statements supporting an
argument the user defined in the prompt. We find that multi-round debates
between two LLMs with opposite stances generate higher-quality and more salient
statements for each, which are important training data to improve the
controllability of LLMs. Motivated by this, we develop a novel debate & tuning
("DEBATunE") pipeline finetuning LLMs to generate the statements obtained via
debate. To examine DEBATunE, we curate the largest dataset of debate topics so
far, which covers 710 controversial topics and corresponding arguments for each
topic. Evaluations by the GPT-4 judge with a novel controversy controllability
metric show that LLMs' capability of expressing diverse perspectives is
significantly improved by DEBATunE. Moreover, such controllability can be
generalized to unseen topics, generating high-quality statements supporting
controversial arguments. Our codes, models, and data will be released at
https://github.com/tianyi-lab/DEBATunE.
\\ ( https://arxiv.org/abs/2402.10614 ,  2032kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10618
Date: Fri, 16 Feb 2024 12:12:05 GMT   (7656kb,D)

Title: Enhancing Role-playing Systems through Aggressive Queries: Evaluation
  and Improvement
Authors: Yihong Tang, Jiao Ou, Che Liu, Fuzheng Zhang, Di Zhang, Kun Gai
Categories: cs.CL
\\
  The advent of Large Language Models (LLMs) has propelled dialogue generation
into new realms, particularly in the field of role-playing systems (RPSs).
While enhanced with ordinary role-relevant training dialogues, existing
LLM-based RPSs still struggle to align with roles when handling intricate and
trapped queries in boundary scenarios. In this paper, we design the Modular
ORchestrated Trap-setting Interaction SystEm (MORTISE) to benchmark and improve
the role-playing LLMs' performance. MORTISE can produce highly role-relevant
aggressive queries through the collaborative effort of multiple LLM-based
modules, and formulate corresponding responses to create an adversarial
training dataset via a consistent response generator. We select 190 Chinese and
English roles to construct aggressive queries to benchmark existing
role-playing LLMs. Through comprehensive evaluation, we find that existing
models exhibit a general deficiency in role alignment capabilities. We further
select 180 of the roles to collect an adversarial training dataset (named
RoleAD) and retain the other 10 roles for testing. Experiments on models
improved by RoleAD indicate that our adversarial dataset ameliorates this
deficiency, with the improvements demonstrating a degree of generalizability in
ordinary scenarios.
\\ ( https://arxiv.org/abs/2402.10618 ,  7656kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10631
Date: Fri, 16 Feb 2024 12:27:15 GMT   (8853kb,D)

Title: BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via
  Self-Distillation
Authors: Dayou Du, Yijia Zhang, Shijie Cao, Jiaqi Guo, Ting Cao, Xiaowen Chu,
  Ningyi Xu
Categories: cs.CL
\\
  The upscaling of Large Language Models (LLMs) has yielded impressive advances
in natural language processing, yet it also poses significant deployment
challenges. Weight quantization has emerged as a widely embraced solution to
reduce memory and computational demands. This paper introduces BitDistiller, a
framework that synergizes Quantization-Aware Training (QAT) with Knowledge
Distillation (KD) to boost the performance of LLMs at ultra-low precisions
(sub-4-bit). Specifically, BitDistiller first incorporates a tailored
asymmetric quantization and clipping technique to maximally preserve the
fidelity of quantized weights, and then proposes a novel Confidence-Aware
Kullback-Leibler Divergence (CAKLD) objective, which is employed in a
self-distillation manner to enable faster convergence and superior model
performance. Empirical evaluations demonstrate that BitDistiller significantly
surpasses existing methods in both 3-bit and 2-bit configurations on general
language understanding and complex reasoning benchmarks. Notably, BitDistiller
is shown to be more cost-effective, demanding fewer data and training
resources. The code is available at https://github.com/DD-DuDa/BitDistiller.
\\ ( https://arxiv.org/abs/2402.10631 ,  8853kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10639
Date: Fri, 16 Feb 2024 12:39:10 GMT   (1814kb,D)

Title: Generalizability of Mixture of Domain-Specific Adapters from the Lens of
  Signed Weight Directions and its Application to Effective Model Pruning
Authors: Tuc Nguyen and Thai Le
Categories: cs.CL
Comments: 18 pages, 15 figures
\\
  Several parameter-efficient fine-tuning methods based on adapters have been
proposed as a streamlined approach to incorporate not only a single specialized
knowledge into existing Pre-Trained Language Models (PLMs) but also multiple of
them at once. Recent works such as AdapterSoup propose to mix not all but only
a selective sub-set of domain-specific adapters during inference via model
weight averaging to optimize performance on novel, unseen domains with
excellent computational efficiency. However, the essential generalizability of
this emerging weight-space adapter mixing mechanism on unseen, in-domain
examples remains unexplored. Thus, in this study, we conduct a comprehensive
analysis to elucidate the generalizability of domain-specific adapter mixtures
in in-domain evaluation. We also provide investigations into the inner workings
of the mixture of domain-specific adapters by analyzing their weight signs,
yielding critical analysis on the negative correlation between their fraction
of weight sign difference and their mixtures' generalizability. All source code
will be published.
\\ ( https://arxiv.org/abs/2402.10639 ,  1814kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10643
Date: Fri, 16 Feb 2024 12:43:26 GMT   (1529kb,D)

Title: `Keep it Together': Enforcing Cohesion in Extractive Summaries by
  Simulating Human Memory
Authors: Ronald Cardenas and Matthias Galle and Shay B. Cohen
Categories: cs.CL cs.AI
\\
  Extractive summaries are usually presented as lists of sentences with no
expected cohesion between them. In this paper, we aim to enforce cohesion
whilst controlling for informativeness and redundancy in summaries, in cases
where the input exhibits high redundancy. The pipeline controls for redundancy
in long inputs as it is consumed, and balances informativeness and cohesion
during sentence selection. Our sentence selector simulates human memory to keep
track of topics --modeled as lexical chains--, enforcing cohesive ties between
noun phrases. Across a variety of domains, our experiments revealed that it is
possible to extract highly cohesive summaries that nevertheless read as
informative to humans as summaries extracted by only accounting for
informativeness or redundancy. The extracted summaries exhibit smooth topic
transitions between sentences as signaled by lexical chains, with chains
spanning adjacent or near-adjacent sentences.
\\ ( https://arxiv.org/abs/2402.10643 ,  1529kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10645
Date: Fri, 16 Feb 2024 12:46:16 GMT   (3523kb,D)

Title: Can Separators Improve Chain-of-Thought Prompting?
Authors: Yoonjeong Park, Hyunjin Kim, Chanyeol Choi, Junseong Kim, Jy-yong Sohn
Categories: cs.CL cs.AI
\\
  Chain-of-thought (CoT) prompting is a simple and effective method for
improving the reasoning capabilities of Large language models (LLMs). The basic
idea of CoT is to let LLMs break down their thought processes step-by-step by
putting exemplars in the input prompt. However, the densely structured prompt
exemplars of CoT may cause the cognitive overload of LLMs. Inspired by human
cognition, we introduce CoT-Sep, a novel method that strategically employs
separators at the end of each exemplar in CoT prompting. These separators are
designed to help the LLMs understand their thought processes better while
reasoning. It turns out that CoT-Sep significantly improves the LLMs'
performances on complex reasoning tasks (e.g., GSM-8K, AQuA, CSQA), compared
with the vanilla CoT, which does not use separators. We also study the effects
of the type and the location of separators tested on multiple LLMs, including
GPT-3.5-Turbo, GPT-4, and LLaMA-2 7B. Interestingly, the type/location of
separators should be chosen appropriately to boost the reasoning capability of
CoT.
\\ ( https://arxiv.org/abs/2402.10645 ,  3523kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10646
Date: Fri, 16 Feb 2024 12:47:11 GMT   (904kb,D)

Title: AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation
  Tuning with Plausibility Estimation
Authors: Zhaowei Wang, Wei Fan, Qing Zong, Hongming Zhang, Sehyun Choi,
  Tianqing Fang, Xin Liu, Yangqiu Song, Ginny Y. Wong, Simon See
Categories: cs.CL
\\
  Abstraction ability is crucial in human intelligence, which can also benefit
various tasks in NLP study. Existing work shows that LLMs are deficient in
abstract ability, and how to improve it remains unexplored. In this work, we
design the framework AbsInstruct to enhance LLMs' abstraction ability through
instruction tuning. The framework builds instructions with in-depth
explanations to assist LLMs in capturing the underlying rationale of
abstraction. Meanwhile, we introduce a plausibility estimator to select
instructions that are more consistent with the abstraction knowledge of LLMs to
be aligned. Then, our framework combines abstraction instructions with
general-purpose ones to build a hybrid dataset. Extensive experiments and
analyses demonstrate that our framework can considerably enhance LLMs'
abstraction ability with strong generalization performance while maintaining
their general instruction-following abilities.
\\ ( https://arxiv.org/abs/2402.10646 ,  904kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10654
Date: Fri, 16 Feb 2024 13:02:11 GMT   (310kb,D)

Title: Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning
  Processes
Authors: Dingzirui Wang, Longxu Dou, Xuanliang Zhang, Qingfu Zhu, Wanxiang Che
Categories: cs.CL
\\
  Numerical reasoning is an essential ability for NLP systems to handle numeric
information. Recent research indicates that fine-tuning a small-scale model to
learn generating reasoning processes alongside answers can significantly
enhance performance. However, current methods have the limitation that most
methods generate reasoning processes with large language models (LLMs), which
are "unreliable" since such processes could contain information unrelated to
the answer. To address this limitation, we introduce Enhancing NumeriCal
reasOning with Reliable procEsses (Encore), which derives the reliable
reasoning process by decomposing the answer formula, ensuring which fully
supports the answer. Nevertheless, models could lack enough data to learn the
reasoning process generation adequately, since our method generates only one
single reasoning process for one formula. To overcome this difficulty, we
present a series of pre-training tasks to help models learn the reasoning
process generation with synthesized data. The experiments show that Encore
yields improvement on all five experimental datasets with an average of 1.8%,
proving the effectiveness of our method.
\\ ( https://arxiv.org/abs/2402.10654 ,  310kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10662
Date: Fri, 16 Feb 2024 13:11:13 GMT   (884kb,D)

Title: Fine Tuning Named Entity Extraction Models for the Fantasy Domain
Authors: Aravinth Sivaganeshan, Nisansa de Silva
Categories: cs.CL
DOI: 10.1109/MERCon60487.2023.10355501
\\
  Named Entity Recognition (NER) is a sequence classification Natural Language
Processing task where entities are identified in the text and classified into
predefined categories. It acts as a foundation for most information extraction
systems. Dungeons and Dragons (D&D) is an open-ended tabletop fantasy game with
its own diverse lore. DnD entities are domain-specific and are thus
unrecognizable by even the state-of-the-art off-the-shelf NER systems as the
NER systems are trained on general data for pre-defined categories such as:
person (PERS), location (LOC), organization (ORG), and miscellaneous (MISC).
For meaningful extraction of information from fantasy text, the entities need
to be classified into domain-specific entity categories as well as the models
be fine-tuned on a domain-relevant corpus. This work uses available lore of
monsters in the D&D domain to fine-tune Trankit, which is a prolific NER
framework that uses a pre-trained model for NER. Upon this training, the system
acquires the ability to extract monster names from relevant domain documents
under a novel NER tag. This work compares the accuracy of the monster name
identification against; the zero-shot Trankit model and two FLAIR models. The
fine-tuned Trankit model achieves an 87.86% F1 score surpassing all the other
considered models.
\\ ( https://arxiv.org/abs/2402.10662 ,  884kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10663
Date: Fri, 16 Feb 2024 13:13:18 GMT   (310kb,D)

Title: Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL
Authors: Dingzirui Wang, Longxu Dou, Xuanliang Zhang, Qingfu Zhu, Wanxiang Che
Categories: cs.CL
\\
  Currently, the in-context learning method based on large language models
(LLMs) has become the mainstream of text-to-SQL research. Previous works have
discussed how to select demonstrations related to the user question from a
human-labeled demonstration pool. However, human labeling suffers from the
limitations of insufficient diversity and high labeling overhead. Therefore, in
this paper, we discuss how to measure and improve the diversity of the
demonstrations for text-to-SQL. We present a metric to measure the diversity of
the demonstrations and analyze the insufficient of the existing labeled data by
experiments. Based on the above discovery, we propose fusing iteratively for
demonstrations (Fused) to build a high-diversity demonstration pool through
human-free multiple-iteration synthesis, improving diversity and lowering label
cost. Our method achieves an average improvement of 3.2% and 5.0% with and
without human labeling on several mainstream datasets, which proves the
effectiveness of Fused.
\\ ( https://arxiv.org/abs/2402.10663 ,  310kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10666
Date: Fri, 16 Feb 2024 13:14:35 GMT   (336kb,D)

Title: Multi-Hop Table Retrieval for Open-Domain Text-to-SQL
Authors: Xuanliang Zhang, Dingzirui Wang, Longxu Dou, Qingfu Zhu, Wanxiang Che
Categories: cs.CL
\\
  Open-domain text-to-SQL is an important task that retrieves question-relevant
tables from massive databases and then generates SQL. However, existing
retrieval methods that retrieve in a single hop do not pay attention to the
text-to-SQL challenge of schema linking, which is aligning the entities in the
question with table entities, reflected in two aspects: similar irrelevant
entity and domain mismatch entity. Therefore, we propose our method, the
multi-hop table retrieval with rewrite and beam search (Murre). To reduce the
effect of the similar irrelevant entity, our method focuses on unretrieved
entities at each hop and considers the low-ranked tables by beam search. To
alleviate the limitation of domain mismatch entity, Murre rewrites the question
based on retrieved tables in multiple hops, decreasing the domain gap with
relevant tables. We conduct experiments on SpiderUnion and BirdUnion+, reaching
new state-of-the-art results with an average improvement of 6.38%.
\\ ( https://arxiv.org/abs/2402.10666 ,  336kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10669
Date: Fri, 16 Feb 2024 13:21:06 GMT   (8623kb,D)

Title: Humans or LLMs as the Judge? A Study on Judgement Biases
Authors: Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, Benyou Wang
Categories: cs.CL
Comments: 18 pages
\\
  Adopting human and large language models (LLM) as judges (\textit{a.k.a}
human- and LLM-as-a-judge) for evaluating the performance of existing LLMs has
recently gained attention. Nonetheless, this approach concurrently introduces
potential biases from human and LLM judges, questioning the reliability of the
evaluation results. In this paper, we propose a novel framework for
investigating 5 types of biases for LLM and human judges. We curate a dataset
with 142 samples referring to the revised Bloom's Taxonomy and conduct
thousands of human and LLM evaluations. Results show that human and LLM judges
are vulnerable to perturbations to various degrees, and that even the most
cutting-edge judges possess considerable biases. We further exploit their
weakness and conduct attacks on LLM judges. We hope that our work can notify
the community of the vulnerability of human- and LLM-as-a-judge against
perturbations, as well as the urgency of developing robust evaluation systems.
\\ ( https://arxiv.org/abs/2402.10669 ,  8623kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10670
Date: Fri, 16 Feb 2024 13:21:33 GMT   (12905kb,D)

Title: OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via
  Vision-Language Foundation Models
Authors: Yuxuan Kuang, Hai Lin, Meng Jiang
Categories: cs.CL cs.RO
Comments: First submitted on Dec. 15, 2023
\\
  Object navigation (ObjectNav) requires an agent to navigate through unseen
environments to find queried objects. Many previous methods attempted to solve
this task by relying on supervised or reinforcement learning, where they are
trained on limited household datasets with close-set objects. However, two key
challenges are unsolved: understanding free-form natural language instructions
that demand open-set objects, and generalizing to new environments in a
zero-shot manner. Aiming to solve the two challenges, in this paper, we propose
OpenFMNav, an Open-set Foundation Model based framework for zero-shot object
Navigation. We first unleash the reasoning abilities of large language models
(LLMs) to extract proposed objects from natural language instructions that meet
the user's demand. We then leverage the generalizability of large vision
language models (VLMs) to actively discover and detect candidate objects from
the scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting
common sense reasoning on VSSM, our method can perform effective
language-guided exploration and exploitation of the scene and finally reach the
goal. By leveraging the reasoning and generalizing abilities of foundation
models, our method can understand free-form human instructions and perform
effective open-set zero-shot navigation in diverse environments. Extensive
experiments on the HM3D ObjectNav benchmark show that our method surpasses all
the strong baselines on all metrics, proving our method's effectiveness.
Furthermore, we perform real robot demonstrations to validate our method's
open-set-ness and generalizability to real-world environments.
\\ ( https://arxiv.org/abs/2402.10670 ,  12905kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10671
Date: Fri, 16 Feb 2024 13:24:05 GMT   (4156kb,D)

Title: Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL
  through Workflow Paradigm
Authors: Yuanzhen Xie, Xinzhou Jin, Tao Xie, MingXiong Lin, Liang Chen, Chenyun
  Yu, Lei Cheng, ChengXiang Zhuo, Bo Hu, Zang Li
Categories: cs.CL
\\
  In-context learning of large-language models (LLMs) has achieved remarkable
success in the field of natural language processing, while extensive case
studies reveal that the single-step chain-of-thought prompting approach faces
challenges such as attention diffusion and inadequate performance in complex
tasks like text-to-SQL. To improve the contextual learning capabilities of LLMs
in text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the
attention and problem-solving scope of LLMs through decomposition.
Specifically, the information determination module for eliminating redundant
information and the brand-new prompt structure based on problem classification
greatly enhance the model's attention. Additionally, the inclusion of
self-correcting and active learning modules greatly expands the problem-solving
scope of LLMs, hence improving the upper limit of LLM-based approaches.
Extensive experiments conducted on three datasets demonstrate that our approach
outperforms other methods by a significant margin. About 2-3 percentage point
improvements compared to the existing baseline on the Spider Dev and
Spider-Realistic datasets and new SOTA results on the Spider Test dataset are
achieved. Our code is available on GitHub:
\url{https://github.com/FlyingFeather/DEA-SQL}.
\\ ( https://arxiv.org/abs/2402.10671 ,  4156kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10675
Date: Fri, 16 Feb 2024 13:28:44 GMT   (8884kb,D)

Title: German Text Simplification: Finetuning Large Language Models with
  Semi-Synthetic Data
Authors: Lars Kl\"oser, Mika Beele, Jan-Niklas Schagen, Bodo Kraft
Categories: cs.CL
Comments: Accepted at Fourth Workshop on Language Technology for Equality,
  Diversity, Inclusion - EACL 2024
ACM-class: I.2.7
\\
  This study pioneers the use of synthetically generated data for training
generative models in document-level text simplification of German texts. We
demonstrate the effectiveness of our approach with real-world online texts.
Addressing the challenge of data scarcity in language simplification, we
crawled professionally simplified German texts and synthesized a corpus using
GPT-4. We finetune Large Language Models with up to 13 billion parameters on
this data and evaluate their performance. This paper employs various
methodologies for evaluation and demonstrates the limitations of currently used
rule-based metrics. Both automatic and manual evaluations reveal that our
models can significantly simplify real-world online texts, indicating the
potential of synthetic data in improving text simplification.
\\ ( https://arxiv.org/abs/2402.10675 ,  8884kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10685
Date: Fri, 16 Feb 2024 13:39:34 GMT   (1961kb,D)

Title: LongHeads: Multi-Head Attention is Secretly a Long Context Processor
Authors: Yi Lu, Xin Zhou, Wei He, Jun Zhao, Tao Ji, Tao Gui, Qi Zhang, Xuanjing
  Huang
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) have achieved impressive performance in numerous
domains but often struggle to process lengthy inputs effectively and
efficiently due to limited length generalization and attention's quadratic
computational demands. Many sought to mitigate this by restricting the
attention window within the pre-trained length. However, these methods
introduce new issues such as ignoring the middle context and requiring
additional training. To address these problems, we propose LongHeads, a
training-free framework that enhances LLM's long context ability by unlocking
multi-head attention's untapped potential. Instead of allowing each head to
attend to the full sentence, which struggles with generalizing to longer
sequences due to out-of-distribution (OOD) issues, we allow each head to
process in-distribution length by selecting and attending to important context
chunks. To this end, we propose a chunk selection strategy that relies on the
inherent correlation between the query and the key representations, efficiently
distributing context chunks to different heads. In this way, each head ensures
it can effectively process attended tokens within the trained length, while
different heads in different layers can collectively process longer contexts.
LongHeads works efficiently in linear time, fits seamlessly with many LLMs that
use relative positional encoding. Our extensive empirical analyses verify
LongHeads's efficacy in extending the usable context window for existing
models, showcasing its promise for enhancing long text understanding.
\\ ( https://arxiv.org/abs/2402.10685 ,  1961kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10688
Date: Fri, 16 Feb 2024 13:46:06 GMT   (166kb,D)

Title: Opening the Black Box of Large Language Models: Two Views on Holistic
  Interpretability
Authors: Haiyan Zhao, Fan Yang, Himabindu Lakkaraju, Mengnan Du
Categories: cs.CL
Comments: 8 pages, 1 figure
\\
  As large language models (LLMs) grow more powerful, concerns around potential
harms like toxicity, unfairness, and hallucination threaten user trust.
Ensuring beneficial alignment of LLMs with human values through model alignment
is thus critical yet challenging, requiring a deeper understanding of LLM
behaviors and mechanisms. We propose opening the black box of LLMs through a
framework of holistic interpretability encompassing complementary bottom-up and
top-down perspectives. The bottom-up view, enabled by mechanistic
interpretability, focuses on component functionalities and training dynamics.
The top-down view utilizes representation engineering to analyze behaviors
through hidden representations. In this paper, we review the landscape around
mechanistic interpretability and representation engineering, summarizing
approaches, discussing limitations and applications, and outlining future
challenges in using these techniques to achieve ethical, honest, and reliable
reasoning aligned with human values.
\\ ( https://arxiv.org/abs/2402.10688 ,  166kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10689
Date: Fri, 16 Feb 2024 13:46:38 GMT   (1124kb,D)

Title: Multi-Cultural Commonsense Knowledge Distillation
Authors: Tuan-Phong Nguyen, Simon Razniewski, Gerhard Weikum
Categories: cs.CL
Comments: 20 pages, 5 figures, 13 tables
\\
  Despite recent progress, large language models (LLMs) still face the
challenge of appropriately reacting to the intricacies of social and cultural
conventions. This paper presents MANGO, a methodology for distilling
high-accuracy, high-recall assertions of cultural knowledge. We judiciously and
iteratively prompt LLMs for this purpose from two entry points, concepts and
cultures. Outputs are consolidated via clustering and generative summarization.
Running the MANGO method with GPT-3.5 as underlying LLM yields 167K
high-accuracy assertions for 30K concepts and 11K cultures, surpassing prior
resources by a large margin. For extrinsic evaluation, we explore augmenting
dialogue systems with cultural knowledge assertions. We find that adding
knowledge from MANGO improves the overall quality, specificity, and cultural
sensitivity of dialogue responses, as judged by human annotators. Data and code
are available for download.
\\ ( https://arxiv.org/abs/2402.10689 ,  1124kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10691
Date: Fri, 16 Feb 2024 13:48:06 GMT   (8323kb,D)

Title: MultiPoT: Multilingual Program of Thoughts Harnesses Multiple
  Programming Languages
Authors: Xianzhen Luo, Qingfu Zhu, Zhiming Zhang, Libo Qin, Xu Wang, Qing Yang,
  Dongliang Xu, Wanxiang Che
Categories: cs.CL
Comments: under review
\\
  Program of Thoughts (PoT) is an approach characterized by its executable
intermediate steps, which ensure the accuracy of the numerical calculations in
the reasoning process. Currently, PoT primarily uses Python. However, relying
solely on a single language may result in suboptimal solutions and overlook the
potential benefits of other programming languages. In this paper, we conduct
comprehensive experiments on the programming languages used in PoT and find
that no single language consistently delivers optimal performance across all
tasks and models. The effectiveness of each language varies depending on the
specific scenarios. Inspired by this, we propose a task and model agnostic
approach called MultiPoT, which harnesses strength and diversity from various
languages. Experimental results reveal that it significantly outperforms Python
Self-Consistency. Furthermore, it achieves comparable or superior performance
compared to the best monolingual PoT in almost all tasks across all models. In
particular, MultiPoT achieves more than 4.6\% improvement on average on both
Starcoder and ChatGPT (gpt-3.5-turbo).
\\ ( https://arxiv.org/abs/2402.10691 ,  8323kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10693
Date: Fri, 16 Feb 2024 13:53:26 GMT   (3904kb,D)

Title: Exploring Precision and Recall to assess the quality and diversity of
  LLMs
Authors: Le Bronnec Florian, Verine Alexandre, Negrevergne Benjamin, Chevaleyre
  Yann, Allauzen Alexandre
Categories: cs.CL cs.LG
Comments: 21 pages, 15 figures, Under Review
\\
  This paper introduces a novel evaluation framework for Large Language Models
(LLMs) such as Llama-2 and Mistral, focusing on the adaptation of Precision and
Recall metrics from image generation to text generation. This approach allows
for a nuanced assessment of the quality and diversity of generated text without
the need for aligned corpora. By conducting a comprehensive evaluation of
state-of-the-art language models, the study reveals significant insights into
their performance on open-ended generation tasks, which are not adequately
captured by traditional benchmarks. The findings highlight a trade-off between
the quality and diversity of generated samples, particularly when models are
fine-tuned with human feedback. This work extends the toolkit for
distribution-based NLP evaluation, offering insights into the practical
capabilities and challenges faced by current LLMs in generating diverse and
high-quality text.
\\ ( https://arxiv.org/abs/2402.10693 ,  3904kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10699
Date: Fri, 16 Feb 2024 14:00:56 GMT   (306kb,D)

Title: Rethinking Human-like Translation Strategy: Integrating Drift-Diffusion
  Model with Large Language Models for Machine Translation
Authors: Hongbin Na, Zimu Wang, Mieradilijiang Maimaiti, Tong Chen, Wei Wang,
  Tao Shen, Ling Chen
Categories: cs.CL
Comments: Under review
\\
  Large language models (LLMs) have demonstrated promising potential in various
downstream tasks, including machine translation. However, prior work on
LLM-based machine translation has mainly focused on better utilizing training
data, demonstrations, or pre-defined and universal knowledge to improve
performance, with a lack of consideration of decision-making like human
translators. In this paper, we incorporate Thinker with the Drift-Diffusion
Model (Thinker-DDM) to address this issue. We then redefine the Drift-Diffusion
process to emulate human translators' dynamic decision-making under constrained
resources. We conduct extensive experiments under the high-resource,
low-resource, and commonsense translation settings using the WMT22 and CommonMT
datasets, in which Thinker-DDM outperforms baselines in the first two
scenarios. We also perform additional analysis and evaluation on commonsense
translation to illustrate the high effectiveness and efficacy of the proposed
method.
\\ ( https://arxiv.org/abs/2402.10699 ,  306kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10712
Date: Fri, 16 Feb 2024 14:15:15 GMT   (634kb,D)

Title: An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient
  Generative LLM Inference
Authors: Atsuki Yamaguchi, Aline Villavicencio, Nikolaos Aletras
Categories: cs.CL cs.AI
\\
  The development of state-of-the-art generative large language models (LLMs)
disproportionately relies on English-centric tokenizers, vocabulary and
pre-training data. Despite the fact that some LLMs have multilingual
capabilities, recent studies have shown that their inference efficiency
deteriorates when generating text in languages other than English. This results
in increased inference time and costs. Cross-lingual vocabulary adaptation
methods have been proposed for adapting models to a target language aiming to
improve downstream performance. However, the effectiveness of these methods on
increasing inference efficiency of generative LLMs has yet to be explored. In
this paper, we perform an empirical study of various cross-lingual vocabulary
adaptation methods on five generative LLMs (including monolingual and
multilingual models) across four typologically-diverse languages and four
natural language understanding tasks. We find that cross-lingual vocabulary
adaptation substantially contributes to LLM inference speedups of up to 271.5%.
We also show that adapting LLMs that have been pre-trained on more balanced
multilingual data results in downstream performance comparable to the original
models.
\\ ( https://arxiv.org/abs/2402.10712 ,  634kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10735
Date: Fri, 16 Feb 2024 14:52:05 GMT   (215kb,D)

Title: Assessing the Reasoning Abilities of ChatGPT in the Context of Claim
  Verification
Authors: John Dougrez-Lewis, Mahmud Elahi Akhter, Yulan He, Maria Liakata
Categories: cs.CL
Comments: 20 pages, 1 figure
\\
  The reasoning capabilities of LLMs are currently hotly debated. We examine
the issue from the perspective of claim/rumour verification. We propose the
first logical reasoning framework designed to break down any claim or rumor
paired with evidence into the atomic reasoning steps necessary for
verification. Based on our framework, we curate two annotated collections of
such claim/evidence pairs: a synthetic dataset from Wikipedia and a real-world
set stemming from rumours circulating on Twitter. We use them to evaluate the
reasoning capabilities of GPT-3.5-Turbo and GPT-4 (hereinafter referred to as
ChatGPT) within the context of our framework, providing a thorough analysis.
Our results show that ChatGPT struggles in abductive reasoning, although this
can be somewhat mitigated by using manual Chain of Thought (CoT) as opposed to
Zero Shot (ZS) and ZS CoT approaches. Our study contributes to the growing body
of research suggesting that ChatGPT's reasoning processes are unlikely to
mirror human-like reasoning, and that LLMs need to be more rigorously evaluated
in order to distinguish between hype and actual capabilities, especially in
high stake real-world tasks such as claim verification.
\\ ( https://arxiv.org/abs/2402.10735 ,  215kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10738
Date: Fri, 16 Feb 2024 14:55:33 GMT   (595kb,D)

Title: Let's Learn Step by Step: Enhancing In-Context Learning Ability with
  Curriculum Learning
Authors: Yinpeng Liu and Jiawei Liu and Xiang Shi and Qikai Cheng and Wei Lu
Categories: cs.CL
\\
  Demonstration ordering, which is an important strategy for in-context
learning (ICL), can significantly affects the performance of large language
models (LLMs). However, most of the current approaches of ordering require
additional knowledge and similarity calculation. We advocate the few-shot
in-context curriculum learning (ICCL), a simple but effective demonstration
ordering method for ICL, which implies gradually increasing the complexity of
prompt demonstrations during the inference process. Then we design three
experiments to discuss the effectiveness of ICCL, the formation mechanism of
LLM's ICCL capability, and the impact of ordering subjects. Experimental
results demonstrate that ICCL, developed during the instruction-tuning stage,
is effective for open-source LLMs. Moreover, LLMs exhibit a weaker capacity
compared to humans in discerning the difficulty levels of demonstrations. We
release our code at https://github.com/61peng/curri_learning.
\\ ( https://arxiv.org/abs/2402.10738 ,  595kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10743
Date: Fri, 16 Feb 2024 14:59:55 GMT   (561kb)

Title: Construction of a Syntactic Analysis Map for Yi Shui School through Text
  Mining and Natural Language Processing Research
Authors: Hanqing Zhao and Yuehan Li
Categories: cs.CL
\\
  Entity and relationship extraction is a crucial component in natural language
processing tasks such as knowledge graph construction, question answering
system design, and semantic analysis. Most of the information of the Yishui
school of traditional Chinese Medicine (TCM) is stored in the form of
unstructured classical Chinese text. The key information extraction of TCM
texts plays an important role in mining and studying the academic schools of
TCM. In order to solve these problems efficiently using artificial intelligence
methods, this study constructs a word segmentation and entity relationship
extraction model based on conditional random fields under the framework of
natural language processing technology to identify and extract the entity
relationship of traditional Chinese medicine texts, and uses the common
weighting technology of TF-IDF information retrieval and data mining to extract
important key entity information in different ancient books. The dependency
syntactic parser based on neural network is used to analyze the grammatical
relationship between entities in each ancient book article, and it is
represented as a tree structure visualization, which lays the foundation for
the next construction of the knowledge graph of Yishui school and the use of
artificial intelligence methods to carry out the research of TCM academic
schools.
\\ ( https://arxiv.org/abs/2402.10743 ,  561kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10744
Date: Fri, 16 Feb 2024 15:01:24 GMT   (1386kb,D)

Title: GenRES: Rethinking Evaluation for Generative Relation Extraction in the
  Era of Large Language Models
Authors: Pengcheng Jiang, Jiacheng Lin, Zifeng Wang, Jimeng Sun, Jiawei Han
Categories: cs.CL cs.AI
\\
  The field of relation extraction (RE) is experiencing a notable shift towards
generative relation extraction (GRE), leveraging the capabilities of large
language models (LLMs). However, we discovered that traditional relation
extraction (RE) metrics like precision and recall fall short in evaluating GRE
methods. This shortfall arises because these metrics rely on exact matching
with human-annotated reference relations, while GRE methods often produce
diverse and semantically accurate relations that differ from the references. To
fill this gap, we introduce GenRES for a multi-dimensional assessment in terms
of the topic similarity, uniqueness, granularity, factualness, and completeness
of the GRE results. With GenRES, we empirically identified that (1)
precision/recall fails to justify the performance of GRE methods; (2)
human-annotated referential relations can be incomplete; (3) prompting LLMs
with a fixed set of relations or entities can cause hallucinations. Next, we
conducted a human evaluation of GRE methods that shows GenRES is consistent
with human preferences for RE quality. Last, we made a comprehensive evaluation
of fourteen leading LLMs using GenRES across document, bag, and sentence level
RE datasets, respectively, to set the benchmark for future research in GRE
\\ ( https://arxiv.org/abs/2402.10744 ,  1386kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10753
Date: Fri, 16 Feb 2024 15:19:46 GMT   (9098kb,D)

Title: ToolSword: Unveiling Safety Issues of Large Language Models in Tool
  Learning Across Three Stages
Authors: Junjie Ye, Sixian Li, Guanyu Li, Caishuang Huang, Songyang Gao, Yilong
  Wu, Qi Zhang, Tao Gui, Xuanjing Huang
Categories: cs.CL cs.AI
\\
  Tool learning is widely acknowledged as a foundational approach or deploying
large language models (LLMs) in real-world scenarios. While current research
primarily emphasizes leveraging tools to augment LLMs, it frequently neglects
emerging safety considerations tied to their application. To fill this gap, we
present $ToolSword$, a comprehensive framework dedicated to meticulously
investigating safety issues linked to LLMs in tool learning. Specifically,
ToolSword delineates six safety scenarios for LLMs in tool learning,
encompassing $malicious$ $queries$ and $jailbreak$ $attacks$ in the input
stage, $noisy$ $misdirection$ and $risky$ $cues$ in the execution stage, and
$harmful$ $feedback$ and $error$ $conflicts$ in the output stage. Experiments
conducted on 11 open-source and closed-source LLMs reveal enduring safety
challenges in tool learning, such as handling harmful queries, employing risky
tools, and delivering detrimental feedback, which even GPT-4 is susceptible to.
Moreover, we conduct further studies with the aim of fostering research on tool
learning safety. The data is released in
https://github.com/Junjie-Ye/ToolSword.
\\ ( https://arxiv.org/abs/2402.10753 ,  9098kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10767
Date: Fri, 16 Feb 2024 15:41:23 GMT   (2371kb,D)

Title: Inference to the Best Explanation in Large Language Models
Authors: Dhairya Dalal, Marco Valentino, Andr\'e Freitas, and Paul Buitelaar
Categories: cs.CL cs.AI
ACM-class: I.2.7
\\
  While Large Language Models (LLMs) have found success in real-world
applications, their underlying explanatory process is still poorly understood.
This paper proposes IBE-Eval, a framework inspired by philosophical accounts on
Inference to the Best Explanation (IBE) to advance the interpretation and
evaluation of LLMs' explanations. IBE-Eval estimates the plausibility of
natural language explanations through a combination of explicit logical and
linguistic features including: consistency, parsimony, coherence, and
uncertainty. Extensive experiments are conducted on Causal Question Answering
(CQA), where \textit{IBE-Eval} is tasked to select the most plausible causal
explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama
2). The experiments reveal that IBE-Eval can successfully identify the best
explanation with up to 77\% accuracy ($\approx 27\%$ above random), improving
upon a GPT 3.5-as-a-Judge baseline ($\approx+17\%$) while being intrinsically
more efficient and interpretable. Additional analyses suggest that, despite
model-specific variances, LLM-generated explanations tend to conform to IBE
criteria and that IBE-Eval is significantly correlated with human judgment,
opening up opportunities for future development of automated explanation
verification tools.
\\ ( https://arxiv.org/abs/2402.10767 ,  2371kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10769
Date: Fri, 16 Feb 2024 15:48:24 GMT   (178kb,D)

Title: Distillation Enhanced Generative Retrieval
Authors: Yongqi Li, Zhen Zhang, Wenjie Wang, Liqiang Nie, Wenjie Li, Tat-Seng
  Chua
Categories: cs.CL cs.AI cs.IR
\\
  Generative retrieval is a promising new paradigm in text retrieval that
generates identifier strings of relevant passages as the retrieval target. This
paradigm leverages powerful generative language models, distinct from
traditional sparse or dense retrieval methods. In this work, we identify a
viable direction to further enhance generative retrieval via distillation and
propose a feasible framework, named DGR. DGR utilizes sophisticated ranking
models, such as the cross-encoder, in a teacher role to supply a passage rank
list, which captures the varying relevance degrees of passages instead of
binary hard labels; subsequently, DGR employs a specially designed distilled
RankNet loss to optimize the generative retrieval model, considering the
passage rank order provided by the teacher model as labels. This framework only
requires an additional distillation step to enhance current generative
retrieval systems and does not add any burden to the inference stage. We
conduct experiments on four public datasets, and the results indicate that DGR
achieves state-of-the-art performance among the generative retrieval methods.
Additionally, DGR demonstrates exceptional robustness and generalizability with
various teacher models and distillation losses.
\\ ( https://arxiv.org/abs/2402.10769 ,  178kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10770
Date: Fri, 16 Feb 2024 15:48:33 GMT   (704kb,D)

Title: How Reliable Are Automatic Evaluation Methods for Instruction-Tuned
  LLMs?
Authors: Ehsan Doostmohammadi, Oskar Holmstr\"om, Marco Kuhlmann
Categories: cs.CL cs.AI
\\
  Work on instruction-tuned Large Language Models (LLMs) has used automatic
methods based on text overlap and LLM judgments as cost-effective alternatives
to human evaluation. In this paper, we study the reliability of such methods
across a broad range of tasks and in a cross-lingual setting. In contrast to
previous findings, we observe considerable variability in correlations between
automatic methods and human evaluators when scores are differentiated by task
type. Specifically, the widely-used ROUGE-L metric strongly correlates with
human judgments for short-answer English tasks but is unreliable in free-form
generation tasks and cross-lingual transfer. The effectiveness of GPT-4 as an
evaluator depends on including reference answers when prompting for
assessments, which can lead to overly strict evaluations in free-form
generation tasks. In summary, we find that, while automatic evaluation methods
can approximate human judgements under specific conditions, their reliability
is highly context-dependent. Our findings enhance the understanding of how
automatic methods should be applied and interpreted when developing and
evaluating instruction-tuned LLMs.
\\ ( https://arxiv.org/abs/2402.10770 ,  704kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10772
Date: Fri, 16 Feb 2024 15:54:24 GMT   (6986kb,D)

Title: Enhancing ESG Impact Type Identification through Early Fusion and
  Multilingual Models
Authors: Hariram Veeramani, Surendrabikram Thapa, Usman Naseem
Categories: cs.CL
Comments: Accepted to FinNLP workshop at IJCNLP-ACL 2023
\\
  In the evolving landscape of Environmental, Social, and Corporate Governance
(ESG) impact assessment, the ML-ESG-2 shared task proposes identifying ESG
impact types. To address this challenge, we present a comprehensive system
leveraging ensemble learning techniques, capitalizing on early and late fusion
approaches. Our approach employs four distinct models: mBERT, FlauBERT-base,
ALBERT-base-v2, and a Multi-Layer Perceptron (MLP) incorporating Latent
Semantic Analysis (LSA) and Term Frequency-Inverse Document Frequency (TF-IDF)
features. Through extensive experimentation, we find that our early fusion
ensemble approach, featuring the integration of LSA, TF-IDF, mBERT,
FlauBERT-base, and ALBERT-base-v2, delivers the best performance. Our system
offers a comprehensive ESG impact type identification solution, contributing to
the responsible and sustainable decision-making processes vital in today's
financial and corporate governance landscape.
\\ ( https://arxiv.org/abs/2402.10772 ,  6986kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10779
Date: Fri, 16 Feb 2024 16:02:33 GMT   (2699kb,D)

Title: A Condensed Transition Graph Framework for Zero-shot Link Prediction
  with Large Language Models
Authors: Mingchen Li, Chen Ling, Rui Zhang, Liang Zhao
Categories: cs.CL
\\
  Zero-shot link prediction (ZSLP) on knowledge graphs aims at automatically
identifying relations between given entities. Existing methods primarily employ
auxiliary information to predict tail entity given head entity and its
relation, yet face challenges due to the occasional unavailability of such
detailed information and the inherent simplicity of predicting tail entities
based on semantic similarities. Even though Large Language Models (LLMs) offer
a promising solution to predict unobserved relations between the head and tail
entity in a zero-shot manner, their performance is still restricted due to the
inability to leverage all the (exponentially many) paths' information between
two entities, which are critical in collectively indicating their relation
types. To address this, in this work, we introduce a Condensed Transition Graph
Framework for Zero-Shot Link Prediction (CTLP), which encodes all the paths'
information in linear time complexity to predict unseen relations between
entities, attaining both efficiency and information preservation. Specifically,
we design a condensed transition graph encoder with theoretical guarantees on
its coverage, expressiveness, and efficiency. It is learned by a transition
graph contrastive learning strategy. Subsequently, we design a soft instruction
tuning to learn and map the all-path embedding to the input of LLMs.
Experimental results show that our proposed CTLP method achieves
state-of-the-art performance on three standard ZSLP datasets
\\ ( https://arxiv.org/abs/2402.10779 ,  2699kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10790
Date: Fri, 16 Feb 2024 16:15:01 GMT   (6329kb,D)

Title: In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs
  Miss
Authors: Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom
  Sorokin, Mikhail Burtsev
Categories: cs.CL cs.AI cs.LG
\\
  This paper addresses the challenge of processing long documents using
generative transformer models. To evaluate different approaches, we introduce
BABILong, a new benchmark designed to assess model capabilities in extracting
and processing distributed facts within extensive texts. Our evaluation, which
includes benchmarks for GPT-4 and RAG, reveals that common methods are
effective only for sequences up to $10^4$ elements. In contrast, fine-tuning
GPT-2 with recurrent memory augmentations enables it to handle tasks involving
up to $10^7$ elements. This achievement marks a substantial leap, as it is by
far the longest input processed by any open neural network model to date,
demonstrating a significant improvement in the processing capabilities for long
sequences.
\\ ( https://arxiv.org/abs/2402.10790 ,  6329kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10811
Date: Fri, 16 Feb 2024 16:35:35 GMT   (906kb,D)

Title: Quantifying the Persona Effect in LLM Simulations
Authors: Tiancheng Hu and Nigel Collier
Categories: cs.CL cs.CY
\\
  Large language models (LLMs) have shown remarkable promise in simulating
human language use and behavior. In this study, we delve into the intersection
of persona variables and the capability of LLMs to simulate different
perspectives. We find that persona variables can explain <10\% variance in
annotations in existing subjective NLP datasets. Nonetheless, incorporating
them via prompting in LLMs provides modest improvement. Persona prompting is
most effective on data samples where disagreements among annotators are
frequent yet confined to a limited range. A linear correlation exists: the more
persona variables influence human annotations, the better LLMs predictions are
using persona prompting. However, when the utility of persona variables is low
(i.e., explaining <10\% of human annotations), persona prompting has little
effect. Most subjective NLP datasets fall into this category, casting doubt on
simulating diverse perspectives in the current NLP landscape.
\\ ( https://arxiv.org/abs/2402.10811 ,  906kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10812
Date: Fri, 16 Feb 2024 16:35:41 GMT   (10592kb,D)

Title: Exploring Hybrid Question Answering via Program-based Prompting
Authors: Qi Shi, Han Cui, Haofeng Wang, Qingfu Zhu, Wanxiang Che, Ting Liu
Categories: cs.CL
\\
  Question answering over heterogeneous data requires reasoning over diverse
sources of data, which is challenging due to the large scale of information and
organic coupling of heterogeneous data. Various approaches have been proposed
to address these challenges. One approach involves training specialized
retrievers to select relevant information, thereby reducing the input length.
Another approach is to transform diverse modalities of data into a single
modality, simplifying the task difficulty and enabling more straightforward
processing. In this paper, we propose HProPro, a novel program-based prompting
framework for the hybrid question answering task. HProPro follows the code
generation and execution paradigm. In addition, HProPro integrates various
functions to tackle the hybrid reasoning scenario. Specifically, HProPro
contains function declaration and function implementation to perform hybrid
information-seeking over data from various sources and modalities, which
enables reasoning over such data without training specialized retrievers or
performing modal transformations. Experimental results on two typical hybrid
question answering benchmarks HybridQA and MultiModalQA demonstrate the
effectiveness of HProPro: it surpasses all baseline systems and achieves the
best performances in the few-shot settings on both datasets.
\\ ( https://arxiv.org/abs/2402.10812 ,  10592kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10835
Date: Fri, 16 Feb 2024 17:15:28 GMT   (9589kb,D)

Title: Time Series Forecasting with LLMs: Understanding and Enhancing Model
  Capabilities
Authors: Mingyu Jin, Hua Tang, Chong Zhang, Qinkai Yu, Chengzhi Liu, Suiyuan
  Zhu, Yongfeng Zhang, Mengnan Du
Categories: cs.CL
\\
  Large language models (LLMs) have been applied in many fields with rapid
development in recent years. As a classic machine learning task, time series
forecasting has recently received a boost from LLMs. However, there is a
research gap in the LLMs' preferences in this field. In this paper, by
comparing LLMs with traditional models, many properties of LLMs in time series
prediction are found. For example, our study shows that LLMs excel in
predicting time series with clear patterns and trends but face challenges with
datasets lacking periodicity. We explain our findings through designing prompts
to require LLMs to tell the period of the datasets. In addition, the input
strategy is investigated, and it is found that incorporating external knowledge
and adopting natural language paraphrases positively affects the predictive
performance of LLMs for time series. Overall, this study contributes to insight
into the advantages and limitations of LLMs in time series forecasting under
different conditions.
\\ ( https://arxiv.org/abs/2402.10835 ,  9589kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10866
Date: Fri, 16 Feb 2024 18:03:42 GMT   (211kb,D)

Title: EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models
Authors: Muhammad Shihab Rashid, Jannat Ara Meem, Yue Dong, Vagelis Hristidis
Categories: cs.CL
Comments: 15 pages, 3 figures
\\
  Large Language Models (LLMs) have achieved state-of-the-art performance in
text re-ranking. This process includes queries and candidate passages in the
prompts, utilizing pointwise, listwise, and pairwise prompting strategies. A
limitation of these ranking strategies with LLMs is their cost: the process can
become expensive due to API charges, which are based on the number of input and
output tokens. We study how to maximize the re-ranking performance given a
budget, by navigating the vast search spaces of prompt choices, LLM APIs, and
budget splits. We propose a suite of budget-constrained methods to perform text
re-ranking using a set of LLM APIs. Our most efficient method, called EcoRank,
is a two-layered pipeline that jointly optimizes decisions regarding budget
allocation across prompt strategies and LLM APIs. Our experimental results on
four popular QA and passage reranking datasets show that EcoRank outperforms
other budget-aware supervised and unsupervised baselines.
\\ ( https://arxiv.org/abs/2402.10866 ,  211kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10884
Date: Fri, 16 Feb 2024 18:42:08 GMT   (4099kb,D)

Title: Multi-modal preference alignment remedies regression of visual
  instruction tuning on language model
Authors: Shengzhi Li, Rongyu Lin, Shichao Pei
Categories: cs.CL cs.AI cs.CV cs.LG
\\
  In production, multi-modal large language models (MLLMs) are expected to
support multi-turn queries of interchanging image and text modalities. However,
the current MLLMs trained with visual-question-answering (VQA) datasets could
suffer from degradation, as VQA datasets lack the diversity and complexity of
the original text instruction datasets which the underlying language model had
been trained with. To address this challenging degradation, we first collect a
lightweight (6k entries) VQA preference dataset where answers were annotated by
Gemini for 5 quality metrics in a granular fashion, and investigate standard
Supervised Fine-tuning, rejection sampling, Direct Preference Optimization
(DPO), and SteerLM. Our findings indicate that the with DPO we are able to
surpass instruction-following capabilities of the language model, achieving a
6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite
small data scale. This enhancement in textual instruction proficiency
correlates with boosted visual instruction performance (+4.9\% on MM-Vet, +6\%
on LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks
compared to previous RLHF approach. In conclusion, we propose a
distillation-based multi-modal alignment model with fine-grained annotations on
a small dataset that reconciles the textual and visual performance of MLLMs,
restoring and boosting language capability after visual instruction tuning.
\\ ( https://arxiv.org/abs/2402.10884 ,  4099kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10886
Date: Fri, 16 Feb 2024 18:43:10 GMT   (10469kb,D)

Title: Reviewer2: Optimizing Review Generation Through Prompt Generation
Authors: Zhaolin Gao, Kiant\'e Brantley, Thorsten Joachims
Categories: cs.CL
\\
  Recent developments in LLMs offer new opportunities for assisting authors in
improving their work. In this paper, we envision a use case where authors can
receive LLM-generated reviews that uncover weak points in the current draft.
While initial methods for automated review generation already exist, these
methods tend to produce reviews that lack detail, and they do not cover the
range of opinions that human reviewers produce. To address this shortcoming, we
propose an efficient two-stage review generation framework called Reviewer2.
Unlike prior work, this approach explicitly models the distribution of possible
aspects that the review may address. We show that this leads to more detailed
reviews that better cover the range of aspects that human reviewers identify in
the draft. As part of the research, we generate a large-scale review dataset of
27k papers and 99k reviews that we annotate with aspect prompts, which we make
available as a resource for future research.
\\ ( https://arxiv.org/abs/2402.10886 ,  10469kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10890
Date: Fri, 16 Feb 2024 18:45:58 GMT   (2011kb,D)

Title: When is Tree Search Useful for LLM Planning? It Depends on the
  Discriminator
Authors: Ziru Chen, Michael White, Raymond Mooney, Ali Payani, Yu Su, Huan Sun
Categories: cs.CL cs.AI cs.LG
\\
  In this paper, we examine how large language models (LLMs) solve multi-step
problems under a language agent framework with three components: a generator, a
discriminator, and a planning method. We investigate the practical utility of
two advanced planning methods, iterative correction and tree search. We present
a comprehensive analysis of how discrimination accuracy affects the overall
performance of agents when using these two methods or a simpler method,
re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical
reasoning, show that: (1) advanced planning methods demand discriminators with
at least 90% accuracy to achieve significant improvements over re-ranking; (2)
current LLMs' discrimination abilities have not met the needs of advanced
planning methods to achieve such improvements; (3) with LLM-based
discriminators, advanced planning methods may not adequately balance accuracy
and efficiency. For example, compared to the other two methods, tree search is
at least 10--20 times slower but leads to negligible performance gains, which
hinders its real-world applications. Code and data will be released at
https://github.com/OSU-NLP-Group/llm-planning-eval.
\\ ( https://arxiv.org/abs/2402.10890 ,  2011kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10891
Date: Fri, 16 Feb 2024 18:47:21 GMT   (7442kb,D)

Title: Instruction Diversity Drives Generalization To Unseen Tasks
Authors: Dylan Zhang, Justin Wang, Francois Charton
Categories: cs.CL cs.AI cs.LG
\\
  Instruction tuning -- fine-tuning a large language model (LLM) on pairs of
instructions and desired outcomes -- is an approach that enables pre-trained
language models to perform real-world tasks and follow human instructions. Its
practical success depends on the model learning a broader set of instructions
than those it was trained on. Yet the factors that determine model
generalization to such \emph{unseen tasks} are not well understood. %To
understand the driving factors of generalization, In this paper, we experiment
with string rewrites, a symbolic task that serves as a building block for
Turing complete Markov algorithms while allowing experimental control of
"inputs" and "instructions". We investigate the trade-off between the number of
instructions the model is trained on and the number of training samples
provided for each instruction and observe that the diversity of the instruction
set determines generalization. Generalization emerges once a diverse enough set
of tasks is provided, even though very few examples are provided for each task.
Instruction diversity also ensures robustness with respect to non-uniform
distributions of instructions in the training set.
\\ ( https://arxiv.org/abs/2402.10891 ,  7442kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10227
Date: Sun, 4 Feb 2024 19:33:44 GMT   (4797kb,D)

Title: Correlational Lagrangian Schr\"odinger Bridge: Learning Dynamics with
  Population-Level Regularization
Authors: Yuning You, Ruida Zhou, Yang Shen
Categories: cs.LG stat.ML
\\
  Accurate modeling of system dynamics holds intriguing potential in broad
scientific fields including cytodynamics and fluid mechanics. This task often
presents significant challenges when (i) observations are limited to
cross-sectional samples (where individual trajectories are inaccessible for
learning), and moreover, (ii) the behaviors of individual particles are
heterogeneous (especially in biological systems due to biodiversity). To
address them, we introduce a novel framework dubbed correlational Lagrangian
Schr\"odinger bridge (CLSB), aiming to seek for the evolution "bridging" among
cross-sectional observations, while regularized for the minimal population
"cost". In contrast to prior methods relying on \textit{individual}-level
regularizers for all particles \textit{homogeneously} (e.g. restraining
individual motions), CLSB operates at the population level admitting the
heterogeneity nature, resulting in a more generalizable modeling in practice.
To this end, our contributions include (1) a new class of population
regularizers capturing the temporal variations in multivariate relations, with
the tractable formulation derived, (2) three domain-informed instantiations
based on genetic co-expression stability, and (3) an integration of population
regularizers into data-driven generative models as constrained optimization,
and a numerical solution, with further extension to conditional generative
models. Empirically, we demonstrate the superiority of CLSB in single-cell
sequencing data analyses such as simulating cell development over time and
predicting cellular responses to drugs of varied doses.
\\ ( https://arxiv.org/abs/2402.10227 ,  4797kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10228
Date: Mon, 5 Feb 2024 07:07:30 GMT   (3640kb,D)

Title: HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement
  Learning Framework for Complex Environments
Authors: Yingru Li, Jiawei Xu, Lei Han, Zhi-Quan Luo
Categories: cs.LG cs.AI stat.ML
Comments: Bridging the theory and practice!
\\
  To solve complex tasks under resource constraints, reinforcement learning
(RL) agents need to be simple, efficient, and scalable with (1) large state
space and (2) increasingly accumulated data of interactions. We propose the
HyperAgent, a RL framework with hypermodel, index sampling schemes and
incremental update mechanism, enabling computation-efficient sequential
posterior approximation and data-efficient action selection under general value
function approximation beyond conjugacy. The implementation of \HyperAgent is
simple as it only adds one module and one line of code additional to DDQN.
Practically, HyperAgent demonstrates its robust performance in large-scale deep
RL benchmarks with significant efficiency gain in terms of both data and
computation. Theoretically, among the practically scalable algorithms,
HyperAgent is the first method to achieve provably scalable per-step
computational complexity as well as sublinear regret under tabular RL. The core
of our theoretical analysis is the sequential posterior approximation argument,
made possible by the first analytical tool for sequential random projection, a
non-trivial martingale extension of the Johnson-Lindenstrauss lemma. This work
bridges the theoretical and practical realms of RL, establishing a new
benchmark for RL algorithm design.
\\ ( https://arxiv.org/abs/2402.10228 ,  3640kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10238
Date: Wed, 14 Feb 2024 18:12:42 GMT   (6372kb,D)

Title: Parametric Learning of Time-Advancement Operators for Unstable Flame
  Evolution
Authors: Rixin Yu and Erdzan Hodzic
Categories: cs.LG
Comments: 32 pages, 13 figures
MSC-class: 65Z05
ACM-class: G.1.8
\\
  This study investigates the application of machine learning, specifically
Fourier Neural Operator (FNO) and Convolutional Neural Network (CNN), to learn
time-advancement operators for parametric partial differential equations
(PDEs). Our focus is on extending existing operator learning methods to handle
additional inputs representing PDE parameters. The goal is to create a unified
learning approach that accurately predicts short-term solutions and provides
robust long-term statistics under diverse parameter conditions, facilitating
computational cost savings and accelerating development in engineering
simulations. We develop and compare parametric learning methods based on FNO
and CNN, evaluating their effectiveness in learning parametric-dependent
solution time-advancement operators for one-dimensional PDEs and realistic
flame front evolution data obtained from direct numerical simulations of the
Navier-Stokes equations.
\\ ( https://arxiv.org/abs/2402.10238 ,  6372kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10240
Date: Wed, 14 Feb 2024 18:44:05 GMT   (4307kb,D)

Title: A Dynamical View of the Question of Why
Authors: Mehdi Fatemi and Sindhu Gowda
Categories: cs.LG cs.AI cs.SY eess.SY
Comments: Accepted at the Twelfth International Conference on Learning
  Representations (ICLR'24). arXiv admin note: text overlap with
  arXiv:2102.02311, arXiv:2012.05123, arXiv:2201.13169, arXiv:2012.05603,
  arXiv:1812.03789 by other authors
\\
  We address causal reasoning in multivariate time series data generated by
stochastic processes. Existing approaches are largely restricted to static
settings, ignoring the continuity and emission of variations across time. In
contrast, we propose a learning paradigm that directly establishes causation
between events in the course of time. We present two key lemmas to compute
causal contributions and frame them as reinforcement learning problems. Our
approach offers formal and computational tools for uncovering and quantifying
causal relationships in diffusion processes, subsuming various important
settings such as discrete-time Markov decision processes. Finally, in fairly
intricate experiments and through sheer learning, our framework reveals and
quantifies causal links, which otherwise seem inexplicable.
\\ ( https://arxiv.org/abs/2402.10240 ,  4307kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10248
Date: Thu, 15 Feb 2024 11:09:22 GMT   (38527kb,D)

Title: A Data-Driven Supervised Machine Learning Approach to Estimating Global
  Ambient Air Pollution Concentrations With Associated Prediction Intervals
Authors: Liam J Berrisford, Hugo Barbosa, Ronaldo Menezes
Categories: cs.LG cs.AI
Comments: Main Paper: 25 pages, 15 figures, 5 tables. Supplementary: 4 pages, 3
  figures
\\
  Global ambient air pollution, a transboundary challenge, is typically
addressed through interventions relying on data from spatially sparse and
heterogeneously placed monitoring stations. These stations often encounter
temporal data gaps due to issues such as power outages. In response, we have
developed a scalable, data-driven, supervised machine learning framework. This
model is designed to impute missing temporal and spatial measurements, thereby
generating a comprehensive dataset for pollutants including NO$_2$, O$_3$,
PM$_{10}$, PM$_{2.5}$, and SO$_2$. The dataset, with a fine granularity of
0.25$^{\circ}$ at hourly intervals and accompanied by prediction intervals for
each estimate, caters to a wide range of stakeholders relying on outdoor air
pollution data for downstream assessments. This enables more detailed studies.
Additionally, the model's performance across various geographical locations is
examined, providing insights and recommendations for strategic placement of
future monitoring stations to further enhance the model's accuracy.
\\ ( https://arxiv.org/abs/2402.10248 ,  38527kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10254
Date: Wed, 7 Feb 2024 12:28:52 GMT   (564kb,D)

Title: Personalized Federated Learning for Statistical Heterogeneity
Authors: Muhammad Firdaus and Kyung-Hyune Rhee
Categories: cs.LG cs.NI
\\
  The popularity of federated learning (FL) is on the rise, along with growing
concerns about data privacy in artificial intelligence applications. FL
facilitates collaborative multi-party model learning while simultaneously
ensuring the preservation of data confidentiality. Nevertheless, the problem of
statistical heterogeneity caused by the presence of diverse client data
distributions gives rise to certain challenges, such as inadequate
personalization and slow convergence. In order to address the above issues,
this paper offers a brief summary of the current research progress in the field
of personalized federated learning (PFL). It outlines the PFL concept, examines
related techniques, and highlights current endeavors. Furthermore, this paper
also discusses potential further research and obstacles associated with PFL.
\\ ( https://arxiv.org/abs/2402.10254 ,  564kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10260
Date: Thu, 15 Feb 2024 18:58:09 GMT   (748kb,D)

Title: A StrongREJECT for Empty Jailbreaks
Authors: Alexandra Souly, Qingyuan Lu, Dillon Bowen, Tu Trinh, Elvis Hsieh,
  Sana Pandey, Pieter Abbeel, Justin Svegliato, Scott Emmons, Olivia Watkins,
  Sam Toyer
Categories: cs.LG cs.CL cs.CR
Comments: Code and data at https://github.com/alexandrasouly/strongreject
\\
  The rise of large language models (LLMs) has drawn attention to the existence
of "jailbreaks" that allow the models to be used maliciously. However, there is
no standard benchmark for measuring the severity of a jailbreak, leaving
authors of jailbreak papers to create their own. We show that these benchmarks
often include vague or unanswerable questions and use grading criteria that are
biased towards overestimating the misuse potential of low-quality model
responses. Some jailbreak techniques make the problem worse by decreasing the
quality of model responses even on benign questions: we show that several
jailbreaking techniques substantially reduce the zero-shot performance of GPT-4
on MMLU. Jailbreaks can also make it harder to elicit harmful responses from an
"uncensored" open-source model. We present a new benchmark, StrongREJECT, which
better discriminates between effective and ineffective jailbreaks by using a
higher-quality question set and a more accurate response grading algorithm. We
show that our new grading scheme better accords with human judgment of response
quality and overall jailbreak effectiveness, especially on the sort of
low-quality responses that contribute the most to over-estimation of jailbreak
performance on existing benchmarks. We release our code and data at
https://github.com/alexandrasouly/strongreject.
\\ ( https://arxiv.org/abs/2402.10260 ,  748kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10280
Date: Thu, 15 Feb 2024 19:15:15 GMT   (2626kb,D)

Title: SusFL: Energy-Aware Federated Learning-based Monitoring for Sustainable
  Smart Farms
Authors: Dian Chen, Paul Yang, Ing-Ray Chen, Dong Sam Ha, Jin-Hee Cho
Categories: cs.LG
\\
  We propose a novel energy-aware federated learning (FL)-based system, namely
SusFL, for sustainable smart farming to address the challenge of inconsistent
health monitoring due to fluctuating energy levels of solar sensors. This
system equips animals, such as cattle, with solar sensors with computational
capabilities, including Raspberry Pis, to train a local deep-learning model on
health data. These sensors periodically update Long Range (LoRa) gateways,
forming a wireless sensor network (WSN) to detect diseases like mastitis. Our
proposed SusFL system incorporates mechanism design, a game theory concept, for
intelligent client selection to optimize monitoring quality while minimizing
energy use. This strategy ensures the system's sustainability and resilience
against adversarial attacks, including data poisoning and privacy threats, that
could disrupt FL operations. Through extensive comparative analysis using
real-time datasets, we demonstrate that our FL-based monitoring system
significantly outperforms existing methods in prediction accuracy, operational
efficiency, system reliability (i.e., mean time between failures or MTBF), and
social welfare maximization by the mechanism designer. Our findings validate
the superiority of our system for effective and sustainable animal health
monitoring in smart farms. The experimental results show that SusFL
significantly improves system performance, including a $10\%$ reduction in
energy consumption, a $15\%$ increase in social welfare, and a $34\%$ rise in
Mean Time Between Failures (MTBF), alongside a marginal increase in the global
model's prediction accuracy.
\\ ( https://arxiv.org/abs/2402.10280 ,  2626kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10282
Date: Thu, 15 Feb 2024 19:18:47 GMT   (48kb,D)

Title: Information Capacity Regret Bounds for Bandits with Mediator Feedback
Authors: Khaled Eldowa, Nicol\`o Cesa-Bianchi, Alberto Maria Metelli, Marcello
  Restelli
Categories: cs.LG stat.ML
\\
  This work addresses the mediator feedback problem, a bandit game where the
decision set consists of a number of policies, each associated with a
probability distribution over a common space of outcomes. Upon choosing a
policy, the learner observes an outcome sampled from its distribution and
incurs the loss assigned to this outcome in the present round. We introduce the
policy set capacity as an information-theoretic measure for the complexity of
the policy set. Adopting the classical EXP4 algorithm, we provide new regret
bounds depending on the policy set capacity in both the adversarial and the
stochastic settings. For a selection of policy set families, we prove
nearly-matching lower bounds, scaling similarly with the capacity. We also
consider the case when the policies' distributions can vary between rounds,
thus addressing the related bandits with expert advice problem, which we
improve upon its prior results. Additionally, we prove a lower bound showing
that exploiting the similarity between the policies is not possible in general
under linear bandit feedback. Finally, for a full-information variant, we
provide a regret bound scaling with the information radius of the policy set.
\\ ( https://arxiv.org/abs/2402.10282 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10283
Date: Thu, 15 Feb 2024 19:19:54 GMT   (3154kb,D)

Title: Backdoor Attack against One-Class Sequential Anomaly Detection Models
Authors: He Cheng and Shuhan Yuan
Categories: cs.LG cs.AI cs.CR cs.IT math.IT
Comments: This work is accepted by the PAKDD 2024. 12 pages
\\
  Deep anomaly detection on sequential data has garnered significant attention
due to the wide application scenarios. However, deep learning-based models face
a critical security threat - their vulnerability to backdoor attacks. In this
paper, we explore compromising deep sequential anomaly detection models by
proposing a novel backdoor attack strategy. The attack approach comprises two
primary steps, trigger generation and backdoor injection. Trigger generation is
to derive imperceptible triggers by crafting perturbed samples from the benign
normal data, of which the perturbed samples are still normal. The backdoor
injection is to properly inject the backdoor triggers to comprise the model
only for the samples with triggers. The experimental results demonstrate the
effectiveness of our proposed attack strategy by injecting backdoors on two
well-established one-class anomaly detection models.
\\ ( https://arxiv.org/abs/2402.10283 ,  3154kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10291
Date: Thu, 15 Feb 2024 19:45:24 GMT   (3564kb,D)

Title: An Evaluation of Real-time Adaptive Sampling Change Point Detection
  Algorithm using KCUSUM
Authors: Vijayalakshmi Saravanan, Perry Siehien, Shinjae Yoo, Hubertus Van Dam,
  Thomas Flynn, Christopher Kelly, Khaled Z Ibrahim
Categories: cs.LG stat.ML
Comments: 16 pages. arXiv admin note: text overlap with arXiv:1903.01661
MSC-class: CCS
\\
  Detecting abrupt changes in real-time data streams from scientific
simulations presents a challenging task, demanding the deployment of accurate
and efficient algorithms. Identifying change points in live data stream
involves continuous scrutiny of incoming observations for deviations in their
statistical characteristics, particularly in high-volume data scenarios.
Maintaining a balance between sudden change detection and minimizing false
alarms is vital. Many existing algorithms for this purpose rely on known
probability distributions, limiting their feasibility. In this study, we
introduce the Kernel-based Cumulative Sum (KCUSUM) algorithm, a non-parametric
extension of the traditional Cumulative Sum (CUSUM) method, which has gained
prominence for its efficacy in online change point detection under less
restrictive conditions. KCUSUM splits itself by comparing incoming samples
directly with reference samples and computes a statistic grounded in the
Maximum Mean Discrepancy (MMD) non-parametric framework. This approach extends
KCUSUM's pertinence to scenarios where only reference samples are available,
such as atomic trajectories of proteins in vacuum, facilitating the detection
of deviations from the reference sample without prior knowledge of the data's
underlying distribution. Furthermore, by harnessing MMD's inherent random-walk
structure, we can theoretically analyze KCUSUM's performance across various use
cases, including metrics like expected delay and mean runtime to false alarms.
Finally, we discuss real-world use cases from scientific simulations such as
NWChem CODAR and protein folding data, demonstrating KCUSUM's practical
effectiveness in online change point detection.
\\ ( https://arxiv.org/abs/2402.10291 ,  3564kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10309
Date: Thu, 15 Feb 2024 20:20:35 GMT   (759kb,D)

Title: Discrete Probabilistic Inference as Control in Multi-path Environments
Authors: Tristan Deleu, Padideh Nouri, Nikolay Malkin, Doina Precup, Yoshua
  Bengio
Categories: cs.LG
\\
  We consider the problem of sampling from a discrete and structured
distribution as a sequential decision problem, where the objective is to find a
stochastic policy such that objects are sampled at the end of this sequential
process proportionally to some predefined reward. While we could use maximum
entropy Reinforcement Learning (MaxEnt RL) to solve this problem for some
distributions, it has been shown that in general, the distribution over states
induced by the optimal policy may be biased in cases where there are multiple
ways to generate the same object. To address this issue, Generative Flow
Networks (GFlowNets) learn a stochastic policy that samples objects
proportionally to their reward by approximately enforcing a conservation of
flows across the whole Markov Decision Process (MDP). In this paper, we extend
recent methods correcting the reward in order to guarantee that the marginal
distribution induced by the optimal MaxEnt RL policy is proportional to the
original reward, regardless of the structure of the underlying MDP. We also
prove that some flow-matching objectives found in the GFlowNet literature are
in fact equivalent to well-established MaxEnt RL algorithms with a corrected
reward. Finally, we study empirically the performance of multiple MaxEnt RL and
GFlowNet algorithms on multiple problems involving sampling from discrete
distributions.
\\ ( https://arxiv.org/abs/2402.10309 ,  759kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10310
Date: Thu, 15 Feb 2024 20:21:40 GMT   (10399kb,D)

Title: Interpretable Generative Adversarial Imitation Learning
Authors: Wenliang Liu, Danyang Li, Erfan Aasi, Roberto Tron, Calin Belta
Categories: cs.LG cs.SY eess.SY
Comments: Submitted to L4DC 2024 (under review)
\\
  Imitation learning methods have demonstrated considerable success in teaching
autonomous systems complex tasks through expert demonstrations. However, a
limitation of these methods is their lack of interpretability, particularly in
understanding the specific task the learning agent aims to accomplish. In this
paper, we propose a novel imitation learning method that combines Signal
Temporal Logic (STL) inference and control synthesis, enabling the explicit
representation of the task as an STL formula. This approach not only provides a
clear understanding of the task but also allows for the incorporation of human
knowledge and adaptation to new scenarios through manual adjustments of the STL
formulae. Additionally, we employ a Generative Adversarial Network
(GAN)-inspired training approach for both the inference and the control policy,
effectively narrowing the gap between the expert and learned policies. The
effectiveness of our algorithm is demonstrated through two case studies,
showcasing its practical applicability and adaptability.
\\ ( https://arxiv.org/abs/2402.10310 ,  10399kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10339
Date: Thu, 15 Feb 2024 21:57:43 GMT   (10843kb,D)

Title: What to Do When Your Discrete Optimization Is the Size of a Neural
  Network?
Authors: Hugo Silva and Martha White
Categories: cs.LG
Comments: Submitted to JMLR
\\
  Oftentimes, machine learning applications using neural networks involve
solving discrete optimization problems, such as in pruning,
parameter-isolation-based continual learning and training of binary networks.
Still, these discrete problems are combinatorial in nature and are also not
amenable to gradient-based optimization. Additionally, classical approaches
used in discrete settings do not scale well to large neural networks, forcing
scientists and empiricists to rely on alternative methods. Among these, two
main distinct sources of top-down information can be used to lead the model to
good solutions: (1) extrapolating gradient information from points outside of
the solution set (2) comparing evaluations between members of a subset of the
valid solutions. We take continuation path (CP) methods to represent using
purely the former and Monte Carlo (MC) methods to represent the latter, while
also noting that some hybrid methods combine the two. The main goal of this
work is to compare both approaches. For that purpose, we first overview the two
classes while also discussing some of their drawbacks analytically. Then, on
the experimental section, we compare their performance, starting with smaller
microworld experiments, which allow more fine-grained control of problem
variables, and gradually moving towards larger problems, including neural
network regression and neural network pruning for image classification, where
we additionally compare against magnitude-based pruning.
\\ ( https://arxiv.org/abs/2402.10339 ,  10843kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10342
Date: Thu, 15 Feb 2024 22:11:18 GMT   (80kb)

Title: Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on
  Efficient Data Utilization
Authors: Yihan Du, Anna Winnicki, Gal Dalal, Shie Mannor, R. Srikant
Categories: cs.LG
\\
  Reinforcement Learning from Human Feedback (RLHF) has achieved impressive
empirical successes while relying on a small amount of human feedback. However,
there is limited theoretical justification for this phenomenon. Additionally,
most recent studies focus on value-based algorithms despite the recent
empirical successes of policy-based algorithms. In this work, we consider an
RLHF algorithm based on policy optimization (PO-RLHF). The algorithm is based
on the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes
knowledge of the reward function. In PO-RLHF, knowledge of the reward function
is not assumed and the algorithm relies on trajectory-based comparison feedback
to infer the reward function. We provide performance bounds for PO-RLHF with
low query complexity, which provides insight into why a small amount of human
feedback may be sufficient to get good performance with RLHF. A key novelty is
our trajectory-level elliptical potential analysis technique used to infer
reward function parameters when comparison queries rather than reward
observations are used. We provide and analyze algorithms in two settings:
linear and neural function approximation, PG-RLHF and NN-PG-RLHF, respectively.
\\ ( https://arxiv.org/abs/2402.10342 ,  80kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10350
Date: Thu, 15 Feb 2024 22:43:02 GMT   (175kb,D)

Title: Large Language Models for Forecasting and Anomaly Detection: A
  Systematic Literature Review
Authors: Jing Su, Chufeng Jiang, Xin Jin, Yuxin Qiao, Tingsong Xiao, Hongda Ma,
  Rong Wei, Zhi Jing, Jiajun Xu, Junhong Lin
Categories: cs.LG cs.AI
\\
  This systematic literature review comprehensively examines the application of
Large Language Models (LLMs) in forecasting and anomaly detection, highlighting
the current state of research, inherent challenges, and prospective future
directions. LLMs have demonstrated significant potential in parsing and
analyzing extensive datasets to identify patterns, predict future events, and
detect anomalous behavior across various domains. However, this review
identifies several critical challenges that impede their broader adoption and
effectiveness, including the reliance on vast historical datasets, issues with
generalizability across different contexts, the phenomenon of model
hallucinations, limitations within the models' knowledge boundaries, and the
substantial computational resources required. Through detailed analysis, this
review discusses potential solutions and strategies to overcome these
obstacles, such as integrating multimodal data, advancements in learning
methodologies, and emphasizing model explainability and computational
efficiency. Moreover, this review outlines critical trends that are likely to
shape the evolution of LLMs in these fields, including the push toward
real-time processing, the importance of sustainable modeling practices, and the
value of interdisciplinary collaboration. Conclusively, this review underscores
the transformative impact LLMs could have on forecasting and anomaly detection
while emphasizing the need for continuous innovation, ethical considerations,
and practical solutions to realize their full potential.
\\ ( https://arxiv.org/abs/2402.10350 ,  175kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10359
Date: Thu, 15 Feb 2024 23:09:42 GMT   (190kb,D)

Title: Can we soft prompt LLMs for graph learning tasks?
Authors: Zheyuan Liu, Xiaoxin He, Yijun Tian, Nitesh V. Chawla
Categories: cs.LG cs.CL
Comments: 4 pages including the references
\\
  Graph plays an important role in representing complex relationships in
real-world applications such as social networks, biological data and citation
networks. In recent years, Large Language Models (LLMs) have achieved
tremendous success in various domains, which makes applying LLMs to graphs
particularly appealing. However, directly applying LLMs to graph modalities
presents unique challenges due to the discrepancy and mismatch between the
graph and text modalities. Hence, to further investigate LLMs' potential for
comprehending graph information, we introduce GraphPrompter, a novel framework
designed to align graph information with LLMs via soft prompts. Specifically,
GraphPrompter consists of two main components: a graph neural network to encode
complex graph information and an LLM that effectively processes textual
information. Comprehensive experiments on various benchmark datasets under node
classification and link prediction tasks demonstrate the effectiveness of our
proposed method. The GraphPrompter framework unveils the substantial
capabilities of LLMs as predictors in graph-related tasks, enabling researchers
to utilize LLMs across a spectrum of real-world graph scenarios more
effectively.
\\ ( https://arxiv.org/abs/2402.10359 ,  190kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10360
Date: Thu, 15 Feb 2024 23:10:45 GMT   (53kb)

Title: Learnability is a Compact Property
Authors: Julian Asilis, Siddartha Devic, Shaddin Dughmi, Vatsal Sharan,
  Shang-Hua Teng
Categories: cs.LG cs.DS cs.LO stat.ML
Comments: 23 pages, 1 figure
\\
  Recent work on learning has yielded a striking result: the learnability of
various problems can be undecidable, or independent of the standard ZFC axioms
of set theory. Furthermore, the learnability of such problems can fail to be a
property of finite character: informally, it cannot be detected by examining
finite projections of the problem.
  On the other hand, learning theory abounds with notions of dimension that
characterize learning and consider only finite restrictions of the problem,
i.e., are properties of finite character. How can these results be reconciled?
More precisely, which classes of learning problems are vulnerable to logical
undecidability, and which are within the grasp of finite characterizations?
  We demonstrate that the difficulty of supervised learning with metric losses
admits a tight finite characterization. In particular, we prove that the sample
complexity of learning a hypothesis class can be detected by examining its
finite projections. For realizable and agnostic learning with respect to a wide
class of proper loss functions, we demonstrate an exact compactness result: a
class is learnable with a given sample complexity precisely when the same is
true of all its finite projections. For realizable learning with improper loss
functions, we show that exact compactness of sample complexity can fail, and
provide matching upper and lower bounds of a factor of 2 on the extent to which
such sample complexities can differ. We conjecture that larger gaps are
possible for the agnostic case.
  At the heart of our technical work is a compactness result concerning
assignments of variables that maintain a class of functions below a target
value, which generalizes Hall's classic matching theorem and may be of
independent interest.
\\ ( https://arxiv.org/abs/2402.10360 ,  53kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10374
Date: Thu, 15 Feb 2024 23:43:53 GMT   (1899kb,D)

Title: Revisiting Experience Replayable Conditions
Authors: Taisuke Kobayashi
Categories: cs.LG
Comments: 22 pages, 8 figures
\\
  Experience replay (ER) used in (deep) reinforcement learning is considered to
be applicable only to off-policy algorithms. However, there have been some
cases in which ER has been applied for on-policy algorithms, suggesting that
off-policyness might be a sufficient condition for applying ER. This paper
reconsiders more strict "experience replayable conditions" (ERC) and proposes
the way of modifying the existing algorithms to satisfy ERC. To this end,
instability of policy improvements is assumed to be a key in ERC. The
instability factors are revealed from the viewpoint of metric learning as i)
repulsive forces from negative samples and ii) replays of inappropriate
experiences. Accordingly, the corresponding stabilization tricks are derived.
As a result, it is confirmed through numerical simulations that the proposed
stabilization tricks make ER applicable to an advantage actor-critic, an
on-policy algorithm. In addition, its learning performance is comparable to
that of a soft actor-critic, a state-of-the-art off-policy algorithm.
\\ ( https://arxiv.org/abs/2402.10374 ,  1899kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10376
Date: Fri, 16 Feb 2024 00:04:36 GMT   (1527kb,D)

Title: Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)
Authors: Usha Bhalla, Alex Oesterling, Suraj Srinivas, Flavio P. Calmon,
  Himabindu Lakkaraju
Categories: cs.LG cs.CV
Comments: 17 pages, 8 figures, Code is provided at
  https://github.com/AI4LIFE-GROUP/SpLiCE
\\
  CLIP embeddings have demonstrated remarkable performance across a wide range
of computer vision tasks. However, these high-dimensional, dense vector
representations are not easily interpretable, restricting their usefulness in
downstream applications that require transparency. In this work, we empirically
show that CLIP's latent space is highly structured, and consequently that CLIP
representations can be decomposed into their underlying semantic components. We
leverage this understanding to propose a novel method, Sparse Linear Concept
Embeddings (SpLiCE), for transforming CLIP representations into sparse linear
combinations of human-interpretable concepts. Distinct from previous work,
SpLiCE does not require concept labels and can be applied post hoc. Through
extensive experimentation with multiple real-world datasets, we validate that
the representations output by SpLiCE can explain and even replace traditional
dense CLIP representations, maintaining equivalent downstream performance while
significantly improving their interpretability. We also demonstrate several use
cases of SpLiCE representations including detecting spurious correlations,
model editing, and quantifying semantic shifts in datasets.
\\ ( https://arxiv.org/abs/2402.10376 ,  1527kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10380
Date: Fri, 16 Feb 2024 00:25:24 GMT   (516kb,D)

Title: Subgraph-level Universal Prompt Tuning
Authors: Junhyun Lee, Wooseong Yang, Jaewoo Kang
Categories: cs.LG cs.AI
\\
  In the evolving landscape of machine learning, the adaptation of pre-trained
models through prompt tuning has become increasingly prominent. This trend is
particularly observable in the graph domain, where diverse pre-training
strategies present unique challenges in developing effective prompt-based
tuning methods for graph neural networks. Previous approaches have been
limited, focusing on specialized prompting functions tailored to models with
edge prediction pre-training tasks. These methods, however, suffer from a lack
of generalizability across different pre-training strategies. Recently, a
simple prompt tuning method has been designed for any pre-training strategy,
functioning within the input graph's feature space. This allows it to
theoretically emulate any type of prompting function, thereby significantly
increasing its versatility for a range of downstream applications.
Nevertheless, the capacity of such simple prompts to fully grasp the complex
contexts found in graphs remains an open question, necessitating further
investigation. Addressing this challenge, our work introduces the
Subgraph-level Universal Prompt Tuning (SUPT) approach, focusing on the
detailed context within subgraphs. In SUPT, prompt features are assigned at the
subgraph-level, preserving the method's universal capability. This requires
extremely fewer tuning parameters than fine-tuning-based methods, outperforming
them in 42 out of 45 full-shot scenario experiments with an average improvement
of over 2.5%. In few-shot scenarios, it excels in 41 out of 45 experiments,
achieving an average performance increase of more than 6.6%.
\\ ( https://arxiv.org/abs/2402.10380 ,  516kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10392
Date: Fri, 16 Feb 2024 01:25:21 GMT   (845kb,D)

Title: Pretext Training Algorithms for Event Sequence Data
Authors: Yimu Wang, He Zhao, Ruizhi Deng, Frederick Tung, Greg Mori
Categories: cs.LG cs.AI
\\
  Pretext training followed by task-specific fine-tuning has been a successful
approach in vision and language domains. This paper proposes a self-supervised
pretext training framework tailored to event sequence data. We introduce a
novel alignment verification task that is specialized to event sequences,
building on good practices in masked reconstruction and contrastive learning.
Our pretext tasks unlock foundational representations that are generalizable
across different down-stream tasks, including next-event prediction for
temporal point process models, event sequence classification, and missing event
interpolation. Experiments on popular public benchmarks demonstrate the
potential of the proposed method across different tasks and data domains.
\\ ( https://arxiv.org/abs/2402.10392 ,  845kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10397
Date: Fri, 16 Feb 2024 01:47:02 GMT   (222kb,D)

Title: LogELECTRA: Self-supervised Anomaly Detection for Unstructured Logs
Authors: Yuuki Yamanaka, Tomokatsu Takahashi, Takuya Minami, Yoshiaki Nakajima
Categories: cs.LG cs.SE
\\
  System logs are some of the most important information for the maintenance of
software systems, which have become larger and more complex in recent years.
The goal of log-based anomaly detection is to automatically detect system
anomalies by analyzing the large number of logs generated in a short period of
time, which is a critical challenge in the real world. Previous studies have
used a log parser to extract templates from unstructured log data and detect
anomalies on the basis of patterns of the template occurrences. These methods
have limitations for logs with unknown templates. Furthermore, since most log
anomalies are known to be point anomalies rather than contextual anomalies,
detection methods based on occurrence patterns can cause unnecessary delays in
detection. In this paper, we propose LogELECTRA, a new log anomaly detection
model that analyzes a single line of log messages more deeply on the basis of
self-supervised anomaly detection. LogELECTRA specializes in detecting log
anomalies as point anomalies by applying ELECTRA, a natural language processing
model, to analyze the semantics of a single line of log messages. LogELECTRA
outperformed existing state-of-the-art methods in experiments on the public
benchmark log datasets BGL, Sprit, and Thunderbird.
\\ ( https://arxiv.org/abs/2402.10397 ,  222kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10401
Date: Fri, 16 Feb 2024 01:58:35 GMT   (40387kb,D)

Title: ManiFPT: Defining and Analyzing Fingerprints of Generative Models
Authors: Hae Jin Song, Mahyar Khayatkhoei, Wael AbdAlmageed
Categories: cs.LG cs.CV
Comments: Under review at CVPR 2024
\\
  Recent works have shown that generative models leave traces of their
underlying generative process on the generated samples, broadly referred to as
fingerprints of a generative model, and have studied their utility in detecting
synthetic images from real ones. However, the extend to which these
fingerprints can distinguish between various types of synthetic image and help
identify the underlying generative process remain under-explored. In
particular, the very definition of a fingerprint remains unclear, to our
knowledge. To that end, in this work, we formalize the definition of artifact
and fingerprint in generative models, propose an algorithm for computing them
in practice, and finally study its effectiveness in distinguishing a large
array of different generative models. We find that using our proposed
definition can significantly improve the performance on the task of identifying
the underlying generative process from samples (model attribution) compared to
existing methods. Additionally, we study the structure of the fingerprints, and
observe that it is very predictive of the effect of different design choices on
the generative process.
\\ ( https://arxiv.org/abs/2402.10401 ,  40387kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10403
Date: Fri, 16 Feb 2024 02:01:24 GMT   (1948kb,D)

Title: Polyhedral Complex Derivation from Piecewise Trilinear Networks
Authors: Jin-Hwa Kim
Categories: cs.LG cs.AI cs.CV cs.GR
\\
  Recent advancements in visualizing deep neural networks provide insights into
their structures and mesh extraction from Continuous Piecewise Affine (CPWA)
functions. Meanwhile, developments in neural surface representation learning
incorporate non-linear positional encoding, addressing issues like spectral
bias; however, this poses challenges in applying mesh extraction techniques
based on CPWA functions. Focusing on trilinear interpolating methods as
positional encoding, we present theoretical insights and an analytical mesh
extraction, showing the transformation of hypersurfaces to flat planes within
the trilinear region under the eikonal constraint. Moreover, we introduce a
method for approximating intersecting points among three hypersurfaces
contributing to broader applications. We empirically validate correctness and
parsimony through chamfer distance and efficiency, and angular distance, while
examining the correlation between the eikonal loss and the planarity of the
hypersurfaces.
\\ ( https://arxiv.org/abs/2402.10403 ,  1948kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10434
Date: Fri, 16 Feb 2024 03:51:14 GMT   (756kb,D)

Title: Parametric Augmentation for Time Series Contrastive Learning
Authors: Xu Zheng, Tianchun Wang, Wei Cheng, Aitian Ma, Haifeng Chen, Mo Sha,
  Dongsheng Luo
Categories: cs.LG
Comments: Accepted by International Conference on Learning Representations
  (ICLR 2024)
\\
  Modern techniques like contrastive learning have been effectively used in
many areas, including computer vision, natural language processing, and
graph-structured data. Creating positive examples that assist the model in
learning robust and discriminative representations is a crucial stage in
contrastive learning approaches. Usually, preset human intuition directs the
selection of relevant data augmentations. Due to patterns that are easily
recognized by humans, this rule of thumb works well in the vision and language
domains. However, it is impractical to visually inspect the temporal structures
in time series. The diversity of time series augmentations at both the dataset
and instance levels makes it difficult to choose meaningful augmentations on
the fly. In this study, we address this gap by analyzing time series data
augmentation using information theory and summarizing the most commonly adopted
augmentations in a unified format. We then propose a contrastive learning
framework with parametric augmentation, AutoTCL, which can be adaptively
employed to support time series representation learning. The proposed approach
is encoder-agnostic, allowing it to be seamlessly integrated with different
backbone encoders. Experiments on univariate forecasting tasks demonstrate the
highly competitive results of our method, with an average 6.5\% reduction in
MSE and 4.7\% in MAE over the leading baselines. In classification tasks,
AutoTCL achieves a $1.2\%$ increase in average accuracy.
\\ ( https://arxiv.org/abs/2402.10434 ,  756kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10445
Date: Fri, 16 Feb 2024 04:32:22 GMT   (38kb,D)

Title: Collaborative Learning with Different Labeling Functions
Authors: Yuyang Deng, Mingda Qiao
Categories: cs.LG cs.DS stat.ML
\\
  We study a variant of Collaborative PAC Learning, in which we aim to learn an
accurate classifier for each of the $n$ data distributions, while minimizing
the number of samples drawn from them in total. Unlike in the usual
collaborative learning setup, it is not assumed that there exists a single
classifier that is simultaneously accurate for all distributions.
  We show that, when the data distributions satisfy a weaker realizability
assumption, sample-efficient learning is still feasible. We give a learning
algorithm based on Empirical Risk Minimization (ERM) on a natural augmentation
of the hypothesis class, and the analysis relies on an upper bound on the VC
dimension of this augmented class.
  In terms of the computational efficiency, we show that ERM on the augmented
hypothesis class is NP-hard, which gives evidence against the existence of
computationally efficient learners in general. On the positive side, for two
special cases, we give learners that are both sample- and
computationally-efficient.
\\ ( https://arxiv.org/abs/2402.10445 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10450
Date: Fri, 16 Feb 2024 04:55:09 GMT   (15009kb,D)

Title: PRISE: Learning Temporal Action Abstractions as a Sequence Compression
  Problem
Authors: Ruijie Zheng, Ching-An Cheng, Hal Daum\'e III, Furong Huang, Andrey
  Kolobov
Categories: cs.LG
\\
  Temporal action abstractions, along with belief state representations, are a
powerful knowledge sharing mechanism for sequential decision making. In this
work, we propose a novel view that treats inducing temporal action abstractions
as a sequence compression problem. To do so, we bring a subtle but critical
component of LLM training pipelines -- input tokenization via byte pair
encoding (BPE) -- to the seemingly distant task of learning skills of variable
time span in continuous control domains. We introduce an approach called
Primitive Sequence Encoding (PRISE) that combines continuous action
quantization with BPE to learn powerful action abstractions. We empirically
show that high-level skills discovered by PRISE from a multitask set of robotic
manipulation demonstrations significantly boost the performance of both
multitask imitation learning as well as few-shot imitation learning on unseen
tasks. Our code will be released at https://github.com/FrankZheng2022/PRISE.
\\ ( https://arxiv.org/abs/2402.10450 ,  15009kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10462
Date: Fri, 16 Feb 2024 05:42:17 GMT   (6883kb,D)

Title: QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large
  Language Model Tuning
Authors: Hossein Rajabzadeh, Mojtaba Valipour, Tianshu Zhu, Marzieh Tahaei,
  Hyock Ju Kwon, Ali Ghodsi, Boxing Chen and Mehdi Rezagholizadeh
Categories: cs.LG cs.CL
Comments: Best Paper Award AAAI EIW Workshop
\\
  Finetuning large language models requires huge GPU memory, restricting the
choice to acquire Larger models. While the quantized version of the Low-Rank
Adaptation technique, named QLoRA, significantly alleviates this issue, finding
the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a
pre-defined rank and, therefore, cannot be reconfigured for its lower ranks
without requiring further fine-tuning steps. This paper proposes QDyLoRA
-Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach
for dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to
efficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables
fine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one
round of fine-tuning. Experimental results show that QDyLoRA is competitive to
QLoRA and outperforms when employing its optimal rank.
\\ ( https://arxiv.org/abs/2402.10462 ,  6883kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10464
Date: Fri, 16 Feb 2024 06:00:31 GMT   (2954kb,D)

Title: FedKit: Enabling Cross-Platform Federated Learning for Android and iOS
Authors: Sichang He, Beilong Tang, Boyan Zhang, Jiaoqi Shao, Xiaomin Ouyang,
  Daniel Nata Nugraha, Bing Luo
Categories: cs.LG cs.NI
Comments: This work has been accepted for demonstration on IEEE International
  Conference on Computer Communications (INFOCOM) 2024
\\
  We present FedKit, a federated learning (FL) system tailored for
cross-platform FL research on Android and iOS devices. FedKit pipelines
cross-platform FL development by enabling model conversion,
hardware-accelerated training, and cross-platform model aggregation. Our FL
workflow supports flexible machine learning operations (MLOps) in production,
facilitating continuous model delivery and training. We have deployed FedKit in
a real-world use case for health data analysis on university campuses,
demonstrating its effectiveness. FedKit is open-source at
https://github.com/FedCampus/FedKit.
\\ ( https://arxiv.org/abs/2402.10464 ,  2954kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10468
Date: Fri, 16 Feb 2024 06:17:50 GMT   (3037kb,D)

Title: Adversarial Curriculum Graph Contrastive Learning with Pair-wise
  Augmentation
Authors: Xinjian Zhao, Liang Zhang, Yang Liu, Ruocheng Guo, Xiangyu Zhao
Categories: cs.LG cs.AI
\\
  Graph contrastive learning (GCL) has emerged as a pivotal technique in the
domain of graph representation learning. A crucial aspect of effective GCL is
the caliber of generated positive and negative samples, which is intrinsically
dictated by their resemblance to the original data. Nevertheless, precise
control over similarity during sample generation presents a formidable
challenge, often impeding the effective discovery of representative graph
patterns. To address this challenge, we propose an innovative framework:
Adversarial Curriculum Graph Contrastive Learning (ACGCL), which capitalizes on
the merits of pair-wise augmentation to engender graph-level positive and
negative samples with controllable similarity, alongside subgraph contrastive
learning to discern effective graph patterns therein. Within the ACGCL
framework, we have devised a novel adversarial curriculum training methodology
that facilitates progressive learning by sequentially increasing the difficulty
of distinguishing the generated samples. Notably, this approach transcends the
prevalent sparsity issue inherent in conventional curriculum learning
strategies by adaptively concentrating on more challenging training data.
Finally, a comprehensive assessment of ACGCL is conducted through extensive
experiments on six well-known benchmark datasets, wherein ACGCL conspicuously
surpasses a set of state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2402.10468 ,  3037kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10470
Date: Fri, 16 Feb 2024 06:22:44 GMT   (5580kb,D)

Title: Theoretical Understanding of Learning from Adversarial Perturbations
Authors: Soichiro Kumano, Hiroshi Kera, Toshihiko Yamasaki
Categories: cs.LG cs.CV stat.ML
Comments: ICLR24
\\
  It is not fully understood why adversarial examples can deceive neural
networks and transfer between different networks. To elucidate this, several
studies have hypothesized that adversarial perturbations, while appearing as
noises, contain class features. This is supported by empirical evidence showing
that networks trained on mislabeled adversarial examples can still generalize
well to correctly labeled test samples. However, a theoretical understanding of
how perturbations include class features and contribute to generalization is
limited. In this study, we provide a theoretical framework for understanding
learning from perturbations using a one-hidden-layer network trained on
mutually orthogonal samples. Our results highlight that various adversarial
perturbations, even perturbations of a few pixels, contain sufficient class
features for generalization. Moreover, we reveal that the decision boundary
when learning from perturbations matches that from standard samples except for
specific regions under mild conditions. The code is available at
https://github.com/s-kumano/learning-from-adversarial-perturbations.
\\ ( https://arxiv.org/abs/2402.10470 ,  5580kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10473
Date: Fri, 16 Feb 2024 06:35:10 GMT   (10468kb,D)

Title: Privacy for Fairness: Information Obfuscation for Fair Representation
  Learning with Local Differential Privacy
Authors: Songjie Xie, Youlong Wu, Jiaxuan Li, Ming Ding, Khaled B. Letaief
Categories: cs.LG cs.CR cs.IT math.IT
\\
  As machine learning (ML) becomes more prevalent in human-centric
applications, there is a growing emphasis on algorithmic fairness and privacy
protection. While previous research has explored these areas as separate
objectives, there is a growing recognition of the complex relationship between
privacy and fairness. However, previous works have primarily focused on
examining the interplay between privacy and fairness through empirical
investigations, with limited attention given to theoretical exploration. This
study aims to bridge this gap by introducing a theoretical framework that
enables a comprehensive examination of their interrelation. We shall develop
and analyze an information bottleneck (IB) based information obfuscation method
with local differential privacy (LDP) for fair representation learning. In
contrast to many empirical studies on fairness in ML, we show that the
incorporation of LDP randomizers during the encoding process can enhance the
fairness of the learned representation. Our analysis will demonstrate that the
disclosure of sensitive information is constrained by the privacy budget of the
LDP randomizer, thereby enabling the optimization process within the IB
framework to effectively suppress sensitive information while preserving the
desired utility through obfuscation. Based on the proposed method, we further
develop a variational representation encoding approach that simultaneously
achieves fairness and LDP. Our variational encoding approach offers practical
advantages. It is trained using a non-adversarial method and does not require
the introduction of any variational prior. Extensive experiments will be
presented to validate our theoretical results and demonstrate the ability of
our proposed approach to achieve both LDP and fairness while preserving
adequate utility.
\\ ( https://arxiv.org/abs/2402.10473 ,  10468kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10474
Date: Fri, 16 Feb 2024 06:39:40 GMT   (486kb,D)

Title: One-Bit Quantization and Sparsification for Multiclass Linear
  Classification via Regularized Regression
Authors: Reza Ghane, Danil Akhtiamov, Babak Hassibi
Categories: cs.LG stat.ML
\\
  We study the use of linear regression for multiclass classification in the
over-parametrized regime where some of the training data is mislabeled. In such
scenarios it is necessary to add an explicit regularization term, $\lambda
f(w)$, for some convex function $f(\cdot)$, to avoid overfitting the mislabeled
data. In our analysis, we assume that the data is sampled from a Gaussian
Mixture Model with equal class sizes, and that a proportion $c$ of the training
labels is corrupted for each class. Under these assumptions, we prove that the
best classification performance is achieved when $f(\cdot) = \|\cdot\|^2_2$ and
$\lambda \to \infty$. We then proceed to analyze the classification errors for
$f(\cdot) = \|\cdot\|_1$ and $f(\cdot) = \|\cdot\|_\infty$ in the large
$\lambda$ regime and notice that it is often possible to find sparse and
one-bit solutions, respectively, that perform almost as well as the one
corresponding to $f(\cdot) = \|\cdot\|_2^2$.
\\ ( https://arxiv.org/abs/2402.10474 ,  486kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10477
Date: Fri, 16 Feb 2024 06:56:59 GMT   (18098kb,D)

Title: Understanding Likelihood of Normalizing Flow and Image Complexity
  through the Lens of Out-of-Distribution Detection
Authors: Genki Osada, Tsubasa Takahashi, Takashi Nishide
Categories: cs.LG
Comments: Accepted at AAAI-24
\\
  Out-of-distribution (OOD) detection is crucial to safety-critical machine
learning applications and has been extensively studied. While recent studies
have predominantly focused on classifier-based methods, research on deep
generative model (DGM)-based methods have lagged relatively. This disparity may
be attributed to a perplexing phenomenon: DGMs often assign higher likelihoods
to unknown OOD inputs than to their known training data. This paper focuses on
explaining the underlying mechanism of this phenomenon. We propose a hypothesis
that less complex images concentrate in high-density regions in the latent
space, resulting in a higher likelihood assignment in the Normalizing Flow
(NF). We experimentally demonstrate its validity for five NF architectures,
concluding that their likelihood is untrustworthy. Additionally, we show that
this problem can be alleviated by treating image complexity as an independent
variable. Finally, we provide evidence of the potential applicability of our
hypothesis in another DGM, PixelCNN++.
\\ ( https://arxiv.org/abs/2402.10477 ,  18098kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10482
Date: Fri, 16 Feb 2024 07:13:12 GMT   (260kb,D)

Title: Understanding Self-Distillation and Partial Label Learning in
  Multi-Class Classification with Label Noise
Authors: Hyeonsu Jeong and Hye Won Chung
Categories: cs.LG stat.ML
\\
  Self-distillation (SD) is the process of training a student model using the
outputs of a teacher model, with both models sharing the same architecture. Our
study theoretically examines SD in multi-class classification with
cross-entropy loss, exploring both multi-round SD and SD with refined teacher
outputs, inspired by partial label learning (PLL). By deriving a closed-form
solution for the student model's outputs, we discover that SD essentially
functions as label averaging among instances with high feature correlations.
Initially beneficial, this averaging helps the model focus on feature clusters
correlated with a given instance for predicting the label. However, it leads to
diminishing performance with increasing distillation rounds. Additionally, we
demonstrate SD's effectiveness in label noise scenarios and identify the label
corruption condition and minimum number of distillation rounds needed to
achieve 100% classification accuracy. Our study also reveals that one-step
distillation with refined teacher outputs surpasses the efficacy of multi-step
SD using the teacher's direct output in high noise rate regimes.
\\ ( https://arxiv.org/abs/2402.10482 ,  260kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10487
Date: Fri, 16 Feb 2024 07:28:59 GMT   (4528kb,D)

Title: Random Projection Layers for Multidimensional Time Sires Forecasting
Authors: Chin-Chia Michael Yeh, Yujie Fan, Xin Dai, Vivian Lai, Prince Osei
  Aboagye, Junpeng Wang, Huiyuan Chen, Yan Zheng, Zhongfang Zhuang, Liang Wang,
  Wei Zhang
Categories: cs.LG cs.AI
\\
  All-Multi-Layer Perceptron (all-MLP) mixer models have been shown to be
effective for time series forecasting problems. However, when such a model is
applied to high-dimensional time series (e.g., the time series in a
spatial-temporal dataset), its performance is likely to degrade due to
overfitting issues. In this paper, we propose an all-MLP time series
forecasting architecture, referred to as RPMixer. Our method leverages the
ensemble-like behavior of deep neural networks, where each individual block
within the network acts like a base learner in an ensemble model, especially
when identity mapping residual connections are incorporated. By integrating
random projection layers into our model, we increase the diversity among the
blocks' outputs, thereby enhancing the overall performance of RPMixer.
Extensive experiments conducted on large-scale spatial-temporal forecasting
benchmark datasets demonstrate that our proposed method outperforms alternative
methods, including both spatial-temporal graph models and general forecasting
models.
\\ ( https://arxiv.org/abs/2402.10487 ,  4528kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10492
Date: Fri, 16 Feb 2024 07:48:59 GMT   (570kb)

Title: Developing an Optimal Model for Predicting the Severity of Wheat Stem
  Rust (Case study of Arsi and Bale Zone)
Authors: Tewodrose Altaye
Categories: cs.LG cs.AI
\\
  This research utilized three types of artificial neural network (ANN)
methodologies, namely Backpropagation Neural Network (BPNN) with varied
training, transfer, divide, and learning functions; Radial Basis Function
Neural Network (RBFNN); and General Regression Neural Network (GRNN), to
forecast the severity of stem rust. It considered parameters such as mean
maximum temperature, mean minimum temperature, mean rainfall, mean average
temperature, mean relative humidity, and different wheat varieties. The
statistical analysis revealed that GRNN demonstrated effective predictive
capability and required less training time compared to the other models.
Additionally, the results indicated that total seasonal rainfall positively
influenced the development of wheat stem rust.
  Keywords: Wheat stem rust, Back propagation neural network, Radial Basis
Function Neural Network, General Regression Neural Network.
\\ ( https://arxiv.org/abs/2402.10492 ,  570kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10500
Date: Fri, 16 Feb 2024 08:19:34 GMT   (982kb,D)

Title: Provably Sample Efficient RLHF via Active Preference Optimization
Authors: Nirjhar Das, Souradip Chakraborty, Aldo Pacchiano, Sayak Ray Chowdhury
Categories: cs.LG cs.AI cs.CL
\\
  Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning
Large Language Models (LLMs) with human preferences. While these aligned
generative models have demonstrated impressive capabilities across various
tasks, the dependence on high-quality human preference data poses a costly
bottleneck in practical implementation of RLHF. Hence better and adaptive
strategies for data collection is needed. To this end, we frame RLHF as a
contextual preference bandit problem with prompts as contexts and show that the
naive way of collecting preference data by choosing prompts uniformly at random
leads to a policy that suffers an $\Omega(1)$ suboptimality gap in rewards.
Then we propose $\textit{Active Preference Optimization}$ ($\texttt{APO}$), an
algorithm that actively selects prompts to collect preference data. Under the
Bradley-Terry-Luce (BTL) preference model, \texttt{APO} achieves sample
efficiency without compromising on policy performance. We show that given a
sample budget of $T$, the suboptimality gap of a policy learned via
$\texttt{APO}$ scales as $O(1/\sqrt{T})$. Next, we propose a compute-efficient
batch version of $\texttt{APO}$ with minor modification and evaluate its
performance in practice. Experimental evaluations on a human preference dataset
validate \texttt{APO}'s efficacy as a sample-efficient and practical solution
to data collection for RLHF, facilitating alignment of LLMs with human
preferences in a cost-effective and scalable manner.
\\ ( https://arxiv.org/abs/2402.10500 ,  982kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10511
Date: Fri, 16 Feb 2024 08:56:22 GMT   (1054kb,D)

Title: Can Transformers Predict Vibrations?
Authors: Fusataka Kuniyoshi, Yoshihide Sawada
Categories: cs.LG cs.AI
\\
  Highly accurate time-series vibration prediction is an important research
issue for electric vehicles (EVs). EVs often experience vibrations when driving
on rough terrains, known as torsional resonance. This resonance, caused by the
interaction between motor and tire vibrations, puts excessive loads on the
vehicle's drive shaft. However, current damping technologies only detect
resonance after the vibration amplitude of the drive shaft torque reaches a
certain threshold, leading to significant loads on the shaft at the time of
detection. In this study, we propose a novel approach to address this issue by
introducing Resoformer, a transformer-based model for predicting torsional
resonance. Resoformer utilizes time-series of the motor rotation speed as input
and predicts the amplitude of torsional vibration at a specified quantile
occurring in the shaft after the input series. By calculating the attention
between recursive and convolutional features extracted from the measured data
points, Resoformer improves the accuracy of vibration forecasting. To evaluate
the model, we use a vibration dataset called VIBES (Dataset for Forecasting
Vibration Transition in EVs), consisting of 2,600 simulator-generated vibration
sequences. Our experiments, conducted on strong baselines built on the VIBES
dataset, demonstrate that Resoformer achieves state-of-the-art results. In
conclusion, our study answers the question "Can Transformers Forecast
Vibrations?" While traditional transformer architectures show low performance
in forecasting torsional resonance waves, our findings indicate that combining
recurrent neural network and temporal convolutional network using the
transformer architecture improves the accuracy of long-term vibration
forecasting.
\\ ( https://arxiv.org/abs/2402.10511 ,  1054kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10517
Date: Fri, 16 Feb 2024 09:06:06 GMT   (1643kb,D)

Title: Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs
Authors: Yeonhong Park, Jake Hyun, SangLyul Cho, Bonggeun Sim, Jae W. Lee
Categories: cs.LG
\\
  Recently, considerable efforts have been directed towards compressing Large
Language Models (LLMs), which showcase groundbreaking capabilities across
diverse applications but entail significant deployment costs due to their large
sizes. Meanwhile, much less attention has been given to mitigating the costs
associated with deploying multiple LLMs of varying sizes despite its practical
significance. Thus, this paper introduces \emph{any-precision LLM}, extending
the concept of any-precision DNN to LLMs. Addressing challenges in
any-precision LLM, we propose a lightweight method for any-precision
quantization of LLMs, leveraging a post-training quantization framework, and
develop a specialized software engine for its efficient serving. As a result,
our solution significantly reduces the high costs of deploying multiple,
different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such
as 3, 4, ..., $n$ bits, into a memory footprint comparable to a single $n$-bit
LLM. All the supported LLMs with varying bit-widths demonstrate
state-of-the-art model quality and inference throughput, proving itself to be a
compelling option for deployment of multiple, different-sized LLMs. The source
code will be publicly available soon.
\\ ( https://arxiv.org/abs/2402.10517 ,  1643kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10551
Date: Fri, 16 Feb 2024 10:29:25 GMT   (17057kb,D)

Title: Personalised Drug Identifier for Cancer Treatment with Transformers
  using Auxiliary Information
Authors: Aishwarya Jayagopal, Hansheng Xue, Ziyang He, Robert J. Walsh, Krishna
  Kumar Hariprasannan, David Shao Peng Tan, Tuan Zea Tan, Jason J. Pitt, Anand
  D. Jeyasekharan, Vaibhav Rajan
Categories: cs.LG q-bio.QM
\\
  Cancer remains a global challenge due to its growing clinical and economic
burden. Its uniquely personal manifestation, which makes treatment difficult,
has fuelled the quest for personalized treatment strategies. Thus, genomic
profiling is increasingly becoming part of clinical diagnostic panels.
Effective use of such panels requires accurate drug response prediction (DRP)
models, which are challenging to build due to limited labelled patient data.
Previous methods to address this problem have used various forms of transfer
learning. However, they do not explicitly model the variable length sequential
structure of the list of mutations in such diagnostic panels. Further, they do
not utilize auxiliary information (like patient survival) for model training.
We address these limitations through a novel transformer based method, which
surpasses the performance of state-of-the-art DRP models on benchmark data. We
also present the design of a treatment recommendation system (TRS), which is
currently deployed at the National University Hospital, Singapore and is being
evaluated in a clinical trial.
\\ ( https://arxiv.org/abs/2402.10551 ,  17057kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10575
Date: Fri, 16 Feb 2024 11:04:31 GMT   (602kb,D)

Title: Symbolic Autoencoding for Self-Supervised Sequence Learning
Authors: Mohammad Hossein Amani, Nicolas Mario Baldwin, Amin Mansouri, Martin
  Josifoski, Maxime Peyrard, Robert West
Categories: cs.LG cs.AI
\\
  Traditional language models, adept at next-token prediction in text
sequences, often struggle with transduction tasks between distinct symbolic
systems, particularly when parallel data is scarce. Addressing this issue, we
introduce \textit{symbolic autoencoding} ($\Sigma$AE), a self-supervised
framework that harnesses the power of abundant unparallel data alongside
limited parallel data. $\Sigma$AE connects two generative models via a discrete
bottleneck layer and is optimized end-to-end by minimizing reconstruction loss
(simultaneously with supervised loss for the parallel data), such that the
sequence generated by the discrete bottleneck can be read out as the transduced
input sequence. We also develop gradient-based methods allowing for efficient
self-supervised sequence learning despite the discreteness of the bottleneck.
Our results demonstrate that $\Sigma$AE significantly enhances performance on
transduction tasks, even with minimal parallel data, offering a promising
solution for weakly supervised learning scenarios.
\\ ( https://arxiv.org/abs/2402.10575 ,  602kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10592
Date: Fri, 16 Feb 2024 11:27:48 GMT   (244kb,D)

Title: Optimizing Adaptive Experiments: A Unified Approach to Regret
  Minimization and Best-Arm Identification
Authors: Chao Qin, Daniel Russo
Categories: cs.LG econ.EM stat.ML
\\
  Practitioners conducting adaptive experiments often encounter two competing
priorities: reducing the cost of experimentation by effectively assigning
treatments during the experiment itself, and gathering information swiftly to
conclude the experiment and implement a treatment across the population.
Currently, the literature is divided, with studies on regret minimization
addressing the former priority in isolation, and research on best-arm
identification focusing solely on the latter. This paper proposes a unified
model that accounts for both within-experiment performance and post-experiment
outcomes. We then provide a sharp theory of optimal performance in large
populations that unifies canonical results in the literature. This unification
also uncovers novel insights. For example, the theory reveals that familiar
algorithms, like the recently proposed top-two Thompson sampling algorithm, can
be adapted to optimize a broad class of objectives by simply adjusting a single
scalar parameter. In addition, the theory reveals that enormous reductions in
experiment duration can sometimes be achieved with minimal impact on both
within-experiment and post-experiment regret.
\\ ( https://arxiv.org/abs/2402.10592 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10617
Date: Fri, 16 Feb 2024 12:11:34 GMT   (271kb)

Title: Multitask Kernel-based Learning with Logic Constraints
Authors: Michelangelo Diligenti, Marco Gori, Marco Maggini, Leonardo Rigutini
Categories: cs.LG cs.AI
Comments: The 19th European Conference on Artificial Intelligence (ECAI 2010)
Journal-ref: Proceedings of the 19th European Conference on Artificial
  Intelligence (ECAI 2010)
DOI: 10.3233/978-1-60750-606-5-433
\\
  This paper presents a general framework to integrate prior knowledge in the
form of logic constraints among a set of task functions into kernel machines.
The logic propositions provide a partial representation of the environment, in
which the learner operates, that is exploited by the learning algorithm
together with the information available in the supervised examples. In
particular, we consider a multi-task learning scheme, where multiple unary
predicates on the feature space are to be learned by kernel machines and a
higher level abstract representation consists of logic clauses on these
predicates, known to hold for any input. A general approach is presented to
convert the logic clauses into a continuous implementation, that processes the
outputs computed by the kernel-based predicates. The learning task is
formulated as a primal optimization problem of a loss function that combines a
term measuring the fitting of the supervised examples, a regularization term,
and a penalty term that enforces the constraints on both supervised and
unsupervised examples. The proposed semi-supervised learning framework is
particularly suited for learning in high dimensionality feature spaces, where
the supervised training examples tend to be sparse and generalization
difficult. Unlike for standard kernel machines, the cost function to optimize
is not generally guaranteed to be convex. However, the experimental results
show that it is still possible to find good solutions using a two stage
learning schema, in which first the supervised examples are learned until
convergence and then the logic constraints are forced. Some promising
experimental results on artificial multi-task learning tasks are reported,
showing how the classification accuracy can be effectively improved by
exploiting the a priori rules and the unsupervised examples.
\\ ( https://arxiv.org/abs/2402.10617 ,  271kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10634
Date: Fri, 16 Feb 2024 12:33:31 GMT   (3304kb,D)

Title: Graph-based Forecasting with Missing Data through Spatiotemporal
  Downsampling
Authors: Ivan Marisca, Cesare Alippi, Filippo Maria Bianchi
Categories: cs.LG cs.AI
\\
  Given a set of synchronous time series, each associated with a sensor-point
in space and characterized by inter-series relationships, the problem of
spatiotemporal forecasting consists of predicting future observations for each
point. Spatiotemporal graph neural networks achieve striking results by
representing the relationships across time series as a graph. Nonetheless, most
existing methods rely on the often unrealistic assumption that inputs are
always available and fail to capture hidden spatiotemporal dynamics when part
of the data is missing. In this work, we tackle this problem through
hierarchical spatiotemporal downsampling. The input time series are
progressively coarsened over time and space, obtaining a pool of
representations that capture heterogeneous temporal and spatial dynamics.
Conditioned on observations and missing data patterns, such representations are
combined by an interpretable attention mechanism to generate the forecasts. Our
approach outperforms state-of-the-art methods on synthetic and real-world
benchmarks under different missing data distributions, particularly in the
presence of contiguous blocks of missing values.
\\ ( https://arxiv.org/abs/2402.10634 ,  3304kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10635
Date: Fri, 16 Feb 2024 12:34:38 GMT   (1370kb,D)

Title: ContiFormer: Continuous-Time Transformer for Irregular Time Series
  Modeling
Authors: Yuqi Chen, Kan Ren, Yansen Wang, Yuchen Fang, Weiwei Sun, Dongsheng Li
Categories: cs.LG cs.AI
Comments: Neurips 2023 Poster
\\
  Modeling continuous-time dynamics on irregular time series is critical to
account for data evolution and correlations that occur continuously.
Traditional methods including recurrent neural networks or Transformer models
leverage inductive bias via powerful neural architectures to capture complex
patterns. However, due to their discrete characteristic, they have limitations
in generalizing to continuous-time data paradigms. Though neural ordinary
differential equations (Neural ODEs) and their variants have shown promising
results in dealing with irregular time series, they often fail to capture the
intricate correlations within these sequences. It is challenging yet demanding
to concurrently model the relationship between input data points and capture
the dynamic changes of the continuous-time system. To tackle this problem, we
propose ContiFormer that extends the relation modeling of vanilla Transformer
to the continuous-time domain, which explicitly incorporates the modeling
abilities of continuous dynamics of Neural ODEs with the attention mechanism of
Transformers. We mathematically characterize the expressive power of
ContiFormer and illustrate that, by curated designs of function hypothesis,
many Transformer variants specialized in irregular time series modeling can be
covered as a special case of ContiFormer. A wide range of experiments on both
synthetic and real-world datasets have illustrated the superior modeling
capacities and prediction performance of ContiFormer on irregular time series
data. The project link is https://seqml.github.io/contiformer/.
\\ ( https://arxiv.org/abs/2402.10635 ,  1370kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10644
Date: Fri, 16 Feb 2024 12:44:15 GMT   (6583kb,D)

Title: Linear Transformers with Learnable Kernel Functions are Better
  In-Context Models
Authors: Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina,
  Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov
Categories: cs.LG
\\
  Advancing the frontier of subquadratic architectures for Language Models
(LMs) is crucial in the rapidly evolving field of natural language processing.
Current innovations, including State Space Models, were initially celebrated
for surpassing Transformer performance on language modeling tasks. However,
these models have revealed deficiencies in essential In-Context Learning
capabilities - a domain where the Transformer traditionally shines. The Based
model emerged as a hybrid solution, blending a Linear Transformer with a kernel
inspired by the Taylor expansion of exponential functions, augmented by
convolutional networks. Mirroring the Transformer's in-context adeptness, it
became a strong contender in the field. In our work, we present a singular,
elegant alteration to the Based kernel that amplifies its In-Context Learning
abilities evaluated with the Multi-Query Associative Recall task and overall
language modeling process, as demonstrated on the Pile dataset.
\\ ( https://arxiv.org/abs/2402.10644 ,  6583kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10665
Date: Fri, 16 Feb 2024 13:14:12 GMT   (920kb,D)

Title: Selective Prediction for Semantic Segmentation using Post-Hoc Confidence
  Estimation and Its Performance under Distribution Shift
Authors: Bruno Laboissiere Camargos Borges, Bruno Machado Pacheco, Danilo Silva
Categories: cs.LG cs.CV
\\
  Semantic segmentation plays a crucial role in various computer vision
applications, yet its efficacy is often hindered by the lack of high-quality
labeled data. To address this challenge, a common strategy is to leverage
models trained on data from different populations, such as publicly available
datasets. This approach, however, leads to the distribution shift problem,
presenting a reduced performance on the population of interest. In scenarios
where model errors can have significant consequences, selective prediction
methods offer a means to mitigate risks and reduce reliance on expert
supervision. This paper investigates selective prediction for semantic
segmentation in low-resource settings, thus focusing on post-hoc confidence
estimators applied to pre-trained models operating under distribution shift. We
propose a novel image-level confidence measure tailored for semantic
segmentation and demonstrate its effectiveness through experiments on three
medical imaging tasks. Our findings show that post-hoc confidence estimators
offer a cost-effective approach to reducing the impacts of distribution shift.
\\ ( https://arxiv.org/abs/2402.10665 ,  920kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10681
Date: Fri, 16 Feb 2024 13:34:51 GMT   (11299kb,D)

Title: Physics-informed MeshGraphNets (PI-MGNs): Neural finite element solvers
  for non-stationary and nonlinear simulations on arbitrary meshes
Authors: Tobias W\"urth, Niklas Freymuth, Clemens Zimmerling, Gerhard Neumann,
  Luise K\"arger
Categories: cs.LG cs.AI
Comments: Submitted to CMAME
\\
  Engineering components must meet increasing technological demands in ever
shorter development cycles. To face these challenges, a holistic approach is
essential that allows for the concurrent development of part design, material
system and manufacturing process. Current approaches employ numerical
simulations, which however quickly becomes computation-intensive, especially
for iterative optimization. Data-driven machine learning methods can be used to
replace time- and resource-intensive numerical simulations. In particular,
MeshGraphNets (MGNs) have shown promising results. They enable fast and
accurate predictions on unseen mesh geometries while being fully differentiable
for optimization. However, these models rely on large amounts of expensive
training data, such as numerical simulations. Physics-informed neural networks
(PINNs) offer an opportunity to train neural networks with partial differential
equations instead of labeled data, but have not been extended yet to handle
time-dependent simulations of arbitrary meshes. This work introduces PI-MGNs, a
hybrid approach that combines PINNs and MGNs to quickly and accurately solve
non-stationary and nonlinear partial differential equations (PDEs) on arbitrary
meshes. The method is exemplified for thermal process simulations of unseen
parts with inhomogeneous material distribution. Further results show that the
model scales well to large and complex meshes, although it is trained on small
generic meshes only.
\\ ( https://arxiv.org/abs/2402.10681 ,  11299kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10695
Date: Fri, 16 Feb 2024 13:58:23 GMT   (650kb,D)

Title: Unlink to Unlearn: Simplifying Edge Unlearning in GNNs
Authors: Jiajun Tan, Fei Sun, Ruichen Qiu, Du Su, Huawei Shen
Categories: cs.LG cs.AI cs.CR
\\
  As concerns over data privacy intensify, unlearning in Graph Neural Networks
(GNNs) has emerged as a prominent research frontier in academia. This concept
is pivotal in enforcing the right to be forgotten, which entails the selective
removal of specific data from trained GNNs upon user request. Our research
focuses on edge unlearning, a process of particular relevance to real-world
applications, owing to its widespread applicability. Current state-of-the-art
approaches like GNNDelete can eliminate the influence of specific edges, yet
our research has revealed a critical limitation in these approaches, termed
over-forgetting. It occurs when the unlearning process inadvertently removes
excessive information beyond specific data, leading to a significant decline in
prediction accuracy for the remaining edges. To address this issue, we have
identified the loss functions of GNNDelete as the primary source of the
over-forgetting phenomenon. Furthermore, our analysis also suggests that loss
functions may not be essential for effective edge unlearning. Building on these
insights, we have simplified GNNDelete to develop Unlink-to-Unlearn (UtU), a
novel method that facilitates unlearning exclusively through unlinking the
forget edges from graph structure. Our extensive experiments demonstrate that
UtU delivers privacy protection on par with that of a retrained model while
preserving high accuracy in downstream tasks. Specifically, UtU upholds over
97.3% of the retrained model's privacy protection capabilities and 99.8% of its
link prediction accuracy. Meanwhile, UtU requires only constant computational
demands, underscoring its advantage as a highly lightweight and practical edge
unlearning solution.
\\ ( https://arxiv.org/abs/2402.10695 ,  650kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10724
Date: Fri, 16 Feb 2024 14:30:46 GMT   (2424kb,D)

Title: Machine Learning based Prediction of Ditching Loads
Authors: Henning Schwarz, Micha \"Uberr\"uck, Jens-Peter M. Zemke, Thomas Rung
Categories: cs.LG
\\
  We present approaches to predict dynamic ditching loads on aircraft fuselages
using machine learning. The employed learning procedure is structured into two
parts, the reconstruction of the spatial loads using a convolutional
autoencoder (CAE) and the transient evolution of these loads in a subsequent
part. Different CAE strategies are assessed and combined with either long
short-term memory (LSTM) networks or Koopman-operator based methods to predict
the transient behaviour. The training data is compiled by an extension of the
momentum method of von-Karman and Wagner and the rationale of the training
approach is briefly summarised. The application included refers to a full-scale
fuselage of a DLR-D150 aircraft for a range of horizontal and vertical approach
velocities at 6{\deg} incidence. Results indicate a satisfactory level of
predictive agreement for all four investigated surrogate models examined, with
the combination of an LSTM and a deep decoder CAE showing the best performance.
\\ ( https://arxiv.org/abs/2402.10724 ,  2424kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10747
Date: Fri, 16 Feb 2024 15:13:30 GMT   (1457kb,D)

Title: Fully Differentiable Lagrangian Convolutional Neural Network for
  Continuity-Consistent Physics-Informed Precipitation Nowcasting
Authors: Peter Pavl\'ik, Martin V\'yboh, Anna Bou Ezzeddine, Viera Rozinajov\'a
Categories: cs.LG cs.AI cs.CV
Comments: Submitted to ICML 2024
ACM-class: I.2.1; J.2
\\
  This paper presents a convolutional neural network model for precipitation
nowcasting that combines data-driven learning with physics-informed domain
knowledge. We propose LUPIN, a Lagrangian Double U-Net for Physics-Informed
Nowcasting, that draws from existing extrapolation-based nowcasting methods and
implements the Lagrangian coordinate system transformation of the data in a
fully differentiable and GPU-accelerated manner to allow for real-time
end-to-end training and inference. Based on our evaluation, LUPIN matches and
exceeds the performance of the chosen benchmark, opening the door for other
Lagrangian machine learning models.
\\ ( https://arxiv.org/abs/2402.10747 ,  1457kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10756
Date: Fri, 16 Feb 2024 15:25:56 GMT   (667kb,D)

Title: Towards Cohesion-Fairness Harmony: Contrastive Regularization in
  Individual Fair Graph Clustering
Authors: Siamak Ghodsi, Seyed Amjad Seyedi, and Eirini Ntoutsi
Categories: cs.LG cs.AI cs.IT cs.SI math.IT
Comments: To be published in "The 28th Pacific-Asia Conference on Knowledge
  Discovery and Data Mining (PAKDD 2024)"
\\
  Conventional fair graph clustering methods face two primary challenges: i)
They prioritize balanced clusters at the expense of cluster cohesion by
imposing rigid constraints, ii) Existing methods of both individual and
group-level fairness in graph partitioning mostly rely on eigen decompositions
and thus, generally lack interpretability. To address these issues, we propose
iFairNMTF, an individual Fairness Nonnegative Matrix Tri-Factorization model
with contrastive fairness regularization that achieves balanced and cohesive
clusters. By introducing fairness regularization, our model allows for
customizable accuracy-fairness trade-offs, thereby enhancing user autonomy
without compromising the interpretability provided by nonnegative matrix
tri-factorization. Experimental evaluations on real and synthetic datasets
demonstrate the superior flexibility of iFairNMTF in achieving fairness and
clustering performance.
\\ ( https://arxiv.org/abs/2402.10756 ,  667kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10765
Date: Fri, 16 Feb 2024 15:39:51 GMT   (992kb,D)

Title: Policy Learning for Off-Dynamics RL with Deficient Support
Authors: Linh Le Pham Van and Hung The Tran and Sunil Gupta
Categories: cs.LG cs.AI
Comments: Accepted by AAMAS 2024 as a full paper
\\
  Reinforcement Learning (RL) can effectively learn complex policies. However,
learning these policies often demands extensive trial-and-error interactions
with the environment. In many real-world scenarios, this approach is not
practical due to the high costs of data collection and safety concerns. As a
result, a common strategy is to transfer a policy trained in a low-cost, rapid
source simulator to a real-world target environment. However, this process
poses challenges. Simulators, no matter how advanced, cannot perfectly
replicate the intricacies of the real world, leading to dynamics discrepancies
between the source and target environments. Past research posited that the
source domain must encompass all possible target transitions, a condition we
term full support. However, expecting full support is often unrealistic,
especially in scenarios where significant dynamics discrepancies arise. In this
paper, our emphasis shifts to addressing large dynamics mismatch adaptation. We
move away from the stringent full support condition of earlier research,
focusing instead on crafting an effective policy for the target domain. Our
proposed approach is simple but effective. It is anchored in the central
concepts of the skewing and extension of source support towards target support
to mitigate support deficiencies. Through comprehensive testing on a varied set
of benchmarks, our method's efficacy stands out, showcasing notable
improvements over previous techniques.
\\ ( https://arxiv.org/abs/2402.10765 ,  992kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10774
Date: Fri, 16 Feb 2024 15:55:59 GMT   (620kb,D)

Title: Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness
  Constants
Authors: Peter Richt\'arik, Elnur Gasanov, Konstantin Burlachenko
Categories: cs.LG cs.AI math.OC stat.ML
Comments: 70 pages, 14 figures, 6 tables
MSC-class: 90C26, 74Pxx
ACM-class: G.1.6; I.2.11; I.2.m
\\
  Error Feedback (EF) is a highly popular and immensely effective mechanism for
fixing convergence issues which arise in distributed training methods (such as
distributed GD or SGD) when these are enhanced with greedy communication
compression techniques such as TopK. While EF was proposed almost a decade ago
(Seide et al., 2014), and despite concentrated effort by the community to
advance the theoretical understanding of this mechanism, there is still a lot
to explore. In this work we study a modern form of error feedback called EF21
(Richtarik et al., 2021) which offers the currently best-known theoretical
guarantees, under the weakest assumptions, and also works well in practice. In
particular, while the theoretical communication complexity of EF21 depends on
the quadratic mean of certain smoothness parameters, we improve this dependence
to their arithmetic mean, which is always smaller, and can be substantially
smaller, especially in heterogeneous data regimes. We take the reader on a
journey of our discovery process. Starting with the idea of applying EF21 to an
equivalent reformulation of the underlying problem which (unfortunately)
requires (often impractical) machine cloning, we continue to the discovery of a
new weighted version of EF21 which can (fortunately) be executed without any
cloning, and finally circle back to an improved analysis of the original EF21
method. While this development applies to the simplest form of EF21, our
approach naturally extends to more elaborate variants involving stochastic
gradients and partial participation. Further, our technique improves the
best-known theory of EF21 in the rare features regime (Richtarik et al., 2023).
Finally, we validate our theoretical findings with suitable experiments.
\\ ( https://arxiv.org/abs/2402.10774 ,  620kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10787
Date: Fri, 16 Feb 2024 16:10:38 GMT   (639kb,D)

Title: EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for
  the Acceleration of Lightweight LLMs on the Edge
Authors: Xuan Shen, Zhenglun Kong, Changdi Yang, Zhaoyang Han, Lei Lu, Peiyan
  Dong, Cheng Lyu, Chih-hsiang Li, Xuehang Guo, Zhihao Shu, Wei Niu, Miriam
  Leeser, Pu Zhao, Yanzhi Wang
Categories: cs.LG cs.AI cs.CL
Comments: Preprint
\\
  Despite the remarkable strides of Large Language Models (LLMs) in various
fields, the wide applications of LLMs on edge devices are limited due to their
massive parameters and computations. To address this, quantization is commonly
adopted to generate lightweight LLMs with efficient computations and fast
inference. However, Post-Training Quantization (PTQ) methods dramatically
degrade in quality when quantizing weights, activations, and KV cache together
to below 8 bits. Besides, many Quantization-Aware Training (QAT) works quantize
model weights, leaving the activations untouched, which do not fully exploit
the potential of quantization for inference acceleration on the edge. In this
paper, we propose EdgeQAT, the Entropy and Distribution Guided QAT for the
optimization of lightweight LLMs to achieve inference acceleration on Edge
devices. We first identify that the performance drop of quantization primarily
stems from the information distortion in quantized attention maps, demonstrated
by the different distributions in quantized query and key of the self-attention
mechanism. Then, the entropy and distribution guided QAT is proposed to
mitigate the information distortion. Moreover, we design a token
importance-aware adaptive method to dynamically quantize the tokens with
different bit widths for further optimization and acceleration. Our extensive
experiments verify the substantial improvements with our framework across
various datasets. Furthermore, we achieve an on-device speedup of up to 2.37x
compared with its FP16 counterparts across multiple edge devices, signaling a
groundbreaking advancement.
\\ ( https://arxiv.org/abs/2402.10787 ,  639kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10793
Date: Fri, 16 Feb 2024 16:20:11 GMT   (613kb,D)

Title: Masked Attention is All You Need for Graphs
Authors: David Buterez, Jon Paul Janet, Dino Oglic, Pietro Lio
Categories: cs.LG cs.AI
\\
  Graph neural networks (GNNs) and variations of the message passing algorithm
are the predominant means for learning on graphs, largely due to their
flexibility, speed, and satisfactory performance. The design of powerful and
general purpose GNNs, however, requires significant research efforts and often
relies on handcrafted, carefully-chosen message passing operators. Motivated by
this, we propose a remarkably simple alternative for learning on graphs that
relies exclusively on attention. Graphs are represented as node or edge sets
and their connectivity is enforced by masking the attention weight matrix,
effectively creating custom attention patterns for each graph. Despite its
simplicity, masked attention for graphs (MAG) has state-of-the-art performance
on long-range tasks and outperforms strong message passing baselines and much
more involved attention-based methods on over 55 node and graph-level tasks. We
also show significantly better transfer learning capabilities compared to GNNs
and comparable or better time and memory scaling. MAG has sub-linear memory
scaling in the number of nodes or edges, enabling learning on dense graphs and
future-proofing the approach.
\\ ( https://arxiv.org/abs/2402.10793 ,  613kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10795
Date: Fri, 16 Feb 2024 16:20:43 GMT   (3117kb,D)

Title: Diversified Ensembling: An Experiment in Crowdsourced Machine Learning
Authors: Ira Globus-Harris, Declan Harrison, Michael Kearns, Pietro Perona,
  Aaron Roth
Categories: cs.LG cs.CY
\\
  Crowdsourced machine learning on competition platforms such as Kaggle is a
popular and often effective method for generating accurate models. Typically,
teams vie for the most accurate model, as measured by overall error on a
holdout set, and it is common towards the end of such competitions for teams at
the top of the leaderboard to ensemble or average their models outside the
platform mechanism to get the final, best global model. In arXiv:2201.10408,
the authors developed an alternative crowdsourcing framework in the context of
fair machine learning, in order to integrate community feedback into models
when subgroup unfairness is present and identifiable. There, unlike in
classical crowdsourced ML, participants deliberately specialize their efforts
by working on subproblems, such as demographic subgroups in the service of
fairness. Here, we take a broader perspective on this work: we note that within
this framework, participants may both specialize in the service of fairness and
simply to cater to their particular expertise (e.g., focusing on identifying
bird species in an image classification task). Unlike traditional
crowdsourcing, this allows for the diversification of participants' efforts and
may provide a participation mechanism to a larger range of individuals (e.g. a
machine learning novice who has insight into a specific fairness concern). We
present the first medium-scale experimental evaluation of this framework, with
46 participating teams attempting to generate models to predict income from
American Community Survey data. We provide an empirical analysis of teams'
approaches, and discuss the novel system architecture we developed. From here,
we give concrete guidance for how best to deploy such a framework.
\\ ( https://arxiv.org/abs/2402.10795 ,  3117kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10802
Date: Fri, 16 Feb 2024 16:25:20 GMT   (2821kb,D)

Title: TimeSeriesBench: An Industrial-Grade Benchmark for Time Series Anomaly
  Detection Models
Authors: Haotian Si, Changhua Pei, Hang Cui, Jingwen Yang, Yongqian Sun,
  Shenglin Zhang, Jingjing Li, Haiming Zhang, Jing Han, Dan Pei, Jianhui Li,
  Gaogang Xie
Categories: cs.LG
\\
  Driven by the proliferation of real-world application scenarios and scales,
time series anomaly detection (TSAD) has attracted considerable scholarly and
industrial interest. However, existing algorithms exhibit a gap in terms of
training paradigm, online detection paradigm, and evaluation criteria when
compared to the actual needs of real-world industrial systems. Firstly, current
algorithms typically train a specific model for each individual time series. In
a large-scale online system with tens of thousands of curves, maintaining such
a multitude of models is impractical. The performance of using merely one
single unified model to detect anomalies remains unknown. Secondly, most TSAD
models are trained on the historical part of a time series and are tested on
its future segment. In distributed systems, however, there are frequent system
deployments and upgrades, with new, previously unseen time series emerging
daily. The performance of testing newly incoming unseen time series on current
TSAD algorithms remains unknown. Lastly, although some papers have conducted
detailed surveys, the absence of an online evaluation platform prevents
answering questions like "Who is the best at anomaly detection at the current
stage?" In this paper, we propose TimeSeriesBench, an industrial-grade
benchmark that we continuously maintain as a leaderboard. On this leaderboard,
we assess the performance of existing algorithms across more than 168
evaluation settings combining different training and testing paradigms,
evaluation metrics and datasets. Through our comprehensive analysis of the
results, we provide recommendations for the future design of anomaly detection
algorithms. To address known issues with existing public datasets, we release
an industrial dataset to the public together with TimeSeriesBench. All code,
data, and the online leaderboard have been made publicly available.
\\ ( https://arxiv.org/abs/2402.10802 ,  2821kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10810
Date: Fri, 16 Feb 2024 16:35:18 GMT   (60kb)

Title: Double Duality: Variational Primal-Dual Policy Optimization for
  Constrained Reinforcement Learning
Authors: Zihao Li, Boyi Liu, Zhuoran Yang, Zhaoran Wang, Mengdi Wang
Categories: cs.LG math.OC stat.ML
\\
  We study the Constrained Convex Markov Decision Process (MDP), where the goal
is to minimize a convex functional of the visitation measure, subject to a
convex constraint. Designing algorithms for a constrained convex MDP faces
several challenges, including (1) handling the large state space, (2) managing
the exploration/exploitation tradeoff, and (3) solving the constrained
optimization where the objective and the constraint are both nonlinear
functions of the visitation measure. In this work, we present a model-based
algorithm, Variational Primal-Dual Policy Optimization (VPDPO), in which
Lagrangian and Fenchel duality are implemented to reformulate the original
constrained problem into an unconstrained primal-dual optimization. Moreover,
the primal variables are updated by model-based value iteration following the
principle of Optimism in the Face of Uncertainty (OFU), while the dual
variables are updated by gradient ascent. Moreover, by embedding the visitation
measure into a finite-dimensional space, we can handle large state spaces by
incorporating function approximation. Two notable examples are (1) Kernelized
Nonlinear Regulators and (2) Low-rank MDPs. We prove that with an optimistic
planning oracle, our algorithm achieves sublinear regret and constraint
violation in both cases and can attain the globally optimal policy of the
original constrained problem.
\\ ( https://arxiv.org/abs/2402.10810 ,  60kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10814
Date: Fri, 16 Feb 2024 16:37:48 GMT   (6090kb,D)

Title: Associative Memories in the Feature Space
Authors: Tommaso Salvatori, Beren Millidge, Yuhang Song, Rafal Bogacz, Thomas
  Lukasiewicz
Categories: cs.LG
Comments: 8 Pages, 4 Figures, accepted for publication at ECAI 2023
\\
  An autoassociative memory model is a function that, given a set of data
points, takes as input an arbitrary vector and outputs the most similar data
point from the memorized set. However, popular memory models fail to retrieve
images even when the corruption is mild and easy to detect for a human
evaluator. This is because similarities are evaluated in the raw pixel space,
which does not contain any semantic information about the images. This problem
can be easily solved by computing \emph{similarities} in an embedding space
instead of the pixel space. We show that an effective way of computing such
embeddings is via a network pretrained with a contrastive loss. As the
dimension of embedding spaces is often significantly smaller than the pixel
space, we also have a faster computation of similarity scores. We test this
method on complex datasets such as CIFAR10 and STL10. An additional drawback of
current models is the need of storing the whole dataset in the pixel space,
which is often extremely large. We relax this condition and propose a class of
memory models that only stores low-dimensional semantic embeddings, and uses
them to retrieve similar, but not identical, memories. We demonstrate a proof
of concept of this method on a simple task on the MNIST dataset.
\\ ( https://arxiv.org/abs/2402.10814 ,  6090kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10816
Date: Fri, 16 Feb 2024 16:41:14 GMT   (519kb,D)

Title: TernaryVote: Differentially Private, Communication Efficient, and
  Byzantine Resilient Distributed Optimization on Heterogeneous Data
Authors: Richeng Jin, Yujie Gu, Kai Yue, Xiaofan He, Zhaoyang Zhang, Huaiyu Dai
Categories: cs.LG cs.CR eess.SP
\\
  Distributed training of deep neural networks faces three critical challenges:
privacy preservation, communication efficiency, and robustness to fault and
adversarial behaviors. Although significant research efforts have been devoted
to addressing these challenges independently, their synthesis remains less
explored. In this paper, we propose TernaryVote, which combines a ternary
compressor and the majority vote mechanism to realize differential privacy,
gradient compression, and Byzantine resilience simultaneously. We theoretically
quantify the privacy guarantee through the lens of the emerging f-differential
privacy (DP) and the Byzantine resilience of the proposed algorithm.
Particularly, in terms of privacy guarantees, compared to the existing
sign-based approach StoSign, the proposed method improves the dimension
dependence on the gradient size and enjoys privacy amplification by mini-batch
sampling while ensuring a comparable convergence rate. We also prove that
TernaryVote is robust when less than 50% of workers are blind attackers, which
matches that of SIGNSGD with majority vote. Extensive experimental results
validate the effectiveness of the proposed algorithm.
\\ ( https://arxiv.org/abs/2402.10816 ,  519kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10818
Date: Fri, 16 Feb 2024 16:42:09 GMT   (498kb,D)

Title: Trading off Consistency and Dimensionality of Convex Surrogates for the
  Mode
Authors: Enrique Nueve, Bo Waggoner, Dhamma Kimpara, Jessie Finocchiaro
Categories: cs.LG stat.ML
\\
  In multiclass classification over $n$ outcomes, the outcomes must be embedded
into the reals with dimension at least $n-1$ in order to design a consistent
surrogate loss that leads to the "correct" classification, regardless of the
data distribution. For large $n$, such as in information retrieval and
structured prediction tasks, optimizing a surrogate in $n-1$ dimensions is
often intractable. We investigate ways to trade off surrogate loss dimension,
the number of problem instances, and restricting the region of consistency in
the simplex for multiclass classification. Following past work, we examine an
intuitive embedding procedure that maps outcomes into the vertices of convex
polytopes in a low-dimensional surrogate space. We show that full-dimensional
subsets of the simplex exist around each point mass distribution for which
consistency holds, but also, with less than $n-1$ dimensions, there exist
distributions for which a phenomenon called hallucination occurs, which is when
the optimal report under the surrogate loss is an outcome with zero
probability. Looking towards application, we derive a result to check if
consistency holds under a given polytope embedding and low-noise assumption,
providing insight into when to use a particular embedding. We provide examples
of embedding $n = 2^{d}$ outcomes into the $d$-dimensional unit cube and $n =
d!$ outcomes into the $d$-dimensional permutahedron under low-noise
assumptions. Finally, we demonstrate that with multiple problem instances, we
can learn the mode with $\frac{n}{2}$ dimensions over the whole simplex.
\\ ( https://arxiv.org/abs/2402.10818 ,  498kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10820
Date: Fri, 16 Feb 2024 16:46:53 GMT   (1222kb,D)

Title: Goal-Conditioned Offline Reinforcement Learning via Metric Learning
Authors: Alfredo Reichlin, Miguel Vasco, Hang Yin, Danica Kragic
Categories: cs.LG
\\
  In this work, we address the problem of learning optimal behavior from
sub-optimal datasets in the context of goal-conditioned offline reinforcement
learning. To do so, we propose a novel way of approximating the optimal value
function for goal-conditioned offline RL problems under sparse rewards,
symmetric and deterministic actions. We study a property for representations to
recover optimality and propose a new optimization objective that leads to such
property. We use the learned value function to guide the learning of a policy
in an actor-critic fashion, a method we name MetricRL. Experimentally, we show
how our method consistently outperforms other offline RL baselines in learning
from sub-optimal offline datasets. Moreover, we show the effectiveness of our
method in dealing with high-dimensional observations and in multi-goal tasks.
\\ ( https://arxiv.org/abs/2402.10820 ,  1222kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10846
Date: Fri, 16 Feb 2024 17:36:51 GMT   (1932kb,D)

Title: FedD2S: Personalized Data-Free Federated Knowledge Distillation
Authors: Kawa Atapour, S. Jamal Seyedmohammadi, Jamshid Abouei, Arash
  Mohammadi, Konstantinos N. Plataniotis
Categories: cs.LG cs.AI cs.DC eess.IV
\\
  This paper addresses the challenge of mitigating data heterogeneity among
clients within a Federated Learning (FL) framework. The model-drift issue,
arising from the noniid nature of client data, often results in suboptimal
personalization of a global model compared to locally trained models for each
client. To tackle this challenge, we propose a novel approach named FedD2S for
Personalized Federated Learning (pFL), leveraging knowledge distillation.
FedD2S incorporates a deep-to-shallow layer-dropping mechanism in the data-free
knowledge distillation process to enhance local model personalization. Through
extensive simulations on diverse image datasets-FEMNIST, CIFAR10, CINIC0, and
CIFAR100-we compare FedD2S with state-of-the-art FL baselines. The proposed
approach demonstrates superior performance, characterized by accelerated
convergence and improved fairness among clients. The introduced layer-dropping
technique effectively captures personalized knowledge, resulting in enhanced
performance compared to alternative FL models. Moreover, we investigate the
impact of key hyperparameters, such as the participation ratio and
layer-dropping rate, providing valuable insights into the optimal configuration
for FedD2S. The findings demonstrate the efficacy of adaptive layer-dropping in
the knowledge distillation process to achieve enhanced personalization and
performance across diverse datasets and tasks.
\\ ( https://arxiv.org/abs/2402.10846 ,  1932kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10862
Date: Fri, 16 Feb 2024 18:00:04 GMT   (538kb,D)

Title: Differential Private Federated Transfer Learning for Mental Health
  Monitoring in Everyday Settings: A Case Study on Stress Detection
Authors: Ziyu Wang, Zhongqi Yang, Iman Azimi, Amir M. Rahmani
Categories: cs.LG cs.CR
Comments: 4 pages, 2 figures
\\
  Mental health conditions, prevalent across various demographics, necessitate
efficient monitoring to mitigate their adverse impacts on life quality. The
surge in data-driven methodologies for mental health monitoring has underscored
the importance of privacy-preserving techniques in handling sensitive health
data. Despite strides in federated learning for mental health monitoring,
existing approaches struggle with vulnerabilities to certain cyber-attacks and
data insufficiency in real-world applications. In this paper, we introduce a
differential private federated transfer learning framework for mental health
monitoring to enhance data privacy and enrich data sufficiency. To accomplish
this, we integrate federated learning with two pivotal elements: (1)
differential privacy, achieved by introducing noise into the updates, and (2)
transfer learning, employing a pre-trained universal model to adeptly address
issues of data imbalance and insufficiency. We evaluate the framework by a case
study on stress detection, employing a dataset of physiological and contextual
data from a longitudinal study. Our finding show that the proposed approach can
attain a 10% boost in accuracy and a 21% enhancement in recall, while ensuring
privacy protection.
\\ ( https://arxiv.org/abs/2402.10862 ,  538kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10870
Date: Fri, 16 Feb 2024 18:13:35 GMT   (6312kb,D)

Title: Best of Three Worlds: Adaptive Experimentation for Digital Marketing in
  Practice
Authors: Tanner Fiez, Houssam Nassif, Arick Chen, Sergio Gamez, Lalit Jain
Categories: cs.LG
\\
  Adaptive experimental design (AED) methods are increasingly being used in
industry as a tool to boost testing throughput or reduce experimentation cost
relative to traditional A/B/N testing methods. However, the behavior and
guarantees of such methods are not well-understood beyond idealized stationary
settings. This paper shares lessons learned regarding the challenges of naively
using AED systems in industrial settings where non-stationarity is prevalent,
while also providing perspectives on the proper objectives and system
specifications in such settings. We developed an AED framework for
counterfactual inference based on these experiences, and tested it in a
commercial environment.
\\ ( https://arxiv.org/abs/2402.10870 ,  6312kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10893
Date: Fri, 16 Feb 2024 18:50:24 GMT   (7046kb,D)

Title: RLVF: Learning from Verbal Feedback without Overgeneralization
Authors: Moritz Stephan, Alexander Khazatsky, Eric Mitchell, Annie S Chen,
  Sheryl Hsu, Archit Sharma, Chelsea Finn
Categories: cs.LG cs.AI
Comments: 9 pages, 9 figures
\\
  The diversity of contexts in which large language models (LLMs) are deployed
requires the ability to modify or customize default model behaviors to
incorporate nuanced requirements and preferences. A convenient interface to
specify such model adjustments is high-level verbal feedback, such as "Don't
use emojis when drafting emails to my boss." However, while writing high-level
feedback is far simpler than collecting annotations for reinforcement learning
from human feedback (RLHF), we find that simply prompting a model with such
feedback leads to overgeneralization of the feedback to contexts where it is
not relevant. We study the problem of incorporating verbal feedback without
such overgeneralization, inspiring a new method Contextualized Critiques with
Constrained Preference Optimization (C3PO). C3PO uses a piece of high-level
feedback to generate a small synthetic preference dataset specifying how the
feedback should (and should not) be applied. It then fine-tunes the model in
accordance with the synthetic preference data while minimizing the divergence
from the original model for prompts where the feedback does not apply. Our
experimental results indicate that our approach effectively applies verbal
feedback to relevant scenarios while preserving existing behaviors for other
contexts. For both human- and GPT-4-generated high-level feedback, C3PO
effectively adheres to the given feedback comparably to in-context baselines
while reducing overgeneralization by 30%.
\\ ( https://arxiv.org/abs/2402.10893 ,  7046kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2402.09430 (*cross-listing*)
Date: Wed, 24 Jan 2024 16:10:14 GMT   (17164kb,D)

Title: WiMANS: A Benchmark Dataset for WiFi-based Multi-user Activity Sensing
Authors: Shuokang Huang, Kaihan Li, Di You, Yichong Chen, Arvin Lin, Siying
  Liu, Xiaohui Li, Julie A. McCann
Categories: eess.SP cs.AI cs.CV cs.MM
Comments: We present WiMANS, to our knowledge, the first dataset for multi-user
  activity sensing based on WiFi
\\
  WiFi-based human sensing has exhibited remarkable potential to analyze user
behaviors in a non-intrusive and device-free manner, benefiting applications as
diverse as smart homes and healthcare. However, most previous works focus on
single-user sensing, which has limited practicability in scenarios involving
multiple users. Although recent studies have begun to investigate WiFi-based
multi-user activity sensing, there remains a lack of benchmark datasets to
facilitate reproducible and comparable research. To bridge this gap, we present
WiMANS, to our knowledge, the first dataset for multi-user activity sensing
based on WiFi. WiMANS contains over 9.4 hours of WiFi Channel State Information
(CSI), monitoring simultaneous activities performed by multiple users in
various environments. Compared to existing datasets, WiMANS not only collects
the CSI of dual WiFi bands but also includes synchronized videos. We exploit
WiMANS to benchmark the performance of state-of-the-art WiFi-based human
sensing models and video-based models, posing new challenges and opportunities
for WiFi-based multi-user identification, localization, and activity
recognition. We believe that WiMANS can push the boundaries of current
WiFi-based human sensing and catalyze the research on multi-user activity
analysis.
\\ ( https://arxiv.org/abs/2402.09430 ,  17164kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10213 (*cross-listing*)
Date: Thu, 30 Nov 2023 02:02:30 GMT   (1323kb,D)

Title: Clustering Inductive Biases with Unrolled Networks
Authors: Jonathan Huml, Abiy Tasissa, Demba Ba
Categories: q-bio.NC cs.AI cs.LG
\\
  The classical sparse coding (SC) model represents visual stimuli as a linear
combination of a handful of learned basis functions that are Gabor-like when
trained on natural image data. However, the Gabor-like filters learned by
classical sparse coding far overpredict well-tuned simple cell receptive field
profiles observed empirically. While neurons fire sparsely, neuronal
populations are also organized in physical space by their sensitivity to
certain features. In V1, this organization is a smooth progression of
orientations along the cortical sheet. A number of subsequent models have
either discarded the sparse dictionary learning framework entirely or whose
updates have yet to take advantage of the surge in unrolled, neural dictionary
learning architectures. A key missing theme of these updates is a stronger
notion of \emph{structured sparsity}. We propose an autoencoder architecture
(WLSC) whose latent representations are implicitly, locally organized for
spectral clustering through a Laplacian quadratic form of a bipartite graph,
which generates a diverse set of artificial receptive fields that match primate
data in V1 as faithfully as recent contrastive frameworks like Local Low
Dimensionality, or LLD \citep{lld} that discard sparse dictionary learning. By
unifying sparse and smooth coding in models of the early visual cortex through
our autoencoder, we also show that our regularization can be interpreted as
early-stage specialization of receptive fields to certain classes of stimuli;
that is, we induce a weak clustering bias for later stages of cortex where
functional and spatial segregation (i.e. topography) are known to occur. The
results show an imperative for \emph{spatial regularization} of both the
receptive fields and firing rates to begin to describe feature disentanglement
in V1 and beyond.
\\ ( https://arxiv.org/abs/2402.10213 ,  1323kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10222 (*cross-listing*)
Date: Sun, 28 Jan 2024 14:29:30 GMT   (1053kb,D)

Title: Autonomous Vehicle Patrolling Through Deep Reinforcement Learning:
  Learning to Communicate and Cooperate
Authors: Chenhao Tong, Maria A. Rodriguez, Richard O. Sinnott
Categories: cs.RO cs.AI cs.MA
\\
  Autonomous vehicles are suited for continuous area patrolling problems.
Finding an optimal patrolling strategy can be challenging due to unknown
environmental factors, such as wind or landscape; or autonomous vehicles'
constraints, such as limited battery life or hardware failures. Importantly,
patrolling large areas often requires multiple agents to collectively
coordinate their actions. However, an optimal coordination strategy is often
non-trivial to be manually defined due to the complex nature of patrolling
environments. In this paper, we consider a patrolling problem with
environmental factors, agent limitations, and three typical cooperation
problems -- collision avoidance, congestion avoidance, and patrolling target
negotiation. We propose a multi-agent reinforcement learning solution based on
a reinforced inter-agent learning (RIAL) method. With this approach, agents are
trained to develop their own communication protocol to cooperate during
patrolling where faults can and do occur. The solution is validated through
simulation experiments and is compared with several state-of-the-art patrolling
solutions from different perspectives, including the overall patrol
performance, the collision avoidance performance, the efficiency of battery
recharging strategies, and the overall fault tolerance.
\\ ( https://arxiv.org/abs/2402.10222 ,  1053kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10224 (*cross-listing*)
Date: Tue, 30 Jan 2024 07:52:38 GMT   (1316kb,D)

Title: Human-Centric Goal Reasoning with Ripple-Down Rules
Authors: Kenji Brameld, Germ\'an Castro, Claude Sammut, Mark Roberts, David W.
  Aha
Categories: cs.RO cs.AI cs.MA
Comments: Proceedings of the Ninth Goal Reasoning Workshop (Advances in
  Cognitive Systems, 2021)
\\
  ActorSim is a goal reasoning framework developed at the Naval Research
Laboratory. Originally, all goal reasoning rules were hand-crafted. This work
extends ActorSim with the capability of learning by demonstration, that is,
when a human trainer disagrees with a decision made by the system, the trainer
can take over and show the system the correct decision. The learning component
uses Ripple-Down Rules (RDR) to build new decision rules to correctly handle
similar cases in the future. The system is demonstrated using the RoboCup
Rescue Agent Simulation, which simulates a city-wide disaster, requiring
emergency services, including fire, ambulance and police, to be dispatched to
different sites to evacuate civilians from dangerous situations. The RDRs are
implemented in a scripting language, FrameScript, which is used to mediate
between ActorSim and the agent simulator. Using Ripple-Down Rules, ActorSim can
scale to an order of magnitude more goals than the previous version.
\\ ( https://arxiv.org/abs/2402.10224 ,  1316kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10236 (*cross-listing*)
Date: Wed, 14 Feb 2024 14:30:42 GMT   (3529kb,D)

Title: Discovering Sensorimotor Agency in Cellular Automata using Diversity
  Search
Authors: Gautier Hamon, Mayalen Etcheverry, Bert Wang-Chak Chan, Cl\'ement
  Moulin-Frier, Pierre-Yves Oudeyer
Categories: cs.MA cs.AI cs.LG
\\
  The research field of Artificial Life studies how life-like phenomena such as
autopoiesis, agency, or self-regulation can self-organize in computer
simulations. In cellular automata (CA), a key open-question has been whether it
it is possible to find environment rules that self-organize robust
"individuals" from an initial state with no prior existence of things like
"bodies", "brain", "perception" or "action". In this paper, we leverage recent
advances in machine learning, combining algorithms for diversity search,
curriculum learning and gradient descent, to automate the search of such
"individuals", i.e. localized structures that move around with the ability to
react in a coherent manner to external obstacles and maintain their integrity,
hence primitive forms of sensorimotor agency. We show that this approach
enables to find systematically environmental conditions in CA leading to
self-organization of such basic forms of agency. Through multiple experiments,
we show that the discovered agents have surprisingly robust capabilities to
move, maintain their body integrity and navigate among various obstacles. They
also show strong generalization abilities, with robustness to changes of scale,
random updates or perturbations from the environment not seen during training.
We discuss how this approach opens new perspectives in AI and synthetic
bioengineering.
\\ ( https://arxiv.org/abs/2402.10236 ,  3529kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10251 (*cross-listing*)
Date: Thu, 15 Feb 2024 16:04:11 GMT   (5572kb,D)

Title: Brant-2: Foundation Model for Brain Signals
Authors: Zhizhang Yuan, Daoze Zhang, Junru Chen, Geifei Gu, Yang Yang
Categories: q-bio.NC cs.AI cs.LG eess.SP
Comments: 14 pages, 7 figures
\\
  Foundational models benefit from pre-training on large amounts of unlabeled
data and enable strong performance in a wide variety of applications with a
small amount of labeled data. Such models can be particularly effective in
analyzing brain signals, as this field encompasses numerous application
scenarios, and it is costly to perform large-scale annotation. In this work, we
present the largest foundation model in brain signals, Brant-2. Compared to
Brant, a foundation model designed for intracranial neural signals, Brant-2 not
only exhibits robustness towards data variations and modeling scales but also
can be applied to a broader range of brain neural data. By experimenting on an
extensive range of tasks, we demonstrate that Brant-2 is adaptive to various
application scenarios in brain signals. Further analyses reveal the scalability
of the Brant-2, validate each component's effectiveness, and showcase our
model's ability to maintain performance in scenarios with scarce labels. The
source code and pre-trained weights are available at:
https://anonymous.4open.science/r/Brant-2-5843.
\\ ( https://arxiv.org/abs/2402.10251 ,  5572kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10294 (*cross-listing*)
Date: Thu, 15 Feb 2024 19:53:11 GMT   (14791kb,D)

Title: LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video
  Editing
Authors: Bryan Wang, Yuliang Li, Zhaoyang Lv, Haijun Xia, Yan Xu, Raj Sodhi
Categories: cs.HC cs.AI cs.CL cs.MM
Comments: Paper accepted to the ACM Conference on Intelligent User Interfaces
  (ACM IUI) 2024
DOI: 10.1145/3640543.3645143
\\
  Video creation has become increasingly popular, yet the expertise and effort
required for editing often pose barriers to beginners. In this paper, we
explore the integration of large language models (LLMs) into the video editing
workflow to reduce these barriers. Our design vision is embodied in LAVE, a
novel system that provides LLM-powered agent assistance and language-augmented
editing features. LAVE automatically generates language descriptions for the
user's footage, serving as the foundation for enabling the LLM to process
videos and assist in editing tasks. When the user provides editing objectives,
the agent plans and executes relevant actions to fulfill them. Moreover, LAVE
allows users to edit videos through either the agent or direct UI manipulation,
providing flexibility and enabling manual refinement of agent actions. Our user
study, which included eight participants ranging from novices to proficient
editors, demonstrated LAVE's effectiveness. The results also shed light on user
perceptions of the proposed LLM-assisted editing paradigm and its impact on
users' creativity and sense of co-creation. Based on these findings, we propose
design implications to inform the future development of agent-assisted content
editing.
\\ ( https://arxiv.org/abs/2402.10294 ,  14791kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10334 (*cross-listing*)
Date: Thu, 15 Feb 2024 21:43:56 GMT   (33184kb,D)

Title: HI-GAN: Hierarchical Inpainting GAN with Auxiliary Inputs for Combined
  RGB and Depth Inpainting
Authors: Ankan Dash, Jingyi Gu and Guiling Wang
Categories: cs.CV cs.AI cs.LG
\\
  Inpainting involves filling in missing pixels or areas in an image, a crucial
technique employed in Mixed Reality environments for various applications,
particularly in Diminished Reality (DR) where content is removed from a user's
visual environment. Existing methods rely on digital replacement techniques
which necessitate multiple cameras and incur high costs. AR devices and
smartphones use ToF depth sensors to capture scene depth maps aligned with RGB
images. Despite speed and affordability, ToF cameras create imperfect depth
maps with missing pixels. To address the above challenges, we propose
Hierarchical Inpainting GAN (HI-GAN), a novel approach comprising three GANs in
a hierarchical fashion for RGBD inpainting. EdgeGAN and LabelGAN inpaint masked
edge and segmentation label images respectively, while CombinedRGBD-GAN
combines their latent representation outputs and performs RGB and Depth
inpainting. Edge images and particularly segmentation label images as auxiliary
inputs significantly enhance inpainting performance by complementary context
and hierarchical optimization. We believe we make the first attempt to
incorporate label images into inpainting process.Unlike previous approaches
requiring multiple sequential models and separate outputs, our work operates in
an end-to-end manner, training all three models simultaneously and
hierarchically. Specifically, EdgeGAN and LabelGAN are first optimized
separately and further optimized inside CombinedRGBD-GAN to enhance inpainting
quality. Experiments demonstrate that HI-GAN works seamlessly and achieves
overall superior performance compared with existing approaches.
\\ ( https://arxiv.org/abs/2402.10334 ,  33184kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10340 (*cross-listing*)
Date: Thu, 15 Feb 2024 22:01:45 GMT   (5613kb,D)

Title: On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting
  the Risks and Vulnerabilities
Authors: Xiyang Wu, Ruiqi Xian, Tianrui Guan, Jing Liang, Souradip Chakraborty,
  Fuxiao Liu, Brian Sadler, Dinesh Manocha, Amrit Singh Bedi
Categories: cs.RO cs.AI
\\
  In this paper, we highlight the critical issues of robustness and safety
associated with integrating large language models (LLMs) and vision-language
models (VLMs) into robotics applications. Recent works have focused on using
LLMs and VLMs to improve the performance of robotics tasks, such as
manipulation, navigation, etc. However, such integration can introduce
significant vulnerabilities, in terms of their susceptibility to adversarial
attacks due to the language models, potentially leading to catastrophic
consequences. By examining recent works at the interface of LLMs/VLMs and
robotics, we show that it is easy to manipulate or misguide the robot's
actions, leading to safety hazards. We define and provide examples of several
plausible adversarial attacks, and conduct experiments on three prominent robot
frameworks integrated with a language model, including KnowNo VIMA, and
Instruct2Act, to assess their susceptibility to these attacks. Our empirical
findings reveal a striking vulnerability of LLM/VLM-robot integrated systems:
simple adversarial attacks can significantly undermine the effectiveness of
LLM/VLM-robot integrated systems. Specifically, our data demonstrate an average
performance deterioration of 21.2% under prompt attacks and a more alarming
30.2% under perception attacks. These results underscore the critical need for
robust countermeasures to ensure the safe and reliable deployment of the
advanced LLM/VLM-based robotic systems.
\\ ( https://arxiv.org/abs/2402.10340 ,  5613kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10381 (*cross-listing*)
Date: Fri, 16 Feb 2024 00:25:53 GMT   (16127kb,D)

Title: UMAIR-FPS: User-aware Multi-modal Animation Illustration Recommendation
  Fusion with Painting Style
Authors: Yan Kang, Hao Lin, Mingjian Yang, Shin-Jye Lee
Categories: cs.IR cs.AI
\\
  The rapid advancement of high-quality image generation models based on AI has
generated a deluge of anime illustrations. Recommending illustrations to users
within massive data has become a challenging and popular task. However,
existing anime recommendation systems have focused on text features but still
need to integrate image features. In addition, most multi-modal recommendation
research is constrained by tightly coupled datasets, limiting its applicability
to anime illustrations. We propose the User-aware Multi-modal Animation
Illustration Recommendation Fusion with Painting Style (UMAIR-FPS) to tackle
these gaps. In the feature extract phase, for image features, we are the first
to combine image painting style features with semantic features to construct a
dual-output image encoder for enhancing representation. For text features, we
obtain text embeddings based on fine-tuning Sentence-Transformers by
incorporating domain knowledge that composes a variety of domain text pairs
from multilingual mappings, entity relationships, and term explanation
perspectives, respectively. In the multi-modal fusion phase, we novelly propose
a user-aware multi-modal contribution measurement mechanism to weight
multi-modal features dynamically according to user features at the interaction
level and employ the DCN-V2 module to model bounded-degree multi-modal crosses
effectively. UMAIR-FPS surpasses the stat-of-the-art baselines on large
real-world datasets, demonstrating substantial performance enhancements.
\\ ( https://arxiv.org/abs/2402.10381 ,  16127kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10393 (*cross-listing*)
Date: Fri, 16 Feb 2024 01:27:21 GMT   (4380kb)

Title: Darwin Turing Dawkins: Building a General Theory of Evolution
Authors: Leonard M. Adleman
Categories: cs.GL cs.AI q-bio.PE
Comments: 247 pages
\\
  Living things, computers, societies, and even books are part of a grand
evolutionary struggle to survive. That struggle shapes nature, nations,
religions, art, science, and you. What you think, feel, and do is determined by
it. Darwinian evolution does not apply solely to the genes that are stored in
DNA. Using the insights of Alan Turing and Richard Dawkins, we will see that it
also applies to the memes we store in our brains and the information we store
in our computers. The next time you run for president, fight a war, or just
deal with the ordinary problems humans are heir to, perhaps this book will be
of use. If you want to understand why and when you will die, or if you want to
achieve greatness this book may help. If you are concerned about where the
computer revolution is headed, this book may provide some answers.
\\ ( https://arxiv.org/abs/2402.10393 ,  4380kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10404 (*cross-listing*)
Date: Fri, 16 Feb 2024 02:12:20 GMT   (8123kb,D)

Title: Explaining generative diffusion models via visual analysis for
  interpretable decision-making process
Authors: Ji-Hoon Park, Yeong-Joon Ju, and Seong-Whan Lee
Categories: cs.CV cs.AI
Comments: 22 pages, published in Expert Systems with Applications
MSC-class: 68T01
Journal-ref: Expert Systems with Applications 248 (2024) 123231
DOI: 10.1016/j.eswa.2024.123231
\\
  Diffusion models have demonstrated remarkable performance in generation
tasks. Nevertheless, explaining the diffusion process remains challenging due
to it being a sequence of denoising noisy images that are difficult for experts
to interpret. To address this issue, we propose the three research questions to
interpret the diffusion process from the perspective of the visual concepts
generated by the model and the region where the model attends in each time
step. We devise tools for visualizing the diffusion process and answering the
aforementioned research questions to render the diffusion process
human-understandable. We show how the output is progressively generated in the
diffusion process by explaining the level of denoising and highlighting
relationships to foundational visual concepts at each time step through the
results of experiments with various visual analyses using the tools. Throughout
the training of the diffusion model, the model learns diverse visual concepts
corresponding to each time-step, enabling the model to predict varying levels
of visual concepts at different stages. We substantiate our tools using Area
Under Cover (AUC) score, correlation quantification, and cross-attention
mapping. Our findings provide insights into the diffusion process and pave the
way for further research into explainable diffusion mechanisms.
\\ ( https://arxiv.org/abs/2402.10404 ,  8123kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10423 (*cross-listing*)
Date: Fri, 16 Feb 2024 03:12:22 GMT   (36kb)

Title: Connect the dots: Dataset Condensation, Differential Privacy, and
  Adversarial Uncertainty
Authors: Kenneth Odoh
Categories: cs.CR cs.AI
\\
  Our work focuses on understanding the underpinning mechanism of dataset
condensation by drawing connections with ($\epsilon$, $\delta$)-differential
privacy where the optimal noise, $\epsilon$, is chosen by adversarial
uncertainty \cite{Grining2017}. We can answer the question about the inner
workings of the dataset condensation procedure. Previous work \cite{dong2022}
proved the link between dataset condensation (DC) and ($\epsilon$,
$\delta$)-differential privacy. However, it is unclear from existing works on
ablating DC to obtain a lower-bound estimate of $\epsilon$ that will suffice
for creating high-fidelity synthetic data. We suggest that adversarial
uncertainty is the most appropriate method to achieve an optimal noise level,
$\epsilon$. As part of the internal dynamics of dataset condensation, we adopt
a satisfactory scheme for noise estimation that guarantees high-fidelity data
while providing privacy.
\\ ( https://arxiv.org/abs/2402.10423 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10481 (*cross-listing*)
Date: Fri, 16 Feb 2024 07:05:49 GMT   (1319kb)

Title: Emoji Driven Crypto Assets Market Reactions
Authors: Xiaorui Zuo, Yao-Tsung Chen, and Wolfgang Karl H\"ardle
Categories: q-fin.CP cs.AI cs.CL cs.LG q-fin.ST
\\
  In the burgeoning realm of cryptocurrency, social media platforms like
Twitter have become pivotal in influencing market trends and investor
sentiments. In our study, we leverage GPT-4 and a fine-tuned transformer-based
BERT model for a multimodal sentiment analysis, focusing on the impact of emoji
sentiment on cryptocurrency markets. By translating emojis into quantifiable
sentiment data, we correlate these insights with key market indicators like BTC
Price and the VCRIX index. This approach may be fed into the development of
trading strategies aimed at utilizing social media elements to identify and
forecast market trends. Crucially, our findings suggest that strategies based
on emoji sentiment can facilitate the avoidance of significant market downturns
and contribute to the stabilization of returns. This research underscores the
practical benefits of integrating advanced AI-driven analyses into financial
strategies, offering a nuanced perspective on the interplay between digital
communication and market dynamics in an academic context.
\\ ( https://arxiv.org/abs/2402.10481 ,  1319kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10510 (*cross-listing*)
Date: Fri, 16 Feb 2024 08:55:23 GMT   (922kb,D)

Title: Human Goal Recognition as Bayesian Inference: Investigating the Impact
  of Actions, Timing, and Goal Solvability
Authors: Chenyuan Zhang, Charles Kemp, Nir Lipovetzky
Categories: cs.HC cs.AI
Comments: Accepted by AAMAS 2024
\\
  Goal recognition is a fundamental cognitive process that enables individuals
to infer intentions based on available cues. Current goal recognition
algorithms often take only observed actions as input, but here we use a
Bayesian framework to explore the role of actions, timing, and goal solvability
in goal recognition. We analyze human responses to goal-recognition problems in
the Sokoban domain, and find that actions are assigned most importance, but
that timing and solvability also influence goal recognition in some cases,
especially when actions are uninformative. We leverage these findings to
develop a goal recognition model that matches human inferences more closely
than do existing algorithms. Our work provides new insight into human goal
recognition and takes a step towards more human-like AI models.
\\ ( https://arxiv.org/abs/2402.10510 ,  922kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10515 (*cross-listing*)
Date: Fri, 16 Feb 2024 09:04:04 GMT   (1081kb,D)

Title: Power-Efficient Indoor Localization Using Adaptive Channel-aware
  Ultra-wideband DL-TDOA
Authors: Sagnik Bhattacharya, Junyoung Choi, Joohyun Lee
Categories: eess.SP cs.AI
Journal-ref: IEEE GLOBECOM 2023
\\
  Among the various Ultra-wideband (UWB) ranging methods, the absence of uplink
communication or centralized computation makes downlink
time-difference-of-arrival (DL-TDOA) localization the most suitable for
large-scale industrial deployments. However, temporary or permanent obstacles
in the deployment region often lead to non-line-of-sight (NLOS) channel path
and signal outage effects, which result in localization errors. Prior research
has addressed this problem by increasing the ranging frequency, which leads to
a heavy increase in the user device power consumption. It also does not
contribute to any increase in localization accuracy under line-of-sight (LOS)
conditions. In this paper, we propose and implement a novel low-power
channel-aware dynamic frequency DL-TDOA ranging algorithm. It comprises NLOS
probability predictor based on a convolutional neural network (CNN), a dynamic
ranging frequency control module, and an IMU sensor-based ranging filter. Based
on the conducted experiments, we show that the proposed algorithm achieves 50%
higher accuracy in NLOS conditions while having 46% lower power consumption in
LOS conditions compared to baseline methods from prior research.
\\ ( https://arxiv.org/abs/2402.10515 ,  1081kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10516 (*cross-listing*)
Date: Fri, 16 Feb 2024 09:05:02 GMT   (422kb,D)

Title: Generative AI for Controllable Protein Sequence Design: A Survey
Authors: Yiheng Zhu, Zitai Kong, Jialu Wu, Weize Liu, Yuqiang Han, Mingze Yin,
  Hongxia Xu, Chang-Yu Hsieh and Tingjun Hou
Categories: q-bio.BM cs.AI cs.LG
Comments: 9 pages
\\
  The design of novel protein sequences with targeted functionalities underpins
a central theme in protein engineering, impacting diverse fields such as drug
discovery and enzymatic engineering. However, navigating this vast
combinatorial search space remains a severe challenge due to time and financial
constraints. This scenario is rapidly evolving as the transformative
advancements in AI, particularly in the realm of generative models and
optimization algorithms, have been propelling the protein design field towards
an unprecedented revolution. In this survey, we systematically review recent
advances in generative AI for controllable protein sequence design. To set the
stage, we first outline the foundational tasks in protein sequence design in
terms of the constraints involved and present key generative models and
optimization algorithms. We then offer in-depth reviews of each design task and
discuss the pertinent applications. Finally, we identify the unresolved
challenges and highlight research opportunities that merit deeper exploration.
\\ ( https://arxiv.org/abs/2402.10516 ,  422kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10524 (*cross-listing*)
Date: Fri, 16 Feb 2024 09:14:49 GMT   (648kb,D)

Title: LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large
  Language Models
Authors: Minsuk Kahng, Ian Tenney, Mahima Pushkarna, Michael Xieyang Liu, James
  Wexler, Emily Reif, Krystal Kallarackal, Minsuk Chang, Michael Terry, Lucas
  Dixon
Categories: cs.HC cs.AI cs.CL cs.LG
\\
  Automatic side-by-side evaluation has emerged as a promising approach to
evaluating the quality of responses from large language models (LLMs). However,
analyzing the results from this evaluation approach raises scalability and
interpretability challenges. In this paper, we present LLM Comparator, a novel
visual analytics tool for interactively analyzing results from automatic
side-by-side evaluation. The tool supports interactive workflows for users to
understand when and why a model performs better or worse than a baseline model,
and how the responses from two models are qualitatively different. We
iteratively designed and developed the tool by closely working with researchers
and engineers at a large technology company. This paper details the user
challenges we identified, the design and development of the tool, and an
observational study with participants who regularly evaluate their models.
\\ ( https://arxiv.org/abs/2402.10524 ,  648kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10580 (*cross-listing*)
Date: Fri, 16 Feb 2024 11:09:16 GMT   (4318kb,D)

Title: Efficient Multi-task Uncertainties for Joint Semantic Segmentation and
  Monocular Depth Estimation
Authors: Steven Landgraf, Markus Hillemann, Theodor Kapler, Markus Ulrich
Categories: cs.CV cs.AI cs.LG
Comments: 17 pages, 5 figures, 10 tables, submitted to peer-reviewed journal
\\
  Quantifying the predictive uncertainty emerged as a possible solution to
common challenges like overconfidence or lack of explainability and robustness
of deep neural networks, albeit one that is often computationally expensive.
Many real-world applications are multi-modal in nature and hence benefit from
multi-task learning. In autonomous driving, for example, the joint solution of
semantic segmentation and monocular depth estimation has proven to be valuable.
In this work, we first combine different uncertainty quantification methods
with joint semantic segmentation and monocular depth estimation and evaluate
how they perform in comparison to each other. Additionally, we reveal the
benefits of multi-task learning with regard to the uncertainty quality compared
to solving both tasks separately. Based on these insights, we introduce
EMUFormer, a novel student-teacher distillation approach for joint semantic
segmentation and monocular depth estimation as well as efficient multi-task
uncertainty quantification. By implicitly leveraging the predictive
uncertainties of the teacher, EMUFormer achieves new state-of-the-art results
on Cityscapes and NYUv2 and additionally estimates high-quality predictive
uncertainties for both tasks that are comparable or superior to a Deep Ensemble
despite being an order of magnitude more efficient.
\\ ( https://arxiv.org/abs/2402.10580 ,  4318kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10642 (*cross-listing*)
Date: Fri, 16 Feb 2024 12:43:01 GMT   (1585kb,D)

Title: Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up
  Speech Diffusion Model
Authors: Xiangyu Zhang, Daijiao Liu, Hexin Liu, Qiquan Zhang, Hanyu Meng,
  Leibny Paola Garcia, Eng Siong Chng, Lina Yao
Categories: eess.AS cs.AI
\\
  Recently, Denoising Diffusion Probabilistic Models (DDPMs) have attained
leading performances across a diverse range of generative tasks. However, in
the field of speech synthesis, although DDPMs exhibit impressive performance,
their long training duration and substantial inference costs hinder practical
deployment. Existing approaches primarily focus on enhancing inference speed,
while approaches to accelerate training a key factor in the costs associated
with adding or customizing voices often necessitate complex modifications to
the model, compromising their universal applicability. To address the
aforementioned challenges, we propose an inquiry: is it possible to enhance the
training/inference speed and performance of DDPMs by modifying the speech
signal itself? In this paper, we double the training and inference speed of
Speech DDPMs by simply redirecting the generative target to the wavelet domain.
This method not only achieves comparable or superior performance to the
original model in speech synthesis tasks but also demonstrates its versatility.
By investigating and utilizing different wavelet bases, our approach proves
effective not just in speech synthesis, but also in speech enhancement.
\\ ( https://arxiv.org/abs/2402.10642 ,  1585kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10659 (*cross-listing*)
Date: Fri, 16 Feb 2024 13:10:14 GMT   (13793kb,D)

Title: Network Formation and Dynamics Among Multi-LLMs
Authors: Marios Papachristou, Yuan Yuan
Categories: cs.SI cs.AI cs.CL cs.MA
\\
  Social networks influence behaviors, preferences, and relationships and play
a crucial role in the dissemination of information and norms within human
societies. As large language models (LLMs) increasingly integrate into social
and professional environments, understanding their behavior within the context
of social networks and interactions becomes essential. Our study analyzes the
behaviors of standard network structures and real-world networks to determine
whether the dynamics of multiple LLMs align with human social dynamics. We
explore various social network principles, including micro-level concepts such
as preferential attachment, triadic closure, and homophily, as well as
macro-level concepts like community structure and the small-world phenomenon.
Our findings suggest that LLMs demonstrate all these principles when they are
provided with network structures and asked about their preferences regarding
network formation. Furthermore, we investigate LLMs' decision-making based on
real-world networks to compare the strengths of these principles. Our results
reveal that triadic closure and homophily have a stronger influence than
preferential attachment and that LLMs substantially exceed random guessing in
the task of network formation predictions. Overall, our study contributes to
the development of socially aware LLMs by shedding light on LLMs' network
formation behaviors and exploring their impacts on social dynamics and norms.
\\ ( https://arxiv.org/abs/2402.10659 ,  13793kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10701 (*cross-listing*)
Date: Fri, 16 Feb 2024 14:02:28 GMT   (8008kb,D)

Title: Does Twinning Vehicular Networks Enhance Their Performance in Dense
  Areas?
Authors: Sarah Al-Shareeda, Sema F. Oktug, Yusuf Yaslan, Gokhan Yurdakul, Berk
  Canberk
Categories: cs.NI cs.AI
Comments: 6 pages, 8 figures, 2tables, conference paper
\\
  This paper investigates the potential of Digital Twins (DTs) to enhance
network performance in densely populated urban areas, specifically focusing on
vehicular networks. The study comprises two phases. In Phase I, we utilize
traffic data and AI clustering to identify critical locations, particularly in
crowded urban areas with high accident rates. In Phase II, we evaluate the
advantages of twinning vehicular networks through three deployment scenarios:
edge-based twin, cloud-based twin, and hybrid-based twin. Our analysis
demonstrates that twinning significantly reduces network delays, with virtual
twins outperforming physical networks. Virtual twins maintain low delays even
with increased vehicle density, such as 15.05 seconds for 300 vehicles.
Moreover, they exhibit faster computational speeds, with cloud-based twins
being 1.7 times faster than edge twins in certain scenarios. These findings
provide insights for efficient vehicular communication and underscore the
potential of virtual twins in enhancing vehicular networks in crowded areas
while emphasizing the importance of considering real-world factors when making
deployment decisions.
\\ ( https://arxiv.org/abs/2402.10701 ,  8008kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10717 (*cross-listing*)
Date: Fri, 16 Feb 2024 14:19:33 GMT   (5275kb,D)

Title: BioFusionNet: Deep Learning-Based Survival Risk Stratification in ER+
  Breast Cancer Through Multifeature and Multimodal Data Fusion
Authors: Raktim Kumar Mondol, Ewan K.A. Millar, Arcot Sowmya, Erik Meijering
Categories: cs.CV cs.AI
Comments: Keywords: Multimodal Fusion, Breast Cancer, Whole Slide Images, Deep
  Neural Network, Survival Prediction
\\
  Breast cancer is a significant health concern affecting millions of women
worldwide. Accurate survival risk stratification plays a crucial role in
guiding personalised treatment decisions and improving patient outcomes. Here
we present BioFusionNet, a deep learning framework that fuses image-derived
features with genetic and clinical data to achieve a holistic patient profile
and perform survival risk stratification of ER+ breast cancer patients. We
employ multiple self-supervised feature extractors, namely DINO and MoCoV3,
pretrained on histopathology patches to capture detailed histopathological
image features. We then utilise a variational autoencoder (VAE) to fuse these
features, and harness the latent space of the VAE to feed into a self-attention
network, generating patient-level features. Next, we develop a
co-dual-cross-attention mechanism to combine the histopathological features
with genetic data, enabling the model to capture the interplay between them.
Additionally, clinical data is incorporated using a feed-forward network (FFN),
further enhancing predictive performance and achieving comprehensive multimodal
feature integration. Furthermore, we introduce a weighted Cox loss function,
specifically designed to handle imbalanced survival data, which is a common
challenge in the field. The proposed model achieves a mean concordance index
(C-index) of 0.77 and a time-dependent area under the curve (AUC) of 0.84,
outperforming state-of-the-art methods. It predicts risk (high versus low) with
prognostic significance for overall survival (OS) in univariate analysis
(HR=2.99, 95% CI: 1.88--4.78, p<0.005), and maintains independent significance
in multivariate analysis incorporating standard clinicopathological variables
(HR=2.91, 95% CI: 1.80--4.68, p<0.005). The proposed method not only improves
model performance but also addresses a critical gap in handling imbalanced
data.
\\ ( https://arxiv.org/abs/2402.10717 ,  5275kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10778 (*cross-listing*)
Date: Fri, 16 Feb 2024 16:00:50 GMT   (3018kb,D)

Title: AutoGPT+P: Affordance-based Task Planning with Large Language Models
Authors: Timo Birr, Christoph Pohl, Abdelrahman Younes and Tamim Asfour
Categories: cs.RO cs.AI
Comments: 12 pages, 16 pages including references and appendix, 5 figures
ACM-class: I.2
\\
  Recent advances in task planning leverage Large Language Models (LLMs) to
improve generalizability by combining such models with classical planning
algorithms to address their inherent limitations in reasoning capabilities.
However, these approaches face the challenge of dynamically capturing the
initial state of the task planning problem. To alleviate this issue, we propose
AutoGPT+P, a system that combines an affordance-based scene representation with
a planning system. Affordances encompass the action possibilities of an agent
on the environment and objects present in it. Thus, deriving the planning
domain from an affordance-based scene representation allows symbolic planning
with arbitrary objects. AutoGPT+P leverages this representation to derive and
execute a plan for a task specified by the user in natural language. In
addition to solving planning tasks under a closed-world assumption, AutoGPT+P
can also handle planning with incomplete information, e. g., tasks with missing
objects by exploring the scene, suggesting alternatives, or providing a partial
plan. The affordance-based scene representation combines object detection with
an automatically generated object-affordance-mapping using ChatGPT. The core
planning tool extends existing work by automatically correcting semantic and
syntactic errors. Our approach achieves a success rate of 98%, surpassing the
current 81% success rate of the current state-of-the-art LLM-based planning
method SayCan on the SayCan instruction set. Furthermore, we evaluated our
approach on our newly created dataset with 150 scenarios covering a wide range
of complex tasks with missing objects, achieving a success rate of 79% on our
dataset. The dataset and the code are publicly available at
https://git.h2t.iar.kit.edu/birr/autogpt-p-standalone.
\\ ( https://arxiv.org/abs/2402.10778 ,  3018kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10798 (*cross-listing*)
Date: Fri, 16 Feb 2024 16:21:15 GMT   (16813kb,D)

Title: VATr++: Choose Your Words Wisely for Handwritten Text Generation
Authors: Bram Vanherle, Vittorio Pippi, Silvia Cascianelli, Nick Michiels,
  Frank Van Reeth, Rita Cucchiara
Categories: cs.CV cs.AI
\\
  Styled Handwritten Text Generation (HTG) has received significant attention
in recent years, propelled by the success of learning-based solutions employing
GANs, Transformers, and, preliminarily, Diffusion Models. Despite this surge in
interest, there remains a critical yet understudied aspect - the impact of the
input, both visual and textual, on the HTG model training and its subsequent
influence on performance. This study delves deeper into a cutting-edge
Styled-HTG approach, proposing strategies for input preparation and training
regularization that allow the model to achieve better performance and
generalize better. These aspects are validated through extensive analysis on
several different settings and datasets. Moreover, in this work, we go beyond
performance optimization and address a significant hurdle in HTG research - the
lack of a standardized evaluation protocol. In particular, we propose a
standardization of the evaluation protocol for HTG and conduct a comprehensive
benchmarking of existing approaches. By doing so, we aim to establish a
foundation for fair and meaningful comparisons between HTG strategies,
fostering progress in the field.
\\ ( https://arxiv.org/abs/2402.10798 ,  16813kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10803 (*cross-listing*)
Date: Fri, 16 Feb 2024 16:28:58 GMT   (2682kb,D)

Title: Modelling crypto markets by multi-agent reinforcement learning
Authors: Johann Lussange, Stefano Vrizzi, Stefano Palminteri, Boris Gutkin
Categories: q-fin.CP cs.AI cs.GT cs.MA
\\
  Building on a previous foundation work (Lussange et al. 2020), this study
introduces a multi-agent reinforcement learning (MARL) model simulating crypto
markets, which is calibrated to the Binance's daily closing prices of $153$
cryptocurrencies that were continuously traded between 2018 and 2022. Unlike
previous agent-based models (ABM) or multi-agent systems (MAS) which relied on
zero-intelligence agents or single autonomous agent methodologies, our approach
relies on endowing agents with reinforcement learning (RL) techniques in order
to model crypto markets. This integration is designed to emulate, with a
bottom-up approach to complexity inference, both individual and collective
agents, ensuring robustness in the recent volatile conditions of such markets
and during the COVID-19 era. A key feature of our model also lies in the fact
that its autonomous agents perform asset price valuation based on two sources
of information: the market prices themselves, and the approximation of the
crypto assets fundamental values beyond what those market prices are. Our MAS
calibration against real market data allows for an accurate emulation of crypto
markets microstructure and probing key market behaviors, in both the bearish
and bullish regimes of that particular time period.
\\ ( https://arxiv.org/abs/2402.10803 ,  2682kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10805 (*cross-listing*)
Date: Fri, 16 Feb 2024 16:31:46 GMT   (23168kb,D)

Title: Generative Cross-Modal Retrieval: Memorizing Images in Multimodal
  Language Models for Retrieval and Beyond
Authors: Yongqi Li, Wenjie Wang, Leigang Qu, Liqiang Nie, Wenjie Li, Tat-Seng
  Chua
Categories: cs.MM cs.AI cs.CL cs.CV cs.IR
\\
  The recent advancements in generative language models have demonstrated their
ability to memorize knowledge from documents and recall knowledge to respond to
user queries effectively. Building upon this capability, we propose to enable
multimodal large language models (MLLMs) to memorize and recall images within
their parameters. Given a user query for visual content, the MLLM is
anticipated to "recall" the relevant image from its parameters as the response.
Achieving this target presents notable challenges, including inbuilt visual
memory and visual recall schemes within MLLMs. To address these challenges, we
introduce a generative cross-modal retrieval framework, which assigns unique
identifier strings to represent images and involves two training steps:
learning to memorize and learning to retrieve. The first step focuses on
training the MLLM to memorize the association between images and their
respective identifiers. The latter step teaches the MLLM to generate the
corresponding identifier of the target image, given the textual query input. By
memorizing images in MLLMs, we introduce a new paradigm to cross-modal
retrieval, distinct from previous discriminative approaches. The experiments
demonstrate that the generative paradigm performs effectively and efficiently
even with large-scale image candidate sets.
\\ ( https://arxiv.org/abs/2402.10805 ,  23168kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10828 (*cross-listing*)
Date: Fri, 16 Feb 2024 16:57:18 GMT   (14101kb,D)

Title: RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented
  In-Context Learning in Multi-Modal Large Language Model
Authors: Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars
  Kunze, Matthew Gadd
Categories: cs.RO cs.AI
Comments: 13 pages, 6 figures
\\
  Robots powered by 'blackbox' models need to provide human-understandable
explanations which we can trust. Hence, explainability plays a critical role in
trustworthy autonomous decision-making to foster transparency and acceptance
among end users, especially in complex autonomous driving. Recent advancements
in Multi-Modal Large Language models (MLLMs) have shown promising potential in
enhancing the explainability as a driving agent by producing control
predictions along with natural language explanations. However, severe data
scarcity due to expensive annotation costs and significant domain gaps between
different datasets makes the development of a robust and generalisable system
an extremely challenging task. Moreover, the prohibitively expensive training
requirements of MLLM and the unsolved problem of catastrophic forgetting
further limit their generalisability post-deployment. To address these
challenges, we present RAG-Driver, a novel retrieval-augmented multi-modal
large language model that leverages in-context learning for high-performance,
explainable, and generalisable autonomous driving. By grounding in retrieved
expert demonstration, we empirically validate that RAG-Driver achieves
state-of-the-art performance in producing driving action explanations,
justifications, and control signal prediction. More importantly, it exhibits
exceptional zero-shot generalisation capabilities to unseen environments
without further training endeavours.
\\ ( https://arxiv.org/abs/2402.10828 ,  14101kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10837 (*cross-listing*)
Date: Fri, 16 Feb 2024 17:20:45 GMT   (47108kb,D)

Title: Pedipulate: Enabling Manipulation Skills using a Quadruped Robot's Leg
Authors: Philip Arm, Mayank Mittal, Hendrik Kolvenbach, Marco Hutter
Categories: cs.RO cs.AI cs.SY eess.SY
Comments: Project website:
  https://sites.google.com/leggedrobotics.com/pedipulate
\\
  Legged robots have the potential to become vital in maintenance, home
support, and exploration scenarios. In order to interact with and manipulate
their environments, most legged robots are equipped with a dedicated robot arm,
which means additional mass and mechanical complexity compared to standard
legged robots. In this work, we explore pedipulation - using the legs of a
legged robot for manipulation. By training a reinforcement learning policy that
tracks position targets for one foot, we enable a dedicated pedipulation
controller that is robust to disturbances, has a large workspace through
whole-body behaviors, and can reach far-away targets with gait emergence,
enabling loco-pedipulation. By deploying our controller on a quadrupedal robot
using teleoperation, we demonstrate various real-world tasks such as door
opening, sample collection, and pushing obstacles. We demonstrate load carrying
of more than 2.0 kg at the foot. Additionally, the controller is robust to
interaction forces at the foot, disturbances at the base, and slippery contact
surfaces. Videos of the experiments are available at
https://sites.google.com/leggedrobotics.com/pedipulate.
\\ ( https://arxiv.org/abs/2402.10837 ,  47108kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10885 (*cross-listing*)
Date: Fri, 16 Feb 2024 18:43:02 GMT   (27637kb,D)

Title: 3D Diffuser Actor: Policy Diffusion with 3D Scene Representations
Authors: Tsung-Wei Ke, Nikolaos Gkanatsios, Katerina Fragkiadaki
Categories: cs.RO cs.AI cs.CV cs.LG
Comments: First two authors contributed equally
\\
  We marry diffusion policies and 3D scene representations for robot
manipulation. Diffusion policies learn the action distribution conditioned on
the robot and environment state using conditional diffusion models. They have
recently shown to outperform both deterministic and alternative
state-conditioned action distribution learning methods. 3D robot policies use
3D scene feature representations aggregated from a single or multiple camera
views using sensed depth. They have shown to generalize better than their 2D
counterparts across camera viewpoints. We unify these two lines of work and
present 3D Diffuser Actor, a neural policy architecture that, given a language
instruction, builds a 3D representation of the visual scene and conditions on
it to iteratively denoise 3D rotations and translations for the robot's
end-effector. At each denoising iteration, our model represents end-effector
pose estimates as 3D scene tokens and predicts the 3D translation and rotation
error for each of them, by featurizing them using 3D relative attention to
other 3D visual and language tokens. 3D Diffuser Actor sets a new
state-of-the-art on RLBench with an absolute performance gain of 16.3% over the
current SOTA on a multi-view setup and an absolute gain of 13.1% on a
single-view setup. On the CALVIN benchmark, it outperforms the current SOTA in
the setting of zero-shot unseen scene generalization by being able to
successfully run 0.2 more tasks, a 7% relative increase. It also works in the
real world from a handful of demonstrations. We ablate our model's
architectural design choices, such as 3D scene featurization and 3D relative
attentions, and show they all help generalization. Our results suggest that 3D
scene representations and powerful generative modeling are keys to efficient
robot learning from demonstrations.
\\ ( https://arxiv.org/abs/2402.10885 ,  27637kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10555 (*cross-listing*)
Date: Fri, 16 Feb 2024 10:36:38 GMT   (1160kb,D)

Title: SPAR: Personalized Content-Based Recommendation via Long Engagement
  Attention
Authors: Chiyu Zhang, Yifei Sun, Jun Chen, Jie Lei, Muhammad Abdul-Mageed,
  Sinong Wang, Rong Jin, Sem Park, Ning Yao, Bo Long
Categories: cs.IR cs.CL
Comments: Under review
\\
  Leveraging users' long engagement histories is essential for personalized
content recommendations. The success of pretrained language models (PLMs) in
NLP has led to their use in encoding user histories and candidate items,
framing content recommendations as textual semantic matching tasks. However,
existing works still struggle with processing very long user historical text
and insufficient user-item interaction. In this paper, we introduce a
content-based recommendation framework, SPAR, which effectively tackles the
challenges of holistic user interest extraction from the long user engagement
history. It achieves so by leveraging PLM, poly-attention layers and attention
sparsity mechanisms to encode user's history in a session-based manner. The
user and item side features are sufficiently fused for engagement prediction
while maintaining standalone representations for both sides, which is efficient
for practical model deployment. Moreover, we enhance user profiling by
exploiting large language model (LLM) to extract global interests from user
engagement history. Extensive experiments on two benchmark datasets demonstrate
that our framework outperforms existing state-of-the-art (SoTA) methods.
\\ ( https://arxiv.org/abs/2402.10555 ,  1160kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10882 (*cross-listing*)
Date: Fri, 16 Feb 2024 18:36:36 GMT   (955kb,D)

Title: Universal Prompt Optimizer for Safe Text-to-Image Generation
Authors: Zongyu Wu, Hongcheng Gao, Yueze Wang, Xiang Zhang, Suhang Wang
Categories: cs.CV cs.CL
\\
  Text-to-Image (T2I) models have shown great performance in generating images
based on textual prompts. However, these models are vulnerable to unsafe input
to generate unsafe content like sexual, harassment and illegal-activity images.
Existing studies based on image checker, model fine-tuning and embedding
blocking are impractical in real-world applications. Hence, \textit{we propose
the first universal prompt optimizer for safe T2I generation in black-box
scenario}. We first construct a dataset consisting of toxic-clean prompt pairs
by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting
toxic prompt to clean prompt while preserving semantic information, we design a
novel reward function measuring toxicity and text alignment of generated images
and train the optimizer through Proximal Policy Optimization. Experiments show
that our approach can effectively reduce the likelihood of various T2I models
in generating inappropriate images, with no significant impact on text
alignment. It is also flexible to be combined with methods to achieve better
performance.
\\ ( https://arxiv.org/abs/2402.10882 ,  955kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10892 (*cross-listing*)
Date: Fri, 16 Feb 2024 18:49:27 GMT   (7088kb,D)

Title: Proving membership in LLM pretraining data via data watermarks
Authors: Johnny Tian-Zheng Wei, Ryan Yixiang Wang, Robin Jia
Categories: cs.CR cs.CL cs.LG
\\
  Detecting whether copyright holders' works were used in LLM pretraining is
poised to be an important problem. This work proposes using data watermarks to
enable principled detection with only black-box model access, provided that the
rightholder contributed multiple training documents and watermarked them before
public release. By applying a randomly sampled data watermark, detection can be
framed as hypothesis testing, which provides guarantees on the false detection
rate. We study two watermarks: one that inserts random sequences, and another
that randomly substitutes characters with Unicode lookalikes. We first show how
three aspects of watermark design -- watermark length, number of duplications,
and interference -- affect the power of the hypothesis test. Next, we study how
a watermark's detection strength changes under model and dataset scaling: while
increasing the dataset size decreases the strength of the watermark, watermarks
remain strong if the model size also increases. Finally, we view SHA hashes as
natural watermarks and show that we can robustly detect hashes from
BLOOM-176B's training data, as long as they occurred at least 90 times.
Together, our results point towards a promising future for data watermarks in
real world use.
\\ ( https://arxiv.org/abs/2402.10892 ,  7088kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10229 (*cross-listing*)
Date: Thu, 8 Feb 2024 19:34:24 GMT   (852kb,D)

Title: Mixture-Models: a one-stop Python Library for Model-based Clustering
  using various Mixture Models
Authors: Siva Rajesh Kasa, Hu Yijie, Santhosh Kumar Kasa, Vaibhav Rajan
Categories: stat.CO cs.LG
\\
  \texttt{Mixture-Models} is an open-source Python library for fitting Gaussian
Mixture Models (GMM) and their variants, such as Parsimonious GMMs, Mixture of
Factor Analyzers, MClust models, Mixture of Student's t distributions, etc. It
streamlines the implementation and analysis of these models using various
first/second order optimization routines such as Gradient Descent and Newton-CG
through automatic differentiation (AD) tools. This helps in extending these
models to high-dimensional data, which is first of its kind among Python
libraries. The library provides user-friendly model evaluation tools, such as
BIC, AIC, and log-likelihood estimation. The source-code is licensed under MIT
license and can be accessed at \url{https://github.com/kasakh/Mixture-Models}.
The package is highly extensible, allowing users to incorporate new
distributions and optimization techniques with ease. We conduct a large scale
simulation to compare the performance of various gradient based approaches
against Expectation Maximization on a wide range of settings and identify the
corresponding best suited approach.
\\ ( https://arxiv.org/abs/2402.10229 ,  852kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10230 (*cross-listing*)
Date: Thu, 8 Feb 2024 21:58:53 GMT   (4699kb,D)

Title: Temporal Analysis of Drifting Hashtags in Textual Data Streams: A
  Graph-Based Application
Authors: Cristiano M. Garcia and Alceu de Souza Britto Jr and Jean Paul Barddal
Categories: cs.SI cs.LG
\\
  Social media has played an important role since its emergence. People use the
internet to express opinions about anything, making social media platforms a
social sensor. Initially supported by Twitter, the hashtags are now in use on
several social media platforms. Hashtags are helpful to tag, track, and group
posts on similar topics. In this paper, we analyze hashtag drifts over time
using concepts from graph analysis and textual data streams using the
Girvan-Newman method to uncover hashtag communities in annual snapshots. More
specifically, we analyzed the #mybodymychoice hashtag between 2018 and 2022. In
addition, we offer insights about some hashtags found in the study.
Furthermore, our approach can be useful for monitoring changes over time in
opinions and sentiment patterns about an entity on social media. Even though
the hashtag #mybodymychoice was initially coupled with women's rights,
abortion, and bodily autonomy, we observe that it suffered drifts during the
studied period across topics such as drug legalization, vaccination, political
protests, war, and civil rights. The year 2021 was the most significant
drifting year, in which the communities detected suggest that #mybodymychoice
significantly drifted to vaccination and Covid-19-related topics.
\\ ( https://arxiv.org/abs/2402.10230 ,  4699kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10231 (*cross-listing*)
Date: Fri, 9 Feb 2024 16:53:19 GMT   (37kb,D)

Title: A Multi-faceted Semi-Synthetic Dataset for Automated Cyberbullying
  Detection
Authors: Naveed Ejaz, Fakhra Kashif, Salimur Choudhury
Categories: cs.SI cs.LG
ACM-class: I.2
\\
  In recent years, the rising use of social media has propelled automated
cyberbullying detection into a prominent research domain. However, challenges
persist due to the absence of a standardized definition and universally
accepted datasets. Many researchers now view cyberbullying as a facet of
cyberaggression, encompassing factors like repetition, peer relationships, and
harmful intent in addition to online aggression. Acquiring comprehensive data
reflective of all cyberbullying components from social media networks proves to
be a complex task. This paper provides a description of an extensive
semi-synthetic cyberbullying dataset that incorporates all of the essential
aspects of cyberbullying, including aggression, repetition, peer relationships,
and intent to harm. The method of creating the dataset is succinctly outlined,
and a detailed overview of the publicly accessible dataset is additionally
presented. This accompanying data article provides an in-depth look at the
dataset, increasing transparency and enabling replication. It also aids in a
deeper understanding of the data, supporting broader research use.
\\ ( https://arxiv.org/abs/2402.10231 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10239 (*cross-listing*)
Date: Wed, 14 Feb 2024 18:24:49 GMT   (1923kb,D)

Title: A Language Model for Particle Tracking
Authors: Andris Huang, Yash Melkani, Paolo Calafiura, Alina Lazar, Daniel
  Thomas Murnane, Minh-Tuan Pham, Xiangyang Ju
Categories: hep-ph cs.LG hep-ex
Comments: 7 pages, 3 figures, A Proceeding of the Connecting the Dots Workshop
  (CTD 2023)
Report-no: PROC-CTD2023-33
\\
  Particle tracking is crucial for almost all physics analysis programs at the
Large Hadron Collider. Deep learning models are pervasively used in particle
tracking related tasks. However, the current practice is to design and train
one deep learning model for one task with supervised learning techniques. The
trained models work well for tasks they are trained on but show no or little
generalization capabilities. We propose to unify these models with a language
model. In this paper, we present a tokenized detector representation that
allows us to train a BERT model for particle tracking. The trained BERT model,
namely TrackingBERT, offers latent detector module embedding that can be used
for other tasks. This work represents the first step towards developing a
foundational model for particle detector understanding.
\\ ( https://arxiv.org/abs/2402.10239 ,  1923kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10242 (*cross-listing*)
Date: Wed, 14 Feb 2024 19:37:30 GMT   (219kb)

Title: Signed Diverse Multiplex Networks: Clustering and Inference
Authors: Marianna Pensky
Categories: cs.SI cs.LG stat.ME
Comments: 8 figures
\\
  The paper introduces a Signed Generalized Random Dot Product Graph (SGRDPG)
model, which is a variant of the Generalized Random Dot Product Graph (GRDPG),
where, in addition, edges can be positive or negative. The setting is extended
to a multiplex version, where all layers have the same collection of nodes and
follow the SGRDPG. The only common feature of the layers of the network is that
they can be partitioned into groups with common subspace structures, while
otherwise all matrices of connection probabilities can be all different. The
setting above is extremely flexible and includes a variety of existing
multiplex network models as its particular cases. The paper fulfills two
objectives. First, it shows that keeping signs of the edges in the process of
network construction leads to a better precision of estimation and clustering
and, hence, is beneficial for tackling real world problems such as analysis of
brain networks. Second, by employing novel algorithms, our paper ensures
equivalent or superior accuracy than has been achieved in simpler multiplex
network models. In addition to theoretical guarantees, both of those features
are demonstrated using numerical simulations and a real data example.
\\ ( https://arxiv.org/abs/2402.10242 ,  219kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10243 (*cross-listing*)
Date: Wed, 14 Feb 2024 22:23:26 GMT   (286kb,D)

Title: Understanding team collapse via probabilistic graphical models
Authors: Iasonas Nikolaou, Konstantinos Pelechrinis, Evimaria Terzi
Categories: physics.soc-ph cs.LG cs.SI
\\
  In this work, we develop a graphical model to capture team dynamics. We
analyze the model and show how to learn its parameters from data. Using our
model we study the phenomenon of team collapse from a computational
perspective. We use simulations and real-world experiments to find the main
causes of team collapse. We also provide the principles of building resilient
teams, i.e., teams that avoid collapsing. Finally, we use our model to analyze
the structure of NBA teams and dive deeper into games of interest.
\\ ( https://arxiv.org/abs/2402.10243 ,  286kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10252 (*cross-listing*)
Date: Thu, 15 Feb 2024 16:16:30 GMT   (61kb)

Title: Online Control of Linear Systems with Unbounded and Degenerate Noise
Authors: Kaito Ito, Taira Tsuchiya
Categories: eess.SY cs.LG cs.SY math.OC stat.ML
Comments: 26 pages
\\
  This paper investigates the problem of controlling a linear system under
possibly unbounded and degenerate noise with unknown cost functions, known as
an online control problem. In contrast to the existing work, which assumes the
boundedness of noise, we reveal that for convex costs, an $
\widetilde{O}(\sqrt{T}) $ regret bound can be achieved even for unbounded
noise, where $ T $ denotes the time horizon. Moreover, when the costs are
strongly convex, we establish an $ O({\rm poly} (\log T)) $ regret bound
without the assumption that noise covariance is non-degenerate, which has been
required in the literature. The key ingredient in removing the rank assumption
on noise is a system transformation associated with the noise covariance. This
simultaneously enables the parameter reduction of an online control algorithm.
\\ ( https://arxiv.org/abs/2402.10252 ,  61kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10289 (*cross-listing*)
Date: Thu, 15 Feb 2024 19:37:39 GMT   (640kb,D)

Title: Thompson Sampling in Partially Observable Contextual Bandits
Authors: Hongju Park and Mohamad Kazem Shirani Faradonbeh
Categories: stat.ML cs.LG
Comments: 43 pages
\\
  Contextual bandits constitute a classical framework for decision-making under
uncertainty. In this setting, the goal is to learn the arms of highest reward
subject to contextual information, while the unknown reward parameters of each
arm need to be learned by experimenting that specific arm. Accordingly, a
fundamental problem is that of balancing exploration (i.e., pulling different
arms to learn their parameters), versus exploitation (i.e., pulling the best
arms to gain reward). To study this problem, the existing literature mostly
considers perfectly observed contexts. However, the setting of partial context
observations remains unexplored to date, despite being theoretically more
general and practically more versatile. We study bandit policies for learning
to select optimal arms based on the data of observations, which are noisy
linear functions of the unobserved context vectors. Our theoretical analysis
shows that the Thompson sampling policy successfully balances exploration and
exploitation. Specifically, we establish the followings: (i) regret bounds that
grow poly-logarithmically with time, (ii) square-root consistency of parameter
estimation, and (iii) scaling of the regret with other quantities including
dimensions and number of arms. Extensive numerical experiments with both real
and synthetic data are presented as well, corroborating the efficacy of
Thompson sampling. To establish the results, we introduce novel martingale
techniques and concentration inequalities to address partially observed
dependent random variables generated from unspecified distributions, and also
leverage problem-dependent information to sharpen probabilistic bounds for
time-varying suboptimality gaps. These techniques pave the road towards
studying other decision-making problems with contextual information as well as
partial observations.
\\ ( https://arxiv.org/abs/2402.10289 ,  640kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10357 (*cross-listing*)
Date: Thu, 15 Feb 2024 22:59:14 GMT   (96kb,D)

Title: Efficient Sampling on Riemannian Manifolds via Langevin MCMC
Authors: Xiang Cheng, Jingzhao Zhang, Suvrit Sra
Categories: math.ST cs.LG math.PR stat.CO stat.ML stat.TH
Comments: This is an old paper from NeurIPS 2022. arXiv admin note: text
  overlap with arXiv:2204.13665
\\
  We study the task of efficiently sampling from a Gibbs distribution $d \pi^*
= e^{-h} d {vol}_g$ over a Riemannian manifold $M$ via (geometric) Langevin
MCMC; this algorithm involves computing exponential maps in random Gaussian
directions and is efficiently implementable in practice. The key to our
analysis of Langevin MCMC is a bound on the discretization error of the
geometric Euler-Murayama scheme, assuming $\nabla h$ is Lipschitz and $M$ has
bounded sectional curvature. Our error bound matches the error of Euclidean
Euler-Murayama in terms of its stepsize dependence. Combined with a contraction
guarantee for the geometric Langevin Diffusion under Kendall-Cranston coupling,
we prove that the Langevin MCMC iterates lie within $\epsilon$-Wasserstein
distance of $\pi^*$ after $\tilde{O}(\epsilon^{-2})$ steps, which matches the
iteration complexity for Euclidean Langevin MCMC. Our results apply in general
settings where $h$ can be nonconvex and $M$ can have negative Ricci curvature.
Under additional assumptions that the Riemannian curvature tensor has bounded
derivatives, and that $\pi^*$ satisfies a $CD(\cdot,\infty)$ condition, we
analyze the stochastic gradient version of Langevin MCMC, and bound its
iteration complexity by $\tilde{O}(\epsilon^{-2})$ as well.
\\ ( https://arxiv.org/abs/2402.10357 ,  96kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10387 (*cross-listing*)
Date: Fri, 16 Feb 2024 00:48:20 GMT   (3748kb,D)

Title: MFBind: a Multi-Fidelity Approach for Evaluating Drug Compounds in
  Practical Generative Modeling
Authors: Peter Eckmann, Dongxia Wu, Germano Heinzelmann, Michael K Gilson, Rose
  Yu
Categories: q-bio.BM cs.LG
Comments: 9 pages, 4 figures
\\
  Current generative models for drug discovery primarily use molecular docking
to evaluate the quality of generated compounds. However, such models are often
not useful in practice because even compounds with high docking scores do not
consistently show experimental activity. More accurate methods for activity
prediction exist, such as molecular dynamics based binding free energy
calculations, but they are too computationally expensive to use in a generative
model. We propose a multi-fidelity approach, Multi-Fidelity Bind (MFBind), to
achieve the optimal trade-off between accuracy and computational cost. MFBind
integrates docking and binding free energy simulators to train a multi-fidelity
deep surrogate model with active learning. Our deep surrogate model utilizes a
pretraining technique and linear prediction heads to efficiently fit small
amounts of high-fidelity data. We perform extensive experiments and show that
MFBind (1) outperforms other state-of-the-art single and multi-fidelity
baselines in surrogate modeling, and (2) boosts the performance of generative
models with markedly higher quality compounds.
\\ ( https://arxiv.org/abs/2402.10387 ,  3748kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10425 (*cross-listing*)
Date: Fri, 16 Feb 2024 03:22:58 GMT   (532kb)

Title: DABS-LS: Deep Atlas-Based Segmentation Using Regional Level Set
  Self-Supervision
Authors: Hannah G. Mason, Jack H. Noble
Categories: eess.IV cs.CV cs.LG
\\
  Cochlear implants (CIs) are neural prosthetics used to treat patients with
severe-to-profound hearing loss. Patient-specific modeling of CI stimulation of
the auditory nerve fiber (ANFs) can help audiologists improve the CI
programming. These models require localization of the ANFs relative to
surrounding anatomy and the CI. Localization is challenging because the ANFs
are so small they are not directly visible in clinical imaging. In this work,
we hypothesize the position of the ANFs can be accurately inferred from the
location of the internal auditory canal (IAC), which has high contrast in CT,
since the ANFs pass through this canal between the cochlea and the brain.
Inspired by VoxelMorph, in this paper we propose a deep atlas-based IAC
segmentation network. We create a single atlas in which the IAC and ANFs are
pre-localized. Our network is trained to produce deformation fields (DFs)
mapping coordinates from the atlas to new target volumes and that accurately
segment the IAC. We hypothesize that DFs that accurately segment the IAC in
target images will also facilitate accurate atlas-based localization of the
ANFs. As opposed to VoxelMorph, which aims to produce DFs that accurately
register the entire volume, our novel contribution is an entirely
self-supervised training scheme that aims to produce DFs that accurately
segment the target structure. This self-supervision is facilitated using a
regional level set (LS) inspired loss function. We call our method Deep Atlas
Based Segmentation using Level Sets (DABS-LS). Results show that DABS-LS
outperforms VoxelMorph for IAC segmentation. Tests with publicly available
datasets for trachea and kidney segmentation also show significant improvement
in segmentation accuracy, demonstrating the generalizability of the method.
\\ ( https://arxiv.org/abs/2402.10425 ,  532kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10429 (*cross-listing*)
Date: Fri, 16 Feb 2024 03:36:03 GMT   (364kb)

Title: Fixed Confidence Best Arm Identification in the Bayesian Setting
Authors: Kyoungseok Jang, Junpei Komiyama, Kazutoshi Yamazaki
Categories: stat.ML cs.LG
\\
  We consider the fixed-confidence best arm identification (FC-BAI) problem in
the Bayesian Setting. This problem aims to find the arm of the largest mean
with a fixed confidence level when the bandit model has been sampled from the
known prior. Most studies on the FC-BAI problem have been conducted in the
frequentist setting, where the bandit model is predetermined before the game
starts. We show that the traditional FC-BAI algorithms studied in the
frequentist setting, such as track-and-stop and top-two algorithms, result in
arbitrary suboptimal performances in the Bayesian setting. We also prove a
lower bound of the expected number of samples in the Bayesian setting and
introduce a variant of successive elimination that has a matching performance
with the lower bound up to a logarithmic factor. Simulations verify the
theoretical results.
\\ ( https://arxiv.org/abs/2402.10429 ,  364kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10433 (*cross-listing*)
Date: Fri, 16 Feb 2024 03:48:55 GMT   (726kb,D)

Title: Fusing Neural and Physical: Augment Protein Conformation Sampling with
  Tractable Simulations
Authors: Jiarui Lu, Zuobai Zhang, Bozitao Zhong, Chence Shi, Jian Tang
Categories: q-bio.BM cs.LG q-bio.QM
Comments: Preprint. Under review
\\
  The protein dynamics are common and important for their biological functions
and properties, the study of which usually involves time-consuming molecular
dynamics (MD) simulations in silico. Recently, generative models has been
leveraged as a surrogate sampler to obtain conformation ensembles with orders
of magnitude faster and without requiring any simulation data (a "zero-shot"
inference). However, being agnostic of the underlying energy landscape, the
accuracy of such generative model may still be limited. In this work, we
explore the few-shot setting of such pre-trained generative sampler which
incorporates MD simulations in a tractable manner. Specifically, given a target
protein of interest, we first acquire some seeding conformations from the
pre-trained sampler followed by a number of physical simulations in parallel
starting from these seeding samples. Then we fine-tuned the generative model
using the simulation trajectories above to become a target-specific sampler.
Experimental results demonstrated the superior performance of such few-shot
conformation sampler at a tractable computational cost.
\\ ( https://arxiv.org/abs/2402.10433 ,  726kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10456 (*cross-listing*)
Date: Fri, 16 Feb 2024 05:27:05 GMT   (16733kb,D)

Title: Generative Modeling for Tabular Data via Penalized Optimal Transport
  Network
Authors: Wenhui Sophia Lu, Chenyang Zhong, Wing Hung Wong
Categories: stat.ML cs.LG stat.AP stat.ME
Comments: 37 pages, 23 figures
\\
  The task of precisely learning the probability distribution of rows within
tabular data and producing authentic synthetic samples is both crucial and
non-trivial. Wasserstein generative adversarial network (WGAN) marks a notable
improvement in generative modeling, addressing the challenges faced by its
predecessor, generative adversarial network. However, due to the mixed data
types and multimodalities prevalent in tabular data, the delicate equilibrium
between the generator and discriminator, as well as the inherent instability of
Wasserstein distance in high dimensions, WGAN often fails to produce
high-fidelity samples. To this end, we propose POTNet (Penalized Optimal
Transport Network), a generative deep neural network based on a novel, robust,
and interpretable marginally-penalized Wasserstein (MPW) loss. POTNet can
effectively model tabular data containing both categorical and continuous
features. Moreover, it offers the flexibility to condition on a subset of
features. We provide theoretical justifications for the motivation behind the
MPW loss. We also empirically demonstrate the effectiveness of our proposed
method on four different benchmarks across a variety of real-world and
simulated datasets. Our proposed model achieves orders of magnitude speedup
during the sampling stage compared to state-of-the-art generative models for
tabular data, thereby enabling efficient large-scale synthetic data generation.
\\ ( https://arxiv.org/abs/2402.10456 ,  16733kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10457 (*cross-listing*)
Date: Fri, 16 Feb 2024 05:27:13 GMT   (3927kb,D)

Title: Learning-Augmented Skip Lists
Authors: Chunkai Fu, Jung Hoon Seo, Samson Zhou
Categories: cs.DS cs.LG
\\
  We study the integration of machine learning advice into the design of skip
lists to improve upon traditional data structure design. Given access to a
possibly erroneous oracle that outputs estimated fractional frequencies for
search queries on a set of items, we construct a skip list that provably
provides the optimal expected search time, within nearly a factor of two. In
fact, our learning-augmented skip list is still optimal up to a constant
factor, even if the oracle is only accurate within a constant factor. We show
that if the search queries follow the ubiquitous Zipfian distribution, then the
expected search time for an item by our skip list is only a constant,
independent of the total number $n$ of items, i.e., $\mathcal{O}(1)$, whereas a
traditional skip list will have an expected search time of $\mathcal{O}(\log
n)$. We also demonstrate robustness by showing that our data structure achieves
an expected search time that is within a constant factor of an oblivious skip
list construction even when the predictions are arbitrarily incorrect. Finally,
we empirically show that our learning-augmented skip list outperforms
traditional skip lists on both synthetic and real-world datasets.
\\ ( https://arxiv.org/abs/2402.10457 ,  3927kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10475 (*cross-listing*)
Date: Fri, 16 Feb 2024 06:41:35 GMT   (245kb,D)

Title: Fundamental Benefit of Alternating Updates in Minimax Optimization
Authors: Jaewook Lee, Hanseul Cho, Chulhee Yun
Categories: math.OC cs.LG
Comments: 77 pages, 2 figures
\\
  The Gradient Descent-Ascent (GDA) algorithm, designed to solve minimax
optimization problems, takes the descent and ascent steps either simultaneously
(Sim-GDA) or alternately (Alt-GDA). While Alt-GDA is commonly observed to
converge faster, the performance gap between the two is not yet well understood
theoretically, especially in terms of global convergence rates. To address this
theory-practice gap, we present fine-grained convergence analyses of both
algorithms for strongly-convex-strongly-concave and Lipschitz-gradient
objectives. Our new iteration complexity upper bound of Alt-GDA is strictly
smaller than the lower bound of Sim-GDA; i.e., Alt-GDA is provably faster.
Moreover, we propose Alternating-Extrapolation GDA (Alex-GDA), a general
algorithmic framework that subsumes Sim-GDA and Alt-GDA, for which the main
idea is to alternately take gradients from extrapolations of the iterates. We
show that Alex-GDA satisfies a smaller iteration complexity bound, identical to
that of the Extra-gradient method, while requiring less gradient computations.
We also prove that Alex-GDA enjoys linear convergence for bilinear problems,
for which both Sim-GDA and Alt-GDA fail to converge at all.
\\ ( https://arxiv.org/abs/2402.10475 ,  245kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10478 (*cross-listing*)
Date: Fri, 16 Feb 2024 06:57:03 GMT   (2857kb,D)

Title: CodaMal: Contrastive Domain Adaptation for Malaria Detection in Low-Cost
  Microscopes
Authors: Ishan Rajendrakumar Dave, Tristan de Blegiers, Chen Chen, Mubarak Shah
Categories: cs.CV cs.LG
Comments: Under Review. Project Page:
  https://daveishan.github.io/codamal-webpage/
\\
  Malaria is a major health issue worldwide, and its diagnosis requires
scalable solutions that can work effectively with low-cost microscopes (LCM).
Deep learning-based methods have shown success in computer-aided diagnosis from
microscopic images. However, these methods need annotated images that show
cells affected by malaria parasites and their life stages. Annotating images
from LCM significantly increases the burden on medical experts compared to
annotating images from high-cost microscopes (HCM). For this reason, a
practical solution would be trained on HCM images which should generalize well
on LCM images during testing. While earlier methods adopted a multi-stage
learning process, they did not offer an end-to-end approach. In this work, we
present an end-to-end learning framework, named CodaMal (Contrastive Domain
Adpation for Malaria). In order to bridge the gap between HCM (training) and
LCM (testing), we propose a domain adaptive contrastive loss. It reduces the
domain shift by promoting similarity between the representations of HCM and its
corresponding LCM image, without imposing an additional annotation burden. In
addition, the training objective includes object detection objectives with
carefully designed augmentations, ensuring the accurate detection of malaria
parasites. On the publicly available large-scale M5-dataset, our proposed
method shows a significant improvement of 16% over the state-of-the-art methods
in terms of the mean average precision metric (mAP), provides 21x speed up
during inference, and requires only half learnable parameters than the prior
methods. Our code is publicly available.
\\ ( https://arxiv.org/abs/2402.10478 ,  2857kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10502 (*cross-listing*)
Date: Fri, 16 Feb 2024 08:21:43 GMT   (1345kb,D)

Title: Late-time transition of $M_B$ inferred via neural networks
Authors: Purba Mukherjee, Konstantinos F. Dialektopoulos, Jackson Levi Said,
  Jurgen Mifsud
Categories: astro-ph.CO cs.LG gr-qc
Comments: 10 pages, 7 sets of figures, 2 tables. Comments are welcome
\\
  The strengthening of tensions in the cosmological parameters has led to a
reconsideration of fundamental aspects of standard cosmology. The tension in
the Hubble constant can also be viewed as a tension between local and early
Universe constraints on the absolute magnitude $M_B$ of Type Ia supernova. In
this work, we reconsider the possibility of a variation of this parameter in a
model-independent way. We employ neural networks to agnostically constrain the
value of the absolute magnitude as well as assess the impact and statistical
significance of a variation in $M_B$ with redshift from the Pantheon+
compilation, together with a thorough analysis of the neural network
architecture. We find an indication for a transition redshift at the $z\approx
1$ region.
\\ ( https://arxiv.org/abs/2402.10502 ,  1345kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10504 (*cross-listing*)
Date: Fri, 16 Feb 2024 08:27:55 GMT   (60kb,D)

Title: Resilience of the quadratic Littlewood-Offord problem
Authors: Elad Aigner-Horev, and Daniel Rozenberg, and Roi Weiss
Categories: math.PR cs.IT cs.LG math.CO math.IT stat.ML
\\
  We study the statistical resilience of high-dimensional data. Our results
provide estimates as to the effects of adversarial noise over the
anti-concentration properties of the quadratic Radamecher chaos
$\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi}$, where $M$ is a fixed
(high-dimensional) matrix and $\boldsymbol{\xi}$ is a conformal Rademacher
vector. Specifically, we pursue the question of how many adversarial sign-flips
can $\boldsymbol{\xi}$ sustain without "inflating" $\sup_{x\in \mathbb{R}}
\mathbb{P} \left\{\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi} = x\right\}$
and thus "de-smooth" the original distribution resulting in a more "grainy" and
adversarially biased distribution. Our results provide lower bound estimations
for the statistical resilience of the quadratic and bilinear Rademacher chaos;
these are shown to be asymptotically tight across key regimes.
\\ ( https://arxiv.org/abs/2402.10504 ,  60kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10547 (*cross-listing*)
Date: Fri, 16 Feb 2024 10:20:42 GMT   (3141kb,D)

Title: Learning Disentangled Audio Representations through Controlled Synthesis
Authors: Yusuf Brima, Ulf Krumnack, Simone Pika and Gunther Heidemann
Categories: cs.SD cs.LG
Comments: 12 pages, 12 figures, accepted as a Tiny paper at ICLR 2024
\\
  This paper tackles the scarcity of benchmarking data in disentangled auditory
representation learning. We introduce SynTone, a synthetic dataset with
explicit ground truth explanatory factors for evaluating disentanglement
techniques. Benchmarking state-of-the-art methods on SynTone highlights its
utility for method evaluation. Our results underscore strengths and limitations
in audio disentanglement, motivating future research.
\\ ( https://arxiv.org/abs/2402.10547 ,  3141kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10553 (*cross-listing*)
Date: Fri, 16 Feb 2024 10:35:01 GMT   (180kb,D)

Title: A novel integrated industrial approach with cobots in the age of
  industry 4.0 through conversational interaction and computer vision
Authors: Andrea Pazienza and Nicola Macchiarulo and Felice Vitulano and Antonio
  Fiorentini and Marco Cammisa and Leonardo Rigutini and Ernesto Di Iorio and
  Achille Globo and Antonio Trevisi
Categories: cs.RO cs.LG
Journal-ref: Proceedings of the 6th Italian Conference on Computational
  Linguistics (CLiC-it 2019)
\\
  From robots that replace workers to robots that serve as helpful colleagues,
the field of robotic automation is experiencing a new trend that represents a
huge challenge for component manufacturers. The contribution starts from an
innovative vision that sees an ever closer collaboration between Cobot, able to
do a specific physical job with precision, the AI world, able to analyze
information and support the decision-making process, and the man able to have a
strategic vision of the future.
\\ ( https://arxiv.org/abs/2402.10553 ,  180kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10609 (*cross-listing*)
Date: Fri, 16 Feb 2024 11:54:34 GMT   (21966kb,D)

Title: U$^2$MRPD: Unsupervised undersampled MRI reconstruction by prompting a
  large latent diffusion model
Authors: Ziqi Gao, S. Kevin Zhou
Categories: eess.IV cs.CV cs.LG
Comments: 17 pages, 6 figures, 5 tables, 2 pseudocodes
\\
  Implicit visual knowledge in a large latent diffusion model (LLDM)
pre-trained on natural images is rich and hypothetically universal to natural
and medical images. To test this hypothesis, we introduce a novel framework for
Unsupervised Undersampled MRI Reconstruction by Prompting a pre-trained large
latent Diffusion model ( U$^2$MRPD). Existing data-driven, supervised
undersampled MRI reconstruction networks are typically of limited
generalizability and adaptability toward diverse data acquisition scenarios;
yet U$^2$MRPD supports image-specific MRI reconstruction by prompting an LLDM
with an MRSampler tailored for complex-valued MRI images. With any
single-source or diverse-source MRI dataset, U$^2$MRPD's performance is further
boosted by an MRAdapter while keeping the generative image priors intact.
Experiments on multiple datasets show that U$^2$MRPD achieves comparable or
better performance than supervised and MRI diffusion methods on in-domain
datasets while demonstrating the best generalizability on out-of-domain
datasets. To the best of our knowledge, U$^2$MRPD is the {\bf first}
unsupervised method that demonstrates the universal prowess of a LLDM, %trained
on magnitude-only natural images in medical imaging, attaining the best
adaptability for both MRI database-free and database-available scenarios and
generalizability towards out-of-domain data.
\\ ( https://arxiv.org/abs/2402.10609 ,  21966kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10641 (*cross-listing*)
Date: Fri, 16 Feb 2024 12:41:31 GMT   (6582kb,D)

Title: A Predictive Surrogate Model for Heat Transfer of an Impinging Jet on a
  Concave Surface
Authors: Sajad Salavatidezfouli, Saeid Rakhsha, Armin Sheidani, Giovanni
  Stabile and Gianluigi Rozza
Categories: math.NA cs.LG cs.NA
\\
  This paper aims to comprehensively investigate the efficacy of various Model
Order Reduction (MOR) and deep learning techniques in predicting heat transfer
in a pulsed jet impinging on a concave surface. Expanding on the previous
experimental and numerical research involving pulsed circular jets, this
investigation extends to evaluate Predictive Surrogate Models (PSM) for heat
transfer across various jet characteristics. To this end, this work introduces
two predictive approaches, one employing a Fast Fourier Transformation
augmented Artificial Neural Network (FFT-ANN) for predicting the average
Nusselt number under constant-frequency scenarios. Moreover, the investigation
introduces the Proper Orthogonal Decomposition and Long Short-Term Memory
(POD-LSTM) approach for random-frequency impingement jets. The POD-LSTM method
proves to be a robust solution for predicting the local heat transfer rate
under random-frequency impingement scenarios, capturing both the trend and
value of temporal modes. The comparison of these approaches highlights the
versatility and efficacy of advanced machine learning techniques in modelling
complex heat transfer phenomena.
\\ ( https://arxiv.org/abs/2402.10641 ,  6582kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10677 (*cross-listing*)
Date: Fri, 16 Feb 2024 13:31:43 GMT   (290kb,D)

Title: Performance Gaps in Multi-view Clustering under the Nested Matrix-Tensor
  Model
Authors: Hugo Lebeau, Mohamed El Amine Seddik, Jos\'e Henrique de Morais
  Goulart
Categories: stat.ML cs.LG math.PR
\\
  We study the estimation of a planted signal hidden in a recently introduced
nested matrix-tensor model, which is an extension of the classical spiked
rank-one tensor model, motivated by multi-view clustering. Prior work has
theoretically examined the performance of a tensor-based approach, which relies
on finding a best rank-one approximation, a problem known to be computationally
hard. A tractable alternative approach consists in computing instead the best
rank-one (matrix) approximation of an unfolding of the observed tensor data,
but its performance was hitherto unknown. We quantify here the performance gap
between these two approaches, in particular by deriving the precise algorithmic
threshold of the unfolding approach and demonstrating that it exhibits a
BBP-type transition behavior. This work is therefore in line with recent
contributions which deepen our understanding of why tensor-based methods
surpass matrix-based methods in handling structured tensor data.
\\ ( https://arxiv.org/abs/2402.10677 ,  290kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10686 (*cross-listing*)
Date: Fri, 16 Feb 2024 13:41:18 GMT   (2371kb,D)

Title: Uncertainty, Calibration, and Membership Inference Attacks: An
  Information-Theoretic Perspective
Authors: Meiyi Zhu, Caili Guo, Chunyan Feng, Osvaldo Simeone
Categories: cs.IT cs.CR cs.LG eess.SP math.IT
Comments: 27 pages, 13 figures
\\
  In a membership inference attack (MIA), an attacker exploits the
overconfidence exhibited by typical machine learning models to determine
whether a specific data point was used to train a target model. In this paper,
we analyze the performance of the state-of-the-art likelihood ratio attack
(LiRA) within an information-theoretical framework that allows the
investigation of the impact of the aleatoric uncertainty in the true data
generation process, of the epistemic uncertainty caused by a limited training
data set, and of the calibration level of the target model. We compare three
different settings, in which the attacker receives decreasingly informative
feedback from the target model: confidence vector (CV) disclosure, in which the
output probability vector is released; true label confidence (TLC) disclosure,
in which only the probability assigned to the true label is made available by
the model; and decision set (DS) disclosure, in which an adaptive prediction
set is produced as in conformal prediction. We derive bounds on the advantage
of an MIA adversary with the aim of offering insights into the impact of
uncertainty and calibration on the effectiveness of MIAs. Simulation results
demonstrate that the derived analytical bounds predict well the effectiveness
of MIAs.
\\ ( https://arxiv.org/abs/2402.10686 ,  2371kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10723 (*cross-listing*)
Date: Fri, 16 Feb 2024 14:30:12 GMT   (1846kb,D)

Title: Conformalized Credal Set Predictors
Authors: Alireza Javanmardi, David Stutz, Eyke H\"ullermeier
Categories: stat.ML cs.LG
\\
  Credal sets are sets of probability distributions that are considered as
candidates for an imprecisely known ground-truth distribution. In machine
learning, they have recently attracted attention as an appealing formalism for
uncertainty representation, in particular due to their ability to represent
both the aleatoric and epistemic uncertainty in a prediction. However, the
design of methods for learning credal set predictors remains a challenging
problem. In this paper, we make use of conformal prediction for this purpose.
More specifically, we propose a method for predicting credal sets in the
classification task, given training data labeled by probability distributions.
Since our method inherits the coverage guarantees of conformal prediction, our
conformal credal sets are guaranteed to be valid with high probability (without
any assumptions on model or distribution). We demonstrate the applicability of
our method to natural language inference, a highly ambiguous natural language
task where it is common to obtain multiple annotations per example.
\\ ( https://arxiv.org/abs/2402.10723 ,  1846kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10727 (*cross-listing*)
Date: Fri, 16 Feb 2024 14:40:22 GMT   (3688kb,D)

Title: Predictive Uncertainty Quantification via Risk Decompositions for
  Strictly Proper Scoring Rules
Authors: Nikita Kotelevskii, Maxim Panov
Categories: stat.ML cs.LG
\\
  Distinguishing sources of predictive uncertainty is of crucial importance in
the application of forecasting models across various domains. Despite the
presence of a great variety of proposed uncertainty measures, there are no
strict definitions to disentangle them. Furthermore, the relationship between
different measures of uncertainty quantification remains somewhat unclear. In
this work, we introduce a general framework, rooted in statistical reasoning,
which not only allows the creation of new uncertainty measures but also
clarifies their interrelations. Our approach leverages statistical risk to
distinguish aleatoric and epistemic uncertainty components and utilizes proper
scoring rules to quantify them. To make it practically tractable, we propose an
idea to incorporate Bayesian reasoning into this framework and discuss the
properties of the proposed approximation.
\\ ( https://arxiv.org/abs/2402.10727 ,  3688kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10748 (*cross-listing*)
Date: Fri, 16 Feb 2024 15:14:16 GMT   (953kb,D)

Title: A Noisy Beat is Worth 16 Words: a Tiny Transformer for Low-Power
  Arrhythmia Classification on Microcontrollers
Authors: Paola Busia, Matteo Antonio Scrugli, Victor Jean-Baptiste Jung, Luca
  Benini, Paolo Meloni
Categories: eess.SP cs.HC cs.LG
\\
  Wearable systems for the long-term monitoring of cardiovascular diseases are
becoming widespread and valuable assets in diagnosis and therapy. A promising
approach for real-time analysis of the electrocardiographic (ECG) signal and
the detection of heart conditions, such as arrhythmia, is represented by the
transformer machine learning model. Transformers are powerful models for the
classification of time series, although efficient implementation in the
wearable domain raises significant design challenges, to combine adequate
accuracy and a suitable complexity. In this work, we present a tiny transformer
model for the analysis of the ECG signal, requiring only 6k parameters and
reaching 98.97% accuracy in the recognition of the 5 most common arrhythmia
classes from the MIT-BIH Arrhythmia database, assessed considering 8-bit
integer inference as required for efficient execution on low-power
microcontroller-based devices. We explored an augmentation-based training
approach for improving the robustness against electrode motion artifacts noise,
resulting in a worst-case post-deployment performance assessment of 98.36%
accuracy. Suitability for wearable monitoring solutions is finally demonstrated
through efficient deployment on the parallel ultra-low-power GAP9 processor,
where inference execution requires 4.28ms and 0.09mJ.
\\ ( https://arxiv.org/abs/2402.10748 ,  953kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10754 (*cross-listing*)
Date: Fri, 16 Feb 2024 15:21:35 GMT   (1856kb,D)

Title: When Dataflow Analysis Meets Large Language Models
Authors: Chengpeng Wang, Wuqi Zhang, Zian Su, Xiangzhe Xu, Xiaoheng Xie,
  Xiangyu Zhang
Categories: cs.PL cs.LG cs.SE
Comments: 15 pages, 16 figures, 5 tables
MSC-class: 68N30, 68T01
ACM-class: D.3.0; D.2.4; I.2.5; I.2.6
\\
  Dataflow analysis is a powerful code analysis technique that reasons
dependencies between program values, offering support for code optimization,
program comprehension, and bug detection. Existing approaches require the
successful compilation of the subject program and customizations for downstream
applications. This paper introduces LLMDFA, an LLM-powered dataflow analysis
framework that analyzes arbitrary code snippets without requiring a compilation
infrastructure and automatically synthesizes downstream applications. Inspired
by summary-based dataflow analysis, LLMDFA decomposes the problem into three
sub-problems, which are effectively resolved by several essential strategies,
including few-shot chain-of-thought prompting and tool synthesis. Our
evaluation has shown that the design can mitigate the hallucination and improve
the reasoning ability, obtaining high precision and recall in detecting
dataflow-related bugs upon benchmark programs, outperforming state-of-the-art
(classic) tools, including a very recent industrial analyzer.
\\ ( https://arxiv.org/abs/2402.10754 ,  1856kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10758 (*cross-listing*)
Date: Fri, 16 Feb 2024 15:28:41 GMT   (3795kb,D)

Title: Stochastic Localization via Iterative Posterior Sampling
Authors: Louis Grenioux, Maxence Noble, Marylou Gabri\'e, Alain Oliviero Durmus
Categories: stat.ML cs.LG stat.CO
\\
  Building upon score-based learning, new interest in stochastic localization
techniques has recently emerged. In these models, one seeks to noise a sample
from the data distribution through a stochastic process, called observation
process, and progressively learns a denoiser associated to this dynamics. Apart
from specific applications, the use of stochastic localization for the problem
of sampling from an unnormalized target density has not been explored
extensively. This work contributes to fill this gap. We consider a general
stochastic localization framework and introduce an explicit class of
observation processes, associated with flexible denoising schedules. We provide
a complete methodology, $\textit{Stochastic Localization via Iterative
Posterior Sampling}$ (SLIPS), to obtain approximate samples of this dynamics,
and as a by-product, samples from the target distribution. Our scheme is based
on a Markov chain Monte Carlo estimation of the denoiser and comes with
detailed practical guidelines. We illustrate the benefits and applicability of
SLIPS on several benchmarks, including Gaussian mixtures in increasing
dimensions, Bayesian logistic regression and a high-dimensional field system
from statistical-mechanics.
\\ ( https://arxiv.org/abs/2402.10758 ,  3795kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10760 (*cross-listing*)
Date: Fri, 16 Feb 2024 15:34:07 GMT   (15275kb,D)

Title: RAGIC: Risk-Aware Generative Adversarial Model for Stock Interval
  Construction
Authors: Jingyi Gu, Wenlu Du, Guiling Wang
Categories: q-fin.ST cs.LG
\\
  Efforts to predict stock market outcomes have yielded limited success due to
the inherently stochastic nature of the market, influenced by numerous
unpredictable factors. Many existing prediction approaches focus on
single-point predictions, lacking the depth needed for effective
decision-making and often overlooking market risk. To bridge this gap, we
propose a novel model, RAGIC, which introduces sequence generation for stock
interval prediction to quantify uncertainty more effectively. Our approach
leverages a Generative Adversarial Network (GAN) to produce future price
sequences infused with randomness inherent in financial markets. RAGIC's
generator includes a risk module, capturing the risk perception of informed
investors, and a temporal module, accounting for historical price trends and
seasonality. This multi-faceted generator informs the creation of
risk-sensitive intervals through statistical inference, incorporating
horizon-wise insights. The interval's width is carefully adjusted to reflect
market volatility. Importantly, our approach relies solely on publicly
available data and incurs only low computational overhead. RAGIC's evaluation
across globally recognized broad-based indices demonstrates its balanced
performance, offering both accuracy and informativeness. Achieving a consistent
95% coverage, RAGIC maintains a narrow interval width. This promising outcome
suggests that our approach effectively addresses the challenges of stock market
prediction while incorporating vital risk considerations.
\\ ( https://arxiv.org/abs/2402.10760 ,  15275kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10797 (*cross-listing*)
Date: Fri, 16 Feb 2024 16:21:02 GMT   (21kb)

Title: BlackJAX: Composable Bayesian inference in JAX
Authors: Alberto Cabezas, Adrien Corenflos, Junpeng Lao, R\'emi Louf
Categories: cs.MS cs.LG stat.CO stat.ML
Comments: Companion paper for the library
  https://github.com/blackjax-devs/blackjax
\\
  BlackJAX is a library implementing sampling and variational inference
algorithms commonly used in Bayesian computation. It is designed for ease of
use, speed, and modularity by taking a functional approach to the algorithms'
implementation. BlackJAX is written in Python, using JAX to compile and run
NumpPy-like samplers and variational methods on CPUs, GPUs, and TPUs. The
library integrates well with probabilistic programming languages by working
directly with the (un-normalized) target log density function. BlackJAX is
intended as a collection of low-level, composable implementations of basic
statistical 'atoms' that can be combined to perform well-defined Bayesian
inference, but also provides high-level routines for ease of use. It is
designed for users who need cutting-edge methods, researchers who want to
create complex sampling methods, and people who want to learn how these work.
\\ ( https://arxiv.org/abs/2402.10797 ,  21kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10831 (*cross-listing*)
Date: Fri, 16 Feb 2024 17:03:08 GMT   (4748kb,D)

Title: GAN-driven Electromagnetic Imaging of 2-D Dielectric Scatterers
Authors: Ehtasham Naseer, Ali Imran Sandhu, Muhammad Adnan Siddique, Waqas W.
  Ahmed, Mohamed Farhat, and Ying Wu
Categories: eess.IV cs.CE cs.LG
\\
  Inverse scattering problems are inherently challenging, given the fact they
are ill-posed and nonlinear. This paper presents a powerful deep learning-based
approach that relies on generative adversarial networks to accurately and
efficiently reconstruct randomly-shaped two-dimensional dielectric objects from
amplitudes of multi-frequency scattered electric fields. An adversarial
autoencoder (AAE) is trained to learn to generate the scatterer's geometry from
a lower-dimensional latent representation constrained to adhere to the Gaussian
distribution. A cohesive inverse neural network (INN) framework is set up
comprising a sequence of appropriately designed dense layers, the
already-trained generator as well as a separately trained forward neural
network. The images reconstructed at the output of the inverse network are
validated through comparison with outputs from the forward neural network,
addressing the non-uniqueness challenge inherent to electromagnetic (EM)
imaging problems. The trained INN demonstrates an enhanced robustness,
evidenced by a mean binary cross-entropy (BCE) loss of $0.13$ and a structure
similarity index (SSI) of $0.90$. The study not only demonstrates a significant
reduction in computational load, but also marks a substantial improvement over
traditional objective-function-based methods. It contributes both to the fields
of machine learning and EM imaging by offering a real-time quantitative imaging
approach. The results obtained with the simulated data, for both training and
testing, yield promising results and may open new avenues for radio-frequency
inverse imaging.
\\ ( https://arxiv.org/abs/2402.10831 ,  4748kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10851 (*cross-listing*)
Date: Fri, 16 Feb 2024 17:44:11 GMT   (2785kb,D)

Title: HistoSegCap: Capsules for Weakly-Supervised Semantic Segmentation of
  Histological Tissue Type in Whole Slide Images
Authors: Mobina Mansoori, Sajjad Shahabodini, Jamshid Abouei, Arash Mohammadi,
  Konstantinos N. Plataniotis
Categories: eess.IV cs.CV cs.LG
\\
  Digital pathology involves converting physical tissue slides into
high-resolution Whole Slide Images (WSIs), which pathologists analyze for
disease-affected tissues. However, large histology slides with numerous
microscopic fields pose challenges for visual search. To aid pathologists,
Computer Aided Diagnosis (CAD) systems offer visual assistance in efficiently
examining WSIs and identifying diagnostically relevant regions. This paper
presents a novel histopathological image analysis method employing Weakly
Supervised Semantic Segmentation (WSSS) based on Capsule Networks, the first
such application. The proposed model is evaluated using the Atlas of Digital
Pathology (ADP) dataset and its performance is compared with other
histopathological semantic segmentation methodologies. The findings underscore
the potential of Capsule Networks in enhancing the precision and efficiency of
histopathological image analysis. Experimental results show that the proposed
model outperforms traditional methods in terms of accuracy and the mean
Intersection-over-Union (mIoU) metric.
\\ ( https://arxiv.org/abs/2402.10851 ,  2785kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10857 (*cross-listing*)
Date: Fri, 16 Feb 2024 17:53:08 GMT   (50kb)

Title: JetTrain: IDE-Native Machine Learning Experiments
Authors: Artem Trofimov, Mikhail Kostyukov, Sergei Ugdyzhekov, Natalia
  Ponomareva, Igor Naumov, Maksim Melekhovets
Categories: cs.SE cs.LG
Comments: IDE workshop @ ICSE 2024
DOI: 10.1145/3643796.3648455
\\
  Integrated development environments (IDEs) are prevalent code-writing and
debugging tools. However, they have yet to be widely adopted for launching
machine learning (ML) experiments. This work aims to fill this gap by
introducing JetTrain, an IDE-integrated tool that delegates specific tasks from
an IDE to remote computational resources. A user can write and debug code
locally and then seamlessly run it remotely using on-demand hardware. We argue
that this approach can lower the entry barrier for ML training problems and
increase experiment throughput.
\\ ( https://arxiv.org/abs/2402.10857 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10874 (*cross-listing*)
Date: Fri, 16 Feb 2024 18:20:33 GMT   (44087kb,D)

Title: Design of 2D Skyrmionic Metamaterial Through Controlled Assembly
Authors: Qichen Xu, Zhuanglin Shen, Alexander Edstr\"om, I. P. Miranda, Zhiwei
  Lu, Anders Bergman, Danny Thonig, Wanjian Yin, Olle Eriksson, Anna Delin
Categories: cond-mat.mtrl-sci cs.LG physics.comp-ph
\\
  Despite extensive research on magnetic skyrmions and antiskyrmions, a
significant challenge remains in crafting nontrivial high-order skyrmionic
textures with varying, or even tailor-made, topologies. We address this
challenge, by focusing on a construction pathway of skyrmionics metamaterial
within a monolayer thin film and suggest several promising lattice-like,
flakes-like, and cell-like skyrmionic metamaterials that are surprisingly
stable. Central to our approach is the concept of 'simulated controlled
assembly', in short, a protocol inspired by 'click chemistry' that allows for
positioning topological magnetic structures where one likes, and then allowing
for energy minimization to elucidate the stability. Utilizing high-throughput
atomistic-spin-dynamic (ASD) simulations alongside state-of-the-art AI-driven
tools, we have isolated skyrmions (topological charge Q=1), antiskyrmions
(Q=-1), and skyrmionium (Q=0). These entities serve as foundational 'skyrmionic
building blocks' to forming reported intricate textures. In this work, two key
contributions are introduced to the field of skyrmionic systems. First, we
present a novel method for integrating control assembly protocols for the
stabilization and investigation of topological magnets, which marks a
significant advancement in the ability to explore new skyrmionic textures.
Second, we report on the discovery of skyrmionic metamaterials, which shows a
plethora of complex topologies that are possible to investigate theoretically
and experimentally.
\\ ( https://arxiv.org/abs/2402.10874 ,  44087kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10894 (*cross-listing*)
Date: Fri, 16 Feb 2024 18:51:42 GMT   (910kb,D)

Title: Fusion of Diffusion Weighted MRI and Clinical Data for Predicting
  Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning
Authors: Chia-Ling Tsai, Hui-Yun Su, Shen-Feng Sung, Wei-Yang Lin, Ying-Ying
  Su, Tzu-Hsien Yang, Man-Lin Mai
Categories: cs.CV cs.LG
Comments: 12 pages, 5 figures, 5 tables
\\
  Stroke is a common disabling neurological condition that affects about
one-quarter of the adult population over age 25; more than half of patients
still have poor outcomes, such as permanent functional dependence or even
death, after the onset of acute stroke. The aim of this study is to investigate
the efficacy of diffusion-weighted MRI modalities combining with structured
health profile on predicting the functional outcome to facilitate early
intervention. A deep fusion learning network is proposed with two-stage
training: the first stage focuses on cross-modality representation learning and
the second stage on classification. Supervised contrastive learning is
exploited to learn discriminative features that separate the two classes of
patients from embeddings of individual modalities and from the fused multimodal
embedding. The network takes as the input DWI and ADC images, and structured
health profile data. The outcome is the prediction of the patient needing
long-term care at 3 months after the onset of stroke. Trained and evaluated
with a dataset of 3297 patients, our proposed fusion model achieves 0.87, 0.80
and 80.45% for AUC, F1-score and accuracy, respectively, outperforming existing
models that consolidate both imaging and structured data in the medical domain.
If trained with comprehensive clinical variables, including NIHSS and
comorbidities, the gain from images on making accurate prediction is not
considered substantial, but significant. However, diffusion-weighted MRI can
replace NIHSS to achieve comparable level of accuracy combining with other
readily available clinical variables for better generalization.
\\ ( https://arxiv.org/abs/2402.10894 ,  910kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10898 (*cross-listing*)
Date: Fri, 16 Feb 2024 18:56:41 GMT   (33kb,D)

Title: The Price of Adaptivity in Stochastic Convex Optimization
Authors: Yair Carmon and Oliver Hinder
Categories: math.OC cs.LG stat.ML
\\
  We prove impossibility results for adaptivity in non-smooth stochastic convex
optimization. Given a set of problem parameters we wish to adapt to, we define
a "price of adaptivity" (PoA) that, roughly speaking, measures the
multiplicative increase in suboptimality due to uncertainty in these
parameters. When the initial distance to the optimum is unknown but a gradient
norm bound is known, we show that the PoA is at least logarithmic for expected
suboptimality, and double-logarithmic for median suboptimality. When there is
uncertainty in both distance and gradient norm, we show that the PoA must be
polynomial in the level of uncertainty. Our lower bounds nearly match existing
upper bounds, and establish that there is no parameter-free lunch.
\\ ( https://arxiv.org/abs/2402.10898 ,  33kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2301.11118
replaced with revised version Fri, 16 Feb 2024 15:50:01 GMT   (332kb,D)

Title: Dual Box Embeddings for the Description Logic EL++
Authors: Mathias Jackermeier, Jiaoyan Chen, Ian Horrocks
Categories: cs.AI cs.LG cs.LO
Comments: Updated version accepted to WWW '24
\\ ( https://arxiv.org/abs/2301.11118 ,  332kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05212
replaced with revised version Fri, 16 Feb 2024 13:00:20 GMT   (3340kb,D)

Title: Interpretable Semiotics Networks Representing Awareness
Authors: David Kupeev and Eyal Nitcany
Categories: cs.AI cs.CV cs.SI
\\ ( https://arxiv.org/abs/2310.05212 ,  3340kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07637
replaced with revised version Fri, 16 Feb 2024 08:17:06 GMT   (1852kb,D)

Title: OpsEval: A Comprehensive IT Operations Benchmark Suite for Large
  Language Models
Authors: Yuhe Liu, Changhua Pei, Longlong Xu, Bohan Chen, Mingze Sun, Zhirui
  Zhang, Yongqian Sun, Shenglin Zhang, Kun Wang, Haiming Zhang, Jianhui Li,
  Gaogang Xie, Xidao Wen, Xiaohui Nie, Minghua Ma, Dan Pei
Categories: cs.AI cs.NI
\\ ( https://arxiv.org/abs/2310.07637 ,  1852kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11539
replaced with revised version Fri, 16 Feb 2024 17:41:01 GMT   (7628kb,D)

Title: KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM
  Does and Doesn't Know
Authors: Shangshang Zheng, He Bai, Yizhe Zhang, Yi Su, Xiaochuan Niu, Navdeep
  Jaitly
Categories: cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2312.11539 ,  7628kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07324
replaced with revised version Fri, 16 Feb 2024 12:42:25 GMT   (10664kb,D)

Title: Small LLMs Are Weak Tool Learners: A Multi-LLM Agent
Authors: Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan,
  Hehong Chen, Ji Zhang, Fei Huang
Categories: cs.AI cs.CL
Comments: On progress, github repo: https://github.com/X-PLUG/Multi-LLM-Agent
\\ ( https://arxiv.org/abs/2401.07324 ,  10664kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06764
replaced with revised version Fri, 16 Feb 2024 17:23:56 GMT   (543kb,D)

Title: GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph
  Alignment via Neighborhood Partitioning and Generative Subgraph Encoding
Authors: Stefan Dernbach, Khushbu Agarwal, Alejandro Zuniga, Michael Henry,
  Sutanay Choudhury
Categories: cs.AI
Comments: To be published in AAAI Spring Symposium: AAAI-MAKE 2024
\\ ( https://arxiv.org/abs/2402.06764 ,  543kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06782
replaced with revised version Thu, 15 Feb 2024 22:09:52 GMT   (7563kb,D)

Title: Debating with More Persuasive LLMs Leads to More Truthful Answers
Authors: Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan,
  Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rockt\"aschel
  and Ethan Perez
Categories: cs.AI cs.CL
Comments: For code please check: https://github.com/ucl-dark/llm_debate
\\ ( https://arxiv.org/abs/2402.06782 ,  7563kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09346
replaced with revised version Fri, 16 Feb 2024 16:58:20 GMT   (2129kb,D)

Title: Developing a Framework for Auditing Large Language Models Using
  Human-in-the-Loop
Authors: Maryam Amirizaniani, Jihan Yao, Adrian Lavergne, Elizabeth Snell
  Okada, Aman Chadha, Tanya Roosta, Chirag Shah
Categories: cs.AI
\\ ( https://arxiv.org/abs/2402.09346 ,  2129kb)
------------------------------------------------------------------------------
\\
arXiv:2303.12023
replaced with revised version Fri, 16 Feb 2024 14:30:33 GMT   (7795kb,D)

Title: Logical Reasoning over Natural Language as Knowledge Representation: A
  Survey
Authors: Zonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, Erik Cambria
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2303.12023 ,  7795kb)
------------------------------------------------------------------------------
\\
arXiv:2304.10813
replaced with revised version Fri, 16 Feb 2024 07:55:59 GMT   (8486kb,D)

Title: Tokenization Preference for Human and Machine Learning Model: An
  Annotation Study
Authors: Tatsuya Hiraoka, Tomoya Iwakura
Categories: cs.CL
\\ ( https://arxiv.org/abs/2304.10813 ,  8486kb)
------------------------------------------------------------------------------
\\
arXiv:2305.01713
replaced with revised version Fri, 16 Feb 2024 12:02:54 GMT   (17003kb,D)

Title: Learning Disentangled Semantic Spaces of Explanations via Invertible
  Neural Networks
Authors: Yingji Zhang, Danilo S. Carvalho, Andr\'e Freitas
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2305.01713 ,  17003kb)
------------------------------------------------------------------------------
\\
arXiv:2305.07303
replaced with revised version Fri, 16 Feb 2024 11:56:20 GMT   (390kb,D)

Title: Multi-Relational Hyperbolic Word Embeddings from Natural Language
  Definitions
Authors: Marco Valentino, Danilo S. Carvalho, Andr\'e Freitas
Categories: cs.CL cs.LG
Comments: Accepted at the 18th Conference of the European Chapter of the
  Association for Computational Linguistics (EACL 2024), camera-ready
\\ ( https://arxiv.org/abs/2305.07303 ,  390kb)
------------------------------------------------------------------------------
\\
arXiv:2305.07358
replaced with revised version Fri, 16 Feb 2024 02:55:03 GMT   (2155kb,D)

Title: Towards Versatile and Efficient Visual Knowledge Integration into
  Pre-trained Language Models with Cross-Modal Adapters
Authors: Xinyun Zhang, Haochen Tan, Han Wu, Bei Yu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2305.07358 ,  2155kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11738
replaced with revised version Fri, 16 Feb 2024 08:17:39 GMT   (653kb,D)

Title: CRITIC: Large Language Models Can Self-Correct with Tool-Interactive
  Critiquing
Authors: Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan
  Duan, Weizhu Chen
Categories: cs.CL cs.AI
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2305.11738 ,  653kb)
------------------------------------------------------------------------------
\\
arXiv:2306.17820
replaced with revised version Fri, 16 Feb 2024 12:46:27 GMT   (9037kb,D)

Title: Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language
  Models
Authors: Yiming Wang, Zhuosheng Zhang, Pei Zhang, Baosong Yang, Rui Wang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2306.17820 ,  9037kb)
------------------------------------------------------------------------------
\\
arXiv:2307.10928
replaced with revised version Fri, 16 Feb 2024 05:04:45 GMT   (5271kb,D)

Title: FLASK: Fine-grained Language Model Evaluation based on Alignment Skill
  Sets
Authors: Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone
  Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo
Categories: cs.CL cs.AI
Comments: ICLR 2024 Spotlight
\\ ( https://arxiv.org/abs/2307.10928 ,  5271kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07876
replaced with revised version Fri, 16 Feb 2024 13:23:08 GMT   (2042kb,D)

Title: Synthesizing Political Zero-Shot Relation Classification via Codebook
  Knowledge, NLI, and ChatGPT
Authors: Yibo Hu, Erick Skorupa Parolin, Latifur Khan, Patrick T. Brandt,
  Javier Osorio, Vito J. D'Orazio
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: Preprint
\\ ( https://arxiv.org/abs/2308.07876 ,  2042kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08295
replaced with revised version Fri, 16 Feb 2024 12:32:52 GMT   (2842kb,D)

Title: CMD: a framework for Context-aware Model self-Detoxification
Authors: Zecheng Tang, Keyan Zhou, Juntao Li, Yuyang Ding, Pinzheng Wang, Bowen
  Yan, Min Zhang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2308.08295 ,  2842kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09124
replaced with revised version Thu, 15 Feb 2024 19:12:10 GMT   (966kb,D)

Title: Linearity of Relation Decoding in Transformer Language Models
Authors: Evan Hernandez, Arnab Sen Sharma, Tal Haklay, Kevin Meng, Martin
  Wattenberg, Jacob Andreas, Yonatan Belinkov, David Bau
Categories: cs.CL
\\ ( https://arxiv.org/abs/2308.09124 ,  966kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00378
replaced with revised version Fri, 16 Feb 2024 14:59:03 GMT   (23106kb,D)

Title: Long-Term Ad Memorability: Understanding and Generating Memorable Ads
Authors: Harini S I, Somesh Singh, Yaman K Singla, Aanisha Bhattacharyya, Veeky
  Baths, Changyou Chen, Rajiv Ratn Shah, Balaji Krishnamurthy
Categories: cs.CL cs.CV cs.HC
\\ ( https://arxiv.org/abs/2309.00378 ,  23106kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02726
replaced with revised version Fri, 16 Feb 2024 14:26:28 GMT   (3030kb,D)

Title: Large Language Models for Automated Open-domain Scientific Hypotheses
  Discovery
Authors: Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik
  Cambria
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2309.02726 ,  3030kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07430
replaced with revised version Thu, 15 Feb 2024 19:37:10 GMT   (12024kb,D)

Title: Adapted Large Language Models Can Outperform Medical Experts in Clinical
  Text Summarization
Authors: Dave Van Veen, Cara Van Uden, Louis Blankemeier, Jean-Benoit
  Delbrouck, Asad Aali, Christian Bluethgen, Anuj Pareek, Malgorzata Polacin,
  Eduardo Pontes Reis, Anna Seehofnerova, Nidhi Rohatgi, Poonam Hosamani,
  William Collins, Neera Ahuja, Curtis P. Langlotz, Jason Hom, Sergios Gatidis,
  John Pauly, Akshay S. Chaudhari
Categories: cs.CL
Comments: 27 pages, 19 figures. Compared to v3, this version adds downstream
  analyses and incorporates reviewer feedback to reinforce the initial findings
\\ ( https://arxiv.org/abs/2309.07430 ,  12024kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08047
replaced with revised version Fri, 16 Feb 2024 12:56:31 GMT   (92kb,D)

Title: Gender Bias in News Summarization: Measures, Pitfalls and Corpora
Authors: Julius Steen, Katja Markert
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.08047 ,  92kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08316
replaced with revised version Fri, 16 Feb 2024 14:37:15 GMT   (13146kb,D)

Title: How to Handle Different Types of Out-of-Distribution Scenarios in
  Computational Argumentation? A Comprehensive and Fine-Grained Field Study
Authors: Andreas Waldis, Yufang Hou and Iryna Gurevych
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.08316 ,  13146kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17452
replaced with revised version Fri, 16 Feb 2024 08:22:59 GMT   (469kb,D)

Title: ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving
Authors: Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie
  Huang, Nan Duan, Weizhu Chen
Categories: cs.CL cs.AI
Comments: ICLR 2024; First two authors equal contribution
\\ ( https://arxiv.org/abs/2309.17452 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00796
replaced with revised version Fri, 16 Feb 2024 16:49:11 GMT   (300kb,D)

Title: Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation
Authors: Matthias Lindemann and Alexander Koller and Ivan Titov
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.00796 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00867
replaced with revised version Fri, 16 Feb 2024 18:39:45 GMT   (304kb,D)

Title: Do Compressed LLMs Forget Knowledge? An Experimental Study with
  Practical Implications
Authors: Duc N.M Hoang, Minsik Cho, Thomas Merth, Mohammad Rastegari, Zhangyang
  Wang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2310.00867 ,  304kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11878
replaced with revised version Fri, 16 Feb 2024 17:31:10 GMT   (863kb,D)

Title: From Dissonance to Insights: Dissecting Disagreements in Rationale
  Construction for Case Outcome Classification
Authors: Shanshan Xu, T.Y.S.S Santosh, Oana Ichim, Isabella Risini, Barbara
  Plank, Matthias Grabmair
Categories: cs.CL
Comments: Accepted to EMNLP 2023
\\ ( https://arxiv.org/abs/2310.11878 ,  863kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12481
replaced with revised version Fri, 16 Feb 2024 14:06:41 GMT   (17259kb,D)

Title: Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in
  Large Language Models
Authors: Wenxuan Wang, Wenxiang Jiao, Jingyuan Huang, Ruyi Dai, Jen-tse Huang,
  Zhaopeng Tu, Michael R. Lyu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2310.12481 ,  17259kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20329
replaced with revised version Fri, 16 Feb 2024 06:09:54 GMT   (14590kb,D)

Title: InstructCoder: Empowering Language Models for Code Editing
Authors: Kaixin Li, Qisheng Hu, Xu Zhao, Hui Chen, Yuxi Xie, Tiedong Liu, Qizhe
  Xie, Junxian He
Categories: cs.CL cs.SE
\\ ( https://arxiv.org/abs/2310.20329 ,  14590kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00237
replaced with revised version Fri, 16 Feb 2024 00:55:48 GMT   (607kb,D)

Title: The Mystery of In-Context Learning: A Comprehensive Survey on
  Interpretation and Analysis
Authors: Yuxiang Zhou, Jiazheng Li, Yanzheng Xiang, Hanqi Yan, Lin Gui, Yulan
  He
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.00237 ,  607kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07453
replaced with revised version Fri, 16 Feb 2024 12:14:05 GMT   (10092kb,D)

Title: ChartCheck: Explainable Fact-Checking over Real-World Chart Images
Authors: Mubashara Akhtar, Nikesh Subedi, Vivek Gupta, Sahar Tahmasebi, Oana
  Cocarascu, Elena Simperl
Categories: cs.CL cs.CV
\\ ( https://arxiv.org/abs/2311.07453 ,  10092kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08011
replaced with revised version Fri, 16 Feb 2024 15:49:42 GMT   (658kb,D)

Title: Forgetting before Learning: Utilizing Parametric Arithmetic for
  Knowledge Updating in Large Language Models
Authors: Shiwen Ni, Dingwei Chen, Chengming Li, Xiping Hu, Ruifeng Xu, Min Yang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.08011 ,  658kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08370
replaced with revised version Fri, 16 Feb 2024 09:42:19 GMT   (11605kb,D)

Title: SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in
  Large Language Models
Authors: Bertie Vidgen, Nino Scherrer, Hannah Rose Kirk, Rebecca Qian, Anand
  Kannappan, Scott A. Hale, Paul R\"ottger
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.08370 ,  11605kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08803
replaced with revised version Fri, 16 Feb 2024 08:06:25 GMT   (319kb,D)

Title: StrategyLLM: Large Language Models as Strategy Generators, Executors,
  Optimizers, and Evaluators for Problem Solving
Authors: Chang Gao, Haiyun Jiang, Deng Cai, Shuming Shi, Wai Lam
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.08803 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08883
replaced with revised version Fri, 16 Feb 2024 14:07:24 GMT   (8457kb,D)

Title: Enabling Large Language Models to Learn from Rules
Authors: Wenkai Yang, Yankai Lin, Jie Zhou, Jirong Wen
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.08883 ,  8457kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09204
replaced with revised version Fri, 16 Feb 2024 05:05:56 GMT   (108kb,D)

Title: Fusion-Eval: Integrating Evaluators with LLMs
Authors: Lei Shu, Nevan Wichers, Liangchen Luo, Yun Zhu, Yinxiao Liu, Jindong
  Chen, Lei Meng
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2311.09204 ,  108kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09613
replaced with revised version Fri, 16 Feb 2024 08:49:07 GMT   (9579kb,D)

Title: Digital Socrates: Evaluating LLMs through Explanation Critiques
Authors: Yuling Gu, Oyvind Tafjord, Peter Clark
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2311.09613 ,  9579kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09630
replaced with revised version Fri, 16 Feb 2024 09:12:21 GMT   (8599kb,D)

Title: Decoding Susceptibility: Modeling Misbelief to Misinformation Through a
  Computational Approach
Authors: Yanchen Liu, Mingyu Derek Ma, Wenna Qin, Azure Zhou, Jiaao Chen,
  Weiyan Shi, Wei Wang, Diyi Yang
Categories: cs.CL cs.CY cs.SI
\\ ( https://arxiv.org/abs/2311.09630 ,  8599kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09665
replaced with revised version Fri, 16 Feb 2024 16:43:53 GMT   (25758kb,D)

Title: The Wisdom of Partisan Crowds: Comparing Collective Intelligence in
  Humans and LLM-based Agents
Authors: Yun-Shiuan Chuang, Siddharth Suresh, Nikunj Harlalka, Agam Goyal,
  Robert Hawkins, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.09665 ,  25758kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09731
replaced with revised version Fri, 16 Feb 2024 17:30:41 GMT   (2378kb,D)

Title: Examining LLMs' Uncertainty Expression Towards Questions Outside
  Parametric Knowledge
Authors: Genglin Liu, Xingyao Wang, Lifan Yuan, Yangyi Chen, Hao Peng
Categories: cs.CL cs.AI cs.LG
Comments: 20 pages
\\ ( https://arxiv.org/abs/2311.09731 ,  2378kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09832
replaced with revised version Fri, 16 Feb 2024 14:58:53 GMT   (8152kb,D)

Title: WatME: Towards Lossless Watermarking Through Lexical Redundancy
Authors: Liang Chen, Yatao Bian, Yang Deng, Deng Cai, Shuaiyi Li, Peilin Zhao,
  Kam-fai Wong
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.09832 ,  8152kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11229
replaced with revised version Fri, 16 Feb 2024 13:49:34 GMT   (222kb,D)

Title: Causal ATE Mitigates Unintended Bias in Controlled Text Generation
Authors: Rahul Madhavan and Kahini Wadhawan
Categories: cs.CL
Comments: 12 pages, 5 figures
\\ ( https://arxiv.org/abs/2311.11229 ,  222kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11075
replaced with revised version Fri, 16 Feb 2024 10:47:16 GMT   (8236kb,D)

Title: Split and Rephrase with Large Language Models
Authors: David Ponce, Thierry Etchegoyhen, Jes\'us Calleja P\'erez, Harritxu
  Gete
Categories: cs.CL
\\ ( https://arxiv.org/abs/2312.11075 ,  8236kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12713
replaced with revised version Fri, 16 Feb 2024 02:19:39 GMT   (327kb,D)

Title: Response Enhanced Semi-supervised Dialogue Query Generation
Authors: Jianheng Huang, Ante Wang, Linfeng Gao, Linfeng Song, Jinsong Su
Categories: cs.CL cs.AI
Comments: AAAI-24 main track paper
\\ ( https://arxiv.org/abs/2312.12713 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16132
replaced with revised version Fri, 16 Feb 2024 10:02:44 GMT   (204kb,D)

Title: RoleEval: A Bilingual Role Evaluation Benchmark for Large Language
  Models
Authors: Tianhao Shen, Sun Li, Quan Tu, Deyi Xiong
Categories: cs.CL
Comments: Our dataset is available at https://github.com/Magnetic2014/RoleEval
\\ ( https://arxiv.org/abs/2312.16132 ,  204kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04854
replaced with revised version Thu, 15 Feb 2024 22:02:32 GMT   (39kb)

Title: Are Language Models More Like Libraries or Like Librarians?
  Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs
Authors: Harvey Lederman, Kyle Mahowald
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.04854 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04883
replaced with revised version Fri, 16 Feb 2024 06:26:01 GMT   (4796kb,D)

Title: Multi-User Chat Assistant (MUCA): a Framework Using LLMs to Facilitate
  Group Conversations
Authors: Manqing Mao, Paishun Ting, Yijian Xiang, Mingyang Xu, Julia Chen,
  Jianzhe Lin
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.04883 ,  4796kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05268
replaced with revised version Fri, 16 Feb 2024 16:19:25 GMT   (564kb,D)

Title: AUTOACT: Automatic Agent Learning from Scratch via Self-Planning
Authors: Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou,
  Yuchen Eleanor Jiang, Chengfei Lv, Huajun Chen
Categories: cs.CL cs.AI cs.HC cs.LG cs.MA
Comments: Work in progress
\\ ( https://arxiv.org/abs/2401.05268 ,  564kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05949
replaced with revised version Fri, 16 Feb 2024 13:45:25 GMT   (13021kb,D)

Title: Universal Vulnerabilities in Large Language Models: Backdoor Attacks for
  In-context Learning
Authors: Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Fengjun Pan, Jinming Wen
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.05949 ,  13021kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08190
replaced with revised version Fri, 16 Feb 2024 08:07:58 GMT   (936kb,D)

Title: MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible
  Pipeline
Authors: Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, Kai Fan
Categories: cs.CL
Comments: Work in progress
\\ ( https://arxiv.org/abs/2401.08190 ,  936kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08295
replaced with revised version Fri, 16 Feb 2024 10:16:52 GMT   (2099kb,D)

Title: SAPT: A Shared Attention Framework for Parameter-Efficient Continual
  Learning of Large Language Models
Authors: Weixiang Zhao, Shilong Wang, Yulin Hu, Yanyan Zhao, Bing Qin, Xuanyu
  Zhang, Qing Yang, Dongliang Xu, Wanxiang Che
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.08295 ,  2099kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10768
replaced with revised version Fri, 16 Feb 2024 11:26:52 GMT   (8020kb,D)

Title: Knowledge Verification to Nip Hallucination in the Bud
Authors: Fanqi Wan, Xinting Huang, Leyang Cui, Xiaojun Quan, Wei Bi, Shuming
  Shi
Categories: cs.CL
Comments: Work in progress
\\ ( https://arxiv.org/abs/2401.10768 ,  8020kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11323
replaced with revised version Fri, 16 Feb 2024 16:43:35 GMT   (713kb,D)

Title: Identifying and Analyzing Task-Encoding Tokens in Large Language Models
Authors: Yu Bai, Heyan Huang, Cesare Spinoso-Di Piano, Marc-Antoine Rondeau,
  Sanxing Chen, Yang Gao, Jackie Chi Kit Cheung
Categories: cs.CL
Comments: Work in progress
\\ ( https://arxiv.org/abs/2401.11323 ,  713kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12192
replaced with revised version Fri, 16 Feb 2024 11:10:57 GMT   (12248kb,D)

Title: Text Embedding Inversion Security for Multilingual Language Models
Authors: Yiyi Chen and Heather Lent and Johannes Bjerva
Categories: cs.CL cs.AI cs.CR
Comments: 18 pages, 17 Tables, 6 Figures
\\ ( https://arxiv.org/abs/2401.12192 ,  12248kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13246
replaced with revised version Fri, 16 Feb 2024 14:16:06 GMT   (7856kb,D)

Title: SEER: Facilitating Structured Reasoning and Explanation via
  Reinforcement Learning
Authors: Guoxin Chen and Kexin Tang and Chao Yang and Fuying Ye and Yu Qiao and
  Yiming Qian
Categories: cs.CL
Comments: Ongoing Work
\\ ( https://arxiv.org/abs/2401.13246 ,  7856kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14624
replaced with revised version Fri, 16 Feb 2024 11:02:50 GMT   (8065kb,D)

Title: Query of CC: Unearthing Large Scale Domain-Specific Knowledge from
  Public Corpora
Authors: Zhaoye Fei, Yunfan Shao, Linyang Li, Zhiyuan Zeng, Hang Yan, Xipeng
  Qiu and Dahua Lin
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.14624 ,  8065kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16240
replaced with revised version Fri, 16 Feb 2024 12:42:28 GMT   (8575kb,D)

Title: Combining Hierachical VAEs with LLMs for clinically meaningful timeline
  summarisation in social media
Authors: Jiayu Song, Jenny Chim, Adam Tsakalidis, Julia Ive, Dana Atzil-Slonim,
  Maria Liakata
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.16240 ,  8575kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17167
replaced with revised version Fri, 16 Feb 2024 15:55:34 GMT   (3628kb,D)

Title: Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool
  Utilization in Real-World Complex Scenarios
Authors: Shijue Huang, Wanjun Zhong, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen
  Liu, Yutai Hou, Xingshan Zeng, Yasheng Wang, Lifeng Shang, Xin Jiang, Ruifeng
  Xu, Qun Liu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.17167 ,  3628kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17882
replaced with revised version Fri, 16 Feb 2024 09:47:38 GMT   (1042kb,D)

Title: I Think, Therefore I am: Benchmarking Awareness of Large Language Models
  Using AwareBench
Authors: Yuan Li, Yue Huang, Yuli Lin, Siyuan Wu, Yao Wan and Lichao Sun
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.17882 ,  1042kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01679
replaced with revised version Fri, 16 Feb 2024 11:27:14 GMT   (12069kb,D)

Title: STICKERCONV: Generating Multimodal Empathetic Responses from Scratch
Authors: Yiqun Zhang, Fanheng Kong, Peidong Wang, Shuang Sun, Lingshuai Wang,
  Shi Feng, Daling Wang, Yifei Zhang, Kaisong Song
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.01679 ,  12069kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01681
replaced with revised version Fri, 16 Feb 2024 07:12:34 GMT   (19133kb,D)

Title: Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social
  Media Communications
Authors: Yuhang Zhou, Paiheng Xu, Xiyao Wang, Xuan Lu, Ge Gao, Wei Ai
Categories: cs.CL cs.AI
Comments: 12 pages, 2 page appendix
\\ ( https://arxiv.org/abs/2402.01681 ,  19133kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03190
replaced with revised version Fri, 16 Feb 2024 15:40:31 GMT   (6239kb,D)

Title: Unified Hallucination Detection for Multimodal Large Language Models
Authors: Xiang Chen and Chenxi Wang and Yida Xue and Ningyu Zhang and Xiaoyan
  Yang and Qiang Li and Yue Shen and Lei Liang and Jinjie Gu and Huajun Chen
Categories: cs.CL cs.AI cs.IR cs.LG cs.MM
Comments: Work in progress
\\ ( https://arxiv.org/abs/2402.03190 ,  6239kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06733
replaced with revised version Fri, 16 Feb 2024 12:08:38 GMT   (7162kb,D)

Title: NICE: To Optimize In-Context Examples or Not?
Authors: Pragya Srivastava, Satvik Golechha, Amit Deshpande, Amit Sharma
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.06733 ,  7162kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06900
replaced with revised version Fri, 16 Feb 2024 12:01:33 GMT   (2732kb,D)

Title: Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework
  and Semantic-Based Metric
Authors: Hyukhun Koh, Dohyung Kim, Minwoo Lee, and Kyomin Jung
Categories: cs.CL cs.AI
Comments: 8 page long
\\ ( https://arxiv.org/abs/2402.06900 ,  2732kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07214
replaced with revised version Fri, 16 Feb 2024 17:29:40 GMT   (1759kb,D)

Title: Through the Lens of Split Vote: Exploring Disagreement, Difficulty and
  Calibration in Legal Case Outcome Classification
Authors: Shanshan Xu, T.Y.S.S Santosh, Oana Ichim, Barbara Plank, Matthias
  Grabmair
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.07214 ,  1759kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07470
replaced with revised version Fri, 16 Feb 2024 15:59:27 GMT   (1930kb,D)

Title: Pushing The Limit of LLM Capacity for Text Classification
Authors: Yazhou Zhang, Mengyao Wang, Chenyu Ren, Qiuchi Li, Prayag Tiwari,
  Benyou Wang, Jing Qin
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.07470 ,  1930kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07616
replaced with revised version Fri, 16 Feb 2024 16:58:04 GMT   (7782kb,D)

Title: Anchor-based Large Language Models
Authors: Jianhui Pang, Fanghua Ye, Derek F. Wong, Longyue Wang
Categories: cs.CL cs.AI
Comments: 16 pages. Work was done when Jianhui Pang and Fanghua Ye were
  interning at Tencent AI Lab. Longyue Wang is the corresponding author
\\ ( https://arxiv.org/abs/2402.07616 ,  7782kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08277
replaced with revised version Fri, 16 Feb 2024 11:49:21 GMT   (8720kb,D)

Title: Towards Faithful and Robust LLM Specialists for Evidence-Based
  Question-Answering
Authors: Tobias Schimanski, Jingwei Ni, Mathias Kraus, Elliott Ash, Markus
  Leippold
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.08277 ,  8720kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08479
replaced with revised version Fri, 16 Feb 2024 09:57:28 GMT   (854kb,D)

Title: Plausible Extractive Rationalization through Semi-Supervised Entailment
  Signal
Authors: Yeo Wei Jie, Ranjan Satapathy, Erik Cambria
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.08479 ,  854kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09615
replaced with revised version Fri, 16 Feb 2024 13:58:38 GMT   (1561kb,D)

Title: API Pack: A Massive Multilingual Dataset for API Call Generation
Authors: Zhen Guo, Adriana Meza Soria, Wei Sun, Yikang Shen, Rameswar Panda
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.09615 ,  1561kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09696
replaced with revised version Fri, 16 Feb 2024 02:19:49 GMT   (1847kb,D)

Title: An Analysis of Language Frequency and Error Correction for Esperanto
Authors: Junhong Liang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.09696 ,  1847kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10137
replaced with revised version Fri, 16 Feb 2024 10:57:32 GMT   (8193kb,D)

Title: TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles
Authors: Yinhong Liu, Yimai Fang, David Vandyke and Nigel Collier
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.10137 ,  8193kb)
------------------------------------------------------------------------------
\\
arXiv:2203.11242
replaced with revised version Fri, 16 Feb 2024 12:48:21 GMT   (3850kb,D)

Title: A survey on GANs for computer vision: Recent research, analysis and
  taxonomy
Authors: Guillermo Iglesias, Edgar Talavera and Alberto D\'iaz-\'Alvarez
Categories: cs.LG cs.AI cs.CV
Comments: 77 pages, 11 figures, 4 tables
Journal-ref: Computer Science Review, 48, 100553 (2023)
DOI: 10.1016/j.cosrev.2023.100553
\\ ( https://arxiv.org/abs/2203.11242 ,  3850kb)
------------------------------------------------------------------------------
\\
arXiv:2203.13423
replaced with revised version Thu, 15 Feb 2024 22:01:00 GMT   (258kb)

Title: Modeling Attrition in Recommender Systems with Departing Bandits
Authors: Omer Ben-Porat, Lee Cohen, Liu Leqi, Zachary C. Lipton, Yishay Mansour
Categories: cs.LG cs.IR stat.ML
Comments: Accepted at AAAI 2022
\\ ( https://arxiv.org/abs/2203.13423 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2206.13508
replaced with revised version Fri, 16 Feb 2024 12:30:20 GMT   (2081kb,D)

Title: Data Augmentation techniques in time series domain: A survey and
  taxonomy
Authors: Guillermo Iglesias, Edgar Talavera, \'Angel Gonz\'alez-Prieto, Alberto
  Mozo and Sandra G\'omez-Canaval
Categories: cs.LG cs.AI
Comments: 33 pages, 9 figures
ACM-class: I.2.6
Journal-ref: Neural Computing and Applications, 35(14), 10123-10145 (2023)
DOI: 10.1007/s00521-023-08459-3
\\ ( https://arxiv.org/abs/2206.13508 ,  2081kb)
------------------------------------------------------------------------------
\\
arXiv:2207.04771
replaced with revised version Fri, 16 Feb 2024 13:48:47 GMT   (578kb,D)

Title: Functional Generalized Empirical Likelihood Estimation for Conditional
  Moment Restrictions
Authors: Heiner Kremer, Jia-Jie Zhu, Krikamol Muandet, Bernhard Sch\"olkopf
Categories: cs.LG math.ST stat.ML stat.TH
\\ ( https://arxiv.org/abs/2207.04771 ,  578kb)
------------------------------------------------------------------------------
\\
arXiv:2207.09031
replaced with revised version Fri, 16 Feb 2024 17:12:46 GMT   (2092kb,D)

Title: Decorrelative Network Architecture for Robust Electrocardiogram
  Classification
Authors: Christopher Wiedeman and Ge Wang
Categories: cs.LG cs.AI
Comments: 24 pages, 7 figures
\\ ( https://arxiv.org/abs/2207.09031 ,  2092kb)
------------------------------------------------------------------------------
\\
arXiv:2212.00322
replaced with revised version Fri, 16 Feb 2024 03:17:40 GMT   (833kb,D)

Title: Hijack Vertical Federated Learning Models As One Party
Authors: Pengyu Qiu, Xuhong Zhang, Shouling Ji, Changjiang Li, Yuwen Pu, Xing
  Yang, Ting Wang
Categories: cs.LG cs.AI cs.CR
Comments: https://doi.ieeecomputersociety.org/10.1109/TDSC.2024.3358081
DOI: 10.1109/TDSC.2024.3358081
\\ ( https://arxiv.org/abs/2212.00322 ,  833kb)
------------------------------------------------------------------------------
\\
arXiv:2212.03733
replaced with revised version Fri, 16 Feb 2024 01:41:07 GMT   (1931kb,D)

Title: Tiered Reward Functions: Specifying and Fast Learning of Desired
  Behavior
Authors: Zhiyuan Zhou, Shreyas Sundara Raman, Henry Sowerby, Michael L. Littman
Categories: cs.LG cs.AI
Comments: For code, see https://github.com/zhouzypaul/tiered-reward
\\ ( https://arxiv.org/abs/2212.03733 ,  1931kb)
------------------------------------------------------------------------------
\\
arXiv:2302.07930
replaced with revised version Fri, 16 Feb 2024 02:02:30 GMT   (14527kb,D)

Title: Interpretable Deep Learning Methods for Multiview Learning
Authors: Hengkang Wang, Han Lu, Ju Sun, Sandra E Safo
Categories: cs.LG stat.ME stat.ML
Comments: Published in BMC Bioinformatics
  (https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-024-05679-9)
Journal-ref: BMC Bioinformatics 25, 69 (2024)
DOI: 10.1186/s12859-024-05679-9
\\ ( https://arxiv.org/abs/2302.07930 ,  14527kb)
------------------------------------------------------------------------------
\\
arXiv:2303.07154
replaced with revised version Fri, 16 Feb 2024 00:24:32 GMT   (8986kb,D)

Title: Differential Good Arm Identification
Authors: Yun-Da Tsai, Tzu-Hsien Tsai, Shou-De Lin
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2303.07154 ,  8986kb)
------------------------------------------------------------------------------
\\
arXiv:2303.08431
replaced with revised version Fri, 16 Feb 2024 08:32:23 GMT   (67kb,D)

Title: Policy Gradient Converges to the Globally Optimal Policy for Nearly
  Linear-Quadratic Regulators
Authors: Yinbin Han, Meisam Razaviyayn and Renyuan Xu
Categories: cs.LG math.OC stat.ML
Comments: 34 pages
\\ ( https://arxiv.org/abs/2303.08431 ,  67kb)
------------------------------------------------------------------------------
\\
arXiv:2303.15103
replaced with revised version Fri, 16 Feb 2024 08:08:17 GMT   (1699kb,D)

Title: Contrastive Learning Is Spectral Clustering On Similarity Graph
Authors: Yifan Zhang, Zhiquan Tan, Jingqin Yang, Yang Yuan
Categories: cs.LG cs.AI cs.CV
Comments: ICLR 2024; We express our gratitude to the anonymous reviewers for
  their valuable feedback
\\ ( https://arxiv.org/abs/2303.15103 ,  1699kb)
------------------------------------------------------------------------------
\\
arXiv:2304.05099
replaced with revised version Fri, 16 Feb 2024 08:47:31 GMT   (1525kb,D)

Title: Feudal Graph Reinforcement Learning
Authors: Tommaso Marzi, Arshjot Khehra, Andrea Cini, Cesare Alippi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2304.05099 ,  1525kb)
------------------------------------------------------------------------------
\\
arXiv:2305.03515
replaced with revised version Fri, 16 Feb 2024 12:41:24 GMT   (262kb,D)

Title: GradTree: Learning Axis-Aligned Decision Trees with Gradient Descent
Authors: Sascha Marton and Stefan L\"udtke and Christian Bartelt and Heiner
  Stuckenschmidt
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2305.03515 ,  262kb)
------------------------------------------------------------------------------
\\
arXiv:2305.10544
replaced with revised version Fri, 16 Feb 2024 10:58:18 GMT   (161kb,D)

Title: Tractable Probabilistic Graph Representation Learning with Graph-Induced
  Sum-Product Networks
Authors: Federico Errica, Mathias Niepert
Categories: cs.LG cs.AI
Comments: The 12th International Conference on Learning Representations (ICLR
  2024)
\\ ( https://arxiv.org/abs/2305.10544 ,  161kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12095
replaced with revised version Fri, 16 Feb 2024 02:23:12 GMT   (25739kb,D)

Title: CARD: Channel Aligned Robust Blend Transformer for Time Series
  Forecasting
Authors: Wang Xue, Tian Zhou, Qingsong Wen, Jinyang Gao, Bolin Ding, Rong Jin
Categories: cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2305.12095 ,  25739kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13503
replaced with revised version Thu, 15 Feb 2024 23:04:03 GMT   (1114kb,D)

Title: Asynchronous Multi-Model Dynamic Federated Learning over Wireless
  Networks: Theory, Modeling, and Optimization
Authors: Zhan-Lun Chang, Seyyedali Hosseinalipour, Mung Chiang, Christopher G.
  Brinton
Categories: cs.LG cs.DC
Comments: Completed the major revision for IEEE Transactions on Cognitive
  Communications and Networking
\\ ( https://arxiv.org/abs/2305.13503 ,  1114kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17342
replaced with revised version Fri, 16 Feb 2024 03:24:34 GMT   (1917kb,D)

Title: Rethinking Adversarial Policies: A Generalized Attack Formulation and
  Provable Defense in RL
Authors: Xiangyu Liu, Souradip Chakraborty, Yanchao Sun, Furong Huang
Categories: cs.LG cs.AI
Comments: International Conference on Learning Representations (ICLR) 2024,
  spotlight
\\ ( https://arxiv.org/abs/2305.17342 ,  1917kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03364
replaced with revised version Fri, 16 Feb 2024 17:08:51 GMT   (2306kb,D)

Title: Learning Representations on the Unit Sphere: Investigating Angular
  Gaussian and von Mises-Fisher Distributions for Online Continual Learning
Authors: Nicolas Michel, Giovanni Chierchia, Romain Negrel, Jean-Fran\c{c}ois
  Bercher
Categories: cs.LG cs.CV
Comments: Fix some typo. Accepted to AAAI24
\\ ( https://arxiv.org/abs/2306.03364 ,  2306kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02973
replaced with revised version Fri, 16 Feb 2024 09:52:58 GMT   (2015kb,D)

Title: Pruning vs Quantization: Which is Better?
Authors: Andrey Kuzmin, Markus Nagel, Mart van Baalen, Arash Behboodi, Tijmen
  Blankevoort
Categories: cs.LG
\\ ( https://arxiv.org/abs/2307.02973 ,  2015kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07881
replaced with revised version Fri, 16 Feb 2024 12:58:10 GMT   (333kb)

Title: Graph Embedded Intuitionistic Fuzzy Random Vector Functional Link Neural
  Network for Class Imbalance Learning
Authors: M.A. Ganaie, M. Sajid, A.K. Malik, M. Tanveer
Categories: cs.LG
Comments: IEEE Transactions on Neural Networks and Learning Systems
DOI: 10.1109/TNNLS.2024.3353531
\\ ( https://arxiv.org/abs/2307.07881 ,  333kb)
------------------------------------------------------------------------------
\\
arXiv:2307.11655
replaced with revised version Fri, 16 Feb 2024 15:03:05 GMT   (344kb,D)

Title: Preferences Evolve And So Should Your Bandits: Bandits with Evolving
  States for Online Platforms
Authors: Khashayar Khosravi, Renato Paes Leme, Chara Podimata, and Apostolis
  Tsorvantzis
Categories: cs.LG cs.AI cs.GT
\\ ( https://arxiv.org/abs/2307.11655 ,  344kb)
------------------------------------------------------------------------------
\\
arXiv:2307.14754
replaced with revised version Fri, 16 Feb 2024 01:42:48 GMT   (9201kb,D)

Title: Fair Machine Unlearning: Data Removal while Mitigating Disparities
Authors: Alex Oesterling, Jiaqi Ma, Flavio P. Calmon, Hima Lakkaraju
Categories: cs.LG cs.AI
Comments: 25 pages, 3 figures, accepted to AISTATS 2024. Code is provided at
  https://github.com/AI4LIFE-GROUP/fair-unlearning
\\ ( https://arxiv.org/abs/2307.14754 ,  9201kb)
------------------------------------------------------------------------------
\\
arXiv:2307.15007
replaced with revised version Thu, 15 Feb 2024 20:10:47 GMT   (1419kb,D)

Title: Discriminative Feature Attributions: Bridging Post Hoc Explainability
  and Inherent Interpretability
Authors: Usha Bhalla, Suraj Srinivas, Himabindu Lakkaraju
Categories: cs.LG cs.CV
Journal-ref: NeurIPS 2023 (Thirty-seventh Conference on Neural Information
  Processing Systems)
\\ ( https://arxiv.org/abs/2307.15007 ,  1419kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12563
replaced with revised version Fri, 16 Feb 2024 17:39:53 GMT   (3868kb,D)

Title: Multivariate Time-Series Anomaly Detection with Contaminated Data
Authors: Thi Kieu Khanh Ho and Narges Armanfard
Categories: cs.LG eess.SP
Comments: 9 pages, 4 tables, 4 figures
\\ ( https://arxiv.org/abs/2308.12563 ,  3868kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04015
replaced with revised version Fri, 16 Feb 2024 16:35:29 GMT   (2968kb,D)

Title: Optimal Transport with Tempered Exponential Measures
Authors: Ehsan Amid, Frank Nielsen, Richard Nock, and Manfred K. Warmuth
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2309.04015 ,  2968kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00429
replaced with revised version Thu, 15 Feb 2024 21:58:41 GMT   (4600kb,D)

Title: On the Stability of Iterative Retraining of Generative Models on their
  own Data
Authors: Quentin Bertrand, Avishek Joey Bose, Alexandre Duplessis, Marco
  Jiralerspong, and Gauthier Gidel
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.00429 ,  4600kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01174
replaced with revised version Fri, 16 Feb 2024 13:39:36 GMT   (36210kb,D)

Title: Light Schr\"odinger Bridge
Authors: Alexander Korotin, Nikita Gushchin, Evgeny Burnaev
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.01174 ,  36210kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01195
replaced with revised version Fri, 16 Feb 2024 14:02:02 GMT   (1417kb)

Title: Federated K-means Clustering
Authors: Swier Garst and Marcel Reinders
Categories: cs.LG cs.DC
\\ ( https://arxiv.org/abs/2310.01195 ,  1417kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02473
replaced with revised version Thu, 15 Feb 2024 19:22:06 GMT   (5399kb,D)

Title: Prompting-based Temporal Domain Generalization
Authors: Sepidehsadat Hosseini, Mengyao Zhai, Hossein Hajimirsadegh, Frederick
  Tung
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.02473 ,  5399kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04741
replaced with revised version Fri, 16 Feb 2024 13:30:44 GMT   (4026kb,D)

Title: Keep Moving: identifying task-relevant subspaces to maximise plasticity
  for newly learned tasks
Authors: Daniel Anthes and Sushrut Thorat and Peter K\"onig and Tim C.
  Kietzmann
Categories: cs.LG cs.CV q-bio.NC
Comments: 15 pages, 6 figures, Substantial Revision
\\ ( https://arxiv.org/abs/2310.04741 ,  4026kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06549
replaced with revised version Fri, 16 Feb 2024 10:10:19 GMT   (6825kb,D)

Title: Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield
  but Also a Catalyst for Model Inversion Attacks
Authors: Lukas Struppek, Dominik Hintersdorf, Kristian Kersting
Categories: cs.LG cs.CR cs.CV
Comments: Published as a conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2310.06549 ,  6825kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08644
replaced with revised version Fri, 16 Feb 2024 18:02:33 GMT   (4569kb)

Title: A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of
  Geoscientific Systems
Authors: Yuan-Heng Wang, Hoshin V. Gupta
Categories: cs.LG cs.AI
Comments: 65 pages, 7 figures in the main text, 10 figures, and 10 tables in
  the supplementary materials
\\ ( https://arxiv.org/abs/2310.08644 ,  4569kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09789
replaced with revised version Fri, 16 Feb 2024 04:40:17 GMT   (1363kb,D)

Title: FLrce: Resource-Efficient Federated Learning with Early-Stopping
  Strategy
Authors: Ziru Niu, Hai Dong, A. Kai Qin, Tao Gu
Categories: cs.LG
Comments: arxiv preprint
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2310.09789 ,  1363kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18884
replaced with revised version Fri, 16 Feb 2024 17:08:12 GMT   (3120kb,D)

Title: Simple and Asymmetric Graph Contrastive Learning without Augmentations
Authors: Teng Xiao, Huaisheng Zhu, Zhengyu Chen, Suhang Wang
Categories: cs.LG stat.ML
Comments: NeurIPS 2023 Main Track
\\ ( https://arxiv.org/abs/2310.18884 ,  3120kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08936
replaced with revised version Fri, 16 Feb 2024 15:47:15 GMT   (3750kb,D)

Title: Confident Naturalness Explanation (CNE): A Framework to Explain and
  Assess Patterns Forming Naturalness
Authors: Ahmed Emam, Mohamed Farag, Ribana Roscher
Categories: cs.LG cs.CV
Journal-ref: IEEE Geoscience and Remote Sensing Letters, 2024
DOI: 10.1109/LGRS.2024.3365196
\\ ( https://arxiv.org/abs/2311.08936 ,  3750kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18048
replaced with revised version Fri, 16 Feb 2024 10:36:55 GMT   (144kb,D)

Title: An Interventional Perspective on Identifiability in Gaussian LTI Systems
  with Independent Component Analysis
Authors: Goutham Rajendran, Patrik Reizinger, Wieland Brendel, Pradeep
  Ravikumar
Categories: cs.LG cs.CE cs.SY eess.SY stat.ME
Comments: CLeaR2024 camera ready. Code available at
  https://github.com/rpatrik96/lti-ica
\\ ( https://arxiv.org/abs/2311.18048 ,  144kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02867
replaced with revised version Fri, 16 Feb 2024 15:52:27 GMT   (1215kb,D)

Title: Semi-Supervised Health Index Monitoring with Feature Generation and
  Fusion
Authors: Ga\"etan Frusque, Ismail Nejjar, Majid Nabavi, Olga Fink
Categories: cs.LG stat.ME
Comments: 13 pages, 8 figures
\\ ( https://arxiv.org/abs/2312.02867 ,  1215kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06528
replaced with revised version Thu, 15 Feb 2024 22:45:28 GMT   (209kb,D)

Title: Transformers Implement Functional Gradient Descent to Learn Non-Linear
  Functions In Context
Authors: Xiang Cheng, Yuxin Chen, Suvrit Sra
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.06528 ,  209kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06635
replaced with revised version Thu, 15 Feb 2024 23:30:24 GMT   (65kb)

Title: Gated Linear Attention Transformers with Hardware-Efficient Training
Authors: Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim
Categories: cs.LG cs.CL
Comments: major update
\\ ( https://arxiv.org/abs/2312.06635 ,  65kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11462
replaced with revised version Fri, 16 Feb 2024 05:18:57 GMT   (203kb,D)

Title: Cascade Speculative Drafting for Even Faster LLM Inference
Authors: Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Jie Huang, Kevin
  Chen-Chuan Chang
Categories: cs.LG cs.CL
Comments: Preprint in progress
\\ ( https://arxiv.org/abs/2312.11462 ,  203kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14886
replaced with revised version Fri, 16 Feb 2024 15:17:57 GMT   (3146kb,D)

Title: Sample Path Regularity of Gaussian Processes from the Covariance Kernel
Authors: Natha\"el Da Costa, Marvin Pf\"ortner, Lancelot Da Costa, Philipp
  Hennig
Categories: cs.LG math.PR math.ST stat.ML stat.TH
\\ ( https://arxiv.org/abs/2312.14886 ,  3146kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00685
replaced with revised version Fri, 16 Feb 2024 09:21:29 GMT   (37585kb,D)

Title: Communication-Efficient Federated Learning for LEO Satellite Networks
  Integrated with HAPs Using Hybrid NOMA-OFDM
Authors: Mohamed Elmahallawy, Tie Luo, Khaled Ramadan
Categories: cs.LG cs.AI cs.DC
\\ ( https://arxiv.org/abs/2401.00685 ,  37585kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05146
replaced with revised version Fri, 16 Feb 2024 15:34:55 GMT   (9588kb,D)

Title: Federated Unlearning: A Survey on Methods, Design Guidelines, and
  Evaluation Metrics
Authors: Nicol\`o Romandini, Alessio Mora, Carlo Mazzocca, Rebecca Montanari,
  Paolo Bellavista
Categories: cs.LG cs.CR
Comments: 23 pages, 8 figures, and 6 tables
\\ ( https://arxiv.org/abs/2401.05146 ,  9588kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06469
replaced with revised version Fri, 16 Feb 2024 10:58:52 GMT   (672kb,D)

Title: Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning
Authors: Kaiyi Zhang, Ang Lv, Yuhan Chen, Hansen Ha, Tao Xu, Rui Yan
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2401.06469 ,  672kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08893
replaced with revised version Fri, 16 Feb 2024 02:40:47 GMT   (4516kb,D)

Title: MADA: Meta-Adaptive Optimizers through hyper-gradient Descent
Authors: Kaan Ozkara, Can Karakus, Parameswaran Raman, Mingyi Hong, Shoham
  Sabach, Branislav Kveton, Volkan Cevher
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2401.08893 ,  4516kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08961
replaced with revised version Thu, 15 Feb 2024 22:21:17 GMT   (1345kb,D)

Title: Cascading Reinforcement Learning
Authors: Yihan Du, R. Srikant, Wei Chen
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.08961 ,  1345kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13200
replaced with revised version Fri, 16 Feb 2024 04:59:28 GMT   (14597kb,D)

Title: Topology-aware Embedding Memory for Continual Learning on Expanding
  Networks
Authors: Xikun Zhang, Dongjin Song, Yixin Chen, Dacheng Tao
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.13200 ,  14597kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01103
replaced with revised version Fri, 16 Feb 2024 17:11:52 GMT   (10353kb,D)

Title: Compositional Generative Modeling: A Single Model is Not All You Need
Authors: Yilun Du, Leslie Kaelbling
Categories: cs.LG cs.AI cs.CV cs.RO
\\ ( https://arxiv.org/abs/2402.01103 ,  10353kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04298
replaced with revised version Fri, 16 Feb 2024 13:50:39 GMT   (291kb,D)

Title: Multi-View Symbolic Regression
Authors: Etienne Russeil, Fabr\'icio Olivetti de Fran\c{c}a, Konstantin
  Malanchev, Bogdan Burlacu, Emille E. O. Ishida, Marion Leroux, Cl\'ement
  Michelin, Guillaume Moinard, Emmanuel Gangler
Categories: cs.LG astro-ph.IM stat.AP
Comments: Submitted to GECCO-2024. 10 pages, 6 figures
\\ ( https://arxiv.org/abs/2402.04298 ,  291kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05973
replaced with revised version Thu, 15 Feb 2024 23:42:40 GMT   (1614kb,D)

Title: Blockchain-enabled Clustered and Scalable Federated Learning (BCS-FL)
  Framework in UAV Networks
Authors: Sana Hafeez, Lina Mohjazi, Muhammad Ali Imran and Yao Sun
Categories: cs.LG cs.NI eess.SP
Comments: 6 pages, 7 figures, 2023 IEEE International Workshop on Computer
  Aided Modeling and Design of Communication Links and Networks (IEEE CAMAD),
  Edinburgh UK
\\ ( https://arxiv.org/abs/2402.05973 ,  1614kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08156
replaced with revised version Thu, 15 Feb 2024 21:50:57 GMT   (149kb)

Title: Group Decision-Making among Privacy-Aware Agents
Authors: Marios Papachristou, M. Amin Rahimian
Categories: cs.LG cs.AI cs.CR cs.MA stat.ML
\\ ( https://arxiv.org/abs/2402.08156 ,  149kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08225
replaced with revised version Thu, 15 Feb 2024 21:18:46 GMT   (1210kb,D)

Title: Improving Black-box Robustness with In-Context Rewriting
Authors: Kyle O'Brien, Nathan Ng, Isha Puri, Jorge Mendez, Hamid Palangi, Yoon
  Kim, Marzyeh Ghassemi, Thomas Hartvigsen
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.08225 ,  1210kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08712
replaced with revised version Thu, 15 Feb 2024 19:24:53 GMT   (7880kb,D)

Title: BECoTTA: Input-dependent Online Blending of Experts for Continual
  Test-time Adaptation
Authors: Daeun Lee, Jaehong Yoon, Sung Ju Hwang
Categories: cs.LG cs.CV
Comments: 17 pages, Preprint, Project page: https://becotta-ctta.github.io/
\\ ( https://arxiv.org/abs/2402.08712 ,  7880kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08787
replaced with revised version Thu, 15 Feb 2024 21:14:18 GMT   (356kb,D)

Title: Rethinking Machine Unlearning for Large Language Models
Authors: Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie
  Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R. Varshney,
  Mohit Bansal, Sanmi Koyejo, Yang Liu
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2402.08787 ,  356kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09345
replaced with revised version Fri, 16 Feb 2024 07:48:27 GMT   (41044kb,D)

Title: Mitigating Reward Hacking via Information-Theoretic Reward Modeling
Authors: Yuchun Miao, Sen Zhang, Liang Ding, Rong Bao, Lefei Zhang, Dacheng Tao
Categories: cs.LG cs.AI
Comments: 26 pages, 28 figures
\\ ( https://arxiv.org/abs/2402.09345 ,  41044kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09492
replaced with revised version Fri, 16 Feb 2024 14:01:44 GMT   (3063kb,D)

Title: PMGDA: A Preference-based Multiple Gradient Descent Algorithm
Authors: Xiaoyuan Zhang and Xi Lin and Qingfu Zhang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.09492 ,  3063kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09631
replaced with revised version Fri, 16 Feb 2024 12:22:04 GMT   (5717kb,D)

Title: MiMiC: Minimally Modified Counterfactuals in the Representation Space
Authors: Shashwat Singh, Shauli Ravfogel, Jonathan Herzig, Roee Aharoni, Ryan
  Cotterell, Ponnurangam Kumaraguru
Categories: cs.LG cs.CL cs.CY
Comments: Preprint
\\ ( https://arxiv.org/abs/2402.09631 ,  5717kb)
------------------------------------------------------------------------------
\\
arXiv:2202.06095
replaced with revised version Thu, 15 Feb 2024 20:36:54 GMT   (5366kb,D)

Title: A Review of Deep Learning-based Approaches for Deepfake Content
  Detection
Authors: Leandro A. Passos, Danilo Jodas, Kelton A. P. da Costa, Luis A. Souza
  J\'unior, Douglas Rodrigues, Javier Del Ser, David Camacho, Jo\~ao Paulo Papa
Categories: cs.CV cs.AI
DOI: 10.1111/EXSY.13570
\\ ( https://arxiv.org/abs/2202.06095 ,  5366kb)
------------------------------------------------------------------------------
\\
arXiv:2211.03295
replaced with revised version Fri, 16 Feb 2024 14:17:23 GMT   (9954kb,D)

Title: MogaNet: Multi-order Gated Aggregation Network
Authors: Siyuan Li, Zedong Wang, Zicheng Liu, Cheng Tan, Haitao Lin, Di Wu,
  Zhiyuan Chen, Jiangbin Zheng, Stan Z. Li
Categories: cs.CV cs.AI
Comments: ICLR 2024 Camera Ready. Preprint V3 (19 pages + 16 pages).
  Implementations refer to https://github.com/Westlake-AI/MogaNet
\\ ( https://arxiv.org/abs/2211.03295 ,  9954kb)
------------------------------------------------------------------------------
\\
arXiv:2301.04195
replaced with revised version Fri, 16 Feb 2024 13:45:11 GMT   (13661kb,D)

Title: Orbit: A Unified Simulation Framework for Interactive Robot Learning
  Environments
Authors: Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, Nikita Rudin, David
  Hoeller, Jia Lin Yuan, Ritvik Singh, Yunrong Guo, Hammad Mazhar, Ajay
  Mandlekar, Buck Babich, Gavriel State, Marco Hutter, Animesh Garg
Categories: cs.RO cs.AI
Comments: Project website: https://isaac-orbit.github.io/
Journal-ref: IEEE Robotics and Automation Letters (Volume: 8, Issue: 6, June
  2023)
DOI: 10.1109/LRA.2023.3270034
\\ ( https://arxiv.org/abs/2301.04195 ,  13661kb)
------------------------------------------------------------------------------
\\
arXiv:2302.03038 (*cross-listing*)
replaced with revised version Fri, 16 Feb 2024 17:42:38 GMT   (4670kb,D)

Title: Single Cells Are Spatial Tokens: Transformers for Spatial Transcriptomic
  Data Imputation
Authors: Hongzhi Wen, Wenzhuo Tang, Wei Jin, Jiayuan Ding, Renming Liu, Xinnan
  Dai, Feng Shi, Lulu Shang, Hui Liu, Yuying Xie
Categories: q-bio.GN cs.AI cs.LG
\\ ( https://arxiv.org/abs/2302.03038 ,  4670kb)
------------------------------------------------------------------------------
\\
arXiv:2305.00875
replaced with revised version Fri, 16 Feb 2024 04:21:53 GMT   (2507kb,D)

Title: Redundancy and Concept Analysis for Code-trained Language Models
Authors: Arushi Sharma, Zefu Hu, Christopher Quinn, Ali Jannesari
Categories: cs.SE cs.AI cs.LG
Comments: 4 figures, 6 tables
\\ ( https://arxiv.org/abs/2305.00875 ,  2507kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12776 (*cross-listing*)
replaced with revised version Fri, 16 Feb 2024 12:38:53 GMT   (995kb)

Title: Assessing Large Language Models' ability to predict how humans balance
  self-interest and the interest of others
Authors: Valerio Capraro, Roberto Di Paolo, Veronica Pizziol
Categories: econ.GN cs.AI cs.CY cs.GT q-fin.EC
\\ ( https://arxiv.org/abs/2307.12776 ,  995kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15237
replaced with revised version Fri, 16 Feb 2024 05:54:35 GMT   (7985kb)

Title: User Experience Design Professionals' Perceptions of Generative
  Artificial Intelligence
Authors: Jie Li, Hancheng Cao, Laura Lin, Youyang Hou, Ruihao Zhu, Abdallah El
  Ali
Categories: cs.CY cs.AI cs.ET cs.HC
Comments: accepted to CHI 2024
\\ ( https://arxiv.org/abs/2309.15237 ,  7985kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05866 (*cross-listing*)
replaced with revised version Fri, 16 Feb 2024 16:39:10 GMT   (4454kb,D)

Title: Generative quantum machine learning via denoising diffusion
  probabilistic models
Authors: Bingzhi Zhang, Peng Xu, Xiaohui Chen and Quntao Zhuang
Categories: quant-ph cs.AI cs.LG
Comments: 5+10 pages, 16 figures. PRL accepted version. Code available at:
  https://github.com/francis-hsu/quantgenmdl
\\ ( https://arxiv.org/abs/2310.05866 ,  4454kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16461
replaced with revised version Thu, 15 Feb 2024 19:16:25 GMT   (1114kb,D)

Title: Norm Enforcement with a Soft Touch: Faster Emergence, Happier Agents
Authors: Sz-Ting Tzeng, Nirav Ajmeri, Munindar P. Singh
Categories: cs.MA cs.AI cs.LG
Comments: 12 pages, 11 figures, 5 tables (and supplementary material with code
  availability and additional results), accepted at AAMAS 2024
\\ ( https://arxiv.org/abs/2401.16461 ,  1114kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06664
replaced with revised version Fri, 16 Feb 2024 04:02:51 GMT   (464kb,D)

Title: LLM Agents can Autonomously Hack Websites
Authors: Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, Daniel Kang
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2402.06664 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08812
replaced with revised version Fri, 16 Feb 2024 18:04:47 GMT   (8617kb,D)

Title: Intelligent Canvas: Enabling Design-Like Exploratory Visual Data
  Analysis with Generative AI through Rapid Prototyping, Iteration and Curation
Authors: Zijian Ding, Joel Chan
Categories: cs.HC cs.AI
\\ ( https://arxiv.org/abs/2402.08812 ,  8617kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09091
replaced with revised version Fri, 16 Feb 2024 10:24:04 GMT   (8165kb,D)

Title: Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit
  Clues
Authors: Zhiyuan Chang, Mingyang Li, Yi Liu, Junjie Wang, Qing Wang, Yang Liu
Categories: cs.CR cs.AI cs.HC
Comments: 13 pages, 6 figures
\\ ( https://arxiv.org/abs/2402.09091 ,  8165kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09604
replaced with revised version Fri, 16 Feb 2024 15:53:27 GMT   (12540kb,D)

Title: Medical Image Segmentation with InTEnt: Integrated Entropy Weighting for
  Single Image Test-Time Adaptation
Authors: Haoyu Dong and Nicholas Konz and Hanxue Gu and Maciej A. Mazurowski
Categories: cs.CV cs.AI
Comments: Code and pre-trained weights:
  https://github.com/mazurowski-lab/single-image-test-time-adaptation
\\ ( https://arxiv.org/abs/2402.09604 ,  12540kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09664
replaced with revised version Fri, 16 Feb 2024 18:35:22 GMT   (2360kb,D)

Title: CodeMind: A Framework to Challenge Large Language Models for Code
  Reasoning
Authors: Changshu Liu, Shizhuo Dylan Zhang, Reyhaneh Jabbarvand
Categories: cs.SE cs.AI cs.CL cs.PL
\\ ( https://arxiv.org/abs/2402.09664 ,  2360kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09786
replaced with revised version Fri, 16 Feb 2024 07:36:29 GMT   (28875kb,D)

Title: Examining Pathological Bias in a Generative Adversarial Network
  Discriminator: A Case Study on a StyleGAN3 Model
Authors: Alvin Grissom II, Ryan F. Lei, Jeova Farias Sales Rocha Neto, Bailey
  Lin, Ryan Trotter
Categories: cs.CV cs.AI cs.CY cs.LG
\\ ( https://arxiv.org/abs/2402.09786 ,  28875kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13330 (*cross-listing*)
replaced with revised version Fri, 16 Feb 2024 16:20:18 GMT   (1135kb,D)

Title: Unsupervised ASR via Cross-Lingual Pseudo-Labeling
Authors: Tatiana Likhomanenko, Loren Lugosch, Ronan Collobert
Categories: eess.AS cs.CL cs.LG cs.SD
\\ ( https://arxiv.org/abs/2305.13330 ,  1135kb)
------------------------------------------------------------------------------
\\
arXiv:2307.10700
replaced with revised version Thu, 15 Feb 2024 21:15:19 GMT   (7855kb,D)

Title: Topics, Authors, and Institutions in Large Language Model Research:
  Trends from 17K arXiv Papers
Authors: Rajiv Movva, Sidhika Balachandar, Kenny Peng, Gabriel Agostini, Nikhil
  Garg, Emma Pierson
Categories: cs.DL cs.CL cs.CY
Comments: Data & code available at
  https://github.com/rmovva/LLM-publication-patterns-public
\\ ( https://arxiv.org/abs/2307.10700 ,  7855kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09618 (*cross-listing*)
replaced with revised version Fri, 16 Feb 2024 16:25:48 GMT   (17724kb,D)

Title: Simulating Opinion Dynamics with Networks of LLM-based Agents
Authors: Yun-Shiuan Chuang, Agam Goyal, Nikunj Harlalka, Siddharth Suresh,
  Robert Hawkins, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers
Categories: physics.soc-ph cs.CL
\\ ( https://arxiv.org/abs/2311.09618 ,  17724kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12716
replaced with revised version Fri, 16 Feb 2024 17:33:33 GMT   (1371kb,D)

Title: BloomVQA: Assessing Hierarchical Multi-modal Comprehension
Authors: Yunye Gong, Robik Shrestha, Jared Claypoole, Michael Cogswell, Arijit
  Ray, Christopher Kanan, Ajay Divakaran
Categories: cs.CV cs.CL cs.LG
Comments: Dataset available at https://huggingface.co/datasets/ygong/BloomVQA
\\ ( https://arxiv.org/abs/2312.12716 ,  1371kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16659
replaced with revised version Fri, 16 Feb 2024 16:55:42 GMT   (274kb,D)

Title: History-Aware Conversational Dense Retrieval
Authors: Fengran Mo, Chen Qu, Kelong Mao, Tianyu Zhu, Zhan Su, Kaiyu Huang,
  Jian-Yun Nie
Categories: cs.IR cs.CL
\\ ( https://arxiv.org/abs/2401.16659 ,  274kb)
------------------------------------------------------------------------------
\\
arXiv:1906.01741 (*cross-listing*)
replaced with revised version Fri, 16 Feb 2024 12:41:29 GMT   (320kb,D)

Title: Fr\'echet random forests for metric space valued regression with non
  euclidean predictors
Authors: Louis Capitaine, J\'er\'emie Bigot, Rodolphe Thi\'ebaut and Robin
  Genuer
Categories: stat.ML cs.LG stat.ME
\\ ( https://arxiv.org/abs/1906.01741 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2206.12532 (*cross-listing*)
replaced with revised version Fri, 16 Feb 2024 12:05:26 GMT   (2335kb,D)

Title: Causal Scoring: A Framework for Effect Estimation, Effect Ordering, and
  Effect Classification
Authors: Carlos Fern\'andez-Lor\'ia and Jorge Lor\'ia
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2206.12532 ,  2335kb)
------------------------------------------------------------------------------
\\
arXiv:2211.03065
replaced with revised version Fri, 16 Feb 2024 04:37:38 GMT   (2949kb,D)

Title: Enabling Deep Learning-based Physical-layer Secret Key Generation for
  FDD-OFDM Systems in Multi-Environments
Authors: Xinwei Zhang, Guyue Li, Junqing Zhang, Linning Peng, Aiqun Hu, Xianbin
  Wang
Categories: cs.IT cs.CR cs.LG math.IT
Comments: Accepted by IEEE TVT
\\ ( https://arxiv.org/abs/2211.03065 ,  2949kb)
------------------------------------------------------------------------------
\\
arXiv:2211.11278 (*cross-listing*)
replaced with revised version Thu, 15 Feb 2024 21:13:51 GMT   (92kb)

Title: Optimal Extended Neighbourhood Rule $k$ Nearest Neighbours Ensemble
Authors: Amjad Ali, Zardad Khan, Dost Muhammad Khan, Saeed Aldahmani
Categories: stat.ML cs.LG
Comments: This manuscript has been submitted for publication in the esteemed
  journal Pattern Recognition Letters
MSC-class: 14J60
\\ ( https://arxiv.org/abs/2211.11278 ,  92kb)
------------------------------------------------------------------------------
\\
arXiv:2212.14424 (*cross-listing*)
replaced with revised version Fri, 16 Feb 2024 02:13:06 GMT   (4204kb,D)

Title: Normalizing flow neural networks by JKO scheme
Authors: Chen Xu, Xiuyuan Cheng, Yao Xie
Categories: stat.ML cs.LG
Comments: NeurIPS 2023 spotlight
\\ ( https://arxiv.org/abs/2212.14424 ,  4204kb)
------------------------------------------------------------------------------
\\
arXiv:2304.00216 (*cross-listing*)
replaced with revised version Fri, 16 Feb 2024 06:08:28 GMT   (15534kb,D)

Title: Cross-scale Multi-instance Learning for Pathological Image Diagnosis
Authors: Ruining Deng, Can Cui, Lucas W. Remedios, Shunxing Bao, R. Michael
  Womick, Sophie Chiron, Jia Li, Joseph T. Roland, Ken S. Lau, Qi Liu, Keith T.
  Wilson, Yaohong Wang, Lori A. Coburn, Bennett A. Landman, Yuankai Huo
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2304.00216 ,  15534kb)
------------------------------------------------------------------------------
\\
arXiv:2304.10640
replaced with revised version Fri, 16 Feb 2024 00:02:49 GMT   (150kb,D)

Title: On the Effects of Data Heterogeneity on the Convergence Rates of
  Distributed Linear System Solvers
Authors: Boris Velasevic, Rohit Parasnis, Christopher G. Brinton, Navid Azizan
Categories: cs.DC cs.LG cs.NA math.NA
Comments: 11 pages, 5 figures
ACM-class: G.1.3; I.2.11; I.2.6
\\ ( https://arxiv.org/abs/2304.10640 ,  150kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01794 (*cross-listing*)
replaced with revised version Fri, 16 Feb 2024 03:29:18 GMT   (27120kb,D)

Title: DiffPack: A Torsional Diffusion Model for Autoregressive Protein
  Side-Chain Packing
Authors: Yangtian Zhang, Zuobai Zhang, Bozitao Zhong, Sanchit Misra, Jian Tang
Categories: q-bio.QM cs.LG
Comments: 37th Conference on Neural Information Processing Systems (NeurIPS
  2023)
\\ ( https://arxiv.org/abs/2306.01794 ,  27120kb)
------------------------------------------------------------------------------
\\
arXiv:2306.15585 (*cross-listing*)
replaced with revised version Fri, 16 Feb 2024 16:12:21 GMT   (540kb)

Title: Optimizing Credit Limit Adjustments Under Adversarial Goals Using
  Reinforcement Learning
Authors: Sherly Alfonso-S\'anchez, Jes\'us Solano, Alejandro Correa-Bahnsen,
  Kristina P. Sendova, and Cristi\'an Bravo
Categories: q-fin.GN cs.LG
Comments: 29 pages, 16 figures
Journal-ref: Alfonso-Sanchez, S., Solano, J., Correa-Bahnsen, A., Sendova, K.
  P., & Bravo, C. (2024). Optimizing credit limit adjustments under adversarial
  goals using reinforcement learning. European Journal of Operational Research
  315(2): 802-817
DOI: 10.1016/j.ejor.2023.12.025
\\ ( https://arxiv.org/abs/2306.15585 ,  540kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11713 (*cross-listing*)
replaced with revised version Fri, 16 Feb 2024 06:55:58 GMT   (12429kb,D)

Title: Quasi-Monte Carlo for 3D Sliced Wasserstein
Authors: Khai Nguyen and Nicola Bariletto and Nhat Ho
Categories: stat.ML cs.GR cs.LG
Comments: Accepted to ICLR 2024 (Spotlight), 25 pages, 13 figures, 6 tables
\\ ( https://arxiv.org/abs/2309.11713 ,  12429kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15126 (*cross-listing*)
replaced with revised version Fri, 16 Feb 2024 15:54:15 GMT   (8849kb,D)

Title: From Peptides to Nanostructures: A Euclidean Transformer for Fast and
  Stable Machine Learned Force Fields
Authors: J. Thorben Frank, Oliver T. Unke, Klaus-Robert M\"uller, Stefan
  Chmiela
Categories: physics.chem-ph cs.LG
\\ ( https://arxiv.org/abs/2309.15126 ,  8849kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16672
replaced with revised version Thu, 15 Feb 2024 19:00:07 GMT   (8126kb,D)

Title: Learning to Transform for Generalizable Instance-wise Invariance
Authors: Utkarsh Singhal and Carlos Esteves and Ameesh Makadia and Stella X. Yu
Categories: cs.CV cs.LG
Comments: Accepted to ICCV 2023
\\ ( https://arxiv.org/abs/2309.16672 ,  8126kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15767 (*cross-listing*)
replaced with revised version Fri, 16 Feb 2024 12:59:07 GMT   (2908kb)

Title: Unpaired MRI Super Resolution with Contrastive Learning
Authors: Hao Li, Quanwei Liu, Jianan Liu, Xiling Liu, Yanni Dong, Tao Huang,
  Zhihan Lv
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2310.15767 ,  2908kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10162 (*cross-listing*)
replaced with revised version Fri, 16 Feb 2024 18:08:30 GMT   (1206kb)

Title: K-space Cold Diffusion: Learning to Reconstruct Accelerated MRI without
  Noise
Authors: Guoyao Shen, Mengyu Li, Chad W. Farris, Stephan Anderson, Xin Zhang
Categories: eess.IV cs.CV cs.LG physics.med-ph
Comments: 22 pages, 5 figures, 3 tables
\\ ( https://arxiv.org/abs/2311.10162 ,  1206kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05780
replaced with revised version Fri, 16 Feb 2024 14:48:47 GMT   (26890kb,D)

Title: PULSAR: Graph based Positive Unlabeled Learning with Multi Stream
  Adaptive Convolutions for Parkinson's Disease Recognition
Authors: Md. Zarif Ul Alam, Md Saiful Islam, Ehsan Hoque, M Saifur Rahman
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2312.05780 ,  26890kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13839
replaced with revised version Fri, 16 Feb 2024 11:18:30 GMT   (38891kb,D)

Title: Q-SENN: Quantized Self-Explaining Neural Networks
Authors: Thomas Norrenbrock, Marco Rudolph, Bodo Rosenhahn
Categories: cs.CV cs.LG
Comments: Accepted to AAAI 2024, SRRAI
\\ ( https://arxiv.org/abs/2312.13839 ,  38891kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08903
replaced with revised version Fri, 16 Feb 2024 02:55:23 GMT   (278kb,D)

Title: PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks
  on Face Recognition Systems
Authors: Fengfan Zhou, Heifei Ling, Bangjie Yin, Hui Zheng
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2401.08903 ,  278kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00418
replaced with revised version Fri, 16 Feb 2024 08:06:42 GMT   (464kb)

Title: Benchmarking Transferable Adversarial Attacks
Authors: Zhibo Jin, Jiayu Zhang, Zhiyu Zhu, Huaming Chen
Categories: cs.CV cs.LG
Comments: Accepted by NDSS 2024 Workshop
\\ ( https://arxiv.org/abs/2402.00418 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00626
replaced with revised version Fri, 16 Feb 2024 15:15:38 GMT   (3178kb,D)

Title: Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks
Authors: Maan Qraitem, Nazia Tasnim, Piotr Teterwak, Kate Saenko, Bryan A.
  Plummer
Categories: cs.CV cs.CR cs.LG
\\ ( https://arxiv.org/abs/2402.00626 ,  3178kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04146 (*cross-listing*)
replaced with revised version Fri, 16 Feb 2024 18:17:15 GMT   (9716kb,D)

Title: Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian
  Process
Authors: Sandipp Krishnan Ravi, Yigitcan Comlek, Wei Chen, Arjun Pathak, Vipul
  Gupta, Rajnikant Umretiya, Andrew Hoffman, Ghanshyam Pilania, Piyush Pandita,
  Sayan Ghosh, Nathaniel Mckeever, Liping Wang
Categories: stat.ML cs.LG
Comments: 27 Pages,9 Figures, 3 Supplementary Figures, 2 Supplementary Tables
\\ ( https://arxiv.org/abs/2402.04146 ,  9716kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10009
replaced with revised version Fri, 16 Feb 2024 09:49:10 GMT   (7103kb,D)

Title: Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion
Authors: Hila Manor and Tomer Michaeli
Categories: cs.SD cs.LG eess.AS
Comments: Examples and code available in
  https://hilamanor.github.io/AudioEditing/
\\ ( https://arxiv.org/abs/2402.10009 ,  7103kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10118
replaced with revised version Fri, 16 Feb 2024 08:52:29 GMT   (615kb,D)

Title: Reusing Softmax Hardware Unit for GELU Computation in Transformers
Authors: Christodoulos Peltekis, Kosmas Alexandridis, Giorgos Dimitrakopoulos
Categories: cs.AR cs.LG
Comments: AICAS 2024
\\ ( https://arxiv.org/abs/2402.10118 ,  615kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
